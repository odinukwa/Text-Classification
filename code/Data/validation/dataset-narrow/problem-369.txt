After the process is over, I run queries against all the tables. Everything goes fine with 9 of them, but with one of the tables the executed query hangs. The next day, when we run the same query against the same table, the execution takes only a moment. I have no idea what the problem is. I tried not adding indexes but it didn't help. I know he way we copy table content might seem weird. 

ACCOUNTING,FINANCE,HR columns of the the CONFIRMATIONS table correspond to the USERID column in the USERS table. I need a query that will put USERNAME corresponding the USERID if in any of the ACCOUNTING,FINANCE,HR columns have a value in it. So suppose we have the following data in the tables: 

I know that there's a one to one relationship between the document types and Instructions, so according to database normalization I should not have a separate table for it. But on the other hand I'll have to include the same columns in two different tables. What do you think? P.S. I won't be able to create a foreign key constraint between Instructions table and the two other. 

The Object Explorer view will not enumerate any of the tables or other objects in the AlwaysOn databases. Attempting to expand them results in an access denied message. You cannot have a Read Only and a non-Read Only connection to the same listener open at the same time. Intellisense for object names in the database does not work. (Oddly enough, Objects get enumerated just fine in the Query Designer that you can launch using Design Query in Editor... from the right-click menu.) Caveat 3 from the prior versions' caveats still applies. 

Third-Party Products LinqPad stores the whole connection string including Application Intent and the database when you save a connection and therefore might be a viable option for performing Read Only queries against Always On databases. 

Unfortunately, you cannot install SQL Express without Administrator rights on the machine and it's not something you can set in the config file or on the command line. If being able to install without Administrator rights is a requirement, you may want to investigate using SQL Server Compact Edition. See the following MSDN topics for more information: 

I have two tables, namely Employees and Payments. Below is the contents of the two: The Employees table 

I think it's obvious what it does but to make it clear let me give a brief explanation. In my table there's DOCNUMBER column. I assign a value to this column in Before Update trigger. And in the above trigger, which of course should fire after the Before Update Trigger, I check if the DOCNUMBER has value (and if it's for the first time) and if it does, I write that value to another table called . The problem is, the above trigger sometimes, although very rarely, fails to insert the to the table. That is, sometimes I see a record in table with DOCNUMBER value that does not exist in table.The table has only one column called DOCID and it's part of a UNIQUE KEY constrsraint. Do you see any problem with this trigger? EDIT: I think you'll get a better idea if I post the BEFORE TRIGGER too: 

From this link: $URL$ Cardinality and Modality are the indicators of the business rules around a relationship. Cardinality refers to the maximum number of times an instance in one entity can be associated with instances in the related entity. Modality refers to the minimum number of times an instance in one entity can be associated with an instance in the related entity. Cardinality can be 1 or Many and the symbol is placed on the outside ends of the relationship line, closest to the entity, Modality can be 1 or 0 and the symbol is placed on the inside, next to the cardinality symbol. For a cardinality of 1 a straight line is drawn. For a cardinality of Many a foot with three toes is drawn. For a modality of 1 a straight line is drawn. For a modality of 0 a circle is drawn. zero or more [b4] 1 or more [b5] 1 and only 1 (exactly 1) [b6] zero or 1 [b7] Cardinality and modality are indicated at both ends of the relationship line. Once this has been done, the relationships are read as being 1 to 1 (1:1), 1 to many (1:M), or many to many (M:M). 

The title was best I could find to explain my question, I don't think it helps though. Anyways, I have two tables: 

EXP(SUM(LN(YR.RATE))) is just a trick to get the multiplication of the column values as there's no ready function available in Oracle. 

Sorry for the title but I could not find anything better that could suit my needs. I'm designing the database of a Document Management (Circulation) application. There are two types of documents - A and B. I've created a table one for each. Any document independent from its type has Instructions information that has to be attached to it. Here are the details of the Instrcutions: 

Now if I want to show only the first ten rows between 50 and 60, the only way I can think of is to first run the above query with ROWNUM pseudocolumn and then select from the result of this query. Something like this: 

Now, there might be multiple entries for one customer_id value in the table B. I need the latest date to be updated into the table A. I wonder, if I order the result by last_login_date column in Ascending order, will the merge statement eventually update the record in A with the latest last_login_date? 

SQL Management Studio (versions 2016 or later) SQL Server Management Studio 2016 or higher can connect with Read Only Application Intent (using the same 6 steps as prior versions) and it does store the Additional Connection Parameters. There are still some caveats: 

As long as they are not nested, you should be fine. @@FETCH_STATUS is set at the time the FETCH NEXT is performed. @@FETCH_STATUS is global to the session, not the entire server. There is no equivalent to SCOPE_IDENTITY() for @@FETCH_STATUS. If you need to nest looped FETCH NEXT operations, make sure that your outer FETCH NEXT is performed after the inner loop completes. If you need to stack FETCH NEXT operations or perform a FETCH NEXT before an inner loop containing a FETCH NEXT, you can DECLARE a temporary variable of type outside the loop, SET the temporary variable equal to @@FETCH_STATUS immediately after your FETCH NEXT statement, and then use the temporary variable as your loop control instead of @@FETCH_STATUS. 

You feel like they should be separated because that makes perfect sense in the relational world. But as you said in the NoSql world and in MongoDB in particular, you want to group like-items together. I've not done extensive research on Mongo, but have spent some time with the online classes and I believe the answer is to store them together. You probably realize there is no such thing as a join in Mongo and therefore if you wanted to get 100 rows and get their corresponding images, you'd have to get the IDs for the 100 rows and then get 100 rows by their identifier (object_id or whatever). In other words, you have to do manual joins. 

You can look in the system dmv for os performance counters. With many of the counters, you have to collect the value, wait some time, then collect it again to get the number of transactions that have happened during that time frame. Something like this will work: 

First I rename the original tables in schema B Then, using SQL Developer I copy the table to copy from schema A to B. (I right click the table to copy and select Copy and choose the destination schema) Then I create all the indices that exist in the original tables.(Copying tables does not copy indexes, so you have to create them manually) 

I have two schemas namely A and B. I regularly copy contents of 10 tables from A to B. Here's is how I do it 

But this way, I'll end up first fetching all the employees whose positions are 1 or 3 and then extract the 10 rows among them. I find this a bit inefficient. Is there any way to achieve this without making full scan first? 

I need to select PERSON_ACCOUNT.PAID_YEAR, PERSON_ACCOUNT.PAID_AMOUNT,PERSON_ACCOUNT.PAID_AMOUNT * multiplication of the YEARLY_RATES.RATE WHERE YEARLY_RATES.RATE_YEAR >= PERSON_ACCOUNT.PAID_YEAR. So for instance if I have the following data in PERSON_ACCOUNT 

I've been asked this question but neither I seem to be able to answer it on my own, nor can I find anything related on the web. So what are the cases that might cause a runtime exception when committing a transaction in Oracle?The only thing that I can think of is the low disk space. Are there any other? 

If the SQL Browser Service is active, you can query the service for SQL Instances with the PowerShell code below. It implements the following commandlets to perform the queries: 

SQL Management Studio (versions prior to 2016) Unfortunately, there are some caveats that make using Application Intent in SQL Management Studio somewhat painful: To connect manually with ReadOnly intent, after bringing up the Connect to Server dialog from the Object Explorer, users must remember to: 

Click Options >>. Go to the Additional Connection Parameters tab. Enter the additional parameter as ApplicationIntent=ReadOnly; (Note: Users must not click the Options << button after entering the Additional Connection Parameters or the parameters will be lost.) Click Connect. Always launch query windows by right-clicking on the desired database in the Object Explorer view and choosing New Query to avoid running into the #3 caveat below. 

The SQL Server is running normally and the SQL Browser service is started. Querying the SQL Browser service with another tool on the same server yields the expected results. I can run regular SQL queries with SQLCMD and get the expected results. I was able to verify that the output above is the same on a workstation where I caused SQLCMD -L to receive no responses at all by enabling the firewall; however, the firewall is not an issue on this server. I've checked the settings and, as stated previously, my other tool gets SQL Browser Service responses to a broadcast just fine. Also, running WHERE reports that the path to SQLCMD is , which is correct per File Locations for Default and Named Instances of SQL Server. Does anyone have any idea why SQLCMD is doing this and how to fix it? 

The stored procedure sp_addsubscription allows for you to specify that you want all articles (i.e. tables) or just one. If you want a subset of articles in a publication, you just need to run the sp_addsubscription once for each article that you want. It looks like the sp_addsubscription defaults to @article = 'all' My suggestion is to go through the New Subscription Wizard and have it generate the script only. Then you can go in and modify the script as you need it. You would need to add multiple sp_addsubscription lines. 

I think source1 has the answer. He (Aaron) states that even when you get an "accurate" count, you could be blocking users trying to write to the table. As soon as you put your eyes on the result, it could be immediately inaccurate as writers were trying to put data in your table while you blocked them for the accurate count. So, I believe using partition_stats works fine for what you want. I wouldn't be afraid to test it though and it might be an interesting test at that. You could create a db of 1000 empty tables. Then a routine that constantly inserts and deletes data in random tables. Then run your partition query and run your count query and compare. I think what might work best is to use partitions and accumulate your data in a table and over periods of days or weeks, any table with consistently zero results is unused or abandoned. 

I have a server with three installed SQL Server 2014 instances with Service Pack 1 (build number is 12.0.4213.0) installed. When I run SQLCMD -L on the server I am getting the bizarre response below. 

Although you can get SQL Management Studio to connect with Read Only Intent, it does not store the Additional Connection Parameters when a connection is added to Registered Servers. Behavior when hand editing the locally registered servers in the RegSrvr.xml file to add the Application Intent is extremely inconsistent and will be overwritten any time a change is made through the GUI making this workaround unreliable. The Always On database must be selected before the query window is opened; otherwise, the connection gets routed to the primary server. If you attempt to select the database using the query window's drop down after the query window has already been opened to a non-Always On database, you will get an error dialog. If you try to change the database to an Always On database with a USE statement after the query window has already been opened to a non-Always On database, the results look like this when you attempt to execute the SQL query: