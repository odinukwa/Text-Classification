The foundation of our task class hierarchy. Note that the most generic base class doesn't even presuppose any published events, which makes the framework extensible to tasks other than time measurement. Our time-related tasks share the following abstract base: 

The remainder is the setup for the DemoApp, which includes rather boring stuff like generating random view positions. 

The isn't actually much more complicated. I dropped the in favour of more obviously readable code. It has the added benefit of making the code short (except for the constants of course). Functionally, we show 

A bug in your line 97 (the should be ); your code might never complete, for some inputs The birectional search does not in practice improve the speed (note; worst-case behaviour will be improved, but at the cost of complexity) yields significant speed up (on average, there are <=10 relations per vertex) Destructing the maps takes significant time. This is one of those rare occasions where I'd suggest purposefully leaking the memory since you know the program will be terminating anyways. 

While cleaning up the code (remove globals, duplication, unused code, bad naming) and running some benchmarks using the supplied input file, I found: 

A simple view to implement would be the digital timer view. We already have the and utilities. Let's decide that the view should show the total elapsed time regardless of lap-times and it should indicate how many laps have been recorded: 

you allocate space for the whole triangle but you only ever use the last two rows. So you only need to store two rows and alternate between them. This has the added benefit that both rows will be hot in cache all the time, avoiding cache misses as you go from line to line. It will also avoid some extra indirections in your inner most loop. Better algorithm The first thing we need to realize is that the problem is kind of recursive (I can't remember the proper term off the top of my head). What I mean by that is that if a base for crumbles, then the all bases for any will also crumble. This means that by solving the easier problem we get information about a lot of combinations for that will crumble. (1) The second thing to realize about this problem is that if crumbles, then will also crumble. And if doesn't crumble and has top value then will also not crumble and have the same top value . (2) The third thing is that for the base the top value will be strictly larger than . This is a useful stopping condition. (3) Knowing the above, you have to unrestrainedly search for the combination of base elements that produces the smallest top value as result. Use a greedy search for the smallest top value and terminate when all the nodes remaining fulfil (3). The difficult part is to generate the nodes to be searched. This can be done effectively by starting with small and then combinatorially incrementing the base elements and adding an additional base element to create triangles with base for any triangle that doesn't crumble. This exploits (1) to prune large sets of crumbling triangles from the start. One should also take care to not generate symmetry pairs to triangles that have already been tried to exploit (2). It is useful to store the base and right edge of the triangle in addition to a of all the values in the triangle. I.e. for the triangle: 

As in the last comment on the old answer, we'll be using to create an interactive terminal UI (TUI). 

Using Memory Mapped Files And Spirit Taking the approach from my answer over at [SO]: $URL$ I changed the implementation of into the equivalent: 

¹ I am aware of the usual implementation in hardware stopwatch devices where the operation modes form a state machine. I also realize that the implementation in code tried to mimick this. Unfortunately, not only did it fall short, it also conflated things with the UI side of things. Consider this answer a finger exercise on my part. 

- Bonus To complete the picture, here's how a uni-directional BFS looks based on the same building blocks (, ): 

Post Scriptum These are random notes. Specifically, I didn't mention important things (like brace style, const-correctness, use of raw arrays etc.) 

Implementing the various timer operations on this is peanuts. Let's for example do a random durations generator in 3 lines of code: 

As you can see, we made the intent of the loop clear: we swap the contents of the queue with an empty one. We then loop over all previously queued nodes (the ). At the end of the step there might be new queued items, but none of the originally queued nodes will be. If there weren't any items to begin with, we return . 

Okay, after I worked my way through the original code, a few things have become clearer. Since I have never done programming with I was eager to try my hand at a better design. Here it comes. It's a sketch only in the sense that I didn't create separate translation units. That is basically a tedious exercise and left for the reader. However, it does implement stopwatch (including lap times and reset), countdown and a bonus "random timer" task. 

Sample Variance vs Population Variance There are two different ways computing a variance for a data set depending on if the data set contains all the values from the population or if the data set is just a sampling of values from some larger population. If you ask 100 people on the street how many friends they have, you get a sample variance. If you ask all the people in the world, you get a population variance. If you ask all the people in your city, you get a sample variance for the world and a population variance for your city. There is a subtle difference that if you don't have the entire population, you will get a bias error in the variance that needs to be corrected for by using sample variance formula. Population Variance Given a population of size \$N\$ and we have \$N\$ samples called \$x_1,...,x_N\$ the population variance is: $$\sigma^2=\frac{1}{N}\sum^N_{i=1}\left(x_i -\mu\right)^2 = \frac{1}{N}\sum_{i=1}^N\left\{x_i^2 - 2x_i\mu+\mu^2\right\}$$ lets simplify it a bit to: $$=\frac{1}{N}\sum_{i=1}^Nx_i^2 - 2\mu\sum_{i=1}^Nx_i + N\mu^2\qquad(1)$$ where \$\mu\$ is the population mean: $$\mu=\frac{1}{N}\sum^N_{j=1}x_j \qquad(2)$$. Inserting \$(2)\$ into \$(1)\$ gives: $$\sigma^2=\frac{1}{N}\sum_{i=1}^Nx_i^2 - 2\frac{1}{N}\sum^N_{i=1}x_i\sum_{j=1}^Nx_j + N\left(\frac{1}{N}\sum^N_{j=1}x_j\right)^2$$ which again simplifies to: $$\sigma^2=\frac{1}{N}\sum_{i=1}^Nx_i^2 - \frac{1}{N}\left(\sum^N_{i=1}x_i\right)^2 $$ if we realise that the summation indexes \$i\$ and \$j\$ can be interchanged. Sample variance With the same population as above, this time we only have \$K<N\$ samples: \$y_1,...y_K\$. We let the (biased) sample variance be: $$\sigma_y^2 = \frac{1}{K}\sum^K_{i=1}\left(y_i -\bar{y}\right)^2$$ where \$\bar{y}\$ is the sample mean $$ \bar{y}=\frac{1}{K}\sum^K_{j=1}y_j$$. Note that the formula for the sample mean and population mean is the same technically the same: "take the arithmetic average of all your samples". However they differ semantically. The population mean is the "true" mean of the entire population, the sample mean is just an estimation of the "true" mean and contains some error (unless \$N=K\$ but then you have a population mean). To realise that \$\sigma_y^2\$ is biased we have to get dirty with some probability theory, hold on to your britches! We use something called "the expected value" which in layman's terms means "the average of very many attempts". So if you throw a fair 6 sided dice, we will call the outcome of one throw for \$z\$. We call \$z\$ a random variable (it's a discrete random variable to be exact) as the it doesn't have a known value, it's random. However we know the values it can take and the probability of each value. Thus we can calculate the expected value as the weighted average of each value and their probability. So for our population, the expected value of any sample \$x\$ taken randomly is: $$E\left[x\right]= \sum^N_{j=1}x_jP\left(x_j\right)$$ where \$P\left(x_j\right)\$ is the probability of \$x_j\$ being the value of the random variable. As we are drawing samples from the population randomly we will assume that the distribution is uniform and thus: \$P\left(x_j\right)=\frac{1}{N}\$ hence we get: $$ E\left[x\right] = \frac{1}{N}\sum^N_{j=1}x_j =\mu $$ How does this help us? Well we computed the (biased) sample variance for one possible sample set of the population. If we use the above to compute the variance for all possible sample sets from the population and average them (the expected sample variance), we will get the variance that we expect to get if we randomly pick a sample set from the population. We will compare this value to the "true" population variance \$\sigma^2\$. Before we go though, there are some ground rules. Let \$a\$ and \$b\$ be random variables and \$c\$ a constant, then: $$E\left[a+b\right] = E\left[a\right]+E\left[b\right]$$ and $$E\left[c*a\right] = c*E\left[a\right]$$ these follow easily from the definition. And then we have $$E\left[E\left[a\right]\right]=E\left[a\right]$$ which means that the expected value of the expected value is, well the expected value which you can prove to yourself or accept it intuitively. Also we will need the very definition of variance and it's expansions: $$Var\left(x\right)=E\left[\left(x-\mu\right)^2\right] = E\left[x^2-2x\mu + \mu^2\right]$$ remember that: \$E\left[x\right] = \mu\$ $$Var\left(x\right)=E\left[x^2\right]-2E\left[x\right]E\left[x\right] + \left(E\left[x\right]\right)^2=E\left[x^2\right]-\left(E\left[x\right]\right)^2$$. Okay so we have: $$\sigma_y^2 = \frac{1}{K}\sum^K_{i=1}\left(y_i -\bar{y}\right)^2 = \frac{1}{K}\sum^K_{i=1}\left(y_i -\frac{1}{K}\sum^K_{j=1}y_j\right)^2$$ because all the samples \$y_1,...y_K\$ are selected randomly, \$\sigma_y^2\$ is a random variable. The expected value is: $$E\left[\sigma_y^2\right] = \frac{1}{K}\sum^K_{i=1}E\left[\left(y_i -\frac{1}{K}\sum^K_{j=1}y_j\right)^2\right]$$ (remember the ground rules, moving constant and expected sum is sum of expected). We'll expand the square: $$E\left[\sigma_y^2\right] = \frac{1}{K}\sum^K_{i=1}E\left[y_i^2 -\frac{2y_i}{K}\sum^K_{j=1}y_j + \frac{1}{K^2}\sum^K_{j=1}y_j\sum^K_{j=1}y_j\right]$$ now things will get a bit messy. We will move the \$y_i\$ term into the sum as it is independent of the summation index and we will change the summation index on the last sum from \$j\$ to \$l\$: $$E\left[\sigma_y^2\right] = \frac{1}{K}\sum^K_{i=1}E\left[y_i^2 -\frac{2}{K}\sum^K_{j=1}y_iy_j + \frac{1}{K^2}\sum^K_{j=1}y_j\sum^K_{l=1}y_l\right]$$ now we will remove the elements from those sums that have the same index and make them into separate terms: $$E\left[\sigma_y^2\right] = \frac{1}{K}\sum^K_{i=1}E\left[y_i^2 -\frac{2}{K}\left(y_i^2 + \sum^K_{j\neq i}y_iy_j\right) + \frac{1}{K^2}\left( \sum^K_{j=1}y_j\sum^K_{l\neq j}y_l + \sum_{j=1}^Ky_j^2\right)\right]$$ now we propagate the \$E\$ by using the rules described earlier: $$E\left[\sigma_y^2\right] = \frac{1}{K}\sum^K_{i=1}\left\{E\left[y_i^2\right] -\frac{2}{K}\left(E\left[y_i^2\right] + \sum^K_{j\neq i}E\left[y_iy_j\right]\right) + \frac{1}{K^2}\left(\sum^K_{j=1}E\left[y_j\right]\sum^K_{l\neq j}E\left[y_l\right] + \sum_{j=1}^KE\left[y_j^2\right]\right)\right\}$$ collect terms (note that: \$\sum_{i=1}^KE\left[x\right]=KE\left[x\right]\$ ): $$E\left[\sigma_y^2\right] = E\left[y_i^2\right]\frac{K-1}{K} -\frac{2}{K}\left(\sum^K_{j\neq i}E\left[y_iy_j\right]\right) + \frac{1}{K^2}\left(KE\left[y_j\right]\left(K-1\right)E\left[y_l\right]\right)$$ almost there, in the general case \$E[ab]\neq E[a]E[b]\$. However when \$a\$ and \$b\$ are independent random variables the equality holds. Are \$y_j\$ and \$y_i\$ independent? Well for small populations picking one \$y_j\$ doesn't affects the probability of \$y_i\$ as all remaining values are equally likely and we have \$j\neq i\$ in our summation. (To any statisticians out there, yeah I know it's a weak argument I don't remember the long proof, you'll just have to believe me here). So if we buy this argument $$E\left[y_iy_j\right]=E\left[y_i\right]E\left[y_j\right]=\left(E\left[y_i\right]\right)^2$$ then the whole shebang reduces to: $$E\left[\sigma_y^2\right] = E\left[y_i^2\right]\frac{K-1}{K} -\frac{2}{K}(K-1)\left(E\left[y_i\right]\right)^2 + \frac{1}{K}\left(K-1\right)\left(E\left[y_i\right]\right)^2$$ then reduce further $$E\left[\sigma_y^2\right] = \frac{K-1}{K}\left(E\left[y_i^2\right] -\left(E\left[y_i\right]\right)^2\right)$$ remember \$\sigma^2=E\left[x^2\right]−(E\left[x\right])^2\$ and finally we have: $$E\left[\sigma_y^2\right] = \frac{K-1}{K}\sigma^2$$ because \$E\left[x_i\right] =E\left[y_j\right]\$. Which essentially means, calculating the sample variance like above will have a systematic error factor of \$\frac{K-1}{K}\$ when compared to the true population variance. This is why we use the unbiased sample variance: $$s^2 = \frac{K}{K-1}\sigma_y^2 = \frac{1}{K-1}\left(\sum_{i=1}^Ky_i^2 - \frac{1}{K}\left(\sum^K_{i=1}y_i\right)^2\right)$$ Your code is Sample Variance We have the population variance: $$\sigma^2=\frac{1}{N}\sum_{i=1}^Nx_i^2 - \frac{1}{N}\left(\sum^N_{i=1}x_i\right)^2 $$ and the sample variance: $$s^2 = \frac{1}{K-1}\left(\sum_{i=1}^Ky_i^2 - \frac{1}{K}\left(\sum^K_{i=1}y_i\right)^2\right)$$. Compare to your code here: