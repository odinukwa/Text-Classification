Or Meir’s comment is almost but not quite right, since it would be satisfied by a proof that P vs. NP is not independent even if the prover didn’t know which. A corrected version is “X is either the hash of a proof that P = NP or the hash of a proof that P $\ne$ NP”, where hash is SHA256, say. Running that statement through a zero knowledge proof system gives the desired evidence. However, if I was given such a proof, I would assign higher probability to someone having found a bug in the logical system being used. It would be quite difficult to surmount that qualification in practice, since bugs in proof systems are fairly common. 

Here's some empirical data for question 2, based on D.W.'s idea applied to bitonic sort. For $n$ variables, choose $j - i = 2^k$ with probability proportional to $\lg n - k$, then select $i$ uniformly at random to get a comparator $(i,j)$. This matches the distribution of comparators in bitonic sort if $n$ is a power of 2, and approximates it otherwise. For a given infinite sequence of gates pulled from this distribution, we can approximate the number of gates required to get a sorting network by sorting many random bit sequences. Here's that estimate for $n < 200$ taking the mean over $100$ gate sequences with $6400$ bit sequences used to approximate the count: It appears to match $\Theta(n \log^2 n)$, the same complexity as bitonic sort. If so, we don't eat an extra $\log n$ factor due to the coupon collector problem of coming across each gate. To emphasize: I'm using only $6400$ bit sequences to approximate the expected number of gates, not $2^n$. The mean required gates does rise with that number: for $n = 199$ if I use $6400$, $64000$, and $ 640000$ sequences the estimates are $14270 \pm 1069$, $14353 \pm 1013$, and $14539 \pm 965$. Thus, it's possible getting the last few sequences increases the asymptotic complexity, though intuitively it feels unlikely. Edit: Here's a similar plot up to $n = 80$, but using the exact number of gates (computed via a combination of sampling and Z3). I've switched from power of two $d = j-i$ to arbitrary $d \in [1,\frac{n}{2}]$ with probability proportional to $\frac{\log n - \log d}{d}$. $\Theta(n \log^2 n)$ still looks plausible. 

Amenta, Nina, Sunghee Choi, and Günter Rote. "Incremental Constructions con BRIO." (2003). Buchin, Kevin. "Constructing Delaunay triangulations along space-filling curves." Algorithms-ESA 2009. Springer Berlin Heidelberg, 2009. 119-130. 

Buchin [2] showed that under "reasonable" assumptions, (polynomial point spread) incremental construction of Delaunay triangulations using the biased randomized ordering of Amenta et al. [1] with a space filling curve takes $O(n \log n)$ even when points are located by walking from the last inserted point. This reduces to $O(n)$ for uniformly random points if the BRIO jumps are treated carefully. Question: Are there any "simple to compute" orders which are worst case $O(n \log n)$ for all general position inputs? I am leaving "simple" intentionally vague, but roughly I mean simpler than computing the Delaunay triangulation itself (from which an optimal order is easy to derive). 

There is a randomized algorithm that takes an input $n > 0$ and if $P$ is false produces a deterministic proof of $\neg P$ with probability at least $1-2^{-n}$. Someone has run the algorithm for, say, $n = 100$, and not disproved the theorem. 

Given $n$ inputs $x_0, \ldots, x_{n-1}$, we construct a random sorting network with $m$ gates by iteratively picking two variables $x_i, x_j$ with $i < j$ and adding a comparator gate that swaps them if $x_i > x_j$. Question 1: For fixed $n$, how large must $m$ be for the network to correctly sort with probability $> \frac{1}{2}$? We have at least the lower bound $m = \Omega(n^2 \log n)$ since an input that is correctly sorted except that each consecutive pair is swapped will take $\Theta(n^2 \log n^2)$ time for each pair to be chosen as a comparator. Is that also the upper bound, possibly with more $\log n$ factors? Question 2: Is there a distribution of comparator gates that achieves $m = \tilde{O}(n)$, perhaps by choosing close comparators with higher probability? 

For the promise problem where we know a succinct circuit of given size exists, what is the best worst case algorithm? Is it possible to do significantly better than enumerate-all-circuits? Are there practical circuit simplification algorithms which are both incremental and complete, in that they start with simple transformations (removing partial common factors, etc.) but are guaranteed to eventually find the minimal circuit? Do the lifting techniques used to reducing polynomial factoring and irreducibility testing to bivariate polynomials extend to finding circuits in any useful way? 

A simple variant of binary search takes $O(\log a(n))$ time assuming $a(n) = O(n)$ is monotonic. To compute $f^{-1}(n)$, find $k$ s.t. $$f(n-2^k) \le n \le f(n+2^k)$$ by trying $k = 0, 1, 2, \ldots$ until the bound holds. This takes $O(\log a(n))$ steps and produces a window of size $O(a(n))$ which can be finished off with binary search. Unfortunately, this is the best you can do without a stronger condition on $f(n)$. Proof sketch: assume that after $O(1)$ queries we've reduced to an interval $[a,b]$ s.t. $$\begin{aligned} b-a &\ge c a(n) \\ f(b)-f(a) &= \Theta(a(n)) \end{aligned}$$ Then $f(n)$ within $[a,b]$ is an arbitrary monotonic function subject to the end constraints, and we can adversarily choose $f$ to make $\Theta(\log a(n))$ optimal. 

What is missing from the analogy is some notion of the relative distances involved. Let's replace Alaska in our analogy with the moon: You're an explorer, searching for a bridge between the North American and Asian continents. For many months you have tried and failed to find a land bridge from the mainland United States area to Asia. Then you discover that the mainland US is connected by land to the moon. You are already confident that the moon is a vast distance away from Asia, so you can now be confident that North America is also a vast distance away from Asia by the triangle inequality. 

C++ concepts map to recursively enumerable languages. Since the C++ type system is Turing complete, any property of types that can be interrogated during template instantition (size, parameters, etc.) can be run through an arbitrary program simulated in the type system. 

Goldreich et al.'s proof that three colorability has zero knowledge proofs uses bit commitment for an entire coloring of the graph in each round [1]. If a graph has $n$ vertices and $e$ edges, a secure hash has $b$ bits, and we seek error probability $p$, the total communication cost is $$O(ben \log(1/p))$$ over $O(1)$ rounds. Using a gradually revealed Merkle tree, the total communication can be reduced to $O(be \log n \log (1/p))$ at the cost of increasing the number of rounds to $O(\log n)$. Is it possible to do better than this, either in terms of total communication or number of rounds? 

Choose a random oracle $f : \{0,1\}^\ast \to \{0,1\}$, and define the logic $ZFC^f$ by adding a fresh symbol $g$, an axiom that $g$ has the correct type, and one axiom $g(s) = f(s)$ for each $s \in \{0,1\}^\ast$. This is a countably infinite set of axioms since $f$ is not a symbol in $ZFC^f$. Within $ZFC^f$ we can define $g$-oracle Turing machines in the standard way, for example by using a second tape for the input to $g$ and letting the machine branch on the value of $g$ on the second tape. We then have the complexity classes $P^g$, $NP^g$, etc. Since $f$ was chosen at random in the metalogic, $P^g \ne NP^g$ is true with probability 1. Question 1: Is $P^g = NP^g$ independent over $ZFC^f$? The proof seems immediate: any finite proof in $ZFC^f$ can interrogate at most finitely many values of $g$. However, I am not confident the definitions are sensible. Question 2: Is there a good reference exploring this type of logic + random oracle construction? 

Say we have $N$ bits that we'd like to store in an $M$-bit error correcting code, where $M > N$. Given $\epsilon > 0$, as long as $N > N_0(\epsilon)$ we can recover the original bits as long as any $N(1+\epsilon)$ of the $M$ bits are correct. Now say the bits are ordered in decreasing order of importance. Can we pick a single $M$-bit code so that if $K > K_0(\epsilon)$ out of the $M$ bits are uncorrupted, we can recover the first $K(1 - \epsilon)$ original bits correctly? Here the difference is that the code is independent of $K$. 

Are existential types on top of basic Hindley-Milner sufficient to express $\nu Obj$? Am I correct that existentials are harder to deal with than $\nu Obj$ ("too much more expessive")? Is there something weaker than existentials that still captures $\nu Obj$, without an object oriented focus? 

For quantum circuits, once the gate error is below a threshold, the error probability of an entire computation can be driven exponentially small with polylog costs in time and space: $URL$ The classical version should be entirely analogous, and was presumably proven long before the quantum case. Who proved the classical version? 

There's a good chance this question is independent of ZFC for most programming languages. In particular, I'd expect it to be true of any language where the shortest Quine is longer than the Kolmogorov complexity decidability bound, the integer $n$ such that no string can be proven to have Kolmogorov complexity $> n$. I don't know to prove this, since even if you could express a Quine by a shorter program it wouldn't be a Quine, but I would expect a similar result to apply. However, if the above argument were true, the argument itself might also be unprovable in ZFC, since it would require showing that no Quines exist below the complexity bound. 

If we have a single straight line program expressing a multivariate polynomial equation with integer coefficients, the Schwartz-Zippel lemma gives a simple randomized algorithm for deciding whether the equation is always true. We can similarly decide if a single polynomial inequation is always true over $\mathbb{C}$, since $p(z_1,\ldots,z_n) \ne 0$ for all $z_i$ iff the polynomial is a nonzero constant. Does the simplicity of randomized checking extend to any systems of multivariate polynomial equations? In particular, is there a simple algorithm for deciding whether $$p(z_1,\ldots,z_n) = 0 \implies q(z_1,\dots,z_n) = 0$$ where $p,q$ are straight line programs over $\mathbb{C}$ with integer coefficients? I'm fairly sure randomness does not provide simple algorithms if we go to arbitrary systems of equations and inequations, but I'm curious where the boundary is between easy and hard. 

I have a vague memory of a series of papers working to reduce the constant factor in the number of comparisons for deterministic linear time median finding, using increasingly elaborate (but interesting!) techniques to do so. In particular, I remember one of the papers using the term "green forests" for a data structure storing information from previous comparisons (in an attempt to reuse them and avoid future comparisons). Does this ring any bells? "median finding green forests" turns out to be a terrible search term. 

Has such a soundness bug fix ever caused a major proof to fail, without modifying the proof? If (1) is true, were major modifications ever required to fix the proof? If (2) is true, has anyone proved a wrong major theorem due to a soundness bug? 

Odersky et al.'s $\nu Obj$ calculus [1] adds just enough dependent typeness on top of object oriented programming to express interfaces that define types (and consequently module systems and other fanciness). Although still undecidable, it seems to have proven itself practical as the base of a language without the need for interactive theorem proving. I am curious about non-objected oriented type systems which are (1) at least as expressive as $\nu Obj$, in the sense that $\nu Obj$ can be embedded within them and (2) not "too much" more expressive (left intentionally vague), and therefore hopefully more tractable from a type inference / checking perspective. For example, a sufficiently powerful non-objected system should be able to type the vtable concept, and then express "objects" as tuples the first element of which is a vtable. Unfortunately, the obvious way to do this requires existential types, which I (perhaps naively) think of as more complicated than $\nu Obj$. So, I suppose my question splits into three parts: 

Given two matrices $X \in \mathbb{R}^{m \times k}$, $Y \in \mathbb{R}^{n \times k}$, maximum inner product search (MIPS) asks for the largest $l$ entries of $X Y^T$. Typically $k \ll m, n$ (many small dot products). In the approximate version, we ask for entries within a factor $\alpha < 1$ of the true largest. Unfortunately, Ahle et al. show that an approximate solution to MIPS in subquadratic time $O(k^{O(1)} (mn)^{1-\epsilon})$ for $\epsilon > 0$ would contradict the strong exponential time hypothesis. Terminology note: Ahle et al. calls it inner product similarity join (IPS) instead of MIPS. Ahle et al. consider only classical complexity. However, it is easy to beat the subquadratic limit using a quantum computer: applying Grover to the $m \times n$ search space gives $\tilde{O}(k \sqrt{m n})$ for $l = 1$. Question: Is $\tilde{O}(k \sqrt{m n})$ the right quantum complexity for $l = 1$? What about for larger $l \ll m, n$? 

Most (all?) proof assistants have soundness bugs fixed on occasion. However, from those I've seen these bugs are usually difficult to come across unintentionally, and results proved before the bug is fixed generally hold up after the fix. Three questions, in order of strength: 

Acer et al. "Finite-time Analysis of the Multiarmed Bandit Problem" show that the Upper Confidence Bound 1 (UCB1) algorithm has expected regret bounded by $$\left[ 8 \sum_{i: \mu_i < \mu^\ast} \frac{\log n}{\Delta_i} \right]+ \left(1 + \frac{\pi^2}{3}\right) \sum_i \Delta_i$$ when run for $n$ steps, where $\mu_i$ are the means of the $K$ arms, $\mu^\ast = \max_i \mu_i$, and $\Delta_i = \mu^\ast - \mu_i$. However, this bound explodes as any $\Delta_i \to 0$, i.e. when there a nearly optimal non-best arm. Certainly the true expected regret does not go singular in this case (for any strategy), and intuitively I would expect UBC1 to do fairly well if there are several nearly optimal arms. Question 1: Is there a nice regret bound for UCB1 which doesn't have this singularity? Question 2: If no, is there a modified version of UCB1 with such a nonsingular bound? 

It's easy to generate non-natural statements that fit: just pick a large instance of any problem where only an efficient randomized algorithm is known. However, although there a lot of mathematical theorems with "lots of numerical evidence", such as the Riemann hypothesis, I don't know of any with rigorous randomized evidence of the above form. 

This gives $O(be \log n \log (1/p))$ communication over $O(1)$ rounds. Update: Here are details of the Merkle tree construction. For simplicity, expand the graph to have exactly $2^a$ vertices by adding a few disconnected nodes (these do not effect three colorability or zero knowledge). Assume a secure hash function taking any size input and producing $b$-bit outputs. For each Merkle tree, the prover chooses $2^{a+1}-1$ random $b$-bit nonces, one for each leaf and nonleaf of the binary tree. At the leaves, we hash the color concatenated with the nonce to produce the leaf's value. At each nonleaf, we hash the two child value with the nonleaf's nonce to produce the nonleaf's value. In the first round, the prover sends only the root value, which provides no information since it is hashed with the root's nonce. In the third round, no information is conveyed about any unexpanded node in the binary tree, since such a node was hashed with a nonce at that node. Here I am assuming the prover and verifier are both computationally bounded and cannot break the hash. Edit: Thanks to Ricky Demer for pointing out the missing factor of $e$.