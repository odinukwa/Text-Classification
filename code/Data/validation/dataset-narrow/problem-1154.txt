Thus a planar drawing with angular resolution $\pi/2$ around degree 3 vertices will be good for my purpose. For a vertex with degree $d$ in the drawing, the angular resolution around it can be at most $2\pi/d$. The question of whether this is tight has been studied in the past, but I can only find asymptotic results. For example, Malitz and Papakostas prove that any planar graph with maximum degree $d$ can be drawn with an angular resolution of $\alpha^d$. But this result doesn't give good bounds for the case when $d=3$. 

Sorry for creating confusion. Editing the question to accomodate issues raised in the comments... We know that finding the convex hull of $n$ points on a plane has a lower bound of $\Omega(n\log n)$ on its running time. However, if the points are given in the order in which they occur along some simple polygon that has those points as its vertices, then their convex hull can be found in linear time. I find this intriguing because there are probably too many simple polygons that have the given points as their vertices and therefore, intuitively, the order along one of them sounds like a very useless piece of information. And yet, it helps. So my question is, are there other places where the same information helps in bringing down the running time of an algorithm? As a side, I also want to know bounds on the number of permutations of a given set of points on a plane for which there is a simple polygon with those points as its vertices so that the order in which the points occur along the polygon is the same as the order in the permutation. What's known about this? 

I am trying to understand the notion of $\epsilon$-coreset and its relation with sampling bounds of a range space having a finite VC-dimension. Although both of them give an $\epsilon$-approximation sketch of the input data. However, for the later case we know a characterization i.e. if a range space has finite VC dimension, then it can be approximated by a small (constant) size sample. On the other hand, for the case of $\epsilon$-coreset (similar to the concept of VC dimension) is there any characterization known over the problems which can or cannot be sketched by a small size sample? Thanks, 

What are the standard problems we can reduce from to prove $\Omega(n\log n)$ lower bounds? Of course, state problems other than sorting and element distinctness. 

I was reading the paper "Recognizing Well-Paranthesized Expressions in the Streaming Model" by Magniez, Mathieu and Nayak where they give upper and lower bounds on the space required to recognize DYCK(2) languages in the streaming model. Quoting a sentence from the introduction section of the paper - "Using a straightforward one-way communication complexity argument for EQUALITY, we can deduce that DYCK(2) requires linear space for deterministic one-pass streaming algorithms." Can anyone see the straightforward argument? (I can't, but perhaps I am being blind.) Some background - DYCK(2) languages are basically sequences of two kinds of parantheses that are well-formed. For example, the sequence [()] is in DYCK(2), but the sequence [(]) is not. One-way communication complexity of EQUALITY refers to the following problem. Alice and Bob both have one n-bit string each. Alice does some computation on her bits and sends some information to Bob. Bob can do computation on this information plus his bits and has to decide whether his n-bit string is exactly equal to Alice's bit string or not. It can be shown using simple communication complexity arguments that the number of bits that must be communicated is $\Omega(n)$. Communication complexity lower bounds are often used for proving lower bounds for streaming algorithms. The usual scheme is the following. Suppose the problem you want to solve in streaming is P. Pick a communication complexity problem P' for which you know of a lower bound on the number of bits that need to be communicated. Then assume that the problem P can be solved in the streaming model using $o(f(n))$ space. Now in the communication complexity problem P', assume that Alice has as her input the first few bits of the input for P (or some mapping of the first few bits to something else). Then assume that the protocol they use for communication is that Alice runs the streaming algorithm on her input, sends the working space to Bob and then Bob finishes off using his input. In the end, prove that the solution to the streaming problem somehow gives a solution for the communication complexity problem. This will show that the communication complexity problem can be solved with $o(f(n))$ bits of complexity. The trick is to choose $f(n)$ in such a way that this leads to a contradiction. 

Carathéodory's theorem says that if a point $x$ of $R^d$ lies in the convex hull of a point set $P$, then there is a subset $P′ \subseteq P$ consisting of $d + 1$ or fewer points such that $x$ can be expressed as a convex combination of $P′$. A recent result by Barman (see paper) shows an approximate version of the above theorem. More precisely, given a set of points $P$ in the $p$-unit ball with norm $p \in [2,\infty)$, then for every point $x$ in the convex hull of $P$ there exists an $\epsilon$-close point $x'$ (under the $p$-norm distance) that can be expressed as a convex combination of $O\left(\frac{p}{\epsilon^2} \right)$ many points of P. Now, my question is that does the above result implies (or have some connection with) some kind of dimensionality reduction for the points in the convex hull of $P$. It seems intuitive to me (however I don't have a formal proof of it) - as for any point $x$ inside the $P$ there is a point (say) $x'$ in a close neighborhood of $x$ which can be written as convex combination of constant many points of $P$, which in some sense dimensionality reduction of $x'$. Pls let me know if I am able put my question clearly. Thanks. 

Chapter 1 of the book The Probabilistic Method, by Alon and Spencer mentions the following problem: Given a graph $G$, decide if its edge connectivity is at least $n/2$ or not. The author mentions the existence of a $O(n^3)$ algorithm by Matula and improves it to $O(n^{8/3}\log n)$. My question is, what's the best known running time for this problem? Let me describe the improved algorithm. First, decide if $G$ has its minimum degree at least $n/2$ or not. If not, then the edge connectivity is clearly less than $n/2$. Next, if that is not the case, then compute a dominating set $U$ of $G$ of size $O(\log n)$. This can be done in time $O(n^2)$, by an algorithm described in the previous section of the book. Next, it uses the following not very difficult to prove fact: If the minimum degree is $\delta$, then for any edge cut of size at most $\delta$ that divides $V$ into $V_1$ and $V_2$, any dominating set of $G$ must have its vertices in both $V_1$ and $V_2$. Now consider the dominating set $U = \{u_1, \ldots , u_k\}$. Since $G$ has minimum degree $n/2$, any edge cut of size less than $n/2$ must also separate $U$. Thus for each $i\in \{2, k\}$, we find the size of the smallest edge cut that separates $u_1$ and $u_i$. Each of these things can be done in time $O(n^{8/3})$ using a max-flow algorithm. Thus total time taken is $O(n^{8/3}\log n)$. 

Given a set $S$ of sets, what is the fastest algorithm to check if elements of $S$ form an anti-chain with respect to subset ordering? That is, how can I quickly decide if there exists two sets $A$ and $B$ in $S$ such that $A \subseteq B$? 

My question is essentially what comes in the subject line: what is the best way to find an induced cycle basis of a graph (i.e., a cycle basis of the graph in which each cycle is an induced subgraph of the original graph)? I can think of the following naive approach: Let $T$ be any spanning forest of $G$. Now repeat operation (a) until there are no two edges $e_1 = (u_1, v_1)$ and $e_2 = (u_2, v_2)$ so that $e_1, e_2 \in E(G) \setminus E(T)$ and the path in $T$ from $u_1$ to $v_1$ contains the path in $T$ from $u_2$ to $v_2$. Operation (a) adds edge $e_2$ to $T$ and removes one of the other edges in the path from $u_2$ to $v_2$ in $T$. Finally, construct the cycle basis of $G$ according to the resulting $T$. Now, there are several things that I am interested in: 

Beck's theorem is a classical result in discrete geometry which describes about the geometry of points in the plane. The result states that a finite collections of points in the plane fall into one of two extremes; one where a large fraction of points lie on a single line, and one where a large number of lines are needed to connect all the points. Formally, the statement of the theorem is as follows: Theorem: Let $S$ be a set of $n$ points in the plane. If at most $(n-k)$ points lie on any line for some $0 \leq k < n-2$, then there exist $\Omega(nk)$ lines determined by the points of $S$. Now, my question is that if the generalization/variation of the above result is known in the case of higher dimension $(dim\geq 3)$? 

For a planar embedding of a planar graph on a plane with straight edges, define a vertex as a sharp vertex if the maximum angle between two consecutive edges around it is more than 180. Or in other words, if there exists a line passing through that vertex in the embedding such that all the edges incident on that vertex lie on one side of the line, then the vertex is "sharp" otherwise it's not. Also, let us worry only about vertices with degree at least 3. I want to draw planar graphs with few sharp vertices. Has anyone studied such drawings before? In particular, I want to draw planar graphs with max degree 3 such that the number of sharp vertices of degree 3 in the embedding is $O(\log n)$ and the coordinates of the vertices can be written down with a polynomial number of bits. 

We have given a set of $n$ binary vectors each of dimension $d$, i.e. a binary matrix of $d*n$. Our goal is to group vectors which are almost similar, $\forall v_i, v_j\in\{0,1\}^d$, we say $v_i$ and $v_j$ are almost similar, if $HammingDistance(v_i,v_j)<\epsilon$. Since comparing two $d$ bit vectors are expensive, and $n$ is large, we can't afford to do $O(n^2)$ comparisons, we use Locality Sensitive Hashing algorithm to computer similarity preserving matrix in subquadratic time. Now, my question is that how to tune parameters of LSH, i.e. # of hash tables $m$, # of entries per hash table $l$, such that error is minimizes (the number of similar items which hashes to different bucket, and the number of dissimilar items which hashes to same bucket is minimizes). Pls let me know if I am missing something obvious. 

Obviously, one can detect bridge arcs in quadratic time by first finding a path from $S$ to $t$ and, for each hyperarc in that path, checking in linear time if $t$ becomes unreachable from $S$ by removing that hypoerarc. I'm looking for anything that might improve on this quadratic upperbound. 

Are there any results known about the size of smallest context free grammar that generates a set of sets? That is, I am given an alphabet $\Sigma$ as well as a set $S \subseteq \mathbb{P}(\Sigma)$ and I want to find the smallest context free grammar $G$ whose language $L$ has the property that $A \in S \Leftrightarrow (\exists x \in L~\forall l \in \Sigma~[l \in x \Leftrightarrow l \in A])$. For instance, if $\Sigma =\{a,b,c,d\}$ and $S=\{\{d,a,c,b\},\{b,c,a\},\{a,b\},\{a\},\{\}\}$, I can have following grammar $G$ of size 14 which corresponds to the given $S$: $$ \begin{array}{l} G \rightarrow ``dacb"\\ G \rightarrow ``bca"\\ G \rightarrow ``ab"\\ G \rightarrow ``a"\\ G \rightarrow \lambda \end{array} $$ However, I can also use the fact that $S$ is a set and have the following smaller grammar $G'$ of size 13 which corresponds to representing $S$ as $``\{\{a,b,c,d\},\{a,b,c\},\{a,b\},\{a\}\}"$ $$ \begin{array}{l} G' \rightarrow aA\\ A \rightarrow bB\\ A \rightarrow \lambda\\ B \rightarrow cd\\ B \rightarrow c\\ B \rightarrow \lambda \end{array} $$ I am interested in the following questions: 

We have given a weighted graph $G=\{V,E\}$, where $V=\{v_1, v_2,...,v_n\}$, and for all $i,j$, the weight of edges $w(v_i, v_j)\in (0,W)$. And we have also given a weight threshold s $w$ (where $0<w<W$) on the edge as the input. Now, our goal is to find (probabilistically) cliques (of any size >=3) in the graph such that the edges which are in cliques having weight at least $w$, i.e., we need to find cliques having edge weight at least $w$. Pls let me know if I am able to put the question clearly. 

A result of Johnson and Lindenstrauss shows that a set of $n$ points in high dimensional Euclidean space can be mapped into an $O(\frac{\log n}{\epsilon^2})$- dimensional Euclidean space such that the distance between any two points changes by only a factor of $(1\pm \epsilon).$ I am looking for reference for a similar result for Hamming space- i.e. a set of $n$ binary vectors can be mapped into a very low dimensional space such that the hamming distance between any pair of vectors is approximately preserved.