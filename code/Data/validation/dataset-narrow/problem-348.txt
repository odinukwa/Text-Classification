Multiple values in a column are against 1st normal form. It's also absolutely no speed gain, since the tables are to be linked in the database. You have to read and parse a string first, then select all categories for the "Deal". The correct implementation would be a junction table like "DealDealCategories", with DealId and DealCategoryId. Bad hierarchy implementation? Also, an FK in DealCategories to another DealCategory looks like a bad implementation of a hierarchy/tree of DealCategories. Working with trees through a Parent ID (so called adjacency list) relation is a pain! Check for Nested Sets (good to read, but hard to modify) and Closure Tables (best overall performance, but possibly high memory usage - probably not too much for your DealCategories) when implementing hierarchies! 

It's a Multi-Level Marketing system! Jeff Moden has written a a pair of articles here and here on efficient implementation of Hierarchical Reporting against a SQL Server database. There are a number of ways to store the hierarchical information but the two main ones are Adjacency List (each child has a parent foreign key) and Nested Sets (each parent stores details of its child hierarchy). Adjacency List is more intuitive and faster to update, while Nested Sets provides faster reporting. Jeff has explained this topic far better than I can, and developed efficient SQL Server algorithms for converting an large Adjacency List tree into a Nested Set representation, 

If you decide to keep a single entry for both sides of a transaction, then by definition you are engaging in single-entry bookkeeping. This may be the most appropriate solution for some simple applications, but be clear that you are losing all the functional and robustness advantages of double-entry bookkeeping, in exchange for a simpler design. Note that when viewed stand-alone Subledgers (though not their corresponding Journals) are often implemented as single-entry, since a control-account in the General Ledger captures the subledger total and the balancing side of the transactions are in the General Journal (GJ) and General Ledger (GL). You also appear to be confusing the distinct concepts of Ledger and Journal in traditional double-entry bookkeeping. The various Journals (of which there will be numerous specialized varieties for specific common transactions of the business in addition to the General Journal) is a chronological history of all transactions entered into the system. The General Ledger is an ordering by account of all transactions entered into the system, and the various subledgers are an ordering by subledger-code of all transactions entered into the corresponding Journal. Two examples of common Ledger and Journal combinations: 

Can a clustered index (or IOT in Oracle) be detrimental, when to be used on a very "broad" table, but only few columns are used? In this case, the "Product" table is used only like a junction table between "ProductCategory" and "Sales". If there was a nonclustered index on pr.ID and pr.CategoryID, the DBMS would do an Index-only-check, which has a very good performance. But, if I am right, a clustered index actually IS the entire table, ordered by the index columns. So, even if the clustered index had pr.ID and pr.CategoryID as it's index columns, the database would still have to load the entire table with all the heavy nvarchar(4000/max) stuff, only for two small columns. 

Yes, they are compiled and stored on the database. In SQL Server, ad-hoc queries (without binding parameters) are compiled and stored, too, filling up the plan cache (except when ad-hoc optimization is enabled). But ad-hoc plans age out quickly and get removed, while frequently used, parameterized query plans will last longer in cache. However, I think prepared SQL is a bit dated and error prone. Both Oracle and SQL Server support parameterized queries for quite a while now, without the need for an extra roundtrip to prepare a statement. By using "sp_executesql" (MSSQL) or "execute immediate" (Oracle), and their support through database interfaces like ADO.NET, people can create parameterized queries identified by SQL string equality, without an extra database trip, and still use them when the original DB connection and command are long out of scope, even across applications - as long as the SQL string is equal (including capitalization, spaces etc.). This is also great for dynamic SQL built within stored procedures, where table or column names may vary. The scope of a prepared statement quickly gets lost in an application, requiring repeated prepare again and again. And I've seen faulty code calling "prepare" before every SQL query, forcing re-calculation or cursor creation every time, even when sending dozens of equivalent queries. There may be a minimum performance gain by sending only a handle to the database, instead of a full SQL string, but it seems no remarkable advantage, far outweighed by the extra trip and the scope/application limit of prepared statements. 

Note that the single clustered index on the view is identical to the (one and only) clustered index on the original table. However, several queries running against the Indexed View run slower (averaging about 3*, ranging up to about 6*, slower) than against the original table. Does anyone know why this could happen? Is it a possible bug in the Engine to not treat two identical clustered indices identically? My test data currently covers only two periods, one year apart. I initially thought it might be due to the columns of the view being nullable, but using isnull to coalesce them simply makes the queries so slow I can't even measure the performance. I am on SQL Server 2014: 

The SQL value null represents the absence of data, so when you pivot and a cell as no data a null will perforce be generated as the cell value. It is your responsibility as the programmer to coalesce the null to whatever domain value is appropriate for missing data, when that makes sense. 

Question currently affects C#/.NET, DB access based on ADO.NET and SQL Server 2008 R2, but I think it applies to other databases as well. I noticed some old modules of a system have non-optimal SQL queries, with multiple concatenated value strings instead of parameter placeholders. They do a polling on a table, like every 10 seconds, to get items added during the last few minutes, which generates a new query plan on every execution. Their performance is not too bad, no SQL injection risk (no web/user forms), they're old and it would be a lot of work to change their queries to correct parameterization. I suggest to do this change, but there's debate that it would be a waste of time, with other things being more important. Edit: The database is supposed to run with mostly parameterized queries (which all newer modules use), so I would like to avoid the "optimize for ad hoc" option. Partially parameterized queries create a plan anyway. Is there a downside when running in ad-hoc optimized mode, with mostly parameterized queries? To me, it seems like these old modules take a huge portion of the database resources, although they are few. Even a single module of this kind would create thousands of query plans over time, while all newer modules together have less. Is it importante to change these, or can I leave them in their state, with optimization/parameterized queries only in current/future modules? SQL is like: 

(Please forgive the SQL Server test case - the problem is common to all SQL implementations because that of common semantics mandated by the SQL Standard.) Even though you have used a LEFT OUTER JOIN, the semantics of SQL can convert this to an implied INNER JOIN if you improperly put constant-test conditions in a WHERE clause instead of the JOIN clause. The example below ilustrates this. Preliminaries to create test data: 

The Key - there must be a Primary key for every relation being normalized. The Whole Key - There must not be any functional dependencies of attributes on any proper subset of the Primary Key. And Nothing but The Key - There must not be any functional dependencies of attributes on non-key attributes. 

I am considering to apply a fix which will generate constant parameter sizes for (n)varchar parameters. The application is based on NHibernate with the old OracleClientDriver (using Microsoft Oracle driver, ODP.NET can't be used in the near future), and NHibernate runs on top of ADO.NET. This driver creates parameterized SQL with the size of (n)varchar parameters set to the actual string size, which, of course, varies: 

perhaps somebody can add the correct syntax how to remove shared cursors for a table or user this way, and tell if full admin rights are required for this. 

On SQL Server 2012, i've got an intermediate/staging table for merging existing with new data, where I want to insert numeric IDs for newly created rows: 

Perhaps this problem has been solved meanwhile. Databases need dedicated knowledge and skills, any 10 year old kid can play around with SQL Server Management Studio, but creating and maintaining databases is not that easy. For SQL Server , make sure that Snapshot Isolation level is enabled. It's disabled by default, with the database in data locking mode, where long running queries block write acces, or even certain read accesses. I think Snapshot Isolation was introduced in SQL Server for analysis/OLAP. Also, the OLTP tables should have proper indices. There should be somebody in the company knowing at least the basics of all this. 

(My apologies for the SQL Server syntax - I don't have MySQL. Please treat as pseudo-code.) A scheme such as the following should work 

In practice relations frequently have multiple candidate keys, and these rules must apply to all of them in turn. Also, when Normalizing one must determine the minimal key(s) for each relation, and use those rather than any Super Key. Your lecturer didn't remove the fifth column altogether because the functional dependency in that column still exists, and must still be accounted for in the Normalization process. Update: The FD AD->C doesn't disappear by virtue of recognizing ADC as a subset of CDA; neither is it sensible to have two relations ADC and CDA both in the model as this is a redundancy of exactly the sort Normalization is designed to eliminate. 

Broken shared cursor (sql re-use) as cause of ORA-00942: I recently had this error, although the table to select from did exist and the user had sufficient privileges. Furthermore, select from SQL Developer or Sqlplus worked correctly, and even other queries on the same table within the application (using NHibernate). Only one specific, but important query to get an item by name failed again and again. The cause of this was a broken shared cursor (similar to a query plan in MS SQL Server) which didn't work anymore after I changed some user rights. My solution was to do an 

Doing this with SQL Server would create a huge number of query plans, one for each combination of string sizes (when multiple string parameters), which is of course highly inefficient. I always thought it's the same in Oracle, but now I've got doubt. I applied a fix to the NHibernate driver, which would send always the max parameter size, e.g. if the above Name column was 32 chars wide, it would always send [Type: String (32)]. My code does not use dedicated Prepare statements, but sends parameterized SQL together with the values (similar to calling EXECUTE IMMEDIATE). I then looked at the statement (actually an INSERT) in Oracle Enterprise Manager, and the old version did not appear in the duplicate queries list. The statement itself showed a parse for each call (total of more than 1000 after some testing), but few hard parses. Thus, I could see no difference in performance between the fixed and the variable varchar length. Does this mean the fix is futile, and different query plans for different parameter sizes occur only in SQL Server (and maybe other DBMS)? Does Oracle check only for SQL string equality, but not for equality in parameter size settings? I also noticed that duplicate SQL was almost only found where parameter values were concatenated into the SQL string, instead of using bind parameters. EDIT: cursor_sharing in Oracle is set to EXACT. 

This has the additional advantage of being data-driven; in the case that your masking needs to be amended, only a data change is required instead of a code change. 

None of these requirements for 1NF is violated by the design you propose. For instance a Junction Table created to normalize a many-to-many relationship will always have a composite Primary Key, with each component being also a Foreign Key into one of the joined tables. 

In EXCEL, from the Data ribbon, Get External Data tab, select From Other Data Sources -> From SQL Server. Follow the wizard to connect to your server and create a query. 

t is not always accurate to the business requirements to unpivot the table as you propose. For example, in a tournament scheduling database the table for Game will always have two distinct FK's to Team, one labelled HomeTeam and the other labelled VisitingTeam. Unpivoting this eliminates the business requirement that a Game is always between exactly two teams. Another example is the case of a database for an online meeting scheduler, where each meeting typically has a single Host and any number of participants. It wold be appropriate to embed the Host FK to the Participant in the main meeting table, with all non-host participants listed in a detail table. If this were an appropriate model for your business requirements then all but one of your reference_n fields would be pivoted out to achieve proper normalization. So, in the absence of the relevant business requirements, we are left wondering whether it is appropriate from a business perspective to unpivot as described. However, let's assume unpivoting is accurate to the business requirements. Why are you worrying about performance so early in the design? If it actually turns out that once you have a few billion rows this one table is at the core of a key performance criteria, there are a dozen or more techniques for addressing that only one of which is the denormalization you inquire about. There is absolutely no possible way that you can determine at this early stage which of those performance enhancing techniques will be an appropriate solution. Meanwhile, if you continue to denormalize your design in this way you will vastly complicate the writing, reading, testing, and development time of the codebase using your application. Are you sure that cost is worthwhile for a very, very, early guess at performance needs? 

to clear the cursor cache (an admin had to do it!). There are probably other ways to do this on user or SQL statement level (the above is for the whole database server), which I did not understand the quick way. An alternative may be: 

Most selective first is useful only when this column is in the actual WHERE clause. When the SELECT is by a larger group (less selective), and then possibly by other, non-indexed values, an index with less selective columns may still be useful (if there's a reason not to create another one). If there's a table ADDRESS, with COUNTRY CITY STREET, something else... indexing STREET, CITY, COUNTRY will yield the fastest queries with a street name. But querying all streets of a city, the index will be useless, and the query will likely make a full table scan. Indexing COUNTRY, CITY, STREET may be a bit slower for individual streets, but the index can be used for other queries, only selecting by country and/or city.