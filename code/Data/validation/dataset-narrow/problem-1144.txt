This structure is surely well-known to students of temporal logic. What is it called, and what is the standard temporal logic notation for it? What is it used for? (I.e., what are some applications?) 

My general sense is that separation is easier to use than ownership, since we need frame properties for nearly every command in an imperative program. (Dave Naumann argues that region logic is more amenable to automation, since the assertion logic remains plain old FOL, and so you can use off-the-shelf theorem provers and SMT solvers.) EDIT: I just found the following paper by Matt Parkinson and Alex Summers, The Relationship between Separation Logic and Implicit Dynamic Frames, where they claim to give a logic unifying the two methods. 

Language researchers take languages as fundamental, because we are interested in expressiveness and impossibility results. With a similar exaggeration, our basic research question is: 

Hindley-Milner type inference is EXPTIME-complete, but on the programs people typically write it is pretty close to linear. 

Algebraic data types let you define types recursively. Concretely, suppose we have the datatype $$ \mathsf{data\;list = Nil \;\;|\;\; Cons\;of\;\mathbb{N} \times list} $$ What this means is that $\mathsf{list}$ is the smallest set generated by the $\mathsf{Nil}$ and $\mathsf{Cons}$ operators. We can formalize this by defining the operator $F(X)$ $$ F(X) == \{ \mathsf{Nil} \} \cup \{ \mathsf{Cons}(n, x) \;|\; n \in \mathbb{N} \land x \in X \} $$ and then defining $\mathsf{list}$ as $$ \mathsf{list} = \bigcup_{i \in \mathbb{N}} F^i(\emptyset) $$ A generalized ADT is what we get when define a type operator recursively. For example, we might define the following type constructor: $$ \mathsf{bush}\;a = \mathsf{Leaf\;of\;}a \;\;|\;\; \mathsf{Nest\;of\;bush}(a \times a) $$ This type means that an element of $\mathsf{bush\;}a$ is a tuple of $a$s of length $2^n$ for some $n$, since each time we go into the $\mathsf{Nest}$ constructor the type argument is paired with itself. So we can define the operator we want to take a fixed point of as: $$ F(R) = \lambda X.\; \{ \mathsf{Leaf}(x) \;|\; x \in X\} \cup \{ \mathsf{Nest}(v) \;|\; v \in R(X) \} $$ An inductive type in Coq is essentially a GADT, where the indexes of the type operator are not restricted to other types (as in, for example, Haskell), but can also be indexed by values of the type theory. This lets you give types for length-indexed lists, and so on. 

Non-monotonic logic is kind of a wide area -- do you have any particular logics in mind? Anyway, defeasibly assuming :) that 

one answer is that you can give a reasonable categorical semantics to any logic for which a sequent calculus with cut-elimination is known. Basically, the types are objects, normal forms of the sequent calculus are morphisms, and cut-elimination tells you how to implement composition. This gives you the initial category in whatever category of models you end up using to prove soundness and completeness. This recipe is independent of monotonicity, and so it can work well even for non-monotonic logics. For example, one of the more important successes in categorical logic is its treatment of linear logic. This is a logic where, intuitively, propositions refer to resources, so that the linear implication $A \multimap B$ can be read as "$A$ can be consumed to produce $B$". The consequence relation of linear logic is non-monotonic (since the fact that $A$ can be consumed to produce $B$ does not mean that $A$ and $X$ can both be consumed to produce $B$). However, it has an excellent proof theory, and its categorical models are closely tied to the theory of monoidal categories. 

Vijay D has mentioned both separation logic and jStar, which is a tool for verifying Java programs. jStar uses the logic Matthew Parkinson developed in his PhD thesis, Local Reasoning for Java, which gives a separation-based Hoare logic for verifying Java programs. It is a very well written thesis, and offers one of the most readable introductions to modern techniques for verifying imperative/OO programs. My own thesis is less well-written, but it shows how to extend this style of separation logic to a full Reynolds-style specification logic, which makes it easier to specify and prove correct higher-order imperative programs, such as those using callbacks. This is not for Java, though, but rather for an ML or Haskell-like language. 

At the same time you are reading one of these books, I would advise downloading and learn how to use the Agda programming language. This language implements the sophisticated type theories described above, and IMO it is incredibly helpful to see how the often quite subtle semantic constructions cash out in type theory. Andrej Bauer can probably give you even better advice. Perhaps he can be persuaded to post. :) 

Actually, in section 4.5.3, he doesn't quite say that EM+impredicativity is inconsistent. He says that when you assume it, the model must becomes degenerately proof-irrelevant (the interpretation of all types other than Prop can have at most one element). Andy Pitts describes a similar phenomenon "Non-trivial Power Types Can't Be Subtypes of Polymorphic Types". For predicative versions of type theory, it's probably easier to just do the consistency proof than to Google for it -- universe stratification gives you everything you need for the simple-minded set-theoretic model of types (i.e., types are sets, terms are maps) to work out. Just observe that sets are closed under indexed sums and products, and get cozy with the axiom of replacement when interpreting universes. This is academic bad practice, of course, but the proof is still worth doing for yourself. 

because the substitutions and applied to would give you and back. As an aside, we needed to specify "least general" because otherwise: 

I think you are looking for a typed variant of anti-unification. Anti-unification can be described as follows. First, suppose that we have a grammar of terms as follows: 

If you're more interested in the engineering aspects, you should look at Brad Myers' work (from the late 80s/early 90s) on the Garnet and Amulet toolkits. They have a TOPLAS paper, Lessons learned about one-way, dataflow constraints in the Garnet and Amulet graphical toolkits, which summarizes what they learned. 

So he says that if $(\sigma, T)$ is a solution for $(\Gamma, t)$ that doesn't overlap with $\mathcal{X}$, then there is a solution $(\sigma', T)$ for $(\Gamma, t, S, C)$ such that $\sigma'$ is an extension of $\sigma$ to cover the variables in $\mathcal{X}$. 

So let's look at the translation of $G(A \vee \lnot A) = \lnot(\lnot G(A) \land \lnot\lnot G(A))$. So this says that the constructive content of the excluded middle is the same as saying that it's not the case that $\lnot P$ and and $\lnot\lnot P$ hold -- ie, noncontradiction. So in this sense, there's not actually very much computational content to the law of the excluded middle. To see what it is concretely, recall that constructively, negation is defined as $\lnot A == A \implies \bot$. So this formula is $((G(A) \implies \bot) \land ((G(A) \implies \bot) \implies \bot)) \implies \bot$. The following bit of Ocaml code will illustrate: 

You get a thunk (because this paper is about lazy evaluation). To force one of these thunks, you need to scrutinize the list to get its head, and then force the evaluation of the head by applying it: 

Yes, Prolog. The specification of unification in the Prolog standard omits the occurs check, and as a result variables range over rational trees. Additionally, many Prolog (such as SWI Prolog and YAP) implementations support tabling, which permits defining and using coinductively defined predicates. 

It would be very unnatural to write type annotations using such synonyms, but that doesn't matter for decidability results! However, pattern unification and higher-order matching are both decidable, and may be useful for your purposes. In higher-order unification, you have two lambda terms $s$ and $t$ containing unification variables, and a unifier is a substitution $\theta$ such that $\theta(s) =_{\beta\eta} \theta(t)$, with the goal finding a complete set of unifiers (ie, a set such that every unifier factors through this set). 

EDIT: Here's why distributivity at the zero type implies the equality of all maps $A \to 0$. To fix notation, let's write $!_A : 0 \to A$ to be the unique map from $0$ to $A$, and let's write $e : A \to 0$ to be some map from $A$ to $0$. Now, the distributivity condition says that there's an isomorphism $i : 0 \simeq A \times 0$. Since initial objects are unique up to isomorphism, this means that $A \times 0$ is itself a initial object. We can now use this to show that $A$ itself is an initial object. Since $A \times 0$ is an initial object, we know the maps $\pi_1 : A \times 0 \to A$ and $!_A \circ \pi_2$ are equal. Now, to show that $A$ is an initial object, we need to show an isomorphism between it and $0$. Let's choose $e : A \to 0$ and $!_A : 0 \to A$ as the components of the isomorphism. We want to show that $e \circ !_A = id_0$ and $!_A \circ e = id_A$. Showing that $e \circ !_A = id_0$ is immediate, since there is only one map of type $0 \to 0$, and we know that there is always an identity map. To show the other direction, note $$ \begin{array}{lcll} id_A & = & \pi_1 \circ (id_A, e) & \mbox{Product equations} \\ & = & !_A \circ \pi_2 \circ (id_A, e) & \mbox{Since $A\times 0$ is initial} \\ & = & !_A \circ e & \mbox{Product equations} \end{array} $$ Hence we have an isomorphism $A \simeq 0$, and so $A$ is an initial object. Therefore maps $A \to 0$ are unique, and so if you have $e,e' : A \to 0$, then $e = e'$. EDIT 2: It turns out the situation is prettier than I originally thought. I learned from Ulrich Bucholz that it's obvious (in the mathematical sense of "retrospectively obvious") that every biCCC is distributive. $\newcommand{\Hom}{\mathrm{Hom}}$ Here's a cute little proof: $$ \begin{array}{lcl} \Hom((A + B) \times C, (A + B) \times C)  & \simeq & \Hom((A + B) \times C, (A + B) \times C) \\ & \simeq & \Hom((A + B), C \to (A + B) \times C) \\ & \simeq & \Hom(A , C \to (A + B) \times C) \times \Hom(B, C \to (A + B) \times C) \\ & \simeq & \Hom(A \times C, (A + B) \times C) \times \Hom(B \times C, (A + B) \times C) \\ & \simeq & \Hom((A \times C) + (B \times C), (A + B) \times C) \end{array} $$ 

That is, if you get not-A and not-not-A, you can just pass the first negation to the second to derive the contradiction you want. Now, what is a continuation passing-style transformations? 

Now this program does not even need the full strength of primitive recursive (since the for-loop can be macro-expanded into a huge nested if-then-else statement), but in most practical cases it will behave just the same as before. Note that it does help decidability in theory -- the program is total, so you can answer questions by running the program and seeing what happens. This is not what we actually want, which is to get answers faster than running the program -- the artificial termination introduced doesn't actually help program analysis in practice, since bugs occur because of errors in the real program logic, and we haven't touched that at all. Furthermore, adding abstraction facilities to a programming language can radically worsen the complexity of the analysis problem, while making it easier to verify programs in practice. For example, proving termination of the simply-typed lambda calculus with natural numbers requires induction up to $\epsilon_0$, but by adding type polymorphism you get System F, whose termination proof as strong as the consistency of second-order arithmetic. Yet, in practice programs written in F are much easier to verify, because the modularity properties of second-order quantification make writing structured programs and correctness proofs much easier.