Quantum computers are not known to solve NP-complete problems in polynomial time. "Quantum magic won't be enough" (Bennett et al. 1997): if you throw away the problem structure, and just consider the space of $2^n$ possible solutions, then even a quantum computer needs about $\sqrt{2^n}$ steps to find the correct one (using Grover's algorithm) If a quantum polynomial time algorithm for a NP-complete problem is ever found, it must exploit the problem structure in some way (otherwise bullett 2 would be contradicted). 

I'm implementing a Unique $k$-SAT solver, whose input is a $k$-CNF formula having at most $1$ satisfying assignment. To test its practical behaviour, I need a set of such formulas. I've searched for them on the web, and found nothing (while, on the other hand, it is very easy to find suites of ordinary $k$-CNF formulas). 

I've some (basic) questions that no one seems to have asked so far on this site (maybe because they are basic). Suppose someone finds a bounded error quantum polynomial time algorithm for $SAT$ (or any other NP-complete problem), thus placing $SAT$ in $BQP$, and implying $NP \subseteq BQP$. 

Here is a further intuitive and unpretentious explanation along the lines of MGwynne's answer. With $2$-SAT, you can only express implications of the form $a \Rightarrow b$, where $a$ and $b$ are literals. More precisely, every $2$-clause $l_1 \lor l_2$ can be understood as a pair of implications: $\lnot l_1 \Rightarrow l_2$ and $\lnot l_2 \Rightarrow l_1$. If you set $a$ to true, $b$ must be true as well. If you set $b$ to false, $a$ must be false as well. Such implications are straightforward: there is no choice, you have only $1$ possibility, there is no room for case-multiplication. You can just follow every possible implication chain, and see if you ever derive both $\lnot l$ from $l$ and $l$ from $\lnot l$: if you do for some $l$, then the 2-SAT formula is unsatisfiable, otherwise it is satisfiable. It is the case that the number of possible implication chains is polynomially bounded in the size of the input formula. With $3$-SAT, you can express implications of the form $a \Rightarrow b \lor c$, where $a$, $b$ and $c$ are literals. Now you are in trouble: if you set $a$ to true, then either $b$ or $c$ must be true, but which one? You have to make a choice: you have 2 possibilities. Here is where case-multiplication becomes possible, and where the combinatorial explosion arises. In other words, $3$-SAT is able to express the presence of more than one possibility, while $2$-SAT doesn't have such ability. It is precisely such presence of more than one possibility ($2$ possibilities in case of $3$-SAT, $k-1$ possibilities in case of $k$-SAT) that causes the typical combinatorial explosion of NP-complete problems. 

I'm curious to know if anyone already dealt with $0$ obstructed vertex induced subgraphs previously, how he encountered them in the first place, and what is known about them. Another way to express question 1. is the following: which is the maximum number $\alpha$ of vertices that can be removed from $G$ without making it $1$ extendible? My sensation is that $\alpha \in O( log\ |V| )$, as I'm inclined to believe that $\alpha$ is strongly related to the diameter of $G$. 

$v_i[j] = 0\ \forall j \geq i$. $v_i[j] = 1\ \forall j < i - \frac{m}{log(m)}$. Those bits that do not fall in the restrictions above can be either $0$ or $1$, but in such case the number of $0$'s can be at most $12$. 

Could anyone please point to both introductory and advanced papers/books showing how to use SAT technology to solve Computer Vision tasks (e.g. edge detection, object recognition, facial recognition, ...)? More precisely: I'm looking for SAT-based techniques to extract knowledge from images and reason about such knowledge (e.g. is object $X_1$ in front of object $X_2$? Is object $X_3$ partially hidden by object $X_4$? Is person $X_5$ in image $I_1$ the same as person $X_6$ in image $I_2$?). 

In the paper The diameter of random regular graphs, Bollobás and Fernandez De La Vega give asymptotic lower and upper bounds on the diameter $d$ of random $r$-regular graphs. Roughly, the lower bound is $\Theta(log_{r-1}n)$, while the upper bound is $\Theta(log_{r-1}(nlogn))$, where $n$ is the number of nodes of the graph. I would like to know (even just empirical) lower and upper bounds on the diameter of non-random $3$-regular graphs. More precisely, such non-random $3$-regular graphs in which I'm interested are those arising after reducing practical real world problem instances (which are known to be structured instead of random) to the $\sharp P$-complete problem $\sharp3$-regular Vertex Cover. In other words, suppose you have a real-world instance of a $NP$-complete problem, and suppose you want to count its solutions: following a chain of reductions, you come out with a $3$-regular graph $G$, whose number of vertex covers encodes the number of solutions of your initial instance. 

Let $G$ be a $3$-regular graph. Let $O$ be the number of vertex covers of $G$ having odd cardinality, and let $E$ be the number of vertex covers of $G$ having even cardinality. Let $\Delta = O - E$. Computing $\Delta$ is $\#P$-hard. Let $A$ be an algorithm behaving as follows: 

I have $m$ bit vectors, each of which is composed by $m$ bits. Let's denote with $v_i[j]$ the $j$-th bit of the $i$-th vector, $i,j \in [1, m]$. Each bit vector $v_i$ is subject to the following 2 restrictions: 

In these days I'm reading the book Decision Procedures - An Algorithmic Point of View. Chapters 3 and 4 deal with the Theory of Equality with Uninterpreted Functions (EUF). The authors give 3 toy examples on using EUF to solve real-world problems: proving equivalence of programs, proving equivalence of circuits, validating the translation performed by a compiler. I'm looking for further practical applications of EUF, possibly non-toy examples. 

Is there any relationship between the number of vertex covers of a graph $G$ and the permanent of $G$'s adjacency matrix? 

Let $G=(V,E)$ be a $3$-regular graph. Let a vertex induced subgraph of $G$ be $i$ extendible if and only if it has both the following properties: 

Consider the $n$ dimensional space $\{0,1\}^n$, and let $c$ be a linear constraint of the form $a_1x_1 + a_2x_2 + a_3x_3 +\ ...\ + a_{n-1}x_{n-1} + a_nx_n \geq k$, where $a_i \in \mathbb{R}$, $x_i \in \{0,1\}$ and $k \in \mathbb{R}$. Clearly, $c$ has the effect of splitting $\{0,1\}^n$ in two subsets $S_c$ and $S_{\lnot c}$. $S_c$ contains all and only those points satisfying $c$, whereas $S_{\lnot c}$ contains all and only those points falsifying $c$. Assume that $|S_c| \geq n$. Now, let $O$ be a subset of $S_c$ such that all the following three statements hold: 

The green nodes separate the graph from the "external environment". Let's call them the graph hull. Now, a graph may have several hulls. Consider the following graph, where the green nodes constitute the hull: 

Surprisingly, #Monotone-2SAT is #P-complete. Now here is the question. Suppose we have an oracle for #Monotone-2SAT, which returns the exact solution count of a Monotone-2SAT formula: how such solution count can be used to solve k-True-Monotone-2SAT? I'm asking this because I do not immediately see how the solution count may give information on how many solutions have k or less literals set to true and how many don't. 

Update 29th May 2016 As pointed out by Emil Jeřábek in his comment, it is not true that Valiant said that the problem $\oplus\text{Rtw-Opp-CNF}$ is degenerate. He only said that a more restricted version of such problem, $\oplus\text{Pl-Rtw-Opp-3CNF}$, is degenerate. In the meanwhile, I continue to not know what degenerate exactly means, but at least now it seems clear that it is a synonym of lack of expressive power. 

Update 11/04/2013 22:40 What if, in addition to the restrictions described above, we also introduce both the following restrictions: 

Let $G = ( V_G, E_G )$ be a graph. Let $E_H \subseteq E_G$. The subgraph of $G$ edge-induced by $E_H$ is $H = ( V_H, E_H)$, where $V_H = \{ v \in V_G : \exists ( u, w ) \in E_H\ v = u \lor v = w \}$ Let $k_1 \leq |E_G|$ and $k_2 \leq |V_G|$ be given in input. I would like to determine the number $k_3$ of edge-induced subgraphs of $G$ having $k_1$ edges and $k_2$ nodes. Clearly, I don't want to enumerate all the (exponentially many) subgraphs of $G$ having $k_1$ edges. 

Let $F$ be a random k-CNF formula with $n$ variables and $m$ clauses. Let $G$ be the undirected graph built in the following manner: there is a vertex $v$ for every clause $c \in F$, and there is an edge $e = (v, u)$ if and only if the clause corresponding to $v$ has at least one variable in common with the clause corresponding to $u$. The question is: how many variables, on average, shall I remove from $F$ before $G$ gets divided into 2 or more connected components? Is such number of variables related to $r = \frac{m}{n}$? Here is what I mean by "removing a variable $v$ from a formula $F$": every clause $c$ in which $v$ is mentioned (no matter if positive or negative) gets shortened; if $c$ becomes empty, then it is removed from $F$. 

It is the case that $b \notin o( m )$. Let $G = ( V_G, E_G )$ be a $3$-regular connected graph. Without loss of generality, let us fix $w = |V_G|$ and focus on $\Delta_{|V_G|} = O_{|V_G|} - E_{|V_G|}$. Pick an arbitrary edge $e = \{u, v\} \in E_G$. Remove $e$ and replace it with the following gadget: 

Take a look at General Electric Global Research. In particular, see here for cool jobs in Computer Science (some of which have a non-empty intersection with theory). 

As Tsuyoshi Ito pointed out, if the drawing is not requested to be planar, then every cycle is a hull, and this observation simultaneously answers all the four questions I've posted. As Hsien-Chih Chang and Peter Shor indicated, if the drawing is requested to be planar, then the questions can be answered by using planar graph embedding algorithms. 

Clearly, if a vertex induced subgraph is $i$ extendible it is also $j$ extendible for any $j < i$. Now, let a vertex induced subgraph of $G$ be $i$ obstructed if and only if it is $i$ extendible but not $i+1$ extendible. I'm interested in $0$ obstructed vertex induced subgraphs. 

If your goal is to find a point in $P$ or determine that $P$ is empty, why don't you do the following. Let $H$ be a set of half-spaces, initially empty. Let $x$ be a point, initially equal to $0^k$. 

This further answer is meant as a feedback to dividebyzero's comment to my previous answer. As dividebyzero says, it is certainly true that CNF and DNF are two sides of the same coin. When you have to find a satisfying assignment, DNF is explicit as it manifestedly shows you its satisfying assignments (DNF Satisfiability belongs to $\mathbf{P}$), whereas CNF is implicit as it wraps and winds to hide its satisfying assignments from your eyes (CNF Satisfiability is $\mathbf{NP-complete}$). We do not know any procedure which is able to unwrap and unwind any CNF formula into some equisatisfiable DNF formula which is not exponentially sized. This was the point of my previous answer (whose example was meant to show the exponential blow-up, although admittedly such example was not the best possible choice). Conversely, when you have to find a falsifying assignment, CNF is explicit as it manifestedly shows you its falsifying assignments (CNF Falsifiability belongs to $\mathbf{P}$), whereas DNF is implicit as it wraps and winds to hide its falsifying assignments from your eyes (DNF Falsifiability is $\mathbf{NP-complete}$). We do not know any procedure which is able to unwrap and unwind any DNF formula into some equifalsifiable CNF formula which is not exponentially sized. At one extremity we have Contradictions, i.e. unsatisfiable formulas. At the opposite extremity we have Tautologies, i.e. unfalsifiable formulas. In the middle, we have formulas which are both satisfiable and falsifiable. In any CNF formula with $n$ variables, every clause of length $k$ manifestedly encodes $2^{n-k}$ falsifying assignments. In any DNF formula with $n$ variables, every term of length $k$ manifestedly encodes $2^{n-k}$ satisfying assignments. A CNF formula without clauses is a Tautology, because it does not have any falsifying assignment. A CNF formula containing the empty clause (which subsumes every other clause) is a Contradiction, because the empty clause (which has $k = 0$) indicates that all the $2^n$ assignments are falsifying. Any other CNF formula is either a Contradiction or one of those formulas in the middle (and it is $\mathbf{NP-complete}$ to distinguish between these 2 cases). A DNF formula without terms is a Contradiction, because it does not have any satisfying assignment. A DNF formula containing the empty term (which subsumes every other term) is a Tautology, because the empty term (which has $k = 0$) indicates that all the $2^n$ assignments are satisfying. Any other DNF formula is either a Tautology or one of those formulas in the middle (and it is $\mathbf{NP-complete}$ to distinguish between these 2 cases). With a CNF formula, distinguishing between the 2 cases above means being able to tell whether all the falsifying assignments collectively brought by the presence of clauses overlap in such a way to cover all the $2^n$ assignments (in which case the formula is a Contradiction, otherwise it is satisfiable). With a DNF formula, distinguishing between the 2 cases above means being able to tell whether all the satisfying assignments collectively brought by the presence of terms overlap in such a way to cover all the $2^n$ assignments (in which case the formula is a Tautology, otherwise it is falsifiable). Under this light it becomes more clear why CNF Satisfiability and DNF Falsifiability are equivalent in terms of computational hardness. Because they actually are the very same problem, as the underlying task is exactly the same: to tell whether the union of several sets equals the space of all possibilities. Such task leads us to the wider realm of counting, which is in my humble opinion one of those avenues to be fervently explored in order to hope to make some non-negligible progress on these problems (I doubt that further research on resolution-based solvers may eventually bring groundbreaking theoretical advancements, while it certainly continues to bring surprising practical advancements). The difficulty of such task is that those sets overlap wildly, in an inclusion - exclusion fashion. The presence of such overlapping is precisely where the hardness of counting resides. Moreover, the fact that we let those sets overlap is the very reason that allows us to have compact formulas whose solution space is nevertheless exponentially large. 

Consider the #P-complete problem of counting the number of vertex covers of a given graph $G = (V, E)$. I'd like to know if there is any result showing how the hardness of such problem varies with some parameter of $G$ (for example, $d = \frac{|E|}{|V|}$). My sensation is that the problem should be easier both when $G$ is sparse and when $G$ is dense, while it should be hard when $G$ is "in the middle". Is this really the case? 

This is not a real answer, either. Certainly the problem here is the presence of an astronomical number of symmetries, which fool even the best SAT solvers on the best supercomputers. Such symmetries map solutions to solutions and non-solutions to non-solutions: in this case probably there is an immense number of almost-solutions (i.e. assignments satisfying all but a small amount of clauses), each of which can be obtained by any other applying a proper symmetry. Hence the solver wastes an enormous amount of time trying each of these almost-solutions, while in a certain sense they are all the same. Exploiting symmetries (see this paper) should be an avenue to explore in order to attack this hard 17x17 instance and make some progress on it. I wonder if anyone already tried to do so. 

Update 06/04/2013 12:55 I would also like to know how hard is determining the parity of the number of satisfying assignments. 

This question turned out to be trivial. As Lev Reyzin said in his comment, the answer was already encoded in the figures. I should have watched them more carefully...Of course, looking at the figures, it was already evident that those hulls are cycles. What was not immediately evident to me is the obvious fact that given whatever drawing like those, you could pick whatever cycle in it (even a 3-cycle), "enlarge" it, and "throw" all the remaining nodes inside it, thus obtaining a hull. So: 

I've found this very interesting thesis by Salil Vadhan: $URL$ The answer to my question seems to be in the proof of statement 9 of Theorem 5.2 (on page 36). 

The closest I was able to find is that $\oplus$3/2 BIPARTITE PLANAR VERTEX COVER is $\oplus$P-complete (see Theorem 2.2 in this paper).