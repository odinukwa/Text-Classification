The vast majority of my tapes are like B and C. A is the outlier. I'm looking for some sort of best practice here. The tapes are verifying OK. I don't want to have a tape fail just when I want to restore, but I also don't want to toss a tape with a handful of errors if it's unnecessary. 

If you've got Mac OS X Server, you should be able to create a SMB share. I don't have a Mac server in front of me, but I believe it's in Server Preferences-->File Sharing. Once shared, your PCs should be able to get to the drive directly by mapping the drive on the client. If you're just using a regular Mac as a server without the Server software, you can still enable Samba at the Unix level, or use SharePoints as a GUI to set up a Windows share. 

We have a VMWare ESXi 3.5 server that we are trying to get performance information on via esxtop logging, but I can't get an SMB share to mount so I can store the logs there. What am I missing? The command I'm trying to use (per the vendor's instructions) is: 

If your boss isn't an IT guy, then you may be able to get his attention more by focusing on business risks. There are two big ones that your boss will need to consider if you're all wireless and... You accept credit cards for payment Congratulations! You're under PCI-DSS regulations from the credit card providers. There's a bunch of work that you will need to do satisfy the auditors that your card information is safe in a wireless environment. You have health care information Congratulations! You fall under HIPAA. I'm less familiar with this since my office is not in health care, but I do know you will have additional work to get auditors to sign off that your wireless is secure. Even if you manage to meet the technical requirements, these business rules will likely drive your wireless off of your internal network. So you're going to end up either running a VPN from the wireless to get internal access, or you'll have to make all of your resources and apps run from a browser. And I'm not even touching on whether your boss would mind if someone could access something confidential to you. At least in a small office, someone would (hopefully!) notice if a random PC was plugged into your network. To directly address his concern that cables are going to change, you should tally up how long Cat5e has been viable while wireless has gone form 802.11 b to g to n, with the a standard a bonus tech cul-de-sac. ;) Tally up the costs, and I think you'll find yourself way ahead with a wired backbone. 

Your network sounds very similar to ours. Here's what we did. We map the drives and the printer by user using login scripts for the drives and Deployed Printer GPOs for the printers. Everyone has the same departmental drive K, but that is mapped to a different share for each department. In our AD, it's easiest since all users are divided into OUs by department whilst the computers are divided into OUs by OS. Since we do not have a mobile workforce, this works fine for us and is minimally fussy. If you want to go all Group Policy on your drives, you can use Group Policy Preferences to map drives. We've had our little batch scripts forever, so we haven't used it, but I've heard good things. 

The short question: can I share file and block level traffic on the same SAN? Perhaps more importantly, should I? The gory details are below... I'm hopefully putting the finishing touches on a new SAN design, and our new planned storage (EMC VNXe3100) will support being an iSCSI target, our original goal. It also supports file-level storage as well via CIFS and NFS. Some of the features we hope to use (particularly deduplication) are only available via file-level shares. The VNXe3100 has 2 controllers with 2 NICs per controller. Each NIC is going to a different switch, so either the controller or the switch can fail, and we should still be in business. This means that both file and block traffic would need to be enabled on each NIC. I'm assured by our rep that this is possible. My plan is to put the VNXe and the 5 host servers on the same VLAN and subnet (call it 192.168.1.x). This should keep my block-level iSCSI stuff only in that VLAN with no route out. But I would have a route out to the rest of the network for the file-level traffic on a different subnet (192.168.55.x). So each NIC would have an IP address for block traffic in the 1.x range and another for file traffic in the 55.x range. Since we are new to the world of iSCSI and the world of SAN/NAS devices, I want to make sure this isn't some horrible intermingling. But it would be really nice to expose our VMWare as NFS and get the VMs deduplicated on our hardware, and not having to maintain another file server would also be a bonus. If there's something else I'm overlooking, I'm all ears. 

If your backup needs will stay under 50 GB, then hosting at $URL$ would work for you. They explicitly allow a backup user as long as you stay under that. They also have SFTP, which is a more secure option than straight FTP. 

At this point, unless your email system is providing your business some impressive value-add, it's probably best to think of it as a commodity. It's going to be hard to find an ROI that is better than an outsourced solution unless you have a very compelling argument as to why you need to bring it in-house. With the Google Apps, you have some flexibility should you change your mind down the road. But without dedicated sysadmin support and some specific value-add that you can justify in savings, I don't think you'll beat Google's price once you factor in hosting, hardware, and the opportunity cost of your sysadmin not working on revenue-producing activities because your mail server's IP is suddenly on a blacklist. 

The most basic and easiest? Get an upstream vendor to do it. I'm going to make an assumption that your site is small (under 100 users), given that you aren't talking about other staff you have. In 2016, I'm hard-pressed to find a good reason for email to be in-house. Even enterprise installs are going cloud on email. Google, Office 365, or hosted Exchange will all do it better and cheaper once you factor in hardware, software, services, and skillsets. If you absolutely must keep it in-house, at least put an integrated email security system in front of it. We use Mimecast, Barracuda if you're getting hardware, but doing it all in Exchange alone, when you have other things to do? I don't see added business value there. 

We're just trying to bring our Macs into the fold here. My original plan was to use Backup Exec's Mac agent. Then I found out that the agent doesn't support 10.9, or even 10.8. So if you're keeping the OS up-to-date, that's out. I've heard legend tell that the next SP will get it up to speed, but I'm not holding my breath. It has been a few years, but Retrospect used to be the gold (and only) standard for Mac backup. Install the agent and you could set a schedule so the Macs would back up once connected to the network. I don't have recent experience with it, though it did work via VPN many moons ago. You'd then want to have it save the backup sets to storage that you would sweep into your existing backup environment. If you get a Mac Mini with OS X Server, you can redirect Time Machine on the laptops to the network, then sweep that connection up with another disk backup tool. I don't know if there's any granularity to Time Machine, though -- I believe it grabs the entire disk, or nothing. I know you mentioned cloud may not be an option, but if that is because of the VMs (which are now out of scope?), then perhaps that makes your CrashPlan/BackBlaze/Carbonite options more palatable. If you do want to bring the VMs in scope, you could install a Windows-based agent in the VM, and treat that as you would anything else. 

I'm reasonably certain that the only way to mitigate this will be to use WSUS, Systems Center, or some other sort of patch management automation. You'll need to create at least one separate group for Servers, then use whatever system you choose to deploy on the patches that you want. WSUS has no additional licensing, so at the very least you'll be able to test this without any purchase, just time. 

What is an acceptable level of hard write errors on tape? Specifically, what is acceptable on HP LTO-2 media? Is it a hard number of errors, a ratio of hours in use to errors, or something else entirely? Further background We are using a MSL6000 library with one LTO-2 drive using Backup Exec 11d (for now). Backup Exec always shows some soft errors for most of the drives, but some are starting to show hard errors. Backups are done with immediate verification, and the verify has yet to fail, so I don't have reason to be alarmed right now. While I can find the duty cycle for the drive (250,000 hrs), I can't seem to find any hard numbers as to when a particular tape should be should just be retired. If there's a best practice for rotating media out, I'd love to hear that, too. We're also soon migrating to LTO-4 media, so thoughts on errors there would also be helpful. Edited to add: I don't have hard error on every tape. To give an idea of what I'm looking at: 

To strictly answer your question, there's always good old mistakes with or . Mistakes with those tools are not fun. I (ahem) have a friend who may have accidentally shut down a server when I -- errr, he -- meant to just logout. But it's not a bad idea if the person legitimately needs those credentials. I log in with mine many times a day, but as a sysadmin, I need them all over. That said, I don't need admin credentials to read email and browse the web. If you're in a regulated industry (PCI DSS, SarbOx, HIPAA), you may be required to separate your duties out as much as possible, so an admin can be putting the company (and possibly him- or herself personally) in legal jeopardy. Frankly, that's what finally got us over to being better with our admin credentials. So the real takeaway, I believe, is to find out why the user is using the domain admin credentials. If the user is creating resources, installing software, etc., then perhaps that's what they need. If you have the time, you can always delegate out lots of AD individual privileges -- we let our helpdesk guy join PCs to the domain and change passwords, but that's about it. But just logging in with domain admin credentials, if you're an admin, doesn't necessarily mean there's cause for alarm. 

Try reducing your jumbo frame size to 8000. If I remember correctly, 9000 is the maximum standard for jumbo frames, and sizes that are over what a switch can handle get broken down. By going to 8000, you won't have the overhead break your maximum frame size. Once you've got that working, then you can start tuning to get you as close as you can to the largest packet size. 

Viable, possibly. There's a lot to look at on that dedicated PC. In general, you're going to want a dedicated piece of hardware to be your router rather than a PC, if for no other reason to reduce the visible attack surface on an internet facing device. A support contract to handle the hardware on the single point of failure wouldn't be a bad idea, either. :) The usual suspects in router or firewall hardware should be able to provide what you need. Be sure to mention load-balancing when you ask, or you may end up with a simple failover solution on the router/firewall. Not that that's necessarily a bad thing.