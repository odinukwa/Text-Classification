Try running a sql profile trace on the server activity ($URL$ - you may have some table locking issues caused by concurrently running queries locking tables/pages/indexes. Try running a perfmon - there may be something/s running that is using all the available disk/IO/CPU/Memory resources when things slow down. Look for any memory swapping - this will slow things down. If it is one of these then you will need to track down the cause. Note CPU at 100% for short period of times is OK (actually can indicate optimum performance). Also check for fragmentation and consider rebuilding indexes. Turn off services/applications that are not needed - they use up resources that are better used for your application. 

And then to be real sure put in a constraint or trigger function to check that prevID matches the nextID of the previous element. 

Try the following tables user(user_id(pk), name, email) flashcard_group(group_name(pk), group_tag) flashcard(flashcard_id(pk), question, solution) flashcard_groupings(group_name(fk), flashcard_id(fk)) <- to join flashcards to groups guess(guess_id, user_id(fk), flashcard_id(fk), is_correct) <- user_id and flashcard_id to be unique key. The above assumes that there is 1 user to 1 set of guesses, and that the flash card grouping is grouping the flashcards together (ie cards a, b and c and in group green, etc). If a user can have multiple go's at this then you need to add another table to hold the session information between the guess and user, a bit like what the table flashcard_groupings is there for. To get the queries you are after, join the tables as needed to get the groupings you need. For guess percentage, just join tables user and guess. To get the flash cards join user, guess and flashcard. And so on. 

The performance drop is probably becase in the 1 day of data, the database engine may be doing a sort 1000* on 1000 pices of data, but the month is sorting 1000* for 1000 pices of data/day * 30 days (where 1000 * 1000 << 1000 * 1000 * 30 ). It may be faster to run the query 30 times for 30 days of data rather than once for 30 days of data beacuse the amount of data that the query has to loop through is less. As others have explaned, you need to analyse your query and try to 1> reduce the amount of data the query looks at by excluding as much as possible 2> Choose indexes so that the data that the query has to search through is optimised. Maybe change the idex type (if the engine has different types) 3> change the order that the joins take place, may change the exection order that the planner chosses. 4> maybe load some of the "where in (" data into a temp table as a seperate query before the main query or create a string if there arnt may values and dynamically create the query (so rather than "... where in (select .. " use "... where in ( 1,2,3...)"). As others have pointed out look at the query plan and see which parts take the longest time - they will be probably be joins and scans (DISTINCT clause) and they to optimise these by making sure there are indexes to assist and maybe restructuring the data. You should have an index on [FACT].[DataMine].PartitionID, but it may make a difference to drop that before the insert and then re-create it, as sometimes this is faster. Possibly try a bulk insert as well. You would need to load the results of this query into a temp table first though, otherwise the sub queies would have no index on this field when they are selecting, which would make the situation worse. 

I am assuming here that the system is not a highly used transactional system and that you have windows of low usage to run the analysis queries. If you need to maintain high levels of performance, you may like to do the above in a separate database (separate hardware) and port across the new data that you get from backups. 

You need a few tables, 1 - The questions ( question id, input type, visible, question type, question text, expected answers....) 2 - The Answers ( question id, user id, activity id, answer....) 3 - The users ( user id, user name......) 4 - A table to hold a question/answer activity (activity id, data/time, user id) You may also like to have a table that specifies the questions that should be applied for each activity - either grouped by user or maybe a question collection. The foreign/primary keys will be the columns that have the same name in multiple tables and should be indexed. If you use this structure, you should be able to add a question or user or change an answer without having to change your schema or presentation code - make sure that the presentation code is dynamically created at run time - you just need to add a record in the appropriate place. This approach may take longer to develop initially than a hard coded approach, but will much simpler to maintain as you only will need to change data to change behavior. (A tip, to create your presentation layer, you will need a query that gets the appropriate questions to be displayed, then loop through this result set and call a method to render to question on the screen, the methods to chose being appropriate to the presentation of that question [text box, radio group, etc]) 

Do you have any locations that the packages are stored in/at (like shelf number/silo/etc)? If you do, then you can have a table with locations and package at that location. To find the adjacent packages you need only to know the location id +/-1 location. The location ID would need to be unique and sequential. If not, then you would could define a constraint (which probably means using a database engine that is more powerful the Access, maybe SQL server express?) Then you could create a trigger function that checks that the X next to Y == Y next to X on any updates. Another option is to create a linked list structure in your table, so that the data element for X has next filled and Y and the previous element for Y has X, with the prev/next/current fields having a unique constraint on them. Eg 

From docs: ($URL$ The Oracle Flashback Database feature, which provides an convenient alternative to point-in-time recovery, generates flashback logs, which are also considered transient files and must be stored in the flash recovery area. However, unlike other transient files, flashback logs cannot be backed up to other media. They are automatically deleted as space is needed for other files in the flash recovery area. When Files are Eligible for Deletion from the Flash Recovery Area There are relatively simple rules governing when files become eligible for deleteion from the flash recovery area: 

Just for fun, and the sake of your question, if are using MySQL, this would be a stupid hack to check if the hourly file has been received on the server: 

This can and will work (theoretically :) ...), but you will need a recursive function (in application or in the database) that can return the parent group id (ex. B-1), or the top group id (ex. A-1) for any group id value given as an input parameter (C-1). But ... like I said: The more info you give, the better responses you get. Hope that helps. 

Your free space in the current innodb file (ibdata[1] ?) is decreasing. It doesn't necessarily mean that your website is slow. Maybe the number of users (concurrent users) has increased in the last few days ? (also causing lots of inserts int he DB?) There is no error in the print screen. it's just your monitoring system, reporting a value that have reached the critical threshold. 

Why not recreating the primary key as (nid, vid), and creating a new index on the column "order" (just for fast retrieval / ORDER BY clause). --> The worst that can happen is to have 2 ingredients with the same "order" value, but the order-change-logic should be already correctly defined in the application. 

Inserting 10 rows each 2 minutes will result in (24*60/2)*10 rows per day (7200) -> that is not a large value to worry about. Also, it's great that you think on the "future - 10 years", but don't lose time with premature optimization. If your only concern it's about this table where you're inserting data every 2 mins, there's no point in creating additional databases (one per year), so let's stick with this table (Table_A). Now, because this table will slowly increase in time, and your want your queries to run fast, you have plenty of options: 

2 questions: - Where is the IO problem ? On the master or on the replicas ? - If the IO contention is on a replica, can you create a TEST replica, replicating from the same master, but having innoDb engine ? The thing with MyISAM storage is that an UPDATE / INSERT query will lock the whole table. And the replication thread on each replica is also a serial process, running just one query at a time (event) from the master binlog. My suggestion would be change the storage engine for all tables, from MyISAM to InnoDb, then re-check the replication parameters. 

you can partition the table by certain criteria. Because your using the table for a "weather station", and your data is time series values, your best option would be to partition by [station_id] then by [created] Create a Table_A_Archive, where you can move data that would be to old to keep in Table_A. How long do you intend to keep data in Table_A ? would it make sense to delete old rows that become obsolete for you application 

... And so on. Best Bet: Partitioning the existing table by [station_id] and [created], you will have "A" partitions for each station, "B" partitions for each month, and a total of AxB posible number of partitions. Once you partition Table_A, do the same thing for Table_A_Archive, and on the end of each year, move the data from Table_A to Table_A_Archive. ** IMPORTANT:** After you make the partitioning schema, keep in mind that all queries should have in the WHERE clause the conditions necesarly so that the query will hit as little partitions as posible. Ex. 

FRA will be purged automatically when free space is needed Put the archived logs in FRA as well Use RMAN to make the backups and use RMAN commands (REPORT OBSOLETE / DELETE OBSOLETE) to manage the purging of backup pieces + archived logs from FRA 

* I'm not an ORACLE Dev, but the function should look something like the one above... Once you create the function, you will create a functional index that will use the function to look for the rows you need: 

The reason for which you first query didn't use the index was because the result of isnull(Price) returned an un-indexed value. 

In the first use case, you (or the application) do a select to retrieve some metadata about a file; In the second one, you enter an ID (select query) and the application does the archiving (insert statement into table_b); What exactly do you need to automate here ? 

This is a sample matrix. The query should find all the green cells, with value = F (as is TO BE FOUND :) ) ... 

Replication was made for small changes, so you're on the right track. To keep the systems in sync, at the database level, you will need to setup a mysql master-master replication scenario. I've used the tutorial from this linke ($URL$ to setup such configuration. Depending on how much data is changed, you can choose between [statement] or [row-based] replication. More info about the differences between the 2 types: $URL$ 

Your questions is a little foggy You don't mention how the archiving process works You're basically asking about a feature on the User Interface side ... not on the DB side. 

Because your hierarchy models is Group A -> Group B -> Group C, it's enough to store in [user_groups_data] table just the user_id, the last group id (in the groups hierarchy tree), and the info So, data in the tables can look like: