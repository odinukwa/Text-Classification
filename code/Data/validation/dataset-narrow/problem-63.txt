Please advise me on technical jargon. What is the proper term for a port that adds an S-tag? How do you call a port adds the outer tag? 

The reasony why VLAN 1 is the default VLAN is not a vendor-specific implementation, but a decision made by IEEE. On page 222, 802.1Q-2014 says about VLAN identifier 1: The default PVID value used for classifying frames on ingress through a Bridge Port. The PVID value of a Port can be changed by management. In more common terms: It's the default VLAN. If a vendor decides not to comply with IEEE's decision, then it's possible that such a feature is available. Native VLAN: If an incoming frame is untagged, then it is associated with VLAN 1, unless configured otherwise. Consequently every untagged frame would be forwarded anywhere. To mitigate this security risk you should configure a shutdown VLAN on every switch. That's a VLAN which catches all untagged frames and prevents them from being forwarded because it only exists on that switch and none of its neighbors. Side note: You can get the 802.1Q-2014 document for free via the IEEE Get Program. 

How do RIPv2/RIPng select between equal routes? Randomly? For example, 2 broadcast domains, 5 routers, all running RIPv2 and RIPng. The pipes represent a switch: 

Is it possible to remove an OSPF area, once it isn't used anymore? I cannot get rid of . I've recreated the problem in a lab environment to simplify it. It's only one router with two operational FastEthernet interfaces: 

How can I change a switched port into a routed port? Compared to IOS, is there an equivalent of ? If there is none, is there a workaround? 

This prevents R0002 from informing R0003 about having blocked its Echo Reply. Otherwise, R0003 would log this message: . But by use of PBR, R0002 will silently discard an Echo Reply received from R0003. R0002 blocks an incoming Echo Reply from R0003 because R0002's interface has an ACL with as a rule. 

It is always misconfiguration because duplex mode is a L1 configuration issue. If there are L1 issues despite matching DM and a flawless cable, at least one NIC is broken. I suggest you consider two facts about auto-configuration: Neither does every device support auto-configuration, nor is there certainty that both interfaces will running in full-duplex mode. Considering power outage, that would be a cause of misconfiguration. The interfaces would have to negotiate once again. Either the manufacturer properly documents its devices or you can only guess how the NIC prioritizes duplex and data rate. Gigabit Ethernet mandates full duplex operation, yet still allows half-duplex. However, duplex mode is a factor whenever Fast Ethernet or less are available. There are still devices in use which do not need more than 100 Mbps because it links to a line which will never deliver more speed. Then there are devices without a Web interface or CLI. These use tiny DIP switches for HD and FD. A method which probably causes confusion among users. And there are devices by the same manufacturer which are out of line with the rest of the series. For example, all devices mandate AN, except for one device. Maybe no one would expect that. So to say - never dismiss misconfiguration. 

Is there a way to change the MAC address of VLAN interfaces (switched virtual interfaces) on IOS? I would like to configure the MAC address of a Catalyst switch. Because all VLAN interfaces derive their MAC addresses from the base MAC () and the command is missing in the context of a VLAN interface, I cannot get the desired configuration. I'm open to any workaround, no matter how obscure. 

Yes, a TTL value of 1 is sufficient. According to RFC791 for IPv4 and RFC2460 for IPv6, the TTL value is a decremented by 1 whenever the packet is forwarded by a node. Since the packet is not forwarded when it is passed on to the loopback interface, the TTL is not decremented and a TTL of 1 is sufficient. The IP datagram does not egress on an interface and therefore is not forwarded to another node. 

What is the proper classification of an IS-IS link state PDU? Here are some suggestions: Cell: Not applicable because TLVs are used. A cell must have a fixed length, according to ITU-T I.113 1997-06 and RFC3985. Datagram: According to RFC1983, a datagram carries enough information to be routed from source to destination without depending on earlier exchanges between that source and destination. But IS-IS LSP are only communicated between intermediate systems. Frame: According to ITU-T I.113 1997-06, a frame is a block of variable length at L2. However, since IS-IS is encapsulated by a L2 protocol, it seems redundant. Message: Way too ambiguous. Sometimes a message is an opcode of an application layer protocol. Sometimes a message is defined by setting a flag (control bit). Packet: Not applicable because according to ITU-T I.113 1997-06, a packet resides at L3. Segment: Not applicable because it is connectionless. A segment is a packet that contains TCP/IP data or an acknowledgement, according to RFC5681. Apart from TCP, a segment implies sequence numbering. 

Is the prioritization of duplex modes mandated by an authority? Is there a list published by a standardization body or industrial consortium? What duplex mode is supposed to be applied by a NIC at what data rate and what to fall back on? Whenever the manufacturer fails to properly document its devices, I can only guess how a NIC tries duplex modes. A framework would help me to base my expectations on something more absolute than other manufacturer's manuals or recommendations. Let's focus on 10/100/1000 interfaces. 

Why we need to modulate signal to send it through the backbone: When modems were used as modulators/demodulators, a phone call was done. A modem dial-up really means dialing up. A modem accesses the internet by dialing a number at the ISP's site. Once, the connection is established the line is occupied. When then someone else tries to make a voice call the internet connection breaks or the caller hears a screechy sound and finally the internet access connection breaks. The experience depends on the telco's wiring. Then, DSL came. From a consumer's perspective it did one very special thing: Talking over the phone while surfing on the net. not need to modulate it to transfer data via UTP cables: Twisted pairs of copper wire are designed for local networks (a room, between rooms, a floor, between floors). The signal from a CPU sockets runs on a bus and after 10 or more centimeters it reaches a slot or onboard device. Nonetheless are both the cable and the bus manufactured for being a medium for a certain signal. The modem as in mo-/demodulator was invented to send data via a signal over a wire that once was used for carrying a human voice sampled by a speaker. Yes, the voice was sampled and the signal then amplified. The connection of two endpoints was switched by a company or a state facility. Hence, PSTN. Are there other modems? For examples, a DOCSIS modem. Its uses a coaxial cable for transmission. Although only a portion within the frequency spectrum is used, that spectrum could be endlessly subdivided. Therefore, it's analog as well. Its definitely not digital. (Look up "coaxial antenna" to get the picture) 

only resolves an A record. sends an HTTP GET request. Sending HTTP messages is much more work than resolving host names. Moreover, both and trigger the local DNS resolver. After the desired record has been resolved, has done its work, while is just beginning to do its actual work. In short: is a tool for HTTP. is a tool for DNS. implicitly depends on DNS. explictily is designed for DNS. 

It is a point-to-point link. As Ron pointed out, logically it is a semantic problem and physically a media issue which does turn out to be a logical issue in the case of a bus network, as explained by Mike Penningtonâ™¦. However, point-to-point and point-to-multipoint are not obsolete terms when it comes to telephony. P2P connects the gateway to a single telephony system and P2MP to one or many phones. Why do I mention this? Because a gateway having more than one voice port logically acts as a hub, if it is set to forward inbound calls to each port (hunt group / ring call). Physically it does not. With regards to your comment at Mar 31 at 15:48, yes, the link is not dedicated to a single device connected to a hub. But this is not a criterion for calling the link P2P or P2MP. A hub regenerates the incoming signal on each port, one incoming signal at a time. There is no multiple signaling at a time on the same link. In short, it's P2P, both physically and logically. 

Even the entries in the system log call a sticky a configured address. After having issued , there are entries such as and and . 

You do not need to satisfy the requirement on the hardware level. There are platforms which support disabling Auto MDI-X. For example: Cisco IOS offers the command. Junos offers the command. Still, I recommend to look up the specifiation because you may get a device which does not run the original image anymore. Look for a feature navigator on the vendor's homepage. 

Is there a work-around to apply an ACL to packet originated by the router itself? How do I block certain packets not forwarded but sent by the router matching the ACL? I do not want to solve this issue by configuring ICMP. For example, three routers, connected by R02. 

Unless the sender sets the Don't Fragement flag, a packet can be fragmented into many Ethernet frames. The recipient assembles the fragments and processes the whole IP packet. No information of the original IP packet will be lost. 

No, the router does not flood the packet on all the other interfaces. Because not finding a path means that the router does not even have a default route. In that case, the packet is dropped. Depending on configuration, an ICMP Destination Unreachable with the apppropriate code is sent to the IP source address to inform about the unreachable destination. Here are the codes: ICMPv4, ICMPv6. 

How does the command work? How does the program look up the ASN? On which protocols or databases does the query depend? Can I recreate the option? I am using this version: . 

There are two possibilities: A) Because the cable tester indicates a problem, the port is flapping all the time causing the switch so send traps en masse. Have a look at the port's uptime. B) By definition an edge port has no other bridges attached to it. But the phone has a switch built in and therefore counts as a bridge. There may be a software feature that is triggered whenever more than one MAC address is learned on the edge port. 

Since you want to measure packet loss from a client, I suggest to run and try to stress the link as much as possible. 

No, PPP, HDLC, FR do not have their own MAC. In the OSI model, MAC is associated with layer 2. PPP, HDLC, FR are on L2 as well. But LLC is associated with L1, the physical layer. MAC is not just a theoretical concept like a sublayer, MAC is a protocol by itself. There are many LLC protocols, but there is only one MAC, with the exception of a VLAN tag field. Let us have a look at the third and the fourth field of a MAC portion of an Ethernet frame: Destination MAC Address; Source MAC Address. An address is 6 bytes long. In comparison, an HDLC address is 1 byte long. FR and PPP frames reach their destination very differently from MAC or HDLC. In a nutshell, Ethernet formats network data on both L2 and L1, while PPP, HDLC, FR stay on L2. That is why none of them needs an extra sublayer. 

Cisco IOS 12.4T and later: informs about the cost for each network learned by OSPFv2. What command does the equivalent for OSPFv3? 

What is the use case of the command? By definition, a sticky address is a learned secure address that is added to the running configuration. By definition, a configured address is a secure address that is statically configured via . By manually adding a sticky address, I've added a configured address. The command seems to me redundant and contradictory to the concept of a sticky MAC address. 

How does R01 decide between R02/R03/R03? Commentary: There are no static routes. Timers are the same on each router. There are only two IPv4 and two IPv6 networks. 

On a switch running Huawei VRP: What is the safest way to deconfigure a dynamic priority value on a root bridge and then configure a static priority value? What's the problem? I can do this: 

In the meantime, the bridge has a priority value of 32768. I can't assign a priority value because the command is not accepted: Error: Failed to modify priority because the switch is configured as a primary root or secondary root. Currently, I have three options. None of them are satisfying. 1) Disconnect the bridge from the network and configure it from a terminal. 2) Decrease the priority value on every other bridge below the default value of 32768. 3) Deconfigure and reconfigure as fast as possible and hope for the best. 

Currently, there is no single icon that depicts a data center. However, there is an abundance of icons depicting elements of a data center. Just to name a few: FC storage; FDDI ring; FC disk subsystem; file server; storage router; storage server. Here you can download them in various formats. 

If a message containing is printed, then either the MTU is less than 1500 bytes or there actually is packet loss. Because prints many lines, it comes in handy to use the option as well. So you'll get a summary when ping is stopped. Alternatively, you can use the option to limit the number of packets sent by ping. By stressing the link, I mean that you should try to creates as much throughput as possible by using other tools than ping. This increases the round-trip time and allows you to test the network as if everyone in the office is working. Even the fastest ping test does not show performance issues. There could be a switch or a router which cannot handle a certain load and consequenly could cause an inacceptable latency or even packet loss. 

Rebooting a router is not an option in a production environment. It seems to me that area 1 will be known to as long as the router is running. 

Currently, your router monitors WAN connections only on layer 1, i.e. if a link is up or down. In order to react to a broken WAN connection, your router should monitor on a higher layer. There are 3 approaches: a) You use a router with two interface directly conntected to the splitter or telephone jack. Your router could still be monitoring a WAN link on layer 1. But your router would dial up via PPPoA. You could monitor both the line and the registration of your router by each ISP's RADIUS or TACACS server. Either your DSL line is run by a protocol such as ADSL2+ and authentication is done by user/password or by a protocol such as VDSL2 and authentication is done by modem MAC address. In case of user/password all you need are login credentials. In case of MAC address you have to personally negiotiate the registration of your router with both ISPs. b) Your router dials up via PPPoE without the need of using new WAN interfaces. This would allow you to monitor each WAN connection on layer 2 which is sufficient enough. Your router would become the gateway and you could reduce the modems to managing DSL lines. This requires both modems to run a firmware supporting dialup by another device. c) Your router monitors by probing a host on the WAN. Usually, there is at least one core router. which replies to ICMP Echo. Try with both connections. It changes the field to request Echo-Reply messages from each hop during the trace. You should at least find one core router for each ISP which you can probe on a regular basis. Then, you have to reduce the TTL of that ping to prevent reaching that destination on detours when the monitored WAN link fails.