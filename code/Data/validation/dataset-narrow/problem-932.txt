That is wrong. If is the output interface then is the source and not the destination. But why allow state ? The easiest would be: 

All packets (connections) from that user are selected for special handling All packets are marked with 41 (disallowed). For allowed connections this is overwritten later. All connections with a locally generated first packet are allowed Everything left was not created locally. Drop everything not from lo Now allow all connections with an allowed source port (sorce of the reply i.e. local port) Last step: Drop all packets which are marked with 41. 

If Group2 is not the primary GID of the users it may be useful to either do once or add to the setfacl -d. 

The rejection is not related to DKIM but to the recipient. But why should Google mail servers try to deliver to wrong MXs? Doesn't make sense to me. The recipient system could be incorrectly configured, of course. Do the failing recipient addresses have the same MX? Something else: Have you verified the DKIM signature? Maybe it's just some technical looking text... 

And, of course, you must configure the OpenVPN server for routing to the WLAN network (if you don't do NAT in the local router). Edit 1: If all Internet traffic is supposed to go through the OpenVPN server then the configuration becomes very easy. As there already is a host route for the OpenVPN server you only have to change the default route from to the OpenVPN server's tunnel IP. 

SNAT / MASQUERADING is in POSTROUTING so by definition routing is not an issue then any more. You can even change the routing table in one of your local systems so that it does not try to deliver packets for 192.168.0.0/21 locally over its link but sends them to the gateway: 

The wording is misleading. The IP layer knows the maximum payload size of the link layer so it does the fragmentation. 

I am not sure whether this requires admin privilege but you might use stunnel. That would look to the firewall like https and the local access by putty will hardly be blocked. 

Your only risk (if you got VMware from a safe source) are exploits in VMware. Not that hypervisors never got cracked in IT history... If you are paranoid about that, 

I don't see a possibility to force sockets to a certain port but you can prevent other ports being successfully used. This can be done by and its module. You can drop all replies from processes belonging to this user from disallowed addresses. If this is an option for you and you don't know how to do it let me know and I will deliver some code in addition. Edit 1 The solution is to block every packet from this user which belongs 

You are right, does not work in INPUT but we need it in OUTPUT only. To make things easier (and faster) we mark all allowed packets (from this user) with 42, all disallowed with 41. Marking the disallowed ones would be enough. It works this way: 

I don't know how much you are willing to do but I can imagine that a FUSE module would help in your case (depending on the file size and read-ahead effectiveness): This module would have to make sure that files are read in one step and that (within the limits of userspace) these accesses are not interrupted. The next step would be sorting file accesses by position on disk, i.e. do on the file level what the kernel (and the disk itself) does with single I/O operations. Instead of having a big file system with directories you could create smaller LVs. Thus you could sort file accesses by name and would get accesses sorted by disk area. If you are willing to change your hardware then this may be interesting: Putting just the metadata on a SSD. And you should try to get write accesses off your cache disks. This may be mainly log files. They are usually not really important so it may help to put them on a file system with long commit time and . If (some of) your cache data is static (and you don't need ACL) then you may test the performance if you move it from ext4 to squashfs (compressed read-only FS). Even transparent compression (FUSE) within ext4 may help if the problem is reading a file in several steps. The file system (and disk-internal) read-ahead would get more of the file (if it is compressable). 

It seems to me that you can solve this problem by creating another file in and abusing $CTIP and $DMZS. You just turn them around, making $DMZS the IP of the container and $CTIP the Internet: 

From a network perspective it is best to solve this on the client: Check for the LAN ip and act accordingly (don't start OpenVPN; adapt routing; whatever). I don't know how to do that on Windows, though. A workaround may be to change the IP addresses the client connects to. Probably it's just a few systems which the clients connects to. It may be possible to configure different IP addresses for the same host name depending on where the query comes from. So you could create a host name samba.vpn.example.org which points to e.g. 192.168.1.15. If you have a local DNS server you can configure it so that it gives e.g. 192.168.2.15 for that host name. Your OpenVPN gateway can make DNAT from .1.15 to .2.15. That does not affect bandwidth. If the client is local it does not try to connect via OpenVPN because the target is in the LAN network, not in the OpenVPN network. So OpenVPN would still (try to) establish the connection if physically connected but the OpenVPN connection would not be used and thus waste minimal bandwidth only. A third option: Throw away the Windows client, keep Linux running in a VM, and route the company addresses (or a client-side mapping) through the VM. In the VM everything necessary can easily be done. The bandwidth loss should be quite small (if using virtio NICs). 

Your configuration says: "If a packet has arrived on br1 then send it via br1." That obviously doesn't make sense. Rules like that are for routers but you don't even mention routing. What you mean is: "Send packets which are a reply to packets which have arrived on br1 via br1." This could be done with Netfilter's packet marks and the option . But here this would not make sense. Because you use rp_filter you can simply route by target address. You don't even need advanced routing (). For that. Plain normal routing should do the job: "Send 10.0.4.0/24 via br1, the rest via eth0." You have explained what you have done but have not mentioned any task that would require a complicated setup. If you "don't do anything" special what goes wrong then? 

You need ACLs to solve the permission problem. You have to give all respective directories group write access and have to set default ACLs for them (in case the users shall be able to create subdirectories): 

Check with tcpdump what the packets look like that reach the server. Maybe SNAT has been activated on the gateway. Edit 1 This refers to the case that the access is from outside the local net so that SNAT is possible but not necessary. 

The problem is that you have read access to the directory but no execute and no write access. Execute access will replace the "?" with useful data, write access will allow you to delete the file. Write access is not enough to delete a directory entry. Execute permission is required, too. 

with the variables set accordingly. Assuming you can set the correct routing for targets in 172.16.20.0/24 only then you can do this easier this way: 

An alternative solution to the original problem may be: Stop the restore process every few minutes and check whether there are e.g. 10000 subdirectorys. If so, create a new directory (maybe not even in this one), move the 10000 directories there and create symlinks to them. This way you get the expected structure without hitting the FS limit. 

Those "admins" should be ashamed. I don't know CentOS thus I cannot give you the exact steps with the distro tool but without doubt you can configure a default gateway. Without a distro tool you may solve the problem by editing the file . 

It has advantages to serve a single CSS file from the same server because your CSS probably references several images. The access to the main server is usually faster because the client has to do a DNS lookup for the CDN host. CSS files are static, can be compressed well and are usually not big anyway. Give the web server a precompressed version of the file so that it can be sent without any delay. 

That is the intended behaviour. If you connect eth2 to a bridge then eth2 becomes unusable (even with an IP address). You use br0 on the software side, eth0 is used physically. In general when talking about bridges it makes sense to give the output of 

If there is no Apache-internal solution and we are talking about Linux here there may be an option to do this with traffic shaping. All you need is a possibility to (quickly) communicate outside Apache that the respective process belongs to a certain user. Perhaps you can rewrite the URLs of your files so that Apache calls a PHP script instead. The script would determine the user and the process ID and write both to a small "daemon" (e.g. Python script) before it sends the requested data. This daemon would add an iptables rule with --pid-owner (doesn't work any more, see below: Edit) in the mangle table and mark the packet (one number for each (active) user). You would define an HTB class for each simultaneously active user. The tc filters would assign each packet one of these classes depending on the nfmark set by iptables. You can decide whether these classes have 200KiB/s as a hard limit or get the spare bandwith in addition. This way it does not matter how many connections a user has open, the limit is global over all of them. From time to time the "daemon" would have to check whether all processes still exist and delete the rules for those which have gone. Edit It seems that the option --pid-owner has been removed from iptables. The possibility to filter based on user or group does probably not help. But a PHP script knows the destination address so that it can call a script with these parameters which creates an respective rule.