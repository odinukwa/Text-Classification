Volume III of Knuth's The Art of Computer Programming (chapter 5, verse 3.2) includes the following table listing the exact minimum number of comparisons required to select the $t$th smallest element from an unsorted set of size $n$, for all $1\le t \le n\le 10$. This table, along with the well-known closed-form expressions $V_1(n) = n-1$ and $V_2(n) = n - 2 + \lceil n/2 \rceil$, represents most of the state of the art as of 1976. 

Finally, the optimal editing sequence transforming the input strings $A[1..m]$ into $B[1..n]$ consists of the optimal sequences transforming $A[1 .. m/2]$ into $B[1 .. Half(m, n)]$ followed by the optimal sequence transforming $A[m/2 + 1 .. m]$ into $B[Half(m, n) + 1 .. n]$. If we compute those two subsequences recursively, the overall running time obeys the following recurrence: $$ T(m,n) = \begin{cases} O(n) & \text{if $m\le 1$}\\ O(m) & \text{if $n\le 1$}\\ O(mn) + \max_h \left( T(m/2,h) + T (m/2, n−h)\right) & \text{otherwise} \end{cases} $$ It's not hard to prove that $T(m,n) = O(mn)$. Similarly, since we only require space for one dynamic-programming pass at a time, the total space bound is still $O(m+n)$. (The space for the recursion stack is negligible.) 

Discrete math Discrete Mathematics for Computer Science by Lehman, Leighton, and Meyer (older version) 

So far, the only improvement to Chazelle's juggernaut is the 2001 randomized linear-time algorithm by Amato, Goodrich, and Ramos. Chazelle's algorithm is still the only deterministic O(n)-time triangulation algorithm known. 

At Pat Morin's suggestion, here is my idea for computing the turning number. Sorry if this is a bit sloppy; I'm still fighting the notation demons. Moreover, Pat's comment to Chris's answer reveals that I've ignored some important degenerate cases. But I'll post this here anyway in case others find it useful. For any index $i$, let $\theta(p_i) = \theta(p_{i-1}, p_i, p_{i+1})$ denote the signed external angle at vertex $p_i$; this is the counterclockwise angle between the rays $\overrightarrow{p_{i-1}p_i}$ and $\overrightarrow{p_ip_{i+1}}$, normalized to the range $-\pi \le \theta_i \le \pi$. (All index arithmetic is implicitly mod $n$.) The turning number of $P$ is defined as $$ Turn(P) = \frac{1}{2\pi} \sum_{i=0}^{n-1} \theta(p_i). $$ Let me call a vertex $p_i$ a spur if the internal angle at $p_i$ is equal to $0$. The external angle $\theta_i$ at a spur is not well-defined; it could be either $\pi$ or $-\pi$. More generally, the turning number of $P$ is well-defined if and only if $P$ has no spurs (and no repeated vertices $p_i=p_{i+1}$). It's not hard to prove that $Turn(P)$ is an integer if it is well-defined; in particular, $Turn(P) = \pm 1$ if $P$ is a simple polygon. Now suppose $P$ contains a walk of the form $p\mathord\to r\mathord\leadsto s\mathord\leadsto r\mathord\to q$, where $p\ne q$ and the path $r\mathord\leadsto s$ is the reversal of the path $s\mathord\leadsto r$. Then $s$ is a spur; call $r$ the root of $s$. In this case, let me define the external angle at $s$ as follows: $$ \tilde\theta(s) = \pi \cdot \mathop{sgn}\theta(p, r, q) = \begin{cases} \pi & \text{if } \theta(p, r, q) > 0\\ -\pi & \text{if } \theta(p, r, q) < 0 \end{cases} $$ (But what if $\theta(p,r,q)=0$? As Pat observes, this can actually happen. Probably there's some sort of recursive way to define $\tilde\theta(s)$ even in this case, but I don't know what it is.) If $P$ is weakly simple, then there is a simple $n$-gon $\tilde{P}$ arbitrarily close to $P$; tet $\tilde{s}$ be the vertex of $\tilde{P}$ closest to $P$. As $\tilde{P}$ approaches $P$, the internal angle at $\tilde{s}$ approaches zero. It's not hard to prove (by induction on the length of $r\mathord\leadsto s$) that the external angle $\theta(\tilde{s})$ approaches $\tilde\theta(s)$. If $P$ consists entirely of a walk followed by its reversal, $r\mathord\leadsto s\mathord\leadsto r$, then the external angles at the spurs $r$ and $s$ are still not well-defined. But in this case, I believe $P$ is weakly simple if and only if the walk $r\mathord\leadsto s$ is non-self-crossing. (There are more complex cases where I cannot define a reasonable modified turning number, in particular, if the polygon wanders back and forth through a single walk. But in all such cases, it appears that the polygon is weakly simple if and only if it is non-self-crossing.) Otherwise, if we define $\tilde\theta(p_i) = \theta(p_i)$ for any non-spur vertex $p_i$, we now have a well-defined turning number $\widetilde{Turn}(P) = \sum_i\tilde\theta(p_i)/2\pi = Turn(\tilde{P})$, which must be $\pm 1$ if $P$ is weakly simple. I'm no longer confident that $\widetilde{Turn}(P)$ can be computed in linear time. The main difficulty is that the walk $r\mathord\leadsto s$ can itself contain spurs. The naive algorithm that finds the root of each spur by brute force actually takes $\Theta(n^2)$ time in the worst case; consider an $n$-gon that has a subwalk of length $\Omega(n)$ that simply alternates between two points. 

It is undecidable whether a given computable number is equal to zero. (So your rational approximation oracle returns 0 for every ε you've tried? Maybe you just haven't given it a small enough ε.) Thus, it's undecidable whether a given computable number between -½ and +½ is an integer. 

Offhand, it looks like a polygon P is decent if and only if the winding number of P around any point that is not actually on P is either 0 or 1. 

Altogether, the total running time of the ellipsoid method is very roughly the square of the number of arithmetic operations. Since the number of arithmetic operations is polynomial in $n$ and $\log(1/\varepsilon)$, so is the running time. 

That really depends on the specific brand of "theoretical and combinatorial aspects of computational geometry" you're most interested in. Any mathematical background has a strong potential to pay off in future research. Moreover, the best topics to study to master existing research is not necessarily the same as the best topics to study to generate new research. To Suresh's excellent list, I would add linear algebra, combinatorics, graph theory, convex geometry, non-Euclidean geometry, differential topology, probability, statistics, optimization, logic, abstract algebra, category theory, randomized algorithms, approximation algorithms, online algorithms, data structures, and computational complexity. And a little computer graphics, animation, robotics, learning, vision, scientific computing, scheduling, VLSI design, compiler optimization, databases, distributed computing, ad hoc networking, sensor networking, game theory, signal processing, cartography, and graphic design wouldn't hurt, either. Or to put it more simply: Everything. Obviously you can't actually learn everything, unless you're Terry Tao. Where you should focus first depends on your specific strengths and interests. If you have a specific topic in mind, it's probably best to jump straight into some recent research papers and work your way backward to the specific background you need. (Got a nail? Look for hammers.) Conversely, if there's a particular area of mathematics you find appealing, study that area deeply and work your way forward to research applications. (Got a hammer? Look for nails.) Or best of all, work from both ends. So... what do you like? 

Consider the obvious $n\times n\times n$ generalization of the Rubik's Cube. Is it NP-hard to compute the shortest sequence of moves that solves a given scrambled cube, or is there a polynomial-time algorithm? [Some related results are described in my recent blog post.] 

This may not be exactly what you mean, but Seth Pettie and Vijaya Ramachandran's optimal minimum spanning tree algorithm is in some sense non-constructive. It is an open question whether there is a deterministic algorithm to compute minimum spanning trees in linear (meaning $O(n+m)$) time. Pettie and Ramachandran describe an algorithm that computes MSTs in linear time if such an algorithm exists. Intuitively, their algorithm reduces any $n$-vertex instance of the MST problem to $O(n/k)$ smaller instances with $O(k)$ vertices in linear time, where (say) $k = O(\log\log\log\log\log\log\log n)$. Then they compute the optimal comparison tree that computes the minimum spanning tree of any $k$-vertex graph by brute force enumeration; even if this takes quintuply exponential time in $k$, that's only $O(\log\log n)$ time. Finally, they solve the small instances using this optimal decision tree. In other words, Pettie and Ramachandran construct an optimal MST algorithm only indirectly, by constructing an algorithm that constructs an optimal MST algorithm.