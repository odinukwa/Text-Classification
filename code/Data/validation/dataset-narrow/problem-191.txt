I don't want to wait the estimated 2 months for the rollback to complete. The transaction isolation level of the database is READ COMMITED. Is it possible to kill this session? How should you act if you find such a query in one of the mission critical databases? 

I have manually resized mdf/ndf files to a big size to avoid autogrow operations on SQL Server databases. Since the files are bigger there is very little free space on disk partitions and the sysadmins keep alerting me that I'm running out of space. Because I resized them, there is a lot of free space in the data files but one can't notice it looking at file sizes/disk free space. How can I monitor the real % usage of data files? I would prefer using perfmon counters. I am conerned that when the file really runs out of space SQL Server won't be able to allocate enough space and will crash. 

You might have to change the SQL Server database engine service account (local account is default). It's simple. I would recommend using Ola Hallengren's Maintenance scripts though. They give you much more versatility. 

You definetely have to separate the files logically using different partitions (I recommend separate partitions for: system, data files (mdf/ndf), transaction log files (ldf) and tempdb files (mdf/ndf). Where you put the tempdb transaction log file is up to you (tempdb drive or log drive), I'd go with log drive. Backup files should not be on the located same device and especially the same VM (I recommend a different SAN/NAS). Placing your data, log and tempdb files on different drives is mostly a way of improving performance. Transaction log is heavily written and is probably the slowest component of SQL Server. The second bottleneck is by design the tempdb database. If these 2 components are placed on different drives (especially if they reside on SSD/RAID10 disks), the performance will be definitely better. You can keep the transaction log and tempdb on the same array as the user database files if you aren’t experiencing performance problems (at least not yet). I would look at the sys.dm_io_virtual_file_stats DMV to verify if there really is a problem with IO performance. If you want to move these files to a different disk in a virtual environment you can create a separate virtual disk for every partition you want to move and assign these disks to different LUNs in the array. 

Share the execution plan, like @YperSillyCubeᵀᴹ said. I think you can improve the performance of this query by creating a nonclustered index on requestStatus, requestId. However: the fact that you are not specifying the column list (I'm guessing SELECT *) may change the plan by introducing Key Lookup or Clustered Index Scan (probably CIS on such a big table). If Clustered Index Scan is inserted to compensate for the lookup, the nonclustered index will not be helpful. I can answer the question more precisely if you update the question with the plan. 

I am looking for a better solution. I heard that SQL Server Integration Services have a MERGE JOIN and a LOOKUP transformations. I'm not sure if one of these provide the possibilities that I'm looking for. What tool can you recommend? I'm sure it can be done efficiently in SSIS but I just don't know the right solution. 

This is a simplified version of the query. I have multiple subqueries, often referring to the same table under different conditions. If I didn't use subqueries I would get many rows for one customer but I need just one row with all the available data. How can I rewrite this query so it runs fast? The current version generates high CPU usage, over 200MB/s IO operations and it takes around 6 hours to collect data about 70 000 customers. 

pg_dump contains only the SQL statements required to recreate the database. The actual data files contain the data you inserted and all other database objects, especially indexes: clustered indexes (the data itself) and nonclustered indexes: selected columns sorted by a specified key. The pd_dump contains a CREATE INDEX statement and the data files contain the index itself (it may be very large). 

Change to . Your soultion would work only if it was an instead. With it works different. The filter is applied for the join predicate and the result is different. 

I have recently upgraded a few SSRS projects and solutions created in Visual Studio 2010 to run correctly on VS 2017. I changed the target SQL Server version to 2012 (since this is the version that is used). One of the problems I face is whenever I upload any of the RDL files to the Report Server I get the rsInvalidReportDefinition error. This problem is further discussed here: Error while uploading a report I used the recommended solution and uploaded the rdl file from the bin folder instead. This time it works fine. The problem is that the files in the main folder still do not work correctly on the report server. I am wondering if I should maybe replace the files in the main catalogue with the files from the bin folder? Can you help me understand this issue better? 

Do not use the SHRINK option. It results in serious performance degradation. Also after you shrink the data file, it will later grow even more. Please watch this video by Klaus Aschenbrenner (Microsoft Certified Master in SQL Server): SQL Server Quickie - SHRINK I would recommend to kill the SPID but be ready for the rollback operation. It can take another 3 hours, maybe less. 

I have read a lot about disaster recovery planning of databases (mainly SQL Server). There are multiple articles/book chapters about backing up databases and recovering them from backup. But what if the operating system fails? After you bring it up you will restore the databases fairly quickly and successfully (if you were prepared for that). What are the options when it comes to recovering OS in: 

I think it's a great news that Microsoft finally changed this default setting. The previous one was really bad - it caused problems with latch contention. Paul Randal's article on the topic 

I have a sample query generated by an application (Microsoft Dynamics AX 2012 in this case) which is ineffective performance-wise (cross joins, order by etc.) I would like to display its execution plan and attempt to tune it by indexing or rewriting some parts of it. I cannot just copy/paste it into SSMS because there are numerous parameters of many data types. I don't even know what are the values for these parameters. Is there a way to quickly identify the execution plan of this query? Maybe by querying some DMVs? I got the query text from monitoring software which must have done it. 

I have a few production SQL Server instances and I get around 100 deadlock alert emails a day (up to 100 for one server, 150 in total). Is this a problem I should investigate? How many deadlocks are problematic and what can be the possible consequence of ignoring this issue? 

What I want to do is to change xmlns to a new value. I tried casting the xml as nvarchar(max), using replace and then casting back to xml but it didn't work (string truncation). I tried using XQuery but I kept failing. Can you recommend a solution? 

This way the engine knows when you want to use Column from TableA and when from TableB. It is a good practice - you should always use aliases when writing SQL queries. The query you are looking for might be: 

First it is ranked by the case statement and then by the second predicate which is specified - date. Look at this code sample: 

and Microsoft websites, there are 4 Cumulative Updates available for SQL Server 2012 SP2 and the newest build is 11.00.9000. Why didn't Windows Update get Cumulative Updates too? Should I download and install them manually? 

I'm sorry if this question breaks the rules of DBA StackExchange. I'm not sure if it is allowed in here. I am planning to lead a basic SQL Server training for my coworkers. The course should include some basic information about relational databases and the SQL querying. This is why I'm looking for some training materials. Obciously I could prepare PowerPoint presentations, SQL tasks and all the other stuff myself but I think since it is a well researched topic I should not reinvent the wheel. Do you know of any training materials available that I could use? I have the training materials for the MS Exam 70-461 - Querying SQL Server but I'm not sure if I would be allowed to use it and also they are pretty big. Can you recommend something? 

Using different users will not reduce or increase the performance. It is clearly a security issue. Usually you don't want users from company A to be able to access company B data. I would recommend to create separate users for each application and grant them only the necessary permissions. If you run the application using a user with admin rights you risk a lot when the application gets compromised. 

The VMs I run SQL Server on are regularly backed up by the VMware backup tool. I would prefer not to use this backup solution but I have no choice but to cope with it. The problem I've come up across is the fact that every time the VM backup us ran, SQL Server thinks it has performed a full backup of the databases to a virtual device (physical_device_name = GUID). I can see those backups in msdb (they are copy_only backups so I don't have to worry). Do you know why it works this way? Is VMware aware of SQL Server existence? Or maybe SQL Server has wrong data in msdb? Can I run into any problems with that solution (other than trying to restore the DB from the virtual device)? 

Create a network share and assign permissions (I go with full control) to the SQL Server database engine service account (it has to be a domain account) Backup database to this share like that: 

I'm a DBA who is responsible for exporting data from an ERP system for Business Intelligence purposes. The data from ERP system is being sent to the data warehouse and they are used in the ETL process. We used to run the export process at the end of the day. The queries which collect the data used the WHERE clause with a document creation/modification date = yesterday. This mostly works just fine - there isn't too much data to export from the source system (less resource consuming on both source and destination systems). But some source tables don't have the modification date column (only the create date). At the moment I can see only 2 possibilities: 

I had a database that was working in FULL recovery mode but never had transaction log backups done. The consequence of that was the fact that the log file kept growing and now its 10 GB in size. I configured scheduled backups for this database (full + differential + log) and now log % space used is around 1%. I want to shrink the 10GB file to a sensible size (maybe 200MB with autogrow 5 MB?). How can I do it and what are the consequences of shrinking the file? 

These two queries should produce the same execution plan and therefore the same results but this one is much clearer. 

It is true that the view doesn't exist in the subscription database. How can i create it without the base tables? 

SQL Server backup operations do not acquire locks, see - Paul Randal's post. Therefore or cannot become deadlock victims. Probably some other queries that take part in your backup jobs take part in deadlocks. If you can describe your backup solution in detail, we might be able to help you. I personally recommend Ola Hallengren's backup solution to do the job.