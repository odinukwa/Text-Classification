Im also using weight decay (L2 reg, lambda = 0.001) The problem is no matter how I play with the filter parameters (size, stride, number) my network keeps overfitting. It fits the training data 100%, but no matter what I do I can't get the test accuracy over 65%. Why is such a small network overfitting? I thought it was a sample size issue, but I've read a number of research papers on EEG and BCI and they occasionally have even smaller sample sizes than I do What else can be done to regularize a CNN? 

The item count is unknown. There could be 0 items in the screenshot, or there could be many There are a lot of possible targets, each one of which are different in shape, colour, and structure Javascript isn't the fastest of languages for this type of work. This is a semi strict requirement unless its absolutely impossible to do this using JS. The fallback language would be Python 

EEG setup Data is collected from participants completing a total of 1044 EEG trials. Each trial lasts 2 seconds (512 time samples), has 64 channels of EEG data, and labelled 0/1. All trials have been shuffled so as to not learn on one set of participants and test on another. The goal is to predict the label of a trial after being given the 64x512 matrix of raw EEG data The raw input data (which I can't show here as its part of a research project) has a shape of train/validation/test splits are then created at 60/20/20% With such a small dataset I would have thought overfitting would be a problem, but training loss doesn't seem to reflect that Code Network architecture: 

I'm wondering how to interpret a recurrent architecture in an EEG context. Specifically I'm thinking of this as a Recurrent CNN (as opposed to architectures like LSTM), but maybe it applies to other types of recurrent networks as well When I read about R-CNNs, they're usually explained in image classification contexts. Theyre typically described as "learning over time" or "including the effect of time-1 on the current input" This interpretation/explanation gets really confusing when working with EEG data. An example of an R-CNN being used on EEG data can be found here Imagine I have training examples each consisting of a 1x512 array. This array captures a voltage reading for 1 electrode at 512 consecutive time points. If I use this as input to a Recurrent CNN (using 1D convolutions), the recurrent part of the model isn't actually capturing "time", right? (as would be implied by the descriptions/explanations discussed earlier) Because in this context time is already captured by the second dimension of the array So with a setup like this, what does the recurrent part of the network actually allow us to model that a regular CNN can't (if not time)? It seems to me that recurrent just means doing a convolution, adding the result to the original input, and convolving again. This gets repeated for x number of recurrent steps. What advantage does this process actually give? 

I'd like to be able to estimate whether a proposed model is small enough to be trained on a GPU with a given amount of memory If I have a simple CNN architecture like this: 

Imagine you have 2 people at 2 different microphones but in the same room. Each microphone is going to pick up some sound from the other person. Is there a good neural network based approach to isolating the signals so that the sound from each microphone only captures 1 person? I remember hearing a solution to this a few years back, but Im not sure if I remember that correctly I ask because a similar problem was mentioned to me today. During EEG brain wave data collection, each electrode can pick up signal from multiple sources in the brain. In that world they try to isolate the sources and reduce the "noise" from other brain areas, and its common to use ICA for such a task. The problem with ICA is that the post-processing stage is very time consuming, so I'm wondering if theres a better ANN/DNN approach that could solve the problem more efficiently, or maybe with better accuracy 

I was looking through a CNN tutorial and towards the end they refer to learning capacity and image coverage during network learning diagnostics What do those 2 terms mean in the context of a convolutional neural network? 

So, after you have instantiated it, just .transform each your upcoming mail_id and use results in upstream applications ( like online learning, for instance ). Obviously n_features is some knob to tune. But this has its flip side: the cardinality of mail ids is apriori high, so unless you have very limited amount of users you will need enormous n_features to minimize collisions. The better would be to take logs, where your ids coappear, and learn item2vec style model ( $URL$ ). This will deliver much denser ( and meaningful ) representation of mail_ids than FeatureHasher would do. Also take a look at this: $URL$ 

The point is to learn useful variations of data instead of just splitting by large categorial variable. Each new row after encoding becomes immediately related with the output, while original categorial variable may be related only in indirect, latent manner. Plus, the interactions between output and the original variable are represented too, by definition. Think of it as of adding this interaction explicitly to add more intuitive justification to this method. So I think this method will be perfect not only for divide-and-conquer RF-style methods but also for plain LR as well. 

Note that the test statistics are labeled “z value” and the p-values are labeled “P(>|t|)” in the table above. The standard errors can be used to construct confidence intervals for the regression coefficients. Roughly speaking, going plus or minus 2 times the standard error from the regression coefficient gives approximately a 95% confidence interval for the coefficient. For example, for Residence Length, the regression coefficient is 0.024680. The next column gives the standard error of the regression coefficient which is 0.013800. Thus an approximate 95% confidence interval for the Residence Length regression coefficient is: $$ 0.024680 \pm 2 \times 0.013800 = 0.024680 \pm 0.0276\ $$ This means that the regression coefficient for Residence Length could be anywhere from -0.00292 to 0.05228 (with 95% confidence). As is well known, we often use the odds-ratio, which is the exponential of the regression coefficient (i.e., $\exp(\beta)$ ), to help to interpret the meaning of the regression coefficient. The odds-ratio for the Residence Length coefficient, as shown in the coefficient table, is 1.0250. This means that there is a 2.5% increase in the odds of buying the magazine associated with each additional year of residence. We can also compute the odds-ratios corresponding to the ends of the confidence interval. These odds-ratios will give us an equivalent confidence intervals for the odds. So continuing the example using Residence Length, the odds ratios corresponding to the ends of the confidence interval are $$ \exp(-0.00292)=0.99708 $$ and $$ \exp(0.05228)=1.05367. $$ Thus, the interval [0.99708,1.05367] is an approximate 95% confidence interval for the odds ratio. This means that there could be anywhere from a 0.292% decrease to a 5.367% increase in the odds of buying the magazine associated with each additional year of residence. Facet 2 -- modeling data with measurements uncertainty Here again many options are known. Take a look at this to get started: $URL$ I will just cite the source: 

I see many facets of your question and in what follows will present my top 2 :-) Facet 1 -- assessing the uncertainty in estimated coefficients In logistic regression, assessing the uncertainty in the estimated coefficients is virtually the same as for least-squares regression . In both logistic regression and least-squares regression, the regression coefficient table will include a column for the regression coefficients followed by a column of standard errors, then by a column of test statistics, and finally a column of p-values. The table below shows the example coefficient table output for the some regression problem (where the probability of buying some magazine for kids is estimated). 

That's okay, as soon as you know how this should be related to the output. 0.5 in this example is just a rough approximation of input, but given that you have learned model even this approximation is still meaningful, as it instructs model -- which path to output to choose. If your model of choice is RF then 0.5 will land at about same set of leaves as more granular distribution ( if you were able to know it ) Meanwhile much stronger approximations are possible, for instance, one can use KDE to estimate response variable distribution and then draw samples from it at test time. Adding uniform random noise is just a hint in this direction. Maybe you want to follow it? Knowing only particular level of categorial variable is way less informative, than this 0.5, isn't it? 

How do these individual vectors get condensed into a single data matrix? In the case of single channel EEG (with only 1 electrode), 256 samples per second, 1 second long samples, how should this data be structured? Would it be 256 vectors? If so, what does each vector represent/contain? Or should it be 1 vector that is 256 elements long? Furthermore, how does this extend to multi channel EEG with, say, 64 electrodes over 256 time samples? I would prefer to use the raw EEG data, rather than trying some dimensionality reduction (calculating means, spectrograms etc) 

I've thought about training something like a convnet, but that feels like it might be slow because I'd need to segment each screenshot with multiple sliding windows and feed each one forward through the net. Creating those sliding window segments for each screenshot will likely take a long time. I'd like the entire detection process to be completed quickly (<1sec) Whats the most efficient way of doing this detection task? I will be implementing this using Javascript The main problems are: 

Assuming 32bit floating point values, how do you calculate the memory cost of each layer of the network during training? and then the total memory required to train such a model? 

I'm looking for a dataset containing audio clips of different languages from around the world, including a label indicating which country the clip comes from. Does such a dataset exist? Or is there an easy way to hack one together? 

I built a CNN to learn to classify EEG data (only about 4000 training examples, 2 classes, 50-50 class balance). Each training example is 64x512, with 5 channels each Ive tried to keep the network as simple/small as possible for testing: 

Ive been using ubuntu and linux mint for a while, and lately Ive been tempted to try out some of other distros (Fedora, OpenSUSE) Given that a lot of development applications and machine learning libraries (e.g. tensorflow) only provide packages for debian and ubuntu, I'm wondering how much hassle it would be to get up and running on a distro like OpenSUSE? I know I'll probably have to build from source for a lot of these things, but I've read about issues trying to find certain libraries, or trying to get cuda working on other distros Does anyone have an experience training ML models (on a GPU) on non-ubuntu distributions, specifically using tensorflow? I want to experience as little downtime as possible, which is why I havent just jumped over. The applications/libraries I typically rely on are Tensorflow, Theano, Android Studio, IntelliJ IDEA, R studio, SPSS, MATLAB, and I want to make sure they're fairly easy to get up and running before I distro hop 

I have trained a simple CNN (using Python + Lasagne) for a 2-class EEG classification problem, however, the network doesn't seem to learn. loss does not drop over epochs and classification accuracy doesn't drop from random guessing (50%): 

I've read that some convolution implementations use FFT to calculate the output feature/activation maps and I'm wondering how they're related. I'm familiar with applying CNNs, and (mildly) familiar with the use of FFT in signal processing, but I'm not sure how the 2 work together When I think of convolutions, I imagine taking a kernel, flipping it, multiplying (and adding) the elements of the kernel with the overlapping input, shifting the kernel and repeating the process. How does a FFT fit into this process? 

I want to be able to detect/localize each item on the floor, however, there 1) can be any number of items in the image and 2) each item is different I have a candidate list of all possible items. In reality each one is labelled and separated into individual image files: 

Are there any consumer level graphics cards (i.e. less than, say, $1500) that have decent half precision (fp16) support? The latest Titan X was rumoured to have native fp16 support but that didn't turn out to be true. Are there any others that support fp16, maybe on older architectures (but are still semi-decent for deep learning)? 

Note: I have tried adding more conv/pool layers as I thought the network wasnt deep enough to learn the categories but 1) this doesn't change the outcome I mentioned above and 2) I've seen other EEG classification code where a simple 1 conv layer network can get above random chance Helper for creating mini batches: