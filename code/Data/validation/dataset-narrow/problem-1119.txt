If I don't misunderstand what you mean by AND&OR gate, it is basically a comparator gate which takes two input bits $x$ and $y$ and produces two output bits $x\wedge y$ and $x\vee y$. The two output bits $x\wedge y$ and $x\vee y$ are basically min$(x,y)$ and max$(x,y)$. Comparator circuits are built by composing these comparator gates together but allowing no more fan-outs other than the two outputs produced by each gate. Thus, we can draw comparator circuits using the notations below (similarly to how we draw sorting networks). 

Note that although this sounds like $\mathsf{ZPP}$, it is not since $\mathsf{ZPP}$ is the class of predicates, and not multifunctions. I really appreciate any pointer to the right sources. 

I would like to ask if there is a name for the class of multifunctions, each of which can be computed by a probabilistic polytime Turing machine $M$ satisfying the following two conditions: 

(This is an interesting question for me because I'm also reading about the Pfaffian.) I suggest the following references: 

We can define the comparator circuit value problem (CCV) as follows: given a comparator circuit with specified Boolean inputs, determine the output value of a designated wire. By taking the closure of this CCV problem under logspace reductions, we get the complexity class CC, whose complete problems include natural problems like lex-first maximal matching, stable marriage, stable roomate. In this recent paper, Steve Cook, Yuval Filmus and I showed that even when we use AC$^0$ many-one closure, we still get the same class CC. To the best of our knowledge at this point, NL $\subseteq$ CC $\subseteq$ P. In our paper, we provided evidence that CC and NC are incomparable (so that CC is a proper subset of P), by giving oracle settings where relativized CC and relativized NC are incomparable. We also gave evidence that CC and SC are incomparable. 

I'm surprised that Petri Nets have not yet been mentioned! Extensions of Petri Nets like Coloured Petri Nets or Petri Nets with inhibitor arcs are Turing-complete. 

Any PP-complete problem is trivially in PSPACE, but of course not known to be PSPACE-complete. We also don't know whether or not PP is contained in PH either, though it follows from Toda's theorem that PH is contained in P$^\text{PP}$. I believe the same also applies for #P-complete problems. 

So if you want to reduce the error to exponentially small, then the running time will also become exponential! Note that both of the results you mentioned show that approximating counting can be done within the second level of the polytime hierarchy $\mathsf{PH}$ or so. On the other hand, $\#\mathsf{P}$ is the class of exact counting, which is widely believed to be harder than approximate counting. Otherwise, $\#\mathsf{P}$ is also contained in $\mathsf{FP}^\mathsf{NP}$, which implies that $\mathsf{PH}$ will collapse at least to the second level. 

Although the circuit $C'$ you mentioned computes a function $F:\{0,1\}^m \rightarrow L$, we can think of it as a sequence of circuits $C'_1,C'_2,\ldots,C'_n$, where $C'_i$ computes the $i^{\rm th}$ output bit of $F$. Since each $C'_i$ computes a boolean function $\{0,1\}^m\rightarrow \{0,1\}$, minimizing the circuits $C'_i$ seems hard according to the above result. 

The sorting problem is actually complete for $\mathsf{TC}^0$ (under $\mathsf{AC}^0$-reduction). A standard source for this is Section 1.4.3 of Vollmer's book. Note that $\mathsf{TC}^0$ is the class of decision problems, but we usually think of sorting as a function problem, i.e., we want to output the numbers, say, in nondecreasing order. However, we can also define sorting as a decision problem as follows: 

I think courses on algorithm design and computational complexity are always challenging for students who not familiar with these subjects because they do require some degree of mathematical maturity and problem solving skill. In my first graduate course on "computational complexity", a friend of mine who had his degree in pure mathematics told me how surprised he was by the fact that although that course didn't require much maths background (at least that's what was told in the course outline), it actually required nearly all the skills he got through all of his pure maths undergrad degree! I found that I got to know about "the way" most (when I first start my graduate study) by reading and doing exercises from Sipser's book. Be sure that you do the exercises because problem solving skill and mathematical maturity is what you want to learn and not just a bunch of facts or definitions. However, Sipser's book is only good for complexity and NP-completeness stuffs, it won't suffice to substitute the CLRS book. The only problem with CLRS book is that its advantage of comprehensive coverage might become its weakness since the book might look quite scary or overwhelming for students. So my advice is that you should really go to the library and search for books on algorithms, scan through one by one and choose the ones that fit your thinking pattern most. And again don't forget to do exercises! For algorithms, I personally suggest the following books (besides the ones suggested by Sadeq and JeffE): 

The Wikipedia page on "Post-quantum cryptography" provides a list of proposals for PKE resistant to quantum attacks. Quantum algorithms can solve DL in finite abelian groups (as well as a few nonabelian and infinite abelian ones), so they get very close to the spirit of the question you are asking. The learning with errors problem mentioned by one of the commenters is on that list. I do not know of a general answer to the question of "What is the power of DL in arbitrary (that is, nonabelian) groups?". 

Social networks typically have many vertices with just one or two connections to the rest of the graph. Such vertices will typically lead to a bad spectral gap. What you could hope for is good vertex/edge expansion for sufficiently large sets. However, if you have tightly-knit communities within the network, then again you would expect low expansion. I'm not sure if it quite answers your question, but the following empirical paper looks at exactly expansion-like properties in social networks. The answer seems to vary from network to network. $URL$ I'm sure there is other work out there along these lines, possibly disguised using alternate terminology ("community structure", cut sizes, etc). 

Here is the problem: if $M$ has low entropy (for example, if the attacker has side information that narrows $M$ down to just two possible messages), then conditioned on $M+K$, the key $K$ also has low entropy (there are only two possibilities for $K$). If the eavesdropper stores the first message (which was an encryption of $K$), then she can use it to figure out which of the two candidate values for $K$ is plausible (i.e. is consistent with some short key $K_{RW}$) once she sees the second message. This problem is essentially unavoidable: taken together, the two messages form an encryption of $M$ (the fact that they were sent separately doesn't affect a passive eavesdropper who sees both). It is easy to show that, using only a short key, one cannot achieve even Russell and Wang's relaxed notion of security in a setting where $M$ has low entropy. For a lower bound on the key length (as well as further constructions), see Dodis and Smith: $URL$ [Disclaimer: I am an author.] 

Howard Barnum, Claude Crepeau, Daniel Gottesman, Adam Smith, Alain Tapp. "Authentication of Quantum Messages", FOCS 2002. $URL$ As with encryption, there is a protocol that requires no computational assumptions. It uses a key of length about 2n+2k to authenticate n qubits with security level 2^{-k}. 

I came across this late. Don't know if this question still matters. I am posting this as an answer since it is too long for a comment. If n can be 1 but m is not too large (say, at most a small exponential in the min-entropy), a random function will be a good extractor with high probability. It will cost you a factor of something like log m in the min-entropy. However, for large values of n, you should be able to find a good deterministic extractor no matter how large m is since, once you condition on x, the distribution on outputs is IID. In particular, it is symmetric, so you can use as idea like von Neumann's trick or its generalizations to extract a small number of random bits just from the ordering of the elements. That might be enough to get the seed for a strong randomness extractor which could extract the rest of the randomness from the source. You have to be careful, but the idea would be to rely heavily on the IID structure to get a a deterministic extractor. Don't know if that's good enough for the application. 

One-way functions are implied by all of these things, and known to imply symmetric encryption and digital signatures. These equivalences are indeed found in (theoretically-focused) text books. The situation for key exchange and asymmetric (i.e., public-key) encryption is a less clear. Asymmetric encryption implies key exchange. Implications in the other direction (from OWF to KE or KE to AE) are not known--in fact, there is evidence that there are no black box reductions of this type. This line of work started with the seminal paper of Impagliazzo and Rudich, and is nicely summarized (as of circa 2000) here: groups.csail.mit.edu/cis/pubs/malkin/GKMRV.ps I think that the simplest generic assumption known to imply asymmetric encryption is the existence of trapdoor injective one-way functions. However, not think all known PKE schemes imply the existence of trapdoor injective OWF (lattice-based schemes imply something messier). So again, there is no simple answer. Much of this latter material is not covered in standard text books; you have to read papers to find it. (I hope my own answer is up to date!) 

I strongly recommend the book Computational Complexity: A Modern Approach by Arora and Barak. When I took computational complexity at my Master level, the main textbook is Computational Complexity by Papadimitriou. But, maybe due to my background in Software Engineering, I found the writing in Papadimitriou challenging at times. Whenever I had problem understanding Papadimitriou's book, I simply went back to Sipser, or read the draft of Arora and Barak. In retrospect, I really like Papadimitriou's book, and I often find myself looking up from this book. His book has plenty of exercises that are quite effective at connecting readers to research-level questions and open problems. In any case, you should have a look at both Papadimitriou and Arora-Barak. People also suggest Oded Goldreich's textbook, but I really prefer the organization of Arora-Barak. 

The chapters are of very high quality and written by leading logicians. This handbook will take you quite far into the world of mathematical logic. 

It's true that you can never learn enough Maths, and that you should learn to pick up new Maths/methods/techniques fast whenever needed. But a solid background will definitely give you an easier start into TCS. I wish you the best of luck and success! 

In general, whenever you study a certain algorithm or data structure, if somehow the exposition in your textbook is not clear enough for you, then the best way is to search on google for lecture notes of that particular topic. In may cases, different explanations of the same thing eventually give you the complete picture. At least, that's how it works for me. 

My all-time favorite application of group theory in TCS is Barrington's Theorem. You can find an exposition of this theorem on the complexity blog, and Barrington's exposition in the comment section of that post. 

A nice reference is "Part C" of Handbook of Mathematical Logic edited by Barwise. Part C includes the following chapters: 

$M$ returns a correct output of the function with high probability $M$ can either return a correct answer or "don't know", and never return an incorrect answer. 

GNU grep is a command line tool for searching one or more input files for lines containing a match to a specified pattern. It is well-known that grep is very fast! Here's a quote from its author Mike Haertel (taken from here): 

Note that these two books cover much more than just randomized algorithms, e.g. they cover Probabilistic Method, Markov Chain Theory, Martingales etc, of course with many applications in TCS. The first book is easier to read with many examples whose proofs were worked out in detail. The second book is really a classic, not very updated, but still very useful. They both have a lot of exercises, so you will have plenty of material to practice what you've learnt. 

I'm currently a PhD student and not a prof, so my suggestion comes from my (limited) personal experience as a graduate student. When I was an undergraduate student, I always worked as a research assistant in summer with different profs in my department. I personal believe that the only way to figure out if TCS is truly for you or not is to work on concrete problems and see what you can enjoy the most. It did take me quite a while to find a prof and a topic that I liked. There's also a "social" aspect in research, and different profs have different working and supervision habits, and thus these summer research jobs will give you a better idea what quality you want the most from a supervisor in the future. There are many interesting fields in Computer Science, and TCS is just one of them. So it's always best to keep your options open and talk to different profs. It's very important to specialize when you're doing PhD, but as as an undergrad I think Mark Braverman's advice is extremely relevant: 

Probability and Computing: Randomized Algorithms and Probabilistic Analysis by Mitzenmacher and Upfal. Randomized Algorithms by Motwani and Raghavan 

[Mark tried to enroll in many courses (well above the limit), and explore different areas of Mathematics and Computer Science when he was an undergrad.] Try to attend lectures and seminars on different topics in your department. When you're in your upper years, you should also ask for permission to audit graduate courses related to your interest. Also depending if you're majoring in Maths or CS, you also have to plan courses you should take to prepare you a solid basic foundation. If you're a Maths undergrad, then you should take more CS courses in algorithms and complexity which give you a more "algorithmic" mind. If you're a CS or Engineering undergrad, then it's always a good idea to learn some basic Maths courses in: