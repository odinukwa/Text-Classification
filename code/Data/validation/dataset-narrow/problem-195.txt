The position requires a broad spectrum of knowledge ranging from development to system administration and even management. Not only must a DBA know about backup, recovery, internal operations, memory and security, but also how to communicate with both developers and management. A DBA could be giving a high level presentation to management, helping a developer tune a query, provisioning disk space for a new system and restoring data from backup all within the same hour. These responsibilities require a wealth of knowledge with little overlap. The consequences of failure are usually greater for a DBA than a developer. DBAs often support dozens, even hundreds of different applications and systems most of which are vital to the success of the company. A security breach, recovery failure, or performance problem could have far reaching and devastating ramifications. This requires a level of knowledge and experience that can’t be gained in a short amount of time. The better a DBA does their job the less visibility they have. A DBA with a database that is secure, recoverable, available, and performing well will lack recognition. DBAs get noticed when there are problems. Not only do they get noticed when their problems are self-inflicted, they also get blamed when the database has problems due to poor coding, improper network setup, or incorrectly configured storage. 

All indexes will be considered in the sense that all indexes on the tables in the query are examined to determine whether they could be used. Those that could be are further examined to determine usefulness. 

I see code from developers using implicit date conversion. I would like a definitive answer to why they should not do this. 

You should implement this on the database side in the method that can be called by your create schedule time and modify schedule time methods. 

It sounds like the transaction is being committed (perhaps automatically) even when the application times out. Changing the behavior would be the best option. If you can't do that, you could put the database action in a package and then have the package check after it runs the action to see how much time has elapsed. If the step took more than 45 seconds then it should , otherwise . Although this answers your question, it is a fragile solution and you really should look into changing the behavior or causing a database disconnect when a timeout occurs. 

Re-reading your question I'm wondering if by "vehicles keep sending data" you are tracking some sort of values that are fluctuating rather than attributes of the vehicle. If this is the case you may need something like this (substituting each stat with a specific name and well defined type): 

According to this and that, a DA is a Data Administrator. The DA is a buisness oriented individual more involved in requirements gathering, analysis, and design than the DBA. They establish the flow of data around the organization and between databases and other systems. Here is a section from the second site: 

A Cold Backup is making a copy of the files with the database closed. These commands are both for hot backups using RMAN. They are both preferred over cold backups due to their flexibility. The difference between the two commands is whether or not archive logs are backed up. If you need be able to recover to any point between your backups or anytime after your most recent backup, then you will need to be in archivelog mode. If you are in archivelog mode then something needs to be done to backup those archive logs and use should use the syntax. It is advisable to periodically test a recovery as well to make sure your backups are working and you can recover. 

Race Conditions require multiple thread of execution, therefore to unit test this you will need to be able to start one or more threads. In Oracle I would use DBMS_Scheduler to run a process to simulate a second user. If PostgreSQL/Perl has a way to initiate a second process programatically, then you should be able to do something like this: Process 1                                                          Process 2 

Option #3 NULLs are your friend. After you select the platform, study how NULLs work for the platform and embrace them. In this case they accurately reflect the absence of a check-in. In Oracle NULLS are not indexed, so you could create a function based index that swaps the NULL state giving you a very small index that contains only the entries not checked-in. Option #1 would need a join anytime you want the Check-In date or status. Joins are your friend too, but in this case there isn't a benefit to separating this data unless there are times when a single checkout can produce multiple checkins. Option #2 requires repeating data or even more NULLS than option #3. Here are the three concepts fleshed out to tables columns and data. 

So, since you were able to specify a scope of SPFILE, we can infer that you are using an SPFILE, so the default for the first ALTER SYSTEM when you did not specify a scope was BOTH which would fail because the parameter is not MEMORY modifiable. You can prove this by repeating your first ALTER SYSTEM and adding SCOPE=BOTH or SCOPE=MEMORY both of which will also fail. 

In this case a function isn't necessary because the nested query will always return a number even if the number is zero. If you want to see the output for all accounts simply remove the clause. 

Occasionally I will get a script that will run fine in SQL Developer or Toad, but requires modification in order to be run successfully from SQL*Plus. Here is a worst case example containing multiple statements each with blank lines, semicolons, and forward slashes: 

See the Oracle Real Application Clusters Installation Guide 11g Release 2 for Linux and UNIX. It has a section on "Converting to Oracle RAC from Single-Instance Oracle Databases" describing several different ways this can be done. 

We have a query that was getting poor performance. The root of the problem could be reproduced using a simple query accessing only one index to retrieve one column (the indexed column) from eight rows. The table had no statistics on it, yet the index did. Gathering new statistics on the index did not change the plan, but gathering statistics on the table did. My understanding was that a query that could be satisfied using only the index would not have to access the table, therefore my mental model was that table statistics would not matter in this case, but experience seems to indicate otherwise. Both the explain plan and auto-trace plan show only index access, yet when the table statistics are not present there is significantly higher cost and cardinality. The autotrace shows higher CPU, DB Time and Consistent Gets. I have not tried to trace it yet, but I can reproduce it by creating/dropping statistics on the table as shown below. Can anyone explain this behavior? 

In theory you should login as the user with as few privileges as possible that can still accomplish the task. In practice you can maintain a better password, save time, and have more useful auditing/logging information if you login as yourself. Here is the guideline I follow: Only login using an account other than your own when… 

We recently increased the LOG_BUFFER value from 8 MB to 16 MB with only a small reduction in the retries. It is still increasing at about 1,000 a day. Should the LOG_BUFFER value be increased further? Since the system is 64 bit I can take it up to 256 MB, but I don’t want to use the memory for this buffer if it won’t be beneficial and I don’t know if there are any downsides to increasing this value that significantly. 

The built in DBMS_ASSERT package is a narrowly scoped version of what you are looking for. For other asserts Phil is correct, you will have to build your own. Here is a simple demonstration of the second option in Phil's answer+1: 

Lack of clarity to future users. (confusion as T.H. said) Interpretation that will be necessary anywhere IsFinanceAvailable is needed. The issue that would arise if zero ever became a valid EmiAmount. Eliminating this column will not have a significant impact on the size of the table or on the amount of code needed. 

Since you can't change the source database to have a materialized view log, every MV refresh has to delete all the records and re-insert them all, which creates your excessive redo. There are many different approaches you could take to this issue. Here are some. 

Another option in your case would be to put each user in a separate XE database. Each database could use database links to another database for either for common data or for all their data. While this would satisfy the requirement and perhaps be more secure all around, it would be more complex to build and maintain. 

With a logical drive striped across an unknown number of disks in a SAN disk group containing 30 drives, I'm not sure what this means. I ran the procedure for values of 1 and 30, yet the IOPS, MBPS and latency were nearly identical. Is this number just for informational purposes or is it actually used? If the the later, then what should it be set to when there is not a one to one relationship between logical disks and physical disks? Block to run procedure: 

It sounds like you are looking for something on an overview level rather than a practical level. If you are interested in learning about Oracle databases from a practical level, there is no better place to start than the Oracle Concepts Guide. 

C. For each of these tablespaces determine which segments need to be moved. (Replace USERS with the name of your tablespace or join it with the previous query) 

There is additional information and restrictions in the above documentation. Here is a demonstration of attempting to reduce the precision of a Number column and reduce the length of a Varchar2. You can try other changes so you will know what will happen. 

I found this just before posting. The difference is the linux environment setting for NLS_LANG. It was set to on the system it added padding to. Unsetting this on one and setting this on the other swapped where the behavior. 

There are some tradeoffs for creating the name_dupe table. To avoid them you could combine your first query with this one as follows: 

Bug 11720698 with the title "SYS.WARNING_SETTINGS$ NOT CLEANED UP WHEN OBJECT IS DROPPED." may be causing this. 

To say that an instance mounts a database means nothing more than the control files have been opened and read. At this point in memory it has knowledge of the database, but the datafiles, redo logs, and therefore the database are not open. The Oracle Concepts guide has already been cited, but here is the 11.2 version of the same. $URL$ 

If you never opened the database on Host2 and you have all the archive logs between the first backup and the second, then you can recover Host2 using them. If you have opened Host2, you still might be able to do this if you have flashback database enabled and can flash back to the point before you opened it. 

By doing the following I can determine if an index will benefit from compression and how many columns should be included in the compression: 

I'm looking for more information about the utility MKSTORE that can be used for creating and modifying a Wallet. I would like to know things like what the -createALO option is and what the difference is between -createSSO and CreateLSSO. A link to the information would be fine or a document number on MOS. My goal is to script the Wallet creation and am wondering if these options can help me in any way. 

The WAIT clause of the SELECT statement. The BULK COLLECT clause of the SELECT statement. The FORALL statement. 

The number of rows in this table would be mostly dependent on how often the rates change. Four building units with four categories that change once a year would have eight rows per year. If the rates changed weekly for every building unit and category there would be 832 rows per year. The table shouldn't need the Building ID as long as the BuildingUnitId is unique. It also shouldn't need the category as that can be derived from the BuildingUnitId. A rentals table would have a building unit and category columns that could be joined with the rates table to derive the cost based on the period of time the building unit is rented even if it crosses rate start/end date boundaries.