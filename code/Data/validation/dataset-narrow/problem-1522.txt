Validating the model on novel data The image you sue has to match stylistically with the ones in the MNIST data set. Ideally they should be of the same distribution. The image you provided luckily gets correctly classified but it should be noted that this may not be the case for other numbers that are typewritten. If you want your algorithm to detect these then you should really train with typewritten numbers as well as handwritten numbers. 

As a short term fix you can train your model using a subset of the data available to you and discard the rest. Doing this really is a shame however. 

Adam and RMSprop are both optimization algorithms which make the vanilla gradient descent more robust. You still need to calculate the gradient however, these algorithms will facilitate the descent in certain directions and punish the descent in others based on some criteria. Before delving into the details consider this analogy; imagine you are on a mountain and you want to get back to the village. To do this you will follow the downhill slope until you get to the bottom of the mountain. However imagine a mountain that looks like the following 

In short PCA, returns an orthogonal set of basis features that best represent the variance in the data. Intuitively, imagine you want to identify whether we are talking about a dog or a cat. Your features are: size, weight, color, fur type, etc... but you also have features like weather, owner name, etc... It should be evident that the first set of features obviously explains the variance between a dog and a cat much better than the second set. Thus, you should only consider those and completely disregard the second set. This is what PCA does however, it adds the extra restriction that all features must be orthogonal. It then assigns a metric to each component based on the amount of variance that feature explains. You can then sort the components and extract the top $n$ components (feature reduction). 

Recommendations There's a lot of resources on how to do this, especially using deep learning in recen tyears. You can look at some of the publications from Netflix, Amazon, SoundCloud. All these groups have very powerful recommendation systems. In essence what you want to do is determine some user metrics such as click through sequence, search history. Then build a database with these features and a target such as the next item that is played. From this you can train your model to predict a set of items the user is likely to click on next given his feature set. 

and is saved as a .csv file. Let's load our data into an X and Y matrix. We will be encoding the labels for 'degree' as values. Make sure these are all well spelled otherwise a new label will be created for the mispelled ones. 

Gradient Descent Code Here is an example using gradient descent to train weights along the logistic regression loss function. 

Gradient Descent The idea of gradient descent is to traverse a function $f_{LR}(\textbf{w})$ and find a local maximum or minimum for a set a values $\textbf{w}$. Gradient descent is an iterative process. At each iteration you will evaluate the function given your current set of values. Then you will take the derivative of that function with respect to those values to see how much each value contributes to the slope of the function. We can then change the values in a proportionate way to move towards this optimal point. The gradient descent equation is described as $\textbf{w}^{(k+1)} = \textbf{w}^{(k)} - \rho \frac{\partial f_{LR}(\textbf{w})}{\partial \textbf{w}^{(k)}}$ where $\rho$ is the learning rate, usually small number. A constant which determines the speed at which we want to change the values we are optimizing. Initializing the values There's many ways to initialize these values. We can either set them all to zero, or set them randomly. Then you can take a random instance in your dataset $x_i$ and $y_i$ and compute the derivative $\frac{\partial f_{LR}(\textbf{w})}{\partial \textbf{w}^{(k)}} = \textbf{x}_i (-y_i \frac{e^{-y_i \textbf{x}_i \textbf{w}}}{1 + e^{-y_i \textbf{x}_i \textbf{w}}})$. Once the compute this you can put the result into the gradient descent equation and update all your weights. You then continue to go through this process until your weights $\textbf{w}$ converge to a value, or some other condition is met. You might want to limit your algorithm with some iteration counter to avoid infinite loops caused by weights that do not converge. 

What happens if the network just learn the identity function!! For this problem the denoising autoencoder was conceived. Add white Gaussian noise to the input and train using the original image at the output. This forces the network to learn the underlining distribution of the images and not just copy them to the output. You can use the same network as above. The noise can be added as follows 

Weight Initialization Weights can be initialized by either setting them all to zero. Or by setting them randomly using a Gaussian distribution centered at 0 with a variance similar to the values of your input features. Is every initialization going to give you the same results? No. Neural networks (NNs) converge to a local minimum. Thus, different initialization of the weights will cause the loss function to be minimized to a different value. If you are unhappy with your results you can always do random restarts with different weight initialization. 

This is the accuracy whilst evaluating our instances in the training set using only the trees for which they were omitted. Now let's calculate the score on the testing set as 

For example: If the dictionary includes the words {car, coffee, motel, hotel, world, van, soup} and we have the description "the soup in this motel is amazing!". Then the resulting vector would be $[0, 0, 1, 0, 0, 0, 1]$. 

A norm is a concept in linear algebra which assigns a size to a vector. Many different norms exist you can read up on their many uses here. In this paper, they are training a recurrent neural network. These are used for time series data. The assumption is a hidden state should be similar for successive time inputs $t$ and $t-1$. Thus, the author proposes an additional constraint based on the relative size of the successive hidden state vectors as $||h_{t}||_2 - ||h_{t-1}||_2$. The more different the norm of these successive states, the greater the cost will be. 

Now we will fit our PCA transformation matrix and we will apply this transformation to both the training and testing set. 

Root words The problem here is that language has many different ways of distorting words to give slightly more information without changing the root of the word. Your database which contains the needed courses for a specific profession should ideally be tokenized into a numerical value. Or if that is not possible you can get the stem of a word as follows 

Make sure not to add too high of a degree to your polynomial or you will be overfitting!! This means although you characterize your training data perfectly, it will not generalize well to new instances. Thus this will be a useless model. That is why you need to split your training and testing data, that way you can verify if the model you build using your training data can generalize. 

3D plot 3 features indicates that your data is 3 dimensional. Thus you can use a 3D plot. The following code will plot 3 dimensional data. $x$ is a numpy matrix with the 3 features as columns, and the rows are the instances. Then $y$ is the cluster label that you obtain from k-means. 

We can now set out prior however we wish. We can assume that the coin is fair, or we can have no clue and just assume that it's a uniform distribution. This is essentially what they are doing here. The value they chose makes no difference in the results. However, I think for clarity making it equal to 1 is more obvious. 

Applying machine learning without labels is called unsupervised learning. These methods are definitely harder to train and evaluate as you have pointed out than supervised learning. I will warn that 200 features is quite a lot, the more features you have the higher the dimensionality and thus the higher the complexity. Unsupervised learning techniques are not well suited for highly complex data. In general, unsupervised learning assumes that your data is separated into $k$ separate classes. Each with a distinct distribution. The model you will choose will try to estimate the distribution parameters which describes each of the $k$ classes. What you can do to measure a fitness for your model? You can try to find an adequate balance between the similarity of instances within clusters and dissimilarity between instances in different clusters. The distance between the points within a cluster (intra-cluster distance) should be minimized. Whereas the distance between instances in different clusters should be maximized. 

You can build a dictionary of character sequences (words) and for each instance of text you will count the occurrence of these words. You can either use groupings of characters n-grams or words them selves using bag-of-words. n-grams n-grams is a feature extraction technique for language based data. It segments the Strings such that roots of words can be found, ignoring verb endings, pluralities etc... The segmentation works as follows: The String: Hello World 2-gram: "He", "el", "ll", "lo", "o ", " W", "Wo", "or", "rl", "ld" 3-gram: "Hel", "ell", "llo", "lo ", "o W", " Wo", "Wor", "orl", "rld" 4-gram: "Hell", "ello", "llo ", "lo W", "o Wo", " Wor", "Worl", "orld" Thus in your example, if we use 4-grams, truncations of the word Hello would appear to be the same. And this similarity would be captured by your features. Bag-of-words Bag-of-Words builds a dictionary of the words it has seen during the training phase. Then using the word the frequency of each word in the example a vector is created. This can then be used with any standard machine learning technique.