I don't think squid implements any logic to automatically retry and SSL downgrade if the connection failed. So you have only the following options: 

Because you did not remove the vulnerable version of OpenSSL it is still on the system. Your new installation did not replace the files, but added new ones. Because the existing applications were linked against the old library the might continue to use it. So better upgrade your system the usual way, because fixed libraries are available for it. 

This means a call of in the browser will connect with TCP to port 443 (default for HTTPS) and then try to start the HTTPS request by starting the SSL handshake. The handshake will fail since your server only expects plain HTTP but not HTTPS on this port and thus does not expect a SSL handshake and thus will abandon the handshake or send some plain HTTP "bad request" as response back. This again is unexpected by the client which then shows a SSL problem in the browser. The reason for this misconfiguration is likely your where you have some listener on port 443 but do not enable SSL for it. It does not help that you have another listener on port 443 in your with SSL enabled because you can only have SSL (i.e. HTTPS) or not SSL (i.e. plain HTTP) on the same IP same port and not both at once. What you probably need to do is enable SSL in and also add some certificate there (you might use the site-specific certificate). 

There is probably a problem with your settings, i.e. how you connect to the server. It looks you are trying to do implicit TLS, where TLS gets used directly after the TCP connection got established. But this is wrong when using the standard FTP port 21, because in this case explicit TLS is expected, where it first creates a plain TCP connection and then upgrades this connection to TLS after issuing a command. 

I don't think that it is a workaround, if the server first negotiates the best cipher it can and then decides based on the quality of the cipher which page to serve to the client, e.g. the secure page or the fallback page for insecure ciphers. On this fallback page it can provide information about the problem or redirect the client to other information. I don't see any advantages to build something like this into a future version of TLS, because it is much better to provide these information with a less secure cipher than not encrypted at all, like it would be done if the negotiation failed completely. But it would be nice if servers added support for easy use of this behavior, e.g. a way to distinguish between secure and less secure ciphers and make it easy to add error pages for the latter case. And of course all want that SSLabs and others can detect this behavior so they don't get bad marks when supporting insecure ciphers just for this error messages. 

If you don't need to have HTTPS from start, that is if the client usually connects first with plain HTTP, then you could try to detect SNI support so that you can redirect the client later. This might be done by including an image, some JavaScript or similar stuff from your HTTPS-site and if the loading succeeds then you know that the client either supports SNI or ignores certificate errors. Of course this leaves everything open to man-in-the-middle attacks, because all the man-in-the-middle has to do is to serve some different certificate or make HTTPS unavailable at all, because in this case you will never try to upgrade the connection to HTTPS. Additionally this can be used to make it look like the clients supports SNI, if the man-in-the-middle does it instead. And not only non-SNI clients are affected by this but SNI-capable clients can only be intercepted. So while this would be possible in theory it is not recommended because you can simply man-in-the-middle everything and thus make the main point of using HTTPS moot. 

In short: there is no way to do what you want. You would need to install a CA certificate in the proxy which is already trusted by the clients, i.e. one which was issued by a public CA. While it was sometimes common in the past to get such certificates fortunately today CA's can get publicly distrusted in the browsers if they issue such certificates so you will not get such thing. Apart from that even if you would get such a thing it will cause problems with connections using certificate pinning, i.e. google, facebook etc. Since SSL interception will change the certificate the browser will complain that the certificate does not match any longer and deny access. Pinning is only automatically disabled by the browsers if the new certificate is issued by none of the default CA's and if this new CA was explicitly added as trusted. 

Chrome on Desktop tries to actively work around this problem but other clients like curl or most mobile browsers don't work around such broken setups. To fix you need to add the missing certificate (i.e. the extra download) to the file used for the setting. Many CA issuers actually provide documentation on how to use their certificates with different servers so maybe you should have also a look there. Apart from that there are other serious problems with the SSL setup of the site, see the SSLLabs report for full details. 

No. According to the rules of the CA browser forum, RFC2818 and RFC6125 only one wildcard is allowed and only in the leftmost label. Which means there is no and no either. You need instead to add all the domains you need in the subject alternative name part of the certificate, but you could have multiple entries and and you can use wildcards, i.e. , etc. Such certificates with multiple wildcard names are common (look at the certificate for facebook) which means there are SSL providers which offer these certificates. But they will cost more than others. 

This means that no SSL handshake was done (no bytes transferred for SSL handshake). The attempt to initiate the handshake caused errno=104 which is at least on Linux "Connection reset by peer". This is caused by the server closing the connection to the client immediately after the TCP connection setup. 

While according to the protocols TLS 1.0 .. 1.2 are supported by the server the problem might be the set of ciphers which is unfortunately not shown in detail. There are a few ciphers which are defined new with TLS 1.2 and which can only used with TLS 1.2, notably all ciphers using SHA-256 or SHA-384 as HMAC. If only these ciphers are accepted by the server then this implicitly means that only TLS 1.2 protocol is accepted. For example the modern profile as currently shown by the Mozilla SSL config generator only includes ciphers available with TLS 1.2 or later and can thus not be used together with older TLS protocol versions. 

The symptoms you describe look very much contrary to this claim. Chrome (and IE?) download missing intermediate certificates by themselves while Firefox and most mobile applications do not. SSLLabs will not mark these intermediate certificates as missing but they are marked as "Extra Download". If this is really not the case check if your server has IPv4 and IPv6 enabled and if the setup for IPv6 is different. SSLLabs does not check the IPv6 setup. If IPv6 gets used depends on the OS, connectivity and preferences of the browser so this might explain such differences too. Other differences in this area are different IP address of the server depending on the location or different tests, i.e. sometimes and sometimes only . 

Unless you have a very low latency but low bandwidth connection the main performance problem is not the number of bytes transferred in the TLS handshake but the several round trips needed for the setup of the TCP connection and then the TLS handshake on top. Thus is would be much better to change your application so that it uses the same TLS connection for multiple messages or at least to implement session resumption. Session resumption not only reduces the round trips needed but also the number of bytes sent within the handshake considerable. And as suggested by the other answer wireshark is a useful tool to track such problems (like number of bytes transferred but also latency) and also to see the effects after optimization. 

Given that the device will not usually have a public domain name you cannot get a certificate from a public CA for it. And, even if it has a public domain name it is usually in control of the customer and not you so you cannot get a public certificate for it either. The common approach is to create a unique self-signed certificate for each device and expect the user to add an certificate exception. Additionally a way for the customer to upload his own certificate should be provided so that the customer can integrate the device better into his own infrastructure. 

There are several problems with the transporting email a secure way and this questions might be better asked at security.stackexchange.com. The mail can be intercepted at various stages: 

It is possible with HTTPS the same way as with HTTP as long as all of the target servers have a valid certificate for the target domain from the redirect. If you control the DNS of the domain another option is to use DNS based load balancing, i.e. have multiple IP addresses (with different servers behind this address) for the same domain and rotate these whenever your DNS answers a new DNS request for the domain. 

Yes, it can still be redirected. But the mail itself can still only be decrypted by the real recipient, i.e. the owner of the target key. 

A browser should never downgrade by itself to http if https does not work, because all an attacker would need to do is do make https unavailable (e.g. block port 443). So the only way this could be done is by instructing the browser from the server to do so, e.g. by sending a http redirect. Of course this should be sent over a secure connection (otherwise a man-in-the-middle could fake it) but unfortunately it is exactly your problem that the secure connection fails. In summary: No, it is not possible and it is better this way. BTW, all modern browsers support SNI, but not all applications (e.g. Java apps etc) do. If you have multiple domains on the web server but only a single one needed by these apps you could set the certificate for this domain it as the default. Otherwise you would need to get a (more expensive) certificate which contains all the needed domains as subject alternative names. Edit with another idea: what you could try to do is to download an image from your side as https and check the success with an onerror handler on the img-tag. Maybe this does not trigger a user-visible warning but instead just fails to load. And if it succeeds you know that https access is possible and redirect the user. Apart from that you have to ask yourself, why you want to offer https at all, if you accept the access with http too. Either there are data which should be protected or they are not. As long as you offer a fallback to http it is easy for an attacker to enforce http instead of https. 

Thus the name nutritionmedicine.org.au is part of the certificate while www.nutritionmedicine.org.au is not. Which means the name in the URL does not match the certificate and therefore the certificate can not be trusted for the name in the URL. If you want to access the site with https and the www prefix then you must create a new certificate which includes this name. No DNS aliasing, redirection etc will work. 

I doubt this. Maybe it was working, but I doubt it was working fine. Maybe you just did not realize before how insecure your configuration was. 

This is not really an answer, but it might grow into one if you provide more details. Here are some steps you need to do to narrow the problem: 

The certificate returned by the server does not match the name in the URL. Based on this description you've ordered a certificate for but try to access the site as which is not the domain the certificate was issued for. This problem might be due to a wrong understanding of how the comparison of the domain in the URL against the certificate works. In general: 

Browsers usually do a SSLv23 handshake. With this handshake they announce the best protocol version they support (mostly TLS1.2 today) but don't restrict the server to this version. Thus a server, which has a properly implemented and configured TLS stack but only supports TLS1.0, will simply reply with TLS1.0 and the connection will succeed on the first try. But, there are some bad TLS stacks or misconfigurations or some bad middleboxes (load balancer etc) on the servers side, which cause this SSLv23 handshake to fail. In this cases browsers downgrade the protocol used in the handshake, that is they try with an explicit TLS1.0 handshake followed by an explicit SSL3.0 handshake (some browsers have disabled SSL3.0 already and thus don't try this). Newer browsers will use the TLS_FALLBACK_SCSV pseudo-cipher if they do such a downgraded connection. If the server capable of TLS_FALLBACK_SCSV detects a downgraded connection with a lower protocol version it supports (e.g. downgrade uses TLS1.0 but server supports TLS1.2) than it assumes that a POODLE-like attack happens and will close the connection. But why might a client downgrade in the first place when contacting your server? 

This kind of setup is not well supported by the servers. The most you can probably do is to allow each of these protocols but then check inside your web application which protocol is used by the client (environment variable SSL_PROTOCOL with apache) and display a warning message instead of the real page to all clients not meeting your minimal requirements. Of course any analyzers like SSLLabs will give you a bad rating because they will only see that you support these older protocols and not how you handle such legacy clients. 

The client does not provide a list of supported versions. The client provides the best version it can offer and the server is supposed to reply with the best version supported by the server which is equal or lower to the clients version. If the client then is not willing to accept a too low version the client will throw an error and close the connection. 

That does not make much sense. With UDP you receive either the full packet or nothing, unless the application explicitly calls send with a length smaller than the packet and set also to accept packet truncation. So any limits here must be done by the application. An UDP packet itself can be up to 64k and I'm sure that Java itself does not change every receive call of the application to use a small length only, otherwise lots of other Java-based applications would have problems. So please check how the reading is done inside the application. I assume that their is an explicit limit when reading. You might also run the application with strace and check how the application calls recv or recvmsg, there you might see the size the application requests. Of course it could also be, that the application never receives the packets because the get filtered outside the application. In this case you should check with tcpdump/wireshark if the packets arrive at the machine at all, e.g. don't check at the sender side but at the recipients side. 

From searching a bit on the web it looks like that Time Warner has IPv6 enabled for their customers. Since all modern OS can do IPv6 and will use it if available this matches my problem description. 

By using an absolute URL you issue a request against an HTTP proxy. To make a request against a HTTP server you need a relative URL. Also, HTTP/1.1 the use a a Host header: 

Basic authentication can be done on any port. Basic authentication also does authentication for a specific resource and does not involve any redirects at all: instead it just requests the same URL again after the password dialog but this time with credentials inside the HTTP request. Since no redirect is involved with basic authentication this means that the redirect you see is not caused by the authentication but is explicitly configured somewhere in your config. 

Thus, either get a single EV certificate which includes all the names you need or get multiple EV certificates. 

The error indicates that you are using https and not http as you've shown in your link. Based on your description the error happens only from a specific network where a proxy is used. This is probably due to the proxy intercepting TLS connections and blocking access to this site for whatever reason. In this case you need to ask the IT administrator responsible for the proxy for help so that the site does not get blocked any longer. 

Yes this can be done. If the protocol is HTTPS you could use a variety of software like Fiddler, mitmproxy or even squid which have this feature for TLS interception and analysis. Also various firewalls have such feature for this purpose. You could also use socat to work as TLS server on one side and TLS client. But in all these cases the original certificate is lost and a new one used and not all of these solutions properly check the original certificate.