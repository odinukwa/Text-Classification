First, you need a network design that allows for network monitoring. One could use lots of monitor ports. Alternatively, use network hubs instead of layer 2 bridges or switches. Then install IDS (Intrusion Detection System) on every network segment. 

This seems to me to be a layer 2 issue. You want to prevent some (all?) broadcasts from going from one user to another. I think most ISPs have a point to point link with their customers. Didn't cable companies use PPPoE to simulate the point-to-point connection? I think managed switches can be set up to filter traffic. Finally, perhaps you could configure DHCP relay if your switch supports it. 

I would recommend against using wireshark to monitor traffic. You'll just get too much data, but you have a hard time analyzing the data. If you need to look at/troubleshoot the interaction between a couple machines, wireshark is great. As a monitoring tool, IMHO, wireshark is not quite the tool you need. 

It depends. As mentioned by others, some applications work better on 32bit OS vs. 64bit OS. The reverse is also true As mentioned by others, 64bit OS allows for more RAM access than even 32bit with PAE enabled, although PAE enabled OS can general access a lot of RAM. What I have found, much to my dismay, is that the chipset/BIOS puts a limit on the maximum memory I can use. Even with 64bit Ubuntu, my system still can only access 3.2GB of RAM. 

The scenario describes seems to be an intended limitation in functionality designed to prevent end users from inappropriately opening holes in the firewall. You may need to 1) Adjust you network config, for example by moving the server to the DMZ 2) Get a different ADSL modem or router 3) Get a firmware update. 

I then looked at the isolinux directory on the CD, and all the boot menus had something like , so I tried: 

Off the wall idea - is the all of the stored information needed or even useful? How much is the information actually worth? It seems obviously ridiculous to spend more in upkeep and management than the data is worth. Is the data in the database appropriate for storage in a database? For example, does keeping compressed multi-gigabyte core files in the support organization's database really provide any actual benefit? Is there a lot of duplicated data in the database? For example, are a thousand people keeping ten copies each of a weekly 10MB newsletter? Does some of the data have an "expiration date" after which it does not provide any value? Returning to the support organization example, for various reasons there is virtually no benefit in keeping around customer core files more than a few months after a fix has been delivered. Another thought - is keeping that much data opening the company to liabilities. Some data one must, by law, keep. Some data, however, should be "shredded" because of the risks posed if it is accidentally, or maliciously, released to inappropriate parties. 

Echoing wombie's concern, I don't think you want the server trying to do big data copy jobs in parallel. Whether you are trying to copy multiple partitions, which wombie predicts would cause the disk heads to thrash and slow things down, or to trying to copy multiple disks over a usb bus, in which each data stream may cause interrupts that would slow each other down, unless you are dealing with a transmission technology specifically designed to handle high throughput from multiple clients, you are going to slow things down if you try to do them in parallel. For example, trying to ftp a single file over 10BaseT Ethernet, I could get over 1 MByte/sec (over 8Mbit/sec) throughput, but if I tried to ftp two files from different machines, even to the same server, the throughput would fall to about 150 KByte/sec/per transfer (i.e., about 300 KByte/sec, 2.4MBit/sec). (This is from memory, and it may have taken 3 transmitting stations to get the 10BaseT throughput to drop from ~90% to ~30%. Still, adding a second station did decrease the overall efficiency, due to collisions.) Besides, its a catch-22: the protocols that can gracefully handle multiplexing high throughput streams generally introduce high overhead. Classic examples of networking protocols that gracefully handle multiplexing high throughput streams: Token-Ring, FDDI, ATM. For example, ATM introduces a minimum 10% overhead (of the 53 bytes in a cell, 5 are header) to the transmission. Whether you use dd, partimage, or clonezilla, I would suggest: 

In the USA, many or perhaps most of the Internet Service Providers are not commonly offering IPv6 connectivity. IPv6 access is available for purchase, but to my knowledge, the service carries a premium. In 2006, I was told that IPv6 connectivity was being widely used in some markets - I believe that China was named as an IPv6 market. Most commercial network gear, e.g., not USA consumer products, supports IPv6 routing and dynamic routing protocols. Microsoft OS products all support IPv6, and most Linux and FreeBSD distributions have supported IPv6 for over ten years. The IETF working group does have the experience of IPv4 to guide their engineering efforts, so it may be that IPv6 will introduce little, if any, performance degradation. The biggest complaint I have heard about IPv6 is that it, at least in the past, required that all hosts be directly reachable by all other hosts, i.e., have a public Internet address. Some security engineers thought this was a bad idea. 

Does the device have an internal fan? From what I could find on the Internet, I can't tell, but even without a fan, the device may have accumulated dust inside which is preventing airflow. 

The distributions I have seen clearly state the minimum requirements for your white box. Go 1.5x to 2x those recommendations. 

I would suggest not attaching a machine directly to the Internet. Place some kind of firewall between the machine and the Internet. This allows you to do security and network monitoring without putting more load on the server. Personally, I find network and function segmentation frequently simplifies network troubleshooting, although on occasion, the additional complexity does make analysis more difficult. The safest, but most annoying to manage, firewall policy is to deny all and explicitly allow only the traffic you must allow. This is annoying, because one frequently needs update the firewall policy as the network needs change. I would also suggest using some kind of interface firewalling on the server - defense in depth is the key. Using non-standard ports for administration related services doesn't hurt. fail2ban is fine. Pursue the more specific questions about security applications on Serverfault to find more ideas. Security is like the joke about the two hikers and the bear - while one can never achieve perfect security, it is helpful to be a more difficult target than the other guys. 

Virtualize your WinXP installation Install Ubuntu 9.04 Install Ubuntu package vpnc (vpn cisco) Install VMWare Run WinXP inside VMWare with NAT interface (not bridging) Establish your VPN with vpnc in Ubuntu, and the Virtual Machine will use that VPN connection. 

The builtin filesharing has always worked for me. Install the vmware-tools and configure vmware to enable filesharing. Granted, this does open a security hole from the virtual machine to the physical machine, so be careful. The FileZilla server is pretty easy to install. Install, and then under settings, configure a user. 

I installed Centos 5.5 on a headless server. The BIOS supports console redirection, so all I needed to do was: 1) Connect over a serial cable, 2) adjust the BIOS, 3) put the CD in, and 4) at the boot prompt, type: 

Divide the problem space in half. Rule out one half of the problem space. Repeat with remaining problem space. 

Protocol analysis is not hard, but it is tedious. The basic process is iterative, with the results from the previous step serving as the input for the next step of the analysis. Basically, you are always comparing what should be happening with what is happening, and noting the anomalies. I would suggest starting with a raw packet capture with a simple filter to limit the capture to the problem subnets. Depending on the Application Layer Protocol, I would limit capture size to ~100 bytes or so - enough to get the TCP and lower layer protocol headers as well as a little bit of the Application Layer. Once you know that you have an example of the problem behavior, load the raw packet capture into your protocol analyser of choice - tcpdump, wireshark, Netscout Sniffer, whatever. Now you can start looking for more patterns that allow you to isolate the problem traffic. If you can isolate the traffic, then you can analyse it. In the comment, mas made a good recommendation for filtering based on SYN/ACK frames and seeing if there are IP addresses which have a large number of open connections. You can then look at the connections from those IP addresses and count how many sit idle and how many exchange actual data. Take a look at the data being exchanged. Does the Application Layer Protocol data make sense for your application? Count the number of connections where it makes sense vs. the connections with anomalies. For some well known problems, Expert Engines have been created that can automate some of this work. In my opinion, this is larger what IDS is, an Expert Engine, or suite of Expert Engines, that automate the analysis of packet captures. You may find a package that does the analysis you need to do. In the meantime, you can start analyzing the data you have. If all you have is tcpdump, you have to use it, but I prefer the graphical protocol analyzers, exspecially if that have some tabulating or graphing functionality. The GUI helps visual the data, and many conveniently color-code parts of the packet for easier reading. 

Your third idea seems most promising: consider an FTP Proxy Server. Your users connect to the proxy with the connection requirements you set, such as encryption, and the proxy server connects to destination server with the parameters you configure. Unless you can either enforce or audit a policy, you can not get users to follow it. And a security framework is only as strong as its weakest link. The scenario seems strange to me - you have a requirement for confidentiality, to be met by encrypting the data traffic, but you are working with a third party that won't meet this requirement. There may be a need to run the problem up the management flag pole, as well. 

The case study of Largo, Florida, may prove informative. They migrated a significant number of non-technical users to a Linux based thin-client network design and realized significant cost savings as well as increased productivity (due to reduced workstation downtime and improved user data backup) as a result. Slashdot profiled the city several years ago. Since that article, it seems that the city migrated to a Citrix solution. 

I would suggest setting up a linux based router to perform the dual-homing. Then configure source based routing to achieve the equivalent of iproute2. It seems to me that split-access routing is a special form of source based routing. Alternate suggestion: move the server to a linux platform and use iproute2 if that is the routing solution that meets all of your needs. Alternate suggestion: virtualize the Windows server, run on a linux server, and let the linuxe server running iproute2 do your routing. It sounds like you will be accepting limited non-connectivity periods, so you will not need to worry too much about the Layer 3 routing issues that plague VRRP or clustering architectures. One thing to keep in mind, however, is that many applications and intermediate packet inspection points will expect consistent routing of a given connection, i.e., "Persistent Connections". Maintaining persistent routing of connections has been a pain in the sides of network and security architects for some time. If you can't afford a commerical product, have you investigated: 

You may need to go to your vendor. Dell has a hardware diagnostic cd for dell hardware. As for a more generally applicable suite of applications, I'm not sure. 

This kind of configuration has been used for years. Establish VPNs between sites. Then enable a dynamic routing protocol to share network information between the sites. In my experience, the routers will have some kind of virtual Point-to-Point link between them, perhaps a GRE tunnel or L2TP. The dynamic routing protocols treat this link like any other interface. There are some vendor/implementation specific configuration issues with the VPN configuration - consult the documentation, the vendor's support organization, or describe what products you are using. One key point relating to network design - you need to treat all the sites as part of one large network. For example, you can't configure all the remote sites to have a 192.168.1.0 subnet. Rather, you might be able to get such a nightmare to work with NAT and with a very convoluted routing configuration, but it is so much easier to design all the sites as being part of one network space. 

A blank white page suggests to me that the webserver is up but not serving any useful data. I would start by assessing network connectivity. Is DNS returning a correct IP address? Am I reaching the intended host? Next, I would check the status of the server. If you control the server, go in and check server logs, lists of processes, standard sysadmin stuff. Note, some people like to check the server first and then check network connectivity. If you have a webhosting provider, you may need to call customer support. 

These purpose built distributions take a lot less of your time to get locked down and running Management drudgery can be handed off more easily 

There are also manual pages online, so if your particular system does not have the man pages installed, you can still get the needed info. Many modern utilities have built-in help info. Try 

I am using a rack mounted device that has 10 Ethernet ports built into it. For 8 of the ports, I would like to bind them to a single vwmare-server-2.0 virtual network interface, so that I can, in effect, use the ports as a hub for the external devices and internal virtual machines. How can I bridge a single vmware vmnet# interface to several physical interfaces? 

On the old machine: $ dpkg --get-selections > installed-software On the new machine: $ dpkg --set-selections Finally, on the new machine: $ dselect and the packages will be installed. 

As an alternative to "rolling your own", consider the appliance vendors. Over the years, I used a number of "White Box" distributions and been quite satisfied. $URL$ There are two big benefits to you: