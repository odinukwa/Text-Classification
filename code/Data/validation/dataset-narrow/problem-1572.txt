EPS is "Encapsulated PostScript". Its meant for embedding like an image in documents, or sending to printers. You can view it with a PostScript document viewer, and there are free PostScript document viewers for Linux, Windows, and Mac OSs. Ghostview, Evince, etc etc. So although you can view the graphic once you've got a PostScript document viewer, you cannot load it into R as if you had just plotted it. 

If a model predicts useful information for a class of customers, maybe customers over 50, or those with more than 1000EUR, then that's useful even without knowing who the individuals in the model are. The actual data doesn't seem to be anonymous data though, it is implied to be synthetic data or possibly from another bank altogether ("does not include any real Santander Spain customers"). 

Possibly because google was predicting based on changes from previous elections, possibly combined with opinion polls, on a precinct-basis. If all the pro-Trump precincts report in first, you'd still see a blue box because of the very low probability of historically Democrat precincts voting Trump. "Calling" something in a US election is not an official result, its a confident prediction by an agency. Sometimes these are wrong. Search for "Dewey Defeats Truman" for a classic example. 

Factors are stored as numbers and a table of levels. If you have categorical data, storing it as a factor may save lots of memory. For example, if you have a vector of length 1,000 stored as character and the strings are all 100 characters long, it will take about 100,000 bytes. If you store it as a factor, it will take about 8,000 bytes plus the sum of the lengths of the different factors. Comparisons with factors should be quicker too because equality is tested by comparing the numbers, not the character values. The advantage of keeping it as character comes when you want to add new items, since you are now changing the levels. Store them as whatever makes the most sense for what the data represent. If is not categorical, and it sounds like it isn't, then use character. 

Did you think this was R code? :) You need to loop over to compute the new values. Use an implicit loop: 

So you want to do a Diebold-Mariano test eh? How about the Diebold-Mariano test function in the package of R? 

You've described a null model for a logistic regression, which in this case would predict P(will buy X)=0.05. Any other model that incorporates your covariates of age and salary, will do better. The fact that there are 19 zeroes for every 1 in your response shouldn't stop you trying to make a model that does significantly better than a null model. So just put age and salary in as linear terms in the regression and go from there. 

There seems to be no method for objects of class , so the default summary method is used which prints the internal list elements. Just print the object to get a nice summary - maybe that's what you want? Is this the output you expected? 

which you can interpret in the usual way. The process you describe about choosing which variables are most influential sounds like you just want the variables with the largest point estimate, but you may want to do some standardisation. For example, in the above, a unit change in from 1 to 2, say, decreases the by -570000, but a unit change in changes the response by +41.09 in the fitted model. But has a much bigger range, so perhaps you want to standardise your numeric variables to have mean 0 and sd=1 first. The factor variables have a similar interpretation - rows with "yellow-yellow" increase response by 48280 compared to the average change in response. I'm not sure how you standardise categorical variables, since they are just a bunch of step-changes... Statistically none of these variables make any difference to the response (none of the t-values are big enough and there's no stars in the P column). 

[I don't know why you say 16, maybe you've filtered or something, I can't reproduce your work because you've not shown your working.] So what were you trying to do? Get the row where is the median value of its values? Well you can do that, but its pretty pointless because there's no guarantee that the median of a set of values will be one of the values. So if you try this you get nothing: 

after being taught enough examples, it should figure out that 73 plus one is 74, even though it really knows nothing about addition, its just discovered the pattern from the examples. So what does look like? Well that could be a neural network, or any other machine learning system really. Your question is quite vague so I cant be specific. 

If you suspect your response is linear with year, then put year in as a numeric term in your model rather than a categorical. Extrapolation is then perfectly valid based on the usual assumptions of the GLM family. Make sure you correctly get the errors on your extrapolated estimates. Just extrapolating the parameters from a categorical variable is wrong for a number of reasons. The first one I can think of is that there may be more observations in some years than others, so any linear extrapolation needs to weight those year's estimates more. Just eyeballing a line - or even fitting a line to the coefficients - won't do this. 

This means as and increase, the fitted probability of zero increases because their signs are negative. As increases, the fitted probability of one increases because it has a positive sign. So.... 

So now see how the change in response from to depends on the change in the (numeric or factor) variables: 

PPI is only relevant for physical objects, like representations on screen or paper. It is pixels per inch, and defines how big an image X by Y pixels appears on that object. So if you have a 256x256 pixel image, and you show it at 256 PPI on a screen it will be 1 inch square if you put a ruler up to it. Many physical devices have natural PPI characteristics. Screens are often 72 PPI, 100 PPI, or about 300 PPI for "Retina" devices. Laser printers start at 300 PPI and higher. So if you show a 256x256 image on a 100 PPI screen as a direct mapping of one image pixel to one screen pixel, you'll get an image 2.56" across. If you zoom into the image you can make it bigger on the screen, and it will now cover more than 256 pixels of the screen. The underlying image data will still be an array of 256x256 values Scanners are another area where PPI is important. If you scan a 1" image at 300 PPI, you get a 300x300 pixel image. Set your scanner to 1200 DPI and you get a 1200x1200 pixel image, that is much finer detail. In summary, PPI is irrelevant to CV or ML algorithms because they only care about the number of pixels. 

I've recoded it to "OK" and "notOK" because your remapping doesn't make sense. The "from values" should be unique, not have repeated "yes" and "no" in them. Note how this is done. Make a tidy data set by melting. Mutate it. Cast it back into untidy format. Yes you could use pipes. 

BMA is "Bayesian Model Averaged". GNET is "Generalized Elastic Net". Have you tried reading the Ishwaran and Rao papers as mentioned in the documentation for ? There's an article in the R Journal as well that might be worth reading too: $URL$ - no sense duplicating it all here. 

So if you set to 4, then is 12, and you've only got 10 observations so it refuses to split. Because: 

All full graphs of the same order and direction are the same, so just make a full graph of the right size and then put your vertex data on the vertices: 

The difference comes about because the sample mean and sd estimators are assuming the data is a random sample from the distribution on the full range of the distribution, in this case -Inf to +Inf. In your second case above there's no data below zero. The sample mean estimator can't see that you've only got half a Gaussian there, it will always be a positive number. On the other hand, the curve-fitting approach of the least-squares fitting procedure (which is presumably possible via EM, or any of a dozen other algorithms) can see the data as part of a curve that has a Gaussian density shape. If the data is just curving upwards, it will fit to the left side of a Gaussian. If its curving down it will fit to the right side. If its flat it will either fit it to the peak with a very large variance, or a tail and might even go degenerate and divide by zero. So you are doing parametric estimation of a distribution, or curve-fitting, which are categorically different things. 

also you probably want to try asking this on StackOverflow, or reading a bit more basic R documentation because it should be well covered. Its a bit simple to be "data science". 

If you have R and the package then you are only away from a list of space-time data sets bundled with the package: 

Because your data has 30 rows and your loop is from 1 to 10 so when it gets to then which is 11 * 3 which is 33 and then you do but is only 30 long so you get: 

Which you would have spotted if you'd tried running yourself. You want the of the first element of the list : 

With the default scale, you see that numbers left of the bar go up by two - hence anything after the 4 is forty-something or fifty-something: 

Examples are the ranking system you describe or any question with categorical but ordered answers often seen in surveys ("always", "sometimes", "never"). 

In statistics coping with missing values is often done by imputation: $URL$ and whole books have been written about it. Suggest you start reading. One method, multiple imputation, works by creating a number of new complete data items by replacing the missing value with some values sampled from a distribution. You then predict from these new data items, and that gives you a set of predictions and variances from which you can compute a pooled prediction and variance. This variance will be larger than that from a complete item because of the variance introduced by the sampling. The increase will depend on how influential the missing variable is to the model and what distribution you put on the missing item. For example, if you have missing age, and your data should be from a population between 16 and 60, you'd sample age from the population distribution a number of times, do the predictions, and pool them according to the multiple imputation methodology. Of course you have to know if your missing data are missing at random, or perhaps biased missing (maybe more women over 40 don't give their age). Lots of interesting complications that will only come to light if you have a careful think about your data. Anyway, as I say, whole books. And you should probably try the statistics stack exchange site too. Its not really data science much. 

You haven't asked a proper statistical question, so the choice of mean or median as "best" as a measure of your runtime is unanswerable. Have you looked at the distribution of run times? Is the algorithm intrinsically variable in its run-time, or is it fixed in its run time but the run times differ because of noise caused by the OS doing other things? Do you want to remove that noise? What if the OS suddenly decides to swap to disk for a bit, or a big network data packet arrives, and the OS goes and does something for a few ms. You could get a long run time for one of your times, and that could pull the mean value way off. The median is a robust estimator which means a single "bad" value can't throw it off. The mean can be thrown off by a single "rogue" value. Is that what you want? Maybe you do. 

The output of logistic regression is exactly that - the probability of an event happening. If your covariates are informative then your model will do better than just saying "P=1000/900000" everytime, because it might say "P=10000/900000" for a positive event, or even "P=0.9" of a positive event given certain covariates. If there's no predictive power in the covariates (ie they don't correlate with the positive events) then yes, the model will say P=1000/900000 but that's the way it is. If you want a binary outcome then you have to decide what threshold of P you choose, based on your fear of false positives/negatives or value of true positives/true negatives. For example, if one false positive means people die, you set your threshold such that you don't get false positives. These thresholds are generally not statistical decisions but based on application logic. ROC curves can help here. 

Why not do an MSc in ooh... Data Science? I wrote a quick review of UK Data Science Masters' offerings recently. That should help you get an idea what is offered. Mostly they are mashups of stats and computing, but there are specialisms (health, finance for example) that might interest you. Note that list was compiled for courses that have already started, so some of those courses might not be available for starting next October, or have different syllabus contents. 

'group' in strictly lower case in the FOREACH is the thing you are looping/grouping over. $URL$ says: 

Stem plots do not always guarantee you can reproduce the data by reversing the process, and that's not what they're for. 

I don't think it can be explained any better than the original paper: $URL$ Did you not search for this? 

Suppose you are investigating if heart rate can predict if a person smokes. You measure bpm for 30x1m consecutive times, and ask if the person smokes in order to build your training model data set. What would contribute to a better predictor? 30 observations from a person who smokes, or 1 observation from 30 people who smoke? Given that one person's heart rate won't change much over 30m, it seems clear that you'd rather have 1 sample from 30 people than 30 samples from one person. 30 samples from one person are not worth as much as 1 sample from 30 people. Because the samples from one person are not independent. I think if you put non-independent samples into a neural net then it won't affect the predictive power too much as long as the non-independence is similar across all your training data. In one extreme, if all smokers and non-smokers have the same heart rate over the 30m period, then all you've done is repeated your input data precisely 30 times and nothing will be changed (except it will take 30x as long to run...). However, if smokers' heart rates are constant, and non-smokers' vary, then you add 30 measurements of each smoker's heart rate to your model, and a bunch of random measurements correlating those rates with non-smoking. Your NN is very likely to predict anyone with one of those smokers' heart rates is a smoker. This is clearly wrong - those 30 measurements from each smoker are only worth one measurement, and putting them in a NN will train the network wrongly. 

but you have some other components that we don't need to bother with, and my data doesn't have row names and is a lot smaller. is a list of 2 (for me) and 13 (for you) parts. The component is also a "List of 1" component. So is trying to get the of a list, and failing: 

A 1 dimensional data set is "linearly separable" at any set of points between the data points. Kmeans isn't a nearest-neighbour type of clustering algorithm, it just divides the space so that each point belongs to its nearest cluster centre, so your data could have huge gaps between the source clusters, but the algorithm would be "wrong" if the clusters had different ranges. For example, if you generate points from two clusters on the intervals (0,1) and (2,20), even though there's a gap between 1 and 2, the clustering algorithm will still assign a lot of the points from the higher source cluster to the lower cluster because they are nearer that cluster centre. Its distance to cluster centre for each points that counts, not distance to neighbours. Use another clustering algorithm (like nearest-neighbours) if you are expecting gaps between clusters. 

and then its NA NA NA NA hey hey goodbye. You should try basic debugging techniques before posting here. Print in the loop to see where it fails. Print intermediate values to check they are as expected. Step through code using RStudio's debugger if you are into that. 

should be zero, which it is, within tolerance. Zeroes and ones in logistic regression responses are like plus or minus infinity in a linear regression, so if you think back to a simple X-Y fit, you get an infinite response to a linear regression when you go to +Inf on the X-axis if the slope is positive or -Inf on the X-axis if the slope is negative. Transform that to a logistic regression and then you get to +1 or 0 at the appropriate signed infinities of the covariates.