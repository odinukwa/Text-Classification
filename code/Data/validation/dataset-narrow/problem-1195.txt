Edit I misinterpreted the problem! See Clement's answer instead. I don't know the context of this paper (not having read it), so I'm not going to address the paper at all, but I can talk about sampling from classical probability distributions which seems to be the question. In that context, the statement you give is only true fixing the support size of the distribution. The more precise statement was shown in [1], see also [2]. The right bound is $O\left(\max\left\{\frac{n^{2/3}}{\epsilon^{4/3}}, \frac{n^{1/2}}{\epsilon^2}\right\}\right)$ samples suffice when support size is $n$. If you fix $n$, then this is $O(1/\epsilon^2)$ as $\epsilon \to 0$ (as the second term in the max is larger). I think for a proof you have to see those papers, maybe my comments below are also helpful. If the question is just to tell whether a distribution is uniform or far from uniform, which sounds relevant for the paper you mentioned, then the complexity of this is simply $O\left(\frac{n^{1/2}}{\epsilon^2}\right)$ whose upper bound goes back I'm not sure how far -- it is the idea of birthday paradox -- and whose lower bound was shown by [3]. To show the $\sqrt{n}/\epsilon^2$ upper bound (which is $O(1/\epsilon^2)$ if support size is fixed) is pretty easy: Draw $O(\sqrt{n}/\epsilon^2)$ samples and check the number of collisions. It will be much higher than expected if the distribution is nonuniform. I think you can find something like this in these lecture notes[4], which also mentions the distinguishing-two-distributions problem where similar ideas can be used. (I also have the uniformity-testing case written up in a paper I could reference if wanted.) To get an idea of a counterexample when support size is not fixed, suppose $\epsilon < 0.5$ and imagine a distribution on support size $n$ where each probability $p_i = \frac{1}{n}\left(1 \pm 2\epsilon\right)$, where say a random half of the coordinates get probability $\frac{1}{n}\left(1 + 2\epsilon\right)$ and the other half get the lower probability. The total variation distance between such a distribution and the uniform distribution is $\epsilon$. But imagine trying to tell whether you have this or the uniform distribution. You can learn nothing if every sample you draw is unique -- you need a "birthday collision". But the chance of a birthday collision gets smaller and smaller as $n$ grows. This construction was used by Paninski to prove the lower bound in [3]. [1] "Optimal Algorithms for Testing Closeness of Discrete Distributions" by Chan, Diakonikolas, G. Valiant, and P. Valiant, 2013. $URL$ [2] "A New Approach for Testing Properties of Discrete Distributions" by Diakonikolas and Kane, 2016. $URL$ [3] "A Coincidence-Based Test for Uniformity Given Very Sparsely Sampled Discrete Data", Paninski, 2008. $URL$ [4] $URL$ 

When (under what conditions/assumptions) do all the strategies converge? That is, for each player $j$, $s_j^{(1)},s_j^{(2)},s_j^{(3)},\dots$ necessarily converges. Under what further conditions is the limit of this sequence actually a Nash equilibrium of the game? (It seems to me that no further assumptions should be needed; i.e., if all the strategies converge, the limit should be a NE.) When does an algorithm for computing $\epsilon$-Nash equilibria necessarily imply an algorithm for approximately computing strategies of a Nash equilibrium? Are the above conditions sufficient? 

The question this response was trying to answer, roughly: "What is a way to reveal information about the outcome of $n$ i.i.d. coin flips that helps you learn the bias of the coin, but wihout revealing too much about the individual bits?" Since the bits are i.i.d., the numbers $h$ and $t$ (number each of heads and tails) are completely sufficient to describe what the sequence tells you about the bias of the coin. (Or if you know $n$, either number alone.) This follows because, for each possible parameter $p$, all sequences resulting in a particular count $(h,t)$ are equally likely. So revealing the individual bits adds no information about the bias of the coin once $h$ and $t$ are known. On the other hand, presumably you don't mind releasing $h$ and $t$ as they do not give too much information about any one bit flip. As a sidenote, one way to approach this general sort of problem is differential privacy. I'm not sure that it's a good fit for your setting, but it formalizes the notion of how much one learns about data from computing some function on the data. The idea is that, if I have two sequences $x$ and $x'$ that differ only on a single bit (say the $i$th bit), then my function, which is randomized, should produce approximately the same distribution over outputs: \begin{equation} \Pr[f(x) = a] \leq e^{\varepsilon} \Pr[f(x') = a] \end{equation} for all $a$. 

Proof. We have $$ \sum_{i \in S_k} \delta_i^2 ~ + ~ \sum_{i \not\in S_k} \delta_i^2 \geq \epsilon^2. $$ Let us bound the second sum; we wish to maximize $\sum_{i \not\in S_k} \delta_i^2$ subject to $\sum_{i \not\in S_k} \delta_i \leq 2$. Since the function $x \mapsto x^2$ is strictly convex and increasing, we can increase the objective by taking any $\delta_i \geq \delta_j$ and increasing $\delta_i$ by $\gamma$ while decreasing $\delta_j$ by $\gamma$. Thus, the objective will be maximized with as many terms as possible at their maximum values, and the rest at $0$. The maximum value of each term is $\frac{\epsilon^2}{k}$, and there are at most $\frac{2k}{\epsilon^2}$ terms of this value (since they sum to at most $2$). So $$ \sum_{i \not\in S_k} \delta_i^2 \leq \frac{2k}{\epsilon^2}\left(\frac{\epsilon^2}{k}\right)^2 = \frac{2\epsilon^2}{k} . ~~~~ \square $$ 

Roughly, yes. The compression algorithm is lossless (bijective), so the entropy of the input is the same as the entropy of the output. Under your hypothetical, the output's length is asymptotically larger than its entropy, so asmyptotically larger than the input's entropy. So the compression algorithm does not reach the Shannon entropy bound. To see one somewhat-formal argument for why the output's length is asymptotically larger than its entropy, suppose for simplicity the length is $n$ and formalize your hypothesis by, e.g. supposing the fraction of ones is always at least $p$, with $p > 0.5$. Using a Chernoff bound, the total number of such strings is at most $q 2^n$ where $q = 2^{-cn}$ for some constant $c > 0$, and so the maximum entropy of the output is at most the log of this number, which is $n(1-c)$, which is significantly smaller than $n$. OK, this isn't totally formal, but I hope it's a useful answer. 

P and BQP are decision-problem classes, i.e. the correct output is always a deterministic functions of the inputs. The only question is whether randomness helps "along the way" to speed up computing this deterministic function (at the cost of sometimes being wrong), or does not. This is the key point: P=BQP says nothing about outputting random strings in general situations, but only for the situation of computing a deterministic function in polynomial time. So for instance the statement 

I'm not sure if this is the generalization you have in mind, but a natural one is, if $p_x := \Pr[f(x) = 1]$, \begin{align} s(f,x) &= \sum_{y\in N(x)} D_{TV}(p_x,p_y) \\ &= \sum_{y\in N(x)} |p_x - p_y| \end{align} where $D_{TV}$ is total variation distance. 

Maybe this is a silly "reason/explanation", but for many NP-Complete problems, a solution is a subset of the input (knapsack, vertex cover, clique, dominating set, independent set, max cut, subset sum, ...) or a permutation of or assignment to a subset of the input (Hamiltonian path, traveling salesman, SAT, graph isomorphism, graph coloring, ...). We could try to read more into it than that, or come up with a more fancily-stated reason, but I'm not sure whether there is anything deeper going on or not. 

I think it's just in the first inequality (the Kraft inequality): you sum over all the symbols $1/2^{\text{length of encoding}}$. There are 3 symbols of length $3$ bits and $d$ symbols of length $8$ bits, so that's how you get the first inequality. (Also, does your question have a typo? $a_4,a_5,\dots,a_{d+3}$ rather than $\dots,a_{d+1}$.) The $5/8$ comes from simplifying the inequality: you get $3/8 + d/256 \leq 1$, which is equivalent to $d/256 \leq 5/8$. 

Now, I believe that if a problem is in $\mathsf{DTIME(t(n))}$, then we have an exponential-time algorithm to find optimal circuits using an exhaustive search: Given $n$, we write down the answers on all $2^n$ inputs (taking time $(2^n)t(n)$); then we enumerate all circuits on $n$ inputs in increasing size until one is found that gives all the correct answers. The search terminates at either the size of the trivial conversion, $t(n) \log t(n)$, or the truth table of the function, $2^n$ if outputs are $\{0,1\}$. (Edit: Thomas points out that the bound is $O(2^n/n)$ due to Shannon/Lupanov.) So we have an unsatisfactory "yes" to question (1): Take a language that is hard for any time above $2^n$, but still decidable; the above procedure outputs a truth table of size $2^n$. So we should refine question (1). I think the two most interesting cases are 

Cl√©ment Canonne and I worked this out at some point. Let $X_j$ be the number of realizations of $j \in [d]$. So $\mathbb{E} X_j = np_j$. \begin{align*} \mathbb{E} J_n^r &= \mathbb{E} \|\hat{P}_n - P \|_r^r \\ &= \frac{1}{n^r} \sum_{j=1}^d \mathbb{E} |X_j - \mathbb{E}X_j|^r \\ &\leq \frac{1}{n^r} \sum_{j=1}^d 3\mathbb{E} X_j & (*) \\ &= \frac{3}{n^{r-1}} . \end{align*} We get $(*)$ by a slightly tedious argument you can find in Theorem 5.1 of [1] or I can add here later. It just uses the extreme cases of $r=1$ or $r=2$. In particular this gives a dimension-free bound. However, one can get a dimension-dependent upper bound simply by using the standard $p$-norm inequalities and the upper bound on the $2$-norm that you state. The dimension-dependent bounds will be better when the dimension is small relative to the number of observations, I believe when $d \leq O(n^{1/q})$ but will have to double-check (where $q = 1 - 1/r$). [1]: "$\ell_p$ Testing and Learning of Discrete Distributions" (Bo Waggoner, ITCS 2015), $URL$ 

Given the interest in this question, I thought it might be helpful to point out more explicitly the reason we should not be at all surprised by the answer and try to give some direction for refinements of the question. This collects and expands on some comments. I apologize if this is "obvious"! Consider the set of strings of Kolmogorov complexity $n$: $$J^K(n) = \{w : K(w) = n\}. $$ There are at most $2^n$ such strings, as there are $2^n$ descriptions of length $n$. But notice that this set is undecidable for general $n$ (otherwise, we could compute $K(w)$ just by iterating from $n=1$ to $|w|$ and checking membership in $J^K(n)$). Furthermore, the function $$g^K(n) = \max_{w \in J^K(n)} |w|$$ grows uncomputably fast. It is a variant of the busy-beaver function: what is the longest output by a Turing Machine of description length $n$? If this grew slower than some computable function, we could decide the halting problem: Given a TM $M$, construct $M'$ that simulates $M$ and prints a $1$ at every step. If the description length of $M'$ is $n$, then either: $M$ halts in at most $g^K(n)$ steps; or $M$ does not halt. Now, to Andrew's question, we have that $I^K(n) = S \cap J^K(n)$, where $S$ is the original language. So the only way to avoid $I^K(n)$ containing inputs very large in $n$ would be if $S$ contains only very uncompressible strings. (Note that, otherwise, we can completely ignore the distinction between worst-case and average-case analysis here, because we average over at most $2^n$ strings but the size of the largest string is growing faster than any computable function of $n$.) I feel that it is likely impossible to construct any nontrivial (i.e. infinite) $S$ that contains only uncompressible strings, yet is decidable. But I don't know. However, hopefully this gives intuition as to why we should not hope for most languages to have $f^K_n$ growing slower than a computable function. To step back slightly, the question is to compare performance on inputs of length $n$ to performance on inputs that can be compressed to length $n$. But we have notions of compression that are much more tractable (and less powerful) than Kolmogorov Complexity. A simple way is to give a circuit of size $n$, which on input the binary number $b$ produces the $b$th bit of $w$. Note that here the blowup in input size is at most exponential (a circuit of size $n$ has at most $2^n$ possible inputs). So we can rephrase the question by letting $$ I^C(n) = \{ w \in S : \text{the smallest circuit implicitly specifying $w$ has size $n$}\}. $$ And define $f^C_n$ analogously. The reason for hope here is that most strings require a circuit almost as large as the string itself, and no strings are more than exponentially larger than the circuit required. Perhaps in this case we could find languages where $f_n$ and $f^C_n$ are similar asymptotically. A pretty closely related question is the complexity of implicit languages like $$ \mathsf{IMPLICIT\_SAT} = \{ \text{circuits $C$}: \text{$C$ implicitly specifies $w$}, w \in \mathsf{SAT}\}. $$ IMPLICIT_SAT is NEXP-complete, and usually the implicit version of NP-complete problems are NEXP-complete. Deciding IMPLICIT_SAT is at least as easy as just using the circuit to write out all of $w$, then running an algorithm for SAT on $w$. So if $f^C_n = \Theta(f_n)$ for SAT, then this seems close to giving evidence that IMPLICIT_SAT in the average-case is almost as quickly decidable as SAT is in the worst-case. But I don't know how one would directly compare your notion to implicit languages because the notion of "smallest circuit for $w$" does not come into play for implicit languages. Hope this is helpful/interesting! I'm not sure of a textbook that mentions implicit problems, but here are some lecture notes: $URL$ 

Here's what I had before, which does something ... let's say "different" instead of "wrong." It minimizes maximum expected cost. The idea is, given an algorithm, consider the node $y$ with highest expected cost $\Pr[y] p(y)$. We want the algorithm that minimizes this value. We might view this as risk-averse, I suppose. 

I think the difficulty is that this wording slightly misleading; as they state more clearly in the Introduction (1.2), "the expected values of the dual variables constitute a feasible dual solution." For every fixed setting of the dual variables $X$, we obtain some primal solution of value $f(X)$ and a dual solution of value $\frac{e}{e-1}f(X)$. (The dual is infeasible in some of these cases, but that's fine.) So the expected value of the primal over all runs of the algorithm is $E[f(X)]$. But $E[X]$ is a dual-feasible solution, so there exists a dual solution of value $\frac{e}{e-1}f(E[X])$. The key trick is that $f(X)$ is linear in the dual variables $X$: In fact, here the dual variables are $\alpha_i$ and $\beta_j$, and each matching of vertex $i$ to $j$ adds a total of $\left(\frac{e-1}{e}\right)(\alpha_i + \beta_j)$ to the primal objective. So $E[f(X)] = f(E[X])$ and the conclusion follows. (As a side note, I feel that, as this point is one of the main focuses of their paper (according to the abstract), it would have been nicer if they had explained this point! It doesn't seem at all obvious to me, and I would like to find out when it's true more generally.)