For small businesses with less than 10 PCs it's often an overkill to even have a central server, much less buy a Windows Domain Server which costs heaps of cash. However the need remains for people to share files and printers amongst their computers. The standard solution would be to share the folders publicly. But sometimes that can be undesireable, for example if some unauthorized laptops appear in the network regularly. Another solution is to create the same users on all computers (including setting the same passwords). This allows for authorized access, but adding new users or chaning passwords is a pain. It would be great if it was possible to set up one of the machines as a central "user database". Other computers could then authenticate against that computer, and even set permissions on shares. Is this possible somehow? 

I've bought a domain, say . I'd like to set up google apps to work with it. As such, I'd like to create subdomains like , , etc. To integrate it with google, I must add to all of them CNAME records to . However my domain registrar says that they can create only one CNAME in my domain, for which they chose . The rest of them, they say, I must create as A type records. Are they being lazy, or is there some limitation here that I'm not aware of? 

I'm looking for some reading material on architecting and system admin tools etc for building web applications from small to large detailing the recommended and appropriate steps as you grow? In my case the environment is a LAMP. 

I've a Ubuntu webserver with Sendmail as my MTA Currently when i email outside my webserver's domain e.g. example.com to something like gmail or any other email outside the example.com domain it works perfectly. I don't want my sendmail daemon to recognize example.com as a local address I want it to send to example.com the same way any other email is sent. There will never be a case were i will use the local users on the webserver to collect these emails for example.com. So how can I disable local delivery? 

Ok, so first off what is the best way to read the figures and are there any issues with really comparing these numbers? Is this in any way a true representation of IO Speed? If not is there any way for me to test that? Note: these 3 machines are using either Ubuntu or Debian (I presume that doesn't really matter) 

Ok, so I realize what is happening here is the user calling the post-commit script isn't 'derek' so it hasn't permission. So my question is what user is calling the post-commit script. the svnserve daemon is run as derek ... I thought that would mean the post-commit command would be called as derek too but it seems not. Any ideas how I can find out what user is calling it and secondly what would be the best practice method to allow access for that user? I don't think adding it to the group would help because the group doesn't have the write access to the .svn directories by default. UPDATE: I've just found that www-data is the user that seems to be calling the script. Now naturally www-data can't run the svn command. 

I'm developing a mobile web application using Django. Currently I can start the Django development server like this: 

Should one always use "www" as the prefix to one's official domain name? I will be launching an e-commerce website shortly and my attorney needs to know the official domain name of my site for copyright purposes. Are there any reasons why I should designate it as "www.example.com" as opposed to "example.com"? I've always used "example.com" during development because it was easier to type and I'd have Nginx rewrite all requests for "www.example.com" to "example.com". Should I perhaps say "www.example.com" is the official domain name and reconfigure Nginx to rewrite all requests to "example.com" to "www.example.com"? Thanks! 

How can I access this site from my phone? This appears to be a pretty common Apache error message that can be caused by any number of things. I've read many articles about it but haven't been able to resolve the problem. What am I doing wrong? Since I can access the site via my computer browser but not from my phone, I'm thinking perhaps it's an OS X firewall issue. Is there some way I can configure my OS X (Mavericks) firewall so that when I start the Django development webserver, my computer will automatically allow incoming connections and I won't get the pop-up window I described above? I've gone into System Preferences > Security & Privacy > Firewall Options and selected "Allow incoming connections" from the three "python" connections that are shown but I'm still getting the popup window. 

Ok, because of a bunch of projects I'm working on I've access to dedicated Servers on a 3 hosting providers. As an experiment and for educational purposes I decided to see if I could benchmark how good the IO is with each. Bit of research lead me to Bonnie++ So I installed it on the server and ran this simple command 

but my main repos don't seem to exist anymore. I know I need to upgrade to lenny but the docs recommend you update to latest etch first before doing a dist-upgrade. Where can I find the latest etch source? 

I've a debian webserver with subversion running on it. I'm trying to use post-commit script to update my staging version of my site. 

And there are lots lots more. ultimately I want to import all the 2011-01-07.sql files into my mysql database. This works for one 

I've created my own deb package for Redis 2.6.11 instead of the very old default version for Ubuntu. All works well except when I do an it tries to install what it thinks is the latest version of How can tell the package manager to stick on my custom one and don't refer to default repository for this particular package? Ubuntu 12.04 

If you need any other info let me know. Biggest problem being I can't find any error message in log when I run 'top' for example I don't see any unusual load on the server or high mem usuage. Any help is greatly appreciated. Derek 

If I turn off the firewall on my file server and rerun both of the above commands, they run successfully and I can mount the file system. But if I re-enable the firewall, both commands fail again. What is baffling is that the firewall rules that I enabled when I rebuild the file server are identical to the rules that are enabled when the file server is initially built since the rules file is built by my Ansible playbook. Here are those rules: 

Is there some other step I'm not aware of? UPDATE Here are the firewall rules if I run the command "sudo iptables -L -n -v": 

I've solved the problem. Here's what's going on. /etc/apache2/apache2.conf includes a call to any config files that have symlinks in /etc/apache2/sites-enabled. Since there was a symlink in that directory pointing to /etc/apache2/sites-available/000-default.conf, that latter config file was being loaded and it was over-riding the blocks and directives in my vhosts.conf file. Once I deleted that symlink, my vhosts.conf settings were able to take effect. The lesson for me was that any file that has a symlink in sites-enabled will be enabled. 

I figured it out. The solution is to set "user = root" in the project's Supervisor configuration file. The documentation says, "If supervisord runs as root, this UNIX user account will be used as the account which runs the program." Thus, setting user this way is equivalent to my running the script manually using "sudo." 

I've got a small jack-of-all-trades server running CentOS 7 Linux. Now I need to add email support to it (SMTP/POP3/IMAP/webmail). Most of it is pretty clear, except for one thing - I'd like to separate linux users from email users. That is, for every email account I don't want to make a server user as well. Also, if there are emails jenny@domain1.com and jenny@domain2.com, I'd like them to be separate accounts. And, of course, users should be able to use their email addresses as usernames for the mail-related services. How can I do that? I don't even know what keywords to search for (yes, I've tried googling, but came up empty handed). 

I use a utility called Eraser. I think it was recommended in some Microsoft TechNet posting or MSDN. And it's totally free. 

I've got a little private webserver where I have several virtualhosts. I know that it's impossible to assign a certificate to each individual virtualhost, because the server finds out which virtualhost was requested only AFTER the SSL connection has been established. But is it possible to have a single SSL certificate which lists several domains? Or at least a wildcard domain, like *.example.com. If yes, what Linux commands do I have to write to make such a self-signed certificate? Added: To clarify - I have just one IP address for all the virtual hosts. 

So can you guys help me out with this. For bonus points if you can tell me how to do the next step which is import all databases in one command doing one at a time that would be great too. I really want to do these in two separate steps because I need to delete a few sql files manually from the tmp dir before I run the restore command. So I need: 1) command to copy all 2011-01-07 sql files to a tmp dir 2) command to import all those files in that dir into mysql I know its possible to do in one but for lots of reasons I really would prefer to do it in two steps. 

Not sure what the terminology is for it but on Vim the 'cursor' is always like an insert/replace cursor instead of the blinking line cursor I'm used to in other gui editors. Is there any way to change this when in insert mode? 

The 3 machines in different hosting providers are all dedicated machines, one is a VPS, other two are on some cloud platform e.g. VMWare / Xen using some kind of clustered SAN for storage This might be a naive thing to do but here are the results I found. 

if I run that command from the command line logged in as user 'derek' then it all works fine but when it runs as post-commit I get the following error in my log file: 

This message repeats every five minutes. I suspect this is the IP address of an earlier build of the file server since I can see it's a linode.com address if I run "whois" on it. But what's more interesting is that if I to a "grep -Rn 45.79.65.48 /etc" I see this address in the /etc/mtab file. I see now that this is the previous file server's IP address since I neglected to unmount the file server's directory before I destroyed and rebuilt the file server. I did "sudo umount -l /var/www/mysite.com" to unmount it. I then did "sudo mount -a" on the web server and now I can see that the file server directory is mounted onto the web server. However, if I re-run "sudo rpcinfo -u fs02 mountd" command on the web server, I still get the "Connection refused" message. I don't see how I can get that message if I'm now seeing the cross-mounted directory. I've been up all night working on this so maybe I'm tired and missing something. 

I have a Django 1.6.2 application that uses Celery 3.1.7 for asynchronous tasks. I'm starting my Celery workers using Supervisor. So far everything is working well except when I reboot my Debian 7.8 server. When that happens, my Celery workers won't restart because when the server reboots, it changes ownership of the celery log files from my "celery" user to "root". Also, the system deletes my /run/celery directory where I write my pid files. If I make these changes by hand and restart Celery, all my workers start normally. Since these changes need to occur before starting the workers, I thought the solution would be to write a shell script which, due to its higher priority, gets executed from my supervisor.conf script before the celery worker commands (See below). However, this setup script won't run. My supervisor log just says, 

I'm trying to pin point what applications are using up my network connection. Is there an app/widget out there that can show me this? I'm aware of net monitor but that doesn't seem to tell me what apps are using what traffic. Ideally what i'd like to see is a bar chart of app usage of net traffic over time. 

I'm scratching my head on this one. Django is installed on my production and dev servers but the live server under not ver heavy load is running extremely slow. Every page is taking more than 30 seconds at times. here is my Apache sites-available/example.com file: 

On Mac, Sequel pro allows you to do by default but I'm still looking for a windows gui client to do it. 

That will nicely restore that database from this backupfile. I want to run a process where it does this for all databases. So my plan is to first cp all 2011-01-07 sql files into a tmp dir e.g. 

I have 2 straight forward websites I want to host. example1.com and example2.org This server was originally set up by someone else and example1.com works as expected. So my task is to add example2.org to the server. I create a basic index.html test page in C:\www\example2 add a virtual host record to httpd-vhosts.conf and for test purposes add a 127.0.0.1 example2.org to my hosts file so I can test locally. This all worked as expected and I can view example2.org which shows my test html file. The problem is... when the DNS propagated it shows the example1.com site when I go to example2.org from anywhere but the local windows server. I'm completely baffled as to why this is happening? Any ideas? At what point does apache read in outside connections differently to local ones?