Your design indeed has a privacy flaw: Once an attacker has figured out the hash value for a user, he can then impersonate the user, i.e. send wrong location data to the server. This stems from the fact, that your hash is static - once known, always known. There is a way around this: On the client concatenate the payload data and the current timestamp, then use the hash to sign this. Send the timestamp (in clear) with the request and have the server reject obviously wrong timestamps. 

(My machine at the time of writing with lots of open programs). This means, that while from a total of ca. 16G (16425304 KB) something like 15.2G (15497560 KB) are used, but of these some 210M (221476 KB) are used for block device buffers and ca. 7G (7057660 KB) are used for file system cache, resulting in less than 8G really used. The command gives a much better idea, as it does the maths for you: In the line it shows you what the system realy uses, with buffers and cache already subtracted. 

First of all: Review your control flow, what you want to do is not a good idea. That said, there is an easy way to achieve what you want without hardcoding a PW: SSH to yourself 

This has the benefit of needing nearly no reconfiguration, if you chose to expand into a road warrior scenario or add more clients. 

Since DRAM does neither have a static mode nor a strict idle (refresh cycles still apply), the number you are searching for is the difference between idle and heavily loaded usage patterns. This seems to be quite significant, as an InfoQ article suggests: 

First of all: Being expensive is no technical property. Be prepared for this drive to fail again, don't put anything important on it. Now: Your second options seems the one with the best ration of success likelyhood vs. hassle: 

We have many such setups running and we typically use , as it provides fast and reliable failover while being very easy to configure and maintain. If the machines in question are Windows, we typically do this within a routing layer in between, that also helps isolating the often vulnerable Windows hosts from the rest of the network. 

The line makes the batch copy data not from the directory it is stored in, but from the directory it is called from. Insert after the first line, to make the latter directory be the former. 

Adding to @MariusMatutiae's answer: If the file is too big to read to memory, you can use a classic but much slower way: 

Theoretically this works fine. Practically the devil is in the details: "Identical hardware" is not really identical, with the Ethernet MAC being one of the more obvious examples. This boils down to the only "identical" machine to wake up on often being the one that you went to sleep at. That said, I regularily move an SSD with a hibernation image between two machines (Ubuntu 16.04, home and work) and things work fine - I did have to manually set up both ethernet connections and a few minor details. 

There is for all of your requirements (but not for saving them), and it is called . As for saving the sessions: 

First of all: Reconsider your workflow. Why not just rename the directory to get it out of the way, run your build on a new dir, then delete the old one in low usage hours. Even 3 minutes seems quite long to waste, if you can go with 0.1s. Now concerning the delete times: I suspect, that for some reason something in that directory is still in use by the OS - maybe the classic (search indexer)? This could easily have been masked by the fact, that creation times were much longer on the rotating disk. You can verify that by trying to delete the stuff after a few hours and only then checking how long it takes. EDIT If the stuff is just 2GB in size, a RAM disk would be the obvious choice. 

With most editions of Windows 10, ultimate control over the machine lies not with the owner, but exclusivly with Microsoft. This implies, that there is no way to stop Microsoft from doing whatever pleases them on your Computer, including installing whatever Software they want, unless you choose to permanently disconnect from their C&C servers, i.e. mostly disconnect from the Internet. There exist a few hacks to mitigate this as much as possible, but none are reliable and most do not survive OS (i.e. built-in spyware) "updates". 

Either your harddisk is dying or there is a loose contact somewhere on the way from SATA controller to disk. The disk is seen by the OS as unplugging and replugging every few seconds. THis is serious, have someone with experience try to recover you files if you don't have up-to-date backups. 

First of all: Hardware time on a Linux system is stored in UTC, so an offset relating to your timezone is to be expected. And yes, this does make problems when dual-booting. In Addition to that, a Linux system will not always and immediately propagate the NTP time to the RTC ("Hardware clock"), but it should do so at least on shutdown. 

It must have at least as many lines as repetitions are necessary, but, again, more won't hurt, so you can use a safe (too high) line count And run ffmpeg (assuming you want 123.456 seconds): 

ffmpeg can do that for you, but you might need two steps Optional Step 1: Find length of original file 

More than one IP address in the same subnet on the same host (whether bound to the same adapter or to different adapters) are not only valid, but are routine in many applications (e.g. web servers). It is, though, quite unusual to configure more than one with DHCP - unusual being the key word, not invalid. 

will extract the audio track only in MP3 format. EDIT A possible workflow for replacing the soundtrack would be 

I thoroughly recommend GhettoVCB as described in VM Community Forum. It allows to back up to different datastores and network locations. 

If the original video has an odd number of frames and you reencode to half the frame rate, it is impossible to hit exactly the same length as the original. You can chose between half a frame (new framerate) more or less. By default will chose the longer version, but by setting "-t ss.sss" you can force the shorter length. 

If a usage pattern for an SSD with 4K block size would consist in changing 1 byte per block, then the total number of bytes written yould be 4096 times the number of payload bytes, i.e. a of 4096 would happen. This would mean, that an SSD with a typical endurance of 10 years would be pushed beyond that endurance limit in less than a day. Countermeasures against this are manyfold: 

Not always did computers have the necessary hardware to power down on their own. More important still, even after they had acquired this feature (mostly through the legacy ACPI mechanism), there was a long period of time, when BIOS implementations were so wildly uncompatible, that the OS was very often unable to successfully access these facilities. Up until NT4 the "Windows has been shut down. You can safely turn off power" screen was standard, with NT5 (a.k.a. Windows 2000) the switch to ACPI poweroff became widespread, and with NT 5.1 (a.k.a. Windows XP) it became the new normal. There was still a lot of hardware, that could not be shut down by XP, so the text remained seen quite often. 

What now happens is, that the output of will be the output of the shell script, so your pipe works. Inside your script, the output of will be redirected to STDERR of the script, so it is not sent to the pipe, but to the terminal 

It is importand to understand, that the executable (e.g. the .exe) is not the result of the compilation process - it is the result of the linking process, which combines the output of compilation with the other ingredients needed to create what I described in 3. 

While the System is quite likely to use whatever RAM you throw at it (after enough I/O), it does by far not need all of that. All usual Distributions of Linux are preconfigured to aggressivly use RAM as a disk cache if - and only if - it is not needed elsewhere: The head of the output of will look something like 

will e.g. create a new file, where the video track is simply copied, while the audio track is converted to MP3. 

Q1: Many computers (e.g. most modern servers) have some sort of built-in flash memory, that may show up as CF, SD, MMC Harddisk or whatever. Post vendor and model of your computer to confirm Q2: Disk 0 has a single megabyte (1024 KB) of space, that is not assigned to a partition - this is different than space, that is assigned to a partition, but not yet used. Small parts of a disk might easily be not assignable to a partition, due to alignment issues. 

The legal question is overriden by a technical one: Are bitcoin generators possible? And this must be answered with a resounding "No". Every single bitcoin ever in existance was "generated" with the start of the blockchain - they only have to be found ("mined"). Bitcoin miners are quite simple machines (or programs running on general purpose computers), that use immense calculation power to find them. They are not only legal, but the standard way to put bitcoin into the cycle. 

Sadly, not all WLAN components are created equal. Even in 2014, some wireless NICs will work badly with some APs. I am quite sure, this has nothing to do with VirtualBox - try pinging from your Host to somewhere else on the network, and I bet you will see the same packet loss. I am also sure, that DHCP is not the reason for, but a symptom of the problem: The DHCP handshake process is very vulnerable to packet loss, so it might easily fail a few consecutive times, giving the impression of a complete failure. Things you can try: 

Most RDBMS allow standard table constructs, that live only in RAM. While they need to be recreated on database server restart, this should be no show-stopper. A noteable exception is PostgreSQL, where unlogged tables are as close as you get. Having said this, I recommend you reconsider: All relevant RDBMS - this time definitly including PostgreSQL - use an elaborated caching system, that will keep often-accessed data in RAM: Most likely this does already what you want. 

When considering compression for small files, keep in Mind the NTFS cluster size of 4K: Files, that are below 4K and can not be compressed enough to fit entirely into the MFT record (i.e. more than a few byte) will use 4K before and after. A file of 7K with a compression rate of less than 1.8 will also use 8K before and after. Text files will benefit most from compression, as they allow for a high compression rate, executables will benefit much less. For a bootable drive, it might be much easier to just leave out some files - do you need and friends? Do you need all locales in ? Edit As @Goyuix tested, the EFI bootloader (bootmgr.efi) has to remain uncompressed. This is to be expected, as it is not read by Windows, but by the EFI firmware, which knows nothing about compressed files. The same holds true for the classic bootloader (bootmgr), which is read by BIOS, again knowing nothing oof file system compression. Both firmware dialects just read a consecutive length of bytes into memory, then transfer control to them, which would obviously not work, if they are compressed. 

If you only need Gbit speeds, you might be able to use a QEMU disk image (qcow2), that can be sparse, compressed and encrypted all at once. Accessing it with will make it mountable for local use. This also puts you in a situation, where virtualizing the whole thing in the future is a breeze.