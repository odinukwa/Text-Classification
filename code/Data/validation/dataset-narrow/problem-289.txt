The format in the database is decimal(18,9). Any suggestions on how to avoid this error without manually changing values in the source file? To put it into perspective. The CSV file contains more than 2.2 million rows with 154 columns each. Which results in a CSV file size of more than 2GB. Currently I am working with a test file. When the final go live comes. I need to switch over fast. Which means I can not analyze and edit the file for several days. Update I played around with the values a little bit. 

I am trying to move data from a csv file into a SQL server database. Some of my values are in the scientific notation. I figured out on how to get most of them converted but for one value I get the Arithmetic overflow error. The value that is causing the error is . If I change the part before the E by removing the 1 so it reads the import works fine. All the other values I need to import work fine. I use a format file to import the data. Below the line for the column that is giving me grief: 

Can you point the 'full path' to the UNC path i.e. instead and see if that works? Just like when you map the "X" drive to just use that in the full path of one of your packages and run to see if it'll work. If one of your jobs work like that (assuming most all are setup the same and this way, etc), you can probably script out the SQL Agent jobs through SSMS by pressing F7 (once SQL Agent jobs is highlighted), selecting them all from the right pane window, right click, then create to new query window, then do a mass CTRL+H and do a find and replace to replace with , and then run that. Just be sure the SSIS proxy account or the SQL Server Agent account has appropriate NTFS and SHARE permissions where the SSIS packages reside to read them. 

Make the ETL software physically close enough to the destination DB so that parallelization (15 connections) is not necessary. So far I've been sending data across the US (ping 48ms) and across the Atlantic (ping 112ms). I'm not certain if getting a response time of 1ms (possible) by putting the two endpoints in the same room would make the SQL write speed much faster. I would love for it to be on the same computer, but don't have the approval. Somehow circumvent this Machine 2's response of trying to cut off my connection. I mean, sure, maybe it's some sort of spam/ DDOS defense. But honestly --- I thought it was quite common for a server to take lots and lots of connections and queries like this. 

How would I join the fact to the dimension table so I know which "company" picked the 100 apples? In this case, logically, given the data, it's 'Da_Big_Apple' ... since John began working there in March 17 until Oct 2017, in which his apple picking task took place. How do I do a join of these tables though? (assuming thousands of records). I just get stuck. I know I should do something like 

I guess it has something to do with the fact, that db2 will return an error code of 1 when no records were found. 

In general, 7 seconds is not to bad if you have a big report. It takes longer to get a coffee. Does it create any other issue than 6 seconds of wasted productivity for an end user? 

I am writing a script to restore databases from backup. The script should need as little input as possible to fulfill this task (in an efficient way). The restore command takes the parallelism parameter. The idea is to set this parameter to the number of tablespaces that are not temporary tablespaces. I found the db2ckbkp command which will not only verifies the backup file, but also outputs lots of (useful) information. Currently I was planning to run it with -t to get the tablespace information. I just have troubles to interpret the information printed. following the output that is printed for one of the tablespaces. 

Consider taking a look at sys.dm_exec_query_memory_grants and sys.dm_os_sys_memory to get your started on your troubleshooting journey. 

I think it depends on how well you want to lock it down really, and how trustworthy they are for what you are allowing them to have access to. 

With the function for the second argument you pass it telling it the format for the time value in the first argument, you should use the or format 

Since the log backup files are purged from the locations which the LSRestore for DR runs and grabs those, then that job will take care of those once the restore job completes successfully. 

Wouldn't you want to ensure the secondary logs are restored to the secondary server at certain intervals? With log shipping, I thought you ship the logs over to your secondary DB and then restore from those to keep the secondary up-to-date as close as possible to your primary. Do you see any reason why you would not want to automate the log shipping restore on your secondary DB in the log shipping configuration? Once those log files are applied to secondary, then the job will purge the logs that are not needed any longer since those are reflected in the secondary DB at that time. Test it out and see how it goes. EDIT: @nulldotzero Okay, since you cannot run restore operations on primary or secondary in an AlwaysOn configuration, and because you are also copying the TRN log files to the seconary as well as the DR for redundancy, then you could just setup a SQL Agent job on the secondary instance to do the below for whatever hours you feel need to be purged. 

AFAIK, you cannot force a plan to stay in cache. However, a query can be thrown out of the cache for several reasons. Read a blog about execution plans. It states some reasons why execution plans get invalidated: 

To determine whether it is an temp space or not, the is not suitable, since it is 6 for all of the tablespaces. However the container type seems to be different for temp spaces. is 0 for temporary tablespaces and 6 for all other tablespaces. When I use the parameter, I get another piece of information. 

Another pitfall could be that the query changes. This can happen if the where clause changes (e.g. you filter by date) and you don't use bind Thinking about, you haven't stated that you checked whether the execution plan is still in cache or not. You should query the cache to figure that out. However, since you say your query runs only once a day, the plan might just be expired. The blog actually mentions the formula on how sql server determines when to expire a plan: