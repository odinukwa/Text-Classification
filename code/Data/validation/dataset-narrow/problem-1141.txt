To my knowledge, such a reduction is in fact known: Hrubes and Wigderson ITCS 2014 show how division gates can be eliminated from non-commutative circuits and formulas which compute polynomials. They also provide exponential-size lower bounds for non-commutative formulas with division (not circuits) that compute any entry of the matrix inverse function $X^{-1}$. Moreover, your main question about lower bounds for non-commutative circuits, is not known (while for formulas it is known as mentioned above), because non-commutative circuits in which each gate computes a polynomial (not a rational function) with division constitutes a class which is at least as strong as non-commutative circuits. But there is no known super-polynomial non-commutative circuit lower bound (see [Hrubes, Yehudayoff and Wigderson STOC 2010] on this). 

The question seems quite open ended. Or perhaps you wish to have a precise characterization of the time-complexity of any possible symmetric polynomial over finite fields? In any case, at least to my knowledge, there are several well-known results about the time-complexity of computing symmetric polynomials: 

It is well known that random $ k $-CNF formulas over $ n $ variables with $ cn $ clauses are unsatisfiable (i.e. they are contradictions) with high probability, for sufficiently large constant $ c $. Thus, random $ k $-CNF formulas (for $ c $ large enough) constitute a natural distribution over unsatisfiable Boolean formulas (or dually, over tautologies, i.e. negations of contradictions). This distribution has been studied extensively. My question is the following: are there any other established distributions over propositional tautologies or contradictions, that can be considered as capturing the "average-case" of tautologies or unsatisfiable formulas? Have these distributions been intensively studied? 

Mining pools are more feasible: In cryptocurrencies, it is often very difficult to win the block reward. Since the block rewards are very difficult to win, miners often mine in things called mining pools in which the miners combine their resources in solving a problem and in which they share the block reward in proportion to the amount of “near misses” they have found. A possible issue for $\mathcal{C}$ is that it may be difficult to produce a qualitative notion of what constitutes as a “near miss” for the problem $\mathcal{C}$ and the algorithm for finding a near miss may be different from the algorithm for solving $\mathcal{C}$. Since the pool miners will be looking for near misses, they may not be very efficient at solving $\mathcal{C}$ (and hence, few people will join mining pools). However, for $\Psi(\mathcal{C})$, there is a clear cut notion of a near miss, namely, a near miss is a pair $(k,x)$ where $(k,x)\in D$ but where $H(k||x||\textrm{Data}(k,x))\geq C$, and the algorithm for finding near misses for $\Psi(\mathcal{C})$ will be the same as the algorithm for finding solutions to $\Psi(\mathcal{C})$. Progress freeness: A proof-of-work problem $P$ is said to be progress free if the amount of time it takes for an entity or group of entities to find next block on the blockchain follows the exponential distribution $e^{-\lambda x}$ where the constant $\lambda$ is directly proportional to the amount of computational power that entity is using to solve Problem $P$. Progress freeness is required for cryptocurrency mining problems in order for the miners to receive a block reward in proportion to their mining power to achieve decentralization. The SLT certainly helps mining problems achieve progress freeness. 

If I understood correctly the question, the so-called Buss-Pudlak game provides a simple transformation from a proof system to such a decision tree (see Buss-Pudlak '94 $URL$ The queries are formulas (not just variables). The tree is also completely deterministic. Other decision trees that correspond to different propositional proof systems exist: e.g., Linear Decision Trees correspond to Res(lin) refutations (cf., $URL$ and $URL$ But there are many other examples (cf., Tonian Pitassi's work on CP-like proof systems). 

Yes. If $ 0< \epsilon <1$ is a constant (or $1/\textit{polylog}(n)$), and you are promised that at least $ \epsilon 2^n $ of all possible assignments are satisfying the input 3CNFs, then you can find such an assignment in deterministic polynomial-time. The algorithms is not difficult: Claim: Under the promise stated, there must exist a constant size set $ S $ of variables that hits all clauses in the 3CNF, in the sense that every 3-clause must contain a variable from $ S $. Proof of claim (sketch): Otherwise, there must exist a large enough family of 3-clauses from the 3CNF, in which each variable occurs only once. But this family, when sufficiently large, has already less than $ \epsilon $ fraction of satisfying assignments. QED Thus, you can run over all possible (constant number) of assignments to $ S $. Under every fixed assignment to $ S $, the 3CNF becomes a 2CNF, by the assumption that $ S $ hits the original 3CNF. Now, you can use the known polytime deterministic algorithm for finding a satisfying assignment for 2CNF formulas. Overall, you get a polynomial time upper bound. The algorithm for 2SAT is I think already in S. Cook famous 1971 paper. The algorithm for 3CNFs is from: L. Trevisan A Note on Deterministic Approximate Counting for k-DNF In Proc. of APPROX-RANDOM, Springer-Verlag, page 417-426, 2004 The original paper showing the result for 3CNF is: E. Hirsch, A fast deterministic algorithm for formulas that have many satisfying assignments, Journal of the IGPL, 6(1):59-71, 1998 

The paper refers the reader to Matoušek's paper [1] for this result. However, Matoušek does not discuss spanning trees of points in [1], rather several methods for computing simplicial partition trees. So I've been trying to track down where the result comes from. I finally found, in [2], the following statement: 

I am interested in range searching purely as a black box in another problem. One problem in particular is that of shooting 2D rays among 2D segments. It seems that Agarwal and Matousek have a well known method for doing this in [1] which requires $O(m^{1 + \epsilon})$ time preprocessing and space to answer ray shooting queries for any fixed positive $\epsilon$. It is not clear to me yet (having read the introduction and conclusion of the paper and skimmed the rest), whether this $\epsilon$ arises due to randomization, approximation, or something else. As I said I only need this as a black box and am somewhat trying to avoid becoming an expert in range searching just to know the answer, and since my initial skimming of the paper didn't produce results I decided to ask it here: which of these three is the algorithm: deterministic, approximate, or randomized? If not deterministic, what is the best that can be done deterministically? [1] Agarwal P. K, Matousek, J. Ray shooting and parametric search. STOC '92 Proc. of the 24th annual ACM symp. on Theory of computing. Pages 517-526 

2-3. $\Psi(\mathcal{C})$ will typically become more difficult than $\mathcal{C}$ and this is a good thing. The difficulty of a proof-of-work problem needs to be finely tunable, but the original problem $\mathcal{C}$ may or may not have a finely tunable level of difficulty (remember that the difficulty in mining Bitcoin is adjusted every two weeks). The difficulty of problem $\Psi(\mathcal{C})$ is equal to the difficulty of finding some suitable $(k,x)\in D$ multiplied by $\frac{2^{n}}{C}$. Therefore, since the constant $C$ is finely tunable, the difficulty of $\Psi(\mathcal{C})$ is also finely tunable. Even though the problem $\Psi(\mathcal{C})$ is more difficult than the original problem $\mathcal{C}$, almost all of the work for solving the problem $\Psi(\mathcal{C})$ will be spent on simply finding a pair $(k,x)$ with $(k,x)\in D$ rather than computing hashes (one cannot compute whether $H(k||x||\textrm{Data}(k,x))<C$ or not until one has computed $\textrm{Data}(k,x)$ and one cannot compute $\textrm{Data}(k,x)$ unless one verifies that $\textrm{Data}(k,x)\in D$). Of course, the fact that $\Psi(\mathcal{C})$ is more difficult than $\mathcal{C}$ presents some new concerns. For a useful problem, it is most likely the case that one would want to store the pairs $(k,x)$ where $(k,x)\in D$ in some database. However, in order to receive the block reward, the miner must only reveal a pair $(k,x)$ where $(k,x)\in D$ and $H(k||x||\textrm{Data}(k,x))<C$ instead of all the pairs $(k,x)\in D$ regardless of whether $H(k||x||\textrm{Data}(k,x))<C$ or not. One possible solution to this problem is for the miners to simply reveal all pairs $(k,x)$ where $(k,x)\in D$ out of courtesy. Miners will also have the ability to reject chains if the miners have not posted their fair share of pairs $(k,x)\in D$. Perhaps, one should count the number of pairs $(k,x)\in D$ for the calculation as to who has the longest valid chain as well. If most of the miners post their solutions, then the process of solving $\Psi(\mathcal{C})$ will produce just as many solutions as the process of solving $\mathcal{C}$. In the scenario where the miners post all of the pairs $(k,x)\in D$, $\Psi(\mathcal{C})$ would satisfy the spirit of conditions 2-3. 

To expand on Mikola's answer, you could modify a sweep-line algorithm like the Bentley-Ottman algorithm ($URL$ to detect the crossings, and maintain an auxiliary graph structure where each polygon is represented by a vertex. Mark each line segment with its polygon's representative graph vertex. Each time you detect a crossing between two line segments from different polygons, add an edge between the corresponding vertices of the auxiliary graph (if one does not already exist). In the end, the connected components of the auxiliary graph will give you exactly what you want. 

The reference [Mat91b] in the quote is my reference [1], and the reference [Mat91a] is a private communication between Wezl and Matoušek. It seems that this sentence from [2] is most likely why [1] is referenced in the paper I'm currently reading. I think this is a slight mistake, since [Mat91b] appears to be included as a reference for simplicial partitions, and [Mat91a] (the private communication) is the reference for the entire section and hence the claim. I must admit the first time I read this sentence I made the same mistake, but after searching in vain for the result in [1], have come to the conclusion above. So my question is this: has the proof of this result appeared in the literature? Perhaps it is obvious (it is not yet obvious to me, but I am still learning about these algorithms). If it is not obvious, I am hoping someone can either provide me with a reference to where it has appeared, or a (rough?) explanation of how a simplicial partition of a point set can be used to construct a spanning tree of the point set with stabbing number $O(\sqrt{n})$ in $O(n^{1 + \epsilon})$ time (or any other $O(n^{1+\epsilon})$ time method, really)? Thanks. References [1] Matoušek, J. Efficient Partition Trees. Discrete Comput. Geom., 8:315-334, 1992. [2] Wezl, E. On spanning trees with low crossing numbers. Data Structures and Efficient Algorithms, LNCS 594:233-249, 1992.