One possible approach is to allow such instances to make demands for new instances based on local request counters, but instead of directly reacting to those demands you would funnel them to a central instance creation logic. That logic would immediately react to the first demand, but also start a "cool off" countdown timer. Any subsequent demand received while the timer is still active would be considered to be caused by the same traffic spike that triggered the first demand and would thus be ignored. A similar logic could be used to gradually shutdown idle instances, if the local counters remain below a minimum level. Note: the local counters would need to operate in a leaky bucket manner or be periodically reset for such approach to be possible - so that the demands are repeated if the traffic remains high. Another possible approach is to just publish the local counters. A central piece of logic would periodically collect and aggregate these local counter values in order to decide on lauching new instances or shutting down existing ones. The advantage of such method is the lack of a single global counter which would require write access locking (to prevent corruption) which would be a scalability limitation. This method is described in mode details in Sharding counters. 

Apparently selecting the option in the very dialogue illustrated in your question will actually delete the branch as well. From Why only "Close" a branch instead of "Delete" it?: 

Artifactory is a Binary Repository Manager product from Jfrog. You're right - being a binary repository manager it is typically used to manage storage of artifacts generated and used in the software development process. From Artifactory's main webpage: 

Since you're talking about CI/CD I presume you have the possibility to automate the model trainings in those configurations. Let's call the scripts able to do that , , etc. Then you could have a wrapper script which checks an environment variable used to select which client environment you desire and invokes the corresponding script. Ideally translating the outcome of the training (whatever that is) into one or more results of the pass/fail type. Then such wrapper script can be integrated in a CI/CD pipeline like a custom test step/stage (or even a build one, if it produces any artifacts you might want to archive). Just like any test executed on a testbed incorporating some non-generic piece of hardware. In other words the CPU cluster makes no real difference. You might not need to install the slave directly on the GPU cluster if the scripts (and thus the wrapper script as well) can be executed on some other hosts and remotely control the GPUs - you can then have the slave on some other host, leaving the GPU cluster free to only do its stuff. 

A successful high-velocity development environment typically relies on a pretty strict automated system involving quality verifications with detection and rejection of faulty changes causing regressions. Feature toggles offer the ability to commit even work-in-progress, untested changes without getting rejected for causing regressions in the integration branch. Which constitues a very good incentive to introduce the feature toggles very early in the life of the feature. One of the disadvantages of deviating from true CI and moving feature development on feature branches is the lack of such incentive. Adding the feature toggle later on, when merging the feature branch into the integration branch is typically more difficult, like any late integration. 

Yes, MTTR is/should always be tied to the business outcome: if things are not stable the very business is at risk. The fact that the expected code/feature/change is still stuck in process in scenario 1 is irelevant: the feature is not stable, so it doesn't bring new business, rolling back is the best you can do at that time from the business prospective. The rollforward is a gamble: keeps the business at risk waiting for a potential fix that in fact has statistically lower changes of success (due to the instability it will always be rushed compared to the change that caused the instability in the first place without even having such pressure on it). The rollforward is a yet another version of the code which hasn't been checked before. If you want to keep the MTTR low you rollback immediately, without debate. This removes the business risk and gives you a chance to check that the fix is actually working before attempting to deploy it. I'd strongly suggest making it a policy as yes, almost always there will be someone asking for a fix instead of the rollback and calling a meeting to negociate/decide on it - all while business remains at risk. Side note: if you're concerned with a high Change Failure Rate then I'd suggest checking the the actual rollback rate instead of deriving it from a low MTRR. Maybe you'd like to add a gate check before deployment for the most frequent failures. If you have such check already automated - why not include it in the CI verification? If you don't have one - maybe its time to start thinking about it? :) 

In this approach the duration of each task execution is reduced to the minimum necessary: one page parsing and only one check if a URL has already been crawled. And each page is only parsed once. The number of pending work items would be the driver for scaling - if it goes above a level new instances can be started, if it drops below a level and multiple instances are running some may be stopped. If implemented as a GAE app the GAE infra can take care of that for you automatically. This solution can be implemented using a noSQL database (like Google Datastore, in a GAE context), which can scale better than a SQL one. 

In many cases compilers are used as-is, already packaged by 3rd parties. When changing the structure of the build systems changing compilers usually translates into switching to a different version, but which is also pre-packaged. If this is your use case the artifact repository offers a single advantage: availability. Even if the provider decides (for whatever reason) to discontinue the version - you still have your copy in the respository. But if the compilers receive any updates without version changes or if you do any sort of customisation - even if it's just building them without touching any code - you want to store those particular versions in the artifactory to be able to reliably obtain them at any time. Side note: how exactly you use the compilers can influence the performance of your builds, especially in very large projects with build times which can take hours. You'd have to analyze your builds' structure and/or usage patterns to figure out if it's worth pursuing optimisations and, if so, in which direction(s). Directly invoking compilers from NFS is convenient - very little configurations required on the build servers. But performance of massive builds can be very sensitive to the NFS performance, of both the NFS server itself and of the network connecting it to the build server(s). Some artifactories may have direct NFS support. You'd have to measure the impact of switching to such service, in some cases it may still be better to use a dedicated NFS server instead. Or maybe switching to VM images that contain the compilers locally installed. 

IMHO from the workflow prospective code coverage measurements are fundamentally similar to builds, smoketests or any other types of QA checks and can thus be performed in similar manner. I can see several options: 

If a hotfix applies only to a production version but not to the it is directly committed to the branch. If it applies to both it's typically committed to first and cherry-picked/double-committed to the branch as well. Now looking at what goes into (which is past the point where the current branch is pulled off), you have 2 options: 

It's not currently possible to list all the accessible buckets across all cloud projects, neither in the developer console browser nor via the command. Both of them only display buckets for one project at a time. But if you specify a particular project (or stick with the one selected by default) you can list the accessible buckets inside that project in both (as shown in @chupasaurus's answer) and in the developer console browser. In the browser you get to the project selection popup by clicking on the project's dropdown menu in the top navigation bar. You could try to file a feature request for it at $URL$ it might get accepted if it shows enough interest. 

Sharding logfiles (using multiple logfiles active in the same time) is itself a technique used by some hosting providers offering high performance, scalable centralized logging services. For example, when exporting logs to files Google's StackDriver Logging produces multiple sharded logfiles. From Log entries in Google Cloud Storage: 

According to How to let install dependencies for me? it sounds like doesn't have support for automatically installing a package's dependencies. Personally I'd switch to a package manager which has such support, like . You'll like it ;) Alternatively you could: 

Note: I didn't use this yet, the answer is based solely on documentation. A GitlabCI pipeline can be triggered via API, see Triggering pipelines through the API. 

The syntax failures are caused by the presence of the expression blocks (normally used for filling the template output with the corresponding content) inside the statement blocks. I only used standalone jinja2 templates, so I'm not 100% certain if this applies to Ansible jinja templates as well, but I suspect so. In the Jinja2 statement blocks variables are referenced directly (and variable assignments are done in statements), so what you're after may be along these lines: 

Oh, and such teams can only be functional if they are complemented by high-quality architect resources which would be necessary to split the product in the smaller, specialized tasks that can be addressed with the specialized resources. In smaller scale or even one-man teams (typically startups or even isolated smaller teams in larger organisations) it's ineffective or even impossible to use such resources and still get the job done: 

A third party app metrics tools is great if it already covers all (or most of) the parameters you want to monitor and it presents the information in a convenient enough manner for your use case - you just need to (find it first and then) learn how to install/use/maintain it and then do that. Advantages: 

I support J.Doe's suggestion for separate Docker files for each of the build and test stage. Such approach also allows you to: 

But I'm not familiar with pygradle, I can't really tell exactly how the plugin works and/or is configured to do the job.