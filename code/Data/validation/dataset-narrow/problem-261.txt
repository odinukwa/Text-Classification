Disabling auto-updating statistics for this table in this situation should be just fine, since you only address each record by its unique clustered index. Since the selectivity of the index is perfect, and it never changes, the optimizer should never have a need to look at other query plans based off of changing statistics. 

There's no way to tell SQL Server not to use the transaction log. What you can do is set the recovery model of the database to SIMPLE, which will overwrite old log entries as space is needed. You should not do this on your production server, however, because you won't be able to do certain types of restores, such as point-in-time restores. Alternatively, you can set your transaction log file to be larger -- as an unscientific rule of thumb I'd make sure that either A) your transaction log has at least about 1.5x more free space than the size of your table or B) that your transaction log can auto-grow to a drive which has at least about this amount of disk space free. You can free transaction log space by backing up the log. If you don't care about the log contents, throw the file away. A shortcut for this is . Again, don't do this on a production server unless you are absolutely sure you understand the implications. Another thing to be careful of (though it's not entirely germane to your question) is to make sure the table you're expanding has a clustered index defined on it. If it does not, the table could incur a very large amount of heap fragmentation, and potentially become needlessly large on a change like this. 

Yes, but it depends on which index key you search by. Think of it like one of those old "white pages" phone books. In a phone book, people are ordered on the pages in the order LastName, FirstName. That means there are two components to the phone book's index. If you're looking for all of the people with the last name of "Smith", you just find the first "Smith" (easy to do since it's in order), then keep reading until you see someone who's not a "Smith". But if you're looking for all of people with the name "William", you're going to have a tough time. You'll have to scan each and every entry in the phone book, collecting answers, even though FirstName is in the phone book's "index". Database indexes (conceptually) work the exact same way. 

50,000 rows joined against a million-row table seems to be a lot for any table without an index. It's hard to tell you exactly what to do in this case, since it is so isolated from the problem that you're actually trying to solve. I certainly hope that it's not a general pattern within your code where you're joining against many unindexed temporary tables with significant amounts of rows. Taking the example just for what it says, why not just put an index on #Driver? Is D.ID truly unique? If so, that's semantically equivalent to an EXISTS statement, which will at least let SQL Server know that you don't want to continue searching S for duplicate values of D: 

The query of the high cardinality column took 84 seconds (elapsed) at thresholds over 1500, and about 14 seconds (elapsed) at thresholds under that number. The query of the low cardinality column took about 250ms (elapsed) at thresholds 500 and under, and 18 (elapsed) seconds at thresholds above 1500. (I didn't try to gauge the exact point at which it switched plans.) Interestingly enough, when parallelism is inhibited, the total CPU time for the low cardinality query shoots up dramatically; perhaps the server stops using batch mode for this query. Heh, ultimately running tests leads to more questions, but that's all blog-fodder, and goes beyond the scope of this question. 

Instead of simply eschewing GUIDs as your clustering key, why not change how you're assigning them? Are you using NEWID() to generate your key? If so, use NEWSEQUENTIALID() and you'll likely do a lot better on fragmentation. 

It's SQL Server that's consuming all of this memory. You aren't seeing the memory it's consuming because SQL Server is probably using "locked pages" which are not part of the server's working set, which is often all you'll see in Task Manager. There's a detailed description on why this is on one of the MSDN Blogs. 

No, there is likely nothing wrong with your database. SQL Server reserves a very large amount of memory for itself for the purpose of caching disk reads, among other things. In server scenarios, it is absolutely common for SQL Server to take tens or even hundreds of gigabytes. What's out of the ordinary is that SQL Server is taking so little memory, but that's because you're using SQL Server Express Edition. 

A lot of folks added a lot of good points in the comments. 1) Separate your transaction logs onto a different drive. That's going to be tough with a laptop. If you can't do that, get yourself an SSD for the laptop, and that should make your life considerably better. 2) Pre-grow your data and log files to a target amount. If you expect to add 1GB of data to your database, make your data and log files at least 1.5GB to start. Data and log file autogrowth are killers for performance and may very well be the reason you see these 10 second "chokes" in performance. SQL Management Studio has some built-in reports (I believe you right click on the DB, then select Reports -> Disk Usage) which should have a table of all of your autogrow events there.) 3) If you can batch your inserts on a per-client basis, then do so using the SqlBulkCopy or BULK INSERT statement. 4) There should be no reason why your tables shouldn't have a clustered primary key. SQL Server should be able to deal with a clustered index on your IDENTITY column in a fairly performant way. 

It's because just gives you a drive letter alias for your current session; it's not a server level thing. This is not going to be possible using this command. Use the real path instead. 

You're right about each of your options. Log Shipping is perhaps the easiest way to almost accomplish what you want, but there is the drawback that you won't be able to read from your secondaries while the logs are being restored. If your goal is simply to become more highly available in case something bad happens, I advise you to at least get your backup strategy up to speed ASAP. That means putting your databases in FULL recovery mode and performing transaction log backups at a high frequency, and putting those backups on a different disk than your database and transaction log. One thing you didn't mention was creating a simple shared-disk cluster. Again, it depends on what type of redundancy you want. If you're just trying to ensure against a server hardware failure, this might be the way to go. The technical solution to your problem that gives you the highest availability and flexibility is to upgrade to SQL Server 2014 Enterprise Edition, which has support for AlwaysOn Availability Groups. AlwaysOn is a technology almost identical to mirroring, but allows for a readable secondary (to be sure, there are other benefits). That upgrade can be costly, however, since it requires Enterprise Edition. It's also pretty tricky to configure the first time, and you'll need to make sure you have a good relationship between your DBA-ish people and network administration team to pull it off. It may even be worth hiring a consultant for this setup alone if you really don't have the DBA resources. Just make sure the consultant has experience setting up this type of cluster. It IS something you can pull of yourself if you know enough about networking, security, clustering, and SQL Server, though. All the resources are out there, you'll just have to do some digging. Brent Ozar has some good resources you can check out, but you'll probably want to look into a book purchase. Another nice thing about AlwaysOn AG's is that you can set up an Azure instance as your secondary without making another hardware purchase. This can be a really smart for DR scenarios where your main data center goes out -- you'll have Azure's completely separate infrastructure to fall back on. Sorry, I think I didn't answer your question too well, I think I just gave you a lot more to think about. That's how this goes, I guess... 

First off, what I'm not asking. I'm not asking what my setting should be. Many are recommending upping the value past the default, and I certainly understand why that's the case for B-Tree based queries. But I've been reading about the (almost) linear scalability of in-memory clustered columnstore indexes, and I'm wondering if setting the cost threshold too high could cause SQL Server to starve columnstore-based queries for CPU cores. So the question is this: Does SQL Server treat columnstore indexes differently when the 'cost threshold for parallelism' is concerned, and should that cause me to change my decision about what my initial setting should be? 

It's because when you rebuild indexes, you also necessarily rebuild the statistics on each index as it happens. That means that SQL Server rebuilds internal "tables" of data about the distribution of data in your indexes themselves. The distribution of data indicated by your statistics govern how the cost-based query optimizer chooses to create plans for your queries. In essence, it means that SQL Server is probably executing your queries using a different plan than it did before because the distribution of your data changed. There's no simple fix to this. It involves real detective work. Start with SQL Server Profiler and get an idea of what queries are actually running in this job. Analyze each query plan in detail and find out what parts of the query are taking the most time, and adjust your query, indexing scheme, or even your schema accordingly. 

Open SQL Server Configuration Manager. That will likely show you the answer to your problem. In SSCM you should be able to see what database instances are installed, what their instance names are, what ports they run on, and whether the services are up. There could be a number of different reasons you're getting this such as the server is not started or you didn't install the server using the instance name you think you did. 

Install the tools you need now, and no others. You can always add more features to the installation later. If you're not installing reports or PowerPivot into your existing SharePoint installation, don't install them. When most people say that they want to get "SQL Server" installed, they mean just the database engine. If that sounds like you, install the Database Engine and Management Tools, and you'll have a basic SQL Server up and running. 

MS Analysis Services (Multidimensional)'s [almost] whole reason for being is to abstract away aggregations so you can simply query tour OLAP data without having to worry about how the aggregate value is being retrieved. In fact, in Analysis Services Multidimensional, you point and click to create your aggregation combinations. I'm not sure what database server you're running, but if you're running SQL Server 2014 you also have the ability to build a clustered columnstore index on your fact table. Doing this will, in a most peoples' cases, essentially eliminate the need for you to perform aggregations entirely. I'd definitely look into that as an option if you're on SQL Server and your data and resources are a good fit. In that similar vein, if you're not running SQL Server, there are a lot of columnar databases out there which service OLAP workloads quite well, such as Tableau ($$$). I'm not sure what the open-source space has to offer, but that's worth checking out as well. 

I have to say that I don't quite agree with some of the comments that state that what you're seeing is necessarily a framework problem. The Linq to SQL framework most certainly does allow you to specify a stored procedure as the means of input for rows. That being said, I'd highly recommend you use that mechanism instead of a trigger if at all possible. You'll be able to add your locking hints inside the stored procedure and your inserts will work fine. Your spec, however seems highly suspicious to me, and IMO the problems you seem to be running into right now are good counterexamples as to why one might not want to pursue a design such as this. So my (purely rhetorical) question is for the designer. What is that problematic sequential number supposed to represent? If it's a sequence in time, then why not just use a time and use the window functions to order the rows in something like a view instead (you can point Linq to SQL to views as well)? Is the intent to have some sort of gapless sequential number? Are you signed on to all of the complexity and locking you're going to have to deal with just to maintain that number? Unfortunately, I think there is no real answer -- no magic combination of hints which will work across your frameworks and design -- which will solve your problem. IMO my answer is take a step back and look critically at your design. 

In versions of SQL Server prior to 2012, in order for anyone to log in to a database using Windows credentials, you first had to create a server-level login, them map that login to a user for each database. In 2012/2014+ my understanding is that this is not necessary, and that a user can be granted access to a database merely by granting their domain login permissions on the database directly. Is there any reason why we as SQL Server Admins should define a server-level login for a Windows account if that account isn't intended to have any specific server-level role (backupoperator, sysadmin, etc)? Is the best practice now to simply forego the server level login definition for Windows accounts altogether, unless you want to grant that login a server-level role? 

will likely cause problems in high throughput situations because multiple readers will get the same MAX(bar) value at the same time. However, you will likely still need some form of non-OLAP operational reports and queries, and snapshot mode can be a great boon for performance there. My point is that you can design and develop your OLTP system for RCS mode, and should consider it. After all, that's pretty much how most OLTP systems in Oracle are written. 

I wouldn't assume that snapshot or READ COMMITTED SNAPSHOT necessarily will increase your overhead that might lead to performance degradation -- the reduced wait times may increase throughput. The danger with RCS and SNAPSHOT has more to do with how the app is coded -- if your programmers assume the blocking semantics of READ COMMITTED when the queries execute in Snapshot mode, you could be in for trouble. For example 

In short, for this pattern, I would not use a LOOP hint. I would simply not use this pattern. I would do one of the following, in order of priority if feasbile: 

Yes, there is a pretty big pitfall you're going to run into fairly quickly, and that is with the size and maintenance of the tables. You are somewhat on the right track by saying that you want to put your data into a temporary table daily, and then move it into your permanent table, but you'll soon run into trouble with this scheme. For example, let's say you want to "roll off" the oldest month's worth of data after two years. In your design, you would have to issue a DELETE statement against your big, big table. This will likely be somewhat slow, depending on the number of indexes you have. Also, it will cause index fragmentation, and the only way to fix that would be to rebuild or reorganize the indexes on this very large table which would also cause performance problems. There are a whole host of other issues with a big single table type design as well. For example, with a big, single table, you can't do FILEGROUP based backups, which means that if you want to have a full backup of your database, it's gonna be BIG, and it's gonna take a LONG time to complete. What's the solution? Table partitioning. Read about this in depth, in as many places as you can. Basically, partitioning allows you to split up your data onto "tables within tables" -- each partition shares the same schema, and is accessed through the table object, but can be indexed and maintained differently. Partitions are basically tables, cut up by some useful key. In your case it will likely be date. They can be dropped just like (and just as fast as) tables, which means that if you partition your big data tables by date, you can simply drop old partitions instantly, with no adverse effect to the indexes on any of the other partitions. You can put partitions on different filegroups, which means that older partitions can be rolled off, or rolled on to cheaper commodity storage if it's not commonly used. Last but not least, in SQL 2012 you'll be able to create COLUMNSTORE type indexes on your older, read-only partitions, while having a different, more insert-oriented indexing scheme on the active partition where you're inserting all your sensor data. Hope this helps. You have a good amount of research to do regarding partitioning and partitioning schemes, but hopefully now you know the direction you need to be looking. P.S.: Oh, and I forgot your bulleted list of questions... Answer 1, 2, and 5. See above. Answer 3: In SQL Server, you can compress on a partition by partition basis, so compress your older partitions aggressively using PAGE compression. But I believe your out-of-row large data types will not be compressed if you do this -- again, you may want to alleviate this problem by normalizing your sensor values. Answer 4: Absolutely not, but if all you want to do is store static data by day and never search on it any other way, compressed flat files may be a much easier way to go. P.P.S: Oh, and another thing. You don't need your two-table solution to make this all work. Large binary sensor data should be of type VARBINARY(MAX) because its values can be stored "out of row" but still be a column in a single table (see the sp_tableoption documentation). You may want to consider normalizing some of your sensor data out of the binary data you have in the table, though, because your database won't be good for much beyond retrieving chunks of sensor data by time if you don't.