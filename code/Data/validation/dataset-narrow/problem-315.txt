This function is very easy to invoke from my Ado.Net client. All I need to do is pass the parameters. However, if I want to test this function from pgAdmin, the result set is open on screen only if I keep my transaction open. This is inconvenient. Of course, it is easy to expose the data as an HTML table or an Excel spreadsheet, but this is kind of a minor inconvenience. 2. Using setof records 

For performance reasons, in most cases we must store current balance - otherwise calculating it on the fly may eventually become prohibitively slow. We do store precalculated running totals in our system. To guarantee that numbers are always correct, we use constraints. The following solution has been copied from my blog. It describes an inventory, which is essentially the same problem: Calculating running totals is notoriously slow, whether you do it with a cursor or with a triangular join. It is very tempting to denormalize, to store running totals in a column, especially if you select it frequently. However, as usual when you denormalize, you need to guarantee the integrity of your denormalized data. Fortunately, you can guarantee the integrity of running totals with constraints â€“ as long as all your constraints are trusted, all your running totals are correct. Also this way you can easily ensure that the current balance (running totals) is never negative - enforcing by other methods can also be very slow. The following script demonstrates the technique. 

The reason: I believe that the result of @DateTime-7 is not documented. Even if it just happens to be equivalent to DATEADD(DAY, -7, @DateTime), it may break in a later release. 

I agree, a T-SQL preprocessor is badly needed. I developed my own one in C#, which took me like an hour. Besides control over order of execution of SQL scripts, it also allows me to have macros that look like scalar UDFs and are convenient to use, but perform as fast as inline UDFs. 

You can store A and B columns in a separate table. Make sure your table has a primary key on A, and a unique constraint on (A,B) Refer to (A,B) from your table. The primary key on A will guarantee only one B per A. The foreign key without ON UPDATE CASCADE will make sure B does not change as long as the row is referred from the child table. 

It is kind of possible: you can invoke a scalar UDF from you CHECK constraint, and it kind of can detect cycles of any length. Unfortunately, this approach is extremely slow and unreliable: you can have false positives and false negatives. Instead, I would use materialized path. Another way to avoid cycles is to have a CHECK(ID > ParentID), which is probably not very feasible either. Yet another way to avoid cycles is to add two more columns, LevelInHierarchy and ParentLevelInHierarchy, have (ParentID, ParentLevelInHierarchy) refer to (ID, LevelInHierarchy), and have a CHECK(LevelInHierarchy > ParentLevelInHierarchy). 

There is a very good blog post by Hugo Kornelis that explains it in good detail: Can you trust your constraints 

As in my previous solution, we can use constraint to ensure the integrity of data (the sequence of dates must have no gaps or duplicates). It is relatively easy. 

The following SQL Server solution uses only constraints. I am using similar approaches in multiple places in my system. 

SQL Server optimizer is closed source, so we cannot see what exactly is going on under the hood. Of course, there are tons of documentation, and just as surely there are lots of cases when this thing does not behave exactly as documented. In my experience, all documentation from all vendors tends to get out of sync quite soon, sometimes before the first release ;). The only 100% accurate source of information is source code, pun intended. So, even if you observe some behavior right now, there is no guarantee it cannot change later on. The reason: we have no idea what is involved, how the optimizer is making its decisions. So, we do not know if we have tested all the situations. As such, I would keep it simple: 

I would probably not even try to have portable DDL. I would be better off if I generated Oracle table definitions off of SQL Server's system views, if needed. I don't think it makes sense to write portable DML either - PL/SQL is totally different from T-SQL. For portability, it is easier to expose your database via an API of stored procedures. Signatures of these procedures must be the same on both platforms, but the implementations can use proprietary features - overall this is much easier than trying to use only ANSI standard SQL. This conclusion is based on several years of experience in developing portable systems, working on both Oracle and SQL Server. 

RequiredAmounts can select from a temporary table, table variable, a TVP. It can be anything. Just replace this sample CTE with whatever suits your needs. 

In such cases I am storing version numbers, and using referential integrity to make sure version numbers begin with 1 and have no gaps. Because I use constraints, I am sure my solution works in high concurrency situations. I have described this approach in this article 

If, however, you really need the table with both Friend1,Friend2 and Friend2,Friend1, you could create a self-referencing foreign key if MySql's implementation of constraints was more complete: 

You can serialize your inserts so that they do not collide: you can invoke sp_getapplock at the beginning of your transaction and acquire an exclusive lock before executing your MERGE. Of course you still need to stress test it. You can have one thread handle all your inserts, so that your app server handles concurrency. You can automatically retry after deadlocks - this may be the slowest approach if the concurrency is high. 

I would use the Profiler to monitor connections and determine if a query completed - it can give you a complete information about what connects and what runs. However, could you solve your problem using a more mainstream approach, such as starting a new thread, opening a normal SqlConnection in it, and executing a SqlCommand? 

The following requirement can be easily implemented with a constraint: any user can refer only 3 other users at max. 

I would redesign the applications so that rows are saved in batches. Typically batches of 500-1000 rows work well for me, but you need to run your own tests. Before saving a batch, I would serialize my batch updates using sp_getapplock. Surely this serialization slows modifications down a little bit, but saving in batches more than compensates for it, so overall this works much faster thgan saving rows one by one. Also I would run my selects with SNAPSHOT isolation level so that they are npot blocked by modifications. That done, you may never have any deadlocks at all - we do not have deadlocks in our mixed load system, and so can you. Good luck! 

To reuse code and reduce maintenance costs, I would have both selects utilize an inline UDF to keep common functionality in one place. Besides, as demonstrated in Martin Smith's excellent answer, UNION leads to a possibly unnecessary sort, which may be very expensive. In general, it pays to write shorter, more specific queries - you have a better chance to get a good execution plan. 

Of course, we need to make sure that VersionNumber starts with one and has no gaps. This is easy to do with constraints. 

Every time I run integration tests, I build my database from scratch off scripts stored in git. In the past, whenever slowness would become a problem, we would run the server locally on developer's workstation, with the database on RAM Disk - that should be fast enough. We did that many times for our applications powered by SQL Server I have not yet done that for PostgreSql, but I am very new to it. 

We have used this approach many times to change large live production tables without downtime, with no issues at all. 

THat done, you can use your second subquery to group data points into interval, or just run it on a client, where is can be solved as one trivial loop. Second approach: instead of intervals I would store sequence of events. there woudl be two kinds of events: interval start and interval end. With every I would store a running total, the number of open events after this event has happened. Essentially this running total is the number of prescriptions active after the event has happened. As in the previous approach, much of the data you calculate on the fly every time you run your query is pre-calculated in this table. We can use trusted constraints to ensure the integrity of precalculated data. I can describe it in more detail later if you are interested. 

That would guarantee that "if the LocationID is specified, then that LocationID should be a Location that belongs to the specified EventID". That done, your approach makes sense. 

Be careful with READ_COMMITTED_SNAPSHOT: if you set it on, it can cause lots of subtle bugs. Also READ_COMMITTED_SNAPSHOT is the default isolation level, which may be overridden by something. Run DBCC USEROPTIONS to determine the actual isolation level your select runs under. I would explicitly SET TRANSACTION ISOLATION LEVEL SNAPSHOT right before your select. That way you will be sure your select never embraces in deadlocks, and you do not break any other code, like READ_COMMITTED_SNAPSHOT might. 

According to MSDN, Getdate(), GetUtcDate(), and CURRENT_TIMESTAMP all return DATETIME. I ran a short test, which confirms that: 

If all you need is to store which step of your process comes after which previous step, then all you need is the following: ProcessID | ParentProcessID | PreviousProcessID Of course, you will need a FK constraint to make sure that (ParentProcessID | PreviousProcessID) points to a valid (ParentProcessID | ProcessID) If I understood your requirements and this design is valid, then it is easy to insert/remove/move around steps in your process - you do not have to propagate any changes to your child tables, because they refer to your primary key on (ParentProcessID | ProcessID). HIH 

It is easy to accomplish with a third party tool like Data Compare, or just do it on the client. In the context of unit testing stored procedures, we just wrote some C# code. Here is the C# code we are using, quoted from an old article:Close those Loopholes - Testing Stored Procedures 

There is no substitute for benchmarking. To answer the question, I would create and populate several possible tables. Then I would expose these tables to your typical workload, and benchmark. Including additional columns in you NCIs will slow down modifications and speed up selects. Based on frequency of both, we can choose which approach uses less resources. If a row is on average read twice per year, your conclusions might be different as compared to the case when every row is on average read twice per minute. Besides, not all queries are born equal. If some queries must complete in certain time no matter what, then you must make sure these requirements are met. Obviously such requirement trump the common good approach described above. Only you can know the actual requirements. 

To speed it up, I might create a filtered index that would include only gaps. This means that all your gaps are precomputed, so selects are very fast, and constraints ensure the integrity of your precomputed data. I am using such solutions a lot, they are all over my system. 

Once we have this redundant data, finding descendants is easy and fast - just one simple query without recursion. Naturally, with this approach we need substantially more storage. Of course, with redundant data there is always the risk that it is inconsistent. We can use constraints to enforce the integrity of redundant data. This is complex but doable. 

If your priority is speed of selects, the following approach allows for extremely fast selects. Instead of storing a period, you want to store two events (period start and period end). Change column is 1 when the period begins, and is -1 when the period ends. If more than one event occurs on the same day, they must have different EventNumberPerDay. RunningTotal is the number of open peroids after the event has happened: 

** Again, the same constraint UNQ_IntegerSettings_SettingID_PreviousFinishedAt guarantees precisely that, as demonstrated below: 

This seriously depends on the expected half-life of your project, on how agile your environment is and so on. If your project is there to last, and changes are possible, I would not assume that your composite PK is never going to change. I have seen too many projects burned by such assumptions. One good example would be the use of Social Security Number. Long ago it was a common practice to use it as a natural PK. Later on, many systems had to restrict its use for privacy reasons, and it became not possible to propagate SS#s all over child tables. The systems that used identities adjusted to the change easily. The systems that used SS#s in child tables had to go through a major overhaul. So, in many cases it is cheaper to use identities as PKs, adding a UNIQUE constraint on what currently is considered to be unique, and referring child tables to ParentID. This way we have to do less work to adjust when the situation changes. 

If you denormalize like this, you do not need to traverse up your tree at all. Note that I added two constraints, a foreign key and a check, to ensure that BillToCustomerID is always correct Another way is to use materialized path. I can post it tomorrow if you are interested. 

Everything necessary to complete a project must be in version control, in a branch. To take over somebody else's unfinished task, all we need to do is pull a branch from git. This applies to test data etc. as well. This will allow the continuous integration server to build/populate a new test DB from scripts downloaded from version control, and run the test suite I am not sure why was this question migrated from stackoverflow - this is IMO strictly programming. 

ERDs used to be more mainstream some time ago. IMO they are more or less optional now. Currently, some highly successful teams do not bother with ERDs at all, yet deliver excellent systems: robust, performant, and easy to maintain in the long run. This is especially true in Agile development, when we start small, with a minimalistic set of tools. ERDs are a good way of communication, of course, but by no means the only way, or the best in all circumstances one. Some teams prefer other ways of documentation and communication. There are no blanket rules in this industry. 

Currently dbcc DATA_PURITY complains about subnormal real values in some of my columns, such as 7.561407E-42. I would rather keep them in my tables, and ask dbcc to ignore them. I am OK with the check on upper limit, and other checks on other column types. So I would rather keep running the utility, but have it stop complaining about legitimate very low values. Is there a way to accomplish that? 

Snapshot isolation is robust. It does work. I have been using it in my system for several years, and it does reduce the amount of deadlocks. However, snapshot isolation adds more workload to your tempdb, so you need a skilled DBA who can ensure that your system withstands the load. Before you start using snashot isolation in production, make sure you have a DBA who understands how it works. 

Once you have created this constraint, you will only be able to insert both rows in one statement. Unfortunately, this is does not work on MySql. 

To view the trace file contents in SMSS, I am using the following function: FN_TRACE_GETTABLE. To run FN_TRACE_GETTABLE, the users don't need any permissions on the server's file system. Only the account that is running the service must have the permissions to access the trace file, but it already has them, because it has created the trace file in the first place. I have an automated job that uploads a trace file into a table every weekend, and neither me nor the account running the job have any permissions on the server's file system. This is running 2008 R2. When I was developing that job, I had run FN_TRACE_GETTABLE from SSMS quite a few times. 

This might be fine for one change per module, but imagine how your code will look like after severol years of development. Before version control was universally adopted, there used to be modules that were 95% history log and only 5% actual code. Fortunately version control automated out that inconvenience long ago. I would completely get rid of change logs in comments - we are much more productive using modern tools. Besides, git and other version control tools allow you do do much more than see the log of changes if one file. For instance, they can will show you all other files changed in the same commit. Also they give you accurate information, while we cannot fully trust comments.