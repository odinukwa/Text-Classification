I have a SQL Server 2008 R2 database server that hosts a data warehouse database. On this server is also installed SQL Server Reporting Services, and its two databases ReportServer and ReportServerTempDB. For performance reasons I want to separate the data warehouse database and the Reporting Services. I have another server that is available to install SSRS onto. Both servers are virtual and so their specifications are easily changed, or other servers could be 'created' if necessary. What I can't get my head around is where the best place would be to host the ReportServer and ReportServerTempDB. I can see three options: 

I'm importing a large amount of data into an empty database, and before I start I disabled all non-unique non-clustered indexes to see if I could improve the performance of the import. Now I want to re-enable the indexes, and I'm wondering if there is anything that I can do to optimize this. There are > 100 tables and almost 2,000 indexes to be rebuilt. The database is 200GB in size. The key section of the script I'm running is this: 

As a very generic answer, if you want to test code of this nature without actually changing the data then it is possible to run your code, query the results, and then undo any changes before they are committed. 

Given that the package works both from the Wizard and from Integration Services I believe the server has everything needed to run the package from SQL Server Agent, but I cannot see how to do it. 

returned "The backup set on file 1 is valid." Which is ironic given that I was only using the MAXTRANSFERSIZE option to try to speed up the restore, but instead it cost me half a day! The error only occurs on backups taken from last night onwards. Earlier backups verify fine whether I specify MAXTRANSFERSIZE or not, which is strange, but I guess I can live with not knowing the reason why that is unless somebody has an insight into that behaviour? 

An alternative way that may be faster is to use the STIntersects and BufferWithTolerance methods to check if one point is within a certain distance of another. 

In the properties of a data source in SSRS, I can specify a Windows domain account to use, like this: 

I have a two-node SQL cluster (2008 R2). Some of the databases within that SQL instance are mirrored to another server on a remote site, using the High safety with automatic failover. The mirroring connection timeout value for those databases is set to 90 seconds. When I move SQL from one node in the cluster to another node, using the Failover Cluster Manager application's option of "Move this service or application to another node" the databases that are mirrored are instantly failing over to the mirror. This is undesirable behaviour. My reason for setting the mirroring connection timeout value is that I only want to fail over to the database mirror if the cluster fails completely and there are no functioning nodes. Is there any way to achieve this? It feels as though it should be possible, otherwise the concept of mixing clustering and automatic failover database mirroring would be unworkable as every node failover within the cluster would trigger a mirror failover. Thanks. 

I have a third-party application that connects to a SQL Server database via a SQL Server Native Client 10 ODBC data source. The application requires that each user of the application must have read/write/execute access on the database. My concern is that this allows each user to connect directly to the database (using Microsoft Excel, SSMS or any other database connection tool) and change or view the data in any way they care to without the controls, restrictions or auditing enforced by the application. I cannot change the requirement for each user to have access to the database (at least not without paying for the application to be re-written somehow!) but I was wondering if there was a way to restrict access so that those users can only connect to the database via the server where the application is hosted, or via that one specific ODBC data source, or when the connection is set up by that specific application? I suspect that I know the answer as I haven't been able to find anything, but I wanted to throw the question out there before I gave up on this. Thanks, Paul. 

This data source was originally deployed from a SSDT project and I need these settings to be reflected in that project - otherwise the data source will no longer work correctly the next time that the project is deployed. Unfortunately SSDT does not seem to provide a way of doing this. If I specify credentials here it automatically assumes SQL Server authentication (which is of no use to me). 

I realise that my question isn't strictly 'database' but I know that a lot of DBA's will use the SQLIO tool to check I/O performance on their systems so I'm hoping that somebody here will be able to answer this question: When I run SQLIO the 3rd line of output says something like: using system counter for latency timings, 14318180 counts per second What does that mean? I get different values on different systems, and because I'm comparing different systems I would like to know what that number is telling me (if anything). 

I have used SQL Servers Import Data wizard to import data from an Excel spreadsheet. The Import Data Wizard worked fine to import the data. It also worked fine to save the SSIS package. I can then log in to Integration Services on the server and run the package from within Integration Services. However, when I try to run the package from SQL Server Agent, I get the error message: 

Using Kenneth's code, and a few other bits and pieces that I pulled together, I came up with the following code which I think should do the job. If anybody sees any potential problem with this I'd be interested in knowing! 

Now I've run this script on about a dozen different servers - mostly virtual but a couple physical, mostly 2008R2 Enterprise but a couple of 2005, some live and some development, and even a physical 2008R2 Express edition that has only ever been a witness server and is currently dormant (no mirrors set up with active failover). On only one of these servers has the value been below 100. In most cases it has been in the 1000's or at least in the mid 100's. On the dormant witness server the value was a ridiculous 20,000 ! Does this indicate that we have widespread problems in this area, or is there some innocent explanation for why this figure should be so high on so many different servers? We are experiencing slow performance on some of these servers, so it would be useful for me to get a better understanding of what this figure is telling me. [1] - $URL$ 

I've just installed SSMS 2012 and when I loaded it up I was presented with a nasty blue background - it looks so bad that I honestly thought something was broken, but apparently it's supposed to look like that! Short of sending the SQL Server Design Team a set of vouchers to have their eyesight tested, is there anything that I can do to change this? I've looked around and found a couple of other people whose sight has been similarly assaulted, but I cannot find a solution. 

With option 1 I worry that separating the Reporting Services from its databases will either impact the data warehouse server, or will impact the reports. With option 2 I worry that installing the database engine on the server will impact the Reporting Services server performance, and with option 3 I worry that I'm wasting resources creating a server that will hardly do anything, and also impacting the reports by separating the Reporting Services from its database. I think the key piece of information I need is to know which part of Reporting Services is the most resource intensive. If it's the Reporting Services application then I can leave the databases with the data warehouse. If it's the databases that are resource intensive then I need to either put them with the Reporting Services application or create a new server. Can anybody shed some light please? Thanks! 

Is it possible to restore the incremental log backups onto the mirror to bring it up to date and back online, or would you need to start from scratch with a restore of a full backup? Depending on the size of the database the difference could be hours of downtime so it would be nice to know. I'm predominantly interested in SQL Server 2008 R2 and 2012 but other people may be interested in other versions if there's a difference. 

In SQL Server 2008 when using database mirroring, it was possible to set up alerts when the oldest unsent transaction exceeded a certain threshold. We could check the state of any database mirroring using the Launch Database Mirroring Monitor menu command. In SQL Server 2012 we now have Availability Groups. It would seem to me that the same potential problem exists that if transactions cannot be sent to the secondary server then the mirroring will be suspended, and potentially the transaction logs will fill up all our disk space. However, the Database Mirroring Monitor tool does not seem to recognise databases in Availability Groups as being mirrored. Is there a way to set up an alert if the oldest unsent transaction exceeds a certain value? Is there a way to set that certain value? 

I considered setting ONLINE=OFF for the alter index statement, but as the indexes start out disabled I wasn't sure that this setting would have any effect. I also considered setting SORT_IN_TEMPDB = ON, but as the tempdb files are on the same drive as the .mdf files of the databases I assumed that there was also no benefit to doing that. Whilst running the rebuild script I have noticed that I have a lot of CXPACKET wait types. I don't really understand why that would be or if it's a problem that I should be looking to address. One final point that may be relevant: my entire server is currently inactive other than this import of data into the database. There is no other user activity to consider or worry about; my only concern is importing the data into the database in the shortest possible time. 

I have a third party database in which a large number of the tables have a 'companyID' column, and most of the queries against those tables include in the where clause. [EDIT: In addition to the where clause, the companyID is generally also used in the order by clause of most queries: e.g. ] In our organisation the companyID column is always populated with 1 (though there is an outside chance that may change at some point in the future). Now I am looking at creating some indexes on these tables. SQL Server recommends some 'missing indexes' and all the recommendations contain the companyID field as the first column in the index. Going on the principle of specifying most selective column first I would think that I should put the companyID column last, or perhaps not even have it in the index at all. But then I'm wondering why SQL Server's missing indexes view always suggests putting it first - surely SQL Server is intelligent enough to know that every row contains the same value for that column? 

I'm in the process of building a new data warehouse and I am starting out by building a few dimensions. I have created a fairly typical "Date" dimension and that seems ok. Now I am trying to create a "Person" dimension which contains "Date of Birth Key" and "Date of Death Key" attributes, both of which are related to the key attribute of the Date dimension. When I process the Person dimension I am receiving an error about duplicate keys. Looking into it, I found that the code that is generated for processing the Date of Birth attribute is incorrectly joining to the Date dimension using the Date of Death column: 

I have Instant File Initialization enabled on our SQL Server, so the 'empty' part of the database file is not zeroed before being allocated. My understanding is that this means that the file could contain 'deleted' data. So now I want to send a copy of a database (probably a backup file) outside of the company. But there's all that potentially sensitive 'deleted' data sitting around inside the file. Now I would like to zero the unused portion of the file. Is that possible? I imagine I could create a new database and copy everything over, or perhaps restore a copy of the database to another server without Instant File Initialization enabled and then be aggressive with a ShrinkFile command to remove most or all of the unused portion of the database file, but is there a less manual and time consuming method? Ideally a command to tell SQL to zero the file as it would have done if Instant File Initialization was not enabled.