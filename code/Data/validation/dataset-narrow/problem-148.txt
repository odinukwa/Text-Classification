You question is confusing. The fragmentation header is 8 bytes. You say the observed payload is 1 byte. The payload is not the header, and the size of the header has nothing to do with the size of the payload. Edit based on your comment and correction of the the question: The fragmentation header is 8 bytes. You may be reading the results incorrectly, or it is possible that the application you are using doesn't correctly understand what it is seeing. 

Based on your edit, it sounds like you want anycast. This is a common among large, global companies. You advertise the same network from multiple places, and routing will take traffic to the closest (from a network perspective) site advertising that network. When one site goes down, routing will automatically take traffic to the next closest location. Basically, you leverage the routing protocol (BGP on the Internet) to automatically handle it. 

That appears to be telephone (Category-3) cable. If you are using this for a phone system, then you are probably fine, but this cable should not be used for modern network cabling. The only currently recognized UTP cable categories are Category-3, Category-5e, Category-6, and Category-6a. Your company should really hire a qualified cable installer to install a decent cabling plant. 

Assuming you want to use iBGP to advertise the routes, R5 would never learn this route through iBGP because the iBGP speaker (R5) cannot learn iBGP originated routes (originated by R2) from another iBGP speaker (R3 or R4), which is why iBGP requires a full mesh or a mitigation (route reflectors or confederations). 

The first half of the ULA address range, , is reserved for a future global authority to assign. The second half of the ULA address range, , can be assigned locally, with restrictions. The next 40 bits must be randomly chosen, and you cannot assign prefixes in any particular order. 

Absent a direct sensor, such as you have proposed, the fastest way a link-down is communicated across the network is via the routing protocol. Your client could be running a routing protocol with the routers (often not allowed in networks) in the network to have the latest routing information, Unfortunately, that information doesn't get sent up to the application, or any layer above layer-3. It would take a custom modification of the routing protocol to create such a connection, and it would likely be tied to a specific application. 

You either need to manually configure routes, or you need to run a routing protocols between the layer-3 devices. The real way to do it is to configure IP routing on all the layer-3 switches, and configure the links between the layer-3 switches and the router to be routed links, not trunks. Then you run a common routing protocol on all the layer-3 devices. 

Without NAT, port forwarding doesn't exist. Port forwarding is a function of NAT that allows traffic addressed to the outside interface, on a particular port, to be forwarded to a host on the inside network. For instance, if I have a web server on an inside network with private addressing, and I would like to let people on the public Internet use the web server, I would need to set up port forwarding on the NAT router to let the public use the public, outside address to get to the private, inside address of the web server. NAT translates an inside address to an outside address. Traffic destined for an inside private address cannot be routed on the internet. The public, outside address belongs to the NAT router. In the example above, traffic sent to the outside, public address of the router on port 80 (HTTP) would be dropped by the NAT router since it is not running a web server. When you configure port forwarding on the NAT router, then it knows to forward that traffic to a particular private, inside address. 

An IP address contains no information about VLANs or switches. IP is a layer-3 protocol which is carried inside layer-2 frames, such as ethernet. Layer-3 is blissfully unaware of which layer-2 protocol is encapsulating it. An ethernet frame may have VLAN information in the ethernet header (see 802.1Q). 

You have not given us the switch model, but you tagged your question with the Cisco Catalyst tag, so I will use the Cisco Catalyst 3850 as an example. The Cisco Catalyst 3850 has a pretty large stack switching capacity: 

The problem with UDP is that it is a simple, connectionless, fire-and-forget, best-effort protocol that has no ordering or guarantees. VoIP and other real-time protocols need to have a reasonably steady data stream that arrives in order. Load balancing of packets causes jitter and out-of-order packet arrival, both of which are VoIP killers. VoIP can tolerate a fair amount of delay, but it simply can't tolerate much variation in the delay (jitter), not can it tolerate out-of-order packet delivery. Instead of hearing, "Hello," you may hear what sounds like, "Oh, hell." It may be possible to load balance individual flows. 

That makes sure that the back-off time is never less than the slot time, and it is quick and easy to calculate. 

Why would congestion on that link cause the transmission rate to drop? I think you are working from an incorrect premise. You can try to run 200 Mbps through a 100 Mbps link, but the transmission rate will stay at 100 Mbps; the excess will just be dropped. This is layer-2 (switches and ethernet) which don't have any facility for re-transmission of dropped frames. When frames are dropped, they are just lost. Let's say that S4 is a 48-port switch, and the 48 access ports are each 100 Mbps, as is the uplink port. If all 48 access ports suddenly decided to transmit 100 Mbps each through the 100 Mbps uplink port, the uplink port would transmit 100 Mbps of the 4800 Mbps of traffic trying to use it, and the other 4700 Mbps would be lost. 

One thing to think about with the outbound ACL is that you probably want to apply it as close to the source as possible, as an inbound ACL. That prevents you from unnecessarily routing traffic that is destined to be dropped. Filtering traffic inbound where possible saves routing resources. Also, for your outbound ACL, you put the at then end of the ACL entry. Another thing is that the ACLs have an implicit as the last ACL entry, so you probably need a as the last ACL entry. 

You are going to need to shut this down on both sides, make your changes, then bring it back up if you want to do it safely and correctly. If you try to do this with either side up, you are likely to cause one or more ports to go into . I'm not even sure why you need to run LACP between Cisco devices if PAgP is working today. Why do you need to send LACP control packets every second instead of every 30 seconds? PAgP fails over extremely fast, much faster than routing protocols like OSPF. 

QoS is something you should set up when you have a bunch of users and VoIP. You really want the VoIP to run in its own VLAN separate from the rest of the network traffic. You should do QoS marking as close to the source as possible (at the switch port, if possible). Even better, the phones may already mark the VoIP traffic as EF, but that depends on the phone. The rest of your traffic can be marked as BE, or you can get into multiple classes if you have a need for that. Once the traffic is marked, you need to set up proper queues in the router for the various traffic types based on the markings. VoIP should probably have a priority queue which you need to size carefully. There are various other techniques such as shaping, RED policing, etc. QoS is a topic far too broad to properly be discussed here. Remember that, unless you pay for it, your ISP won't honor QoS markings, and any other ASes through which your traffic travels to your VoIP provider will not honor any QoS markings either. The goal for QoS in your network is to make sure that VoIP gets first priority exiting your network. 

First, it is Layer 2, not level 2. The OSI model specifies seven layers. Layer 2 is the Data-Link Layer. Layer-2 addresses, e.g. MAC addresses, are used in layer-2 frame headers to get the layer-2 frame from one host on a LAN to another host on the same LAN. Layer-3 addresses, e.g. IPv4 addresses, are used in layer-3 packet headers to get the layer-3 packet from the source network to the destination network. A router (layer-3 device) will look up the destination layer-3 address of a layer-3 packet in its routing table to determine where to forward the layer-3 packet towards the destination layer-3 address. 

Also, understand that by extending layer-2 VLANs to a remote location, you increase the chances of a layer-2 problem, and the problem will affect two sites instead of one. There is almost nothing today that requires have two hosts on the same layer-2 domain. We live in a layer-3 world, and routing works well. I don't see any advantage to extending the VLAN to a remote location, but I do see some disadvantages to doing it. 

That depends on the specific tunneling protocol in use. Some tunnel protocols support multicast, and some do not. Multicast routing is very different from unicast routing, and all routers in the path of the multicast packets must support and be configured for multicast routing. 

That would be incorrect. A modem will modulate (the in modem) an analog carrier to encode digital information on it, and demodulate (the in modem) an analog carrier to convert the encoded information back to digital information. A CSU/DSU connects two different types of digital signals. A CSU/DSU is actually two different devices, the CSU and the DSU, which may be in separate boxes. In the old days, These were separate pieces of equipment. Today, they are almost always in a single box (e.g., a DSL "modem"), or a single interface module for the DTE (e.g. a T1/E1 WIC for a Cisco ISR). The DSU (Data Service Unit) is what connects to your DTE (Data Terminal Equipment), and converts your DTE serial communications to frames which the CSU can understand, and vice versa. The CSU (Channel Service Unit) is what connects to the communication circuit, and places the frames from the DSU onto the communications circuit, and vice versa. 

A socket consists of an IP address and port combination. Your OS is only going to allow a single application to bind to a single port on a given transport-layer protocol. If you have a single application which can run all the services, then that application can bind to a single port for all the services, but multiple applications cannot bind to a single port on the same transport-layer protocol. Whether or not performance would be better with a single application running all those services on a single port is going to depend on the application, and it would be off-topic here. The fact is that each of the services you list has a separate, well-known port, and it would be awkward to use a different port than the well-known port for any of those services since clients would assume the well-known ports are being used. 

If that is the case, you configure the host DNS entries to those addresses, not the SonicWALL. You only configure the host DNS entries to the SonicWALL if it is your DNS server. 

If a malicious host somehow knows the IP and UDP addresses being used in the conversation, it could spoof the NAT and client into thinking that they are receiving data from the server, when it is really bad data from the malicious host. How that could damage the host depends on the process in the host that receives the bad data, and what the process does with the bad data. Unfortunately, applications, programming, and protocols above OSI layer-4 are all off-topic here. To learn more about what could happen in your specific circumstance, you could try to ask on Stack Overflow, where there are many network-savvy programmers. 

The only real control over traffic that you have is traffic on your network, or the traffic leaving your network. You don't have any real control over how fast an outside server sends traffic to your network. That's why DoS attacks work. By the time the traffic reaches your network the WAN bandwidth is already used. There are ways to try to mitigate this: 

If you read your IPMI article carefully, you will see that IPMI is primarily used for PCs and servers, and it is a specification for out-of-band management of a system. IPMI also requires specialized hardware. On the other hand, SNMP is a software specification which is used across a wide variety of hardware, usually in the OS, and it is primarily used in-band. SNMP can be set to send traps to an SNMP server, or it can be polled by an SNMP server or application. 

CSMA stands for Carrier Sense Multiple Access. In other words, the hardware is listening for a carrier before it will send. This is like when you pick up the telephone and you listen for a dialtone. If you hear a dialtone, then you dial. If you don't hear a dialtone, then you know the phone is disconnected, and you don't dial. Ethernet has a carrier wave that is sent along the line. It is the modulation of this carrier that are the bits that get sent along the line. 

That depends on whether or not you need to public addresses to be available outside you company. If so, you are going to need to work with the ISPs to set up BGP routing with them. If you don't need for your internal, public addresses to be seen outside your network, then you can simply NAT with the failover ISP. NAT doesn't care that addresses are public or private; there is nothing inherent in IP that distinguishes public or private addresses. 

You routers know about the networks that are directly connected to them, but they need to be told about networks that are not directly connected to them. You can either put in static routes to the other networks, or you can run a common routing protocol between the routers. 

Eddie is correct, but I think the IEEE 802.3 standard predates the RFC. Unfortunately I can only get the current 802.3 standard, although his answer is a valid answer, regardless of the timing. It is also defined in the 802.3 standard: