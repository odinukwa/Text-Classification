I've seen it happen twice after SQL Server service packs were applied - service packs that took a really long time (hours) to finish the upgrade, or broke as they were trying to apply changes to the system databases. The fact that you mentioned there was a scheduled restart clues me in, too - that means someone had probably staged updates to happen on restart, and they did - but then they caused an outage. As a result of this, I'm never a big fan of automatically applying updates (Windows or SQL) on SQL Servers. 

Performance not only depends on the query, but also the kind of data you store in the table. Let's zoom out and talk about how you choose a clustering key for a table. Generally, you want the clustering keys to be: 

Microsoft isn't improving sqlcmd - all their bets are on the PowerShell horse. When you're doing automated deployments like you describe, PoSH will give you more options for all kinds of dependency stuff. I'd focus there. 

Rather than picking random metrics and trying to tune them, what you want to do is step back and ask SQL Server, "What have you been waiting on?" My favorite way to do that is with the open source tool sp_Blitz (because, uh, I wrote it.) You can download it from FirstResponderKit.org, run the sp_Blitz.sql script to install it in the database of your choice. When you run it with just plain old: 

As of today, you can't. There are things inside the engine like advanced read-ahead that you simply can't turn off. There was a Connect request for it, and Microsoft turned it down. 

There's several different questions in here: Q: Why weren't the queries spilling before? They were, but SQL Server Management Studio didn't surface this as a clear error prior to SQL 2012. It's a great example of why when you're doing performance tuning, you have to go deeper than the graphical execution plan. Q: Why do queries spill to disk? Because SQL Server didn't grant them enough memory to complete their operations. Perhaps the execution plan underestimated the amount of memory required, or perhaps the box is under memory pressure, or they're just big queries. (Remember, SQL Server uses memory for three things - caching raw data pages, caching execution plans, and workspace for queries. That workspace memory ends up being fairly small.) Q: How can I reduce spills? By writing sargable T-SQL statements, having up-to-date statistics, putting enough memory in the server, building the right indexes, and interpreting the execution plans when things don't work out the way you expected. Check out Grant Fritchey's book SQL Server Query Performance Tuning for detailed explanations of all of those. 

Yes, run a Profiler trace while you run the report. It's just plain old T-SQL. (Source: that's how I wrote the Performance Dashboard Reports chapter in the book SQL Server 2008 Internals and Troubleshooting.) 

Compatibility levels are set at the database, not at the table or column. You might be thinking of collations, which are set at the column level. You can't simply change the collation on those objects - you have to build new objects with the right collations and push the data into them. You might also be thinking of deprecated datatypes like text, ntext, image, or vardecimal. You don't have to change those yet, but you'll have to fix them before a future SQL Server version. 

This is SQL Server updating the statistics on a table. It can be triggered automatically when about 20% of the data in your table changes, or on demand if you run jobs that update statistics. You have a few ways to lessen the pain. You can enable asynchronous statistics updates, which lets SQL Server update stats behind the scenes without blocking other queries. It'll still have a performance impact, though, as Kendra Little explains in The Secret IO Explosion. You can manually update statistics proactively during maintenance windows. This doesn't stop the chances that 20% of your data will change, but it just reduces the likelihood. You can disable automatic updating of statistics altogether. This is generally a pretty bad idea since your execution plans may be wildly incorrect, and it doesn't stop folks from manually updating the statistics either. 

To capture wait stats for 30 seconds. Then, you can tune based on your top wait type, and how much of it you're actually seeing. For example, if your top wait type is PAGEIOLATCH, that means reading data pages from a data file. To reduce that wait, here's the fixes that I'd recommend, in order: 

Yes, but only manually - Azure Blob Storage isn't integrated into the log shipping wizards. You would need to do your own backups to Azure Blob Storage, and then have the secondary server(s) restore from ABS using the same certificates. When other shops have done similar projects with AWS's S3 storage (writing their SQL Server backups there using 3rd party tools), I've seen them either log backups to a text file that the other SQL Servers check for backup files, or log entries into a centrally accessible table (like SQL Azure or Amazon RDS) and fetch the backup file names from there. 

While the other answers are good, they're missing an important piece: Windows' file cache. On 64-bit Windows, there's no cap to the amount of memory Windows will use to cache files. Windows can drain your system completely dry of memory, and at that point, you'll start swapping to disk. It's been documented in a few places: 

Take a step back and ask, why are we copying log files? SQL Server LDF files contain transaction log data, but they're not terribly useful by themselves. To work with a database, you need both the data files and log files. In order to access those files directly via the file system, though, you'll either need to stop SQL Server or use snapshot technologies like VSS. If you're trying to synchronize transactions across multiple databases, check out technologies like transactional replication. If you're trying to back up the database, use the SQL Server BACKUP DATABASE command. 

Are you considering using SQL Server to query data inside the mesh? For example, are you trying to pass in a list of points, and then have SQL Server figure out whether points in that area match up to any of the mesh data for existing rows inside the database? If so, you're gonna have a bad time - that's not what SQL Server is designed for. If, on the other hand, you're just using SQL Server as a key/value store ("give me the mesh data for this desk design") then you can use any data type you like, like JSON or XML or BINARY. You're correct, though, by saying that this rarely ends well. If you're looking for key/value storage, then use a key/value data store instead. 

SQL Server's STRING_SPLIT documentation does not specify an order guarantee. There is an open feedback request to add more features to STRING_SPLIT, including sorting, but as of this answer (mid-2018) the status is just "under review." For now, if you need to guarantee output order, try other string splitting techniques. 

Rather than looking at index utilization, I'd look at the plan cache to find your queries with the highest amounts of logical reads. Usually when I'm dealing with partitioning, I find just a handful of queries that are dominating reads - like 50-80% of the servers' reads overall. Check those queries to see if they're successfully doing partition elimination. If they aren't doing partition elimination, but you think they should (based on your partition scheme), then work with the query writers to get partition elimination. If they aren't eliminating partitions, and they can't (because of the way the query is written or the partition is designed), then it's time to start asking tough questions. If the biggest logical read queries don't have anything to do with your partitioned table, then move on and focus on those other queries instead. 

You'll see the difference before & after. To illustrate the full details, run SET STATISTICS TIME ON in SSMS first, and you'll get output on the Messages tab of SSMS showing how much CPU time you use in both examples. You'll find that it's much higher with the leading % sign. You can learn more about that in my post, Sargability: Why %string% Is Slow. 

I bet you've configured the virtual CPUs in a way that some of the CPU nodes and/or memory nodes are offline. Download sp_Blitz (disclaimer: I'm one of the authors of that free open source script) and run it: 

Without the execution plans, it's impossible to say, but I've seen a similar scenario pop up. Had a client once whose database was teetering on the edge of being able to fit in memory. Someone added an index on a large table, queries started using it (which kept big chunks of it in memory.) Things all used to fit in memory perfectly, but they hit a tipping point where they started to hammer the disk much more frequently. We caught it via the sudden spike in disk access and a sudden drop in Page Life Expectancy. Ideally, you should capture these Perfmon counters continuously on database servers, and alert when they drop dangerously. That'd help correlate things so you'd know if the PLE drop also happened, and that'd indicate you might be running out of memory to cache things. 

That update just hangs - he's blocked. Don't cancel him - leave him running. He's blocked by the Window #1 transaction. In RCSI, writers still block each other when they're trying to work on the same rows. Switch back over to Window #1, and run: 

The first result set is a set of deadlock details, and the second is analysis that looks at which apps, users, and tables are most commonly involved in your deadlocks. It also gives you parameters to use for sp_BlitzCache to analyze their query plans, and sp_BlitzIndex to analyze over-indexing and under-indexing issues on the tables that may be contributing. 

The easiest way is to buy an off-the-shelf monitoring tool. They all give you this kind of information - Idera SQL DM, Quest Spotlight, SentryOne SQL Sentry do these kinds of things with really low impact. The next easiest way is to build something yourself. If you're going that route, I'd start by logging sp_WhoIsActive to a table - especially with the @get_transaction_info = 1 switch. If you try the roll-your-own approach, you need to be aware that queries aren't the only thing that will cause the transaction log to grow. For example, if you're using replication, database mirroring, or Always On Availability Groups, SQL Server needs to retain history when one of those replicas is offline. To learn more about what's causing that, check log_reuse_wait_desc: 

It sounds like you want users to be able to restore databases from existing backup files, but only to new names. I actually wouldn't give them permissions to restore databases at all - instead, create a stored procedure that will do the restores, and then grant permissions to the stored procedure rather than the user. Erland Sommarskog's article on granting permissions via certificates is fantastic. In a nutshell, you're going to: 

You have two questions: 1. Do you have to restart after setting auto update stats on? No. 2. Should you consider dropping the user created stats? If your statistics updates jobs (typically done with a maintenance plan or with Ola Hallengren's maintenance scripts) are taking longer than your maintenance window allows, then yes. SQL Server has to read the entire index (or table) to update each statistic, so you can end up with really long stats update jobs. If your maintenance window isn't a problem for you, then you can leave 'em as is.