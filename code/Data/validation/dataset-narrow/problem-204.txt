Also, the PK_ID column that you described should be the PK and clustred index for the table, otherwise its just a heap. Also, in terms of logging server logins, perhaps you'd have better luck using an Audit or extended event session? The "The definition of object ‘my_proc’ has changed since it was compiled" error that you described should be resolvable by calling sp_recompile. 

Check the "On Success" for the last step of your job. It should say "Quit the Job Indicating Success" It won't close the window you used to start running the job, but it will commit the job history once all job steps are complete. If the On Success is already correct it seems likely that one of the steps never completes, but with newer versions of SSMS you should see all completed steps regardless of whether the entire job completes. 

At my current job the majority of database objects use (NO LOCK) or READ UNCOMMITTED isolation. I agree it is a bad solution, but if you go back far enough it is used quite often. I was not familiar with the acronym RCSI, but in googling it I found that it referred to using Read Committed Snapshot Isolation. Yes, if you are having issues with locking and dead-locking, you should use SNAPSHOT ISOLATION. I think the best way to prove this out would be to create two virtualized clones of your production system, use snapshot isolation on one of them, simulate a typical usage pattern on each, and verify that it improves performance. Or just do it in the lower environments and then once you've verified it is stable migrate the changes to production. No one ever got fired for using Snapshot Isolation. 

Sounds like there may be a maintenance job running intermittently in the background which is filling your log. Do you have access to the job system to look? On non-production systems you'll also want to put the database in simple recovery mode in most circumstances. 

You shouldn't have issues altering the columns to a lower size. You can only alter column types one at a time with ALTER TABLE statements anyway. While the total data file size will grow intermittently, it can't grow bigger than the original with corrected column types. So here are the steps I would take: 1) Ensure that there is a primary key, or at the very least a clustered index on the table. 2) Run the ALTER TABLE ALTER COLUMN statements one at a time 3) If the database runs out of space, run a DBCC SHRINKFILE on the appropriate file. 4) Continue with steps 2 and 3 until all columns are their new appropriate types 5) Rebuild all indexes on the database to remove the fragmentation caused by the previous steps. Very easy. I've also gone the route of creating a new table with correct definitions, copying the entirety of the existing table to the new one, dropping the original table and renaming the new one to the original. However to do this you'd need more space at least intermittently. You could try putting one of the SQL Server data files on a network drive in the interim, but I think just altering the existing columns as outlined above will be easier. 

It depends on the type of authentication you are doing. A common practice in SQL Server is to use only windows authentication, then to use SUSER_SNAME() as the default when saving to a User_ID column. I am not certain which MySQL equivalent is correct, but based on a quick Google it is likely either 

Yes, if you want to reuse an image between multiple entities and a single entity can have multiple images you'll need to make it a many-to-many relationship. And yes, you would want to create multiple associative tables for each many-to-many relationship. The only way around this would be to create one Entity table that all of the other tables pulled their IDs from, but I think what you currently have is easier to maintain going forward. 

As the previous answer stated, it will drop and recreate objects in many situations where it does not need to. Your best bet is updating either the database or the source control definition for the objects to be in sync with each other, and not rely on the SSDT reconciliation tool, which makes very bad choices. For example, if there is a difference in a constraint name, for example, between source control and the database environment, you could rename it in the database environment to be consistent with SC, or rename it in SC to be consistent with the DB. Otherwise the SSDT will want to drop and recreate it because it always takes the path that will work in all situations, not the path of least resistance. I've seen it drop and recreate a table just to change the column definition on one column from VARCHAR(50) to VARCHAR(100). 

What you are asking to do is pretty straightforward, and I see some good answers already, but here is the simplest version. You need to join the consumer_client table twice to the person table. So 

I'd have to run it through SSMS to know for sure, but that syntax looks off to me. While I've defined columns as primary keys inline (like you are doing) I've never attempted to define a clustered index that was not the primary key in that fashion. What you are doing simply isn't possible in the SQL Server version you are using. I think you should define the clustered index using a standard CREATE INDEX after the table has been created via the CREATE TABLE statement. 

I can only speak definitively regarding SQL Server, and it appears this is not consistent across all database implementations. But in SQL Server, functions may not produce side effects. This is a hard and fast rule I've tried to circumvent a number of times with no success. If you are thinking about functions in the general sense, there are SQL modules that allow side effects (for example stored procedures) but user-defined functions do not. There is a saying "Clever Solutions Don't Scale" which is especially true with Microsoft products. I've seen a number of clever workarounds in early versions of SQL Server that became obsolete in later versions, because MS added them as true features. The ones that never became features, honestly, never became features because they fundamentally broke some aspect of T-SQL development. Side effects in functions is one of them. 

SSDs are great in general, but they are much better suited to read heavily applications, while tempdb is a mix of reads / writes and the nature of tempdb generates a consistent churn which is not good for an SSD. But it sounds like you've already gotten your tempdb off the SSD, so that step is already done. I've not tried swapping out a local SSD for a SAN drive, but I've been involved in a migration which did that process in reverse (swapped a SAN drive for a local SSD) We failed over to a virtualized server during the process (the main server was a physical server) but it was very straightforward. We might have been able to swap in the SSD and done the entire process in a 30 minute window and skipped the fail over altogether had we known. But the policy at my current work is that any downtime longer than 30 minutes needs to be a failover. So TLDR, it was very easy, but I wasn't the person responsible for doing the physical drive swap, only for failing over SQL Server, then doing the work to utilize the new SSD, which was reassigned the drive letter that we used for data originally. 

You've written the SQL using an EXCEPT DISTINCT clause, when you would likely have much better luck adding a NOT EXISTS to your WHERE clause. Essentially you are adding rows to the destination table where they do not already exist there. 

I don't believe there is any way to do what you are asking. At the point you start using schemas you should always include the schema prefixes to your naming. It is a best practice and helps to avoid the problems that you are describing. The only reason that SQL Server allows you to reference objects without specifying schema is that it is legacy functionality. Originally SQL Server had no concept of schema, so everything was effectively in the same one. 

One easy solution is to include all possible attributes as columns on the main clothes table, and make all of the brand specific columns nullable. This solution breaks database normalization, but is very easy to implement. 

No, you must create a table variable, insert the records into that table variable, then pass the table variable to the procedure. The way that people used to do what you are describing is with pipe delimited text strings. For example, passing '1|Tyler|Smith|2|Jack|Blade|3|Someother|Guy' and then having the stored procedure unpack it. But that methodology has completely fallen out of favor compared to using table valued parameters. 

Then you can check the value saved in the table against the value for the person changing the record. And no, this does not break database normalization. As a practice it is quite common to include metadata like AddTime, AddUserID, LastChangeTime, LastChangeUserID. In fact in some environments I have worked in, it was a requirement that all tables have these columns for audit-ability. 

Although you'll probably want to alias the columns in the select statement, since you'll get duplicate columns with a straight up SELECT * 

I would stay away from LOOP joins, or specifying any specific joins. The SQL Server query optimizer is very good at picking the best join method to use, and it will pick LOOP on its own if that is the right one. Also, if there is a missing non-clustered index that could improve the performance of the query, SQL Profiler will find it and suggest it to you. One thing that I noticed missing from your query is the use of common table expressions. I haven't had a pivot yet where I haven't needed to use common table expressions to get the data ready to pivot. I would rewrite your query using common table expressions (particularly the correlated subquery) and see if that improves performance. If it doesn't another thing you could consider doing is changing the order of columns in the clustered index on the tblRespostaINT to better support that query. Finally I believe that what you are doing in the inner select can be accomplished with a windowed function, and if it can that should be faster. 

If you can compact the SQL making these types of adjustments the SQL becomes a lot more readable, will fit on one page, and you'll be much more likely to see how to fix the functional parts. As for the functional parts, one red-flag that jumps out is that you are calling DISTINCT in scenarios where you are using a GROUP BY clause. Calling DISTINCT and GROUP BY in the same select is redundant, since GROUP BY will already ensure the results are distinct. I'm not sure if removing the DISTINCTs will speed up the query, but since they are redundant it is worth a shot. Also, the generic advice of "run your script in SSMS, turn on the actual execution plan and add any suggested missing indexes" is usually helpful in these situations, you could be missing an index, and if it is an important one you could get significant performance gains without much analysis. 

Using SELECT statements instead of SETs can improve performance and readability, and may get you around the stated error. So instead of: 

I can't immediate see anything that will improve performance, but I can see dozens of things you can do to better format your SQL so that that someone on here can give you a performance improving answer. First of all, it is completely valid in SQL to write many words on the same line. In fact, SQL server will ignore all of the white space in parsing, so you can write as much or as little on a single line as you want. So the lines that currently read 

Yes, using ENABLE_BROKER in the RESTORE conflicts with NORECOVERY. If your database is in NORECOVERY you'll need to wait until it is recovered before you renable the broker. If you are having trouble, try renabling with rollback immediate: