One option I have used before is to define your own delivery wrapper (I use maildrop for actual delivery - but this should work with any) then you can just do your stuff there. Something like this in : 

Probably a silly question, but have you verified that the is also sufficiently high? 4000MB/s would be blindingly fast, that must be the cached speed not the actual disk speed. 

You cannot do that with bare UML, it just does not provide this feature. So you will have to install some kind of server inside the UML guest so it can be accessed from the host. Generally one would go for SAMBA (fairly straightforward to setup) or NFS (harder imo), but there are a multitude of options there, even some esoteric ones like sshfs which are very easy to setup and may just be the ticket for you. 

Postfix Virtual Readme is the mandatory starting point and covers what you are trying to do, then google for more (ie: distro specific instructions, etc). There are many tutorials out there, all slightly different, most use a database like mysql as backend, but if you understand the postfix concept of maps then using plain text files is no different. 

I am trying to automatically set the heap size on my ElasticSearch v5.6.3 nodes (Ubuntu 16) The machines are hosted on Azure and I want to do this so that when I scale up the machine, it automatically sets the heap size to an appropriate level without needing to manually open the /etc/elasticsearch/jvm.options file and set it and then restart the service. As far as I can work out the suggested ways to set the heap size (in ElasticSearch 5) are: 

This is an ASP.NET YSOD. I am not sure why ASP.NET is getting involved at all as it's a static .jpg file I'm requesting. I tried turning on failed request tracing and this is the specific error: 

I am using DD4T on an SDL Tridion project and am using the following configuration in the storage config in order to publish Binaries (binaries in this case being anything store in Multimedia Components) to the filesystem but keep Pages in the Content Delivery database. I am finding that as requirements change for what Binary files are needed e.g. the customer wants to offer Adobe Illustrator files for download, I am needing to add more types to the list by changing the config and restarting the deployer which is not ideal. 

On a SQL 2008R2 box we recently had a number of jobs fails for various reasons that were mostly memory related, including one stating the page file was full. The Windows 2008R2 VM had 16GB of RAM and a dedicated disk for a 6GB page file. For now we moved the page file back to the C: drive and increased its size to 8GB. The long term effects of that are yet to be seen. Our Server Admin, this morning, increased that "swap" drive to 25GB as was recommended by the GUI. What struck me as odd is that the admin also changed virtual memory to be mananged automatically across all drives. This strikes me as waste of space but I don't really understand how Windows automatically manages the page file. Here is a snapshot of the current virtual memory settings to help with the description. 

With regards to the SendRequestUsingProxy failed, that should fail. The server does not have access to Microsoft websites so it will be blocked from being able to go there. What I can't figure out is why it isnt getting the updates from the WSUS server directly. We do not use a proxy nor is one configured. On the WSUS Server side of things I see that it get a download failed status for each of the updates. So in short the communication is there but the client is trying to download the updates from externally. It is a 2k16 server and reading the logs with has not proven useful. This is the only external server I have to the network so I do not have any comparison systems to know exactly where the system is. In an attempt to testing connectivity to the server I try to browse to $URL$ which is met with page cannot be displayed on the client server. (That link works fine on the internal network) Why is my Windows Update client not honoring the WSUS path for updates and instead attempting to go externally for Microsoft? 

If advanced routing is a bit difficult for you then I would not recommend this approach. The problem is that by default each dhcp lease will claim the default route and also update the list of dns servers. Then there is the firewall issue: how to detect when the it is down, etc Buying a cheap and easy to configure router might just be better for you. 

The Xvnc session and application startup: I would place all this in a script and start it from xinet.d The tricky part is to prevent users from re-connecting to an existing session. This is a unusual requirement since that is one fundamental feature of VNC. You may be able to get away with parsing the output of the Xvnc process and killing it (with the app) when you see a disconnection event. For killing the Xvnc when the app terminates, just wait for the appication to terminate in your script and kill Xvnc if it is still running at that point. 

Our MySQL server seems to be using a lot of memory. I've tried looking for slow queries and queries with no index and have halved the peak CPU usage and Apache memory usage but the MySQL memory stays constantly at 2.2GB (~51% of available memory on the server). Here's the graph from Plesk. Running top in the SSH window shows the same figures. 

Does anyone have any ideas on why the memory usage is constant like this and not peaks and troughs with usage of the app? Here's the output of the MySQL Tuning Primer script: 

Can anyone tell me if there is a way to change the above so that the contents of all Multimedia Components for the given publication go to the filesystem? I can't seem to find this in the documentation. 

Can I access my network drives on my host machine on the VM? This is possible on Virtual PC but I can't find the setting on VMware. I am running Windows 7, VMWare workstation 7.1.3 build-324285 

I have a Windows 2008R2 print server hosting about 40 printers. For the longest time we had a Point and Print GPO that allowed the users to install the drivers for these printers without administrative interaction. Administrative Templates\Printers\Point and Print Restrictions: Disabled. This is still in place. Recently though that is no longer working. Take the "sales" printer for example. People have been connected to it for years now and in the last few days, when someone tries to print, their computers (All Windows 7) have been asking to install a print driver. Even new users that have not been attached to that printer before are being asked for admin rights to install the printer. This has affected about half of the printers on this printer server. So some printers users are able to install just fine. So when someone has the issue I hop on their machine and provide my rights so the print driver will install. I am sure I know of the catalyst that caused this but I have no idea how it directly relates. For inventory purposes, I updated the host names of the printers. To clarify I went on the web interface off all the printers and in each of their network IPv4 configurations I updated the host name from its generic Ricoh to be the same as the DNS record I made for the printer. So each printer has a share name, port name on the printer server which are both the same as the physical printers host name e.g. "sales". No changes have been made to the print server hosting these printers. I don't understand how that change would cause this. In the case of the "sales" users it is preventing them from printing. We have to allow the driver to update before they can print. That is how we knew there was an issue and was able to tie it to my inventory update. These users are not all in the same OU in AD and both have the same policies applied anyway. I am testing different GPOs as when you look up network printer driver GPOs there are more things people change than just the one I mentioned above. Any ideas why what I did is causing this issue? Perhaps I am chasing the wrong tail and something else is wrong? 

I have to say that this list here is helpful, if a bit confusing: it includes low level protocols (like NX, VNC, and now xpra) as well as high level wrappers (like neatx, freenx, and now winswitch). Also it points to some VNC implementations, but not the more recent TigerVNC fork... 

Short answer is that you cannot do this with VNC. Each TCP port will be bound to a unique session. As far as I am aware you will have the same conundrum with Xpra, VNC, NX, etc. (going via a server, like freenx, and connecting via ssh is a workaround but not an ideal one!) You may be able to workaround this by writing a simple load-balancer type of application, but this will still require one port per client. As for the rest of your question: the dimensions are specified with -screen. For just starting Xvnc with these options, something like this should work: 

I need the Owner as that is how I associate devices to people in our organization. Using PowerShell I am able to successfully log into the website and get a 200 response for a basic page. My problem comes when I try to use the same session to get to the data I want I keep getting a page with just breadcrumbs and no actual data. 

These are Windows Server 2008r2 machines in an AD environment. Some time in the past a DHCP server failed. An admin replaced it using a different host but the same address. Later on, when troubleshooting a rogue DHCP server, it was discovered that both the old and current servers were list as authorized DHCP servers. Once discovered it was de-authorized. That de-authorized our current productions DHCP server for a short period until that was reversed. Those two, with the same ip address, were listed for almost 6 months. Would that have been causing any issues or merely just an harmless entry in the authorized list? 

I have a virtual directory in my site (test environment). It is a UNC share which is also used as a public FTP. It is configured to connect as a domain admin account and "Test settings" says everything appears to be working. However when I try to connect to it I get: 

If I change the "Physical Path Logon Type" from ClearText to Network. I get the following IIS error: 

We had a major network issue where our secondary domain controller (responsible for Win2k3 boxes) died and had to be rebuilt (I beleive this is what happened, I am a developer not network admin). Anyway, I am working remotely via VPN at the moment and since this happened, I am getting an authentication box when trying to access certain areas of SDL Tridion via IE (Tridion 2009 SP1 is IE only) it seems like somewhere my credentials are not being passed correctly or the ones cached on my laptop do not match the ones the Domain Controller has. This only seems to affect Windows 2003 servers. Our IT support thinks that the only way to sort it out is to connect my laptop directly to the network. I am not planned to go to the office for a few weeks at least and this issue means I have to work with Tridion via Remote Desktop. We thought changing the password on my account might work but this didn't help. So basically my question is, is there any way I can reset my credential cache without having to reconnect to the network? Or is it IE that is causing the problem perhaps, since I can RDP to servers and use Tridion 2011 instances in other browsers fine? I am on Windows 7 using SonicWall VPN client. 

Can a Windows OS that is automatically managing paging file size for all drives effectively use a drive that is dedicated for that purpose? 

I got the same results from the MXtoolbox. So I guess I understand why my mail gateway thinks it is supposed to send mail to itself. Internic.ca is a CA Registrar. This does not make sense to me. Even less when I try to email them from my google account and I don't get a bounce back. What does this mean? I don't think there is an issue on my side but I don't understand why this means. 

I have seen this message in relation to configuration issues with PostFix. My environment is Exchange 2010 with a Barracuda Spam & Virus appliance as our mail gateway. When I looked up the MX record with nslookup I got 

From what I can tell from looking at other tutorials about setting this up that should be showing up. For me though, there should be other objects there that have been deleted over the past couple of weeks. I am running my tests as a Enterprise Admin user. Searching for "recycling bin" and "active directory" leads me to other users that have similar issues but most of them are addressed by either actually enabling the feature or being at a lower forest level. In my case both are correct. Not sure what I am doing wrong here or assuming. A fact that is quite possibly related is that I cannot see this "Deleted Objects" container from ldp.exe either as per this guide I was using for comparison. The last step to see the container being: 

According to this answer to the same question on the devkit mailing list used by upower, newer versions no longer emit that signal since this handled by systemd. The replacement in systemd-land is logind, which has a signal called PrepareForSleep: "The PrepareForShutdown() resp. PrepareForSleep() signals are sent right before (with the argument True) and after (with the argument False) the system goes down for reboot/poweroff, resp. suspend/hibernate." Here is a simple python script for watching suspend / resume events: 

You can run it before or after calling maildrop, important note: the script will receive the email via stdin, so you will have to buffer it if you intend to pass it to maildrop later (which the example above does not do). I would probably recommend using perl for this as there are more mail handling libraries there than in bare shell. Edit: If you only want to do this for a single user then this is overkill, have a look at maildrop - in particular the section about "external commands" via backticks. Define a maildroprc for this user with the rules required.