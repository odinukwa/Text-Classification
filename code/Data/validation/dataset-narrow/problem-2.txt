I'm not 100% sure I understand from the question, but it sounds like the Docker solution would be to go from having an (physical?) appliance with an OS and your app installed on it, to having an appliance with an OS and Docker on it, running a single container with your app in it. That doesn't obviate the need to update the OS in the host, and it adds a layer of complexity (and more updates to contend with, as you'll now have to keep Docker and the OS patched) with no readily-apparent benefit as far as the specific areas mentioned in the question are concerned. However, if you're talking about going from a virtual appliance to a Docker container, that could potentially smooth things out for you, but it also adds Docker as a dependency for your product; you're shutting out anyone who isn't using Docker and doesn't want to add it to their stack just to use your product. You could continue to support those that don't/won't use Docker by continuing to ship the (now "legacy") virtual appliance as before, but now you've just doubled your workload because you have two distributions to support instead of one. 

It depends a lot on what your infrastructure situation is. If you're doing auto-scaling, the health of individual instances is mostly irrelevant. The important metrics are total cost, and cost per unit of work (e.g. per request). Personally I don't like to monitor individual instance state if I can possibly avoid it - I try to focus more on broader service-level and application-level metrics: 

Some of your listed metrics like user registration over time, to me, don't belong in an infrastructure monitoring system like Zabbix. What is anyone watching Zabbing going to do about a 10% drop in registrations? Nothing. This is business reporting data that should be exposed to whoever wants it via a reporting DB, possibly rendered in a nice dashboard because pointy-haired bosses love dashboards. 

I've seen some custom settings files used in combination with TFS build, but nothing native. Does Team Foundation Server or Visual Studio Team Services have a Jenkinsfile-like, declarative method for defining a build process? 

Site Reliability Engineers (Google's term for admin/engineers with coding skills) are rare Rare skills are expensive Rare skills are... rare. 

Agile Software Development is not required to do DevOps, but I believe the argument can be made that the value proposition for DevOps is often a lot lower without Agile. DevOps is a lot of things, but automation is a central theme. The value of automation increases in direct proportion to the frequency with which Development creates new releases. Frequent deployment has a positive impact on certain types of products, specifically consumer applications. High velocity all the way through the software delivery lifecycle returns value for each iteration (anyone seen the CA ad where the zombies want new features in their apps?) Without Agile, high-frequency releases are extremely difficult, if not impossible. If the Development team is releasing software once a quarter, or twice a year, DevOps can still automate the process, but then what is the point? The investment in time, training, and resources to adopt DevOps may be partly returned in quality, however, the best value is in maintaining high velocity throughout the delivery lifecycle. One could also argue that if you're going to adopt DevOps, why wouldn't you also adopt Agile? The principles that make them both work, work well together. Practicing DevOps by itself, without Agile, could create an imbalance between Ops and Dev, in which Ops is outperforming Dev for service delivery. 

As in most cases, it's not all-or-nothing. The guidance of "one process per container" stems from the idea that containers should serve a distinct purpose. For example, a container should not be both a web application and a Redis server. There are cases where it makes sense to run multiple processes in a single container, as long as both processes support a single, modular function. 

One of the things I have the hardest time getting across to people is that the man-hours saved doing repetitive tasks is often only a small part of the value of automation. The bigger part is often hard to measure, and nearly impossible to estimate when automating something for the first time: how it changes the way you work. Talking with developers, this is a lot easier, because they know test automation (unit tests) and it's an easy comparison to make. It's things like: 

Consul is for service location; a service can use it to find another service so long as it knows the name of the service it wants to find. Consul is not for dependency tracking, and provides no insights into what services are using what other services (in fact it cannot, because service location does not have a way to indicate what service is doing the requesting, only what service is being requested). 

It does nothing. A huge part of the purpose of Chef is that converges are idempotent, meaning you can run them repeatedly with no cumulative effects. I highly recommend consulting the Chef documentation as it will provide an excellent foundation for understanding how and why Chef works. 

It really depends on your situation, but I'll go with the generic/naive approach and say do exactly what your developers do in the same situation: I pulled an hour ago, I made some changes, now I want to push my changes, but other changes have been pushed in the meantime. I commit, pull/rebase, then push. Assuming that whatever Jenkins is committing isn't tied to the commit it built from, this is the same process it should follow to get the same result. 

There are two general strategies for dealing with traffic surges: increasing capacity and reducing cost. Increasing capacity means auto-scaling, which everyone was very excited about when public clouds first became available. In its most basic sense, this will boot up more webservers for you based on load and add them to a load balancer, but since can be a pain to manage, there are more automagic solutions as well, like Elastic Beanstalk. The trouble with automated capacity expansion is that its also automated bill expansion - 10x normal traffic means 10x servers means 10x money you have to pay. That's why, while it's a useful strategy to keep in mind, I think you should always start by seeing how much you can cheat. By cheat, I mean cache, which rests on the idea that most of the time you can give users slightly out of date data and they won't notice, and that can save you tremendous amounts of time. Imagine that you have a page that you decide it's ok if it's five seconds out of date, and it gets 20 req/s. Without caching, you're running that calculation 1200 times a minute, whereas with caching it's only 12. You can see how this can make a tremendous difference. There are of course many types of caching, and a successful website will use several of them. But for your use case, there are two pretty good and easy options. The first is to make the site completely static. This assumes that you can do so, but if you can, then you just have Nginx serve up the html directly, and it can serve tons of requests with no sweat. If you need some level of dynamicity, then doing some full-page caching is a good option. Nginx has some capability to do this, but I really like Varnish because of its flexibility. Whatever option or options you go with, make sure you do load testing to validate that you've set it up properly; sometimes fixing one spot exposes a new bottleneck.