I recently found out that lots of different types of devices have a MAC address not just ethernet devices. I was thinking that it is possible for Internet protocol to run over many other types of technologies besides ethernet layer. Are there scenarios where other technologies besides ethernet are more practical or efficient or reccomended to use for communication of INTERNET protocol (like connecting through the internet) ? 

I noticed that many companies that manufacture switches including Cisco, Avaya and Extreme Networks use processors that use PowerPC architecture. However they are inferior as speed to Intel and AMD and other x86 architecture processors. Why are PPC processors STILL so popular? 

I am curious to know if in an Ethernet network the techniques used in LTE or other High Speed wireless Network that allow different devices to send data at the same time can still be used. For example would it be possible to use different frequencies for different devices on the same network and still have the devices "understand" each other? I know that when hubs were used and many computers would have access to the same wire there was a system called CSMA/CD that would prevent two devices from talking in the same time. What if the devices would communicate at the same time but at different frequencies? 

It's likely that the subnet masks are throwing you off. As long as you keep in mind that the below rules no longer apply, you should be fine. Ultimately classful addressing came down to the most significant (or "leading") bits in the address. Nothing more, nothing less. 

Once you have your remote hypervisor added, go back to the "IOS images" tab and add in IOS images. There are two important bits to remember: Use the "Linux format" directory notation (see screenshot), and make sure that the "use the hypervisor manager" checkbox is unchecked so that you can select the remote hypervisor that you just added. When adding the IOS image with the remote hypervisor, make sure that your remote hypervisor is selected. 

On most hardware/platforms, the Ethernet checksum is handled by the NIC before it's passed up to Wireshark. There's no way (or really any reason) to pass this up to higher layers because of the fact that the NIC does this in hardware, unless you've coded the hardware/driver to behave this way. Refer to the Ethernet wiki on wiki.wireshark.org for more information. 

Presumably would be what you want, and represents the amps in tenths? EDIT: Try telling Net-SNMP where to load the MIBs from when you run . Assuming you've saved those MIB files to you would do the following: 

I am interested in the terminology of the following words: baseband, broadband and bandwidth applied in tellecomunications. What is the meaning of the words, how were they formed and what do they mean in the context of modern networks? 

I know there are certain error detecting and correcting codes added at the Layer 2. However is it still possible to have errors go undetected eve with the addition of these codes? What type of networks are more prone to undetected errors? Are wireless networks more prone to undetected errors? 

What are the algorithms that are used by Fabric Connect protocol/protocols to choose the best path to forward a packet? I know that Fabric Connect is used to replace the Spanning Tree Protocol that chose the paths that are to be disabled in specific VLANs to prevent network loops. 

The code depends on the platform, programming language and many other things. However you need to have in mind some basic concepts. This type of communication will be called IP BROADCAST communication. The destination MAC address of the packets should be FF:FF:FF:FF:FF:FF (Broadcast destination MAC address). The destination IP address of the packets should depend on the specific network so this will be harder. I do not exactly know how will you make the receiving phones or wifi devices NOT to drop the packets when they will see an IP that does not belong to their network. Maybe 255.255.255.255 will work but I am not sure. Also, you should use the UDP protocol and not the TCP protocol, so you will open an UDP socket instead of a standard TCP one. In UDP the receiving device does not send anything back. It only receives. 

My non-EE-background stab at an answer is: I don't believe fiber was initially looked at as a transmission medium purely for bandwidth reasons, but more for the fact that it [light] can travel much longer distances without amplification/regeneration, and the fact that it's immune to factors that electrical transmission mediums are not immune to, like noise for example. The higher bandwidth rates are a product of engineering done at the PHY level - I'm not sure if you're familiar with WDM technology, but basically it multiplexes light at various wavelengths to increase the total capacity of a single fiber pair - this gives you a higher aggregated bandwidth, but each wavelength still has a max of 10G (disclaimer: I'm not up on optical engineering news so it's possible that you can get higher rates per wavelength). There are certainly specs for (and maybe even small deployments of) 10G over copper, typically over what's called "direct attach" cables - they're twinax copper cables with SFP's on either side. There are also QSFP's which are 40G capable, but these will typically have one QSFP on one end and break out into 4 10G cables on the other end. 

All routers within an OSPF area keep a link state database (note that this is completely separate from the main routing table), where they're aware of all other routers and their links within the area. Each router within the area builds a topology tree of the area, with shortest paths to all other links/routers with itself as the root. This last part is important. When an area grows large, the link state database (the tree or topology) that each router must maintain also grows large. This means that it can become more and more intensive for the router to process link state (topology) changes as there are now a large number of entries in the link state database. The tree grows larger and is more difficult to "keep up" with as there become more and more branches/leaves of the tree. Something else to keep in mind is that as the area (network) grows larger, there is greater potential for link state changes, and thus a greater potential for recalculations of the link state database. While the details of which are somewhat "out of scope" of this answer, the OSPF link state update process is also relevant here. Ultimately, as a single area grows larger and larger, the SPF recalculations themselves will take longer to complete, and you have more risk of those SPF recalculations happening due to various reasons - the moral of the story is your routers' CPUs will be sad. The "advantage" of OSPF areas is that they provide a means to alleviate the demands placed on the routers if they were otherwise in a single area, by way of cutting down entries within the link state database and pushing responsibilities of the link state database maintenance to area border routers for their respective areas. It allows for a way to keep the tree size manageable. Thorough thought and planning is mandatory for designing/implementing multi-area OSPF - there are a number of situations where poor design in multi-area OSPF can bite someone. Using areas doesn't necessarily increase the "speed of communication" but it can have significant performance benefits (if done properly) to the routers in your OSPF network, especially if your network is very large, because their CPU's aren't having to work as hard. 

So the question is in the title. If a switch does not support Virtual Local Area Network capabilities can it still be considered a managed switch? What other features should it have to be considered managed switch besides VLAN capabilities? 

What was the protocol usually used on top of X.25 and ATM or even Frame Relay Networks when those were commonly used? Was it also TCP/IP protocol stack or some other protocol stack? From what I know since the beginning of the Internet (ARPANET, NSFNET) TCP/IP protocol stack was widely used at least on the access layer of the networks (host machines). However I know that X.25 and ATM networks were also widely used but I am not sure at what level did those networks work. Were they also carrying TCP/IP traffic? 

I do not understand what was the purpose behind those two cable standards. Cross-over cable and Straight-through cable What was the initial purpose that they were build for and what was the necessity for them? 

I am not a native English speaker and I know what MAC addresses are and how are they used in networks. However what I do not understand is where this words came from independently. What does Media mean in terms of networking and what does media access refer to? Maybe for an English speaker it would be more easy to make this connections but for someone that is not is not that easy. 

Keep in mind that if you're monitoring input queue drops, you'll see these happen across multiple subsets of ports, because (IIRC - someone correct me if I'm wrong) these map to the port ASIC's and the buffers are shared across port groupings (ASICs). Here's a link to a Cisco doc that has some other good info: $URL$ 

means that traversal of that hop is mandatory. means that you let CSPF/MPLS TED (populated by OSPF or ISIS) handle how to get to the destination. And then when you create your LSP, you can tell it to use that specific path as primary/standby etc, ie: 

The most common way is to segregate access based on group membership. Within ACS there is/should be an "Access Policies" menu - I won't go into every detail of configuring ACS here, because that's why there's a manual, but you can essentially define your policies of which groups members can log in (and which group members have access to run which commands) there. In addition, there's also a way to define hierarchy in your actual network devices themselves (via hostnames, "Locations", etc) that can provide further criteria on network device access. ACS is a decent product if you need point-and-click AAA, but there are far cheaper ways to do the same thing if you have staff that know what they're doing (see tac_plus or FreeRADIUS). Taking some care in defining and laying out elements in your Network Resources/Network Device Groups in ACS will make your life easier when it comes to defining new access policies and allowed command sets. 

I am interested to know if the CSMA/CD protocol is still active in modern Networks and if it is active why and what is his role now? 

I know that most wireless communication devices have a MAC address. I wonder if the pool from which their MAC addresses are assigned is the same address pool from which physical Ethernet devices also take their MAC addresses from. In my opinion it should not be the same pool because it would be no problem if a Bluetooth Device and a Network Interface Card in a Personal Computer have the same MAC address? It is not possible, in my opinion, to have a collision. Even if a Bluetooth device would somehow be connected to an Ethernet LAN it would be through some sort of network adapter and the network adapter should have a MAC address of its own and that MAC address will be the one that will be used to communicate in the respective LAN. Is this right? 

I know that there are devices that now support agentless SNMP configuration. However I believe that the SNMP agent application has to run somewhere. It has to be some code running somewhere stored on some flash memory or a similar device. 

Subnetting will always be a "2n" bit-wise operation. Subnets fall on specific bit boundaries, no matter what. You can't subnet on arbitrary boundaries or addresses. Just think, "always equal proportions." Not any different than cutting a pie into equal proportions. I always use a /24 as a "base" to start from. Again, this is a 2n (ie multiply or divide by 2) operation, so you go from a single subnet, to two halves, or four quarters, or eighths, sixteenths, etc. If you split a /24 into two halves, each one of those halves will be a /25 subnet. If you split a /24 into four quarters, each one of those quarters is a /26 subnet. If you split a /24 into eighths, each eighth is a /27 subnet. The converse is true when going in the opposite direction. If you combine two /24's into a single subnet, you end up with a /23. If you combine four /24's into a single subnet, you end up with a /22. If you combine eight /24's into a single subnet, you end up with a /21. To answer your question, if you split 10.10.16.0/24 into two equal-sized subnets, you end up with: 

To be clear: packet switching does not maintain a virtual circuit. There are certain packet switching technologies, or technologies designed for packet switching that do (ie Frame Relay, MPLS, TCP), but as a whole, this isn't how packet switching works. Circuits were required in circuit-switched networks. Since circuits (used in this context) are not relevant anymore, but the technology used that relied on them (the phone system) is, we now have the need to emulate them in packet switched networks, hence "virtual circuits." In relation to Frame Relay (actually not just Frame Relay - this is relevant in ATM and X.25), SVC and PVC are two different types of virtual circuits. Virtual circuits are paths that are determined through a Frame Relay or ATM/X.25 network (although not many of these exist anymore, aside from some DSL ISPs). PVC is predetermined and configured explicitly (hence "permanent") and SVC is set up "on the fly" in a more dynamic fashion (as-needed basis). IIRC there wasn't much support for SVC's in Frame Relay and from an operational perspective PVC's were more desirable because there were less moving parts (things to troubleshoot/go wrong). This Wikipedia snippet on virtual circuits in the context of Frame Relay and ATM goes into detail on what "virtual circuit" actually means. 

I want to know what is the EtherType field of a BPDU frame in various versions and variants of the Spanning Tree Protocol. 

Here's my scenario: Redistribute ospf routes into bgp Create a route-map that matches on route-type=local and modifies the nexthop (to 4.4.4.4 for example) Assign this route-map as outbound routemap (route policy out) for a bgp neighbor. Will the routes advertised by BGP (the ones redistributed from ospf) have the nexthop changed to 4.4.4.4? What happens if this route-map was applied with the redistribute command? 

I never worked with bridges because they were no longer used when I first got internet at home, and I have some troubles understanding what exactly was the role of the switches. From my search I concluded that a bridge can be viewed as a simple switch with two ports and it does MAC switching between two SETS of MAC addresses instead of two MAC addresses. Is this way to view a bridge correct? 

For someone who does not speak English as a native language some acronyms can be really confusing and I would like to make sure that I understand how were this acronyms formed and what was the initial thinking behind them. I am not exactly sure what the term Carrier refers to in terms of computer networking. Is the copper considered carrier? What does sensing the carrier mean? 

Voila, load balancing. This also scales per interface, as adding a new port will involve adding another interface and a static route. 

Packet switching can use many paths to send data, but some network layer protocols that run on top of the packet switched network (ie MPLS, ATM, Frame Relay, etc) have a requirement that data is always sent over the same path - the path is the virtual circuit. In MPLS it's called an LSP. In Frame Relay (and ATM) it's called a PVC. 

What problem are you trying to solve? Most CDN's work off of Anycast/GeoIP to serve the requested content as close to the source of the request as possible. I'm also not sure what you mean or what you're trying to accomplish by "finding out what users are doing" - CDN's are designed to be transparent to users to provide a better user experience when browsing the web (the biggest use case for a CDN - there are obviously others). I'm having a tough time thinking of what an average user would use a CDN for for reasons that would warrant monitoring of this nature. If you did want to build a lookup table or a database, it shouldn't be that difficult, since you could query whois or an IRR to get IP/routing information (assuming the CDN operator does the Right Thing and puts their info in the IRR - most of the big ones do). I'd start with figuring out which CDN's own which blocks and then cross-referencing which IP belongs to which block, and then you could make a distinction on who owns the CDN IP that your users were hitting. It's possible, but it will get hairy very quickly and may very well be an exercise in futility (thinking if CDN's resell services to other smaller companies and solutions like CloudFront). 

I know that when hubs were used there was the possibility of collisions whenever two devices would transmit data at the same time. However on that time I believe that communication was done in Half-Duplex mode rather than Full Duplex like today. Hypothetical Question: Suppose that Full-Duplex communication would have become really popular before the invention of Ethernet Switches. Would Switches have been so necessary in that case. Would it have been possible that switches would have been delayed because they would not be needed AS MUCH as they were because the communication between Network Interface Cards would have been already Full-Duplex and collision would have been already rarer? 

Do managed switches have regular x86 CPUs? Do they have proprietary CPUs? Or other types of processors like PowerPC processors or others? Is it possible to find out what type or model of CPU is inside a particular model of managed switch for example CISCO or AVAYA switches? 

I am analyzing the block diagram of a switch and I would like to know more about the difference between PHY chip and ASIC (which I think is also called Switch controller). What is the JOB of each device and how do they communicate?