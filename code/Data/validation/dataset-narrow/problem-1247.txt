Snake In our construction, the snake's head will be chasing its tail at some small distance and will be forced to repeat the same cycle with minor modifications. We embed each edge of the constraint graph as in the figure (edges shown in red), where we indicate the position of the snake by thick lines. An edge has two sides (vertices) and the snake takes the central route at the vertex to which the edge is directed. 

Moving a snake from one position to some other is PSPACE complete. Snake is trivially in PSPACE. We give a PSPACE hardness reduction from Hearn's Nondeterministic Constraint Logic. Nondeterministic Constraint Logic Let a constraint graph be a directed graph with edges of weight $1$ and $2$, such that every vertex has incoming weight $\geq 2$. Given two constraint graphs, it is PSPACE hard to transform one into the other by reversing edges, one at a time, keeping the incoming weight of each vertex $\geq 2$. This still holds if the graph is planar and each vertex has degree $3$, and either $1$ or all $3$ of its edges have weight $2$. The former type of vertex is called And and the latter is called Or (see figure). 

The cost of the optimal path is then the minimum over all $T[m-1,n-1,min_p,max_p,min_t,max_t] + max_t - min_t + max_p - min_p$ Disclaimer: This solution is not at all optimized and you still have to fill some blanks. Neither does it give you the actual path, but you can get it with some small tweaks. 

Let $p$ be the query point, and assume the interval tree is sorted by lower endpoint and each node stores the maximum endpoint in its subtree. Perform a tree-walk and stop the recursion whenever the lower endpoint of the current node is greater than $p$, or the maximum is smaller than $p$. Now at most one downward path (of length $O(\log n)$) reports no interval, namely the path that would be taken by binary searching the lowest endpoints for $p$. Illustrated example: 

Consider the first backward edge $(u,v)$ on the requested path from $s$ to $t$, then $u$ must be reachable from $s$ in the graph $D$. Now, for each such reachable backward edge $(u,v)$, it suffices to show that there is a path from $v$ to $t$ in $G$, such that this path avoids a path (in $D$) from $s$ to $u$. The problem is in $P$ if this can be decided in polynomial time, since there are $O(E')$ candidates for the first backward edge. Update: (I'm don't think the following works, because flow from $v$ to $t$ might propagate back into the path from $s$ to $u$) 

Consider this interval tree and query point, the rightmost path visited by the pruned tree-walk is the path of orange nodes (which may not contain a reported interval if their maximum endpoints are all less than $p$). All intervals in the right subtrees have a minimum endpoint greater than $p$ so can be ignored. However, all intervals in left subtrees (red) of this path have minimum endpoint at most $p$. This means we are sure to report an interval for each (red) subtree (and subtrees thereof) whose maximum endpoint (over the entire subtree) is at least $p$. Then, whenever we report a subsequent interval, we do so within $O(\log n)$ time. Since there are $m$ such intervals, this amounts to $O((m+1)\log n)$ time. Because we prune a tree-walk we also have an upper bound of $O(n)$ time. This reduces to $O(n)\cap O((m+1)\log n)=O(\min(n,(m+1)\log n))$ time. 

Here is a variant of the SAT problem in which a satisfying assignment must have additional properties. Input: A 3-CNF formula $f$ with variables $x_{1\dots k}$. Output: For an assignment $S$ of $x_{1\dots k}$, let $\overline S$ be defined such that $x_i=true$ in $\overline S$ if and only if $x_i=false$ in $S$. Is there an assignment $S$ such that both $f(S)$ and $f(\overline S)$ hold? Is this problem still NP-hard? Examples: 

$f=(x_1\lor x_2\lor x_3)\land(x_1\lor x_2\lor \neg x_3)\land(x_1\lor \neg x_2\lor x_3)\land(x_1\lor \neg x_2\lor \neg x_3)$ This requires $x_1=true$ in $S$, but then $x_1=false$ in $\overline S$, so $f(S)$ and $f(\overline S)$ cannot simultaneously hold. $f=(x_1\lor x_2\lor x_3)\land(x_1\lor x_2\lor \neg x_3)\land(x_1\lor \neg x_2\lor x_3)$ $S=\{x_1:true,~x_2:false,~x_3:false\}$ $\overline S=\{x_1:false,~x_2:true,~x_3:true\}$ Then both $f(S)$ and $f(\overline S)$ hold. 

The target position of the snake can be derived by the same transformation. Hence, reconfiguring a snake is PSPACE complete, even on planar graphs. 

To decide whether such path exists, consider the following flow network: In the graph $G$ let every edge of $E$ have capacity $|E'|+2$, and all edges of $E'$ have capacity $1$. Now add a source and sink $S$ and $T$ with edges $(S,s)$ and $(u,T)$ of capacity $|E'|+2$, and edges $(S,v)$ and $(t,T)$ of capacity $1$. Now the maximum flow from $S$ to $T$ is $|E'|+3$ if and only if there were edge-disjoint paths from $s$ to $u$ in $D$ and $v$ to $t$ in $G$. For vertex-disjoint paths, we can treat each vertex as a pair vertices (one for incoming edges, one for outgoing edges) with a capacity $|E'|+2$ between them. 

We cannot hope to prove a general impossibility result since if one-way functions exist (and we believe they do), then in particular it follows that the statement "If $P \ne NP$ then one-way functions exist" is true. However, we can prove that certain proof techniques are too weak to prove that statement. In particular, the following paper of Akavia, Goldreich, Goldwasser and Moshkovitz proves that this statement cannot be proved by certain black-box reductions (conditioned on plausible assumptions): $URL$ 

The lines oracle is used to decrease the query complexity of the test from $d+1$ to $2$, at the expense of using a larger alphabet. If you don't mind making $d+1$ queries, then the lines oracle is indeed unnecessary. However, it is usually better to make two queries over a large alphabet than $d+1$ queries over a small alphabet. One reason is that, very roughly speaking, we have a technique, called PCP composition, which can be used to reduce the alphabet size of PCPs, but cannot be used to reduce the query complexity. Hence, it is preferable to have small query complexity and large alphabet size rather than the other way around. A second reason, which is very related to the first one, is that many hardness-of-approximation results are proved based on $2$-query PCPs, but can not be proved using based on a PCP with more queries. It should be mentioned that there are also generic techniques for reducing the query complexity at the expense of the alphabet size, so one could avoid the lines oracle and apply those techniques instead. However, using the lines oracle gives a more direct proof. 

I think the solution to your problem is not reading a probability book, but reading more papers in TCS. Most papers in TCS don't actually use very advanced probability tools. Most of them use a small collection of basic and well known probability tricks. The reason you have a hard time following them is that you are not yet familiar with this bag of tricks, and many of those papers don't bother to explain these tricks because they assume the reader knows them. Some of those tricks are not taught in most probability books, at least not in the specific form they are used in TCS papers. Another reason is that TCS papers use a slightly different terminology than the one taught in basic probability courses - e.g., in TCS papers a random variable can usually take values in $\{0,1\}^n$, while usually in probability courses random variables are defined as taking real values. So, by reading more TCS papers, you will get more familiar with the bag of common tricks and with the terminology, and and with time they will be easier to understand. That said, reading a book on probability is always a good idea. Among the books suggested above, I am only familiar with "Probability and Computing: Randomized Algorithms and Probabilistic Analysis" of Mitzenmacher and Upfal, and it is a very good read - in particular, it will help you get familiar with the some of the terminology and tricks used in TCS. 

Renyi entropy is analogous, in some sense, to $\ell_p$-norms, so let's first recall why those norms are useful. Suppose we have a vector of numbers $a \in \mathbb{R}^n$. We want to have a single number that represents, in some sense, how does the typical element of $a$ look like. One way to do so is to take the average of the numbers in $a$, which roughly corresponds to the $\ell_1$ norm: $\mathbb{E}_{1 \le i \le n}[|a_i|]$. This is often useful, but for some applications it has the following problems: First, the $\ell_1$ norm does not give us a good upper bound on the largest element of $a$, because if there is a single large element and many zeroes, the $\ell_1$ norm will be significantly smaller than the largest element. On the other hand, the $\ell_1$ norm also does not give us a good bound on how small the elements of $a$ are, for example, how many zeroes $a$ has - this problem occurs in exactly the same scenario as before. Of course, when the elements of $a$ have a lot of variance, such as in the extreme scenario as above, no single number can give solve both problems above. We have a tradeoff. For example, if we only want to know the largest element, we can use the $\ell_\infty$ norm, but then we will lose all information about the smaller elements. If we want the number of zeroes, we can look at the $\ell_0$ norm, which is just the size of the support of $a$. Now, the reason for considering $\ell_p$ norms is that they give us the whole continuous tradeoff between the two extremes. If we want more information on the large elements we take $p$ to be larger, and vice versa. Same goes for Renyi entropies: Shanon's entropy is like $\ell_1$ norm - it tells us something about the "typical" probability of an element, but nothing about the variance or the extremes. The min-entropy gives us information on the element with the largest probability, but loses all information about the rest. The support size gives the other extreme. The Renyi entropies give us a continuous tradeoff between the two extremes. For example, many times the Renyi-2 entropy is useful because it is on the one hand close to Shanon's entropy, and thus contains information on all the elements on the distribution, and on the other hand gives more information on the elements with the largest probability. In particular, it is known that bounds on the Renyi-2 entropy gives bounds on the min-entropy, see, e.g., Appendix A here: $URL$ 

Let's say I have a way to represent $N$ bits such that those bits are in a superposition of the $2^N$ possible states those bits can have and that I can do XOR and AND on those superpositional bits to be able to map $N$ superpositional input bits to $M$ superpositional output bits using any logical circuit (since XOR and AND are supported). Let's say that I am also able to include bits in those logic circuits which are not superpositional but have specific values of either 0 or 1. This way I can do a NOT by XORing a superpositional bit by 1 for example. At the end of the logic circuit when I have the superpositional result, I can then decide what values the input bits had, and get a non superpositional result out for those inputs, without evaluating the circuit again. I can interpret the result as many times as I want for the entire permutation of input bits if I want to. I'm wondering, would a superpositional logic technique like this be useful, and does anyone know if any work has been done in this area? I know this is similar to how quantum computing works, except for a few key differences like not having interference, nor having probabilities for states, but it does allow cloning of the superpositional result, unlike quantum computing which only allows one question to be asked of the result. I can think of a couple usage cases, but am wondering if there are others? 

Using a superpositional result as a way of letting an untrusted person specify how inputs should map to outputs (basically, "script" some process), without havign to sandbox the logic, worry about division by zero, infinite loops, and the like. I have yet to experiment deeply with this, but if you need to know several values of some function $f(x)$, it may be more computationally efficient to evaluate it superpositionally and then re-interpret the superpositional result several times, versus just calculating the function several times. 

I recently came across a strange concept and was wondering if this was a known / named concept in the realm of CS. The concept is that you evaluate some computation or logical circuit that takes in N number of binary inputs and gives an output. (and if doing multiple of these in parallel it could then be N inputs to M outputs) You can then change your mind after the fact about the values of the binary inputs you want the algorithm to have evaluated and essentially re-interpret the result of the original equation to get the answer for those different inputs. The benefit here would be that if there was some complex and lengthy computation, that you could do it once, and then just re-interpret the result to get the answer of that same algorithm for different inputs. It sounds strange I know, but are there any existing methods for this, or terminology at least? Example: As an example let's say you calculated a function $f(x) = x * 8$ for $x=5$ in a specific way that gave you a resulting bit string. When an operation was done on the bit string, which was a function of the inputs (say, XOR against a number which was the function of the inputs for example), that the value came out to be 40. But then, you say "OK but what would it be for $f(6)$?". Since 6 is different than 5, the XOR constant changes, but you can use that new XOR constant against the result you already got from the previous calculations, to get the new correct answer of 48. The decoding process is the same regardless of the algorithm/function being evaluated. It is a function of the inputs, and has nothing to do with the details of the function itself (so isn't iterative computation). it almost seems a little related to a karnaugh map, in that you get something boiled down to the results of the algorithm, no matter how complex the steps were to get to that result originally. 

For what it's worth, it turns out that this is how quantum computing works. When it does it's computations, it does so on cubits which can be 1, 0, or a superposition of 1 and 0. If you use superpositional bits, it means that it does the computations on all possible bit value permutations. It's only when the result is "observed" that it collapses into an actual value. Unfortunately you can't (or they don't know how) to clone the result in the unobserved state, so you can't get access to all permutations, only one of them.