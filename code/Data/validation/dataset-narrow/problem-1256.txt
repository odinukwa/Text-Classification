The almost-universal family of hash functions $$\{h_a(\vec{x}) = \sum a^i x_i \bmod p: a \in \mathbb{Z}_p\}$$ has a nice property here: $h_a(\vec{x}) + a^{|\vec{x}|}h_a(\vec{y}) = h_a(\vec{x} \circ \:\vec{y})$, where "$\circ$" denotes concatenation. If you cache at the root of each tree both its hash value and $a^{|\vec{x}|}$, you can calculate the hash of the concatenation of two trees in $O(1)$ operations on $\mathbb{Z}_p$. This is both associative and pretty fast. The collision probability of $\vec{x} \neq \vec{y}$ is $O(\min(|\vec{x}|,|\vec{y}|)/p)$. See CLRS or Dietzfelbinger et al.'s in "Polynomial Hash Functions Are Reliable". 

It doesn't. It has a polymorphic weight of 1. The polymorphic weight strips the binder before it looks at the weight. For the $\rho$ you chose: $[\![\forall \overline{\alpha} . \rho]\!] = wt(\rho) = 1 \neq 2 = wt(\forall \overline{\alpha} . \rho)$ 

I think I know how to provide the queue operations in $\Theta(1)$ time and delete and depth in $\Theta(\lg n)$ time (all expected amortized). Is it possible to do better? My proposed solution is as follows: Maintain a balanced tree with $O(1)$ operations at the ends. Nearly any finger tree will do. This tree will store the keys in queue order at its nodes. Also, annotate every non-spine node with its number of descendants. Keep a hash table mapping keys to pointers to nodes in the tree. To enqueue a key k, add k to the back of the tree. This invalidates $O(1)$ node pointers and creates $O(1)$ new node pointers, so we need only perform $O(1)$ hash table operations. Dequeue is similar. To delete a key, we look it up in the hash table and find its location in the tree, then delete it from the tree. This takes $O(\lg n)$ time in the tree, and invalidates $O(\lg n)$ slots in the hash table. We must also maintain non-spine node size annotations in the tree, but this also only takes logarithmic time. To find the depth of a key, we first annotate the spine nodes of the tree with their number of descendants. This takes $O(\lg n)$ time. We then look up the key in the hash table and find its location in the tree. We then follow parent pointers until we reach the root, summing the annotations at left siblings. Note that this is the exact depth. 

I will consider the case of non-negative weights only. As I mentioned in the comment the problem is related to the minimum k-way cut problem where the goal is to partition a given graph G into k non-trivial components to minimize the number (or weight in the weighted case) of edges crossing the partition. This is the same as maximizing the number of edges in the partitions. Since k-way cut is NP-Hard the maximization problem is also NP-Hard. However, for fixed $k$, $k$-way cut can be solved in polynomial time. In terms of approximation here is an idea to get a simple $(1-2(k-1)/n)$-approximation which is good if $k$ is small compared to $n$. To see this, simply take the partition to be the $k-1$ smallest degree vertices and the rest of the vertices. In terms of the k-way cut the # of edges cut by this partition is at most $2(k-1)/n \cdot |E|$. Thus the number of edges remaining inside the large piece is at least $(1- 2(k-1)/n) |E|$. Update: If the weights can be negative then I believe the problem is inapproximable via a reduction from k-coloring. Given a graph $G$, set each edge's weight to be $-1$. Thus we are seeking a $k$-partition to minimize the number of edges inside the parts. If $G$ is k-colorable then we can achieve 0, otherwise it will be at least 1. In terms of the negative weights the max value will be 0 if $G$ is k-colorable, otherwise no more than -1. 

In hash tables that resolve collisions by linear probing, in order to ensure $O(1)$ expected performance, it is both necessary and sufficient that the hash function be from a 5-independent family. (Sufficiency: "Linear probing with constant independence", Pagh et al., necessity: "On the k-Independence Required by Linear Probing and Minwise Independence", Pătraşcu and Thorup) It is my understanding that the fastest known 5-independent families use tabulation. Picking a function from such a family may be expensive, so I would like to minimize the number of times I do so while still preventing algorithmic complexity attacks as described in Crosby and Wallach's "Denial of Service via Algorithmic Complexity Attacks". I'm less worried about timing attacks (i.e. adversaries with stopwatches). What are the consequences of reusing the same function: 

I'm not sure I understand the question completely. Do you want an algorithm that, given two sorted sequences each containing no duplicates, outputs a new sorted sequence containing every item in either of the input sequences and also no duplicates? If so, you can represent the larger sequence as a special type of tree (a finger tree), then merge the sequences in $O(i \lg (k/i))$ merges, where $i$ is the size of the smaller sequence and $k$ is the size of the larger sequence. I think this is asymptotically optimal in the pointer machine model. 

I strongly disagree with the last paragraph. Blanket statements like that are not useful. If you look at papers in many systems areas such as networking, databases, AI and so on you will see that plenty of approximation algorithms are used in practice. There are some problems for which one desires very accurate answers; for example say an airline interesting in optimizing its fleet scheduling. In such cases people use various heuristics that take substantial computational time but get better results than a generic approximation algorithm can give. Now for some theoretical reasons for studying approximation algorithms. First, what explains the fact that knapsack is very easy in practice while graph coloring is quite hard? Both are NP-Hard and poly-time reducible to each other. Second, by studying approximation algorithms for special cases of a problem one can pin-point what classes of instances are likely to be easy or hard. For example we know that many problems admit a PTAS in planar and minor-free graphs while they are much harder in arbitrary general graphs. The idea of approximation pervades modern algorithm design. For example, people use data streaming algorithms and without the approximation lens is hard to understand/design algorithms because even simple problems cannot be solved exactly. 

I think (because of Goedel's incompleteness theorem) showing a system of inference rules is consistent necessarily appeals to an even more powerful set of inference rules. 

By functional finger trees I mean "Purely functional representations of catenable sorted lists" by Kaplan and Tarjan. 

Section 3 of Brodal et al.'s "D$^2$-Tree: A New Overlay with Deterministic Bounds" shows how to update a weight-balanced tree in $O(1)$ amortized time, but the worst-case time is still $\Omega(\lg n)$. 

The return types of each constructor in a datatype are all the same: and both return . If we allow the constructors to return different types, we have a GADT: 

I think Purely Functional Worst Case Constant Time Catenable Sorted Lists by Brodal et al. supplies what you want. 

One potential issue is when reading from a hash table, the elements should not be read in the order of the slots if all hash tables use the same hash function. This is because those elements, in that order, can cause the insert procedure on a smaller hash table with the same hash function to go quadratic, assuming that the max fill factor is over $1/2$. See: 

One should be able to get a simple constant factor approximation as follows. Partition the nodes of the graph randomly into two sets $V_1$ and $V_2$ - each node decides with probability half to go to $V_1$ or not. Ignore all the edges with both end points in the same side. We are left with a bipartite graph. Now solve a bipartite matching problem where the nodes in $V_1$ have capacity $1$ and the nodes on the right have no capacity limit. This gives a collection of stars. Analysis is left as an easy exercise :). 

One standard view of dynamic programming is the following. Start with a recursive algorithm and then memoize it. By memoization we mean that solutions for intermediate problem instances/sub-problems are stored and not recomputed. If one can argue that the number of sub problems generated in the recursive algorithm for an instance of size $n$ is polynomial in $n$ then memoization leads to a polynomial time algorithm. A recursive algorithm for a problem instance $I$ generates, naturally, an associated DAG $G(I)$ whose nodes are the sub-problem instances and the arcs are the dependencies generated by the recursive algorithm on $I$. It is possible to interpret several different computational problems as solving some kind of shortest path problem on this DAG $G(I)$ (not all though). In these cases it is possible to take out the scaffolding of the recursive algorithm and memoization and directly use a shortest path algorithm in an associated graph. This should not lead one to conclude that all of dynamic programming can be reduced to shortest path computation. 

Update: see below for an update on the incorrectness of this join operation Here is a very rough sketch of a possible solution: I think I may have a solution to this problem using a type of randomly-balanced B+-tree. Like treaps, these trees have a unique representation. Unlike treaps, they store some keys multiple times. It might be possible to fix that using a trick from Bent et al's "Biased Search Trees" of storing each key only in the highest (that is, closest-to-the-root) level in which it appears) A tree for an ordered set of unique values is created by first associating each value with a stream of bits, similar to the way each value in a treap is associated with a priority. Each node in the tree contains both a key and a bit stream. Non-leaf nodes contain, in addition, a natural number indicating the height of the tree rooted at that node. Internal nodes may have any non-zero number of children. Like B+-trees, every non-self-intersecting path from the root to a leaf is the same length. Every internal node $v$ contains (like in B+-trees) the largest key $k$ of its descendant leaves. Each one also contains a natural number $i$ indicating the height of the tree rooted at $v$, and the stream of bits associated with $k$ from the $i+1$th bit onward. If every key in the tree rooted at $v$ has the same first bit in its bit stream, every child of $v$ is a leaf and $i$ is $1$. Otherwise, the children of $v$ are internal nodes all of which have the same $i$th bit in the bit stream associated with their key. To make a tree from a sorted list of keys with associated bit streams, first collect the keys into contiguous groups based on the first bit in their streams. For each of these groups, create a parent with the key and bit stream of the largest key in the group, but eliding the first bit of the stream. Now do the same grouping procedure on the new parents to create grandparents. Continue until only one node remains; this is the root of the tree. The following list of keys and (beginning of) bit streams is represented by the tree below it. In the bit stream prefixes, a '.' means any bit. That is, any bit stream for the key A with a 0 in the first place with produce the same tree as any other, assuming no other key's bit stream is diffferent. 

The relaxation of Calinescu, Karloff and Rabani for the undirected Multiway Cut problem is one my favorites. Had a big influence on subsequent work. $URL$ 

Fan Chung and Linyuan Lu. Concentration inequalities and martingale inequalities: a survey available at $URL$ or at Fan Chung Graham's web page. 

I am interested in understanding the structure of the class of graphs $G$ such that there is no vertex induced subgraph on four vertices that is a perfect matching. Stated differently for any four vertices $a,b,c,d$ in $G$ if $ab$ and $cd$ are edges then the graph should have at least one more edge on the four vertices. Has this class been studied previously? Any references or insights would be appreciated. We understand this class when restricted to bipartite graphs but the general case seems more tricky. 

There is a nice book by Gartner and Matousek on SDPs and their applications to approximation algorithms. It covers a lot with the added benefit of giving a good introduction to the theory of semi-definite programming. See $URL$ 

An algorithm with a run-time of $n^{poly(k)}$ should be easy where $n = |V|$ and $k = |T|$. Think of an optimum solution which is edge-minimal. It will be a tree and one can see that the number of nodes with degree $\ge 3$ in this tree will be $O(k)$. We can guess those nodes $A$. Once we guess those nodes we create a graph on $A \cup T$ with edge-lengths between two nodes $u,v$ equal to the shortest node-weighted path between $u$ and $v$ in $G$ (we do not include the end points in this calculation). Then find a min-cost spanning tree in the resulting graph. 

The shared prefix pairs at A, C, and E all remain valid. The left shared prefix of B and the right shared prefix of D also remain valid. The right shared prefix at B is the left shared prefix at D from before the rotation. The left shared prefix of D is the minimum of the left shared prefixes of D and B from before the rotation. The right rotation and deletion cases are the same, mutatis mutandis. 

The cleanest introduction to this I have seen is in Tim Griffin's "A formulae-as-types notion of control". 

Pat Morin demonstrates a structure with $O(\sqrt{n})$ extra words of space and $O(\log^2 n)$ insert, delete, and predecessor search in his notes for his advanced data structures class. 

There are a number of purely functional deques that support $O(1)$ operations at each end. None that I know of are "uniquely represented" - deques with the same number of items can have different shapes. In the imperative case, doubly-linked lists are uniquely represented deques with constant-time operations. Hoogerwoord, in "A Logarithmic Implementation of Flexible Arrays", showed that Braun trees have logarithmic deque operations, and they are purely functional and uniquely represented. Is there a deterministic uniquely represented purely functional deque with $o(\log n)$ operations?