I was having problems with mine at 512MB of RAM until I switched to FastCGI. That made the performance improve a lot. I had a 30MB free (not counting cache, of course) until then, and I ended up with over 100MB free. Your mileage may vary, of course, depending on the traffic of your site. And once the traffic starts to turn up, you can switch to nginx. To buy myself some comfort, I upped to 1GB. BTW, I'm hosted at prgmr.com, and I haven't seen anyone touch their prices yet. 

A filesystem must be cluster aware, meaning that it has to know to look for the lock manager (or the nodes need to know to look at the quorum disk). You can't just strap on an DLM and have it work. That would be neat, though. Sorry! 

Still, all but 205 MB of memory are used in that example, even if it is by cache. As it was explained to me (or at least, as I understood it), if a program suddenly requests > 205 MB of memory, that chunk comes out of swap, so that the process can have the memory chunk, then it comes out of swap once it gets a chance to free memory being used by cache. As pfo mentioned, the underlying mechanisms are complex, and far beyond my understanding. I asked Ted Ts'o nearly this identical question at a class he was teaching at LISA, and I've given you essentially the same answer he gave me, or at least as I understood it. Incidentally, I suspect that the memory "used" in swap isn't actually used by anything at this point, but that it just hasn't been freed yet. 

It's a tradeoff. I've heard that big log files are more efficient, but we don't use them because A) the bigger the loss of a file, the bigger the loss of your data and B) our bandwidth is crap This leads to us spooling files for a few hours then transferring and running updates on the various standby machines. We're actually using Oracle8i still, and because the database was designed a long time ago by someone who wasn't a DBA, I still have to manually create new datafiles and control files. /sigh 

By default, CentOS is pretty restrictive in its package selection and slow in the updates to new packages because it literally is a repackage of RHEL, and RHEL is slow and steady for reliability sake. That being said, you have the ability to add other repositories which feature a wider selection and newer packages. Check this link for more possibilities: $URL$ I myself have used EPEL to a decent amount of success. 

There is always the possibility that your admin has installed the equivalent of malware to watch you, but that's pretty draconian for most places unless someone has given them a reason. Generally speaking, when it comes to work PCs, the employer has a right to do whatever they want, because it's their hardware, their software, and their time. Your admin should be acting as the enforcement arm of management, so if he's monitoring your computer, your issue isn't with him, it's with the corporate decisionmakers who decided that it was necessary. Also, if your admin is monitoring you and you disable it, I can promise that he or she will be displeased and it won't earn you any brownie points. It'll probably just make your life more difficult. 

It's possible, but it's not advised, especially when there are better ways. First, switches don't care about your IP addresses, they care about your MAC addresses. They're "layer 2" devices. IP addresses are layer 3, so they're pretty much irrelevant to the switching side of things. To make sure that I've got your infrastructure correct, you have servers A, B, C, and D. Each one of them has 2 NICs. You want to take NIC#1 on each server, and configure them with external, internet facing IP addresses, then take NIC#2 on each server, and configure them with private IPs? I have to ask why, at this point. If it's for dedicated bandwidth, you would be better served to bond NIC#1 and NIC#2 into one logical interface, which can double the bandwidth. If it's for security, then you'll have to give some more information, because there's no added security from using private IPs on a switch with public network connections. You aren't going to be broadcasting anything to the internet*, but at the same time, any network broadcasts from the network cards on the private IP block (things like ARP/RARP requests and the like) will get sent to your upstream router. It won't forward them or respond, but it certainly doesn't do anything for you. (* - probably not, anyway) Now, if you're still security conscious, why not use VLANs on the switch to segregate the external network from the internal network? The VLANs will create two logical switches*, which will prevent the leaking of your layer 2 broadcast info to the router, and in general, segregation of "private" networks into distinct logical layer 2 networks is preferable. (* - I'm simplifying, but in essence, this is what it does) 

If you can physically touch the drives, you should be able to determine which one is clicking, since the click is (I believe?) caused by the motor moving the head to the end of the platter repeatedly. 

Archive all of the rpm files using rsync (or manually select which RPMs you want, and put them in the directory) Run the 'createrepo' command on the directory Make the directory accessible via a web server 

My thoughts are that performance talks and bullspit walks. Since you're discussing recreating the array, why not do it both ways and run test load on it, then graph the results? edit If, as you say in your comment, that real life shows that performance doesn't depend on the underlying RAID level (and you actually do have the numbers to back it up), then I would recommend going with the level that will continue to give you the greatest performance as you scale, which is almost always going to be RAID-10. If you're not going to expand, then it literally doesn't matter, since you apparently get the same performance from either option, and you're painting the bikeshed. 

It depends on what's wrong. In some cases, the part that's "bad" can be replaced (in a clean room), or the platters can be moved from one drive to another (again, in a clean room). Worst case scenario, I have heard of microscopic evaluation of the platter surface, though I can't imagine what that would cost. 

If it turns on, I would recommend booting it up, then attaching an external drive to it and pull the data off like that. A DVD burner for 100GB is not going to be a day for you. 

It doesn't look like Team Foundation Server 2008 supports application-level load balancing. Apparently 2010 does, though? 

I've found that performance really lags on the cheaper NAS boxes. I'm not sure what the problem was, but I had a few lower-end SNAP! appliances and they were dogs. If you're building the NAS, then you have the opportunity to build speed into the devices. Since you're probably not wanting to use expensive disks (ie faster w/ more cache), you should probably make up for it with spindles. 8 1TB drives in a RAID10 will give you plenty of speed and reliability with 4TB usable. RAID6 on the other hand would give you 6TB usable but you'd have to compute two checksums for every write. 

The id_dsa (or rsa) is your private key. The .pub is your public key. It's a text file, so you can cat it. 

You probably need to call Dell and see if this is a supported system config. They've been getting much more stringent about what goes in their servers. If it's unsupported, they'll let you know, and if it's supported, they'll tell you how to fix it. They might even tell you how to turn off that warning even if the card itself is unsupported. 

It looks like your best bet is to use VBScript to do the setup. Here's a short example: $URL$ Microsoft also has a guide available for programmatically administering IIS 6: $URL$ 

In my infrastructure, we've got a staging environment called preview where the data goes before it hits production. The application code in the staging level is exactly the same as the production code, so any data problems will show themselves there. We've also got a test environment for new code, which operates on old, known-good data (which we can mangle when needed to test how the code fails). Generally speaking, this segregation allows us to have a pretty solid production environment, and we don't see too many unexpected things crop up. I should note that I don't deal too much with that, since I'm not a programmer. I just sync the databases and refresh the files when they ask me to ;-) 

How remote are the offices? If they're in the same local area, talk to some telcos and see if you can get metro ethernet between the lines. That would be the most ideal, considering the cost of bandwidth is typically so much lower for such a higher speed. If your sites aren't ultra-local, and MPLS is in your budget, you should absolutely consider it. MPLS by itself is very secure. If you feel like security is incredibly warranted, IPSEC over MPLS is as completely secure as you're going to get for a piece of bandwidth. When we had multiple offices, I was heavily considering MPLS as our option. Since we only have two real locations, the cost wasn't worth it for us, and I ended up using IPSEC tunnels over internet facing ethernet lines. 

There isn't one end-all be-all of mail configuration testing, because no one aside from yourself knows how you want your server configured. There are tests, as other people have suggested, which allow you to test very specific conditions that frequently cause problems for mail administrators. Another such is the abuse.net test for open relays: $URL$ In the end, verify that you can send mail to your domain by using an external mail server (such as gmail or yahoo), that you can receive the mail that you sent, and that when you reply to the mail, your external account receives the reply. After you have basic functionality down, test again as you change the configuration. 

When you run top, and sort by memory, is httpd at the top (several times)? If your site isn't popular, you should be able to tweak the settings in httpd.conf to limit the number of StartServers, MaxSpareServers, MaxClients, ServerLimit, etc. Also, you may want to consider something like nginx ($URL$ to act as a proxy to apache, which would almost certainly lower your memory footprint. 

I just recently got a fiber line installed into my office, and with the exception of an odd bit of trouble that we're having, things are working good, and the network response is really amazing. The problem that we're having is that every once in a while, my router will flake out and drop packets. It's not the line, and it's not the switch. It's the router itself, and I've switched out the hardware, and both pieces do it. The piece of equipment that I'm using is a Juniper Netscreen SSG5. Here are the symptoms: I do a pingflood to the "internal" interface, with 

Here is a gigabit dual ethernet NIC with the ability to be programmatically assigned to fail-open or fail-closed on power loss. 

@Complex on twitter found it for me. The issue is that there's a maximum field length setting that was getting in the way. As found at $URL$ you go to Console, then Configuration/Settings, and alter Maximum Field Length to a more reasonable number, and it starts working shortly. 

Without knowing whatsoever about the situation, I've got to say, if YOU don't know the underlying impact, you definitely shouldn't do this during general hours. This is the kind of thing labs were invented for. Also maintenance windows. 

I'm pretty sure that this setting is domain-wide, which means that you would need to establish another tree in your active directory forest. Probably not worth it unless there are other reasons to segregate those user accounts. 

If not, check /var/log/samba/* and /var/log/messages to see why it didn't actually start Step 3 - Can we connect to it remotely 

You can see the benefit of VMs to yourself, but they don't see the benefits to them, so what you've got to do is translate the your benefits into their benefits. Lower power consumption for you equals lower recurring costs for them. Larger uptime for you equals more reliability for them. Easier administration for you equals more time for you to work on other projects. It's a pretty easy equation to understand after you simplify all the terms and break it down into things management can understand. 

I've got an older Apple XRAID, and in order to administer it, I've had to rely on the Apple XRAID software utility that is native only on OSX. My only Mac laptop is going to be heading the way of the dinosaur soon, and I'd love to have another way to address this piece of hardware. Has anyone found, used, or even heard of a software solution for admining these things without a Mac? 

Here's a quick rundown, but a screen tutorial will get you farther: 'screen' invokes it, and you use ctrl-a then d to disconnect (and leave it running) 'screen -ls' lists open sessions 'screen -r' connects to a disconnected session. 'screen -x' connects to an active session (if you want to run two terminals at once) Here's a screen tutorial: $URL$ 

Can I interest you in reverse DNS? Essentially, the client is doing the reverse DNS on the server, or vice versa. I propose a test: Disable DNS lookups on the server by editing /etc/ssh/sshd_config and making sure "UseDNS" is set to "no". Run "service ssh reload" (or whatever causes your ssh daemon to reread the config), then try again. Incidentially, it doesn't happeen to finally prompt you after a long period of time, does it? Another thing you might check is looking at the contents of /etc/hosts on the server to make sure that nothing is wrong there. 

A RAID-6 array has two parities. That means for every bit written there are (essentially) two computations made, one for each parity. Using RAID-6 on an array with less than 5 isn't recommended (and on less than 4 is counterproductive!). Essentially you're doing way too much work for no gain whatsoever. 

Windows has an entire suite of tools to remotely administer Windows machines. In addition to the remote administration tools in the admin toolpack, you can use something like psexec or even powershell to issue remote commands. As for automation, I'd recommend powershell. All of my Windows-admin buddies swear by it. 

We use a program called Atomic Mail Sender ( $URL$ ), but there's an entire suite of software on that site to do what you're looking for. Note that it will be difficult to find another mail server that lets you send 50,000 emails out of it, except for the spam houses, who will charge you. It's possible to send directly from the desktop running the Atomic Mail program, but in order for it to work right, you should go to efforts to configure DNS such that the desktop looks like your mail server. This includes things like SPF records if you have them, reverse DNS, and so on.