What you want is to influence routing decisions in the kernel. This is typically called policy routing: in your case, I assume you want a policy that acts on the source address to route back to that interface. Net traffic that arrives on one interface should exit on that interface and packets on a certain interface should use rules specified for that interface rather than the default table. The command you want is . Having said that, there is not enough information in OP to say for certain that policy routing is needed. 

Under the assumption that we are talking whole machines (whether physical or virtual) for the production environment (rather than, say Google App Engine), my best practice is thus: Take the extra effort to create proper packaging for your software. For example with Java development targeted for RedHat, use/write Maven tools that create RPMs. For your database, for all changes and include them in the RPM and either have them apply automatically or create a small tool that applies them for you. Essentially, you are done when a server can be installed/upgraded with a single command. Yes, this means including config files tailored to that system. This is a bit of extra work, but it is so worth it in the long run. Once you have properly packaged your software, the question of dev/staging/demo servers isn't so important anymore, because you can always build another. And with proper installers, you know exactly what changes you need to do to a machine to get it working properly. Don't bother with migrating virtual machines around: you would need to change lots of configuration parameters on migration anyway. If you tell us a bit more of OS/dev tools concerned, I may have some more specific tips for you. EDIT: PHP/MySQL. Assuming this is LAMP. Create a deb/rpm with your PHP code that depends on the various Apache, mysql, mod_ssl, php packages that you need. Look at other PHP applications packages on that platform for inspiration. Typically, you don't want to have a package dependency on a database, because the db may be on a different box. Your package should prolly contain two SQL scripts (that comes from your source repo): one that initializes a new database, including grant statements, loading stored procedures, creating indices, etc and one that loads some demo data that can be used on test systems to get up and running quickly. Also, successive versions of your packages may contain patch scripts that upgrade the database. Most modern Linux distributions frown on overwriting files from other packages, so try to avoid it if possible. EDIT2: How to build a Debian package from just a directory tree. You need a directory tree that looks like the installation you want to make (lets say in a dir called build), and you need some control files: controlfiles/control: 

Congratulations on giving us no information whatsoever, such as your server operating systems, your client operating systems, your domain functional level or the amount of time that this device will be off site. The good news is that, yes, it's possible. By default the user device will cache the credentials of recently logged on users and allow them to log on even when there is no connection to the domain controller. Start reading here to discover if this is a solution for your cirumstances: $URL$ 

The redirect from the root to the Citrix site is performed on Web Interface server, rather than through the Secure Gateway. You should check the Web Interface directly, bypassing CSG. 

The advantage here being that mail.company.com will still go through your normal forwarding routes meaning you don't have to mess. The disadvantage is that you need to be very aware of what you do and don't have zones for. Troubleshooting may be more difficult and it will get terribly messy if you want subd2.company.com and subd3.company.com in the future. The best solution will really depend on what is currently doing and what you want it do. You also need to very clear that running the zone locally means you now have two possible places where DNS entries are. You should ensure you clearly understand your forwarders and so on. 

In order to make this work, I had to modify /etc/fstab on nic-standalone so that it pointed to the correct root file system and swap partition (/dev/sda, /dev/sdb). Kernel and initrd above are standard CentOS 6 stuff. (There may be various performance parameters/tweaks desired as well, I just haven't gotten that far yet.) 

Answers from dynasy and Jared are both pointing in the right direction, but leaves a bit unexplained. The short answer to my question is that virt-install is perfectly capable of supplying a partition to the booting system. In fact, for block devices, it doesn't care and just pass it on to qemu that sets it up like a block device. Then the kernel uses /dev/sda (rather than /dev/sda1) as root file system. The command line for achieving this looks like this: 

It is the tradition of serverfault not to unduly question the preconditions, but I have to ask: is it not possible for you to either 1) mount the remote filesystem via SMB, or 2) use fuse/sshfs instead? (I assume here that the sending machine is a Linux box as you are using bash and ssh.) To actually answer your question, I think your problem is simple. Consider: 

I've found the answer and it was quite funny - ARP table overflow. The traffic in the test environment was gerenated from many IPs that resided in directly-connected networks, so the system had to use ARP first to figure out MACs, and the default hard limit of ARP table in Linux is just 1024 entries, which gives number of connections between networks connected to 2 different interfaces close to 512. When I increased net.ipv4.neigh.gc_thresh1 and also .gc_thresh2 and .gc_thresh3, the problem was solved. 

Just use "DEVICE partitions", it will try all devices listed in /proc/partitions, and you wouldn't have to worry at all what the device names are. UUID of an array is stored on each device belonging to it, so each array will be assembled correctly even if you have several of them. 

As you all probably know, ipv4 route cache has been removed in 3.6 Linux kernel series, which had serious impact on multipath routing. IPv4 routing code (unlike IPv6 one) selects next hop in a round-robin fashion, so packets from given source IP to given destination IP don't always go via the same next hop. Before 3.6 the routing cache was correcting that situation, as next hop, once selected, was staying in the cache, and all further packets from the same source to the same destination were going through that next hop. Now next hop is re-selected for each packet, which leads to strange things: with 2 equal cost default routes in the routing table, each pointing to one internet provider, I can't even establish TCP connection, because initial SYN and final ACK go via different routes, and because of NAT on each path they arrive to destination as packets from different sources. Is there any relatively easy way to restore normal behaviour of multipath routing, so that next hop is selected per-flow rather than per-packet? Are there patches around to make IPv4 next hop selection hash-based, like it is for IPv6? Or how do you all deal with it? 

MySQL failed to start. The reason this occurred should be logged in /var/log/mysql.log (or is it still /var/lib/mysql/.err?). Can you paste the output from there, and maybe we can figure out what went wrong. 

Your immediate problem is that you declare a default gateway on both entries. I suppose that this will result in whichever comes up last to "win" and be default gateway. However, if you remove the one on eth1, you will have the problem that the box will reply on eth0, even if the packet arrived on eth1. In order to tell the box to reply on a particular interface, you need to do what is commonly referred to as source routing. I have the following set of commands on a box: 

The short answer to your question would be NFS, I think. Alternatively, for a more fancy setup, you could have the two servers share a DRBD volume. However, I wonder what's wrong with HTTP as a file access protocol? Can't server B have it's own web server and serve its files for itself? Split your webapp so that the part that processes the files resides on server B? Update: I didn't read OP carefully enough. If "geographically separate locations" means long latency, both drbd and nfs may be suboptimal, depending on the exact use case. If there is more than a 50 ms latency between the two locations, I would try to use different web servers that both talk directly to the end user, if possible. 

You can setup transparent proxy on your client machine that will have your company's proxy as a parent and add authentication information when forwarding requests to the parent. You will need to install a proxy server that supports transparent proxying, I'd recommend squid; and you will need a firewall that will redirect your traffic to a proxy server, many windows firewalls can do that. Google for "squid transparent proxy", there are a lot of manuals. 

I can't think of any tool that would do that out of the box. This is quite rare scenario, as you can't create correct two-way NAT mapping if you only change port. Do you really need just one-way traffic ? However you can always write your own netfilter module (it's not that difficult) and alter packet headers in any way you want. 

Well, things are a bit more complex. Modern hard drives don't just detect errors, they have some spare sectors and smart controllers that try to relocate bad sectors. That is, when you try to read some logical sector and it doesn't read at first time, the controller tries to read it several times, and sometimes it can read it after some retries; then it writes the data back to the spare sector, remaps logical sector to the new one and marks old sector as bad, and finally gives you your data. All those processes are completely transparent to the reader, you wouldn't notice any error. However this will normally be reflected in S.M.A.R.T statistics, and if this happens more and more often, you can see that the drive is going to fail before it actually fails. That's why it's really important to use SMART monitoring tools on your system. When a sector doesn't read at all, or the controler runs out of spare sectors, read error will be returned by the drive. Error detection is now pretty bulletproof, it uses some kind of CRC for sector data. When read error is returned, mdadm will see it, mark the drive as unusable and switch an array into degraded mode. 

Yes, this is a bit paranoid, but is typically a faster route to revovery than trying to study the problem and be smart. It seems from OP that this is a mail server. If so, you have an additional problem in that the box may be sending out contaminated mail. I would not release my mail queues until I was absolutely sure it was not being rewritten enroute. 

Since smtpd is chrooted, saslauthd_path is relative to /var/spool/postfix. I use bind mounting to get /var/run/saslauthd into private. 

The variants of 2 are best for complex machines (that sort of mimics traditional physical machines with multiple roles). However, one of the big advantages with virtual machines is that you no longer need to have multi-role machines. Instead, you can have several very simple machines with exactly one role each. If you go to the extreme, the answer is none of the above. If you want to be extreme, you create a master image and then each VM gets a read/write snapshot of that master as its root partition. With this design, plus a little creative scripting using a dhcp server and cfengine/puppet, you can create and start a virtual machine in under 15 seconds. 

Given that you've tried two techniques without success, I'd say this is almost certainly going to come down to a permissions issue. Go back to using Group Policy Preferences - you can then review the event log to see why they're failing. Edit: Please don't use the Default Domain Policy for this stuff, if you break it you'll be stuffed. Create a new GPO and link it as appropriate. 

Your support company are dead on right - it's all fine now, but the day that it starts randomly blue screening will be the start of a mass panic, with no one to turn to. Do the upgrade now, while you have time to plan it and implement it properly - not when you've just found out that you can't buy a replacement motherboard. Supportability is important, almost more important than the quality of the software itself. 

If you create backup using tar, it will be enough to copy all your files, however the problem will arise when you try to extract that archive into a live system. Some binaries or libraries may not update because they will be in use by the system, and you will end up in a mess. You should get some support from your hosting provider (for the new VPS). Usually they have some features for that case, for instance my hosting provider allows me to switch my VPS to "repair mode", in that mode new VPS is created with basic Linux software, and my VPS's disk is mounted there, allowing me to change it however I want and then leave repair mode. Or maybe you can just send your tar archive to the support team, asking them to extract it to your VPS when it's offline. 

You can achieve almost what you want my delegating permissions in Active Directory, however, I think the lowest you can go is by Organisational Unit. I do not know of a way to limit influence by security groups. Source: $URL$ 

The vmx file simply holds the virtual machine configuration. Simply create a new VM using the vSphere Client and associate your current vmdk files with it. 

Yes, that's a classic SQL injection attack. Your only real long term defence is to secure the application, though you can ban IPs as required and there are various tools out there which will attempt to automate this. Ultimately, unless it becomes a DOS attack, they should be relatively harmless if your site is injection proof. That side of things is more StackOverflow, though. 

Why don't you just rebuild your server? Apart from as an "Emergency Get Out" I'd never trust restore modes etc on a Server Operating System. 

You need 32-bit version of libraries to run 32-bit applications on 64-bit system. Unfortunately Redhat doesn't have package like ia32-libs which would install most of them, it is supposed that you should install all 32-bit applications with yum and it will install the appropriate libraries for you. If your application is third-party, try installing 32-bit version of each library it needs, they usually have .i586 suffix, so you execute something like "yum install libusb.i586". 

I suppose you know how hashing generally works: it calculates some function out of the data (IP, pair of IPs, etc) and uses value of that function as an index in the table to locate the structures associated with that data. Each cell in the table (which corresponds to one possible value of hash function) is usually called hash bucket. Unfortunately different sets of data may produce the same value of hash function, and will be associated with the same hash bucket. That's why hash bucket may contain several hash entries, which are usually stored as a linked list. Thus, when a lookup is done, hash function is calculated first and a hash bucket is selected, and if it contains several hash entries, they are analyzed one by one to find the approriate hash entry. Thus hashlimit-htable-size limits the number of hash buckets (size of hash table itself), and hashlimit-htable-max limits the number of all hash entries (stored in all hash buckets).