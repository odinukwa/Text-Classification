Some scenarios for M:M relationships in a data warehouse model Most OLAP servers and ROLAP systems have a means to deal with M:M data structures now, but there are some caveats about this that you will need to pay attention to. If you do implement M:M relationships you will need to keep an eye on your reporting layer and what tools you want to support. Scenario 1: M:M dimension onto a fact table An example of this might be multiple drivers on a motor policy. If you add or remove a driver, the policy adjustment transaction may have a relationship to a list of drivers that changes with the adjustment. Option 1 - M:M driver-fact bridge table This will have quite a large volume of data, as it has drivers x transactions rows for a given policy. SSAS can consume this data structure directly, but it is slower to query through a ROLAP tool. If your M:M relationship is based on entities that are specific to the fact row (e.g. drivers on a car) this might be appropriate for a ROLAP tool as well, providing your ROLAP tool supports M:M relationships (e.g. using contexts in Business Objects). Option 2 - Dummy 'combinations' dimension table If you are mapping a list of common codes to a fact table (i.e. the linked entities are not peculiar to the fact row) then you can take another approach that will reduce the data volumes. An example of this type of scenario is ICD codes on an inpatient visit. Each inpatient visit will have one or more ICD diagnoses and/or procedures listed against it. The ICD codes are global. In this case, you can make up a distinct list of the combinations of codes on each case. Make a dimension table with one row for each distinct combination, and have a link table between the combinations and the reference tables for the ICD codes themselves. The fact table can have a dimension key to the 'combinations' dimension, and the dimension row has a list of references to actual ICD codes. Most ROLAP tools can consume this data structure. If your tool will only work with an actual M:M relationship then you can create a view that emulates the M:M relationship between the fact and the coding reference table. This would be the preferred approach with SSAS. Advantages of option 1: - Appropriately indexed, queries based on selecting fact table rows with a certain relationship through the M:M table can be reasonably efficient. 

Audit information should include at least: user who created/changed the record, date/time of the creation/change, nature of change (insert, update, delete). You may want to use logical deletion (i.e. a 'Deleted' flag) if you have the option of doing so. Otherwise you need to capture the user and date/time from the session and put these on the deletion record, probably along with the last state of the record. If this is not available from the connection (often the case on N-tier apps where the user is not being impersonated at the database level) then you need to find some other way to get it. 

As with your other questions, this one could be more specific. However, I will assume that you want to document mappings arising from analysis work. An ETL tool will actually load the data, and the mappings tend to take the form of wiring loom diagrams that show the mappings graphically. If you have relatively simple transformations the internal self-documenting capabilities of the ETL tool may suffice for this. However, this tends to break down on more complex transformations or in situations where not all of the logic is done within the tool. The more complex the transformation the more likely it is to happen. In many cases a decisions is made to do all the transformation work in stored procedure code and the ETL tooling is largely used for extraction, logging and load control tasks. If you are mapping for documentation or specification purposes, people often use spreadsheets if no other tool is available. However, this really carries an assumption of an isomorphic 1:1 mapping from source to target and does a poor job of capturing and documenting complex logic. Some CASE tools have meta models for mapping available. I've done this with Sparx Enterprise Architect, although we had to build a custom metamodel for this. Some tools have this feature out of the box. Finally (and this is my preferred approach) a functional specification for the ETL can be written up as a technical document. This can describe mappings and detailed specifications for the logic. In the case of a system with complex logic in the ETL a spec document is by far the best way to do this. 

Any way you look at it, you're up for a table scan. The export would probably be more expensive than the distinct processing on the query, so at a guess the query would be faster. 

In principle, 2-3TB should be possible to do without resorting to a shared-nothing architecture, but Vanilla PostgreSQL still does not have good parallel query functionality. All you will achieve by replication is the ability to apportion queries out to individual nodes. I don't believe that PostgreSQL supports federated queries out of the box and I would be very surprised to find that Tableau has direct support for sharding at the client. I'd guess that PostgreSQL won't perform very well on a data set this big. There is work in progress on a parallel query facility for PostgreSQL, but AFAIK it's not included in 9.1. My gut instinct is that some release in the next few years will include this feature but it's not quite there yet. I don't see evidence of much effort being put into a federated query facility. Another option: SQL Server Unless you're married to Postgres you might find that SQL Server provides a cost-effective option to Greenplum for a 2-3TB data set. It is licenced by the socket rather than the core, so a loaded 2-4 socket Xeon or Opteron box gives good bang for buck as a platform. For a smaller user base I believe you can still licence Enterprise Edition by CAL as well. A couple of 24/25 disk arrays on a high-end RAID controller will do sequential reads fast enough to saturate a PCIe-x8 slot (2GB/sec). A simple table scan query with SQL Server will handle data at this rate without using much CPU (obviously depending on the actual computation), so you have some headroom to add controllers and arrays if you want faster I/O. SQL Server also comes with a passably good set of B.I. tooling, including an OLAP server. Tableau is not cheap, around Â£1,800/seat last time I looked. Depending on the number of users you may find that the 'good-enough' tooling that comes with SQL Server could offset the costs of the DB licence anyway. Most third party reporting tools also play nicely with SQL Server. Disclaimer: I'm not any sort of die-hard Microsoftie but I have done a lot of B.I. work with SQL Server and Oracle over the past decade or so. SQL Server is actually quite a good B.I. platform. 

Operational vs. analytic reports Ad-hoc reporting against an operational system is a bad idea, so the answer depends on what your real-time requirements are. 

In Relational algebra, projection means collecting a subset of columns for use in operations, i.e. a projection is the list of columns selected. In a query optimiser step, the projection will manifest itself as a buffer or spool area of some description containing a subset of the columns from the underlying table or operator, or a logical view based on those columns that is used by subsequent operations. In a view, a projection equates to the list of columns selected in the query underlying the view. 

You could put the original query using into a subquery and wrap it with a query that filters the results. 

If eventual consistency is acceptable and all your queries are aggregates then perhaps a low-latency OLAP system might work for you. Your requirement sounds a bit like an algorithmic trading platform. This type of architecture is often used in trading floor systems that have a requirement to carry out aggregate statistical analysis computations on up to date data. If you can partition your data by date and old rows don't get updated then you can build a hybrid OLAP system using a conventional OLAP server such as Microsoft Analysis services backed by an ordinary RDBMS platform. It should be possible to make this cope with ~4TB of data and both SQL Server and SSAS will do shared-disk clusters. Similar OLAP systems (e.g. Oracle/Hyperion Essbase) are available from other vendors. OLAP servers work by persisting data in a native store, along with aggregates. Most will support partitioned data. In addition, most will also work in a ROLAP mode, where they issue queries against the underlying database. The important thing to note is that the storage strategy can be managed on a per-partition basis, and you can switch a partition from one to the other programatically, In this model, historical data is stored in MOLAP partitions with aggregates of the data also persisted. If a query can be satisfied from the aggregates then the server will use them. Aggregates can be tuned to suit the queries, and correct aggregates will dramatically reduce the amount of computation needed to resolve the query. Very responsive aggregate queries are possible with this type of system. Realtime data can be implemented by maintaining a small leading partition - for the current month, day or even hour if necessary. The OLAP server will issue queries against the database; if this partition is small enough the DBMS will be able to respond quickly. A regular process creates new leading partitions and converts closed historical periods to MOLAP. Older partitions can be merged, allowing the historical data to be managed at any grain desired. The clients writing to the database just write straight out the the underlying RDBMS. If historical data remains static they will only be writing to the leading partition. 4TB is a practical volume to use SSDs for if you need extra DBMS performance. Even mainstream vendors have SSD based offerings with faster SLC units as an option. 

Beyond that, you're probably up for a commercial repository tool like Powerdesigner, Erwin or Embarcadero. If you're actually working for an outfit that has 'hundreds of enterprise databases' on the books there's a chance they might have something like this on the books already. 

I don't think MS actually guarantee anything about the underlying SSRS database, so you're getting into unsupported territory by actually frigging with that. Generally this sort of thing is best avoided on production servers. You can query data sources and other report server metadata programatically through a web service API exposed by SSRS. The API would allow you to tree-walk the folders hunting for shared data sources in places they aren't supposed to be. Hunting and exception reporting is probably the best way to deal with this, rather than attempting to prevent it at the source. You can also query the report metadata to hunt for reports with references to sources outside the designated areas. This allows you to educate the authors. MS provide a utility called that lets you write scripts to do this in VB.NET. Later versions might also support C# but I haven't done this with anything more recent than SSRS 2005. Essentially it tops and tails your script with a bit of boilerplate, compiles it and then executes it. Some documentation about the utility and SSRS web service API can be found here, here and here.