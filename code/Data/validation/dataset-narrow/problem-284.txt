This may work for you, but by no means do I consider it elegant (or even good for that matter). First, configure the server to log all successful and failed logins. 

Next time, click the button and then execute the generated SQL script at both the local and remote databases. 

I think @SqlWorldWide is onto the root cause of your issue, which is that pessimistic concurrency is allowing values to change between the time you make calls to the nested UDFs and when you insert values into Table1. If you either enable READ_COMMITTED_SNAPSHOT or ALLOW_SNAPSHOT_ISOLATION on the database (both are unneeded as they accomplish relatively the same objective here) and run the statement again, I suspect you'll no longer run into data consistency issues. Note: If you opt to enable ALLOW_SNAPSHOT_ISOLATION, please update your SP to include the command, as enabling this database setting also requires the transaction isolation level be explicitly defined in contrast to the READ_COMMITTED_SNAPSHOT setting, which implicitly enables this functionality across all queries within the database (ref). Don't interpret this as both features function in the same way, rather they both achieve optimistic locking which likely will help with your situation. I would suggest enabling READ_COMMITTED_SNAPSHOT over ALLOW_SNAPSHOT_ISOLATION because I'm lazy and I've seen this setting benefit transactions more often than not at the database level. However, if you want to be surgical in your approach with optimistic concurrency, utilize the ALLOW_SNAPSHOT_ISOLATION feature instead which allows you to pick and choose which queries utilize optimistic concurrency via the SET TRANSACTION ISOLATION LEVEL operation. 

First, what's so great about a filtered NCCI? Filtered NCCIs are pretty cool in that when data being returned by a query isn't entirely contained within a filtered NCCI, the engine will still use it to pull out what data it can and then go to the (traditional) row-store indexes to pull the remaining data. This is especially helpful regarding analytical/aggregate workloads where NCCIs tend to out perform their row-store index counterparts. This concept is better explained in this MS article, but I find this info-graphic, also taken from that article, to be quite effective at summing it all up: 

is logically equivalent to as you suspect, so if you want to make this an ed boolean operation you'll need to adjust the syntax as you also suspect. To answer your bonus question, create a global temp table (e.g. ) prior to the call and insert your results into that global table. After you execute , it's just a matter of querying the global temp table to access all the results. 

This may be helpful with your database design depending on how complex you wish to get, but more importantly as Grant Fritchey explains, it can also help improve performance for certain types of queries if you do create a FOREIGN KEY constraint on it. Secondly, and this is probably the more important fact, a UNIQUE constraint will likely be a smaller index to it's Non-UNIQUE equivalent, even if the Index Definitions are exactly the same. When a Nonclustered Index is defined as UNIQUE, the clustered key is implicitly stored as (an) INCLUDE column(s) (because nonclustered indexes have to point back to the clustered key). Alternatively, if the Nonclustered index is NOT defined as UNIQUE, the clustered key is implicitly appended to the Nonclustered Index Key and will exist at all levels of the index (making it bigger). Kalen Delaney does a much better job of explaining why this occurs in her posts on Nonclustered Index Keys and More About Nonclustered Index Keys. 

all tables containing PostGIS related data for every database on the server drop all tables containing PostGIS related data (same that were just backed up) for databases that had PostGIS tables in step 1 run script in against databases that had PostGIS tables in step 1 Uninstall PostGIS 1.0 gppkg () Upgrade GreenPlum database via the gpmigrator/gpmigrator_mirrors standard process Install PostGIS 2.0 gppkg () run script in against each database that had PostGIS tables in step 1 all tables exported in step 1 

If you don't, enable it immediately, as I've yet to see a downside with this feature. This option became available with Standard Edition with SQL 2008 R2 and it's one of the first sp_configure options you should enable after setting up a new instance. This will help speed up backups dramatically. Another option is to stripe your backups to multiple files. This will increase the I/O throughput of the backup and reduce the backup speed dramatically as well. One of the things I don't like with Ola's scripts is that this setting isn't dynamic or configurable based on the size of the database. What I will suggest though is to specify 4 paths for the DIRECTORY parameter and increase/decrease after testing it out. From Ola's Backup Page about striping: 

This is something that's bugged me though has never caused any real issues as I can generally locate this information by other means, but can someone explain why may be returned in the , , and columns from the DMV, yet yield results in the column? I understand why output would return inversely (e.g. all columns but would yield data), but I've seen this behavior a number of times across a number of SQL Server versions and the documentation doesn't seem to imply this should be possible, or I'm just reading it wrong. Here's an example of the query I'm running: 

Correct. If you never modify a record after it's added to the end of the table, you'd really only ever need to update statistics in order to avoid the Ascending Key Problem as identified by Scott Hodgin's comment below. 

If you detach a database from an instance, you will need to perform an OS-level delete of the file. The safer approach is to drop the database instead. What I suggest is taking a final backup of the database after you put it into Read Only mode (as this will ensure no activity is occurring during the backup), after which remove it from you system by way of a Drop Database command. The full set of commands would look similar to the following: 

Usually copying your latest database backup (from the production backup location) to an area where you can restore it over the top of your staging area should have no effect on your production database. The only time where this would affect it is if you're storing the backups on the same drive(s) that is hosting the live database, but hopefully you're not. In this instance, the file copy operation will just generate extra, and competing, I/O with your normal database operations. Your question doesn't provide enough context around how this sync process works, but a 15GB file copy isn't usually large enough (in my experience) to crash a server or cause any severe performance degradation. 

Add a proxy account for Operating System (CmdExec) jobs and run the Ola jobs under that. This way the proxy account can have proper access to only the backup shares, etc. and you don't have to elevate permissions on the SQL Agent account (and inherently any other jobs running under the SQL Agent security context). Principle of Least Privilege for the win! :) 

The upgrade path we have identified is the very definition of kludgy, but it looks to get all the data over with an end result of both an upgraded database and upgraded PostGIS extension. The (admittedly Rube Goldberg-ish) steps are as follows and are required for every database on the server being upgraded containing PostGIS data: 

You've got a number of methods you can employ, but they will all have to iterate through your jobs (and potentially job steps) to add a new step. I think your easiest and cleanest approach is to add a new job step at the end of your current job step list, check to see if you're running on the Primary Node, and then go back to the original job start-step and run the job as normal. This approach makes the least amount of edits against a job and the code to do that is as follows (obviously test before you run in production): 

Can you do it? Sure, but if you really want to add the check as step 1 in each job, it would likely be safer to edit each job manually through the GUI especially if you deal with complex job steps or job step dependencies. 

TL\DR I'm looking for a way to efficiently identify the object located closest to the end of a SQL Server data file. This approach needs to remain performant against large data files. What I have so far The following query utilizes an undocumented Dynamic Management Function that shipped with SQL 2012: ; this DMF provides a rough equivalent to the command. The following query identifies the last object in a given data file (Warning: Don't run this against a database larger than 25 GB unless you want to cancel it at some point): 

The only way you can pass Windows Credentials between servers to my knowledge is via delegation as you've already taken a look at. If there's truly no way to get delegation working in your environment, an alternative to delegation is to fall back to mapping your Domain accounts to SQL authenticated accounts on the remote server that are setup with permissions that are equivalent to your needs. The downside here is any query executed remotely will be logged to the SQL Authentication account and not your AD Account, so audits won't be as transparent unless you take pains to map a one-to-one SQL Authenticated account to AD Account. To map AD accounts to remote SQL Authenticated accounts, you first need to create the SQL Authenticated accounts on the remote server and then configure local logins to map to them on your linked server. If you're happy to use the GUI, you can find this window under Server Objects → Linked Servers → Linked Server Name → Properties → Security Page: 

I've not found any references indicating this is expected behavior, but before I submit a connect item, I first wanted to reach out and confirm this isn't a localized problem. Can someone either point me to documentation identifying this is expected behavior or alternatively confirm this is, in-fact a bug? EDIT: In response to the comments about not including an clause, I was always under the assumption the TOP keyword returned the data in the order in which it was inserted, which should, in this case, be the order dictated by the clustered key. When running the same statement against a formal table, the expected behavior is returned: 

I had to reread the vendor requirements a couple of times, but I take the instructions to mean the local administrator account only requires elevated permissions during installation. Run one of the following blocks prior to the app install to set this up. 

The source reference from MS can be found here. Now to make your Network Admin's Day (e.g OU configuration that allows for Self Registering SPNs) Your Network Admin can create an OU on the domain which contains all your SQL Server Service accounts that can be configured in such a manner that the Service Account can create an SPN for itself and itself alone. The method is mainly following Ryan Reis's blog, but has some slight tweaks so that over-grants are not performed. This process describes the creation of an OU on the domain that allows accounts within it to self-register their own SPNs: 

You cannot update an IDENTITY field, so you have to insert new records into the table if you want to manipulate the IDENTITY value. You are able to adjust the IDENTITY SEED throughout the process though to fit your needs or outright ignore it (so long as you do not insert duplicate values) with . Here's an example script that hopefully illustrates what you can do better than my explanation: 

The Cross Apply is being used in conjunction with the XQuery call. This is basically getting every element for the XML arrays constructed within the CTE statement. The outer XQuery call then extracts that value and casts it to a , returning a record for each code within each group. I find it easier to digest if I break up the query. Maybe you will as well. 

This isn't a verified query as I'm not running replication, but after scripting out definitions for both and via and smashing together the underlying logic, I came up with the following that may give you what you're looking for. 

As this is effectively what the statement is doing from a logic standpoint. I'm assuming is a column in this scenario and not just a random string of 's, correct? 

So I did some analysis on the various approaches posted so far, and in my environment, it looks like Daniel's approach wins out consistently on the execution times. Surprisingly (to me) sp_BlitzErik's third CROSS APPLY approach wasn't that far behind. Here're the outputs if anyone's interested, but thanks a TON for all the alternative approaches. I learned more from digging into the answers on this question than I have in quite a while! 

Just as an FYI, the module_hash column in v$session has less to do with the app running it and more to do with the value passed to the DBMS_APPLICATION_INFO.SET_MODULE procedure (ref). For instance, you can emulate any module_hash using SQL Plus to call this procedure, so I wouldn't count on your current approach being entirely foolproof. I don't expect you to take my word for it though, so check this AskTom post in the comments section; Tom responds to Khandaker Anwar's INTERESTING FINDINGS FROM V$SESSION TO PROTECT UNWANTED ACCESS TO DATABASE comment countering this exact line of thought. With that out of the way, if you still wish to emulate what is occurring in your Oracle environment, others (e.g. @Scott Hodgin and @Ste Bov) above have pointed out the program_name field in either the sys.dm_exec_sessions DMV or sys.sysprocesses VIEW will provide similar identification as the module_hash field within Oracle as it can also be set to whatever you would like when establishing the connection to the database. Frankly, I think this approach has too many ways around it and instead you should whitelist only those IPs or subnets that you want to allow connectivity from on your firewall and use Active Directory accounts or Groups to restrict access to only those parties who are allowed to connect to the database which echo's @Vladimir Oselsky's earlier comment. 

I suspect it is related to the fact that this DMV is updated asynchronously behind the scenes. Per the sys.allocation_units BOL page: 

After this, you'll want to look for any Jobs that ran scripts against the database. I would suggest you just wait to see what fails (after which you can script out/delete the job) as there are numerous ways a job can reference a database (not all of which are easy to identify). Finally, you'll want to remove any users from the instance that only had access to this database. This script should identify whom those users are, though Max's version is much cleaner (I didn't realize he posted an approach until after I edited my answer to include this): 

I believe you're going to want to look at Temporal Tables. These provide functionality to do exactly what you're looking for and are available in Postgres with the proper extensions. This concept looks to be pretty DB agnostic as well, as it's offered on a variety of RDBMS platforms. 

It's likely that your Test-User account has different SIDS on the primary and replica servers. The best way to resolve this is drop the Test User on the Secondary, generate a create user statement from the primary with the proper SID (e.g. sp_revlogin, dbatools.io, or another tool of your choice) and then run that generated CREATE USER statment against the secondary. This will create a user that has the same SID on both replicas so you won't get this error going forward. Another option, and one that I would recommend instead, is to convert this database to a Partially Contained Database so that all SQL Logins you create with the database are automatically transferred over to any replicas you setup. This is a very handy feature (if the limitations don't adversely affect you) for databases in Availability Groups and will help minimize the administrations headaches that come with them. 

Conceptually this is what should be happening within your database when the end of the physical file is reached: 

The data isn't deleted until a commit is implicitly issued. Because a timeout occurs before the commit is issued, an implicit rollback is issued instead and the result is no data changes are made. More information on how transactions work in SQL Server can be found here.