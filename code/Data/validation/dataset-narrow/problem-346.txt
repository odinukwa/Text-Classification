If you are executing your second query from the database then the value from is being coerced into 's collation, not the other way around as you suppose. 

Since existing records are static they can be converted to the new schema any time after the migration process is signed off. Specifically, as soon as the new DB is available you can start loading it with old data. Start with the oldest and work your way forward one day at a time. Assuming it takes less than twenty four hours to convert one day's data, you can run this process non-stop until you have caught up with today's data. Thereafter you can continue with a more-or-less parallel run, with two options. Option "A" is to load each new day's data into the old DB, then run the convertion process to get it into the new DB. Option "B" is to run the old ETL to load today's source file into the old DB and run the new ETL to load the same soruce file into the new DB. Option "B" has the advantage of testing your new ETL process, too. Then on cut-over day you will have at most one day's worth of data to move. As for the convertion process itself I would suggest you read all the data for one business transaction from the old tables (i.e. a referentially in tact set), convert it, then write it entirely to a set of staging tables in the new schema. Note not to the live tables themselves but to a parallel set of staging tables. Then you can write lots of reconciliation checkes against these staging tables. If there's a problem you can bulk-delete the staging tables without affecting the new DB's data, fix the convertion and re-run. Once the reconciliation completes successfully you can bulk-copy from the staging to the live tables. The staging tables can have additional columns to help with reconciliation that you would not want in the live tables (source surrogate ID, for example). Retain the staging tables for a few months for any post-mortem on your go-live problems. I know it sounds like all this staging and reconciliation is eating precious time during a cut-over. And it is, but is nothing compared to the time it will take to post-fix two hundred million rows in an active production system. 

I think has definitons for functions, too, but I don't have an instance to hand just now to check. Alternately you could create a view with the same query you are about to put into your function and check the catalogue for that. 

Add a self-reference to User. Call this reference "approved by". I'd suggest implementing this as a separate table but if you're absolutely sure you'll only ever need three approvers it can be colums in the user table. As a current member approves an application count the number of approvals, compare it to the number of required approvals, and the number of current members and proceed accordingly. Remember to account for the case where the current applicant is the first to apply to a club. I think it's an error to make a user exclusive to one club. Wouldn't it be good to know that the same person plays polo, frisbee, netball and Jai-Alai? It would be better to have an intersection table called Membership. 

Query processing takes the given SQL, applies a set of rules and heuristics and comes up with an execution plan. This states which indexes are used and how. The outcome is dependent on the input and how the rules have been written. Given a slightly different start point a different end point may result. It's somewhat like dropping a ball in a landscape of river vallies. Drop it once and it will roll into a river. Drop it in a slightly different place and chances are it will roll down into the same river. Occasionally, if you're near a ridge, you drop the ball in a slightly different place and it rolls into the next river instead. 

My first suggestion would be to tweak the existing processes. Then I'll describe a re-design. I don't think you need to re-sequence the whole set on every write. As long as you're prepared to accept gaps in the sequence and different numbers of decimal places you can avoid many executions. For DELETE there is no need to change the sequences of the other rows. Just accept the gap in the numbers. I take it the user cannot UPDATE the sequence directly so this is a non-issue. For Move Up / Down simply swap the sequence numbers of the rows involved -- two single-row updates. For Move Top allow the sequence to become negative. This may look weird but the computer will handle it just fine. Similarly for Move Bottom just let the sequence become large. INSERT is the only tricky one. Now you cannot rely on neat additions of 0.5. Since we may now have gaps you have to split the sequences of adjoining rows (or add/ subtract 1 for inserting at the top/ bottom). As you say this could produce numbers with many decimal places. I don't think this is a concern, however. These numbers are not for human consumption (they're not, right?). Only the computer has to deal with them. By expanding the sequence to NUMERIC(38,28) you can have up to 90 adjacent inserts before the differences degenerate to zero. Since NUMERIC is exact (as opposed to FLOAT) there is no chance of increasingly small numbers being confused with each other. The application can track the number of decimal places in a newly-added sequence and trigger the re-sequencing when required. The task which re-distributes the sequence numbers can be taken out of the interactive user session. A separate asynchronous process can take care of this. It can run on a schedule if writes are infrequent or be triggered after a certain number of user edits. Further it does not need to touch every row in every execution. Only rows after the changed rows need ever receive a new sequence number. Yes, a "move to top" action will affect all rows but a "move to bottom" will not require any further updating. If many rows are inserted consecutively it is likely their sequences will become increasingly small: 10.5, 10.25, 10.125 ... 10.0009765625. If you know which rows have changed since the last re-sequencing (if there is a last_updated column, for instance) the impact of re-sequencing can be further reduced. Take the FLOOR() of the lowest changed sequence number and CEILING() of the highest changed, then evenly distribute the changes between those limits. This will not return every sequence to an integer but it will reduce the number of decimal places involved. Fewer rows will be touched by the UPDATE. 

First off - are you sure the self-relationship is one-to-many? It seems to me that should not only be a member of but also of . Similarly not only contains but also . This would be many-to-many, and you've already solved that problem for user/ group interaction. Solve it the same way here. If you're absolutely certain it's one-to-many there are several different implementation of trees in SQL. 

To stress a system you can either increase load or decrease resources. There are many commercial load-creating packages out there, and probably some freeware ones, too. It sounds like you've isolated it to just one SP, though, so a custom script to fire off 500 examples of it in parallel should do the trick. Running the script on the DB server will induce even more stress on SQL Server. I have some Powershell scripts that can max out a four-core server for minutes at a time! Multiplying the data with some (with appropriate primary key adjustments) will make the SPs work harder, too. On the resource side, reducing max server memory will cause more bufferpool paging and, maybe, more SP recompiles. Setting the instance processor affinint so it has fewer cores available will cause each to work harder. Of course these may cause subtle, secondary changes which affect what you're trying to measure. New, different execution plans for example. On the monitoring side I've found these DMVs helpful to understand locking. HTH 

The points you raise are valid. An alternative design would be to store only the changed values in the history store. A record there would then be (primary key field(s), name of changed field, old value, new value, written by, written when). Storing only the parts that changed leads directly to two new problems. One, you have a mandatory overhead of separating the changed parts for every single write, as opposed to deferring the diff costs until needed. Typically we want OLTP writes to be fast and audit reporting is not performance critical. So it would seem reasonable to me to defer that work to when the report is required. Of course the cost of writing the whole record will be more than of writing the shorter diff-only record. You can benchmark your infrastructure to see if that is significant. Two, by storing only deltas you loose the data context in which a change occured. By this I mean by only storing deltas we can read a single audit record to see that, say, field 53 changed from "A" to "B". What values did all the other fields have when that change occurred? To find out you have to read backwards through history until you find the most recent value for the other fields in which you're interested, potentially all the way to the "live" record. Again, I don't know if this is a valid use case for your scenario, or if change frequency and audit read frequency would make it a problem, but it is something worth considering. To show "last updated by .. on .. " with either scheme is a single record lookup of the last history record written because both ways store this information on every history record. To find what changed with the full record stored will require a second read and a diff, which is not needed with a delta store. To reconstruct a historical record will be light work with full records and processing-heavy with deltas. I don't know your requirements or constraints well enough to fully evaluate one design over the other. My preference would be for the one I documented due to ease of implementation and (likely) performance. I will note, however, that there are no free lunches. The work has to be done somewhere, sometime. 

Moving 1GB is not a trivial operation, though not a scary one either. A simple would have too much impact on the running server. Batching so only n-thousand rows are moved at a time would be better. 'n' can be tweaked to the idiosycracies of your particular system. Better still would be a dedicated ETL package. You'll need a SQL Server developer to help you with that, most likely. If it is important that you never miss a row be sure to build reconciliation into whatever solution you adopt. Be careful using a datetime to determine whether a row should be copied or not. Times have to rounded to the resolution of the column holding them. It is therefore possible that a new row is rounded down and missed by the next extraction. It sounds like you will be copying data that is 13 days old so will not be at risk. 

"Is this a good plan to copy the indexes." Indexes exist to improve the response time of read operations. Those operations arise from the workload. If the workload differs, the indexes should differ, too. For example, it is not uncommon to have one server handle the OLTP activity and replicate the data to a second server where analytic queries are processed. The first server's work will be mostly single row lookups. The second's range scans and aggregates. In this case having different indexes on each server makes good sense. If all indexes were on both servers there would be additional, unnecessary work to do during writes and maintenance.