Lastly, consider what happens when we try the dynamic programming algorithm from SS on SP. Because we use products rather than sums, the numbers involved blow up enormously, and the arbitrary precision math required suddenly becomes a factor in the running time. This is why the algorithm cannot solve SP instances quickly even if the numbers are in unary. 

I proved a variation on Rice's theorem that answers your question for most properties. I'll try to explain myself clearer this time (Travis Service's answer was much clearer and more general than my previous answer). 

Recall that a Turing Machine (TM from now on) decides a language if it accepts all strings in the language and rejects all strings outside the language. Note that we can take $f(n)$ to be something other than a polynomial, so the theorem is more general than just $P$. We formalise the notion of a 'property' as some freely selectable set of languages $S$ 'with that property'. Deciding whether a language has the property is then equivalent to deciding whether the language is a member of $S$. Just like in Rice's theorem, we investigate if we can decide whether the language decided by the input TM has the specified property, and so is in $S$. Note that we require $S \subseteq R$, ie, that $S$ contains only decidable languages. Please note that we are talking about properties of languages, not of TMs. Your question about runtime exponents is not a special case of this theorem. Properties of, say, $P$, studyable by taking $S \subseteq P$, may interest you more than properties of TMs running in polynomial time. You can do all sorts of cruel stuff to a TM, while retaining its correctness and running time, but you can't do the same to languages. The requirement that all languages in $S$ have to be infinite is a technicality needed for the proof, but since all finite languages are decidable in constant time and are therefore usually uninteresting, I don't think it's a major one. I expect the generalised version that also allows finite languages to be true as well. Proof of the theorem. Assume we are given some TM $P(E)$ with a TM $E$ as parameter, such that $P$ always halts and accepts iff $E$ decides some language in $S$, and where $E$ is promised to always halt with running times as above. Let $(A, i)$ be an instance for the Halting Problem, ie, $A$ is a TM and $i$ is an input for $A$, and we now want to decide whether $A$ halts on $i$. Since we assumed $S$ was nonempty, we have some $s \in S$. Since $S$ contains only decidable languages, there exists some TM $C$ deciding $s$. In particular, we choose the $C$ with running time $g(n)$ as assumed in the theorem. We now consider the following TM: 

I'm looking for a name or any references to this problem. Given a weighted graph $G = (V, E, w)$ find a partition of the vertices into up to $n = |V|$ sets $S_1,\ldots,S_n$ so as to maximize the value of the cut edges: $$c(S_1,\ldots,S_n) = \sum_{i \ne j}\left(\sum_{(u,v)\in E : u \in S_i, v \in S_j}w(u,v)\right)$$ Note that some of the sets $S_i$ can be empty. So the problem is essentially max k-cut, except $k$ is not part of the input: the algorithm can choose any $k$ it likes so as to maximize the value of the cut edges. Obviously, the problem is trivial if edge weights are non-negative: simply place every vertex alone in its own set, and you cut all of the edges. But, to make things interesting, negative weight edges are allowed. Is this a studied problem? References to algorithmic or hardness results would be appreciated! 

Secure cryptographic protocols that can be executed by humans are considered in this paper: $URL$ (Hopper and Blum, "Secure Human Identification Protocols"). Reading the abstract, it sounds like they don't fully solve the problem. I suspect a satisfying solution is not known, but if you want to look into it I would start by looking at papers citing this one. 

To add one more perspective to the rest of the answers: because you can actually do stuff with finite automata, in contrast with Turing machines. Just about any interesting property of Turing machines are undecidable. On the contrary, with finite automata, just about everything is decidable. Language equality, inclusion, emptiness and universality are all decidable. Combined with that finite automata are closed under just about every operation you can think of, and that these operations are computable, you can do pretty much anything you'd ever want to do with finite automata. This means that if you can capture something using finite automata, you automatically gain a lot of tools to analyze it. For instance, in software testing, systems and their specifications can be modeled as finite automata. You can then automatically test whether your system correctly implements the specification. Turing machines and finite automata therefore teach people an interesting and ubiquitous contrast: more descriptive power goes hand in hand with less tractability. Finite automata can't describe much, but we can at least do stuff with them. 

Inspired by the extensive hierarchies present in complexity theory, I wondered if such hierarchies were also present for type systems. However, the two examples I've found so far are both more like checklists (with orthogonal features) rather than hierarchies (with successively more and more expressive type systems). The two examples I have found are the Lambda cube and the concept of k-ranked polymorphism. The first one is a checklist with three options, the second is a real hierarchy (though k-ranked for specific values of k is uncommon I believe). All other type system features I know of are mostly orthogonal. I'm interested in these concepts because I'm designing my own language and I'm very curious how it ranks among the currently existing type systems (my type system is somewhat unconventional, as far as I know). I realise that the concept of 'expressiveness' might be a bit vague, which may explain why type systems seem like checklists to me. 

There is work on this, and abstaining can sometimes help. You might be looking for this paper: Trading Off Mistakes with Don't Know Predictions by Sayedi et al. from the last NIPS. 

Yup. Everything follows from duality. (I am only half joking). A partial list: Boosting The Hard-Core Lemma Online Learning The ability to actually solve LPs efficiently A large fraction of approximation algorithms results. Much more To develop algorithms, you often need a constructive or algorithmic version of the duality theorem (which is essentially equivalent to Von-Neumman's min-max theorem in game theory -- the applications in game theory are also huge). This is essentially what the multiplicative weights algorithm is. See Arora/Hazan/Kale for the best survey on this topic: $URL$ 

The problem you describe is called "stochastic convex optimization in the bandit setting". This is the problem considered and solved here: $URL$ They show that after $k$ queries, you can get within $O(1/\sqrt{k})$ of the minimum value of the function (ignoring a poly$(d)$ dependence on the dimension $d$ of the domain of the function $f$). 

I am wondering if the following problem has a name, or any results related to it. Let $G = (V,w)$ be a weighted graph where $w(u,v)$ denotes the weight of the edge between $u$ and $v$, and for all $u,v \in V$, $w(u,v) \in [-1,1]$. The problem is to find a subset of vertices that maximizes the sum of the weights of the edges adjacent to them: $$\max_{S \subseteq V} \sum_{(u,v) : u \in S\ \textrm{or}\ v\in S} w(u,v)$$ Note that I am counting edges both that are inside the subset and that are outside the subset, which is what distinguishes this problem from max-cut. However, even if both $u$ and $v$ are in $S$, I only want to count the edge $(u,v)$ once (rather than twice), which is what distinguishes the objective from merely being the sum of the degrees. Note that the problem is trivial if all edge weights are non-negative -- simply take the whole graph! 

As proven here, general TSP is NPO-complete. This means you can either solve it exactly in exponential time or approximate it in polynomial time with an approximation factor growing exponentially with the length of the input. This means that your algorithm, when applied to general TSP instances, cannot give a better guarantee than a factor $O(2^{n^\epsilon})$ for some $\epsilon > 0$. Different versions of TSP admit different bounds on approximation algorithms. For an overview, the site "A compendium of NP optimization problems" lists a number of TSP variants here. Wikipedia also gives a lot of bounds on its page about TSP, in this section and in this section. 

If you prove the above problem NP-hard, then for any $k$, if we modify the problem so that every vertex chooses $k$ vertices and every vertex is chosen exactly $k$ times, then the resulting problem is NP-hard as well: you can reduce the problem for any $k$ to the Hamiltonian Subcycle problem by 'eating up' the number of choices each vertex has by attaching certain gadgets to them. I came across this problem when I believed a certain problem was in fact equivalent to the above problem. That later turned out to be wrong, but the problem interested me nonetheless. I've developed an algorithm that sometimes finds the correct answer, but often doesn't end at all. It's based on using Lagrangian relaxation on the TSP variant of the above problem, which is simply TSP where more than one cycle is allowed (as long as the cycles are disjoint). I doubt it's possible to fix the algorithm so it works all the time, so I haven't included it in this question, though I could always do that if needed. 

Juntas can be tested in an attribute efficient manner given a membership query oracle: $URL$ Testing is an easier problem than learning, and the attribute efficient result is relatively recent. This might be a good place to start poking around. Incidentally, the hard part about learning juntas is just identifying the set of $k$ relevant variables. Once that is done, you can learn the junta through exhaustive querying with $2^k$ queries (which might be "attribute efficient" since it is independent of $n$. In any case, you can't hope to do much better since the function can be arbitrary on these $k$ bits...). The testing algorithm linked to seems to work by trying to identify relevant variables, so it might solve your problem if you go through the details. 

Along the lines of Jeff's answer, two algorithms are similar if the author of one of them expects that the author of the other one might be reviewing her paper. But joking aside, in the theory community, I would say that what problem algorithm A is solving is rather tangental to whether it is "similar" to algorithm B, which might be solving a completely different problem. A is similar to B if it "works" because of the same main theoretical idea. For example, is the main idea in both algorithms that you can project the data into a much lower dimensional space, preserve norms with the Johnson-Lindenstrauss lemma, and then do a brute-force search? Then your algorithm is similar to other algorithms that do this, no matter what problem you are solving. There are some small number of heavy-duty algorithmic techniques that can be used to solve a wide variety of problems, and I would think that these techniques form the centroids of many sets of "similar" algorithms. 

A friend of mine and me have decided to try the brute-force method and compute some values of $t$ for small values of $n$ and $d$. This is totally impossible without employing pruning, and we hope that the tricks we have found will give some insight in the rest of the problem. So far, we have not managed to get the doubly-exponential running time of the brute force method down significantly (roughly $3^{2^n}$ is our best bound so far) and hence we have not yet reached our original goal of somehow predicting the function behind $f$ from its first few values. We have also not studied all the comments of the previous threads in detail, so some of this may already be known - we basically had fun making our code fast and wanted to post our results somewhere, if I had a functioning LaTeX environment I would have put this on the ArXiV. Code (it's not exactly production code...): $URL$ Values: 

The concept seems to be introduced in "Extending context-free grammars with permutation phrases". Therein it is also described how to parse these phrases in linear time using an LL(1) parser. The paper "Parsing permutation phrases" describes a method for parsing permutation phrases using parser combinators. These are the only two papers I've found that talk about permutation phrases and how to parse them. Seeing that we can easily parse these kinds of permutation phrases with LL(1) based parsers, my guess would be that we can do the same with LR(1) style parsers. My question is therefore: 

If $f$ stores no state, then I think the answer will be "no". In that case, $f$ is just a function mapping $f: \{0,1\}^2 \rightarrow \{0,1\}$. If $f$ can be randomized, then you can just view it as a function $f: \{0,1\}^2 \rightarrow [0,1]$, mapping its input to its expected value. In this case, if $r_1,\ldots,r_n$ are uniformly random bits, then: $$E[\sum_{i=1}^nf[r_{2i-1},r_{2i}] = \frac{n}{4}E[f(0,0) + f(1,0) + f(0,1) + f(1,1)]$$ But if I just have the very simple (non-random) sequence $x_1,\ldots,x_n$ defined to take values $0, 0, 0, 1, 1, 0, 1, 1, \ldots$ forever repeating, then: $$E[\sum_{i=1}^nf[x_{2i-1},x_{2i}] = \frac{n}{4}E[f(0,0) + f(1,0) + f(0,1) + f(1,1)]$$ That is the long term average will be the same. Of course there is an obvious repeating pattern in the sequence of $x$s, but you can't detect it without state. 

In some fields (like e.g. Economics and Mathematics) single authored papers -are- a good thing to have when you go on the job market. In theoretical computer science, collaboration is much more common, and it is not unusual for even relatively senior researchers to have very few single authored papers. It is not at all suspicious if a student on the market has only papers with co-authors. This is partly because we publish more frequently, so it is ok that each individual paper has less signal about your specific contribution. This is not to say that single authored papers are not impressive, but credit is super-additive across authors, and all things being equal, you should prefer to have a better paper with more coauthors than a worse paper with fewer. This should push you towards collaboration, since collaborators make your research both more fun and generally stronger. Of course in hiring decisions, committees will attempt to figure out whether you were a significant contributor to the papers you worked on or not -- but this will be done largely through the letters you obtain. If you have several good papers with the same senior co-author, that coauthor should write a letter for you, since they can explain in detail your contribution. It also helps if you have a clear research agenda of your own. Many papers on -your- topic with a rotating cast of coauthors conveys that you are leading the research direction, in contrast to a series of papers on disparate topics, each in the research agenda of your various coauthors.