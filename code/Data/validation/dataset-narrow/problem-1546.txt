Let me first explain an easier concept, which is the subset? Subset is related " in general" to ensemble classifiers. In ensemble classifier, you combine the outputs of multiple outputs of different base learners " weak learners". One of the important keys is having diversity. That different base learners have different abilities. One of the easiest way to achieve diversity is by using subset learning. Each time you train a base learner you sample from the training data, since this sampling process is randomly, these base learner will not be the same and thus you support diversity. What about subspace? In subspace, you use all samples, but each time you select different set of dimensions. For example, let us say that each sample is a vector 1*100 The first base learner can be trained by using all samples but only the first 10 dimensions. So the first base learner will be trained by using samples where each is 1*10. By choosing random number of dimensions and random dimensions you support diversity 

When use any sampling technique ( specifically synthetic) you divide your data first and then apply synthetic sampling on the training data only. After you train you use the testing set ( which contains only original samples) to evaluate. The risk if you use your strategy is to have the original sample in training ( testing) and the synthetic sample ( that was created based on this original sample) in the testing ( training) set. 

I have worked on several practical projects. In these projects we designed classifiers to deal with practical problems. Before I started working on practical I used to be very interested in : overall classification accuracy, sensitivity and specificity. In practical life, usually they care in two values: Positive predictive value " precision " and positive percentage PP "how many samples are predicted as positive". The pp value measures the usefulness of this practical solution while the ppv value measures the quality of your solution. Based on my understanding if the pp value is zero the ppv value is not defined and the solution itself is not valid 

Usually removing noise is a process depends on the type of noise. Noises can be high or low frequency , you are using low frequency filters ( average and median) . 1. You did not describe the source of noise, we can not tell if it is a high frequency or low frequency . 2. Try to use a higher order low pass filter 

You need to understand to which of these cases your problem belong. if you have very severe data imbalance + very few number of samples + wide variation within the majority class and similarities between different classes. regular oversampling or down sampling techniques will not help you as well as most of the synthetic oversampling techniques designed specifically to deal with the data imbalance but the assumption is to have enough number of samples. Try to focus more on ensemble techniques that designed mainly to deal with data imbalance. SMOTE-Boost RUSBoost SMOTEBagging IIIVote EasyEnsemble 

Having more data may not mean that you have online learning. Above answers are correct if you assume that the coming new data samples will not have any concept drift. Meaning the true data generating distribution may change in a way that your current ( most recent model) is in a point that is very far away from the optimal minimum for the new data distribution. I recommend you to read more about “concept drift” and understand if your data will have any concept drift. Try to search for how NN deals with concept drifts. Unfortunately I am not aware of any technique or method can make NN adapt to concept drift. If you do not expect any concept drift, then saving the check point and continue training will be good solution if you are using stochastic gradient descent 

Recently I discussed the following topic with a friend. The setting is that we have a one-dimensional set of data. (In the example it was points of students, we would be grading.) The goal was to make a density estimation, but not to use anything "fancy" like kernel density estimation, but just use a Gaussian as the estimation. (Sure that makes the big assumption that the data is Gaussian, but that is not the point here.) We discussed two ways: 

Make a density estimation using an unsupervised learning method, e.g. using EM-algorithm. In this case the claim is that simply calculating the mean of the data and the standard derivation is already giving one the parameters to get the Gaussian parametrized the right way. Add up the number of occurrences for each value and then use a supervised learning regression with the Gaussian as function, powered by an optimisation algorithm. 

What is the intuition interpretation of those two approaches? None of them can be wrong in itself, but one of them could be wrong in the sense of wrong-usage. Meaning: I want to do something and I use the wrong method for it, because of misunderstood interpretation of what happened. So in that sense: Is one of them wrong in a way? 

What are the advantages/disadvantages of those compared with each other? As I found out only provides additional tools. 

Example Code I managed to express myself in R code. As one can see from the plots, the result is definitely not the same for any dataset. Only if the data is Gaussian and large the results get to be similar. But that means little to me ($2^x$ and $exp(x)$ converge both to $infinity$ for $x -> infinity$ and they have little in common otherwise). 

Recently I stumbled upon a R package that works similar like caret, but I can't remember what it's name was and I can't find it. It seemed to be less well known, but at least as extensive. Someone able to help? Edit: I search for this other package to widen my horizon. There is no specific other reason. This question is specific however, since I am not looking for a general recommendation, but I am looking for this specific package I have in mind. I will recognize the package once I read its functions and descriptions. I had searched for a couple of hours myself and did not find it by searching for "package similar to caret". That is why I need help. 

Preliminary question: Is it correct that the EM-algorithm will have the same result as just calculating mean and standard deviation from the data? 

Through discussion we found out that the two clearly have different outcomes - though probably similar. For case 2 we optimise the parameters of the Gaussian such, that the sum of the distances from the Gaussian to the occurrences is minimized (along the y-axis if you will). For case 1 we optimise the parameters along the x-axis if you will. Questions 

My Question Asking the right question is been said to be the most important thing in Data Science and in most situations I managed to find a good concrete (programmable) question, but this one seems tough to me. So: How do I ask a specific question to answer the general question? Of course I have already asked/answered simple questions regarding the winning players of each game. For example: "What is the type of points that contributes most to the total points for winning players (whose success proved them right)?" Now I'd like to make analyses over the entire field of players. An even more advanced question would consider that player reoccur across games. 

I read all answers, I think the simplest answer to this question is based on the understanding of the train - test strategy. There is NO exact or correct answer to this question. Any split that can guarantee I am not under fitting or over fitting the data is a good a split. under fitting and over fitting are two different problems and are directly connected to the bias error and the variance error. You are highly recommended to read these two tutorials: $URL$ $URL$ 

I am not sure if the validation set is balanced or not. You have a severe data imbalance problem. If you sample equally and randomly from each class to train your network, and then a percentage of what you sampled is used to validate your network , this means that you train and validate using balanced data set. In the testing you used imbalanced database. This means that your validation and testing sets are not equivalent. In such case you may have high validation accuracy and low testing accuracy. Please find this reference that talks mainly about data imbalance problem for DNN , you can check how they sample to do the training, validation and testing $URL$ 

This is interesting !?! The way you think about it is as solving a linear system equation, which is not exactly the case. Imagine if you have thousands of data points , and you try to find the set of weights that solves ( satisfies) these points, what if an exact solution is not exist. What if there is no set of weights that exactly solves all these points. It becomes an optimization problem. What is the set of weight that minimizes the error. In this case the error is the difference between the actual output and the output you get by using the current weights. In fact, in current machine learning technology, it is difficult even to construct such a linear system. We use optimization techniques to solve this and find the optimal set of weights 

You should use the testing set without any change, as answered by others. But it is very important to understand the difference between average accuracy and overall accuracy. In overall accuracy you find ( number of samples predicted correctly/ total number of samples) in average accuracy, you find the overall accuracy per class and then you find the average of these overall accuracies. When you know that you are working with imbalanced database, where all classes are important, you should use the average accuracy To understand what this means: imagine you have two classes, class A and class B , and the ratio is 90 to 10 . If you are sampling randomly for the training and testing, then the ratio is still 90:10 in the testing set. If your model is very biased , that predicts all the samples to be class A , then: Overall accuracy = 90% Average accuracy = 50 % ( 100% for class A + 0% for class B) / 2 The overall accuracy is really high but it does not reflect the actual quality of the model. The average accuracy gives you a better indication of the quality 

Data Imbalance Data Imbalance + Very few number of samples (minority class) Severe Data Imbalance + Very few number of samples (minority class)