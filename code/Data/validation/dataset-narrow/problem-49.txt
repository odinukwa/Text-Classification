We can simulate this type of interaction using recursion and weighting each light path according to the actual reflectance/transmitance at the corresponding incident point. A problem regarding the use of recursion is that the number of rays increases with the deepness of the recursion, concentrating computational effort on rays that individually might contribute almost nothing to the final result. On the other hand, the aggregate result of those individual rays at deep recursion levels can be significant and should not be discarded. In this case, we can use Russian Roulette (RR) in order to avoid branching and to probabilistic end light paths without losing energy, but at the cost of a higher variance (noisier result). In this case, the result of the Fresnel reflectance, or the TIR, will be used to randomly select which path to follow. For instance: 

According to the code above, the path branches recursively if the ray has bounced up to two times. After two bounces, however, RR is used to select the path to be followed. This is also Ok for me. What is a bit confusing is the fact that the radiance returned by both possible non-branching paths (refraction and transmission) is scaled. I understand that there are different probabilities regarding reflection and transmission. However, if for instance Re = 0.3 and Tr = 0.7, and 100 rays strike the surface, about 30% of the rays will be reflected and 70% of will be transmitted due RR. In this case, I understand that there is no path termination neither energy loss, so there wouldn't be anything to compensate for. Thus, my first two questions are: why are these radiances scaled? Should they be scaled, or would it work without scaling at all? My third question is related to the scaling factors: Why the author has used P, RP and TP instead of Re and Tr? Any indication of a good reading about this topic is also very welcome!! Thank you! 

When light hits a conductor or a diffuse surface, it will always be reflected (being the direction of reflection related to the type of the BRDF). In a multilayer material, the resulting light path will be the agregate result of all those possibilities. Thus, in the case of a 3-layer material, assuming that the first and secong layers are dielectrics and the third layer is diffuse, we might end up with the following light path (a tree actually): 

[1] Paul S. Heckbert. Adaptive radiosity textures for bidirectional ray tracing. SIGGRAPH Computer Graphics, Volume 24, Number 4, August 1990 [2] The Siggraph 2001 course "State of the Art in Monte Carlo Ray Tracing for Realistic Image Synthesis" says the following: "Distributed ray tracing and path tracing includes multiple bounces involving non-specular scattering such as . However, even these methods ignore paths of the form ; that is, multiple specular bounces from the light source as in a caustic." [3] Eric Veach. Robust Monte Carlo Methods for Light Transport Simulation. Ph.D. dissertation, Stanford University, December 1997 

The difference is how well a mapping preserves locality and how easy it is to encode/decode the keys. The paper "Linear Clustering of Objects with Multiple Attributes" by H V Jagadish says: "Through algebraic analysis, and through computer simulation, we showed that under most circumstances, the Hilbert mapping performed as well as or better than the best of alternative mappings suggested in the literature". On the other hand, z-order is a bit simpler to use, for example compare the various methods listed in Bit Twiddling Hacks for z-order and Wikipedia for Hilbert-order. As for the applications, I think the main advantage in using space filling curves is that they map points from higher dimensional space to space of lower dimension. For example, they make it possible to window query for points using traditional B-tree database index. Again, on the other hand, the disadvantage is that one needs to know the bounds of the input in advance as it is difficult to "resize" the mapping later. PS: "Z-curve" is the same as "Morton code". PPS: Additional mappings include Peano curve and for applications see also Geohash. 

I would like to rotate a raster image by an arbitrary angle. I don't really care for speed: the rotation should be of highest quality possible. Could someone please suggest a suitable algorithm? I'm familiar with the rotation by three shears but I'm not sure if the shears would not cause too much aliasing/blurring. 

Splitting Bezier curve into two parts at some parameter is easy thanks to De Casteljau's algorithm. Is there a similar algorithm for NURBS curves? How to split a NURBS curve? 

Radiosity does not account for specular reflections (i.e. it only handles diffuse reflections). Whitted's ray-tracing only considers glossy or diffuse reflection, possibly mirror-reflected. And finally, Kajiya's path-tracing is the most general one [2], handling any number of diffuse, glossy and specular reflections. So I think it depends on what you means by "ray-tracing": the technique developed by Whitted or any kind of "tracing rays"... Side-note: Heckbert [1] (or Shirley?) devised a classification of light scattering events which took place as the light traveled from the luminaire to the eye. In general it has the following form: 

After five bounces, RR is used to determine if the ray path continues or not. If it continues, the BRDF is scaled to compensate for it. This is Ok for me. RR is also used in a second code segment to select between refraction or transmission during the rendering of dielectrics (expanded code version): 

You've said that "... bilinear filtering on in the texture ...". It seems that you are interpolating the depth values of the shadow map. The correct way of using interpolation with the shadow map is to apply it over the outcomes of the shadow tests (as far as I remember, OpenGL supports that). You might even combine the interpolation of the outcomes of the shadow tests with PCF, which will deliver much better results. However, as you might have noticed, aliasing is a plague that always pursue the shadow mapping :) Although I understand that you are looking for solutions regarding shadow mapping (even because it is quite simple to implement), have you ever considered the use of shadow volumes? It is much more intricate to implement, but does not suffer from aliasing at all, and I think would fit nicely your purposes. 

We can evaluate the total amount of radiance $L_r$ reflected by a multilayer BSDF considering each layer as a individual object and applying the same approach used in ordinary path tracing (i.e. the radiance leaving a layer will be the incident radiance for the next layer). The final estimator can thus be represented by the product of each individual Monte Carlo estimator: $$ L_r = \left( \frac{fr_1 \cos \theta_1}{pdf_1} \left( \frac{fr_2 \cos \theta_2}{pdf_2} \left( \frac{fr_3 \cos \theta_3}{pdf_3} \left( \frac{fr_2 \cos \theta_4}{pdf_2} \left( \frac{L_i fr_1 \cos \theta_5}{pdf_1} \right)\right)\right)\right)\right)$$ The paper by Andrea Weidlich and Alexander Wilkie also takes absorption into consideration, i.e. each light ray might be attenuated according to the absorption factor of each transmissive layer and to the distance traveled by the ray within the layer. I've not included absorption into my renderer yet, but it is just a real coefficient computed according to the Beer's Law. Alternate approaches The Mitsuba renderer uses an alternate representation for multilayered material based on the "tabulation of reflectance functions in a Fourier basis". I have not yet dig into it, but might be of interest: "A Comprehensive Framework for Rendering Layered Materials" by Wenzel Jacob et al. There is also an expanded version of this paper. 

Why are they deprecated? Those functions are deprecated because the OpenGL API moved in favor of a programmable pipeline in contrast with the old fixed pipeline. The programmable pipeline allows the flexibility necessary to enable a wide variety of effects and solutions that before were difficult or not possible at all to implement. You can still access those functions, as they are necessary to support older systems, but besides that I don't see any reason not to use the programmable pipeline. How to pass parameters If we are talking about parameters of lights (such as their position or intensity), you can pass those using uniforms. Particularly, you should look at Uniform Buffer Objects. It would look like this in a shader: 

Note that this already gives you a way of passing the lights that you want. Make your shaders expect an uniform buffer with 8 lights, copy those into an array in your client code and send them using the API. For more info on how to do this using the API functions, check the link above about UBOs and this. Is it too complicated to call every frame? Depends on your scene and the criteria for how to choose the lights that you want to pass to the shader. If you are worried about sending the data of 8 lights through uniforms each frame, then don't, that is the least of your worries. The problem lies in the selection process of the lights and the complexity of your scene/shading model, as the cost increases with the number of lights. 

Warning: I am not a physicist. As Dan Hulme already explained, light can't travel through metals, so dealing with IOR is a lot more... complex. I will answer why that happens and how to calculate the reflection coefficient. Explanation: Metals are filled with free electrons. Those electrons react to external fields and reposition until electrostatic equilibrium is met (the electric field is zero inside a conductor in electrostatic equilibrium). When electromagnetic waves hit a metallic surface, the free electrons move until the field that they create cancels the field of the incoming wave. Those electrons grouped together radiate a wave going out nearly the same as the one that hit the surface (i.e. with very low attenuation). How much is attenuated depends on the material properties. From this explanation it is clear that conductivity is a key part of the high reflection coefficient on metals. Math-wise, what you are missing is the complex index of refraction. On good conductors, such as metals, the complex term of the IOR is relevant and key for explaining this phenomena. Practically, in rendering, achieving good metal parameters is more visual based. Artists adjust to their preference until it looks believable. Often you see a metalness parameter with specific handling for materials marked as metal. Involved answer: The complex index of refraction can be seen if we use Ohm's Law $J = \sigma \vec{E}$, which holds for conductors, on the Amp√®re-Maxwell equation using sinusoidal waves $\vec{E} = e^{i\omega t}$: $$ \vec{\nabla} \times \vec{H} = \sigma\vec{E} + \frac{\partial \vec{D}}{\partial t} = \sigma \vec{E} + i\omega \epsilon \vec{E} $$ $$ = i\omega \left( \epsilon - i \frac{\sigma}{\omega} \right)\vec{E} = i \omega \epsilon_m\vec{E} $$ Note how we can interpret that whole term as a complex permittitivity $\epsilon_m$ and that $\sigma$ is the conductivity of the material. This affects the IOR, as its definition is given by: $$ n' = \sqrt{\frac{\epsilon_m}{\epsilon_0}} = \sqrt{\frac{\left(\epsilon - i \sigma / \omega\right)}{\epsilon_0}} = n_{\text{real}} + in_{\text{img}} $$ This shows how $n'$ can be complex. Also, note how very good conductors have a relevant complex term, as $\sigma \gg \epsilon_0 \omega$. Since it would take a lot, I will skip some steps with a reference, page 27: it can be shown that, since $\sigma \gg \epsilon_0\omega$, (we are dealing with $\omega$ of the visible spectrum): $$ n_{\text{real}} \approx n_{\text{img}} $$ and reflection from metals with normal incidence, from a medium with IOR $n$, given that $n' \gg n$: $$ R = \frac{(n_{\text{real}} - n)^2 + n_{\text{img}}^2}{(n_{\text{real}} + n)^2 + n_{\text{img}}^2} \approx 1 $$ Agreeing that a good conductor is, in general, a good reflector. The famous Introduction to Electrodynamics from Griffiths, pages 392-398, explains this and a lot more in a similar fashion. 

When the light ray is moving from a more dense to a less dense medium, the same principle described by the Fresnel reflectance applies. However, in this specific case, total internal reflection (a.k.a TIR) might also happen if the angle of the incident ray is above the critical angle. In the case of TIR, 100% of the energy is reflected back into the material: 

I was taking a look at Smallpt ($URL$ more specifically at the Russian Roulette part. Actually, RR is used in two places along the code: first, to determine ray termination; and second, for the rendering of dielectrics. That is the code for the ray termination case (expanded code version): 

As can be seen, TIR or Fresnel reflectance might keep some rays bouncing indefinitely among layers. As far as I know, Mitsuba implements plastic as a two layer material, and it uses a closed form solution for this specific case that accounts for an infinity number of light bounces among layers. However, Mitsuba also allows for the creation of multilayer materials with an arbitrary number of layers, in which case it imposes a maximum number of internal bounces since no closed form solution seems to exist for the general case. As a side effect, some energy can be lost in the rendering process, making the material look darker than it should be. In my current multilayer material implementation I allow for an arbitrary number of internal bounces at the cost of longer rendering times (well... actually, I've implemented only two layers.. one dielectric and one diffuse :). An additional option is to mix branching and RR. For instance, the initial rays (lower deep levels) might present substantial contribution to the final image. Thus, one might choose to branch only at the first one or two intersections, using only RR afterwards. This is the case with smallpt. An interesting point regarding multilayered materials is that individual reflected/transmitted rays can be importance sampled according to the corresponding BRDFs/BTDFs of the current layer. Evaluating the Final BSDF Considering the following light path computed using RR: 

This question is somewhat related to this one. As Alan has already said, following the actual path of the light ray through each layer leads to more physically accurate results. I will base my answer on a paper by Andrea Weidlich and Alexander Wilkie ("Arbitrarily Layered Micro-Facet Surfaces") that I have read and implemented. In their paper they assume that the distance between two layers is smaller than the radius of a differential area element. This simplifies the implementation because we do not have to calculate intersection points separately for each layer, actually we assume that the intersection points are the same for all layers. According to the paper, two problems must be solved in order to render multilayered material. The first one is to properly sample the layers and the second is to find the resulting BSDF generated by the combination of the multiple BSDFs that are found along the sampling path. Sampling In this first stage we will determine the actual light path through the layers. When a light ray is moving from a less dense medium, e.g. air, to a more dense medium, e.g. glass, part of its energy is reflected and the remaining part is transmitted. You can find the amount of energy that is reflected through the Fresnel reflectance equations. So, for instance, if the Fresnel reflectance of a given dielectric is 0.3, we know that 30% of the energy is reflected and 70% will be transmitted: 

And just to add: it is called "conflation" artifact and this is what AntiGrain Geometry used the compound shapes rasterizer for, see: flash_rasterizer.png $URL$ Also, this is what NV Path Rendering claims to improve on: An Introduction to NV_path_rendering (p. 67) or NV_path_rendering FAQ (#29). 

Could anyone please explain to me how drawing works with respect to scrolling? Suppose there is a window, which has an area where one can draw to (a "canvas"). Are there two copies of this canvas? One for CPU and one for GPU? I mean, I know one could copy from main memory to graphic card memory very quickly with OpenGL (PBO) so one way to draw is first draw with software in main memory and the "blit" it to GPU, right? Now, I would assume that when the window scrolls there is some GPU functionality which would rapidly copy over the non-dirty parts in the appropriate positions without leaving GPU-land and then ask the CPU for redrawing the dirty/missing parts. But now I have two un-synced copies of the canvas! So how does it work? Do I do the scrolling in main memory in software and upload the whole canvas to GPU as the first time? Or perhaps I scroll the canvas in GPU and "download" then download it to CPU-land and "repair" it there? Or something else entirely? Perhaps somehow I don't have to care that the copies are unsynchronized? 

The most common way I saw is to have photons of several different wavelengths. One then renders with each wavelength and blends the results into the final image. "Existing work": Psychopath Renderer and The Secret Life of Photons. 

"L" stands for luminaire, "D" for diffuse reflection, "S" for specular reflection or refraction, "E" for eye, and the symbols "*", "|", "()", "[]" come from regular expressions notation and denote "zero or more", "or", "grouping", "one of", respectively. Veach [3] extended the notation in his famous dissertation by "D" for Lambertian, "S" for specular and "G" for glossy reflection, and "T" for transmission. In particular, the following techniques are classified as: 

How to draw a NURBS curve? Compared with Bezier curve, I just evaluate the Bernstein polynomial, multiply it with control point positions and that's it. Looking at the "General form of a NURBS curve" paragraph of NURBS Wikipedia page I have a hard time seeing a polynomial in it. Maybe this "basis function" is a polynomial in the end? Is there an efficient way of constructing the basis function and evaluating it?