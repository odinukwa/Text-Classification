When I train, do I simply zero fill these outputs, then not backpropagate error coming from these nodes? So for cid12, error from all nodes backpropagates, but for cid51, only error from pKa and logP? If this is the case, is that a standard procedure implemented in most NN libraries (Theano, for example) and what is that called? 

You might find it useful to treat n-grams of characters as your feature space. Then you could represent a string as a bag of substrings. With or greater, you would capture things like ".com" in emails, "##.#" in dates, etc. It might also help to encode all single digits as one reserved number-only-character. An easy way to to this might be to create all the n-gram substrings for each string in your dataset, then simply treat that list of words as a document. Then you could use term frequency or tf-idf vectors in your supervised step. For example to create the substring uni-, bi-, and tri-grams for "whatever@gmail.com": 

Adding class weight but not changing the way you measure performance will usually degrade overall performance as it is designed to allow increased loss on lower-weighted classes. I would recommend also weighting your accuracy measures. This is a bit tricky with accuracy/precision etc. so maybe calculated the weighted logloss and compare it to the unweighted logloss of the unweighted model. Basically it comes down to the question: are you happy with a model that performs worse overall but better on your heavily weighted classes? 

Hugely dependent on both user and audience preference (violin plots being more unusual could throw people), so it is mostly up to you. One main reason to go for a violin plot is to give more detail about the distribution, as box plots just give hard stops at the mean, stddev and 2 stddevs. Therefore if you think there is interesting information contained in the distribution between those points go violin. The other main reason is they're more eye catching on Kaggle ;) 

Both the answers from @Emre and @Madison May make good points about the issue at hand. The problem is one of representing your string as a feature vector for input to the NN. First, the problem depends on the size of the string you want to process. Long strings containing may tokens (usually words) are often called documents in this setting. There are separate methods for dealing with individual tokens/words. There are a number of ways to represent documents. Many of them make the bag-of-words assumption. The simplest types represent the document as a vector of the counts of words, or term frequency (tf). In order to eliminate the effects of document length, usually people prefer to normalize by the number of documents a term shows up in, document frequency (tf-idf). Another approach is topic modeling, which learns a latent lower-dimensional representation of the data. LDA and LSI/LSA are typical choices, but it's important to remember this is unsupervised. The representation learned will not necessarily be ideal for whatever supervised learning you're doing with your NN. If you want to do topic modeling, you might also try supervised topic models. For individual words, you can use word2vec, which leverages NNs to embed words into an arbitrary-sized space. Similarity between two word vectors in this learned space tends to correspond to semantic similarity. A more recently pioneered approach is that of paragraph vectors, which first learns a word2vec-like word model, then builds on that representation to learn a distributed representation of sets of words (documents of any size). This has shown state-of-the-art results in many applications. When using NNs in NLP, people often use different architectures, like Recurrent Neural Nets (like Long Short Term Memory networks). In some cases people have even used Convolutional Neural Networks on text. 

I guess it depends on exactly what you want - do you want a forecast per store? If so you would need to leave the data set aggregated only to a store level. If not, and you want the overall sales by day, then you are OK to leave it aggregated. Another consideration is whether there are many store-level features e.g. location, weather, local demographics. These features may be useful for the model to learn from and would lose resolution if store level data is aggregated, therefore leaving it un-aggregated would be better. The counter point to this is whether you have enough data for each store to learn representative trends! So it does get kind of circular and depends on the size of your data set. 

1) Activation is an architecture choice, which boils down to a hyperparameter choice. You can make a theoretical argument for using any function, but the best way to determine this is to try several and evaluate on a validation set. It's also important to remember you can mix and match activations of various layers. 2) In theory yes, many random initializations would be the same if your data was extremely well behaved and your network ideal. But in practice initializations seek to ensure the gradient starts off reasonable and the signal can be backpropagated correctly. Likely in this case any of those initializations would perform similarly, but the best approach is to try them out, switching if you get undesirable results. 

The answer depends on what exactly outlier detector you use. and authomatically adjust to the scale and correlations between variables, so you cannot easily change relative weights of features. With you can increase importance of B by just replicating this column several times and so icreasing its relative frequency in the tree splits. Other algorithms, such as and kernel-based are explicitly based on the Euclidean distance between the points. And distance depends on feature scaling. It means that you can multiply B by some factor after normalization, and its importance will increase by this factor. 

The second way, predicting $x=cos(\alpha)$ and $y=sin(\alpha)$ is totally okay. Yes, the norm of the predicted $(x, y)$ vector is not guaranteed to be near $1$. But it is not likely to blow up, especially if you use sigmoid activation functions (which are bounded by they nature) and/or regularize your model well. Why should your model predict a large value, if all the training samples were in $[-1, 1]$? Another side is vector $(x,y)$ too close to $(0,0)$. This may sometimes happen, and could indeed result in predicting wrong angles. But it may be seen as a benefit of your model - you can consider norm of $(x,y)$ as a measure of confidence of your model. Indeed, a norm close to 0 means that your model is not sure where the right direction is. Here is a small example in Python which shows that it is better to predict sin and cos, that to predict the angle directly: 

tf-idf vectors are an easy start, but it can be hard to cluster very high dimensional data. You might try topic modeling (LDA, LSI for example) to reduce the dimensionality of your features. A newer approach is paragraph vectors, which learn a distributed representation of arbitrary-length text. Here is an implementation in python. Learning a reasonable, lower dimensional representation of the text can help with the issues that arise trying to cluster high dimensional data. 

This great tutorial covers the basics of convolutional neuraltworks, which are currently achieving state of the art performance in most vision tasks: $URL$ There are a number of options for CNNs in python, including Theano and the libraries built on top of it (I found keras to be easy to use). If you prefer to avoid deep learning, you might look into OpenCV, which can learn many other types of features, line Haar cascades and SIFT features. $URL$ 

Good question, probably a good handful of ways to look at this as it sounds like a classic classification problem. Define the label First step, before you choose a model, would be to define the label. As you suggested, I would go for email opened = 1 and email not opened = 0. I would then think about throwing the email not sent (-1) data away, as I'm not sure you can learn anything about a probability of opening when it wasn't sent. It would then be important to check the balance of the label: are they mostly opened or mostly ignored? This will influence any decision to add a class weight if it is imbalanced towards one class. Define features So time series could work, but I think another good angle might be to bin the send times into half hourly slots (or shorter, depending on your needs) and treat them as a categorical variable. You would then one-hot encode this feature. Anything else you can add to this will probably help, such as the kind of person it was sent to and what country they're in etc. Choose model So it's classification, so have a play around with either logistic regression or something tree based (random forests, gradient boosted trees) to get something going quickly. It's usually a case of trial and error to find the best one. You can then use traditional metrics of precision/recall/accuracy to measure how good it is. Make sure you know which metric is most important to your problem when optimising the model. 

Class imbalance is a very common problem. You can either oversample the positive class (or undersample the negative) or add class weights. Another thing to remember in this case is that accuracy is not a very useful metric here. You might consider AUC or F1 score. Changing your decision threshold may seem appealing, but will obviously lead to (in this case likely drastically) increased false positives (though perhaps FPs aren't as bad as FNs in the case of medical diagnosis, if tests aren't prohibitively expensive). A more in-depth analysis of options in the case of class imbalance is provided here. 

How much help this community can be depends on where in the process you are. If you have a dataset and are looking on ways to determine the most predictive features, then you could include some information as to what kind of data you've collected/have access to. If instead your question is specifically "what data should I collect to predict purchases from banks, and what models should I use," you are unlikely to get a very specific answer.