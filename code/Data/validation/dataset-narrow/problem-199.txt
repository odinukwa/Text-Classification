When you restore (either via SQL or GUI), you have to be sure that you don't leave the database expecting more restores (of differentials or logs). This is done in SQL using In the GUI, it is the "Recovery state" panel (point 12): 

Either way works it depends on your maintenance window. For larger tables, I prefer the duplicate table approach because it eases locking/maintenance window problems. I'd use a batch size of 50000 or 100000 too 

No, because you can't mix assignments and result set generation But what you can do is make the total row count into an extra column 

In the FROM clause, you can only use table valued functions See "Types of Functions" in MSDN Edit: As an observation, I'd tend to avoid: 

In this special case you've highlighted, they do the same thing, There is no difference. I'll guess that ALTER SCHEMA was added for completeness (eg there is CREATE and DROP why not ALTER) or in preparation for other features (such as renaming a SCHEMA, say) 

The only shortcut will be Both @parameter and EXISTS still requires processing for the "general case" ( say) Saying that... what does the actual execution plan say as well as profiler capturing recomplition events? (I dislike estimated plans as per Jao's answer) 

A subquery is evaluated once if possible. I can't recall what the "feature" is called (folding?) sorry. The same applies to GETDATE and RAND functions. NEWID is evaluated row by row because it in intrinsically a random value and should never generate the same value twice. The usual techniques are to use use NEWID as input to CHECKSUM or as a seed to RAND For random values per row: 

So "JOIN" in the client does add any value when you examine the data. Not that it isn't a bad idea. If I was retrieving one object from the database than maybe it makes more sense to break it down into separate results sets. For a report type call, I'd flatten it out into one almost always. In any case, I'd say there is almost no use for a cross join of this magnitude. It's a poor example. You have to JOIN somewhere, and that's what RDBMS are good at. I'd not like to work with any client code monkey who thinks they can do better. Edit: after comment To join in the client requires persistent objects such as DataTables (in .net). If you have one flattened resultset it can be consumed via something lighter like a DataReader. High volume = lot of client resource used. 

That is: IN, EXISTS and INTERSECT give the same results. Now, we may as well cover the inverse case too since you asked about LEFT JOINs. Here we have NOT IN, NOT EXISTS, LEFT JOIN and EXCEPT. Let's say we want rows from t1 with no rows in t2 where t2.c3 = 'foo' 

varchar doesn't support : unicode only This includes the literal which is varchar too Collation here doesn't matter: this is sorting and comparison only never seen this before! Some more SQL to use your table 

Basically, permissions are needed on (see permissions section). The roles are described in "SQL Server Agent Fixed Database Roles" (linked from above) Edit, Jan 2012 After the anonymous downvote 2 years after I answered... Read the question. It says 

2. Separate rows Or have a languageID column to support several languages.This has the drawback that collation will be fixed for all languages which means poor sorting/filtering PageParent 

The column level setting inherits from the table, database and instance settings Globally, it is set at command line on startup. I'd think carefully about the consequences of changing this because it affects more than the default for tables. For that, then look at ALTER DATABASE 

SQL Server 2000 has ALTER TABLE foo ENABLE TRIGGER ALL too And to generate the required ALTER TABLE script use this 

If a car can exist without a student, I'd consider a many-many table with a constraint on carid to allow zero or one owning student Note: This is where tools like NORMA come in useful to capture these relationships and constraints in plain English. See Object Role Modelling 

From the audit/history tables you can get the state of the database at any point in time. Edit: It's quite common to have multiple write paths onto a table and a trigger will ensure that changes are captured. This changes if you use table valued parameters to allow only one stored proc to actually write, which is called by other code. You also have to consider when the actual end user is a proxy away (eg via a web server) where your client code must pass in credentials (or enable pass-through). 

Never heard such rubbish. FULL or BULK LOGGED mode means you require log backups. SIMPLE means you don't. You always require full backups. Copying the MDF and LDF files isn't a backup and can potentially lose** you data if you are in a transaction as you shut SQL Server down. If the mdf files are missing it means they aren't there because of finger trouble or this "backup" method. You can't lose mdfs unless someone removes the files or removes a drive when SQL Server is shut down ** Edit: By losing data I mean "open transactions won't be committed". Typically, client code won't handle a retry and, say, log the error but not the data to be written. Any data in committed transactions, no matter how old or new, will be safe 

In addtion to Martin Smith's answer Stored procs are pre-parsed for syntax but not compiled until needed. This can be seen with "Deferred Name Resolution": a stored proc can be created that references non-existent tables. The error will happen at execution time when the stored proc is actually compiled. The "myth" or pre-compiled goes back to SQL Server 6.5. The optimiser changed massively in SQL Server 7+. This can be seen in 2 MSDN whitepapers for SQL Server 2000: 

...then just copy it over, restore. This can be scripted in powershell, perl, cmd.exe etc. When you say "window" I assume you mean 5 hours window on production. It can't be that critical to load development. 

char(3) is the natural key It's short enough that not using it (adding a surrogate key and using joins all over) adds more overhead then having an extra 2 bytes per row over tinyint. Re-phrasing, not using it adds unnecessary opaqueness and code complexity. And from experience, it is just easier to use (3) even with 40k writes/second and billions of rows. 

SQLLite is single user (in practice) and embedded. That fails the "more than one executable to be able to access the DB concurrently" requirement. Otherwise, all RDBMS are in memory once the data is loaded from disk so you can use any you want (SQL Server (not CE), MySQL, PostgreSQL, whatever). 

Usually you'd do weekly or daily depending on your usage and maintenance windows. You pretty much never shrink, especially not scheduled. For rebuild/reorganise and statistics there are scripts (such as SQL Fool's one) that do a better job. Note: reorganise and rebuild are not exclusive but work different ways. 3rd party scripts can choose the best option (based on fragmentation). We use weekly index/DBCC and daily statistics. And never shrink. From Simple-talk: Don't Forget to Maintain Your Indexes Edit: The URL is has â€™ that messes up $URL$ 

This is no single answer The optimiser will choose an index that best suits the predicates of the query. This depends on 

However, you can't build this into a larger query on the local server. UDFs + linked servers + parameters just don't play nice together... 

I'd usually use an OUTPUT parameter because it's the lightest way. Note: If you are inserting several rows into one table and only getting the last IDENTITY there is no guarantee that the previous rows are contiguous 

For SQL Server 2008 R2, "max memory" is for buffer pool only (basically data cache) Other memory is used for other things. This is changed in SQL Server 2012 

The relative cost of the components of an execution plan is not always reliable. The INSERTED scan or deleted scan shouts TRIGGER to me. This is your problem most likely. Are you looping over 400 rows and sending an email in an extreme example? 

2 results sets in a stored procedure. Page on the first (IssueID) then "JOIN" back to the 2nd to get detail tows Add a DENSE_RANK() OVER (ORDER BY issueID) to generate a number for paging 

I started using NORMA (link fixed Oct 2011) which is useful to capture the relationships and constraints in plain English. See Object Role Modelling too. This will generate XSDs and SQL scripts. I hope never to see an ERD ever again... 

This example shows why. XACT_STATE() doesn't register an implied transaction (eg no explicit BEGIN TRAN) 

There is a known issue in some circumstances. As far as I can see, you need to use NO CACHE so numbers cannot be reused 

There is no way to distinguish the meaning of 01/02/2011 and 02/01/2011 if you have this values mixed in the same column. It is that simple. 

A prepared SQL statement will be executed every time too. If you have a fat client then caching is local to the client. This is mostly OK unless you have millions of blobs for example. In our web client we render all the results but only show the top 100 and have a "Show all" button than expands a hidden DIV with rows 101+. We don't cache in the web server and we don't offer paging. 

Note: id remains as primary key to preserve the model. Even if id is serial, it's would be wrong from a modelling perspective to change the PK to (id,topic_id) 

You need to stop SQL Server or even reboot the server Or more likely you could be rolling back a huge UPDATE or such: wait or restart, up to you... Edit: Aaron's comment of changing a DB status may work as well as an intermediate step. 

You do manipulate and restrict data: GROUP BY, ORDER BY, TOP, JOIN, lock hints, etc You just don't change the database state when you do. It boils down to whether you read "manipulate" to include "change state" 

You are running RTM (10.50.1600.1): the latest patch is SP2 + CU3 (10.50.4266) These include patches like KB 2498786 

Personally, I'd use NULLs and avoid empty strings unless there is a good reason for doing so. Mainly for sanity and consistency. Bollocks to theory. 

This dmv is therefore reset for each stored procedure recompile, which can happen, for example, after index and statistics maintenance 

No, unless you logged it via a DDL trigger or such You want to look at who as ALTER rights in that database, or membership of sysadmin/db_owner/ddl_admin role. This would be better as a general review rather than a witch hunt. There are probably other people with rights to make unapproved and unauthorised changes too 

At the end there are some settings to check too: what if OFF at the DB level which overrides an ON at the index/stat level? HTH... 

server option only exists on SQL Server 2005+ to enable legacy mail features. For SQL Server 2000, it doesn't exist because you have to use Agent XPs for mail. There is no SMTP support via Database Mail I really would considering upgrading, given SQL Server 2012 is due in a few weeks. There is no direct upgrade path from '2000 to '2012. 

A quick search of StackOverflow would show you use sys.sql_modules or OBJECT_DEFINITION to search for code on SQL Server 2005+. You can simply restore onto SQL Server 2005+ but not run to allow this. That's if you want fool proof T-SQL. You could concatenate the syscomments rows together if you wish though in SQL Server 2000. I wouldn't personally. Alternatively, use the SQL Server 2008 upgrade advisor to pick out most (maybe all) possible issues (such as ambiguous ORDER BY clauses which caught me out). Note the Red Gate SQL Search sensibly only supports SQL Server 2005+ 

Both posts are wrong syntax. I've -1 in the first from SO and left a comment on the second. Create a table 

High CPU in SQL Server is very often caused by poor indexing Unfortunately, SQL Server 2000 lacks the tools of later versions to track these down easily Saying that, if you run SQL Profiler you will be able to find high CPU queries and start looking at query plans to work out what indexes are missing