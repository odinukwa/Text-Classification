Basically you can use every other representation of rotations, but Euler Angles. Matrices, Quaternions even Axis Angles will do what you want. 

Just store the static orientation difference between parent and child once at startup, lets call it qDiff. Whenever you set the new parent orientation, set child's orientation like this: 

Or use them as the inspiration for your own. Forget about fancy design patterns in this case, here performance is priority No. 1 

Of course it depends on the particular position you are aiming for, there is no general answer, but here is my list about most important skills for programmers as I see it (I should mention my area is Console & Performance): 

If you are using OpenGL, gluSphere does it for i.e. a sphere. Make sure to read the part with gluQuadricTexture at the bottom of the page. 

You can try to contact the publisher, and ask them directly. Google Books states the publisher is "Thomson Course Technology". This page says they were renamed to "Cengage Learning", it also has all contact information, so you can just call or send an email to ask them directly for their ebook offers. 

There is a conceptual misunderstanding in your question. positionX is to the same degree a "core" property as health or speed is. In fact, speed is nothing else than the length of the rate-of-change-vector of the position. This conceptual wrong distinction leads to a bad design. The update- and the render-logic use the same properties, i.e. position, rotation and even health. health can be rendered as a number above the monster for instance, there is no fundamental difference between those properties. The entire question is trying to force a design pattern into data rather than the other way around, drawing a line where no such line exists. 

The most important thing when optimizing is: Profiling. Learn your code from the hard profiling-facts. Don't make any assumptions about your code, profile it! Make automated profiling runs and compare the results in a machine readable format, make them easy accessable in graphical performance charts, so you will more likely know which checkin caused new hitches, or whether optimizations were good or not. 

Both of these vector/matrix-libs have optimized SSE2 code, Sony also has an Altivec for PowerPC compile switch: 

The rotation-axis is not the z-axis, it is the cross product of the normal of the triangle and the z-axis: 

Can you show the code where you check if it has arrived? Do you make a floating point comparison using ? That would not work, check for or Edit: You are right, isn't correct, because the velocity should already be the direction, assuming that the velocity has been set properly. It should only be , but I dont think thats the problem, since it actually arrived as your logging shows. Edit: Instead of: 

where the first 3 terms are the imaginary part (the rotation axis) and the last term commonly known as w, the encoded rotation angle. Following is the code I used to calculate the orienation. If you don't have a linear algebra library yet, this is the point where you either want to roll your own or to download one. glm for instance. Now back to the calculation. The first orientation frame you describe is basically the identity matrix for the opengl camera (facing -z axis, +y pointing up). The second orientation frame is our target orientation: facing +x axis, +z pointing up. Calculating the matrix from those 2 parameters is pretty straight forward, the following function is an example how to do it: 

How do you store the ellipsoid? If it has a position, orientation and the radius in local x- and y-axis, it might be easier to calculate the inverse transformation matrix, that transforms the ellipsoid to a circle. Transform the AABB into that space too using that matrix, then you can make simple triangle-circle collision tests. Edit: In the left image you see the original ellipsoid with the axis aligned box. In the right image both objects are transformed by the ellipsoid's inverse matrix, so the ellipsoid becomes a circle and the AABB becomes 2 triangles. Now its only a matter of 2 triangle-circle tests. 

The simple solution is not to store the orientation of the object as angles around axes (X-, Y-, Z-axis), as for instance in euler angles. Store the orientation of the object as a matrix or a quaternion. This can cause gimbal lock, using euler angles: 

Note the acceleration is constant here, its simply the gravity. EDIT: It seems both, the article on gaffergames and the code in the question use a simpler form of Euler's Method, namely: 

First we need to analyze the rotational part of the matrix: , and . As you can see the right vector () is not in the x-axis anymore , it is in the z-axis . That means it is rotated by 90 degrees around the y-axis. The same happens to the eye-position. Rotating by 90 degrees around the y-axis lets it end up at , voila, there it is. It is and not because the world is moved and not the camera, so the world is moved in the opposite direction, still 300 units away from the camera at . And again, it is moved towards the negative z-axis, because that is where the OpenGL camera looks to, as mentioned above. 

2. Negating each term of an identity-matrix is not a rotation matrix anymore, it is a reflection matrix. Hence, this is no valid input for , it will return a result however, but that is not correct, as that function only works with valid input. So negating each term in a rotation matrix will not result in a rotation of 180 degrees. 

As Gary already mentioned, you need to take the local forward vector of your torpedo into the calculation (not necessarily the 3rd row of the identity matrix, depends on how your model was created). If the local forward vector of the torpedo is different, change it there. 4 things changed in your code: 

If I understand your question right and you want to connect the nodes by springs, that apply forces when the spring is not at its rest-length (in other words when you drag it), then look at Hook's Law, it explains the mechanics and forces of springs. But you will need any kind of physics-engine. 

Note: there is an an anomaly in your example, the normalized vector from eye-position to look-at-point must not be the same as the up-vector, so: 

You are right, you would end up with the same problems. The key is to store the current orientation (matrix, quaternion) of your object and apply only a delta when changing the orientation. When you want to turn 10 degrees around y, just create a delta matrix/quaternion for that and post multiply it with your current orientation (if you use post multiplication for matrices). If you multiply it the other way around, it will rotate the system around world's y-axis rather than around object's y-axis. I find this resource very useful, it also comes with source code and explains the theory very well. 

Given the minimal information in the question, there is no general answer to that. Except: Real games use engines that implement features as scene-graphs and proper data structures for collsion-detection and -optimizations, rendering, network, input, serializing and much more. The idea is to write a game, not an engine. Unity3D is such an engine for C#. 

Note: If the movement is still wrong, then and are most likely absolute values, rather than the relative orientation change in this frame, then the first line would have to be: 

The isometric projection is not so much relevant for the underlying tools you choose, it's just the way (angle, no perspective distortion) you look at things, but that doesn't change the underlying principles of how things behave. That means you don't need to particularly search for isometric tools. The physics for instance is the same, no matter how you look at it. The math of a car crash does not change, whether you look at it from the left or from the right, it does not depend on the observer. The same is true for a bouncing ball, it does not care if you watch it from the left or from top, it will bounce the same way always. If you have particular doubts about how to implement such things in terms of isometric projections, please tell us an example and we can go into details :) 

Messages are a hell to debug and maintain. It sounds good in theory, but once put in practice it gets messy with lots of duplicated data sending around. The jump-sound-effect will need a lot more data at the end, for instance the position, velocity, material the character is on, you name it, the list will be long at the end. So either you will need to collect this data and send it to the AudioManager via a very specific event/message with the data copied in it or you will send a reference to the character in the message, so the AudioManager can access the data, both ways end up messy, and now the audio-manager has to choose a sound for the material-underground etc. So at the end the specific event (which is a very specific class for only this message) will couple those classes very deep again. Not much won and at the end you will have a messy big list of very specific events/classes which only serve the purpose of sending data around, which exists already, and may be out of date and will suffer from all other problems of duplicated data. So there will be a huge list of unnecessary classes to maintain which introduce a deep coupling between the character and the AudioManager, except now it is scattered all over the source-code. Not only in the Character- and AudioManager-classes. Still a good idea to decouple your code, but Messages are really just another way of deep couplings. Some code just has to be coupled, use the most direct way to couple them, don't over-engineer. 

So internal position [0.5, 0.25] is transformed to actual screen position [400, 150] This was the basic stuff. The real advantage of vectors is the application in Linear Algebra where you can use matrices to transform your vertices (rotate, scale, mirror etc), i.e. to easily rotate all your internal position by 90 degrees, or you have to swap the screen-y position 0 from top to the bottom of the screen, because i.e. a third party library that you use, uses this convention. 

Up to these days threading in games can still be described as not very mature. It is mostly used for island calculations like sounds, physics, particle-systems, I/O, path-finding etc, stuff that can be easily outsourced from the main-thread without running into too many locking problems. Yet the main game entity update chain is very hard to distribute over multiple threads with current state of the art engine designs. Unfortunetly there is still no language available on consoles (or in the general game development scene) that has the concept of threads in its very language design, like for instance Erlang has it. This is long overdue in my point of view. The PS3 tried to adress that issue with its SPUs, but overall it's pretty much fair to say that it's concept didn't exactly succeed for the majority of the developers with some exeptions like for instance Mike Acton. Making properly use of the SPUs meant completely rewriting big parts of the engines for only one platform, making cross platform development even harder. Only some PS3 exclusive titles could really afford to go that route. It's very interesting to see where it's going with the next gen consoles, since simply adding more cores only helps if we find better concepts of really making use of them, other than 1 big main game thread and some small low priority worker threads. 

Now to get the matrix that transforms mB-space into mA-space -> and and will be the position of marker B in the local space of marker A Btw. you dont need to pull out x, y and z to pass it to glTranslated, just upload the entire matrix using glMultMatrixd, in order to not only apply the position but also the correct orientation. 

To make it more general than in the comments. There is nothing special in an ECS regarding memory management and object ownership. The same rules apply as to any other application. Personally I'm an advocate of the most restricted version, clear object ownership and an explicit lifetime management for objects and memory in C++. Shared resource management only where it really has to be for instance for Audio-resources, Render-resources etc, but that is the exception. There is also the legitimate opinion that everything has to be smart-pointer-ed and we don't care about life time management. Each of both paradigms comes with its own set of problems, I will focus on the first one. It basically comes down to a Fail-fast system vs. a Fail-safe system. Despite the names, in my experience the first one produces more robust code due to facing errors early and thus solving them is easier and less time consuming when it comes to debugging. Bugs will come in both version, the difference is the visibility, a good read on it is here. So I would go with the easiest version, the responsibility over the lifetime of its attributes has the Entity. This goes for a health attribute as well as for a mesh or sprite or sound. They will make sure all resources that they need are available as long as they need it. If some of those resources are shared like Textures, then it makes sense to use smart-pointers for those special cases. Btw. I don't consider performance implications of smart pointers a valid argument in the discussion whether to use them or not. If you need the functionality they offer, then go for it. Just pick wisely when you need the functionality.