The image above, from panohelp.com, shows the basic idea. For each point on the image, there are multiple rays arriving at that image point, via every point on the 2D lens surface. Therefore, generating an image like this using Monte Carlo will require picking, for each ray, both a 2D sample point on the image plane and an independent 2D sample point on the lens surface. The user-facing parameters to set will be the lens radius (as a physical radius in scene units), which controls how shallow the focus range is (larger lens = shallower focus range), and the distance at which you want objects to be in focus. To generate eye rays into the scene, you can calculate the position and direction of rays leaving the lens surface; in this model there's no need to explicitly simulate the image plane and the refraction through the lens. Basically, think of the lens as centered at the camera position and oriented to face the camera direction. Based on the image location, construct a ray from the camera position (lens center) into the scene, just as you would in the pinhole model; then find its intersection with the focal plane. That's where all the rays from that image location should converge. Now you can offset the starting point of the ray to a randomly chosen point on the lens, and set its direction to be toward the convergence point. You can generalize this a bit by allowing the focal plane to be something other than a plane, or the lens to be something other than a circular disc, and following the same process. That can produce some interesting if not-quite-physical effects. It's also possible to go beyond this simple model with a more physically realistic simulation of a camera's lens elements—but that's beyond my expertise. 

The line you quoted says it: $E_L$ is the incoming irradiance at the surface due to the light source under consideration. Less technically, it's a vector representing the intensity and color of the incoming light at that point. The equation you mention is for a punctual light source (directional, point, or spot light), so in this case $E_L$ would simply be the user-defined RGB color of the light source, multiplied by any relevant attenuation factors—such as shadows, distance attenuation for point/spot lights, and cone angle attenuation for spot lights. 

For explicit light sampling: yep, you just evaluate the BRDF for that incoming direction and the output direction back toward the camera. There's probability involved in the case of an area light: you have to randomly choose a point on the light source, and include a factor to convert from probability density over light source area to a density over solid angle at the receiver point. It looks like you've got that already (that's the $\cos \theta' / (p(\mathbf{x}')\|\mathbf{x}-\mathbf{x}'\|^2)$ in the formula you mentioned). For sampling the reflection ray, you could just pick a random ray from the hemisphere and weight it by the BRDF, the same as you'd do for diffuse. But that won't be very efficient; it'll take a huge number of samples to converge to a low-noise result. What you really want to do is importance-sample the BRDF. This means choosing a random ray not uniformly over the hemisphere, but using a probability distribution that matches the shape of the BRDF (as much as possible). Then you'll automatically get rays that are clustered around the sharp reflection peak in the BRDF when the roughness is low, and spread out more over the hemisphere when the roughness is high. So how do you do it? The paper Microfacet Models for Refraction through Rough Surfaces by Walter et al (which popularized the GGX distribution in graphics) handily gives importance-sampling formulas for several common BRDFs in section 5.2. Their approach is to importance-sample the normal distribution of the BRDF, then leave the Fresnel and geometry factors to be factored into the path weight. For example, their GGX importance-sampling formula is (equations 35–36): $$\begin{aligned}\theta_m &= \arctan \left( \frac{\alpha_g\sqrt{\xi_1}}{\sqrt{1-\xi_1}} \right) \\ \phi_m &= 2\pi\xi_2\end{aligned}$$ where $\alpha_g$ is the GGX roughness parameter, and $\xi_1, \xi_2$ are a pair of uniform random numbers in [0, 1]. The outputs $\theta_m, \phi_m$ are the polar coordinates (relative to the surface normal) of the sampled microfacet normal. So to use this, you'd choose random inputs $\xi_1, \xi_2$, then evaluate the above equations, and use the resulting normal to compute the reflection ray, as well as the Fresnel and geometry factors from the BRDF. 

Forward rendering is the process of computing a radiance value for a surface fragment directly from input geometry and lighting information. Deferred rendering splits that process into two steps: first producing a screen-space buffer containing material properties (a geometry buffer, or G-buffer) built by rasterizing the input geometry, and second producing a radiance value for each pixel by combining the G-buffer with lighting information. Deferred rendering is often presented as an optimization of forward rendering. One explanation is that lighting is fairly expensive and if you have any overdraw then you are lighting pixels that will never be seen on screen, whereas if you store material properties into a G-buffer and light afterwards, you are only lighting a pixel that will actually appear on-screen. Is this actually an advantage of deferred, given that you can also do a depth pre-pass and then do a forward rendering pass with depth test set to or or the equivalent? Deferred rendering also has the potential to schedule better on the GPU. Splitting one large warp/wavefront into a smaller geometry wavefront and then smaller lighting wavefronts later improves occupancy (more wavefronts in flight simultaneously). But you also end up with a lot more bandwidth use (writing out a large number of channels to the G-buffer, then reading them back in during lighting). Obviously the specifics here depend a lot on your GPU, but what are the general principles? Are there other practical performance considerations when deciding between forward and deferred rendering? (Assume that we can use variations of each technique if necessary: i.e. we can compare tiled forward to tiled deferred as well.) 

You can see that the last two images look sharper, but also have the characteristic color fringing. Here's the downsampling pattern used for the last image: 

One way to think of it is that graphics cards often implement non-power-of-2 textures simply by padding them until they are a power of 2 in each direction. This makes most things "just work": tiling and hardware filtering, for example. The only thing that needs to change is the conversion from texture coordinates to image coordinates. If implemented like that, it's obvious how to do mipmapping: nothing changes. Even if you have a GPU that supports non-power-of-2 textures without padding, the mipmap levels would end up with "padding". e.g. a 3x3 texture would have a 2x2 texture as lod 1. 

There's your problem—by generating the rays that way, you're doing something more like a spherical projection or fisheye projection, not a linear perspective projection. The nonlinear projection is the origin of the curvature you see. To get a perspective projection instead, conceptually the way to think about it is that there's an image rectangle floating in front of the camera, divided into grid squares (pixels), and you fire a ray from the camera position toward the center of each pixel. In code, this looks something like: 

Yes, your theory is correct. A gamma-correct blur entails converting the input pixels to linear color space, performing the blur weighting and accumulation in that space, and then converting back to gamma space at the end. As noted in the comments, the actual transform is not literally squaring and square-rooting, that's just an approximation (and not that good of one). For the true sRGB gamma transform, see the equations in this Wikipedia article (look down the page for the equations involving $C_\text{srgb}$ and $C_\text{linear}$). By the way, some visual comparisons of gamma-correct and gamma-incorrect blurs can be found on this page by Elle Stone, which shows why this whole thing matters. 

A bicubic filter (e.g. Mitchell-Netravali) could capture the shape of the sinc even more precisely, including its first two negative lobes. The reality of filter selection is a bit more subtle than "approximate sinc as well as possible", since there are different kinds of artifacts that can be generated by "non-ideal" antialiasing filters, such as aliasing, overblurring, and ringing. Also, different filters may be more or less computationally expensive. So it's a game of trying to trade off the different artifacts against each other and against performance. Different scenes/images may favor one choice or another, and it's also partly an aesthetic judgement. As for why smallpt uses a tent filter in particular, I would guess for a combination of performance (it's a quick filter to evaluate) and brevity—it can be done in a couple lines of code, while a bicubic filter would take a bunch more code. Incidentally, smallpt actually uses a 2x2 subpixel grid and places a tent filter at each subpixel, then averages together the results of the four subpixels. So the overall effect is, curiously, that of the sum of four tents, which ends up looking like a pyramid with a flat top: 

If your goal is to learn GPU programming, it doesn't matter at all whether you have DDR3 or GDDR5 memory. The way you program it isn't going to change based on how fast the memory is. It will affect performance, but if that's not a primary consideration for you, then you don't need to worry about it. Do make sure that you get a GPU that supports the latest graphics APIs (DX12 and OpenGL 4.5). I think that's essentially all GPUs made in the past 5 years or so. 

Substituting the definition of $D$ into (4) gives: $\int_{\Omega'} D(\omega_m) d\omega_m = \int_{\Omega'} \int_{\mathcal{M}} \delta_{\omega}(\omega_m(p_m)) d p_m d\omega_m$ A fundamental property of the Dirac delta function is that it integrates to 1 over its domain. Unfortunately, we are integrating over $\Omega'$, not $\Omega$. Here I will stop pretending that I am actually able to use formal mathematical terms, and I will just note that the integrand has non-zero values only where both $d p_m \in \mathcal{M'}$ and $d\omega_m \in \Omega'$, which is what (3) says. As long as at least one of our integration domains is restricted to the set we care about, we will get the same result. So I'll just move the prime around a tad: $\int_{\Omega'} D(\omega_m) d\omega_m = \int_{\Omega} \int_{\mathcal{M}'} \delta_{\omega}(\omega_m(p_m)) d p_m d\omega_m$ Now we can apply the Dirac delta identity, since we're integrating over the whole domain: $\int_{\Omega'} D(\omega_m) d\omega_m = \int_{\mathcal{M}'} d p_m$ Ta-da! 

You could try some form of color quantization algorithm, which generally extract the most dominant N colors. The one I've seen referenced most is modified median cut quantization [.pdf], which is based on median cut quantization [.doc]. The benefit to this kind of algorithm is that instead of simply averaging every color in the image, it extracts and discards other highly prominent colors instead of allowing them to pollute the average. The concept is that color space (RGB space) is partitioned into 3D axis-aligned rectangular regions (the paper calls them vboxes) and iteratively subdivided by splitting vboxes, attempting to leave half of the pixels on each side of the split. The result is color clusters that should correspond to color clusters in the image. The largest color has a strong likelihood of being "perceptually similar" to the image. There's a JavaScript implementation and demo of this algorithm called Color Thief. 

In a path tracer, when tracing a ray to a light with a shape other than a point ("area light" is the usual terminology), you generally select a random point on the surface of the light. When enough samples are taken, this results in both softer specular highlights and softer shadow penumbras: light will be reflected off a surface from a random distribution of incoming directions, and rays cast towards an area light with an obstruction in the way may or may not intersect the obstruction depending on which point on the light was sampled. In real time, approximations to area lights are used. This Unreal Engine 4 white paper describes some of the considerations. One possibility is the "representative point" method, which per-pixel selects a point in the light's volume (brightest or nearest) and uses that as though it was a point light. Another option is modifying the roughness that is fed into the specular calculation based on a cone that approximates the area light (in the paper as "Specular D Modification").