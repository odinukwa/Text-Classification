To make an application, OpenGL or not, run for extended time safety-critical applications are mostly built to restart on error, be it a driver bug, your application is broken, or hardware failure. If function during the downtime is required you need redundnacy. In your case maybe start multiple sessions of the same program, or even having dual computers running the same program outputing to the same display. OpenGL is no different from other applications, BUT vendors optimize and test for certain kinds of customers. Your scenario is not common so be suspicious. 

Depending on your renderer you could make the surfaces more "fuzzy" applying a miniscule offset in multisampling depth using a noise like algorithm. This should result in a sort of blending effect for surfaces in close depth proximity. For what I know setting per-sample depth in fragment shader is quite recent in OpenGL and then only as an extension. OpenGL has the PolygonOffset but requires knowing in advance you are about to render something coplanar. While not feasible for surfaces within a single model it could work nicely for example when rendering a road overlaid on a piece of land. 

When cylinder and frustum intersect the hole will be a view into the internals of the cylinder. The hole is where external of the cylinder occlude frustum and internal of the cylinder does not occlude the frustum. Knowing this you should simply do the following to find the surface (or rather volume) made from the hole. Render frustum without color and set depth accordingly. Set a stencil bit where the cylinder (external) occludes the frustum, without setting new depth. Clear the previously set bit where the internal of the cylinder occludes the frustum, without setting new depth. Alternate between internal and external using CullFace. The stencil mask you now have is the surface where the frustum hole is. Render the frustum (in color) masking it with the stencil. Make sure to use LEQUALS depth test to allow the frustum to render on top of itself. Voila. Note that the depth buffer holds the value of the unmasked frustum so If you need to render something within the hole you would have to render the whole scene again and mask it to only render in the hole unless depth test passes against the frustum. Left as student exercise. 

So thanks to joojaa I finally got a hint, and I searched on the net further and found this link which cleared all the doubts and has my answer. Though I am still posting it here as a summary. So anyone reading this and who has similar problem to mine here is what I understood. Suppose we are considering the tait-bryan angle order X-Y-Z that is rotate first along X then Y and finally Z. Also to make it clear we are rotating around the "fixed" axes since a rotation matrix always rotates around the fixed axes. The matrix doesn't know anything about the axes moving or anything. 

First of all we need to understand why do we need 4x4 matrices in the first place. With 3x3, we couldn't represent translation as it wasn't a linear transformation (it displaces the origin). So in order to avoid extra work, homogeneous coordinates and affine transformation was introduced. Now instead of doing $v' = Lv + t$ where is a linear transform and is the translation, we can do $v' = Av$ Where is the affine matrix. This makes it cleaner. So 4x4 matrices are a real necessity, we just can't work without them. In order to distinguish between vectors and points we use for points and for vectors. So you are suggesting to make this 4th dimension implicit and don't store it as it'll actually use space/memory. 

Next is differential radiance. We can think of it as an infinitesimal quantity of radiance emitted or recieved in a very small solid angle $d\omega$. Next is Irradiance. Irradiance isn't normally associated with a direction. According to Wikipedia it's 

I Recently posted this question on SO but didn't got any response so i thought to post it here since it's somewhat related to Raytracing. I am making a real time ray tracer in OpenGL using Compute Shaders for my project and was following this link as a reference. The link tells to first draw a full screen quad, then store all the individual pixel colors gotten through intersections in a texture and render the texture to the quad. However i was thinking can't we use Frame Buffer Objects to display the texture image instead of rendering the quad and save the over head? Like I save all the colors using ImageStore and GlBindImageTexture in a texture, then attach it to a FBO to display it. And since I won't be using any rendering commands I won't be causing a Feedback loop as in writing and reading the same texture? Here is the snippet 

My interpretation of this is may perform worse than if the index count or implicit range exceed that of the implementation dependent GL_MAX_ELEMENTS_{INDICES|VERTICES}. For platforms where is always superior I would expect to ignore and all together, and the value of GL_MAX_ELEMENTS_{INDICES|VERTICES} to be "infinite". Correct usage according to spec is therefore whenever exceeds GL_MAX_ELEMENTS_{INDICES|VERTICES} and does not. For reference, on my GTX 970 the value is 1048576 for both. 

While I have no idea how UnrealEngine handle depth I have thought of a technique which would result in that kind of depth output. The idea is to better manage Z-fighting by rendering multiple (stacked) frustums with shorter distance between ner and far planes. Each step in the cascade would represent one of those frustums. Using near/far plane clipping, stencil masking and optional vf-culling you can easily render the cascade front-to-back, or each frustum to each own framebuffer and consolidate afterwards, with only minor hit in performance. This should scale better than increasing depth buffer bit depth which has diminishing returns. Besides not all platforms support high bit depth. OpenGL ES require only 16 bits! It is just a theory though :-) 

I'm finding myself increasingly interested in rendering techniques utilizing multisampling for anti-aliasing (obviously), out-of-order transparency, volumes etc. Searching the web for performance numbers of MSAA in the range only gets me to gamer verdicts for this and that game, and even then MSAA is only used for AA, and being compared to screen based post process AA which can hardly be used for anything else than AA. Is there any rule of thumb (other than memory consumption) of how much it costs having x2, x4, x8, x16 MSAA? Linear? Logarithmic? 

I have a white line on black background. Because of the pixel geometry the line does not look really white; one edge looks red-isch, the other blue-isch. Other fore- and background yield other sensations. Pixel geometry is striped RGB, DPI is ~90, viewing distance about an arms length. Is there any software based solution or maybe visual design that makes this less appearant, assuming we know the screen pixel geometry in advance? 

That's what I want to hear about more. Why "non-orthogonal" ? I can't see any problems with making the axes orthogonal? What does making the axes non-orthogonal give us? 2) After the above one has been answered. How is this all tied up to rotational matrices? How can we achieve gimbal lock through the matrices when they only rotate a given point/vector around global axis. For example If i multiply a column vector with a Rotation matrix around X-axis and then with It will rotate around the global X-axis first, then global Y-axis second. So how can I achieve the lock situation using matrices? EDIT:- To make it more clear I've also heard about rotation orders like in the order when Y axis rotates then and rotate with it but when rotates only rotates with it. thus making the axes non-orthogonal. I am assuming that's what it means by non-orthogonal mentioned in the wiki article. Here is a picture. 

I think you are confusing all the binding targets thingy. From what I see your vertex data is coming from compute shader after some processing and now you want to pass it to the vertex shader. You can create a buffer object once, use it as an SSBO for use in a compute shader then use it as a VBO for use in rendering. i.e 

Now we have to rotate around Z as this is the last in the order. But notice that the rotation around is similar to the previous rotation we performed around . The plane will roll either towards the left or right. This means we lost 1 degree of freedom. I think this is what joojaa was trying to explain to me. So everywhere where people are saying the axis coincide its the local, moving frame coinciding with the original frame. Initially both the local and global frames are coinciding. In this case when we rotated around by 90 the local moving X-axis collapsed on to the fixed original - axis causing this gimbal lock 

Obj files give pre-calculated normals which are in model coordinates. So I wonder if they have to be transformed into eye coordinate (where I use them to calculate lighting)? I'm thinking yes because it makes sense, but only rotation and scale because translation doesnt really make sense for free vectors. 

I have figured out the solution to this, and want to share it. What I am doing is I calculate different barycentric coordinate for each of the step I need to do: z-buffer, interpolation of normal, etc. The correct way is to use the barycentric coordinate that is calculated when checking z-buffer to test whether a pixel inside a triangle. Because that barycentric coordinate is guaranteed to be inside the [0, 1] range (otherwise the pixel wouldnt have been drawn), it is the correct barycentric coordinate to use. 

I am reading a model from an obj file and draw it using glDrawElements. For some reasons, only a small part of it is drawn, even though I dont even have back-face culling turned on. 

We can pretend that the 3 dimensional vector is 4 dimensional (imaginary w = 1). This is after all Computer Science, not math. When multiply matrices with vector, we don't need to crush z. Keep it and use it. When drawing, we can pretend that z is already crushed. You will have to do division by w after projection matrix anyway. Why not divide by z then and get something like [x/z, y/z, z] (I'm not dividing by z because it will be used for z-buffering). Both of them cost 1 computation. 

All the books and reference I have read say that the view vector is calculated by subtracting the point where eye is at, from the point where we want to calculate light. But since, eye is at (0, 0, 0) the view vector would be just the negative of the point where eye is at? Is my understanding correct? 

Sort the particles by Z each rendering cycle using an algorithm such as bubble sort which is good when element changes position in small steps. If the perspective does not change much the errors would be few enough over time to be unnoticable. The technique is easy to configure between quality and performance dependning on the target platform by adjusting how much sorting is performed each cycle. 

I display text using OpenGL which is aligned with (2D) objects that can have any rotation on display, making the text also appear at any angle on display. So far I've used FreeType to render raster for each supported symbol to file and load the files into OpenGL as textures at application start. When the text is rotated (other than 90 degree increments) the hinting goes all bad. My plan so far to remedy the situation is to render a few rotated (transformed) variants of each symbol and linearly interpolate between the two closest variants. As an example I may render variants in 15 degree increments between 0-75 degrees inclusive, and if text is rotated by 5 degrees on display I'd take 1/3 from the 0 degree raster and 2/3 from the 15 degree raster. ...but this is only a theory of mine. Should work if having small enough increments, but if there are too many of them I'll run out of memory resources and making the shader awfully inefficient. I imagine the increment count depends on the pixel size of each symbol which could turn problematic... So I reach out to the expert community: How to render rotated text with proper font hinting in real-time? Using OpenGL ES 2.0. Font size ~16pix. If you need more info ask away. 

I´m looking into uses of high amount of samples in multisampling. If I have an RGBA8 framebuffer and render to it using multisample with many samples, say 32, dithering on per-sample level should yield a reasonably accurate 8-bit color from the 32 samples in 4-bit color when resolved. The spec (4.5) seems a bit scary though (glBlitFrameBuffer): 

This is the simple case for just a projective transformation. Consider the vector going through 5 or 6 transformations and in the last comes the projective transformation. If we pre-multiply all these transformation to create a single matrix, you will notice that now when we multiply the vector with this combined transformation matrix the division factor isn't just a simple value. The 4th row of the matrix won't be as in the standard projection matrix. It might have changed due to multiplying all the transformations together. Now when you multiply that 4th row with the 4D vector, you will get your value by which you need to divide now. 

To be honest, terms like these are very confusing as they aren't clear cut and on one side of the border. They are more grayish. I'm gonna tell you how I convinced myself, as I too had this confusion as soon as I read your question. But I managed to convince myself through this argument. First of all we are gonna clear up 4 terms, Radiance, Irradiance, Differential radiance and Differential Irradiance. "Radiance" is what you say associated with a certain direction. To be more formal and according to wikipedia, 

If we rotate around the X-axis we can see that the plane will either roll left or right. Suppose we rotate a little around the X-axis and then rotate around so that it is pointing straight up like this 

In the book Computer Graphics Principles and Practice, they use the term specular reflection when they want to imagine things resembling a mirror and glossy reflection when things like a polished door knob or an orange skin. The charts shows you exactly that. When a material has more specular color, it should have less diffuse color due to the conservation of energy. That is, the sum of the light reflected specularly and light absorbed and emitted in random directions must be less than equal to 100% (the amount of light incident on the surface). Hence when you increase the specular color the material tends to go white or have a slight tint of the color like in metals. Where as glossy surfaces can have more diffuse color but show a specular highlight like the surface of an orange skin. So assuming the CGPP's point of view, we can say in pure specular reflection, the diffuse part is much less than the glossy part. Where as in glossy reflection the diffuse part is usually greater.