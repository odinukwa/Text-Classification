This is known as the rectilinear crossing number $\overline{\mathsf{cr}}(G)$, which is the minimum number of crossings among all possible straight-line drawings of the graph $G$. Compare to the normal crossing number $\mathsf{cr}(G)$, one can see that $\overline{\mathsf{cr}}(G) \geq \mathsf{cr}(G)$. And your question is essentially as the same as asking whether $\overline{\mathsf{cr}}(G) = \mathsf{cr}(G)$ if $\mathsf{cr}(G) \leq k$ for some constant $k$. In the paper Bounds for rectilinear crossing numbers, Bienstock and Dean proved that 

If the required number of times we need to cover an element is 2, we have the following densest k-subgraph problem: (Imagine the edges are elements and nodes are sets.) 

In complexity class $\mathsf{P}$, there are some problems conjectured NOT to be in the class $\mathsf{NC}$, i.e. problems with deterministic parallel algorithms. Maximum Flow problem is one example. And there are problems BELIEVED to be in $\mathsf{NC}$, but a proof is not found yet. Perfect Matching problem is one of the most fundamental problem raised in graph theory: given a graph $G$, we have to find a perfect matching for $G$. As I could found on the internet, despite of the beautiful polynomial time Blossom algorithm by Edmonds, and a RANDOMIZED parallel algorithm by Karp, Upfal and Wigderson in 1986, only a few subclasses of graphs are known to have $\mathsf{NC}$ algorithms. In Jan. 2005 there's a post in the blog Computational Complexity that claims it remains open whether Perfect Matching is in $\mathsf{NC}$. My question is: 

Nondeterministic Strong Exponential Time Hypothesis (NSETH) asserts that for every $ε>0$, there is $k$ such that $k$-TAUT (the complement of $k$-SAT) is not in $\mathrm{NTime}(O(2^{(1-ε)n}))$ ($n$ is the number of variables; the number of clauses is poly(n), though $\tilde{O}(n)$ might suffice). A nonuniform strengthening asserts that $k$-TAUT does not have nondeterministic i.o.-$O(2^{(1-ε)n})$ circuits. The strengthening implies that $\mathrm{BPTime}(O(n))$ (or just $\mathrm{coRPTime}(O(n))$ with $O(\log n)$ randomness) does not have nondeterministic i.o.-$O(n^{2-ε})$ circuits. However, the plausibility of NSETH is not clear, and especially its non-uniform strengthening perhaps sounds too strong to be true. The connection of NSETH with BPTime is proved in (Williams 2016). Note: In fine-grained complexity, $\mathrm{Time}$ usually refers to RAM-based machines, but the above discussion also applies to multitape Turing machines. 

Your description looks correct. The equivalence between deterministic models (under essentially linear time translation) for algorithms using $t^{o(1)}$ space (if the input size is $t^{o(1)}$ or we have random access to the input) is a nice observation. (However, we do know yet know whether randomized algorithms might be faster (even for $t^{o(1)}$ input size), and quantum algorithms are likely to be faster and might even violate extended Church-Turing thesis.) Two other cases of essentially linear translation: 

The machine executes Code for up to Time steps (Code and Time are obtained from the privileged data tape). The code is executed and each step is counted as if we initially set the code to Code (and the data tape to its current state.) All instructions (including TimedVirtualize) are permitted. The code sees the privileged data tape as initially empty. The previous privileged data remains unchanged (but the hierarchy theorems would also hold if it were erased instead). TimedVirtualize returns a value coding Timeout/Accept/Reject depending on whether the code timed out or called an Accept/Reject instruction (that would otherwise Accept/Reject the input). (optional extension) an optional space bound can be set. Full-speed condition: The number of steps TimedVirtualize(Code, Time) takes equals the number of steps Code took (which is ≤Time), plus Time-independent overhead, plus $O(\log(\mathrm{Time}))$. A number of features can be added to TimedVirtualize, but they are not needed for the hierarchy theorems. For example, we might want to set initial privileged data, get the time spent by Code, allow unbounded time, or (optionally) do nonvirtualized execution that replaces the code and (visible) privileged data (thus saving space). 

So you are looking for the circumference of a connected cubic bridgeless graph $G$. Since a bridgeless cubic graph is immediately 2-connected, Bondy and Entringer showed in the paper 

he prove that $\mathsf{BPL} \subseteq \mathsf{ZP^*L}$, which is rather surprising because it seems that two-way access may be stronger than the one-way access. 

I'm interested in but not familiar with this topic. Searching for "Average case complexity theory", I found a thesis written by Tomoyuki Yamakami: 

We can do $2\sqrt{n}-1$; consider the complete $\sqrt{n}$-partite graph, as long as there are two parties both with more than one node inside there is an induced $C_4$, so it cannot be inteval. Therefore we have to remove at least $(\sqrt{n}-1)^2 = n - 2\sqrt{n} + 1$ nodes to destroy all the induced $C_4$. 

Distance preserver is also known as an emulator; many related work can be found on internet by searching the term spanner, which requires H to be a subgraph of G. But in my applications we can use other graphs as well, as long as H preserves the distances between T in G. 

Let $G$ be an $n$-node graph that can be decomposed into two disjoint union of spanning trees. In particular, $G$ has $2n-2$ edges. It is not hard to show that the girth of $G$ is at most $O(\log n)$. However, I don't have an example with girth bigger than $4$. 

Sliding blocks is PSPACE complete even in its simplest form involving 1x2 and 2x1 blocks (without rotation or fractional positions) in a rectangular area, with goal being to move a designated block to the designated space. See "PSPACE-Completeness of Sliding-Block Puzzles and Other Problems through the Nondeterministic Constraint Logic Model of Computation" by Robert Hearn and Erik Demaine. However, that paper does not give a more precise characterization than PSPACE. Is existence of a solution in sliding blocks (deterministic) linear space complete? One choice of completeness is to use polynomial time linear space reductions with at most linear increase in instance size, but gadget based reductions also work in LOGSPACE and might even work in uniform linear size AC$^0$. Offered using Q/A format. 

As Peter Shor's answer notes, to rule out membership in a comparison-based model, we need to know how the element compares with every member. Thus, using $k<n$ random queries (the number of members smaller than the queried nonmember is random), we gain $Θ(n \log k)$ information relative to having $n$ unsorted values. Therefore, for some constant $c>0$, using $c \, n \log k$ preprocesssing, we cannot have $≤c \, n \log k/k$ query cost. This is optimal up to a constant factor since we can sort the data into $k' = k / \log k ≤ n/\log n$ approximately equal buckets (each bucket unsorted) in $O(n \log k') = O(n \log k)$ time, which allows $O(n/k')$ query cost. In particular, using $O(n)$ preprocessing, we cannot have $o(n)$ query cost. Also, $o(n \log n)$ preprocessing corresponds to $k$ in $O(n^ε)$ for every $ε>0$ and thus $Ω(n^{1−ε})$ query cost. 

(There may be a Karp reduction, but if we allow a Cook one, consider the following reduction: Replacing the given degree d node into a complete subgraph of size d with proper outgoing edges. Then for each edges in the complete graph we can query the oracle that solves Problem P. Note that a chordless even cycle passing through the given node corresponds to a chordless odd cycle of length greater than 3 passing through one of the edges in the complete graph.) Now for the main reduction. Given an instance of Problem P, first we detect if there are any triangles passing through $e$; if so, delete every node that forms a triangle with $e$. Note that deleting nodes that forms a triangle with $e$ will not removing any chordless odd cycles passing through $e$ (by the chordless property). Next, for each edge $f$ other than $e=(u,v)$ we add an auxiliary node $v_f$ and two edges $(v_f,u)$ and $(v_f,v)$. Observe that the new graph $G'$ has the following property: 

The answers here already cover most of the nice suggestions about intuition. Still I would give it one more, which is usful when developing intuitions during paper writing. This is suggested by my own teacher, Hsueh-I Lu, which I found it very useful. Whenever a result is written down, and the correctness seems to be verified, rewrite the whole article. This time we have to enforce ourselves not to use any words or definitions similar to the previous versions. This makes us think in a completely new different way, and new intuitions will develop. Also, perturb every parameters used in the paper, see if any set of parameters differ from the one we used originally works still. Often some mistakes exposed when rewriting the article. Come up with new ideas to overcome them. Finally, after rounds of rewriting, we will have a nice round intuition about our own result, and we won't be too optimistic/pessimistic to the power of the new ideas presented in the paper, since we're tried for a couple of times, and it is clear that what is working and what is not. The same method works if you are reading a new paper, and wants to get some more intuitions other than the one given by reading. 

Here is a counterexample, i.e. a language with an $O(n^{2+ε})$ algorithm (using multitape Turing machines) for every $ε>0$, but not uniformly in $ε$: Accept $0^k 1^m$ iff $k>0$ and the $k$th Turing machine halts in less than $m^{2+1/k}$ steps on the empty input. Other strings are rejected. For every $ε$, we get an $O(n^{2+ε})$ algorithm by hardcoding all sufficiently small nonhalting machines, and simulating the rest. Now, consider a Turing machine $M$ deciding the language. Let $M'$ (on the empty input) be an efficient implementation of the following: for $n$ in 1,2,4,8,...:      use $M$ to decide whether $M'$ halts in $<n^{2+1/M'}$ steps.      halt iff $M$ says that we do not halt but we can still halt in $<n^{2+1/M'}$ steps. By correctness of $M$, $M'$ does not halt, but $M$ takes $Ω(n^{2+1/M'})$-steps on input $0^{M'} 1^{n-M'}$ for infinitely many $n$. (If $M$ is too fast, then $M'$ would contradict $M$. The $Ω(n^{2+1/M'})$ bound depends on $M'$ simulating $M$ in linear time and otherwise being efficient.) 

The square-root barrier for finding primes is likely a number theory problem rather than a computational problem. The naive algorithm (with a polynomial time primality test) for finding the least prime above $x$ is conjectured to be polynomial time (in $\log x$). A provable deterministic algorithm finding a decimal $k$-digit prime in time $O(10^{k/2-ε})$ using RAM but not TM could potentially come from some memory-intensive partial derandomization that does not answer the question of $O(n^{1/2-ε})$ upper bound between consecutive primes. For SETH, the convention is to use RAM machines. Otherwise, its key application, the fine-grained complexity lower-bounds, would not work for RAM machines. It appears open whether any of the implications SETH ⇒ "SETH for TM" ⇒ "SETH for $2^{o(n)}$ space" can be reversed. Also, under SETH, for a number of problems, the best exponent is the same for TM and RAM. For example, for minimum edit distance, the essentially quadratic time algorithm can be made to work for TM while the i.o. essentially quadratic lower bound works even for RAM. 

If the edges are permitted to be laid both inside and outside the circle, then it is called the 2-page graphs; if edges can only be laid inside the circle, it is the 1-page graphs, which is also know as the outerplanar graphs. See the book embedding entry in Wikipedia for more information. By your comment, I guess the term you're searching for is outerplanar, since the complete graph on 4 vertices is 2-page. Outerplanar graphs can be recognized in linear time; see 

See the paper for details. Some other papers by Mihai are relevant and nice, too. UPDATE: I found that his PhD thesis "Lower Bound Techniques for Data Structures" providing lower bounds for many central data-structure problems using the techniques he developed. It certainly worths a read. 

There is a close connection between sub-exponential time solvability (SUBEPT) and fixed parameter tractability (FPT). The link between them is provided in the following paper. 

Let $\mathsf{ZPL}$/$\mathsf{RL}$/$\mathsf{BPL}$ denote the classes of the languages which are accepted (with zero/one-side/two-side error) by a logspace Turing machine with one-way access to the random tape. Let $\mathsf{ZP^*L}$/$\mathsf{R^*L}$/$\mathsf{BP^*L}$ denote the corresponding classes by replacing one-way to two-way access. We have the normal inclusions $\mathsf{ZPL} \subseteq\mathsf{RL}\subseteq\mathsf{BPL}$ and $\mathsf{ZP^*L} \subseteq\mathsf{R^*L}\subseteq\mathsf{BP^*L}$. In the following paper, 

We prove that the problem is NP-hard even in its decision form, i.e. ''Is the input graph $G$ already a chordless odd-cycle completion?'' by reduction from the following problem: 

To add to the previous answers, this problem is not only undecidable but $Σ^0_2$ complete. Thus, it is undecidable even if the decider has an oracle for the halting problem. To clarify the completeness, while the P-time promise condition is also $Σ^0_2$-complete, there is a decidable set of codes $S$ such that all machines in $S$ are polynomial time and the $O(n^2)$ question is $Σ^0_2$ complete on $S$. To prove this, choose a $Σ^0_2$ complete $φ$, $φ(x) ⇔ ∃k ∀m \, ψ(x,k,m)$ with $ψ$ polynomial time computable (for binary numbers). Then $φ(x)$ holds iff the following machine is $O(n^2)$ where $n$ is the input length (the machine only cares about the input length): for $k$ in 0 to $n$:     if $∀m<n \, ψ(x,k,m)$: # tested using a loop         halt     wait for $n^2$ steps halt Note that for every not-too-small $c$, whether a program always halts in (for example) $≤n^2+c$ steps is $Π^0_1$-complete, but asking about bounds in a robust manner gives $Σ^0_2$-completeness. 

When algorithm asymptotic runtimes are given without explicitly noting the computational model, what is the convention for the exact model used? My understanding is that most problems use unit-cost RAM, but some (such as integer multiplication) use log-cost RAM, and for some others it depends on the author. Unit-cost appears simpler to map to commonly used algorithms (hence its popularity), while log-cost RAM appears more accurate as a true measure of compute. For log-cost RAM, it appears there is a standard up to linear time equivalence. For unit-cost RAM, there are different possibilities (in order of decreasing model power): a. (naive) unlimited word size with multiplication b. unlimited word size (with addition but not multiplication) c. log-limited word size (with different conventions) d. pointer machines (also called storage modification machines; this is a natural model but appears more restrictive than unit-cost RAM). I think the convention is (c), but even here we have different choices. A sensible choice for $\mathrm{TimeSpace}(t, s \, \mathrm{cells})$ with $s≤t$ is to have cells $0..O(s)$ each storing a number $0..(s+\mathrm{inputlength})^{O(1)}$, with $\mathrm{Time}(O(t)) = \mathrm{TimeSpace}(O(t), O(t) \, \mathrm{cells})$ (assuming $t$ is above the input length; I am not sure if using $t^{O(1)}$ cells gives an equivalent model). However, I do not know whether this is the standard, whether there are other subtleties, or if the unit-cost RAM is undefined for algorithms using more than polynomial space. 

This is called the Bézout's identity/lemma (not to be confused with Bézout's theorem in algebraic geometry), which states: 

Hence we can assume the theorem holds on the larger n, apply the second observation, and concludes a contradiction by the first case, by setting n' satisfies $({n' \atop k})$ > K > $({n'-1 \atop k})$; such n' must exist by the fact that $({n \atop k})$ > K and K > $({k \atop k})$, n' must lie between n and k+1. 

And the bags can be taken with size $k+1$, with a total $k(k-1)$ bags. I'm not sure if this is what you want, so feel free to do it if you modify the definition of "optimal". 

First we prove the theorem is false for all k > 1, K > 1, and any n satisfies ${n \choose k}$ > K > $({n-1 \atop k})$. In order to construct a counterexample, for any large N and A = [N], we have to construct a coloring function f such that for all n-subset A' of A, if B' consists of all k-subsets of A', some of the K-subsets of B' have different colors. Here we have the following observation: 

I've been trying to prove such a lemma with some approaches, but despite of using the exactly same techniques (thus the same requirements) which is clearly no better than the original one (since with the same requirements you can indeed isolate a unique subset), there is no success to reduce any conditions we need, even in the case that we only need a loose bound on the number of minimum subsets, say polynomial. (please read on if you need motivations to the problem! The part below contains some technical details to the usage of the lemma, and some applications to log-space computations.) 

To refine the $\mathrm{S}_2\mathrm{P}$ answer, for every $k≥1$ and $c$, either * The 3-SAT search problem does not have $\tilde{O}(n^k)$ circuits, or * Some problem in $\mathrm{O}_2\mathrm{P}$ with time (and witness size) restricted to $\tilde{O}(n^{k^2})$ does not have i.o.-$O(n^k (\log n)^c)$ circuits (i.o. means infinitely often). If in place of 3-SAT search problem, we used the decision problem, $\mathrm{O}^2\mathrm{P}$ time $\tilde{O}(n^{k^2+k})$ suffices, and if we used the decision problem for bit $i$ in the lexicographically minimal assignment for 3-SAT, $\tilde{O}(n^{\min(k^2+k,k^3)})$ suffices. One decision problem not computable with i.o.-$O(n^k (\log n)^c)$ circuits is the least number $N$ (queried using its binary digits) that is not the truth table of a circuit with $n^k ⌊(\log n)^{c+1}⌋$ gates. If NP is in P/poly, the problem has an irrefutable oblivious witness consisting of the following: (1) $N$ (2) a circuit that given $N' < N$, shows that $N'$ has a sufficiently small circuit. (3) (only used for the $\tilde{O}(n^{k^3})$ bound) a verifier that enables us to run the opponent's circuit for (2) only $O(1)$ times (getting 1 bit per run). On a separate note, for every $k$, there are decision problems in (MA ∩ coMA)/1 that do not have $O(n^k)$ circuits. '/1' means that the machine gets one bit of advice that depends only on the input size. Also, the string Merlin sends can be chosen to depend only on the input size (with this restriction, MA is a subset of $\mathrm{O}_2\mathrm{P}$), and the advice complexity $Σ_2^P$. The proof (Santhanam 2007) generalizes IP=PSPACE and PSPACE⊂P/poly ⇒ PSPACE=MA by using a certain well-behaved PSPACE-complete problem and padding the inputs to get minimum circuit sizes that are infinitely often between $n^{k+1}$ and $n^{k+2}$, using advice to detect enough examples of such $n$, and for these $n$, solving the padded problem by having Merlin produce such a circuit.