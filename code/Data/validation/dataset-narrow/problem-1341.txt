The MSDN documentation for the class is probably worth a read, and if you aren't familiar with templates -- which is what uses to allow it to store "anything" -- you should brush up on the basics there as well. Which also leads me to a final point: while it looks like a vector can store anything, it does have constraints for what is allowed in it and these sometimes throw beginners. In particular, the type you store in a vector must be copyable, because the vector will need to make a copy of the objects when resizing its internal storage, for example. C++'s rule of three is something to keep in mind here. 

No, you shouldn't. While you should build systems that can allow you to track currency transactions (especially if that currency is a real-money obfuscator), you don't need to track every individual unit of that currency. The thing you are interested in tracking is the aggregate movement of funds, particularly in large volumes (as this could be an indicator of potential external/unsanctioned real-money transactions). Tracking individual units of currency doesn't help with that (in fact it may hurt), and only complicates the upkeep of the relevant data by requiring you to generate lots of IDs and store relatively massive volumes of information in your database. There is some value to this idea for real-money obfuscator currencies (that is, thing you buy with real-world money that are used to purchase in-game content, such as the gems in Guild Wars 2). In particular, this desire might arise in order to disambiguate between units of currency introduced into the economy for "free" versus those that were introduced via an actual real-world transaction. Generally you can only ultimately recognize revenue on the latter, so it's important to keep them distinct. However, you can track the information you need simply by storing the numbers in aggregate (total units in, total units out, for both 'free' and 'paid' instances of the currency). This goes along with the general idea that you want to track these things in aggregate, generally a the transaction level and not the current level, as it both tracks the data you really need and makes it easier to trace large chunks of funds as they disperse throughout your system. 

The latest SDK version, as of this writing, is the June 2010 SDK. It contains what you'd need to build against both 9.0c and 11. 

This I don't know much about, never having really used Python 3. I don't expect Python 2.X will vanish any time soon, however, so I don't even suspect the lifetime of your game would be such that you'd need to transition over if you didn't want to, which makes this concern something of a non-issue. As others have said, you're worrying too much about the language selection aspect of your project. Python is a powerful, useful tool for developing games and if you know it and like it you should develop with it. It doesn't matter how fast your game runs if it never gets finished because you worried too much about the language. 

And so on. There are really several reasons this can happen, and walking through all possibilities isn't really viable here. The correct solution is to ensure that is non-null before you access it. The way to arrive at that solution is to learn how to use your debugger to do things like set breakpoints and examine the state of variables, as well as to understand the context of your code and the differences between objects and their types. 

Any kind of diagramming/graphing tool should be able to handle this. OmniGraffle on the Mac is an excellent choice -- options on Windows include Visio, Dia or the drawing program that comes with OpenOffice (maybe, not ever used it). As far as I know there are no real dedicated storyboarding software that supports egregious branching like that. There are programs like Scrivener (Mac, Windows) that can be used to organize story concepts and flow, but they don't generally bother with a graph view because it is a limited practical utility. 

It's unclear to me what is, and how its implemented -- it's possible the problem lies in there, or elsewhere, as the code you've provided doesn't seem terribly suspect. The biggest thing that stands out to me is that you never seem to actually loop (reset) the position of the background images at all. The "Position" value will increase without bound, unless there's a modulus operation that occurs in the setter for that property or something. You also seem to render all the background images every frame, which is potentially wasteful as probably at most two (given their sizes) will be onscreen at once. You may want to check out this resource on 2D scrolling background in XNA. 

Pixel. The term voxel is short for "volume pixel" or "volume picture element." Consequently a voxel without the "volume" bit is just a regular pixel. The term for gameplay structural unit in a 2D game that can be deformed, damaged or destroyed in the course of game play (as in Terraria and games of its ilk) would most commonly be called a "tile," although you're likely to see more variety in terminology there. I'd venture to guess what has you confused is that you're assuming a voxel represents a unit-of-destruction in 3D (because it does, or at least appears to, in Minecraft). This is not, however, necessarily true. It's just a particular pattern that has been popularized by the Minecraft-like genre of games right now (especially since historically complex voxel models have been CPU-intensive and it was a useful optimization to keep them one-to-one). However, just like a pixel need not represent a unit of destruction in 2D, a voxel need not do the same in 3D. 

I would challenge this assertion. A game made in C++ demonstrates the developers C++ skills more than a game made in some other fashion, but just because something was written in C++ does not mean it better demonstrates a developer's overall ability, nor does it necessarily demonstrate deeper knowledge about anything. 

You are seeing a performance difference due to the fact that in actual full-screen mode, you get exclusive access to the GPU for that adapter, so you won't have to content with any other process for that particular resource. This may be slightly faster, but mostly it is just more predictable (and thus smoother), since nothing else will be allowed to make requests of the GPU that would cause it to hold any kind of lock on any resource or any kind of pipeline stall. Those sorts of unpredictable, out-of-your-control demands are the cause for your occasional stutters when in full-screen windowed mode. It's akin to how loading all your game data from your disk will take longer if there is also another program (like a virus scanner) poking around at all those same files at the same time.