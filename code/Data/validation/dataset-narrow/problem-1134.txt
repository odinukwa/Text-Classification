For Class 2, one somewhat silly example is R(p, a) = {p is an integer polynomial, a is in the range of p, and |a| = O(poly(|p|)}. R is in Class 2 but undecidable. 

A first observation is that the stochastic version of an optimization problem will always be at least as hard as the deterministic version, since fixed constant values in an optimization instance are degenerate special cases of random-variable values. Now to understand the computational complexity of stochastic optimization problems in more detail, it's important first to explicitly define the "order of quantification" for the problem, and also the measure of "success." Suppose $x = (x_1, x_2, \ldots, , x_n)$ are the variables to be set by the optimizing party, and $r = (r_1, \ldots, r_m)$ are the "stochastic" variables. For simplicity of discussion, let's assume these variables are all Boolean. Let $F(x, r): \{0, 1\}^{n + m} \rightarrow [0, 1]$ be the objective function we want to maximize. The $r_i$'s are going to be set uniformly at random by Nature, let's assume. But, are these variables set before the optimizer's decision, or after? Or, are the optimizer's decisions "interlaced" with the random settings? (I'm assuming that in whatever model we use, the optimizer can at least "see" the random settings that have already been made by Nature.) The different possibilities described above can be denoted concisely as (O1) $[x_1, \ldots, x_n, r_1, \ldots, r_m]$ (optimizer moves first); (O2) $[r_1, \ldots, r_m, x_1, \ldots, x_n]$ (nature moves first); (O3) $[x_1, r_1, x_2, r_2, \ldots, x_n, r_n, r_{n + 1}, \ldots, r_{m}]$ (alternation; assuming here that $m \geq n$). So that's the "quantification order" question. Next, we also have to decide how to measure the "success" of an optimizer's strategy. There are two simple possibilities: (S1) Measure success as the expected output value of $F(x, r)$; (S2) Measure success as the probability that $F(x, r)$ meets or exceeds a desired threshold value $\alpha \in [0, 1]$. To make my life easier, I'm only going to discuss (S1), but most of what I say also applies to (S2). Finally, we have the issue of what kind of function $F$ is. There are two fairly general options to consider: (C1) $F$ is a general poly-time computable function into $[0, 1]$; (C2) $F$ is defined by a collection of local Boolean constraints $\psi_1, \ldots, \psi_\ell$, each acting on $O(1)$ variables in $(x, r)$, and $F(x, r)$ equals the fraction of satisfied constraints. OK, now having multiplied the number of questions to consider, I can at least answer some of them. First, if one is asking for extremely accurate information about the achievable success, then the problem becomes ridiculously hard, even if there are no optimization variables! (i.e., even if x is empty.) For instance, if $F(r)$ is a SAT instance outputting 0 or 1 according to whether it is satisfied, then (S1) asks us to compute the fraction of accepting inputs, which is $\# P$-complete. So I think it is more reasonable to settle for a .001-approximation (accurate to within $\pm .001$), or maybe more ambitiously for a $1/n^c$-approximation to the success measure, for each $c > 0$. Now with these goals in mind: -For (O1), in which the optimizer sets all the $x$-variables first, the problem is $MA$-complete to .001-approximate, if $F$ is a general Ptime-computable function. (One has to define completeness for an approximation problem appropriately, and for the promise class $MA$, but morally this is the right complexity classification.) The proof just follows the definitions. If $F$ is as in (C2), then the problem is in $NP$, since we can use linearity of expectations to compute the expected value of $F(x, r)$ for any fixed $x$. -For (O2), in which Nature sets the $r$-variables first, the problem is $AM$-complete to .001-approximate, whether $F$ is as in (C1) or (C2). For (C1) this follows from an easy black-box application of the Cook's theorem plus the PCP theorem. For (C2), I showed the $AM$-completeness result in the following paper: $URL$ -For (O3), interlaced variable-settings, the problem is $PSPACE$-complete to .001-approximate, even in the (C2) case; this was shown by Condon, Feigenbaum, Lund, and Shor in this paper: $URL$ Hope this is helpful... 

From the common sense point of view, it is easy to believe that adding non-determinism to $\mathsf{P}$ significantly extends its power, i.e., $\mathsf{NP}$ is much larger than $\mathsf{P}$. After all, non-determinism allows exponential parallelism, which undoubtedly appears very powerful. On the other hand, if we just add non-uniformity to $\mathsf{P}$, obtaining $\mathsf{P}/poly$, then the intuition is less clear (assuming we exclude non-recursive languages that could occur in $\mathsf{P}/poly$). One could expect that merely allowing different polynomial time algorithms for different input lengths (but not leaving the recursive realm) is a less powerful extension than the exponential parallelism in non-determinism. Interestingly, however, if we compare these classes with the very large class $\mathsf{NEXP}$, then we see the following counter-intuitive situation. We know that $\mathsf{NEXP}$ properly contains $\mathsf{NP}$, which is not surprising. (After all, $\mathsf{NEXP}$ allows doubly exponential parallelism.) On the other hand, currently we cannot rule out $\mathsf{NEXP}\subseteq \mathsf{P}/poly$. Thus, in this sense, non-uniformity, when added to polynomial time, possibly makes it extremely powerful, potentially more powerful than non-determinism. It might even go as far as to simulate doubly exponential parallelism! Even though we believe this is not the case, but the fact that currently it cannot be ruled it out still suggests that complexity theorists are struggling with "mighty powers" here. 

Since it is easy to check whether a graph is one of the graphs allowed by the Theorem, this provides us with a polynomial-time algorithm for the decision problem. Notes: (1) The proof of the theorem is not at all easy. (2) Once we decided that two disjoint circuits exist, it seems less clear how to solve the associated search problem, that is, how to actually find such circuits. The theorem does not give direct advice to that. 

Several answers pointed out that the premise of my question (the relative scarcity of natural $\mathsf{NPI}$-candidates) might be questionable. After some thinking, I must accept that they indeed have a point. In fact, one can even go as far as to make the case that there are actually more natural $\mathsf{NPI}$ candidates than natural $\mathsf{NP}$-complete problems. The argument could go as follows. Consider the LOGCLIQUE problem, which aims at deciding whether an $n$-vertex input graph has a clique of size $\geq \log n$. This is a natural $\mathsf{NPI}$ candidate. Now, the same type of "scaling down" can be carried out on any $\mathsf{NP}$-complete problem. Simply replace the question "does the input string $x$ have a property $Q$?" by the scaled down question "does $x$ have a logarithmically sized substring that has property $Q$?" (We may restrict ourselves only to those substrings that represent the appropriate type of structure, such as subgraphs etc.) Arguably, if the original problem was natural, the scaling down does not change this, since we only alter the size of what is sought for. The resulting problem will be an $\mathsf{NPI}$ candidate, since it is solvable in quasi-polynomial time, but still unlikely to fall into $\mathsf{P}$, as the mere size restriction probably does not introduce new structure. This way, we can construct a natural $\mathsf{NPI}$ candidate for every natural $\mathsf{NP}$-complete problem. Additionally, there are also generic candidates that do not arise via scaling down, such as Graph Isomorphism, Factoring etc. Thus, one can indeed make the case that "natural-$\mathsf{NPI}$" is actually more populous than "natural $\mathsf{NPC}$." Of course, this scaling down process, using Scott's nice metaphor, gives an obvious reason for resisting the "gravitational pull" of SAT. While there are papers published about LOGCLIQUE and similar problems, they did not draw too much attention, as these problems are less exciting than the the generic $\mathsf{NPI}$ candidates, where there is no clear understanding of how the gravitational pull is resisted, without falling into $\mathsf{P}$. 

I believe that we can show: Claim. There's a value $0 < c < 1$ such that the following is true. Suppose there's a deterministic poly-time algorithm that, given an $m$-clause 3-SAT instance $\phi$, outputs a list $S$ of at most $m^c$ values, such that $M(\phi) \in S$; then the polynomial hierarchy collapses. The proof uses Fortnow and Santhanam's results on infeasibility of instance compression from their paper $URL$ Specifically, by looking at their proof of Thm 3.1, I believe one can extract the following (I will re-check this soon): "Theorem" [FS]. There are integers $0 < d' < d$ such that the following is true. Suppose in deterministic poly-time, one can transform an OR of $n^d$ Boolean formulas (each of length $\leq n$, and on disjoint variable-sets) into an OR of $n^{d'}$ formulas (again variable-disjoint and of length $\leq n$), preserving satisfiability/unsatisfiability of the OR. Then $\mathsf{NP} \subseteq \mathsf{coNP/poly}$ and the polynomial hierarchy collapses. The proof of our claim will be a reduction from the OR-compression task mentioned in the above theorem[FS], to the problem of list-computing $M(\phi)$. Suppose $\psi_1, \ldots, \psi_{n^d}$ is a list of formulas whose OR we want to compress. First step: define a polynomial-size circuit $\Gamma$ on input strings $(v, y_1, \ldots, y_{n^d})$. Here the string $y_i$ encodes an assignment to $\psi_i$, and $v \in \{0, 1\}^{d \log n + 1}$ encodes a number between $0$ and $n^d$. We have $\Gamma$ accept iff either $v = 0$, or $\psi_v(y_v) = 1$. Now let $M^*(\Gamma)$ denote the maximum value $v$, such that the restricted circuit $\Gamma(v, \cdot, \ldots, \cdot)$ is satisfiable. (This quantity is always at least 0). Suppose we can efficiently produce a list $S$ of possible values for $M^*(\Gamma)$. Then the claim is that in our list $\psi_1, \ldots, \psi_{n^d}$, we can throw away all $\psi_i$ for which $i \notin S$; the resulting list contains a satisfiable formula iff the original one did. I hope this is clear by inspection. Conclusion: we can't reliably produce a list $S$ of $\leq n^{d'}$ possible values for $M^*(\Gamma)$, unless the poly hierarchy collapses. Second Step: We reduce from the problem of list-computing $M^*(\Gamma)$ to that of list-computing $M(\phi)$ for 3-SAT instances $\phi$. To do this, we first run Cook's reduction on $\Gamma$ to get a 3-SAT instance $\phi_1$ of size $m = poly(n^d)$. $\phi_1$ has the same variable-set as $\Gamma$, along with some auxiliary variables. Most important for our purposes, $\phi_1(v, \cdot)$ is satisfiable iff $\Gamma(v, \cdot)$ is satisfiable. We call $\phi_1$ the `strong constraints'. We give each of these constraints weight $2m$ (by adding duplicate constraints). Then we add a set of `weak constraints' $\phi_2$ which add a preference for the index $v$ (defined in step 1) to be as high as possible. There is one constraint for each bit $v_t$ of $v$, namely $[v_t = 1]$. We let the $t$-th most significant bit of $v$ have a constraint of weight $m/2^{t-1}$. Since $v$ is of length $d \log n + 1$, these weights can be made integral (we just need to pad to let $m$ be a power of 2). Finally, let $\phi = \phi_1 \wedge \phi_2$ be the output of our reduction. To analyze $\phi$, let $(v,z)$ be the variable-set of $\phi$, with $v$ as before. First note that given any assignment to $(v, z)$, one can infer the value of $v$ from the quantity $N(v, z) =$ (total weight of $\phi$-constraints satisfied by $v, z$). This follows from the hierarchical design of the constraint-weights (similarly to a technique from Luca's answer). Similarly, the maximum achievable value $M(\phi)$ is achieved by a setting $(v, z)$ that satisfies all strong constraints, and where (subject to this) $v$ is as large as possible. This $v$ is the largest index for which $\Gamma(v, \cdot)$ is satisfiable, namely $M^*(\Gamma)$. (Note, it is always possible, by setting $v =$ all-0, to satisfy all strong constraints, since in that case $\Gamma(v, \cdot)$ is satisfiable.) It follows that, if we are given a list $S$ of possible values of $M(\phi)$, we can derive a list of $|S|$ possible values of $M^*(\Gamma)$. Thus we can't have $|S| \leq n^{d'}$ unless the poly hierarchy collapses. This gives the Claim, since $n^{d'} = m^{\Omega(1)}$. 

Let $L$ be a decidable language. Let $X^L$ be the set of deterministic Turing machines which decide $L$. Define two machines $A,B\in X^L$ to be time-equivalent if $t_A(w) = t_B(w)$ for all $w \in \Sigma^*$. Define a metric on the set of equivalence classes as folows: Let $R>0$ be some big number. $d(A,B) = \sup_{w\in \Sigma^*} | t_A(w) -t_B(w)|$ If this number is bigger than $R$, set $d(A,B) = R$. This defines a complete metric space on the set of equivalence classes of $X^L$. Let $f:X^L \rightarrow X^L$ be the function which to each equivalence class of dTM maps it to its "linear speedup-theorem"-Machine. Suppose that for each machine $M$ we have: $t_{f(M)}(w) = 1/2 t_M(w) + |w| + 2$. Then this map $f$ is a contraction. By the banach fix-point theorem there must exist an equivalence class $M$ such that $f(M) = M$ . But then $t_M(w) = t_{f(M)}(w) = 1/2 t_M(w) + |w| + 2$ and we get $t_M(w) = 2 |w| + 4$. However this seems very absurd. Does it contradict something known? The only assumption which I made is that we have equality instead of $\le$ in the linear speedup theorem. 

If the prime decomposition of the order $|G|$ of a group $G$ is given and a prime $p$, than combining Cauchy's theorem and Lagrange's Theorem, than it can be checked in time $m$, where $m$ are the number of distinct prime divisors of $|G|$, if $G$ has an element of order $p$. Now it is clear, that this is not constant time if $m$ is variable, but if you try to naively find such an element, for example by trying all elements of $G$, then the time will be at least $|G|$. If $m$ is fixed than it is constant time, namely time $m$, which is constant because $m$ is fixed. On the other hand, if one is pedantic, than one can argue, that no constant time algorithm exists if the algorithm first has to read a variable-length input to make a decision. 

Over the integers, it looks like Subset Product is at least as hard as the Exact Cover problem $URL$ parametrized by the number of sets used in the exact cover. (For the reduction, one assigns a distinct prime to each element of the ground set.) I couldn't find a reference, but I'm guessing that this problem is $W[1]$-hard and so unlikely to have an $f(k) \cdot n^{O(1)}$-time algorithm. (I would look in Downey-Fellows or other textbooks on FPT theory.) Maybe one could rule out $n^{o(k)}$ running time under the Exponential Time Hypothesis or Strong ETH. The paper of Patrascu-Williams might be a starting point. Sorry for not knowing much, but I figured this is better than nothing. 

I don't know the answer to Question (1)---I suspect there is no such reduction. But I have a couple of points to make. A non-adaptive random self-reduction (to the uniform distribution) is a poly-time randomized algorithm $R$ which, given an arbitrary instance $x$ of length $n$ for the decision problem for $L$, produces a list $y^1, \ldots, y^{q(n)}$ of instances of some second length $\ell(n)$. We have the properties that (i) each $y^i$ generated by $R$ is individually uniformly distributed, for any fixed input $x$ (although there may be complicated dependence between the $y^i$s, which in some way "encodes" $x$); (ii) there is a second poly-time algorithm $R'$ which, given $y^1, \ldots, y^q$ along with their $L$-membership values $b^1, \ldots, b^q$ ($b_i = [y_i \in L]$), outputs the desired value $[x \in L]$. Feigenbaum and Fortnow prove that if an $NP$-complete language has such a reduction then the Polynomial Hierarchy collapses. The argument applies to other target distributions, not just the uniform distribution. Now my first point (regarding your Question (2)) is that their result directly gives evidence for the difficulty of showing a worst-case to average-case connection for proof systems certifying unsatisfiability of Boolean formulas. Because the most natural way you'd try to argue that random unsatisfiable formulas are hard to refute (assuming $NP \neq coNP$), is by exhibiting a random self-reduction as above. Now in the study of proof systems, it might be natural to allow the recovery algorithm $R'$ to be nondeterministic, for example; I believe the [FF] analysis techniques would still apply, but this would take verification after defining things carefully. My second point is that non-adaptive random self-reductions as above are not the only way to establish average-case hardness of a language $L$ (assuming its worst-case hardness). There is also the more general notion of a self-correction reduction, which converts any mostly-correct algorithm for $L$ into a fully-correct algorithm, but which does not guarantee that its individual queries be distributed according to any particular distribution (they may depend heavily on the input $x$). Bogdanov and Trevisan proved that non-adaptive self-correctors for $NP$-complete languages would also collapse the Polynomial Hierarchy. But it is completely open whether adaptive self-correctors exist for $NP$-complete languages. I believe the question of adaptive random self-reductions for SAT is also open. Even if all of these could be ruled out (under the assumption $PH$ does not collapse, say), there might conceivably still be some other, less constructive proof of a worst-case to average-case hardness connection for $NP$. Bogdanov-Trevisan give a good discussion of the issues here (see also their survey on average-case complexity).