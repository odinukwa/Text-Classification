You may want to look at the hier man page. This kind of flag is often found in /var/run. You should use a marker file for both processes to prevent multiple processing scripts from running. 

If you have sensitive files, start with securing the files. This should be done even if you don't intend to let other users to access the system. Linux/Unix has a strong permissions system. Ensure none of the sensitive files are readable by . This can be done by restricting access to any directory along the path. It is not uncommon to set the permissions on home directories to which allows only the user and root to see the contents. Set your restricted user up with their own group, which on some distributions is the default. Audit you system to see what files can be accessed with permissions. If some of the sensitive data is in a database, audit its permissions. If you allow the user access to a browser, they will normally be able to browse the file system. Besides kiosk mode, you may want to consider using a to restrict them to a small portion of the system. This can be difficult to setup for an X-window environment as it needs a fair number of files and devices. I would expect an profile would need to allow access to the same files and directories as chrooted environment. You can combine approaches it implement Defense in Depth. It is relatively, easy to attempt to restrict an X-window user to a single application. Just start that application instead of starting a Window Manager. As long as that application can not start other applications you have restricted them to that application. Explore the capabilities of that application as it may be able to browse and possibly execute files. Does it have a mode intended to restrict access. If there are flaws in the configuration, the user may be able to escape their jail. I have used a few tricks to gain access to and other tools on a supposedly locked-down Windows system. This is where correct file and directory permissions come in to full force. 

If the message arrives in a single transmission, then Exim should removed duplicates in the process of delivery. This is unlikely to happen if the domains are different, or have been forwarded. Check your logs for the offending messages. If the messages arrive in separate deliveries with the same Message-Id it is fairly easy to remove duplicates using that header. Exim will deliver using which can do the deduplication during delivery. During testing the duplicates can be delivered to a separate mailbox so nothing is lost. I would encourage the user to unsubscribe from the duplicate addresses. A forward rule can be used to distribute to other addresses. 

Just add the local clock as the server and disable all servers. Fudge the priority so if anyone includes your host as a server they don't think you are running and atomic clock. This is the configuration I use for my servers: 

Your access log should show all requests. It is likely that the applet is trying to access resources from your server. This is the default classloader behaviour. If the ZIP is on the classpath of the JAR, files will be loaded from it if the web access fails. There is a way to change this behavior, but I have not dealt with this problem for a few years. 

Watch the output of to see when you get a trusted clock source. Normally wants a number of responses from a trusted time source before it trusts it. This is indicated by the first character on each line. While NTP likes more sources, any odd number of time sources within one stratum level should work well. As you only have two servers and a GPS clock the priority (stratum) of the sources should increase from GPS, clock on server A, clock on server B. Increasing the stratum between each by three or four levels will ensure priorities are respected. EDIT: If you have the busybox NTP server on server A, it may be worthwhile installing the full ntp server package. Understanding what is happening with server A should go a long way to solving your problem. You will need at least one trusted time source there before server B should trust it. If doesn't work, then you can try . Both these commands allow you to query other hosts. A log could also be useful. On server B use ntpclient as documented the busybox ntp howto to log what is happening on it The clocks should be reasonably close to the correct time if the servers haven't been down for long. If you need to sync the two systems, that should be sufficient. The GPS will bring the time into sync with the real world eventually. 'ntpd -q' synchronizes quickly, but exits (ntpdate behaviour). It needs to be followed by an command without the quit option to have continuous synchronization. EDIT2: I check my server and found one of the servers was off by a second. While fixing this I played with the settings. gets a server trusted very quickly. ensured the clock driver was trusted if there weren't multiple other trusted sources. The clock took a little more than a minute before it was locally trusted and could be trusted remotely. When testing you should be able to restart the process once it is synchronized and test how fast settings work. In the above case Server B may need to be restarted to test how fast it synchronizes. When monitoring changes I use a line like: 

Try doing your configuration in the apache configuration. This can be an include of file like . This will allow you use configuration directive. However you will need to restart apache to load changes. Use the graceful restart if you don't want service interruptions. To restrict directory accesses with files, they need to be in the appropriate directories. They function much like the contents of a configuration directive. You may need to enable the required options in your apache configuration. This method is not as efficient as using command in the apache configuration as it needs to be reparsed frequently. 

I don't know if Exchange will handle this, but I enabled user suffixes. I use a different suffix for each newsletter, marketer, etc. Instead of , is use . If one of these addresses becomes a spam source I create an alias that fails delivery. The alias i use is . Users can create as many aliases as needed this way, and legitimate email is delivered. Blocking addresses is relatively easy. You will need to check how to implement both the suffix and alias mechanism in Exchange. Aliases need to be checked before suffixes are removed for this to work. 

Use followed by the value you want. Zero is considered success, non-zero failure. This is typically used within a script, and terminates the (sub)shell it is exited from. If you want to capture the status of a command, assign to a variable. This allows you to save the value after displaying it. 

If you have exposed you mail server to the Internet, expect most of the connections to be from spambots, and other illegitimate senders. I would consider just matching rejects for any Errors in fail2ban. legitimate senders should rarely generate and error, and will retry later if they do get banned. I do some nasty things to suspected spammers, and it has been years since a legitimate sender has had problems other than delivery delays. I use a few tests to check the legitimacy of senders: 

Yes, it is possible to silence bind. Check your configuration for category and channel definitions. If this is showing up in your syslog, then find the channel(s) mentioning syslog. There is also a default_syslog channel built in. Then find the categories logging to these channels. Comment out the category or redirect it to a different channel. You may want to redirect to a log while you test. More detail can be found here: $URL$ 

The local routing table allow the system to route to the appropriate interface to reach an address. Routes are selected using the most specific (smallest subnet) route. However, you testing routes handled by loopback address. Ping is responded to at the interface level, and does not require a listening process. when pinging an address which is assigned to an interface on the host, the network short-circuits routing to the loop-back interface. Routing to the loopback interface prevents remote servers from snooping the traffic. As you have assigned the route to a local interace, ping will be responed to by the loopback interface. Normally you the routing table will have the following routes: 

I get a lot of mass mailings which score high on antispam filtering. Mostly these are a result of violating standards applicable to email messaging. These include: Lines exceeding size limit; missing headers (usually message_id); invalid sender address; failure to encode data correctly; and incorrect DNS setup. I would expect these factors are considered by Hotmail. DNS problems may result in rejection during when specifyinf the recipient. This could be interpreted as no such recipient. 

You could configure a default virtual host to handle all domains except your server. Configure it to return a 403 request to all queries. This will have minimal content. Also configure it not to allow keep alive to free resources quickly. Using a separate log for the default virtual host with fail2ban configured to drop any connecting hosts will quickly block the hosts. Fail2ban will clear the blocks after a configurable time. 

As other have noted, they are likely doing brute force scanning. If you are on a dynamic IP address they might be more likely to scan your address. (The following advice assumes Linux/UNIX, but most may be applied to Windows Servers.) The easiest ways to block them are: 

Your low TTL for your addresses is causing your records to age out of cache long before the nameserver records get updated. Your TTL should be at least as long as the TTL for your NS records. If you can keep your records up on the old server until the NS records time out you should be OK. The old NS records will time out within a couple of days and your propagation issues will resolve themselves. Things should clear up after 2 days (172800 seconds). It is good practice to shorten the TTL on records before you change them. I am not sure whether your providers allow you to change the TTL on the NS records. At lease two days before the change I would have reduced the TTL on the NS records to about an hour (or 1800 as that is what you are using on your records). This is the process I follow: 

It looks like it can't read the key file. You should have an unpassworded key. Add openldap to the ssl-cert group. Make the group on the key ssl-cert and the permissions 440. The openssl s_client command can be used to debug tls issues. 

Try using which should contain the domain portion of the sender address. Expansion variables are listed in chapter 11 of the Exim Specification. 

The Ubuntu/Debian Exim configuration handles this out of the box. Configure your server to forward all mail to a Smarthost (your ISP's relay server). I use this configuration internally to consolidate all email on a single server. 

The first listed host depends on the order the configuration is loaded. Different distributions have different mechanisms for loading the files. On Ubuntu, sites should be defined in . These are enabled by creating a link in . The enabled sites are loaded in canonical order from that directory. Files in that are not linked to are not loaded. The configuration loads files from and are loaded prior to loading sites from . If you enable the info module, you should be able to review the configuration by navigating to . Check the order of the virtual hosts on this page. It is also possible to place Redirects in file located in the document directory. It is also possible to place redirection directives inside containers other than a container. You may want to verify which redirections are occurring by dumping the server headers. I use a command like . This will show the headers from each redirection. 

What you are trying to do is called hairpin NAT. It would be better for host2 just to connect to host1. If you want this to work, use SNAT as well as DNAT for traffic originating on the local network going to the external IP. Another solution that could work is to add a route for host2 to host1 which routes the traffic to the router rather than to the local network.