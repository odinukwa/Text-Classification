A model is built on a specific set of features, which may include categorical features encoded using one-hot encoding. If you have new data with additional categories, your model has no idea how to interpret the significance of those categories. You should either map the new value to none of the 1-hot values identified in training, or to an 'other' value. For example, say you trained on data that had color=[blue,green]. Your one-hot fields would have color_blue and color_green. You could also have a field called color=other, that you might use to encode very infrequent values. That's a data preparation choice. So for 'red', you could encode that as either: 

This doesn't answer your Tableau questions, but I can say a few things about ggplot2. ggplot2's has reasonable defaults but nearly everything about the plots can be changed to make the results more clean aesthetically. One package I use all the time is called 'cowplot': $URL$ ggplot2 also has built-in themes ($URL$ and there's a package for community-develeoped themes ($URL$ 

I would try using Google's CausalImpact package. Your use-case isn't causal inference exactly, but CausalImpact relies on bayesian structural time series models (using the bsts package) and has some good defaults that keep you from needing to dive into bsts immediately. Basically, you fit a model to the first part of your data, then forecast the rest. You see where your model deviates from the forecast. Using bayesian models means you can get error bounds - so you can have a degree of confidence in the deviation. In your case, you'd set your 'intervention' point to wherever timestamp you want to separate your modeling data from your forecasting data. Then compare the forecast to the actual data (look up 'nowcasting'). Here is a tutorial to get you started, and here's an introductory video. 

The result will be an ordered list of orthogonal vectors (eigenvectors), and scales (eigenvalues). This set of vectors/values can be viewed as a summary of your data, particularly if all you care about is your data's variance. I think there is an implicit assumption that the orthogonality implies independence of the resulting vectors, and from what I understand that's true if the data is Gaussian but not necessarily true in general. So I suppose whether your data can be modeled as Gaussian may or may not matter, depending on your use case. 

It's roughly 10 times faster than Jan van der Vegt's solution(BTW he counts valid values, rather than missing values): 

I'm assuming you are looking for medical datasets, not only what you have stated in the question, but also some image recognition datasets which need something like CNN. Try this one. This dataset contains several medical features including blood sugar, serum cholesterol etc, and wants you to find out the presence of heart disease. In the meanwhile, there are some medical competitions and datasets on Kaggle, including the famous Data Science Bowl. 

At this moment, the answer is no. Tensorflow uses CUDA which means only NVIDIA GPUs are supported. For OpenCL support, you can track the progress here. BTW, Intel/AMD CPUs are supported. 

will ignore parameter , while accepts. In , boosting iterations (i.e. ) is controlled by (default: 10) In your case, the first code will do 10 iterations (by default), but the second one will do 1000 iterations. There won't be any big difference if you try to change into , References $URL$ $URL$ 

8 Tactics to Combat Imbalanced Classes in Your Machine Learning Dataset. This blog shows a common workflow dealing with imbalanced class issue. Class Imbalance Problem in Data Mining: Review. This paper compares several algorithms created for solving the class imbalance problem. 

As we all know, XGBoost constructs trees based on gradient. I wonder how does XGBoost define gradient of MAE loss, as MAE itself is not differentiable. After some digging of the source code, I found the implementation of MSE loss here. But I can't find any implementation of MAE loss. The original paper didn't discuss MAE loss either. 

Theoretically speaking, more data leads to better model. However, in practice, more features often leads to the difficulty of model training. Assuming there are 30 "main" features of your dataset. Feature set A contains 20 "main" features, so it might be easy (20 out of 26) for one "main" feature to be "chosen" and "trained", under certain hyperparameters (in your case, 100 trees). When it comes to feature set B, which contains all "main" features, it's hard (30 out of 96) for one "main" feature to be chosen, and it's harder when there's only 100 trees (cause there are 66 "minor" features which should not be trained, relatively). That's what we called "under-fitting". Back to your question, when the model is under-fitting, if we're lucky that model trained on feature set B (namely model B) contains all these 30 "main" features, the model will be good, and we might not able to find out it's under-fitting. But in most cases, we're not that lucky, model can be ruined with all 66 "minor" features, relatively. In my ML practice, I'll try more training iterations when more samples come, and try a more complex model when more features come, with the control of over-fitting/under-fitting. 

Feature-extraction mechanisms like GIST, HOG, etc are built and optimized to improve performance on given datasets. Because of this, they don't perform as well across datasets. It's kind of like putting specialized fuel in a vehicle that isn't built to utilize it - it might even do harm. Hand-engineered features are, as a rule, brittle. I once heard it said that the dirty secret of machine learning is just knowing how to transform your domain-specific information into meaningful features - after that, you can use an extremely simple classifier and it may do surprisingly well. The drawback is that the rules you built are very specific to your domain. Deep neural networks, and convolutional neural networks in particular, are an advancement in that they learn what features are useful about raw data - for CNNs, these are the raw pixel or time-series values. Instead of hand-building feature extraction mechanisms, these architectures automatically build them. One benefit of this is that if you use a CNN to identify images in general, you can re-use the top few feature extraction layers of the CNN on a different image recognition dataset, and re-train the bottom few layers to make the network specific to recognizing e.g. dog breeds. You can transfer what you learned about the statistical structure of natural images in general to other, more specific questions (general -> specific). In your case, the 'top few layers' are analogous to your GIST/HOG methods - and they wouldn't be expected to perform well when the task changes, because they were constructed for a specific task (specific -> other specific). 

Rather than trying to predict each value explicitly, you could pick a parametric function that you think might be expressive enough to describe your target function (for example, a polynomial of order 6), and get your network to learn the parameters of that polynomial that give you a minimum loss. This is basically polynomial regression. Since your function is periodic you might use a more cycle-based basis set, like linear combinations of sine waves. For example: 1) Input data to the network 2) output a set of values that you treat as parameters to your function 3) for each target point, compare it to the predicted value from your function 4) calculate a loss (like mean squared error) based on the difference between the function predictions and the targets. Using a function will (1) restrict the space of possible outputs, and (2) make nearby outputs be self-similar. 

Deep Residual Learning for Image Recognition (the famous ResNet paper). Checkout figure.1 This kernel of Two Sigma Financial Modeling Challenge on Kaggle says: 

It's a regression machine learning problem. Assuming you have 30 characteristics, one-hot encoded into 30 columns. And your target is the character score, min-max scaled into $[0, 1]$. So we have , (just like what ncasas has stated), thus we can train a regression model using your favorite algorithm (linear regression, random forest, even neural network). After we have this model working, as each person has and only has 3 characters, we can predict the character score for each character combination one by one. The time complexity is roughly $O(n^3)$. However, $n$ is small in your case, so maybe we can just brute-force every score on every combination. That's what you want. 

implements CV by , which supports custom scorer. However, and implement CV by , with MSE as scoring hard-coded. impletments CV by , with MSE as scoring hard-coded too. If you want to use other scorers, maybe you can use directly. References $URL$ $URL$ $URL$ 

The first and the second convolution layers should have ReLU activations, at least not linear activations. Why do your output layer have 11 neurons? There are only 10 digits, right? is used for sparse input. Your seems to be a non-sparse array, thus is better. Your 3x3, 4x4, 5x5 convolution layer seems weird, but I can't give you a reason (maybe someone else?). If I were you, I would use 3x3 layer. If you want to print your output shape of your model, try . It's awesome! If I were you, I'll try rather than . 

Same period last year is what you want. BTW, your question does not appear to be about data science. References $URL$ $URL$ 

There are pseudo-codes in this paper, Section 3.1 shows an example of incremental mining by this algorithm. However I can't find any projects or working source codes with this algorithm implemented. 

While that may be a little dramatic, sockets in my opinion are one of the most amazing pieces of software that the world of computer networks has put together in several years. I'm not sure what you're using for your message-passing layer right now, but if you're using something traditional like , you're liable to see speedups of multiple orders of magnitude (personally noticed about 500x, but depends a lot of architecture) Check out some basic benchmarks here. 

Hope that makes sense, if you're specifically looking for clustering algorithms, a few that I personally enjoy that might be good in this scenario are FLAME and tsne. Alternately, looking at the spectacular gensim library in python will get you a long way toward the clustering you're looking for. Hope that helps and makes sense, leave a comment if you've got any questions. 

Again, can't say what this means for the future, but these are just a couple of relevant points when it comes to evaluating in my opinion. 

Really great question, and one that I find that most people don't really understand on an intuitive level. is in fact often predicted over accuracy for binary classification for a number of different reasons. First though, let's talk about exactly what is. Honestly, for being one of the most widely used efficacy metrics, it's surprisingly obtuse to figure out exactly how works. stands for , which curve you ask? Well that would be the curve. stands for Receiver Operating Characteristic, which is actually slightly non-intuitive. The implicit goal of is to deal with situations where you have a very skewed sample distribution, and don't want to overfit to a single class. A great example is in spam detection. Generally spam data sets are STRONGLY biased towards ham, or not-spam. If your data set is 90% ham, you can get a pretty damn good accuracy by just saying that every single email is ham, which is obviously something that indicates a non-ideal classifier. Let's start with a couple of metrics that are a little more useful for us, specifically the true positive rate () and the false positive rate (): 

As your network is working without dropout, I think your problem is about how many epoches you run. In your code, it seems that only one epoch will be run. With dropout enabled, each neuron has 50% percent (for example) chance to be activated. Maybe there are some un-trained neurons in your network, which ruin your accuracy. I think it is worth trying more epoches. In my experience, 100 epoches is always a good start. 

After I read the source code, I find out that doesn't actually load the plain text data and convert them into vector, it just loads the vector which has been converted before. As for your problem, I assume you want to convert your into vector. Maybe you can try sklearn.feature_extraction.text.CountVectorizer. I'm no expert on NLP, but I've encountered problems (e.g. Two Sigma Connect: Rental Listing Inquiries) which need such technique. In the meanwhile, there are other word2vec/embedding techniques you may try. Here is a tutorial from keras which shows a detailed example on word embedding. References $URL$ $URL$ $URL$ 

As you have commented, you are concerning about over-fitting. In fact, cross validation will help to weaken over-fitting, but it can't eliminate over-fitting. Model scoring on TRAIN dataset sometimes exceeds scoring on TEST dataset. Here are some examples I can find: 

The short answer is yes. While your classes are imbalanced, model will be more likely to "learn" class 2 during the training, especially when it comes to mini-batch updates. (even though each mini-batch is unlikely to have only class 2 samples) The common solution is to use weighted loss function or feed weighted samples to mini-batch. In my practice, I always use weighted samples if I have sufficient data, even when my classes are not that skewed. Weights always help, at least it never ruins the model. 

It seems like a reinforcement learning question with continuous action space. is the observation, the action, and the reward. You may refer to this paper from DeepMind, which provides a general deep Q-learning way solving this type of question, including some physics tasks. However, as I don't have your data, I'm not sure if you have sufficient instances. ~70,000 instances with 4 features is (roughly) sufficient for general ML problems, but in reinforcement learning world, people use a simulation environment which is able to generate infinite samples, thus ~70,000 instances may be the bottleneck. If you encounter such problem, you may try classic reinforcement learning (without "deep") techniques. 

To clarify, I feel like the original question references by OP probably isn't be best for a SO-type format, but I will certainly represent in this particular case. Let me just start by saying that regardless of your data size, shouldn't be your limiting factor. In fact, there are just a couple main issues that you're going to run into dealing with large datasets: 

When it comes to dealing with many disparate kinds of data, especially when the relationships between them are unclear, I would strongly recommend a technique based on decision trees, the most popular ones today to the best of my knowledge are random forest, and extremely randomized trees. Both have implementations in sklearn, and they are pretty straightforward to use. At a very high level, the reason that a -based approach is advantageous for multiple disparate kinds of data is because decision trees are largely independent from the specific data they are dealing with, just so long as they are capable of understanding your representation. You'll still have to fit your data into a feature vector of some kind, but based on your example that seems like a pretty straightforward task, and if you're willing to go a little deeper on your implementation you could certainly come up with a custom tree-splitting rule without actually having to change anything in the underlying algorithm. The original paper is a pretty decent place to start if you want to give that a shot. If you want pseudo-structural data from your text data though, I might suggest looking into , recently developed by Google. I don't think there are any good open-source implementations now, but it's a pretty straightforward improvement on the algorithm, which has implementations in at least and . Hope that helps! Let me know if you've got any more questions. 

Personally going to make a strong argument in favor of Python here. There are a large number of reasons for this, but I'm going to build on some of the points that other people have mentioned here: