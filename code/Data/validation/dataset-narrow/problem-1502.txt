It was requested ad-hoc for no specified reason Critical/frequent problems originating from a certain module or modules too often It was scheduled It was requested by the dev 

It's usually #4 and if you're playing with some fast and loose code cowboys, #2. So, in both of those scenarios you're basically the mentor/master and less of a peer. You're not there for the thumbs up/down, you're there to take charge and in some cases, teach. edit 2: Expanding on code audits and analysis as well as quality, I feel the living legend John Carmack does a good job describing it: 

edit: Emphasis on code-reviews. Gilles brings up some good points when taking into consideration more in-depth criticism of code in the form of code-audits. These steps above are not the steps I would take for a code-audit. In regards to the main difference between the two and how to interact with people, I would say it has more to do with understanding the core intent of the code and figuring out where it falls short and coming up with more elegant solutions. For example, someone might accidentally re-invent something from C++11's library to help sort something from their class. In a code-review, I would mention this and advise you to remove it and replace it, but in an audit, I would tell you to fix it and remove the redundant code, because of the conversation's scope and the relationship's connotation. If I'm auditing your code, it's usually because of one of these reasons: 

I don't see why the same practices in the industry wouldn't apply to students. Professionals don't come pre-loaded with firmware to handle code-reviews :) Here's been some of my experiences both giving and receiving code reviews: 

Coding/Programming is interacting with a computer or device with a language the computer can translate and actually use, because computers and other devices don't understand human languages. Sometimes those interactions are things like asking questions, or taking notes to keep track of things, or looking up old information wherever you put it. Programs are big collections of those designed interactions to achieve a specific goal, like, make a spreadsheet, send an email, or draw an object in three dimensions on the screen -- stuff like that. Programming languages are just languages that humans code with so computers know how to translate and use their designed interactions. Those languages are usually a mix of human words and symbols to help with the creation of those instructions for those interactions. The computer/device only cares about the instructions, not the language used to make those instructions, so it takes those translated, coded interactions as instructions for a language that the computer knows how to use that we humans can't really read without help of a translator. Coders/Programmers are basically the middle-men to help with communication and action between machines and humans. 

Prove you're right if telling someone they're wrong. Making a baseless assertion up-front is equivalent to, "I told you so," until proven otherwise. Show how the code will break if you think it will break. Don't just spout rhetoric in favor of different way to do it. Don't add your improvements to the list of changes needed. Keep necessary changes and improvements separate. If the coder likes your improvements, they will implement them. Appeal to their vanity by showing them how you can make their code even better instead of showing all its pitfalls. Learning new tricks is easier than losing bad habits. If you think something is wrong but you can't prove it, ask questions about its use instead of making assumptions. Even a solid educated guess gone-wrong can make a code review less constructive from that point on, and waste your time in the process. 

As for CS, in your original question you say that you "have no problem understanding what Computer Science is and what it includes", but let's talk about CS as well. The definition from the report you pasted in your own answer is good enough for me: 

The way I understand the report and especially the bit you pasted in your own answer, the key to IT is in the T. It's IT if it has a clearly technological approach (and by necessity the treatment is at theoretical level rather than practical). 

In my opinion the report correctly characterizes it as having longevity and being wholly technology-independent. 

I don't see how this follows in general. It depends, of course, on what the test is testing: if the test is about finding the one incorrect value in a 2000-entry trig table I would expect a computer would be of help, but that's not what a CS test usually looks like, is it? I expect a good test would in general measure the student's understanding of a topic, not their ability to mechanically carry out repetitive tasks. 

I am going to take a shot in the dark here and make an assumption about your question: your question is about programming and the "test" is writing a program. Under that assumption I would answer with a loud "yes" - you'd want students to be able to reason about programs and convince themselves of the soundness of their reasoning, not just go by trial and error until they converge to a working program by sheer chance. That's, after all, no way to program in real life. On the other hand, I would also have a lab test to measure the student's ability to actually compile and run a program and/or launch a debugger - after all, once you've produced a correct program you still need to compile it to be useful... 

Further rule of thumb: if there is a big $O$ somewhere, or a proof by induction, or a graph, or an inductively generated structure, it's probably CS. If kids who are good at the IT and DL parts fail miserably at it, there you have the conclusive proof. 

Disclaimer 1: I'm not sure if this is more of a comment or an answer and Disclaimer 2: this is not really my field, but I'd be inclined to answer: no. Firstly, I don't think it's necessary. I think the intuition behind the concept of transform (at least as in Fourier or DCT) is within the grasp of most high school kids that know some trigonometry: you can probably get away with drawing a simple function, decomposing it into sine waves and writing down their phase and amplitude while waving your hands a lot. You'd then proceed to reconstruct the original function by means of summation and persuade the students that the representation in terms of 4/5 phase/amplitude coefficients is "more compact" (finite, even) than its representation in terms of infinite pairs in $R \times R$. I'd argue it's also not possible because the DCT is also very much one of - if not the - working principles behind JPEG, differentiating it from classical lossy compression techniques. Operation in the frequency domain is ultimately the reason why *PEG performs subjectively better than, say, resampling and requantizing at 240x180@16 colors, which is essentially a much naiver lossy compression method that operates purely in the space/time domain. Note, finally, how the excellent Smith handbook - along with probably several other resources - explicitly calls JPEG “transform compression”. 

In my experience, the most confusing aspect here is not that there are rounding errors. Students should already have an understanding that there are physical limits that keep us from calculating with infinite precision all the time. What is confusing though is that these rounding errors occur in cases that might seem trivial to students. What is important to understand here is that there are real numbers which do have a finite representation in decimal, but lead to an infinite representation when being converted to binary. is probably the canonical example here. Students are used to thinking in base 10, so the most surprising rounding errors are those, where the operation still stays within the number of a couple of significant decimal digits, and therefore looks completely harmless when writing it down on paper, but still blows up because the numbers cannot be represented exactly in binary. Think: $0.1 + 0.2 - 0.3 \neq 0$. Hammering this single point home should be enough to convince students that floats can lead to surprising rounding errors. The more challenging task is then to restore their faith in floats by explaining that the behavior of floats is still completely deterministic and one can actually give exact bounds for the expected rounding errors produced by a specific calculation. For instance, many programming languages these days have support for hex float literals, which allow specifying float literals in base 16, eliminating potential rounding errors when converting the base 10 numbers from the source code to the base 2 numbers used by the machine. You still can get rounding errors on each calculation, but knowing what the numbers look like to the machine makes it much more predictable when to expect such rounding errors. Also, most of these calculation rounding errors are actually not due to the machine being unable to calculate exactly, but rather the fact that the result of the calculation can not be represented exactly in float. So the result is often really the closest to the correct result that you can possibly achieve with the chosen representation. 

Production lines in factories, eg. for assembling cars. In theory, a single person could assemble a whole car from scratch. But they would need to memorize a lot of different steps and it would be hard for them to find an ideal working mode. For example, they might need very different clothes and tools for assembling the engine and painting the body. What is worse, the only way of speeding up production is assembling cars in parallel. Parallelising tasks this way does not scale well in the real world (eg. think of the logistics for getting the parts to every worker as you keep growing the number of parallel workers). In software and hardware design this also applies: (Task-)Parallelism introduces significant additional complexities into a system, so we can't blindly rely on it for scaling up indefinitely. What factories do instead is, they break the assembly process down into distinct steps and then have each worker specialize in a specific step. This allows them to quickly learn how to complete that singular step as efficiently as possible. In software and hardware design this is also true: Finding the most efficient design for a single, self-contained step is way easier than optimizing a big system that does everything. Now, breaking your assembly line into its parts has its caveats: You introduce additional overhead by eg. having to move the car from one assembly station to the next. So a single car might in the end still take longer to move through the assembly line than if one person did the assembly all by themselves. But we can compensate for that by making use of the fact that the steps are independent: As soon as the first car leaves my assembly station, I immediately start working on the next one, even though the previous car still has to reach the end of the assembly line before it's completed. The latency is the total time it takes for a car to pass through the complete assembly line. Throughput is how many cars the assembly line can complete per hour. Once the pipeline is full, this is determined by the time of the slowest assembly step - the clock rate of the line. The challenge in designing such a system is to find the sweet spot for how much work we can fit into a single step. If a single step gets too big, it again gets difficult to optimize. If it gets too small, the overhead of managing the line might be bigger than the actual work required for the assembly. But we also need to ensure that all tasks are about the same size: If a worker is able to complete a task significantly quicker than the clock rate, they will be forced to idle for the rest of their time until the next car is delivered to their station. All of this applies 1:1 to software/hardware design. Now, in our production line scenario we have a pretty ideal situation, as the different cars can all be assembled in isolation. If someone makes a mistake in assembling car A, that affects only car A but none of the others (unless the mistake somehow forces you to stop the complete production line). In hardware or software this is often untrue. For instance, deciding whether you take a branch in a CPU instruction pipeline often depends on the result of a previous operation. However, if that operation has not reached the end of the pipeline yet, we might not have that information available. So we have to delay the execution of the branch until that information becomes available (stall the pipeline). If we have a lot of those dependencies (often referred to as hazards), we might lose all our pipeline parallelism again and our throughput approaches the latency of the pipeline.