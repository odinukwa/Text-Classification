If I will be able to route somehow the traffic that goes to Ireland VPN from IPs in to my current VPC in Ireland with , I could use my routing table and my IPSEC instance to route it back to Virginia VPC. Is it possible at all to route traffic from different subnet to another CIDR block in VPC ? IP to ? 

I was able to fix this issue and chef-client complete successfully after I removed the RPM files from the machine, and run again chef-client. Somehow the files were corrupted from the first time that chef-client ran on the machine. 

I tried to install mysql 5.7 on centos 7 and I got the below error, I was able to install successfully more than one time on different machines with centos 7, but this is the first time I encountered this issue. Any idea why it's happening and how to fix ? 

I have some complicated question: I have connected multiple VPCs with EC2 Instances (IPSec) for routing traffic between VPC's across regions, I have VPN connection to each aws region, and everything is working fine. 

I recently bought a Cisco RV110W Small Business wireless vpn firewall and I want to open a support case with the Cisco TAC. I assumed small business products came with a support period but I can't determine how to open a support case without a contract. 

I have a Cisco RV110W small office router (this configuration process is common to many Linksys/Cisco routers) and I am trying to define QuickVPN clients. When add a client of type "QuickVPN" the router gives me the following warning: (You can find a larger version of the screenshot here 

You can create a text file containing the computer names you want to scan (1 name per line) and then run the MBSA command line program as follows: 

My internal subnet is 192.168.1.0/255.255.255.0. I don't understand the warning message since a 10.x.x.1 network can just as easily conflict with a remote network as a 192.168.1.x network can. How shoud I proceed? 

The approach we standardized on was to put up a username/password logon web page (in front of the application) which accepts the credentials. When the credentials are submitted, the application would in turn validate those credentials against the directory and then respond accordingly (in a .NET world you could use Forms Authentication $URL$ to force access to the application via this login page). Since the application is doing the credential validation, you get rich information as to the nature of the login failure. In addition, even if the login succeeds but there's relevant information to display to the user, e.g. their password will expire shortly, etc., this provides an opportunity to do so. UPDATE: I forgot to mention that if you adopt this approach, you'll need to allow anonymous access to the IIS application root. This will allow access to the login web page without first attempting the automatic NTLM authentication. It's up to you whether you leave NTLM authentication enabled; perhaps you do want some clients to still automatically log in. 

I was finally able to duplicate my Magento store and now i'm able to develop in/with git. The Copy is stored in So i thought i duplicate my apache/site-available/shop to dev.shop in the same folder, change the configuration from ServerName shop.com to dev.shop.com and that should be enought. Unfortunatly all my requests for dev.shop.com become redirected to shop.com, and i can't figure out why. Logs say it is a 302 redirect. So it cant be a Javascript, and since it is before the php stuff will run it has to be the apache Server. Here is some config. /etc/apache2/sites-available/shop.com 

It might say hes missing headers. You can ignore that for now. I'm not sure what "defaults" does, I think it means to start as a "normal" programm, after finishing booting and doing deep System stuff. 

When you access a file throught a Webbrowser, you dont use the user you used to login into your domain. Instead, apache uses a localuser(at the webserver) to access this files. There are now 2 ways how to keep files from unwanted access. 

Some Information will appear in this Window. Now try to connect to the Server with a client. You can shut the openvpn down by hitting CTRL+C If this was the solution, do following for executing the OpenVPN-Server after your System has booted. Do this in the terminal: 

AWS VPN in Virginia goes down. All the traffic from my office route to AWS VPN connection LINE 1 in . So if I ping now from my office to it will be route to Ireland VPN. 4.but the VPC in Ireland with . 

I want to make sure that if one of the vpn connections/lines goes down, I will still be able to access the aws region via the other VPN in the other aws region. For example if goes down, automatically all the traffic from my office to will go to and from there it will route to with my IPSec instance, like nothing happen. example: if goes down: As I said I have connection between my regions with IPSec instances. My question is, if VPN connection LINE 2 goes down, all the traffic to will route automatically from my office to , but I guess the will reject them because the IPs are not in the same network of . So I wonder what are my options (without removing my current AWS VPN) ? To be more specific, how I can route traffic from IPs (lets say for example ) via Ireland VPN connection (when Virginia VPN connection goes down), to , but the IPs that I route are not in the same network ? The question refers only to AWS side, after I route the traffic from my office. I will be happy to answer any questions if my question is not understandable. Thanks in advance!! UPDATE - I will try to be more specific with my question: 

Making backups is really a game of probability. Assuming the data is successfully written to any media (as confirmed by the backup program's "verify backup" function), the weak link becomes the shelf life/survivability of the media. Backup tapes can break and be demagnetized. Hard disks can crash. Optical media (like DVDs and Bluray disks) degrade over time. I view the question of "Is it safe to back up to media X?" less of a yes or no question and more one of your goals and retention requirements. If you're looking at a one-time/one-off/adhoc backup that you plan to use in the short term for recovery, then it's less an issue of reliability and more a question of convenience. Assuming that you're looking at a corporate server backup solution (e.g. ongoing backups, some media rotation schedule and some retention period requirement for each backup), it's still less of an issue of reliability (since you'll assumingly have at least a daily backup) and more one of convenience. So assuming your backup process is rigorous (done according to a schedule and verified for errors) and frequent, I see no issue taking advantage of the larger capacity of Blu-ray disks. Under no circumstances though would I rely on any optical media for long term storage. For long term, tape will be most reliable. To really reduce risk of long term backup storage and avoid restore failures I think it's important to have multiple backups stored in different locations. 

which will bound every interface to listen to ssh! now change it to the ip of the internal device like: 

If you use 2 Interfaces you can configure your ssh to only listen to one of the interfaces! In your sshd_conf you'll find a line 

Assuming one would like to re-/move or mark all the Emails that contain a credit-card number. How could that be archived on a Windows Exchange Server? 

Additionally the Apache2 of the mailserver is hidden behind a nginx reverse proxy. My question is, how do i configure Apache to map all $URL$ request to my folder and all $URL$ requests to the folder. I wrote the following configuration. But the problem with this is that every request is mapped to and if i request $URL$ Apache tries to access . I could do a at this point but this doesn't feel right or is it? 

The magic Keyword in this case is Logical Volume Manager (short LVM). With this you can create volumes, and attach harddrives to this volume. Then you can mount this volume on /var/wwww It is possible to use this LVM with RAID system in Combination. So you can push speed or data-security(in case one drive breaks down). And i would reccomend you to use RAID 5 or RAID 6 if you have that many drives. Otherwise if one drive brokes down you might take it out of your lvm but you would leave a hole in you filesystem. And this will bring you in stressy troubles. I used to spend nearly to weeks to understand lvm, Raid and to set up a testing environment. But it's worth. It's like juggleing with data. 

The question about which is better is completely subjective and I wouldn't even recommend bringing it up. In fact I assume one of the forum moderators will either edit down or close the question in about 4 milliseconds because of this. The "which is better" discussion is HIGHLY specific to your needs, your politics, development and operations skills, preferences, biases and many many more options. It can't be determined by an outsider whether the change makes sense or is a management game -it could be a combination of all or none. This question should be asked to people in your company. Costs are also nearly impossible to compare without significant analysis. Microsoft and Lotus generally sell each corporate deal at a different price, not to mention possible resellers and their discounts, and features you want, number of servers, etc. It's impossible to answer any of this so I recommend directing the questions inward towards your organization. 

I've got a standalone Windows Server 2003 running SQL Server 2005 and a Windows Server 2003 Active Directory domain controller. Using maintenance plans/SQL Server Agent, I'm trying to write the database dumps from the standalone box to a share on the DC. I know the usual rules about accessing remote shares (e.g. must use a logon account which has proper rights, etc.). In fact, writing the dumps to another non-DC server in the same domain as the DC works fine. I'm trying to set the SQL Server Agent account's logon credentials to "domain\username" (or username@addomainname). If I specify a username in the form of "domain\username, the error I get (regardless of password) is: "The account name is invalid or does not exist, or the password is invalid for the account name specified". If I specify a username in the form of "user@addomainname", the error I get (regardless of password) is: "The specified domain either does not exist or could not be contacted." I've turned on logon failure auditing on the DC and I see no failures in the log, which suggests to me that the machine isn't even trying to authenticate, but rather failing prior to that. I know that users on non-member servers can authenticate to shares on a DC, because doing an interactive logon (e.g. "net use * \dcname\c$ /user:username@addomainname", or using the other form of the username) works fine. The above example is about SQL Server but applies to any Windows service. Why can't the service log on with the domain account, but an interactive logon (drive mapping) using that same account works? 

It seems like, you do not have an automatic start script! Which is usual. OpenVPN does not install such a thing by it self, as far as i know... First of all try executing the server manually. This will need an own Terminalwindows to run. And for the time OPENVPN is running this way, the TerminalWindow will not be usable for other stuff. Try this (replace /etc/openvpn/config-files/ with your Path): 

This will send every 60 Seconds a keep-alive message to your remote-host. if this does not work you can reduce the time. The other way (not so secure like the first way) On the Server, open your /etc/ssh/ssh_config create if it does not exist And add: 

This will send evry 300 seconds a keep alive signal for a maximum of 2 times. If you like to have it to the infiniti set ServerAliveCountMax to 0 If you wold like to keep alive FROM the client, do exactly the same but without "Host *" 

but when i echo $COMMAND it looks very right, in fact, when i copy it from and insert it, it works..