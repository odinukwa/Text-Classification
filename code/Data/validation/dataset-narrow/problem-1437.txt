Solution 1: Make your texture blue/white coloured This will only work if your use-case only requires you to draw on the blue background you've shown in your example. If so, you could use a disc texture that goes from blue (edges) to white (center) and use the standard 100% source pixel blending. Be sure to render from left to right obviously. 

Solution 3: Closest Hex Center by Comparing Two Points To save a minuscule amount of processing time, you could note that only two points need to be checked. Instead of taking two points of the row above, and two points of the row below, only take one point of each row. Choose the point that is closest in x-coordinate. After all, when calculating the distance to these points, only their x-distance is different (dist = sqrt(pow(x_dist,2) + pow(y_dist,2)); ). Note that this trick doesn't work to compare points of different rows, as their x-coordinates are not aligned (due to the shift of odd rows). The image below illustrates this. You'd do roughly the following steps: 

It has the property that it will be zero for two perpendicular vectors and one for two parallel vectors. Note that this last property assumes your vectors are normalized (=they are unit length). For intermediate angles, the value of the scalar product increases monotonically with increasing angle between the vectors. 

I may have made some mistakes concerning the terminology of the transformations and the coordinate systems, but this should give you the general idea. 

You should see the back-end as a centralized component for storage and processing. This structural component is used for various reasons. 

Alpha calculation approach To get rid of aliasing, you could adapt the previous approach. Rather than using the circle's parameters to check whether to discard the fragment, you could also use it to calculate an alpha value. To get rid of aliasing, you could specify to radii rather than one [r_inner, r_outer]. Everything inside the inner radius will be fully opaque and everything outside the outer radius will be fully transparant. For fragments inbetween the radii, you could apply linear interpolation. This will give you a smoother border. For example, using a 2 pixel difference between the inner and outer radius will give you a 2 pixel border around your circle where pixels smoothly transition from fully opaque to fully transparant. In short, you would calculate the alpha as (pseudocode, not actual GLSL): 

Solution 1: Rotate your UVs in your vertex shader The vertex shader is where per-vertex attributes can be modified before rendering. These values are then linearly interpolated to generate values for all fragments of your polygons. As a rotation of your your UVs is a linear transformation, you only need to recalculate the values in the vertices, and linear interpolation will take care of all intermediate fragment values. (Note that in case of a non-linear transform such as u_new = sin( u_old ), you'd have to calculate the new UVs for every fragment individually. In this case, the transformation belongs in your fragment shader.) The UVs are a 2D coordinate pair with values ranging from 0 to 1: [0, 1] x [0, 1]. (These values can sometimes be outside of the 0,1 range, in this case clamping, wrapping or repeating of the texture occurs, depending on how you specified your OpenGL texture hints) In your vertex shader, you could rotate your UVs prior to the texture sampling to obtain a rotation. I'll assume you want to rotate the texture around its center coordinate (0.5, 0.5) (if the rotation origin varies from texture to texture, you could pass it as a 2f uniform). To enable animation, I'd suggest passing the angle as a 1f uniform. You can calculate the new UVs using a concatenation of three transforms. You can't just do a single transform (a rotation) because this would rotate the texture around the (0, 0) point. Instead you want to use (0.5 ; 0.5) as your origin of rotation. The image by Jon at the bottom of this answer illustrates this. You can use these transformations: 

These parameters will let you use a wide variety of sheets. Here's an explanation and motivation for the chosen parameters: 

Solution: Clipping Between Z-Values 0 and 1 If you want to clip in range [0, 1] instead of the default [-1, 1], you simply need to remap the z-coordinate to this range. You'll want to do this after all vertex transformations have been applied in your vertex shader, so that you are working with screen coordinates. These are the coordinates where clipping is applied. To remap, simply do: 

In short: Where do I move my object, if I know exactly where the first collision would occur, without getting it stuck in other object? 

In summary: the easiest way to solve the problem for your use-case is to separate your movement axes. The way it is solved in commercial games involves calculating more information such as the collision position and the collision normal. 

Calculate your motion_vector as (velocity_x, velocity_y) Perform a ray-cast along ths motion vector to obtain collision_position and collision_normal Move your object to the collision_position (or slightly before it) so that it doesn't intersect with other objects anymore Calculate your new motion_vector, as your current motion_vector minus the component of your motion_vector parallel to the collision_normal Shorten the length of your motion_vector by the distance you've already moved Repeat steps 2 through 5 until the object managed to move the full length of the motion_vector 

Normalize the local X-axis and Y-axis vectors of your cube Calculate the scalar product of your up-vector with both these normalized axis vectors If the scalar product with the X-axis is bigger, bind horizontal mouse movement to its rotation and bind vertical mouse movement to the Y-axis rotation. If the scalar product with the Y-axis is bigger, do the inverse. 

A feasible way you could do it For that reason, I think only a procedural approach is viable in this case. A procedural approach means that the planet's geometry and textures get generated on-the-fly, via some mathematical formula. Techniques for this are readily available if you do a search on "terrain generation" or "procedural planet generation". This approach let's games like Spore and Destiny generate planets with detail up to surface level. A downside is that you have less control over what your planet will exactly look like. For example, if you want a planet that looks exactly like earth (with bodies of water, mountians, roads, etc in the same position, you will not be able to do this in a procedural way). It is however the only feasible way to "store" an entire planet in great detail. (The key is that, rather than "storing it", you are encoding it as a set of parameters, generating actual geometry and textures on-the-fly). Something to be aware of (as Rioki pointed out), is that textures slapped onto a sphere will not cut is. At higher levels of detail, you will need to deform the geometry (for example around mountains). Techniques such as displacement mapping or tessellation will help here. Procedural methods will give you this data for free however. In short, your question is a problem of scale. A procedural approach is a very feasible approach for planet rendering in general, but I'm not sure if it fits your use case. If your use case is to render planet earth specifically, at a level of detail Google Maps offers, I'd say that's impossible as you cannot store the amount of satellite imagery that would be needed for the textures. 

I ran into a question I can't seem to solve, while implementing the movement/ collision checking code for my game. The game is in 2D and all objects in my game use either AABBs or circles as collision masks. My current procedure for moving a game object uses the following steps: 

In summary, area lights are a lot more difficult to calculate than point lights, using a standard rasterizer. If your use-case is non-time-critical (e.g. an offline renderer for videos), you could look into raytracing as a rendering technique. If your use-case is time-critical (e.g. a video game), you might be more interested in approximating your area light using multiple point lights. I'm not an expert at lighting equations, so I can't provide you with the exact mathematical solution unfortunately. I may have also butchered some of the lighting and rendering related terminology. 

I think you have two options. (1) You could go for a view-space effect where you render everything first and then apply the heat-distortion as a post-processing effect to the 2D rendered scene. (2) Alternatively you could apply the effect in world space through vertex displacements in the vertex shader stage. (1) View-space distortion could make use of the depth buffer to estimate the depth of objects, as jzx mentions. The advantage of this approach would be simplicity, as you are just operating on a part of the final 2D projected scene - i.e. a zone of pixels. The disadvantage you only have information of the nearest polygon for every pixel. This means distorting a foreground object can never reveal objects located behind it. This effect might therefore look give the impression of a fake, flat effect. (2) By working in world-space, you can apply distortions to the vertices of all objects. The advantages are: 1. distorted objects can reveal objects behind them, 2. distorting of vertices is very efficient compared to distorting individual pixels (as the rasterization process, in hardware, will take care of all fragments inbetween the vertices) - unless you have a scene with (extremely) many overlapping objects, 3. you can use a 3D function to drive the intensity of the distortion throughout space, which seems very elegant as compared to using a bounding volume. 4. the effect is applied in 3D, meaning distant objects will natrally have less distortion than near ones, solving the problem of the "fake, flat distortion effect". In summary, you can go for 2 broad approaches: distortion in view-space and distortion in world-space. I would use the latter for better visual results, more elegancy and likely more performance. 

Solution 1: Z-buffering for pre-rendered backgrounds Ideally, you would want to exploit this mechanism for your pre-rendered backgrounds as well. Technically, you would do this by rendering two passes of your background: the actual color data and a depth map. 

How I assume VAO's should be used From simple examples, I've understood that correct usage of VAO's is as follows: Setup 

For example, a point that is exactly at the left bound of your camera will have a screen x-coordinate of 0. A point exactly at the right bound of your camera will have a screen x-coordinate of screen_w. Other points will evaluate to their correct position as well. 

Basically, what is happening is that your check the "vertical boundaries" of your hex cells in steps 1 through 4. In step 5, you check one "diagonal boundary" to figure out what cell the points belongs to. 

This article by Itay Keren excellently covers camera systems: $URL$ He explains a plethora of 2D camera techniques, along with their advantages and disadvantages. This should let you design the perfect camera for your use case. You'd probably be most interested in zoom-to-fit, anchoring, region-focus and multi-focal camera techniques. As he covers too much material to summarize in a small reply here, I hope this link-only reply covers what you need. 

Approach 2: Approximate the area light with point lights Alternatively, you could approximate your area light with multiple standard point lights. While your render will not be physically correct, it is possible to closely approximate the area light. This way, you can make use of standard rasterization. Rasterization is less flexible than raytracing, but it is much faster. This makes it suitable for real-time rendering. Rasterization (in its basic form, using a simple illumination model such as Phong illumination) is a local lighting solution. When a polygon is being rasterized into fragments, only the fragment itself is considered. The entire rest of the scene is considered non-existant at that point. You are just calculating angles relative to the normal, camera and light sources, ignoring shadow casting, reflecting of light rays etc. 

Note that this will only work assuming all-rectangular geometry and small movement steps. For example, your objects would not be able to through an upward corridor smoothly as they will - as a matter of speaking - alternate between banging into the ground and into the roof. An additional disadvantage of your technique is that objects will not be able to stop right against the wall. For example, if their horizontal velocity is 10 and their is a wall nearby, they will end up with a gap of 0 to 9 units between them and the wall. 

Solution: Calculating the bounds of your assets The above explains how to find where a single point would end up in screen coordinates. You can use this information to figure out where an asset goes as well. Again, rather than thinking of an asset as a centerpoint on top of which a scalable sprite is drawn, you could think of it in terms of bounds. If you know the left, right, top and bottom bounds of your sprite (in screen coordinates), you know exactly where and how to draw it. In essence, you are just drawing a rectangle on your screen, and you are drawing the asset inside of this rectangle. Thus, to figure out how to draw your asset in screen coordinates, do the following: 

Here are two options you could consider; an easy one and one that you'd find in commercial games. Solution 1: Separate your movement axes If your objects are all rectangles (assumed to be axis-aligned), you could consider horizontal and vertical motion separately. This means your motion procedure will be updated to: 

Active Polling means that you check the state of your input devices on every update. For example, the Logic() (or Update() or Tick()) function of your character could contain: if (input.isPressed(KEY_RIGHT)) { positionX += 10; }. Events on the other hand let you react to events generated by your input device. For example, if you press the right arrow key on your keyboard, a KeyboardKeyEvent(KEY_RIGHT, ACTION_PRESS) event might be launched. You can then register your character object to listen for these events.