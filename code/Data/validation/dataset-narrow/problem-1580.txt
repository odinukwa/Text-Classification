It sounds like your dataset is going to need some work before you can start applying a model to it, for example you will have to translate customer location into something more usable, like distance from depot, or Urban/Suburb/Country labels perhaps? Andrew Ng's Coursera course is a popular starting point, many beginners seem to find it useful. You'll get a good understanding of machine learning algorithms and how to arrange your data to get the best out of the models. Good luck! 

There are a number of methods to automate the optimisation of your hyper-parameters, such as GridSearch and RandomSearch which the article you linked discusses briefly. The main reason to choose one over the other is if you want the best possible parameters, and don't care how long it takes to get them: go for GridSearch. On the other hand, if you don't want the optimisation to take a long time, but still want some good parameters then go for RandomSearch. These two implementations in ScikitLearn aren't exactly "advanced packages" but they'll get the job done for any model in Scikit (Random Forest, MLPClassifiers, etc). Emre's comment also has some pretty cool advanced packages which are Scikit compatible or Tensorflow compatible. 

First of all when I plot the feature importances I get some interesting results which do not agree with the feature importances from my random forest. I put this down to algorithmic differences since many do align or are similar, but my specific concern is the binary feature 'registeredEmail', which is clearly very important in the random forest model, but not in the other (gradient boost, as shown). This is not my main concern, just adding in case relevant to my question. Secondly, and this is the part I am most confused by. Why does my feature importance chart show that 'playerInAlliance' (another binary feature) is significantly more important than 'registeredEmail', since when I check the partial dependencies there is a much steeper slope for 'registeredEmail'. I would interpret this as the churned prediction being highly influenced by a player not having a registered email, compared to being less likely to predict churn if the player does have their email registered. Is this correct? 

I need some help understanding my partial dependence plots for binary features passed to a GradientBoostClassifier when comparing them to the feature importances. For some background, my goal here is to investigate user churn. My classifications are: 0 = not churned, 1 = churned. 

You're an online retailer. Like Amazon. You keep your purchase data for different categories of items in different tables, but all website users have one account with one ID. Inner Join: You have two datasets, one with User IDs and purchases of clothing data, the second dataset has User IDs and purchases of books data. You want to find out who purchases both clothes and books from your site. You inner join to find the User IDs and purchases for people who bought clothing and books. Any users who didn't buy both of these items will be 'dropped' from the final table. Left Join You have one dataset with User IDs and account info(e.g. age, name) of all users. You want to build out this table to include some sales data, without dropping people from your complete list of users. So you would left join your purchase dataset to your account dataset (with User ID as the key). Any user who has made no purchases won't be dropped from your final table. Right Join The opposite of left join. E.g. you want to keep all purchase data, even if for some reason your Account Info table does not contain the ID of the user who made that purchase. Full (Outer) Join You want to Join account info and purchases tables, but you don't want to lose any entries in either table. So your final table will include Users who have made no purchases, and (perhaps mysteriously) purchases who have no users associated with them. 

You could calculate the RSI (relative strength index) of your sales, over 1 month, 3 months, a year or however long of a time interval you wanted to measure if the current positive trend is outweighing the negative trend over that amount of time (or vice versa). You could also try ADX (average directional movement index) combined with +- directional indicators to again detect the strength of the trend, and also identify if it is a positive or negative one depending on which directional indicator outweighs the other. I'm sure there are many other traditionally stock oriented statistics you could use too. 

What you're describing is known as a multilabel classification. You want to predict some output labels (adverse effects) in a given set of possible output labels (all possible adverse effects), using a given a set of input indicators (previous adverse effects, drugs prescribed, dosage levels). For those final two features, I'd combine them into one feature where if the drug is not prescribed obviously dosage is 0, and higher otherwise. In R, you can use the mlR package for this prediction stage. You may also be able to use the generateFilterValues() function in this package to extract feature importances, however I'm not sure if this will work with multilabel and additionally this will only tell you how much your classifier believes your input features contribute to the output labels relative to all the other input features. It won't give you other valuable information for example at which dosage levels of a particular drug does the probability of negative symptoms increase most. Again, I'm unsure how this would work with multilabel classifications as I don't have any experience with them, but take a look at plotting partial dependency plots to get this last bit of information on how each input feature contributes to your output labels. 

Finally, if I plot both of these binary features against each other on one plot, we can see that dependence on a player not having a registered email is massively higher than if the player is in an alliance or not. So why is the registeredEmail feature less significant in terms of importance? TL;DR: How do I interpret Binary features in partial dependence plots? Any help appreciated. 

I think you need to be teaching them a popular Data Science language like Python or R. Excel is not going to help them in a real job, and isn't practical for data science purposes. I would probably say Python would be most valuable to them in the long run, and with packages like scikit-learn your regressions and classifications can be demonstrated in very few lines of code which they can read and understand more easily. It is not always easy to understand what R is doing by just reading it. Another word of advice: Don't waste time forcing your students to set up an IDE and download the necessary packages, if you use python create a virtual environment for them with all the necessary packages, and set up an IDE like pycharm(they can get this and most other IDEs under a student/academic license) where then can develop and run their code through UI rather than console which they may find daunting and confusing. If you go down the R route then make sure you have an IDE like RStudio set up for them and make sure all of the includes and package installs are either included in your example code or fully described. 

Split my data into training/test. Use GridSearch with 5Fold Cross validation to train and test my estimators(Random Forest, Gradient Boost, SVC amongst others) to get the best estimators with the optimal combination of hyper parameters. I then calculate metrics on each of my estimators such as Precision, Recall, FMeasure and Matthews Correlation Coefficient, using my test set to predict the classifications and compare them to actual class labels. 

Rather than sampling your negative values to achieve a 1:1 ratio, you should try weighting your classes. You can simply use the existing positive:negative ratio you have already calculated and pass this to the class_weights parameter in skflow (assuming you are using skflow, if not, there are almost always equivalent ways of doing this in any ML package/language). Example: class_weight = tf.constant([0.9, 0.1])) skflow.models.logistic_regression(X, y, class_weight=class_weight) The reason I suggest this is that your random all positive or all negative outputs may be down to the way you're down-sampling the negative examples. If you're doing this at random, you may be removing some key examples in the dataset that the classifier would have used to learn to strongly distinguish between the classes (for example: stock data, where two candles show a drop, but one shows much more of a drop). By weighting you can train on all of your data and remove this potential problem, whilst still 'learning' both classes equally. 

Please note these plots are in descending order of feature importance, from left to right. Also ignore the title. Comparing this to 'playerInAlliance' I can see that no matter if the player is in an alliance or not, this feature is influential in predicting a churned player. I don't understand how to interpret this, as if there's no real difference between players churning when in an alliance compared to having no alliance, then why is my GradientBoostClassifier considering it to be a highly important and highly dependent feature? 

You're right that it is a predictive model, but analysing feature importance, partial dependence and other metrics/plots are intended to allow you to gain an understanding of the classification process of models which are often very black-box in nature. Yes, you can use this to gain some understanding of the underlying data, but I'd be more inclined to say that the primary reason for looking at feature importance is to understand your model rather than to understand your data.