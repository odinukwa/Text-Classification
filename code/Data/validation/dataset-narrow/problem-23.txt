So the new height would be 607.5, but it probably gets rounded up to 608 since that's an even number. So now, when each frame is displayed, it first gets scaled down (probably using the GPU since it's very fast at that) from 1920 x 1080 to 1080 x 608. 

Simple lines will just be combinations of and sequences. The curves will be either circular arcs, quadratic bezier or cubic beziers. To convert a Bezier curve into line segments, you can do the following: 

You should look into Medial Axis Transform or Straight Skeletons. They are often used to generate the information you need to bevel things properly. They give you a distance from the bounds of your object at any point in space, which allows you to generate bevels correctly. They are difficult to calculate, though. You may also want to look into this paper from Valve Software for an alternative way to generate the distance field. 

That quote is a very strange way to phrase things. We definitely use 3x3 matrices in computer graphics. They tend to be most useful for doing affine transformations of 2D objects. It allows you to have scale, rotation, shearing, and translation (in 2D), but not perspective transformations. I believe that's what the quote is trying to say. To get a perspective projection of a 3D scene onto a 2D plane (such as your computer screen), at some point you will need to divide the x and y components of each point in your geometry by some factor that foreshortens lines in the z dimension. That's the divide that they are referring to. See this article for more details. In it, they describe: 

Note that if you attempt to calculate perceptually uniform amounts of color saturation you end up with systems like the Munsell Color System where the shape of the color space is highly irregular compared with the cubes and cylinders of other color spaces. If you tell us what you're trying to achieve, we might be able to offer an actual solution to whatever the problem is. 

And just add the distances of each part of the curve until you're out of curves. Note that this assumes cubic bezier curves. For quadratic, the function would calculate a quadratic one (like described here). 

Yes. Generally scatter-gather algorithms work poorly in OpenGL as the data locality is poor. When you have to grab bits from all over the input image for each output pixel, it's going to be slow or nearly impossible to implement in a typical GL shader. Likewise, when a calculation on an input pixel results in data that needs to be spread out to several output pixels, it's a poor fit. It's hard to tell just from the abstract, but it looks to me like this wouldn't be a great fit for a typical OpenGL shader. I think you'll end up needing to sample an area of the input, classify it, then sample from one or more of a large set of small exemplars. I mainly work on iOS and macOS which both come with OpenGL and OpenCL. Not sure how Windows and Linux handle that. 

No, it's not. The way many of the 3D sensors work is by projecting an infrared pattern onto the surface and measuring how it distorts. But with a 2D image, the pattern will simply be projected onto the flat 2D image, not onto the objects in the scene. So the 3D sensor will only sense a flat card. Other methods work by combining 2 images taken with different cameras at different angles. This will have the same result as above. The 2D photo will be interpreted as a 2D surface by the combination of the 2 cameras. 

Generally, the "a" channel is referred to as the alpha channel. It represents the amount of a pixel that an object covers. For computer-generated imagery this is pretty straightforward. If I generate a circle, parts of the circle on the interior will cover a full pixel, but parts on the edges of the circle may only cover a portion of the pixel. Internal pixels would have an alpha channel of 100%, whereas those edge pixels that only cover a portion of the pixel would have a value less than 100%. Outside the circle all pixels would have an alpha value of 0%. For captured imagery, generally all pixels have an alpha value of 100%. However, if you use keying or matting tools to remove a green screen or blue screen, for example, there will be some pixels that are partially green (or blue), and they may end up with an alpha value between 0% and 100%. pixels that are solid green (or blue) will have an alpha value of 0%, and pixels that contain no green (or blue) will have an alpha value of 100%. This is used when you want to composite one image over another. For normal compositing, you will mix the top and bottom pixels by multiplying the color value of the top pixel by its coverage or alpha, multiplying the color value of the bottom pixel by the inverse of the alpha (1.0 - alpha), and adding the two together. So it would look something like this: 

Now you have the 4 pixels needed for interpolation, but there was no fragment shader math done to get the texture coords, so there are no indirect look-ups which are slower than direct look-ups. 

OpenGL uses column-major matrices. For example, the translation values will be in the last row rather than the last column of the matrix. For example when loading matrices into uniforms in glsl, the function takes its matrix parameters in column-major order: 

The most straightforward way would be to put x into the red channel, y into the green channel, and z into the blue channel of each pixel. You could also make a 1 channel image where intensity is used to describe height. This is called a height map. Usually the x and y of the mesh are the same as the x and y of the image, so no need to store them in any way. 

What's probably happening is that you're doing indirect texture fetches. Are you doing something like this: 

One technique is adaptive shadow maps where you use a low resolution map for shadows that are rendered far away from the camera, a mid-resolution map for stuff that's in the mid range, and a high resolution map for stuff that's close. 

This is a very broad question. The basic way to do this is to generate 2 frames - one for each eye. You can do this using any modern 3D API from OpenGL to DirectX to Vulkan, Metal, or DX12. You'll need to understand things like interocular distance, depth cues, and 3D compositing. You'll also need some way to allow the user to move the curser in and out, so you'll want to read up on various user interface, visual interface, and user experience (UI, VI, and UX, respectively) techniques. 

I think if you further modified this algorithm to make the radius of each cell be related to the luminance of an input image at its location or make its color lighter for higher luminance positions, you could get such a half-tone look. 

There are a few ways you can do it. Basically, you need to get your photo into a format that's suitable for using as an environment map. There are a number of such projections available. The most common ones I've run into for 360° video are: 

And a similar function for the icosahedron. Next, you'll create a matrix for every object you draw and send that to your shader before drawing it. So you'll have the translations for the vertices of the (big) icosahedron, and you want to draw a (small) icosahedron at each one. You'd do the following: 

However, calling frequently can hurt performance, so it's best to conditionalize it so it only gets called in debug builds by doing something like this: 

From the same link, though, there are several non-regular pentagons you can use to tile it as shown in the above link. Additionally, you can do a pentagonal/hexagonal tiling of the plane. If I've misunderstood what you're trying to do, please clarify. 

Where is a function that calculates a point on a bezier curve given the parameter and control points through : 

A few points that you probably already know, but that I just want to put out there for others reading this. Filtering in this case refers to low-pass filtering like you might get from a Gaussian Blur or a box blur. We need to do this because we are taking some media that has high frequencies in it, and rendering it into a smaller space. If we didn't filter it, we would get aliasing artifacts, which would look bad. So we filter out the frequencies that are too high to be accurately reproduced in the scaled version. (And we pass the low frequencies, so we use a "low pass" filter like a blur.) So let's think about this first from the point of view of a blur. A blur is a type of convolution. We take the convolution kernel and multiply it by all the pixels in an area and then add them together and divide by the weight. That gives us the output of one pixel. Then we move it over and do it again for the next pixel over, and again, etc. It's really expensive to do it that way, so there's a way to cheat. Some convolution kernels (particularly a Gaussian blur kernel and a box blur kernel) can be separated into a horizontal and vertical pass. You can filter everything with just a horizontal kernel first, then take the result of that and filter it with just a vertical kernel, and the result will be identical to doing the more expensive calculation at every point. Here's an example: Original: 

Keep in mind that you don't have to set every value for every control point you pick. I assume any you don't set are defaulted to 0, or some reasonable default. So for a hill, you would set to the height of the hill at a few point. You would then set and to be horizontal, so yes, . The length of and will decide how quickly the hill falls off as you move away from the ridge line. It looks like the above 6 values are turned into another cubic Bézier curve that is perpendicular to the first one for the rasterization step in section 4 of the paper. doesn't factor into the shape of the perpendicular Bézier because it's also perpendicular to it. So , , , and are used to make a cubic Bézier. I believe that you can think about like this: the point on the ridge line where you define one of these control points is the link between 2 cubic Bézier segments - one to the left of the point and the other to the right. The images they show only show a single point between 2 Bézier curves, but the way I'm reading it there are actually 2 curves that look something like this: 

Due the way floating point values work, you probably don't want to do an exact equality check. You probably want something more like this in your loop: 

In order to dissociate the rotation from the CPU speed, you need to use some sort of timing in your function. I recommend passing in a time value of some sort. The easiest thing is to pass in the number of seconds since the app started. How you do that depends on your particular OS, which you didn't specify. Once you have a timing constant, you can generate a triangle wave to control the rotation. This formula will oscillate between 0 and 1 over time: 

Yes, it's quite common in tools that allow compositing. In Photoshop, for example, there are blend modes that are even named "Add" and "Multiply" and they do just that. (Though they assume the values are normalized to a 0-1 range.) If you are working with 8-bit per channel images, they will clamp, but if you are working with 32-bit per channel images, they will simply have values greater than 1. If done with linear RGB, this mimics light fairly naturally, and you can apply natural photographic operations to the results, like decreasing the exposure and it will bring those blown-out highlights back into the displayable range. As we move into the era of High Dynamic Range output devices, this type of processing will only become more common and useful. 

While clamping or saturating may be the most common way of handling this situation, it's usually not the best. (But wrapping/modulus is one of the worst.) Another way of handling it is to either convert the input to 16 or 32-bit ints or floats and do all calculations in the higher bit depth. You may want to down-convert the final output, if required by your processing tools. Or, if you can choose the output format you can leave it in a higher bit depth. Most professional tools these days do their work in higher bit depths. Regarding your question: 

In this case, the direction vector is already normalized. If you do have rotations around all 3 axes, you can calculate the direction vector using spherical coordinates: 

If so, that can slow you down. What you can do instead is to pass the texture coordinates for each sampler call into your vertex shader from your CPU code. You'd set it up as if you were doing multi-texturing, but all the texture units would point to the same texture, just sampled at different coordinates. Taking the 2D case, instead of passing in a single texture coordinate like this: 

If you can't go to OpenGL 4.3 (or your implementation doesn't support s), you can simplify your code in a number of ways. The first is to move the calls to into a function like this: 

Generate a list of random locations for the "cells" At each cell's location, draw a small circle where: 2.1. the depth at each pixel is the inverse of the distance to the center (so highest at the center, and lowest at the edges) 2.2. the color is a radial gradient (in this case black at the center, white at the edges) 

where is the rotation around the y axis and is the angle above/below the x/z plane. The direction should already be normalized in this case, too. 

It's not entirely clear what the scene in your picture is supposed to look like, so I'm not sure I'm interpreting it correctly. But if I am, it seems like the problem is that you're projecting the shadow properly, but onto the wrong face of the gray wall. If you want to research it, you might have luck looking up winding order (sometimes called winding number), or back-face culling. If I'm correct in what's shown above, I think you could solve it in your shader by only applying the shadows to the current fragment if the vector from the light source to the surface points away from the surface normal. In other words, take the dot product of those 2 vectors, and if it's negative apply shadows, otherwise, don't. 

It appears to me that in the image, the saturation of the original image is 0 since it is grayscale. (Unless I've misunderstood the question.) When the saturation is 0, the hue is undefined, but it appears that the GIMP is setting the hue to 0. It is then using the sliders from the dialog to modify the HSL. It adds 180° to the hue (so from 0 to 180 which gets you from red to cyan). The saturation is increased by 75 units, and the lightness is kept the same. It may have special protections for black and white, given that the white part of the letter "A" remains white in the final image (although even there, it appears to have an RGB value of (0.99, 1.0, 1.0), whereas on the right, it's fully white (1.0, 1.0, 1.0)). You can always look at the source code since it's an open source package.