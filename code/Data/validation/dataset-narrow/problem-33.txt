So in the original problem, you've been given the requirement of having a field of view for your camera of 135° (3/4π radians), and a window that's 15x15 pixels large. So you need to construct a projection matrix with a 135° field of view and an aspect ratio of 1:1. The view parameters aren't specified in your problem, so I guess the camera is at the world's origin, so camera/view space and world space are the same for now. Normally, you'd take the world coordinates, ( and ) and multiply them by the view matrix (identity here, so no work to do), then multiply that result by the projection matrix to get to clip space. Once you have coordinates in the normalized range, you can multiply them by the window's size (15x15 in this case) to get them into screen space and determine which pixels to set. 

Swapping color channels is called channel swap or swizzling. (Searching for "RGB channel swap" brings up lots of relevant results.) Looking at your image, the squares in the right half appear to be flipped across the diagonal. Most editing apps have a channel mixer tool that allows you to calculate each output color channel as some combination of the input color channels. In Photoshop it's called "Channel Mixer". I've also seen it referred to as "Color Operations" or just "Color Ops". The flipping can usually be done with a horizontal flip followed by a vertical flip. 

You could always write each frame out as an image and then use another tool to compress it into a movie that you playback. You don't say what type of system you're running on, but I believe that ffmpeg could do it for you, as it's a cross-platform tool. It's hard to estimate how long the compression will take since it will depend on the content you're compressing. I'm not sure whether you can continuously feed it frames, or if you need to generate them all at once. 

Overview The main reason for Virtual Texturing (VT), or Sparse Virtual Textures, as it is sometimes called, is as a memory optimization. The gist of the thing is to only move into video memory the actual texels (generalized as pages/tiles) that you might need for a rendered frame. So it will allow you to have much more texture data in offline or slow storage (HDD, Optical-Disk, Cloud) than it would otherwise fit on video memory or even main memory. If you understand the concept of Virtual Memory used by modern Operating Systems, it is the same thing in its essence (the name is not given by accident). VT does not require recomputing UVs in the sense that you'd do that each frame before rendering a mesh, then resubmit vertex data, but it does require some substantial work in the Vertex and Fragment shaders to perform the indirect lookup from the incoming UVs. In a good implementation however, it should be completely transparent for the application if it is using a virtual texture or a traditional one. Actually, most of the time an application will mix both types of texturing, virtual and traditional. Batching can in theory work very well, though I have never looked into the details of this. Since the usual criteria for grouping geometry are the textures, and with VT, every polygon in the scene can share the same "infinitely large" texture, theoretically, you could achieve full scene drawing with 1 draw call. But in reality, other factors come into play making this impractical. Issues with VT Zooming in/out and abrupt camera movement are the hardest things to handle in a VT setup. It can look very appealing for a static scene, but once things start moving, more texture pages/tiles will be requested than you can stream for external storage. Async file IO and threading can help, but if it is a real-time system, like in a game, you'll just have to render for a few frames with lower resolution tiles until the hi-res ones arrive, every now and then, resulting in a blurry texture. There's no silver bullet here and that's the biggest issue with the technique, IMO. Virtual Texturing also doesn't handle transparency in an easy way, so transparent polygons need a separate traditional rendering path for them. All in all, VT is interesting, but I wouldn't recommend it for everyone. It can work well, but it is hard to implement and optimize, plus there's just too many corner cases and case-specific tweaks needed for my taste. But for large open-world games or data visualization apps, it might be the only possible approach to fit all the content into the available hardware. With a lot of work, it can be made to run fairly efficiently even on limited hardware, like we can see in the PS3 and XBOX360 versions of id's Rage. Implementation I have managed to get VT working on iOS with OpenGL-ES, to a certain degree. My implementation is not "shippable", but I could conceivably make it so if I wanted and had the resources. You can view the source code here, it might help getting a better idea of how the pieces fit together. Here's a video of a demo running on the iOS Sim. It looks very laggy because the simulator is terrible at emulating shaders, but it runs smoothly on a device. The following diagram outlines the main components of the system in my implementation. It differs quite a bit from Sean's SVT demo (link down bellow), but it is closer in architecture to the one presented by the paper Accelerating Virtual Texturing Using CUDA, found in the first GPU Pro book (link bellow). 

Rigid transforms - translation, reflection, and rotation. These perserve the distances between every pair of points on objects. Affine transforms - rigid + shear and (possibly non-uniform) scale Perspective transforms - Affine plus perspective transforms (think of a rectangle rotated around the upward pointing axis - the part closer to the camera/viewer may be larger than the pre-transformed object, and the part farther away may be smaller) 

If you look at chapter 8 of the SVG Specification, it describes how to parse a element. The short version is that you'll want to find the attribute of the element. That element should be a string describing the curve. It will contain the following commands: 

When you use in your source code and in your shader, you're telling OpenGL that you want to represent the texture coordinates as 0 to width of texture and 0 to height of texture rather than using normalized (0-1) coordinates in both directions, which you would get with and . A shift of 0.2 might be hard to notice on a large texture, as it's less than 1 texel. If you want to move it by 20% of the width, you need to pass in the texture width and height to the shader and calculate it (or just pass in what 20% of the width is). 

The usual way of doing this is to work in a color space that has luminance as one of the color components. For example the Y'CbCr color space has a luminance channel (you can think of this as similar to brightness which is a poorly defined term) and 2 chrominance or color channels. There are other color spaces that separate colors in this way, too, such as HSV, HLS, Lab, etc. There are conversions to go between RGB and these various color spaces. The article linked above shows some of the conversions for going between Y'CbCr and sRGB, for example. You don't say what type of graphics library you'll be using, so it's hard to give any more detail than what I've given you here. But I've done this in OpenGL, Core Graphics, and others. It shouldn't be too difficult. Edited to add: I should point out that HSV defines value or "brightness" not as luminance but as the average of red, green, and blue, and as such does not track very will with our intuitive sense of brightness. HLS also suffers from some issues (discontinuities, bad but popular reference implementations) and is generally not recommended by the color scientists that I know. 

In detail, supposing I want to change the second vertex element to (1.0f, 1.0f, 1.0f). I know it with compute shader, but I just wnat to know how to do it without compute shader or OpenCl. Is it possible to change the buffer data uploaed directly without uploading the whole data again? 

Let's suppose the start point of the ray $R$ is $(x_0, y_0, z_0)$ and direction is $(x_d, y_d, z_d)$, then $R = (x_0, y_0, z_0) + (x_d, y_d, z_d)t$ , and the plane is $ax + by + cz + d = 0$. To check intersection between the ray and plane, calculate the below equation. $t = \frac{-(ax_0 + by_0 + cz_0 + d)}{ax_d + by_d + cz+d}$ If $t \ge 0$, then there is a possibility of intersection, so go second step. else there is not intersection. 

I'm working with OpenGL and facing some difficulties because I'm not familiar with OpenGL. I tried to search related example in Google, but I could not find some useful code. There are five arrays. I bound and changed the data in compute shahder as follows: 

for better accuracy, instead of GLfloat, I bound double data and tried to use the data in shader as follow: (I just tested to draw single triangle.) cpp. 

I'm using GLUI for user interface. I tried to add Listbox with several items. I could add items like below image, but if I clicked the one of them (None, AAA, BBB). The program was stopped. It looks like accessing NULL data. 

I'm studying spring model. There is a suggested equation (Hooke's Law vector form) But, I couldn't understand how to derive that equation. I'm reading 'Computer Animation Algorithms and Techniques Third Edition. In the page 241, (7.88) The equation is as follows: ($|v_{1}^* - v_{2}^*|$: the rest length) ($|v_{1} - v_{2}|$: the current length) $F_s = (\frac{k_s|v_1 -v_2| - |v_{1}^* - v_{2}^*|}{{|v_{1}^* - v_{2}^*|}})\frac{(v_1 -v_2)}{|v_1 -v_2|}$ // I think the $k_s$ is typo. (I referred to $URL$ So, maybe $F_s = k_s(\frac{|v_1 -v_2| - |v_{1}^* - v_{2}^*|}{\color{red}{|v_{1}^* - v_{2}^*|}})\frac{(v_1 -v_2)}{|v_1 -v_2|}\cdot\cdot$(1) is correct. However, I wonder why the red part divides the equation? If I know correctly about Hooke's Law, $F=-k_s(current_{lenght} - rest_{length})\cdot\cdot$(2). It means that the red part should be removed like that $F_s = k_s(|v_1 -v_2| - |v_{1}^* - v_{2}^*|)\frac{(v_1 -v_2)}{|v_1 -v_2|}\cdot\cdot$(3). For example, in 3D space, there is a spring that has the spring term as $k_s$. The rest length is '3', so let's assume the left rest point, $v_2^*$, is fixed at (0,0,0) and the right rest point, $v_1^*$, is (3,0,0). And then we moved $v_1^*$ to $v_1 $=(5, 0, 0) with some force. We can calculate the force with (2), and the force is $-2k_s = -k_s(|(5,0,0) - (0,0,0)| - |(3,0,0) - (0,0,0)|)$ But if we use equation (1), the result is different. $F_s = -k_s(\frac{|v_1 -v_2| - |v_{1}^* - v_{2}^*|}{{|v_{1}^* - v_{2}^*|}})\frac{(v_1 -v_2)}{|v_1 -v_2|} \\\quad= -k_s(\frac{|(5,0,0) -(0,0,0)| - |(3,0,0) - (0,0,0)|}{{|(3,0,0) - (0,0,0)|}})\frac{((5,0,0) -(0,0,0))}{|(5,0,0) -(0,0,0)|} \\\quad= -\frac{2}{3}k_s(1,0,0)$ The force amount of $-\frac{2}{3}k_s(1,0,0)$ is totally different from $-2k_s$ Could you explain what I'm wrong? 

That's the easiest way to handle it. Another way to handle it would be to pass either as scale factor or scale matrix into your fragment shader and apply that to your texture coordinates before doing any sampling. To elaborate on the texture matrix idea, it's fairly simple. In your fragment shader, you'd have a uniform, say that's a . It can contain any sort of transform you need - scale, rotation, translation, shear, etc. Before you sample any texture, you run the uv coordinates through the texture matrix, and then use the resulting values for your lookup. Something like this: 

It's called trilinear interpolation. You first do a bilinear interpolation of the higher-res texture, then do a bilinear interpolation on the lower-res texture, then interpolate between the 2 results. The weight of the final interpolation is based on where between the 2 textures your Z-coordinate falls. If 0 is fully the low-res texture and 1 is fully the high-res texture, then you can use the standard glsl function to combine the two: 

I've done something similar. The way I did it was by using a height map image to change the positions of vertices in a grid in a vertex shader. I generated a 2D grid of vertices that were evenly spaced on the CPU. I uploaded the vertices and a texture and in the vertex shader, I sampled the texture and used the resulting brightness as the height. So I'd keep the x and y positions of each vertex and change the z based on the image. Now there are other ways you could utilize this to get something more like in the video. You could either have a 2D grid of single vertices, or even a grid of small 3D models. Change the z position based on the color of a height map. But additionally, you could upload a small noise texture and vary the x and y positions by a small amount based on the noise, which might vary per frame. I would see if you could vary the x and y by a large enough amount to see, but small enough that no point ever crossed over a neighbor point. That would get you some more movement and randomness. 

I think it is fair to say that the reason there are so many niche variations of GLSL/HLSL/Cg/whatnot is because no programming language is nor will ever be a one size fits all tool. Different problems require different tools, so sometimes it is worth the effort of developing a custom built tool if it is going to pay off in the long run. Stock GLSL is by itself pretty much unusable. It hasn't even acquired much stability across versions, so for a program that targets more than one OpenGL version, some sort of preprocessing is a must. HLSL is a bit more stable across versions, but again, if targeting more than one D3D version, some work will need to done to get good portability. The kinds of things people usually do are pretty much what you said, like a adding support for basic programming features such as modules and uniform syntax across versions, or even portability across different APIs (GL/D3D) without having to rewrite the shader code. More sophisticated things include fully fledged material systems or things like generating shader programs on-the-fly. Shading languages will probably get better and more generic in the future, incorporating things that today are commonly hand-rolled as core features. The new GCN architecture is a sign of that. So shading languages will be more usable out-of-the-box a while from now, but custom built solutions will never go away because there's only so much you can generalize. 

When applying multiple textures to a mesh, like for bump-mapping, I usually bind the textures to the first few fixed texture units, e.g.: diffuse = unit 0, bump = unit 1, specular = unit 2, then keep reusing those to each different mesh with different textures. But I've always wondered why supports so many texture units (in the previous link, it says at least 80). So it occurred to me that one possible way of managing textures is to bind distinct textures to each available unit and leave them enabled, just updating the uniform sampler index. That should improve rendering perf by reducing the number of textures switches. If you have less textures than the max texture units, you never have to unbind a texture. Is this standard practice on real-time OpenGL applications (I believe this also applies to D3D)? And are there any non obvious performance implications of taking this approach? Memory overhead perhaps? 

While normally I'd agree with @Survival Machine's assessment, since you say that the geometry is correct, but the color is showing up as solid red, it sounds like your colors have their color channels backwards. Normally, I would expect RGBA to be correct. But if everything's red, that sounds like the alpha is where the red should be. (Your alpha is always 1, so if they're swapped, now your red is always 1.) You can test this by changing your fragment shader to try different color orderings. Just add a swizzle to the last line: 

The best way to do proper outlines, is unfortunately, also the hardest. It involves calculating an offset curve, usually from the medial axis or straight skeletons, which are non-trivial to calculate. Once you have those, you can calculate the distance of any point from the input object and decide whether to draw the outline or bolding, or not. A simpler way to do it, which doesn't look quite as nice, is to use a MinMax filter. This is a sliding 2D box filter that calculates either the minimum or maximum value within the box at each input pixel. If you have black text on a white background, you can use the min filter to generate an outline. Use the max if it's white text on a black background. Other shapes, such as a circular area will produce different results that may be better for some fonts. In most cases, you'll end up either rounding off or squaring off corners as the size of the filter increases. 

When drawing in OpenGL and related APIs you usually have one or more models. In your case, this would be the cylinder and the icosahedron. The models are generally generated at some default size which may or may not suit your needs for a particular task. You move them, scale them, and rotate them by setting up a model matrix which describes how you want to transform them. So how would this work in your case? I recommend having 2 functions. One which draws a unit cylinder and another which draws a unit icosahedron. Say you have a data structure for your coordinates like this (I'm assuming a C-like language):