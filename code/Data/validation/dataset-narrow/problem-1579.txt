An easy one to try would be average ranks, where you take the mean of the ranks for each feature. For your example, $ \begin{array}{cc} \textbf{Feature} & \textbf{Avg. Rank} \\ 1 & 1.5 \\ 3 & 2 \\ 2 & 2.5 \\ 4 & 4 \\ \end{array} $ You could also weight the ranks by the size of the dataset, if the datasets that you are testing on are not the same size. 

You need to scale the values by some constant factor so the sum of every entry in the matrix results in 1.0. You can achieve this by using , where is your matrix. 

By converting a nominal attribute to a single numeric attribute as you described, you are implicitly introducing an ordering over the nominal labels which is a bad representation of the data, and can lead to unwanted effects from a classifier. Does it make sense to say that UDP should be inbetween TCP and ICMP? (no!) Imagine you are training a $k$-NN model on this data. It doesn't make sense to say that ICMP should be "further away" from TCP than UDP, but if you adopted the mapping that you suggested, the representation of the data has this assumption built-in. Alternatively, what if you are training a decision tree-based model? Usually, in decision trees, binary split points are chosen for numeric attributes. There could be some randomness in your training data where splits at certain values of the numeric attribute results in overfitting to noise. Typically when converting a nominal attribute to numeric, one numeric attribute per nominal label is created. Each attribute is set to one if the corresponding nominal label is set, and zero otherwise. For example, if a nominal attribute called has labels {, , }, then this dataset: $$ \begin{array}{ccl} \text{inst.} & \text{protocol} & \text{other attributes} \\ \hline 1 & \text{tcp} & \dots \\ 2 & \text{icmp}& \dots \\ 3 & \text{icmp}& \dots \\ \vdots & \vdots & \ddots \end{array} $$ could be converted as follows: $$ \begin{array}{ccccl} \text{inst.} & \text{tcp} & \text{udp} & \text{icmp} & \text{other attributes} \\ \hline 1 & 1 & 0 & 0 & \dots \\ 2 & 0 & 0 & 1 & \dots \\ 3 & 0 & 0 & 1 & \dots \\ \vdots & \vdots & \vdots & \vdots & \ddots \\ \end{array} $$ This is what the filter does in WEKA. As you mention, the downside of this is that a large number of additional attributes can be introduced if the number of distinct nominal values is high. If the dimensionality is too high after the conversion, you may want to consider using a dimensionality reduction technique such as random projection, PCA, t-SNE, etc. Note that this will reduce the interpretability of your model. You could also use feature selection techniques to remove some of the less useful attributes. It is possible that some of the nominal labels are not useful for your model, and you will improve performance by removing them. Another thing you could try is to use your domain knowledge to reduce the number of categories. For example, TCP and UDP are both transport protocols, maybe for your application the distinction between TCP and UDP is not that important and you can put instances with protocol $\in$ {, } into a new category, removing the old ones. 

(I don't have a machine with install at the moment, so don't expect the code to run with some debugging) 

Yes that's exact what RNN is suppose to be. In any type of RNN, you always only have one real RNN cell. The unrolled RNN cells are merely representations of that RNN cell overtime. In a single training step, the weights of those unrolled RNN cells are always the same. 

In other word, how to we do interpolation with a fixed value, or a operation but ignore the at the beginning and end of the array. 

Not necessarily. It depends on your loss function and sample size. In linear regression, the loss function is MSE. MSE is not robust against outliers, so linear regression is not robust against outliers. Having larger sample will mitigate the impact of a outliers. Same principles applies to neural network. 

If your old data is representatively of the underlying population and there is no radical shift of the underlying true model over time. Than you don't need to update you model as new data comes in. You might want to retrain you model with the new dataset periodically, but you don't need to do it in real time on your production machine. If the underlying true does change radically overtime and you don't have any existing data to captures those changes then doesn't matter what algorithm you use, it's not going to work. The information is simply not there. 

You don't need to do this kind of feature engineering like you would in traditional machine learning. If you have a network that is sufficiently wide and deep, your network should be able to "engineer" those features itself. Just think about how the first hidden layer is a linear combination of input features. And the second hidden layers is a linear combination of the first hidden layer. It should be obvious to see how a neural network is capable of structuring any polynomial features itself. Suggestion Try using the original 60 features directly and start with 3 layers. Given the size of the network, the training should take less than 10 minutes on a GPU-enabled machine. Compare your result with your benchmark model (you should always have a benchmark model). Play around with the hidden size and layers, rinse and repeat. 

The $x$- and $y$-values for each point have changed, but they are still all in the same place relative to each other. If you look closely, you can see that the structure of the data is identical, even though it has been scaled a bit, and you could go back to something that looks more like the original by simply 'zooming in' on the data. Because the structure of the data is the same, we say no information was lost. Now consider a transformation where we only take the $x$-value, and set all $y$-values to $0$: 

Sadly no, there is not a T-SNE implementation for WEKA. If you can install python packages in your environment, then you can use the package (in WEKA's package manager) to run scikit-learn's T-SNE implementation on data you have loaded into WEKA. Use this code in the 'CPython Scripting' panel (which appears after successfully installing ): 

You can't get the shape of a theano tensor, because it is not fixed. The output of the convolutional layer is just a symbolic variable and its shape depends on whatever you put into the layer as input. You can get the shape of the output for a specific input by making a theano function for the output of the layer, and feeding a numpy array through the function: 

You can serialise the scaler to disk using and re-load it when predicting new data points if necessary (i.e., the model training and testing happen in two different scripts). 

MNIST has 28x28 pixel greyscale images, so there are $28\times28 = 784$ features per image. CIFAR10 contains colour images that are 32x32 pixels. At first glance it may seem like there are only $32\times32 = 1024$ features per image, but there are red, blue and green channels in the image, which means each image is actually $3\times32\times32 = 3072$ features. Usually, these channels are arranged as three separate 32x32 pixel images, rather than a single 32x32 image with RGB pixels. In keras, you can load the CIFAR10 dataset like so: 

Being faster or lower is a relative term and must be understood in the context of what it is comparing to. So, in order to understand this, we must first consider how gradient descent works with other types of the activation function. Example Setup Consider an MLP with $n$ hidden layers of size one. $z_1 = W_1 x + b_1 $ $a_1 = f(z_1)$ ... $z_n = W_n a_{n-1} + b_n$ $y = f(z_n)$ where $f$ is the activation function. Tanh and Sigmoid - Vanishing Gradient Suppose $f$ is Tanh or Sigmoid activation function. The derivate of those functions are bounded between -1 to 1, or formally $f'(x) \in (-1, 1)$ for any $x$. This causes a very important problem in deep learning known as "gradient vanishing problem". Let's consider the derivative of $y$ w.r.t $W_1$. By chain rule, we have $$ \frac{df}{dW_1} = \frac{df}{dW_{n}} \frac{dW_{n}}{dW_{n-1}} ... \frac{dW_{2}}{dW_{1}}$$ and for any $0 < i < n$, notice that $$ \frac{dX_{i}}{dX_{i-1}} = f'(W_{i-1}a_{i-2} + b_{i-1}) \times a_{i-2} \in (-1, 1)$$ (The first term is between $(-1, 1)$ because $f'$ is bounded as discussed earlier and $a_{i-2}$ is also between $(-1, 1)$ as squash the input value.) So $\frac{df}{dW_1}$ is basically a product of lots of terms each is between (0, 1). The larger the $n$ (deeper the network) is the more of that term we need to multiply and as a result of the $\frac{df}{dW_1}$ becomes exponentially smaller. Because of this exponential relationship, the gradient quickly becomes so small we can effectively consider it as zero. The consequence of having zero gradients is no learning can happen at all because our update rule for gradient descent is based on that gradient. RELU and Dead Neuron Relu is invented to deal with the vanishing gradient problem because its derivative is always 1 when $a_i > 0$ so when $f$ is RELU we have: $$\frac{dX_{i}}{dX_{i-1}} = a_{i-2}$$ $$\frac{df}{dW_1} = a_1 a_2 a_3 ... a_{n-1}$$ It all nice and well when $x > 0$ but things fall apart whenever $x < 0$, this time not only is the gradient very close to zero, it is a pure zero. Once a neuron reaches there, there is not chance to get back at all. That's why this is known as the "Dead Neuron" problem Leaky RELU and ELU Leaky RELU and ELU is the natural development after RELU. They are similar to RELU as such the derivative equal to 1 when $x > 0$ but avoided "dead neuron" by avoiding zero derivates when $x<1$. I quote the original paper for the difference between the two. 

It seems that a convolutional ply is exactly the same as an ordinary convolutional layer. From their paper, they argue that the term "CNN layer" usually refers to a convolutional layer followed by a pooling layer. In an attempt to reduce confusion, they name the convolutional part a "convolution ply" and a the pooling part a "pooling ply": 

Filling the values is called imputation. Try a range of different imputation methods and see which ones work best for your data. 

Do your leaf nodes return a probability distribution rather than a single class value e.g., the majority class of training instances that arrived at the node? If so, a better approach would be to send the test instance down every child path and average the probability distributions that are returned by the leaves. Your good score of 95.45% on iris data suggests to me that it's not a very important detail in this example. Try some more challenging datasets and see if it makes a difference. 

So you want your algorithm to pick a horse that is more likely to win than the odds would suggest in order to profit? The odds are set in a way such that, taking expert opinions and loads of data into account, the bookkeepers are minimising their expected losses. It seems unlikely that you have more information on the horses than they do, so I think it will be difficult to beat them at their own game! Anyway, the approach I would take here would be to add a new feature into the data that looks at the horses position and odds in their most recent race(s): $$ score = \frac{odds}{position} $$ So, for example, say the odds for three particular horses and their corresponding positions in some race are as follows: $$\begin{array}{c|c|c|} & \text{Predicted Odds} & \text{Actual Position} & \text{Score} \\ \hline \text{Horse A} & 9:1 & 2 & 4.5 \\ \hline \text{Horse B} & 1.3:1 & 1 & 1.3\\ \hline \text{Horse C} & 3:1 & 6 & 0.5 \\ \hline \end{array}$$ This way, horses that are overperforming will have high scores, and horses that are underperforming will have low scores. You can average this score over the last $n$ races that the horse participated in to get a more stable scoring to evaluate the horses current form. This feature can then be used by any standard machine learning algorithm along with any other features you may have. 

Yes, it is very common and sometimes necessary to use the target variable for stratified sampling. Consider the case of fraud detection, given a bunch of features about a person (e.g. income, gender, position etc) we want to predict the likelihood of that person has committed the crime (a boolean value indicating whether the person is a suspect). This dataset is likely to be very asymmetric with very few positive examples. Now if we want to use k-fold cross-validation, we must stratify the samples using the target variable. If we don't, we might end up with a fold without any positive example at all and no metrics can be calculated from that fold. 

Summary Say you have a single sequence [a,b,c,d,e,f,g,h,i]. Using a sliding window of size 6, your single squence will be transformed into 4 sequences 

It's hard to answer your question because it is too general. It's like asking how to build a website without even knowing how to do programming. But I will be constructive Given your background and assuming you are familiar with Python, I recommend you using auto-sklearn. is a library based on the popular sklearn which automates a lot of steps involved in general machine learning, like feature selection and hyperparameter-tuning. It also tries a bunch of algorithms and ensemble them to produce the best result. It is hard to beat its performs as a machine learning beginner. While is very good for a beginner to get started, I must warn you that, without fully understand the how and why it is extremely risky for you rely on it blindly. One thing of particular importance is you need to understand how to evaluate the performance of a machine learning algorithm. Please make sure you understand the difference between train, validation and test dataset, before you make any claim on how good/accurate you prediction is. 

Cosine similarity is for comparing two real-valued vectors, but Jaccard similarity is for comparing two binary vectors (sets). So you cannot compute the standard Jaccard similarity index between your two vectors, but there is a generalized version of the Jaccard index for real valued vectors which you can use in this case: $J_g(\Bbb{a}, \Bbb{b}) =\frac{\sum_i min(\Bbb{a}_i, \Bbb{b}_i)}{\sum_i max(\Bbb{a}_i, \Bbb{b}_i)}$ So for your examples of $t_1 = (1, 1, 0, 1), t_2 = (2, 0, 1, 1)$, the generalized Jaccard similarity index can be computed as follows: $J(t_1, t_2) = \frac{1+0+0+1}{2+1+1+1} = 0.4$ Alternatively you can treat your bag-of-words vector as a binary vector, where a value $1$ indicates a words presence and $0$ indicates a words absence i.e. $t_1 = (1, 1, 0, 1), t_2 = (1, 0, 1, 1)$. From there, you can compute the original Jaccard similarity index: $J(t_1, t_2) = \frac{2}{2+1+1} = 0.5$ 

Ironically, this has increased the confusion, leading to this post. At the time I suppose it was not common to have several convolutional layers in a row before a pooling layer, but we see this often in modern architectures. To answer your other question about the network structure; they state the structure of the network that they use in the Experiments section (Section V-B). To hopefully reduce confusion, I've replaced the word "ply" with "layer": 

The variable contains the outputs of the final convolutional layers of your network. The final convolutional layer of VGG16 outputs 512 7x7 feature maps. All you need to do in order to use these features in a logistic regression model (or any other model) is reshape it to a 2D tensor, as you say. 

You method could work in theory. But typically, for vanilla LSTM, the setup looks like $x = [x_0, x_1, ..., x_{n-1}]$ and $y = [x_1, x_2 ..., x_n]$. I suspect you will get much better result if you do it this way. Highly recommend Colah's excellent explaination if you haven't read it already. 

Correction. Had a bit better thinking about it. it is not a gradient explosion problem here. More likely it is due to saturated neuron. Because LSTM uses sigmoid function, if the input is not properly scaled or weight is initialised inproperly, it could cause neuron saturation very quickly. 

Randomness does not implies a 50/50 probability. You can have randomness as long as the probability is not 100/0 or 0/100. However, I believe it is not the randomness you are really asking here. In stead what you really asking is how to test whether the random process is bias. Formally, let's use $Z \in (0, 1)$ to denote the outcome of an experiment of assigning a user. Then $X$ follows a Berniulli distribution with some probability $p$. The sum of the a series of Berniulli variables $X = Z_1 + Z_2 + ...+Z_n$ is a Binomal variable. We have also obvsered samples ($n = 304,641$) drawn from the experiment. Your question is essentially a hypothesis test with $H_0: p=.5, H_a:p\neq.5$. Given the null hypothesis, the mean and variance of X can be calculated. $\mu_x = np = 304,641 \times 0.5 = 152,320.5$ $\sigma_x^2 = np(1-p) = 76,160.25$ ($\sigma_x = 275.97$) Because our $n$ is large, we can approximate the binomal distirbution by a normal distribution $N(\mu_x, \sigma_x )$. It follows, assuming the null hypothesis, then $P(x >= 154,490) \approx 0.0$ . So we can reject the null hypothesis and conclude that the random process is not a 50/50 process.