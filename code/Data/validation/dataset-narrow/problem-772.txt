Is there any particular reason why your server has to answer to external queries at all ? Ideally, you would setup an external resolver for your public resolver (used to resolve all resources that must be accessed externally: MX, web server, etc), use the windows DNS server only for your internal network and block all incoming DNS queries at your perimeter. There is one thing, however, that you simply cannot prevent: as soon as you have a DNS server that answers to external queries, even if it's only for your own domain, it can be used in a DNS bounce attack. You can configure it to prevent DNS amplification but not to prevent simple bounces. It shouldn't be a big deal unless you're getting DDoS'ed yourself though. Edit: The typical way to setup dns in small(ish) structure is the following: 

In theory, power saving mode for cabled NICs is only used for disabling the card if no network cable is plugged in. This shouldn't cause any issue with MSSQL during normal operation or any change in the NIC performances. It's not impossible that it could some issue if you're using a clustered solution, specifically it could cause the NIC to have some delay in the node coming up if the load balancer doesn't maintain connectivity to that NIC, but that's all I can think of (and I'm not sure how likely that is: I have never encountered a clustering solution that does that kind of things, I just can't rule it out). 

The simplest way is to naviagte to $URL$ authenticate and then click on "Download a CA certificate, certificate chain, or CRL". Another way is to connect to the CA server, start an MMC, add a "certificates" snap-in, point it to "computer account", navigate to "personal"\"certificates" and then export the root. Edit: once you have the certificate, you can either import it directly on the target machine or distribute it across your whole domain using GPO. 

It would help if you let us know what software you're using. In this particular case, it could be that your SMTP server has blacklisted you. 

When you say "Administrator@domaincontroller", you mean you're trying to perform a logon against the local administrator account on the machine ? That will not work on a DC because there is NO local account any more on a DC: you need to perform a logon using the AD administrator. 

You can get request timeout on intermediary node in a traceroute if these specific devies are setup not to answer to ICMP ECHO themselves. Furthermore, some firewall will only allow a limited number of ICMP ECHO requests per second on a particular path. In your case, I would get the full path from your remote office and compare them: it is possible that you have a split path issue and that your packets aren't going through the same route in one way and the other. This is usually due to incorrect router configuration but can also be due to a VPN configuration problem or even a multi-homed server with an incorrect netmask. 

The proper practice is to use your local domain as sender, possibly using a non-existing address as user name. 

It depends on how, exactly, the application interacts with the file and the user. The root question is: "Is there a way to use a different security principal that the one from the user login session to access the file" ? If the answer is "no", then you can't restrict it. More pragmatically: If the application is a traditional desktop application used either remotely (application running on the client's computer accessing the file through a SMB share) or locally (for instance through Terminal Services), then no, you usually cannot restrict the access to the file using permissions because the process uses the user's identity to access it. If the application is a n-tier application (for instance, a web app), then you usually can put extra restriction on data file access: you restrict the rights of the users and allow whatever level of access is necessary to the security principal that will be used (that's usually done by assigning a specific service service account to the relevant process). Be mindful, however, of the fact that it is possible for a n-tier application to impersonate the user access token. In such a case, you should make sure that the files you want to protect are, in fact, not available though SMB at all (for instance, by placing them into a drive or folder that is not accessible from any SMB share available to the users). Note that, regardless of what the answer to the above it, if the application supports UNC path, you could make it less likely for the user to access the file by hiding it under a hidden network share. All you need to do to hide the share is to add a dollar sign ($) at the end of it's name. It will not prevent anyone but the less tech-savvy users from acessing the file should they want it but it might prevent accidents. 

Straight answer: you need a valid TS license for every Citrix connection. The reason is that Citrix runs ON TOP of TS: each Citrix ICA session is actually a RDP session that uses a different connector. It's a real pain because the two systems have different licensing models, but it's the way things are. As to why it used to work, it's pretty simple: RDP clients, when they initially connect, are issued a temporary license that are converted to permanent license when they expires. The TS license server itself never stops running, even though you have not installed any license. However, to have the clients successfully connect once their temporary license have expired you need to either have the TS license server issue them a real license (which can only be reclaimed if the client doesn't connect for, IIRC, 3 month) or delete the temporary license on the client and have a new one re-issued. 

That should do it. Edit: that's the "scripting" syntax. If you want to type this at the command line, you must remove one of the % sign each time they are doubled, linke this: 

A little more complex: install URL rewrite for IIS and configure it to forward all HTTP request to the equvalent HTTPS request. Watch it because you might need to rewrite the host name as well (see this for a simple exemple) 

Assigning the anonymous inet user "modify" rights the default temp folder isn't very wise. As such, it's not a security failure but you're making some classes of attack against your web infrastructure easier. As always, the devil is in the details. As long as nothing but innocuous data is written to that folder, there is no specific risk: the folder itself isn't different from numerous other places on your system. Of course, if you want to setup rights for the IUSR_ on that folder, it also means you intend to use that assignment so that web application can write to it. There comes the risks: - Having "execute" rights on that folder removes a security layer: if an attacker finds a way to upload an executable on the temporary folder and have the system run it, it will run. If you deny the "execute" right, it simply won't work. - The folder is right on top of the most sensitive part of your system: the location of the operating system files. It means that, should someone manage to use a directory traversal bug in a web app that exploits a temporary file, he'll be one step away from the system root and all the interesting things that sits in it. None of these are really too risky in themselves but a good security relies on security in depth. In this case, I'm pretty sure you can have your PHP app use a different folder than the system's temporary directory to save its temporary files. Best would be to place that on a separate partition, in a folder where the IUSR_* have read/write access but NOT execute permission. 

A customer of mine is experiencing a very strange problem: a scheduled task is running twice from time to time, with only a few seconds between execution. The task is scheduled using the windows scheduler. It's a very basic task that connects to a local service using TCP/IP, authenticate, gives it a command and disconnect. It is nearly instantaneous (the longest part actually is the SSL handshake). It triggers an issue in the server application which I will fix but my problem is that I simply cannot find any valid reason why the task is being run twice: nobody is ever connected interactively to that machine (it's an application server) and the server log shows that all these connections come from 127.0.0.1. Sending port and thread IDs both changes indicating in a very clear way that this is no some internal "ghosting" effect in the server application. Unfortunately, task history wasn't enabled on that server so I don't have a run trace on that front. I have enabled it and, should the problem occur again, I will check it. I've verified, however, that nobody was connected to the machine around the time to issue occurred. Can anyone explain this ? Thanks 

You most likely have a limitation in place for the source connection of the root user in your /root/.ssh/authorized_keys file. Specifically, your probably looks like this: 

I have a Windows 2008R2 server that experience high CPU load for several minutes about once every couple of weeks. Unfortunately, when these events occurs, logging into the system through RDP or at the console takes so long that, when it's done, the problem is usually gone. So, I've prepared a perfmon data collector that will capture the relevant data (CPU time per process, details of IIS worker processes, etc.) but I have no idea how to trigger it automatically under high-CPU load condition (as defined in Nagios: that's >99% CPU usage for more than 300 seconds). Unfortunately, the low frequency of these events makes it difficult to let the data collector run on its own. I have currently worked around it by using circular logging and leaving it running but that's not really a satisfactory solution. So, is there a (simple) way to get that data collector started under these conditions ? 

It uses the host name (e.g. www.mysecureservice.com) provided by the user or application to get an IP address through the client's DNS resolution. It opens a TCP connection to that server and requests the remote machine's X509 certificate (in practice, there are usually more than one cert provided but let's assume there is only one for simplicity). It verify the authenticity of the certificate. For this, it uses it's own internal rules (since it's not too important right now, and quite complex, I won't elaborate on exactly how). It verify that the name inside the X509 certificate it received from the server is the same as the host name it used in step 1 for finding out the IP address. 

You can change the source address of any IP packet. That is called "spoofing". However, for TCP, that will not help you: TCP isn't a datagram-oriented protocol but a stream-oriented one. In practive, that means it's designed to simulate a stream-oriented communication channel using discreet packets transparently. In order to do that, it will reassemble packets together to form a stream but also has built-in mechanism to recoved from packet loss and incorrect packet arrival. It also has mechanism to handle connection status management. All that means that, before you can send "a request" to a TCP server, you much first establish and that will require the client and server to perform a 3-way handshake: a negotiation between the client and server that ensure that both end of the connection agrees to open the connection. During that negotiation, both client and server will pick a random sequence number that must be used by the other peer. This number is then increased by one for every packet sent by one end of the connection and used to check whether packets have been dropped or received in the incorrect order. Since you will not get the answer to your first SYN packet (the first packet in a TCP handshake) and since you need to know the sequence number that is contained in that reply before you can finalize the connection and start sending data, you can't just spoof the source IP in a TCP connection. Now, if you're somewhere on the path between the server and the spoofed client, you could stiff out the answer (even if it's not meant to you), finalize the connection by using the proper sequence number and send your request (you can even read the answer. This, however, will require you to be connected to one of the segment between the client and the spoofed server AND be able to read these packets as they go through the segment. This condition is quite difficult to achieve in practice outside of the final segment and even then you need to use either special hardware (a switch that can use a port as a "monitoring" port) or use ARP poisoning on the switch. It's not really trivial and will still allow you only to spoof a machine on your local segment.