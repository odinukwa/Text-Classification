What I would do is to create a table with the stock_id (that can be the alphanumeric code or a integer), the timestamp of the measurement and the current value. That is your entry data, 3 columns. From that point you can add columns for calculations (the difference absolute or percent) with the previous value. Having all in the same table will simplify the model and ease your queries. Try to create a date (not timestamp) column and create a partition by it. It may lighten a bit the access to the table as long as you set it in your queries. 

You don't specify if you want to count the number of rows in the step_count or summing it. I've assumed that it contains a previously calculated count and thus I've used the SUM function. It would work the same with a count() instead a sum(). If you need to add the high_fives as a general total (at member level, instead of at activity type level, you need to do an outer query. Try this. This will not work in SQL Fiddle as MySQL does not support full outer joins, but it should work in Teradata. 

ETL is not a tool, but the process or group of processes intended for data integration from a source system to the destination system, generally a Data Warehouse. So if you are doing an extraction, transformation and further loading of that transformed data, you are performing de facto ETL. Someone could argue that it is harder to maintain, and more prone to errors. And they'd be right. But a poor code does not convert it into something different than code. Same applies. The tricky part is the final load process in the sense of being moving Excel files in a file system. We use at work a MapR DB based on MapR FS made out of independent files and yes, we do data integration and we finally "load" those files into their final destination, despite they are actually just files. In the end they are accessed by a Query Engine. So in the present day I would consider it an heterogeneous database and your system an ETL. Don't see why not. $URL$ 

As you cannot use the vendor name as identifier, since it may be duplicated (as it happens with the name of a person), you need a true identifier to be set upon creation. When someone identifies themselves in your system, they cannot identify by their name, since it is not enough to uniquely identify them. Thus you don't have any other option than assigning them any proper id when you create the record on your vendors table and identify them by that id. For this purpose you can create an ad-hoc column (filled in its most simple approach with an auto incremental series), or use a contact email, Tax identifier or any other key that you know is unique, instead of the name. 

Normalization are a set of techniques that, in a theoretical scenario, are intended to pursue consistency and ease of management and save costs. However, since you cannot work with a theoretical scenario, but with a real DBMS, you may find that a database with a certain high degree of normalization may be hard to use, and hard maintain in terms of coding and daily maintenance in a real-world scenario. Said that you may find that, under certain circumstances, you can achieve the same goal of normalization by means of other techniques while not compromising computation cost or maintenance cost, and with an acceptable increase in storage cost. This is why, along with Normalization, there is another technique called Denormalization. Of course you should do specific benchmarking tests and case studies to know which queries are the most common ones, so you can choose which columns to denormalilze. For example, if you have a country with several different country specific columns, and you have the country id (say, an INT) as a FK in the rest of your tables, as probably that id is meaningless (as intended, since it is an ID), you may foresee that every single query will need to join with the country table to retrieve the country name. So you may want to denormalize the common country name, which is used for daily reporting, while you may find that you don't need to denormalize the formal country name, which is used much less common. So analyzing the real use of your database you may try to improve their computational costs. After 17 years designing and managing databases, particularly in the DWH world, my rule of thumb is start at 3FN which to me is a good trade off between usability, consistency and computational, maintenance and storage costs. 

Why do you have two separated tables? Your model is faulty by design. You don't need in this case two different tables. Just create one table for user accounts and a different one for user account type. 

I'll make an assumption first. A real world person could have more than one user in your system? If a real world person (persons table) can have (and should have) only one user, why do you allow a one to many relationship between those two tables? Or I've misunderstood your model or I would create in the Logical Data Model only one table for users AND persons with all the information related to that person, including the email address. If you want to store two different email addresses you can use two different columns, as far as you don't intent to store an unlimited number of email addresses (like it happens with the delivery addresses you can have associated to your profile in online shops). I would use a user_login NOT NULL column (what you call John user, which has to be an email address used for validation upon signing up) and another column email_contact (what you call John person), with an additional flag for allowance to be contacted or not. Anyway you may want to validate this second email as well. Beside this, you can maintain a person_to_person relationship table for the dates and use it as an indicator to control if a person in publicly available or not and display its profile or not in the application layer. 

You can set seq_page_cost and random_page_cost per tablespace via ALTER TABLESPACE without restarting Postgres. 

Canceling queries (or, equivalently, rolling back a transaction) in PostgreSQL doesn't have any database corruption hazards which you might have been spooked by in certain other databases (e.g. the terrifying warning at the bottom of this page). That's why non-superusers are, in recent versions, free to use and to kill their own queries running in other backends -- they are safe to use without fretting about database corruption. After all, PostgreSQL has to be prepared to deal with any process getting killed off e.g. SIGKILL from the OOM killer, server shutdown, etc. That's what the WAL log is for. You may have also seen that in PostgreSQL, it's possible to perform most DDL commands nested inside a (multi-statement) transaction, e.g. 

(though as you hopefully got to see, canceling that expensive can save the database from a lot of unnecessary grinding if you're just going to anyway.) 

and raise an alarm if that max. age is over 1 Billion or so. And if it gets close to 2 Billion you are in imminent danger! You can use a tool like check_postgres.pl which will handle this check for you with some configurable thresholds. 

Erwin's answer does a good job of answering the question as originally stated, however you added the comment: 

Amazon's RDS only offers PostgreSQL versions 9.3.x, and it seems unlikely that they'll ever offer to host older versions of Postgres. So by jumping from a local 8.4 install directly to RDS, you would in effect be making two significant changes at once (jumping up several Postgres versions, as well as switching to managed hosting). That may be alright or not -- it all depends on what features you're using and depending on. You should do some reading on RDS's limitations (no external hot standby, limited extensions, no shell access to the database instance, etc.) and benefits (hopefully much less maintenance work) and decide whether it's right for you. Also, I suggest you walk through the steps of dumping and restoring your data into RDS and ensure your application works OK, as well as reading through the Postgres major-version release notes for 9.0, 9.1, 9.2, and 9.3, paying particular note to the incompatibilities listed to see if any of them would affect you. 

You can look under the "base" subdirectory of the data directory, and you should see something like this: 

However, this is really not what CHECK constraints are supposed to be used for, and it actually introduces a race condition if you have multiple transactions writing to example_table at the same time (can you see how?). Use the UNIQUE constraints that PostgreSQL provides. If your values are too large for the UNIQUE constraint's B-Tree index, create the UNIQUE constraint on an MD5() of the value. 

Assuming you are talking about the column in , or some status field derived therefrom, you can create an invalid (or intentionally corrupt, if you prefer) index like so: 

Supposedly it is possible to hook up Bucardo to RDS now that RDS Postgres supports the session replication role, but if you want a nightly snapshot I think you'll be much better off using RDS instance snapshots. 

The output of that command will give you a decimal count of what number WAL file you are on. Run that command again in, say, 10 minutes. Subtract the first number from the second, and that gives you how many 16 MiB WAL segments would need to be synchronized between the primary and the standby in that period of time. That will answer your question about: 

although semantically the UPDATE above is exactly the same as if it did not have a WHERE clause (ignoring triggers), you will avoid a ton of I/O for tuples which already have some_bool_column = false. Second, if you can, try to take advantage of Heap-Only Tuples aka the HOT optimization for your UPDATEs. So if you can avoid having an index on some_bool_column, these bulk UPDATEs will be faster and may be able to avoid contributing to index bloat. Often, you don't want to have an index on a boolean column anyway, since the selectivity will be low (or if one of the boolean values is rare, just use a partial index on that value.) 

In addition to Craig's thorough answer, I wanted to add that the cover of the book you reference says: 

This behavior is controlled by the parameters max_standby_streaming_delay / max_standby_archive_delay. You can fiddle with these parameters in the RDS Parameter Group used by your Read Replica instance to allow more time for queries against your Read Replica to complete. 

So, 300k rows total doesn't seem like a huge amount, I wouldn't be overly worried unless you have a particular cause for concern (e.g. your UPDATE taking way too long, holding row locks for too long, etc). But two suggestions which may be helpful for your particular use-case: First, make sure that your UPDATE statement does not touch rows it does not need to. If you want to set all values of some_bool_column to false, do it like this: 

Postgres can actually (in the following contrived case) use an index to satisfy queries without adding range scan kludges like the suggested . See the comments on Craig's questions for why Postgres is choosing this index in this particular case, and the note about using partial indexes. 

which will more-or-less double the size of the table, since the UPDATE has to keep old row versions around. You may want to read up a bit on how PostgreSQL implements MVCC and how vacuuming works, but to answer this question: 

I am wondering if anyone knows the history of why is the default transaction isolation level for PostgreSQL, SQL Server, Oracle, Vertica, DB2, Informix, and Sybase. MySQL uses default REPEATABLE READ, at least with InnoDB, as do SQLite and NuoDB (they call it "Consistent Read"). Again, I am not asking for what the differences are between different isolation levels, but rather for some explanation of why the default was chosen to be in so many SQL databases. My wild guesses are: small performance benefit, ease of implementation, some recommendation in the SQL standard itself, and/or "that's the way it's always been". The obvious downside of this choice is that tends to be quite counterintuitive for developers and can lead to subtle bugs.