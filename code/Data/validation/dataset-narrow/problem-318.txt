Are you backing up the transaction logs? Not the database but the logs? THe log will grow until it uses up the entire hard drive unless you back it up. I suiuggest at aleasta daily backup or more frequently if you havea lot of transactions. 

Employees can have more than one address, you should have a join table to address for the many to many relationship. The phone table is designed incorrectly. You do not want to add a column when you get a new phone type. The whole leave thing makes no sense. Please explain if this is a system to manage leave requests? You should have individual records for leave accumulated and leave taken. Do you need an approvals table? The balance shoudl be figured out at the time of query. The pormotions table makes no sense at all. You want a table to store the organizational position (And it should be a history table so it should have start and end dates). This table should be updated everytime the postion title changes (they are not always promotions). Same with salary, you want a start and stop date. You seem to be missing data on who the person reports to. Generally reporting will need to be able to sort through the reporting hierarchy. YOu also seem to have designed this solely on the basis of the data entry GUI. This is HUGE mistake of epic proportions. With this kind of information, reporting is a far larger problem and you need to consider reporting and how you will need to see the data over time. For someone who works for the comany for ten years, what type of information do you need to call up ablout his history? This is a business critical database and should have been designed by a professional database programmer. There are legal implications to this data, there are security concerns. How are you planning to protect this information? Pretty much all of this should be unavaliable to most users and admins. It should be encrypted. This is critical privacy data. Employee records typically include benefits information. They also typically include information concenring awards and performance appraisals and performance warnings. The use of ID as the PK is a SQL Antipattern ($URL$ You should use tablenameId. 

My scenario is I have a thick client side app using a SQL DB. The use of the app is divided into 10 "sites" mostly by Geography but some by Usage. I've written an SP to archive selected data out of the DB and put it elsewhere as the DBs are growing to nearly 500GB in size at this point. Further whilst most servers host one application DB there is one server (and instance) that hosts 3 DBs. My dilemma is I want to write code that is the same everytime. Ideally I would write something like 

I only noticed this was for Firebird part way through so sorry if that doesn't work. You could also do: 

So it looks like the database context was set in the 'background'. What would lead to this circumstance? (Of the GUI showing blank) It's a minor mystery that is nagging at me now. 

This should modify your execution plan. If it doesn't help could you post your plan for both this query and your original query (minus the Option line). Actually, eitherway I'd be intrigued to see this data. 

That way I can have the same SP installed on each server, any bug/feature fixes doesn't need me to manually type up 10 different SPs to install. In the real world I'm also limited by SQL2000 however I'd be interested to hear in ideas using SQL2000 and/or SQL2008R2. The servers are a mixture of both. As I understand it Synonyms wouldn't help as on the instance with 3 DBs I'd still end up with each DB having it's own copy of the SP with it's own hard coded definition of which named synonym to use. I also don't feel dynamic SQL statements would be a good fit. There's more to it than the example snippet above and I use table variables to marshal all the work to be done- so that would be out of scope for all the other statements I need to work with. 

Is it possible to get the file as a .csv file which can be imported like a text file and will not have this problem? I generally kick back all Excel files and ask for another format because SSIS and Excel do not play well together on many levels. 

When you are building complex queries, you should build them in stages checking the results as you go, not build one whole huge query and then try to figure out what is wrong. Here is what I do. First I list all the columns I want on a spearate line and comment out all the columns except those in the first table. Then I add the from clause and any where conditions on the first table. I note how many records are returned. Then I add each table one at a time and check the results again. I am especially concerned when the number jumps up really high unexpectedly or goes down unexpectedly. In the case of the number jumping up high, you may have a one to many relationship that needs further definition. This is especially true if the field or fields you are getting from the table are almost always the same. You may need a derived table or a specific where condition to resolve. You might even want to do some aggregation. Now I'm not saying it always bad if the record counts go up, only if they go up when you didn't expect them to or when the result set looks suspicious. In the second case, you generally have an inner join where you need a left join. The number of records went down because you did not have a matching record in the joined table. I often check each inner join with a left join to ensure that I return the same number of records. If it does then the inner join is appropriate, if it doesn't then I need to decide if it is filtering records I should be filtering or if I need a left join. When I am not sure why the record counts are off from what I expect, I use a select * (temporarily) just to see all the data, so I can determine why the data is off. I can almost always tell what the problem is when I see all the data. Do not ever use select * in a multiple join query like this or you will be returning much more data than you need and thus slowing the query as at a minumum the join fields are repeated (plus it is truly unlikely you need every field from 20 joins!). To find you issue, you are going to have to repeat this process. Start with just the from clause and the where condtions on it and add tables until you find the one (s) which are causing the query to eliminate all records. Don't stop with the first find, it is possible in a long query like this that you have multiple problems. 

I have a trigger (MS SQL Server) on TABLE_A that fires on an update that looks basically like the following (I've changed the names of the tables/trigger to simplify): 

TABLE_C has the same index as above. All indexes were rebuilt at the start of testing. Both tables have additional triggers that are being fired - however, during testing, I disabled these triggers to determine where specifically the performance hit was. Disabling all other triggers did not improve performance. EXECUTION PLAN: I'm not super savvy on execution plans for triggers, but as far as I can tell, you can view them from the profiler with the showplan option turned on. I believe this is the relevant plan: 

This table represents records and associated actions from other tables, where the TABLE_UNIQUE_ID and TABLE_USER_ID are a composite key from the table they originated, XML is the XML representation of the data, and the DATE_INDEX a date the user defines for record (which doesn't necessarily equal the timestamp). This table is large, ranging from 1-10 million records. The main table that records are inserted from is generally in the thousands to hundreds of thousands range. In short, each set of TABLE_UNIQUE_ID and TABLE_USER_ID combined (and TABLE_NAME, but I'm not worried about that right now) represents one record in the main table and everything that ever occurred to this record (they are uniquely identified by the UNIQUE_ID) For example, here's a set of records from this table that represents ONE record and its history in our 'main' table: 

I recently got my first taste of the error "Heterogeneous queries require the ANSI_NULLS and ANSI_WARNINGS options to be set for the connection." which I hadn't come across. Nearly every resource I found on this site and elsewhere dealt with this occurring in a stored procedure. My specific scenario is a Vendor supplied app and DB, I can change the DB but not the app. I created a view on the DB to another server (via a linked server) but when I attempted to use this in the app I got the aforementioned error. I used SQL Profiler to prove the app opens its connections with ANSI NULLs and WARNINGS OFF. I used SSMS to prove the link works with the default values there. But unlike an SP I can not set ANSI WARNINGS on a view. I set ANSI WARNINGS on the DB as a test, the app now worked, but I couldn't afford a full regression test of the app so this solution does not work. I also tried many different ways of implementing the linked server including created an ODBC connection on the server and then using that. But it's like the app is explicitly setting ANSI WARNINGS to off. Not sure if matters but the source server is SQL2000 and the remote server was SQL2008R2. 

Shortly our development team will be moving an existing SQL Server 2008R2 instance into our Production environment. Ahead of this I have been asked to review all the code on it for anything malacious (e.g. wiping their own order history from the sales database). This has come out of the blue and I'm not sure it's particularly physically feasible but I'm willing to give it a good go, at least from the SQL side of things. The question is, where can code exist? My draft list is 

Thanks in advance for the help! This has been wracking my brain. Note that I am re-writing the original query, which had HORRENDOUS performance due to lots of joins on the main table (which is also large). Oh yeah, there should be another couple of columns in the index, but I'm more concerned with getting the correct result set first and THEN I'll consider the performance next. 

I'm not sure what to think about this plan, other than I noticed that 80% on that clustered index scan. Hopefully this is what was being asked for, if not, I can repost. Thanks again! 

However, I am running into problems where I don't capture all the history OR original records (because the DATE_INDEX on the historical records don't necessarily fall in the date range defined by the user). My question is - what might be the best way to produce output, for a given date range (which will be queried in the date_index column), that selects the following: 

Currently by design, there is only ever ONE record in the INSERTED table (we are only updating one record at a time from a UI). What we have discovered is that, as the number of records increase in TABLE_B, trigger performance degrades rapidly. For example, with around 12000 or so records in TABLE_B, this update statement takes around 40 seconds (we established a timeout of 30 seconds). As I remove records from TABLE_B, performance gradually improves. As this was an unacceptable solution, I had to find ways to improve this update statement. Through testing/profiling, I found that the problem was with the second update statement (update TABLE_B). The first update statement works without problem; if I change the second update statement to its equivalent SELECT statement, it also runs fast. The solution that I found was to shove the singular record in the INSERTED table into a #TEMP table and join on that instead. I was also able to do this with a table variable as well, but performance was terrible until I created an index on it. This immediately resolved the problem and the update now runs almost instantaneously. My question is this - why did this solve the performance problem? Perhaps I am looking at this in the wrong way, but I can't imagine why I would need an index on a one record table. I have read that the INSERTED table isn't created with an index on it, but it still seems odd to me that I should need one. Thanks in advance! EDIT: As pointed out, I forgot to mention some other relevant table structure tidbits. TABLE_B indeed has a compound primary key/index created as follows: 

It is a very poor practice to expect to maintain the PK/FK relationships from the application. In any db that is nontrivial, data has close to a 100% chance of being changed from other sources including ad hoc queries, data imports, etc. It is irresponsible to think the data in the database is protected because the application has protections. Sure you think that you will force all changes through a webservice or some such, but the reality is that no one is going to add a million new customer records, from that company you just bought, one record at a time through a service. Your database has to be designed to account for the fact that people will change it directly at the source or through other applications (some of which may not be able to use your web service). Further, the application interface is usually more likely to be thrown away or redesigned than the data and when that happens, you may lose all or some of the data integrity rules. In designing databases, you have to think about how the data needs to be protected over time not about what is easiest and most convenient for the programmer or the intial application. A data professional sets up PK/FK relationships in the database because that is where the data lives and it is the best place to set it up to protect the data quality. I see data from a lot of different companies in my current position and I see all too many that didn't set up their PK/FK relationships in the database because they have data integrity problems that a correctly designed database would not have. 

I am getting failures on insert with the error: Msg 233, Level 16, State 2, Line 1 The column 'ID' in table 'dbo.pre_case' cannot be null. The insert statement is of the form 

As I said in the question a lot of resources dealt with this issue but not when it pertained to a view. The solution I found was to 1- create a 2nd DB on my source server. 2- create a view on the 2nd db using the linked server to the remote DB 3- alter the view on the 1st DB to use the view on the 2nd DB 4- alter the 2nd DB to use ANSI WARNINGS ON at a DB level 

One of our developers created a view whereby one of the select columns was wrapped in an RTRIM function. That same column on the underlying table has a non-clustered index on it. Now queries using this view and a where statement on this column are failing to use the index due to the RTRIM. I need the view's output to be consistent as unfortunately this developer is no longer with us and I can't read his code. What are some options for optimisation? I've got one, but I'm hoping there is better. 

It is acceptable for the last person aspect of the answer to be fuzzy or loose. i.e. as long as it's someone who went near it sometime vaguely recently that is "OK". 

1) Why is there a difference? 2) Does it matter? Environment: Server SQL2000 SSMS 2008, Script for server version set to SQL Server 2000 Production destinations SQL2000, SQL2008, SQL2008R2