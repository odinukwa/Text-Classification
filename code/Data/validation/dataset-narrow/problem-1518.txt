You may need to treat this as an optimization problem. Without a formal notion of what should be a cluster, you will not be getting far. A blackbox algorithm will not solve your problem, you need to open the box (understand the algorithm) and adjust it as necessary for your problem. Once you have this notion, you can try to match it to algorithms. For example, if you can quantify similarity (e.g. if distance times attribute dissimilarity matches your desires) then you can try HAC. Or if you have thresholds (distance at most 100mi, attribute difference at most y) then you can run DBSCAN and merge neigh ors transitively into clusters. But don't expect anything automatic. 

K-means doesn't work reliable with such distance hacks. It can prevent the algorithm from converging. The usual approach would be to rather use this information to guide cluster extraction from a hierarchical clustering. I.e., you would merge clusters if they agree on the desired words, and stop merging if they do not agree, rather than cutting at a single height. 

Try HAC with Gower's similarity. It is a very heuristic approach - there is nothing going to save you from weighting variables - but worth a try. 

For text data, linear SVMs are still state of the art. For named entity recognition, look up some NER toolkits. 

You forgot to preprocess your data. K-means is really sensitive to scale and outliers. Also: clustering is not classification. It may well be that e.g. one of your classes has a dense subtype. Just because the clustering found something else than your labels (and it's not supposed to find the same, is it?) does not mean it failed. It just didn't classify the data the same way as your labels (but if that is what you wsnt, then you should have been using a classification method instead.) 

k-means - as the name means suggeyts - needs to be able to compile meaningful centers using the mean. Now this only works for continuous numerical variables. The mean minimizes squared deviations, but not geographics distance. So in short: Do not use k-means on latitude,longitude,category data! The results will be nonsense. Earth is a sphere (not a plane), and 1 degree north is not 1 degree east, and not 1 pizza or 1 "chinese food". Do not do this. 

then it may be reasonable to run a clustering algorithm on each class, to obtain two different kinds of A, and learn two separate classifiers for A1 and A2, and just drop the cluster distinction for the final output. Other common unsupervised techniques used include PCA. As for your football example, the problem is that the unsupervised algorithm does not know what it should be looking for. Instead of learning to separate american football and soccer, it may just as well decide to cluster on international vs. national games. Or Europe vs. U.S.; which may look like it learned about american football and soccer at first, but it put american soccer into the same cluster as american football, and american football teams in Europe into the Europe cluster... because it does not have guidance on what structure you are interested in; and the continents are a valid structure, too! So usually, I would not blindly assume that unsupervised techniques yield a distrinction that matches your desired result. They can yield any kind of structure, and you will want to carefully inspect what they found before using it. If you use it blindly, make sure you spend enough time on evaluation (e.g. if the clustering improves your classifier performance, then it probably worked as intended ...) 

At this data size, you can still use hierarchical clustering. You can stop the clustering early when the similarity is too low. But all of these approaches are pretty inefficient. The proper approach is to not use clustering at all. Instead, use similarity search or, e.g., minhash. 

I disagree with the assertion that SOM are frequently used. IMHO they are difficult to use and rather obscure. Nevertheless they have some interesting properties, although I'd rather call them a visualization than a clustering technique. What you are ignoring is the"map" property. Neurons aren't independent but connected in a particular layout. Without this, the method is useless AFAICT. You need nodes to usually not be empty, but rather the average of many points, and thus M< 

Why - because any two rows will be different in exactly 0 or 2 columns. With this weight, the one-hot encoded attribute yields a distance contribution of exactly 1 or exactly 0. The easiest way is to encode your data using these weights instead of 0 and 1 rather than applying the weights in later calculations. 

You want to understand the data. So you run a clustering, then study how the points in a cluster differ from the points not in a cluster. Then based on these observations, you form a hypothesis. For example, you may notice that a cluster contains countries who eat a lot of fast-food and who are overweight. Then you can formulate the hypothesis that fast-food causes overweightedness, and then test that hypothesis. This is a form of explorative data analysis. There is not a mathematical function to maximize, but it is a tool for humans to understand their data and then be able to formulate new hypotheses that would not have sprung to your mind otherwise. 

Infinite. Mini-batch k-means never converges, you need to use an iteration limit or similar heuristic, and you can never guarantee to have found a local optimum. In essence, mini-batch k-means is: 

These are only heuristics. The are not very reliable, and often fail. They will not detect if you did not preprocess your data well. So instead of looking at some number, look at your data instead. 

You can train a decision tree classifier on the result. A decision tree is one of few algorithms capable of producing an "interpretable" result. But you need to understand that the clusters are much more complex than a simple if-then rule. 

This is not at all a typical clustering problem, so I doubt any of these algorithms will help. If you want to try clustering, you will need to do appropriate feature extraction. Don't expect things to work on the raw data. But I guess once you have good features, the problem will already be solved. Instead of trying to frame this as a clustering problem, look at it either from a sequential pattern view point, or even better: look at the few questions on how to learn regexps from a set of strings. 

Clustering is not predictive. A new data point could cause DBSCAN clusters to merge, so it could drastically change the result. So if you want to classify using clusters, it usually means you are solving the wrong problem. If you really really need this (despite the warnings) just train any classifier on the cluster labels. For example, a SVM with RBF kernel could work very well for DBSCAN cluster (at least with Euclidean distance). 

Don't blindly run clustering. Your work is likely 70% figuring out how to adequately preprocess your data, 10% clustering, and 20% making sense of the outcome. Don't underestimate how difficult preprocessing is. The way you used k-means, you assumed 1 gallon of water = 1 second. That assumption is probably wrong... Last but not least: since you have labels, why use clustering at all? 

Cosine is for continuous values. It's not the most appropriate thing here. For binary values, look at 

You can either use the medoid, you can sometimes compute a centroid (and just ignore that it may be outside of the cluster), or you can do pairwise comparisons and take the average of that rather than comparing centers. 

I'd rather not rely on clustering (clearly, DBSCAN would be the first method to try). Instead, I'd look for a projection that removes the correlation (x - y maybe?), then define a grid to separate the data. 

There is no good evaluation data at all for clustering that would allow such conclusions. There isn't even good real data where you could say variant 1 of k-means is better than variant 2 of k-means. There is also no good evaluation measure that would handle the notion of "noise" well either. So: don't go by some number. Clustering is about solving a data problem. You have data, you try algorithms and parameters and study (important) the result. Until you find something interesting. This cannot be automated, or measured (well, you could ask 100 scientists on what worked for them - i remember having seen e.g. some astronomy papers that used DBSCAN with success - but of course they can't put a number 0 to 1 on this). You may want to look e.g. at this Case Study with DBSCAN - I doubt k-means is of much help on this data set. In particular, it will assign the noise points to some cluster, when they should not be assigned at all. 

R and most of its CRAN modules are licensed using the GPL. In many companies, legal departments go crazy if you propose to use anything that is GPL in production... It's not reasonable, but you'll see they love Apache, and hate GPL. Before going into production, make sure it's okay with the legal department. (IMHO you are safe to use your modified code for internal products. Integrating R into your commercial product and handing this out to others is very different. But unfortunately, many legal departments try to ban all use of GPL whatsoever.) Other than that, R is often really slooow unless calling Fortran code hidden inside. It's nice when you are still trying to figure out what to do. But for production, you may want maximum performance, and full integration with your services. Benchmark yourself, if R is the best choice for your use case. On the performance issues with R (I know R advocates are going to downvote me for saying so ...): 

First of all, start with a subset until you know what you are doing. There is no use in waiting for hours for a result that doesn't work, or to run out of memory, or to optimize, just to find out it does not work. Secondly, makes sure your preprocessing is very very well done. Bad preprocessing will hurt your algorithms. From my experience, one-class SVM does not work well. It assumes all your training data is normal, and this a representative sample of all normal data. Secondly,.you need to tune kernel parameters, but how.can you do so without labeled anomalies? Instead, I'd try knn outlier detection, LOF and LoOP. But for these you need to make sure your distance is a very good measure of similarity. If you don't preprocess well, distance does not work, and then nearest-neighbor methods don't work either. 

Measure, don't guess It appears to be common to try k=1024, 2048, 4096, ... and use what works best for classification. This is possivle because the clustering is actually a quantization task, and you do classification afterwards. 

The radius in OPTICS is a maximum value, and it can be set to infinity! So you don't need to know it, and you should give OPTICS as well as DBSCAN a try. There are heuristics to choose their parameters, if you know your data. Similarly, try hierarchical clustering. There are good heuristics on how to extract flat partitions out of it. You want something that handles noise well - this calls for DBSCAN, OPTICS and HAC. 

On such data, naive bayes (and maybe non-naive Bayes variants) should perform extremely well. Because all your inputs are binary. It's also incredibly cheap to train, evaluate, and explain. You can combine it with frequent itemset to make it less naive, but I wouldn't be surprised if that does not improve the accuracy much. You would use FIM to find dependencies worth modeling, and build an optimal Bayes for non-overlapping frequent subsets (e.g. engineering subjects vs. languages). Then combine these partitions assuming independence. 

K-means will work really bad on such data, because the method is designed to process continuous values, where squared errors need to be optimized. Rather than trying to find a hammer that matches your "nail", you first need to understand your "nail" as it might be a screw! So what is your objective, what is an answer result, and when is a result good? Only then you can find an algorithm to optimize this problem. If you simply try random algorithms, forcing your data into some unnatural form that doesn't preserve the relevant properties, this is a waste of time. They will literally be solving a different problem. 

None of the internal evaluation metrics will work well on text in my experience. Probably because of the curse of dimensionality. Furthermore, DBSCAN does not cluster everything, but can also produce noise points. Few evaluation methods (and even fewer implementations...) handle it well that noise is not a cluster. 

Why would you put a classifier in this process? It can only increase the error. Just compute the centers of each class in your training data. 

Finding the minimum number of spheres likely makes this problem NP-hard, and a variant of the set cover problem. Good luck. It gets particularly difficult if you want to consider balls tha can be inbetween of data points. There are two obvious clustering algorithms that will find an approximation: Leader and Complete Linkage Agglomerative Hierarchical Clustering. But they will not find the maximum, only an approximation, and probably with no guarantees. Also, your assumptions are wrong: if you cover the data with balls of radius r, the maximum distance of two points will be 2*r. 

Don't just mix the data of two different sources. It looks as if the two sources may simply be using different scales. But that makes the whole PCA analysis meaningless. All your plot may be visualizing is that the other samples use a different way of measuring things. It seems all the variance is in the other samples. In particular this means that you cannot draw any conclusions about how your samples relate to the reference samples. 

There is no one-size-fits-all Also, data dimensionality is not the same everywhere. Text data, which is inherently sparse, has a very different intrinsic dimensionality than e.g. random Gaussians. For text data, linear SVMs are known to work very well. RBF kernels do not work well with high-dimensional data, because they are distance-based at the core, and choosing the sigma parameter becomes next to impossible. If you can "fold" dimensions, you also get very different behavior. Im image recognition, you usually have thousands of pixels. However, you never look at all of them at once. Instead, you use convolutional kernels that move over the data space, and they may have only say 32x32 pixels. That is still 1024 dimensions, but not millions anymore. 

K-means uses the mean. K-means is designed for least-squares. It only works reliably with (variants of) squared Euclidean distance (= sum of squared deviations). Counterexample: Assume you have the two hours 0 and 23. If they get assigned to the same cluster, k-means will compute the mean. The mean of the two values is 11.5. It is not 23.5. Abusing k-means with a cyclic "distance" may no longer converge, and will return nonsense results. But there are more cases where the concept of a cluster center is not viable on cyclic data. For example, given an event on every full hour, what is the center? The arithmetic mean is 12 - but if you take cyclic space into account every hour is an equally good choice in cyclic space. Therefore, the concept of a "center" in cyclic space is fragile. Alternate clustering algorithms You can try e.g. PAM or DBSCAN instead, with an appropriate similarity measure. Projection techniques As pointed out by other answers, you can project the time to the unit circle via sin/cos(time/24*2pi). By computing the angle of the centroids, you can map this back to a point in time. But once you want additional attributes it gets hard to meaningfully normalize the data (to combine attributes), and you can get undefined time (e.g. if there are two points in a cluster, one at 6 and one at 18). I didn't discuss this because I wanted to point out that modifying the distance function is not a good idea for k-means. 

PCA is usually implemented by computing SVD on the covariance matrix. Computing the covariance matrix is an embarrassingly parallel task, so it scales linear with the number of records, and is trivial to distribute on multiple machines! Just do one pass over your data to compute the means. Then a second pass to compute the covariance matrix. This can be done with map-reduce easily - essentially it's the same as computing the means again. Sum terms as in covariance are trivial to parallelize! You may only need to pay attention to numerics when summing a lot of values of similar magnitude. Things get different when you have a huge number of variables. But on an 8 GB system, you should be able to run PCA on up to 20.000 dimensions in-memory with the BLAS libraries. But then you may run into the problem that PCA isn't all that reliable anymore, because it has too many degrees of freedom. In other words: it overfits easily. I've seen the recommendation of having at least 10*d*d records (or was it d^3). So for 10000 dimensions, you should have at least a billion records (of 10000 dimensions... that is a lot!) for the result to be statistically reliable. 

Gaussian mixture modeling can - if your data is nicely gaussian-like - be used for outlier detection. Points with a low density in every cluster are likely to be outliers. Works well in idealistic scenarios.