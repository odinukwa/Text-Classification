Is it possible to translate a boolean formula B into an equivalent conjunction of Horn clauses? The Wikipedia article about HornSAT seems to imply that it is, but I have not been able to chase down any reference. Note that I do not mean "in polynomial time", but rather "at all". 

It's easier if we ignore the name of the macro and treat that as an abbreviation. Then we have simply to create the list and set to be shorthand for that list, the same way a sets as a shorthand for . As for evaluating a macro, that is also easy --- just pass the function inside the macro your arguments, but WITHOUT evaluating them first. 

Yes, in fact it's quite similar to evaluating . Of course, in the paper has no rule for evaluating , since it's treated as an abbreviation. From the definition of in emacs LISP (link): 

Yes. There is for example beta + the rule {s = t | s and t are closed unsolvable terms}. This is as far as I remember not equal to beta-eta, and is consistent. See mathgate for a short description and reference to Barendregt. 

As far as I understand, the "impossibility" of proving or disproving the thesis is that there is no formal definition of "effectively calculable". Today, we take it to be precisely "computable by a Turing machine", but that rather begs the question. Models of computation that are strictly more powerful than a Turing machine have been studied, take a look at $URL$ for some examples. Or just take a Turing machine with an oracle for the Halting Problem for Turing Machines. Such a machine will have its own Halting Problem, but it can solve the original Halting Problem just fine. Of course, we have no such oracle, but there's nothing mathematically impossible about the idea. 

I think ACM's Computer Science Teachers Association has a K12 curriculum on its Curriculum Resources page though it is probably way too light for talented teenagers. 

The truth of a statement is different from it having a (short) proof in a proof system. The language is expressive but it doesn't mean that all valid statements in the language have short proofs in the system. The theorem doesn't say that you can check the truth of a statement or even the correctness of an arbitrary long proof or of arbitrary theorems. It is for proofs of membership in an $\mathsf{NP}$ set, which by definition have polynomial size proofs (certificates) of membership. The theorem only says that you don't need to read the full (polynomial size) proof of membership in an $\mathsf{NP}$ set to decide its correctness. One implication of the theorem is applying it to the set of theorems in an arbitrary language which have short (i.e. an arbitrary polynomial) proofs in an efficient proof system (i.e. it is decidable in polynomial time if a given string is a proof of a given statement). For example, theorems of ZFC which have proofs of size $n^{100}$ where $n$ is the size of the formula. If the proof system is sound then you can probabilistically verify the correctness of the theorems which have short proofs with reading a small part of their proofs. I think this is the intend meaning of the informal statement "proofs written up in strict mathematical language can be checked efficiently to any desired degree of accuracy without the requirement of reading the entire proof". 

Sanjeev Arora has a nice document for a grad course (for 1st year students) he taught called the "theorist's toolkit," which has a lot of the basic material a theory student should know. A lot of this stuff you can wait until grad school to learn, but it will give you a good idea of what you'll need to know and some of the prerequisites. 

Yes - I think you are looking for "multiclass VC dimension," and there are a couple different generalizations of VC dimension to multiclass classification. A good paper on this is by Ben-David et al. ('95). In addition to proving learnability results, they give a nice history and references to previous extensions of VC dimension to the multiclass case. Another perhaps relevant work by Haussler and Long ('95) generalizes Sauer's lemma to more general versions of VC dimension. 

Imagine having to predict 0 or 1 on every round, and we are counting the number of correct predictions. Every deterministic strategy has an adversarial sequence that makes it always predict incorrectly. Hence, no competitive ratio is possible. However, the randomized strategy of flipping a coin on every round will be $2$-competitive with the best constant prediction (in expectation). 

One recent model trying to capture such a notion is by Balcan, Blum, and Gupta '09. They give algorithms for various clustering objectives when the data satisfies a certain assumption: namely that if the data is such that any $c$-approximation for the clustering objective is $\epsilon$-close to the optimal clustering, then they can give efficient algorithms for finding an almost-optimal clustering, even for values of $c$ for which finding the $c$-approximation is NP-Hard. This is an assumption about the data being somehow "nice" or "separable." Lipton has a nice blog post on this. Another similar type of condition about data given in a paper by Bilu and Linial '10 is perturbation-stability. Basically, they show that if the data is such that the optimal clustering doesn't change when the data is perturbed (by some parameter $\alpha$) for large enough values of $\alpha$, one can efficiently find the optimal clustering for the original data, even when the problem is NP-Hard in general. This is another notion of stability or separability of the data. I'm sure there is earlier work and earlier relevant notions, but these are some recent theoretical results related to your question. 

Yes, there are such sets, take any $\mathsf{NP}$-intermediate set (any set that is provably $\mathsf{NP}$-intermediate assuming $\mathsf{P}\neq\mathsf{NP}$), e.g. construct one from SAT using Ladner's theorem. Note that your $L$ needs to considered an $\mathsf{NP}$-intermediate problem, since it is in $\mathsf{NP}$ but not complete for it. Note also that you are assuming that $\mathsf{P}\neq\mathsf{NP}$ otherwise there is no such $L$ as every non-trivial problem would be complete for $\mathsf{NP}$ if $\mathsf{NP}=\mathsf{P}$. Additionally the conditions that you have given does not imply to completeness so the question in the first part is not the same as the question about constructiveness of completeness. 

Background A propositional proof system (pps) $Q$ can thought of as an nondeterministic algorithm for $TAUT$ (a certificate for a tautology is called a $Q$-proof for that formula). A $Frege$ system is a pps similar to those used in the logic textbooks. It has lines which are formulas (using an adequate set of connectives, e.g. $\land, \lor, \lnot$) and a finite complete set of deduction rules and axioms. Extended Frege, usually denoted by $EF$ or $EFrege$, is the pps resulting from adding the extension rule (which allows introducing new propositional variable as shorthand for formulas, i.e. it allows definitions). An equivalent system which might be easier conceptually to imagine is Circuit Frege (denoted by $CF$ and $CFrege$) and has circuits as its lines in place of formulas (refer to Emil Jeřábek's thesis for the definition of $CFrege$). Soundness formula for a pps $Q$, denoted by $Sound(P)$ is the (family of) propositional tautologies that express the soundness of $Q$, i.e. 

Let me give a partial answer from a learning theory perspective. As your question isn't well specified, this answer won't be either. In my answer, I'm assuming your question was inspired by your blog post, linked from your profile. Say that you are thinking about programs that are just functions (so they have to halt, etc.). You can ask whether certain classes of such functions can appear randomly by, perhaps, looking at the probability a random program (from some distribution that you think is likely) lands in that class or not, with the hope that probability is polynomially large. I haven't really thought this argument through. You can also ask whether such a class is efficiently evolvable according to Valiant's model of evolution (also in @Artem's pointer in comments): luckily what is efficiently evolvable is known to be the class learnable by correlational statistical queries; taking "crossover" into account, you get parallel correlational statistical queries. One thing to note is that just because evolvability is characterized, it is still a separate and sometimes difficult task to determine whether a particular class is evolvable (learnable with CSQs) or not. If you find a class of "programs" that is neither randomly occurring nor evolvable, perhaps you can conclue it has a "creator/programmer," though that conclusion may still take a leap of faith. 

We've just put up a paper on arXiv ($URL$ that proves generic lower bounds for "statistical algorithms" for optimization problems, with each "problem" having its own lower bound depending on its various properties. Coordinate descent (and pretty much anything else we can think of) can be seen as a statistical algorithm in our framework, so hopefully this paper has some results that will be of interest to you. 

I think what you are missing is probably the complexity of the proof verification algorithm for IPS. It is generally true that if we have a Cook-Reckhow proof system and have short proofs for a coNP-complete problem (e.g. TAUT) then that would imply NP=coNP. But not every proof system is a Cook-Reckhow proof system, e.g. it can be the case that the proofs could be verified in deterministic polynomial-time. If the IPS proof system was a Cook-Reckhow proof system then the existence of polynomial-size proofs for the complement of subset-sum would imply an NP-algorithm for a coNP-complete problem. But it is not clear if IPS has a deterministic polynomial-time verification algorithm. 

It easy to see that if $\mathsf{NP}\cap\mathsf{coNP} \neq \mathsf{P}$ then there are total $\mathsf{NP}$ search problems which cannot be solved in polynomial time (create a total search problem by having both the witnesses for membership and the witnesses for nonmembership). Is the converse also true, i.e. 

You haven't specified exactly what kind of circuits, but in general since parity $\oplus$ is in $\mathsf{TC^0}$ and majority $Maj$ is complete for $\mathsf{TC^0}$, you get a similar subexponential size lower-bound for majority. A simple reduction would go through counting function $NumOnes$ that counts the number 1s in the input in binary. This would give $2^{n^{\Omega(1/d)}}$ where constant in the exponent comes from the size of $\mathsf{AC^0}[Maj]$ circuit computing $numones$. I don't know the best value of the constant but it shouldn't be big. On the other hand, there are $2^{n^{O(1/d)}}$ circuits for majority as majority is in $\mathsf{NC^1}$ and any $\mathsf{NC^1}$ circuits can be converted into a subexponential-size bounded-depth $\mathsf{AC^0}$ circuit (by performing a layered brute-force). 

Some classes (e.g. automata) are cryptographically hard to learn. Some classes (e.g. k-term DNF) are NP-hard to properly learn. 

You can improve this bound by grouping the arms into "almost optimal" arms (arms whose expected payoffs are within a fixed $\Delta$) and the rest of the arms. So you can bound the regret contributed by the almost optimal arms by $O(T \Delta)$, which actually implies that the worst-case setting of $\Delta$ is approximately $\sqrt{N/T \log T}$, and not a value arbitrarily close to $0$. This is explained, for example, in section 1.3 of these lecture notes. 

Yes - Satyen Kale, Rob Schapire, and I have a recent paper (NIPS '10) on this very problem, where we consider choosing "slates" of arms instead of individual arms. We consider the cases when position matters in the slate and when it doesn't. We also analyze both the experts MAB "contextual" model and the expertless setting. 

An early paper that made this observation explicitly -- that rotations preserve inorder traversals -- is (in Figure 2 of) Sleator and Tarjan's 1983 Self-adjusting binary search trees. The move-to-root heuristic was studied in Allen and Munro's 1978 Self-Organizing Binary Search Trees paper. 

Unless I misunderstood, your problem is NP Hard in general. You have different tracks of different lengths, and you need them to add up to a given distance. Subset sum reduces to your problem, so I am afraid there's little hope for an efficient algorithm in the worst case. However, it does not mean the cases you care about are hopeless. For example, if you don't have too many track sizes, you can even do brute-force search. If you don't need a very small margin, then dynamic programming should work. 

Take cycles $C_{p_1}, \ldots, C_{p_n}$ where $p_i$ is the $i$th prime number. We can find the first $n$ prime number in time polynomial in $n$. 

The website of Computability and Complexity in Analysis Network has extensive bibliography. See their page for books. For computability, see 

Razborov and Rudich's result in their natural proofs paper is quite general. It is not restricted to $\mathsf{P}$ vs. $\mathsf{NP}$. I personally like the clarity of the explanation in Stasys Jukna's recent book "Boolean Function Complexity: Advances and Frontiers": 

Question: Let $M\in \mathsf{PF}$ generate formulas. Does $\{ M(1^n) \mid n\in \mathbb{N} \land M(1^n)\in SAT\}$ belong to $\mathsf{P}$? $succinctSAT \in \mathsf{E} \implies$ Yes: The assumption about the generation of the formulas in polynomial time from $1^n$ means that the formula can be succinctly given. You want to decide their satisfiability in time $n^{O(1)}$. Given $\varphi = M(1^n)$ we can find an $n$ in polynomial time in $|\varphi|$. Then $\varphi$ can be stated succinctly in $\lg n+O(1)$ bits using $M$ and $n$. We can use our $succintSAT$ algorithm in $\mathsf{E}$ to decide this in time $2^{O(\lg n)} = n^{O(1)}$. Yes $\implies succinctSAT \in \mathsf{E}$: Let $M \in \mathsf{PF}$ s.t. given a circuit $C$ in unary, $M$ computes the string succinctly encoded by $C$, and returns the result if it is a formula and $\bot$ otherwise. Assume that $\{ M(1^n) \mid n\in \mathbb{N} \land M(1^n)\in SAT\}$ belong to $\mathsf{P}$. To solve $succinctSAT$ we write the given succinct formula in unary and then use our assumption to solve it. Question: Can we generate in polynomial-time instance-solution pairs for $SAT$ such that the instance is hard? We have to clarify what we mean by the instance being hard as any instance by itself is (theoretically) easy as it can be solved by either the algorithm that always says yes or the algorithm that always says no. It seems to me that you tried to get around this issue by imposing uniformity. Thinking in cryptographic terms, without some information that is not revealed to adversary there is no point in hiding the rest of computation as the adversary can simulate the protocol. Assume that we have a polynomial-time algorithm that generate instance-solution pairs. The adversary can use the same algorithm to find the answer if it knows $n$ and finding $n$ is not difficult from the formula. The more reasonable way is to use a randomly chosen secret key to get around this and relax the hardness condition to be probabilistic: no polynomial-time algorithm can find a solution with high probability (without knowing the secret key). 

Perhaps some good news?: we're unlikely to find an NP-Hardness of learning result unless the polynomial hierarchy collapses. 

It seems you're trying to PAC learn a language by only seeing positive examples. This is called "learning from positive examples (only)." But you also have the power to get some of your own made-up examples labeled: if the oracle were fully truthful, then these would be membership queries, so your model would be known as "learning from positive examples and membership queries." In this framework, there are some results - for example, tree languages are learnable (not secure). DFA are not due to crypto hardness results. (Also see this.) Of course, your setting isn't quite this. Your membership queries are more limited. It seems that then the known intractability results would transfer to your setting from the model I described, but the learnability results would leave you with some work to do. But Mr. X's scheme is secure or not depending on the "pattern" he uses. Also - it seems like a strange requirement to be able to prove "Mr. X's passwords are inherently unpredictable." Isn't it usually enough to just be able to generate a new valid password to break such a system? But this seems to be the query to Mr. Y's algorithm itself... 

One thing to keep in mind is that if you want a job where you actually do research, you pretty much do need a Ph.D. Rarely do big companies these days hire people without a Ph.D. as research scientists. If you decide to do a Ph.D. there are lots of areas where you can possibly focus, and TCS is just one possibility. You can also do a mix of theoretical and applied research or many other options. Whether or not you'll get a job in industry will more depend on their interest in your work (theoretical or not), whether you can sell yourself, the market, etc. Lots of Ph.D.s in TCS work in industrial research, lots work in academia, and lots leave research all together. There aren't nearly enough academic slots for all TCS Ph.D.s to into academia, so an expectation that you do would be unrealistic and generally isn't there (industrial research jobs are competitive too though). Perhaps your advisor might "expect" you do choose one career path over another, but it's your life and you ultimately get to decide what to do with it.