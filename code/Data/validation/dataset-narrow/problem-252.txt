will return only . How does it work? Well, I just took a straightforward approach to assigning order to the returned list by using . In this example, I've used a CTE to add the row after the fact, but in your case you could add it into your query like so: 

Then, when you run a query such as above, the planner should be able to make use of the index for much faster querying. Hope this helps! 

When you run , it only returns to you the plan that the database has created for running the query. When you remove , then you are asking the database to actually perform the query. What this implies is that, essentially, your query is going to take 3 minutes. Okay, so it's more complicated than that... Try By running a query with at the beginning, you are asking the database to actually perform the full query, but rather than returning results to you, return a report about the actual query performance. Here's a few things to keep in mind: the first time you run a query with , it probably has to access data and indexes which are on disk, and so this will slow down the query for sure. However, try running the same query a second time, and most often all of the data you need to perform the query is residing in the shared buffers. Then, you'll see the query typically finishes much more quickly on subsequent runs. In the end, is a good tool for hunting down how long your query is actually going to take, and if your query plan is any good. I personally don't really use prepared statements, so maybe someone else will chime in about these. I know that it may have an effect on the quality of the query planner results. 

Installing extensions into are, as far as I'm aware, not advised. You should use the default schema, which is also in the by default. Why? As an example, I will work with the extension which I've already created within the schema. All functions are, by default, accessible to all schemas in the database if they are located in the schema. Now, I try moving it to the schema, using 

Let me begin this reply with a caveat: I've never encountered this exact problem, so I don't fully understand the nature of it. However, I'm going to give my advice so that perhaps it gives you the insight you need to complete your task. extension Recent versions of PostgreSQL come with an extension which you can install that allows you to inspect the individual pages of elements stored in your database. First things first, all you need to do to use it is to run the command 

Refer to the Postgres documentation on within for more info. A word of caution: By applying , it will keep only the first row, regardless of how the order is returned by your query. Unless you apply some criteria, you may end up with inconsistent results! In effect, you could run the query once and get as , but run it later and get as . I would highly recommend some additional predicate criteria to ensure proper and consistent results are returned. 

Just my humble opinion, but these changes may make a big difference for you. Try out that change first thing, at the very least. Best of luck! EDIT: Build an additional index to assist sorting So, if over time your product line expands, certain queries may return many results (thousands, tens of thousands?) but which may still only be a small subset of your total product line. In these cases, sorting may even be quite expensive if done in memory, but an appropriately designed index may be used to assist the sort. See the official PostgreSQL documentation describing Indexes and ORDER BY. If you create an index matching your reqirements 

I would then select a composite primary key as . With this definition of , you can lookup the processed text according to date, version, which text in the base table it pertains to, etc. Unfortunately, if you're processing a whole lot of text data, this table could grow to be very large and bloated, and in this case partitioning along the attribute could be very beneficial. The partitioned structure would still permit you to run queries such as, 

Note that the sub-select , given no additional criteria, will extract rows in ctid order. By selecting appropriate and values for a rnage of queries, you can essentially run the query in parallel. Denormalization Don't rule out denormalization as an option as well. In this case, it can help you to much more quickly get you the results you want. Of course, denormalization will only really help if your query criteria will be fairly stable over time, so that you can effectively pre-calculate results sets. I'd say there are two decent options: (1) Use partial indexing, where a given index is designed around some set of criteria, as 

The problem with MySQL's InnoDB engine is that the storage architecture is essentially using index-organized tables, which will unfortunately experience performance issues for large numbers of inserts when the target table is large. The only case in which this can be mitigated is if you have sequential inserts, that is, the primary key of the set of inserts follows a sequence. For high ingestion rates of data, different technologies are recommended, such as Cassandra which uses a log-structured merge tree, or in your case, I might recommend TokuDB, which has been acquired by Percona, and which uses a technology called a fractal tree index, which is essentially creative way to cache, pre-sort, and batch insert into your index-organized table (of course, there's much more to it than that, but no need to go into the details here.) Long story short, if you have moderately high ingestion rates, you need a heap organized table structure implemented by databases like Oracle and PostgreSQL, and if you have very high ingestion rates, you need to use a database like Cassandra or TokuDB. 

Try out this SQL Fiddle, and see if it gives the results you're looking for. I did the best I could with the description I had. 

which lets you first order to put entries at the top, and then sort by a secondary column, where in this case I've chosen as the secondary column. You can use this SQL FIDDLE to confirm it works for your needs. 

so that you are only applying the to your window, and have both and clauses for your , I believe you'll get the results you want. Refer to this SQL FIDDLE as a reference, where I have a generic field, a field to represent the competition id, and a field to represent a competitors time. 

If we can make the assumption that the operation will always return a set where a member "id" is listed followed by its role in a single column of entries, then I've written up a quick solution for you. I made a quick data set as: 

In Cassandra, as far as I know, for filtered by non-key attributes, you only have three options: (1) Application side filtering. That is, if you get your results from a CQL , use your application to filter the results. For all but the smallest data sets, this is ill-advised. (2) Bite the bullet, and create those secondary indices. (3) Probably the most common option, duplicate your data by having rows composed of keys. That is, for whatever filter condition you want to apply, create a new entry in your database where you store the keys of all the relevant row entries which match the filter. Note that while the third option is most common, you will almost eventually develop some data inconsistency due to the inherent de-normalization. Apache Cassandra is not a cure all, it simply handles some applications very well. Good luck! P.S.: Here's a couple decent blog entries which can explain a bit of the theory at the logical data model level. Part 1 & Part 2 

In this table, would store the time stamp for the first entry in the array, and each subsequent entry would be the value of a reading for the next second. This requires you to manage the relevant time stamp for each array value in a piece of application software. Another possibility is 

WARNING! Depending on authors response, my answer could be way off. Awaiting his reponse. reconbot, we need more info, because it seems like your entire subquery 

and finally all of your measured facts, with foreign key references to the dimension tables (that is references & references ) 

to remove all the instances of , , , and any whitespace characters, or more specifically, to replace any instances of these characters with , and to apply this globally, signalled by using the flag . Next, simply split the string to an array using as 

It's pretty straightforward how it works: First, take the string in , and strip it down to the useful information. This is done by using as 

SQLish Honestly, your best bet is probably ANSI-compliant SQL, because it can eliminate the ambiguities which you've mentioned. If there's any trouble with it needing to be written in "natural language", just add in the apprpriate prepositions and such... 

to set my back to being sufficiently large for my query. EXPLAIN & Recheck Condition So, running my query with only as 

Presumably, though you should check against your own data, you can see the number of bytes which the index key is using by checking the field. I am betting that you can use this to check against your pre-determined limit which you said was 2712 bytes, and in that case use this info to collect all the s which violate your condition. Broadly speaking then, you need to create a PL/pgSQL function to iterate over the pages of your index(s), checking the , and collecting the s of the heap entries, and once completed, since you state it was OK to drop violating rows, then wrap it all up with a 

Honestly, there's probably lots of ways to rewrite this query, and I'd work on it until the planner yields what you want. Sequential Scan In your last question, you ask why the even has to touch the table. Well, the database has to get this count from somewhere, so it either needs to scan through the entire index and count the number of entries, or scan through the whole table and count the number of entries. It just so happens that a table scan is the faster way of completing this operation. EDITED for more query options Based on your feedback, you might also try: 

Now, will have all columns from , plus any additional column fields which you believe are necessary for that particular table. Rinse and repeat to make other tables which are children of the table. Further, you can then query for all comments which match a particular condition 

With respect to these approaches, I have no idea if this will help, as I've simply never encountered a failing for any other reason than a syntax error, or something else relatively benign. Use to make CSVs As a last ditch effort, if you aren't able to pg_dump your schema or tables, in a compressed format, you could always extract the data using the command to make comma-separated variable (CSV) files. 

as in this SQL Fiddle. So, I'm either misunderstanding, or your originally given data set doesn't reflect the significance of your sub- in the original question. IGNORE EVERYTHING BELOW HERE... Until we get clarification from the author... I think I've got your solution. At the very least, I've made a SQLFiddle with the results, and it appears that it will be much less costly. sub-queries in PostgreSQL I love chances when a sub-query can be used to save some time in your query. Unfortunately, I think I'm pretty bad at explaining when and where it should be used, and I'm only OK at recognizing instances of when to use it. It just doesn't come up too often in my particular query designs. Take a look at the Postgres documentation on keyword for some ideas, and also I really like this SlideShare presentation by Markus Winand for helping to explain a bit better. In essence, it has a flavor of a "for each" statement in typical pseudo-coding vernacular. The reason I looked into it for your case was: you were building the table, and then using the attribute of in your sub-, where you checked if was the distinct returned set of values. Using attributes in the clause of the sub- was the red flag for me. OK, so I realize the explanation of my motivation isn't so hot, sorry! :P On to the results... New Query Without further ado, here it is: 

yields s in the fields if the was only present in table . See this SQL Fiddle for confirmation of the error. However, applying my query with will yield the desired results. Use this SQL Fiddle, to confirm. 

and you get the exact same results. See the SQL Fiddle here. If you're still concerned about distinct s, you could also use 

I encountered this problem myself once, and for my case, I was able to fix it by setting the to a low value for just the query, as 

I didn't take a lot of time to make this batch function in tip-top form; what I mean is that I simply hard-coded several of the numerical values for simplicity's sake. In your case, you may want to get more detailed and include: 1) Something that checks for the maximum id value so that you set your bounds appropriately, and 2) even though I hard-coded batches of 500, you could easily make this a function input parameter. Sorry I don't have time to test this or make sure it really works well. Good luck! 

I immediately see a few things that confuse me here, which you should look into. Unnecessary Join In your query you have a portion 

Also, you can note that we get back identical results as those you are expecting, except for a single added column on the right side, as part of the selection, which is a replication of the . I'm not super experienced with (it doesn't come up too often in my work), so if that's a problem for you, we can sort out a way to drop it. :P results So, we don't have the big data set which you have, so that we can really test out the results of the new query versus the old. I'm relying on the estimates here, but... Using the Old SQLFiddle, we can see that the overall estimated cost is approximately 283,000 Postgres units. :P Using the New SQLFiddle, we can see the much better estimated cost of only 204!! 

You're permitted to do multiple joins in a single statement. You can do this even when joining on the same table by using aliases. I created a very quick example which is similar to your case. I just used ids instead of a time, and I stored the whole name as a single field; you'll just have to concatenate the first and last name on your own ;) Schema and Data 

I'm going to give you one example of a way to do it, where I use CTEs, which are available in many databases, particularly PostgreSQL which I am most familiar with. CTEs and the clause CTEs allow you to form temporary, single-query table like structures, but since you can make CTEs rely on one-another in a single query, it provides a convenient and easy-to-understand way to make your query semi-procedural. In your case, I would try 

I believe this can be solved relatively quickly, by adding a clause to your , and adding an additional equality condition, namely, adding . 

Please test this, and see if my guess is correct. Sorry for any errors, but I've got to go sit in some traffic now. :P 

I think it could be helpful for you to think of MapReduce as (essentially) a distributed query engine. I know it isn't one-to-one, but with and aggregate functions such as , , etc being very much like a operation, and being very much like a , there is quite a bit of similarity. In your case, on the same hardware, I think the only benefit you will be able gain from using MapReduce is the distribution to multiple cores. Remember that query processes in PostgreSQL only use a single core per query, so there is a lot of waste of your 64 cores if you're running this one query at a time. Parallelization Perhaps you'd be able to break up the query and run it over only segments of your locations table, and then run these queries in parallel using a connection pooler like pgBouncer? For some good info about access data in sequential chunks (pagination), check out this amazing blog post by Markus Winand. An example of how you might break this into chunks might be (warning:untested SQL ahead)