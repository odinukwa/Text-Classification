Blend my samples in sRGB space. This would be incorrect though. At step 1.5 apply tone mapping to the samples, then at step 2.5, apply inverse tone mapping to the combined result. This feels like a hack. Use harmonic mean to combine the colors instead of a regular mean, to make the average favor smaller numbers when present. 

You can perform color conversion as a post process step to convert a rendered frame's colors to a different set of colors. You might make darker colors more blue for instance, to give it a night time feel, or just modify colors to give it a different mood. One way of doing this is using a volume texture. You use the RGB value of a source pixel as texture coordinates into a volume texture, where you read out the RGB value that you should replace that pixel with. This page on the 4d texture GL extension says this though: 

A 3x3 matrix is much like a vector in that it describes directions but has no position. If you have a 4x4 matrix that represents a 3d transform and uses homogeneous coordinates for a translation, you can then use that to get the X,Y,Z orientation vectors, but also can get the translation from the origin. Hopefully this is helpful, and not misleading for less common matrix types. I saw an answer deleted which said similar, so fingers crossed :P 

If $closestPoint$ == $P$, you know that $P$ lies on the line segment defined by $A$ and $B$ because the closest point on that line segment is the point $P$ itself. However, in practice, you might run into some numerical precision problems due to having limited accuracy in computer calculations. To get around this, instead of checking if $closestPoint$ and $P$ are the same, you should probably instead consider them the same if the distance between them is smaller than some threshold value. This way, if precision issues come up, you'll still get the answers you want in the cases you want them. Too large of a threshold value, and you will get points being considered on the line that you really wouldn't consider on the line (if this happens, make your threshold smaller). Too small of a threshold value, and some points that you would consider being on the line will not register as being on the line (if this happens, make your threshold larger). 

One idea is to use low discrepancy sequences and do quasi monte carlo integration ($URL$ Low discrepancy sequences are related to blue noise, which has only high frequency components. Going these routes, you get faster convergence of $O(1/N)$ instead of $O(\sqrt{N})$. These give better coverage of the sample space, but since there is some randomness (or random like qualities) to them, they don't have the aliasing issues that regularly spaced sampling does. Here is a "jittered grid" where you sample on a grid, but use small random offsets within a cell size. This was invented by pixar and was under patent for a while but is no longer: 

I'm working on a shadertoy "snake" game, using the new multi pass rendering abilities to save game state between frames. I'm using raytracing to render the board (an AABB), and am planning on using spheres to render sections of the snake's body. The game board is a 16x16 grid and each grid can either have a sphere there (a segment of the snake's body) or not. Snake body segments don't move, they are just either there on the grid or not. When the snake moves, a new sphere appears in the front and an old sphere disappears from the back. The problem I'm trying to solve is how to render the snake body spheres. For instance, a naive approach would be to store a 16x16 grid in pixels specifying whether there was a snake body in that grid cell or not. I would then do a ray vs sphere check for up to 256 different spheres within my pixel shader, which seems like a no go. Another method might be to figure out where the ray begins and ends on the game board (when it's between the high and low height values of where the spheres are) and then use something like bressenham line algorithm to go from the start to the end of the line the ray takes on the board, and check only the grid cells that the ray hits. The problem there is that it requires a dynamic loop. Maybe a more practical solution would be to make the camera have a nearly top down view and where the ray enters the playable game world, test any sphere in the cell it hits as well as the 8 neighboring cells. I'm betting there are some much better solutions that I'm not thinking of. Does anyone know of any interesting techniques or creative solutions? Thanks! Edit: here is an older version of this game i made, which was CPU / software rendered, to give an idea of what I'm planning. 

Raytracing is a great way to render spheres as Daniel mentioned. Below is a screenshot of a raytraced scene I made but lost the source code to unfortunately. 

Premultiplied alpha itself does not give you order independent transparency, no. This page talks about how it can be used as part of an order independent transparency solution however: $URL$ Other benefits of premultiplied alpha include: 

In PC game development, after loading models, textures, shaders, etc in a loading screen, some games will render the models once to an off screen target to make sure the driver and gpu have done all the work required to keep there from being a hitch the first time the models render. Does anyone know how much that actually helps? Does it only affect certain cards and drivers? 

Here is an example of a medium sized angle (~90 degrees) interpolating the same time t between the angles: 

It does look like raytracing to me as well. It could also possibly be ray marching, which would be easier to make the tubes with since ray marching is all based on distances from things (center of tube in this case). Refraction is what's used to bend the light that goes through the tubes. You are then just left with how to model the tubes. One way could be to use 3d bezier curves (: This may not have been made in real time, but may have been rendered frame by frame and then assembled into the smooth "movie" you are showing. 

You might want to look into signed distance field textures. They are very efficient and high quality for monochromatic images like shadows. They work by storing a clamped, signed distance from the pixel to the shape. Working this way, the distances play nicely with the bilinear interpolation of texture sampling, so you can have very nice results for very low resolution. You can also give them a soft edge to have a soft shadow look. 

One way that might work is to have boids but render each boid as a group of birds. This way the simulation is still simple but the rendering makes it seem complex. You might even allow a little drift per bird in the boid to make it look so obviously rigid. 

It's common knowledge that branching in a GPU program is costly because it may have to run both the if and else logic for every pixel being evaluated in the same wave, but only applying each result to the appropriate pixels. I was curious if branching was still a performance issue if there is only an if statement and no else statement? In this case, it seems like it has the ability to be faster compared to branchless logic, for the case when no pixels need to do the work inside the if statement, and can instead skip past it as a group. Is this true? Are there any other downsides? like perhaps older video cards still doing the if logic anyways, even though no pixels evaluate to true? 

Subpixel rendering is used most commonly to anti alias fonts. It works by leveraging the physical layout of the color components of a display to give geometry details to an image that are smaller than a pixel. For instance, this font has a height of 3 pixels but can easily be read: 

The missing link between sample locations and the greyscale noise texture is "ordered dithering". Ordered dithering is a list of pixel locations with a "rank" (order) for each pixel. If you have a white background and want to add two black dots, you add them at the locations for the two pixels rank 0 and rank 1. Choosing how to rank the order of the pixels to turn on can vary dramatically with different results though. For instance, a bayer matrix is a specific ordering of the points, and blue noise sample points are as well. White noise is just shuffling the points so that they have a random ordering. How we get from this "ordered dithering" (stippling) to the greyscale color noise images is that we divide each point's rank by the number of points to get a value from 0 to 1, and use that as the points greyscale color. That gives you the greyscale noise textures. The greyscale blue noise texture was created with the "void and cluster" algorithm which makes it so that each new dot placed goes in the middle of the largest void. This has the nice property that you can threshold the blue noise texture at any value, and the result will be blue noise samples of the desired density. This paper is a great read that talks about these things more deeply: $URL$ 

In practice, lerp/nlerp are pretty good at getting a pretty close interpolated direction so long as the angle they are interpolating between is sufficiently small (say, 90 degrees, or 120 like Simon mentions in his comment), and nlerp is of course good at keeping the right length, if you need a normalized vector. If you want to preserve the length while interpolating between non normalized vectors, you could always interpolate the length and direction separately. Here is an example of the three interpolations on a large angle. Dark grey = start vector, light grey = end vector. Green = slerp, blue = lerp, orange = nlerp. 

I've worked with various interpolation algorithms including linear, cubic (and bilinear and bicubic), other bezier (and nurbs, etc) based interpolations, lagrange interpolation and some others, but I haven't been able to find any basic info that explains what bilateral upsampling is. I have been able to find information on Joint Bilateral Upsampling which as best as I can tell interpolates multiple pieces of data in parallel, using hints from each to help the other interpolate better, but am having trouble separating the info of BU and JBU. Is anyone able to explain the basic ideas of what bilateral upsampling is about? 

If you have a bunch of particles to render, using different shaders and/or render states, that have some level of transparency, the naive solution of sorting all particles by depth can be extremely inefficient since it is likely to turn a few large batch draw calls into a multitude of smaller batch calls which have expensive render state switches between those draw calls. I know of a few ways to help fake things such as sorting particle batches by their emitter location, or using something like additive blending, which gives the same result regardless of draw order. I also know there are some techniques out there to attempt order independent transparency, often being an approximation, requiring a (sometimes unbounded) amount of memory, or some combination thereof. Does anyone know any pragmatic solutions (fast/low extra memory requirements/absolute correctness not required) for dealing with this situation? 

Because of these two points, it might be more correct to process all octaves for each mip level, but to low pass filter the data for the octaves that are above nyquist. That would be a lot more expensive than just omiting octaves at a cutoff point though (: 

What you are looking for is a way to get access to individual pixels in an image, in a way that you can modify those pixels with CPU code. Once you have that you'll want to find software rendering / software rasterization methods. A great place to start would be reading up in the various bresenham algorithms. There is a lot of info out there on that. Here is one article for instance: $URL$ 

I think the best way to tackle this problem would be to make an algorithm that works on a single triangle at a time. You'd look at each terrain triangle individually that the road intersected and process it. Looking at one triangle individually, you would have to chop that triangle into one or more meshes to make it contour to the road. Here are some of the cases to consider in the image below. I'm betting there is some triangle slicing algorithm that would be suited to this, and I'm not sure of a name off hand, but the main contribution I'm proposing here is that you deal with triangles individually. 

Video games and real time simulation software has the primary need of needing representations that can be rendered in real time, and are also fairly efficient as far as size, so that they don't take up too much memory. Modern day movies, and also "offline processes" (such as baking out lighting into textures to use in real time applications) use non real time rendering techniques to get high quality results. Speed and memory still matter to them of course, but the restrictions are much more relaxed as it isn't uncommon for a single frame of a movie to take a day to render, using a cluster of machines in a rendering farm. Both real time and non real time graphics also have needs beyond rigid body geometry though, and also are interested in finding good ways to model liquids, gasses like smoke, and other things. Besides the obvious representations of geometry like triangle and quad meshes, or bezier and other analytical shapes, there is also a lesser known concept of the signed distance field which can be used for both 3d modeling as well as 2d vector graphics. In both cases, when you zoom into these shapes they have inifinite detail - such as being completely smooth, or having a perfectly sharp corner or whatever else. Here's a video about one way subdivision surfaces can be achieved in real time graphics, shown at SIGGRAPH 2016, regarding the call of duty game: $URL$ If I were in your shoes, part of what I would research would be what the "demo scene" is up to. By demo scene I mean the people who make 64KB executables that when you run them, display photo realistic movies and have music and audio as well. Those folks have some very creative ways of doing things that often ties into cutting edge research topics. Shadertoy.com is a place where you can write webgl pixel shaders and share them with others. There is a lot of really interesting and exotic graphics stuff there, including signed distance field rendering, ray marching, ray tracing, path tracing, and more. $URL$ Here's a demoscene website: $URL$ Lastly, if I may humbly point you towards a paper I wrote, I came up with a technique for getting the GPU texture sampler to calculate Bezier curve points of arbitrary dimension for you as part of a texture lookup, using the non programmable N-dimensional texture sample linear interpolator. I believe it can be expanded to support Bezier surfaces as well, but haven't yet looked into the details of that. $URL$