they have different asymptotic performance? for a special class of graphs, one algorithm requires $\Omega(n)$ time while the other requires $O(n^{1/3})$ time? they have different terminating conditions? (recall, they are solving the same problem) the pre-processing step is different in the two algorithms? the memory complexity is different in the two algorithms? 

Among many possible notions of the sparsity, I am particularly interested in the one that relies on edge density (or alternatively, average degree). 

Edit: The question is clearly very context dependent and is subjective. I was hoping that the above five conditions, however, will allow getting some suggestions. I am happy to further modify the question and give more details, if needed to get an answer. Thanks! 

From the POV of language implementors, can this restriction simplify type checking? How much? From the POV of programmers using languages like Agda, Idris, Coq, etc., are there any useful programs that type check without the restriction, that can't be refactored into equivalent programs that type check under the restriction? From the POV of type theorists and other mathematicians using type theory as a foundational system, does the restriction prevent any useful mathematical results from being obtained internally to type theory? 

There is a direct and obvious correspondence between the expressive power of the metalanguage's type system and the classes of syntactic structures that can be represented accurately in it. For example: 

For instance, talking about graph traversal, breadth-first and depth-first traversal satisfy the above two conditions; for shortest-path computations, breadth-first and Dijkstra's algorithm satisfy the above two conditions (on unweighted graphs, of course); etc. Are these also sufficient conditions? More specifically, suppose two algorithms satisfy the necessary conditions to be similar. Would you indeed call them similar, if 

In their paper Approximate Distance Oracles, Thorup and Zwick showed that for any weighted undirected graph, it is possible to construct a data structure of size $O(k n^{1+1/k})$ that can return a $(2k-1)$-approximate distance between any pair of vertices in the graph. At a fundamental level, this construction achieves a space-approximation trade-off --- one can reduce the space requirements at the cost of a lower "quality" of the solution. 

Damas-Milner is a subset of System Fω that gives up expressivity (type-level computation) for usability (type inference). The experience with Haskell and ML attests to the practical value of this tradeoff. Does a similar Damas-Milner-like subset of the calculus of constructions exist, for which global type inference is possible? I expect the answer to be “yes” based on the following clues: 

However, I don't want full-blown dependent types, or even GADTs, or any of the other crazy things certain programmers use. I just want to define subtypes by “carving out” inductively defined subsets of existing ML types. Is this feasible? 

There is a three-way trade-off between space, query time and approximation in the distance oracle problem. One can trivially answer each query exactly (approximation = $1$) in $O(1)$ time by storing the all-pair distance matrix; or in $O(m + n\log n)$ time by running a shortest path algorithm. For massive graphs, these two solutions may require prohibitively large space (to store the matrix) or query time (to run a shortest path algorithm). Hence, we allow approximation. For general graphs, the state-of-the-art is the distance oracle of Thorup and Zwick, which for any given approximation $k$, requires optimal space. It also gives you a nice space-approximation trade-off. For sparse graphs, a more general space-approximation-time trade-off can be shown. 

What would be the effect of imposing the following restriction on a programming language with inductive type families? 

I need an extension of System $F_\omega$ with subtyping, and where the variance of type constructors is reflected in their kind. Unfortunately, System $F^\omega_{<:}$, as defined in chapter 31 of Pierce's Types and Programming Languages, doesn't address the latter requirement, so I decided to roll my own. Here is the list of additions to $F_\omega$'s I've made so far: Polarities 

In formal language theory, there exist several classes strictly between context-free and unrestricted grammars. For example, in increasing order of expressive power: 

I do not work in theory, but my work requires reading (and understanding) theory papers every once in a while. Once I understand a (set of) results, I discuss these results with people I work with, most of whom do not work in theory as well. During one of such discussions, the following question came up: When does one say that two given algorithms are "similar"? What do I mean by "similar"? Let us say that two algorithms are said to be similar if you can make either of the following claims in a paper without confusing/annoying any reviewer (better definitions welcomed): Claim 1. "Algorithm $A$, which is similar to algorithm $B$, also solves problem $X$" Claim 2. "Our algorithm is similar to Algorithm $C$" Let me make it slightly more specific. Suppose we are working with graph algorithms. First some necessary conditions for the two algorithms to be similar: 

One of the few things that I dislike about Okasaki's book on purely functional data structures is that his code is littered with inexhaustive pattern matching. As an example, I'll give his implementation of real-time queues (refactored to eliminate unnecessary suspensions): 

As can be seen isn't exhaustive, because it doesn't cover the case where the rear list is empty. Most Standard ML implementations will generate a warning about it. We know that the rear list can't possibly be empty, because 's precondition is that the rear list one element longer than the front stream. But the type checker doesn't know - and it can't possibly know, because this fact is inexpressible in ML's type system. Right now, my solution to suppress this warning is the following inelegant hack: 

I am not aware of a general theory being developed on approximation algorithms for problems in P. I know of a particular problem, though, that is called approximate distance oracles: 

Here is a rough proof to the above claim: We start with a copy $H$ of $G$. We scan the vertices of $H$ one by one. If a vertex $v \in V(H)$ has degree $\leq k$ then we leave it as it is. Otherwise, consider $v$ and the edges coming out of it. We replace $v$ by a list of $\lceil \deg(v)/(k-2) \rceil$ vertices, all of them connected by a new path of edges having weight $0$. Each of these new vertices is assigned at most $k-2$ edges that were adjacent to $v$ (these reassigned edges keep their original weight). We repeat this process till the degrees in the new graph are bounded by $k$. The number of new vertices created during this process to replace a single vertex $v$ is $ \lceil \deg(v) /(k-2) \rceil$. The number of new edges created for $v$ is $ \lceil \deg(v) /(k-2) \rceil - 1$. Clearly, the new graph has $\sum_{v \in V(G)} \lceil \deg(v)/(k-2) \rceil \leq n + \sum_{v \in V(G)} \deg(v) /(k-2) \leq n + 2m/(k-2)$ vertices. The number of new edges added because of $v$ is at most $2m/(k-2)$. In the new graph, we replace every vertex by a path (with all its edges having weight $0$), and kept the original edges. As such, the distance between two vertices in the modified graph is the same as the distance between the corresponding vertices in the original graph. A corollary of the above claim is: