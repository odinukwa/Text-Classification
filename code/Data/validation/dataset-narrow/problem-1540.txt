This doesn't answer your Tableau questions, but I can say a few things about ggplot2. ggplot2's has reasonable defaults but nearly everything about the plots can be changed to make the results more clean aesthetically. One package I use all the time is called 'cowplot': $URL$ ggplot2 also has built-in themes ($URL$ and there's a package for community-develeoped themes ($URL$ 

You are describing every binary classifier. However, you are missing a key point. If your classes are separable by the value of just ONE feature, you can do what you're saying and find e.g. F1 > 1.23 as a threshold. If classification involves a combination of features, you will need to describe some combination of thresholds for each feature, or (equivalently) some relationship between the features that tells you about the class label. It's the job of every binary classifier to do exactly this - they just do it in different ways. See for example this post. Your desire to have a combination of fixed sets of thresholds will only work if you can have a set of thresholds that will encompass/describe/classify every combination of feature values. If you want a set of easy-to-read thresholds like you mention, you should read about decision tree classifiers. They'll do something like what you want - but will also ensure that you provide a class label for every possible combination fo features values. The nice about about decision trees is that they'll let you leave out your current feature selection step - they just do it for you by (1) picking the feature that best discriminates the two classes overall, (2) picking the threshold value of that feature that gives the most information about the class label (usually), and (3) repeating (1-2) several times. 

Check the correlation of the variables or features, the ones with the least correlation can be discarded and the ones with highest must be considered to perform prediction. However, least and highest is subjective to your requirement 

This depends on the nature of your data. If you can effectively simulate 435 samples using any given oversampling methods such as SMOTE or ADASYN for instance, then I would say oversampling would be better. Because it would provide data for various scenarios. But if exact replication is not possible and if replicated there might be an inherent problem with the model then you should choose to undersample. A good example for undersampling would be that for example if you're conducting a scientific experiment and the data you have are limited to a few scenarios and the other data is yet to be fully observed, you would choose to undersample. Oversampling would be the reverse case, where you can effectively simulate data for various classes based on some parameter and the generated data can mimic actual data and scenarios, then you should oversample. 

You should choose mean or average over median. Let me explain why, specifically in your case since you're checking for expected computational time. Mean or average could be one of the three cases. 

The following image shows a scatter plot of my data. The Y axis points are the labels, labeled from 1 to 6 and X-axis are dimensionally reduced values of all my features. I reduced them for better visualization. 

Let's start by answering your first question. Is it required to balance the dataset? Absolutely, the reason is simple in failing to do so you end up with algorithmic bias. This means that if you train your classifier without balancing the classifier has a high chance of favoring one of the classes with the most examples. This is especially the case with boosted trees. Even normal decision trees, in general, have the same effect. So it is always important to balance the dataset Now let's discuss the three different scenarios placed. Choice A): This would be what I explained all along. I'm not saying necessarily you will have a bias. It depends on the dataset itself. If the nature of the dataset has a very fine distinction with the boundaries then the chance of misclassification is reduced, you might get a decent result but it's still not recommended. Also if the data does not have good boundaries then the rate of misclassification rises a lot. Choice B): Since you are placing weights for each sample you are trying to overcome the bias with a penalty. This is also called as an Asymmetric method. Normally these methods increase the accuracy of a model by a slight margin but that mostly depends on the machine learning algorithm you are using. In examples like Adaboost such a model the effectivity of the model increases. This method is also called Asymmetric Adaboost. But this might not necessarily work with all algorithms. Choice C): Assuming you have weighted the samples accordingly it should do the same as either choice A or choice B. I'll leave this for you to extrapolate based on my previous explanations. 

Feature-extraction mechanisms like GIST, HOG, etc are built and optimized to improve performance on given datasets. Because of this, they don't perform as well across datasets. It's kind of like putting specialized fuel in a vehicle that isn't built to utilize it - it might even do harm. Hand-engineered features are, as a rule, brittle. I once heard it said that the dirty secret of machine learning is just knowing how to transform your domain-specific information into meaningful features - after that, you can use an extremely simple classifier and it may do surprisingly well. The drawback is that the rules you built are very specific to your domain. Deep neural networks, and convolutional neural networks in particular, are an advancement in that they learn what features are useful about raw data - for CNNs, these are the raw pixel or time-series values. Instead of hand-building feature extraction mechanisms, these architectures automatically build them. One benefit of this is that if you use a CNN to identify images in general, you can re-use the top few feature extraction layers of the CNN on a different image recognition dataset, and re-train the bottom few layers to make the network specific to recognizing e.g. dog breeds. You can transfer what you learned about the statistical structure of natural images in general to other, more specific questions (general -> specific). In your case, the 'top few layers' are analogous to your GIST/HOG methods - and they wouldn't be expected to perform well when the task changes, because they were constructed for a specific task (specific -> other specific). 

There might be different ways to do that, like considering implicit ratings like views or clicks. But basically, you can consider a rating of 1.0 for each user-item pair you have. This way, your prediction will be between 0 and 1 which you can consider similar to a click prediction probability. 

Personally, I don't use vagrant with a local provisioning. I have installed a Spark cluster locally without HDFS which allows me to experiment and develop easily without the overwhelm of a virtual machine. HDFS is not a requirement for local clusters, and it's also a kind of a system administration nightmare if you just need it for local testing. Spark works very fine with local file systems which you'll also have to port when deploy on your Cloud of course. Nevertheless, You can use vagrant with AWS provisioning to create a cluster for heavier testing. Note: AWS works with S3 and not HDFS. AWS's HDFS version is ephemeral and if you put down the cluster, you'll loose all your computations. For persistence, you'll need to write back to S3. 

You are actually creating and Frequent Pattern model without generating candidates. So actually you'll need to generate afterwards if you need to use them with : 

You can also set a threshold where you can drop a recommendation at a certain limit of your prediction level. E.g. Let's say an user alpha has the following recommendations with a threshold of 0.7 

Now, concerning the usual flow of building and validating such models, your code doesn't deal with that. It's usually done through quality measures variations. It can also be done through Feature extraction techniques. With such techniques, you should consider the following. For each association rule, you'll need to measure the improvements in accuracy that a commonly used predictor can obtain from an additional feature, constructed according to the exceptions to the rule. In other terms, you'll have to select a reference set of rules that should help your model perform better. I strongly advice you to read this paper about the topic. So now what does that mean ? This means that you'll need to implement that pipeline yourself because it's not implemented in Spark yet. I hope that this answers your question. 

I'm curious what other people will say, but one option is to use KL-divergence. If your two histograms have the same x-axis, you can divide every column by the total count to convert counts to proportions. Then you could treat the histograms as probability mass, and use the KL-divergence. KL-divergence is really a measure of the distance between two probability distributions, but histograms are an approximation of a discrete distribution. You'd pick one of your distributions to be P and one to be Q, then calculate $-\sum_x P(X)\log{\frac{P(X)}{Q(X)}}$ where $x$ is each bin on the x-axis of the histogram. Be aware that this distance metric is directional, meaning you'll get a different answer if you swap P and Q. The reason is that the KL-divergence is a measure of information transfer (like, the information you gain by moving from Q to P), but I think that's irrelevant to your question. 

People talk a lot about data imbalance, but in general I think you don't need to worry about it unless your data is really imbalanced (like <1% of one label). 50/200 is fine. If you build a logistic regression model on that dataset, the model will be biased towards the majority class - but if you gave me no information about an input to classify, the prior probability is that the new input is a member of the majority class anyway. The question you want to be able to answer is whether you are differentiating classes fine - so if you do have a minority class, do NOT use 'accuracy' as a metric. Use something like area under the ROC curve (commonly called AUC) instead. If your data is really super imbalanced, you can either over-sample the minority class or use something called 'SMOTE', for "Synthetic Minority Over-Sampling Technique", which is a more advanced version of the same thing. Some algorithms also let you set higher weights on minority classes, which essentially incentivizes the model to pay attention to the minority class by making minority-class errors cost more. To learn to differentiate between lots of classes, I think (a) you will need to have a ton of examples to learn from and (b) a model that's expressive enough to capture class differences (like deep neural network, or boosted decision tree), and (c) use softmax output. If those still don't work, you might try a 'model-free' approach like K-nearest-neighbors, which matches each input to the most similar labeled data. For kNN to work however, you need to have a very reasonable distance metric. 

Like @SeanOwen pointed out its called . spark.mllib’s FP-growth implementation takes it as a hyper-parameter under . It is the minimum support for an itemset to be identified as frequent, e.g : if an item appears 4 out of 5 transactions, it has a support of 4/5=0.8. Usage: 

You can drop the i3 recommendation since you don't consider it good enough. Nevertheless, the parameter of your recommendation engine must be determined of course with the help of your evaluation metrics. Considering software solutions, I use Apache Spark MLlib with Scala as a base for my recommendation engine algorithms where you can compute item cosine similarity easily per example where you are using in-house implementation or an approximation with the DIMSUM algorithm. I hope this helps! 

You are probably thinking in terms of regular SQL but spark sql is a bit different. You'll need to group by field before performing your aggregation. Thus the following, you can write your query as followed : 

Since you are using Spark 1.6, I'd rather do these kind of transformations with DataFrame as it's much easier to manipulate. You'll need to use SQLContext implicits for this : 

EDIT: To avoid confusion for some concerning PCA and Dimension Reduction, I add the following details : PCA will allow you compute the principal components of your vector model, so the information are not lost but "synthesized". Unfortunately there is no other imaginable way to display 39 dimensions on a 2/3 dimension screen. If you wish to analyze correlations between your 39 features, maybe you should consider another visualization technique. I would recommend a scatter plot matrix in this case. 

A policy is a state-action mapping. A 'state' is a formalism used in AI that represents the state of the world, i.e. what the agent's idea of the world is. The action is, naturally, what action it should take in that state. A policy just maps states to actions. One of the basic problems in AI is how to maximize reward over time at some task. One strategy for the agent is to try to understand the system and predict the results of their actions, and the reward that would follow. Another strategy is for the agent to try lots of things and record the results. Either of these (eventually) allow the agent to calculate a good policy - and once it's calculated, the difficult computational work is done and the agent just has to 'look up' what action to take in each state. 

A model is built on a specific set of features, which may include categorical features encoded using one-hot encoding. If you have new data with additional categories, your model has no idea how to interpret the significance of those categories. You should either map the new value to none of the 1-hot values identified in training, or to an 'other' value. For example, say you trained on data that had color=[blue,green]. Your one-hot fields would have color_blue and color_green. You could also have a field called color=other, that you might use to encode very infrequent values. That's a data preparation choice. So for 'red', you could encode that as either: 

The question is poorly phrased, I've tried to edit it to the best of my abilities. However here are the problems you've stated, 

Can I say this about your dataset? Say a well is a point on your map, so if each well has an x-axis and y-axis coordinates. So effectively I've converted your dataset into an 11-dimensional problem. Now let's talk prediction. First, perform PCA on your dataset with the number of components = 1.Remember the dataset which you have to perform PCA is on all the data but the X-axis and Y-axis values Then plot your data set in a 3D scatter plot with the other two dimensions being X-axis and Y-axis and the third being the new Dimensionally reduced values done by PCA. Look at the scatter plot if the following data follows any type of regression trend. Then use that type of regression to predict the values. 

The paragraph you mentioned explains a the parametric procedure of creating training data and testing data from a given data set. Let us take an example let us consider that the distribution of a certain dataset follows Normal distribution (Gaussian) This means that 68% of the data lies near the mean of the dataset. Also since the dataset has been identified as gaussian we also know the expected probability function (pdf) of the data set assuming we know the mean and variance of the given dataset. $P(x) = \frac{1}{\sqrt{2 \pi \sigma ^2}} e^{\frac{-(x-\mu)^2}{2 \sigma ^2}}$ Now that we have the formula we can use random variate generation techniques on this formula to create training and test data separately which can be used for the model to learn and test its efficiency. To learn more about random variate generation I'd direct you to this resource here. It has a great chapter which can help you with understanding the statistical technique behind it. 

FP-growth is a frequent pattern association rule learning algorithm. Thus it's a rule based machine learning algorithm. When you call the following : 

We will use implicits now to convert our data into a DataFrame after converting it to Transactions : 

Now all we have to do is a simple group by and perform a collect_list aggregation on the first dataframe : 

You can also write it using a SQL dialect by registering the DataFrame as a temp table and then query on it use the SQLContext or HiveContext : 

I hope that this answers your question. Note: If you wish to know what's the difference between RDD and DataFrames, I advice you to read Databrick's blog entry about it here. 

Scan based operations are basically all the operations that require evaluating the predicate on an RDD. In other terms, each time your create an RDD or a DataFrame in which you need to compute a predicate like performing a filter, map on a case class, per example, or even explain method will be considered as a scan based operation. To be more clear, let's review the definition of a predicate. A predicate or a functional predicate is a logical symbol that may be applied to an object term to produce another object term. Functional predicates are also sometimes called mappings, but that term can have other meanings as well. Example : 

Like I said in the comment, you'll need to perform dimension reduction, otherwise you'll not be able to visualize the $\mathbb{R}^n$ vector space and this is why : Visualization of high-dimensional data sets is one of the traditional applications of dimensionality reduction methods such as PCA (Principal components analysis). In high-dimensional data, such as experimental data where each dimension corresponds to a different measured variable, dependencies between different dimensions often restrict the data points to a manifold whose dimensionality is much lower than the dimensionality of the data space. Many methods are designed for manifold learning, that is, to find and unfold the lower-dimensional manifold. There has been a research boom in manifold learning since 2000, and there now exist many methods that are known to unfold at least certain kinds of manifolds successfully. One of the most used methods for dimension reduction is called PCA or Principal component analysis. PCA is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. You can read more on this topics here. So once you reduce your high dimensional space into a ${\mathbb{R}^3}$ or ${\mathbb{R}^2}$ space you will able to project it using your adequate visualization method. References :