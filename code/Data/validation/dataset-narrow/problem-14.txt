Repeat step 2 to find the next branch with the least divergence. Merge the two branches found in step 2 and step 3, resolving all conflicts. Merge your these two branches into your branch. Test the code in temp_master code to see if it compiles, and builds, and run any other automated tests you have for sanity. 

Our team has two separate repositories for a frontend/backend system. They would like to have these two repositories deployed to the shared team environment together. My understanding of the JenkinsFile is that it will only work on the repository it committed to and only run when the SCM system sends a request to Jenkins informing it of new changes. Is it possible to have the JenkinsFile in the two separate repositories communicate with each other so that when one is built the other is built as well and only after both are built, they will then be deployed? My main goal here is to avoid creating a separate Jenkins job within the Jenkins UI. 

NO I would argue that Mature DevOps operation, does require a Mature Agile process. You are unlikely to be able to get the full confidence to continuously deploy or allow your developers to initiate the deployment process without a mature Agile process in place. However, I believe it is very important to make it clear that an organisation does NOT need to adopt their agile process before building up their DevOps culture and infrastructure. In fact, I would argue that it is actually easier to adopt Agile once you have some basic DevOps working in your company. Rather than Agile being a prerequisite for DevOps, I would suggest that DevOps be used to help advance your agile implementation. 

A file added by some docker instruction and removed by some later instruction is not present in the resulting filesystem but it is mentioned two times in the docker layers constituting the docker image in construction. Once, with name and full content in the layer resulting from the instruction adding it, and once as a deletion notice in the layer resulting from the instruction removing it. For instance, assume we temporarily need a C compiler and some image and consider the 

From the operational perspective, this breakdown of the parametrisation matches the natural degrees of freedom of the deployment problem – aside from the credentials that could be bundled with the runtime configuration, but it is better to separate them to avoid spreading them carelessly. 

Alice rename DatabaseClient to DatabaseClient_v1 and create a delegate class called DatabaseClient that uses an object DatabaseClient_v1 and implements an interface called DatabaseClientInterface. (If possible, this DatabaseClientInterface should be a code artefact but duck-typed languages not always support this.) Bob reviews changes made by Alice in 1 and is aware that his maintenance job should happen on DatabaseClient_v1. Alice introduces a new configuration flag in the application that governs the behaviour of the DatabaseClient delegate and implements a DatabaseClient_v2 placeholder, a class implementing the DatabaseClientInterface whose methods all throw a “Not implemented” exception. 

You are running into the limitations of managed services. Many managed services are great to start experimenting with ideas and can show a great value for the casual user. But they trade ease of use for flexibility and functionality, therefore as soon as we start to have serious needs and are ready to spent a significant amount of effort building up on these ideas, the first move we should do is to opt for a flexible a fully functional solution. In my judgement, you are hitting the line where you need to look for something else – and you will notice that you need to learn a different tool, because that's not free software and you cannot run it yourself. Here we are speaking about Circle CI, but the following limitations are common to many managed services: 

So I have a local jenkins server building artifacts. I trigger this build through VSTS. When browsing through the logs when uploading the artifacts to VSTS from the local jenkins server I found this: This could pose an issue where the triggered build may get beat out by another build (theoretically). While this is not a high likelihood, I would like to mitigate all possible artifact confusions as that is one of the main points my company is moving to build automation and continuous integration. I did see something related to the VSTS Task in issue 4110 but not sure what is coming of this. What I'm trying to accomplish is that when Jenkins finishes the build it uploads it to VSTS. Because of the size of the build, when we go to releases, I would like to not download the artifacts back to our local premise, (using deployment groups) but instead copy it internally directly from the Jenkins build server. Does anyone have any suggestions? 

the release log displays but on the computer the script is running, the process isn't launched. If I run via powershell on the computer from the A1 folder, the process starts as expected. What am I missing? EDIT: Revisiting this after a bit. I have redone the script to use start-process . I have used combinations of using runas with credentials setting working directorys. The process does start...sort of. My target application is a WPF window. It launches but doesn't any any GUI drawn. I know the process is running because i can use which returns a valid process. Does anyone know how to launch wpf/console application and have it draw it's GUI? 

First, while Docker is sometimes seen and used as a ad hoc packaging system, it actually solves a totally different problem: Docker is about running programs. The Docker system allows to describe services, that can be scaled at will and to control swarms of containers. Debian packages are for installing programs and they are able to handle dependencies between software versions. Docker certainly don't qualify as a descent packaging system: each “package” can only have one dependency, the system has no “recursive build” option and does not support complex version constraints! A possible answer would be that, if you are willing to write a Debian package for your application, you can also use Docker to deploy your application. This can be achieved with a configuration script which would look like 

It is a common misbelief that “DevOps” is a role in a company. The word merely denotes an organisational shift in software companies, this shift can be shortly described by the “You build it, you run it!” coined by Werner Vogels: 

Feature flags are an engineering device that can be used to avoid long-lived branch and conflicts in product development. Here is how it can be used the context of an object-oriented language to help developers collaborate on a specific product feature while one handle a new version. This solution can also be used in non object-oriented contexts, provided a notion of “interface” exists. (cf. OCaml module system.) For the purpose of illustration, we assume a tool presenting reports about data stored in a database. The code implements a DatabaseClient class used to perform requests. As the dataset grows, it becomes clear that some alternative data layout would improve the application performance. Therefore Alice will develop a new version of the DatabaseClient able to retrieve data from the structures with improved layout, while Bob will maintain the historical DatabaseClient. With the following steps, Alice and Bob can collaborate on short-lived branches while minimising their conflicts. 

As far as I was able to tell it's not possible. We solved this by using Pipeline putting the custom messages in the Jenkinsfile. The flexibility gained by having the Jenkinsfile in the git repo really helped our developers have a better understanding of the build process, and allowed them to make necessary changes to the build commands without the communications headache. It's a nice excuse to move over to Pipeline. It also happens to be very easy to do: 

DevOps is about the merging of information and processes between those who develop the software and those who deploy the software. If the goal of your hackathon is to produce code which can not, or should not be deployed anywhere, then sure it makes sense to get your proof of concept (PoC) coded before developing the deployment process. However, if you wish for a client (i.e. anybody who isn't one of the developers) to be able to view the PoC from a remote computer, then part of your hackathon already includes DevOps. To not use DevOps for a PoC or Minimally Viable Product (MVP) would be a waste of resources and time, as you work on coordinating the deploy process with operations. DevOps will aide you in deploying more iterations of your PoC or MVP to be reviewed by more people in less time, and will thus enhance your Hackathon efforts. 

In this case, "Fire and Forget" doesn't mean what you think it means. It isn't the case that you fire the build and then forget about the outcome. What actually happens is that you fire the event, and then forget about what the process is doing up until the point where the process returns feedback to you and reminds you about what was fired. As an example, the old way of doing things might be to trigger a build and then let it run while you watch the output. You sit there watching the results of the build as they occur and don't work on anything else productive during that time. Or you do work on something productive, but you have one eye on the build process. When it is done, you need to either be paying attention, or remember to check on it to see the results and then continue based on that. In Jenkins model of "Fire and Forget", you have some automated process do the build for you, and your mind is not focused on the build process until something goes wrong, or the build completes. At that point, you get a message from Jenkins, either as an email or in a program like slack, which now reminds you of the build process and tells you all the information you need to know to move on. In the meantime, you were working on some other task with your full focus, because you knew that you didn't have to keep an eye on it. The automated system would alert you to anything you needed to know. 

So I wrote this powershell script to be ran by the deployment group agents who have access to the server locally. 

I have hosted VSO build agents building our exe and that pipeline setup. I am trying to deploy the produced exe to a dev testing environment. I have a deployment group set up and the files are being successfully copied to the correct location. What I'm trying to do now is launch the exe after it was copied. I have an inline powershell script task running consisting of 

I have a few CI environment in VSTS. One for dev, QA, staging, and production. my builds are created in the cloud by VSTS build agents. these builds are not obfuscated by design and need to be deployed to my dev and test environments as such. When they are approved by QA in the test env, they go to staging. What I need to have happen is for the build to be obfuscated before being put in this environment. Is it possible for me to run a docker container in VSTS release pipeline to obfuscate the build in the container and then download the result in a deployment group? My main questions are boiled down to this: I would like to have a container image in the cloud running a tool installed on the image. During my VSTS release pipeline, I would like for my release agent to pull down this image, pass in the build artifact, and then do something with the results of running my artifact through this tool. I have an Azure subscription. I've tried reading the documentation but I'm confused as to what I need to set up. This container is not hosting any web app, but is being used as a way to run my tool locally on the release agent. I see that I can run a docker run command as a VSTS pipeline action. What do I need to set up on my azure subscription to host this image for the VSTS agent to pull it down? 

each having three scripts called , and . Now that the organisation of automation items has somehow been clarified, let's turn our attention to configuration. The main conditions and requirements about the configuration organisation are set by the verb when applied on a service-like artefact. The verb should have the following parameters: 

After this, Alice and Bob can collaborate without explicit synchronisation, because code written in their respective iterations is subject to the DatabaseClientInterface. This minimises the risk of a conflict resulting from their concurrent work. Iterations from Alice can be very short, like implementing a test, implementing a method, or even partially doing so, because in production, the code is not selected for use and does not need to be fully functional. The automated testsuite should be configured so that the DatabaseClientInterface always uses DatabaseClient_v1 while Alice can easily toggle to DatabaseClient_v2 when running the testsuite locally – or in a custom CI setup. Once everything is ready, a single commit can perform the change, by updating the configuration value governing the DatabaseClient delegate. 

My understanding of Ansible roles is that they are the unit of reusability when implementing processes using Ansible, similarly to what a class or a package is in some computer languages. It therefore seems natural to use roles to make all tasks understanding a given data structure in a specific role, but this yields the question of how to select the actual tasks to perform? Is it a good design to use the file in the task as an entrypoint to select actual tasks according to role parameters? For instance a role could feature: 

The biggest challenge I've encountered with people resisting the change is the reason of not seeing the value of it, or build automation in general. His idea was, we were our getting product(s) out the door, and on time. So why do we need to "create work" for ourselves, and take a developer off of working on a product. After 4 years since then I've finally got the owners on board with CI by capturing metrics on how much time is wasted copying files manually from bin folders to SVN for qa to copy from SVN to environments. This along with the overhead of managing what build is in what environment and other pitfalls of you are testing the wrong build etc. He has finally came around, but I think he feels that the time could be put to other tasks. 

Relatively speaking, the concept of devops is new and still defining itself in my opinion. I currently fulfill a devops engineer role. For me, this means I facilitate and develop the tools and processes used by both our dev and ops teams freeing them to focus on the product that generates revenue for the company. The ops and dev teams spin up their own servers and such as needed. I just hook up the CI for our products, ensure our processes makes sense and seek out what process can be improved/automated. I meet with all of our departments, from sales, to warehouse, to developers and operations (QA and release managers) to see what they are doing and how I can improve their process.