Implementing resumptions naively is slow, because the continuations in most programming languages are used in highly stylized ways. That is, in most languages you can implement continuations as a single imperatively-updated stack, and the overhead of trampolining simply arises from the fact that a linked list of thunks is a less efficient representation than a mutable stack. So compiler writers often use resumption-based intermediate forms, but implement them using better data structures. (And as a programmer, you can often use an optimized continuation representation "by hand".) Papers you might want to look at: 

A simple model of temporal logic is via time-indexed truth functions. This lets us model the Boolean connectives, as well as the next-step operator and modal always operator: $$ \begin{array}{lclll} n & \models & P & \iff & \mbox{condition}\\\hline n & \models & \top & \iff & \mbox{always} \\ n & \models & P \land Q & \iff & n \models P \mbox{ and } n \models Q \\ n & \models & \lnot P & \iff & n \not\models P \\ n & \models & X(P) & \iff & n+1 \models P \\ n & \models & G(P) & \iff & \forall k.\; n + k \models P \\ \end{array} $$ Thinking of this in terms of the Kripke-Joyal semantics for modal logic leads us to observe that the natural numbers, viewed as a discrete category, are monoidal with addition, and so the Day tensor product exists. Specializing the definitions gives us the following forcing clauses: $$ \begin{array}{lclll} n & \models & P & \iff & \mbox{condition}\\\hline n & \models & P \otimes Q & \iff & \exists j, k.\; j + k = n \mbox{ and } j \models P \mbox{ and } k \models Q \\ n & \models & P \multimap Q & \iff & \forall k.\; k \models P \mbox{ implies } n + k \models Q \\ \end{array} $$ So we have an adjunction between $\otimes$ and $\multimap$. Namely, we can show that $P \otimes Q \vdash R$ if and only if $P \vdash Q \multimap R$. My questions are: 

Francesco Tiezzia, Nobuko Yoshida I haven't read this one, but only just discovered it when Googling for the other papers. Given the authors and the subject, I suspect this is right up your alley! 

If you are interested in provability, then what you want is the Gödel-Tarski embedding of intuitionistic logic into classical S4 modal logic. The idea is that you can interpret the box modality of modal logic as a "provability" modality, and then stick a box in front of each connective. Then you can put the formula into whatever variant of modal CNF you need. See Mints' paper The Gödel-Tarski Translations of Intuitionistic Propositional Formulas. If you are interested in retaining the ability to translate intuitionistic proofs, then one popular idea is to use Tarski's "high school identities" to put intuitionistic formulas into a normal form related to the exponential polynomials. This gives a normal form for types, whose translation preserves the beta-eta equivalence of proofs of that type. See Danko Ilik's The exp-log normal form of types. 

Can someone point me to the reference for the non-definability of the modulus of continuity functional in PCF? $\newcommand{\N}{\mathbb{N}}$ $\newcommand{\bool}{\mathsf{bool}}$ Andrej Bauer has written a very nice blog post exploring some of the issues in more detail, but I'll summarize just a bit of his post to lend some context to this question. The Baire space $B$ is the set of natural number sequences, or equivalently the set of functions from naturals to naturals $\N \to \N$. For this question, we will restrict our attention only to the streams which are computable. Now, a function $f : B \to \bool$ is continuous if for every $xs \in B$, the value of $f(xs)$ depends only a finite number of the elements of $xs$, and it's computably continuous if we can actually compute an upper bound on how many elements of $xs$ are needed. In some models of computation, it's actually possible to write a program $\mathsf{modulus} : (B \to \bool) \to B \to \N$ which takes a computable function on the Baire space and an element of the Baire space, and gives back the upper bound on the number of elements of the stream. One trick for implementing this is to use local storage to record the maximum index into the stream seen: 

With fancier graphics geometry (say with vector graphics), each widget will store a data structure representing the shape, and your hit test gets more complicated to match. The things to Google for: 

It's fairly well-known that classical and intuitionistic arithmetic are equiconsistent. One way of showing this is via the "negative embeddings" of classical logic into intuitionistic logic. So, suppose $\phi$ are formulas of classical first-order arithmetic. Now, we can define a formula of intuitionistic logic as follows: $$ \begin{array}{lcl} G(\top) & = & \lnot\lnot\top \\ G(\phi \land \psi) & = & \lnot\lnot(G(\phi) \land G(\psi)) \\ G(\bot) & = & \lnot\top \\ G(\lnot\phi) & = & \lnot G(\phi) \\ G(\phi \vee \psi) & = & \lnot(\lnot G(\phi) \land \lnot G(\psi)) \\ G(\forall x.\;\phi) & = & \forall x.\; \lnot\lnot G(\phi) \\ G(\exists x.\;\phi) & = & \lnot \forall x.\; \lnot(G(\phi)) \\ G(P) & = & \lnot\lnot P \\ \end{array} $$ Note that $G(\phi)$ is basically a homomorphism that sticks in extra not-not's everywhere, except that at disjunctions and existentials, it uses de Morgan duality to turn them into conjunctions and universals. (I'm pretty confident this is not the exact Godel-Gentzen translation, since I cooked it up for this answer -- basically anything you write using double-negation + de Morgan duality will work. This variety actually turns out to be important for computational interpretations of classical logic; see below.) First: It's obvious that this translation preserves classical truth, so that $G(\phi)$ is true if and only if $\phi$ is, classically speaking. Second: It's less obvious, but still the case, that for formulas in the $\forall, \implies, \land, \lnot$ fragment, provability in intuitionistic and classical logic coincide. The way to prove this is to first look at formulas drawn from this grammar: $$ \newcommand{\bnfalt}{\;\;|\;\;} \begin{array}{lcl} A,B & ::= & \forall x.\;A(x) \bnfalt A \implies B \bnfalt A \land B \bnfalt \lnot A \bnfalt \lnot\lnot P \end{array} $$ And then we can prove as a lemma (by induction on $A$) that $G(A) \implies A$ is derivable intuitionistically. So now, we can show the equiderivability of negative formulas by doing an induction over the structure of the proof (in, say, the sequent calculus) and use the previous lemma to simulate the law of the excluded middle. So, how should you think about this intuitively?