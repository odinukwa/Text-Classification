Your existing opinion is correct, though there's one extra detail. The geometric normal is the normal of the actual triangle, based on the vertices' positions (the cross-product of edge vectors, as you said). The shading normal is altered from this, and is used when shading a fragment or ray hit. In the simple case, it's the interpolation of the three normals set at the vertices. This is used for curved objects to hide the polygon boundaries. It might also be modified by a bump map or normal map, to add finer detail to the appearance than the polygon mesh has. 

Have a "reset up" button that applies the pitch angle limits and resets the up vector to world up. Give the user arbitrary control over camera roll. (This can be confusing, so it's best to offer the first option as well if you do this.) Constrain the up vector so that its dot product with the world up is non-negative. That is, if the user pitches so far up or down that they are upside-down, roll the camera 180 degrees. 

The remark about "$\delta$-functions" is because of a function (properly speaking, it's not a function but a distribution) called the Dirac $\delta$ (delta). This distribution is an infinitely thin, infinitely tall spike at $x = 0$, whose area is 1, just like our perfect specular BRDF. 

I think the formula at the top only makes sense if $n$ and $m$ are pixel indices, i.e. row and column numbers from 0 to $H - 1$ and 0 to $W - 1$. This way, for the four surrounding pixels, the weight varies from 0 to 1. As a representation of the algorithm it's unnecessarily opaque, because you don't actually want to look at every pixel of the image and multiply most of them by zero. To implement it, you want to do something closer to your diagram, where you only look at the four surrounding pixel values, and sum just those four, multiplying each by $(1 - |x - m|)(1 - |y - n|)$. This is exactly how your four-line equation for $V$ (near the bottom of your question) expresses it, and the code should look most like this. 

As Simon F says, the meterological phenomenon that causes this is haze. A physicist would call it an example of scattering, but the visual effect is known as aerial perspective. Reproducing this in paintings has been a technique since the 15th Century, with Leonardo da Vinci one of the early proponents. Since your question included a nice real-world example, here's a computer graphics example, courtesy of Dave Pape (CC-by) from a slide deck of techniques for showing depth: 

The reason Vulkan makes this less of a problem is that it creates a new kind of state, which depends on the blend mode, the shaders, and other parameters. Changing any of these requires the app to perform an explicit "prepare" operation, which might recompile shaders. This gives the app control over when it happens, including the ability to prepare all the combinations of parameters up-front in a loading screen or in a background thread. It means that shader compilation never happens without the app asking for it explicitly. And it means the app can just go back to the old state by switching to it, instead of changing the blend mode and waiting for another shader recompilation. 

The relevant equation on the Wikipedia page follows: $$ I_\text{p} = k_\text{a} i_\text{a} + \sum_{m\;\in\;\text{lights}} (k_\text{d} (\hat{L}_m \cdot \hat{N}) i_{m,\text{d}} + k_\text{s} (\hat{R}_m \cdot \hat{V})^{\alpha}i_{m,\text{s}}) $$ In this equation, all the $k$ and $i$ terms are not numbers but colours (RGB intensities). This includes $k_a$ and $i_a$: the ambient colour of the object, and the ambient illumination colour, respectively. The ambient term isn't a constant: it's a product of an intensity and albedo, just like the specular and diffuse terms. The only difference is that the lighting and view directions aren't a factor in this term. It's very unusual to have $k_a$ different from the diffuse colour, so you can think of it as multiplying by "the colour of the object". That said, it's quite common for $i_a$ to be a distinct colour: for example, if you're using a yellowish directional light for direct sunlight, you might want the ambient light to be blueish to stand in for skylight. 

Yes, the back buffer is for the GPU to write the frame that's in-progress, while the display controller sends the front buffer to the attached display. Swapping buffers doesn't really copy anything these days. The display controller has a register which tells it the address of the frame buffer to display. Swapping buffers simply puts the address of the old back buffer in this register, making it the front buffer. You should only swap buffers when the frame is complete (i.e. you've finished drawing all the objects) and you want it to be displayed. This frame will continue to be displayed until you swap buffers again. In some configurations (e.g. in EGL it depends on what swap interval you set) swapping buffers will make the CPU wait (block) until the GPU has finished rendering that frame. In others, the CPU can continue to queue up draw calls for further frames. 

I don't know if this is the real reason, but here's one possibility: because it's unnecessary. It's easy to say "JPEG = lossy = bad", but JPEG doesn't just destroy image quality willy-nilly. What the format gives you is a slider with "small, low-quality" at one end and "big, high-quality" at the other. You can easily reduce the byte size of a PNG image by encoding it as JPEG, and if you use the high-quality end of the slider, you needn't lose any visual appeal. Although normal JPEG is lossy in the information-theoretic sense, a high-quality JPEG appears identical by the naked eye. Many image hosting sites will replace images by lower-quality JPEG files not because of the storage cost but because of the bandwidth cost. If you're serving millions of views of these files every day, the cost of sending those extra bytes is pretty big, even though the cost of storing them is tiny. And those bytes don't just cost money: they also cost load time for your users on mobile networks. These sites will have targets for how long their pages take to load on slow connections, and reducing the quality slider ever so slightly makes a big total difference. In summary, JPEG encoding can be lossless or lossy, and it can be as high- or low-quality as you want. Using PNG everywhere instead of JPEG would be costly for image sites and would make you wait longer for your art files to download. Choosing a high-quality JPEG file can give you something that looks just the same as the PNG without being as "heavy". 

I'm trying to investigate the effect of certain image operations on how VR scenes are perceived. To do this, I'd like to run an off-the-shelf SteamVR application, capture the frames as they come out of the compositor, run a shader or compute kernel on them, and have them displayed as normal on the HMD. I'm aware of the latency penalty for doing this; I have some ways to minimise it, and in this case I can tolerate extra latency, so don't worry about that for now. I can see three strategies which might let me achieve this, but I don't know enough about the display subsystem to know which will work and how to start. 

There have been lots of papers over the years on different techniques for drawing height-field terrain in a ray-tracer. Some algorithms ray-march the grid directly (or via a quadtree); others transform the terrain into a polygon mesh and use a standard ray-triangle intersection test. The research seems to have moved on in the last few years, and it's hard to find papers written in the last decade, but the balance between memory and compute (both CPU and GPU) is still changing. What kind of algorithm gives best performance on high-end desktop machines nowadays? Or if there isn't a single answer, how do the performance characteristics of the current best algorithms differ? 

It's like this because the surface area of a unit sphere is $4\pi$. As ratchet freak points out, the integral of a probability distribution over its domain has to be 1. Put another way, the probability of choosing some direction is 1. The surface area of the hemisphere is $2\pi$. The constant that you integrate over a $2\pi$ domain to get 1 is $\frac{1}{2\pi}$. The connection to steradians is that 1 steradian is defined as the solid angle that subtends a surface of area 1 on a unit sphere. That is, a unit sphere has $4\pi$ steradians and a surface area of $4\pi$. It's just the same in two dimensions. A radian is chosen so that a 1 radian arc of a unit circle has length 1, so there are $2\pi$ radians in a circle, and $\pi$ radians in a semicircle. If you choose 2D directions uniformly on a semicircle, the probability distribution is $\frac{1}{\pi}$. 

Sorry this is a bit of a half-answer but it's too big for a comment. I hope it gives you a good starting point at least. 

I made a visualization a little like yours years ago, and used a similar technique, because it's the natural approach to the problem. After I spent a few years writing drivers for modern GPUs, I wouldn't use this approach at all, because it creates a dependency from one frame to the next, which introduces pipeline stalls and/or false sharing. Today, I'd keep the path of the particle as part of its state, and render the whole trail each frame, instead of reusing the old FBO. It's conceptually a little more difficult, and you have to be careful that the way you draw the past path is exactly the same each frame. For example, if you represent the path with some kind of cubic spline, you can't just move the final control point forward, because it'll change the whole curve slightly. A circular buffer of (x, y) positions would be much easier to get right. What you get in exchange for this extra complexity is (a) reduced frame time, memory use, and power consumption (which might be important if you're on mobile), and (b) you won't have your current problem at all. Maybe the frame time advantage won't be as big in WebGL, because you'll end up doing extra work in Javascript, which is slow, but memory bandwidth is more of a bottleneck on most systems. Just make sure that the co-ordinates you store are in the space of your simulation, and transform them into screen co-ordinates when you draw them, just like you would with a 3D scene.