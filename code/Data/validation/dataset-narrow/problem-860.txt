if available is the tool of choice. A command like should return data required to locate the nameservers. This will include records, , and optionally records. 

On Ubuntu these options are available but commented out in . You would need to uncomment the appropriate lines in this file as well as uncommenting the line including this file in . This would be an appropriate file to add the configuration to if you have it. It has been my experience that will accept configuration data in any order as long as it is in a file that is included in the configuration. The command will display the configuration data after includes and comment removal. 

After the dot is when the email is delivered. It is also when your email is scanned. Things that can/do happen include: 

Both SPF and DKIM are entirely optional, but they do help distinguish your server from a spambot. I recommend using SPF for all domains. This can be as simple as for domains sending email, for mail servers, and for all other domains. 

Consider using Expires for your expiry information. In the absence of max-age in Cache-control, it provides the same functionality. Use Cache-Control for additional cache control information. If you configure the caching correctly, you should see fewer validation requests which pass through intermediate caches. This will reduce your bandwidth. Check RFC2616 section 14.9 for the cache values you might want to override. Cache-Control is mainly for overriding cache behavior of intermediate caches. However, there are directives for the browser cache as well. 

You need name resolution to work differently from inside and outside the network. Your dyndns entry will route to external IP. Firewall routes are different from inside and outside the route. Add you dyndns name with the servers IP address to the local hosts file on OpenWrt and the dnsmasq DNS service should override the entry from the Internet. If you have it configured correctly, your name should resolve to your server's IP address from inside your network, and your external IP address from outside the network. It is possible to configure hairpin NAT, but it is relatively difficult and fragile. 

Looks good. I would drop the accept on port 20 as it should be handled as a related packet to an FTP connection. I usually put port 123 as the first check to minimize latency for NTP. For the OUTPUT chain only DROP or REJECT rules would make sense. The existing output rules don't do anything but duplicate the policy. You may want to look at your counters and adjust the ordering of rules accordingly. Consider using different chains for new connections on each interface. Consider logging non-accepted packets. Using a tool like Shorewall to build the firewall might make it easier to get everything in place. 

Try using the host command to see what the servers are returning. The command will tell you what believes the addresses of are. (Google should have multiple addresses, most sites have one.) Try it with each of your DNS servers. Also try . This will give the resolver's version of addresses for google.com. You should get roughly the same results for all four commands. EDIT: To run the host command you may need to install the package to get the command. However, it is often installed by other packages. EDIT2: If one of the DNS servers is not returning a result to you, check its security setting. You may also want to temporarily enable logging on the DNS server. The following is the logging specification block I have used on my bind server. 

I sign for multiple sender domains with the following configuration items. All site use the same private key. Setting the private key would be more difficult if they used different keys, but it can be done. I use the selector as the private key file name extension. It is currently . 

Check the configuration settings for the wireless device. The command should give the settings. These are the settings I have configured: 

If you are looking at graphing your monitoring output you may find Munin simpler to setup than Nagios. Either will only monitor total bandwidth usage on the interface. Munin's alert configuration is not as flexible as Nagios's. I have had success using n2rrd to generate graphs from Nagios monitor output. Ntop is an entirely different tool than Nagios and Munin. It is designed to give breakdowns on bandwidth usage. Breakdowns include interfaces, hosts, protocols, and ports. 

Exim allows override/modification the configuration in ACLs. You could use a connection ACL to select the configuration you want based on IP address. The configuration mechanism has extensive conditional capabilities. There are a variety of methods you can you to achieve your goals. 

Using the common directory allows me to avoid repeating definitions for each server. The config file has a path variable which can be used to specify a list of directories containing common files. If you have a lot of servers with the same rule set you could place the rules in the directory, or a different shared directory for that class of servers. I use to build the firewall scripts and . These are then distributed and executed using . I've tried some of the graphical firewall builders, but find shorewall with its config files easier to work with and verify. I keep the whole configuration directory in to allow me to easily verify changes before implementation. 

If you have mail incoming on these servers blocking, IP ranges that big is not a good idea. The attempts should not burn much bandwidth as they should be blocked before the message is actually transmitted. Consider using spamhaus.org blacklist, that should cut out most of these attempts. 

It is likely you have the default setting for login and administration set. // Secure login and admin define('FORCE_SSL_LOGIN', true); define('FORCE_SSL_ADMIN', true); If you have allow insecure (non-HTTPS) logins, you should be able to login over HTTP. However, you also need to allow insecure (non-HTTPS) administration to access the administration pages over HTTP. It is preferable to setup HTTPS and access these functions securely. You can use self-signed certificates, a certificate from Let's Encrypt, or a certificate from any number of certificate authorities. 

Localized error messages are commented out in the default configuration. You need to modify the file to enable the localized pages. It is dependent on a number of modules being enabled. If you are getting the files from the error directory, these files contain the response in multiple languages. The content is determined based on the language(s) specified by the browser. The header and footer are found in the include sub-directory. Some browsers may use their own message in place of the server provided content. Try using , , or to test the output. Once you are getting a correct message with one of them try using your browser. Your browser may also have cached a response from a previous attempt, so you may need to force a refresh or flush the browser cache. You can use different files for different sites or use server side includes to add site specific information. The URL for the error pages should be in the form . EDIT: I am working from the install on Ubuntu 13.10 but the configuration has been quite stable over the years. The provided pages require the alias, include, and negotiation. I believe you need a restart to enable them if they are not already enabled. The command should tell you if they are already enabled. Try using to verify the page can be served. Until that works, the error page will not be served. Then try using to test the output. The error pages don't seem to be served if the client doesn't provide the language negotiation information. The directory should have the header and footer for the message. You can use directives to change the directory these files are read from to customize by site. You need to ensure this directive occurs before the directive for the directory. Doing the aliasing in the site configuration would be appropriate. Something like: 

Configuration for DKIM. doesn't seem to be on the standards track anymore. Replace with the key name you use in when signing. 

I've translated your DNS records into bind format. Unless you are changing an entry, you shouldn't need to specify the TTL for an entry. I've converted the SPF records to the simplest form. The rule for the record is for for SPF checks. The rule for indicates that no email should be sent from the sub-domain. Entries for your domain, sub-domains follow. (Note the dot at the end of the MX record. If it is missing you should just use ). 

The parts are always reassembled in order. The 255 character limit is on parts of the TXT record. Without EDNS0, there is an addtional 512 byte limit for a UDP response. However, this does not apply to a TCP response. 

The configuration files have a syntax that allows complex processing. This is the language that was used for the configuration snippet you included. The language is Documentation for Exim documents the configuration. It is possible that the mail is being sent by software other than . If it was sent by , the transmission should be recorded in or the rotated log for the period. The default configuration is usually quite secure if the configuration variables are set correctly. Normally it is configured with a number of macro definitions starting . In your case, I would expect enabled. Fix your recipient processing, and you should be able to revert the changes made by your provider. To revert it, just enable the preprocessor directives at the top and bottom of this section. Relay restriction is usually handled in the recipient ACL. Check the configuration file for a line beginning . Then search for the value on the right side of the assignment. This will be the ACL used to check recipients. Read through this checking for and blocks until your get to a line like . That block should read something like: 

The order of reboots is important. Rebooting the server after the clients can result in this situation. The stale NFS handle indicates that the client has a file open, but the server no longer recognizes the file handle. In some cases, NFS will cleanup its data structures after a timeout. In other cases, you will need to clean the NFS data structures yourself and restart NFS afterwards. Where these structures are located are somewhat O/S dependent. Try restarting NFS first on the server and then on the clients. This may clear the file handles. Rebooting NFS servers with files opened from other servers is not recommended. This is especially problematic if the open file has been deleted on the server. The server may keep the file open until it is rebooted, but the reboot will remove the in-memory file handle on the server side. Then the client will no longer be able to open the file. Determining which mounts have been used from the server is difficult and unreliable. The option may show some active mounts, but may not report all of them. Locked files are easier to identify, but require the locking to be enabled and relies on the client software to lock the files. You can use on the clients to identify the processes which have files open on the mounts. I use the and mount options on my NFS mounts. The option causes IO to be retried indefinitely. The option allows processes to be killed if they are waiting on NFS IO to complete. 

To your followup question yes and no. Applications can specify encryption, but the encryption is done at the application level. There are a wide variety of unencrypted/encrypted protocol pairs using different ports such as HTTP/HTTPS, LDAP/LDAPS, IMAP/IMAPS, and SMTP/SSMTP. These all use SSL or TLS encryption. Some services will offer a startTLS option which allow an encrypted connection to be started on the normally unencrypted port. SSH is an application which always uses an encrypted connection. Encryption is end to end for these cases. (There is a NULL encryption algorithm which can be used, and the encrypted content will be transported unencrypted.) IPSEC is configured by the administrator and the application will not be aware whether or not the connection is encrypted. I have mainly seen IPSEC used to bridge traffic between LANs over unsecured connections (VPN connections). I believe IPSEC may apply to only part of the route, so on some network segments the data is transmitted in the clear (unencrypted). Given a choice I will use application encryption as network encryption is not heavily used.