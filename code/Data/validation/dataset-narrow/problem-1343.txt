Just because you want to use an entity-component architecture doesn't mean you need to use it for every game object. Forget about trying to implement edge cases like terrain as a collection of generic components, because all you'll end up with is components specific to terrain. Entities are really only good for game objects that exist in many different forms with differing behaviours depending on the values of certain properties, like mobs. Instead of making terrain an entity, make it an intrinsic part of the physics/collision systems. That way, your entities can raycast and pathfind and bounce around and interact with the terrain without needing to handle the terrain differently than the other static objects in the scene. Hell, the entities don't even need to know that the terrain exists, just that they can interact with it if it does. 

If you want to implement a messaging system, I would still use spatial data structures so that you can skip sending messages to objects that are too far away to care. In that case, you might implement a "message radius" and have the message dispatch system automatically make the query using, for example, an octree for objects within that radius, then send the message to the objects that the query returns. You'll save a lot of precious cycles that don't deserve to be wasted on sending messages to objects that don't need to receive them! 

Alternatively, if you were doing a more complex physics system you could implement gravity as a generic, persistent force. This would make it easier to do things like different gravities in different regions of space. 

Should I just grit my teeth and make every entity a blob of the same data? Or is there something else I can do to create distinctions between entity types? 

on IOS as far as i know you can't create a framebuffer with a different resolution than the screen so the only approach is to render everything into a texture and draw it afterwards on the screen. Also targeting different resolutions is quite hard and there are a couple of things to take into accound: Aspect ratio of the screen: The texture you create should have the same aspect ratio as the screen otherwise thing will look stretched afterwards. If you don't want to do this you can fake the aspect ratio of the game from the projection matrix but in this case the pixels from your texture will be stretched unevenly creating aliasing artifacts that will just look bad. Aspect ratio again: It is important that the projection matrix takes into the account the screen also because on a wider screen you will see more on width so the projection has to be wider otherwise the content will look differently. Menus: This is a really hard topic but the general approach is to have the elements scaled evenly on width and height but this is not possible if you have different aspect ratios unless you split the menus into parts and apply uneven scale only on salable elements(like the empty space between buttons). Much like how html pages are made , you have the content that is fixed(or scaled evenly) and the borders are stretched until it fill the whole screen. Artifacts generated by scaling: When rendering to a texture and scaling it up you will get aliasing artifacts and they are hard to remove. You can use an upscale filter, but this can be expensive on mobile devices. 

It sounds like you're making an RTS, so let's say you need to have a building that automatically attacks the nearest enemy within 100 metres. 

Since this is the case, I wouldn't keep using a hierarchical scene graph. I'd break down the world using an octree. The benefits of using an octree are that you get your frustum culling and LOD determination without needing to fit every object into a hierarchy. If you want, of course, you can still have object hierarchies like in a traditional scene graph. On the flipside, octrees are a little more difficult to maintain, especially considering objects whose bounding volumes may lie inside multiple octants, but they integrate much more easily with physics simulations. Just remember to choose an appropriate maximum subdivision level to avoid having a lot of objects in more than one octant. 

The only real reason to write API code in classes vs. free functions (I hope you're grouping them in namespaces!) is so that you can swap in concrete API code if you decide to switch implementations. To do this, you have interfaces for all your API-specific objects (renderer, mesh, texture, etc.).Program all your high-level code (like models, lights, materials) to these interfaces. 

To make this work without a variable time step, you can keep your current value and remove the multiplication by (delta time). However, I strongly suggest that you move to a time step that is refresh-rate-agnostic. 

Although my game uses a peer-to-peer model, I've still taken the approach of assigning each player to be a partial authority. What I mean by this is that each player acts as the "server" for a number of objects, and all the other players are "clients" for those objects. It also uses a fixed update interval and variable render interval. I'm considering a 50Hz update frequency, so the rendered time lags behind the current time by 20ms in order to guarantee two states to interpolate between. When a client recieves authoritative state information from the server, it comes with the timestamp of when it was sent. Because all the players use a synchronized clock, the client can correctly place the state along its "update timeline", the state history of which is stored for up to 100ms back. However, in order to hide latency, each client also predicts the behaviour of all objects in the vicinity, regardless of whether or not they are the authority. The difficulty I'm having is figuring out how to correct errors on the client side and stay synchronized with the server state, when server information for a specific object will be coming far less often than updates happen. What is a good way to smoothly correct current information with information from the past after several prediction steps have been done on estimated information? 

Requirement: and must be normalized, and must be in the same space(radians or degrees) Also for 8 directions you must specify all 8 directions vector and the angle_threshhold must be 

Setting the position before you destroy an to 'infinity' will handle all the appropriate collisions calls. 

I would not call this a that simple but here is now you can do it: Let's first have some notations (given the picture you provided): 

Here is an approach to get the direction regardless of the how many directions you have and how many space dimensions: 

interleaved: if is easier to understand and to manage memory wise and code wise. Instead of having 4 buffer you have just one and that's it. separate: there is no practical advantage in sharing resources, managing 3d stuff is difficult as it is and by having the separate streams shared would only create a nightmare. You might gain some memory by doing that but it's not worth it. on performance there might be a difference , at least on some Android GPU i know that you will have different results. 

Changing to should solve the problem in this case but it's not a general rule. Here is why: Some math facts first, for matrix multiplication ; also So when you take a tutorial or code and you see in code and you want to do the same in the shader you have to think how and will end up in the shader code (transposed or not transposed). If it will end up transposed then you need to do to achieve the same result, otherwise A * B. If a matrix ends up transposed or not is dependent on the rendering Api and the matrix library you used. When you sent the matrix to the shader the api will copy a series of floats and interpret that as a matrix, but it doesn't know if the first 4 floats represent a column or a row in the matrix. 

Below-surface water should fill up the remainder of the voxel Surface-level water should have the same total density, , as its neighbours Water may be displaced to any voxel immediately adjacent and below it, provided that the neighbour's total density is less than its total density 

The best practice is to create the new window first, sharing the resources with the old window's context, and then destroy the old window. However, it doesn't seem like GLFW (even v3.0) supports resource sharing yet. For now, you may have to retain the data in RAM and then just recreate the OpenGL objects in the new window's context. To accomplish this in GLFW, you pass an existing object to . (Thanks @tecu for the clarification!) 

I once created a system like this as an experiment. The way I did it was to abstract the pixel-perfect path of the mouse away into a series of quantized directions of travel (rounded to 45Â° for my purposes). My primitive matching algorithm just tried to fit a pattern to the series when the user released the mouse button. If I were to do it again I would add in these features to make it more robust: 

I found that with plain SDL2, even using the accelerated renderer, that redrawing all the tiles every frame was a huge bottleneck. I would suggest drawing as much as you can to a static texture that you then draw using the camera's clipping rectangle. Now, I was drawing around 3600 tiles per frame (8x8 tiles at 640x360) on a relatively low-powered laptop, so your use case may not be a problem. The best solution would probably be to dip into OpenGL and store all the tiles in a vertex buffer (or a grid of vertex buffers) and draw those instead. Advantage being that it would save memory and you wouldn't be constricted to using basically only one tile layer per texture. 

The application is usually tested on the targeted platform with the worst case scenarios and you will always be prepared for the platform you are targeted. Ideally the application should never crash, but other than optimization for specific devices, there are little choices when you face low memory warning. The best practice is to have preallocated pools and the game uses from the very beginning all the needed memory. If your game has a maximum of 100 units than have a pool for 100 units and that's it. If 100 units exceeds the mem requirements for one targeted device then you can optimize the unit to use less memory or change the design to a maximum of 90 unit. There should be no case where you can build unlimited things , there should always be a limit. It would be very bad for a sandbox game to use for each instance because you can never predict the mem usage and a crash is a lot worst than a limitation. Also the the game design should always have in mind the lowest targeted devices because if you base your design with "unlimited" things in it then it will be a lot harder to solve the memory problems or change the design later on. 

To simplify the problem i think you want to check if any point ,, is visible from the players's perspective and in this case you don't have to take a reference point you can just do , 'BD' and . You would basically have to do this for each polygon in your scene which is not that efficient in the end (if you have a big scene). The best approach is to use spacial partitioning like a BSP tree (best for your case) but this is not that trivial to implement. If you scene is small or relatively small you can use the approach described above and you can even improve this by adding a 2d Grid to store references to polygons that you want to test. 

There's an error in . Instead of setting , you're setting , which is an input variable. I don't know why that's not being reported as an error because it is certainly not permitted. As for the flashing problem, I have a feeling it might be due to a ping-ponging double-buffer setup. Since you only draw the scene once, every other swap is showing a blank buffer. Try moving your draw calls to the main loop. 

You can use the GJK distance algorithm as well as a simple iterative contact resolver. Basically, given a position and rotation of the body expressed as functions of time t, you want to solve for a value of t that reduces the distance from the point to the edge of the convex hull to exactly zero, and integrate the body using that time value. It's possible to do this analytically, but for a game it's more efficient (and much easier) to do it numerically. With the numerical method, you just incrementally change t until you get a distance value that lies within an acceptable threshold. The simplest way to do this is to switch the direction you change t and also reduce the magnitude of the delta whenever the point switches from interior or exterior, or vice versa. A more complex algorithm (that will also converge in fewer steps) will increment based on the arc length between the point and the body (this is what Bullet does in its CCD solver). Note that this only makes sense when you know the linear and angular velocities of the body, or reduce the movement to either translation or rotation, not both. Otherwise, the problem is vastly underdetermined and will have a huge number of complex solutions. 

So basically you want to know if any line , , , or intersects your polygon The naive way to do this is to define the polygon with a a set of segments you can use line-line intersection to test this: 

This method stores the first value in the integer part (by multiplying by 1000) of the float, and the second value in the fractional part. Using the magic number allows you reliably to store values from up to and for texture coordinates this is more than enough. For a 2028 texture there is almost no precision loss using this method compared to the classic approach. The computing cost if the unpacking is unnoticeable. 

There is not ideal way of doing it, this is a common problem with meshes. To solve this there, are two different approaches: Considering that you have a vertex that has to be used with two different textures coordinates here is how you can approach the problem: 1:Duplicate the vertex and assign to each one the different texture coordinates. As a result you will have two vertices with exactly the same position but two different texture coordinates. How you store the vertex data is not really relevant at this point (interleaved or not). There is no way i know right now (someone correct me if i'm wrong) to be able to represent an indexed vertex buffer in order to share vertex position and different texture coordinates. 2:Separate the geometry in multiple parts and draw it with two (or multiple) draw calls but this has a couple of drawbacks. First need to create extra code to be able to share the vertex position, second you need to make multiple draw calls and this hurts performance especially on mobile devices. I think the first approach is the industry standard because in the end the extra memory used is not much of a problem. One way to overcome the memory usage is to use streaming and it's much easier to use that (not to mention that you can significantly increase the amount of geometry) instead of the second approach in which adds a lot of code complexity.