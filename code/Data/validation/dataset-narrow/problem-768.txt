This handshake should look similarly wrong for any TCP connection to the cluster, not specifically for ssh. (And in fact the same problem should occur for udp). Can yuo confirm (by sniffing) that other (working) services continue the handshake with the virtual address or with one of the member addresses? (Sorry, this should be a comment, but I can't) 

Interestingly, no corresponding error messages occur with deeper subdirectories (e.g. ). What causes this error (or the read attempt in the first place)? And how can I get rid of it? 

If your update is finally selected for installation, you will find additional entries in the windowsupdate.log 

I used this sequence of commands to rename one of our AD controllers from "A" to "B". The rationale was that the historical name "A" would be much better suited for a new server that we want to install. This renaming "A" to "B" went smoothly and in particular, the AD functionality is up and running. However, we managed to install the new server only under a new name "C". It can also readily be renamed to "D" or "E" or ... , but not to "A". I thought it might be the old DNS name still being around. Instead of waiting weeks until it would age out automatically, I deleted it from DNS, but ths did not help. What I get when attempting to rename "C" to "A" is an "internal error", without any notable messages in the event log. What could cause this? 

Is there any way for the users to update the login credentials with the second scenario above? Preferably, is there any way to configure the profile such that connection failure due to login failure makes the username/password dialog pop up, thus allowing the user to store the changed credentials? 

There is no specific entry in Apache error.log, but it seems that the problem occurs because the Auth requirement is evaluated before the Rewrite, thus invokes the ErrorDocument and that is wrongly still http?? 

I just noticed by pure chance that one of my Cisco 4500 switches has its clock going wrong: it is more than 2 minutes behind in spite of seemingly functional ntp. In my opinion, even a single second should not be considered acceptable for the systems involved. Also, I wouldn't have noticed the difference from diagnostics, had I not compared it to a simple wall-clock. Some details Here's ntp information for some of my hosts (10.0.99.1, 10.0.99.2, 10.0.1.119, 10.0.99.241) that are partly referencing one another for fallback, but mainly should all ultimately by syncing with 10.0.0.1, which again pulls the time from outside. So the time discrepancy cannot result from different original time sources. As the observations made me somewhat paranoid, "has correct time" in the following means: (or ) produced an output that matches my wall-clock and my local system clock (which is fine according to $URL$ with an error certainly below 1 seconds (accuracy of me hitting ENTER while watching my local clock) 10.0.1.119 (Ubuntu) has correct time 

Yes, On the file server: In "server manager", "roles", "file services", right click "manage shares and memory", "manage open files". You can do so also remotely if you run server manager elsewhere and right-click connect to a remote server. Normally, hardly anyboday besides an Administrator can do that and should not do so lightly (with great power comes great responsibility). 

The arp table lists for each interface, which MAC is (directly or indirectly) reachable via that interface. That may be a single directly connected device per interface. If you have several switches, you should ignore the "trunk" lines between switches in your counting: Every MAC reachable indirectly via the connected switch wll be listed there. If you even have redundant links between your switches, a specific MAC may be listed only with one of several possible trunk links, depending on which link is currently considered best. Finally, note that a MAC may not be listed if no traffic with that device has passed the switch yet; typically, at least at boot time they will talk something to the net (e.g. DHCP) and thus "tell" their MAC. In summary: For interfaces with directly connected devices, your guess should be fine. Just make sure to drop info from links to wother switches. EDIT: While explaining the arp table in general, I missed the point of your final question: For a router the visibility of a MAC may be less prelevant: Since there will typically be a switch between the router and any other device on the connected LANs, a packet from that device will reach the router only if the device taklks to the router specifically (e.g. in order to reach another net via that router) or vice versa. Thus a router may never learn MACs of devices in the LAN that never bother to talk to any non-LAN host. 

If you build your Zimbra server in a private range behind some NAT router it could be handy to install a tool like DNSMASQ on that server. That way your hostname and dns resolver are always in sync on that server and Zimbra should be happy whatever NAT, proxy, VPN or other real connection you have to the internet. 

Zimbra uses most of the functionality of spamassassin. Only it is wrapped with some management tools from zimbra itself. For Zimbra 8.6+ (maybe older versions also) You can check if your system is configured for updates with: 

Maybe the following commands can help you. It works for the ZWC, but you have to test if it works for relayed smtp mails: Strip X-Mailer header 

And at last you can check for the latest updates in /opt/zimbra/data/spamassassin/state/ and subdirs in there If you want to configure spamassassin you can create a file /opt/zimbra/data/spamassassin/localrules/sauser.cf with your own settings. After looking into spamassassin, you also can try to use some RBL's that can be done in the admin interface in the MTA settings. 

As far as I know there is no way to restore the transactions if those transactions are lost. Redo logfiles are normaly moved to incremental backups, so maybe you can restore them from there. Otherwise, start a full backup and that will give you a 'clean' start for the new redo logfiles. 

If you have the Network Edition, mayby you can use the build in backup and restore funcionality to backup the accounts on the local installs and restore them on the datacenter server. That should restore all data and not just the e-mails. Last time I did that we did lose some settings on the sharing part. Mostly because the internal id's changed. As you are merging from several servers, i'm not sure you are able to fix that. You can however create a script which loops through the mailboxes and displays the sharing info wich you can use to recreate the setup on your new server. An quick example: 

Zimbra uses postfix for delivering the mail to external addresses. So you can use a lot of the commands on $URL$ For example. Log in as the zimbra user and the command to give you a delivery report on a specific destination would be: 

You will need some plugins or use software that is compatible with industry standards as caldav and carddav. I have sucessfully synchronized with Evolution and Thunderbird Lightning with Zimbra using IMAP for e-mail, caldav for the calendar and cardav and ldap for the address books. For thunderbird you'll need some plugins to make it work. The basic key is to know the URL wich is: $URL$ I've bought the Caldav sync app ( $URL$ )for my android phone and still use that because ActiveSync doesn't support multiple calendars. We did move to the Network Edition bacause of the Outlook connector, ActiveSync and the supurior backup implementation. You can see some examples on how to use ActiveSync on your iPhone on $URL$ 

You can make a 'backup' by copying all data from /opt/zimbra. Make sure the data is not in use by shutting down Zimbra completely or create a snapshot with the correct system tools. To use the copy on a new/different server, you need to install exactly the same version of Zimbra on that new server and then overwrite /opt/zimbra with the data from your copy. 

Remember to be compliant to the license when using de Open Source version. A snippet from $URL$ suggests to place 2 banners in the /opt/zimbra/jetty/webapps/zimbra/logos/ folder and change the configuration with: 

The result will be sent to the zimbra user, so you need to have the zimbra@ mail address configured in your system, for example as an alias on your own account. 

Maybe you can use authenticated SMTP on you webserver. That way the mail will be treated as from a local user instead of a remote mail system. 

Open /opt/zimbra/common/conf/main.cf.default file for editing with your favorite editor (e.g. vi). add smtp_bind_address = 192.168.146.197 su - zimbra zmmtactl restart 

One remark when installing Zimbra: Do not install zimbra-dnscache, because you're running your own version now. 

Install the dnsmasq package with your prefered package tool. Edit your /etc/hosts file to include your mailserver and IP address: 

On my Windows 2012R2 RDS server for some users the temp folder (based on sessionid) is not created. The end user has problems with printing. If I start a cmd and do the temp folder with the session ID is created and the user is happy. As I am not able to figure the cause of the problem I tried to create a script in 'C:\ProgramData\Microsoft\Windows\Start Menu\Programs\StartUp' to create the directory but suddenly the %TEMP% variable does not contain the sessionID anymore and my problem is not solved 

I believe this will answer your question: $URL$ In short, windows permissions vary based on whether the target is a folder or a file. The tables in this explain it in detail. The information is dated but for the most part this should still be intact. This is a bit more up to date and applies now but a bit more complicated. $URL$ 

Sure. Create a GPO with a WMI filter scoped to Windows XP, and apply the deny logon interactively/locally rights to DOMAIN\Domain Users, that should prevent them from being able to logon. Although you might get some loud and rowdy responses, be careful of that if you have any concerns of more senior management frowning upon it. However, they cannot actually "hide" this from you. The operating system is included in the computer object and you can perform a powershell query to detect the systems that still report as that version of Windows. As I recall this value should update if they use install a new operating system. A powershell script that seeks out the values of each computer object's attributes called "OperatingSystem" and "OperatingSystemVersion" would tell you what you needed to know. For example on my workstation, these return: OperatingSystem: Windows 8.1 Enterprise OperatingSystemVersion: 6.3 (9600) (Major,minor and build#) Does that help? 

You could install a second NIC and multi-home the server if the software will support it. The risk to your AD domain depends on whether the credentials are used to log in to that server. If so, they are at risk and you probably want to manage it with an account that only has permissions to that box. Yes, someone owning it could be a further risk if you don't notice it, but as long as you're not logging into it with domain admins and the like, it wouldn't necessarily immediately compromise the whole domain. Long run though, it would be best to get the extra network gear and configure a DMZ and separate it from your internal network and open only absolutely required ports back to AD or anything else internal. 

If I'm reading you right, then I guess it depends on how you're assigning IPs from your VPN and whether or not you have the capability of setting the DNS Server address, guessing you would but not every implementation is the same. You'd want to configure the zones that you want to be answered internally on that DNS server and leave recursion intact (although this is generally a bad idea if that DNS server is exposed to the public, as it opens you to potential denial of service attacks, and you might need to be more complex if so). This way your DNS server will answer what it can first and if it has no zone for the query it will forward to the internet roots by default. 

Your best solution to this problem is absolutely a conditional forwarder, although as someone pointed out above a delegation would probably work as well if your DC can route to it. Of course, if it can't you'd have bigger problems. Otherwise, just typical split-brain DNS scenario where you're answering most clients for what you have, but anything specifically to www.contoso.com you can forward to this other server. With the contoso.com it gets a wee bit hairier for kind for the same reason the other commenter mentioned. Your AD domain is using this. If your domain were instead a subdomain such as 'ad.contoso.com' you could also simply use a CF here. That, of course, assumes your dept1/dept examples are correct and not like dept1.ad.contoso.com If your servers are at least 2012 R2 I believe there is a new functionality for this: $URL$ Hope this helps. 

Pardon, because I'm not super experienced with various Linux distros, but from the Active Directory side, this sounds like you may have a problem with your RID Master. If the pool is depleted and not being replenished, it may be that the other domain controllers cannot communicate with it for some reason. What happens under the hood is the RID master allocates a pool of (i think) 500 RIDs at a time for new objects, and when that gets to less than half a new request is made and a new block of RIDs is assigned. I would recommend trying a full dcdiag and looking through it for any errors (or asking your AD admin to do so) as a starting measure. 

Is the Windows 10 system part of a domain? When you load this module by default, it attempts to look at your service records, find an efficiently located domain controller and map it to the PSDrive "AD:\" so that you can navigate it within powershell and review records. It is possible to disable this: $URL$