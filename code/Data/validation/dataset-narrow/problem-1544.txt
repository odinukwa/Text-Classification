I don't think any of the clustering techniques "just" works at such scale. The most acalable supposedly are k-means (just do not use Spark/Mahout, they are really bad) and DBSCAN (there are some good distributed versions available). But you will be facing many other challenges besides scale, because clustering is difficult. It's not as if it's just enough to run the algorithm and then you have clusters. Clustering is an explorative technique. There is no "correct" clustering. But rather you will need to run clustering again and again, and look at every cluster. Because there will not be a single parameter setting that gets everything right. Instead, different clusters may appear only at different parameters. But the main challenge in your case qill likely be the distance function. Except for idealized settings like MNIST, Euclidean distance will not work at all. Nor will anything working on the raw pixels. So first you need to do feature extraction, then define a similarity function. When it comes to clustering, work with a sample. Cluster the sample, identify interesting clusters, then think of a way to generalize the label to your entire data set. For example by classification (your labeled data points are your training set, predict the labels of unlabeled points). 

The estimation quality of the mean improves with sqrt(n). So with more and more data, your mean will get more precise. But the improvements get slower and slower. It won't help with all the other issues of k-means, such as problems with different cluster diameters and outliers. And more data probably means more outliers... You can try a similar approach to that paper with k-means with some slight modifications. Try plotting the sum of squared deviations between the holdout set, and the nearest centroid each. As long as this improves, the clusters fit your holdout set better. 

You can do DBSCAN, OPTICS, HDBSCAN with cosine similarity - they do not expect or require Euclidean distances. Yes, one version of cosine distance corresponds nicely to Euclidean distance of L2 normalized vectors. So there is some theoretical support to L2 normalize the vectors and try Euclidean then. Given how word2vec is trained, I don't think Euclidean nor cosine distance is the right thing to do. Instead, use the raw dot product as a similarity measure (beware that it is not limited to [0;1] but allows negative similarity - values may be too spread out to use them intuitively). t-SNE, which was discussed in the comments and another answer, also relies on distances to the nearest neighbors. So it does suffer from the same problem - cosine, Euclidean, dot product? The fast ELKI implementation does support cosine distance as well as Euclidean, but you appear to be set on Python already. Also, if you have a large vocabulary, t-SNE may give you some scalability trouble. The standard algorithm will need O(nÂ²) memory and time. 

In the end, having parameters is a feature, not a limitation. Cluster analysis is not something to fully automate. It is an explorative method. You try it, change parameters, try it again, change parameters again, ... until you have learned something about your data. Any method that does not allow you to reiterate is badly designed. 

Never apply k-means to columns Even when they are numerical: the mean does not make sense. Instead, you need to aggregate and pivot your data appropriately. One row per entity that you want to cluster (e.g. stores, or employees?). Multiple columns. No except the row number. Don't forget to normalize / preprocess your data. E.g. one store may have a much better location. If you cluster your employees, expect to find a cluster that is simply a store. (Which is a correct, albeit useless, clustering). 

This can be interpreted as an unusual error correcting code with a non-binary signal. Essentially, you want the tuples to be very different, i.e. there are no two tuples that agree in two positions. That is also a simple strategy for producing such codes (the more interesting question is what (x,y) combination is best.) Think of it this way: assuming you only know part of the tuple. Say the first is unknown: (?, 2, 4) then you want to be able still conclude the correct triple. $URL$ 

k-means is based on averages. It models clusters using means, and thus the improvement by adding more data is marginal. The error of the average estimation reduces with 1/sqrt(n); so adding more data pays off less and less... Strategies for such large data always revolve around sampling: If you want sublinear runtime, you have to do sampling! In fact, Mini-Batch-Kmeans etc. do exactly this: repeatedly sample from the data set. However, sampling (in particular unbiased sampling) isn't exactly free either... usually, you will have to read your data linearly to sample, because you don't get random access to individual records. I'd go with MacQueen's algorithm. It's online; by default it does a single pass over your data (although it is popular to iterate this). It's not easy to distribute, but I guess you can afford to linearly read your data say 10 times from a SSD? 

For finding similar objects, k-means does not make much sense. k-means is based on similar objects. If squared deviations don't work for your data then k-means will not work either. Thus, instead of using k-means, just use a nearest neighbor search without k-means. 

There is no "correct" solution in clustering. It's about discovering some new pattern and you cannot out the "new" into an optimization equation. The point is to try, and try again. It is not a drop-in replacement for classification. 

Topics are clusters There is next to no difference between subspace clustering and topic modeling, except maybe that text is sparse and integer while subspace clusterers usually assume dense and continuous data. So rather than trying to cluster again, just use your topics. Yes, documents can belong to multiple topics. That is because text usually is this way, and forcing everything to have a unique label reduces the quality, because it cannot reflect reality anymore. If you insist every document should have a unique cluster, just use argmax(topic weights). 

K-means clusters are always convex. Even when not converged. This does not hold here, and therefore that slide is not depicting k-means correctly. It's probably a bad set of slides. There are many many really bad slides on clustering. 

State of the art as in: used in practise or worked on in theory? APRIORI is used everywhere, except in developing new frequent itemset algorithms. It's easy to implement, and easy to reuse in very different domains. You'll find hundreds of APRIORI implementations of varying quality. And it's easy to get APRIORI wrong, actually. FPgrowth is much harder to implement, but also much more interesting. So from an academic point of view, everybody tries to improve FPgrowth - getting work based on APRIORI accepted will be very hard by now. If you have a good implementation, every algorithm has it's good and it's bad situations in my opinion. A good APRIORI implementation will only need to scan the database k times to find all frequent itemsets of length k. In particular if your data fits into main memory this is cheap. What can kill APRIORI is too many frequent 2-itemsets (in particular when you don't use a Trie and similar acceleration techniques etc.). It works best on large data with a low number of frequent itemsets. Eclat works on columns; but it needs to read each column much more often. There is some work on diffsets to reduce this work. If your data does not fit into main memory, Eclat suffers probably more than Apriori. By going depth first, it will also be able to return a first interesting result much earlier than Apriori, and you can use these results to adjust parameters; so you need less iterations to find good parameters. But by design, it cannot exploit pruning as neatly as Apriori did. FPGrowth compresses the data set into the tree. This works best when you have lots of duplicate records. You could probably reap of quite some gains for Apriori and Eclat too if you can presort your data and merge duplicates into weighted vectors. FPGrowth does this at an extreme level. The drawback is that the implementation is much harder; and once this tree does not fit into memory anymore it gets a mess to implement. As for performance results and benchmarks - don't trust them. There are so many things to implement incorrectly. Try 10 different implementations, and you get 10 very different performance results. In particular for APRIORI, I have the impression that most implementations are broken in the sense of missing some of the main contributions of APRIORI... and of those that have these parts right, the quality of optimizations varies a lot. There are actually even papers on how to implement these algorithms efficiently: