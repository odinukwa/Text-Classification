The relation $pu^{k+1}q\in L\Rightarrow pu^k vu^kq\in L$ is true for any $p,q$, $k$ big enough and $v$ with $alph(v)\subseteq alph(u)$. Let call $a$ the first letter of $u$ and $b$ the last letter. Let $v$ with $alph(v)\subseteq alph(u)$, and $v'=uvu$. Then by using the relation above with $v'$, we get that for any $p,q$ and $k$ big enough, we have $pu^{k}q\in L\Rightarrow pu^k vu^kq\in L$, which is the definition of $u^kvu^k\leq_L u^k$. In particular, for $n$ big enough, we get $u^{n!}vu^{n!}\leq_L u^{n!}$, which is the wanted equation. 

The answer to the first question is that we don't know, because we don't know whether $\mathbf P=\mathbf L$, so it could be that all $\mathbf P$ problems use only logarithmic space. For the second one, most reasonable problems need at least linear time to read the input, even for non-deterministic machines, so it does not make a lot of sense to say "polynomial non-deterministic lower bound", or you need to make your question more precise. Finally, the last question is like the first one: it could be that $\mathbf{NL}=\mathbf{NP}$, in which case such a problem wouldn't exist, we don't know... 

The Black Hole Information Paradox seems relevant to me, as it concerns information theory, which can be seen as close to computer science. To sum up, the paradox in question is that when objects or in general waves carrying information are swallowed by a black hole, the information they carry seems to be destroyed. This violates principles in quantum physics, roughly saying that information must be preserved. Hawking showed that black holes actually emit radiations, and therefore slowly evaporate. This hints towards different ways of solving the paradox, listed in the wikipedia page. 

There is a very nice metaphor on decidable languages in the book Gödel, Escher, Bach by Douglas Hofstadter. He uses drawings as an analogy for recursively enumerable languages. Every time you draw something on a sheet of paper, the part of the sheet that is not used by your drawing can be viewed as the "complement" of your drawing. The artist M.C. Escher plays with such complements in pictures like this one: We can view this as a picture of birds, whose complement is a picture of fishes. The behaviour is the same with decidable languages: both the language and its complement are recursively enumerable. To sum up, it allows to graphically view an object that "makes sense" (in this case it stands for recursively enumerable), such that the "hole" it leaves in the universe also "makes sense". 

Answer to questions 1 and 2: No. This is also the answer to 3 if $M$ is required to always halt (isn't this implied by the definition of a learner, or do you mean the program produced by $M$ must always halt?). Here's a proof for 1 (2 and 3 work pretty much the same way) Suppose there's an algorithm $A$ which takes the description of a learner $N$ and outputs a learner $M$ which behaves differently. Let $N$ be a learner which runs $A$ on its own description, then runs the output $A(N) = M$ on the input text to produce a program that enumerates some set. By definition of $A$, $N$ and $M$ behave differently, but by construction they behave identically, contradicting the existence of $A$. This same argument can be pretty easily modified to show that 2 is impossible. It can also be used to show that 3 is impossible if $M$ must always halt. Not a real answer for question 4, but you might find this helpful: A prudent learner cannot converge on both every text for a finite set and on every text for the set of all natural numbers. Let $N$ be a learner that converges on every text for a finite set. Now, consider the following text for the set of all natural numbers. It contains all numbers in order, separated by differing numbers of 1s. After $i$, it contains enough 1s that $N$ will return a program which enumerates the set $1,\ldots,i$. $N$ will do this because it converges on every finite set. Because it will return a program enumerating a finite set after every prefix of this text, it does not converge on this text. So if you let $M_1$ be the learner which converges on all finite sets and $M_2$ be the learner which converges on all texts for the set of all natural numbers, at least one of $M_1,M_2$ satisfies the criteria in question 4. You can extend this to a countably infinite set of learners in which at least all but one learner satisfies the criteria in question 4 by producing a machine $M_i$ for each natural number $i$ which converges on all finite sets which don't contain powers of the $i$th prime and also converges on the set of all powers of the $i$th prime. By similar logic as above, $N$ can't converge on all sets that any two of these converge on. You can use this set of learners to find an $M$ satisfying 4 with arbitrarily high probability. 

It seems strange to me that almost all the answers are about computational complexity, while the question asks for problems in all computer science. To counter-balance a little bit: Decidability of the dot-depth hierarchy: Given a first-order formula on finite words and an integer $k$, is there an equivalent first-order formula with only $k$ quantifier alternations? Recent progress has been made, it has been showed decidable for $k=2$ in a 2014 paper by Thomas Place and Marc Zeitoun, but the general problem is still wide open. 

It would be great, not only for Maths and TCS, but also for natural sciences. There the problem is even bigger: by publishing only positive and "sexy" results and ignoring negative and boring ones, we can end up with a very wrong idea of what actually happens. This is because statistical noise can be presented as relevant, when only results above a certain threshold of "interest" are published. For instance if 5 studies try to answer the same question and only one finds a surprising result, it might be the only one getting published. In Maths and TCS the picture is different, as the main interest is to help the community by putting a "dead-end" sign at the beginning of some roads, and save time for people who were going to explore them. Collecting counter-examples is another very interesting aspect of the project. Bottomline I'm all for it, and would support initiatives like the one usul is proposing. 

The solution of the restricted star-height problem inspired the rich theory of regular cost functions (by Colcombet), which in turn helped to solve other decidability problems and offers new tools to attack open problems. This theory is still developing and was extended to infinite words, finite trees, infinite trees, with its own set of deep results and open problems. Here is a seminal paper of the theory, and a bibliography, from Colcombet's website. So while it is not directly an application of generalized star-height, it shows that progressing on seemingly useless problems such as star-height is likely to mean better understanding of regular languages, and yield new results on different problems. Reference : Thomas Colcombet. “The theory of stabilisation monoids and regular cost functions”. In: ICALP 2009 

Your construction for bad prefixes is not correct on NBA's. For instance take the NBA on alphabet $A=\{a,b\}$ with two initial states $q_a$ and $q_b$ where for both $x\in A$, $q_x$ goes to an accepting sink if the first letter is $x$ and to a rejecting sink if the first letter is not $x$. Then the language recognized is $A^\omega$, but the set of "bad prefixes" you compute is $A^*$ i.e. all finite words. Your construction works if you first determinize the automaton. Then the new final states are the ones which cannot reach a component $C$ accepting some infinite word. Careful though, the deterministic automaton is no longer Büchi, it is a parity automaton, so it is a little more complicated to check if a component has empty language. As for your question about closure, you are essentially asking if for any regular language $L$, $Closure(L)$ is the set of words containing no bad prefixes of $L$. This is true: a word is in the closure of $L$ iff it is arbitrarily close to words in $L$ iff it does not contain a bad prefix. 

Let $k$ be the finite number of agents you're allowed to choose. If there are no critical services, finding the set of $k$ agents that maximize the number of non-critical services provided is equivalent to Max-$k$-Cover, which can't be approximated better than $1-1/e$ unless P=NP. So for an exact solution, you'll have to use something capable of solving NP-hard problems. If you'd be happy with an approximation, you're in luck. The constraint that you have to choose $k$ agents such that there are sufficiently many of them providing each service is a matroid constraint. This paper shows how to achieve a $1-1/e$ approximation on set cover problems under a matroid constraint. If you want something simpler to implement, the matroid greedy algorithm gives a $1/2$ approximation on submodular set functions. So you just need to repeatedly choose the agent that provides the most new non-critical services, after at each step removing from consideration any agent whose choice would prevent you from providing enough of each critical service before reaching $k$ agents. If each agent provides unique non-critical services, you're just maximizing a weight function over a matroid, so the greedy algorithm gives an exact solution. 

If you understand the algorithm for 2SAT, you already know why it's in P - this is precisely what the algorithm demonstrates. I think this comic illustrates my point. As you already know why 2SAT is in P, what you probably want to know is why 2SAT isn't NP-hard. To understand why 2SAT isn't NP-hard, you have to consider how easy it is to reduce other problems in NP to it. To get an intuitive understanding of this, look at how SAT can be reduced to 3SAT and try to apply the same techniques to reduce SAT to 2SAT. 2SAT is just not as expressive as 3SAT and other SAT variants. 

Your solution can be modified to do everything in $O(1)$ amortized time. Instead of maintaining a balanced tree, just keep track of the number of successful enqueue operations (those in which something was actually added to the back of the queue) and attach that number to queue along with the item during a successful enqueue operation. To find the depth of a key, just take the difference between the number of successful enqueues when it was enqueued, and the number of successful enqueues when the item currently at the front of the queue was enqueued. Note that this gives exact depth. If you're worried about storing excessively large values after many enqueue and dequeue operations, you can also keep track of the size of the queue and whenever the size is less than half the number of enqueues, re-number the number of enqueues for each entry to 1 through the size, and reset the number of enqueues to the size. Everything takes constant amortized time and each key takes at most one more bit than necessary to represent its depth at insertion. 

The VPA $A$ is forced to do pop on factors of the form $C_i^R$. $A$ can non deterministically guess a violation of either property, and verify it. The key is that it can either push on $C_i$, or do nothing, which allows to verify all conditions (actually guess their violations). In particular, it can guess that the first (or second) occurence of $C_i$ does not match $(\overline{C_i})^R$, by ignoring the other component. It can also guess that $C_i\to C_{i+1}$ is not a valid transition, by pushing both occurences of $C_i$, then popping one, push no $C_{i+1}$, and compare $(\overline{C_{i+1}})^R$ to the stack content. For other $C_j$ that are not part of the guessing, one component is pushed and the $(\overline{C_j})^R$ is popped. Pushing words As for the variants where words are pushed, it seems that the determinizability proof in the original paper on VPAs can be adapted to this setting. It suffices to adapt the construction so that stack symbols are of the form $(S,R,u)$ where $u\in A^*$ is a prefix of a word that can be pushed according to the transition function. When popping a letter $a$, $(S,R,va)$ is turned to $(S',R',v)$, where $S'$ and $R'$ are updated normally to reflect the current powerset construction status. However, this time we a priori get a deterministic pushdown automaton that is not visibly pushdown. At least this means that equivalence and universality are decidable. 

The theory of regular languages of infinite trees gave rise to several hierarchies, that are currently studied, with many questions that are still open. When using automata on infinite trees, the parity condition (or Mostowski condition) is of special interest, because non-deterministic parity automata can express all regular languages of ininite trees, and the structure of the acceptance condition is simpler than others like Rabin or Müller. Every parity automaton has a rank $[i,j]$ where $i\in\{0,1\}$ and $i\leq j$, describing the structure of the acceptance condition. Therefore, if a language $L$ is recognizable by a (det/ND/alt) automaton of rank $[i,j]$ we say that $L$ belongs to the $[i,j]$-level of the (respectively): 

Here is a natural candidate: the alphabet is $\{a,b,c\}$, and to a word $v=u_1cu_2c\dots cu_k$ with each $u_i\in\{a,b\}^*$, you want to associate $$f(v)=\sum_{i=1}^k \min(|u_i|_a,|u_i|_b)$$ There is a straigthforward exponentially ambiguous automaton for this $f$: each time you see a $c$, guess if you go to a state counting $a$'s or to a state counting $b$'s. It should not be too hard to prove that only exponentially ambigous automata recognize this function $f$. As a very sketchy argument, consider the fact that between two $c$'s, you must either count the number of $a$ or the number of $b$, so you need two different states to do so. Since you must be able to oscillate between these states, it means that you have distinct paths of the form $p\stackrel{w}{\longrightarrow} p$ for some state $p$ and word $w$ (containing some $c$'s).