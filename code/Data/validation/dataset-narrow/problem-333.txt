I would guess that you are not backing up the transaction log. This is not the same thing as backing up the database. I would suggest you back it up at least daily. In a high transaction environment, it should be backed up more frequently, ours is backed up every 15 minutes. 

And a good bit of slow performance in databases is due to bad indexing or badly performing queries. No amount of tweaking the server itself will fix those. Findout what they are doing when it gets slow. Don't just randomly fix things and hope that things improve. Know what is causing the issue and measure before and after to make sure you have improved. 

Bill gave an excellent answer. I would add that I would login to the user interface as a test user and try to understand exactly what the users do with the data. It will help you understand the why behind some of the stored procs or design. Understanding what the data means and is used for is critical to understanding a a database. If the database is on a business function or subject you are in general unfamiliar with (say it does flight planning and you have previously only worked on financial applications), then ask the users for some reading material on the subject matter or go to the library yourself or search the Internet about the subject matter. Ask the users if there are legal or regulatory issues you need to be aware of. Again some of this subject matter background may explain what seem to be odd design choices. 

Something else that can cause ridiculous growth is failing to backup the transaction log frequently. It will grow until it uses up all your space if you don't back it up. This is separate from the database backup. Check out the size of your transaction logs too. 

Usually we just delete the record that no longer needs to exist. You are deleting both records and reinserting the keeper record. 

I would prefer the first version because you probably only rarely need to see the history but you will frequently need to see the current value. A history table should be populated from a trigger, so you don't need to worry about the data getting out of synch generally. So suppose you have a million records in MyObject and then you have 10,000,000 record in MyObjectHistory. Do you really want to join to a table with that many records to get the current value? Now if you are going to need to query the history as freqently or more frequenlty than the current value, then the second struture would work. (And if you are going to be displaying the value as of a particular date, I would have a begindate and enddate field in it to make querying simpler.) BTW I would add a date field to the history table to be able to tell what order the changes happened. You can't rely on identities for temporal order. PLus if there is a question about a previosu value and when it changed, you will need to knwo. I might also put in values for the application the change came from (if you have multiple applications) and/or the person who made the change. 

Do you really really need to match the id from the table you are merging after the load? Or do you only need it to get child records in and properly related? If it is only used to get child tbales properly related, then this a process we have used on occasion: Stage all the data in work tables. Add a column for the Id from the other table to the table you are loading to. Let the process create new records with identities for the parent table. Update the staged child table to use the new id by joining on the column you added. Have the process load the child tables. 

The main reason I support surrogate keys is that natural keys are often subject to change and that means all related tables must be updated which can put quite a load on the server. Further in the 30 years I have been using a variety of databases on many topics, the true natural key is often fairly rare. Things are supposedly unique (SSN) are not, things that are unique at a particular time can become non-unique later and some things like emails addresses and phone numbers may be unique, but they can be re-used for different people at a later date. Of course some things simply don't have a good unique identifier like names of people and corporations. As to avoiding joins by using a natural key. Yes that can speed up the select statements that don't need the joins, but it will cause the places where you still need the joins to be slower as int joins are generally faster. It will also probably slow down inserts and deletes and will cause performance problems on updates when the key changes. Complex queries (which are slower anyway) will be even slower. So simple queries are faster but reporting and complex queries and many actions against the database can be slower. It is a balancing act, that may tip one way or the other depending on how your database is queried. So there is not a one-size fits all answer. It depends on your database and how it will be queried and what type of information is stored in it. You may need to do some testing to find out what works best in your own environment. 

I worked on an application many years ago that produced complex sales quotes for RFPs. It can get fairly complicated. One of the biggest things I see missing from your tables is a way to do discounts. Another is a way to separate the quote into parts, for instance you might havea quote for a house renovation that would include a kitchen quote and a bathroom quote, etc. That way the customer can see each chunk and make independent decisions based on the parts. You may not need this, but it is something to consider. YOu may also need to consider ifyou need to quote labor as well as parts/ So table structure (this is just a starting place) 

The answer depends on what you intend to do with the data. Are you going to be doing frequent queries against it or do you only need it for the occasional lookup of what happened when, to rollback bad changes, or for regulatory reasons? In the first case above, use the second process you descibed although I would add a column to denote the active record. I would also add a record created date. I wouild also create a view of just the active records and use that for allof my code that needs to see the current data. In the second case above, I would have audit tables that are populated through triggers (the offective way to create audit records for any change to the db) and that also include the data of the change and the user or application that made the change as well as the old and new values. 

In SQL Server 2000, you cannot use the results of a stored proc directly in another query. However, you can insert them into a #temp table and then join to that. But you must create the temp table structure first, you can't use select into. 

First your problem is that you are using Access which is not going to perform well with the struture you need. Your problem is most commonly solved using what is called an EAV table. This is because you will probaly be adding more and more exam typoes weach of which will have differnt parameters. I would structure it so that I had all the common details about the patient in a patient table (and some related tables for things that change over time or that have mulitple values such as address). I would then Have an exam table with dates and a study table with the current studies. Exams patient and studies would all relate through join tables (patientid and studyid inthe patient study table and examid and studyid in examStudy table and patientid and examid in the patient Exam table). This is becasue you ahvea many to many relationshsip between these things. Now as to the exam details this is where the EAV table comes in. It would include Examid, parameterName, parameter value) You would then inseter a record for each value you want to store. That way you can add and change them as the exams change. They are harder to query this way but this is exactly the use case that EAV tables were designed to meet, frequently changing values that can't be knwon in the original design. Access may not handle this well, so you might want to consider a nosql database for this part. And of course you need to protect patient information according to HIPAA rules. Be very sure you are aware of them and how to prtect the data. Again Access may not be the best choice for that unless you don't intend to store client names and addresses etc at all. 

Well we tend not to care what the originators table structure is, but only if it meets our requirements (which we send to them). If you are trying to figure out how to design a way to store the data permanently because you don't currently have a structure, then this is the method I use. Import the file into a staging table (not the final permanent table, I highly recommend you do that anyway, so you can clean the data before moving it to its final location) that has everything defined as varchar(max) or nvarchar(max). Now you can examine each field and see what is in in, look for the max lentgth of each field, check to see if numeric fields contain only numbers etc. Then you will know from the data what types of fields you need in the production table. Since this is only the first file, I would tend to create my final table with slightly larger fields than the data indicates, so it doesn't fail on the second file you get. After you have some history, you can tighten them. If you use SSIS instead of bcp, there is a data profiling task you can use to see what the data is really like. 

Well first there is no way that all of this should ever be in one table. You need related tables for information such as Assignee and Inventor. Please read about normalization. But in reality are you sure that mysql is the way to go for this since you seem mostly to want to search on data that is not always in the same form. Personally, for this type of thing I would look at NoSQL databases for the text parts in conjuction with a relational database for the data that is easy to describe and determine the size of. 

And of cousre you would want associated tables for the customer, Customer address, partslookup, labor tasks lookup, etc. You might also want to provide a place on your form to add a new part or labor task to the appropriate lookup table. Sometimes you may need to quote something that is not already in the system and the best place to do that is while you are creating the quote. 

Knowing what has been called recently only helps for frequently called things and many objects in a complex database are not called that often but are still needed. I know no simple way to identify what isn't being used. What I would do is start up Profiler on my dev or qa box and then take every application that hits it and run through the functionality. (if you have a formal QA, a good set of regression tests would help this). I would set up my trace to write to a table. Now at least you know what procs the applications call and can eliminate them from the list. Make sure every job on the prod server has an equivalent job on your test server and run them. That should find some more. By now your list of potential sps is much smaller. Your list of active tables should include only those mentioned in one of the procs and tables you know you need like audit tables. YoOu can create a list of potentials for eliminating from there. Now once you have the list of potentials to eliminate, you will probaly see some fairly obvious ones like usp_my_proc_Old (when you have a USP_My_proc in the db). Those are my first candidates to eliminate. Tables with no data are another set of obvious ones at this point. Tables/procs that clearly refer to a functionality you know has been eliminated would be the next ones. Suppose you recently replaced the functionality for storing survey results with a new design. You may want to keep the table (You may need the data) but the procs that call that table are probably all out of date and can go. Depending on your legal constraints, you may not want to eliminate any table with data. We have client specific data for clients we no longer have, becasue we are in a regulated industry and are occasionally asked to provide data to auditors and regulators and lawyers. However, you can move these tables to another archive db if you want to clean out your actual production database. Then start looking at what they do. You can eliminate any proc that will not run especially if one of the tables it references no longer exists. If a table has a datefield, are there any recent dates? If the last time the data field was populated witha date was 2008, that is a good candidates for a table we don't need anymore. Once you have a list of several potential objects to delete, then send the list around to all of your developers and ask them if they use the table/proc or know what it was for. Do not do this with a huge list of 1000s of objects. Send out no more than 10-20 at a time and try to group them so they are clearly on related topics. For potentials to eliminate, you can add a logging process to the proc or a logging trigger to the table and set a date when the object will be eliminated if there are no entries by that date.