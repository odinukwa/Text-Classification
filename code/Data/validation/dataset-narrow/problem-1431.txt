So there is a reason for strive for higher quality sound than can be appreciated through the tiny speakers on many mobile devices. Stereo sound would be important for any listed situations and the lower on the list you go the higher quality the sound needs to be for optimal appreciation. To address #3: You can check to see if they are the same. Split the channels, invert one and subtract it from the other. If there's nothing left afterward, then you know they were the same. See here for steps on how to do that. 

Generate the requirements for your game. That includes technology, features, time requirements, etc. Self assess your own skills and availability. Coding, artistic, time available, time/desire to learn new tech, etc. Compare the above to the capabilities/limitations of your different options. Make a choice. 

For completeness, there are plenty of situations that knight666's answer misses. For example testing just the center of the triangle would miss this case: 

This would be the same as implementing a boids algorithm, and using Box2D to apply the forces required: 

One good option is to use something larger than pixels and use marching squares to create smooth terrain. The server and client only talk to each other in grid terms, thus reducing bandwidth. The client is responsible for running marching squares and interpolating the grid into a smooth terrain. However, all the collisions and terrain interactions use that grid for their calculations, making them faster. Additionally, this allows you to easily test the performance of one size square and easily scale the squares up or down depending on where you find your performance. For example, create a grid that's 5X pixel size. Test the performance, if it's good, scale the grid down to 3X pixel size for smoother terrain and more accurate collisions. As an example, there's a Ludum Dare game from a while back that used marching squares for destructible terrain: 

The majority of your code is not going to involve the Facebook API. It will be more work to integrate after code completion, but it's certainly possible to integrate it after the game has been written. Make sure you learn about any restrictions about Facebook apps that may impact the functionality of your game. So you can code around those. Ultimately this is your decision. You need to weigh the possibility of completion against the cost of an SSL certificate. You should also consider the income that can potentially be generated from such a game, to see if you'd ever make your money back. Perhaps seek outside funding to get the SSL certificate. 

To position SpriteB to be aligned with SpriteA you just need to do a little math. Getting the bounds will return a rectangle. From that rectangle, you can get the width and height of the bounding box. Combine that with the position and you can get the location of the points you're interested in. 

Thanks for the answers on this, they are very helpful. I'll add an answer to my own question because I've moved on a bit from there. In the end I realized that perhaps wanting to entirely separate the shaders from the drawable items was a mistake. They are fundamentally coupled in "real life" so it's perhaps not an issue that they are in the design. For example a water shader will only work with an element that wants to draw water. Only that will be able to provide the data it needs. A shader that draws animated models will only work with an animated model element. I can't suddenly decide to use a water shader with an animated model. It simply can't work. Trying to make an abstract shader class that can load any shader and work with any drawable element is a mistake. It doesn't reflect how they can be used in reality. So instead I've made shader classes AnimatedModelShader, WaterShader, LandscapeShader. Each of these might well have options that cause it to load different physical shader files but they will always have the same basic interface by necessity because a landscape shader will always need the same kind of data input. So the shader is now responsible for creating it's own input layout and it tells the element using it how to layout it's vertices by having a public typedef called vertexType that the element uses if it wishes to use that shader. Not at all my original design but as it turns out, the desire to separate the two concepts wasn't very useful anyway. 

I'm having trouble understanding some of the math behind normal map textures even though I've got it to work using borrowed code, I want to understand it. I have a terrain based on a heightmap. I'm generating a mesh of triangles at load time and rendering that mesh. Now for each vertex I need to calculate a normal, a tangent, and a bitangent. My understanding is as follows, have I got this right? normal is a unit vector facing outwards from the surface of the triangle. For a vertex I take the average of the normals of the triangles using that vertex. tangent is a unit vector in the direction of the 'u' coordinates of the texture map. As my texture u,v coordinates follow the x and y coordinates of the terrain, then my understanding is that this vector is simply the vector along the surface in the x direction. So should be able to calculate this as simply the difference between vertices in the x direction to get a vector, (and normalize it). bitangent is a unit vector in the direction of the 'v' coordinates of the texture map. As my texture u,v coordinates follow the x and y coordinates of the terrain, then my understanding is that this vector is simply the vector along the surface in the y direction. So should be able to calculate this as simply the difference between vertices in the y direction to get a vector, (and normalize it). However the code I have borrowed seems much more complicated than this and takes into account the actual values of u, and v at each vertex which I don't understand the need for as they increase in exactly the same direction as x, and y. I implemented what I thought from above, and it simply doesn't work, the normals are clearly not working for lighting. Have I misunderstood something? Or can someone explain to me the physical meaning of the tangent and bitangent vectors when applied to a mesh generated from a hightmap like this, when u and v texture coordinates map along the x and y directions. Thanks for any help understanding this. 

Kerbal certainly didn't go for the realism you're seeking. For a game, it's much easier to just fake it. While the ships designed in Kerbal are configurable, the parts are predefined in a very detailed way. Just look at what goes into making a part. That's a pretty big hint. All the parts have their drag defined. So, it's really just a matter of summing up the drag of the parts defined, factoring in their orientation, then using that value in your drag equation. You can do some raycasting from the heading of your craft to see what parts are "visible" to that velocity. Include those parts in your sum of drag, factor in the atmosphere density and speed. Improving the simulation is to not sum the force of drag and apply to the whole craft, but instead apply it to the individual parts producing the drag and let your connection system figure out how to distribute the forces. Further improvement means adding elements like heat, and shearing forces for damage. Finally you can add features such as lift to your model. This is really just a matter of adding perpendicular forces based on the forces that part is already experiencing. If you want things to get hairy at higher speeds, add some noise to your lift forces to simulate turbulence. 

Just go really simple. Add a flag to the player that tells the camera when they're jumping. If they're jumping, don't follow them up. The other situation you need to handle is when the player jumps up or down to different levels. In this case it would be pretty simple to start tracking again when the player touches down, or if the player goes below the starting position of the jump, the camera will start following again. For keeping the camera in bounds, you have a few different options. 

If I were in this situation, I would create each part of the boss as a separate entity. These "sub-entities" would include some kind of or component. This component would include a reference to the parent entity and an offset from the parents position. When updating the position, they check the parent position and apply the offset to generate their own position. Additionally, it could make checks to ensure the parent entity still exists. Further, you can have a component that tracks the existence of sub entities for the parent entity. This allow you to do things like only make the core of the boss vulnerable when the arms with shields are destroyed. I currently use a component in my game, which is used for turret tracking and when goblins are going to pick up a resource. It can check the target entity's position and change it's behavior accordingly. Entities that have no position are never added as a target, so there's no worries there. However, when getting to be more in depth, like checking the parent or child entity health, shield, power reserves or whatever, you'll have to ensure the parent or child entity actually has the related component. Making each part it's own entity maintains the flexibility of the entity/component framework by allowing you to add additional and different components to each part of the boss. For example, one part of the boss might have a gun component and a health component while another would have a shield component and a health component. I found another discussion on this topic here. In which the users are discussing adding multiple components of the same type to one entity (which seems like a bad idea to me). It seems like a useful conversation, though I haven't read the whole discussion. 

Casting requires mental discipline but also requires large amounts of energy to flow through the body. Holding the body in the right position and directing the flow with precision requires physical strength to hold the arms and the head and the fingers just "so" while huge energies are flowing through them. If your muscles are not strong you have to take it very slowly and carefully, the more physical strength you have, the more you can rely on your body to hold the necessary position while you channel the energies. Which is to say that strength affects casting speed and channeling rate. Maybe 

I'm doing a small direct3d11 project for "fun". I know d3d11 quite well but not really any opengl. My program is designed for d3d11 but I would like it to be easy to port to opengl at some point so I'm trying to keep to abstractions that are somewhat portable. The concepts of vertex buffer, index buffers, vertex shaders, pixel shaders, and some form of input layout structures all seem to map pretty well. But what about constant buffers? all of the examples I see of opengl shaders have code getting "pointers" to named variables in the shader and setting them whereas my code for d3d11 just creates a big c++ struct and copies it into a constant buffer. This then maps directly to a cbuffer in the shader. Does the same concept exist in opengl? I've been unable to find it with any quick searches. Or do I have to set named variables only? 

But never actually use the data in the shader, does that incur any runtime cost at all? It's convenient for me to do this but not at the cost of slowing runtime performance at all. When I time it I don't see any difference... 

(Originally asked on stackoverflow) Using visual c++ express 2010 for direct3d you have to download the directX sdk and there is a tool called pix for debugging shaders, looking at 3d resources etc. With visual studio 2012 express the directx sdk is included in the windows sdk that comes with it but this does not seem to include the winpix.exe tool. Is this very useful tool still available? I guess I can still use the one from the previous sdk but it seems wrong to install the entire sdk just for that tool. Is there a version for VS2012 express that I'm missing? 

So you write you main rendering loop and in there you call something like Game::getSetting(ScreenWidth); That all works well and is nice and easy. Then one day you decide "wouldn't it be nice to let the player make screenshots. And it would be good to do them at higher resolution than the screen so they look good!". Now you have a problem. All your code depends on getting the screen size from the static getSetting function. If instead you'd provided your rendering code with the screen size instead you could simply call it with different parameters. Another problem - You write an amazingly cool user interface class for your game. It needs to know what font the user has chosen so you call Game::getSetting(UIFont) to get it. Nice and easy. But then you decide to write a separate game editor... Now in order to use your amazingly cool UI class there is a dependency on your static Game class so you need to drag in the whole of the Game class too... Which probably has dependencies on other components. Sure you can change the code to work differently, but then you have two copies to maintain. If only you'd provided the font as a parameter to the class instead of letting it go get it itsself it would be much easier. In general having functions go out and get the data they need to do their job introduces coupling between components and makes your code harder to maintain and change, and reuse, and you are better providing from outside the data that is needed either as a function call parameter or when you construct an object or whenever. And the main advantage of static data or singletons is that they are easily located in random bits of code. If you don't need random bits of code to reach out to the static data then it doesn't need to be static any more... Note that this doesn't mean you need to pass dozens of separate bits of data. You might create a group of related settings where you generally need all of them for related purposes and pass that. For example make a UserInterfaceSettingsContext structure and store the related settings needed to draw a user interface in there and pass an instance of that to anything that needs it...