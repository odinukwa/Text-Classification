Recently Vishnoi gave an algorithm which finds TSP tours of length at most $n + O(n/\sqrt{k})$ in a $k$-regular simple graphs (talk & blog). The analysis crucially uses the van der Waerden conjecture (aka the Egorychev-Falikman theorem): the permanent of any doubly stochastic $n \times n$ matrix is at least $n!/n^n$. Egorychev and Falikman's proofs used deep results in convex geometry (in particular the Alexandrov-Fenchel inequality). On the other hand, a recent proof by Gurvits uses only elementary complex analysis and is quite a gem (nice presentation by Laurent and Schrijver in the MAA Monthly). Leaving the real line for the complex plane seems essential to Gurvits's proof and simplifies matters a lot. 

The following is based on Jiri Matousek's Geometric Discrepancy book. Define a range space in $\mathbb{R}^d$ parametrized by $a_1, \ldots, a_p$ as follows. Let $f$ be a degree $D$ polynomial in $d + p$ variables. For each $a \in \mathbb{R}^p$, the set $S(a)$ is defined as $S(a) = \{x \in \mathbb{R}^d: f(x, a) \leq 0\}$. For example, circles are defined as $(x_1 - a_1)^2 + (x_2 - a_2)^2 - 1 \leq 0$. We can get a bound on a quantity which is more delicate than VC dimension in this model. Define $\pi(m)$ as the maximum number of distinct sets induced by $\{S(a)\}$ on any set of $m$ points, i.e. $$ \pi(m) = \max_{X \subseteq \mathbb{R}^d}{|\{S(a) \cap X\}|}, $$ where the max is taken over sets $X$ of $m$ points. This is the primal shatter function of the range space $\{S(a)\}$. Notice that the VC-dimension of of the range space is that maximum $m$ such that $\pi(m) = 2^m$. Also, if the VC-dimension of a range space is $k$, then its shatter function is bounded by $O(m^k)$. For $m$ polynomials $f_1(a), \ldots, f_m(a)$, $\sigma = (\sigma_1, \ldots, \sigma_m) \in \{-, +\}^m$ is a sign pattern if there exists some $a$ such that for all $i$ the sign of $f_i(a)$ is $\sigma_i$. A result from algebraic geometry is that the maximum number of distinct sign patterns of $m$ degree $D$ polynomials in $p$ variables is bounded by $2^{O(p)}(Dm/p)^p$. Let's use this theorem. Define $f_i(a) = f(x^i, a)$. We get that $|\{S(a) \cap X\}|$ is exactly the number of distinct sign patterns of $f_1, \ldots, f_m$. So, in particular, if a range space is given by a family of constant degree polynomials in $p$ parameters, its shatter function is bounded by $O(m^p)$. 

a) There are $k$ nonzero $x_i$. Then if you subtract one from each $x_i$ you get a partition of $n-k$ with at most $k$ parts. So there is a bijection between partitions of $n$ with exactly $k$ parts and partitions of $n-k$ with at most $k$ parts. b) There are less than $k$ nonzero $x_i$. The number of such tuples is $q(n, k-1)$. The recurrence you get is $q(n, k) = q(n-k, k) + q(n, k-1)$. The initial conditions are $q(0, k) = 1$ and $q(n, 0) = 0$ for $n>0$. As to 3., I think $q(n, k)$ is at least on the order of $n^k$ which would imply that there aren't very succinct representations. 

Notice that if $x \in L$, then starting from $0^{p(|x|) + i}$ and repeatedly applying $P$ we run through all strings $z$ of size $p(|x|) + i$. Otherwise, there is some $w$ such that $M(x, w) = 1$ and the repeated applications of $P$ reach at most $2^{p(|x|)}$ strings. Taking $i$ to be a large enough polynomial in $|x|$ finishes the proof. 

So it seems that the answer to your question is "no": any language decidable in polytime by some machine is decided by a provably polytime machine. But maybe your question should be: 

On the positive side, there is a $\mathsf{OPT} + O(\log n)$ approximation (as opposed to the $\mathsf{OPT} + O(\log^2 n)$ approximation known for general bin-packing): either using the Karmakar-Karp rounding of the Gilmore-Gomory relaxation of bin packing (check David Williamson's book), or, more recently, using the connection with discrepancy of permutations and the $O(\log n)$ constructive upper bound on the discrepancy of 3 permutations of Bohus. 

I do not quite understand the question because you do not define some of your paremters (what are $p$ and $q$?). But let me try to clarify things. Let $N$ be the true number of distinct items. First Flajolet's guarantee for Wegman's algorithm is that, if the space used is $n$ words, then the estimate $X$ produced by the algorithm satisfies $$ \sqrt{\mathbb{E}[(X - N)^2]} \leq \frac{1.2}{\sqrt{n}}N. $$ Your suggestion is to sample at a rate $p$ from the $N$ different distinct items. I.e., you make a pass over the sequence of item identifiers and you keep each identifier in memory with probability $p$. Let the number of items stored in memory be $n$. Then the expected value of $n$ is $pN$, and the variance of $n$ is $p(1-p)N$, so we have $$ \sqrt{\mathbb{E}\left[\left(\frac{n}{p}-N\right)^2\right]} =\sqrt{\frac{1-p}{pN}}N = \sqrt{\frac{1-p}{\mathbb{E}[n]}}N. $$ So you are right that this is in some sense comparable with the guarantees for adaptive sampling (as well as those for the Flajolet-Martin probabilistic counting algorithm). However, the issue is that without knowing $N$, you do not know ahead of time what your space complexity $n$ will be, even in expectation. Or to put it another way, you do not know what to set $p$ to in order to make the space complexity acceptable. A natural fix is to start with some sample rate $p$, say $p = 1$, and once your memory fills up, you lower $p$. This is essentially the adaptive sampling algorithm. The simple sampling algorithm you suggest can be useful to refine a rough estimate of $N$. For example, you can use adaptive sampling or Flajolet-Martin to get a constant factor approximation to $N$, and then use that approximation to set $p$ in the Bernoulli sampling strategy. See this nice paper by Daniel Kane, Jelani Nelson, and David Woodruff, where they use this idea and a lot more to get an optimal space algorithm for the distinct items problem. The paper also gives references to much of the more important work on this problem. 

First note that this algorithm only computes $\lceil \log_2 v \rceil$, and as the code is written, it works only for $v$ that fit in a $32$-bit word. The sequence of shifts and or-s that appears first has the function of propagating the leading 1-bit of $v$ all the way down to the least significant bit. Numerically, this gives you $2^{\lceil \log_2 v \rceil} - 1$. The interesting part is the de Bruijn trick, which comes from this paper of Leiserson, Prokop and Randall (apparently MIT professors spend time doing bit hacks :) ). What you need to know about de Bruijn sequences is that they represent all possible sequences of a given length in a way that's as compressed as possible. Precisely, a de Brujn sequence over the alphabet $\{0, 1\}$ is a binary string $s$ of length $2^k$ such that each length $k$ binary string appears exactly once as a contiguous substring (wrap around is allowed). The reason this is useful is that if you have a number $X$ whose bit representation is a de Bruijn sequence (padded with $k$ zeros), then the top $k$ bits of $2^iX$ uniquely identify $i$ (as long as $i <k$). 

The log of the partition number is a lower bound on the deterministic communication complexity and the square of the log of the partition number is an upper bound. In other words, if $CC$ is the communication complexity, then we know that $\log_2 p \leq CC \leq (\log_2 p)^2$: this is Theorem 2.11. in the Kushilevitz-Nisan monograph. It is an open problem whether $CC = \Theta(\log p)$, and the biggest known gap is a factor $2$. So $p < r^{\log^d r}$ for a constant $d$ implies that the communication complexity is at most $\log^{2d+2} r$. In other words there exists such a constant $d$ if and only if the log rank conjecture is true. I believe the best we know is $p < r^{C\sqrt{r}}$ for a constant $C$, by a recent result of Lovett. 

There are many links between discrepancy theory and computer science, and Bernard Chazelle has beautifully surveyed some of them in his book. A number of links have been found more recently as well, for example Kunal's blog post talks about the connection to differential privacy from [MN] and [NTZ]. Another example is Larsen's idea of using discrepancy to prove update/query time lower bounds for dynamic data structures. Many of these links can be instantiated with homogeneous arithmetic progressions (HAPs). This would give: 

You can take any degree 3 bipartite graph $G$ and take its disjoint union $G'$ with a cycle $C$ of length 2m. The new graph $G'$ is bipartite, and has average degree $\frac{3n + 2m}{m+n} = 2 + \frac{n}{n+m}$. Also, the number of perfect matchings in $G'$ is exactly twice the number of perfect matchings in $G$, because the perfect matchings of $G'$ are the disjoint unions of a perfect matching of $G$ and a perfect matching of $C$, of which there are two. So, there is a polytime parsimonious reduction from counting perfect matchings in 3-regular bipartite graphs to counting perfect matchings in graphs with maximum degree 3 and average degree $2 + \frac{1}{\mathrm{poly}(n)}$. 

Sorry to be necromancing on this, but I thought you might want to look at some work on distributed continuous monitoring on streams, where you are given several streams and the goal is to monitor some aggregate statistic at a central monitoring site while minimizing communication. The model sounds to me closely related to your motivation. Look at the references in Muthu's book. One paper is this. Ganguly's paper is very interesting, too. 

Start with a feasible dual solution $y$. Attempt to find primal feasible $x$ such that $(x, y)$ satisfy complementary slackness. If step 2. succeeded we are done. Otherwise an obstruction to finding $x$ gives a way to modify $y$ so that the dual objective function value increases. Repeat. 

It depends how the polytope is represented. In the V-polytope presentation (i.e. $P$ is given in terms of its vertices), the problem is trivial, as Tim mentioned in the comments. In the H-polytope presentation (i.e. $P$ is given as an intersection of halfspaces defined by inequalities), Brieden proved that a constant factor approximation is impossible unless P=NP. You can check that his proof indeed uses a polytope contained in the positive orthant. As far as approximation algorithms, I believe the best known result is a factor $O(\sqrt{n/\log n})$ approximation ($n$ is dimension). The algorithm only needs a separation oracle, and this is the best possible approximation achievable in the oracle model, even by randomized algorithms. The same factor is achievable if the $\ell_2$ norm is replaced by an $\ell_p$ norm for $1\leq p \leq 2$. A better approximation may be possible for H-polytopes, but I am not aware of any such result. EDIT: Actually I am now realizing that for the very similar problem of approximating the radius of a centrally symmetric H-polytope with $m$ facets, there is a simple SDP rounding that gives $O(\sqrt{\log m})$ approximation. Let's say the problem is given as $\max\{\|x\|_2: -b \leq Ax \leq b\}$, where $A$ is an $m\times n$ matrix and $b \geq 0$ is an $m$-dimensional vector. The following vector program, which can be turned into an SDP in the usual way, is a relaxation: $$ \begin{align} &\max \sqrt{\sum_{j = 1}^n \|v_j\|_2^2} \text{ s.t.}\\ &\left\|\sum a_{ij} v_j\right\|_2 \leq b_i \ \ \ \ \ \forall 1 \leq i \leq m \end{align} $$ where $v_1, \ldots, v_n$ range over $n$-dimensional vectors. A Gaussian projection now gives an approximation. Let $g$ be a standard $n$-dimensional Gaussian, and define a random $x$ by $x_i = \frac{1}{C\sqrt{\log m}} g^Tv_i$ for a big constant $C$. Then $\mathbb{E}\|x\|_2 = \frac{SDP}{C\sqrt{\log m}}$, where $SDP$ is the value of the relaxation, and for $C$ sufficiently big, $$ \mathbb{E} \max_{i = 1}^m \frac{|(Ax)_i|}{b_i} \leq 1. $$ The last inequality comes from the fact that $(Ax)_i/b_i$ is a Guassian random variable with variance $(C^2\log m)^{-1}$, and the maximum of $m$ such random variables is no more than 1 in expectation by a standard concentration argument. I very much doubt that I am the first one to think of this approximation: does anyone know of a reference? 

Are you aware of the reconstruction and edge reconstruction conjectures? They have the same spirit as your probelm and, as far as I know, are open. Check out this beautiful half page proof by Lovasz that the edge reconstruction conjecture is true for any graph that has more edges than its compliment. 

Yes: the dual is a multicommodity flow problem which can be solved using the Garg-Koenemann algorithm $URL$ 

Your problem has a logarithmic approximation by reduction to a quadratic programming problem. The MaxQP problem is the problem of approximating the quadratic form $x^TMx$ for a $n \times n$ matrix $M$, where $x$ ranges over $\{\pm 1\}^n$. MaxCut can be written in this form by setting $M = \frac{1}{2n}(\sum{w_e})I - \frac{1}{2}A$ where $I$ is the identity matrix and $A$ is the adjacency matrix. The MaxQP algorithm of Charikar and Wirth gives an $O(\log n)$ approximation for MaxQP as long as $M$ has a non-negative diagonal. So as long as $\sum{w_e} \geq 0$, MaxCut with negative weights has a logarithmic approximation. 

There is the theory of limits of dense graph sequences, developed in the work of Lovasz and B. Szegedy. It has implications for certain property testing problems on graphs. See $URL$ Basically the idea is that they define a suitable metric on graphs and a notion of taking limits of graph sequences, and then they show that a graph property is testable if the function that maps a graph to the edit distance to the property is continuous in the metric space on graphs that was defined. And then there is of course Flajolet and Sedgewick's magnum opus dedicated entirely to using analytic methods for asymptotic analysis of combinatorial structures, including analysis of algorithms. This is mostly generating function tricks relying on complex analysis 

In the streaming model, any constant approximation of the maximum requires $\Omega(N)$ space. Showing this for a small enough constant approximation follows by an easy reduction from the disjointness problem in two-party communication complexity. This is now very standard, but I will describe it below anyways. Recall that in the disjointness problem Alice is given a binary string $x \in \{0, 1\}^N$, and Bob is given a string $y \in \{0, 1\}^N$, and they want to decide whether $x$ and $y$ have disjoint support. It is a classic result that this task requires that Alice and Bob exchange at least $\Omega(N)$ bits, even if they share random bits and only want to succeed with probability $2/3$ over the choice of randomness. Assume there is a randomized streaming algorithm $A$ with space complexity $S$, which with probability $2/3$ computes a number $a$ such that $\frac{a}{F_\infty} \in (\frac{2}{3}, \frac{4}{3})$, where $F_\infty$ is the true maximum frequency of the $N$ elements. This gives a protocol for the disjointness problem with $S$ bits of communication, and therefore $S = \Omega(N)$. In the reduction, Alice simulates the algorithm $A$ and feeds it a stream which increases the frequency of element $i$ by $1$ for each $i$ such that $x_i = 1$. Then Alice takes the memory state of $A$ (which has only $S$ bits) and sends it to Bob, who then simulates $A$ starting from the state he received from Alice and feeds it a stream which increases the frequency of $i$ by $1$ for each $i$ s.t. $y_i = 1$. Then Bob decides that $x$ and $y$ have disjoint support if the output $a$ of $A$ is at most $4/3$. For more information, see Alon, Matias, and Szegedy's beautiful paper, which introduced the streaming model and proved the first lower bounds. 

As Steven notes, the canonical example is $\mathsf{IP} = \mathsf{PSPACE}$. This collapse does not relativize, in the sense that there is an oracle $A$, subject to which $\mathsf{IP}^A \ne \mathsf{PSPACE}^A$. The intuition why the known proof of this result avoids the relativization barrier is that it uses arithmetization (Yonatan alluded to this in a comment): an interactive protocol for the $\mathsf{PSPACE}$-complete problem TQBF is given by considering an extension of a quantified boolean formula to a low-degree polynomial over a suitably large field. If we are given a relativized boolean formula (with oracle gates), such an extension does not exist. There is a refinement of the relativization barrier -- algebrization -- due to Aaronson and Wigderson. Generically, the arithmetization technique is not enough to circumvent the algebrization barrier. A complexity class inclusion $\mathsf{C} \subseteq \mathsf{D}$ algebrizes if for any oracle $A$ and any extension $\tilde{A}$ of $A$ to low-degree polynomials over a finite field, $\mathsf{C}^A \subseteq \mathsf{D}^{\tilde{A}}$. A separation $\mathsf{C} \not \subset \mathsf{D}$ algebrizes if for all $A$, and all extensions $\tilde{A}$, $\mathsf{C}^{\tilde{A}} \not \subset \mathsf{D}^{A}$. Aaronson and Wigderson show that $\mathsf{IP} = \mathsf{PSPACE}$ algebrizes, but many other results, including $\mathsf{NP} \not \subset \mathsf{P}$, do not. A recent example of a technique that does not algebrize or relativize is Ryan Williams' proof that $\mathsf{NEXP} \not \subset \mathsf{ACC}$. The separation does not algebrize: there is an oracle $A$ and a low-degree extension $\tilde{A}$ such that $\mathsf{NEXP}^{\tilde{A}} \subset \mathsf{ACC}^A$. Intuitively the reason why the proof avoids the barrier is that it relies on the existence of a faster-than-trivial satisfiability algorithm for $\mathsf{ACC}$ circuits, and the algorithm uses non-relativizing and non-algebrizing properties of such circuits. Ryan notes in the paper that all known faster-than-trivial satisfiability algorithms break down when oracles or algebraic extensions of oracles are added. There is also an interesting approach to understanding relativization through logic. In an old manuscript, Arora, Impagliazzo, and Vazirani define a system of axioms such that the relativizing results are exactly those that follow from the axioms, while non-relativizing results are independent from the system. A paper by Impagliazzo, Kabanets, and Kolokolova does something similar for algebrization by introducing an additional axiom to the ones defined by Arora, Impagliazzo and Vazirani. They show that most known non-relativizing results follow from their axioms, while P vs NP, among others, is independent of them. Apologies if I got something wrong, I am not quite an expert.