Why an index scan? It's likely that SQL Server estimated a lower cost for scanning the data (only 1.7MM rows, so a relatively small table) as compared to a loop-seek approach. Scanning the table will process all rows, and your plan shows that this performs . A loop-seek is estimated to perform (the cardinality estimate for the outer side of the loop). If we assume a seek is a binary search, we might therefore estimate that this will have a complexity of , or . This is slightly lower than the rows processed by the scan, but the number of logical reads will be much higher since each of the (estimated) ~70K seeks will perform several logical reads, yielding many more logical reads then the scan that was chosen. This might be the reason that a loop-seek plan yields a higher estimated cost and was not selected. Viewing the plan without scans If you'd like to compare this plan to the execution plan that uses a loop-seek is used for both joins, you can try adding the query hint to your query so that both joins use a loop join. It would be informative to post this actual execution plan for comparison. Why no partition elimination? I believe that partition elimination does not occur for because SQL Server does not have a hash join algorithm that performs partition elimination based upon the observed partitions on the build side of a hash join. This would be a nice optimization in some cases, but to my knowledge is not available in the current optimizer: Partition elimination is available for the inner side of a loop join, but not for the probe side of a hash join (unless the query contains an explicit predicate on the partition column). For further reading, SQL Server does have the concept of a collocated join, in which the hash join is applied independently for each partition of two tables that are partitioned the same way. However, this optimization is only available 2-way joins, so your query does not qualify. Paul White describes this and other partitioned table join considerations in much more detail in Improving Partitioned Table Join Performance. 

Full script Here is a full repro of the situation using a stats only database. What I've tried so far I dug into the statistics for the relevant column and found that the density vector shows an estimated ~1.1 billion distinct values. SQL Server 2012 uses this estimate and produces a good plan. SQL Server 2014, surprisingly, appears to ignore the very accurate estimate provided by the statistics and instead uses a much lower estimate. This produces a much slower plan that does not reserve nearly enough memory and spills to tempdb. I tried trace flag , but that did not fix the situation. Lastly, I tried to dig in to optimizer information via a combination of trace flags , as demonstrated in the second half of this article. However, I wasn't able to see any information explaining the bad estimate until it appeared in the final output tree. Connect issue Based on the answers to this question, I have also filed this as an issue in Connect 

It looks like SQL Server is generating a parameterized query plan that can work for any value of @CustomerPartitionKey. In order to do so, it seems to treat @CustomerPartitionKey as both a partition and a column you are seeking upon. If we take a look at the query plan where we have the bad estimate (3000 rows estimated, 300000 actual), we see that there are actually two separate seek predicates on related to : 

You can apply your final filter on after computing the running averages. In order to reduce the number of rows that need to be processed for the running averages, you can apply an earlier filter on to ensure that you are processing the minimal amount of rows but still including all the rows that are necessary to including the following 11 rows in the running average. (The only reason for the is to handle values at that fall at the end of the year.) This will still result in a table scan given your current table structure, but at least the rows can be filtered out earlier on in the query plan. 

CLR - one scan, full ungrouped result set The main function body There isn't a ton to see here; the main body of the function declares the inputs (which must match the corresponding SQL function), sets up a SQL connection, and opens the SQLReader. 

(Image from Preshing on Programming) Why do you want to do this? It seems unusual to use so heavily for something that seems similar the type of row-level locking that SQL Server provides on its own. I'm assuming you've carefully considered other approaches, but if you haven't yet done so, you might want to think about other solutions that don't involve sp_getapplock. 

Bonus: T-SQL #2 - the practical approach I'd actually use After trying to think about the problem creatively for a while, I thought I'd also post the fairly simple, practical way that I would likely choose to tackle this problem if it came up in my daily work. It does make use of SQL 2012+ window functionality, but not in type of groundbreaking way that the question was hoping for: 

I have a script that was easy to adapt and may work for your needs. You'll simply need to replace with the appropriate table (including schema name). The script is commented, but the general summary is to loop over all databases where you have permissions, check the number of rows and size of the table (if it exists), and add a corresponding row into a #temp table. At the end, you can review all of the results by looking in this temp table. There may be a possibility of using the undocumented ms_foreachdb procedure to make the code a bit less verbose, but that's not something I have tried or would rely upon in production. Also, note that this approach may be slower on SQL 2014+ if you have a large number of tables. This is due to an outstanding performance bug in SQL 2014 

Or if you want just yesterday's transactions, you can combine the two approaches to create a date range that encompasses all of yesterday. Note that you can't use in this case since you probably don't want the end of the range to be inclusive. 

Without these filtered statistics, SQL Server will take a more heuristic-based approach to estimating the cardinality of your join. The following whitepaper contains good high-level descriptions of some of the heuristics that SQL Server uses: Optimizing Your Query Plans with the SQL Server 2014 Cardinality Estimator. For example, adding the hint to your query will change the join containment heuristic to assume some correlation (rather than independence) between the predicate and the join predicate, which may be beneficial to your query. For the final test query, this hint increases the cardinality estimate from to , but is still quite a bit shy of the correct row estimate produced with the filtered statistics. Another approach we've used in similar situations is to extract the relevant subset of the data into #temp tables. Especially now that newer versions of SQL Server no longer eagerly write #temp tables to disk, we've had good results with this approach. Your description of your many-to-many join implies that each individual #temp table in your case would be relatively small (or at least smaller than the final result set), so this approach might be worth trying. 

As Aaron mentions, this logic could be done in a view rather than a function. If you do need to use a function, however, an alternative might be to use rather than . It has a column, and as far as I can tell from the documentation it provides the same information you are looking for. 

Size comparison Because we are loading random data, columnstore achieves only a modest reduction in table size. If the data was not as random, the columnstore compression would dramatically decrease the size of the columnstore index. This particular test case is actually quite unfavorable for columnstore, but it's still nice to see that we get a little bit of compression. 

I think that what is happening is that SQL Server uses the definition of the first creation of to try to compile the remainder of the plan. In fact, I can't see anywhere in (or the procs it calls) that would drop prior to invoking . I could be missing something, but it seems like you would also get a run-time error if you invoke the procedure with a value for the parameter, thereby causing the procedure to have to be invoked. 

Using , this approach reports , which confirms the "single pass" over the table. For reference, the original loop-seek query reports . As reported by , the CPU time is . This compares favorably to for the original query. CLR quick summary The CLR summary can be summarized as the following steps: 

sp_whosiasctive While running this script, you can use sp_whoisactive to view the current server activity. You can often view the query plan for the specific statement that is currently executing. In my case, I see the following because the statement is most likely to be running at any given moment in time: 

Based on the documentation, it does not seem like -1 is an expected value for . But it does seem to appear somewhat frequently, and the procedure does not make an attempt to handle this situation. In my testing, I'm not even sure if it would be possible to do so; even when calling with the default values (to find the whole plan, not the specific plan based on the offset), I found that no plan was provided when was -1. I think there is the possibility of a situation where the request exists, but the plan and current statement offset are not yet available. I do not know the full explanation for this behavior though. Since a is available (at least in my cases with a plan), I don't think it's the case that plan compilation is still happening. Reason #2: It took more than 5ms to find the query plan The full implementation of actually uses a cursor to look up each plan one at a time. There is a of to avoid delaying the entire procedure too much in case a lock is encountered when looking up a particular plan. 

In this case, the table has 22,000 rows, SQL Server estimated that you are going to use 7,700 of those rows, and you actually only used 1,827 of those rows. Because SQL Server uses statistics in order to allocate ranges of rows to threads, the poor cardinality estimates in this case are likely the root cause of the very poor distribution of rows. Thread skew on 1,872 rows may not matter much, but the painful point is that this then cascades down to the seek into your 1.5 billion row fact table, where we have 30 threads sitting idle while 600 million rows are being processed by 2 threads. 

Here is a solution that uses to find the value for the latest quarter that ends on or before the daily date. If your table is indexed by , this query will be quite efficient with a one-row seek to look up the quarterly value for each daily date. This solution also does not require a calendar table and makes no assumption about the duration of a quarter. However, it does make an assumption that you have no gaps in your quarterly table. For example, if you were missing a year's worth of quarters, the solution would identify the "most recent quarter" as the quarter that happened a year ago. This may be perfectly fine, but you should at least be aware of the assumption. 

The primary downside is that the view definition needs to all resources and languages before using applying the to the local and default resources. Even so, this is going to be a fairly efficient plan with 4 singleton seeks assuming that you have the proper indexes.