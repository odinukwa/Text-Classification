If the scene does not entirely fit into memory, you are entering the field of out-of-core rendering. There are essentially two approaches here: a) Generate your scene on-demand b) Load your scene on-demand The former approach aligns well with most animation workflows, where models are heavily subdivided using e.g. Catmull-Clark and can become very memory-intensive, but the base meshes themselves easily fit into memory. Pixar have a few papers about this (e.g. Ray Differentials and Multiresolution Geometry Caching for Distribution Ray Tracing in Complex Scenes), but the gist of it is that models are only subdivided when they are hit by a ray, and only subdivided as much as is reasonable for such a ray (e.g. diffuse interreflection need less accuracy than mirror reflections). The rest is handled by a geometry cache, which keeps the subdivided models in memory and hopefully makes the process efficient by a good eviction strategy. As long as all your base meshes comfortably fit into memory, you can easily go out-of-core and render meshes at subdivision levels that would never fit into memory. The geometry cache also scales nicely with the amount of memory you have, allowing you to weigh RAM vs. render times. This was also used in Cars I believe. The second approach is more general and does not rely on heavy use of subdivision. Instead, it relies on the fact that your scene was most likely made by an artist and already comes partitioned into reasonably small objects that fit into memory individually. The idea is then to keep two hierarchies (kD-tree or bounding volume hierarchy): A top-level hierarchy that only stores bounding boxes of the objects in your scene, and a low-level hierarchy that stores the actual geometry. There is one such low-level hierarchy for each object. In this approach, you ideally already store a bounding box along with each object on disk. As the scene is loaded, you only build the top-level hierarchy initially, meaning you only have to look at the bounding boxes and not the geometry. You then start tracing rays and traverse them through the hierarchy. Whenever a ray hits a leaf node in the top-level hierarchy (i.e. it hits the bounding box of an object), that object is loaded into memory and its low-level hierarchy is built. The ray then continues down into tracing that object. Combined with an object cache that keeps as much of the low-level hierarchy in memory as possible, this can perform reasonably well. The first benefit of such an approach is that objects that are never hit are never loaded, meaning that it automatically adapts to the visibility in your scene. The second benefit is that if you are tracing many rays, you don't have to load an object immediately as it is hit by a ray; instead, you can hold that ray and wait until enough rays have hit that object, amortizing the load over multiple ray hits. You can also combine this approach with a ray sorting algorithm such as Sorted Deferred Shading for Production Path Tracing to avoid thrashing due to incoherent rays. The mentioned paper describes the architecture of Disney's Hyperion renderer, used for Big Hero 6 I believe, so it most likely can handle scenes at production scale. 

I believe a common solution is to split the camera transform used to project the grid from the camera transform that is used to render the grid. At perspectives close to top-down, the two cameras coincide, but as the viewing camera gets close to a horizontal perspective, the projection camera deviates and tries to keep a minimum inclination, i.e. it hovers somewhere above the view camera and looks down slightly. The tricky bit is making sure that the field of view of the projection camera always covers the region of the scene seen from the render camera. I don't have a resource at hand that details how to compute the appropriate transforms, and it might be tedious to derive by hand. A different solution is to grab the signal processing toolbox: The artifacts seen in your image are essentially aliasing, caused by insufficient sampling of the wave heightfield by the projected grid. Therefore, one solution is to filter the heightfield appropriately, depending on the projected area of a grid cell. I believe this is used in offline rendering of oceans, and it essentially makes sure that waves at the horizon go flat. However, I'm not sure how feasible this is in real-time rendering, since you would need high-quality anisotropic filtering to make this approach look reasonable. 

Signed distance fields are popular in minimal graphics applications, such as the demo scene, where interesting objects can be synthesized from few simple analytic primitives such as spheres or cubes. However, signed distance fields are not restricted to these simple objects, and they do not necessarily need to be designed by a human. For example, you can synthesize the analytic signed distance field of a triangle mesh without any artist interaction - the SDF is just the signed distance to the closest triangle after all, which can be easily computed. This allows you to keep using your traditional mesh pipeline while also using SDFs in the background. In the case of the Unreal Engine, the SDF is precomputed automatically once for all static meshes and is then sampled into a low-resolution 3D texture. It can then be cheaply evaluated everywhere using a simple texture lookup, allowing them to do soft shadows and similar at runtime. This Unreal presentation at GDC 2011 mentions the 3D textures briefly (slide 27). 

I believe the most prominent spectral effect that can't be faithfully reproduced with RGB is dispersion, caused by dielectrics with spectrally varying index of refraction (usually modelled with the Sellmeier equation). Other spectral phenomena are usually caused by wave effects. One example that is encountered in real life every now and then is thin-film interference, which is caused by one or more reflective surfaces layered closely on top of each other (e.g. oil slicks, soap bubbles). Another wave effect that can sometimes be observed is diffraction, caused e.g. by diffraction gratings, which is what causes the funky appearance of CDs. 

Most descriptions of Monte Carlo rendering methods, such as path tracing or bidirectional path tracing, assume that samples are generated independently; that is, a standard random number generator is used that generates a stream of independent, uniformly distributed numbers. We know that samples that are not chosen independently can be beneficial in terms of noise. For example, stratified sampling and low-discrepancy sequences are two examples of correlated sampling schemes that almost always improve render times. However, there are many cases where the impact of sample correlation is not as clear-cut. For example, Markov Chain Monte Carlo methods such as Metropolis Light Transport generate a stream of correlated samples using a Markov chain; many-light methods reuse a small set of light paths for many camera paths, creating many correlated shadow connections; even photon mapping gains its efficiency from reusing light paths across many pixels, also increasing sample correlation (although in a biased way). All of these rendering methods can prove beneficial in certain scenes, but seem to make things worse in others. It's not clear how to quantify the quality of error introduced by these techniques, other than rendering a scene with different rendering algorithms and eyeballing whether one looks better than the other. So the question is: How does sample correlation influence the variance and the convergence of a Monte Carlo estimator? Can we somehow mathematically quantify which kind of sample correlation is better than others? Are there any other considerations that could influence whether sample correlation is beneficial or detrimental (e.g. perceptual error, animation flicker)? 

In traditional stereo 3D, I don't believe that there is a way to make a fixed focal plane feel natural to the viewer. When looking at an out-of-focus object in stereo 3D, the object remains out-of-focus, causing conflicting cues. The lens in the eye tries to adjust to bring the object into focus, but of course it won't succeed, causing eye strain and headaches. However, there is hope outside stereo 3D: Lightfield displays, such as this nvidia prototype, go a different route. In stereo 3D, the light in the scene is already captured by two virtual (or physical) cameras, "baking" in the focal plane. Head-mounted displays like Oculus Rift then attempt to tape two displays in front of your eyes in such a way that the retina receives the exact same image that was captured by the camera. Lightfield displays go a different route: Instead of capturing two images ahead of time, they reproduce the entire 4D light field in front of your eyes, allowing your eyes to capture the image as if they were sitting directly inside the virtual scene. This has a number of benefits, including much smaller and lighter hardware as well as giving your eyes the ability to refocus. If there is a way to make lightfield displays technically and commercially viable, then I believe they can remove the need for depth of field and fixed focal planes entirely and make VR feel a whole lot more comfortable for the viewer. However, it is likely not possible to construct lightfield screens, so televisions and cinemas won't be able to use this technology.