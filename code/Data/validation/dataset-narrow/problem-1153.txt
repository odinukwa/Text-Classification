There are many applications of real analysis in theoretical computer science, covering property testing, communication complexity, PAC learning, and many other fields of research. However, I can't think of any result in TCS that relies on complex analysis (outside of quantum computing, where complex numbers are intrinsic in the model). Does anyone has an example of a classical TCS result that uses complex analysis? 

Let $d,q \in \mathbb{N}$, and let $f:\mathrm{GF}(q) \to \mathrm{GF}(q)$ be a univariate polynomial. In this case, it is possible to test whether $f$ is of degree at most $d$ (or whether $f$ is at least $\epsilon$ from it) using the algorithm of (RS96). The testing algorithm works by simply trying to interpolate the function $f$ on $\Theta(1/\epsilon)$ collections of $d + 2$ uniformly selected points, and checking whether the resulting functions are all polynomial of degree at most $d$. Is there an extension of this result that allows to test whether a bivariate polynomial $g:\mathrm{GF}(q)^2 \to \mathrm{GF}(q)$ is of degree at most $d$ in the first variable, using $O(\epsilon^{-1} \cdot d)$ queries? 

The high level question is as follows: Suppose some group (here assumed to be a vector space of $\mathbf{F}_2^n$) has a low-distortion embedding into $l_1$. Under what condition does the quotient of the group by a (normal) subgroup inherit this low-distortion embedding? Formally: Let $G$ be a group with a Cayley graph ${\cal G} = Cay(G,S)$ for some generator set $S\subseteq G$, that can be embedded into the hypercube $\mathbf{F}_2^n$ with low-distortion, i.e. there exists $i: G \hookrightarrow \mathbf{F}_2^n$ such that for any $x,y\in V({\cal G})$, $\frac{1}{c} d_{\cal G}(x,y) \leq \Delta(i(x),i(y))\leq c d_{\cal G}(x,y)$, where $\Delta(a,b)$ is the Hamming distance between $a,b\in \mathbf{F}_2^n$, and $d_{\cal G}(x,y)$ is the length of a shortest path connecting $x,y$ in ${\cal G}$. Suppose now that $N\subseteq G$ is a (normal) subgroup of $G$. Is there a choice of generators $S_N\subseteq G / N$, for which the quotient group $G/N$ has a low-distortion embedding into $l_1$ as well ? 

Oded Goldreich's In a World of P=BPP is one of the best written papers that I read. This is mostly due to the clarity of the exposition, the conceptual perspective, and the choice to include reflections regarding the meaning of the results in the paper. 

What is the story behind the unusual author ordering in these papers? Are there any other examples of major TCS papers in which the order of the authors is not alphabetical? 

In the adjacency matrix model, there is a lower bound of $\Omega(n)$ on the query complexity of testing whether an $n$ vertex graph consists of two isomorphic copies of some $n/2$-vertex graph (see Introduction to testing graph properties - Goldreich for a survey). Also, there are many lower bounds that depend on $n$ for testers with one-sided error, e.g.: testing $\rho$-Clique,$\rho$-Cut, and $\rho$-Bisection (see Property testing and its connection to learning and approximation - Goldreich, Goldwasser, Ron) Moreover, in the bounded degree graph model, testing 3-Colorability requires $\Omega(n)$ queries, whereas testing 2-Colorability (i.e., Bipartiteness) requires $\Omega(\sqrt n)$ (see Property testing in bounded degree graphs - Goldreich, Ron). 

You can check out the graphs at NetWiki. Of course, you'll have to sort out the formatting yourself. 

Knowing the exact value of the chromatic number $\chi$ cannot help by more than a factor of $n$. Since there are only $n$ possible values of $\chi$, you can 'guess' its value, i.e., run processes $P_1,\dots,P_n$, where $P_i$ runs an algorithm assuming $\chi=i$. This whole scheme can find an optimum colouring in time at most $n$ times the time that it takes $P_\chi$ to find an optimum colouring. On the other hand, if you're talking about parameterizing the running time by $\chi$, then it's a much more interesting question. It's in FPT if you parameterize by $n-\chi$ S. Khot and V. Raman, ‘Parameterized Complexity of Finding Subgraphs with Hereditary properties’. If you parameterize by $\chi$ I would assume it's W[1]-hard. 

The DMANET mailing list is one of the major resources I use. It "spreads information on conferences, workshops, seminars etc. relating to discrete mathematics and algorithms." They also sometimes have job postings for positions at the faculty, postdoc, and grad student levels. 

Are there any known constructions of binary locally testable codes with very low (e.g., independent of the length of the codeword) query complexity and "good" rate (e.g., mapping strings of length $k$ to strings of length $k^{1+c}$, for a small constant $c$) that are also locally decodable (even if the query complexity for decoding is very large (but still sublinear))? 

While the rule of thumb is that in TCS papers the authors are ordered alphabetically, there are some notable counterexamples that comes to mind, wherein the authors are ordered in a different way, e.g., 

We say that a Boolean function $f: \{0,1\}^n \to \{0,1\}$ is a $k$-junta if $f$ has at most $k$ influencing variables. Let $f: \{0,1\}^n \to \{0,1\}$ be a $2k$-junta. Denote the variables of $f$ by $x_1, x_2, \ldots, x_n$. Fix $$S_1 = \left\{ x_1, x_2, \ldots, x_{\frac{n}{2}} \right\},\quad S_2 = \left\{ x_{\frac{n}{2} + 1}, x_{\frac{n}{2} + 2}, \ldots, x_n \right\}.$$ Clearly, there exists $S \in \{S_1, S_2\}$ such that $S$ contains at least $k$ of the influencing variables of $f$. Now let $\epsilon > 0$, and assume that $f: \{0,1\}^n \to \{0,1\}$ is $\epsilon$-far from every $2k$-junta (i.e., one has to change a fraction of at least $\epsilon$ of the values of $f$ in order to make it a $2k$-junta). Can we make a "robust" version of the statement above? That is, is there a universal constant $c$, and a set $S \in \{S_1, S_2\}$ such that $f$ is $\frac{\epsilon}{c}$-far from every function that contains at most $k$ influencing variables in $S$? Note: In the original formulation of the question, $c$ was fixed as $2$. Neal's example shows that such value of $c$ does not suffice. However, since in property testing we are usually not too concerned with constants, I relaxed the condition a bit. 

The following question arises from the study of quantum error correction, and high-dimensional expanders: Is there an algorithm that for given numbers $n>0,d≤n,r≤n$ samples uniformly a linear operator $:\partial: F_2^n \to F_2^n$ , such that $\partial^2 = 0$, and each row/column of ∂ has Hamming weight at most d, and rank(∂)=r? Note, that for d = n, i.e. boundary operators that are not sparse, the question can be solved in a straightforward manner as outlined in a paper of Bravyi and Hastings [Homological Product Codes]: one considers some fixed matrix Q - such that dim(im(Q))=r, and im(Q)⊆ker(Q) - and then the distribution is the set of matrices $A Q A^{-1}$ where A is a random invertible matrix over $F_2^n$. Hence, the question is, whether the extra sparsity condition makes the problem intractable or not. The importance of the question relates to the quantum LDPC conjecture: ideally, we would want to find such a procedure for $d=O_n(1)$. A solution to this question would at least imply that there exists some candidate ensemble where we can find good quantum LDPC codes - specifically of a type called CSS codes. 

An $\mathcal{MA}$ communication complexity protocol is communication complexity protocol that starts with an omniscient prover that sends a proof (that depends on the the specific input of the players, but not on their random bits) to both players. The players then communicate with each other, in order to verify the proof (for more details, see: On Arthur Merlin Games in Communication Complexity, by Hartmut Klauck). The are quite a few lower bounds (e.g., On the power of quantum proof, by Ran Raz and Amir Shplika) of the following form: Suppose we have a communication complexity problem $\mathcal{P}$ with a tight bound of $\Theta(T(n))$ on its communication complexity (for some function $T$). There exists a lower bound that shows that every $\mathcal{MA}$ communication complexity protocol that communicates $c$ bits and uses a proof of size $p$, must satisfy $c \cdot p = \Omega(T(n))$. So one can think of it as a tradeoff between the work that prover has to do, and the work that the verifiers have to do. Moreover, it seems that for every communication complexity problem that I know of (with a tight bound of $\Theta(T(n))$ on its communication complexity), there exists a protocol wherein the prover sends a proof of size $\tilde O(T(n))$, and the verifiers only uses $\tilde O(1)$ bits of communication (cf. the two papers I mentioned above). Thus, in a sense, all of the work has been delegated to the prover (achieving the extreme case of the aforementioned lower bounds). Is there a result that shows that a verifier-"heavy" protocol implies the existence of a prover-"heavy" protocol? Is there a counter example? What about other models (such as $\mathcal{MA}$ decision trees/query complexity) wherein our understanding of the behaviour of $\mathcal{MA}$ protocols is deeper? 

It is known that most computational problems related to linear algebra can be computed in $NC^2$ - i.e. for an $n\times n$ matrix $A$, over the reals or a finite field, we can compute the rank of $A$, $\det(A)$ or $A^{-1}$, etc. in parallel time $O(\log^2(n))$. The complexity of these linear-algebra problems remains the same (i.e. in $NC^2$) even if the rank of $A$ is say $n^{\epsilon}$ for some small constant $\epsilon>0$ (just by padding zeros) but what happens to the complexity of these problems when $A$ has very small rank, i.e. $r = rank(A) \leq \log(n)$? Clearly, if $A$ is ${\it given}$ to us as a full-rank matrix of dimension $n \times r$ then the complexity drops: Consider for example the problem of determining whether $A x = b$ for inputs $A,b$ over $\mathbf{F}_2$ for some vector $x$. One can simply enumerate over all possible column subsets of $A$, and compute $x$ in constant parallel time ($ACC^0[2]$). However, the problem is that we do not posses such a succinct description from the input. Naively, trying to brute-force enumerate over all possible column combinations of size $\log(n)$ results in time ${n\choose \log(n)}$ which is quasi-polynomial. 

To complete the search process for an element $x$ that is not in the tree, you have to go all the way to a leaf node to confirm that $x$ is not in the tree. If you repeatedly search for such an element $x$, you have to splay on an unsuccessful search, otherwise the query time for $x$ could indefinitely remain as bad as $\Theta(n)$, even in an amortized sense. 

"someone" is right. Timothy Chan's paper "Dynamic Planar Convex Hull Operations in Near-Logarithmic Amortized Time" appears to solve the problem with insertions/deletions taking $O(\log ^{1+\epsilon}n)$ amortized time, and queries taking $O(\log n)$ time. He solves your problem which is dual to the convex hull problem. 

This is the well-studied problem of Nearest Neighbour Search. There is not one 'best solution' to the problem---you'll want to choose trade offs based on the input and requirements. Do you need the creation of the data structure to be parallel? Or just the queries? Depending on how many thousands of points you're talking about, if your set of points is static it might not be impractical to do something naive like: 

Compared to spectra of undirected graphs, which correspond to symmetric matrices, the spectra of directed graphs is not very well known: It is known that a directed graph $G = (V,E)$ has an adjacency matrix $A(G)$ whose eigenvalues are binary $\{0,1\}$ if $G$ is a-cyclic. This follows by sorting the vertices into strongly connected components: this fixes an enumeration of the vertices $v_1,.., v_n$ such that the permuted Laplacian according to this ordering is upper-triangular with $0/1$ entries. But what is known if $G$ is the other extreme end - i.e. $G$ is a strongly-connected graph on $n$ vertices - meaning that there is a directed path between any pair of vertices. Generally, one would need to compute the characteristic polynomial of $A(G)$ and compute its roots. Despite $A(G)$ being a $\{0,1\}$ matrix this seems like a daunting task. In particular, the roots of this polynomial are in general complex numbers. The Perron-Frobenius theorem implies that at least the top eigenvalue is real and simple, but does not reveal information about the rest of the eigenvalues. However, what if we're interested only in very weak bounds of the following form: $\textbf{Conjecture: Dichotomy of eigenvalues}$: Let $G$ be a directed graph on $n$ vertices. Then either all eigenvalues of $A_G$ are real, or there exists at least one eigenvalue $\lambda$ such that $im(\lambda)\geq 1/poly(n)$. Do such bounds follow trivially from known theorems? Alternatively, can a directed graph have an eigenvalue with an exponentially small imaginary component?