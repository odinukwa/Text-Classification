Sidenote: I'm aware some kernels can cope with memory disappearing -- they usually need to be told the memory is going to go first. 

VMware does have some products for this sort of workflow; Lab Manager or Orchestrator could automate most of what you want. If you want to save some cash on the provisioning / teardown process, you could roll your own solution using their scripting API and V(I)MA(forums here) To automate your app deployment and configuration, Puppet or cfengine will do config management and application installation, and can be used with Capistrano for general purpose automation. 

This is a more localised, *nix based answer. I've not found any good tools to emulate it under Windows. There's a few ways to implement this ... and to catch it when you forget. Revision control systems like subversion, git, cvs or RCS is a good way of tracking the history of a config file. If you don't want to install a revision control system on your production servers, storing configuration file directories either locally or remotely using something like rsnapshot will give you most of the benefits of a RCS, but you lose the possibility of auditing or leaving commit logs (although this could be worked around with comments inside the files themselves). To help you remember to log the changes, automated reporting of configuration changes via a nightly, cron'ed tripwire run is a good start. After building tripwire's database of the current state of files, any change to them will result in an email during the next run. You will continue to receive this mail until the database is updated, thus "resetting" the tripwire. 

I'd expect that to be management traffic; vCenter Server need to confirm hosts are alive and send them tasks; the hosts need to confirm their licenses are valid and obtain tasks from vCenter. Each node also needs to communicate with the others in it's cluster to confirm which nodes are alive. You might be able to track down what the traffic is by looking at the port numbers, and tracing that back to an owning process (Process Explorer and Wireshark on the vCenter server would probably help here). 

IIS' SMTP server isn't particularly bright. I'd recommend using a more intelligent mail server that you control perform the address munging and/or forwarding to a specific address, and having IIS' SMTP server "smart host" to it. The "more intelligent" mail server need not be a normal mail MTA application; a simple script that opens a network socket, implements just enough SMTP to catch the mail from IIS and write it out to a log file would work just fine. 

You don't specify a platform, so I'll assume Windows. Give psinfo (part of the Sysinternals PSTools) a shot - feed it a text file with a list of machines to query, and an administrator-level username / password and output either CSV or plain text. CSV and a little data munging should get you most of the way there. 

Your first example is telling ssh to connect to the ssh daemon (server side process) on a non-standard port, in this case, 25. If the sshd daemon is not listening on this port, the connection attempt will time out or error with a protocol problem, as you're experiencing. The second is telling ssh to make a normal connection to "serveraccount" (which happens over port 22) and remotely execute a telnet command to localhost:25. The output of create-smtp-message becomes stdin to your telnet command, thus allowing your SMTP server to receive your message. 

As for your VM, your best bet for flexibility is to store your fileserver's boot disk as a VMDK on the SAN so that you can have other hosts boot it in the case of a host failure. Using VMware's HA functionality, booting your VM on another host is automatic (the VM will boot on the second host as if the power had been pulled; expect to perform the usual fsck's and magic to bring it up as in the case of a normal server). Note, HA is a licensed feature. To mitigate against a VM failure, you can build a light clone of your fileserver, containing the bare minimum required to boot and have SAMBA start in a configured state and store this on each host's local disk, awaiting you to add the data drive from the failed VM and power it on. This may or may not buy you extra options in the case of a SAN failure; best case scenario, your data storage will require a fsck or other repair, but at least you don't have to fix, rebuild or configure the VM on top. Worst case, you've lost the data and need to go back to tape... but you were already in that state anyway. 

Start off by verifying your network is behaving itself. Assuming you have managed switches, look at the interface statistics for speed/duplex mismatching or a mismatched MTU. Consider checking / replacing cabling if anything is running errors (eg: trying to run GigE over Cat5 instead of Cat5e will likely give grief). Run some tests to prove you can get wire-speed transfers between the two machines and to the external machine; netcat, ftp or http transfers are a good start here (scp may get CPU bound, and thus, may not be the best test). Test the same query locally on the Postgres server. If it completes in an appropriate timeframe, you know it's not the database. If it doesn't complete or takes "too long", then you have a bad query or other database problem to debug. Make sure to consider the storage I/O side of things; you may be saturating what your disks are capable of providing. Check the VMware performance graphs to confirm / deny. Assuming that works, disable the firewall and run the same query against the postgres server from "box1". If that works, the VM->VM connectivity is likely fine. Assuming that works, bring the firewall back up and test again. If that works, then your problem is likely external to that host, leaving the switch or the external host to debug. 

If you've got a managed switch, look at the switch logs, port statistics and port link state to identify/confirm the symptoms during the failure. Faults with autonegotiation, bad cabling, mismatched speed/duplex and framing errors are easy to pick up with this info. "show log" and "show interface <interfacename>" will do the trick on Cisco gear, I presume managed switches from other vendors will have similar commands. 

A few things to start with: 1) Which end is dropping the connection? With the VPN up, run pings in both directions and perform packet captures at both ends, ideally of both the encrypted an unencrypted traffic. Cause it to fail and see what happens; you'll likely find one end is sending packets, but the other end is either silently dropping them on the floor or actively rejecting them. 2) Start simplifying things and try again at each step; remove the wireless and plug directly into your router. Bypass the router and plug directly into your modem. Do the same at the other end. You can identify some forms of NAT problems by removing routers from the equation and putting your public IP's directly on the end kit; the amount of networking gear with NAT or ALG bugs is astounding. 3) Check VPN event logs and firewall logs to see if they provide any insight as to why the VPN tunnel is dying. At a minimum, they should be logging the failure and, ideally, a reason why. 4) Try a different PC, try a different OS. See if you can nail down a combination that works. Once you've found the faulted equipment, you can continue to troubleshoot; you may need to upgrade a device's firmware, add in some NAT/port forwarding entries or adjust your firewall ruleset. If it's specific to a machine, it may need a rebuild or networking stack to be reset (some Broadband Optimiser application can horribly break network stack configuration, and other apps that inject themselves into the network stack may be mangling the packets).