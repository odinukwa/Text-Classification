works as follows. First, the inner will find the 1st dot. As index is 0, it starts from the beginning of the string. +1 will point to next character after the dot and pass that as the starting index to the outer . The outer will find index of the second dot. Finally, will return a substring from zero to the 2nd dot. Edit As per ypercube's request, let's handle some errors. First, add some data with invalid values: 

For future use, consider adding CHECK constraint to the ClientGenderName column. This prevents inserting rows that don't contain valid values in the first hand. While you are at it, you should consider adding constraints too. 

Yes, there is, though you got to use command anyway. Instead of , use to get a detailed view of files in the backup. The Size column tells the file size in bytes. 

Yes, it is possible and this may or may not be desirable. The database theory has a concept of isolation, which is about transaction visibility to other processes. MySql's documentation about has a discussion about product specific syntax. 

Trying to find an opitmal value for fill factor is a well-researched problem. In the described scenario, there is not much to be done unless the table doesn't use default values. 

Another a way would be based on a view. If you can't modify the source table - often the case for 3rd party products - a view can save your day. Like so, 

Paul Randal (of the SqlSkills and fame) has a nice set of pre-corrupted databases and instructions how to work with those too. 

Unfortunately, you have encountered quite old a "feature". There's been a Connect ticket open since 2008, and for almost ten years this hasn't been significant enough to warrant a fix. The standard workaround is, like you figured, a followed by comparing table metadata. Another possibility is binary searching the offending column, but that's manual work too. There are some hacks for metadata comparison, but simple, elegant solution doesn't exist. Maybe some third-party tools would be of help, but I am not aware of such. 

Sql Server's locks are described in good detail in the product documentation. A lock might or might not be escalated to page/table level, the behavior is documented, again, in Technet. A fairly common tactic for deleting large amount of rows is to split up the delete in batches. Instead of deleting all the 5M rows at once like so, 

Just like ypercube commented, Sql Server doesn't adhere to similar a boolean evaluation as, say, Java or C#. There are lots of articles about this, but facts are a bit hard to find. As per the documentation: 

Marked transactions make it easy to do point-in-time restores without worrying about actual time. Instead, one restores to logical event. As a practical example, consider an update that needs to be undone. Now, the update was such that a database restore is needed as wrong value was set to all the columns and original data is lost. Maybe a business analyst set VAT to 24% for all the products instead of a subset. We don't know what the previous VAT values were, so the update cannot be undone. Restoring the database is simple, but how much of the transaction logs would be applied? At which point-in-time would one stop the log restores? Does the business analyst keep a detailed journal that records the beginning of the update? Maybe all that is remembered is that "it was done Monday afternoon". If there are lots of changes in the database, recovery to right spot is trial and error. Should the update be done within marked transaction, the DBA will simply query table and pick symbolic name for the transaction. Log restore is then done with parameter to stop at the precise point before marked transaction was started. 

As you explained in the comments section, the objective is to prevent developers logging into production server. There are many ways to achieve this. The first and foremost is a company policy. Talk with your manager. Prepare for the meeting by making a list of reasons why access should be restricted. In addition, try to invent a number of counter-arguments and ripostes for those too. Bear in mind that developers are working for the company too, so you (the company, that is) should have development servers and whatnot so that developers can, say, develop. If you are in agreement with management that no developer should connect to production servers, the policy must be informed to the developers. In addition, tell what resources the developers are supposed to use. If they don't know better, how would they change the ways? Should the management decide that developer access is needed to production, dont't try to enforce limits by your own. This is about company property, not your personal playgroud. If you got a policy that denies developer access, you can start erecting technical limits to enforce the policy. Start by looking at Windows authentication and Active Directory groups. 

Shrinking a database is not a good idea. There are specific cases, but they require a DBA to carefully evaluate the situation. Sql Server cannot have an idea if a query in, say, next five minutes is going to hit tempdb heavily. Without such a knowledge, shrinking and expanding tempdb is going to be unnecessary and, to be honest, stupid too. TempDB is re-initialized whenever Sql Server service is restarted. In most of the cases, this effectively shrinks the tempdb data and log files. 

There is no such a thing as "best option". If would always be superior, why bother offering as sub-par alternative or vice versa? It really depends on a lot of things, so further analysis is needed. Start by reading some materials about data compression. MS has published a technical article Data Compression: Strategy, Capacity Planning and Best Practices that's a bit old, but fits your Sql Server version. By the way, Sql Server 2008 has been out of mainstream support since Q2 2014 (and this is assuming you are running the latest SP4 patchlevel). You really should consider upgrading into more recent a version of Sql Server. 

Not at all. A SAN snapshot is a bitwise copy of raw disk state. If and only if all the responsibility for business continuity is on SAN storage team, this might be acceptable. For most of the business cases, this will present such RPOs and RTOs that the solution is not going to be feasible. What's more, restoring a SAN snapshot to a stable state is problematic. If the server was up and running during the snapshot, the server will return to state like someone hit a reset button. That is, a dirty filesystem, incomplete transactions and stuff. To get a clean snapshot, the server must be shut down a priori - and that's a business outage. With SAN snapshotting only, you'd miss at least point in time recovery, as all the databases must be in simple recovery model due missing log backups. This is a killer for most recovery objectives. Denny Cherry has written a whole article about failure points. 

Create a user account for application usage and grant it appropriate rights to the database objects. Create stored procedures to handle all the required data operations. Use to run those procedures as the application user. Grant your users rights to execute the procedures. Revoke other permissions from the users. 

Use Sql Server's Configuration Manager to check for any typos on the path too. Mind especially extra spaces in the path, which are easy to miss. 

You could use a sequence for generating the values for ID fields. The documentation explicitly mentions your use case. The example C on said page has a sample implementation about this. 

Please do not change Sql Server's TCP port unless you know what you are doing. This needs to be done only in specific scenarios like manual SPN registrations with failover cluster setups or restricted environments. Sql Server installations contain a special service called Sql Browser Service. When a client tries to connect to a named instance, it will ask connection details from the browser service. As per the documentation: 

Deleting data is easy from the syntax point of view, but that's not all there is. might give you a few nasty suprises. Constraints might block you from doing a delete. A textbook example is a foreign key constraint between customer and order tables. Deleting a customer is not allowed as long as there are orders. If it would, you would end up having orders not linked to a customer. In order to overcome this kind of limitations, you must understand the database schema and start deleting from the right table first (or rely to constraint to use .) Don't underestimate the performance effects. If you are lacking proper indexing (or have overextensive indexing), deleting records might generate serious a performance hit by creating lots of unnecessary index updates and/or full table scans. In addition, write locks will cause overall performance hits for any operations from the affected tables. Transaction log management is a common gotcha. Pruning a lot of data will generate lots of transactions and you log files will go sky high. As a best practice, break the delete operations in batches. Aaron's blog has detailed discussion about the issue. 

Sql Server doesn't really support regular expressions too well. If all you need is to pick a substring from the beginning to the second dot, combine and , like so: 

I've tested Buffer Pool Extensions on an Amazon EC2 virtual machine. The VM has a single non-persistent high-performance SSD. On VM shutdown the SSD contents are lost, so it can't be used to store business data. In order to leverage the SSD, both tempdb and buffer pool extensions were configured to use it, not unlike what was done on Azure (save the Task Scheduler-Powershell hoop jumping). Some 100 GB BPE was allocated, tempdb data files and logs are currently around 20 GB. The system has been running nicely for about a week. Yesterday, for no apparent reason, buffer pool extensions were disabled. None of the admins admit changing the configuration. It looks like the default trace doesn't record BPE changes, ditto for the system_health Extended Events. Unless I missed something, would there be a way to see what triggered the change? All the Application log and Sql Server's log contain is an entry that states the pool was disabled: 

If you are worried about unauthorized data modifications, consider implementing an auditing strategy. As far as I know, there are no documented features that are of much help for post-change ad-hoc change detection if no auditing has been implemented. One, tedious way would be to restore a previous version of the database to a different a name. Script both the old and current databases into text files. Pick the relative complement of the scripts and you will get the changes. If there has been multiple changes on same a row, only the final result is seen. That being said, there are 3rd party tools available that can compare databases. There are tools that claim to be able to parse transaction logs too. ApexSql and Redgate seem to be popular picks. Whether any of these tools are any good, I cannot say. In addition, there is an undocumented fn_dblog function, which could be used to read transaction logs. 

I wrote some time ago a Powershell script that splits a huge SQL script into more manageable chunks. (Reason for this was to export whole database from 2008R2 to a 2008 system in different an organization.) Smaller files can be executed with Management Studio or by running . The script is below. It uses 25000 rows for limit, which in my test case creates sql files about 6 Mb each. Adjust the to suit your particular needs.