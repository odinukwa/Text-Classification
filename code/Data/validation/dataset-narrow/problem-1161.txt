This is not an answer but maybe this will point you or someone else in the right direction. I found the paper by D. Kozen and S. Zaks called "Optimal Bounds for the Change-Making Problem" wherein they give conditions for when a coin change instance's greedy change making algorithm is optimal. I will use their notation. 

EDIT: (2014-08-08) As Douglas Zare points out in the comments, the argument below, specifically the 'bridge' between the two probabilities, is incorrect. I don't see a straight forward way to fix it. I'll leave the answer here as I believe it still provides some intuition, but know that $$ \Pr(E_m) \le \prod_{l=1}^{m}\Pr(F_l) $$ is not true in general. 

Though I'm having trouble finding the exact exponent of the power laws they purport to exist, Kroc, Sabharwal and Selman have a paper called "An Empirical Study of Optimal Noise and Runtime Distributions in Local Search" where they show WalkSat (a local solver for SAT) has power law tails in the runtime. If the power law does not fall within the range of $(2,3]$, then obviously this isn't quite what you're looking for, but maybe it's still of some use. It looks like Gomes, Selman, Crato and Klautz also have a paper called "Heavy-Tailed Phenomena in Satisﬁability and Constraint Satisfaction Problems" though from a quick spot check it looks like they're reporting the exponent in the power law in the range of $(1,2)$ which would violate your finite first moment condition. Also Gomes, Selman and Crato published a paper called "Heavy-Tailed Distributions in Combinatorial Search" which might be of some relevance. From what I understand, the sporadic nature of runtimes (for example, because of a power law behavior in running time) gives one a good reason to use random restarts and might even inform you as to when a restart should occur. Figure 8a in the paper "Heavy-Tailed Distributions in Combinatorial Search" gives two run time plots, one without restarts and one with, where the algorithm with random restart clearly wins out. 

Each point represents the average of 10000 instance creations for $m$ shown and each element chosen to be distinct but otherwise uniform and at random from the range of $[1 \cdots N]$. Given that we know the probability of the greedy algorithm being optimal for $m=3$ goes as $\frac{8}{3} N^{-\frac{1}{2}}$, from just looking at the graph I would hazard a guess that the probability of the greedy algorithm being optimal goes as: $$ p_m(N) \propto N^{-\frac{(m-2)}{2} } $$ where $p_m(N)$ is the probability that $m$ distinct coins drawn uniformly at random from the range of $N$ is greedy optimal (otherwise known as 'canonical'). In the large $m$ limit, the probability that the greedy solution is optimal goes quickly to 0 for any non-trivial value of $N$. If the above equation holds, then it is easy to see, but there might be other ways of looking at it that give the same conclusion. For example, looking at Borgs, Chayes, Mertens and Nair's Random Energy Model work indicate that the energy is too jagged at the bottom to expect local moves (i.e. greedy moves) to give an optimal solution. This is of course for the Number Partition Problem and is only provided to give some intuition rather than a definite answer. At the risk of answering a question that you did not ask, I wanted to point out that "real world" coin systems do not follow a uniform distribution for coin denominations. For example, the USA has at least 12 denominations (including bills: $ (1, 5, 10, 25, 50, 100, 200, 500, 1000, 2000, 5000, 10000)$ ) which do not appear to be uniformly distributed. Perhaps looking at other distributions to generate the coin denominations would yield non-trivial results in the large system limit. For example, a power law distribution might yield coin denominations that are more similar to the USA's. 

The Pigeonhole Version of Subset Sum (or Subset Sum Equality). Given: $$a_k \in \mathbb{Z}_{>0}$$ $$\sum_{k=0}^{n-1} a_k < 2^n - 1$$ By the pigeonhole principle, there must exist two disjoint subsets, $S_1, S_2 \subseteq \{ 1, \dots, n \} $ such that: $$\sum_{j \in S_1} a_j = \sum_{k \in S_2} a_k $$ The pigeonhole subset sum problem asks for such a solution. Originally stated in "Efficient approximation algorithms for the SUBSET-SUMS EQUALITY problem" by Bazgan, Santha and Tuza. 

This book does not explicitly mention parallel algorithms, but Yap's book "Fundamental Problems of Algorithmic Algebra" is a very good reference and discusses the complexity of many Linear Algebra questions. There is a chapter specifically on Linear Systems discussing the time/bit complexity of determinant calculation, matrix inversion, Hermite normal form algorithms, amongst others. The book also deals with complexity of multiplication, Grobner bases and Lattice Reduction techniques (such as LLL). I can't recommend it enough and I bet you'll find something of worth therein. 

If I've understood your question correctly, this relates closely to phase transitions of NP-Complete problems and determining where the transition point is. If you're not familiar, the idea is to take an ensemble of NP-Complete instances and choose random instances based on a parameter. Tuning that parameter will land you in the 'easy', 'hard' or 'middle' region, where 'easy' means the probability of solution is almost surely 1, 'hard' means the probability of a solution is almost surely 0 and the transition point is somewhere in the middle. For example, choosing the ensemble of Erdos-Renyi random graphs with the average degree as a parameter for the Hamiltonian path problem. Low degree leads to almost surely no Hamiltonian cycles whereas high degree yields a Hamiltonian cycle almost surely. When increasing the average degree (somewhere around 2, say), there is a rapid transition, the so called phase transition, from almost surely no solution existing to a cycle almost surely being present. A good introduction is the article by Brian Hayes "The Easiest Hard Problem" and Cheesman et all's "Where the Really Hard Problems Are". Searching for "NP-Complete phase transitions" should give you plenty of material. The common folklore is that "hard" problems, that is, problems that are difficult to solve, happen right in the middle of the transition point, where you'd expect maybe something like probability $\frac{1}{2}$ of finding a solution. This intuition turns out to be wrong and there should be a distinction between the probability of finding a solution and the difficulty of finding a solution. For example, for Hamiltonian cycles in Erdos-Renyi random graphs, there's provably almost sure polynomial times to determine whether a graph is Hamiltonian or not, even precisely in the middle of the transition point. This doesn't mean random graphs are easy, it just means that a particular distribution (the Erdos-Renyi distribution for random graphs, say) is easy and that another should be chosen (maybe on graphs whose vertex degrees are power law degree distributed, say) if intrinsically difficult instances are desired. To me, choosing random instances of NP-Complete problems is "natural" in the sense you use above. I think it's accepted at this point that all NP-Complete problems have a phase transition (on some "natural" parametrization of an ensemble). Once you have an ensemble to choose from and you believe a phase transitions exists, you can tune the parameter to wherever on the curve you like to find the desired proportion of solvable instances you want.