The Problem To compute the sum in equation (1) i need to add the gradient to each $\frac{\partial D_c}{\partial q_k}$. Therefore I need to access $D_c$ in each instance, compute the derivative and add it to the instance $\frac{\partial D_c}{\partial q_k}$. This is done in a Fragment Shader which has as input the figure 1 (as GL_TEXTURE_RECTANGLE). Since each instance $i$ does not know where to find the raw depth image ($D_c$) in the texture, an additional input with the x- and y-offset from the current pixel/texel position to the according pixel/texel position of the raw depth image is provided: Let's call it $O_x$ and $O_y$. So to get the coordinates of the raw depth image $D_c$ in the Gradient-Texture $G$, we do: $$ x_D = x + O_x(x,y)\;\, \;\;\;\;\;\;\;\; \\ y_D = y + O_y(x,y)\;, \;\;\; (2)$$ where $x$, $y$ are the current pixel coordinates (gl_fragCoord) in the Fragment Shader and $x_D$ is the x-coordinate of the raw depth image $D_c$ assigned to the Gradient-Texture at $(x,y)$. The Offset-Texture $O_x$ is shown in the image below. 

Figure 4: Raw depth $D_c$ extracted from the Gradient-Texture $G$ using the computed Base-Coordinates 

The Question And finally my question: Does anyone knows where these artefacts come from? As you saw in the figures above, the steps before give corret results. Only the access to the Gradient-Texture with 

The Code To provide useful feedback I assume you'll need the Code. In the following you'll find the Fragment Shader code. The variable GradientsTexture contains $G$ and 

causes unexpected results as shown in figure 4. Thanks in advance and sorry for the long explanation. Cheers, Christian Bloch. 

Figure 3: The Base-Position $x_D$ of the raw depth image for each instance. And now when I access the values of the Gradient-Texture with the computed Base-Coordinates $$D_c = G(x_D,y_D)$$ I get a fault. The result is shown in Figure 4 below. I would expect each instance to show it's assigned raw depth $D_c$. But as you see there are black artefacts. Afterwards I would have to compute the gradient image $\nabla D_c$ but I stopped here since there is obviously a fault. 

THE LONG VERSION Basically my problem deals with texture access in OpenGL's Fragment Shader. First I'll describe the background of the problem and what I want to accomplish. Afterwards I'll explain the point at which my problem arises. And finally I'll present a few lines of code followed by my actual question. If you're not interested in the details you can skip directly to the problem. But maybe the detailed explanation is useful for others. 

I am sure that less rays reach the light cause I have been calculating them in both cases: small area light and big area light. I also tried to compensate the dark image by increasing the light contribution when reached by the path, but it brings to white pixels (for paths that reach the light) and the black ones (cause of invalid path) keep remaining black, so not a good approach. It seems that (as far as my understanding arrives) the target framework applies direct lighting, that's why such a good image BUT according to theory direct lighting only decrease variance, reducing the noise.. which means that using or not direct lighting shouldn't change the resulting image so much. Is there a way according to you to fix this problem? Thanks in advance. 

Everything fine so far and for any implementation detail you can just look at my code but problems come with my Gradient Domain Path Tracer. Indeed, there, as also in the reference link above implemented by Tzu-Mao Li, I need the final probability density of a whole path to compute the final gradient image. How did I calculate it in case without Next Event Estimation (NEE) ? In that case (since I have only Diffuse surfaces) this probability is the product of CosTheta / PI at each bounce in the scene. Everything is fine and the resulting gradient image is shown above. Instead, in case I use NEE things don't work anymore because the Probability Density of my whole path changes and I don't manage to understand how it is. The resulting Gradient Domain Image with Next Event Estimation is: 

I need to understand how to calculate the final density probability of a path. Can you help me doing it? Thanks in advance! 

As far as I understand (please correct me if wrong), this implements a uniformly random sampling. Also, as far as I have read, this kind of sampling has a PDF = 1 / (b - a) . My question: do I have to use this PDF as well like I do for my Cosine-weighted Random Distribution or not, since it is a uniform distribution? if yes, what's the range (b - a) that PDF talks about? Thanks in advance! 

You can't see it in the code because it gets simplified witht the cosTheta attenuation term of the rendering equation and PI of the diffuse BRDF where my BRDF is indeed diffuse/PI because we want to consider only DIFFUSE objects. (For any question about the code do not hesitate to ask.) I tried to emulate this code with the difference that I don't calculate the path at the moment I want to calculate the color of a pixel (like the target framework does above), but I calculate a path beforehand: My generatePath() method indeed tracks the path into the scene and checks all the vertices it hits. The checkIfRayIntersectSomething(t) method you will see used, it's just a pseudo method implemented in my framework and that I omit posting cause of its length. I use it to check if my ray hits something in the scene, if it does, it update the "t" with the distance to that object. NOTE: the light is not considered an object itself. Hence, I also have a checkRayLightIntersection(hitLightPoint) which checks the intersection with the light, if there is any, the hitLightPoint is updated with the point on the light I have been hitting. The light is a 2D surface of area 2x2 placed at the same position (-1,20,9), as the target framework does. First the definition of a couple of struct to store some info: 

THE SHORT VERSION In my fragment shader I'm reading (using texelFetch) multiple times the same texel from a texture (created by another fragment shader) and write it to the output render buffer. After a few read operations on the texel a fault occurs. The texture and my rendered image have a size of (n*256) x (m*192) pixel/texel. In the first instance (texel [0,...,255]x[0,...,191]) the texture contains a human. Here is the minimal code of the fragment shader which should copy the human: 

Figure 1: The raw depth images and depth gradients with respect to parameters. (referred to as Gradient-Texture $G$ ) 2. The Image Gradient The image gradient is well known and has to be computed by applying the convolution operator $\left[-1/2, 0, +1/2 \right]$ to $D_1$ and $D_2$. By doing this in x and y direction we get: $$ \nabla D_c = \left[ \frac{\partial D_c}{\partial x}, \frac{\partial D_c}{\partial y} \right] $$ Summation In the end I have to sum up the two derivatives: $$ \nabla D_c \frac{\partial \vec{x}}{\partial \vec{q}} + \frac{\partial D_c}{\partial \vec{q}} \;. \;\; (1)$$ Where $\frac{\partial \vec{x}}{\partial \vec{q}}$ is the change of the image coordinates of a projected point on the model with respect to a parameter. This becomes a constant for a orthographic projection as it is applied here. 

As you can see, there is an Problem after the third instance. Additionally I noticed, that the position of the black boxe/stripes varies randomly. Do you have any idea how this problem could be fixed? 

The Background Basically I want to compute the partial derivatives of a rendered depth image according to equation (15) in this paper Wei, Xiaolin, Peizhao Zhang, and Jinxiang Chai. "Accurate realtime full-body motion capture using a single depth camera." ACM Transactions on Graphics (TOG) 31.6 (2012): 188. Derivatives In this paper there are two important derivatives which has to be computed: 1. The Parameter Gradient The change of depth value $D$ with respect to a parameter $q_k$ which moves the skeleton: $$ \frac{\partial D_c}{\partial \vec{q}} = \left[\frac{\partial D_c}{\partial q_1},\; ...,\; \frac{\partial D_c}{\partial q_k},\;...\right] $$ But since I'm using multiple cameras, I've different depth images of the same model. In the example here two cameras are used ($c=1,2$) and therefore we have $D_1$ and $D_2$. This gradients can be seen in the image below. I'll refer to this image as Gradient-Texture $G$. Here we see multiple instances of the same model. The first instance (0) in the upper left corner is in contrast to the other instances not a derivative but the raw depth value $D_1$. The second instance is the depth change with respect to a rotation of the whole model around the x-axis (which is parameter $q_1$)($q_7$ is the angle of the upper leg). After all $\frac{\partial D_1}{\partial q_k}$ are computed we proceed with $c=2$ and see $D_2$ and so on. 

It's rendered with 16SPP. The core code behind it is quite straightforward. It just implements the rendering equation and it is shown and commented below: 

I am trying to implement my own Gradient Domain Path Tracer by following the code of this guy who already implemented it: $URL$ I already managed to go through various steps but I wanted to do something more. I expanded the code of in the reference by implementing next event estimation and here are some results. Normal Path Tracer image: 

I am trying to implement my own path tracer but before arriving to the question I want to give you a short overview: In the implementation of the rendering equation I use some particular technique in order to sample surfaces. For example: when one of my rays hits a diffuse surface, the next ray bouncing from that surface will be calculated using a Cosine-weighted Random Direction. This implies that in my rendering equation I have to take into account the PDF that this specific random distribution implies and specifically divide for this PDF in my equation. So far everything is okey. Now, my question. I want to implement a particular technique called "next event estimation" which simply samples the lights. In order to do so, I want to pick a random point on my light, which is spherical, by using the following code (C++): 

The DiffuseReflectionCosineWeighted implies that we have to apply a PDF to our recursive step. This PDF is cosTheta/PI and we have to divide for it in: 

Results are already good.. but as said before, I wanted something more. So I implemented Next Event Estimation and here is the result of the basic Path Tracer: 

Things work really fine and the results are shown with the picture above. One more thing: I have only diffuse surfaces in my scene. Now, the problem is that in this method I use 2 kind of PDF's: 

This is because not many rays reach the light, so many paths are not considered valid. If instead I make my light broad (20x20) results are better but I lose the shadows: 

The BSDFDiffuseReflectionCosineWeighted() just calculates the new direction like in the target framework, tested and working. What remains last is the Sample method which calculates the final color of the pixel using the path calculated right above: 

The problem is that the image is really dark, you can see how small the light is, but somehow in the target framework it works correctly while in mine: