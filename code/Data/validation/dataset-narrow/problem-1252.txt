It depends. Evolutionary algorithms are highly sensitive to the state space of the fitness function. In general crossover gives you nothing better than flipping a random coin. On some fitness functions like maximize the number of 1 bits, crossover speeds convergence at an amazing rate. If you know little about your fitness function, keep track of how many times crossover(A,B) has yielded a fitter result than A or B. Report this result as part of your analysis of the fitness landscape. 

$U^L$ and $Bool^{L^{L}}$ if you convert to exponentials. Just examining their type signatures putting graphs in canonical form looks easier, but as shown above GC makes GI easy. 

I would focus your research in the "devops" space until you find a hard problem that could lead to a good theory question. Many configuration files out there need property checkers over tree structures. Pay particular attention to the Curry/Uncurry in the the third chapter of Drewes lecture notes. I think that is the kernel of an idea to make Tree Automata much more useful. $B^{A}$ counts functions from an $A$ set to a $B$ set. $C^{AB} = C^{A}C^{B}$ is an algebraic identity you learn in Jr. High. For functions it means you can pass in a touple of arguments, or pass a touple of functions each taking one argument. Simple, but very useful in refactoring software. Space is another issue. Succinct representations of trees is very efficient, but rarely used in databases. You easily get 20x speedup by using succinct trees. Also anything in Celko's book. Worst SQL project I ever had was dealing with a DAG in a database. Lots of open research questions on how to add tree operations to databases. Blass (1994), Seven Trees in One Fiore (2004), Isomorphisms of Generic Recursive Polynomial Types Drewes (2009), Lecture Notes on Tree Automata Celko 2012, Trees and Hierarchies in SQL for Smarties Sadakane 2011, Succinct Trees: Theory and Practice 

I computed the black box number of permutations you have to test to prove the only automorphism is the identity permutation. $URL$ If you can inspect the structure of the graph you can solve it much faster in practice. 

The search keywords are "dynamic all pairs shortest paths". Demetrescu and Italiano have a good survey, $URL$ Looks like $O(n^2 log^3 n)$ cost per update if you want to maintain fast lookups. 

Fun with Semirings: A functional pearl on the abuse of linear algebra Efficent Divide and Conquer parsing of context free languages 

In 1978 Willem H. Haemmers published "An upper bound on the Shannon capacity of a graph". Tims has a survey of more recent results his thesis. What is the computational complexity of computing Haemmer's minimum rank function? 

One popular metric is edit distance. Although for your problem, it seems close to automata intersection. You start with the empty string then accept a word who's symbols match a path in your graph for both G1 and G2. Their intersection contains only their shared words. 

At worst you have to check one element of each repeated prime cycle, $URL$ Smaller than n! but still large. 

Lattices are one abstraction for eventually consistent distributed computations. $URL$ How you would usefully apply LLL in this context would take some category theory foo to abstract it. 

What goes in that function is arbitrary. In practice this is both a blessing and a curse. You have the freedom to recycle the buffer, but you are burdened by losing bits of the input as you overwrite them with the output. 

I would memoize the partial products: {.}{..}{....}{........}{....}{..}{.} On the incoming end you combine them as they grow to a size equal to their left neighbor. On the outgoing end you spit them in half until you get one element. Amoritized work is log(n) per update to maintain the total product. 

There is a paper by Ritt from 1923 that calls the relation, $f(g(x)) = g(f(x))$, permutable functions. Is there a more recent terminology used in the literature, or is this still the standard? 

A co-Relational Model of Data for Large Shared Data Banks by Erik Meijer and Gavin Bierman, $URL$ Good article describing SQL and No-SQL databases as categorical duals. 

You can put a tree into a cannonical form in $O(n)$ with leaf contractions. Read, Ronald C. (1972), "The coding of various kinds of unlabeled trees", Graph Theory and Computing, Academic Press, New York, pp. 153–182 When $T_{1}$ and $T_{2}$ are isomorphic you can solve it in $O(n)$. Shamir's paper has $O({k^{1.5}\over log\ k}n)$ for testing if one tree is a subtree of the other, $URL$ Not sure if that helps. 

This is very similar to overlaps of gene sequence in genome assembly. Chapter 4 of Ananth's thesis. In parallel you search for promising pairs and maintain a distributed union find data structure. See Tarjan and Vishkin for their hook and shortcut algorithm to collapse the connected components. Also, you can try recent DeBrujin graph methods on 64 bit row chunks of pixels. I think this will give you the best results. To help with quantization problems I would first reduce dimension of the pixels to 16 or 8 bit black/white. You then apply a parallel sort the 64 bit chunks, and then use them to infer edges between images. 

In place means you overwrite the memory location of the input with the output, and you are allowed to cannibalize this buffer for use in the computation. 

Fastest protocol I can think of is alternate exchanging a random adjacent pair of elements, throwing away stuff that gets skipped over. Alice[1,10,26,49,50] Bob[2,3,25,49,51] Alice adjacent pair is [10,26] so Bob throws away 25 Alice[1,49,50] Bob[2,3,49,51] Bob adjacent pair is [3,49] match on 49 so halt 

Get Spinrad's book on efficient graph representations: $URL$ Also check out Li and Vitanyai's book on Kolmogorov Complexity: $URL$ You will get an appriciation for each graph class by studying succinct data structures. When you understand why certain classes of graphs take less storage than others you gain a great understanding of how to tailor optimization problems on to them. 

A cheap approximate solution using a "locality-sensitive hash" would be to convert each point to it's bit interleaved form: [xxx,yyy,zzz] -> xyzxyzxyz then radix sort for preprocessing. Pick your point to query on and go $k$ points in both directions to get a size $2k$ set; then take the $kth$ nearest to your point. Also see this paper by Connor and Kumar. Also see this paper by Callahan and Kosaraju. 

Hook and shortcut from: AN EFFICIENT PARALLEL BICONNECTIVITY ALGORITHM Tarjan and Vishkin $URL$ This Ruby implementation returns the partitions of a transformation where elements x,y interact with each other. 

I learned from the school of Ashlock. The big take away I got was how useful it was to take the $n^2$ table of outcomes between agents and use K-Means to cluster the rows into strategy groups for analysis. 

Tree isomorphism is polynomial time. Reed, Ronald C. (1972). "The Coding of Various Kinds of Unlabeled Trees". Graph Theory and Computing: 153–182 

The smallest set of permutations you have to check to verify that no non-trivial permutations exist in a black box setting is better than $n!$ but still exponential, OEIS A186202. The number of bits needed to store an unlabeled graph is $log_{2}$ of ${n\choose 2}-n log(n) +O(n)$. See Naor, Moni. "Succinct representation of general unlabeled graphs." Discrete Applied Mathematics 28.3 (1990): 303-307. The compression method proof is a bit cleaner if I recall. Anyway, lets call that set $U$. Let $L=2^{{n\choose 2}}$ for labeled graphs. 

Here, I cranked out the black box optimum to detect any subgroup of $S_{n}$, still exponential but much faster than brute force over all $n!$ permutations. Your problem is around $O(n \log rb)$ with repeated doubling if you have $g$ given to you as a permutation in $S_{n}$. 

I cut my teeth on Combinatorial Algorithms Generation Enumeration Search, $URL$ I would highly recommend it. You learn much faster coding group algorithms as by hand examples break down really quickly. Also grab a system like Sage or Magma to play with to use as a bench calculator. 

Exact classical bounds are known, $URL$ , you only have to sample certain prime cycles as they form a min dominating set on $S_n$ under a detection relation. Smaller than $n!$ but still about the order of $p!$ were $p$ is the largest prime less than or equal to $n$. 

Pick one at random. If it is random, then there is no way to compress the representation. You have to store the entire truth table. $URL$ 

Not the answer the author was seeking, see comments that clarify what "permutation" is in this context. I cranked out the size of the minimum dominating set for the monogenic permutation group inclusion digraph: $URL$ All you have to do is test one member of every prime cycle decomposition. For each prime cycle it should be sufficent to code the elements as (10101010...), then (01010101..)? ------Clarification------ The goal of this approach is to model your 2^n test cases as a digraph. If one sucessful test case implies another sucessful test case, then you only have to test the min dominating set of this test space digraph. In the space of permutations OEIS A186202 is the max you have to test to either detect a non-trivial subgroup or prove none exist; this number is still large but much smaller than n!. --Musing-- By using n-1 zeros and 1 one in n iterations you can detect the fixed permutation you are looking for. After that in O( n{(n-1) \choose (k-1)}(2^(k-1)) you can test that every set of (k-1) variables does not effect each index of the shuffle. Since k is fixed that is polynomial. Am I missing something?