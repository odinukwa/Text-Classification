Let the equation of your segment be (x0 + t(x1-x0), y0 + t(y1-y0), z0 + t(z1-z0)) where Segment.Start(x0,y0,z0) and Segment.End(x1,y1,z1). For simplicity, let's forget about v, w and all those letters that were used until now and call x1-x0 = u, y1-y0 = v, z1-z0 = w. That means that a POINT on the segment is written as vec3(x0+tu, y0+tv, z0+tw), where t should be in the [0,1] interval! Remember the condition for a POINT to also lie ON the SURFACE of the Cylinder? Now we get: -- (x0+tu)^2 + (y0 + tv)^2 = r^2 which is equivalent to -- (u^2 + v^2)t^2 + 2(u x0 + v y0)t + x0^2 + y0^2 - r^2 = 0. Let A = (u^2 +v^2), B= 2(u x0 + v y0)t, C= x0^2 + y0^2 -r^2. You end up with a quadratic equation. Delta = B^2 - 4AC. 

You need to have an orientation defined for your object/entity. This is usually an unit vector in 3D, v=(vx,vy,vz). If you have an object oriented bounding box, then you can use that to determine where the "bottom" part of your object is. If not, just consider a "safety" distance/radius from the center of your object. Say this distance is d. Then one can simply find where the circle's center is by using the object's center, the v direction and the d distance: 

Forward kinematics is just a prerequisite for inverse kinematics, in general. For forward kinematics you must have some knowledge of transform chains (what a scene-graph means for example). As an example, a good point to start from is the Denavit-Hartenberg convention. For inverse kinematics, study Cyclic Coordinate Descent (CCD), Jacobian Inverse, Jacobian transpose and pseudo-inverse solutions. These concepts and algorithms offer you a decent overview of what the mathematics and partly the physics of this field is all about. The library, programming language or file formats are never the key element if you also want to understand what happens behind the magic. If you need a quick solution or a component for some application you're working on, then do take a look on some of the libraries provided in the other answer(s) here. If you're into robotics, then OpenRAVE is a monster worth mentioning. Also, there is a Matlab Robotics Toolbox (provided you're into that kind of engineering side of the matter). All of the concepts yield a truckload of relevant results on your favourite search engine, so sorry for not posting that many links to nice materials. 

Hmm, when you update a point, e.g. your center, you destroy the information! Here's a snippet to illustrate my point: 

How to draw the circle? It might seem like a difficult task. In 2D, that's trivial: circle(x,y,r) = (rcos(u) +x, rsin(u) + y), where u between [0, 360] degs. In 3D you can draw it in the xOy plane, via its parametric equation: circle(0,0,0,r) = (rcos(u), rsin(u), 0) What to do next? Rotate the circle in such a way that its local Z(up) axis points in the same direction as the v vector. How? Find the rotation that aligns Z with v: 

Let v = normalize(Cylinder.End - Cylinder.Start) Compute w = cross(v, vec3(0,0,1)). If w's not the null vector, you have to perform a T transform for both Cylinder and Segment to align the scene with the oZ axis: T = Rotation(w, arccos(dot(v,z)) ) x Translation( - Cylinder.Start) Use this transform to find the coordinates of the transformed Cylinder and Segment: Cylinder2 = T x Cylinder1, Segment2 = T x Segment1. For simplicity, I'll call the results Cylinder and Segment. 

Nevertheless, disregarding the remarks, is there an already known scenario similar to this one where a triangulation is built upon a partially given set of triangles/edges? 

Consider the above picture to be your water surface given as a quad. When you prepare it for shader drawing, you must supply a texture to it (if you don't want to colour it solidly). When you bind a texture to it, for that to work, you mus associate to each of the vertices an attribute containing the texture coordinates. This is the so called uv set and, if there's no tiling involved on your quad, these could be given as A(0,0), B(1,0), C(1,1), D(0,1), where A, B, C, D are the vertices of your quad. in the vertex shader compute the position as you normally would and pass the uv you get as an attribute input variable to the fragment shader through a varying variable. The other, simpler way, could be to pass the object frame vertex coordinates (the positions prior to their multiplication through the worldvieprojection matrix). The varying descriptor will make sure the fragment gets an interpolated position on the quad. Let's call this variable xy and pass it to the frag shader as said. in the fragment shader Pass as an input uniform the world transformation matrix because you'll have to transform the computed normals to align with your lights position. Alternatively, pass the light already transformed in the object's own frame of reference and use the directly computed normal (should be quite faster and more elegant). xy is a 2D vector and we need a 3D point on a water surface. Gerstner waves associate to a flat rectangle a height for each of its inner points. If you then consider the union of all of those xy displaced via that height function, you get the wave surface. We can't discuss positions in the frag shader, but we can compute and use normals for other purposes. The normal you are looking for can be computed from the xy pair following this rationale: $URL$ . For the waves to be animated, make sure to pass a time instance as an input uniform to the fragment shader. It is required as an input for the H(x,y,t) height function. That should be all. The rest is just plain lighting computation in a frag shader. 

The scenario A chain of points: (Pi)i=0,N where Pi is linked to its direct neighbours (Pi-1 and Pi+1). The goal: perform efficient collision detection between any two, non-adjacent links: (PiPi+1) vs. (PjPj+1). The question: it's highly recommended in all works treating this subject of collision detection to use a broad phase and to implement it via a bounding volume hierarchy. For a chain made out of Pi nodes, it can look like this: 

can be anything: gravity, - a drag force or even a constant if you plan on having a rocket boosted hockey puck. Note that is used to geometrically reflect the velocity, but also to "dampen" it as an effect of an imperfect elastic collision. If you don't want to lose energy, just use instead for perfect reflections. For a sliding behaviour use this trick: 

If the integrator preserves energy, the phase diagram will basically look like an ellipse, whereas if the integrator loses or gains energy, the phase diagram will resemble a spiral. Recommended reading To better understand what's with this stability and symplecticity fuss, one of the best web resources for viewing the effects of these implicit, explicit and semi-implicit integrators is here. 

Mathematical explanation with readable symbols and LaTeX format - great for 1st or 2nd year undergrad students or skilled high-school people. Well, if you need to understand the maths behind it rather than copy-pasting some code that might not be the good one (it's not clear whether it will work or not judging from the SO code). Here are the steps: 

This way, you make sure the newVelocity has the same magnitude as the oldVelocity, hence no energy loss, hence no "slowing down". Collision with a circular rink/wall Refer to the picture below: 

An example: Say you have a ship and a camera object in a simple scene. Let Qs be the ship's current quat and Qc the camera's current quaternion. Let this configuration Qs,Qc be the initial, reference position you are talking about. If the ship changes its orientation from Qs to Qs', then you could write: Qs' = Qs' * inv(Qs) * Qs Qs' * Qs is a relative rotation transformation. If you apply it to the camera's initial orientation, you get a new camera orientation that should be in tune to your ship's change in orientation, as desired. The inv(Qs) factor tells us that there is an intricate orientation coupling that is initially set, between our camera and ship objects. inv(q) denotes the inverse of a quat, which is the same as its conjugate q* if the quat is normalized. Further explanation: To get a result, the camera's orientation, when the ship changes heading, should look like: Qc' = Qs' * inv(Qs) * Qc Be aware that the _ operator is the quaternion multiplication. When rotating vectors, you will multiply the orientation quat q with (0,_v_) and then right multiply it by q. 

Refer to the following picture: Say we are at instance t and we want to get the velocity and position of the entity at time t+Dt. The simplest and crudest pseudocode comprising collision detection, handling and naive Euler integration would then be: 

My idea is to use a camera for each window and have a reference position and orientation for a meta-camera object that is used to correctly offset the other cameras (e.g. like in the above figure where the render targets of the two cameras reproduce the star when stitched together). Since there are quite some elements to consider (window specs, viewport properties, position-orientation of each render camera), what is the correct way to update the individual cameras considering the position and orientation of the central, meta-camera? I currently cannot make the cameras present the scene contiguously (and I am reluctant in working out the transformations without checking whether this is the actual way of doing things). 

I am not asking how to implement them (that's not particularly difficult), but I'm having a bit of trouble understanding why the moments used in Chebyshev's inequality (indirectly) are equal to the depth value and the depth squared respectively. To grasp why things are like this, I tried to understand what the random variable one computes the moments for is, and what is its density (the in those integrals used to compute the moments). Now that one can equate the moments to the depth and depth^2 is the magic behind this approach, hence the key of justifying it mathematically. Can anyone please explain what is, and why is the first moment equal to the value? I can probably pick up from there and easily understand the second moment. 

Of course, the correct way to achieve rolling behaviour from a physically accurate POV is to use the inertia tensor. I implemented a short OpenGL demo using the following snippet to update the rolling sphere (and it worked): 

Conclusion: since a UDP implementation could outperform (by a factor of 3x) a TCP one, it makes sense to consider it, once you've identified your scenario to be UDP friendly. Be warned! Implementing the full TCP stack on top of UDP is always a bad idea. 

The 0 suffix denotes the initial state. These two quats shouldn't change from now on, they remain untouched. The relative orientation of a target with respect to a source is not the difference of the quaternions, but this guy: Qsrc0.inverse * Qtgt0 (something similar to a division operator for quaternions - but since the quaternion multiplication does not commute, such an operator is "left or right handed"). At a certain time instance, the source has the Qsrc orientation. To couple the orientation of the target to that of the source, set the orientation of the target to be: Qtgt = Qsrc * Qrelative = Qsrc * Qsrc0.inverse * Qtgt0 The picture below illustrates an experiment where a cube gets the camera's quat and a parabolic tube springing from one of the cube's faces rotates with the cube although they are not attached to the same scene node, nor is the cube's node a child of the cube's node (talking hierarchical node transformations in a scene graph). Both objects are linked to the world independently, but when the scene is created, their relative orientation is stored separately for future updates. 

Theoretically, it shouldn't miss any scenario for your polygons, but it's not the optimal solution. "Edge" case remarks 

Problem: I need to quickly build a triangulation around the subgraph (e.g. as shown in figure C), but under the constraint that I have to keep the already present edges in the final result. Question: Is there a fast way of achieving this given a partially triangulated mesh? Ideally, the complexity should be in the O(n) class. Some side-remarks: 

Requirements: to write a test function that, given a moving OBB (oriented bounding box) and a triangle, returns whenever the OBB hits the triangle. The OBB is described by its half extents (), principal axes () and center point . The triangle is described by its three vertices (). The OBB moves with the velocity over a period of time of 1 second (hence is also the displacement the OBB undergoes). Question: I have already read Dave Eberly's mini-paper on the matter and made an attempt at an implementation. Also related to this paper, I have a question regarding the projection of the velocity: - is equal to , where is one of the 13 possible separating axes he considered in that paper? 

March counterclockwise on the edge list. For each Pi-Pi+1 edge, project Pi and Pi+1 inwards via the perpendicular on the Pi_Pi+1 line. Use a simple positive/negative half space check to get the correct inwards direction. ( perpendicular vectors in 2D, positive half plane/negative half plane). Once you compute the normally displace Pi_Pi+1 line, you can check it for intersection with its homologous Pi+1_Pi+2 normally displaced line. The intersection is called P'i+1 and is a vertex of your required polygon. Do note that the set of points that are d units away from the initial polygon and lie inside your polygon do not form a polygon, they form a "rounded corner" polyline. Note the little circle I drew. The arc between the two green and blue radii is part of this set. But the polygon you get is a "decent" approximation. 

Of course, it depends how you store your vectors, but that is a convention (i.e. as rows or columns). That convention is not to affect the rationale of how you can jump from one frame to another passing through the world frame: just take care of matrix multiplication consistency and you'll be fine. 

Update: Tested and confirmed to work: Say you have two objects: a source and a target object. The target must keep a certain relative orientation with respect to a source object. This is really the case with most fundamental camera tracking scenarios (e.g. camera behind a flying entity - plane, spaceship, etc.). If a given reference pose of this binary system is given, say at start-up, then we must grab hold on the orientations of the source and target objects: 

Now, since I'd like to switch or perhaps let different instances of the same deformable object use integration method A and the other use integration method B (for "benchmarking" purposes), is it safe to "think inside the box" and refer to either the Bridge or Strategy pattern? Which is more suitable and less ambiguous, especially performance-wise? What about a self collision controller approach? Should I just add a self collision method to the class or "befriend" it with a Collider class? And what about external collisions, with different objects - how should that aspect be approached, at least at the conceptual level: considering the naive, brute pair-to-pair collision queries, if two objects collide, their points must be updated in the sense that velocities and forces are added to the points' own dynamic quantities (e.g. restitution forces, penalty forces, friction forces, reflected velocities, impulse conservation velocity changes, etc.). I know this is a broad topic, but I don't know which book does tackle these natural problems for a novice programmer. Thanks for your patience in reading this question!