This is the simplest I can make it. If it were me, I would do something a bit more complicated to antialias the meniscus and apply more refraction there or something, but I can't tell you what because it would take aesthetic iterations. Also, I would tint and blur the water sample. But I'll leave all that up to you. 

We use svn at work, but the repository is getting pretty large, so checkout times are long, as is update when artists batch process all assets. The problem is that we also store work in progress, prototypes, small tests, legacy stuff and other 'junk'. Now theoretically, some of it can remain local and uncommitted until it is ready to be included, and a lot could be deleted and cleaned, but that's irrelevant for this question. Imagine a new asset, a level for example. The artists and designers want it shared and in version control so several of them can work on it, but everybody else does not need it. Obviously multiple repositories are an answer: one for artists, one for programmers and one for the main project (for testers, building demo versions and finally the main product), which would be really tight and minimal, with everything needed for the game and no more. And then when an asset or some code is 'mature', it can be copied into the main project and committed. Here's my questions: is it possible to have multiple repositories overlapping so that the artists can have one with everything they need, likewise for programmers. And only when ready commit the same file into the project repository too? This would eliminate bloat of having multiple copies of files and avoid conflicts between repositories. I know old svn versions stored invisible files in all directories, so it would be impossible, but new svn versions store all meta data in the root directory right? So it might be possible. I imaging the answer will be 'impossible', or even if possible, yuck! I suppose it would cause all kinds of headaches. Like checking it out in one repository would mean it would need to be committed/deleted in another. But how about making a svn_commit script, that overrides svn commit with additional checks and housekeeping? This is just a thought experiment really. I was wondering if anyone has ever done anything like this, and what would be the issues to be resolved. Cheers 

When I hear game designer, I think of somebody who thinks up game mechanics, freedoms and constraints, level designs, balance etc. The may well have no part in actually programming the game. What you are is a game artist. That doesn't mean that you can't have more than one role in creating a game, and at the same time each role influences every other role significantly. At any rate, you are a game developer, as are programmers, testers, sound engineers, etc. 

First of all, when you say "creating" a tileset, I presume you mean that you want all these tile transitions pre-baked into textures. I'd recommend against that as the number of transitions you need increases quadratically as your number of tile types increases, and isn't worth any extra efficiency usually. What you want to look into is Texture Splatting and do this all runtime. I can't think of anything in particular that would give you extra efficiency when it comes to cell types, except the usual voxel optimizations like storing voxels in an octree and optimizing that, occlusion culling, frustum culling, LOD, optimal mesh generation etc. On that last one, as the saying goes, premature optimization is the root of all evil, but if you end up having some extra FPS, you could try rendering your cells to look more "detailed" with algorithms like marching cubes / marching diamonds (think the 3D Worms games' destructible terrain). Edit: Here's a good read on voxel terrain meshing. 

As others have already explained, your question is based on a false premise: that most 3D games don't allow you to move. I think what you actually meant is "Why do you control movement in most VR-first games with a controller, rather than with your body?". The answer is not to do with technical limitations, but human factors. For most games, it's completely unnecessary - and indeed makes for frustrating gameplay - to control your character with your body rather than a controller. Nobody wants to clear out their room to play a VR FPS only to get annoyed with wires and obstacles. There would be too many compromises needed (unless you literally had a huge hall to play in) to make it so that the controls don't interfere with the game. That doesn't mean that there aren't games where you can at least move a bit by actually moving your body. The controller is your primary mode of ambulation, but you can still look around corners by moving your head etc. On that front, the commenters on your question are right; most games actually do let you do that. Regardless, your reasoning is still wrong. Think of a VR headset as just a set of double monitors; anything that can be done on your actual screen can be done here too. IMU input from a headset is equivalent to WASD + mouse or a controller. It has nothing to do with caching a scene in VRAM or limitations of your graphics card or anything of the sort. The actual UX pitfalls with VR headsets are resolution, FOV, and latency. People might decide to limit how you control your player based on those for example, to prevent motion sickness among other things. 

So the first iteration has a strength of 1/num_iterations and the last has a strength of 1. This makes my simulations smoother and more stable than simply using the same number of iterations with a fixed strength. 

I am a bit curious, how will you render the geometry once you have it? I ask because clouds are fuzzy and if you just render a bunch of polygons, the clouds will look opaque. I suggest rendering with a volumetric raycasting shader. You can use your marching cubes geometry for the basic shape. Basically, you can render the backface depths/positions to an offscreen buffer and frontface depths/positions to another, and then cast a ray from front to back, summing a noise function (like perlin). Adding lighting is important but can be very slow unless you find a good hack. It is a bit complicated, but it would produce better results than simply rendering the faces of a blobbly cloud shape. 

As mentioned, Euler integration is simple and fast, but not very accurate. It is fairly easy for a stiff system (one requiring large forces to satisfy constraints) to explode in a small number of frames. Choosing an integration method is highly dependent on what it's for. If you are making a simple particle system, there is no point in having variable-timestep, 4th-order Runge Kutta integration. Euler is fine but you NEED some damping. To simulate air resistance, just subtract a small portion of the velocity every frame. To simulate energy loss due to collision, reduce the component of the velocity in the direction of the normal. For a collison with a horizontal floor, this is just the y component. 

You want to go from a planar world, to a cylindrical one. A rotation around the x axis (in homogeneous coordinates) looks like this: 

Everything that is not affected by the difficulty value should be constant and consistent, so that there is just a single variable to tweak for difficulty. This is to make level design and tweaking as simple as possible, which is incredibly important for balancing. It's a bit of a vague answer, but I hope it gives you some ideas. 

However, you don't need to check for all of these. What you have is a decision tree; all you need to check are if the orange ones match your center tile. If they do, then you proceed to check the secondary ones. Here's a diagram that connects the orange tiles to their secondary tiles (notice how you only need to do four checks for the green matches and not 8 since the other 4 are the same): 

Here's how I would do this. First, make sure you have the object's UVs or world coords (which you can pass through from your vertex shader) available to you. If it's just a background, you could also just use fragment coords (). For instance, let's say we're using UV coords. A fragment shader with only: will look something like: 

Of course you can simplify a lot of things and play around much more with the variables and constants. Here's a live demo of me doing just that: $URL$ Edit: If you want to do this with different colors, fortunately there's an easy solution! Pass in your colors as uniforms (or hardcode them in the actual fragment shader) and change the last line to this: 

One way would indeed be to try out the four possible moves for every tile (or rather the ones that have recently moved) and check for matches, then store them as hints. You wouldn't need to do this as often as you might think (especially if you only do it for tiles that have moved); you would only need to swap every tile twice since the one next to it would overlap its two other swaps. Like this: 

Depending on what kind of data structure you store tile data in, these checks can be extremely fast to the point where you don't really need to do any significant optimization. I personally can't think of any other shortcuts. 

The next step is animation. To do this, you just need to play around with the "r" value and change the radius. You don't need to worry about clamping color values between 0.0 and 1.0 because that happens automatically anyway. All together it would look something like this: 

It seems from your picture that you want the path to be straight, which is not what would happen if the ball was subject to gravity. Also from your description, it sounds like you want the ball to come to a sudden stop before accelerating downwards again. Anyway, from what I understand of your problem, here is how I would solve it. If I have misunderstood something, I apologise. I would store position (vector), direction (normalised vector) and speed (scalar). 

First of all, you need to learn the basics of vectors. Just find a good tutorial. It will not take long. Try this. For simulating a physical object: Newton's first law of motion: "An object at rest remains at rest unless acted upon by a force. An object in motion remains in motion, and at a constant velocity, unless acted upon by a force." What this means is that it takes no energy for an object to remain at its current velocity (linear and angular). It only takes energy to change the velocity. (Note: speed is the magnitude of velocity and is a scalar). This energy can come from thrusters, gravity, explosions, collisions, and even light falling on it. Newton's second law of motion: "The acceleration of a body is directly proportional to, and in the same direction as, the net force acting on the body, and inversely proportional to its mass. Thus, F = ma, where F is the net force acting on the object, m is the mass of the object and a is the acceleration of the object." This means that we can calculate the acceleration on a body if we know the total force acting upon it, and its mass. Newton's third law of motion: "When one body exerts a force on a second body, the second body simultaneously exerts a force equal in magnitude and opposite in direction to that of the first body." I won't deal with collisions here, but this means that in a collision both bodies receive the exact same collision force in the same position, but in opposite directions. Once you can calculate acceleration, you can integrate it to find velocity, and integrate velocity to find position. If your object looks like this: 

Yes, step 1 and 2 are correct. Here's proof. Step 3 and 4 can be done in a couple of ways. Following your steps to the letter, you can convert the RGB color to HSV, modify the values, and convert back to RGB. The first conversion is unnecessary though, since the image is grayscale, so we know the only thing that is affected is the "V" in HSV. So you can use your image as V, and use those constant H and S values to get something similar to what you want. Subject to tweaking and experimentation of course. Converting back and forth between RGB and HSV isn't the best idea for something like this though; perhaps in this case - if all you want is a slight blue/cyan tint - then you could instead just slightly tweak the blue and green channels and get the same result. 

This does not necessarily have to be as computationally expensive as you imagine. First of all, as you hinted at, you don't have to check every entity; just the ones that are moving. As such it might be wiser to have individual entities update their grid cell in their update method. As for checking coordinates, there are a few optimizations you could make. For example, say your grid is made up of cells that are 64x64. To figure the coordinates of which cell an entity is in, all you have to do is and (try it). This is faster than, say, a grid with cells that are 100x100, where to find out the cell coordinates you would need to do and and floor the result (or cast into if it's not already). Thus it's more efficient on a larger scale to pick cell sizes that are powers of 2 for example. Although I doubt that this will be a bottleneck (remember premature optimization is the root of all evil), you could also update zones less frequently than on every game tick and maybe even make the rate depend on the maximum/expected speed of any given entity. 

Another way would be to check the surrounding tiles without simulating anything in the background. The best way to do that would be as follows. Say you have a tile that just moved (center). Focus only on its color and the tiles that are occupied in this diagram: