In high dimensions you can use LsH (locality sensitive hashing) for this. In low dimensions, any data-structure for approximate nearest neighbor can answer your question approximately. However, a much easier scheme is to use a grid of appropriate size, and for each point retrieve all the points stored in its grid cell and adjoining grid cells. In low dimensions, [CK95] showed that for each point one can compute its $k$ nearest neighbors in $O(n \log n + n k )$ time. Not quite your question, but a pretty surprising related result. [CK95] P. B. Callahan and S. R. Kosaraju. A decomposition of multidimensional point sets with applications to k-nearest-neighbors and n-body potential ﬁelds. J. Assoc. Comput. Mach., 42:67–90, 1995. 

There is a beautiful theorem of Koebe (see here) that states that any planar graph can be drawn as kissing graph of disks (very romantic...). (Putting it somewhat differently, any planar graph can be drawn as the intersection graph of disks.) Koebe theorem is not very easy to prove. My question: Is there an easier version of this theorem where instead of disks one is allowed to use any fat convex shapes (convexity might be open to negotiations, but not fatness). Note, that every vertex can be a different shape. Thanks... Clarification: For a shape $X$, let $R(X)$ be the radius of the smallest enclosing ball of $X$, and let $r(X)$ let me the radius of the largest enclosed ball in $S$. The shape $S$ is $\alpha$-fat if $R(x) /r(x) \leq \alpha$. (This is not the only definition for fatness, BTW.) 

Result It is NP-Hard to decide if a $k$-regular graph has a Hamiltonian cycle for $k\geq 3$. The problem remains NP-Hard even if the graph has an odd number of vertices. 

The answer is yes - the problem is still NP-Complete. for every set $S_i$ create a fake elements $e_i', e_i''$ and create a new sets $S_i' = S_i \cup \{ e_i'\}$ and $S_i'' = S_i \cup \{e_i''\}$. It is easy to verify that any hitting set of the old system is a hitting set of the new system. Furthermore, except for the fake elements, every element now hit at least three sets. Next, for every pair of sets in the new system (lets call them $T_i$ and $T_j$ to avoid confusion), create a fake element $x_{ij}$ and add it to both $T_i$ and $T_j$. Clearly, in the resulting set system all sets pairwise intersect, but the original optimal hitting set is still the optimal hitting set for this newest system. Without any further restrictions the problem looks as hard as the original problem. BTW, proving that indeed the optimal solution would not use any of the fake elements is not trivial. First, we can assume that a given hitting set for the new system does not include any $e_i'$ or $e_i''$, since otherwise we can move the elements to the original elements of the sets, and get a hitting set of similar size. It is slightly more subtle to see why the elements $x_{ij}$ are not in the optimal hitting set. Since it is tedious I would just leave a hint: build a graph connecting two sets $S_i$ and $S_j$ in the original system if $x_{ij}$ connects two sets that are derived from these sets. Argue that this graph in the minimal hitting set must be $3$ regular, and as such the number of edges in it strictly exceeds the number of sets present as vertices. As such, one can find a smaller hitting set for these sets. 

If S is convex, that this is a linear programming problem. It can as such be solved/approximated in polynomial time using the standard sequence of reductions to the ellipsoid algorithm. I do not know any reference that describe it cleanly, but the following book describe it: Geometric Algorithms and Combinatorial Optimization (Algorithms and Combinatorics) [Paperback] Martin Grötschel, Laszlo Lovasz, Alexander Schrijver 

Change all the alphabet characters to a single specific character. Now, you have PDA defined over a single character. Its language is a context-free grammar. However, context free grammar over a single character is regular. So, convert the CFG into a regular language, and then check if it contains a word of length k. Now, all these conversions tends to require exponential time, but it seems to me unlikely that the problem is NP complete. Especially if you allow polynomial time in $k$. I might be wrong, and I apologize for my initial snippy answer... BTW, the fact that a CFG over a single letter is regular follows from Parikh's theorem. Although a direct proof is not too hard. See the link for more details on Parikh's theorem - it is a beautiful result... $URL$ 

As pointed out in the comments already, this is an instance of weighted geometric set cover. Here are some recent relevant references: $URL$ See also the follow up work: $URL$ 

See chapter 18 here $URL$ If the dimension is really high then bounding box is the wrong creature to work with. A better choice is the ellipsoid. See chapter 16 about John's ellipsoid - it can be computed efficiently using quadratic programming in some cases. 

New answer: The number of pseudo line arrangements is 2^{Theta(n^2)} $URL$ . Which is in turn can be used to bound the number order type of n points in the plane. Thus if you want to check some concrete conjecture on point configurations in the plane, you are going to get your desired running time. Examples of algorithms using this approach and use this approach are here: $URL$ . If you do not aggressively cut the search space for the specific conjecture you are checking, you would get the running you want. In a similar direction, checking if a generic property holds for all binary n*n matrices, would take this running time. This in particular, the time it would take to verify a "generic" property over graphs over n vertices. As a "silly" example, think about a Ramsey type conjecture: Every graph over n vertices contains either a clique or an independent set of size $\Theta(\log n)$. Ha! This running time is explicitly mentioned in the Wikipedia page: $URL$ 

See the following paper. The two problems are equivalent more or less. To see that, assume that the points are on the unit sphere centered at the origin, and observe that if your NN query $q$ is on the sphere, then the far neighbour for the antipodal point $-q$ is the NN to $q$. There might be a full version written somewhere on the web... 

Here are the lower bounds I can show. I conjecture that for a fixed $\epsilon$, the right lower bound is $\Omega( \log n)$, but naturally I might be wrong. I am going to use a decreasing sequence (just for convenience). The basic mechanism is breaking the sequence into $L$ blocks. In the $i$th block there are going to be $n_i$ elements (i.e., $\sum_i n_i = n$). In the following, we want the algorithm to succeeds with probability $\geq 1-\delta$, for some parameter $\delta >0$. First lower bound: $\displaystyle \Omega\left( \frac{1}{\epsilon} \log \frac{1}{\delta} \right)$. The $i$th block has $n_i = 2^{i-1}$ elements, so $L = \lg n$. We set the value of all the elements in the $i$th block to be $(1+X_i)/(2n_iL)$, where $X_i$ is a variable that is either $0$ or $1$. Clearly, the total sum of this sequence is $$ \alpha = \sum_{i=1}^L \frac{1+X_i}{2n_i L} = \frac{1}{2} + \frac{1}{2L}\left(\sum_{i=1}^L X_i \right). $$ Imagine picking each $X_i$ with probability $\beta$ to be $1$ and $0$ otherwise. To estimate $\alpha$, we need a reliable estimate of $\beta$. In particulate, we want to be able to distinguish the base $\beta = 1-4\epsilon$ and, say, $\beta=1$. Now, imagine sampling $m$ of these random variables, and let $Z_1, \ldots, Z_m$ be the sampled variables. Settings $Y = \sum_{i=1}^m (1-X_i)$ (note, that we are taking the sum of the complement variables), we have $\mu = E[Y] = (1-\beta) m$, and Chernoff inequality tells us that if $\beta =1-4\epsilon$, then $\mu = 4\epsilon m$, and the probability of failure is $$ P\left[ Y \leq 2\epsilon m \right] = P\left[ Y \leq (1-1/2) \mu \right] \leq \exp \left( -\mu (1/2)^2 / 2 \right) = \exp \left( -\epsilon m / 2 \right). $$ To make this quantity smaller than $\delta$, we need $\displaystyle m \geq \frac{2}{\epsilon} \ln \frac{1}{\delta}$. The key observation is that the Chernoff inequality is tight (one has to be careful, because it is not correct for all parameters, but it is correct in this case), so you can not do better than that (up to constants). Second lower bound: $\Omega( \log n / \log \log n)$. Set the $i$th block size to be $n_i = L^i$, where $L = \Theta( \log n / \log \log n)$ is the number of blocks. An element in the $i$th block has value $\alpha_i = \Bigl(1/L\Bigr)/n_i$. So the total sum of the values in the sequence is $1$. Now, we might decide to pick an arbitrary block, say the $j$th one, and set all values in its block to be $\alpha_{j-1} = L \alpha_j$ (instead of $\alpha_j$). This increases the contribution of the $j$th block from $1/L$ to $1$, and increase the total mass of the sequence to (almost) $2$. Now, informally, any randomized algorithm must check the value in each one of the blocks. As such, it must read at least $L$ values of the sequence. To make the above argument more formal, with probability $p=1/2$, give the original sequence of mass $1$ as the input (we refer to this as original input). Otherwise, randomly select the block that has the increased values (modified input). Clearly, if the randomized algorithm reads less than, say, $L/8$ entries, it has probability (roughly) $1/8$ to detect a modified input. As such, the probability this algorithm fails, if it reads less than $L/8$ entries is at least $$ (1-p)(7/8) > 7/16 > 1/3. $$ 

Given a matrix of size $n\times n$ with numbers, where every row is sorted, one can compute the $k$th smallest element in $O(n \log^2 n)$ time by simulating (implicitly) quick-select on this matrix (removing one log factor can be done by avoiding the binary search in each row). In fact, $O(n )$ time algorithm is possible if the columns are also sorted, see below for details by simulating some variant of QuickSelect on this matrix. My question: Is there a reference that describes and analyzes how to simulate QuickSelect on an implicit set (like a row sorted matrix)? This is not hard, but I would prefer not to write it down if somebody already did it. Thanks in advance... More details Fredrickson and Johnson showed how to get $O(n)$ time algorithm if both rows and columns are sorted $URL$ It is in fact not too hard to come up with an $O(n)$ randomized time algorithm by massively sampling in each iteration, estimating the interval where the element must lie, and then recursively continuing on the elements lying in this interval (this selection algorithm due to Rivest and Floyd is described in pages 47-51 in the randomized algorithms book by Motwani and Raghavan). 

Computational Geometry is full of such examples. The main technique to address such problems is called parametric search - which is a rather non-trivial technique and too painful to explain. A classical "easy" example would be the problem of given a sorted matrix of size n by n (rows are sorted, and columns are sorted - say both in increasing order), output the smallest k number in the matrix. Clearly, given a value t, one can count in time $O(n)$, the number of entries in the matrix smaller than $t$ (hint - find the number of elements smaller than $t$ in the first row, and move to the next row, etc). A natural step would now to do a binary search over the values of t. For example, if you knew the values in the matrix are integers between $1$ and $O(n^3)$, this would yield an efficient algorithm to answering the original problem. 

Intuitively, you are looking for partitioning the given numbers into k sets that are all have the same sum. Indeed, otherwise, you can benefit by taking two of these sets, and make them as balanced as possible. As such, you can take an approximation algorithm for subset sum, and turn it into an approximation algorithm to partition. Then extend it to handle $k$-sides partitions, and you would get the required approximation. See for example here: $URL$ Then you need to modify it to care about your target function.... I assume that going through the details, you would get an $(1+\epsilon)$-approximation algorithm and its running time would be $(n/\epsilon)^{O(k)}$. But better running time might be possible if you are more careful. 

Note, that a $k$-regular graph, for $k$ odd, must have an even number of vertices (just count the edges), As such, there are no $k$-regular graphs with odd number of vertices, with $k$ being odd. 

That is - interleaving horizontal rays. Now, the path must wiggle between these rays in the natural way. It is now trivial to add an almost vertical ray that its tail (i.e., non ray part) intersect all the horizontal rays. The path must intersect the two lines defining this ray as many times as we want. 

Quick reminder: In the set packing problem, given a family of sets, find the maximal subset of sets, that comply with some additional constraint (say, no element is covered more than 10 times, etc). 

For TSP, checkout this book... $URL$ My understanding is that there is no one tool to kill them all. Arguably any recursive solution deploying backtracking and some scoring function is using branch and bound. As such, a large fraction of solvers to NP hard problems use some form of branch and bound. 

This seems like a hard problem. You might be able to do something if the rectangles have some packing property. For example, if no point is covered more than $t$ times (for some constant T), and the rectangles are not too long and narrow. Then one can probably prove that a curve intersects $O( \sqrt{n})$ rectangles, using some $k$-ply planar separator argument. 

First, my understanding is that the paper you linked to does not show that 3sum is incorrect. It just shows that the 3sum conjecture is false in a model of computation that is not realistic (i.e., we do not have currently a truly subquadratic algorithm for 3sum). As for your question, if you are looking for a line containing more than $k$ points, and $k$ is sufficiently large (at least say $10\log^2 n$), then it is not hard to get a subquadratic number of candidate lines such that one of them with high probability is the desired line. Then using standard trade-offs between space and query time for range searching, you would get something subquadratic. The question is of course what to do in the in-between range, when the heaviest line has between $3$ and polylog n points on it. (For ref, see $URL$ I think the interesting open question, is whether one can get $O(n^2/k^2)$ (the above would get you at best $O(n^2/k^2 * polylog n)$ [but its probably slower than that]). 

It depends on the answer. If the algorithm created has running time $(d n)^{Ackerman(10000)}$, then it would have very little impact. On the other hand, if it leads to a new way to solve LPs it might have tremendous impact. For example, if I remember the history correctly (and I might be completely wrong) the ellipsoid algorithm for example, besides its theoretical significance, lead (?) to the development of the interior point method, which was faster in some cases than the simplex algorithm. This lead to significant speedup in practice, as both approaches were squeezed for the maximum limit of what can be done. 

For your second question... For any distribution, if you take a sample $R$ of size $O(1/epsilon^2 )$ of it, and compute its center point, it is going to be a $\geq (1/3-\epsilon)$-center point, with probability $\geq 1-\epsilon^{O(1)}$ by the $\epsilon$-net theorem (well, more precisely the $\epsilon$-sample theorem). You can apply this to a polygon - you just need to sample points uniformly inside the polygon. This works in any constant dimension. Similar ideas were used in the paper "Approximating Center Points with Iterated Radon Point". Or so I believe... ;) Computing the exact center point for a general polygon is nasty as far as I know. There was a paper in the 90s by Dan Halperin on maintaining the area bisector of a polygon, and it was painful, if my memory serves me right. 

Assume all integers involved are even. This can be ensured by multiplying $P$ by 4. Here is an idea that should speed up the search. Let $p = \left\lceil\sqrt{P}\right\rceil$. If $p*p = P$ we are done. Otherwise, let $k$ be the minimum such that $\alpha =p*p-k*k \leq P$. If $\alpha=P$, we are done as $(p-k)\times(p+k)$ is the desired solution with perimeter $$4p$$. Otherwise, clearly $1 \leq k^2 \leq 4p$ (otherwise, $p$ would be smaller). But this implies that $k \leq c*P^{1/4}$, where $c$ is some small constant. This implies that $P - \alpha \leq c'*P^{1/4}$. Thus, the solution with rectangles $$(p-k)\times(p+k) \text{ and } 1\times(P-\alpha)$$ has perimeter $4p + O(P^{1/4})$. Since $4(p-1)$ is a lower bound on the solution by the argument below (intuitively, the isoperemeteric inequality), this implies that search range is only of size $O(P^{1/4})$ (as far as the overall perimeter is concerned). It definitely feels like this argument can be pushed even further... And Jeff is right - this is not necessarily too interesting as far as pure theory. But it is fun... Lower bound. Observe that if the two rectangles are $x_1\times y_1$ and $x_2\times y_2$, then the rectangle $(x_1+x_2)\times(y_1+y_2)$ must have area larger than $P$. But then, the minimum perimeter rectangle of a given area is a square. This implies that $2(x_1+x_2+y_1+y_2) \geq 4\sqrt{P}$.