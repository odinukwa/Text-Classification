The following is a proof over any ring of characteristic zero that the Hamiltonian cycle polynomial is not a polynomial-size monotone projection of the permanent. The basic idea is that monotone projections of polynomials with nonnegative coefficients lead to the Newton polytope of one being an extended formulation of the Newton polytope of the other, and then applying the recent lower bounds on extended formulations. Let $f(x_1,\dotsc,x_n)$ and $g(y_1,\dotsc,y_m)$ be polynomials with nonnegative coefficients (as is the case here). Suppose that $f$ is a monotone projection of $g$ under the assignment $\pi$ (following the notation of the question). Under $\pi$, each monomial of $g$ gets mapped either to 0 (iff one of its variables gets mapped to 0) or to a monomial of $f$: there can't be any cancellations because of non-negativity. Let $New(f)$ denote the Newton polytope of $f$, and similarly for $New(g)$. Claim: there is an extended formulation for $New(f)$ in $\mathbb{R}^m$, using $\leq n+m$ variables, and a number of constraints that is at most $n+m$ plus the number of constraints defining $New(g)$. Here's how: Let $e_1,\dotsc,e_m$ be the coordinates on $\mathbb{R}^m$ (in which $New(g)$ lives; viz., an integer point in $\mathbb{R}^m$ with coordinates $(e_1,\dotsc,e_m)$ corresponds to the monomial $y_1^{e_1} \dotsb y_m^{e_m}$). For those $i$ such that $\pi(y_i)=0$, intersect $New(g)$ with $\{e_i = 0\}$ (since only monomials that do not involve $y_i$ can contribute to the projection); this adds at most $m$ additional constraints. Let $P$ denote the resulting polytope. Then $\pi$ induces a linear map $L_{\pi}\colon \mathbb{R}^{m} \to \mathbb{R}^{n}$, such that $L_{\pi}(P) = New(f)$. This last part follows from the lack of cancellations. Thus we get an extended formulation for $New(f)$ by taking $n+m$ variables, the constraints for $P$ on the $m$ variables, and the constraints defining $L_{\pi}$ (of which there are at most $n$, one for each $x_i$). QED Claim Now take $f$ to be the $n$-th Hamilton cycle polynomial and $g$ to be the $m$-th permanent, and suppose that $f$ is a monotone projection of $g$. The Newton polytope of the permanent (and determinant, incidentally), is the cycle cover polytope. This polytope is easily described by the "edge" variables $e_{ij}$ and the $m$ equations stating that every vertex has degree exactly 2. The Newton polytope of the Ham. Cycle polynomial is the Hamiltonian cycle polytope (surprise, surprise). But this polytope is the TSP polytope, which requires $2^{n^{\Omega(1)}}$ equations to describe any extended formulation of, which, when $m$ is subexponential, contradicts the small extended formulation given by the cycle cover polytope and $L_{\pi}$ as above. (Note that this argument fails if $f$, $g$, or $\pi$ can have negative coefficients, as then there can be cancellations, so $L_{\pi}$ need not map onto $New(f)$.) It's interesting to note that the geometry of these polytopes is closely related to the fact that matching is in $\mathsf{P}$ while Hamilton cycle is $\mathsf{NP}$-complete, but I think the above reasoning shows how the geometric structure here really can add something beyond that complexity classification. 

If your objects are Turing machines, there are several reasonable possibilities for morphisms. For example: 1) Consider Turing machines as the automata they are, and consider the usual morphisms of automata (maps between the alphabets and the states that are consistent with one another) which also either preserve the motions of the tape head(s), or exactly reverse them (e.g. whenever the source TM goes left, the target TM goes right and vice versa). 2a) Consider simulations or bisimulations. 2b) Along similar lines, you can consider when one TM can be transformed (by a computable function) to simulate the other one. This can be done at the level of step-wise behavior, or as Yuval suggested in the comments, at the level of input-output, that is, a morphism from $T_1$ to $T_2$ (or maybe the other way around) is a computable $f$ such that $T_1(x) = T_2(f(x))$ for all $x$. 3) Consider the transition graph of the Turing machine (each vertex is a complete description of the state of the machine and the tapes, with directed edges corresponding to the transitions the TM would make) and consider morphisms of graphs. For TMs this is a very coarse relationship, however, as it essentially ignores the local nature of computation (it ignores, for example, what the contents of the tapes are). I think the real question is: what is it you want to know about TMs or to do with them? In the absence of this, it's hard to give arguments for any one definition over another, beyond naturality (in the usual sense of the word, not the categorical sense). 

The answer to your questions depends on what kind of reductions you are using for your notion of hardness. If you are using polynomial-space reductions, then I believe Daniel's answer is correct. If you are using polynomial-time reductions, however, just the opposite is true. Namely, assuming $EXP \neq PSPACE$, there is a problem in $EXP$ which is neither in $PSPACE$ nor hard for $PSPACE$ under polynomial-time reductions. This can essentially be constructed by diagonalizing against all possible polynomial-time reductions to $QBF$ (preventing the constructed language from being in $PSPACE$) and from $QBF$ (preventing the constructed language from being $PSPACE$-hard). Also, by the general version of Ladner's Theorem, if $EXP \neq PSPACE$ then there are problems in $EXP \backslash PSPACE$ which are not $EXP$-hard under polynomial-time reductions. 

Formula Isomorphism is in $\mathsf{\Sigma_2 P} \subseteq \mathsf{PSPACE}$, is easily seen to be $\mathsf{coNP}$-hard, but is not known to be $\mathsf{NP}$-hard. Note that FI is not $\mathsf{\Sigma_2 P}$-complete unless $\mathsf{PH}$ collapses to the third level. All of this can be found in Agrawal & Theirauf. 

I think the answer is no, assuming $\mathsf{P}_{\mathbb{R}} \neq \mathsf{NP}_{\mathbb{R}}$ (I believe I give a proof below, but there are enough potentially nitpicky definitional issues here that I'm being cautious about my claims). Proof that the answer is no assuming $\mathsf{P}_\mathbb{R} \neq \mathsf{NP}_{\mathbb{R}}$: In fact, I believe the following stronger statement holds: Lemma: For any BSS decision problem $L$ over $\mathbb{R}$, if $L$ poly-time-BSS$_{\mathbb{R}}$ reduces to a problem on integer inputs, then $L \in \mathsf{P}_\mathbb{R}$. Proof of lemma: Suppose there were a polynomial-time BSS$_{\mathbb{R}}$ reduction from $L$ to a problem on integer inputs, given by a machine $M$. For inputs consisting of $n$ real parameters, unroll the computation of $M$ into an algebraic computation tree. There are only finitely many leaves, and the result at each leaf is a single rational function in the input parameters. In order for a rational function of real inputs to always output an integer value, it must be a constant function, and therefore not depend on the input. However, which constant function is used at each leaf can, of course, depend on the branches. However, since $M$ is a uniform machine, there can be only $O(1)$ output nodes, and thus only $O(1)$ output values. Thus $M$ can be trivially modified to in fact decide $L$ in polynomial time. QED Now, take $L$ to be real feasibility of real polynomials. If $\mathsf{P}_{\mathbb{R}} \neq \mathsf{NP}_{\mathbb{R}}$, then $L \notin \mathsf{P}_{\mathbb{R}}$, and by the Lemma there is no reduction from $L$ to any problem on integer inputs (in particular, to real feasibility of integer polynomials). Promise problem issue?: Another potential issue with your question is that real feasibility of integer polynomials may not be in $\mathsf{NP}_{\mathbb{R}}$, but only in its promise version. The issue here is that to verify that an input (such as the coefficient of a polynomial $f_i$) is an integer takes time that depends on the magnitude of $x$, whereas the set of instances (all instances, not just yes-instances) for an $\mathsf{NP}_{\mathbb{R}}$ decision problem should be decidable in $\mathsf{P}_\mathbb{R}$, the latter meaning that it takes polynomial time in the number of parameters, and not their magnitudes. This is, I believe, closely related to the fact that the integers are not first-order definable within the reals. (Essentially the best a BSS$_{\mathbb{R}}$-machine can do to test if an input $x$ is an integer is to compute the integer part of $x$ by computing powers of $2$ and doing "binary search." Once it's computed the integer part of $x$, it just checks whether that is equal to $x$.) So I think the probleam of real feasibility of integer equations is in $\mathsf{PromiseNP}_{\mathbb{R}}$ but probably not in $\mathsf{NP}_{\mathbb{R}}$ (or at least it seems nontrivial to prove that it is in $\mathsf{NP}_{\mathbb{R}}$). 

You may also be interested in other work of Agrawal, Allender, and others on related questions. Agrawal's and Allender's webpages have links to most of the relevant papers (I only say "most" in case I've missed a few that didn't involve either Agrawal or Allender, but I haven't done a thorough literature review recently). On creativity, Joseph and Young's construction is still one of the main pieces of evidence against BH. 

Although not (yet) getting all the way down to $\mathsf{P}$, Group Isomorphism and other algebraic problems can depend heavily on the format of the input, in a way that is far more subtle than merely padding. For example, when the groups are given by multiplication tables, there is a trivial $n^{\log n + O(1)}$-time algorithm to test isomorphism. When the groups are given by generating sets (say, of permutations, or of matrices over finite fields), there are slightly sub-exponential algorithms, but the problems are at least as hard as Graph Isomorphism. When the groups are given by generators and relations, the problem is undecidable (indeed, when given by generators and relations, even telling if the group is trivial or not is undecidable, whereas this particular task is trivial in the other mentioned formats). Another algebraic example is provided by Grobner bases. Most questions about ideals in polynomial rings become comparatively easy if you're given a Grobner basis for the ideal instead of an arbitrary generating set. But computing a Grobner basis is $\mathsf{EXPSPACE}$-complete. (This is perhaps not as good an example, since it is a property of the input, rather than a the format of the input that is affecting complexity here. But I thought I'd mention it anyways.) 

It depends on the relationship between $m$ and $d$. If $m \geq 3$ is fixed, but $d$ is allowed to grow without bound, then the corresponding class of functions is exactly the same as functions with polynomial formula size [Ben-Or and Cleve]. (For $m=2$, it is not as powerful [Allender and Wang]). [Update: As far as I know, this is only true for iterated matrix multiplication $tr(M_1 M_2 \dotsb M_d)$, rather than matrix powering $tr(M^d)$. When $m$ is allowed to grow these two are essentially equivalent, but for fixed $m$, e.g. $m=3$ I don't know if $3 \times 3$ matrix powering is poly-formula-size-complete.] If $m$ can grow but $d$ is fixed, then this is the same as matrix multiplication, up to $O(\log d) = O(1)$ factors. Since circuits for matrix multiplication can be converted to bilinear circuits with only a factor 2 blow-up in size, circuit and formula size here are essentially the same, and the question boils down to the classic open question of the exponent of matrix multiplication. If both $m$ and $d$ can grow, then this is equivalent in power to the determinant (corresponding to the Boolean class $\mathsf{DET}$ and the algebraic class $\mathsf{VP}_{ws}$). So here the question becomes about the circuit/formula complexity of the determinant. Both of these are well-known open questions (obviously there are cubic circuits; the best known upper bound on formula size of the determinant is quasi-polynomial). Most (perhaps even all) nontrivial algorithms for matrix multiplication use cancellations in a crucial way, so I would expect there is a difference between the monotone case and the unrestricted case. Also, note that the equivalence between matrix powering and determinant in the last case is necessarily non-monotone (since matrix powering is a polynomial with all nonnegative coefficients, but the determinant is not). 

An isomorphism-invariant coloring of graphs is a method of assigning colors to vertices so that if two graphs are isomorphic, then there is an isomorphism between them that sends vertices of color $c$ to vertices of color $c$. Some authors will call this coloring canonical - and indeed, in the paper you cite, it may be that all they need is a canonical coloring in which each color class has bounded size. (Other authors might say that a canonical coloring is an isomorphism-invariant coloring in which each vertex receives a color that is distinct from that of all other vertices. In this way, if we can compute a canonical coloring of two graphs $G,H$, then there is only one possible isomorphism between them (namely, the one that respects the colors), and it is easy to check whether this is actually an isomorphism.) 

(Of possible interest: a follow-up paper arXiv:1610.04092 [math.GT] uses this to develop an algorithm using Grobner bases.) *Technically it's stated that recognizing the 3-sphere among integer homology 3-spheres is in coNP assuming GRH. I'm not an expert in this area, but it seems clear to me that one can compute the integer homology given a triangulation in poly-time, and if the integer homology doesn't match that of a 3-sphere then it's definitely not the 3-sphere. 

Combined with individualization, one can talk about a coloring that is canonical relative to the individualizations that have been made previously. That is, it may not be canonical with respect to isomorphisms of the original graphs, but given the vertices that have been colored so far it is canonical. (In this sense individualization is a form of "partial brute-force search," in which you keep track of how the guesses you've made so far constrain your future search.)