Also, was mentioned in the comments that you may not need to generate the _unique key depending on how the data is stored. Might compare results to confirm: 

Also, if possible, you can typically gain a performance improvement by moving the sort into the application layer. 

It seems likely that the subquery in the SELECT is causing a row-by-row lookup. Try moving into a LEFT JOIN to get the set-based performance we're looking for: 

Overall, your code is good, but there are a couple of performance improvements that were pointed out above. Here is a more explicit answer on how to improve the performance of the code provided. Improvement 1: Eliminate subqueries in SELECT Subqueries in the select statement referencing an external table will take a major hit in performance (row by row look vs set based join). In this case, we can avoid that by joining the Users table. We're assuming Id is the primary key in Users, so no duplicates to worry about. We are doing an inclusive join, which assumes all UserIds have a corresponding record in the Users table (typical foreign key constraint). Improvement 2: Eliminate redundant external references We reference the Posts table twice in the original query, once to limit to closed posts and another time to sort by the ClosedDate column. Instead, we can do an inclusive join to Posts, set our criteria either in the WHERE clause or in the JOIN, and do the sort, so that it is all done in one pass. We again assume that Id in Posts is a primary key, so no duplicates to worry about. 

As mentioned in the comments, there are benefits to casting/storing that unique key in the table during the ETL process, especially if it's going to be used in other places than just this query. Most likely, the performance hit is coming from using IN (typically results in a row by row lookup) and from de-duping with the casted key. You could get a performance gain from JOINing the subequery instead of using IN. You could also use ROW_NUMBER which, in my experience, is typically more performant than the GROUP BY with HAVING clause. Here's my example using ROW_NUMBER and CTE's for easier reading: 

Thus incorporating another vector into U is of order n*n Note that the initial step is a little different in that we want to find U so that 

You don't need to keep all of Pascal's triangle. In fact you need only keep one row. Below is a C routine that I use, I hope it explains the idea. 

Of course this is just the transpose of the lower factor. We want to find a cholesky factor of the inverse of 

A better, (faster, more accurate) way to evaluate polynomials, given the coefficients, is to use horner's rule, eg 

A standard trick to reduce the error accumulation in vnp's code is to notice that the real part of base will be close to 1; we can use the multiple angle formula to compute base-1 (with base as in vnp's post) more accurately as 

Though this more computation, it will have less rounding error. Unless hardware rules it out you should consider computing base as doubles, and accumulating the complex exponentials in doubles; you can use one variable W say with 

then stored W in tab[i] -- hence converting it to float. Again more computation but less accumulation of error. 

It may seem that no progress has been made -- we have to do m factorisation and multiply up the results -- but in fact Z can be computed simply, and so can the products. We note, by Woodbury, that 

I hesitate to write this as it's not about the code obove but about a different algorithm. However I think, for small enough m (eg a quarter of n -- here A is nxn and B is mxm), that this is faster. The below is in terms of upper cholesky factors, ie an upper triangular matrix U so that 

An implementation, that I believe works, is below. The matrices are in column order. The ws parameter is workspace, 4*C doubles (though only C doubles are used in the second routine). Note that v is assumed to have dimension C, while really U is CxC, but could be stored in a RxC array. If you are using square matrices set R to be C. The function mat_ut_vec( R, C, U, v, w); computes U*v (for upper triangular U) in w. 

the rest just sets up the boundaries (i.e. the kth limit). So once you have seen a bit of recursion, you intuitively zoom in on the important lines and sort of 'ignore' the rest :) As a side note, there is a slight gotcha with using recursion in Python: Python has a recursion limit of 1,000 calls. So if you need to recurse on large numbers it will often fail. I love recursion, it is quite an elegant way to do things. But it might also be just as easy to do the function as an iterative function, or using a generator. 

By testing for both, you avoid having to use and start looping through the file again. Of course, this is based on the sample input above - and assumes that there isn't multiple lines with 'Time taken' or 'Sample Text' in them. Hope this helps 

EDIT Added loadData(); slight tweak to return a dictionary of dictionaries, instead of a list of dictionaries: 

RECURSIVE SOLUTION: NOTE: it assumes that the input is already sorted. You could sort this at every call of the function, but this seems like unnecessary overhead. 

It seems that you are looping through the entire job data set 3 times (once each for salaries, descriptions and titles). You will be able to speed this up three-fold, if you extract all the info in one pass: 

Other than the few things pointed out above, it might be 'cleaner' to use a helper function to test primality. For example: 

Now, I know you didn't want any "fancy-pants" coding, but this is actually really really similar to your iterative approach, just using recursion. If we look at each bit we have the following: : this tests whether the value is prime or not. Just like in your code, it takes the modulo of against all of the primes up to the square root of . This isn't too tricky :) The function gathers all of these tests and returns a boolean (True or False). If all of the test (from 2 up to sqrt(x)) are False (i.e., every single test confirms ti is prime) then it returns this finding, and we know x is prime. : ok, this is recursive, but it isn't too tricky. It takes 3 parameters: