Nevermind, it's a really weird issue. Looks like Point and Print is only in effect when you actually use a print server. I happen to be researching and my laptop running Win7 was idling for 2-3mins and it suddenly just started downloading the drivers and the printer was added successfully. Even a test print went through. As a test, I disabled all other settings except the "Prevent printer driver installs" policy, ran the gpupdate /force command and rebooted again. I was still able to install different printers. The only thing i noticed was that when I initially added the printer, I was prompted with the UAC prompt for an admin password. If you cancel out of that box, you see a PnPX Device Association UAC box. Here is where I happen to idle for 2-3mins and then my laptop just started downloading and installing the print drivers. I'm not really sure if this is a solution per se but it seems to work by just waiting a few mins. I've googled around and looks like their isn't a specified device GUID for PnPX so I settled for this 

Our WSUS previously had https bound to port 8531 in IIS, but I was trying to get SSL to work on it and ran into some issues where my WSUS console wouldn't connect anymore. The event logs were showing some errors about not being able to reach 0.0.0.0:8531. I wasn't sure how to undo the wsusutil.exe configuressl command so that my wsus server wouldn't use SSL anymore so I attempted to just delete the SSL certs from IIS and removed the binding. After I did all that and tried to re-add the same port binding for https, the OK button is now greyed out and I can't re-add it. Anyone know how to fix this? At this point, I'm thinking it might be easier to just scrap everything and start from scratch as this was originally set up by someone else and I am just trying to go over all the configurations and make adjustments. 

Create a wildcard DNS entry for example.com so any subdomain of example.com is directed to your server. 

It sounds to me like you're describing AFS, the most common implementation of which is OpenAFS. The key OpenAFS concepts are described here: $URL$ AFS is: 

What symptoms are you seeing when you try to reach the host? What do you mean by "doing a traceroute on the ip:443"? Ports are not relevant to traceroute. Can you show us the actual traceroute command you're executing? Is the traceroute to the DNS server or to the destination host? I'm also puzzled by "the trace fails after reaching the dns", as a traceroute to a host does not also traverse the DNS server used to resolve the host... Unless the DNS server is also your gateway. Can you clarify this statement a bit? That trace looks fine, assuming the destination host is 208.173.55.226. Traceroute is actually of somewhat limited utility as it frequently will be dropped when normal TCP / UDP packets for legitimate ports will be accepted. In some cases using tcptraceroute can be illuminating. 

Run "" Edit and add above the definition of the interface. Run "". It should come up with the address you assigned in . 

If you have access to the source code, you need to create the socket with the option mentioned by Jacek. Also of interest are the and kernel flags (on Linux). The real problem is in the protocol design, which you may or may not be able to change. Interesting threads on the topic: 

If that all looks good, it's time to check out your config. Start be moving your directory out of the way, like so: 

I like PmWiki quite a lot. It's easy to install, easy to customize, and it doesn't require a database. (Neither does Ikiwiki, which I've also used and liked.) PmWiki supports a great deal of customization through cookbook recipes, and it appears that it can do what you want in terms of user authentication and permissions. See this page. But I haven't used it that way, so I can't confirm how well that works. Edit: In response to the OP's comment, I have used BoltWire (check your spelling and your link - both are incorrect), and I prefer PmWiki. They are both reasonably easy to install and both pretty lightweight. I didn't dislike BoltWire at all, but I found PmWiki more fully-featured and easier to configure. Edit 2: Since you mention in a comment on another post that you are using Debian, both Ikiwiki and PmWiki are available in Debian and pretty trivial to install using your favorite APT package manager. I would recommend that you try both out and decide for yourself. 

The utility runs under a very limited environment. This means that you have to spell things out in a way you would not if you were working interactively at the command line (e.g., rather than just ). It's also worth saying that Apple recommends over . (I actually use myself on Macs, but only because I'm used to it from Linux machines. I keep meaning to switch.) 

But again, getting such a command just right might very well take longer than installing a minimal system and setting it running. 

Some versions of support a or flag to show the tree of relationships between processes. If your version doesn't have that, check for , a stand-alone package to do the same thing. It can help in cases like this. 

I'm trying to configure a GPO setting to allow non-admins the ability to add a network printer. So far I've set the "Devices: Prevent users from installing printer drivers" to disabled and Enabled the "Point and Print Restrictions". Also under that setting, I've set the "When installing drivers.. and When updating drivers..." options to "Do not show warning or elevation prompt". Ran gpupdate /force on my Win7 client, rebooted for safe measures, ran gpresult /r to confirm the GPO was applied. While attempting to add a printer I am still prompted for an admin password on a non-local admin user account. I also have a windows 10 pro client machine in the same OU as the Win7 box and i can add the printer just fine with no admin prompts and using the same non-admin account. We don't have any print servers in our environment since we only have about 5 printers in the office. I'm not sure if the Point and Print restriction policy only applies if you actually have a print server on the network. We're running all Windows Server 2012 R2 boxes. I've also tried enabling the "Allow non-admininstrators to install drivers for these device setup classes" and supplied the GUID for printers. Still no changes so I'm all out of ideas. I would like to have it working for both Win7 and Win10 clients because although we have a small windows environment theres a mix of win7 and win10 clients. Anyone have any other thoughts on what to check? 

Basically, the reason I am thinking the structure of the disk is the cause of the problem is I spun up a fresh VM with nothing installed other than the AWS Command Line tools (AWS prereq) using a Gen1 Hypervisor in Hyper-V and the import works flawlessly. My next logical step was to try and recreate the 350MB MSR partition that Windows creates automatically on a fresh install of Windows Server 2012 R2 but I have a couple questions: 

You need to have the bash-completion modules installed. Some distributions bundle them with bash, others package them seperately. Once they're installed, you need to activate them. In Ubuntu/Debian, that's done by sourcing in your .bash{rc,_profile}. For CentOS 5, the process is documented here: $URL$ 

Create a single DRBD device mark it as a PV for LVM. Create Pacemaker resources for the DRBD volume and each of the LVM logical volumes, with the logical volumes depending on the DRBD volume. Your pv filter looks correct. Have you verified that the DRBD device is correctly marked as a PV? If it doesn't have metadata on it, it won't show up. Try using the command to verify this: 

Something is connecting to the port and then never sending data. HTTP 408 is a "timeout" error. There's a good writeup here: $URL$ 

If you have access to the Apache config, add the authentication stanza to the that has SSL enabled. Then the redirect will always happen first. Also, using to perform a simple redirect is a bit of overkill. Use the Redirect directive instead. It's possible this may even fix your problem, as I believe rules are some of the last directives to be processed, just before the file is actually grabbed from the filesystem. 

I can't think of any reasonable solutions that will prevent theft of the media, so the best you can hope for is to render the data useless: Place the subversion repository on an encrypted partition. This way the filesystem is unreadable without the (lengthy) pass phrase. 

I would wager that is not being set properly in your script. If any of the mysql command-line tools receive a that is not immediately followed by a password, they assume they should prompt for the password. If, on the other hand, you supplied an incorrect password (or the wrong username or similar) it would just fail to connect. So, I suggest placing an command at the start of that line and re-running your script, like so: 

If the problem is simply that you have to start every session in the terminal with this: , then I can think of two solutions. 

Put this at the bottom of your file (it's in your user's home directory): cd /a/very/very/annoyingly/long/path/name Edit your . This method gives you quick tab-completable directory changes Mine looks like this, for example, since most of my shell work is in the two directories 'iliumSvn' or 'unix.varia'.: export CDPATH='.:~:~/iliumSvn/:~/unix.varia' 

The system wouldn't allow you to remove without a fight, just like it wouldn't let you remove your kernel or the tools themselves. One problem is that your question mixes two problems: (1) what is the set of packages that are minimally required for nginx as apt dependencies and (2) what is a sane minimal set of packages for a healthy working system. As an example, nginx probably may not depend in any libapt sense on sshd, but you don't want to remove ssh from a remote server. I actually think @tigran's answer is safest and quickest. You can install a minimal system in minutes, and this question has already been here a half-hour. Other than that, I would suggest that has very powerful search and matching options. You could probably craft a command line like this: 

LSB is Linux Standard Base. Take a look at the Debian wiki page that the warning recommended. There's lots of good information on it. 

If you can see the "Hello world" post from the blog's front page, but you get a 404 error from WordPress when you click the link, then I wonder if it's a problem with WordPress's links structure. Have you changed the permalinks defaults? (If so, the blog itself will offer you a handy-dandy set of rules to put into a .htaccess file.) 

Yes, large directory sizes can be a problem. It's generally best to avoid them by hashing files into subdirectories. If that's not an option, there is an ext3 feature that can dramatically improve the lookup performance in large directories: 

Installing a certificate / key on a linux machine is as simple as copying the file to the server. Most distributions have a standard location for storing the certificates and/or keys. The harder problem is configuring your application to look in the correct directory. What application will need to use this certificate? Are you installing a root certificate to validate the server cert? Or are you installing a certificate and key so you can serve information over SSL? 

If, however, you just want to set up a proxy server on your box that they can connect to directly from their browser (by configuring your box as the proxy server) then you want something like squid. 

The idea is that the matches the start of the header value if the string does not exist anywhere in the header. If the match succeeds (meaning "max-age" does not already exist) it then inserts "max-age=604800," followed by the rest of the original header. EDIT: Apache uses PCRE, so you might need to use instead of in the replacement string. 

Actual directory permissions are screwed up. This is fairly easy to test by changing to the user that Apache runs as and attempting to list that directory (assuming Apache is configured to run as "www-data"): 

There are ways to add it to your script, but I would suggest an easier, more robust approach would be to add the path to the config for the dynamic linker directly: