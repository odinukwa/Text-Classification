If you are mostly loading the data, and seldom querying it, then no need to split the table. Any "home-grown" attempt at splitting the table is bound to cause grief; use Oracle's (extra-cost) partitioning option if your queries are usually date-ranged; but be careful; partitioning does not necessarily improve performance. It can if the queries include critera that are the same as the partitioning criteria. It does come in handy for archiving and purging old data; if your business rules are such that you don't need to retain the charge details beyond say 3 years or whatever. With some appropriate indexes, even without partitioning, you should get good results querying millions of rows. You do not mention whether your users (or the loading process) are complaining of long response time or load time. It will come down to monitoring the database using tools such as the Automatic Workload Repository to determine where your bottlenecks are, and how to resolve them. If you are looking for future issues, look at whether the charge detail data can be summarized after xx years, etc. and purge old data after it is summarized. 

All the comments above are appropriate. If you provide the details on the data types and any not null constraints of the columns in question, then we can better determine whether your function-based approach is appropriate. I would also gather statistics on the indexes and look at the unique values, etc. You can compress your baseline index too, if say you have very few unique values for company and hire date, you can compress by two levels. In fact, you can do the following to see if index compression will help: 

Application Express, together with either ORDS (Oracle REST Data Services) or a similar web service such as Glassfish, is basically a set of processes that run in the database, usually connecting via internal connections. Application Express basically serves HTML web pages to allow access the database and display reports, and so end users only need a web browser to use an Application Express application, with reports, ability to modify data, etc. This is a huge simplification, but for your purposes, all you need to know is that end-users just need a fairly current web browser that supports HTML5 to get all the fancy web pages for accessing/modifying the data in the database. APEX comes with a security framework, or you can also use your own corporate security. 

An index-organized table (IOT) is just that; an index with no "real" table. All data besides the primary key are just "tacked on" to the primary key, so for example if you have an IOT with 6 columns, 2 of which make up the primary key, behind the scenes you just have an index with 6 columns, of which the first 2 columns make it unique. So, sorry, only way to fix it is to recreate the table; to improve the rebuild time temporarily disable logging. You cannot use an hint on inserts to an IOT table to force a direct path insert: 

If you are generating the primary key using an Oracle sequence (PS: you should), you are guaranteed to get integers so you don't have to have Oracle checking precision, etc. if you specified to specify an integer with up to seven digits. And then if you ever exceed 7 digits, not a fun day to fix that. As the links you've referenced above mention that Oracle only uses enough space to store the number, no need to worry about how much storage is being used; and therefore you don't have to concern yourself with indexing of the columns holding the primary and foreign keys. The only time I would specify a fixed length integer would be if the primary key was an intelligent key such as a USA social security number, but intelligent keys are a bad idea too. 

Query V$SESSION and include the column which contains the value you are looking for. The link $URL$ defines all the column meanings for that dynamic view; I would probably do a query like 

Using a different blocksize would probably be the last thing to try, and only if there is a good reason to do so. A good reason might include 

Balaz Papp's comment above is correct in the explanation why connected without a connect string is working. And his comment below is correct; any user could be granted query access to the view, when connected to the regular Oracle database instance. 

Cases like this can usually be solved for certain by enabling trace events 10046 (if you have authority to do so), and run tkprof on the resulting trace files: 

You should be fine; the would be a show-stopper if you were using it Temporarily, there would be "holes" in the data files, but Oracle will fill them in without any intervention. Ensure you have automatic tablespace management enabled. You can invoke that command but no need to; let Oracle take care of it The person that stated that probably meant there was a one-time hit, or he didn't know what he was talking about, or using an old version. You tested; his environment may have had a rat's nest of views. 

Also, if your application is a commercial application, there may be several tables with zero rows. Or if you have partitioned tables, you may have some partitions with zero rows. You can drop space allocated to them with 

But in general, I would say a larger blocksize won't help for most environments; and for those that need it, it would be a minor improvement. Derek's comment above has good things to look at; if you post the query and the explain plan, we can provide better advice. Based on the query and explain plan you provided, you may want to try a composite index; the order of the columns below is assuming there are fewer unique values for MATCH_CODE_DATE_OF_BIRTH than for MATCH_CODE_INDIVIDUAL: 

What appears to be happening is index statistics are not being gathered (or not gathered correctly) on 10.2.0.5; if specifying doesn't help, you may want to consider gathering the index statistics separately via . You might also need to set a higher value for estimate_percent. You may also want to ensure you have set system statistics (which tell the optimizer how fast your disks are, how much CPU you have, etc.) via the and procedures. I think that 10.2 is more sensitive to system statistics. 

command and see what LANG or LC_ALL is set to; you may need to set export NLS_OS_CHARSET=UTF-8 or the like. You can list the available locales in Unix with 

If you use DataPump and commands, you can include the parameter in the impdp command parameter file. The log file will then time-stamp each phase. 

To supplement Robert's answer, if after you create a new UNDO tablespace, and are unable to drop the old one, after trying those queries with no results, and the period has elapsed, you can try this: (It worked for me when querying showed unexpired status for some rows): 

As long as the developers only have that role in the development database instance, I see no problem; far better to tune now than later. Now, if your development machine has limited resources, there may be a reason to limit access. But in general, usually tuning tasks run from a few minutes to the default maximum of 30 minutes; during that time, Oracle may try parallel queries (if enabled), so that could be an issue, but you can play with the instance parameters and to minimize the impact. There are no changes between Oracle 11g and 12c that would affect your decision. 

Then find your trace file, and run tkprof on it, and the output of tkprof will show you execution statistics (logical reads, physical reads, etc). And you can look at the actual trace file if the tkprof output doesn't help. If that doesn't help, you can enable 10053 event, and enjoy wading through thousands of lines of output; but sometimes you need to do that. But first things (well easier things) first; do as others suggested; see if table statistics are up to date, optimizer settings, etc. If you are using the exact same criteria each execution, could be the optimizer is "on the border" on plans. I would ensure you have collected system statistics; see the documentation on dbms_stats.gather_system_stats pl/sql. 

? If you are using the first option, is it prompting you for a password, and are you entering the password for the user? Did you create a local Windows user account as the user, not the user id used to install the software? Oracle recommends a user with administrators group install the software, and a local Windows user be defined for the sys and system accounts. 

It could be that one of the default parameters for Oracle 10G is different in Oracle 11G. I would try adding the 

Try removing the from the configuration of the channel, and use on the backup command. The following is working for me: 

Any application that uses GPUs for processing has to be custom-written to access the GPU's processing power. Where GPUs shine are in floating point arithmetic, something that is low on the list of database operations, so no, no database software is likely to harness GPUs for processing. 

I agree with EdStevens' comment, but here are some other ideas: The AskTom website has a script that will list exactly how much you can shrink the existing data files. If that doesn't yield much savings, you can find the biggest indexes on that tablespace and rebuild them; that is a temporary solution and the indexes would eventually get big again, but if budget doesn't allow more disk space now, try this query: 

And as others have mentioned, processes are interrelated with sessions, so if you set one, you probably need to set the other. 

Some might say depends how big the databases are; I think regardless of the size, accessing the databases directly instead of having a "local" mySQL database to query would be best, for the following reasons: 

Invoke the SQL a few times; the first time will likely cache the data and subsequent runs will reflect that. Then do the same queries via your client. If you had DBA access you could turn on tracing for a session and see exactly how much time is used for each part of the query. 

If you have access to the server, use SQLPlus running on the server to invoke your query; that will eliminate the network delay. To get somewhat precise timings, invoke these SQL*Plus commands first: 

Is this a one-time update, or are you planning on running that query after inserts, where the insert statement is not setting SSC.NUMAR_CHEST? If you want ascending numbers, it sounds like you need to use an Oracle sequence; your subquery will return the max value at the point in time of the start of your query. To use a sequence, create it once (using the name in the examples below), then reference it in your update; every time you access you will get a unique number: 

Not really; but if the goal is to improve the performance of the query that aggregates information in the GTT, you could set OPTIMIZER_DYNAMIC_SAMPLING to maybe 5 or higher, if your Oracle version is 10g or higher. That would encourage Oracle's optimizer to work harder to determine the best plan by running some sampling queries. You can use that via a hint: 

I would guess that there is a weekly maintenance job that is running on that one day a week, that is affecting something that is forcing extra reads of the index. Is your daily batch job always run long on the same day of the week each week? If so, look at the views and to see if any jobs are scheduled. Now the bigger question is, is why not fix the UPDATE statement to include criteria to avoid updating rows where the value is already the correct value? 

The includes a variety of files, including online AND archived redo log files; take a look at the Oracle Backup and Recovery User's Guide chapter 5 to find out the best solution; disabling is not a good idea. You can try 

I have Oracle 12c installed on two servers. On server I have an instance that hosts an RMAN recovery catalog. On server I have two instances, and . I can use the following command on server to connect to my catalog, and target of , but when I try to connect to it gives an 

RMAN command to see what can be deleted. Then do a to delete them. Do you have any guaranteed restore points defined? 

If disk space is not at a premium, you could be able to create a "work" copy of the table, say , using CTAS (Create Table As Select) with criteria that would omit the records to be dropped. You can do the create statement in parallel, and with the append hint to make it fast, and then build all your indexes. Then, once it it finished, (and tested), rename the existing table to and rename the "work" table to . Once you are comfortable with everything to get rid of the old table. If there are a bunch of foreign key restraints, take a look at the PL/SQL package. It will clone your indexes, contraints, etc. when using the appropriate options. This is a summation of a suggestion by Tom Kyte of AskTom fame. After the first run, you can automate everything, and the create table should go much quicker, and can be done while the system is up, and application downtime would be limited to less than a minute to doing the renaming of the tables. Using CTAS will be much faster than doing several batch deletes. This approach can be particularly useful if you don't have partitioning licensed. Sample CTAS, keeping rows with data from the last 365 days and : 

Also, are all the bind variables always populated with a value, or are sometimes some of them null? IE: if and are often null, you might be able to do something to your query like 

Going forward, you could implement auditing of commands which would capture when a user ID is unlocked, but would not capture when it got locked due to number of unsuccessful login attempts. Take a look at $URL$ for details. If you really need to know when it was locked, etc. as Balazs suggests, you could invoke SQL every hour/day/week that would insert into a table a copy of the columns of interest from dba_users: 

You can use the clause on an alter table statement for things like partitioned tables, or tables you know will not have any rows. 

In the above example, compressing the index at level 3 would save 28% space. So if it is a good amount, you would 

I get OK results from from server to both instances on , and I can connect to a non-dba user from server to both instances. I recreated the Oracle password file using on server instance and that did not help. Instance names are and on server . Here are my password files on that server: 

How much real memory free does Windows task manager show, before you start up Oracle? If less than 2.5 GB, that's the problem. You can try increasing slowly from its current value, say in 256 MB increments, until it errors again, then I'd back it off by maybe 512 MB to give some headroom.