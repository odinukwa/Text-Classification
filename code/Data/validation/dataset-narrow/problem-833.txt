SELinux contexts must be set correctly. In recent releases of RHEL/CentOS, Tor has an SELinux policy applied to it. To fix broken SELinux labels: 

You installed a libdwarf package for a completely different Linux distribution than the one you are running. This is not guaranteed to work. RPM is only a packaging format and cannot guarantee that the package contents are appropriate for your system. The dependency information within the packages does that, and that is why you can't perform this installation. In theory you could have multiple versions of a library installed, but in practice this only causes problems unless it's done in a very carefully controlled way (such as Software Collections). To fix the problem, go back to the correct libdwarf package, which is the one that came with CentOS. 

just saved you from accidentally destroying all your data. Don't try this again until you fully understand what's going on. I recommend you familiarize yourself with at least these parts of the LVM HOWTO: 

Anyway. The RPM build of PHP actually rebuilds PHP four separate times. It is built once without any extensions enabled, for loading into Apache as , then built again with all extensions as shared memory objects. And a build for the PHP command line, and a fourth build whose purpose I forget right now. So what happens is, the builds with no extensions built in are bundled with the extensions built from the other build. Thus PHP may say that it was configured without any extensions, yet still be able to use them. 

To resolve the issue, advertise a valid global prefix. If you haven't received one from your ISP, use a random prefix in the Unique Local Address range. 

You don't need to create a new subnet for every single Layer 3 subnet that the network people create. Instead, create subnets corresponding to the IP address allocations for the entire site. Here's a quick example. Say you have two sites. Let's call them "New York" and "Mountain View". New York's entire IP allocation is 10.187.128.0/22. Mountain View has 10.187.132.0/22, but it also has some old cruft hanging around in 10.244.0.0/16. The network guys will divide all those addresses into tiny subnets of as small as /29, there will be thousands of them, but they're all contained within those supernet blocks. Within AD, though, the New York site only needs the one subnet defined, and the Mountain View site only needs the two subnets defined. They cover all the possible IP addresses within their respective blocks. 

Change it to something a bit longer, say 86400 (1 day) or even 604800 (1 week). Keep in mind that this applies to negative caching of any nonexistent record, not just AAAA records, so you'll need to take additional steps when adding new records of any type (reduce this value, wait, add the new record, increase this value again). 

Your domain's certificate has two paths to two different root certificate authorities. On modern desktop browsers such as Google Chrome, as well as on newer Android versions, the path being taken is to the more recent USERTrust RSA Certification Authority root certificate. (I get this on Android 7.0 NPD90G.) On older Android versions, the path being taken is to the older AddTrust External CA Root root certificate. On this second path, you are missing an intermediate certificate. This is the one shown in the SSL Labs test as an "Extra download". In order to resolve the problem, you need to obtain this intermediate certificate and add it to the certificate chain in your web server. 

Anyone can set the PTR record to whatever they want. It doesn't have to be valid, or it could be valid and later become invalid. If you really want to know about an IP address, use whois on it. (This example uses GNU jwhois, which most Linux distributions ship.) 

You can also use fail2ban to do this, as it already has preconfigured jails which process the postfix logs; they merely need to be enabled. For instance, put in your : 

They may be the same port, but they are not the same address - or even the same address family! Yes, the IPv4 versus IPv6 matters. To fix the problem, have the desired process bind to both IPv4 and IPv6 address families. 

The obvious problem that I can see is that the options you've provided are not correct. Instead of your question has , containing a single Unicode emdash instead of the expected two standard hyphens. The same is true of where your question instead shows . 

The easy way to do this is to run on your desktop and set it up with a remote ssh connection to the hypervisor, the server running your new virtual machine. The hard way to do this is by running on the server. This will give you VNC connection information. But, you may find this unusable as it may only be bound to 127.0.0.1, or firewalled. 

If DNS resolution simply times out and a response never comes back from the DNS server at all, or the return is SERVFAIL, then the message is supposed to be queued and tried again later. If DNS resolution returns NXDOMAIN (the name doesn't exist) then the message is supposed to be returned immediately. See RFC 5321, section 5.1: 

Actually, you do not want to open port 11211 in your firewall. In your setup, memcached is running on the same machine, and localhost is rarely or never firewalled from itself since this causes the universe to implode. Thus processes on the same machine can talk to each other (e.g. PHP to memcached) without any other special firewall rules. But, until very recently memcached had no built-in access control, and even if present it's typically off by default, so anybody who can reach it on port 11211 can read or write anything to the cache. Definitely not what you want! Thus the firewall port needs to stay closed. 

Was it ever useful? Yes, and it still is, for exactly the sort of scenario shown in the ancient example. Messages with multiple authors are supposed to have all of them listed in the From: header, with the Sender: set to the person who actually hit Send in their email program. 

Small businesses with small budgets, especially nonprofits, typically are not going to be able to afford high availability. The question is, if you have virtually no budget, as is commonly the case in situations like this, what is your restore strategy? I do have some clients like this, and this is what I do: First, for some of them I have an incremental backup and full database dump every six hours. One client was already using CrashPlan Pro so I just used that. Whatever you do, you need to make sure you have a restorable backup. I have a simple ansible playbook I put together in about an hour (not having previously worked with ansible) that installs nginx, php-fpm and MariaDB and prepares them to host a web site or sites. Running this playbook results in a server (or servers) that are ready to host a typical web application, and I can simply restore the nginx virtual host, application files and database to it. The result of this is that I can bring up such a web site from backup in just a few minutes, as opposed to the manual way which could take an hour or more. 

PHP logs this particular event, (along with many other abnormal things) so you should also watch its logs. For example: 

Keep in mind that if you haven't done so already, you should test your application for compatibility, as the new PECL library may have introduced backward incompatible changes. 

This is probably the root cause of the problem. You seem to have registered nameservers from two different companies, and each of them is returning different data: 

Get rid of any installed packages which don't match your installed repositories and resync your system's installed packages to the packages which are available. This can be accomplished with a single command: 

Even if you could send packets with a source IP not within the cloud provider's control, you could not receive packets to that IP address at the cloud provider. So the short answer is no. Cloud architectures deal with this issue in different ways, using internal load balancers (or you can roll your own). Consult the provider's load balancer documentation. 

No, any programs still attached to the terminal, and not placed into the background with something like , would be killed. This is why there are virtual terminal solutions like and the older which create sessions which continue running even if you are disconnected, and to which you can reattach later. 

All of your systems have a mask, while they should have a mask. Only use for localhost. In the Debian config, you should change: 

Like the other rules you have, this one needs to be stateful, so that FTP data transfers are considered RELATED: 

Though, I see you mentioned being on a VPS from GoDaddy. Unfortunately they give you a "CentOS" which GoDaddy themselves have modified in various ways and may not be built correctly. I suggest moving away from there, if you haven't already done so. 

The documentation you linked to states that nginx will try the upstream again after seconds, to see if it has come back up. 

The fastest way to move such large volumes of data is still Sneakernet. Thus, Amazon Snowball. This device gets shipped to your location, where it can move up to 80 terabytes of data directly off your local network via a 10 Gigabit Ethernet connection. It then gets shipped back to Amazon where they upload the data into your Amazon account. 

So long as the system shuts down cleanly, it will not normally attempt to the drives on next restart (unless the filesystem mount/time count is exceeded, but that's another story). 

There's no reliable public list of IP addresses for Starbucks Wi-Fi, nor is there any reliable way to determine if an IP address corresponds to Starbucks Wi-Fi. After Starbucks (at least in the US) switched from AT&T to Google for Wi-Fi service, Google has been providing the service via any available ISP, whether Google owned or not. For example, the local Starbucks Wi-Fi provided by Google is run on a Comcast Business cable connection. There's no way to distinguish its IP address from any other Comcast Business customer. Therefore, in order to ban Starbucks customers, you will need a fairly large IP address range: 

111 means Connection refused. That is, nothing is listening to that socket. This means that MariaDB is not running, or was configured to listen to some other socket (or none at all). Check your MariaDB configuration to ensure you specified the socket correctly, then restart it. 

This will cause nginx to handle itself. It will then try to serve the file out of the defined document . By the way, you probably want to use different error pages for 4xx errors and 5xx errors. "Not found" or whatever is not what you want people (or search engines!) to see if a backend is down temporarily. 

Anything that had not been shut down by then will be killed. This is in the script which is the final script run at system shutdown. After this runs, begins unmounting filesystems and doing other pre-shutdown final cleanup, then finally actually halts or reboots the system. All other distributions using upstart or SysVinit do something similar. Note that this issue does not arise with systemd, because it is always aware of what services it has started. 

Users can still change the desktop background color in Windows Server 2012. Note that you need to have Desktop Experience installed. You also have to allow users to change their colors and desktop background in Group Policy. The way I did this was: Open Control Panel. Under Appearance and Personalization, choose Change desktop background. 

As you can see, it refers to the email header, and there are invalid characters in that header. Your sender should check his mail program and ensure that it is constructing email headers properly. 

You've booted your system with OVH's custom kernel, which is built to OVH's specifications, and may not be suitable for running software you want to run. Change your dedicated server configuration to boot from the kernel installed on the hard drive provided by your Linux distribution.