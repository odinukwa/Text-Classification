Let me expand on that because it seems to be approaching what I'm looking for. Suppose an error-prone machine is one such that, with a fixed probability $p$, an attempt to change a value on any working tape fails. If it writes the existing value back that always works. Halting with an output means halting strictly more than half the time with the same output, and halting within a certain time means always halting within that time. The latter definition means that if we write each bit by sitting in a loop until it is correct, then the time becomes unbounded; this lets us simulate an ordinary Turing machine, but not with any time translation. But we can simulate an ordinary machine on the error-prone machine up to $t$ steps by writing every bit $b(t)$ times, making its probability of being incorrect $p^{b(t)}$. There is a $b(t) \in O(\log{(t)})$ sufficient to operate perfectly for $t$ steps more than half the time, and $t \times b(t)$ is only loglinear, but for a time translation we need a universal machine and in that situation we're not given $t$ in advance. So what I'm curious about now is how tightly we can constrain the time translation factor and still be correct half the time, and whether we can use something like this to characterize essentially linear time translations vs. quasilinear ones. I believe I can demonstrate that this model has a time translation in $O(t^k)$ for every $k>1$. Dedicate a working tape to keeping track of the number of times our universal machine is to repeat each emulated step. It contains a string of ones delimited by blanks at the ends. The outer loop of the universal machine walks the list of ones from left to right, attempting to repeat the same emulation step in between movements. When it reaches the end, it attempts to write another one, and then resets itself by walking back to the left end, finally advancing to the next emulation step. This seems like more than enough repeats to be half-correct, and we're effectively taking $b(n) \approx (1-p) \times n$ resulting in an essentially quadratic time translation. We can generalize this by adding another counter tape to control the extension of the first one, bringing the time translation down to $O(t^{\frac{3}{2}+o(1)})$, and repeating this gives us any superlinear polynomial bound. Is it possible to bring this down to quasi- or essentially linear? 

Your question suggests the following tentative reduction to obtain a OWF from a secure KE: Given an input, interpret it as the private random coins of two simulated parties to the KE protocol. Based on their private randomness, there will be some sequence of public messages between them, and at some point they will stop and agree on a private shared key (with some high probability). Then the public transcript of their communications is the output of the OWF. You are right to be worried about this approach in the case when Alice and Bob may not agree with probability 1. In fact, this construction is NOT necessarily a OWF in that case. Rather, it is a weaker primitive known as a "distributionally one-way function," which is, informally, a function for which it is hard to generate a uniformly random preimage of $f(x)$, when $x$ is selected at random. It is known (but non-obvious) that one can construct a true one-way function using a distributionally one-way function. See e.g. Exercise 17 on page 96 of Goldreich "Foundations of Cryptography: Basic Tools," which is available for limited viewing on Google preview. To directly see why the above reduction does not suffice, we can use the common "make a stupid modification" technique. Namely, suppose there exists a secure KE protocol where Alice and Bob agree with probability $1 - \epsilon$. Consider a stupidly-modified KE protocol that works as follows: Alice and Bob first each flip 100 coins. For each of Alice, Bob individually: If all 100 coins are zero, their remaining randomness is interpreted as encoding a sequence of messages which they send to the other party, ignoring whatever the other one says; then they just output a random "shared key." (Otherwise they follow the original protocol.) Given any public transcript, it is possible (with negligible probability) that it was generated by an Alice and Bob who BOTH flipped 100 zeroes and happened to have their remaining randomness specify that exact sequence of messages. This is still a secure KE protocol, where Alice and Bob will agree with probability at least $1 - \epsilon - 2^{-100}$. On the other hand, it utterly fails to be a OWF in the hoped-for manner, since an attacker who sees some public transcript can always trivially invert by choosing 100-zeroes randomness for each of Alice and Bob. 

This is similar to how we define computability in terms of Turing machines and then promptly forget about Turing machines. Since it turns out a Turing machine is as good a definition as any other, we use it as an anchor for an entire equivalence class of models, and we end up with the same class no matter which element we generate it from. Basically this is the Church-Turing thesis and it defines the set of computable bit strings. Similarly, to define computability on a different set $S$, we anchor it with a particular partial function from bit strings to $S$. Actually it does not matter if this function is a bijection or an injection or any other type of function (for a case where we don't really want it to be an injection, consider a group defined by its presentation where we don't have a unique representation for its elements). It doesn't even have to be a surjection if we permit that singleton sets can be uncomputable. By composing this function with any computable bijection from bit strings to bit strings (a concept already defined), we get a definition of computability for $S$ that's invariant with respect to the function we originally picked (as long as we picked something reasonable). That is, a CT thesis for our set $S$. But if we don't pick a reasonable function, we get a different definition of computability. This function also serves to define the computability of other functions with domain or range equal to $S$. By changing the range to $S$, keeping the domain as $\{0,1\}^*$, we also get an $O(1)$-invariant definition of Kolmogorov complexity for $S$. And we can finally say that the function we have chosen is itself computable. So I think the answer to your question is NO. We have to define computability for each set that we want to talk about, because there are non-equivalent definitions. Aside from a very technical or pedagogical discussion it shouldn't be necessary, since a reasonable person can imagine a reasonable definition independently. But wait, let $S$ be a countably infinite set, and that's it. What's our reasonable definition of computability for $S$? Knowing that the set of bijections between $S$ and $\{0,1\}^*$ is non-empty doesn't tell us which are reasonable. We're out of luck without more details. And we may encounter multiple inequivalent but equally-reasonable alternatives. Suppose every tree has some number of red leaves and some number of green leaves, and that for every $r \in \mathbb{N}$ there exists exactly one tree with $r$ red leaves, and that for every $g \in \mathbb{N}$ there exists exactly one tree with $g$ green leaves. Both bijections are reasonable in the sense that we can count the leaves and distinguish the colors, and we can walk aimlessly around the woods counting leaves on trees until we find the tree with exactly $23$ green leaves, or the one with $23$ red leaves. It's not clear whether to identify a tree using its red leaf count or its green leaf count because this choice leads to inequivalent definitions of computability for sets of trees. If we instead make our definition by combining the counts with a computable pairing function bijective from $\mathbb{N}^2$ to $\mathbb{N}$ (having appropriately defined computability on $\mathbb{N}^2$), that also uniquely identifies each tree, but the situation is even worse because this is not a bijection between trees and $\mathbb{N}$, now maybe all computable sets of trees are finite! So in order to avoid the entire discussion it should be understood not only that there exists a reasonable definition of computability on the set in question, but also that there is exactly one class of reasonable definitions. I think the situation becomes a lot more interesting if we bring time complexity into the picture. Even when just considering integers, our choices matter more. For example, what if we wanted to represent each number as a sum of four squares? We can find such a representation, starting from a base representation, in expected quadratic time with access to randomness. Or instead, as a list of its prime factors, which may or may not be possible to compute in polynomial time. To the extent that we allow hard representations, we lose precision in time complexity. For example, we can't meaningfully say that some function $F:\mathbb{N} \rightarrow \mathbb{N}$ is computable in quadratic time if we have a representation of $\mathbb{N}$ that might require more than quadratic time to convert to or from a base representation. I think this perspective reveals that base representation is a somewhat arbitrary standard. (Standard in the sense that base representation is what anyone has in mind when they say something like "$F:\mathbb{N} \rightarrow \mathbb{N}$ is computable in quadratic time", if the underlying model is one that computes bit strings from bit strings and we're supposed to infer the meaning.)