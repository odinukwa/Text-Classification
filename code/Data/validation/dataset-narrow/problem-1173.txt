If I understand correctly, you just want a definition of the logic; then you can find a definition of MSO$_j$ (where 2 is a special case) by example in ''Elements of Finite Model Theory'' of Leonid Libkin. Even if you are interested in infinite model, the definition of the logic is the same. Quickly is just the set of formulae with existantial quantification over sets of the elements of the universe, then existantial quantifications over those sets; then a first-order formula. 

Usually I give the factoring problem as example; I first ask for the number that divide 15; usually people can answer 3, 5, and have fun wondering if 1 and 15 are correct answer. Then I give a huge number (more than 10 digits) and ask if they can tell me what are the dividers; and I explain that, even for computer scientist, this is a really hard question. Then if I have time, I try to explain that the question is either to figure out how to solve this problem, or to prove that it will always take a lot of time( a notion that we precisely know how to define). And then a little word of cryptography, to explain why it is usedd, and a word about how many time it take team of scientist to break the key of number with hundreds of digit (I avoid to speak of bits because people seems to better know what a digit is) 

${\Sigma_2^P}^{NP}$ is the set of language decided by an alternating turing machine in existential, and then universal state, with an oracle in NP. Both the universal and the existantial part can querye NP. Hence, in this case you decided to write this as $(NP^{NP})^{A}$ then the way you should think of it is as $(NP^{NP^A\cup A})$ (by $\cup$ I mean an oracle either to $A$ or to an $NP^A$ language). Hence ${\Sigma_2^P}^{NP}$ is equal to $(NP^{(NP^{NP})})^{NP}$ which is certainly equal to $(NP^{NP^{NP}})$ since every query you could make to the $NP$ oracle, you could make it to the $NP^{NP}$ oracle. 

To answer to your comment, I guess I should make another answer, speaking only on Krom and Horn (May be I should ask a question about those to CSTheory) I suggest that you read section 5.3 page 34 of my paper about the problem I met on Horn and Krom in High Order logic. You will meet the same problem in Variable Order (which is clearly a superset of High Order). I don't know if you did pay attention to it, but SO(krom) is equal to P when the first order is universal; indeed you can express NP-complete problem if you add existantial first order variable. (I don't remember the example I had before, I can try to search it if you want it) I don't know what this syntactical resctriction would become for high order or variable order logic... my point is just that you should also think of a good way to restrain quantifiers, because restraining the quantifier-free part alone is not usefull (at least for Krom formulae) 

I have a question that seems to me really natural and have probably already been studied. But keyword search on this site or google does not seems to help me to find any relevent paper. I have got a finite non deterministic automaton $A$ over an alphabet $\alpha$ without epsilon-transition. What can I tell about the number of different path the automaton could take for accepting a word ? In particular, I want to know if this number is bounded, or if for every $c$ I can find a word $w_c$ that is accepted in at least $c$ different way by the automaton. Right now, I can find some necessary, and some sufficient condition, but not any necessary and sufficient condition, for the number to be unbounded By clarity, I'll define the way I cound the number of accepting path. Let $w\in\alpha^*$ and $q$ a state, I can define the number of path to $q$ by inuction on $|w|$ by $N(\epsilon,q)=1$ if $q\in I$ else $0$, where $I$ is the set of initial state and $F$ of final state. $N(ws,q)=\sum_{q'\in Q\atop \delta(q',s)=q}N(w,q')$. Then the number of path is $\sum_{q \in F}N(w,q)$. 

I guess that notions I describe are already well known, may be by combinatorician, but I do not know their name or any book/article about them. So if you have a link/title I would love to read it. Let $r$ be an integer, let $P_r$ be the set of partial (pre)order over $[1,r]$ and $T_r$ the set of total (pre)order. I say that $P\in P_r$ is included in $T\in T_r$ if it is included as subset of $[1,r]^2$, or to state it another way, if for all $i,j$ with $i$ less than $j$ for $P$ then it is also less for $T$. Finally I say that $P$ and $P'$ are incompatible if there is $i,j$ with $i<j$ for $P$ and $i>j$ for $P'$ (or if $i=j$ for $P$ and $i<j$ for $P'$). Let $P\in P_r$ with $i<j$ and no $k$ such that $i<k<j$, then $P_{i,j}$ is the same partial (pre)order, except that $i$ and $j$ are incomparable. I would like to find an efficient data structure to store a subset of $P_r$. I can't imagine something better than a trie of depth $r(r-1)/2$, with one level for every pair $(i,j)$ with $i<j$. If possible I would want to be able to efficiently add and remove elements from the set, or at least to easily transform $P$ into $P_{i,j}$ as defined above. I need to know if for a given subset $S$ of $P_r$ and $P\in P_r$, $P$ is incompatible with every $P'\in S$. Or an equivalent problem would be to figure out if a set $S$ is such that its element are one to one incomparable. I also need to know if for every total (pre)order $T$ it is a superset of a partial order $P\in S$. Intuitively, to each $P\in P_r$ is associate its set $T_P=\{T\in T_r\mid T\supseteq P\}$ and I need to know if $(T_P)_{P\in S}$ form a partition of $T_r$. Then let $T_S=\bigcup_{P\in S} T_P$, then I would also be interested by having a way to efficiently describe $T_S$. (That is, efficiently checking for a given $T\in T_r$ if $T\in T_S$. 

As a side note that came up in the comments, it seems to me that when restricting $\mathrm{Fix}$ to positive type constructors, i.e. type constructors $F$ for which (for each variable $f$) there exists a term $t_f$ such that $${A:*,B:*,f:A\rightarrow B\vdash t_f:F\ A\rightarrow F\ B} $$ then there is no encoding of $\mathbb{N}$ such that induction over $\mathbb{N}$ holds. The rough argument for this is to encode $\mathrm{Fix}\ F$ by $$\forall X, (F\ X\rightarrow X) \rightarrow X$$ which is the usual encoding for positive inductive types in system F, and show that every well typed term in the system with $\mathrm{Fix}$ can be well typed in the CoC without it. Then we can conclude by the fact that CoC doesn't have any inductive definitions of $\mathbb{N}$ itself. The encoding is a little messy, though, I think. I'd be happy to have a reference for this. 

I assume you're looking for ways to compute the Control Flow Graph of a given program. This graph depends (obviously) on the actual operational semantics of the language you are interested in, and so cannot be computed from the AST alone, in the sense that some knowledge of the language semantics themselves are needed. For a given language it is possible to build the CFG given the AST of the parsed program, and indeed this is usually a very important step in compilers to perform flow analysis. This can be found in chapter 17 of Appel or in chapter 7 of Muchnick. 

The way Haskell solves this problem is through Type Classes (see these papers for an overview). The idea is that it is not the $\_^{-1}$ operation that is special; it is the $\mathrm{\bf Array}$ type! To this end, you can define a (statically) overloaded method $map$ which works on each such type. In Haskell: 

The answer is yes, depending on the meaning of "nothing to do". I'll leave the Curry-Howard aspect aside somewhat, but it underlies a lot of this approach. The key idea is Wadlers' Theorems for Free! building on ideas from Reynolds, which allow you, given a program type, to deduce something about the program. For example, a program $f$ of type 

I think there's a larger discussion to be had about how to distinguish syntactic notions from semantic notions in the field of formal logic, but I'm not sure I feel qualified to have it. 

Let me clarify one subtle point: first order logic is only undecidable for certain given languages. In particular the language $\cal{L}$ that contains only monadic predicates, that is, predicates of the form $P(x)$ and no function symbols, is decidable. If you allow function symbols or predicates with more than 1 argument, then $\cal{L}$ usually becomes undecidable. The proof involves encoding Turing machines and their computation sequences using the symbols of the logic. Then one adds a finite series of axioms $\phi_1,\ldots, \phi_n$, and build a formula $\psi(x,y)$ such that $$ \phi_1,\ldots,\phi_n\vdash \psi(\overline{n},\overline{m})$$ is provable if and only if 

Typically, the first 1-3 phases turn what is essentially a stream of bytes or tokens into something more naturally seen as a tree structure. Everything before the tree creation will be labelled as "syntax" and everything after as "semantics". In C, the syntactic phases are typically tokenization, macro expansion and parsing, everything after that will be semantics. Again, opinions may differ, since there is no accepted formal distinction. 

There's no agreed upon "bible" for CT for computer science in the same way as for mathematicians (Mac Lane), probably because the field is younger and a bit broader. It really depends on whether you want to understand . Here are a few computer science concepts with category theory counterparts: 

By the way, this class exists and is widely used in Haskell, it goes by the name Functor (and $map$ is $fmap$ for some reason). This solves the problem for unary operators. For binary operators, or more generally, $n$-ary operators, things get a tiny bit trickier: in this case, you need to figure out what you mean. Do you want the sum of all pairs of numbers in each array, or just the "fusion" of the arrays by the operation sum? I suspect you mean the latter. The way to solve this problem is by another Type Class named $\mathrm{\bf Applicative}$. The definition is 

The answer, I believe, is no. Certainly, direction $\Longrightarrow$ holds, as you noted. But in the other direction, we have some counter-examples: in system $\mathrm{F_\omega}$, the fact that "type level" computations are normalizing is a consequence of the normalization of the simply-typed $\lambda$-calculus, and in Girard's system $U^-$, type level normalization holds, whereas it does not at the term level! Things are a bit more complicated in the CC though, since type level terms can contain arbitrary term-level terms. The trick is that to compute head-normal forms, you don't actually need to dip into those terms to do reductions. In PTS lingo, you only need to reduce $(*,\square, \square)$ and $(\square, \square, \square)$ redexes. But normalization of those only relies on the normalization of some variant of the simply typed $\lambda$-calculus (though I won't detail that argument right now). The hard part is showing that reducing the 2 types of redexes above must result in a head-normal form for terms of type $*$ or $\square$. You can do this by contradiction by supposing there is a head redex at that level, and showing that it must be of one of the two forms above.