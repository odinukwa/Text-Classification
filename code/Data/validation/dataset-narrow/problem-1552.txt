The convolution neural networks take into consideration that an image already has a two-dimensional structure. This is a domain knowledge provided by humans and not something the neural network can pick up. For example, consider if I would give you the raw pixels shuffled in a flattened array (or any other signal for that matter). Would you be able to reconstruct its 2D structure? Probably this would be almost impossible (unless someone builds an algorithm to solve jigsaw puzzles at the pixel level!) Applying convolution with various strides we are looking for patterns in the patches of the image and not on the individual pixels. Using different strides we are explicitly expressing the amount of resolution that we are investigating the patterns of the image. So given that these convolution filters are trainable we are able to have an architecture that works well with images. In fact, it would work well with any signal that would have a 2D structure. 

Note that we have already experimented with various steps and repetitions. So in one example we took the first 200 batches and trained them two times before going to the next batch and so on until an epoch is completed. In another example we took the first 10 batches and trained them only once and then the next 10 and the next 10 and so on until the end of the epoch. All of the experiments so far have concluded that the neural network is has a relatively ok accuracy at the beginning and this gets worsen as the more difficult instances come along. The final accuracy is much worse than expected and in addition the maximum accuracy is still quite bad. Why is this curriculum learning not working? Is anything missing? 

Given that we have not applied any preprocessing steps then the first thing that comes to mind is that SVM works better in higher dimensionalities while kNN suffer from the curse of dimensionality and therefore our dataset should have a large dimensionality especially in comparison to the number of instances. Therefore we have a very sparse space where the points can be separated by a hyperplane while the k-nearest neighbors give little information of the actual class. Note that we should take under consideration that we have no knowledge if the test size follows the rule of thumb (10%-20%) and we have not repeat the experiment with multiple train-test splits to get a range of possible errors. We might have a very unlucky test set.. Finally we should take into account that we have little knowledge if the classes are imbalanced and how much. A highly imbalanced class could cause most of classifications to belong to the majority class while SVM would be worse because it would try, in vain, to find a hyperplane that would separate the classes and thus have a larger error. However here we have the opposite scenario. Therefore we should assume the case where the classes are more or less balanced but they are separable only in one (or a few) dimensions while pretty much mixed in the rest of the dimensions. Since knn does not learn any weights for each dimension, it assumes that the ones closest are the correct points while svm has taken a clear cut decision that anything beyond this line should be classified as such. 

Consider the following problem, simplified: Let's say that you have some data where you have only 5 columns/attributes. One of them can be seen as a reward. How well we performed at a given run. The other two attributes, let's say x and y are our input to the system. A human hand picked thoseÂ x, y values and they were recorded. Another two attributes are observations from sensors. These are not inputs and we have no control over them. Let's call them ob1 and ob2. All are real values (nothing is discrete). One idea is to look at this dataset as an ordered dataset. We are asked to choose input values x and y that maximize our reward. We have available ~70.000 instances of this five-tupled dataset This is one approach that comes into mind but not sure if it is correct in principle or the simpler one. We could build a predictor which takes as input the four attributes (x, y, ob1, ob2) and has as target the reward. Then try a repetitive process for the inputs by using reinfocement learning (?) to get the inputs that maximize the reward?.. 

First of all we have a classification task. So we use the typical softmax cross entropy to classify. Current implementation of curriculum learning is as follows. 

So we have that a word to vector model has been trained on a certain corpus to be able given words as inputs (one hot encoding is one way for the input) to represent the word as a vector of a usually high dimensionality of numbers. Does it carry information? Yes, we could say that it carries the information about the words. Similar words have similar vectors. The point is that us as humans we do not define the similarity but the neural network defines the similarity for us based on the corpus that we are providing. Does it carry meaning? No, because this representation does include grammatical or syntactical information and therefore the answer would be that it does not carry any meaning. 

First of all, we should say that a single affine layer of a neural network without any non-linearities/activations is practically the same as a linear model. Here we are referring to deep neural networks that have multiple layers and activation functions (non-linearities as relu, elu, tanh, sigmoid etc.) Second of all nonlinearities and multiple layers introduce a nonconvex and usually rather complex error space which means that we have many local minimums that the training of the deep neural network can converge to. This means that a lot of hyperparameters have to be tuned in order to get to a place in the error space where the error is small enough so that the model will be useful. A lot of hyper parameters which could start from 10 and reach up to 40 or 50 are dealt with bayesian optimization using Gaussian processes to optimize them which still does not guarantee good performance. Their training is very slow and adding the tuning of the hyperparameters into that makes it even slower where in comparison the linear model would be much faster to be trained. This introduces a serious cost-benefit tradeoff. A trained linear model has weights which are interpretable and give useful information to the data scientist onto how various features play a role for the task at hand. 

Facing this dataset: $URL$ I treated it both as a classification task and as a regression task. Due to the fact that classifying the quality of a wine as 8 while it is a 3 is a huge difference while classifying a 5 as 6 is not that bad This has two subquestions: 

In a nutshell it is promising but it lacks in multiple points. The research below explains why it is better/worse than Numpy and Tensorflow: $URL$ 

Are there any other approaches with perhaps material we are not familiar with like for instance Bayesian Inference?... 

In this link: $URL$ Ipsative Standardization is explained intuitively and also the why it is useful. Is there any python code or math formulas online someone could use to calculate the z1 score? 

When using Batch Normalization we can use the batch mean and batch variance during training. However this fails to give good enough results in validation case. We get the best results by providing the population mean and population variance. However this means that the two graphs will be almost identical except the part where we need to subtract the mean and divide the standard deviation (we use the population ones) Our current implementation worked ok but it includes too much boilerplate code. Note that in other frameworks you can extract and manipulate layers of a model quite easily for such cases, but tensorflow has an only append policy to its graphs What is the optimal way to go in such cases where one graph is slightly different from the other? 

The sigmoid function becomes asymptotically either zero or one which means that the gradients are near zero for inputs with a large absolute value. This makes the sigmoid function prone to vanishing gradient issues which the ReLU does not suffer as much. In addition, ReLU has an attribute which can be seen both as positive and negative depending on which angle you are approaching it. The fact that ReLU is effectively a function that is zero for negative inputs and identity for positive inputs means that it is easy to have zeros as outputs and this leads to dead neurons. However, dead neurons might sound bad but in many cases, it is not because it allows for sparsity. In a way, the ReLU does a similar job of what an L1 regularization would do which would bring some weights to zero which in turn means a sparse solution. Sparsity is something that, lots of times, leads to a better generalization of the model but there are times which has a negative impact on performance so it depends. A good practice when using ReLU is to initialize the bias to a small number rather than zero so that you avoid dead neurons at the beginning of the training of the neural network which might prevent training in general. 

We are looking for ways to have a project already implemented in the aforementioned stack scale for very large datasets by doing the minimum amount of work Counter examples would be to rewrite everything in Tensorflow framework or use industry tools that are unrelated with Python.