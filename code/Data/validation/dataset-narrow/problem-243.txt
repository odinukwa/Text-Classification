There is no perfect answer for this only you can decide the best option. And it has a lot to do with tradeoffs, and preferences. How much time you have now vs maintenance in the future. You could have: 

What do you mean by archive? Do you want to delete the data? Moving it to another db or server will still take up the same space. It may improve performance but you're just moving the problem around. Do you need the data in its original form? Can you aggregate the data to reduce the number of rows? If you do move the data to another table you may find a clustered columnstore may give better compression and save disk space. Personally i would be using ssis and not a sp using linked servers. As you have a primary key id be using that as a way of identifying what has been inserted and then can be deleted from the original table. An index on the column you are using for date range would be extremely beneficial. Id also consider using smaller batches such as one days worth at a time. 

Log in with the user that has the User DSN entries that you with to convert to System DSN Open the Registry Editor and navigate to HKEY_LOCAL_CURRENT\SOFTWARE\Microsoft\ODBC\ Right-click ODBC.INI and select export to save it as a file on the desktop (or anywhere else you fancy) Open the .reg file with a text editor such as Notepad Replace the text HKEY_CURRENT_USER with HKEY_LOCAL_MACHINE. Save your changes Double click the .reg file and proceed to import it into the registry. 

This is as far as I managed to get unfortunately. My skill is limited where it gets to do an INSERT with a condition with the NOT EXIST condition. How can I make sure that I only insert rows in the xxx.MyTable where rows with the same primary key (period AND genusId AND subjectId AND waitingStageId) does not exist already? 

I am trying to create a contained user for a database in SQL Azure that only as read-only access. What I am doing is: 

I'm trying to get started with R services on SQL Server 2016. I was following this example from MS on youtube. 2 min 22sec into the vid John shows a reference to library(RODBC)... which I don't have. So I have downloaded the RODBC driver from CRAN According to the documentation: 

For simplicity if this was for a limited time frame, i would go with backup and restores. Unless you are required to respond within minutes you could probably get by with daily backups. You could even set up a job to take a backup and FTP it on demand, after they have reported a defect. I wouldn't even bother restoring daily unless the issue was specific to newly created data. In my experience UAT is more about functionality. So you test with existing data and create new data. Having a db a few days behind theirs means you should be able repeat the test and reproduce the defect. This approach would get slower, more difficult and less feasible the bigger the db is. If you will be providing ongoing support, the db is huge, and you have SLA saying you have to respond within minutes, replication/synchronization of some sort might be worth the effort to set up. The best solution for you may be dictated by client security or technical policies. Work with them to ensure what you are proposing is acceptable. If not keep in mind there are lots of variations on this theme. But it all boils down to: 1. you need something to take a back up, and 2. you need something to FTP the files either on a schedule or on demand. If I was doing this personally I would begin with Ola Hallengren's backup scripts. After installing that I'd create a SQL Agent job which invokes the backup. I would then edit the agent job to add a step to FTP the files. I just had a quick search and found this example of how to use SSIS to FTP a file and I found this example of an FTP script for SQL Server. My recommendation is that you set this job up to run on a schedule, and only run it on demand if truly required. If you don't have remote access and permission to execute tasks, you can request someone on the client site execute the task for you. 

This only seems to happen with ONE table (all other tables are fine). So we tried recreating the table, but the same problem persists. Any idea of what might be causing this issue? 

As part of an ETL process, I need to copy a varchar field in table1 (staging table) which is meant to contain an integer value to a tinyint field in table2 (production table). However I am having some issues when trying to combine the following two business requirements: 

If I connect to the SQL server's SSIS store (not the DB engine) via SQL Server Management Studio (SSMS) and I right click on the job in question that is stored under "Stored Packages\MSDB" and choose to execute it, the job will run without any issues. This happens whether I am using the local SSMS installed on the SQL Server in question, or if I am using a SSMS installation on a remote workstation. However if I schedule the job through the same SQL's server database engine, the job will fail -- both on a schedule and if I try to run the job manually. Now here is the puzzling bit: The job will not fail if I have on the background a remote desktop connection into the SQL server with that super user account (i.e. spadmin) while I run the job. By on the background I mean that I am not doing anything on this remote desktop connection except login in with the super user account. When the job fails, I get the following "Bad Gateway" error (see end of post) that suggests the problem is accessing SharePoint. However since I can run this job via the SSIS store with the same account for which the job has been scheduled, there is no doubt that this job is capable of running from the SQL Server. Server build: 10.50.1617 I am going mental here. Any ideas of what the problem might be? Here is the full error message for completeness sake: 

Our organization has an externally hosted website that displays data from a monitoring system. Data is currently provided by FTPing csv files. The data is stored in a SQL Server Database. The exported data sets are only a few hundred rows of data, updated hourly. I would like to replace this with a a web service so that data can be retrieved on demand, and we avoid handling files. I'm a DBA not a programmer, so i'm out of my comfort zone. Is there a conventional approach for doing this? Are there standard Microsoft products that we could configure to achieve this? Or do we need to hire a programmer to develop something custom from scratch? I know SSIS can consume data from a web-service but can it provide data as a web-service? 

Only you can answer if the the change is worth the effort. Yes restructuring would be marginally more efficient. The main advantage being that new sensors can be added in the future without altering your table structure. The downside is that it's harder to enforce business rules and logic. The advantage of your current design is that if the sensor types are consistent per column you could have different data types, int, float, bit, varchar etc, whereas if you put all of your sensor values in a single column you'll need to use the lower common denominator (most likely a varchar). Given that you're talking about sensors, I expect you will be recording over time, so you'd probably need a timestamp, and decide on what else need to be stored with the value. Quality codes? non-detects? what about the sensor type? 

As soon as I logged off from that SQL server, the behavior occurred again. This was hard to troubleshoot at first because the drive installs without throwing any errors and it even works at first. But I am confident now that this has to do with permissions. Any ideas what I might be missing? 

I have a date dimension table in which I need to add a new column in which I define the iteration of the day of the week within the month (2 for the second Mon/Tue/Wed/Thu/Fri/Sat/Sun etc). Is it possible to do this be making calculations solely on the date column of the table, which is of type 'date'? 

If the value of the source varchar field is blank, then it will be set as NULL in the target tinyint field. For that I would normally use NULLIF. If the field contains anything that could not be cast as a tinyint value (e.g. punctuation, letters, or a value higher than 255) then the value will be set as 255 in the target tinyint field. The rationale behind this is that the tinyint field would never have a 255 value unless there is an error -- and it wouldn't be appropriate to have errors set as NULL or 0 because these are perfectly valid values and we wouldn't be able to differentiate between a valid entry and a mistake. 

No it shouldnt cause any issues. "CU" means cumulative update. It has all fixes that apply since the last service pack. They are designed exactly for the scenario you described. 

Running SQL 2012 SP2 (11.0.5343) on Win 2008 R2 Standard SP1 on VMWare. Edit: Updated to SQL 2012 SP3 (11.0.6020). Edit: have added more context and clarified. Following suggestion from @TheGameisWar, I queried ssisdb SELECT * FROM [SSISDB].[catalog].[event_messages] WHERE event_name = 'OnError' Returns no error messages for that date. 

If its an ETL process high PLE isnt very important. Your focus should be on throughput rather than cacheing. PLE is an indication of how long a page is held in memory. The more volatile the data is the lower the PLE. For lookup of reference data, A high PLE indicates that is is being cached in memory and is being reused without reloading from disk. ETL processes will have low PLE as they are deleting tables and reinserting, anything that queries that data will have to fetch it from disk. That's normal. You will also see high IO (disk reads & writes) when inserting and deleting data, regardless of how much memory you have. All transactions are written to the transaction log (even if your db is in simple recovery mode) before being saved to the datafile. The more tables and queries your data moves through during your ETL the more read and writes you will see. To improve throughput and performance you should investigate if your ETL process is using bulk inserts and truncates rather than individual deletes and inserts, as significant gains can be made here. Depending upon the ETL tools you are using there are a number of features that may assist with throughput performance. I suggest you start by reading about optimizing bulk imports and minimal logging. 

The solution was to change the processing in Visual Studio (VS) from Default to Full. The issue is described in detail here: $URL$ (see Cathy Dumas's reply under "Posted by Microsoft on 8/25/2011 at 2:02 PM") 

Here is the answer. Even though the job was configured to run via a PROXY Account, the SQL Server Agent is still responsible for the job. I had a look and the SQL Server agent was configured to run under the Local System Account on that server. So what I did is to put the agent to run under the superuser admin account and it worked as expected. Now in this case the fact that the job no longer needs a proxy since the Server Agent itself is running under the ultimate account. However I appreciate that this is not the right way moving forward (even though this isn't my server and I hope I never get to touch it again!) I will be advising the customer to reconfigure SQL so every service runs under a dedicated domain account (i.e. created solely for this purpose), which is the way it should be! Now what I would love to understand is why the job would run as long as the proxy account used for scheduling the job was logged into the SQL server!