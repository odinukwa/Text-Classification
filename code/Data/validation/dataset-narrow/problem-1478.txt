I'm sure we can agree that there are some parts of theoretical CS that are unlikely to change - topics such as computation theory (Turing machines, automata) will be useful and applicable for many years to come, [or at least until quantum computing becomes hugely pervasive]! I think a strong theoretical base is extremely important and I greatly enjoyed the modules I took at university which covered this sort of theory. Most courses do include a practical aspect, though. One of my frustrations as a student who started at university with experince in professional software engineering in industry, was that a lot of the more practical content was incredibly outdated, not in line with industry best practices or just downright irrelevant. As a concrete example, I took a web development module in 2015 which focussed on PHP 4, taught us to write code riddled with XSS, CSRF and SQLi vulnerabilities and didn't place any emphasis on architecture design. Some parts of the course were useful (such as the primer on how HTTP works at a protocol level) but it was obvious the same slide deck had been reused year after year and I came away feeling like I'd wasted my time. Some of these details seem to be out of the lecturer's hands and determined by the department, but the end result is still the same. I do realise web is perhaps one of the worst areas for this, but it's equally applicable when e.g. introducing Java and not covering the "new" generics support, or the more functional features in Java 8. What can educators do to try and prevent this from becoming too big an issue? I think trying to keep up with each new shiny JavaScript framework that becomes momentarily popular is unrealistic and useful for neither student nor educator - but there must be some middle ground. Should these sort of modules be entirely theoretical to avoid this problem, or perhaps collaboration with industry could be sought out to explain how things really work in practice? 

The NetLogo agent-based modelling environment is a teaching tool which uses an enhanced Logo that can have inputs hooked up to sliders. I have played with its simulators sometimes just for my own amusement. A nice simpler model is the one displaying how freeways get clogged, just playing with the rate at which cars accelerate and decelerate. Important most models have both a Setup and a Go button. If you don't press Setup nothing happens. All the nodels have an Info tab at the bottom which explains how the model works and usually the effect of its input variables. 

I think you can teach 2D graphics using a tactile approach on a phone or tablet. Students are very likely to be used to tracing their fingers over a mobile device (I have watched awesome videos of blind iPhone users in action). So position and shape change of an object could be detected by them dragging their fingers over to get auditory feedback at the edges. Ideally you want a truly tactile surface but I don't know of any such devices. I have my own reasons for wanting a computer-controlled version of one of these For actual image processing maybe map colours to tone, so as they move their finger over an image they hear a tone vary depending on the average pixels underneath. Then, as transforms are applied, the tones would change. You could possibly teach colour theory by varying the mapping to only one channel of RGB. Update in one of those great examples of synchronicity, a few days after writing the above I was listening to Matthias Tretter of MindNode on a podcast mention how he had been convinced to add accessibility to MindNode, a visual mindmapping product by a blind user. His blog posting has videos showing VoiceOver in action. This is a bit tangential to the original answer but a nice anecdote about how preconceptions about the intended purpose of a tool as being visual doesn't rule it out for blind users. 

Computer Science lends itself to a variety of different assessment methods - from the more practical assignment based programming problems, to the more formal final exams that cover the theoretical content. More formative assessments give students and educators the ability to improve learning and teaching whilst it is ongoing, but there is usually some concept of a "high-stakes" summative exam in most courses. I'm sure we can all agree that portions of one module (e.g. on data structures) will be exceptionally useful in later modules, and so it makes sense to do as much as possible to help students master these concepts. Often (from my own experience as a student), the "feedback" from the summative exam is a simple number or grade, which isn't broken down by assessment component. Further, there is usually no way to see the exam script or any form of mark scheme or question breakdown, which makes it difficult to improve in the general context of the university degree. The summative exam does nothing other than provide a coarse measure of the candidate's performance during the exam. Is this sort of status quo widespread throughout undergraduate level CS, and what can be done to give students the opportunity to learn from and reflect on these sort of assessments? Should a more formative approach be taken for the majority of each module? 

I was used to mathematics, and a calculator which used fixed point arithmetic - an environment in which the answers were always "right" (to a given number of decimal places)! The way most programming languages handled decimal numbers felt broken and almost certainly useless for storing monetary values or for use in scientific code. I didn't appreciate the trade-off being made in terms of speed and space by taking the approach IEEE 754 did until much later on when I looked at the specification in detail and the analogy with standard scientific form was brought up. At what stage should students be introduced to how floating point numbers are stored, and (more importantly) how can the motivations for this approach be properly conveyed?