To maximize the expression, given $x_i$, should set $y_i = m$ for the value of $i$ maximizing $x_i - \alpha$. This implies you should set $x_2, \dots, x_n = \alpha$ and $x_1 = 1 - (n-1) \alpha$. (I assume here that $0^0 = 1$; otherwise you must set $x_i = \alpha + \epsilon$, and the resulting function does not realize its supremum) 

Seemingly, giving $\rho(\pi(1))$ the distribution on the minimum of $n$ uniform variables, and so on, would work. Is there are any reference? 

I am a PhD graduate student in applied math who faced this exact problem last year. At my university, the applied math track offered much more flexibility in terms of course requirements. The CS track required various theory courses, which I wanted to take, but also required courses in networking, operating systems, and other things that held no interest for me. The applied math track basically allowed me to mix and match courses from either department with almost unlimited freedom. I actually am taking more CS theory classes than I would have been allowed to as a CS student. 

The posting on MathOverflow tells how to go from a small number of independent Uniform[0,1] random variables to a larger number of pairwise-independent Uniform[0,1] random variables. You can of course go back and forth between Uniform[0,1] and Gaussian by inverting the CDF. But that requires numerical analysis as the CDF is not closed-form. However, there is a simpler way to from Gaussian to uniform. Given two independent Gaussians $X_1, X_2$, the angle $\arctan(X_1/X_2)$ is uniform in the range $[0,2 \pi]$. Similarly, the Box-Muller method transforms two independent Uniform[0,1] variables into two independent Gaussian random variables. Using these two transformations, you consume two Gaussians to produce a uniform or two uniforms to produce a Gaussian. So there is only a factor of $O(1)$ in the sampling efficiency. Furthermore, no inversion of the Normal cdf is required. 

I have an idea that might work. We start with a generalized suffix tree for sequences $S$ and $T$. Each internal node with suffixes of both $S$ and $T$ in its subtree corresponds to some common substring of the sequences. Let us call such nodes non-trivial. The common substring is maximal, if the corresponding node has no non-trivial children. If node $v$ is non-trivial, we store the largest string-depth of a non-trivial node in its subtree as $lcs(v)$. If $r$ is the root, then $lcs(r)$ is the length of the longest common substring of $S$ and $T$. Updating the tree after deleting a substring from one of the sequences should not be too hard. We first delete the leaves corresponding to the deleted suffixes, updating their ancestors when required. Then we start processing the suffixes preceding the deleted substring. Let $v$ be the lowest non-trivial ancestor of the current leaf. If the length of the suffix is $k$ (we are $k$ steps from the deletion) and $k < lcs(v)$, we have to move the suffix to its proper position in the tree, updating the ancestors when required. If $k \ge lcs(v)$, we are done, as we are not interested in subtrees with trivial roots. The overall algorithm repeatedly finds the longest common substring of $S$ and $T$ and deletes one of its occurrences from both sequences, as long as the length of the LCS is large enough. There are some technicalities, but the general idea should work. 

If you can compute the raggedness of a line without knowing anything about the other lines, then you can model the problem as finding a minimum-weight $M$-link path in a graph. With concave integer weights for edges, there is an algorithm that solves the problem in $O(N \log U)$ time, where $U$ is the largest absolute edge weight. Another algorithm solves the problem in $N 2^{O(\sqrt{\log M \log \log N})}$ time for any concave edge weights, assuming $M = \Omega(\log N)$. Both algorithms assume that you can compute the weight of an edge in constant time. You could also use binary search to find a line width such that SMAWK uses $M$ lines with it. In some cases, this algorithm does not guarantee a solution with exactly $M$ lines, however. 

Let $H$ be some complex hash function (almost any function will do), mapping long bit strings down to a single bit. Then to decide whether $H( A \times B ) = 0$, you will basically need to multiply $A \times B$ and compute $H$ on the resulting product. Unless $H$ has very special properties, there won't be any short cut to this. 

The typical approach is to analyze the expected value of the running times of the algorithm. In this case, the expected value of the running time, on any problem with $m,n$, is something like $k^2$ times the expected running time of the subproblems partition[$A_i,B_j$]. For each $i,j$ the size of the subproblem is a random variable. The length of the list $A_i$ is the difference between the $i$th and $i+1$th order statistics when $k$ elements are drawn u.a.r from $\{1, ..., m \}$. This is essentially $m \beta(k,m+k+1)$ (i.e. is $m$ times a draw from a $\beta(1,m+k+1)$ random variable). Hence we have $$T[m,n | k] = C (m + n) + k^2 \int_{x,y} T(m x, n y) \frac{x^{k-1} y^{k-1} (1-x)^{m+k-1} (1-y)^{n+k-1}}{\beta(k,n+k+1) \beta(k,m+k+1)} dx dy $$ There is some rounding error here, as we should really have $T(\lceil mx \rceil, \lceil ny \rceil)$ in the integrand. Unfortunately $k$ is a random variable so one must also integrate over it. I don't think there is any point to requiring $k \leq n$, as if $k \geq n$ you will simply partition the data into a sub-lists which may be empty. So say $k$ is chosen uniformly at random from $1,\dots, c$. Then you obtain $$T[m,n] = C (m + n) + \sum_{k=1}^c \frac{k^2}{c} \int_{x,y} T(m x, n y) \frac{x^{k-1} y^{k-1} (1-x)^{m+k-1} (1-y)^{n+k-1}}{\beta(k,n+k+1) \beta(k,m+k+1)} dx dy $$ This recurrence is pretty horrible, but if you have a guess for a bound on $T[m,n]$ (I suppose $T(m,n) = O( (\log m + \log n) (m+n) )$ ) you can plug it in and verify it. 

Let a real $k\times n$ ($k\le n$) matrix ${\bf A}$ with the property that any collection of $k$ columns is full rank. Q: Is there an efficient way to deterministically find a vector ${\bf a}$ such that the augmented matrix ${\bf A}' = [{\bf A}\;{\bf a}]$ preserves the same property as ${\bf A}$: any $k$ columns are full rank. Relevant Sidenote: A matrix that has this property is the generator of an $(n,k)$ Reed-Solomon Code: adding columns that preserve its Vandermonde structure preserves the rank property. 

I have a question relevant to the number of graphs with prescribed spectral ratio. Let $A$ be the adjacency matrix of a graph on $n$ vertices. Let $\lambda_i$ be its $i$-th largest (signed) eigenvalue. Moreover, assume the following (larger than a constant) spectral ratio constraint $$\frac{\lambda_1}{\max\{\lambda_2,|\lambda_n|\}} \ge \Omega(\log(n)).$$ What is the fraction of graphs (out of the $2^{O(n^2)}$ many) that have the above spectral property? In other words, what is the fraction of graphs on $n$ vertices with increasing spectral ratio? Edit: A relevant question that might be easier: 'is there a combinatorial property on the graph that leads to an $Î©(\log(n))$ ratio?' 

Let $A$ be the $n\times n$ adjacency matrix of a (non-bipartite) graph. Assume that we are given the amplitudes of its eigenvalues, i.e., $|\lambda_1|=a_1,\ldots, |\lambda_n|=a_n$, and we would like to calculate their signs. Is there a faster way of computing the signs of these eigenvalues, other than recomputing the eigenvalues themselves? 

This approximation "Nuclear norm minimization for the planted clique and biclique problems", by B. Ames and S. Vavasis ( $URL$ ) finds a biclique for some specific type of graphs in poly-time, but has no general approximation guarantees. The authors recast the biclique problem to a rank minimization, subject to affine constraints. Then, they solve a relaxation using a nuclear norm heuristic, which can be posed as an SDP. This heuristic is a pretty exciting gadget of the compressed sensing paraphernalia. This relaxation usually admits some cute optimality conditions when the set of constraints exhibit "an appropriate type" of randomness. 

The following fact seems to be used implicitly in cs theory, particularly algorithms. Given a RAM machine $M$ running in time $O(f(n))$, another RAM machine $M'$ can simulate $M$ in time $O(f(n))$. This differs from the case for Turing machines, where $M'$ may require $O(f(n) log(f(n))$ time. I say this is often used implicitly because many papers will simply say something like "run $M$, but keep track of certain auxiliarily information as you do so". This is really simulating $M$, but for RAM machines the distinction is not so important because running times are not (asymptotically) affected. Is there a reference for this theorem? I am summarizing the situation correctly? 

So you want to evaluate the polynomial $$ p(x) = \sum_{C} x^{|C|} $$ where $C$ ranges over all nearly-minimum cuts in a graph (say, all minimal cuts of size $\alpha c$ where $c$ is the edge connectivity.) Here $\alpha$ is a small constant $>1$. You can do any precomputation you want. Two obvious algorithms present themselves. This is a polynomial of degree at most $\alpha c$, so you can do this is $O(c)$. Alternatively, for $\alpha$ sufficiently close to 1, the number of such cuts is at most $O(n^2)$, so you can do this in $O(n^2)$ work. Are there any other algorithms available? For $\alpha$ sufficiently close to 1, the set of all such cuts has a very nice structure, and these cuts can be represented in $O(n)$ space. 

Another typical case of $NPI$ problem is when there is a witness of length $\omega(\log n)$ but smaller than $n^{O(1)}$. The problem of the existence of a clique of size $\log n$ in a graph is a typical example -- in this case, the witness (the specific clique) requires $O(\log^2 n)$ bits. Assuming the Exponential Time Hypothesis, such a problem is easier than an $NP$-complete problem (which requires time $\exp(n^{O(1)})$) but harder than a polynomial time problem. 

The problem becomes easier, if we consider long deletions and substring copying instead of transpositions. Assume that we are using the standard dynamic programming algorithm for edit distance computation, and that an expensive operation of length $k$ increases the distance by $ak+b$, for some constants $a,b \ge 0$. These constants may be different for long deletions and substring copying. A long deletion is the deletion of an arbitrary substring from $x$. Supporting them is easy, if we break them down into two kinds of simple operations: deleting the first character (cost $a+b$) and extending the deletion by one character (cost $a$). In addition to the standard array $A$, where $A[i,j]$ is the edit distance between prefixes $x[1 \dots i]$ and $y[1 \dots j]$, we use another array $A_{d}$ to store the edit distance, when the last operation used was a long deletion. With this array, we only have to look at $A[i-1,j]$, $A[i-1,j-1]$, $A[i,j-1]$ and $A_{d}[i-1,j]$ when computing $A[i,j]$ and $A_{d}[i,j]$, allowing us to do it in $O(1)$ time. Substring copying means the insertion of an arbitrary substring of $x$ into the edited string. As with long deletions, we break the operation down into two simple operations: inserting the first character and extending the insertion by one character. We also use array $A_{s}$ to store the edit distance between prefixes, provided that the last operation used was substring copying. Doing this efficiently is more complicated than with long deletions, and I am not sure whether we can get to amortized $O(1)$ time per cell. We build a suffix tree for $x$, which takes $O(|x|)$ time, assuming a constant-size alphabet. We store a pointer to the current suffix tree node in $A_{s}[i,j-1]$, allowing us to check in constant time, whether we can extend the insertion by character $y[j]$. If that is true, we can compute $A[i,j]$ and $A_{s}[i,j]$ in constant time. Otherwise $zy[j]$, where $z$ is the inserted substring that was used to compute $A_{s}[i,j-1]$, is not a substring of $x$. We use the suffix tree to find the longest suffix $z'$ of $z$, for which $z'y[j]$ is a substring of $x$, in $O(|z|-|z'|)$ time. To compute $A_{s}[i,j]$, we now need to look at cells $A[i, j-|z'|-1]$ to $A[i,j-1]$. Finding suffix $z'$ requires just amortized $O(1)$ time per cell, but computing $A_{s}[i,j]$ with a brute-force approach takes $O(|z'|)$ time. There is probably some way to do this more efficiently, but I cannot find it right now. In the worst case, the algorithm takes $O(\min(|x| \cdot |y|^{2}, |x|^{2} \cdot |y|))$ time, but a better analysis should be possible. The resulting edit distance with long deletions and substring copying is not symmetric, but that should not be a problem. After all, it is usually easier to reach the empty string from a nonempty one than the other way around. 

Consider a graph with $n$ vertices and maximum degree $\Delta$. I would like to find if the graph has any $s$ cliques, where $s \leq \Delta$ and both of them are small compared to $n$. I only need to find a single such clique (or certify that none exist) There is a straightforward way to do this: for each vertex $v$, test all $s$-subsets of the neighbors of $v$. The work is thus $\approx n \binom{\Delta}{s-1}$. Are there any more efficient algorithms than this? Even achieving an exponential speed-up would be good? 

I am PhD student also interested in studying theoretical computer science, not really interested in other areas of CS. The route I took was to enter a PhD program in Applied Mathematics. (Pure mathematics may work as well, but this may require more mathematics coursework than you want). This gives a lot more flexibility in courses. In fact I am taking more TCS courses than I would be allowed to as a pure CS student. My dissertation will be in TCS and I will work with a CS advisor. I found that this was basically the best way out of the dilemma you are facing. 

Suppose I have a code $C$ over $GF(2)$. I would like to count exactly the number of codewords of $C$ of weight $k$. Here $k$ should be thought of as small compared to the dimensions of the code. What is the best algorithm for this? Even exponential algorithms could be useful. 

The set of transcendentals is not open in $\mathbf R$ (in particular, it is dense and codense in $\mathbf R$. Hence it is undecidable. 

There seem to be many very interesting problems in graph theory that can be written in the form of maximizing/minimizing a quadratic form on either the Adjacency ${\bf A}$ or the Laplacian matrix ${\bf L}$ of a graph. Examples are: 1) the max-cut problem is equivalent to finding a sign vector that maximizes a quadratic form $$\max_{x_i\in\{\pm1\}} \sum_{i,j}A_{i,j}(1- x_ix_j),$$ 2) The densest $k$-subgraph is equivalent to $$\max_{x_i\in\{0,1/\sqrt{k}\},\;{\bf 1}^T{\bf x}=1} {\bf x}^T{\bf A}{\bf x}$$ 3) The Grothendieck constant involves maximizing the adjacency's quadratic form over sign vectors $$\max_{x_i\in\{\pm1\}} {\bf x}^T{\bf A}{\bf x},$$ 4) The spectral profile of a graph $$\Lambda(\delta) = \min_{d(\text{supp}({\bf x}))\le \delta} \frac{{\bf x}^T{\bf L}{\bf x}}{\sum_i \text{deg}(\text{vertex}_i) x_i^2},$$ where $d(\text{supp}(x))$ is the fraction of edges incident on vertices within the support of vector ${\bf x}$ ( $URL$ ) 4+$\epsilon$) Anything involving the computation of an adjacency/laplacian eigenvalue. ${\bf Q}$: Which other problems in graph theory can be expressed as maximizing/minimizing a quadratic form? 

Let a graph on $|V|$ vertices and $|E|$ edges. We randomly sample $s= c \cdot \frac{|V|}{{d_{\text{av}}}}$ vertices, without replacement, where $d_{\text{av}}$ is the average degree of $G$ and $c$ is some big constant. Let $G'$ be the subgraph induced by these $s$ vertices. What is the size of the largest connected component of $G'$? I'm tempted to say $O(\log(s))$, under a mild assumption like |E| = o(|V|^2). Some intuition comes from the fact that the probability of 2 vertices in $G'$ being friends is $\sim \frac{E}{V^2} = \frac{{d_{\text{av}}}}{|V|}=1/s$; if $G'$ was an ErdÅsâRÃ©nyi then, it would be below the giant component threshold. Obviously, the edges in $G'$ are not independent, so the result does not immediately hold. Examples where the O(log(s)) claim is true: star, cycle graph, union of cliques of size o(n). Any ideas? 

Do you mean construction or inversion of BWT? For construction, the best algorithm is probably the one by Okanohara and Sadakane. It takes $O(n)$ time and usually requires $2n$ to $2.5n$ bytes of memory for an input of length $n$. There is an implementation available at Google code. I am not that familiar with BWT inversion algorithms. The papers of KÃ¤rkkÃ¤inen and Puglisi at ESA 2010 and CCP 2011 might provide a good starting point. 

This sounds similar to superbubbles in bioinformatics. We have a directed graph $G = (V, E)$. A superbubble is an induced subgraph defined by vertices $s, t \in V$ (with $s \ne t$). We have the following requirements: 

The first rule of concurrent data structures is: You do not want concurrency. In the ideal case, distributed/parallel/concurrent computing means that you have a number of completely independent sequential processes. Each process has its own data and resources, and the process is not even aware of any other processes. In the worst case, you have a shared memory system with multiple threads querying and updating the same data structures concurrently. Something has probably gone horribly wrong, if you are seriously considering this. Of course, when we are talking about concurrent data structures, some degree of concurrency is unavoidable. We still want to minimize it. The longer a process can work sequentially without touching mutexes, doing atomic operations, or passing messages, the more likely everything works correctly and the performance is acceptable. Static data structures with batch updates require less synchronization than dynamic data structures. You should try to make your concurrent data structures static, or at least as close to static as possible. If your algorithm requires interleaving queries with updates, try changing the algorithm before resorting to shared dynamic structures. The same design principle also applies to updating static data structures. The more independent you can make the processes updating the structure, the better everything works.