I'll try to explain my misunderstandings by the following example. I didn't understand fundamentals of the . Consider the query the plan of which is this: 

PostgreSQL 9.4 I have a table called which has a column integer. After runnig against it I got the following statistic for the column: 

The tables are relatively small (~100 rows each). Now, I need to select all pairs for users who have s. That's a classic usage of . But the table has the following statistic: 

So where did I go wrong with getting the standby server to be up? UPDATE As suggested by one of the answers, I have tried adding a into and then starting the postgres as user . Same result. 

The script successfully sends the WAL file every 2 mins to using the and leaves them inside Now I then use to backup the master database at then I stopped the postgres to stop the continuous archiving every 2 mins in I run 

This fails with the error: ERROR: syntax error at or near "RETURNING" What I am trying to achieve is to be able to run an update statement on table t with a returning clause. I know that I can achieve this if the statement run in the rule was an INSERT or an UPDATE (by adding a RETURNING clause), but I do not know how to achieve this when it is a SELECT statement. If I remove the RETURNING part of the query above, then I get the following message: cannot perform UPDATE RETURNING on relation "t" query: UPDATE t SET c = true WHERE id IN (1000460) RETURNING id hint: You need an unconditional ON UPDATE DO INSTEAD rule with a RETURNING clause. It was this error message that lead me to attempt the syntax in the beginning of this post. 

QUESTION: Why is there so different expected and actual rows? I ran right before the execution but it didn't help. How can I deal with such a distinction? 

Note that the cost is decreased more than 4 times ( now). My question is how exactly the cost of depends on the correlation? How is it computed? It would be good if you pointed out to some references to the postgresql documentation. 

My understanding of this node: As explained there, the reads table blocks in sequential order, so it doesn't produce random-table-access overhead which happens as doing just . After has been done, PostgreSQL doesn't know how to fetch the rows optimally, to avoid unneccessary (or if there is a hot cache). So to figure it out it generates the structure () called which in my case is being generated by generating two bitmaps of the indexes and performing . Since the bitmap has been generated it now can read the table optimally in a sequential order, avoiding unnecessary . That's the place where a lot of questions come. QUESTION: We have just a bitmap. How does PostgreSQL knows by just a bitmap anything about rows' physical order? Or generates the bitmap so that any element of it can be mapped to the pointer to a page easily? If so, that explains everything, but it's just my guessing. So, can we say simply that the is like a sequential scan but only of the appropriate part of the table? 

I am considering a model where I use PostgreSQL COPY to copy data from a file into a table. I was wondering what kind of performance to expect on high-end hardware. An interval in MB/s would be nice so I can start estimating. If you just know the answer on other databases, I would be interested to compare. 

Now I would like to utilize the test view or any other view in a GUI which simply presents an editable table. This is possible with the recently introduced automatically updatable views. I want the table to contain drop-downs containing all possible values every time there is a column with constrains on it, like the column with a foreign key in this case. How can this be achieved? I know that I could utilize enums and I do that today, but on Amazon RDS they are unreasonably hard to modify. I had an idea of (ab)using the "EXPLAIN SELECT" query for each column and check which table and column is being queried. 

My system is able to do the daily processing of QuotationLineItem and PurchaseOrderLineItem fairly okay. The issue is with reporting which leads me to do many joins which is crazy. I have looked at the Kimball book 3rd Ed of Data Warehouse Toolkit: Dimensional Modeling. I am convinced that I need to have a separate database instance which is meant for OLAP situations to satisfy the reporting requirements. Because of this, I need to design dimensional and fact tables. My question is, it appears that I have at least 2 fact tables. and . How do I generate a report like the above? Because from what I understand fact tables are not supposed to have foreign keys to each other else I will get back a snowflake schema. EDIT As requested, these are my source tables which are in a 3NF database schema. Tables include but not limited to: 

I have about 26 applications interacting with our database, some of these will store email addresses in mixed case. Since I made the mistake of using varchar as the column type and not a case insensitive one, I would like to make some kind of trigger or rule, which will convert all attempts to save email addresses as mixed case, into lower case. I could achieve the same using the following query: 

If I share a Google Spreadsheet with someone, I will share a link like this: $URL$ This means that I am free to edit the name of the file (for example to "Book - absolute final version really7"), the Google Drive folder it is located in etc. etc. while the link will still work. Would it be possible to simulate something similar in a database? The problem I am trying to solve is the intense pain and risk I have to go through every time I need to rename a column, table, view or stored procedure in the database, or if I want to move it to a different schema. Because of this pain, I currently have all sorts of naming of all these things so you will see a primary key lookup looking like this in my database: 

return . So, as long as the index scan 2 random page reading (The first one is to read an index, and the second one to read the actual table), it's not clear what the means in the plan? Could you explain how it's computed? 

Since the optimizer doesn't have statistic for the value , it made a wild guess for of the table size. So, the estimated row count should have been . But the optimizer returned the . Why? Maybe I didn't understand the estimating process of row counting of unknown values? Couldn't you explain it a bit? 

I think it is incredibly tedious to administer user rights in Postgres. If I for example want to give editing access to an updatable view, then I need to ensure that the user has the right accesses to, the database, the schema, the underlying table, the sequences used (for inserts), all the columns. So often something goes wrong, also due to my lack of understanding. However, I don't really see any need to have a deep understanding of this. I can imagine a tool where I could just grant access to the view and the tool would somehow cascade (is that the right expression) the user rights to all needed objects. I understand that this could get difficult to do if giving access to, say, a pl/pgsql function, which touches 100s of objects under various conditions, but for basic objects like views, tables, etc. this should really save me a lot of time. Please tell me if this question should have been in the softwarerecs StackExchange, I felt that this was the right place, since it is so specific to postgres and, I believe, of general interest. 

both fact tables need to have some common dimensions and the row headers must be from the common dimension table. perform queries on the 2 fact tables separately perform an outer join on the 2 result sets. 

EDIT 2 After some clues and further revision, I realized that maybe the solution is to have two fact tables that have common dimensional tables. The 2 fact tables are: 

This creates a inside I then use rsync to send the base file across to . I stopped the postgres in willie as root using . Then I run the following as user: 

We are creating SAAS where we will at most have 50.000 customers. We are considering creating a user in the Postgres database for each customer. We will map each user that logs into our service to a user in the database in order to be very sure that they only have access to their own data. We also want to implement an audit trail directly in the database by this solutions, which utilizes triggers. If each customer has its own database user, then it would be very easy to see who did what, even if two customers would share the same data. Will we be running into some unexpected problems because we have 50.000 users in our database? Performance-wise or administration-wise. Maybe connection pooling would be more difficult, but I do not really know whether we would need it.