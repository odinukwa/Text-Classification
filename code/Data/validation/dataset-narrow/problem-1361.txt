If you are sure that J will always be a diagonal matrix then you can avoid calculating a matrix inverse, store the three real numbers making up J in a vector instead, like you did, and use a weird component-wise "vector division" which I guess is what you originally wanted to do with , and it will work. But it will harm the realism of the physics of many solids that aren't shaped in a an axes-friendly way. Next: 

and H(a)/d(a) can naturally be set to 2/3.sqrt(a) so that we get the following graph, with the red curve being H(a)/d(a): 

To use Newton's second law and compute the angular velocity of our solid, we must either bring our problem back to the solid's own frame (base version of the solid, centered at the origin, using the usual axes of space) and use the one J we know, or we can re-compute J every time the solid rotates. Bringing the problem back within the solid's frame is much easier as it requires only linear transformations (understand rotations) of the variables of the problem (ω, T, etc) whereas computing J again would mean integral calculus over the rotated solid, etc, it would be a mess. The linear transformations in question are simple: since what separates our inertial frame (relative to the inertial frame, the solid has moved) from the solid's own frame (in the solid's own frame, the center of mass is at the origin, etc) is a rotation (+ a translation but it does not interest us as we are looking at torque and rotation stuff), all you have to do is undo this rotation (apply the inverse of the orientation matrix) to the torque T and to ω(t), do your computations in the solid's frame where J is the constant matrix you know and then rotate everything back: J.inverse(R).(dω)/(dt) = inverse(R).T therefore dω = dt.R.inverse(J).inverse(R).T i.e. ω(t+dt) = ω(t) + dt.R.inverse(J).inverse(R).T but R is an orthogonal matrix because it is a rotation matrix so we have transpose(R) = inverse(R), and a matrix transpose is much easier to compute than an inverse, so finally, ω(t+dt) = ω(t) + dt.R.inverse(J).transpose(R).T Notice that the formula for the inverse of the inertia matrix in the inertial frame has appeared: we do have R.inverse(J).transpose(R) = inverse( R.J.transpose(R) ) Finally, here's the (unbelievably simple) code: 

Angular velocity is, conceptually, the first derivative of some kind of "orientation", that is to say, it is the amount by which this orientation varies with respect to time. The angular velocity's components are indeed in radians per second. Ideally, you'd like to be able to multiply the angular velocity of a solid with a timestep dt and get a small variation of some kind of "orientation vector" which you could then add to the current orientation vector, pretty much like you did with velocity and position. But it isn't that easy. What we call an "orientation" is in fact a rotation operation. Asking "How is my solid orientated?" in fact amounts to asking "What rotation should I apply to my solid so that its orientation is the one I want?". There are three ways to encode rotations: 

1) 2) 3) 4) End result: You can also create the edges and faces as you run the loop that makes the circle. Same complexity, same thing. Make one vertex on the circle, store it into your array of vertices, add the corresponding edge (pair of indices) to the array of pairs of indices if you feel like it, and finally add the corresponding face to your array of triplets of indices. Move on to the next vertex. The cylinder and the tube: not doing the same work twice, and quads Again, for the tube it starts with a vertex and a circle which will be the center of either the top or the bottom disc of the cylinder: 

But how do we even use that matrix to get our solid in position anyway? Well, let's imagine that the solid is at the center of the 3D space, and in its original orientation. If G is its center of mass, then G is exactly at the origin of 3D space. The solid has not ever been rotated. Let's call the solid, when it is in this position, the "base version" of the solid. 

X, Y and their Minkowski sum, X+Y Supposing (-Y) is the set of all -y for y &in; Y, then given the previous paragraph, X and Y intersect if and only if X + (-Y) contains 0, that is, the origin. Side remark: why do I write X + (-Y) instead of X - Y ? Well, because in mathematics, there is an operation called the Minkowski difference of A and B which is sometimes written X - Y yet has nothing to do with the set of all x - y for x &in; X and y &in; Y (the real Minkowski difference is a little more complex). So we'd like to compute the Minkowski sum of X and -Y and to find whether it contains the origin. The origin is not special compared to any other point, so that to find whether the origin is within a certain domain, we use an algorithm that could tell us whether any given point belongs to that domain. The Minkowski sum of X and Y has a cool property, which is that if X and Y are convex, then X+Y is too. And finding whether a point belongs to a convex set is much easier than if that set were not (known to be) convex. We can't possibly compute all of the x - y for x &in; X and y &in; Y because there are an infinity of such points x and y, so hopefully, since X, Y and X + Y are convex, we can just use the "outermost" points defining the shapes X and Y, which are their vertices, and we'll get the outermost points of X + Y, and also some more. These additional points are "surrounded" by the outermost ones of X + Y so that they do not contribute to defining the newly obtained convex shape. We say that they don't define the "convex hull" of the set of points. So what we do is that we get rid of them in preparation for the final algorithm that tells us whether the origin is within the convex hull.