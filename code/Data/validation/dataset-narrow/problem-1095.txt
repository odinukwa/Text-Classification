so to sketch this out, one route to your problem is to create the TM that one wants to compute, and then convert it to a RD model as described in the paper. the RD model is then in a special class of differential equations. 

there is probably a graph theory way/formulation to solve this, but this problem sounds more like a permutation problem to me where some of all permutations are rejected and others are valid. the permutations are employees and the positions are "positions" in the company. a permutation is rejected if it doesnt fit the requirements of "person [x] wants position [y]". the distinction of units/depts/org boundaries is apparently somewhat superfluous to the solution in this case. this type of permutation problem with constraints can be readily converted into a instance of SAT (satisfiability) problem. the boolean variable assignments represent employees, and the constraint clauses represent the "person [x] wants position [y]" constraints. there are nearby classic examples of this, one usually called the "dinner table" problem where you have seating positions and guests and not all guests want to sit next to each other (or very similarly some guests want to sit next to other guests). and of course there are sophisticated SAT solvers for fairly large instances involving roughly up to hundreds of variables and clauses, on PC, and if the problem is not "hard", in the thousands. see eg [1] for a professional reference and [2] for a class exercise. there is also some structural similarity to what are known as "pigeonhole problems" which are well studied in SAT circles where pigeons are assigned to pigeonholes and you have more or less holes than pigeons. in that case however the pigeons are generally seen as interchangeable. in other words the dinner table problem is like the pigeonhole problem with stronger constraints and the guests/pigeons have required preferences. note of course/keep in mind that for these types of problems, depending on the constraints the answer may be "no such constrained solution exists". [1] the dinner table algorithm, by crato [2] CS402 princeton HW SAT [3] Satisfiability problem, wikipedia 

maybe all the major/preferred algorithms of interest to this audience have been mentioned at this point. however, a few more deserve mention for completeness. & some analysis of what is considered a significant algorithm is relevant here. in CS & IT fields there seems to be a phenomenon noticed long ago in AI called "moving the goalposts". this is a psychological phenomenon where the field advances relatively quickly but people quickly mentally adjust to "the new normal" and take real or even breakthrough advances as mundane or unremarkable in retrospect, after accomplished, ie downplayed or minimized. this is highly captured in this question in the way that algorithms move from R&D into "deployment". quoting the author of the question in later comments: 

along these lines a property relating PCP and NP can be found in the following citation p63 from P vs NP by Mayordomo (2004): 

the title of the question refers to "trivial implications" but the contents do not exactly specify that criteria, so this is a bit of a mixed message. one semifamous item/ example that comes close to the general theme is the proof of the (then ~4 decade old) Strong Perfect Graph Conjecture in 2002 by Maria Chudnovsky, Neil Robertson, Paul Seymour, and Robin Thomas. the problem of algorithmic complexity of recognition of perfect graphs turned out to be closely tied/ tightly to the proof mechanics of the strong perfect graph conjecture, although this was not exactly well understood or known prior to the proof of the conjecture. in other words there was an informal open conjecture that "perfect graph recognition is in P" (or "low complexity" etc) relatively quickly resolved by building on the analysis/ properties/ mechanics of the strong perfect graph theorem. A polynomial algorithm for recognizing perfect graphs Gérard Cornuéjols, Xinming Liu, Kristina Vušković 2003 

specifically considering one part of the question (eg for $k_1$=1,$k_2$=2), Lokam studied "2-slice" functions in this paper & proves that strong lower bounds on them can be generalized, therefore this is a very hard open problem related to basic complexity class separation & any such construction/explicit function would be a breakthrough; from the abstract: 

of course QM computing is highly similar to probabilistic computing (a nice ref that emphasizes this is One Complexity Theorist's View of Quantum Computing by Fortnow) & there is some hint these approaches might be extended there, eg in the work in parallel QM simulation. 

GCT is a research program for proving complexity theory bounds and in a way defies a wikipedia-style abstract/summary due to its heavy abstraction, but for the TCS crowd good surveys are available.[2][3][4] (and surely Wikipedia is the best place for wikipedia entries). it was formulated in the early 2000s by Mulmuley and is both relatively new in complexity theory and very advanced, using and applying advanced mathematics (algebraic geometry) that did not originate in TCS/complexity theory. the approach is considered promising by some but possibly too complex by other authorities ie it is not proven and therefore controversial whether it could overcome standard known "barriers". (in this sense it does exhibit some signs of a so-called Kuhnian "paradigm shift".) even Mulmuley proposes that it realistically might not succeed (in proving major complexity class separations) after decades of further development. here is a skeptical opinion by Fortnow, a leading authority in the field of complexity theory:[1] 

this is a 32p survey that just appeared on the subject focusing on the circuit lower bounds angle (there is strong overlap in the content with other answers here). 

a very challenging/advanced/provocative question; following, a brief/sketchy/tentative answer [maybe/hopefully better than none] considering geometry in QM computing in general & a few refs/leads. geometry is used in a variety of ways in QM in general, and it appears to be somewhat of an open question and challenging work-in-progress how to determine a coherent/natural "geometric picture" for QM, and there apparently are multiple ways to do it, and currently no generally-agreed, unified or standard approach. also, some directions can be highly abstract reflecting the direction of mathematical research developed largely independently of physics. the 2-qubit state has been more extensively studied and there is more chance of creating a picture there 1st and maybe using it as a somewhat "toy" area that can be expanded later. (note that adiabatic QM computing is still based on qubits.) also there is a relatively new study of "quantum dischord" which is seen as promising by some (but also controversial) & might be part of the answer as in the following ref. 

the Perimeter institute in canada seems apparently not affiliated with a particular university but has a strong research program incl quantum foundations, quantum information processing etcetera. (questioner did mention "institute" & seems like artificial restriction to strictly limit to universities.) 

expert researchers in this area basically assert that phase transitions are a universal feature of NP complete problems although this has yet to be formulated/ proven rigorously and it is not yet widely regarded/ disseminated in the larger field (it emanates more from an empirical-oriented branch of study). its nearly an open conjecture. there is strong evidence. there are no plausible candidates for non phase-transition NP complete problems. here are two refs that support this pov: 

from Q&A with you in the comments you seem to be interested in studying something defined as the stack distance in this set of slides, On the mathematical modelling of caches 

you dont mention your data type, which certainly helps to narrow the literature, but it appears that much or even most literature related to this is based on finding shortest paths in highway networks. this is a new phd thesis that looks promising & focuses on sparse graphs 

there are many references on "global optimization of neural networks". the techniques are similar to simulated annealing [see other answer]. the basic idea is to restart the network gradient descent starting at many different weight starting points, sampled randomly or systematically. each result of the gradient descent is then like a "sample". the more samples are taken, the higher probability that one of the samples is the global optimum, especially if the target function is "well behaved" in the sense of continuous, differentiable, etcetera. online refs [1] Global Optimization of Neural Network Weights by Hamm et al [2] A global optimization approach to neural network training Voglis/Lagaris [3] Calibrating Artificial Neural Networks by Global Optimization Pinter [4] Global Optimization of Neural Networks using a Deterministic Hybrid Approach Beliakov [5] Global Optimization for Neural Network Training Shang/Wah 

part of the challenge in this area is there seems not to be a strict formal/mathematical definition of the term "fractal". originally as coined by Mandelbrot in 1975 it had an informal geometric interpretation but is now seen as more general, eg applying to misc important mathematical objects created/discovered before unifying principles/properties of fractals were recognized, such as Cantor dust or the Sierpinsky triangle or even the Weierstrauss function. of course as in these examples an algorithm to draw fractals has fractal complexity properties. however there seems to be a much deeper connection between fractals and algorithms (maybe fundamental?) as uncovered in the links between fractal computations and undecidability (maybe two faces of the same phenomena?). one alternative is to consider the closely related iterated function systems. eg try 

there are many old SAT search, variable ordering, backtracking etc heuristics that can be regarded as verging on learning algorithms and many are applicable/generalizable to CSPs, and exact boundaries here may be blurring. possibly these have generally traditionally been two different fields, machine learning and constraint satisfaction, but with increasing intersection in more recent times. here are a few research leads. this appears to be an emerging area with some crosspollination with big data/datamining eg: 

unfortunately as pointed out in many comments the question is still a little too vague to be very specific. graphs are used in very many contexts in image analysis. however as mentioned in other answer(s), indeed one of the main uses is image segmentation, basically the representation of the local spatial areas of an image as a graph, and there are several surveys in this area. image segmentation seems to be sufficient for your application but it would be better if you were more specific about your application. here are several surveys on graph-based image segmentation. 

Aaronson recently wrote a blog refuting the idea that there could be some "glitch" in the formulation of the P vs NP conjecture[1] which reminds me of this following question. the Blum speedup theorem has been characterized by Goldreich [3] (p149) 

another book by stasys jukna[1] who was cited in the other answer has some key "applications". as always applications can be subjective, but in TCS a key application many would agree on is complexity class separations. there are about 30 references to ramsey graphs in this book as counted by a pdf search. jukna makes the case that ramsey theory is about large structures guaranteed to contain some kind of "feature" and these can be shown to exist by shannon-style or shannon-reminiscent counting arguments, but building explicit constructions is very difficult and rarely accomplished in the literature. this is apparently highly analogous, possibly even directly linked, to the inability to explicitly construct complex circuits even though they are known to exist. construction of such circuits is key in complexity class separations. this is further emphasized by constructions in juknas book that directly tie graph complexity to complexity class separations (although so far not directly through ramsey graphs). therefore this (same?/crosscutting?) phenomenon seems to be something like the "dark matter" of computer science. it is known to be there, it can be indirectly measured, but cannot be directly exhibited so to speak, and it is yet mysterious. jukna implies ramsey type thms or constructions could possibly be a bridge-type thm for complexity class separations (see sec 1.7). see also sec 1.5, "where are all the complex functions"? however it is not so straightfwd, in constrast see also sec 11.7 where he uses ramsey graphs to show how "combinatorially complicated" graphs eg ramsey graphs are not nec "computationally complicated". [1] Boolean function complexity, Advances and Frontiers by Stasys Jukna, 2011 

these are some probably very hard but possibly significant and deep questions related to an unusual but intriguing possible "recursive" construction/formulation in SAT, with some important "structure" or "dynamics" not previously considered. do not expect definitive answers in the short term but am sharing in a preliminary/sketchy form & maybe somebody in cyberspace/TCS will find it interesting also. consider a resolution based proof for a set of SAT clauses. that resolution proof is in the form of a 2-indegree DAG (or in special cases, a tree) where the leaf nodes are clauses, and every node has 2 "ancestors". (this is potentially very interesting though because circuits are usually/often represented as DAGs and can be and are also converted to 2-indegree.) 

so their formulation (in the introduction, eg eqs.1,2) can be seen/regarded as a seminal new "bridge theorem" between two previously mainly different fields of dynamical systems theory and the theory of NP completeness. this has happened many times in the past with NP completeness, and an early notice of this came in 1988 by Papadimitriou, in NP Completeness, a retrospective, which can be seen as a kind of TCS sequel/analog to the famous essay Unreasonable effectiveness of mathematics in the physical sciences, Wigner. they cite the following as nearest references to their work: 

much of the bkd on this is in this wikipedia page church-turing thesis. am not sure of the exact specifics but the wikipedia article seems to indicate it was Rosser(1939) who 1st proved this equivalence between lambda calculus and turing machines. maybe/presumably his paper has a stack-like mechanism for converting the (possibly recursive) lambda calls to the tm construction? Rosser, J. B. (1939). "An Informal Exposition of Proofs of Godel's Theorem and Church's Theorem". The Journal of Symbolic Logic (The Journal of Symbolic Logic, Vol. 4, No. 2) 4 (2): 53–60. doi:10.2307/2269059. JSTOR 2269059. note of course for anyone interested in the principles the modern Lisp language and the variant Scheme intentionally have a strong resemblance to the lambda calculus. studying the interpreter code for expression evaluation leads to ideas that were originally contained in papers for lambda calculus' turing completeness.