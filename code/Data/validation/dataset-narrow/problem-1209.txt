@Chandra, @Emil, and myself solved the question in the comments. The solution is $$f(n) = 2^{\Theta(n \log \log n / \log n)} \ .$$ To see the lower bound, apply the recurrence definition $\log n$ times, to get $$f(n) = 2f(n - \log n) + f(n - \log n - 1) + \ldots + f(n - 2 \log n) \ge \log n \cdot f(n - \log n) \ .$$ Use this inequality $n / \log n$ times, and we get that the solution is $2^{\Omega(n \log \log n / \log n)}$. To get the upper bound, use the recurrence $\log n$ times and get that $$f(n) \le (\log n + 1) \cdot f(n - \log n) \ . $$ Use this inequality $n / \log n$ times, and we get that the solution is $2^{O(n \log \log n / \log n)}$. 

Proof: Let $G$ be a graph which is not strongly connected. We will prove that $\delta^+(G)+\delta^-(G) < n$. Write the decomposition of $G$ into strongly connected components. Let $S$ be a strongly connected component which is a sink (i.e. no edges go outside of $S$) and $T$ be a source (i.e. no edges go into $T$). Since no edges go $S$ to outside of $S$, then $\delta^+(G) \le \delta^+(S) \le |S|-1$. Similarly we get $\delta^-(G) \le |T|-1$, and taking these two things together we get $$\delta^+(G)+\delta^-(G) \le |S|+|T|-2 \le n-2 \ .$$ QED. 

Answer to question 1: $\left\lceil \log_2 \binom{M-1}{r-1} \right\rceil$ bits suffice to encode the variables. Proof: Count how many ways there are to choose $y_1,\ldots,y_r$ such that $y_i \ge 0$ and $\sum y_i = M-r$. There are exactly $\binom{M-1}{r-1}$ such ways (see e.g. here). Now, if there are only $k$ possible values for a variable, then $\lceil \log k \rceil$ bits suffice to encode that variable. Therefore, $opt=\left\lceil \log_2 \binom{M-1}{r-1} \right\rceil$ bits suffice to encode our input. Answer to question 2: This is a bit more tricky. The best approach is to check the literature on succinct rank-select or other succinct data structures: I suspect you can match the results for succinct rank-select, so to get something like $opt+o(M)$ space and $O(\log M)$ running times for all operations. If you're interested, tell me in the comments and I'll try to look it up and tell you my best guess on what's possible. You might also want to check out Dodis-Patrascu-Thorup for some ideas. 

The problem is NP-complete. I sketch a reduction from 3-SAT. Consider Boolean variables $x_1,\ldots,x_n$ and clauses $c_1,\ldots,c_m$ over these variables, so that each clause consists of three literals. The question is to decide whether there exists a truth setting of the variables so that every single clause is satisfied (and hence contains at least one true literal). We construct a matrix with $n+m+1$ rows and $2n+1$ columns. 

This problem is known to be NP-complete (since the 1970s). Dyer and Frieze established its NP-hardness even for the highly restricted special case where the graph $G$ is planar and bipartite: 

Lemke has shown that it is NP-hard to decide whether a cubic graph on $n=2k$ vertices has a spanning tree in which $k+1$ vertices have degree $1$ and $k-1$ vertices have degree $3$. 

Yes, Max-Cut is still NP-complete in unweighted graphs. This is explained in pretty much every survey article on tthe Max-Cut problem, and in many texbooks (as for instance "Computational Complexity" by C.H. Papadimitriou). The first proof goes back to the year 1976: 

Since the complement of a vertex cover is an independent set, the problem of finding a minimal vertex cover of maximum cardinality is equivalent to the problem of finding a maximal indpendent set of minimum cardinality. 

Consider a positive point for the constructed set of polynomials. Then $x_i^2-0.9^2>0$ and $1.1^2-x_i^2>0$ enforce that either $-1.1<x_i<-0.9$ holds (in which case we set $u_i$ false) or $0.9<x_i<1.1$ holds (in which case we set $u_i$ true). Suppose for the sake of contradiction that under the constructed truth setting, some clause $c_j$ would contain three false literals. Then each of the three corresponding real variables is $\le-0.9$. Then the polynomial corresponding to that clause has value at most $-0.9-0.9-0.9+1.1<0$; contradiction. 

Consider all Boolean formulas $\Phi$ in 3-CNF. The objective value of an $n$-variable formula $\Phi$ is $1/n$ if $\Phi$ is satisfiable, and is $1$ if $\Phi$ is not satisfiable. The corresponding minimization problem has an asymptotic PTAS (it is easy to come within an additive $+1$), but is not in APX (as this would allow you to solve 3-SAT). Consider all undirected graphs. The objective value of a graph $G$ is its edge-chromatic number $\chi'(G)$. Holyer has proved that deciding $\chi'(G)=3$ is NP-hard. The corresponding minimization problem is APX-hard (there is no approximation algorithm with worst case ratio better than $4/3$), but the problem has an asymptotic PTAS (Vizing's theorem tells us that $\chi'(G)$ is between the max-degree and the max-degree plus 1). 

I agree with @usul. I've also never seen the term empirical mutual information mentioned, but I've seen the term empirical entropy quite a lot, especially in the compression community. The definition of empirical information is $-\Sigma p_i \log p_i$, where $p_i$ are the empirical probabilities, i.e. the fraction of the time that each value appears in your samples. To compute empirical mutual information given samples $(x_1,y_1),\ldots,(x_n,y_n)$, I'd just compute the empirical entropy of the $x$'s separately, and of the $y$'s separately, and of the pairs together, and then I'd use the standard equation $I(X:Y)=H(X)+H(Y)-H(XY)$, where all quantities on the right hand side are replaced by their empirical analogue. I don't know if this is equivalent to the equation that @usul gave, and I don't know if this is the quantity referenced in the articles you're reading, but this seems like the natural interpretation to me. 

I suspect you won't get a closed form solution for the distribution you're looking for. Think of the seemingly easier problem where $k$ is always chosen to be exactly $2$, and where you get the set $\{1\}$ "for free". This problem is just like asking for a closed-form distribution of the cardinality of the connected component in the Erdos-Renyi graph $G(n,s)$ that contains a specific vertex $v_1$. I'm pretty sure that there's no known closed-form description of this distribution, and that in fact the lower order terms are poorly understood, especially around the critical value $s \sim n \log n$. 

Depth-2 TC0 probably can't be PAC learned in subexponential time over the uniform distribution with a random oracle access. I don't know of a reference for this, but here's my reasoning: We know that parity is only barely learnable, in the sense that the class of parity functions is learnable in itself, but once you do just about anything to it (such as adding a bit of random noise), it ceases being learnable. But depth-2 TC0 is strong enough to represent all parity functions and strong enough to represent perturbed versions of parities, so I think it's safe to guess that depth-2 TC0 cannot be PAC learned. However, parities and noisy parities can be learned in polynomial time if we're given a membership oracle. So it might be interesting to check whether depth-2 TC0 can be learned using a membership oracle. I wouldn't be totally surprised if the answer is yes. On the other hand, I doubt that $O(1)$-depth TC0 can be learned with membership queries. It might be good to start with AC0[6] (or even AC0[2]) and go from there. 

You'd have to tweak the limits (in particular may be too low), but for at least some "real" problems this is within the bounds of Knuth's algorithm M. See also 

This has been studied in the case of the specific linear equation $$x + y = w + z$$ where (allowing trivial solutions such as $x=w, y=z$) the set $S$ is a Golomb ruler / Sidon set. In this case, the answer to 1 is that $\max_{a \in S}a = \Theta(n^2)$, and the answer to 2 is that actually minimising the width of $S$ is unknown but speculated to be NP-hard; finding asymptotically optimal rulers is linear in $n$ using some geometric techniques. 

This is equivalent to the property that you can construct a Hamiltonian path by greedily taking an arbitrary edge at every vertex. Searching for greedy Hamiltonian path turned up: Greedily constructing Hamiltonian paths, Hamiltonian cycles and maximum linear forests, Tankusa and Tarsib, doi:10.1016/j.disc.2006.09.031, which points to Randomly Traceable Graphs, Chartrand and Kronk, SIAM J. Appl. Math., 16(4), 696–700, doi:10.1137/0116056 as characterising these graphs as precisely the graphs you mention in the question. 

There are $\sum_{j=1}^4 \binom{11}{j} = 561$ smaller subsets, and each $x^\phi$ contains $\sum_{j=1}^4 \binom{5}{j} = 30$ of them. If you put all $462$ $5$-element sets in a priority queue with priority corresponding to the number of subsets which haven't yet appeared, after each pop you have to check $30$ subsets to see whether they're appearing for the first time, and for each $k$-element subset that is appearing for the first time you have to update the priorities of $\binom{11}{5-k}$ sets. There's an easy upper bound on the number of updates of $25410$. As a follow-up optimisation, once every smaller subset has been seen (which happens after you've removed the first 90 elements from the priority queue), you can just iterate through the rest. If your priority queue is e.g. a binary heap, this will save you a lot of $O(\lg n)$ pops. 

I've rethought this and my initial bound was correct. In the worst case, $|S| = \Theta(m \; 2^\frac{m}{\lg m})$ Proof is in two parts. Firstly, $|S| = O(m \; 2^\frac{m}{\lg m})$. Consider the possible values of $s$ of a trajectory which ends at $v_x$. Every bit $s[j]$ for $j \ge x$ is 0, and every bit $s[j]$ for $j < x - \frac{m}{\lg m}$ is 1. Therefore there are only $2^\frac{m}{\lg m}$ values which $s$ can take. Multiply up by the number of $v_x$ and we have the upper bound. Secondly, consider 

Some work in progress: I'm trying to prove a lower bound of $4^k$. Here is a question that I'm pretty sure would give such a lower bound: find the minimum $t$ such that there exists a function $f:\{S \subseteq [n], |S|=k/2 \} \rightarrow \{0,1\}^t$ that preserves disjointness, i.e. that $S_1 \cap S_2 = \emptyset$ iff $f(S_1) \cap f(S_2) = \emptyset$. I'm pretty sure a lower bound of $t \ge 2k$ would almost immediately imply a lower bound of $2^{2k}=4k$ for our problem. $f(S)$ roughly corresponds to the set of nodes the NFA can get to after reading the first $k/2$ symbols of the input, when the set of these $k/2$ symbols is $S$. I think the solution to this question might already be known, either in the communication complexity literature (especially in papers dealing with the disjointness problem; maybe some matrix rank arguments will help), or in literature about encodings (e.g. like this). 

Here is a wrong answer: it outputs some vertices that are part of non-simple paths from $s$ to $t$ and that are not a part of any simple path from $s$ to $t$ of length $\le \ell$. The answer might still be relevant to the asker's application, so I'm leaving it here. Here is an algorithm that runs in time $O(|V|+|E|)$ (and actually is faster than this when $\ell$ is small). The algorithm runs a BFS search from $s$ that terminates at depth $\ell$. This BFS gives a set $V_s$ of all vertices reachable from $s$ with a path of length at most $\ell$, and it also computes the distances $dist(s,v)$ for each $v \in V_s$. Then I'd do the same from $t$ and get the set $V_t$ and distances from $t$. Finally, the vertices you're looking for are exactly $V_{solution}=\{ v : v \in V_s \cap V_t, dist(s,v)+dist(t,v) \le \ell \}$. The edges are exactly those edges in $E[V_{solution}]$ ($=(v,u) \in E : u,v \in V_{solution}$). The running time of this algorithm is surely $O(|V|+|E|)$ because it just does two BFSs. But the running time is actually $O(|V_s| + |V_t| + |E[V_s]|+|E[V_t]|)$ which will be much smaller than the size of the graph when the $\ell$-radius neighborhoods of $s$ and $t$ are small. Edit: there's probably a somewhat faster algorithm in practice that does a BFS from $s$ and $t$ of depth only $\ell/2$ rather than $\ell$. This discovers all the paths, and then with a bit of bookkeeping you can find all the vertices. This cuts the running time by a square root for the case of a large random-looking graph when $\ell$ is small. 

Furthermore use the old equation system $Ax=b$. There exists a 0-1 solution to the original system $Ax=b$, if and only if the new system has a solution in which at least $100(n+m)n$ variables are zero. 

A recent paper by Custic, Klinz, Woeginger "Geometric versions of the three-dimensional assignment problem under general norms", Discrete Optimization 18: 38-55 (2015) contains (and proves) the following proposition: 

Some tiny improvements of this result are also known, for instance by Liśkiewicz and Schuster (2014) to $O(1.255^n)$. 

No, you cannot infer hardness of P1. (And your question looks suspiciously close to homework.) Consider the special case where 

For every concrete Turing machine $M$, the halting problem (Problem $P_M$ without input: "Does the Turing machine $M$ halt on the empty input $\varepsilon$?") is decidable. The corresponding decision algorithm is either the algorithm that outputs "Yes" and halts, or the algorithm that outputs "No" and halts. 

For a word $w=w_1\ldots w_{\ell}$ and for two integers $i,j$ with $1\le i\le j\le \ell$ we denote by $w(i,j)$ the subword $w_iw_{i+1}\ldots w_j$ of $w$. Furthermore we let $w(0,0)$ denote the empty word. 

To my knowledge, Seidel's construction has only been published in O'Rourke's book and nowhere else. In one of his papers, Seidel even refers to O'Rourke's book for a description of his own construction. He writes on top of page 253 of his joint paper with Jim Ruppert: "In his book [9, p. 255] O'Rourke describes n-vertex three-dimensional polyhedra that require $\Omega(n^{3/2})$ guards." 

Why don't you simply read the paper, instead of posting such nonsense claims? On page 239, Papadimitriou carefully discusses these issues and defines the underlying variant of the Euclidean metric for his proof. 

The paper shows that the following problem is NP-hard: Given a graph $G=(V,E)$ and two vertices $s,t\in V$, is there a "good" partition of $V$ into two sets $V_1$ and $V_2$, so that $G[V_1]$ and $G[V_2]$ are connected and so that $s,t\in V_1$. Take such an instance, and make $w(s)=w(t)=1$ and $w(x)=0$ for $x\in V-\{s,t\}$. If there exists a good partition, you can reach $w(V_1)=2$. If there is no good parition, the best you can get is $w(V_1)=1$. (This argument shows NP-hardness, and also in-approximability within a factor of $2-\epsilon$.) 

Here is one algorithm. It runs in time at most $O(|S|*|w| + c)$, and probably runs much faster for many inputs. We create a directed graph $G$. The graph $G$ has a vertex $(s,i,j)$ for any $s \in S$ and any indices $i,j$ such that $s=w[i..j]$. Then the graph has a directed edge from $(s,i,j)$ to $(s',i',j')$ whenever $i'=j+1$. Finally, the graph has two additional vertices, a source and a sink. There is a directed edge from the source to any vertex of the form $(s,0,j)$ and from any vertex of the form $(s,0,|w|)$ to the sink. Now notice that the directed paths from the source of the sink correspond exactly to the parsings of $w$. So it's enough to enumerate all (source,sink)-paths in $G$, which can be done in time $O(|G'|+c)$. Note that the graph $G$ kind of represents all the parsings, so for many applications you wouldn't even have to enumerate them explicitly. 

It's possible to build the suffix array of $s$ in linear time from $s$ and $BWT(s)$ in a somewhat easy way. You do need to build a rank-select data structure on $s$ in order to do this. To see how to do this, look at Ferragina and Manzini's paper "The FM-Index". The LF mapping they describe also essentially computes the suffix array. 

When $p^q>3k$, then there is an easy randomized algorithm for this problem (with error probability zero and polynomial expected running time). The algorithm prints "IMPOSSIBLE" if there is $i$ such that $B|w_i$ has more than $n-|w_i|$ rows that consist entirely of zeroes. Otherwise, the algorithm prints "POSSIBLE". It then chooses uniformly random matrices $A$ until it finds one that satisfies the requirements. Below I'll prove that a random matrix $A$ satisfies these requirements with at least constant probability. If the algorithm printed "IMPOSSIBLE" then it is easy to see that indeed there is no $A$ that satisfies the requirements. Now, consider the case that the algorithm printed "POSSIBLE". A known result states that a random matrix over a field of size $|F|$ is full-rank with probability $\ge 1-2/|F|$. To see this consider the columns one by one, and compute the size of the linear subspace that they have to "dodge". Work from the last column to the first. You get that the chance that the matrix is full rank is $(1-1/|F|)\cdot(1-1/|F|^2)\cdot\ldots$. In fact, the same proof can be seen to work even if some of the entries are fixed to zero, as long as no entire row is fixed to zero. QED.