The problem persisted after VACUUM ANALYZE (I did that just in case, although the tables have not been altered since import, except for the creation of a GIST index on a geometry column in ). When I feed Query 2 as a CTE and look for the 1478 identified in the table, sure enough they ain't there - 

Setup: PostgreSQL 9.3.5 I have two tables with a common identifier () that is unique in one table () and not unique in the second table (). There are no NULL in either table [UPDATE: not true, there were NULL in ], and datatype is INTEGER in both tables. The following queries - which I assert are functionally identical - return different results: Query 1: 

MySQL queries are not case-sensitive by default. It is possible that you have created case sensitive tables when importing data. Check if you have collation, that makes it case-sensitive. Reimport your data then using . Also, if you have collation, it will make queries case sensitive. Your collations changed when re-importing data. 

is file in your directory. The meaning of tilde in unix is current user's home directory. The values from different configurations are overwritten. So will overwrite was what defined in , and will overwrite what was in previous files. 

I don't see why would't you just store the coordinates for every user. You can keep them either in the same table as users or in some kind of user details table, depending on how your current schema is designed. There is no point in having all geo to postcode locations mapped. To get the locations you can use something like google maps API. 

The logic of the query is as follows: each is known to have a portion of its area in more than one . The aim is to find all the that each touches. Eventually the proportion of area in each zone will be calculated, but I decided against trying to do that inside this query on the basis that the thing was already taking too damn long. That can happen intra-table (and hence be more faster) once this query finishes. The are known to be multi-zone because the that are a have already been identified in a query that only took 14 minutes (which still seems a long time): 

And yes, the clause could be written as , but the discrepancy in results is a BIG problem given that the query using took a third of a second and the query structure is canonical by the definition in TFD (it is a 'condition that evaluates to a boolean'). Bear in mind - this is not an efficiency issue in the sense that one formulation takes longer than another, valid formulation. This is the fact that two formulations that are set-theoretically identical, produce very different result sets. UPDATE: the Query Plans produced by EXPLAIN ANALYZE are as follows - Query 1. 

Every process under linux runs under specific user privileges. Services (like MySQL) usually need to open ports and access various system resources during startup, so they are required to be started as user. However, it is not safe to have all the processes run under as it is not required for continuous operation of services, thus it is recommended to create a special user, which will be used to run MySQL service. MySQL will only be able to access what special user can, and this is going to be limited to MySQL files on the system. This is usual practice in linux. If you, however, use you distributions built-in package manager to install MySQL, this will be done for you automatically (in most distributions at least). 

That's as expected, but frankly I can't trust that result given that Query 1 returns zero rows when it should return 1478. Three part question: 

That query looks an awful lot like the one above, but was working with a 3.45 million-row table ( is the unmatched data from ). I first tried Query 1 with no , using as per Query 2: that ran for upwards of an hour without producing results. I have ed, d, and ed both tables (nothing remotely interesting was found). And now - finally - my question: is there some 'trap for young players' that I have fallen into here? SPEC: PostgreSQL 9.3.5 64-bit; PostGIS 2.1.3 r12457 on localhost under Win7 Ultimate. Machine has 16GB RAM, quad i7-4770 @3.4GHz, and a 500GB SSD (PostgreSQL data is on an internal 2TB HDD). My machine is not under any perceptible load when the query is happening: the postgres process hovers at 5-9% of CPU and 15MB RAM, tops... I would be happy to hand it 8GB if it would just get the job done. SPEC-related question: I have a CUDA-capable card (nVidia Titan X with 12GB RAM of its own - oh hell yeah). I've read that PGStrom can 'smartly' figure out how to push calc load to the GPU. Has anybody here had a crack at that sort of thing? I can't be bothered installing PostgreSQL 9.5dev if it's not going to give a tangible boost. I ask mainly because I've CUDA-fied some Python stuff (big aerial-image analyses and some number crunching) with mind-boggling results. I would be happy to chuck the whole of postgresql and the related data at the GPU and let it whizz around there until it finishes, if that were possible. 

MySQL has "General Query Log". This logs everything that is going on MySQL server: users connecting, disconnecting, queries etc. This query log is a file on your filesystem or (from 5.1.6 versions) table Control the general query log at server startup as follows: 

I would use for this. - $URL$ - The script imports only a small part of the huge dump and restarts itself. The next session starts where the last was stopped. 

Yes this is normal. When RAM is no longer needed it is not freed at the same time. It is kept as cached in case the server would decide that it needs to access it again. This would save you extra time that you would otherwise need for data to appear in RAM. Cached memory is freed only when new applications request more RAM. 

That last one, if 'Y', is a gigantic PITA given that is not the 'usual' way that most folks get 'A not in B', because often the clause is a subset of the set of variables in the primary - e.g., 

I'm doing what I would consider a pretty straightforward query on two tables that aren't huge (~626k records in one; ~47k records in the other). Both tables have GIST indices on their spatial columns ( and in the query below): these are the columns being used for the conditional part of the query. The spatial columns are WKB MultiPolygons: they have been checked using ST_IsValid(), and are all lovely and clean. The offending query looks like this: 

While you seem to have fixed the issue, I will quickly explain why it happened in case anyone finding this will want to understand where the problem was. When setting foreign keys, the Primary Keys Columns must be of exact same type and attributes. E.g. If you have unsigned attribute on one primary key, you must have it on another. If you have INT data type on one column, then another column must also be INT (NOT TINYINT, MEDIUMINT etc.). As you have only one ID set to unsigned, I would go and set it to all IDs. As it is usually good idea to have unsigned attribute on primary keys (if you do not use negative IDs), I would have changed all IDs to have unsigned attribute, as it will improve your query performance. Also, take a look at what values you can get with various integers (when they are unsigned). WHat you set your lenght to - does not matter: