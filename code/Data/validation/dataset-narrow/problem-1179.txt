Assume you have two coins $A,B$ with biases $P_A,P_B$ respectively. We would like to make $N$ coin tosses and get the maximal number of heads possible. Unfortunately, we know $P_B$, but $P_A$ is unknown at the beginning of the process. What is the best possible strategy for selecting which coin to toss at each step? Note: I'm looking for actual optimization of the selection, not heuristic methods. You may assume a reasonable prior distribution on $P_A$ if needed. 

Not exactly what I was asking for, but I've noticed that this problem is $FPT$ with respect to $k$ (while Set Packing seems to be $W[1]-hard$ with respect to the required number of sets alone (i.e. without bound on the set sizes)). The algorithm utilizes the famous color coding scheme in the following way: -Notice that we can assume w.l.o.g that all set sizes are at most $k-1$ (otherwise we can select the single largest set). 

Maximal independent set is known to be hard in many meanings (hard to approximate, $W[1]$-hard, etc.). But if the number of edges is very small, then the problem becomes simpler. Here, I'm interested in learning an independent set from an unknown graph. Assume that we have an unknown sparse graph $G=(V,E)$ with at most $k\ll |V|$ edges ($k$ is given). We want to find a maximal independent set for the graph using as few queries as possible, where Query$(S)$, for some $S\subseteq V$ returns 1 if $S$ is an independent set and $0$ otherwise. For example, if $k=1$, we can find an independent set of size $n-1$ using $\Theta(\log n)$ queries. 

1] R. Boyer and J. Moore. A Fast Majority Vote Algorithm. Technical Report 1981-32, Institute for Computing Science, University of Texas, Austin, 1981. 2] M. Fischer and S. Salzberg. Finding a Majority Among N Votes: Solution to Problem 81-5. Journal of Algorithms, 3:376–379, 1982. 

Finding the minimum cost path under delay constraint (a.k.a. the Constrained Shortest Path problem) seems to fit here. Suppose you have a graph $G=(V,E)$, a delay function $d:V\to\mathbb {N}^+$, a cost function $c:\to\mathbb {N}^+$, a number $D\in \mathbb {N}^+$ and two vertices $s,t\in V$. The problem is finding the minimum cost $s-t$ path, such that the delay of the path no more than $D$. If the problem is unweighted ($\forall v\in V:d(v) =1$, a.k.a $hop-count$), the problem is trivial (given as homework in basic algo courses sometimes). If the problem is weighted, it becomes the Constrained Shortest Path, which is known to be NP-complete even on DAGs. 

Given a number $k'\in[m]$, is there a set $\mathcal{S'} \subseteq \mathcal{S}$, $|\mathcal{S'}|=k'$ such that all of the sets in $\mathcal{S'}$ are disjoint. Given a number $k''\in[n]$, is there a set $\mathcal{S''} \subseteq \mathcal{S}$ such that $|\cup_{s\in\mathcal{S''}}s|\geq k''$ (i.e. it covers at least $k$ elements) and the sets in $\mathcal{S''}$ are disjoint. 

Assume we have a set of integers $X_0=\{x_1\ge x_2\ge\ldots\ge x_n\}$. Let $r\in(0,1]$ be a parameter and consider the ranking process: 

Andreas Bjorklund et al. gave a $O^*(2^k)$ algorithm for finding the shortest simple cycle through $k$ vertices (i.e. in this setting $\forall e:w(e)=1$). Can this result be extended (or is there other known result) for finding a minimum weight cycle through $k$ vertices (for a weighted graph) in $FPT$ time? 

Proof: If $(u,v)\in E$ we're done. Denote $A=\{x\in V: (u,x)\in E\}, B=\{y\in V: (y,v)\in E\}$. Notice that since $(u,v)\not \in E$, $A \cup B \subseteq V\setminus \{u,v\}$, hence $|A\cup B|\leq n-2$. But then $$n-1 \leq \delta^+ + \delta^- \leq |A|+|B| = |A\cup B| + |A\cap B| \leq n-2 + |A\cap B|$$ And therefore $|A\cap B| \geq 1$, i.e. $\exists w\in V:(u,w),(w,v)\in E$ and $d(u,v)=2$. $ \blacksquare$. 

This problem is NP-hard in general. This is equivalent to finding the sparsest vector $y\in \mathbb F^n$ such that $$Ay = -Ax$$ As finding a sparsest solution vector for an underdetermined system of linear equations is NP-hard, so is your problem. 

This problem is known as Geometric Set Cover (which deals with covering with different shapes, so unit balls are a special case). I'm unaware of any relation to $k$-means which is a clustering algorithm whose clusters are not limited in radius. The problem is known to be NP-complete for $k\geq 2$. For unit balls, this problem has a PTAS ( ($1+\epsilon$)-approximation) ) for the 2D case and is known to be APX-hard for $d=3$. 

The player who is forced to select a "dead end" (i.e. a vertex with no outgoing edges) loses. In which graph families is the optimal strategy computable in polynomial time? For example, it's easy to see that if $G$ is a DAG, then we can easily compute the optimal strategy for the players. 

Not quite what you ask for, but here's a lower bound - a graph for which any coloring will result in an independent set colored by $\sqrt{n}$ colors: Take $\sqrt{n}$ copies of $K_{\sqrt{n}}$, and connect all vertices to a single vertex $s$. Obviously, every set of $\sqrt{n}$ vertices from different $K$'s is independent, and in every copy of $K_{\sqrt{n}}$ you can find at least one "new" color. This lower bound can easily be improved to $\sqrt{2n}$ or so if we connect $K_1,K_2,..$ to a single vertex, but it remains only $\Omega(\sqrt{n})$ colors. 

Editing my answer to give a stronger result: The problem is NP-hard for k=2 and called the Minimum Bisection Problem. 

Let $U=\{1,\ldots, u\}$ be a universe of elements for some $u\in \mathbb N$. Given some $n\in\mathbb N$, we are interested in computing some function $f:U^{\le n}\to\mathbb R$ over range queries. In the Range Query problem we get an integer array $A[1,\ldots,n]\in U^n$ and wish to compute queries that takes as input parameters $1\le i\le j\le n$ and return $f(A[i,\ldots,j])$. Consider the function $f$ that returns the number of distinct elements in the queried range. The goal is to preprocess the input array $A$ and create a small summary that allows efficient computation of these distinct queries. 

If $N$ is unknown (i.e. the process may end arbitrarily after an adversarialy chosen number of tosses), the best strategy would be to occasionally explore coin $A$ to get a better $\widehat {P_A}$ estimation, and in the rest of the time toss coin $A$ iff $\widehat {P_A} > P_B$. In the scenario $N$ is known, it's not hard to argue that the optimal strategy makes a series of tosses of coin $A$ and then either keeps on tossing $A$ for the remaining tosses or starts tossing only coin $B$, but how can we optimize the cutoff (i.e. the switch to tossing coin $B$) to maximize the expectancy of head results? 

(i.e. its action on $\Sigma^*\setminus I$ could be arbitrary). If such automaton exists, we say that $L$ is regular with respect to $I$. 

By adding a small ($O(k)$) sized graph to each vertex you could reduce $VC$ of size $m$ in a 3-regular graph $G$ to $VC$ of size $m+2k|V|$ in a $k-regular$ graph. 

For $k=1$ we can also find the edge using $\Theta(\log n)$ queries. The run time of the algorithm here can be arbitrary, all I care about is the query complexity. 

It's not hard to show that if $f$ is strongly convex then this approach converges and you can give an upper bound on the number of iterations needed. If $f$ is strictly convex, but not strongly convex, this might converge very slow or even not converge at all. If $f$ is not convex at all, you are likely to end in a local optima rather than the global one. You might also like to take a look at the Expectation–maximization algorithm which is quite similar, but also deals with unobserved values. Also, there are applications where $f$ is a non-convex function at all with respect to $x,y$, but it is convex w.r.t. each of them (e.g. $f(x,y)=xy$). In these problems, the iterative process you describe is many times used in practice and works quite well. 

Specifially, I'm mainly interested in the $r=2$ case, but this might be useful to others for general $r$ values. 

If there exists a path with sum T, the algorithm return TRUE. If there is no path summing up to a number between $(1 − c)T$ and $T$ for some $c\in (0,1)$, the algorithm return FALSE. If there is a path summing to a number between $(1 − c)T$ and $T$, the algorithm may output any answer. 

I think you could show that for any $k$, $3k$-regular graphs (e.g. 6-regular) the problem is NP-hard by a reduction from cubic graphs: Let $G=(V,E)$ be a 3-regular graph, then construct $k$ copies of $G$, and connect them: $$G_k=(\cup\{V_i|i\in [k]\}, \cup \{E_{i,j}\ | i,j\in [k] \})$$ Where $$V_i=\{v_i|v\in V\}, E_{i,j}=\{(u_i,v_j)|(u,v)\in E\}$$ Now it shouldn't be hard to argue that if a vertex set $H\subseteq V$ is a vertex cover for $G$, then $\cup \{H_i|i\in [k]\}$ is a vertex cover for $G_k$, and the smallest VC for $G_k$ can't be smaller than $k$ times the smallest VC for $G$, every $V_i$ has to be properly covered. 

I think you can make the classical local ratio algorithm by Bafna et al. give a $2-o(1)$ approximation on the following family of graphs: Take $G_n$ to be a $K_{n,n}$ (the complete bipertite graph with $n$ vertices on each side), and then delete a single edge. The following shows that the algorithm might output all of the "blue" vertices ($2n-4$ in number) as the FVS approximation, while there's a much smaller FVS for $G_n$. 

Not sure if this is the right SE forum for it, but the answer is yes. I'll give the reduction in two steps: 

The $q$-Dimensional $p$-Matching is defined as follows: Given disjoint universes $U_1,\ldots,U_q$, think of an element in $U_1\times\ldots\times U_q$ as a set that contains exactly one element from each universe $U_i$. Now, given a family ${\cal S}\subseteq U_1\times \ldots\times U_q$ and a parameter $k\in\mathbb{N}$, determine if there exists a subfamily of $k$ pairwise-disjoint sets in ${\cal S}$. We abbreviate by $\#q$-Dimensional $p$-Matching (or $\#(q,p)$-matching in short) the counting version of the problem, asking how many matchings exist in $\cal S$. 

Now it is obvious that if $G$ was transitive, this algorithm return true. Now assume $G$ wasn't transitive. Let $e_1=(v_i,v_j),e_2=(v_j,v_k)\in E$ such that $(v_i,v_k)\notin E$ (there has to be such edges as $G$ is not transitive). The probability that $e_1,e_2\in G'$ is $\frac{1}{6}$, therefore in each iteration the probability that the algorithm will figure $G$ wasn't transitive is $\frac{1}{6}$ and after $O(\log{(\delta)})$ iterations the failure probability is at most $\delta$. 

Given a planar graph $G=(V,E)$, there exists a quadratic algorithms for 4-coloring $G$ (and $G$ is surely 4-colorable). Assume you are given a set of $k$ constraints of the form "$v_i \text{ is colored by }f(v_i)$". Our goal is to determine whether this precoloring could be extended into a valid 4-coloring of $G$. 

This is impossible in general. The reason is that the grid-distance is symmetric while being closest neighbors is not a symmetric relation. If you have many points ($\geq 9$) close to the origin and then a point at, say, $(M,0)$ for some large $M$. The point at $(M,0)$ has a its closest neighbors as some of the points which are closest to the origin, but these points consider it as the farthest point from them. 

Batch Decrement and Space Saving does not seem to have a simple generalization to the weighted case, as both maintain a data structure that allows finding the minimum counter in constant time, which might not be doable in the weighted setting. 

The answer is no. I'll give an example of a language $L$ which is regular in binary but not in unary: Consider $L=\{10^k|k\in \mathbb{N}\}$. The corresponding language in unary is $L'=\{1^{2^k}|k\in \mathbb{N}\}$. It's easy to see that $L$ is regular while $L'$ is not even context free. L'' also isn't regular either, by the link @Sylvain posted in his comment. 

You may assume that the size of a memory word is $\Theta(\log n)$ and we can read the $x,y$ indices in constant time. 

I'm interested in a variant of the maximum weight matching in a graph, which I call "Maximum Fair Matching". Assume that the graph is full (i.e. $E=V\times V$), has even number of vertices, and that the weight is given by a profit function $p:{V\choose 2}\to \mathbb N$. Given a matching $M$, denote by $M(v)$ the profit of the edge $v$ is matched with. A matching $M$ is a fair matching iff, for any two vertices $u,v\in V$: $$(\forall w\in V:\ \ p(\{w,v\})\geq p(\{w,u\}))\to M(v)\geq M(u)$$ That is, if for any vertex $w\in V$, matching $w$ to a vertex $v$ gives higher profit than matching it to a vertex $u$, a fair matching must suffice $M(v)\geq M(u)$. 

Assume we know a parameter $n\in\mathbb N$, and then get to observe a sequence of elements $x_1,\ldots, x_n$, one at a time. Our goal is to count the number of distinct elements in $x_1,\ldots, x_n$, and succeed with probability $1-\epsilon$. A simple approach would be to compute a $\log\left({n\choose 2}\epsilon^{-1}\right)$ bits fingerprint of each element, and then count the number of distinct fingerprints. Since the number of distinct elements is a most $n$, with probability of at least $1-\epsilon$, all fingerprints of distinct elements will be different. This gives us a total of $\approx n\log (n^2\epsilon^{-1})-n$ bits of space. But is this anywhere close to optimal? Can we perhaps use only $O(n\log\epsilon^{-1})$ bits for the problem? What would be a lower bound for this problem? 

The goal is to compute the rank distribution $D$ of $x_i$ (the input is the set $X$ and an index $i$). 

EDIT: Similar trick could be applied to get $2k$-regular graphs as well. Consider for example the following reduction from 3-regular to 4-regular graphs; Partition the graph vertices into pairs (duplicate the graph if the number of vertices is odd), and for every pair $u,v$ connect each to a blue vertex in the following structure: 

Implementing in $O(\log n)$ time is straightforward. I'm specifically interested in showing a lower bound. 

The motivation for the question comes from peer-to-peer routing networks (of a specific type, which can approximately be represented as regular graphs). Assume that the source node $s$ broadcasts a message to all vertices such that every vertex propagates the message upon arrival. I'm interested in the number of nodes we need to query before we find a neighbor that received the message already. 

If it helps, you may assume $M=N$ (which is what I need for my application), but I think it's interesting for general $M,N$. 

Reservoir sampling - maintain a sample of $k$ elements, uniformly sampled from the set of items which appeared in the stream so far, in $O(k)$ space. Approximate bit counting on a sliding window - given a bit-stream, answer queries approximating the number of $1$'s in the last $N$ bits, and do so using $O(\log N)$ space. Sampling from a sliding window (at any point, give a sample of $k$ elements uniformly chosen from the last $N$ elements of a stream). Exact item counting - answer queries of the form "how many times did $x$ appear in the stream" such that the answer will be correct w.h.p. using only sublinear space (modern data structures are quite complex, but Count–min sketch seems to fit undergrad course). 

Such result would be weaker than the FG's conjuncture, as counting bipartite matchings is the same as $\#(2,p)$-matching. 

Let's prove 1: Let's assume $f(x)$ is of degree $n$ and write it as $f(x) = x^n - a_{n-1} x^{n-1} - ... - a_0$. If $\alpha \leq 1$, then all of the roots of $f$ in [-1,1] will end up in [-1,1] in $h$. Let $x$ be a root of $f$ such that $|x|>1$. This means $x^n = a_{n-1} x^{n-1} + ... + a_0$. since $|x|>1$, $|x^n| > x^{n-1},x^{n-2},...,1$ hence $$|x| \le \text{max}(1, |a_{n-1}| + ... + |a_0|).$$ Define $\alpha = \text{max}(1, |a_{n-1}| + ... + |a_0|)$ and you're done. 

Denote $[n] \triangleq \{1,2,\ldots,n\}$. Assume we would like to have a data structure $S$ which kinda works as a dictionary from $[k]$ to $[v]$, and supports add/remove/update/query functionality, without any dynamic memory allocation (everything has to be pre-allocated). In general, one cannot do better than using $\Omega(k\log\frac{v}{k})$ bits in a succinct implementation or $k\log v$ bits in a simple array/hash table.. But if we assume that no more than $m$ items may be added to $S$ (assume you may ignore any additional add operation until some item is removed), we can improve the memory requirements, especially if $m<<k$. Without getting into succinct implementation (is such known (which takes $m$ into consideration)?), one may simply allocate an array of $c\cdot m$ (where $c$ is a space/time performance parameter) linked lists, and whenever key $x$ is added to the structure, place it in list $h(x)$ for some function $h:[k]\to [cm]$. If $h$ spread the actual keys (about) evenly, then the structure may still perform operations in $O(1)$ time for constant $c$ (as $\frac{1}{c}$ is the expected list length), while reducing the space requirement to $O(m(c+\log (vk)))$ (this can be done without any need for dynamic allocation). I'm currently working on a succinct implementation of such structure for some application, and was wondering if there is a name for it.