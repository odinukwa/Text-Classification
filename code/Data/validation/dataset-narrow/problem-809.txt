I'm trying to get logrotate to rotate some logs for me. I feel like this should be simple, but it doesn't seem to be behaving as I would expect. Here's my config: 

This is not a disk space issue as there's plenty available on all disks. We're thinking it could be something with Windows internal database? but I honestly have no clue how to start troubleshooting that. There are other information messages in the Application log for MSSQL that may support this theory? 

I've got several files in this directory that are non-zero files that haven't been rotated in days. If I run it tells me "log does not need rotating". also only has status for the last time I forced the rotation with -f (which was days ago). Just to make sure logrotate was actually even set to run I checked and the job to run cron.daily is definitely in there and cron.daily had an entry for logrotate. Anywho - IDK what's going on. Anyone have any ideas? 

We have a RDP Server farm based on Window 2008R2 servers with DNS round robin. That is, people make a connection to "myfarmname", which first takes them to the ip of some "farmmemberX.mydomain.local" and then they are redirected to some "farmmemberY.mydomain.com" according to load. That's also how everything workd until yesterday. As of today, most users experience problems and cannot connect to the farm, usually with an error message like this (translated from German): 

You need an entry in your Additionally, it may be necessary to modify the entries in the line , otherwise it may happen that works but applications (i.e. , , etc.) don't (or was it the other way round?). 

Your general question is not really answerable. There is no clear difference between "a shell" and "a console" in functional terms. There are nuances ... As conventionally used (in the UNIX / Linux) a shell deals with interpreting the shell language, running commands, maintaining session state, and leaves the mechanics of user input and output to something else. The something else might be the OS / hardware, or it might be something like a local or remote xterm. One usage of the term "console" is for the thing that does that bit. But the term "console" is also used to refer to something that provides higher level functionality; i.e. something that extends or subsumes the functionality of a classical shell, to provide something closer to what the user is presumed to want to do ... most of the time. I suspect that this is the case here. So to answer your question: 

/tmp is shared by default. You can probably make it work out of there, but probably better to use a more traditional document root. If you want the files to be cleared out with any regularity you could just clear the directory with a scheduled script. 

If the event originated on another computer, the display information had to be saved with the event. The following information was included with the event: NT AUTHORITY\NETWORK SERVICE [CLIENT: ] The specified resource type cannot be found in the image file It may be helpful to note that this seemed to start happening right after we migrated WSUS from another server onto this one. I've been searching all over the internet for someone who has this exact error and I'm not finding anything. (Don't you hate it when that happens?) I've tried completely removing the WDS role and reinstalling it -- didn't help. I suppose I'd really just like some guidance on where to go from here if anyone has any ideas. 

Is it good practice to version control the nodes and roles when using chef? If so, what is a good way to do it? It looks likes one should be able to take a tree of JSON files created using and simply check it into VC. Are there better alternatives? Update It turns out that checking in the JSON produced by is not a good solution. The problem is that the JSON produced by the script is not stable. Each time the hashes come out in a different order and the resulting files are totally different to the previous versions ... even though they means the same thing when parsed as JSON. I would be better of just saving compress tarballs. (But if I could canonicalize the JSON files by ordering the attributes ... ) 

So then I went back and found that the Applications log was full. Saved the contents, cleared it, tried to restart WDS (to see if any new errors showed up in the app log now that it was empty) and to my surprise was greeted by a "The WDS service was started successfully!" message. Wat. No idea what happened. This was literally the only thing that I did... I did have to grant myself permission to get into the MSSQL folders... but that shouldn't have fixed anything. Can a full event log really brick WDS like that? That doesn't seem right... does it? 

Okay, so I've managed to get the service started... but I'm a little confused as to what happened. I started poking around in the SQL logs in C:\Windows\SYSMSI\SSEE\MSQL.2005\MSSQL\LOG and found a line that said 

If you are asking if you can set things up so that one machine can talk to another machine's loopback IP (e.g. ) then the answer is that it cannot be done directly, and it is probably a bad idea. 

We've got a CentOS server running a cluster of virtuals. Occasionally the cluster's internal network drops out for a minute or so ... and then comes back. The problem is somehow related to the actual network traffic, but it is not a simple load issue. (The system is generally lightly loaded, and the problem occurs irrespective of actual load.) The setup: 

I've exhausted all my options apart from switching to KVM ... or slaughtering more roosters. Any suggestions? 

HPKP in a webserver works by adding headers to replies containing hashes of various public keys, such that at least one of the hashes is valid = of a certificate in the certificate chain of the current server certificate, and at least one is invalid = not occurring in the chain and considered a backup cert. Several strategies exist for choosing the cert to represent the valid entry: 

I just noticed by pure chance that one of my Cisco 4500 switches has its clock going wrong: it is more than 2 minutes behind in spite of seemingly functional ntp. In my opinion, even a single second should not be considered acceptable for the systems involved. Also, I wouldn't have noticed the difference from diagnostics, had I not compared it to a simple wall-clock. Some details Here's ntp information for some of my hosts (10.0.99.1, 10.0.99.2, 10.0.1.119, 10.0.99.241) that are partly referencing one another for fallback, but mainly should all ultimately by syncing with 10.0.0.1, which again pulls the time from outside. So the time discrepancy cannot result from different original time sources. As the observations made me somewhat paranoid, "has correct time" in the following means: (or ) produced an output that matches my wall-clock and my local system clock (which is fine according to $URL$ with an error certainly below 1 seconds (accuracy of me hitting ENTER while watching my local clock) 10.0.1.119 (Ubuntu) has correct time 

Have I over-simplified this? Does anyone know of any examples that contradict this? (Lets assume that we are talking about the "recommended" RPMs / DEBs for the respective platforms ...) 

I have a Chef Node that I'm initially setting up on one network that I need to (physically) move to another network. When this happens, I want the IP address and domain name to change, and the Node name and simple hostname to stay the same. I know I can do this by deleting the Chef Node and recreating it. Could I also do it by editing Node attributes? Or would that break the client keys or (worse still) the server? (I'm using Chef 10.16.2 ...)