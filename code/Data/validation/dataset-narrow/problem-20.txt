I am trying to implement my own Gradient Domain Path Tracer by following the code of this guy who already implemented it: $URL$ I already managed to go through various steps but I wanted to do something more. I expanded the code of in the reference by implementing next event estimation and here are some results. Normal Path Tracer image: 

The problem is that the image is really dark, you can see how small the light is, but somehow in the target framework it works correctly while in mine: 

As far as I understand (please correct me if wrong), this implements a uniformly random sampling. Also, as far as I have read, this kind of sampling has a PDF = 1 / (b - a) . My question: do I have to use this PDF as well like I do for my Cosine-weighted Random Distribution or not, since it is a uniform distribution? if yes, what's the range (b - a) that PDF talks about? Thanks in advance! 

Things work really fine and the results are shown with the picture above. One more thing: I have only diffuse surfaces in my scene. Now, the problem is that in this method I use 2 kind of PDF's: 

It's rendered with 16SPP. The core code behind it is quite straightforward. It just implements the rendering equation and it is shown and commented below: 

I am trying to implement for research purposes a path tracer, but so far but results are not so good and I will explain you why. The general idea before getting to the code: I am working on paths that are generated before sampling them. I mean that the first step of my algorithm consists in calculating a path for a certain pixel (x,y). This path will perform some bounces within the scene and, if terminates on the light, will be considered valid, which means that I can calculate its contribution for the pixel (x,y) which otherwise will be black. But first, my aim is to reach an "acceptable" image with a few number of SPP (around 16-32 spp). This is because I am following the code of a target framework, easy implemented, that manages to reach such good results in few steps (explained later). The following is the target image that I want to reach, produced by the target framework: 

It's correct as far as it goes, but there's more. The view vector goes from the shading point to the eye point, but it's normalized (divided by its length so that it has length 1). So if you're shading $p$ and the eye is at the origin, the view vector is $\tfrac{-p}{|-p|}$. It's not always this simple, because the "eye" isn't always at the origin. In a scanline rasteriser, you use the view transform to move things to the origin, because that's the only way to make the projection work - but not in a ray-tracer. The camera can be anywhere in a ray-tracer, and when you trace secondary rays (for reflections, refractions, or some other effects), it's definitely not in the same place as the camera. In the case you're working on right now, it sounds like you can just normalize $-p$, but you need to remember that it's really the direction from the shading point to the eye point, so that you can do the right thing when the eye point isn't the origin. 

I was concerned at first that Vulkan is much harder to program against, and that while it would be OK for the experienced developers at larger companies, it would be a huge barrier to indies and hobbyists. I posed this question to some members of the Working Group, and they said they have some data points from people they've spoken to who've already moved to Vulkan. These people range from developers at Epic working on UE4 integration to hobbyist game developers. Their experience was that getting started (i.e. getting to having one triangle on the screen) involved learning more concepts and having longer boilerplate code, but it wasn't too complex, even for the indies. But after getting to that stage, they found it much easier to build up to a real, shippable application, because (a) the behaviour is a lot more predictable between different vendors' implementations, and (b) getting to something that performed well with all the effects turned on didn't involve as much trial-and-error. With these experiences from real developers, they convinced me that programming against Vulkan is viable even for a beginner in graphics, and that the overall complexity is less once you get past the tutorial and starting building demos or applications you can give to other people. As others have said: GL is available on many platforms, WebGL is a nice delivery mechanism, there's a lot of existing software that uses GL, and there are many employers hiring for that skill. It's going to be around for the next few years while Vulkan ramps up and develops an ecosystem. For these reasons, you'd be foolish to rule out learning GL entirely. Even so, you'll have a much easier time with it, and become a better GL programmer (and a better GPU programmer in general), if you start off with something that helps you to understand the GPU, instead of understanding how they worked 20 years ago. Of course, there's one option more. I don't know whether this is relevant to you in particular, but I feel I should say it for the other visitors anyway. To be an indie games developer, or a game designer, or to make VR experiences, you don't need to learn Vulkan or GL. Many people get started with a games engine (Unity or UE4 are popular at the moment). Using an engine like that will let you focus on the experience you want to create, instead of the technology behind it. It will hide the differences between GL and Vulkan from you, and you don't need to worry about which is supported on your platform. It'll let you learn about 3D co-ordinates, transforms, lighting, and animation without having to deal with all the gritty details at once. Some game or VR studios only work in an engine, and they don't have a full-time GL expert at all. Even in larger studios which write their own engines, the people who do the graphics programming are a minority, and most of the developers work on higher-level code. Learning about the details of how to interact with a GPU is certainly a useful skill, and one that many employers will value, but you shouldn't feel like you have to learn that to get into 3D programming; and even if you know it, it won't necessarily be something you use every day. 

The BSDFDiffuseReflectionCosineWeighted() just calculates the new direction like in the target framework, tested and working. What remains last is the Sample method which calculates the final color of the pixel using the path calculated right above: 

I need to understand how to calculate the final density probability of a path. Can you help me doing it? Thanks in advance! 

Everything fine so far and for any implementation detail you can just look at my code but problems come with my Gradient Domain Path Tracer. Indeed, there, as also in the reference link above implemented by Tzu-Mao Li, I need the final probability density of a whole path to compute the final gradient image. How did I calculate it in case without Next Event Estimation (NEE) ? In that case (since I have only Diffuse surfaces) this probability is the product of CosTheta / PI at each bounce in the scene. Everything is fine and the resulting gradient image is shown above. Instead, in case I use NEE things don't work anymore because the Probability Density of my whole path changes and I don't manage to understand how it is. The resulting Gradient Domain Image with Next Event Estimation is: 

I am trying to implement my own path tracer but before arriving to the question I want to give you a short overview: In the implementation of the rendering equation I use some particular technique in order to sample surfaces. For example: when one of my rays hits a diffuse surface, the next ray bouncing from that surface will be calculated using a Cosine-weighted Random Direction. This implies that in my rendering equation I have to take into account the PDF that this specific random distribution implies and specifically divide for this PDF in my equation. So far everything is okey. Now, my question. I want to implement a particular technique called "next event estimation" which simply samples the lights. In order to do so, I want to pick a random point on my light, which is spherical, by using the following code (C++): 

Because $x_{proj}$ doesn't vary from $0 \to width$, it varies from $-\tfrac{width}{2} \to \tfrac{width}{2}$. What's important is not the width, but the minimum and maximum values of $x_{proj}$. Because $(0, 0)$ is in the centre of the viewport, not the corner, the min and max values are plus and minus half of the width. 

When you offline-render an animation, you take all the 3D information and turn it into a video file: a sequence of 2D images. The video file doesn't have any of the 3D information. A 3D game doesn't work this way because when you run the game, the 3D information needs to be present. It's quite common to use 3D models for making 2D games. Donkey Kong was a very early example of this: all the characters and backgrounds were modelled and animated in 3D, then rendered to 2D animations. These 2D animations were then used as sprites in the game. It allowed the sprites to look a lot more impressive than could be achieved with hand-made 2D sprites, using 3D ray-tracing techniques that weren't usable in real-time on the available hardware. More recently than that, it was common for cutscenes in 3D games to be pre-rendered videos. Cutscenes don't need to be interactive like the rest of the game. Using a pre-rendered video means that the artists can use expensive, non-real-time techniques to make the cutscenes look better than the interactive parts of the game can look. This isn't so popular nowadays: video files are big, and real-time rendering is a lot better than it used to be, so doing the cutscenes in real-time gives you lighting and shading that's almost as good, without having a fixed-resolution video file with compression artefacts. That said, there are some graphics techniques that use an offline pre-processing step to allow you to use techniques that wouldn't be possible in real-time (on the target hardware). The simplest example is texture compression: the compression is typically quite expensive, but it makes the textures use less memory, so you can have higher-quality textures. If your scene has static lighting, it's common to bake lighting including global illumination and shadows into a texture, or into static light probes. This replaces the expensive lighting calculations with a simple texture look-up at run-time. However, at the stage you're at, you shouldn't start worrying about any of these techniques yet. All of the rendering you'll be doing will be at run-time, so you need to optimize it to run at the target frame-rate on your target hardware. 

You can't see it in the code because it gets simplified witht the cosTheta attenuation term of the rendering equation and PI of the diffuse BRDF where my BRDF is indeed diffuse/PI because we want to consider only DIFFUSE objects. (For any question about the code do not hesitate to ask.) I tried to emulate this code with the difference that I don't calculate the path at the moment I want to calculate the color of a pixel (like the target framework does above), but I calculate a path beforehand: My generatePath() method indeed tracks the path into the scene and checks all the vertices it hits. The checkIfRayIntersectSomething(t) method you will see used, it's just a pseudo method implemented in my framework and that I omit posting cause of its length. I use it to check if my ray hits something in the scene, if it does, it update the "t" with the distance to that object. NOTE: the light is not considered an object itself. Hence, I also have a checkRayLightIntersection(hitLightPoint) which checks the intersection with the light, if there is any, the hitLightPoint is updated with the point on the light I have been hitting. The light is a 2D surface of area 2x2 placed at the same position (-1,20,9), as the target framework does. First the definition of a couple of struct to store some info: 

Results are already good.. but as said before, I wanted something more. So I implemented Next Event Estimation and here is the result of the basic Path Tracer: 

The DiffuseReflectionCosineWeighted implies that we have to apply a PDF to our recursive step. This PDF is cosTheta/PI and we have to divide for it in: 

I am sure that less rays reach the light cause I have been calculating them in both cases: small area light and big area light. I also tried to compensate the dark image by increasing the light contribution when reached by the path, but it brings to white pixels (for paths that reach the light) and the black ones (cause of invalid path) keep remaining black, so not a good approach. It seems that (as far as my understanding arrives) the target framework applies direct lighting, that's why such a good image BUT according to theory direct lighting only decrease variance, reducing the noise.. which means that using or not direct lighting shouldn't change the resulting image so much. Is there a way according to you to fix this problem? Thanks in advance. 

You also need to scale the cylinder. Take the magnitude (length) of $p_1 - p_2$, and divide it by the length of the cylinder (or don't divide, if your cylinder is already unit length). This gives you the scale factor. You can apply this as a non-uniform scale along the axis that the cylinder points in. (Since you were able to work out how to apply a rotation, I assume you can make the appropriate scaling matrix given just the scale factor and direction.) Make sure to scale it before rotating or translating the cylinder! If you translate before you scale, you'll translate the start point of the cylinder as well as its length, so the start won't be attached to $p_1$ any more. If you rotate before you scale, you won't be scaling the length of the cylinder, you'll be stretching it in the wrong direction, so it won't be a cylinder any more. 

It's easy to work out if you consider not that case but the angle at v3 (if the "cube" were continued past v3). By the time you get to v3, the angle is simply the desired bend angle. (That's not quite right, though: because you've got alpha on the decreasing side, it's 90 degrees minus the desired angle.) You have to split that angle equally among all the segments going along the bend direction, so the angle alpha is simply 90 degrees minus (the desired angle divided by the number of vertices). So for A, where the requested angle is 90 degrees, each vertex gets a third of that angle: alpha is (90 - 30) degrees. For cube B, if the total bend is 35 degrees, each vertex gets just over 11 degrees, so alpha is just under 79 degrees. This doesn't generalise to an arbitrary axis, though. It only works for bends like those in your diagram. For a more general (and more useful!) tool, it makes more sense to simply apply a 2D rotation (about the axis) to each vertex independently. Exactly what rotation to apply depends on some design decisions about how the bend should behave when the axis goes through the mesh, and how strong a bend you want to allow. The crucial point is that it's not the same rotation for every vertex: the rotation is proportional to the perpendicular distance from the vertex to the neutral line of the transformation. In your example, the neutral line is the bottom edge of each cube.