I'm just wondering what happens if you do some kind of bulk insert for instance which inserted 5000 new records into this table. What would happen to the identity column then? 

If more records are added to Table2 I would want the contents of Table1 to be changed accordingly when the sync process next occurs. I found out that this is not working. Is it because when I tried it Table2 was not an article in my merge replication publication, or is it because merge replication doesn't support subqueries like this and recognising when a filter has changed. What it would mean is that the sync process would have to scan each filter, and work out what tables the subquery relates to and figure out whether these have changed. Does it do something like this? UPDATE: I have tried a number of things, but they don't work. 

You will have to perform character to character searches and comparisons that are much less efficient than relational integer search and comparisons beacause the latter are indexed, and algorithms to perform them are much more sophisticated than pure string compare. Unless you are really constrained (and even if so I would say...) use table relations, referential integrity constraints, indexes and so on, they will save your time ! 

Definitely, in a Relational Data Management System, you SHOULD use relationships between your entities. A simple example query where you will be stuck by using the first option : 

It would be a great design approach to put the clustered index on your FK if you will be fetching all the lines (I mean all the data of the line) for a given FK, or making joins based on that key : as they're stored physically in this order, they will come up much more quickly. Otherwise you could use a non-clustered index (slower fetching of the actual rows' data). You may encounter a slower insert performance though. Values can be multiple in a CI, they are also very efficient to quickly fetch a single, uniquely identified, row's data (Is it the reason why MS puts the clustered index on the PK by default ?). I would recommend to make some tests using both approaches with a subset of your data, and check the estimated execution plans using CTRL+L, then include the EP via CTRL+M before you execute the test statement. This way you will see wich indexes are being used and how the engine modifies its strategy depending on what you are querying. I generally avoid includig dates for unicity beacause on multiple insert statements the date for all inserted records is the same... I prefer using the default value of the uniqueidentifier type : myCol uniqueidentifier NOT NULL DEFAULT(NEWID()) which ensures uniqueness among all. If you need all the columns' data of several rows with the same foreign key I would use the clustered index on non unique values. You may however check the execution plans. 

Turn off all default constraints for a particular table (not ideal if some of them are not for sequences) Create the sequences manually at the client side, and then create a custom handler to apply the correct sequence values at the server when syncing 

I am running merge replication with SQL 2012. There seems to be a nasty consequence of the delete triggers added for replication in SQL 2012. Inside the delete triggers are this, 

I must put out a warning. We have now moved away from merge replication, and I would have to suggest that the scheme above could be a major performance issue. The exception to that would require you to have a small publication with either small amounts of filtering and/or filters that are not multiple levels deep. For instance 10 - 100 articles might work fine. If you push the above scheme too much you could have performance / locking issues. Every time you insert a record into the top filtering table the merge replication trigger has to process all the child tables. It also adds records to MSMerge_contents and MSMerge_genhistory for all the child tables. The more child tables you have, and if they have a big amount of records in them then the more processing power will be required. We had a problem with sp_MSsetupbelongs being too slow and timing out. In the end we came to the conclusion we were pushing merge replication too much and that this technology would not work for us. This leads me to suggest that if the filtering scheme in merge replication out of the box is not flexible enough for your situation then either don't filter, or don't use merge replication. Test Test Test though of course, every situation is different. 

I know how to filter the list of tables - using the filter icon. However, this icon is disabled when the Databases node is selected. Still, I am wondering whether it is possible. 

Stupid, right? We also came to think so. So, I had to write a tool, which given an old export file converts it to some database independent binary format. The problem is how to unit test it? At the very least, I need 4 databases with identical data, which brings me to my question: 

All it does is fire a simple SQL query to a local Sql Server 2012 instance using async I/O API. Notice the Max Pool Size is set to 1000. This function is then called by the following code: 

I am using merge replication with SQL 2012. When performing a clean sync using the merge agent I am getting a timeout on this stored procedure, sp_mssetupbelongs I am wondering what the best way to diagnose this issue? When I take the SQL that was run, and run it in management studio it runs instantly. I can only assume that part of the required operations were done the first time it was run and timed out. More specifically it must be a particular group of merge articles that are slowing it down, so how can I isolate this? 

I am in the process of implementing merge replication in SQL 2012 with web sync. I am wondering two things, 

The user inserts a record 'Shape' at a client The 'Shape' has a default constraint to set the NumberOfSides field to 4 The client user edits the value of NumberOfSides and sets it to 5 They sync with the server 

You may run packages created with "any" earlier version of SSMS using dtexec, they seem to maintain backward compatibility. $URL$ Check out the "Other versions" widget ! 

you could add an empty field at the end of each line, and remove the last column of your table. Otherwise check for all the line terminations (\r\n vs. \n), or the presence of \n in your contents... It should work at the end, good luck ! 

Edit : And if you want to include the UNION and filter on userId, instead include them in an inline table user defined function with a @UserId parameter. Second, consider (cautiously) using something like SET TRANSACTION ISOLATION LEVEL SNAPSHOT on your session in order to avoid discrepancies between the COUNT and the SELECT statements. However you could use the highest PhotoId at a given moment to prevent selecting more recent photos. Third, you could use a lazy loading mechanism on your front end, loading posts progressively as the users scroll down the page, and avoid the (somehow outdated) paging system. 

I am using merge replication in SQL 2012. Why can't you mark default constraints as NOT FOR REPLICATION? You can disable all default constraints for a merge article, but it is all or nothing so it doesn't seem to offer enough control. How about this scenario, 

I am using Merge Replication with SQL 2012. I look in the snapshot directory, but the largest file in there is a prc file which is 646 KB. I know for sure that the biggest of my replicated tables is 25 MB in the database after replicating, so I am not sure I understand why there aren't larger files in the snapshot directory? Also is there a place I can look for the snapshot files as they are downloaded to the subscriber? For instance the merge agent outputs messages such as,