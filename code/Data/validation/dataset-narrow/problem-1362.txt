You tell the GPU how to produce triangles from the input data. You specify vertex buffers containing all your vertex data, you also specify a primitive topology (lines, triangles, et cetera) and an optional index buffer. The topology tells the GPU how many and what pattern to take the indices in (for example, in sets of three for a triangle list). The indices simply refer directly to entries in the bound vertex buffer. If you don't specify an index buffer, the GPU takes the data from vertex buffer directly in the pattern indicated by the topology. 

The cross product is not defined between a vector and a scalar, so it means nothing (it would be a malformed operation, or perhaps a mis-interpretion of a simple vector scaling operation). In the link you provided, angular velocity is a vector when it is used in the equation that involves its presence as an operand of the cross product: 

Your problem is inherently serial -- you must complete an update of the simulation before you can render it. Offloading the simulation to a different thread simply means the main UI thread does nothing while the simulation thread ticks (which means it is blocked). The commonly held "best practice" for concurrency is not to put your rendering on one thread and your simulation on another, as you are proposing. I strongly recommend against that approach, in fact. The two operations are naturally serially related, and while they can be brute forced, it's not optimal and it does not scale. A better approach is to make parts of the update or rendering concurrent, but leave updating and rendering themselves always serial. So for example, if you have a natural boundary in your simulation (for example, if houses never affect each other in your simulation) you can shove all the houses into buckets of N houses, and spin up a bunch of threads that each process one bucket, and let those threads join before the update step is complete. This scales much better and is a much better fit for concurrent design. You're over-thinking the rest of the issue: Dependency injection is a red herring here: all dependency injection really means is that you pass ("inject") the dependencies of an interface to instances of that interface, typically during construction. That means if you have a class that models a , which needs to know things about the that it is in, then the constructor might look like: 

I have only tried development with the DS and the PSP, myself, and I've found the PSP to be a vastly nicer system to work with. The DS required a lot more hardware when I last looked into it -- I imagine some of these limitations have been worked around more recently, however. Both are, of course, going to be a little tedious because of the fact that they are consoles and the fact that you'd be developing on them via unsupported means (and so jumping through some hoops, et cetera). Developing on the PSP works best with older machines -- the "fat" original PSP seems to be the most broadly supported. You used to have to obtain a tool-mode battery -- either by purchasing one or making one yourself by taking apart a regular battery and applying a soldering iron. With that battery you could use some software to create a bootable memory stick that would let you install custom firmware to the device. That may not be necessary any longer, however, it looks like the latest versions of the custom firmware installers available can simply be copied to a memory stick and launched from the standard firmware OS, which seems nice. They have to be re-run every time the machine suffers a hard power off though. The PSP homebrew dev kit I used is here. They have toolchains for a number of other platforms as well. I built a (very small) CLR implementation for the PSP using this setup (it could run most C# programs with simple console output as long as they didn't thrown exceptions, because I never got around to implementing that). With some judicious use of batch files or shell scripts you can make the code -> build -> deploy -> test loop fairly quick, although it's always going to be more annoying than simple PC development. 

If every object in your game can be treated exactly the same, then it may be more efficient to create a common base interface for them, and manage them all as a big list of that base interface. As the complexity of your game objects grows, this may be less true and it may be more beneficial to split them out into different lists based on type or other criteria. As a rule of thumb, if you keep objects that will be processed similarly with each other organizationally, you'll be able to process them more efficiently. "Processing" here includes not just what they do when they every frame, but also how long they exist in the game world, what their lifetime management policy is, whether or not they must be processed in a certain order, et cetera. Keeping them together may also improve locality of reference and allow you to spread processing out onto multiple threads, which can be a big win for performance. 

In C++, a is an integral data type. It holds numbers, and on most implementations you are likely to encounter, it will be capable of holding 8 bits of data. The 24-bit RGB color representation (the most common one that does not encode transparency information) uses 8 bits for each red, blue and green channel. Thus, a single can hold a single color channel of data, and three objects can hold a complete RGB pixel. If you include transparency via an alpha channel (RGBA), four objects can completely define a pixel. Thus, an array of objects can hold the pixel information for a image of by pixels. Note that one usually uses s in practice, since signed s allow negative values which is generally semantic nonsense for color values. 

One can use the Visual Studio IDE for PS3 development -- the PS3 relevant bits are, of course, just command-line tools (GCC ports) that can be plugged in to makefiles that VS can understand, after all. There's also a toolchain called ProDG, by SN Systems that is used for PS3 development. GDB, as an aside, is its own, command-line program. Some IDEs (such as Xcode on the Mac) have a GUI that wraps it. So it's not "built in" to CodeWarrior or any other IDE, per se, but ships with some as GUI layers some times communicate with it. 

You can't render a rounded shape perfectly on a computer screen; a computer screen is composed of discrete cells (pixels) so some approximation is always going to happen. It's just a matter of hiding that approximation so the result is visually appealing enough. Nearest texture filtering is generally not what you want to create the appearance of smooth curves, because it simply chooses a color based on the closest integer boundary of a texel. Linear filtering ( in libgdx) is more akin to what you want, since it will interpolate between the surrounding integer-boundary texel coordinates to arrive at a color. This can remove harsh edges, but often has the result of overcompensating and making those edges "blurry," especially for uneven scale factors. If adjusting the filter does not provide the results you want, you may want to look into some methods of anti-aliasing.