with a reference to AlexNet paper. It was indeed released a little later than Glorot's initialization but however there is no justificaion in it of the use of a normal distribution. In fact, in a discussion on Keras issues tracker, they also seem to be a little confused and basically it could only be a matter of preference... (i.e. hypotetically Bengio would prefer uniform distribution whereas Hinton would prefer normal ones...) One the discussion, there is a small benchmark comparing Glorot initialization using a uniform and a gaussian distribution. In the end, it seems that the uniform wins but it is not really clear. In the original ResNet paper, it only says they used a gaussian He init for all the layers, I was not able to find where it is written that they used a uniform He init for the first layer. (maybe you could share a reference to this?) As for the use of gaussian init with Batch Normalization, well, with BN the optimization process is less sensitive to initialization thus it is just a convention I would say. 

In your case, I guess that you do have $d > 3$ and so you're in the second situation. Third question: when to apply PCA Well that depends in fact... First of all PCA performs an SVD which can become very expensive if you have a lot of features. In fact, there are several way to compute PCA, your way is the closer to the mathematical theory but in practice computing an SVD directly on $X$ avoids computing $X^T X$ which is expensive and allows to retrieve the same space. But in any case, in some situation, it can be expensive and time consuming and thus not practical. As PCA sorts the new feature space according to the value of the eigenvalues, i.e. of the variance of the new directions, removing the last direction might in some cases remove some noise which can help. However, at the same time, it can throw away valuable discrimative information, this is why other methods such as LDA can be interesting. So to summarize, the answer to the question is no, it is not a good choice to use it everytime, it depends of your problem. 

$X$ is also a $n \times 3$ matrix, then you do not perform any kind of dimension reduction and you should have $X_{new} = X$ (at least in theory, in practice you might have a very small numerical approximation error but it will basically be the same) $X$ is a $n \times d$ matrix with $d > 3$, then you do perform a dimension reduction and in this case you get rid of some information contained in the original data and in this case $X_{new} \neq X$ 

First question: computing $\Sigma$ Actually, you perform the same computation but using a matrix operation instead of a scalar operation. You might be mislead by your notation of the feature matrix $X$ that you write $x$ because in fact $$X = (x_j^{(i)})_{i,j} =\Big(x^{(1)} ... x^{(i)} ... x^{(n)}\Big)^T$$ that is to say, $i$-th row of the matrix $X$ contains $x^{(i)}$ the $i$-th sample. So, when you want to compute the empirical covariance matrix $\Sigma$ (which is indeed a $n \times n$ matrix), you have: $$\Sigma = \frac{1}{n} \sum_{i=1}^n x^{(i)} {x^{(i)}}^T = \frac{1}{n} X^T X$$ you can check that this is exactly the same computation. In your code, you are actually computing directly $\Sigma$ using the matrix operation (i.e. $\Sigma = \frac{1}{n} X^T X$) and this does give you a $n \times n$ matrix. So, both your implementation and the formula is correct. Second question: reconstructing $X$ Your new feature matrix $Z$ is a $n \times 3$ matrix according to your code. In fact, as your code does not show the original size of the feature space, we will see the two cases here: 

If you pass , assumes that the first row contains data and names the columns '0' to '12'. Instead you should pass to specify that the column names are in the first row or equivalently skip the header argument. You can then still continue with , because calling returns a array without the column names. Alternatively, you could also select your feature columns like so: 

Keep in mind that while Naïve Bayes is a decent classifier for many applications, the generated probabilities are usually not very representative. 

The VC dimension is an estimate for the capability of a binary classifier. If you can find a set of $n$ points, so that it can be shattered by the classifier (i.e. classify all possible $2^n$ labelings correctly) and you cannot find any set of $n+1$ points that can be shattered (i.e. for any set of $n+1$ points there is at least one labeling order so that the classifier can not seperate all points correctly), then the VC dimension is $n$. In your case, first consider two points $x_1$ and $x_2$, such that $x_1 < x_2$. Then there are are $2^2=4$ possible labelings 

One thing you can do about your 'participation' variable is including the beginning and end of your data window. Let's say your data ranges from $start=$ 1/1/2016 to $end=$ 1/1/2017. Instead of only calculating the difference between the second and the first post, you'd calculate the difference between the first post and 1/1/2016 and then the difference between the second and the first post. (If some people join the platform after 1/1/2016 then you'd take the minimum of the join date and 1/1/2016). And you'd also calculate the difference between 1/1/2017 and last post. So, if someone only had two posts $p_1$ and $p_2$, you would get the difference $(p_1-start, p_2-p_1,end-p_2)$. The differences $p_1-start$ and $end-p_2$ would then be larger than the differences $p_1-start$ and $end-p_n$ for some with $n>>2$ posts. 

If you only add parts of the vector (i.e. not update all weights) at each iteration, you cannot be sure that you make sufficient progress or even move in the right direction of the solution space. 

I think the documentation, at least for your example, is not too obscure. It doesn't tell you what kind of interpolation it uses because it doesn't use any: is a deferred operation. In order for it to work you have to call it in conjunction with a function that performs the interpolation, e.g. or (as can be seen from the examples in the documentation). Sometimes, it can be helpful to also look into the release notes for additional information. Scipy and Numpy are sometimes not very detailed, but they contain at least a decent number of references with more information. (There is also a section on Matlab versus Numpy here. If you are familiar with in R , then the pandas cheat sheet can help as a quick comparison between R and pandas for typical data wrangling operations.) I'm not aware of a "second more detailed documentation" that encompasses the entire libraries. You probably have to fall back to books (which, of course, aren't complete or always up to date). 

I am looking for a machine-learning (classification) model that can adapt to new data. By that, I don’t mean completely new samples in the online-learning sense, but rather additional details about an existing sample. Over time, for an existing sample, I learn an additional feature value that was missing before. (Once it is learned it doesn't change.) When I run an updated sample through the model, I would like to get a "more accurate" prediction. The time and order at which new data becomes available are not consistent and features can still be missing at the end. In my training data, I only know the data that is available at the end and not when which feature value became available. Most of the features are categorical and I am wondering if it would be enough to train a model by creating many synthetic samples where I randomly replace some of the categories with a “missing category” value. It seems to me though that this approach would not make sure that additional data would lead to a more confident prediction. The model would just see it as a new and different sample. How can I make sure that an update leads to a better prediction? Any hints are appreciated.