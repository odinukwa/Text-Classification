On solomon I don't get these warnings. Also 95% of these warnings mention the federated user, even though 99% of my queries are done using a different user. So that's why I suspect it's an issue with the federated tables / user. I wouldn't be too worried about it except that I've been having lots of problems on serenity and I suspect this might be related. Today my database just crashed and I had to reboot the server. Lots of angry customers. Other Symptoms For the last few months I've been running into what appears to be concurrent connection limits, as I have several scripts that loop through many rows of data (typically between 50 and 200) and use ajax to call a send email script for each row, or in another case calculate hours. Sometimes these looping scripts don't execute all the calls in the loop but instead die with 500 errors, in particular when there are many rows in the loop (like more than 100 rows). Smaller loops (like 50 records) usually have no problem. Yet according to Microsoft there are no longer any concurrent connection limits in IIS. Timeouts on the website Also sometimes not all of my sql statements are able to finish executing on page load. They just time out and return no rows. Then when I refresh the page usually they load, but if it takes more than a few seconds to load I know there will be some statements timing out. Timeouts in MySQL Workbench And if I try to execute a long running query in MySQL Workbench it will always time out between 10 to 12 seconds. So if I want to select or delete more than 30,000 rows at a time it will typically time out so I always have to limit the number of rows I'm working with. Backup fails Also, my backups have been failing using MySQLDump. They always fail when trying to backup a rather large table. The smaller tables are no problem but it can't backup this large table which has about 600 MB of data in 7 million rows. Idle Threads Several weeks ago the server was running really slow and I realized there were hundreds of idle database threads sticking around and not dying. I sort of solved this problem by setting my wait_timeout to 20 seconds, but that seemed like more of a patch than a solution. Open files limit I ran into another problem when I added open-files-limit=4096 to my.ini as that was recommended based on another setting that I adjusted in an effort to try to solve these issues I've been having. However adding open-files-limit=4096 was a big mistake as that pretty much killed the server. As soon as I commented that out it started working normally again. I've "fixed" all these problems so far, but I feel like I'm not addressing the root cause of all these issues, just patching them up. Bottom line, I know there's something wrong with my server and I suspect it may all be related to this warning I keep seeing in mysqlerror.log. Can anybody shed some light on why I might be getting this warning message? Could it be a dns problem? A permissions problem? A bug in MySQL? I should point out that my servers are both very busy. Each executing several thousand select statements per second. My entire my.ini file (I've removed comments to keep it short) 

Job.Title is not unique Why would you even have a relationship between two char(30) This is just messed up You give contradicting requirements in comments You need to step back and do a proper data design 

Drop the user from the database Delete and re-create the user Add the user to the database You might not need the 2nd but it should not hurt 

this sorts on PlanCode - if you have duplicates then use a row_number() if you need a particular order then you need to have that order in the table 

SqlDependency was not really intended for real time type processing. 3 ms is still pretty good. You should only retrieve required columns and you should use a Reader. DataTable has a lot of overhead. And you should be using . 

I don't know about a best but at a small number there is no real value. There is some overhead to using the index. Yes an index seek is faster than a table scan but there is some overhead to using the index. Index maintenance clearly has overhead. If a table has a PK then you should use that as a PK and typically clustered. Consider a table of USstates (50 rows) ID PK identity tinyint Name varchar(20) Region tinyint Region would be use to group states like NE, SE, ... I personally would never use indexes on Name or Region - a table scan is still very fast. Region would be a FK but that does not automatically create an index (to my understanding). The whole table is right at the 2K page size. If sort on State.Name is used a lot then yes that index would be used but I just don't think you could even measure the performance gain. Over a million rows then yes start building indexes up front. Between a thousand and a million then consider building indexes on a case by case basis. Even at 10,000 rows there are going to be a lot of cases of obvious indexes. A column like AddDate that is not likely to change and would be used a lot for search and sort I would index and maintain (de-fragment). A table with more than 10,000 rows that reference State as a FK I would index that column up front. But since you are asking the question maybe wait and optimize for real life queries. I would not want you to take the other extreme and put an index on every column as it might be used. An index has overhead. An index will slow down insert and update. A highly fragment index can be slower than a table scan. I get a lot of the users on this site want to optimize up front and have theoretical discussions. This is real life advice for a newbie in DBA. 

SHOW ENGINE INNODB STATUS; Didn't fit in this post so I posted it in pastebin: $URL$ SOLOMON The following are from Solomon. All these solomon values were taken on Saturday mid day, a time when there is very little traffic on the server. Solomon my.ini $URL$ Solomon Show Global Status $URL$ Solomon Show Global Variables $URL$ Solomon Show Engine InnoDB Status $URL$ 

I have two servers (solomon and serenity). Both servers are Windows Server (2012 & 2016), IIS, PHP (5.6.31), MySQL (5.5). On solomon I have several federated tables set up that point to tables on serenity. And I have a user set up on solomon specifically for connecting to these federated tables. Accounts are split over the two servers. Half my user accounts are on solomon, half on serenity. However, the login page for all users is on serenity. This is so that the serenity server can check to see which server this user should be directed to, it then redirects to the password page on either solomon or serenity. So in this way, all users go through serenity, but not all users stay there. However, there are a few other scripts running on solomon that do check these federated tables on serenity, so even though the user has passed through to solomon, the site is still querying these federated tables and thereby hitting serenity. As far as I know that shouldn't be a problem, but on serenity I'm getting lots of warnings in mysqlerror.log like this: (username, dbname and ip address obfuscated) 

If you need really tight control enforced at the data level. For example extensive auditing. Auditing is not much good if several users share the same account. If you have some users that have a need to access the data database directly. If security is that tight you typically don't even expose the database directly. You have a service and the client must get data from the service. In a web application the database (e.g. port 1433) is typically not exposed directly so you have a level of security. Even if the web application accesses the database directly the users still does not have direct access to the database. If the login and password is in the client application then it can be hacked. If a domain you can use integrated security. At the database you can have pretty fine controls. But row level control is a bit of work. A database is not a good tool for business rules. Business rules and detailed security are typically enforced at the application level. You can have a mixed mode where there are some stored procedures used by admins and you want to track which admin. And you may give read only access to users that generate report directly. 

This is a strange one as is unique. There could be a case to use [UserId] alone as the PK. You will get less fragmentation compared to a composite clustered index TenantId, UserId. I get you plan to use TenantId, UserId in all queries but you don't need to. UserID will uniquely identify each user. If you have reports by TenantId you can include that in the tables and put a non-clustered index on it. As far as email I would just put a non-clustered unique index on it. You can include the other fields if you like but it is just picking up data from a single row so it is not really necessary. Right now there is nothing to prevent duplicate email. Could argue repeating TenantId in all tables is not 3NF as it can be derived from UserId. I personally would not repeat TenantId and have a for each table that brings it in. I get you don't want to do it that way but it is still my answer. 

No cursors, No need to use a template, multiple DBs created provided you have the dbnames in a table 

you have given the user sysadmin, which basically has all rights under the sun in SQL server world. I would recommend you spend some time reading BOL Fixed server role sections to understand what each fixed server role does. 

Just on a quick glance with out doing much analysis I would change t_transaction_type to only have a credit and debit and convert the t_transction_type table to be a payment_type or some other meaningful name. I would also avoid using t_ , table name should just have a meaningful description and adding the prefix makes it hard to manage at the database level.(i.e. intellisense here would show all your tables since they all start with t_) 

First of all you shouldn't provide instance level permissions to your SQL server service account and database level permission, this goes against all security best practises. when you assign the service account through SQL server configuration manager SQL will provide the user with the necessary permissions to run the SQL server service. so I would first of all remove all the permissions you have given, and to the issue you are having, the service account need to be given the local security policy "Log on as a service", you will find this under local security policy under user rights assignment. you can get to the local security policy by under windows run box write secpol.msc P.S , also you should realise that when 

Not sure if it will improve performance but you are killing a number of left join with the where WHERE ipx.ProviderType IN ('IRF') turns that into a join 

A table can only have one clustered index. There is a chance of duplicate date so I would make ID a clustered PK. You will get efficient joins. You don't need to tell SQL the dates will be in order. But that will help keep fragmentation down. Just try a separate non-clustered index on Date. Not sure if including the ID would benefit here. Look at the execution plan and tune from there. 

The last two left joins are not used and should be deleted Have index on all ON conditions I would try materialize in a #temp with a PK 

It is more efficient to check if data has changed in the web application Sending an update to myslq with the same value uses more resources If there are no changes then send nothing Better yet don't even have the button active until there are changes If there are changes only send the changes and do so in one statement 

Delete does not always use the same indexes as a select but optimize the select will typically help try this 

Don't use here is going to be an arbitrary order. Are you sure that is what you want. without and order by is . You are partially batching but then you exclude partial batches. This seems wrong to me. You can just use in the final select. 

On your primary do you have a transaction log backup job running ? what would be happening is this db is larger by the time restores are completed the primary db would've had another transaction log backup taken, you just need to copy this transaction log backup to the DR and restore and mirroring should initialise. other option is if you have enough disk space on your log drive you can disable log backups on the primary until you complete mirroring setup and re-enable it. hope this help. 

I think your best option here is to pick the tables which are most important to you and look at rebuild/reorganize indexes on those tables first, you can put it into a separate job and run this regularly, and do the other tables on a different schedule. by the way why are you adding and removing .ndf files, can't you just drop the tables and re-create them ? and in that process remove/re-create the indexes ? more context around your process would be nice so we can understand and see if we could help you improve these processes. 

I see everyone has jumped away from the obvious, the query result only show 2 files, it should show 3 files, 2 .mdf files and the .ldf , what has occurred here I think is the OP has run the query in the context of master database or in another database context hence showing the incorrect results. this query need to be run under the context of the database you are looking to get the results from as the query refers to dbo.sysfiles. put use [database_name] where database_name is the database which you are interested in finding the space used and available.