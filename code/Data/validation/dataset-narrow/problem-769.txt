Your best solution to this problem is absolutely a conditional forwarder, although as someone pointed out above a delegation would probably work as well if your DC can route to it. Of course, if it can't you'd have bigger problems. Otherwise, just typical split-brain DNS scenario where you're answering most clients for what you have, but anything specifically to www.contoso.com you can forward to this other server. With the contoso.com it gets a wee bit hairier for kind for the same reason the other commenter mentioned. Your AD domain is using this. If your domain were instead a subdomain such as 'ad.contoso.com' you could also simply use a CF here. That, of course, assumes your dept1/dept examples are correct and not like dept1.ad.contoso.com If your servers are at least 2012 R2 I believe there is a new functionality for this: $URL$ Hope this helps. 

You could install a second NIC and multi-home the server if the software will support it. The risk to your AD domain depends on whether the credentials are used to log in to that server. If so, they are at risk and you probably want to manage it with an account that only has permissions to that box. Yes, someone owning it could be a further risk if you don't notice it, but as long as you're not logging into it with domain admins and the like, it wouldn't necessarily immediately compromise the whole domain. Long run though, it would be best to get the extra network gear and configure a DMZ and separate it from your internal network and open only absolutely required ports back to AD or anything else internal. 

The general answer is "no". uid and gid on the filesystem will be as set at the moment of the write and if they don't match on a different machine, then privileges won't match either. If you do not want to make a small revolution with uids/gids on several machines, you could try using acls to set the desired permissions for all desired users on all machines. I suspect this will use numerical uids internally, so it could happen, that giving access to your account foo on machine A, uid a, will give access to your files to a random guy bar on machine B, uid a. It also seems like it's more hassle than it's worth. I think that saner approach would be to use tar to migrate your development tree. I have also had a half-baked idea of carrying around a Subversion repository (with files writable only to root and appropriate access configuration files), and relying on svn server being present on all machines which you are going to use, but I do not think it's excessively sane. 

mysql-4.1.20-2.RHEL4.1 in the output of rpm -qa means, that you have this package installed. You can read the description of the package by running yum search mysql on my RHEL6 gives following short descriptions: 

If you do not plan any HA features, then iSCSI NAS isn't going to buy you a lot. About the only scenario would be failure of the server, when you could re-install the server and just re-connect the datastores on the NAS. OTOH it gives you one new failure mode, where NAS box fails and you have to restore the data somewhere. I assume the disks will be connected to some HW RAID card? Otherwise a single HDD failure will take out your datastore, which is bad. Locally attached disks chould give you better performance than NAS, especially in response times. I have another question: if you do not plan to grow beyond single VMware server, then maybe you could take a look at the free VMware hypervisor? Or just buy support for ESXi? If your VMs do not exceed limitations of the free license (RAM and CPUs of the host), then maybe you don't really need vCenter? 

Pardon, because I'm not super experienced with various Linux distros, but from the Active Directory side, this sounds like you may have a problem with your RID Master. If the pool is depleted and not being replenished, it may be that the other domain controllers cannot communicate with it for some reason. What happens under the hood is the RID master allocates a pool of (i think) 500 RIDs at a time for new objects, and when that gets to less than half a new request is made and a new block of RIDs is assigned. I would recommend trying a full dcdiag and looking through it for any errors (or asking your AD admin to do so) as a starting measure. 

I think you might be looking for 'erroractionpreference', this article explains through a few examples of how it works. $URL$ Given the other answer, if your script is larger than this function and you need to continue troubleshooting, then you'd need to wrap this bit of code by setting it at the beginning and setting it back to 'continue' after you are done processing. This will suppress non-terminating errors like that so they are not printed to your host. 

If I'm reading you right, then I guess it depends on how you're assigning IPs from your VPN and whether or not you have the capability of setting the DNS Server address, guessing you would but not every implementation is the same. You'd want to configure the zones that you want to be answered internally on that DNS server and leave recursion intact (although this is generally a bad idea if that DNS server is exposed to the public, as it opens you to potential denial of service attacks, and you might need to be more complex if so). This way your DNS server will answer what it can first and if it has no zone for the query it will forward to the internet roots by default. 

Would disabling password authentication and requiring public-key based be OK with you? You could also disable ssh completely, if you have some remote HW management solution, like remote console (IBM RSA or HP ILO). If you have physically separate management and production LANs, then it wouldn't be possible to break into the server by logging into it. 

One helpful thing to evaluate responses -- ask for references, CVs of people who will be assigned for the job, experience they have with similar projects. This may be hard on companies who want to enter the market (how to get the first reference?), but it reduces your risk (lower chance they will be learning too much on your project). 

Well, it says in your logs: USB disconnect. Could be a loose cable/connector or maybe ovecurrent on USB port if your backup script causes a lot of activity. If you want to be absolutely sure it's not filesystem-type related you could re-format the external drive and see how it behaves with ext4/3, NTFS or VFAT. 

You've been lucky ;). Drivers make assumptions about functionalities present and bugs absent in firmware newer than version X. Only certain combinations of drivers and firmware are tested, because it would be prohibitively resource-consuming to test all the pairs. Also it doesn't make sense to verify new versions of code against an ancient counterpart with known bugs fixed in later releases. If the server was running for a long time it may be that you've found another working combination or simply lucked out and didn't hit any bugs possible because of the mismatch. Worst case (based on experience with non-Dell hardware): server corrupts data on the disks, crashes and refuses to boot. HW specialist called to repair it will wonder why did it ever run with this FW/driver combination. Best case: server runs without any problems until it is replaced by a newer one 10 years since. My recommendation would be to upgrade to the next to newest version of firmware and the corresponding recommended driver. This should give you something that is reasonably new and supported, but not cutting edge code which may have yet undiscovered bugs. Note: Always have tested backups when modifying storage system configuration. 

Not sure if this answers your question because there's not a lot of detail but this is just asking you how you'd like to configure this new domain controller. Do you want it to be a DNS server? Do you want it to be a Global Catalog? I'm guessing the plan is not to have an RODC since you already have one there, but those are recommended if the site does not have a secure computer closet or data center. You only have to have the DNS service on one domain controller technically, but in all honesty with most scenarios I set them up as both a DNS server AND as a Global Catalog. DNS for the local site if there are enough users, and Global Catalog so it can answer queries through 3268/3269 TCP (GC queries). The GC is a read-only port that returns partial data for your entire forest, if you have a multi-domain environment this is especially useful. Old documentation says you should only use it as you need to, but this was based on slow connections in days gone by, these days with reliable network speeds that's usually a non-issue. 

Is the Windows 10 system part of a domain? When you load this module by default, it attempts to look at your service records, find an efficiently located domain controller and map it to the PSDrive "AD:\" so that you can navigate it within powershell and review records. It is possible to disable this: $URL$ 

A quick solution: make a symlink from the old directory to the new one. This will allow you to have possibly several versions of perl symlinked from /usr/local/whatever to /usr/lib/perl5. Another solution: looks like you've recompiled perl and it chose the default prefix of /usr/local. You may try to recompile it once again, but with --prefix="/usr" given to configure script. This will put your perl into the place the rest of the system expects to find it. 

Verify that the chain from root name servers to your current name servers is correct. Root name servers most probably do point to the correct TLD servers, but from there on it is worth checking if the DNS servers have the correct data (e.g. for www.mydomain.example.com it would be worth checking that nameserver for example.com points to the correct name servers for mydomain.example.com, and that these in turn have correct data on www.mydomain.example.com). After that you can only read on bad ISPs who break their DNS servers on purpose and contemplate ugly aspects of human nature. 

Ad 1) To the best of my knowledge it does not. You will have changed rules for creating new passwords, not requirements for accepting already set passwords. If you want to force users to change their passwords to one complying with the new rule set age their passwords. Ad 2) pam_cracklib doesn't give you that kind of flexibility. You either enforce minimum number of characters of given type (Xcredit<0) or you give extra credit (+1 "length") for up to N characters of given type (Xcredit=N>0) and set minlen high enough to be satisfactory even if user choses characters of one type only. 

Sure. Create a GPO with a WMI filter scoped to Windows XP, and apply the deny logon interactively/locally rights to DOMAIN\Domain Users, that should prevent them from being able to logon. Although you might get some loud and rowdy responses, be careful of that if you have any concerns of more senior management frowning upon it. However, they cannot actually "hide" this from you. The operating system is included in the computer object and you can perform a powershell query to detect the systems that still report as that version of Windows. As I recall this value should update if they use install a new operating system. A powershell script that seeks out the values of each computer object's attributes called "OperatingSystem" and "OperatingSystemVersion" would tell you what you needed to know. For example on my workstation, these return: OperatingSystem: Windows 8.1 Enterprise OperatingSystemVersion: 6.3 (9600) (Major,minor and build#) Does that help? 

data 52e is the key, this means that the AD server is saying 'Invalid Credentials.'. I'm not readily familiar with JXplorer so I'm not sure how you set the credential for the session, but something is going awry there. 

I believe this will answer your question: $URL$ In short, windows permissions vary based on whether the target is a folder or a file. The tables in this explain it in detail. The information is dated but for the most part this should still be intact. This is a bit more up to date and applies now but a bit more complicated. $URL$ 

You might revise step 1. You are limiting your customer base. You could limit the set of distributions supported, but I wouldn't go for just one. For application distribution I would provide packages in a format native to the OS of the nodes. Application update would be a simple package update then, which is as easy on sysadmin as it can be. If you want to prepare a whole environment, from the OS level up, you have 2 options. You might prepare a VM file in a chosen format (or formats). Then you would have to prepare the whole environment, from the OS up. Or you might prepare an automated-install system/image (using the distributions native non-interactive installation system, or combine it with e.g. xCat), which would deploy a complete OS, configure it, and then load your application on top. 

If the virtual machine runs sshd (usually they do), you could just ssh to it. If it gets its address via DHCP you can look up its MAC address in VMware Client and then check leases on your DHCP server. 

If the server is not exposed to the world, and you trust your users absolutely (generally that's not a good idea), then if it's working, you could just leave it be. If it is in any way exposed to the outside world, and/or you entertain the idea of legitimate user playing with it in an illegitimate way, then you absolutely need fixes and patches to your installed software. In this case, you have two options: