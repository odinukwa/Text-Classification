perturb a single weight upwards forward prop 4 timesteps perturb the weight downwards forward prop 4 timesteps get 4 deltas sum the 4 deltas to get a total change in Cost 

Edit As a solution, I've tried to determine how close to $e^{−1}$ and $e^1$ the $e^x$ actually lies. Basically an 'inverse lerp', and it indeed puts values to 0 and 1 range. However, I am not sure if it affects derivatives & if it no longer will work out of the box when using Cross Entropy. Here is the usual way to compute softmaxed-vector: 

Wouldn't this break parallelism? Previously, each of our example-threads could have computed everything, up to the activation values for the entire layer $l$. However, now we have to: 

The authors use notation $w_t(i)$ to signify i'th element in the vector. I cannot understand how $s_t$ is applied to the $w^g_t$ and what makes it possible to actually perform shift, say by +1 

Therefore, you are training against the version of your network from the similar but previous correction session. Notice, you don't store ["next state", action Y] because by the time you select it to train (maybe you didn't select it for several minibatches), the network might have a different q-value for that action Y. You could also copy your network to the second network (target network), but only, say, every 200 timesteps. You would still proceed to punish your network for any differences with the target network in the meantime, after every 30 timesteps. Notice, the intuition of why this works is because: the Q-values are sort-of "flowing" from finish back to the beginning, by a little with every new journey. And you are always updating the current state towards the successor state. And train the successor state against its own (even further) successor. 

And if I were to do the sliding-window approach in Feed-forward network, would it result in overfitting? I would assume yes, because network is exposed to the same information-regions for a very long time. For example, exposed to E F G H I J K for a long time 

When trying to overfit the network, what is the practical maximum depth of an LSTM Neural network before it will start to fall apart? If possible, what is the state-of-the-art depth in LSTMs produced by companies like Google? I know that in real world number of layers is implementation-dependent & number of neurons can be estimated, [1], [2] but curious if my results are typical: I've implemented a system in C++, using iRProp+ (here is the paper, section 3.1), however, I am struggling to get the error-propagation under control when going above 10 layers. I am just trying to overfit the network, it successfully and very quickly converges with 4-6 layers (~100 iRProp iterations and the error is down to 0.00001, where it started at around 4.00000) The network tries to predict the next character in the alphabet (made from 26 characters), so each layer has an LSTM that works with 27-dimensional vectors. The error-propagation happens after 25 timesteps If I crank-up to 10-14 layers, the error is really hesitant to even begin climbing down, and seems to simply oscillate around the 3.6 value. In incredibly rare cases if a weight initialization was lucky, with 10-14 layers the error will decrease, but usually it will just oscillate. Is that usual? I am using datatype, however, tested the double datatype and the oscilations still happen, so I doubt it's anything to do with precision. Adjusting the $n$ value (acceleration) doesn't seem to affect it either Been looking at examples [3], but so far getting impression people use a maximum of 2-4 layers... 

However, simply squeezing a one-hot into a 300-dimensional representation isn't enough - it must have a meaning. And we ensure this meaning is correct using an additional second matrix - which connects Hidden to Output We don't want to activate a hidden layer because activation-function won't be needed during runtime, however, in that case we will need a second matrix, going from Hidden to Output. This second matrix will make an entirely different one-hot from your embeding. Such a one-hot will represent a most likely word to be nearby (contextually) of your original one-hot. In other words, this output won't be your original one-hot. That's why a second matrix is needed. At the output, we perform a softmax, like in a classification problem. This allows us to express a relation "word"-->embedding-->"context-neighbor-word" Now, backpropagation can be done, to correct the Input-Hidden weights (Your first matrix E) - these are the weights we really care about. That's because Matrix E will end up being used at Runtime. Once again, recall, there is no activation at the hidden layer. We can dictate the network what "one-hot" must have been created in response to your original "input one-hot", and can punish network if it fails to generate a correct answer. 

Regarding training in parallel with minibatches instead of Parallelizing via Synthetic Grads: Longer sequences are more precise than minibatches, however minibatches add a reguralization effect. But, given some technique to prevent gradient from exploding or vanishing, training longer sequences can provide a lot better insight into context of the problem. That's because network infers output after considering a longer sequence of input, so the outcome is more rational. For the comparison of benefits granted by SG refer to the diagrams page 6 of the Paper, mainly being able to solve longer sequences, which I feel is most beneficial. (We could parallelize computations via minibatches anyway) 

As seen from 3, the Online Network gives us the action that it thinks is highest. However, we let the Target Network have the final say on the score of such an action. Were we to let Target Network select a best action, it could have very well selected some other index. 

Synthetic Gradients tell us we can rely on another "mini-helper-network" (called DNI) to advise our current layer about what gradients will arrive from above, even during fwd prop. However, such gradients will only come several operations later. Same amount of Backprop will have to be done as without DNI, except that now we also need to train our DNI. Adding this Asyncronisity shouldn't not make layers train faster than during the traditional "locked" full fwdprop -> full back prop sequence, because same number of computations must be done by the device. It's just that the computations will be slid in time 

I am having a hard time understanding difference between Target Network and Double DQN From this blog: Target Network generates the target-Q values that will be used to compute the loss for every action during training. The target network’s weights are fixed, and are frequently but by small amounts updated towards the primary Q-networks values. Double DQN: instead of taking the max over Q-values when computing the target-Q value for our training step, we use our primary network to chose an action, and our target network to generate the target Q-value for that action That sounds very similar to me, and the equation makes me lost too :( 

Ok, how about adding SG only on the last layer to predict "gradient from future" and only if it's the final timestep during forward prop. This way, even though our LSTM has to stop predicting and must backpropagate, it can still predict the future-gradient it would have received (with the help of DNI sitting on the last timestep). 

I see problems with both of them. Please note, I really like Synthetic Gradients and would like to implement them. But I need to understand where my trail of thought is incorrect. I will now show why Point 1 and Point 2 don't seem to be beneficial, and I need you to correct me, if they are actually beneficial: 

Neural network will pick up on any patterns as long as your environment is not fully stochastic. Here is what authors of Using Recurrent Neural Networks to Frecasting of Forex used to determine the amount of stochasticity: page 7, bottom. "Hurst Exponent". 

Question 1: Well, I was able to get benefit of "semantically grouped" words via that final softmaxed-output layer. However, it seems way too large - it still has dimension of 10 000 probabilities. Can I actually use that 300 neurons result in some other networks of mine? Say, in my LSTM, etc? Then weights of my LSTM wouldn't take so much space on disk. However, I would need to decode the 300-dimensional state back into my 10k-dimensional state, so I can look it up Question 2: ...are the "Encoded" vectors actually already sitting in the Hidden layer or in the Output Layer? References: first, second, third 

Pick and save initial weights of the network randomly (let's call it $W_0$) Split data into $N$ equal chunks Train model on $N-1$ chunks, validating against the left-out chunk (the $K$'th chunk) Get validation error and revert the weights back to $W_0$ shift $K$ by 1 and repeat from 3. Average-out the validtion errors, to get a much better understanding of how network will generalize using this data. Revert back to $W_0$ one last time, and train the network using the ENTIRE dataset 

I have a parent-gradient flowing into the activations of my final layer in my net (call this gradient $G$). Is it possible to first compute the gradient for all the preceding layers of my net, and then multiply these grads by $G$ to complete gradient-gathering for these layers? Or it really must be sequential, and no terms can be temporarily withheld as we descend down the net? (Feedforward net) 

Why is RMSProp in many cases converging faster than Momentum? Momentum: $$v_{dW} := \beta v_{dw} +(1-\beta)dW$$ $$W := W-\alpha v_{dw}$$ RMSProp: $$ S_{dw} = B \cdot S_{dw} + (1-B)\cdot (dW)^2$$ $$W := W- \alpha \frac{dW}{\sqrt{S_{dw}}}$$ Where $\alpha$ is the learning rate (0.01 etc), $\beta$ is the momentum term (0.9 etc), similar to B From my point of view, both momentum and RMSProp have "tendency to keep moving". Well, I can see how RMSprop will naturally accelerate on flat surfaces due to $$\frac{1}{\sqrt{S_{dw}}}$$ when $S_{dw}$ is small, but is there another benefit that RMSprop provides? 

When working with LSTM I am using a softmax classifier and a one-hot encoded vector approach. The softmax looks like this: $$S(h_i) = \frac{e^{h_i}}{\sum e^{h_{total}}}$$ notice, LSTM's result is a $h=tanh(c) \circ \sigma(p)$ Where c is the cell state passed through a tanh as well, and $\circ$ is the component-wise product of two vectors. Recall that tanh never goes beyond -1 and never goes above 1; The $\sigma$ never goes below 0 and never above 1 Does this mean that if we have a 4-neurons on the output - the best guess a network can make will be 1, -1, -1, -1? When softmaxed, this will produce $\frac{2.72}{3.83}, \frac{0.37}{3.83}, \frac{0.37}{3.83}, \frac{0.37}{3.83}$ which are these probabilities: 0.7, 0.1, 0.1, 0.1 and we can never get ~100% certainty, (Edit: or in this example even above 70%) no matter the learning rate? Is there a way to combat it without destroying its derivative working nicely with Cross-entropy? 

Do I require to remember something like , or just keeping hold of the previous momentum will suffice? Am I correct that this way we don't need to keep two matrices for momentums, but only need matrix? 

Yes, I can, because the outcome of the hidden (orange in the picture) already represents the actual embedded vector. It should be now used in any other custom networks (LSTM, etc), NOT the one-hot encoded vector. Using this orange vector will indeed greately reduce number of required weights if there are multiple custom networks under development. Answering Question 2: Indeed the encoded values are already in the result of a Hidden Layer (orange on on the picture). You can use them in any other algorithm. The red output layer is only needed during training. The weights leading from InputVector to HiddenLayer is what you ultimately care about 

I am pretty sure you have to: wait for enough data in the memory (100 entries, etc), then take out a minibatch (for example, 10 random elements), run backprop 10 times - once for each selected element. You then take an average of resulted gradients and finally, correct your network with that averaged gradient. After, put the elements back in the memory (where you've taken them from) or throw it out. Then keep playing the game, adding say, another 30 examples before doing another weights-correction session using a batch of 10 elements I refer to the "Session" meaning a sequence of backprops, where the result is the average gradient used to finally correct the network. 

if Word2Vec is nothing but a transformation of one-hot into a dense vector, why can't I just feed one-hot into LSTM (or for that matter sacrifice first dense layer, in any network that will end up using the embedding) and call it a day? Why would I actually spend time pre-computing Word2Vec embeddings? Yes, the resulting embedings are vectors that are clustered together if the words have similar meaning. But I presume a feed-forward classifier would figure out a good internal model anyway? 

In c++ we have [source] so there is no need to approximate anything. ...If we maintain our table sorted in a descending order. 

One of the issues of backpropagation is the concept of exploding and vanishing gradient. The former is mitigated by the use of the squashing functions, such as $sigmoid$ or $tanh$ in the layers, and the latter with correct weight initialization, for example with a Xavier init (this paper, page 523, or this post The two things to watch out for - if the weights are too large (0.1 on average per weight and we have 1000 neurons in a layer below), then the incoming value will be 100, and the activation functions will be saturated. In that case, the gradient will vanish at the activation function, when backpropagated. If the weights are very small (0.0000001 on average), then with our 1000 neurons, total input will be 0.0001, and any $tanh$ on such a neuron will have 0 input and a nice slope, so gradient flows nicely through them. However, the gradient will now get stuck in the weights, so it's a dragon with two heads. Techniques such as Resilient backpropagation (Rprop) don't require the magnitude of gradient's components - they only look at whether the partials keep staying the same or if they changed. If changed, it means the weight should be reverted as the local minimum was overshot, and the 'learning rate' for that specific weight should be reduced. Question