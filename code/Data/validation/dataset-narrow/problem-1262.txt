I'm not a native English speaker but this is what I do in single-authored papers. (Or rather, what I would hypothetically do if I wrote single-authored papers :) ) 1) Throughout most of a technical paper, I use "we" to refer to the joint effort of the author and reader. That is, my interpretation is that a sentence in the abstract such as "We show that C=D." talks about a technical result that is present or reported on by the paper. 2) In subjective parts, such as the acknowledgement section, or when talking about opinions or conjectures, then I would use the first person. So I think "I conjecture that this result can be extended to the X setting" is better than saying "we conjecture..." 

As far as I know (and can interpret your question) no such result is known. There are two reasons: 1) Generally unique games hardness results (as well as NP hardness result) do not yield "instance based" hardness. That is, the UG-hardness result have the following flavor - "if the unique games conjecture is true then for problem $P$ no algorithm can get a better approximation ratio than $\alpha$, where $\alpha$ is the ratio that the canonical SDP achieves on $P$" 2) While the theta function is an appealing algorithm, and yields a $\sqrt{n}$ approximation for independent set for random graphs, in the worst case its approximation ratio can be close to $n$ (am not sure what's the extreme example), which can of course be achieved trivially. In fact, Hastad has gave an NP-hardness result ruling out an $n^{1-\epsilon}$ approximation for independent set for every $\epsilon>0$. So, even without the unique games conjecture, we know that essentially there is no non-trivial worst case approximation algorithm for this problem. It would be very nice if there was a natural class of graphs where the theta function yields, say, an $O(\sqrt{n})$ approximation, and it is NP or UG hard to do better. I don't know of any such result. 

Much of extractor literature is about minimizing the seed length, which is important for derandomization application. However, it may not be crucial for yours. Also, often the literature focuses on relatively large error (e.g., 1/100), which is fine for derandomization but may be problematic in other settings, that require an exponentially small error. In your setting, it may be OK to generate once and for all a long random seed (say by tossing coins), and then use it to extract. In this case you could use pairwise independent hash functions that have rather efficient implementations. I wrote a paper with Shaltiel and Tromer on this issue. You may also be able to use almost independent hash functions, that can be more efficient and have smaller seed. (Don't know offhand a good reference for their efficient implementation, though there have been several works on this.) If you have multiple sources that are independent, then you can do better things. The classical Hadamard extractor works if the entropy rate is larger than 50% (this should be mentioned in the surveys above). If the entropy is smaller than 50% then we had one simple construction with Impagliazzo and Wigderson . The dependence between of number of sources and error achieved on the entropy rate is not ideal, though to really understand it you'll need to look at the exact bounds given by today's state of the art sum product theorems. (And if you're willing to assume certain number theoretic conjectures you can get even more efficient extractors.) This construction has been greatly improved in various ways, some of which could be relevant to your application. A great source for these is Anup Rao's Thesis. 

Additive combinatorics / number theory was used a lot in extractor literature. I think the first instances come from noticing that Paley graphs could be used as good extractors, and some open questions in additive number theory would imply better ones. Earliest reference I know is Zuckerman 1990 (see his thesis), but in last few years this has been an active area with interesting back and forth between TCS and additive combinatorics. (One of the highlights being Dvir's proof of the finite field Kakeya conjecture, but this is of course a TCS contribution to math and not the other way around.) A-priori it's not clear why these kind of mathematical questions, on sums and products of sets, would be important for CS. 

I don't know an answer, and am guessing it's an open question. There are very few known examples of such "surprising simulations" similar to Yao/Beigel-Tarui and Barrington. One thing along these lines that springs to mind is Valiant's result that for every $f:{0,1}^n\rightarrow{0,1}^n$ that can be computed by $O(\log n)$-depth $O(n)$-size circuit, there exists $g$ in $NC_0[n^{\epsilon}]$ that agrees with $f$ on $2^{n-o(n)}$ inputs. (And if the circuit for $f$ uses only linear operations than so does the circuit for $g$, leading to the lower bounds / matrix rigidity connection). But unlike $NC^1$ this is about multiple-output functions, and also only for linear sized circuits. Note also that a non-trivial reduction to depth 4 is known for arithmetic circuits. 

For constraint satisfaction problems, the property of having no better approximation algorithm than random assignment is known as approximation resistance. This is has been studied by several works in the last few years, with some results based on $P\neq NP$ and other more general results based on the unique games conjecture. A good source for this is Per Austrin's thesis. 

As David said, we do not have such reductions for AES. However, this does not mean that Rabin or RSA cryptosystem are more secure than AES. In fact, I'd trust the (at least one-way, probably also pseudorandomess as well) security of block ciphers such as AES/DES etc.. (perhaps with a bit more rounds than standardly used) more than the assumption that factoring is hard, precisely because there is no algebraic structure and so it's harder to imagine that there will be some kind of breakthrough algorithm. One can construct block ciphers directly from one-way functions, which is a minimal assumption for much of crpyotgraphy, but the resulting construction will be terribly inefficient and hence not used. 

Am guessing it may be easy to show that many exact problems (and perhaps strong approximation problems as well) are NP-hard on expanders. The idea is that if you take an arbitrary constant degree graph $G$ on $n$ vertices, and add another expander $H$ on $n$ disjoint vertices, and put a matching between $G$ and $H$, then you get an expander. The reason being that any set of less than half the vertices, will have either a constant fraction of the matching edges outside it, or its intersection with $H$ will have at most say $0.51$ fraction of $H$'s vertices. Since you can choose $H$ arbitrarily (say take a random graph) you can know the optimal solution for your NP problem in $H$, and hence there may be hope (depending on the problem), that given a solution for the combined graph you can get at least an approximate solution for $G$. But I didn't verify this for any concrete problem. Of course, as mentioned above, there are natural problems (most notably unique games) where one cannot do such tricks and in particular algorithms are known for expanders and not known in the general case. One should also be able to come up with some contrived example of a problem that's NP hard in general but easy on expanders (e.g., take some arbitrary NP hard problem on graphs, and modify it so that all instances with spectral gap more than $1/\log n$ are YES...). 

I think both are NPI under the stronger assumption (but obviously true) that NP is not in "infinitely often P" - i.e., every polynomial time algorithm A and every sufficiently large n, A fails to solve SAT on inputs of length n. In this case, such languages are not in P, but they also cannot be NP complete, since otherwise a reduction from SAT to a language L with large holes will give an algorithm for SAT that succeeds on these holes. Such an assumption is also necessary, since otherwise the languages can be in P, or NP-complete, depending on where the "easy input lengths" are located. 

Randomwalker's answer is very good. My favorite gap between theory and practice is the random oracle model. This seems a very secure heuristic in practice (assuming people don't do something stupid and at least do length extension propertly, see also randomwalker's answer) but we don't have any positive theoretical results about it. In fact, all theoretical results about this heuristic are negative. I think this is a great research question and hope some day some interesting positive results about this model will be proved. Regarding obfuscation as far as I know even in practice, although it's in widespread use, obfuscation is not considered as secure as encryption, and it's not considered prudent to use obfuscation to hide a long term and very sensitive secret. (As opposed to encryption using random oracle, which people are completely comfortable with using for this.) So in that sense, the gap there between theory and practice is not as large. (i.e., obfuscation is hugely interesting area which we are far from understanding both in theory and in practice.) 

Yes, the question of whether coNP has an interactive proof where the prover is weaker than #P (say, polytime with access to NP oracle) is a well known open question. The following recent paper of Haitner, Mahmoody and Xiao discusses this question and shows some consequences of the assumption that this cannot be done.