My raytracer supports a wide variety of objects. To intersect them, I use the standard technique of transforming rays into object-space. This works fantastically until I add motion blur. I model motion blur as a sequence of transforms (to simplify discussion, let's say exactly two) instead of one. My approach is to take the inverse transform of the ray at both keyframes and lerp the positions/directions. This seems to work fine for translations, but it breaks down for rotations. E.g. here are two triangles undergoing 30 and 90 degree rotations: 

Every orientation can be represented as a quaternion Quaternions represent a single rotation Multiplication of quaternions produces another quaternion (closure), and is equivalent to composing the rotations. Therefore any number of rotations can be represented as a single rotation! 

(4 samples, MN reconstruction, the red samples came from near the two keyframes) At the corners, I would expect the lerped samples to lie on a straight line between the the two vertices. Instead, they bulge outward. This is wrong. In more interesting scenes with more interesting transformations, it causes a variety of failure modes. E.g. here's a propeller undergoing a 45 rotation: 

Interpretation 2: Attempt to simulate the end data the human eye might receive for visualization purposes or compensation for LDR displays. This one has a less useful meaning, I think. Essentially, you're trying to produce an image that tweaks the way the brain perceives it for fun/profit. For example, there was a paper at SIGGRAPH this year where they simulated afterimages and color reduction to make images appear perceptually different. Of course, the only reason they do this at all is because the displays we're working with are all low-dynamic range (LDR). The point is to simulate the effects someone might see if exposed to a real high-dynamic range (HDR) display as actual image data. In practice, this turns out to not work very well. For afterimages, for example, we see afterimages because of a very bright stimulus exhausting color cells. If you instead try to stimulate the effect with a fake afterimage, it might look kindof similar--but since it's a completely different mechanism, it's not very convincing. This sort of graphics is actually underexplored in the literature if you want to make a go at it. The mentioned paper is an example of more-or-less the most state-of-the-art approaches we have. I think the current consensus, though, is that it is not really worth trying to simulate (at least at this time), since at best you'd only be approximating real vision effects by substituting different ones, and that this doesn't really work. 

Essentially, this is saying that the at a particular lerps between the texture at that and the material color. 

I don't need the fastest-possible algorithm. The easiest reasonable algorithm is what I'm looking for. Can someone help me find an ray + 3D-cubic-Bézier intersection algorithm satisfying the above? A paper link, a description, help pinning down references, etc. 

(100 samples, normals visualized) Some problems are due to the BVH breaking (it assumes the extrema of objects lie at keyframes), but even a brute force render is incorrect. I can fix all this by doing forward transforms only (transform object, not the ray), but this only works for objects where that is possible (only triangles, really). 

I've put together an !ASCII-art diagram of what I think I understand for the example of a mipmapped cubemap array texture containing two cubemaps (from my site, reproduced poorly here): Basically, I'd like corrections and additions to the above diagram, but if phrased as my key questions: 

I'm generalizing my texturing code, and have become confused by the frankly contradictory information about textures' pieces. Specifically, I'm looking to understand what exactly the terms "face", "layer", "layer-face", and "mipmap chain" refer to. 

The answer depends on what you mean. Modern hardware (e.g. with bindless textures) really doesn't care too much how many textures are "bound". The real question is how many you use. Textures generally store data in a cache-friendly way (a Morton curve, I believe). If you use more textures, you'll get more cache misses, since now the textures compete with each other for space. This really just comes down to the well-known, old shader programming heuristic: texture taps are slow; don't use too many. 

I would like to point out that bringing quaternions in wasn't just random math-ese. In contrast to the other answers, the favored approach in graphics is actually to represent rotations as quaternions, since they take up less space and are faster to combine. There are easily-Googleable ways to convert between rotation matrices and quaternions, depending on which you prefer. The point is that rotations are the quaternions in a mathematical sense, so combinations thereof are also single rotations. 

In font rendering, it is common to use subpixel rendering. The basic idea here is to break the pixel into its RGB components and then compute a value for each separately. Since each component is smaller than the entire pixel, higher quality antialiasing is possible. There is an obviously analogous way to do the same for a ray tracer. You do reconstruction filtering on each subchannel separately. I was surprised that, however, I couldn't really find reference to ray tracers doing this. Especially if you're already doing spectral rendering, this seems like an obvious thing to do. There's this paper from a journal I've never heard of which seems to be related. But overall, subpixel rendering just doesn't seem to be a common thing to do. My question: why not? 

I'm trying to get an algorithm for intersecting a "tube" (a 3D cubic Bézier curve extruded by a possibly-changing radius) with the following required properties: 

You can usually track down the answer for such questions in the OpenGL specification. Assuming the value you care about is consistent across OpenGL versions (often the case for initial values, seldom the case for minimum values). In this case, in the GL 4.5 specification, check out the following: 

The best analogue I can think of would be doing some sort of 3D texture lookup anisotropically along the viewing ray. This isn't quite a direct analogy. Anyway, the problem with this is that when you're doing volume rendering, you actually care about how this integration is done a lot more than in the 2D case. Do you want absorption? Emission? (You need to do some kind of exponential to integrate.) Do you want scattering? (You need to do some kind of recursion or approximating blur to integrate). And this doesn't even get into what kind of data the 3D texture stores. Does this texture mean opacity, or does it mean density? Is it diffuse or is it emission? All of these need to be integrated differently. 

1Rod+3*cones, the usual case. Approximate because humans may have as few as zero functional light receptors up to a conjectured maximum of seven (with the highest ever observed being five). 

Yes; you can just pass it as-is, but (looking at the docs) it is not supported directly. You'll need a custom shader to interpret it. Depending on your needs, expanding the pixels first (or even doing the full conversion CPU-side) may be worthwhile. At a guess, I'd say that expanding the pixels (as you copy from the camera's driver buffer into a staging buffer for upload to the GPU) is a pretty consistently good idea, unless you're extremely bandwidth-saturated. I'd add the color-space conversion for easier debugging, or if the image doesn't update frequently. 

I am not aware of hardware support for 3D anisotropic filtering. I could be wrong about its existence though. I believe it has been tried. The motivation for 2D anisotropic filtering is to prefilter the texture function over the screen area it falls under in a more accurate way than doing a box filter in texture space (usually, anisotropic filtering does several box filters in texture space). This definition is key. 2D anisotropic filtering tries to: 

How can I make my raytracer produce linear approximations to transformation (especially rotation) by transforming rays, not objects? 

Think about that. Starting from object space, you can rotate your object into any orientation using only a single rotation. 

New developments in volume rendering are happening all the time, and in any case there are so many variously suboptimal ways you could try to generalize 2D anisotropic filtering, the doubt expressed by the specification makes sense to me: 

Render your image using correct radiometric calculations. You trace individual wavelengths of light or buckets of wavelengths. Whatever. In the end, you have an image that has a representation of the spectrum received at every point. At each pixel, you take the spectrum you rendered, and convert it to the CIE XYZ color space. This works out to be integrating the product of the spectrum with the standard observer functions (see CIE XYZ definition). This produces three scalar values, which are the CIE XYZ colors. Use a matrix transform to convert this to linear RGB, and then from there use a linear/power transform to convert linear RGB to sRGB. Convert from floating point to uint8 and save, clamping values out of range (your monitor can't represent them). Send the uint8 pixels to the framebuffer. The display takes the sRGB colors, does the inverse transform to produce three primaries of particular intensities. Each scales the output of whatever picture element it is responsible for. The picture elements light up, producing a spectrum. This spectrum will be (hopefully) a metamer for the original spectrum you rendered. You perceive the spectrum as you would have perceived the rendered spectrum.