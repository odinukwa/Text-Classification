Here I would establish something that I call context state. Your functions and daemon run in a certain context that is established. 

You have data sets within code. Code should ideally be independent from the data. We can see that target_meter, update_timestamp_col, data_lifespan, move_back_amount, secs_in_res all correlate. I would extract the data from the functions: 

This way changes of the data do not change the executing code, also the data is actually grouped together - you have a chance to spot erroneous data just by pattern recognition (or rather that something disturbs the pattern). Then we see that the data also correlates with the input parameter. We can include this, too: 

a) There is one conceptional behavior I personally don't like: The code will leave a partially written file behind in case of an error, instead of removing it. b) 

With this change it's a breeze to adapt data and also to add new resolutions without accidently messing up code that worked before. 

Regarding the updated code: I don't like the use of the term in the functions. You have this prototype: 

The context is passed (as pointer) to each function so it knows the circumstances it operates under. This also groups relevant data together. The second context you operate under is a specific meter. 

For N = 109 you have to store about 7.4 GB data, if you merely use longs with a size of 4 bytes. This is not feasible, which, I assume, is the whole point of the problem. You have 200000 Qs - storing the Qs instead takes up only 11 bytes per Q - one byte to determine the operation, 8 bytes for the two values which can go up to 109and 2 bytes for c, which fits into 16 bits, as it's not higher than 10000. If you store Q, you end up with around 2 MB of data. You could apply the Q operations at runtime to a single long (long) and then print each value individually. This algorithm will be memory-efficient, but very very slow. Looking at the problem, we will have a lot of array members having the same value. There are only 200000 operations but 1000000000 array entries - we can change 200000 different values - if we do this, we still have (109 - 200000) array entries with the very same value. Even if we modify 200000 ranges, this doesn't change the fact, merely the distribution of distinct values. So it's much more efficient to store a value and then for which array range this value is valid. In example: 

The first question I would have is: What head are you talking about, there is and ? Does the function modify a global variable called head? Not even your unit test creates a variable called head as head of the list ;) Your data is a void pointer and then you use , that's not legal, because the size of a void pointer can be different from the size of an int. It's already strange that you don't store a pointer to an int in data, but the int itself. Is this really what you want? Didn't you want to store the pointer to the int? I assume you actually wanted: 

You added the cast likely because the compiler complained. But the problem was not the missing cast, but that you converted an integer to a pointer, instead of an integer pointer to a void pointer, which can be done without cast. When you store a correct pointer, you can then display it correctly with: 

This is a mixture of storing the value and storing the operation. When you need to print a value, you look up the range and print the value. As you see, we need less than 64 bytes for this simple example to store the information how the 7.4 GB data is compromised of, yet we can print the value for each and every array position. Even if we have reduced the data, we still need to optimize the remaining data. We can't just dump it into a linked list - as stated, we can end up with Q+1 entries (200001), we can't afford to search the entries with an effort of O(n), let alone (r-l) * O(n). But if we put the data into a binary search tree, we can find the correct array entry in 18 steps within the 200000 entries. This costs us some more memory, not only due to the overhead, but as we need the array index as key, we can't group all identical values together, just as long as they are one range. So the example above will need 4 nodes in the binary search tree, to separate the value 0 twice. So, when you combine these two approaches, you should be able to get a decent memory and processor time consumption. 

Documentation syntax For each subroutine in the code, I created a short documentation following this syntax: 

This is highly debatable. You should use the pre-increment operator, rather than the post-increment operator. For example, this 

Can be simplified if you store the buttons in an an array, and then loop through the array as you add them to . This looks just like the code I wrote in the last recommendation: 

This is a little nit-picky. In the signature of , since you aren't looking for any command like arguments, you should put in the arguments space. 

Element retrieval Versatility Your is good; it allows for easy element finding. However, it isn't very flexible and would not be very useful in a larger application. What if there is an element that can only be selected with ? This function would not work for this as it can only one type of selector and one selector at a time. Now, to make this function more versatile, there are one of two things you can do: 

Two ones or twelve? This is kind of a continuation from the top one. Let's say your compressor went to go compress this file: 

(if you follow the above tip, these variables will probably be in an array instead). Now, you are doing much less constant field access, overall speeding up your code. 

This same idea can be used for the rest of your code, even in spots where, instead of the parameter changing, the object that is being acted upon is changing. And, if you come to a spot where the changing parameter/object name is not only changing by a single number, you can consider using an array to hold the different parameters/object names. Then, you can just easily iterate over the array and call the method with each element of the array. 

is going to give you the same value every time. Why keep on doing the same string operation when you could save the output in a variable and just use that? 

This should be up at the top of your class along with the other fields. Other than that, it is a little confusing why that is residing so far away from your other fields. 

Change.. all the objects! No, don't do that. Downgoat already explains why. This will touch every object. I shouldn't be able to do this: 

Adriano Repetti already did a wonderful job reviewing your generated assembly, so I'm mainly going to touch into the C a little bit. 

Add some documentation! Especially with complicated math operations, you want to add documentation to each of your functions/methods so that, as someone is reading your function, if they have any questions, they can just check the documentation. In your documentation, you should give a summary of what the function does (maybe include some mathematical formula in there if the function is using one), and should tell the type and purpose of each argument being passed into the function. Also, you should include the type of the return value, and what the return value means. And, if there are any error return values, you should write in the documentation what each return value means. Here is an example: 

See something similar about the cases 1 through 6? They do the same exact thing. The only different one is case 0. To make this much more efficient and clean, just simple replace this switch statement with an if statement checking that the random number is 0. That would look like this: 

I have created a virtual piano that turns the home row (excluding 'g' and 'h') and the top row (excluding 'y' and 'u') of the user's keyboard in a piano's keyboard. The home row is used for white keys and the top row is used for black keys. The keyboard's keys bind to piano's as shown below: 

When the end of a line is reached, then this is called. However, since the file you are reading is from and also reads from , then this call is going to remove the next character from the file. See the input again, but with the characters removed by : 

You aren't even using half of these. I recommend that you all of the elements that you do not need; it's adding serious unreadable noise to your code. 

I may be missing something about this question because my solution seems almost too simple, so I apologize if that is the case ahead of time. 

There are still some problems with this directive. The biggest one? Scope. Right now, there would be some major problems if you were to try and use this directive because, since there are going to be multiple elements with this directive, there would be repeats and what-not in the directives' content because they are all using the same scope. To fix this, we can add a property to the object that our directive is returning. This property allows the directive to define attributes that it will read and then pass on to it's template in it's own scope. This makes sure there is no overlap and repeat when using multiple elements with this directive. Adding the property would look like this: 

There are a number of articles describing why a is quite a bad idea when defined on top of ORM framework. Main reasons are leaked abstractions and abstraction on top of abstraction. And your example shows why - any optimization requires the knowledge of underlying implementation (Entity Framework). Running this process in multiple threads may help, but will put additional load on the database server and thus is not scalable. Note that Entity Framework's is not thread-safe, so you cannot share the in multiple threads unless you spin up a new per request there (which I wouldn't recommend doing). Assuming that you're using Entity Framework 6.0, method returns , you can try asynchronous implementation of the data retrieval: 

When using one of the ORMs mentioned above you will have a notion of "session" (NHibernate) or context (Entity Framework). Both terms refer to the same concept of unit-of-work + repository. These objects are very cheap to create, so usually you create one (sometimes more) session/context per form, and don't share them between forms. 

Here we release the UI thread and running a (computing) task in parallel. The reason I've added "computing" is that you're manually spanning a new task using . In case of I/O-related task returned by .NET framework it won't actually represent a new thread as it can wait for external data on the same thread. 

As a result, I would try to have a single structure/class that provides required functionality. If you want to be able to reference the functionality without explicit reference to , consider switching from to and define an interface for functionality. 

Note that you're using a separate thread (on a threadpool) each time you write something to log, so there will be contention between multiple threads if you write to log quickly enough. I would recommend using existing logging frameworks like NLog or log4net to avoid inventing a wheel. 

It's hard to suggest good solution based on the information you've provided... Will the number of entities change over time? Can they "loose" SpriteComponent? Assuming that the number of entities is the same and only Z will slightly change you can cache the sorted list from previous run, and apply TimSort or Insertion sort (you would need to implement it yourselves or grab from Wikipedia) to nearly-sorted list. 

This kind of condition may actually hurt performance quite a lot, because query optimizer most likely will be confused by complex filtering condition and would have to do a table scan rather than index seek (assuming that you have index over and ). In order to improve querying performance you should simplify clause. Best solution would be to utilize client-side ORM capabilities to generate dynamic filtering based on available data. If that isn't possible, then I would go with one of the following approaches (I would prefer Multiple queries approach in case when select statement is simple): 

Other than that encryption/decryption looks properly implemented, and I completely agree with original code writer that initialization/duplicate steps should be taken to constructor. It will reduce the number of parameters in Encrypt/Decrypt methods to one - actual payload. Depending on your specifics you can also expose methods accepting and returning encrypting/decrypting streams for large encryption volumes if necessary. 

Cache the instance of and objects so that you don't need to create them each time you measure the length of string. I've created a small test to see which part takes most of the time: 

I've seen many attempts to abstract particular ORM from application, and all of them at a certain stage of maturity had to break this abstraction. One of the reasons for that is when you need to optimize certain use cases (like eager-loading related entities, or combining several round trips to server into one) you'll need to use ORM specifics that you're abstracting from. And another (more obvious) reason why I vote against abstracting ORMs from the code is that ORM is already an abstraction, so what you do is an abstraction on top of another abstraction that brings little benefits. There is a very good series of articles that describes best practices for managing NHibernate in ASP.NET: 

Let's start from your code. The recursion can be implemented in a much clearer way if the method returns the collection of all child folders, instead of updating parent folder's collection. In this case it could look as simple as 

Also I would suggest to use CancellationToken to stop asynchronous processing. UPDATE Based on comments it is really needed to wait for all workflows to be configured before starting them. So cancellable implementation can look like this: 

About method - if this a method is in Controller class, and taking previous bullets into consideration, the code would look like that: