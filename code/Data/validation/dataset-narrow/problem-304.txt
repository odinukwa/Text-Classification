As far as I know if you change recovery model of database during maintenance window time or when load is relatively less there wont be any problem. It wont create a situation. 

Please note that recovery model controls amount of logging in transaction log file and in case of disaster how much data you can recover. It has no relation to amount of inserts or updates or number of column in you table. You can keep simple recovery model if you do not require point in time recovery. You can take full and differential backup in simple recovery model. If you want to change recovery model to full then make sure you take regular transaction log backups after changing recovery model. Changing recovery model should depends on how much data loss you can afford or agreed between you and client basically the RPO and RTO. 

As per this message its clear that network connecting principal and mirror flipped I dont know for how much time but this caused the failover. What happens is Mirror server keeps sending pings to Principal whether it is alive or not and suppose Principal goes down and now mirror will send 10 pings to principal(By default) and if it does not gets any response it will initiate failover with help of Witness( its little more complex with witness ,it involves quorum) If it was for moment you can increase partner timeout( not a good practice though) but you should Immediately speak to network team and ask them RCA for this glitch as this caused failover. My course of action would be to meet them personally ask them to run bunch of commands to check response time 

Scenario 3 Not all crash dumps are because of bug in SQL Server many occur due to poorly configured SQL Server or some rouge queries running. But since you have not shared detailed errorlog it is difficult to say at this point. Make sure your SQL Server is configured correctly. Again if such is the case MS support will point this out. Moral: If SQL Server is not updated with latest SP first update it, look for the cumulative updates released after the SP(you can get that from first link I have shared) and make sure bug you are facing is not fixed in CU releases ONLY then open case with Microsoft. 

Yes SQl server installation is likely to fail if you are trying to install it on root of a drive which is never advisable and has security implications. Actually it is related to permission setting on specific folders. As per this connect item 

I believe it is possible to restore with same old answer that subject to condition you are not using any "enterprise specific" features. This you can get from 

Find out what is that which is running on system and requesting more memory. Perfmon counters Process: Private bytes and Working set can come handy here. 

Its very difficult to find SQL Server management studio which is not express version because Microsoft only ships complete version with installable of Licensed version. If you have setup of any SQL Server 2012/2014 licensed version you can run the setup and install SSMS by selecting client tools. There is no addictional license for SSMS. Other good point is you can use SQL Server 2012 express Management studio with SP1/SP2 to get all functionalities of SSMS which is shipped with licensed version. So you dont need to worry about simple SSMS an express SSMS2012 with SP1/SP2 would do all jobs for you. I would suggest you to install SQL Server 2012 SP2 management studio using This Link. The download would be SQLManagementStudio_x64_ENU.exe assuming you have 64 bit machine. You can manage previous versions(till 2005) as well as SQL Server 2014 using SSMS 2012 express management studio 

To solve this Either start SQL Server service account with Local System or Create a new local account having admin privileges on the local machine and try starting SQL Server service with that account. This Blogs.msdn article says such error can come if account profile is corrupt As you are already aware running SQL Server service with account having admin privileges is not advised. In such case for time being you can run the account with local system but you can use Configure Windows Service Account and Permissions to create account with minimum privileges. EDIT: 

Yes this is doable. For your scenario if you want to remove the database from availability replica you must do that from secondary replica. So you have to login into secondary replica from which you need to remove database. Expand availability database, right click on it and select . You can choose multiple databases to remove at one go. More information in BOL document If you want to do using TSQL which I prefer just login into secondary replica from which you want to remove the database and run 

I don't think there is any issue because of this, this is totally normal looking at this you cannot draw any conclusion. 

command and share output on some shared link(onedrive,dropbox...) and post link into your question so that we can see how much memory SQL server is using. Edit: 

No, recovery model affects the way things are logged and the recovery it does not means that there would be no logging. Every operation in SQL Server is logged in some way or the other and recovery model defines way logging will happen. In simple recovery model which is almost same as Full logs are truncated when transaction commits or when checkpoint is given this happens automatically when transaction commits or also when Log file reaches 70 % of its size. The only thing that would stop log truncation is if some long running transaction still requires that portion of log. While in full recovery you have to take transaction log backup to truncate the logs Hope this is clear.Please let m know if you don't understand few points 

For statistic,s if you are running query on secondary replica which is different from primary replica and the query on secondary requires column statistics which is not there on primary you have option to create temporary statistics. Please read 

If you are upgrading from enterprise evaluation edition to enterprise you do not have to worry and inplace upgarde would likely to be smooth. If you are upgrading from from enterprise evaluation edition to standard edition problem can arise. You can use evaluation edition for 180 days and if you have used any enterprise feature in 2014 evaluation edition, but upgrading to standard edition, your upgrade might fails( this was bug in 2008 but still can be noted in 2012) This link has similar information. Even if successful you would loose all enterprise features so you need to plan accordingly. 

The column is memory utilized by SQL Server database engine in MB. You can put this query in stored procedure which would run at defined interval and give the output to you Edit: The script you wrote in question is already giving you information about memory utilized by various databases. What else do you need it seems correct query to me. Again IMO there is no point in taking action based on how much memory each database is utilizing the value is might change continuously and you cannot draw any inference regarding performance after looking at database utilizing more memory. This happens because you run a query which requested a large page read and that pages happened to be from particular database so its not any issue its what memory is designed for to cache as much possible to avoid any physical reads. Again its better monitoring memory at instance level than database level. I would also say if you have set max server memory correctly considering every aspect in picture you should not worry much about memory. You would be going in wrong direction if you are going for threshold. Why do you want to set threshold if certain database or object utilities memory. Its totally normal for database to use memory as set by max server memory limit. Again no body can tell you threshold value. You should focus on monitoring costly queries. Its highly unlikely memory spike will occur because when SQL Server utilities memory it only releases it when SQLOS asks it to do so. So even if you set a threshold your alerts wont stop because SQL server is not going to release memory under normal condition Please let me know if you require further information 

There is other method involving mathematical calculations and this would give accurate results. As already pointed backups would be best to refer to data growth since you said you need to calculate and predict size of database below Microsoft links would help you Estimate Size of Database Estimate Size of Clustered Index Estimate size of heap Estimate size of table 

Yes you can take compressed backup of 80 compatibility database hosted on SQL Server 2008 R2 subject to condition SQL Server edition is Standard or higher edition 

Yes you can. You can run alter command on primary database and at same time a select query can run on secondary database accessing same table this is perfectly fine and allowed. But when the changes you made on primary are replayed on secondary then the users might be disconnected and unless restore operation completes you wont be able to run query. This is subject to condition that you have selected check box 

More details can be taken from This Link and This Link To read more about SQL Server profiler refer to this Microsoft Online Doc 

Located HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Microsoft SQL Server in registry Right click and go to Permission Click on Advance Tick on both check box (I. Inherit from parent the permission... II. Replace permission entries on all child objects...), click OK Click OK again 

Is there any trace flag enabled or any tweaking into startup parameters of SQL server can you please check that.Have a look at below Microsoft support article $URL$ Keep max lock value to default EXEC sp_configure 'locks', 0 ( I guess you tried this option already) Now if you face this issue again best possible workaround will be to decrease the amount of rows being transferred. Every lock has memory associated with it so if there is lock contention I believe its somehow related to SQL Server buffer pool memory. Did you run DBCC MEMORYSTATUS and check value for OBJECTSTORE_LOCK_MANAGER (Total) KB please do tell us what is value of single page and multi page allocator for above clerk 

There are other methods as well. Perhaps you can use two DMV's. Please note that both will only work for SQL Server 2008 and above A non zero value for locked pages allocation KB would tell that SQL Server account has Locked pages in memory privilege 

Simply because it does not have any idea about the status of transaction, like whether it will commit or rollback so it does its job of backing up enough transaction logs so that if transaction commits it can bring database to consistent state after backup is restored and if transaction does not commits before backup is finished the backups takes it as uncommitted and when it will be restored their would be no information about changed made by that transaction in restored database. Read The First Myth pointed out by Paul Randal 

Reason I say I don't see any logic is because SQL Server memory code is designed to cache as much objects and data as possible to avoid any physical I/O. This actually helps in SQL Server performing better. If you set max server memory to 450 G SQL Server can immediately use all that memory, any why not you have asked it to use it. Also note that there are certain components, memory to which, are allocated by buffer pool but if allocated that will also be counted under SQL Server memory utilization. 

Additional Information If you face issue while uninstallation where by normal method you cannot uninstall SQL Server or uninstallation is giving error you can use below method to remove registry keys related to SQL Server. ONLY USE THIS MESSAGE AS LAST OPTION WHEN YOU HAVE MESSED UP WITH UNINSTALLATION. DELETING REGISTRY KEYS MIGHT CAUSE INCONSISTENCY. Use this on your own risk I have personally used this many times and works well DONT USE IF YOU HAVE MULTIPLE INSATNCE. If you have messed up things and dont want to use below method you can raise a case with Microsoft 

Prior to SQL 2012, the buffer pool both “managed” memory and was a consumer of memory for database pages. It’s management of memory meant it allocated 8Kb pages of memory for other consumers like plan cache. Roll forward to 2012+, the buffer pool is a pure consumer of memory from SQLOS which manages all of the memory. 

There is not much to check if you have found out that SQL Server requires more memory.Is your OS on physical machine or Virtual if it is virtual adding memory would be easy. You must know that SQL Server 2012 standrd edition supports maximum of 64G memory. After adding new memory you must change max server memory setting using sp_configure. Set max server memory to appropriate value. Leave enough memory for OS and SSIS, SSAS,SSRS if you are using these features.