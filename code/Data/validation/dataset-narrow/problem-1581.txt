There is this very cool idea in a paper by Ryabko[1] which is not yet very well known. This is called the telescope distance $D_H$. To make a valid assessment about two time series, just looking at the data is not enough. You need to compare the underlying stochastic process that generates them, i.e. you want to compare two probability distributions. And the telescope distance is precisely a metric on the space of probability distributions. It goes like this (kind of formidable to the uninitiated). For a set of functions $\mathbf{H} = (\mathcal{H_1, H_2, \ldots})$, the telescope distance is defined as $$D_{\mathbf{H}}(P, Q) \equiv \sum_{k=1}^{\infty} w_k \sup_{h \in \mathcal{H_k}} | E_P [h(X_1,\ldots,X_k)] - E_Q [h(Y_1, \ldots,Y_k)]|$$ where $P,Q$ are the distribution that generates $X$ and $Y$ respectively, and $w_k$ are some exponentially decaying weights (see the paper for details). You don't know $P$ and $Q$; all you got are the time series. It turns out you could use the following empirical quantity $\hat{D}$ to estimate the true telescope distance, $$ \small \hat{D}_{\mathbf{H}}(X_{1:n}, Y_{1:m}) \equiv \sum_{k=1}^{min(m,n)} w_k \sup_{h \in \mathcal{H_k}} \big|\frac{1}{n-k+1} \sum_{i=1}^{n-k+1} h(X_{i:i+k-1})- \frac{1}{m-k+1} \sum_{i=1}^{m-k+1} h(Y_{i:i+k-1}) \big| $$ where $X_{1:n}$ and $Y_{1:m}$ are your observed time series. Notice the nice thing about this metric is, the two time series don't have to be equal in length. Now, everything seems to be fine except that you notice you need the $h(\ldots)$? And it's not just one function, but a sequence of functions. The cool idea is that these $h(\ldots)$s could be modeled as a binary-classifier well known in machine learning. For example, one could use SVM for these $h$ to discriminate between a subsequence $X$ and a subsequence of $Y$. Once you've trained these binary classifiers, and there are $min(n,m)$ of them, you run them through the subsequences of the same length of $X$ and $Y$, sum them up and you're done. [1]Ryabko, D., & Mary, J. (2013). A binary-classification-based metric between time-series distributions and its use in statistical and learning problems. The Journal of Machine Learning Research, 14(1), 2837-2856. 

web_A= (1*(.037))/ (1*.041 + 1*(.0352) + 2*.047+ 1*.037 + 1*.081) web_B= (1*(.081))/ (1*.041 + 1*(.0352) + 2*.047+ 1*.037 + 1*.081) All other variables would have zero contribution for the purchase. A could make this a binary outcome, “purchased” or “not purchased” and use logistic regression, but I thought this approach would give a more granular view. Is this a sound approach? 

meaning that they have now been served an ad on desktop and mobile, and seen digital_ctr_B twice, once on web_A and once on web_B. Again, count all the purchases they make before they receive their next ad exposure. Since this is sensitive the time between exposures I thought about modeling the problem as the average purchases per day between exposures. For example if person p_1 made a purchase on oct 2 and oct 3, then we would have y= 2(purcahses)/4(days)=.5 Could I get the coefficients for each variable and then for each purchase divide each unique coefficient-variable product by the sum of the coefficient-variable product for that exposure. Suppose the coefficients for the variables are 

The key advantage of using minibatch as opposed to the full dataset goes back to the fundamental idea of stochastic gradient descent1. In batch gradient descent, you compute the gradient over the entire dataset, averaging over potentially a vast amount of information. It takes lots of memory to do that. But the real handicap is the batch gradient trajectory land you in a bad spot (saddle point). In pure SGD, on the other hand, you update your parameters by adding (minus sign) the gradient computed on a single instance of the dataset. Since it's based on one random data point, it's very noisy and may go off in a direction far from the batch gradient. However, the noisiness is exactly what you want in non-convex optimization, because it helps you escape from saddle points or local minima(Theorem 6 in [2]). The disadvantage is it's terribly inefficient and you need to loop over the entire dataset many times to find a good solution. The minibatch methodology is a compromise that injects enough noise to each gradient update, while achieving a relative speedy convergence. 1 Bottou, L. (2010). Large-scale machine learning with stochastic gradient descent. In Proceedings of COMPSTAT'2010 (pp. 177-186). Physica-Verlag HD. [2] Ge, R., Huang, F., Jin, C., & Yuan, Y. (2015, June). Escaping From Saddle Points-Online Stochastic Gradient for Tensor Decomposition. In COLT (pp. 797-842). EDIT : I just saw this comment on Yann LeCun's facebook, which gives a fresh perspective on this question (sorry don't know how to link to fb.) 

I’m working on a problem that is analyzing purchases at a store and determining the effect from advertising. There are many advertising variables, but to keep things simple, assume there are three types of media: 

All other variables would have zero contribution for this purchase. Suppose there was 1 purchase for vector_2. I would get contributions of 

Each time an ad is served to a given person they may have any combination of the above. For Example, on October 1, person p _1 may be servered with an ad with the following attributes: 

so person p_1 was served an ad on oct 5 on mobile, with digital_ctr_B on web_A. There is a cumulative effect to advertising, so this exposure takes the form of 

Before you do that, you may want to check for outliers. Say 99% of the data lie in range (-5, 5), but one little guy takes a value of 25.0. Your normalized array would cluster around (0, 0.3), and that would cause problem for the neural net to learn. 

He cited this paper which has just been posted on arXiv few days ago (Apr 2018), which is worth reading, Dominic Masters, Carlo Luschi, Revisiting Small Batch Training for Deep Neural Networks, arXiv:1804.07612v1 From the abstract, 

Typically, folks would transform the variable. When it is strictly greater than zero, a log transform is usually sufficient. If zero is included, as in your case, one popular alternative is the box-cox transformation. 

To receive requests with csv file, I would use flask microweb framework. I would run flask with gunicorn and gevent. For every csv file received, I would parse the rows based on varied columns and store them in a mongodb. I would write queries on the collection in mongodb to extract information. 

There are multiple ways to extract NER. Primarily, based on the construction of statements NER was extracted with the use of POS tags. But overtime with the change of how information was being conveyed, there has been a migration from traditional methods to learning methods. Currently, take a look at sequence to sequence tagging for NER detection. If you have the appropriate dataset, you have the capability to extract anything you consider as NER. 

One technique is optical flow [1], [2], which is popular with people doing modeling of action videos. There is a github implementation of [3] here, which she calls ConvLSTM, and is coded in Lua. [1] P. Weinzaepfel, J. Revaud, Z. Harchaoui, and C. Schmid. Deepflow: Large displacement optical flow with deep matching. In Computer Vision (ICCV), 2013 IEEE International Conference on, pp. 1385–1392, Dec 2013. doi: 10.1109/ICCV.2013.175 [2] Philipp Fischer, Alexey Dosovitskiy, Eddy Ilg, Philip Hausser, Caner Hazirbas, Vladimir Golkov, Patrick van der Smagt, Daniel Cremers, and Thomas Brox. Flownet: Learning optical flow with convolutional networks. CoRR, abs/1504.06852, 2015 [3] Patraucean, V., Handa, A., & Cipolla, R. (2015). Spatio-temporal video autoencoder with differentiable memory. arXiv preprint arXiv:1511.06309. 

This equates to person_1 being served an add on desktop with digital_ctr_B on web_B. Count the number of purchases that this person makes before getting another ad ( they may only get one ad). If they do not purchase before they receive another ad they receive a 0. For example, 4 days later they receive another ad of the form 

I am building a model that is trying to predict the 5 year sales production of a salesperson in every zip code in the United States. I have only 7 years of data. This task is one piece of a larger model whose purpose is to determine the most optimal location (as determined by the zip code) to place a new salesperson. My firm has approximately 20,000 sales people located in 10,000 zip codes. These 20,000 salespeople have customers in 35,000 different zip codes. I have historic sales information and zip code demographics. My problem is I am trying to determine the “home zip code” to place a salesperson. Only 10,000 of the 43,000 zip codes are the “home zip code” to a salesperson. My company faces different constraints in every state, so our representation is not uniform, but we are in most states. I can build a machine learning model to predict the production of the salesperson in the 10,000 “home zips”, but how should I approach the other 33,000 zip codes. I’ve thought about building a similarity index of the zip codes based on demographics, sales history and regulatory constraints and building a unique model for each these groupings, but this seems like I’m adding a lot of uncertainty with the clustering/similarity grouping not to mention smaller data sets for learning. My data is based on an annual contracts, i.e., the customer signs a one-year agreement on an annual basis. Any ideas how to best approach this problem would be appreciated 

There is this prevailing perception that convolution neural net is the panacea to solve all image recognition problems, which understandably comes from the many success stories in the computer vision literature. However, keep in mind what a ConvNet does for you. It finds the translational invariant features, like edges. From what you described here, a ConvNet is unlikely to do much for you. A raindrop or a dirt spot is equally likely to fall on any location of the video frames. The problem you have is one of texture classification /discrimination, and there is a huge literature on that subject. A google scholar search finds this[1]. [1] Smith, J. R., & Chang, S. F. (1994, November). Transform features for texture classification and discrimination in large image databases. In Image Processing, 1994. Proceedings. ICIP-94., IEEE International Conference (Vol. 3, pp. 407-411). IEEE. 

Using random forest is appropriate. But as features to the random forest it would be better to use word vectors as input to the model. That would take into account products with same labels to have a very strong similarity score based on their names. 

I do not think you can achieve the result with just one single model. If you need more information, I could elaborate on it. But this should give you a starting point. Here is an explanation of the 2 above points: 

Train a classifier, with all the data points you have with labels as Target 1/ Target 2. For this you could use any family of classifier. But you need to be very careful in the evaluation. If this models performs poorly, you will have a problem, as your classification would affect your next model. You also need to check if the distribution between target 1 and target 2 are appropriate before using a model to classify them. Once the classifier is done, you can then use regression with all the input features + class of the entry ( target 1 or 2 ).