If it is not then you have to restart MySQL to make it happen. Oracle is working on getting more and more variable dynamic and you can see the trend going through 5.5 -> 5.6 -> 5.7. Unfortunately though there's always going to be parameters which require restart. 

Now, matching (same structure) is actually not mandatory if you only want to drop it but you need to have a valid frm file. If you do this you should be able to drop the table afterwards: 

(from $URL$ The most flexible and probably the best performing option for search is to have a feature table where you store all the possible features. And there's a table to store what property has what feature. The downside of this approach is space. Assuming your using InnoDB every row will have a 13 bytes overhead + the two columns (property_id, feature_id) ~ 6 bytes. So you can expect something around . (To compare the first option will need ) 

By having more tables you're certainly not going to increase the memory allocation for MySQL. Number of tables has no influence on memory usage. Discussing memory allocation is out of scope now but this may be useful to take a look: MySQL memory calculator. 

I have a SQL script that I run when I want to see what is happening in the database. The script has many queries that return information from DMVs, but the two I use most are "Requests" (sys.dm_exec_requests, sys.dm_exec_sessions, etc.) and "Locks" (sys.dm_tran_locks). The output is similar to SQL Server Activity Monitor, but it displays more information. Sometimes, a request appears in the Requests query, but it completes before the Locks query runs. For example, the Requests query may show SPID 51 is waiting on a lock resource, but the Locks query does not include any lock information for SPID 51. (I know about the and columns from .) Is there a way to ensure that these two separate queries display a coherent snapshot of database activity? I expect commercial database monitoring applications must encounter the same problem. I have experimented with running these queries in SERIALIZABLE concurrency and added locking hints to the DMV joins, but the queries acquire no locks. I would not want to do this in production anyway. The best ideas I have so far are: 

Be aware in this case you will get the results back in arbitrary order not necessary the order of ids produced by the subselect. 3) Run the select directly You can get the ids to a list from the database and use them to query from your model. 

tl;dr No. The binlogs contain a command before every transaction exactly to prevent this situation. This will only affect the session scope of the SQL thread so running queries meanwhile won't report date 600 seconds in the past. To take a look you can run: 

OOM is usually a result of bad my.cnf config and actually your my.cnf overcommits memory big time. Run mysqltuner.pl to get a sense of what parameters needs tweaking. Many of your configuration values are way high and some of them doesn't even make sense. For example: 1) binlog_cache_size 3Gb? default value is 32768 bytes(!) According to dev.mysql.org: 

That really depends on the size of your system and the level of automation you're using. What you can do with log_bin and log_slave_updates: 1. Quick failover It's not hard job to find the most up-to-date slave promote and determine the position on the other slaves and point them to the new promoted master. There are tools to help your there. Since MySQL 5.6 and the different flavours of it provides different implementation of GTID which makes it even less painful. Without binlogs being enabled a restart is required which (again depending on your setup) can take significant time (hours even). 2. Incremental backups and point in time recovery You can take a base backup of your slave with mysqldump, percona xtrabackup or any tool you prefer and make sure you don't run out of binlogs in case of a recovery you can just replay those logs. 3. Analytical capabilities In case of statement based replication you can process those binlogs and gather useful statistics about which tables are the most written to, updated etc. +1 Scalability Once you reach that point you will already have the tools and configs for it but it worth to be mentioned: you can move your slave around easily, change the topology whenever you feel so. 

Yes, MySQL will be able to do 20 million inserts per day, no problem. My calculator says 20 million per day translates to roughly 231 inserts per second. See e.g. this blog post from Percona's Database Performance Blog from 2010 which talks about more than 36 thousand writes per second. With the optimisations that have gone into MySQL since then as well as the hardware improvements, this number will obviously be even higher today. And from a skills perspective, since you're already using MySQL, it might make sense to use that for this aggregation database as well. Note that you can increase write throughput by combining multiple inserts together in a single transaction. However, you may be able to use multi-source replication: 

Once it's time to merge all the databases, all will be fine because the primary keys are composites with one component unique for each employee, and one component unique within each individual database, so together this makes each primary key value unique across all databases. I've tested this successfully in MariaDB 10.2. 

You need to be able to index your queries otherwise it's always going to be slow and scaling the server will not really help much due to the amount of data it has to read on every full table scan. Option #1 I would suggest (if you want to keep everything in MySQL) to build a term index: 1) you need two more tables 

Relational DB seems a perfect fit for your needs. MySQL will probably give you some hard time on some of the queries you want to run or you need to off-load that to the application side but that doesn't mean it cannot be done. Depends on the size of your tables too. Databases with window function capabilities (practically anything else than MySQL) will help you with queries like "time between commits". I'd personally recommend PostgreSQL which is great for analytical queries as well. Document stores and key-value stores are not meant to be backends for serving analytical data retrieval. You might gain some performance on read/write operations but since your application has to do the heavy lifting instead of the database the number of operations increases significantly. So with those it might even be slower at the end. How you represent that data at the end is more of a business questions than technical. What your users want. Do they prefer excel sheets over for example an internal websites where they can check? Google Sheets might be a middle ground there. Just an idea. 

These are the first queries in the query plan. The first starts from the hundred-million-row table and joins to @Keys. The second starts from @Keys, but it does a clustered index scan on this table. I know temporary @Tables are questionable in most cases, so I changed my query to use a temporary #Table: 

We build SQL Agent jobs in the SQL Server Management Studio UI on a developer machine. Then, when we are done, we script it out and delete the job. We add the script to source control and include it in our product's install program. Because it is in source control, we can manage how updates are created and released. Once installed in a production system, nobody touches the job by hand. Any change to the job must be coded in the script, checked in, and tested first before delivering to the customer system. (Experienced customers do sometimes modify schedules or setup alerts or the like.) 

A customer of ours is experiencing frequent deadlocks. The deadlocks are mostly on the same statement. The deadlocks follow a pattern in which both SPIDs have acquired an update (U) lock on a page, and both try to up-convert the U page lock to an intent exclusive (IX) lock. Sometimes there is only one page involved; sometimes several. We captured a deadlock trace using the trace flag 1222. The SQL Server log shows many, many deadlocks with the following pattern (in bottom-to-top order): 

The error itself is happening because there is a create view (or procedure) statement and the slave doesn't have the user who created it. In mysqldump you will have this format: 

(These are only the very basics there are much more your can do here) You should worry about IO usage first because CPU and Memory are less likely to become the bottleneck. Keep an eye on IO utilization to identify when you're likely to hit the ceiling of what your system can do. If your writes are competing for locks it's possible you will experience high CPU usage but that's usually coming from mutex contention not "actual" CPU work. These can be remedied designing your tables and queries in such a way that they don't lock out each other. 

If you see [tablename].MYD and [tablename].MYI files it's MYISAM. If you see [tablename].ibd or only the .frm file then it's InnoDB. Based on that the process: MyISAM You can simply copy the .MYI, .MYD and .frm files to any existing MySQL instance's database directory. For example: You have your database in then you find the table which will be (.MYD, .MYI and .frm). You can copy only these three files to an arbitrary database on a newly installed MySQL like . InnoDB It's possible to import tablespace since 5.6 but if you can it's much simpler to copy the whole directoy. It's a common procedure to clone new replication slaves. In your case you copy the whole directory onto the new server and start MySQL over it. Make sure the innodb_log_file_size and innodb_log_files_in_group matches the size and number of what you have now. These are the files called and . 

I don't think you can get the CPU usage unless you have the Resource Governor enabled. If you do, look at sys.dm_os_performance_counters: 

I started two threads running inserts into this table at the same time. The first thread is running the following code: 

Both processes are running the same UPDATE statement to set a flag on this tempdb table. This tempdb table holds information that needs to persist between client calls until the client is done. The table has a fairly long index that starts with a GUID representing a unique process ID. I am having difficulty understanding and simulating this deadlock. I have tried various amounts of records with simulated data. My questions: Why are these processes acquiring U locks and then converting to IX? I would expect the DELETE to acquire IX locks to begin with. How can I prevent the deadlock? The statement causing the deadlock is below. The process has just done a lookup of costs for a list of items at a single store. It is trying to note that there was a cost found. Note that there is a deprecated (NOLOCK) on an UPDATE statement. Would this be a contributing factor? 

The behaviour you're describing is called MVCC (Multi version concurrency control). Strictly saying it's not delete + insert. It is more like: 

More on this and how to set it up with Nginx: Typeahead and autosuggest with pure Solr and Nginx Option #3 You can use also redis for prefix match using ZRANGEBYLEX. That is quite limited compared to the previous options but also a way to implement autosuggest. 

2) Populate your data and keep it up to date This can be done from your application, cronjob or trigger in mysql. Whichever you feel safe with. 3) Change your query to something like this 

Properly sized and configured servers with the right indexes can still serve queries from tables in the TB scale in the milliseconds range. If you do point queries (single row lookups with primary key or fully covering secondary keys) then you will find that the significant time is spent on parsing the query string and authenticate/authorise the user not the actual data fetching. 

NDB Cluster really is quite fast, with 200 million (NoSQL) QPS reported back in Feb 2015, see MySQL Cluster Benchmarks. What worries me about it is that they're still using those benchmarks now in 2018, as if no progress has been made since 2015. I also get the sense that, for whatever reason, NDB Cluster's popularity is fading compared to Galera and other solutions. (See e.g. the stats for the various tags here on DBA.SE.) 

You can then each of these files to see if you find any instances of or . For reference, you should be able to see the current value of the variable with: 

EDIT 2: We don't want to accidentally remove helpful/important modes from the system variable, so ideally we should only remove specific modes, in this case NO_ZERO_DATE and NO_ZERO_IN_DATE. This can be done this way: 

You can manipulate system variables like innodb_old_blocks_time (increase this - 1000 = 1 second) and innodb_old_blocks_pct (default is 37 - allowed range is from 5 to 95, set a smaller value to evict data from and similar faster). Both these variables are dynamic, so they can be given special values just before you run mysqldump, and then restored to the original values once it has completed. For details, see Making the Buffer Pool Scan Resistant. With MySQL 5.6+ (or MariaDB 10.0+) it's also possible to run a special command to dump the buffer pool contents to disk, and to load the contents back from disk into the buffer pool again later. (See MySQL Dumping and Reloading the InnoDB Buffer Pool | mysqlserverteam.com.) This way you can still use or other tools that "pollute" the buffer pool and then restore it afterwards. A way to prevent that running backup is unintentionally evicting your working set data at all would be to replace your backup method with Percona Xtrabackup or another physical backup tool that doesn't access the InnoDB buffer pool as such. Physical backup methods are also faster, and can be less disruptive than mysqldump. The disadvantage is that you'll need the exact same MySQL version and configuration on the system where the backup is restored. 

I am running SQL Server 2014 Developer Edition on my PC. I am trying to view the data in the system_health session. In SSMS, I have connected to the database, expanded the server / Management / Extended Events / Sessions. I see AlwaysON_health (stopped) and system_health (running). When I right-click on the system_health session, I get the following error: 

I expand system_health and see the targets package0.event_file and package0.ring_buffer. If I right-click either target and choose "View Target Data", I get this error: 

Run these queries simultaneously from different sessions. Join requests and locks together in one query. Considering that I have seen over 100,000 locks at once, this join would return a lot of duplicate request and session data, but it might work. 

If you really want to limit SQL Server memory, look at the Maximum server memory option. Getting memory usage is possible, but it depends on what you really want. Do you want to see memory usage as a percent of the "Maximum server memory" option? If so, look at sys.dm_os_process_memory: