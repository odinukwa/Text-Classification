The problem of giving an explicit formula for $A_q(n,d)$ is sometimes referred to as "the main problem in coding theory." The value of $A_q(n,d)$ is given by the maximum number of codewords in a q-ary code of length $n$ and distance $d$. More specifically, let the hamming weight of an element of $\mathbb{F}_q^n$ be its $l_0$-pseudonorm, the number of non-zero components, and the hamming distance between two elements $f,g$ the weight of their difference $d(f,g)$. Then $A_q(n,d)$ is the largest set $S \subset F_q^n$ s.t. for two elements $f,g \in S$, $d(f,g)\geq d$. There are a number of famous upper bounds on $A_q(n,d)$, including Hamming's sphere packing bound. The best are given by a linear programming approach (now improved to a semi-definite programming approach) given by Delsarte in the late 70s. I have recently been searching for an explicit formula for Delsarte's Linear Programming Upper Bound for $A_q(n,3)$ in the literature, which correspond to single error correcting codes, and have not had much luck for non-binary codes. For binary codes this appears to be well known, and shown as early as 1977 by Best and Brouwer. Non-binary codes seem to be a completely different story. There is a paper called "Some upper bounds for codes derived from Delsartes inequalities for Hamming schemes" by C. Roos and C. de Vroedt, which the authors claim deals with the q-ary case, but I have not been able to find a copy. There appears to have been a very large amount of work in this field so I would be shocked if no such formula exists (well, at least a formula for some special cases of n,q). Is there a body of work in this area I am missing? Do such formulae exist? Note: I have also posted this question to MO, since I think $A_q(n,d)$ has received significant attention from both communities. The link is: $URL$ 

Yes, your reading is correct. $U$ forgets that something is a domain, but does not change the underlying carrier set or the poset structure. The least element is still there. It is also the case that $U \circ F$ can be equipped with the structure of a monad, as you say. And yes, predomains need not have a least element. 

If you look at Notes on Chapter 8 you will see what has already been formalized, and I think that's a lot. There are the Coq HoTT library and the Agda HoTT-Agda library which formalize large chunks of Homotopy Type Theory. To get things done in Coq we needed a special version of Coq that was patched just for the purposes of HoTT. However, Coq is moving in the direction of supporting homotopy type theory, so before long we might be able to do it with standard Coq. In Agda one has to turn on the option, otherwise Agda thinks all types are 0-types. There are some lingering doubts as to whether really gets rid of the assumption that everything is a 0-set, or perhaps one could reintroduce it into Agda with tricky uses of pattern matches. The following aspects of Coq and Agda formalizations are not satisfactory: 

A Turning machine with insertion and deletion operations can be simulated by an ordinary Turing machine with a quadratic time cost. Do we know how insertion and deletion fit into the polynomial time hierarchy though? In particular, does anyone know a quadratic single-tape Turing machine that cannot be simulated by a linear time single-tape Turing machine with insertion and deletion? I've gathered that separation results are often more powerful, and maybe easier to prove, for the nondeterministic time hierarchy. Is that perhaps an easier place to attack this? If so, that's great because I'm ultimately most interested in rewrite systems anyways. 

I'm interested in an computational geometry problem that's sensibly expressed as an infinite dimensional 0-1 integer program. I'm not worried about finding an actual minimum for the objective function, any solution with isn't stupidly big will do. It thus seems natural to apply an approximation algorithm that starts by running simplex or similar restricted to $[0,1]$. I'd expect the solutions usually require only a few hundred dimensions, but any naive restriction of the problem space yields millions of dimensions. As I understand it, good implementations of a linear program solver should be polynomial time in both the dimension and constraints on average cases, but nevertheless this problem chokes GLPK. Should GLPK really choke on a million dimensions? I've therefore started looking for less naive restrictions of the problem space, which lead me to LP-type problems. In particular, there is a claim that Clarkson's algorithm applied to linear programs are equivalent to running the simplex algorithm on the dual problem. In what sense is this true? I find this claim highly dubious with respect to complexity for several reasons. First, Clarkson's algorithm does not exploit any $[0,1]$ solutions with fast average case solutions, but merely randomly chooses pivots. Second, Clarkson's algorithm has running time worse than exponential in the dimension $O(dn + d! d^k \log n)$, which doesn't rule out polynomial time for average cases, but I haven't found that fact yet. As an aside, any nice examples of improving a restriction of an infinite dimensional linear program over time? 

"for all $\ell$ there is $k$" will give us a map which takes lists to lists, "such that $k$ is ordered" will give a funciton which runs through $k$ and checks that it is sorted, and "$k$ is a permutation of $\ell$" will give a permutation which takes $\ell$ to $k$. Note that is not just a mapping, but also the inverse mapping together with programs verifying that the two maps really are inverses. 

Have a look at the work of Alex Heller and Robert di Paola on dominical categories. There should be further references there to older work. 

Type theories in which every type is inhabited are far from being useless. True enough, through the eyes of logic they are inconsistent, but there are other things in life apart from logic. A general purpose programming language has general recursion. This allows it to populate every type, but we would not conclude from this fact that programming is a useless exercise, would we? In the theory of programming languages types are used as a safety feature ("A well typed program does not go wrong" sad a famous man), as an organizational device, and a tool for program analysis. None of these purposes requires that there be an empty type. Type inference is only one aspect in which type theory relates to programming languages. Some other uses of types in programming languages are: 

Consider that hamiltonian cycles of the bipartite graph are isomorphic (that is we can always permute the rows amongst themselves and the columns similarly to reach any other hamiltonian cycle). Consider the planes given by $(x,-,-)$ and $(-,y,-)$. Permuting these planes is exactly the same as permuting the rows and columns of the $(-,-,z)$ planes. Thus we may permute in this way to get any of the hamiltonian cycles on one of the $(-,-,z)$ planes, giving us $n!(n-1)!/2$ unique xyz-graphs. Further, we may then permute the (-,-,z) plane, fixing only the one we specifically permuted before, each of which also must give a unique xyz-graph. Thus in total we have: $H(n) \geq \frac{n!(n-1)!^2}{2}$ Which implies the bound $C(n) \geq \frac{1}{2}\sum\limits_{i=2}^n \frac{(n)_i^3}{i^2}\\$. EDIT: In fact, we can do better. We have only considered sub cubes here, but we may also consider xyz graphs inside ([n],[n],[m]). Since all simple cycles are hamiltonian on $K_{n,n}$, we take $m \leq n$ to avoid double counting. Then using the same construction for our first cycle and then the same permutation process above we get: $H(n,m) \geq \frac{n!(n-1)(m-1)!}{2}$ and then $C(n) \geq \frac{1}{2}\left (\sum\limits_{i,j=2 \ | \ i \geq j}^n \frac{(n)_i^2(n)_j}{ij} \right )$. 

There is a sound theory of overloading operators and functions realized by type classes in Haskell, and to rougher extent by traits in Rust, etc. In mathematics however, there are many situations where one set carries the same structure in multiple ways, like the integers being a monoid under both addition and multiplication. We normally just give these different names, but potentially one wants to abstract some tougher mathematical function, or a proof done in the type system. In principle, one could do this the way mathematicians do it, by applying a type class to a type that associates the operations to the base type, as opposed to just the set itself with a fixed association. Is that the "right" or "only" way to gain this flexibility? Or is there something else? In particular, there are a bunch of languages like Scala that do overloading of overloading rules in a rather dangerously complex ways, well even incoherent instances in Haskell probably. It'd be interesting if there were clearer "more parametric" way to achieve the ends that motivates those design decisions. 

I am attempting to find a complexity for computing the order polynomial of partially ordered sets on a special family, and have come across the following problem. Assume we have the following values from a polynomial: $P(0)...P(n-k)=0$ and $P(n-k+1)...P(n)=y_{n-k+1}...y_n$: what is the complexity of polynomial interpolation in this environment? Looking at the Lagrange polynomials, the first n+1-k become 0, and the k last follow a simple form: $L_i = \frac{\Gamma(x+1)\Gamma(i-n)y_i}{\Gamma(x-n)\Gamma(i+1)(x-i)}$ This suggests to me the coefficients for each x should have some nice form. However I'm a bit lost at this point as to the complexity of computing the $L_i$ (which we need k of), perhaps there is a better method? 

There appears to be much interest in the subject of the spectra of Cayley graphs. Indeed it appears that the spectra are highly related or may be computed through the irreducible representations of the underlying group in the graph, but I cannot seem to find a general formula for such a computation. For instance an old paper on the spectra of Cayley graphs $URL$ seems to compute the sum of eigenvalues. In particular, I am interested in computing the largest and smallest eigenvalues of a particular Cayley graph of the symmetric group with a symmetric but not normal generating set. Is the above the only tool to do something like this? I am struggling to find anything more. 

On the positive side, it is decidable whether a one-tape Turing machine runs in time $n \mapsto C \cdot n + D$ for given $C, D \in \mathbb{N}$, see: 

A universal function can be written quite easily in a Haskell-like language (no side effects, higher-order functions), namely: 

Yes, there are convincing reasons to believe that recursion can be turned into iteration. This is what every compiler does when it translates source code to machine language. For theory you should follow Dave Clarke's suggestions. If you would like to see actual code that converts recursion to non-recursive code, have a look at in the MiniML language in my PL Zoo (notice that the function at the bottom, which actually runs code, is tail recursive and so it can be trivially converted to an actual loop). One more thing. MiniML does not support mutually recursive functions. But this is not a problem. If you have mutual recursion between functions $$f_1 : A_1 \to B_1$$ $$f_2 : A_2 \to B_2$$ $$\vdots$$ $$f_n : A_n \to B_n$$ the recursion can be expressed in terms of a single recursive map $$f : A_1 + \cdots + A_n \to B_1 + \cdots + B_n,$$ 

Consider a function $F: \mathbb{F}_2^d \to \mathbb{Z}^n = (f_1,\ldots,f_n)$ with the property that if $y \in \mathbb{F}_2^d$ is a rotation of $x \in \mathbb{F}_2^d$, i.e. $y$ is $x$ permuted by an element of the cyclic group generated by $(1 \ 2 \ldots d)$, then F(x)=F(y), and the additional constraint that $f_i$ is of the form $ f_i(x) = \sum\limits_{j=1}^d \alpha_jx_j $. There are obvious examples of such functions, for instance $F(x) = \sum\limits_{1}^d x_i$. For any $x \in \mathbb{F}_2^d$ and $\pi \in S_d$, $F(x)=F(\pi x)$. We say that such a function is fixed under $S_d$. However, as you might expect from my initial explanation, I am interested in such a function which is fixed strictly under rotations, that is for any permutation $\pi$ which is not a rotation, then $F(x) \neq F(\pi x)$, or at least fixed under rotations and not fixed under all of $S_d$. Do such functions exist? I am having difficulty finding any example which is not the one I gave above. 

How much is known about nondeterministic linear time? I'm aware that $$ \mathrm{NTIME}(n) \neq \mathrm{DTIME}(n).$$ Is there an $m > 1$ so that $\mathrm{NTIME}(n) \not\subset \mathrm{DTIME}(n^m)$? Are there any arguments that $\mathrm{NTIME}(n) \subset \mathrm{P}$ should be unlikely? 

I'll concur with JɛﬀE that MS degrees are viewed as "consolation prizes" in the sciences in the U.S. because people usually take them when they fail qualifying exams in Ph.D programs. And who pays to do an MS when they'll pay you to do a Ph.D directly? I'd also concur with David Harris that mathematics might prove the most efficient route to doing serious theoretical work, but this depends entirely upon the program. Ask any math or comp. sci. departments who make offers how they feel about students taking courses outside the department though. I do recommend that you broaden your interests in more applied computer science of course, but do so by reading something. There are mathematically entertaining topics around databases, like Bloom filters, as well as fun applied papers, like the CryptDB articles.