You can unpivot the columns using a table value constructor and then count the number of occurrences. 

The query plan for the XML query shows a table scan of and a nested loops join with a seek on the primary key into the system table that holds the value you are looking for. 

Update: If you for some reason don't need to visit every row in the table you can do the equivalent of the "jump to next date" version that is implemented in the recursive CTE by Paul White. The data type does not need the ID column and you should not use a hash index. 

I really can't answer that but thought I should share what I know anyway. I don't know why a scan of PRODUCT table is considered at all. There might be cases where that is the best thing to do and there are stuff concerning how the optimizers treats UDF's that I don't know about. One extra observation was that your query gets a good plan in SQL Server 2014 with the new cardinality estimator. That is because the estimated number of rows for each call to the UDF is 100 instead of 1 as it is in SQL Server 2012 and before. But it will still make the same cost based decision between the scan version and the seek version of the plan. With less than 500 (497 in my case) rows in PRODUCT you get the scan version of the plan even in SQL Server 2014. 

SQL Server expands the view and considers accessing the base tables instead. If the view or the base tables are used is a cost based decision by the optimizer. To force SQL Server to use your indexed view you should use the noexpand hint. 

Not sure I get what you are want here but it could be that you can use a sort order value in your table expression and then use that in an order by clause. 

The behaviour depends on SET XACT_ABORT and is explained in the following quote from the linked article. 

You can find the min value for where there are 6 consecutive rows by using a correlated query doing and then count the number of rows that is greater than or equal to . Use that min value for against the table to get the 6 consecutive rows that you want. 

ID is not unique in this table, it is unique for each combination of and . There are some comments in the procedure that tell you what it does but overall it is calculating the running total in a loop and for each iteration it does a lookup for the running total as it was 45 days ago (or more). The current running total minus the running total as it was 45 days ago is the rolling 45 days sum we are looking for. 

Vote on this connect item for a change of behavior. You can use for xml explicit to build your XML instead. Something like this: 

You could create a table that you then fill up until the end of time or disk space runs out whichever comes first. 

From Error message 1785 occurs when you create a FOREIGN KEY constraint that may cause multiple cascade paths 

Have a look at the blog post Split strings the right way â€“ or the next best way by Aaron Bertrand for a couple of versions to choose from. 

Get the value from the XML as a string and do some string manipulation in a case statement. Example with simplified XML: 

Here is a recursive CTE solution using a technique that Paul White blogged about in Performance Tuning the Whole Query Plan. 

In theory, the data model allows for phones to be related to more than one patient. We did have some issues in the past, and most of the phone numbers have been denormalized. I need help to understand: 

I am planning a migration to 2016 Reporting Services and have noticed a particular problem. On two separate new installs for SSRS 2016 Developer Edition, I am unable to save a subscription to a report. The report was downloaded as RDL from the current 2008 R2 server, then saved to the new 2016 environment. The new server is build 13.0.4466.4. I also installed the same version of SQL onto a second server and have reproduced this problem. All servers are running in Native Mode. Within Report Manager, I am able to view the contents of the report (on-demand). When I try to create a subscription, the web page is stuck with the "Loading...." prompt after I click 'Create subscription'. It does this regardless of the Render Format I choose. I am attempting to deliver via e-mail. I have domain credentials for the SSRS service account configured, and I have put the IP address for the SMTP server in RS Configuration Manager, so I know it is configured for email ok. In the SSRS log file, I see these types of errors at the time I try to create the subscription: 

The problem: I am trying to add some logic to a database refresh job that will de-identify PHI-related fields, and have come across a strange side effect. I am attempting to exclude a group of patients, in order to allow that set to see their true information, and mask all of the other sets. When I run my update statement, it also updates a large portion of that set I was trying to exclude. Why is that, and how can I change my update statement to avoid that behavior? The data model: 

I then run this update statement against the Phone table to default every other patient's phone number to 555-5555: 

Patients table - ID as the key, and GroupID to differentiate groups of patients. Also has an IsDeleted field (bit) to indicate soft deletion.t item Phone table - ID as the key, and has three business fields: AreaCode, PhoneNumber, and Extension. Also has an IsDeleted field (bit) to indicate soft deletion. PatientPhone - Has an ID as the key, but also uses the combination (PatientID, PhoneID) to link these tables. 

My understanding of SQL security is such that in order to have access to the server, you need a login. To access a database on that server, your login needs to be associated with a user in that database. Over time I have been consolidating security so that developers, QA, etc. are in various roles implemented as active directory groups. Those groups have logins on the server, so individual windows logins are not so prevalent on SQL instances. I grant proper accesses to the databases through these roles. A common service I provide is to refresh non-production versions of databases from production. I'll do a backup and restore of a production database onto a development server, de-identify the data, rename it, etc. I have noticed something recently in my environment that is troubling. I was granting access to a key table in production for a development team, when I noticed by using 'fn_my_permissions' that one of the developers has read and write access to the entire production database, rather than being restricted to read-only as I designed. I compared all of this developer's group memberships and found another group he was a member in that was assigned read/write - but only as a database user. There is no associated login for that group on that SQL server. It is typical here to copy a 'baseline' production database and repurpose it elsewhere - whether as another production database, or a non-production version. I am starting to find other examples of this anomaly. To sum it up, the problem I see is that when a database is copied to another server, the users in that database that were previously linked to logins on the original SQL instance seem to retain the same level of access on the new SQL instance - even if there is no related login for them on new SQL instance! This strange effect seems limited to users representing windows group logins. Here is a summary of what I am seeing: Server1 has a login tied to windows group A, which has been granted read/write to database X. Database X is then backed up and restored onto Server2, with logins but none related to group A. The database still shows a user for group A, and when I run 'fn_my_permissions' for a user in that group he has read/write to the database. This particular server is running SQL Enterprise 2012 SP3 with the latest CU. I've seen the same thing on servers running 2008 R2, as well as a current build of SQL 2016. Why is this happening? And what can I do to ensure that users do not inadvertently circumvent the controls I have put in place? It seems obvious I need to remove all of the unnecessary users after the databases are copied, but why? 

Restore the backup you have from to a new database and then you can delete the rows from your production database that does not exist in your restored backup database. There is no hidden datetime information in the table that you can use. 

Nothing comes to mind that would improve the performance of your query so until you actually see a performance issue with this query you could use it as is. Provided it does return what you want. 

Makes it easy to divide the work between you and your friend that does the same going from the bottom of the list of procedures. And it will not leave you with a half done gigantic wall of code when you need to go on a break or something. You also minimize the time span where you can end up with editing conflicts between what you do and what someone else did with the same procedure. If you after 4 days of editing the 46,000 lines of code eventually execute the thing you could overwrite something someone else has done during the time of your editing. If you use source control (and you should) you would "only" end up with having to merge the changes. 

When there is an XML index created on a table, SQL Server will always use that index (the internal tables) to get the data. That decision is done before the optimizer has a say in what is fast and what is not fast. The input to the optimizer is rewritten so it is using the internal tables and after that it is up to the optimizer to do its best as with a regular query. When no index is used, there are a couple of table-valued functions that are used instead. The bottom line is that you can't tell what will be faster without testing. 

You can easily test the performance yourself. Create a regular table that you can test your queries on. 

Update: I took the liberty to execute the test rig provided by wBob on SQL Server 2014 with Compatibility level 110 (SQL Server 2012) and 120 (SQL Server 2014) Result: 

If you don't specify the node, SQL server have to generate a plan that works with mixed content XML, concatenating all the node values from the sub nodes. 

I have not been able to produce a query that use XML indexes using the method so I can't really tell you in what cases it may work. If you want to tackle a performance issue with your query I would recommend you to shred the XML using and and then reconstructing the XML using . It is usually faster than building the XML with and you could also probably make use of selective XML indexes if you need it. 

Instead of persisting to temp table you can use in a derived table to force SQL Server to evaluate the result from the joins before the UDF is called. Just use a really high number in the top making SQL Server have to count your rows for that part of the query before it can go on and use the UDF. 

Move the content of (25 rows) to the remote server to avoid doing a join between tables on different servers. One way of moving the rows from to the remote server is to execute the query on the remote server using with the values as a parameter in a XML structure. Unpack the XML to a table variable and use the table variable in your main query. 

It is a good thing in general to always specify the node. The query plan is simpler and more efficient.