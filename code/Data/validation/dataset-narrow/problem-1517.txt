The second way, predicting $x=cos(\alpha)$ and $y=sin(\alpha)$ is totally okay. Yes, the norm of the predicted $(x, y)$ vector is not guaranteed to be near $1$. But it is not likely to blow up, especially if you use sigmoid activation functions (which are bounded by they nature) and/or regularize your model well. Why should your model predict a large value, if all the training samples were in $[-1, 1]$? Another side is vector $(x,y)$ too close to $(0,0)$. This may sometimes happen, and could indeed result in predicting wrong angles. But it may be seen as a benefit of your model - you can consider norm of $(x,y)$ as a measure of confidence of your model. Indeed, a norm close to 0 means that your model is not sure where the right direction is. Here is a small example in Python which shows that it is better to predict sin and cos, that to predict the angle directly: 

rpart is a decision tree model and as such is very much interpretable. You should visualize your decision tree. See the examples here, one of them is 

The size of each tree depends very much on its depth. Thus, change the maximal depth (). Try to set it to finite number (as opposed to the default "None") and then try to reduce this number. In addition (or as alternative) try to increase or . You can also analyze you features and keep only important ones. The simplest way would be to have a look at the of your forest. (In general, finding important features is an art and science on itself.) Exclude non-relevant features and rebuild the forest. 

With (d=256, n=10) as well as with (d=16000, n=1000) you are under the curse of dimensionality. The essence of the curse (quoted from Wikipedia) 

converts strings to integers, but you have integers already. Thus, LabelEncoder will not help you anyway. Wenn you are using your column with integers as it is, treats it as numbers. This means, for example, that distance between 1 and 2 is 1, distance between 1 and 4 is 3. Can you say the same about your activities (if you know the meaning of the integers)? What is the pairwise distances between, for example, "exercise", "work", "rest", "leasure"? If you think, that the pairwise distance between any pair of activities is 1, because those are just different activities, then is your choice. 

If I understood the question correctly, you have trained an algorithm that splits your data into $N$ disjoint clusters. Now you want to assign prediction $1$ to some subset of the clusters, and $0$ to the rest of them. And amont those subsets, you want to find the pareto-optimal ones, i.e. those who maximize true positive rate given fixed number of positive predictions (this is equivalent to fixing PPV). Is it correct? This sounds very much like knapsack problem! Cluster sizes are "weights" and number of positive samples in a cluster are "values", and you want to fill your knapsack of fixed capacity with as much value as possible. The knapsack problem has several algorihms for finding exact solutions (e.g. by dynamic programming). But a useful greedy solution is to sort your clusters in decreasing order of $\frac{value}{weight}$ (that is, share of positive samples), and take the first $k$. If you take $k$ from $0$ to $N$, you can very cheaply sketch your ROC curve. And if you assign $1$ to the first $k-1$ clusters and to the random fraction $p\in[0,1]$ of samples in the $k$th cluster, you get the upper bound to the knapsack problem. With this, you can draw the upper bound for your ROC curve. Here goes a python example: 

If you do not use target for feature selection, then there is no leakage and you can apply this on the whole dataset. The methods that you mention: checking correlation between features and checking variance of each feature belong here. If you do use target for feature selection, then you should use 'train' subset only. Otherwise there is a leakage. The examples here would be (i) you check correlation between each feature and target (ii) use build Random Forest model and take best features 

Recall depends on the quality of your model (and thus also on the quality of your training data) and on the chosen probability threshold. By decreasing probability threshold, you improve recall but worsen precision. Which tradeoff to choose is again a business decision. 

You are using RandomForest with the default number of trees, which is 10. For around 30 features this is too few. Therefore standard deviation is large. Try at least 100 or even 1000 trees, like 

You can see that under high imbalance (left-hand side of the picture) L1 regularization performs better than L2, and both better than no regularization. But if the imbalance is not so serious (the smallest class share is 0.03 and higher), all the 3 models perform equally well. As for the second question, what is a good loss function for imbalanced datasets, I will answer that log loss is good enough. Its useful property is that it doesn't make your model turn the probability of a rare class to zero, even if it is very very rare. 

There are multiple approaches to optimization in scikit-learn. I will focus on generalized linear models, where a vector of coefficients needs to be estimated: 

Use inherently sparse models like or . Normalize your features with , and then order your features just by . For perfectly independent covariates it is equivalent to sorting by p-values. The class will do it for you, and will even evaluate the optimal number of features. Use an implementation of forward selection by adjusted $R^2$ that works with . Do brute-force forward or backward selection to maximize your favorite metric on cross-validation (it could take approximately quadratic time in number of covariates). A scikit-learn compatible package supports this approach for any estimator and any metric. If you still want vanilla stepwise regression, it is easier to base it on , since this package calculates p-values for you. A basic forward-backward selection could look like this: 

No. There is no similar mechanism for continuous variable. If it worries you, that , you can 1) demean the price, that is subtract mean price from all price values. Then negative values will clearly show below-average and positive above-average prices. 2) after demeaning you can divide values by the standard deviation of the price. (1) and (2) together is called "standardization". Alternatively you can 3) rescale your price to the range of values you want. Usual choice is (0,1) range. If you do this for one feature (price in your case), then it makes sense to do the same for other features. Whether or not this will help to get better prediction results depends on the model. Some models, a typical example would be SVM, do require such transformation. 

does this for you. To get intuition of the grid-search process, try to use To extract scores for each fold see this example in scikit-learn documentation 

The simplest solution is to build 10 models, one per stage. It will enable you to use different features or even different algorithms for each stage. 

Use inherently sparse models like or . Normalize your features with , and then order your features just by . For perfectly independent covariates it is equivalent to sorting by p-values. The class will do it for you, and will even evaluate the optimal number of features. Use an implementation of forward selection by adjusted $R^2$ that works with . Do brute-force forward or backward selection to maximize your favorite metric on cross-validation (it could take approximately quadratic time in number of covariates). A scikit-learn compatible package supports this approach for any estimator and any metric. If you still want vanilla stepwise regression, it is easier to base it on , since this package calculates p-values for you. A basic forward-backward selection could look like this: 

For an intuitive explanation I also found this IMHO, for your problem with n=10 you can use d<=2, for n=1000 d=3 (maybe 4, at most 5). Why d=3 for n=1000? Roughly speaking this would correspond to 10 points along each dimension (10^3=1000), which is reasonable to fill the 3D space. For d=5 it is like 4 points in each dimension, which is not so good but not a disaster. IMHO, you should try to reformulate your problem and significantly reduce dimensionality (maybe try to use SVD or PCA). This may automatically solve your problem of noisy data. 

I think, even before doing LDA, you should remove words which appear in more than "x" percent of your documents. Try different "x" starting from 80% and then going down. The logic is that if the word is common for many documents, it does not distinguished those and should be neglected. 

The further exploration of your data would help. Are there, for example, some clusters where relative frequencies of your classes are much different from average? Now I see two ways to increase sensitivity of your algorithm toward class 2: 

You can see that OLS beta differ from your optimal beta not only in the first two coefficients (that have been negative), but the rest of coefficients were also adjusted. 

The blue dots are (FPR, TPR) tuples for all $2^{10}$ subsets, and the red line connects (FPR, TPR) for the pareto-optimal subsets. And now the bit of salt: you did not have to bother about subsets at all! What I did is sorted tree leaves by the fraction of positive samples in each. But what I got is exactly the ROC curve for the probabilistic prediction of the tree. This means, you cannot outperform the tree by hand-picking its leaves based on the target frequencies in the training set. You can relax and keep using ordinary probabilistic prediction :)