One quick approach to figure out the structure and delimiters of your files would be to use a desktop application with a data import function. MS Excel works very well for smaller files but I find MS Access works very well for larger files and has the advantage of suggesting suitable data types that can port over to SQL Server later. You can toggle the delimiter options in both wizards to see how the file would be parsed. One distant possibility is that the file is fixed width so that the values begin and end in specific column positions. For example, first name can be in the first 15 characters of a line while last name would be 15 characters starting in position #16 on a row. Because of the spaces you're seeing, the space may be the actual delimiter. Visual inspection would be the way to confirm this. 

I have a table with a PK based on a identity and will have a clustered index on a column. I've created a partition function based on calendar quarter and was planning on two partition schemes (one each for clustered and non-clustered indexes) to able to spread out I/O. However, if I plan to implement a sliding window to age out older data, do I need to keep all indexes in the same partition scheme or it doesn't matter as long as the partition function is the same and using the same key column? 

On larger DDL scripts for deployments, usage of SET NOEXEC ON as part of error handling is commonplace to prevent execution of downstream code. If coupled with SET NOCOUNT ON, there's a chance code execution may be inadvertently suppressed unless the proper post-deployment checks are done. I checked sys.dm_exec_sessions and there is no attribute for either SET statement. Is there an alternate method of checking the status of either statement for the current session or any user's session from the DBA perspective? Here's a quick example... 

Check for any external connections connecting on the same port # using . Sometimes application services that automatically connect to the instance keep connecting even if you've declared a maintenance window like this. Once identified, disable them at the source, or temporarily block them by firewall rule, change in port # or disabling the external protocols in Configuration Manager (Named Pipes and TCP/IP). 

I investigated an instance that experienced an unexpected restart and came across the usual service control event but no user login associated with it. Is it still possible that an authorized user initiated the restart or is this due to Windows issuing the command? If an authorized user, outside of the SQL error logs and event logs, where could I go about attempting to identify the user? 

I'm setting up an ETL process using SQL Server 2014 and SSIS. My Bulk Insert Task is utilizing a source file and format file on ShareA targeting a DB on InstanceA. The package is run via SSA job on InstanceB. Both InstanceA and InstanceB are running the same service account for the Agent. The service account has and read/write privs on the target table. It also has appropriate permissions on both the share and folder via NTFS. When the SSA job runs on InstanceB, the job fails on the step with code 5 for Access Denied attempting to access the source file. However, if I run the T-SQL generated by the task in a job on InstanceA it succeeds, confirming share/NTFS permissions are OK. Am I forced to run the SSIS package on my target server InstanceA to allow to function or are additional permissions needed in order to run on an alternate server? 

I have a handful of archive DBs located on an Isilon NAS share that SQL lost sight of during a heavy I/O event, resulting in 823 errors like the one below. 

Pinal Dave has an excellent script identifying object dependencies across databases. SQL SERVER â€“ Find Referenced or Referencing Object in SQL Server using sys.sql_expression_dependencies 

In the scenario of adding a column to a VLDB-class table, it may be worth exploring creating a new table with the new structure and moving records from the old table to the new table in small ranges. It'll keep the individual transaction size small so the high-water mark for the Tlog in Simple recovery would be relatively low. You can't avoid ACID requirements altogether but if you can batch your upgrade as smaller steps instead of one single transaction, you may be able to work around the disk space constraint. 

On Server B, review of the execution plan indicates that the criteria for table [a] at bottom is part of an index seek operation at the start of the execution plan, which makes the subquery [c] resultset execute rapidly. On Server A, review of the same execution plan indicates that subquery [c] is executing first with full index scans due to outer criteria not being applied. Indexes utilized in both execution plans are identical. Table rows counts are slightly larger on Server A since the restore due to normal operations but index fragmentation is nearly identical. A participating index on table [t] in the subquery on Server B has double the number of pages as on Server A but identical row count. Statistics are updated nightly at the same time on both servers. I've attempted index rebuilds on table [t] and manually updating statistics to attempt to get both execution plans to match. What other factors may be causing this change in the order of execution? I suggested to the developer replacing the subquery with a UDF that takes the EID and UniqueDocumentNumber fields from [doc] as arguments. What other options can I explore with the developer to increase the probability of the execution plans on QA being utilized on Prod? 

I was able to get guidance from the following MSDN link: Configure Windows Service Accounts and Permissions. The pertinent section (bookmarked in the link) is "File System Permissions Granted to SQL Server Per-service SIDs or Local Windows Groups". The short version for the solution is that either Read, Execute or Full Control are needed on the leaf-level subdirs under the system root (DATA, LOG, FTDATA, etc). On my host, NTFS permissions weren't propagating from the mountpoint volume root or the folder by which the SYSDB LUN were mounted. Applying NTFS permissions on the leaf-level corrected the issue. 

In the above code, filegroup [DATA_1995] would retain the values beyond the last partition boundary. So, if I elect to add new filegroups and split the function for additional boundaries in this fashion... 

If you can define the T-SQL query to filter the rows to a specific date range, create a view and choose the view object within Excel instead of the source table. 

It may be due to the fact that you specified in your connection string. This will have the connection be established using Windows authentication using the domain account you're currently logged in as at the client, effectively ignoring the UID parameter. Try removing that parameter and rerunning. You may need to provide the password in the connection string, but since that's not secure, you could also keep the Trusted Connection parameter, put your users' Windows logins in a domain group and grant the domain group SELECT privs to the appropriate objects. 

Reading up on Kimball dimensional modeling, use of a surrogate key, in particular an IDENTITY, will help reduce fragmentation caused by page splits as you'll be appending rows at the end of leaf pages vs. attempting to insert them in the middle if the keys are not in the order of the index (ascending or descending depending on how the index was defined). But, if the surrogate key will not be used in joins with other tables or in WHERE criteria, use of the surrogate key as a clustered index might not provide any additional benefit after the initial data import. If you keep the composite clustered key, sorting the source data in the order of the six-column composite key prior to import should avoid the page split fragmentation you're witnessing. As for a choice of clustered index, the best candidate may be the column(s) that would be used in your most common/critical queries for WHERE criteria or joins. Use of a NCI will require a bookmark lookup to obtain the remaining values in the table if needed in the resultset. 

A data warehouse ETL process is querying a read-only secondary in an availability group. The ETL process queries a single table incrementally using datetime range criteria of a minute and read committed isolation level. At the time of execution, 5 records that meet the criteria are committed on the primary, but another 3, with slightly earlier timestamps than the first 5 (but within the criteria range) are still in open transactions. Does the nature of availability groups require all transactions to be applied in LSN order (delaying the visibility of all 8 records until all are committed) or do the delayed 3 records get later LSNs and are applied as soon as they are committed, potentially after the ETL process has adjusted its date criteria? 

Server A and Server B have identical hardware and instance configurations (A is Production, B is QA). B's DBs were restored from A's backups from one week ago. I was provided this query by the development team. 

I'm practicing splitting a partition function on a QA box to add new partitions for the upcoming calendar year. I'm running SQL Server Enterprise edition, patch level 12.0.4100. Here is the current function definition... 

Two options come to mind. If you go with the local copy approach you can manually use the AutoFill feature to fill in the blank values. For an automated approach, the one approach I would think of in T-SQL is to use a cursor to loop through all the rows, capture the non-NULL Dates and Types into variables and when you encounter a NULL value, UPDATE the current record with the last non-NULL value. An integer identity may be needed in the table you're importing to confirm the rows have been inserted in the order as found in the file. 

You can also look at the application event log. For SQL 2008 R2 instances, the event ID to filter on would be 8957 (at least for success codes). 

In the above example, if the filegroup doesn't exist, preventing table creation, the check will call the statement which will prevent actual execution of the subsequent statement. I've seen this type of code generation with tools such as RedGate SQL Compare. The catch is, if the person executing the code neglects to run to reset the session during debugging, they may get a message regardless of the actual outcome. 

One option may be to add individual sysadmins as users to the databases in question, add those users to a custom role with no explicit permissions (effectively like public), and setup the database audit specification to track actions for the custom role. 

During debugging, the OLEDB connection was created from scratch using the SQL Server Data Tools GUI. The notable difference in the connection string was the the addition of a space to have parameter now read . Connections to the secondary are now successful. It appears that the OLEDB driver accepts both variations but only the one with the space is implemented properly with SSIS packages. Is this an anomaly with SSIS or with the SQL Server Native client/OLEDB driver? 

By any chance are multiple instances of SQL Server running on this host machine? Check the Services console to see if more than one service of SQL Server is running. If so, the newly discovered one may be running the older service pack. 

In the process of removing an usused disk resource from cluster, I would follow these steps: 1) remove dependencies on the disks by the SQL Server resource, 2) take the disk resource offline, 3) delete the disk resource from the SQL resource group, 4) delete the disk resource from the Available storage pool. I was proceeding to do this on a two node cluster with two SQL 2008 R2 named instances (running Win2K8R2 SP1 build 6701, 64-bit). On deleting a resource in step 3 above, SQL would go offline, all the disk resources would go back to the Available pool, and the SQL resources (Net Name, SQL Server, SQL Server Agent) would disappear. SQL services do restart OK after via the services console as all the disks are still on the same node. I ran from the command prompt and I see the SQL service resources are still there, in Available Storage, and offline. To bring the resources back to the appropriate group, can I move them via commands? 

Three partition schemes were defined at the time of function creation but only the first one is utilized with data files added. 

I'm in the midst of creating a set of SSIS 2014 packages (using Visual Studio 2013) importing data from 50 tables. Since most of the tables are being filtered on the same column and I'm importing all columns into my staging area, I opted for a reusable design where my package name matches the table name, my OLE DB source query is defined with an expression-based variable (starting with ) and my OLE DB destination table is also defined with an expression-based variable (in this case schema.tablename). Both the source query and target table have matching column names and match in most cases on data types and sizes. As I'm cloning my packages for each table, I copy/paste a previously-created package and visually inspect the mappings to confirm the expressions and the data flow mappings are valid for the new package name. I'm seeing that the expressions are working as planned, but I have to manually do the drag/drop mapping of most columns in the destination editor. Is this expected behavior of the editor as the design metadata of the original package is invalid under the new package name and is there a way to have Visual Studio delete and remap all columns in the data flow based on column name? 

This may be a good candidate for Biml. One approach would be to create a reusable template that would migrate data for a single table in small date ranges with a For Each container. The Biml would loop through your table collection to create identical packages for each qualifying table. Andy Leonard has an intro in his Stairway Series. 

The filegroup [DATA_1995] remains in the last partition position for values beyond the upper-most boundary. Can this last filegroup be changed after the partition scheme is initially created or is the only way through creation of a new partition scheme and rebuilding all indexes utilizing the old scheme into the new scheme? 

One book that I've personally found best describes the calling of a DBA is DBA Survivor: Become a Rockstar DBA. It goes beyond the technical and talks about your relationship to your customers and your responsibilities to the tradecraft. It helped reaffirm my belief that the DBA profession is an essential one that is challenging, honorable and rewarding. 

I'm running time trials on a new Windows 2012 Server host with PureStorage FlashArray SSD LUNs attached. I conduct an initial test with CrystalDiskMark and discover write performance is substantially better than read performance. 

I came across an unusual scenario on an instance where only half the physical RAM was being utilized but I was observing PLE values between 1-3 seconds constantly. 

All of the DBs were in read-only mode and I have validated full backups I can recover from (already restored to an alternate server and ran DBCC as part of the test). I tried running DBCC CHECKDB, toggling the DBs to the offline state, emergency state, detaching them and lastly a DROP DATABASE command but am blocked by the same error. The files are still on the NAS media with the last date modified equal to the time they were put in read-only mode weeks ago. I figure my next (and last) option would to be to schedule a downtime to restart SQL services to see if the read-only state protected the data files. If not, I figure I need to stop SQL, move the damaged files out of the way, restart SQL, drop the now-Suspect DBs and recover from backup. Is there another option I may have missed that will keep the instance online? We have a live DB on a local volume for the VM that we'd like to keep up but a short service restart isn't out of the question. 

I think this is a pitfall. I recall SQL 2005 and older versions requires active nodes to be updated. SQL 2008 and later versions allow passive node updates like the walkthrough you described. A posting from Linchi Shea explains it well.