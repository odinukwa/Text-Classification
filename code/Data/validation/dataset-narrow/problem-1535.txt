No. Neural network is generally difficult to understand. You trade predictive power for model complexity. While it's possible to visualize the NN weights graphically, they don't tell you exactly how a decision is made. Good luck trying to understanding a deep network. There is a popular Python package (and it has a paper) that can model a NN locally with a simpler model. You may want to take a look. 

The derivative you see here is important in neural networks. It's the reason why people generally prefer something else such as rectified linear unit. Do you see the derivative drop for the two ends? What if your network is on the very left side, but it needs to move to the right side? Imagine you're on -10.0 but you want 10.0. The gradient will be too small for your network to converge quickly. We don't want to wait, we want quicker convergence. RLU doesn't have this problem. We call this problem "Neural Network Saturation". Please see $URL$ 

Why are you using the whole data set for training? You are using the test set for training then evaluate the performance on it again? In your code, I didn't see you actually used the training set you created. 

What you are looking for is groupby with multiple columns. Based on this example your pandas groupby would look like this : 

The first model would predict if its either Target 1 or Target 2 by looking at 100 columns The second model then would look at the 100 columns and additionally the output of model 1 and then predict 0 or 1 in case of target 1 or 0-100 in case of target 2. 

You always avoid feeding direct strings into neural networks. This thread here explains why you should avoid doing this : Neural Network parse string data? Once you convert the strings you have into vectors or any other form of numerical representation and encoding your labels as categorical, it will solve the problem you have at hand. If you need me to elaborate more on this, I would be more than happy to do so. 

Generate Ground truth data by drawing bounding boxes on elements you need comparison for and label them i.e if you want to compare text on the hard cover, you need to mark the text region and add a label to it like font type. Once you have ground truth data, you can then use object detection networks like different variants of Yolo to identify multi class objects in each document. 

I've created an application on Facebook's wit.ai. In my application, I have a single intent with the following phases: 

Now, we know our second move will eventually be chosen. When it does happen, the value network gives a +1000. This will increase , which makes the second move much more likely be chosen in the later simulations. Given enough simulations, the number of times the second move is chosen for simulation should be more than the number of times the first move is chosen. Finally, the move that AlphaGo decides to make is (quoted from the paper): 

You'd only use GPU for training because deep learning requires massive calculation to arrive at an optimal solution. However, you don't need GPU machines for deployment. Let's take Apple's new iPhone X as an example. The new iPhone X has an advanced machine learning algorithm for facical detection. Apple employees must have a cluster of machines for training and validation. But your iPhone X doesn't need a GPU for just running the model. 

I think you're confused. Maximum likelihood is a general technique for giving the most likely parameter estimation. There is no closed solution for maximum likelihood for logistic regression, so both R and Spark must numerically estimate it. How exactly the likelihood is estimated could be slightly different and might depends on the implementation. For example, Spark prefers to add regularization, R might like to use something else (you'll need to check the documentation). Summary: Both R and Spark try to estimate ML for you. Please check the R documentation on how R do it. 

There are multiple ways to extract NER. Primarily, based on the construction of statements NER was extracted with the use of POS tags. But overtime with the change of how information was being conveyed, there has been a migration from traditional methods to learning methods. Currently, take a look at sequence to sequence tagging for NER detection. If you have the appropriate dataset, you have the capability to extract anything you consider as NER. 

I totally understand your concern and I appreciate the fact that you want to teach the underlying math. From my experience I think a Better approach would be to not go straight to the math but teach math with problems you are already solving i.e hackers way of learning math. As programmers it's always a positive affirmation that we can program what we learn. Here is a very good repository that has content on these lines : $URL$ 

Given that the data model you receive is different, it makes best sense for you to use a NoSql store like mongodb. Here is the tech stack I would use ( python ) : 

Using the features you generated, you could add a label to each of the column for these features. You could generate these feature value for different time slices. Once that is done, you can then run something as simple as Random Forest Classifier on this data. 

Variable importance is measured by decrease in model accuracy when the variable is removed. The new decision tree created with the new model without the variable could look very different to the original tree. Splitting decision in your diagram is done while considering all variables in the model. What variable to split at the root (and other nodes) is measured by impurity. Good purity (e.g: everything in the left branch has the same target value) is not a guarantee for good accuracy. You data might be skewed, your right branch have more responses than your left branch. Therefore, it's no good just correctly classify the left branch, we also need to consider the right branch as well. Therefore, the splitting variable might or might not be an important variable for overall model accuracy. Variable importance is a better measure for variable selection. 

In general, we're talking about Apple and Orange. They have different interpretation, different equations, different formula, different way to estimate solution etc etc. 

Click on the "page 127 and 128" link (not shown here, but in the Stackoverflow answer). You should see: 

EDIT: How many respondent you need is hard to tell. Apple probably needs better data quality than you. If you'd like to do it statistically, you may calculate the minimum sample size and it's related to statistical power. 

This normalization means if the number of mutual voting of $u$ and $v$ is low and not enough proportional to total number of voting (here from $\frac{25}{10000} = \frac{1}{400}$ to $\frac{50}{10000} = 0.005$) you should apply this fact into the similarity in some way. Because, your observation is not enough and you can't talk about it confidently. In other words, it can be confident factor which you apply into the similarity measure. 

Factors of variation are some factors which determine varieties in observed data. If that factors change, the behavior of the data will change. The bold phrase said these factors are usually independent and one factor does not change by changing the value of the others. 

A solution can be using DBSCAN algorithm to cluster data. Then, if you set a proper radius for the DBSCAN algorithm, you will get three clusters. Therefore, you can detect some clusters as anomaly that number of their members is less than a threshold. 

As activation of a neuron depends on the sum of inputs, biologically makes sense that activation function be an increasing function. 

You can find the paper here. In sum, this paper gives a rank on keywords based on the defined $\chi^2$-measure. 

To understand why, imagine you have a network with many layers and nodes (multiple neurons in your question). If you don't have a non-linear activation function such as ReLu or sigmoid, your network is just a linear combination of bias and weights. Your network won't be useful for classifying non-linear decision boundary. But if your inputs can be linearly separable, you don't need neutral network... That's why all neutral networks almost always have a non-linear activation function. ReLu is the most popular, but there are other possibilites. When you pipe up a dozen of non-linear outputs like in neutral network, your network will be able to classify a non-linear decision boundary. The more your have, the better it can perform (but also easier for overfitting). 

No, you don't need to understand measure theory and real analysis to do machine learning in data science. However, it'd be hard to for you to read academic papers (eg: kernel methods) if you don't have the knowledge. Unless you want to be a mathematician or wish to pursue a Phd, you really don't need to know too much about the theories. In fact, most of the engineers working in the field I know don't understand statistics at all. They simply use some framework, get some good results, then get paid. 

For your problem at hand, I would use approach 1. Would be more than happy to elaborate in case you want me to. Here is a link that explains how to do transfer learning with Keras : $URL$ 

Train a classifier, with all the data points you have with labels as Target 1/ Target 2. For this you could use any family of classifier. But you need to be very careful in the evaluation. If this models performs poorly, you will have a problem, as your classification would affect your next model. You also need to check if the distribution between target 1 and target 2 are appropriate before using a model to classify them. Once the classifier is done, you can then use regression with all the input features + class of the entry ( target 1 or 2 ). 

I do not think you can achieve the result with just one single model. If you need more information, I could elaborate on it. But this should give you a starting point. Here is an explanation of the 2 above points: 

To receive requests with csv file, I would use flask microweb framework. I would run flask with gunicorn and gevent. For every csv file received, I would parse the rows based on varied columns and store them in a mongodb. I would write queries on the collection in mongodb to extract information. 

Note that is the number of moves searched for the move and it's in the denominator. The more likely our first move is searched, the smaller the function is. Thus, the probability for selecting our second move improves because AlphaGo actually picks a move by this equation: 

2 hours for a batch is too long even with a CPU. How many layers you have? How many neutrons you have? Have you tried a single layer? Try to reduce the complexity of your model. You might want to train a softmax regression, just to get an lower bound for your computation power. If you find your machine struggle even with softmax regression, you'll need to upgrade your hardware. 

Let's be precise. "Distance" has lots of meanings in data science, I think you're talking about Euclidean distance. The Gaussian kernel is a non-linear function of Euclidean distance. 

This value would be -1000, because this simulation would lead to a loss. Now, AlphaGo needs to generate the second simulation. Again, the first move would be much more likely to pick. But eventually, the second move would be pick because: 

The statistics here are obviously very good, in fact too good for any practical data set. Your model is almost perfect... Unfortunately, it's practically useless and I'll explain. In machine learning, if you see something like this you know you are in trouble. That can happen if there are problems with your data workflow. For example, you might have removed all outliers that you shouldn't, or you actually used a subset of your training data for the test set. It's fine if you're just toying SVM, but you'll never encounter something like this in real life. 

The renown algorithm for stemming is Porter stemming algorithm. Hence, you can use for the stemming. you can test it using . Moreover, you can see this post for more details. 

Finding valuable association rules is defined base on their support and confidence. ANN is not proper for these kinds of specifications. Hence, if we use ANN for learning association rules, we can't guarantee the specified support and confidence are satisfied. Anyhow, there are other machine learning methods to learn association rules such as decision tree. Although, that methods not such that proper to find rules with specified support and confidence. You can know more about this in this article. 

General speaking, you can assign the cluster to the new value using the method of that clustering method. For example, if you are using the DBSACAN algorithm, you can assign the cluster to the new value, if it is reachable from one core points of a cluster (in a tie situation, we can assign the more density cluster or many other methods). 

As you mentioned their goals are different. In clustering, we try to group data such that they have the same variability. For example, clustering customers of a company into different clusters, somehow, members of each cluster have the same behavior in their buying. On the other side, in blocking we try to reduce the variability, to record linkage, as an instance. 

They are the two very popular online free books. The first link even has working deep learning code. 

You have to try to understand those outliers to come with up a decision. 2:) You can just explain like "the probability of the difference in means is (or isn't) significant". 3:) You should draw a box-plot for each group. 

Most of us want to build a recommendation engine as accurate as possible, however, an experienced chief data scientist believes a practical machine learning algorithm should randomly shuffle the results, therefore non-optimal results. This is known as "Results Dithering". Slide 15 at: $URL$ While I understand how to do it (adding a small fixed Gaussian noise), but why would we do it to make the performance worse. 

Convolutional neural networks work because it's a good extension from the standard deep-learning algorithm. Given unlimited resources and money, there is no need for convolutional because the standard algorithm will also work. However, convolutional is more efficient because it reduces the number of parameters. The reduction is possible because it takes advantage of feature locality, exactly what @ncasas writes. 

has the details. There're also lots of papers on Google. Alternatively, you can use the cross-validated likelihood as a performance measure, although this can be slow, since it requires fitting each model N times, where N is the number of CV holds. Slide 17 in