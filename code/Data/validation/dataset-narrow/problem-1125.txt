Knowing the exact value of the chromatic number $\chi$ cannot help by more than a factor of $n$. Since there are only $n$ possible values of $\chi$, you can 'guess' its value, i.e., run processes $P_1,\dots,P_n$, where $P_i$ runs an algorithm assuming $\chi=i$. This whole scheme can find an optimum colouring in time at most $n$ times the time that it takes $P_\chi$ to find an optimum colouring. On the other hand, if you're talking about parameterizing the running time by $\chi$, then it's a much more interesting question. It's in FPT if you parameterize by $n-\chi$ S. Khot and V. Raman, ‘Parameterized Complexity of Finding Subgraphs with Hereditary properties’. If you parameterize by $\chi$ I would assume it's W[1]-hard. 

In the context of evolution and evolutionary algorithms it's called a fitness landscape. In other areas such as statistical physics it's called an energy landscape. If you have a continuous landscape (in evolution you typically don't) things are fairly straightforward. From Wikipedia: 

Many here are probably aware of Alon's recent super-linear lower bounds for $\epsilon$-nets in a natural geometric setting [PDF]. I would like to know what, if anything, such a lower bound implies about the approximability of the associated Set Cover/Hitting Set problems. To be slightly more specific, consider a family of range spaces, for example, the family: $\big\{(X,\mathcal{R})$ : $X$ is a finite planar point set, $\mathcal{R}$ contains all intersections of $X$ with lines$\big\}$ If, for some function $f$ that is linear or super-linear, the family contains a range space that does not admit $\epsilon$-nets of size $f(1/\epsilon)$, what, if anything, does this imply about the Minimum Hitting Set problem restricted to this family of range spaces? 

Suppose that $p$ is a polynomial. Then does there exist a polynomial $q$ where if $f:\{0,1\}^{n}\rightarrow\{0,1\}^{n}$ is a bijection where both $f$ and $f^{-1}$ are computable by circuits with at most $p(n)$ gates, then there exists involutions $\iota_{1},\dots,\iota_{k}:\{0,1\}^{n}\rightarrow\{0,1\}^{n}$ where $$f=\iota_{k}\circ\dots\circ\iota_{1}$$ and where each $\iota_{i}$ is computable by a circuit with at most $q(n)$ gates and where $k\leq q(n)$. 

So the two-dimensional reversible cellular automaton Critters (which you can simulate online at $URL$ on the Torus does not seem to follow the second law of thermodynamics. For example, if a $10\times 10$ block in the torus is filled with random data and the cells outside of this $10\times 10$ block are all zero, then there is a good chance that after several million generations on a $256\times 256$ grid, one can see the cellular automaton return to its original state. Why is the Poincare recurrence time for the reversible cellular automata Critters so small in this case? The rule Critters is universal for reversible computation, and it exhibits chaotic behavior that one would expect in a reversible cellular automaton. Therefore, one would expect that after running the cellular automaton for enough generations, the entropy would constantly increase and hence one would need to wait a very long time (much longer than several million generations) to observe the Poincare recurrence. Is there any explanation for this phenomenon? Are there other chaotic, reversible, universal cellular automata of dimension 2 that exhibit this phenomenon? 

In discrete landscapes things are more complicated since we need to specify which states neighbour each other, essentially giving us a graph underlying the set of solutions. What you end up with is not as clean as you'd like. It's a triple $(X,N, f)$, where $X$ is the set of solutions (i.e., possible genotypes), $N:X\to 2^X$ is a neighbourhood function, and $f:X\to\mathbb{R}$ is the fitness function. For a treatment of these discrete fitness landscapes that should satisfy you with regard to mathematical rigour I would direct you to this survey paper by Reidys and Stadler. 

Glencora Borradaile has an interactive graph with turnover time (among other information) for several journals from several sources. However, not included is the first journal I think of with a fast turnaround time: Information Processing Letters (IPL). I couldn't find any statistics, but rapid dissemination is their first goal. When refereeing for a similar Elsevier journal with the same goal of rapid dissemination but with a slightly different scope, they requested my report within about 5 weeks. 

The DMANET mailing list is one of the major resources I use. It "spreads information on conferences, workshops, seminars etc. relating to discrete mathematics and algorithms." They also sometimes have job postings for positions at the faculty, postdoc, and grad student levels. 

2-3. $\Psi(\mathcal{C})$ will typically become more difficult than $\mathcal{C}$ and this is a good thing. The difficulty of a proof-of-work problem needs to be finely tunable, but the original problem $\mathcal{C}$ may or may not have a finely tunable level of difficulty (remember that the difficulty in mining Bitcoin is adjusted every two weeks). The difficulty of problem $\Psi(\mathcal{C})$ is equal to the difficulty of finding some suitable $(k,x)\in D$ multiplied by $\frac{2^{n}}{C}$. Therefore, since the constant $C$ is finely tunable, the difficulty of $\Psi(\mathcal{C})$ is also finely tunable. Even though the problem $\Psi(\mathcal{C})$ is more difficult than the original problem $\mathcal{C}$, almost all of the work for solving the problem $\Psi(\mathcal{C})$ will be spent on simply finding a pair $(k,x)$ with $(k,x)\in D$ rather than computing hashes (one cannot compute whether $H(k||x||\textrm{Data}(k,x))<C$ or not until one has computed $\textrm{Data}(k,x)$ and one cannot compute $\textrm{Data}(k,x)$ unless one verifies that $\textrm{Data}(k,x)\in D$). Of course, the fact that $\Psi(\mathcal{C})$ is more difficult than $\mathcal{C}$ presents some new concerns. For a useful problem, it is most likely the case that one would want to store the pairs $(k,x)$ where $(k,x)\in D$ in some database. However, in order to receive the block reward, the miner must only reveal a pair $(k,x)$ where $(k,x)\in D$ and $H(k||x||\textrm{Data}(k,x))<C$ instead of all the pairs $(k,x)\in D$ regardless of whether $H(k||x||\textrm{Data}(k,x))<C$ or not. One possible solution to this problem is for the miners to simply reveal all pairs $(k,x)$ where $(k,x)\in D$ out of courtesy. Miners will also have the ability to reject chains if the miners have not posted their fair share of pairs $(k,x)\in D$. Perhaps, one should count the number of pairs $(k,x)\in D$ for the calculation as to who has the longest valid chain as well. If most of the miners post their solutions, then the process of solving $\Psi(\mathcal{C})$ will produce just as many solutions as the process of solving $\mathcal{C}$. In the scenario where the miners post all of the pairs $(k,x)\in D$, $\Psi(\mathcal{C})$ would satisfy the spirit of conditions 2-3. 

The question should not be if there is a measure, but rather which measure is most appropriate. That depends on the problem you're trying to solve. 

If you're overwhelmed by all of the work that's out there, why don't you start out pretending that nobody's worked on the problem before? If your goal is to eventually build a competitive SAT solver, it's going to be a fairly long journey. By starting out just playing around without 'checking the solutions', so to speak, you have more to gain than to lose. First build the simplest solver you can and make sure it works. This will probably be a brute force algorithm whose running time depends more or less only on the $n$, the number of variables, and $m$, the number of clauses. Then implement something a little bit smarter like branch-and-bound. Write (or find) a generator that will give you random instances for given values of $n$ and $m$. Do some benchmarking tests for your branch-and-bound solver. See how it does for varying values of $n$ and $m$. Then improve your solver to make it faster. See how far you can get without reading about other work. When you run out of ideas, do some of the reading suggested in the other answers. 

$\textbf{Other Advantages of this technique:}$ The SLT offers other advantages than conditions 1-4 which are desirable or necessary for a proof-of-work problem. 

The following simple technique which I call the solution lottery technique (SLT) can be used in conjunction with other techniques (such as having multiple POW problems, the technique mentioned in Noah Stephens-Davidowitz's answer, etc) to help transform computational challenges into viable proof of work problems. The SLT helps ameliorate issues with cryptocurrency mining problems other than conditions 1-4. Suppose that $\mathcal{C}$ is a computational challenge of the form “find a suitable hash $k$ along with a string $x$ such that $(k,x)\in D$.” Problem $\Psi(\mathcal{C})$ setup: Suppose that $D$ is a set, $H$ is a cryptographic hash function, and $C$ is some constant. Suppose furthermore that $\textrm{Data}(k,x)$ is a piece of information that is easy to obtain after one determines that $(k,x)\in D$ but which cannot be obtained otherwise. Problem $\Psi(\mathcal{C})$ objective: Find a pair $(k,x)$ such that $k$ is a suitable hash and where $(k,x)\in D$, and where $H(k||x||\textrm{Data}(k,x))<C$. Let us now investigate how problem $\Psi(\mathcal{C})$ satisfies requirements 1-4. 

Mining pools are more feasible: In cryptocurrencies, it is often very difficult to win the block reward. Since the block rewards are very difficult to win, miners often mine in things called mining pools in which the miners combine their resources in solving a problem and in which they share the block reward in proportion to the amount of “near misses” they have found. A possible issue for $\mathcal{C}$ is that it may be difficult to produce a qualitative notion of what constitutes as a “near miss” for the problem $\mathcal{C}$ and the algorithm for finding a near miss may be different from the algorithm for solving $\mathcal{C}$. Since the pool miners will be looking for near misses, they may not be very efficient at solving $\mathcal{C}$ (and hence, few people will join mining pools). However, for $\Psi(\mathcal{C})$, there is a clear cut notion of a near miss, namely, a near miss is a pair $(k,x)$ where $(k,x)\in D$ but where $H(k||x||\textrm{Data}(k,x))\geq C$, and the algorithm for finding near misses for $\Psi(\mathcal{C})$ will be the same as the algorithm for finding solutions to $\Psi(\mathcal{C})$. Progress freeness: A proof-of-work problem $P$ is said to be progress free if the amount of time it takes for an entity or group of entities to find next block on the blockchain follows the exponential distribution $e^{-\lambda x}$ where the constant $\lambda$ is directly proportional to the amount of computational power that entity is using to solve Problem $P$. Progress freeness is required for cryptocurrency mining problems in order for the miners to receive a block reward in proportion to their mining power to achieve decentralization. The SLT certainly helps mining problems achieve progress freeness. 

This is the well-studied problem of Nearest Neighbour Search. There is not one 'best solution' to the problem---you'll want to choose trade offs based on the input and requirements. Do you need the creation of the data structure to be parallel? Or just the queries? Depending on how many thousands of points you're talking about, if your set of points is static it might not be impractical to do something naive like: 

What you're talking about is, I believe, called the Capacitated Facility Location Problem. Searching for that should give you plenty of papers to read. It appears the problem is NP-hard, but in Euclidean space a simple search procedure gives you a 6-approximation [PDF]. 

The vertex/vertex guarding problem is APX-hard for simple polygons (Eidenbenz (1998)). The best algorithms for the art gallery problem for simple polygons are based on building small $\varepsilon$-nets. In simple polygons, the range spaces induced by visibility polygons have constant VC-dimension. Circles do as well. Therefore a range space induced by visibility polygons in a simple polygon, circles, and the intersections and unions thereof, will also have constant VC-dimension and you can get a $O(\log(\rm{opt}))$-approximation algorithm. I thought about this problem a little for my thesis, but came to the opinion that there weren't any particularly interesting variants that didn't seem to reduce fairly closely to a known problem involving single-guarding. 

Improving the security/efficiency balance: The SLT will help in the case that $\mathcal{C}$ may be too easy to solve or too difficult to verify. In general, $\Psi(\mathcal{C})$ is much more difficult to solve than $\mathcal{C}$, but $\Psi(\mathcal{C})$ is about as easy to verify as $\mathcal{C}$. Removal of a broken/insecure problem: The SLT could be used to algorithmically remove bad POW problems in a cryptocurrency with a backup POW-problem and multiple POW problems. Suppose that an entity finds a very quick algorithm for solving problem $\mathcal{C}$. Then such a problem is no longer a suitable proof-of-work problem and it should be removed from the cryptocurrency. The cryptocurrency must therefore have an algorithm that removes $\mathcal{C}$ from the cryptocurrency whenever someone has posted an algorithm that solves problem $\mathcal{C}$ too quickly but which never removes problem $\mathcal{C}$ otherwise. Here is an outline of such a problem removal algorithm being used to remove a problem which we shall call Problem $A$. 

a. Alice pays a large fee (the fee will cover the costs that the miners incur for verifying the algorithm) and then posts the algorithm which we shall call Algorithm K that breaks Problem $A$ to the blockchain. If Algorithm K relies upon a large quantity of pre-computed data $PC$, then Alice posts the Merkle root of this pre-computed data $PC$. b. Random instances of Problem A are produced by the Blockchain. Alice then posts the portions of the pre-computed data which are needed for Algorithm K to work correctly along with their Merkle branch in order to prove that the data actually came from $PC$. If Alice's algorithm fed with the pre-computed data $PC$ quickly, then the problem is removed and Alice receives a reward for posting the algorithm that removes the problem from the blockchain. This problem removal procedure is computationally expensive on the miners and validators. However, the SLT removes most of the computational difficulty of this technique so that it can be used if needed in a cryptocurrency (instances which this technique is used will probably be quite rare). 

As you asked it, the question about the expected length (given the height) does not make sense without a prior distribution on the length of the string. You should instead consider the number of times you get tails before you get $h$ heads in a row, since this will give you the number of descendants of a node of height $h$ in a skip list. Let's represent this value with the random variable $X=X(h)$. When we start, or right after we get tails, the probability of starting and finishing a run of $h$ or more heads before getting tails again is $2^{-h}$. If we hit tails before getting $h$ heads in a row, we're back to square one. Thus $X$ is actually distributed as $\mathrm{Geometric}(2^{-h})$ and we have $\mathrm{E}(X) = (1-2^{-h})2^h$. Edit: Sorry, this gives you the expected number of towers in the left subtree. The number of nodes in the left subtree will be of the same order of magnitude though, since towers in between the first and last will have geometrically distributed height with expected value 2. Also, if you want to consider the right subtree as well, which probably makes more sense, you simply go until you get $h+1$ heads in a row instead of just $h$. In this case you get the number of descendants as defined in Devroye's paper that you linked to.