Edit: As Dömötör Pálvölgyi points out in comments, taking $k=1$ and $R$ to be the usual ordering on rationals seems to yield a counterexample. First, the set $A$ cannot be empty, as $R[A]$ is then also empty and $A$ would then have to contain 0 by the cube condition, a contradiction. If the non-empty set $A$ has an infimum then it cannot contain any rationals greater than this, so it must be a singleton, which contradicts the upper shift condition. If on the other hand $A$ has no infimum then $R[A] = Q$ so $A$ must be empty, a contradiction. Any comments on whether there are any hidden non-obvious definitional problems, such as perhaps an implicit nonstandard model of the rationals? Further edit: The argument above is roughly correct, but is wrong in the application of the upper shift. This operator only applies to non-negative coordinates, so setting $A$ to be any negative singleton set yields a fixed point, as desired. In other words, if $m < 0$ then $A = \{m\}$ is a solution, and there are no other solutions. 

For completeness, recall that a literal is either a variable or its negation, and that $n$-variable SAT consists of formulas built from at most $n$ variables (we can fix the set of variable names, e.g. $\{1,2,\dots,n\}$), in conjunctive normal form (a conjunction of disjunctions), where each disjunction of literals is known as a clause. Horn-SAT$_n$ is the fragment of $n$-variable SAT where each clause in an input formula contains at most one non-negated variable. An equivalent counting formulation of the question is: how many different values can #Horn-SAT$_n$ have? However, note that I am not interested here in the complexity of #Horn-SAT, just its range of values for each number of variables. 

Computing the core of a graph is hard: even deciding if a given 3-colourable graph is a core is co-NP-complete, see Hell and Nesetril. There are settings where core computation can be done efficiently, see Efficient Core Computation in Data Exchange by Georg Gottlob and Alan Nash for a database setting; here some reasonable restrictions on the kinds of constraints in the database schema allow cores to be computed efficiently. Edit: Other than the Gottlob/Nash work mentioned above, I am not aware of any other attempts to provide efficient algorithms for core computation. Pointers to any algorithms better than brute force (exact or otherwise) would be welcome. 

Edit: I am leaving this answer as is (for now) to illustrate the messy process of proving theorems, something that is left out of published papers. The core intuition here is that it is enough to focus on the top ball, as it sweeps away all below it. Please see the comments (in particular @Michael pointing out that gaps can occur) and @Joe's later answer for how errors were identified and corrected. I especially like Joe's use of experiments to double-check that the formulas were sensible. 

If only one pass is allowed, the input data can be stored as a partition of the set of vertices, merging sets if an edge is seen between vertices in two different sets. This clearly requires at most $O(n\, \log\, n)$ space. My question is about $k > 1$: how can one use more passes to reduce the required space? (For avoidance of triviality, $k$ is a parameter that cannot be bounded a priori by a constant, and the space bounds are expressions involving functions of both $n$ and $k$.) 

I'm not altogether convinced by the previous work on open and interactive constraints. An attempt to study the tractability questions was: 

(KN 2.10) Is it true that $D(f) \le O(\log \chi(f))$? Here $\chi(f)$ is the minimal number of $f$-monochromatic rectangles into which the input space $X \times Y$ can be partitioned. $D(f) \ge \log \chi(f)$ was shown by Yao in 1979, and $D(f) \le O((\log \chi(f))^2)$ was shown by Aho/Ullman/Yannakakis in 1983. See also KN Theorem 2.11 and the historical discussion in the bibliographic notes to Chapter 2. (Log-rank conjecture, KN 2.20) Is it true that $\log\chi(f) \le (\log rk(M_f))^{O(1)}$ for every Boolean function $f$? Here $M_f$ is the communication matrix of $f$, and $rk(M)$ denotes the rank of matrix $M$. Is it true that $R(f) \le poly(Q(f))$? Here $R(f)$ denotes the bounded-error communication complexity of $f$ and $Q(f)$ the quantum communication complexity. Note that $Q(f) \le R(f)$ by definition. Prove that $D(IP^k_n) \ge n^\epsilon$ for, say, $k = \lceil (\log n)^2\rceil$ and some fixed constant $\epsilon > 0$. Here $IP^k_n$ denotes the inner product function with respect to $k$-party NOF protocols. 

In an instance with $n$ variables, clearly a prop of cardinality $n$ will be a solution. There may exist large props, with cardinality up to $n-1$, that are not contained in any solution. In the terminology you propose, the set of props with maximum cardinality $k$ forms the penumbra, perhaps even with some additional leeway $d$ (so cardinality at least $k-d$). The second part of your question also appears highly interesting, but I am not aware of any work related to it. 

Noble actually shows much more. Suppose the class of graphs under consideration has treewidth at most $k$. Then the weighted graph polynomial $U$, which evaluated at a certain point yields the number of independent sets of a graph, can be evaluated in $O(a_k n^{2k+3})$ operations at any point. Here $a_k$ depends only on $k$ and $n$ is the number of vertices in the input graph. Hence there is a polynomial-time algorithm to count the number of vertex covers on trees (or any other class of graphs of bounded treewidth). 

Rephrasing as a set system, each row represents a subset $E_i$ of some set $X$, for $i=1,2,\dots,m$. You want a set $Y \subseteq X$ with at most $k$ elements, such that $E_i \cap Y \ne \emptyset$ for each $i$. In other words, you want a hitting set of size at most $k$; this problem is NP-complete. 

One problem with minor-closed graph properties is that they are "small"; excluding even one minor excludes lots of graphs. This is perhaps one reason Robertson-Seymour structural decomposition works: there are few enough remaining graphs for them to have a nice structure. 

One candidate might be k-ISOLATED SAT, which requires finding a solution to a CNF formula with no other solutions within Hamming distance k. However, proving the lower bound seems is tricky, as usual. It is obvious that checking a Hamming k-ball is clear of potential solutions "should" require $\Omega(n^k)$ different assignments to be checked, but this is by no means easy to prove. (Note: Ryan Williams points out this lower bound for $k$-ISOLATED SAT would actually prove P ≠ NP, so this problem does not seem to be the right candidate.) Note that the theorem holds unconditionally, regardless of unproved separations such as P vs. NP. An affirmative answer to this question would therefore not resolve P vs. NP, unless it has additional properties like $k$-ISOLATED SAT above. A natural separation of NTIME would perhaps help to illuminate part of the "difficult" behaviour of NP, the part which derives its difficulty from an infinite ascending sequence of hardness. Since lower bounds are hard, I will accept as an answer natural languages for which we may have a good reason to believe a lower bound, even though there may not yet be a proof. For instance, if this question had been about DTIME, then I would have accepted $f(k)$-CLIQUE, for a non-decreasing function $f(x) \in \Theta(x)$, as a natural language that probably provides the required separations, based on Razborov's and Rossman's circuit lower bounds and the $n^{1-\epsilon}$-inapproximability of CLIQUE. (Edited to address Kaveh's comment and Ryan's answer.) 

b-perfect graphs allow induced 5-cycles (unlike perfect graphs), and were shown to have a polynomial-time algorithm for colouring by Hoàng, Maffray, and Mechebbek, A characterization of b-perfect graphs, arXiv:1004.5306, 2010. (It is a pity that the wonderful compendium of graph classes at the ISGCI only covers cliquewidth, independent set, and domination. It does not include information about colouring.) 

This is not a complete answer, since it does not deal with the "typical" complexity of database queries, but even with worst-case analysis there are easy queries. 

Unfortunately I don't know any up to date general textbooks or surveys covering this fast-moving field. I have found two older surveys useful. First, 

If $p$ is constant, then the size of the maximum clique in the $G(n,p)$ model is almost everywhere a constant multiple of $\log n$, with the constant proportional to $\log (1/p)$. (See Bollobás, p.283 and Corollary 11.2.) Changing $p$ should therefore not affect the hardness of planting a clique with $\omega(\log n)$ vertices as long as the clique is too small for an existing algorithmic approach to work. I therefore expect that with constant $p \ne 1/2$ the hardness of Planted Clique should behave just like the $p=1/2$ case, although it is possible that the case of $p$ very close to 0 or 1 might behave differently. In particular, for $p\ne 1/2$ the same threshold of $\Omega(n^{\alpha})$ for $\alpha = 1/2$ for the size of the planted clique applies, above which the problem becomes polynomial-time. The value of $\alpha$ here is $1/2$ (and not some other value) because the Lovász theta function of $G(n,p)$ is almost surely between $0.5\sqrt{(1-p)/p}\sqrt{n}$ and $2\sqrt{(1-p)/p}\sqrt{n}$, by a result of Juhász. The algorithm of Feige and Krauthgamer uses the Lovász theta function to find and certify a largest clique, so it relies on this threshold size for the planted clique. Of course, there may be a different algorithm that does not use the Lovász theta function, and that for values of $p$ far from $1/2$ can find a planted clique with say $n^{1/3}$ vertices. As far as I can tell this is still open. Feige and Krauthgamer also discuss when $p$ is not constant but depends on $n$, and is either close to 0 or close to 1. In these cases other approaches exist to find planted cliques, and the threshold size is different. 

Since the quantification is over an infinite set of structures $T$, this may be undecidable. It is known to be NP-hard [3] as a special case of a more general question over positive semirings (the non-negative integers with addition and multiplication is a semiring); $\Pi^P_2$-hardness was claimed two decades ago but remains unclear. If instead of conjunctive queries, slightly more general queries are allowed, then the problem does become undecidable, via reductions from Hilbert's 10th problem [4,5]. What makes this question interesting is that for most positive semirings of interest the general question is decidable, and actually in $\Pi_2^P$. In fact, for the Boolean semiring case the question becomes: given two conjunctive queries, is it always true that when the first query has an answer then so does the second? Ashok Chandra and Philip Merlin showed in 1977 [6] that this is equivalent to checking whether there exists a homomorphism between the queries, which is in NP. Moreover, in typical databases the queries are usually small or even fixed, while the data is large and changes frequently. This means that even brute force search for a homomorphism between the input queries may be worthwhile. So it might be a good idea to look quite closely at two small fixed conjunctive queries to decide which is the better one to use. Yet we don't know if such queries can be compared based on the number of answers they generate. 

I am not sure there is more work along these lines. As with many other areas with the potential to separate P from NP, after some promising early progress new ideas now appear to be necessary. 

Here are two applications from a different part of TCS. Semirings are used for modelling annotations in databases (especially those needed for provenance), and often also for the valuation structures in valued constraint satisfaction. In both of these applications, individual values must be combined together in ways which lead naturally to a semiring structure, with associativity and the one semiring operation distributing over the other. Regarding your query about modules, neither monoid has an inverse in these applications, in general. 

Let the solution space of a SAT instance be the set of Boolean vectors of satisfying assignments of $\{0,1\}$ to the variables (that result in the formula evaluating to TRUE). In other words, a solution space represents all the solutions of an instance. There are at most $2^n$ solutions, so $n$-variable SAT (denoted here by SAT$_n$) can represent at most $2^{2^n}$ different solution spaces. Moreover, it is easy to see that this number can actually be reached, by considering instances where every clause contains all $n$ variables. We know that Horn-SAT is less expressive than SAT, for instance there is no Horn-SAT$_2$ representation of the solution space of $a \lor b$, which contains the 3 vectors $01, 10, 11$. Hence Horn-SAT$_n$ must be able to represent fewer than $2^{2^n}$ solution spaces. 

Immerman showed (doi:10.1137/0216051) that a version of directed reachability in which the desired path (but not the graph itself) is deterministic, is complete for L under first-order reductions, which are computable by AC$^0$ circuits. This might then perhaps be adapted to show that USTCONN is complete for L under FO-reductions. However, although AC$^0$ is strictly contained in L, AC$^0$ is again a circuit class and I am not aware of any way to perform FO-reductions in sublogarithmic space. 

The biggest "advance" in relational databases has been the cleaving apart of the monolithic RDBMS model into discrete components, that are then put together in novel ways. These include data stores that have weak consistency (Google Percolator), column stores (NoSQL), and graph databases. The ideas are not new, but the different ways of combining the components are novel. 

I realise many questions related to CSLs are undecidable (for instance deciding if a given CSG generates the empty language). Even given a CSG for SAT one would still have to overcome the obstacle that deciding membership in the language given by a CSG is PSPACE-complete in general. But it might be the case that the membership problem for the CSG that defines SAT is in NP, due to some special structure of the language. Rephrasing, to address comment by MCH: But it might be the case that the membership problem for the CSG that defines SAT could be shown to be in NP due to some special structure of the grammar, and not because we already know it must be in NP. 

Immunity seems to make your problem easier in one sense, as it means that infection can be modelled by a process that terminates after a linear number of infection attempts. The stipulation that the probability of infection does not depend on the neighbours of a vertex also helps. However, the model does not seem easy to analyse: the immune vertices in effect are removed from the graph, so a vertex may effectively become unreachable. The infection, immune, and unreachable probabilities depend on the intermediate graphs created by the infection process. It might be useful to start with cliques, paths, and cycles with a single initially infected vertex. Cliques are trivial. For a path with one infected endpoint the probability that the other endpoint becomes infected at some stage is the product of the probabilities of infection of all the intermediate vertices. For a cycle, the probability depends on both paths; the longer path becomes relevant if an immune vertex appears on the shorter path. Trees also have a single path between any two vertices, so can be handled as for the path case. It might then be feasible to extend the analysis to $k$-connected graphs and regular grids, though this already seems quite tricky. 

I am interested in "hard" individual instances of NP-complete problems. Ryan Williams discussed the SAT0 problem at Richard Lipton's blog. SAT0 asks whether a SAT instance has the specific solution consisting of all 0's. This got me thinking about constructing SAT instances that are likely to be "hard". Consider a SAT instance $\phi$ with $m$ clauses and $n$ variables, where $\alpha = m/n$ is "large enough", in the sense that it falls into the region beyond the phase transition, where nearly all instances are unsatisfiable. Let $x$ be a random assignment to the values of $\phi$. 

The USFP Theorem seems to be a $\Pi^1_1$ statement, so it might be "close enough" to computability (such as checking non-isomorphism of automatic structures), to impact theoretical computer science. For completeness, here are the definitions from Friedman's MIT talk from November 2009 (see also the draft book on "Boolean Relation Theory"). $Q$ is the set of rational numbers. $x, y \in Q^k$ are order equivalent if whenever $1 \le i,j \le k$ then $x_i \lt x_j \Leftrightarrow y_i \lt y_j$. When $x \in Q^k$ then the upper shift of $x$, denoted $\text{us}(x)$, is obtained by adding 1 to every non-negative coordinate of $x$. A relation $A \subseteq Q^k$ is order invariant if for every order invariant equivalent $x,y \in Q^k$ it holds that $x \in A \Leftrightarrow y \in A$. A relation $R \subseteq Q^k \times Q^k$ is order invariant if $R$ is order invariant as a subset of $Q^{2k}$, and is strictly dominating if for all $x,y \in Q^k$ whenever $R(x,y)$ then $\max(x) \lt \max(y)$. Further if $A$ is a subset of $Q^k$ then $R[A]$ denotes $\{ y | \exists x \in A R(x,y)\}$, the upper shift of $A$ is $\text{us}(A) = \{\text{us}(x) | x \in A\}$, and $\text{cube}(A,0)$ denotes the least $B^k$ such that $0 \in B$ and $A$ is contained in $B^k$. Let $\text{SDOI}(Q^k,Q^k)$ denote the set of all strictly dominating order invariant relations $R \subseteq Q^k \times Q^k$.