I would guess that you have indexes that are unusable. Check and see if you have unusable indexes. If you do, then you need to rebuild them. 

Is possible that for the 10g client you either have a the date format set in a local login.sql file or set in the $ORACLE_HOME/sqlplus/admin/glogin.sql. If you can find this date formatting, then you should set it the same for your 11g client and it should work. Essentially, when you put in a date string Oracle look at the nls_date_format to see how a date is formatted by default. If that is different than how you have it in a sting, Oracle might have an issue handling the date. you can also use the to_Date funtion to force a particular date format to work. 

Since it is a database you can create a table with key and value. You can create a package that has a CONSTANT which points to a key. You can create a function that takes a key and returns a value from your key/value table. Then you would just look up and use that values from the key/value table and rather than changing code you change data. 

It looks like a permission issue. As Raj pointed out make sure that export_user has select privilege for the accounts table. Which it will have if it owns the table. Also make sure that it has select, insert, update and delete for MIKETESTCONSOLE_OWNER.ACCOUNTDEPLOYMENTLINKS and all of the tables in that schema. If tables are added over time, you probably need to add grants. 

When you install Oracle there are a set of registry entries under hkey_local_machine, software, oracle. If you don't have the registry, and don't want the Oracle home, then you can just delete the directories. There is an Oracle directory under program files, which stores your Oracle Inventory, and your Oracle Home where ever you put that. 

I wonder if there is a modal window that is under your current installer window that is waiting for you to click finished/done... 

Personally I would schule it using the Windows Scheduler. You might want to get robocopy if you are copying your backup files to another server. Having an email client that you can send emails from DOS can be helpful. But otherwise you would just write .BAT files and or a Powershell file. You want a process that creates a backup log and checks that log for errors. Then you can send out emails when there are errors. There should be something already written online somewhere. 

If you have advanced partitioning and can partition by date range and sub partition by charge, then you can reduce the amount of logical IO required to get to your data, provided that you are doing date range based searches that mostly use charge to filter. If you don't have advanced partitioning, you can still partition it your self, but will have a higher level of complexity to replicate what Oracle will sell you. As with anything look at the trade offs and see what make sense. 75 million rows is a lot, but not unheard of and Oracle should certainly be able to manage that amount of data. Ideally you want to avoid full table scans of 75 million records given the high cost in terms of logical IO. You also want to partition so that you don't have global indexes if at all possible. 

If it was me, I would partition by hour, then you can just drop 24 partitions every day. That is only 744 partitions per month. Then if you query within a relatively short window, you will look at much less data. You may also want to look at your index strategy. My guess is that the cost of maintaining the indexes is too high. IMHO, smaller partitions should help speed up the queries and make it easier to maintain the partitions. 

It looks like you need to write dynamic SQL. Dynamic SQL is SQL the creates SQL that can get run. Here is a simple example of dynamic SQL. 

Find a small recordset and see how many loops your code does. How many does it do for 10, 100 and 1,000 records? You will see that the number of loops increases exponentially as the number of rows increases. My rough guess is that you have asymptotically big O(2^n+2^n+2^n). Asymptotics is a way of estimating work where the amount of rows processed is not known. Can't you just write some minus selects that are union all joined together? My example is looking at indexes and I use different filters so that the data sets will not be the same. Hence you will see the difference. 

I use set events 10046, for the explain plan and 10053 for the waits. Once you run the queries that you want timings for, assuming that you are using 10g/11g/12c go to the diag trace folder, find your trace files and run tkprof against the trace files. 

Also data pump can run in parallel, and has the option to append data, truncate tables or replace table. In addition you can change table spaces when you import. It is a better utility in every way than the older exp/imp tool. 

And now you have a working copy of the database. You want to make sure that you don't use the newest controlfile available. You want to make sure that the controlfile is older than the newest archivelog that you have available. 

One option would be to use something like Erwin or Toad to reverse engineer the schema, create a new 11g/12c database with all of the tablespaces that you need, generate a script that would create all of the schemas and their types and tables without constraints, create the schemas and tables, create a db link to the old database from the new database, do insert into for each table, generate and run a script to create all of the constraints, views and stored procedures. You will want to script it and run the conversion more than once while you work out the details. You also want to check your stored procedures for any code for set transaction commands. As you probably know rollback segments went away with 9i, thus you can no longer set certain code to use a particular rollback segment. You will also be switching from dictionary managed table spaces to locally managed table spaces. Which means that you want to ignore any segment information in your tables. Just use automatic storage for your segments and let the database track extent size and make sure that pctincrease is set to 0. In addition check your block size 8k is usually good for most applications. If you have a lot of data, you can split the inserts into multiple scripts that run from different sessions, thus you can do it in parallel. While you can do an export/import you don't want to import the data dictionary from 8i and you don't want to keep the old extent sizes. 

Here is a url that explains set events. The two events that you want are 10046 and 10053. Oracle TKPROF & SQL_TRACE You can generate better data on how your query is running by doing the following. Using sqlplus turn on tracing, set the trace file identifier so that you can find your trace file. 

Rather than putting the data in a case statement where you might have to repeat that case statement in more than one query, you could create a table with static data, then joint to that table. The advantage would be that you can change the data in the table easier than changing all of the queries that have that case statement. 

If you are creating comma delimited files, you should probably use double quote and comma delimited. Excel can read the files as is, and you are less likely to have the data in the database mess up the format. I have also used the tilde ~ as a delimiter. It doesn't get used very often so it tends to be safe to use. 

Not likely. It is possible that your procedure is being run more than you realize? How does the procedure get run? 

It seems like the obvious answer is that you can't stop a shutdown that is happening, but you can do either a shutdown immediate or shutdown abort from a different session if you did a shutdown normal. As long as you have enough online redo log groups and each group has a log that is large enough, then you can recover from either a shutdown immediate or shutdown abort. If you ask someone what is the exact risk that someone faces for doing a shutdown abort, they may struggle to answer you. On shutdown immediate transactions are rolled back as sessions are killed. On shutdown abort, all transactions are rolled back when the database is started up. Hence the same thing should be happening in both cases, just in a different order. Once you start a shutdown normal, either ask everyone to log out, or just do shutdown immediate in a different session. Either way, Oracle won't allow a database to open, if it isn't in a consistent state. 

Another possibility is that you have cases where two triggers are firing on different tables and they are each trying to update the other table. You can tell this by looking at the trace log that gets produced from a dead lock and figuring out what objects and rows are locked, then check your triggers to see if any of your triggers are running that SQL. It would be helpful to know how long this has been happening and what changed around the time that this became an issue. 

Rather than using tnsnames, I have used openLDAP to do the names resolution. You can replicate across two nodes if you want. As soon as you change the entry in openLDAP every server has the new information. You can even get the resolution from an ssl encrypted port if you want. using openLDAP the name resolution