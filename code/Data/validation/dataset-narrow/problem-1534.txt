You can simply use the attribute to select the features with the highest importance score. So for example you could use the following function to select the K best features according to importance. 

Yes I suspect you are overfitting. When you build your first stage of models (nearest neighbors, random forest, gradient boosting, etc...) is the process like this? 

Doing things this way will emulate real life since your predictions will be gathered using models that did not include that data when training them. You can build your final model using all of your prediction variables as input. If you struggle with actually executing on this either edit your question with code or post a new question on how to run everything with a CV loop. 

Perform dimensionality reduction (such as LSA via ) on your sparse data to make it dense and combine the features into a single dense matrix to train your model(s). Add your few dense features to your sparse matrix using something like scipy's into a single sparse matrix to train your model(s). Create a model using only your sparse text data and then combine its predictions (probabilities if it's classification) as a dense feature with your other dense features to create a model (ie: ensembling via stacking). If you go this route remember to only use CV predictions as features to train your model otherwise you'll likely overfit quite badly (you can make a quite class to do this all within a single if desired). 

To me, this $-Q(s_t, a_t)$ term at the very end is redundant. ...If we set gamma to $0.8$, the future rewards will decay anyway. Yes, if instead we now set $\gamma=0$, then the $-Q(s_t, a_t)$ term will drag our value down - is there a case when it would be useful and what would the result be? 

So, at every timestep we are feeding in vector {1,1} and expect {0,1} at the output. Assume I perturb the weight inside LSTM, then perform 4 forward props - one for each timestep - how do I now get delta of the cost function that this single perturbation has caused? Am I allowed to simply add-up the change in Cost from all 4 timesteps to treat it as derivative estimate? 

pick an element. As seen, it contains stateFrom, taken action (action X), qvalue of action X, reward you received, state that it led to. Run forward prop as if you were in "next state" (mentioned in that element). Get its best [action Y, qvalue]. In fact, your action Y doesn't have to be the action with highest q-value. Instead, it could be the "epsilon-greedy decided action" from the "next state" - you would then have SARSA instead of Q-learning obtain delta by which the "qvalue of action X" differs from the immediate reward + "qvalue of action Y". get deltas of several other elements that you've chosen from the bank sum up all deltas, punish your network with them. 

From this you should see that the euclidean distances between the non-standardized versions are dominated by the third column because its range of values is much larger than the other two. However when the data is standardized this no longer becomes an issue and weights each feature as being equal when calculating the distance between each data point. 

This exact problem was a kaggle competition sponsored by Facebook. The particular forum thread of interest for you is the one where many of the top competitors explained their methodology, this should provide you with more information than you were probably looking for: $URL$ In general, it appears that most people treated the problem as a supervised one. Their primary feature was a tf-idf, or unweighted BOW, representations of the text and they ensembled 1000s of single-tag models. Owen, the winner of the competition, noted that the title text was a more powerful feature than the content of the body of the post. 

To get total error before back propagating - it is common to take an average of all the forward-pass errors. This is what's done in RNN such as LSTM. In the case of linear regression and logistic regression, The traditional Mean Squared Error Function can produce such a value. In essence, this value is represented by an average of errors: $Q(w) = 1/n{\sum_{i=1}^n Q_i(w)}$ Also, as a reminder, speaking of an actual backpropagation - from wikipedia: When used to minimize the above function, a standard (or "batch") gradient descent method would perform the following iterations: $$w:=w - {\eta}\nabla Q(w) = w - \eta{\sum_{i=1}^n}\nabla Q_i(w)/n$$ notice the $/n$ When used with the ${\sum_{i=1}^n}$ it results in the average of all gradients := means 'becomes qual to' $\eta$ is the learning rate 

I can't understand the meaning of $-Q(s_t, a_t)$ term in the Q-learning algorithm, and can't find explanation to it either. Everything else makes sence. The q-learning algorithm is an off-policy algorithm, unlike SARSA. The Bellman equation describes q-learning as follows: $$Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha\left[ r_t + \gamma \cdot argmax(Q(s'_t, a'_t)) - Q(s_t, a_t) \right] $$ "The q value for action $a$ taken in state $s$ at time $t$ becomes equal to: that same q-value plus small amount of: currently received reward (reward could be zero) with some amount $\gamma$ of the best Q-value available in the new state, minus our current value 

All three approaches are valid and have their own pros and cons. Personally, I find (1) to typically be the worst because it is, relatively speaking, extremely slow. I also find (3) to usually be the best, being both sufficiently fast and resulting in very good predictions. You can obviously do a combination of them as well if you're willing to do some more extensive ensembling. As for the algorithms you use, they can essentially all fit within that framework. Logistic regression performs surprisingly well most of the time, but others may do better depending on the problem at hand and how well you tune them. I'm partial to GBMs myself, but the bottom line is that you can try as many algorithms as you would like and even doing simple weighted ensembles of their predictions will almost always lead to a better overall solution. 

If I understand correctly, you essentially have two forms of features for your models. (1) Text data that you have represented as a sparse bag of words and (2) more traditional dense features. If that is the case then there are 3 common approaches: 

The second approach somehow seems more correct, because LSTM will have a hidden state ..Is this correct intuition or it won't matter? 

In the first case, I am able to get 4 loss-values, but in the second example, I only have 1 source of gradient, at _t3 My main concern is in first example, I demand LSTM to make prediction of 'b' and 'c' without supplying it enough previous context. It's fine for 'd' and 'e', but asking for answer at timestep 0 and 1 is a bit unfair? What would be best for this particular example? 

Edit: Please remember that cell state is erased between training batches, so LSTM will have a "hammer to head" at these times. It's unable to remember what was before OPQRSTUVWXYZ. This means LSTM is unable to ever learn that "O" follows the "M". So I thought (thus my entire question), why not to give it intermediate (overlapping) batch in between ...and in that case why not use multiple overlapping minibatches - to me this would provide a smoother training? Ultimately, that would mean a sliding window for an LSTM. 

Standardizing data is recommended because otherwise the range of values in each feature will act as a weight when determining how to cluster data, which is typically undesired. For example consider the standard metric for most clustering algorithms (including DBSCAN in sci-kit learn) -- , otherwise known as the L2 norm. If one of your features has a range of values much larger than the others, clustering will be completely dominated by that one feature. To illustrate this look at the simple example below: 

And clearly if you wanted to selected based on some other criteria than "top k features" then you can just adjust the functions accordingly. 

If so you are plugging in the same data you used to train the models into your models to get probabilities, which will lead to better probabilities than should be expected in real life and thus much better than should be expected results on your 2nd stage model (i.e.: overfitting). To remedy this you must do everything within a cross validation loop, that looks like this in pseudo code: