The bottom line is that it's hard. (Thanks to epsfooling for pointing me to this paper.) The literature on cryptographic pseudorandom number generators has lots more on this topic. 

If this solution is not acceptable, you need to think through your requirements more carefully and edit the question accordingly. 

Alternatively, we could solve your problem in a straightforward way using integer linear programming (ILP). In practice, ILP solvers are pretty good on many problems. However, their worst-case running time is still exponential, so this is not going to give an algorithm with polynomial worst-case running time. Let me know if you want me to elaborate on how to formulate this using ILP. 

I doubt that your first problem will have any general solution that is much more efficient than the naive approach of testing $x$ against each element of $S$, one by one. The naive approach has running time $\Theta(nd)$. If $x$ and each element of $S$ are distributed uniformly at random, then you'd expect about $n/2$ elements of $S$ to be orthogonal to $x$, so there doesn't seem to be much room for hope for a general solution whose running time is asymptotically better than $d \times n/2 = \Theta(nd)$. If you know something about the distribution on your vectors or if they have some structure, perhaps one could do something better in that specific setting -- but I wouldn't expect to see a generally applicable algorithm that doesn't make use of that kind of additional information. 

Let $p$ be a large prime. Let $A$ be a $2\times 2$ matrix with coefficients in $GF(p)$ (i.e., coefficients taken modulo $p$). Let $B=A^k$, where $k$ is an integer not given to us. Given $p$, $A$, and $B$, the problem to find some $k$ such that $A^k=B$. For which matrices $A$ does this problem have an efficient (poly-time) solution, and for which is it hard (say, as difficult as the discrete logarithm in $GF(p)$)? I realize that for some choices of $A$ this problem can be solved efficiently. For others, it is as hard as the discrete log problem modulo $p$, for which there is no known poly-time algorithm. Can we somehow classify which matrices $A$ make this easy and which make it hard? I can work out the answer for some special cases but am having a hard time getting the overall picture. In other words: When is there an efficient algorithm to solve the discrete logarithm problem, when working with $2\times 2$ matrices over $GF(p)$? Follow-up question: is there a neat way to generalize from $2\times 2$ matrices to $n\times n$ matrices? 

There is a straightforward solution, using known techniques for obfuscating point functions. Background. First, go read the following papers: 

There's a straightforward algorithm for your original problem, based on a linear scan through the array with two pointers, one to $a$ and the other to the largest $b$ such that $|a-b|<c$. The running time will be linear in the number of pairs $a,b$ that need to be output. So, this answers your question in the positive: if $c$ is small enough that the number of such pairs is $O(n)$, then the running time will be $O(n)$. 

One simple approach: Use a Bloom filter. Alice can construct a Bloom filter for the set of strings she has, and then send it to Bob. The Bloom filter will have size $O(cn)$ bits, and Bob's error probability is exponentially small in $c$. In practice, you can take $c$ to be a small constant (e.g., 100). 

Given such a data structure, it is easy to see how to process the $x$'s in a streaming way. When we receive $x_{n+1}$, we first set $y_{n+1} = \text{find}(x_{n+1})$ and then execute $\text{append}(x_{n+1})$ to update the data structure. We iteratively repeat this for each $x$-value we receive. I'm going to show a data structure that supports each of these two operations in $O(\lg n)$ time. This shows that we solve this problem, for the sequence $x_1,\dots,x_n$, in $O(n \lg n)$ time. 

This certainly follows for $n=1$. Fix any algorithm $A$ that attempts to distinguish, and let $p_{a,b} = \Pr[A(\langle a, C(b) \rangle)=1]$, where $a,b\in \{0,1\}$. Now by the assumption that $C$ is statistically hiding, we have $p_{0,0} \approx p_{0,1}$ (for otherwise you could prepend the fixed constant 0 to any commitment and then use $A$ to break the hiding property) and we have $p_{1,0} \approx p_{1,1}$ (for a similar reason). Expanding out the definitions, we have $\Pr[A(X)=1]=(p_{0,0}+p_{1,1})/2$ and $\Pr[A(Y)=1]=(p_{0,0}+p_{0,1}+p_{1,0}+p_{1,1})/4$ (where $X,Y$ are samples from your distributions ${\cal X},{\cal Y}$). But from the facts above, we have $2p_{0,0} + 2p_{1,1} \approx p_{0,0} + p_{0,1} + p_{1,1} + p_{1,0}$, i.e., $\Pr[A(X)=1]\approx \Pr[A(Y)=1]$, and thus $X,Y$ are statistically indistinguishable. I think from this you should be able to see how this generalizes to $n>1$ as well. Basically, you note that $$\Pr[A(X)=1]=1/2^n\sum_b \Pr[A(\langle b,C(b)\rangle)=1]\approx 1/2^{2n}\sum_{a,b} \Pr[A(\langle a,C(b)\rangle)=1] = \Pr[A(Y)=1],$$ using the fact that $\Pr[A(\langle a,C(b))\rangle)=1] \approx \Pr[A(\langle a,C(b')\rangle)=1]$ for all fixed $a,b,b'$ (otherwise you could distinguish $C(b)$ from $C(b')$ by simply prepending $a$ and then running $A$). 

Yes, this can be solved in polynomial time. Here is an algorithm. To rephrase the problem statement: 

Note that as far as practical consequences go, Impagliazzo says it doesn't matter much whether we have $\mathsf{P} = \mathsf{NP}$ or $\mathsf{NP} \subseteq \mathsf{BPP}$ (he even calls the latter the "moral equivalent" of $\mathsf{P} = \mathsf{NP}$). See also What if P = NP? for additional practical consequences (assuming the constants hidden by asymptotic notation are not too large). I don't know whether there are any surprising implications for complexity classes. 

If you want bounds for a fixed $k$ (rather than an asymptotic expression/formula that works for all $k$), one approach might be to use random sampling: repeatedly choose a random coloring, check whether it meets your criteria, and count how many of the trials were successful. This gives you an estimate of the fraction of colorings that meet your criteria. This can be converted into a rough estimate of the total number of colors that meet your criteria (just multiply by $k^{mn}$). You can then use a Chernoff bound to get upper and lower bounds on the number of colorings that meet your criteria, where these bounds hold with probability $\ge 1-2^{-100}$ (taken over the random trials). In other words, you would have to be extremely unlucky in your choice of random trials for those bounds to be wrong. 

Also, it's useful to understand that standard encryption schemes do not hide the length of the message. Therefore, when defining the security of a cryptosystem, our theorems often essentially restrict consideration to a set of messages of fixed length (all possible messages have the same length) and show that the cryptosystem does not give the attacker any clue which of them were sent. So, in some sense, when we go to prove our security theorem, we are effectively forced to work with a finite message space, due to the fact that the length is fixed and known to the attacker. This is fundamental: it's not realistically possible to build a cryptosystem that prevents the attacker from learning something about the length of the message. 

When $p \gg \epsilon$, the justification is easy to work through by hand. With $n$ observations, the number of heads is either $\text{Binomial}(n,p)$ or $\text{Binomial}(n,p+\epsilon)$, so you want to find the smallest $n$ such that these two distributions can be distinguished. You can approximate both of these by a Gaussian with the right mean and variance, and then use standard results on the difficulty of distinguishing two Gaussians, and the answer should fall out. The approximation is fine if $p \ge 5/n$ or so. In particular, this comes down to distinguishing $\mathcal{N}(\mu_0,\sigma_0^2)$ from $\mathcal{N}(\mu_1,\sigma_1^2)$ where $\mu_0 = pn$, $\mu_1 = p+\epsilon)n$, $\sigma_0^2 = p(1-p)n$, $\sigma_1^2 = (p+\epsilon)(1-p-\epsilon)n$. You'll find that the probability of error in the optimal distinguisher is $\text{erfc}(z)$ where $z = (\mu_1-\mu_0)/(\sigma_0+\sigma_1) \approx \epsilon \sqrt{n / 2p(1-p)}$. Thus, we need $z \sim 1$ to distinguish with constant success probability. This amounts to the condition that $n \sim 2p(1-p)/\epsilon^2$ (up to a constant factor)... when $p \gg \epsilon$. For the general case... see the paper. 

There is no relationship. They both use the word "free", but with different meanings of the word "free". It's just an accidental collision, which will happen when you have a language like English with a fixed number of words and the number of concepts we want to talk about exceeds the number of words in the language. A free group is, roughly, a group that is about as generic as possible (it has no special structure). Here, the word free means "having no non-trivial strucutre", i.e., free of special equalities. In contrast, a free theorem about some code (e.g., a function) is one that follows automatically from the type signature of that code. Here, the word free means "at no cost" or "with no extra reasoning about the code required"; it's an automatic consequence of the type signature, regardless of the implementation/code of the function. 

As Igor Shinkar explains, no Turing-complete language can satisfy your criteria. This sharply limits the expressibility of any such language. On the other hand, regular expressions would be one example of a language that does satisfy your condition, since it is decidable to find a minimal regular expression (or minimal DFA) that is equivalent to a given regexp. (However, there is no guarantee that the normal form can be computed efficiently; in the worst case, it might take exponential time.) Another example would be boolean circuits: it is decidable to find a minimal boolean circuit that is equivalent to a given circuit. (However, the normal form cannot be computed efficiently.) 

You can encode a pair (or triple, etc.) of integers as a single integer, using any number of standard techniques. In particular, there is an efficiently computable bijection $\varphi : \mathbb{N} \times \mathbb{N} \to \mathbb{N}$. So, take any P-complete problem that take as input a constant number of integers, and you can convert it to a P-complete problem that takes as input a single integer. 

If I had to do this in practice, I would use a SAT solver. The question of whether there is a DFA with $k$ states that accepts $x$ and rejects $y$ can be easily expressed as a SAT instance. For instance, one way is to have $2k^2$ boolean variables: $z_{s,b,t}$ is true if the DFA transitions from state $s$ to state $t$ on input bit $b$. Then add some clauses to enforce that this is a DFA, and some variables and clauses to enforce that it accepts $x$ and rejects $y$. Now use binary search on $k$ to find the smallest $k$ such that a DFA of this sort exists. Based on what I've read in papers on related problem, I would expect that this might be reasonably effective in practice. 

Roughly $O(k \log(n/k))$ queries suffice, in the regime you are talking about. We can equivalently think of this as finding a minimal vertex cover for $G$, given ability to query whether a particular set is a vertex cover or not. The algorithm is as follows: 

Yes, of course you could do that. Whether it is useful will depend on the task. I'd suggest spending some quality time on tutorials/classes on neural networks, and then spend some time experimenting with different network architectures to see how the choice of feature vector affects the performance of the network. 

It's possible you might be able to get a slightly better solution by using 2-D range trees, where you store the $n$ points $(i,x_i)$ in the tree (for $i=1,2,\dots,n$). However, I'm not certain. 

Yes, if the encryption algorithm achieves IND-CPA security (semantic security), this implies that an adversary cannot predict any linear combination of encrypted bits better than random guessing. The easiest way to see this is to note that IND-CPA (left-or-right indistinguishability) implies real-or-random indistinguishability under chosen-plaintext attack: an attacker cannot distinguish the encryption of messages $M_1,\dots,M_n$ (chosen by the attacker) from the encryption of random strings $R_1,\dots,R_n$ (chosen randomly and not revealed to the attacker). This fact is proven in Bellare & Rogaway's lecture notes, or is easy to derive yourself via a hybrid argument. Now your result follows. Let $\ell$ be any linear function of the message. Then it follows that no attacker can predict $\ell(M_i)$ better than random guessing. Why? $\ell(R_i)$ is a random bit. So, if knowledge of $E_k(M_i)$ lets you distinguish $\ell(M_i)$ from random (i.e., distinguish $\ell(M_i)$ from $\ell(R_i)$), then it would also let you distinguish $E_k(M_i)$ from $E_k(R_i)$, which would violate semantic security.