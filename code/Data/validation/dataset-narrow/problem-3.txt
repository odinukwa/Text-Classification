You have 3 methods to get secrets to an app inside a docker container. The first 2 involve docker configuration. The last one is to have your apps directly fetch secrets from a secret store. 1 - Environment variables According to "The 12 Factor App" guide, secrets are merely config, and they should always be set in the environment. You could set your secrets as environment variables during the docker run, and your app accesses them from there. 2 - Mounted volumes You could have your secrets all within a particular configuration/secrets file, then mount that to your instance as a mounted volume. 3 - Fetch from secret store As @030 mentioned, you can use Hashicorp Vault (or "Amazon Secrets Manager", or any service like that). Your app, or a sidecar app can fetch the secrets it needs directly, without having to deal with any configuration on the Docker container. This method would allow you to use Dynamically created secrets (a very appealing feature of such systems) and without having to worry about the secrets being view-able from the file system or from inspecting the env variables of the docker container. Personal Opinion I believe env variables is the way to go. It's easier to manage, and you can still pull from a secret store like Hashicorp Vault, if you have you CI build system pull the secrets during the build and set them when you deploy. You get the best of both worlds, and the added benefit of your developers not needing to write application code to fetch secrets. Devs should be focused on their code functionality, and not dealing with admin tasks like fetching passwords. Your application's code should be focused on it's own app functionality itself, and not dealing with backend tasks like fetching passwords. Just like the 12 Factor App states. Edit: changed last sentence to remove implication of Developer vs SysAdmin silo-ing. The tasks themselves should be separate from a code perspective, but DevOps is about the same persons keeping both in mind and not be limited. 

You already have a pipeline. This is where you would stick the deployment command. AFAIK there is no standard tooling for registries to 'push out' any changes; you'd have to code a custom integrator that continually polls for changes and compares. This would be a lot of work. Same goes for having your servers directly listen for changes on the registry. It would be far easiest (and is the standard) to simply have your build pipeline issue the deployment command after its successfully pushed a new image to the registry. There are benefits to this: 

Its very common to provide a StatusPage (like Statusy.co or StatusPage.io). There are numerous examples of major providers having them: 

Break down the important ones: Unique network names In a deployment, it doesn't matter the network name as any pod should be able to pick up the work. However, when you are running things like a stateful cluster, you might have the concept of 'masters' and 'slaves'. This unique network naming will allow you to keep the distinction by referencing a specific pod, with a predictable incremental name. Ordered Termination This is important for re-deployments in a stateful cluster as you could have critical machines that need to stay up longer. If you have coordinator or 'leader' nodes, those should be the last to be restarted. StatefulSets allows you to do this. The linked blog post even shows an example of this. Persistent stable storage Arguably, this is the most crucial difference. By default, StatefulSets retain their storage. Kubernetes does not automatically delete the storage that was attached to a StatefulSet. The same can be achieved with deployments, but its not the default. 

Back when Kubernetes announced the new StatefulSet feature with K8s v1.5 (converting it from the old name), they put out a really good blog post walking through an example of its usage: $URL$ The first paragraph has a really good description of differentiating features that always stuck out in my mind: 

There are no limits to the number of slashes (/) you can use for scoping. This makes it very easy to tell the exact commit of an image when it's deployed. For this reason, my company almost never uses the generic 'latest' tag. We prefer the specificity of which image is deployed where. You can still apply this same 'multiple docker repo' logic to any Docker registry. You'll have to look up that specific system's capabilities for creating new repos on the fly, and how easy it is to integrate with your CI/CD pipeline. 

It really depends on how you run and deploy your apps. I worked at a company that used Amazon Beanstalk for production, and they shipped all their secrets and configs as a 'secrets.conf' file that was created by the Jenkins build server, and shipped with the app when it deployed. The problem there was that it was unruly to manage the same secrets across multiple jobs in Jenkins. So, I came in and moved them towards using Chef to create the file instead, and Chef pulled the file from Hashicorp Vault. At my current company, they are heavy users of GitLab with it's built-in CI/CD build system. Because the the apps are all Docker instances running in Kubernetes, all secrets are actually set as environment variables set during the deployment. The build server gets those from 'secret variables' that is set from the GitLab UI. These fit the business need at the time, and that's what really matters at the end of the day. However, you if want to be proper, you should store secrets as environment variables because you specifically mention micro services. It is highly recommend that you follow the 12 Factor App approach to configs: $URL$ 

If you are going to always be using the 'latest' tag, you could then have your devices ship with a cronjob that has then always shut down their local running Docker container, and re-pull the 'latest' tag every week during a scheduled maintenance window. 

So you'll definitely want to leverage environment variables. How you set those environment variables could be your Configuration Management system (Ex: Chef), or your build server when it does the deploy, or your apps could be coded to pull directly from your secret storage and set the variables internally. For actual place where the secrets live (where they are retrieved from), you have a few options. But the 2 you mentioned in your comment would be really great ones to use: Hashicorp's Vault and AWS Parameter store. Others would be Chef Vault, home-grown solution using encrypted files stored on object storage, 1password, etc.... Ideally, you'd want something that can be programmatically access from your build pipeline or apps. 

The short answer: you would need to use a secret management system that provides 'audit logs'. There are many options for handling secrets out there, but unless the tool you use has audit logging, you'll never truly know for sure what credential was used where. I see you mentioned using Hashicorp's Vault. That's a really good one. They have audit logging built in; they call it 'Audit Devices', which can be enabled with a simple command. Again, there are many perfectly valid options for handling secrets: Chef vault, 1password, environment variables injected by your build instances, home-grown solution using encrypted files stored on object storage, etc.... However, unless the system specifically says it provides audit logging, you'll never truly know what was used when and where. You can design a secrets system with the best intentions, and make many rules that employees have to follow in regards to have the secrets are accessed, but for auditing purposes, you want to know where the rules have been broken. If you have a firm requirement to always know when and where secrets where used, I recommend you stick with something like Hashicorp Vault, turn on Audit Devices, and then ship the log somewhere for storage or processing. 

I disagree with @levi, you should stick to the 12 factor app method. Leaving configs in the repo scusk. Simple changes to a password or a small setting requires a commit and a pipeline rebuild. That's too much work for something so simple. The last 3 companies I've worked for are using 'single branch' repos. There is only the master branch, and developers create 'feature' branches off of that. All the different environments are built from master. In that case, we have a static config file, but all the values are injected by the build pipeline. You can pull them in from different sources: build server env variables, an external tool (Hashicorp Vault), or even another repo that has all the configs (this last one is least desirable). 

So, I know I’ve praised GitLab in my other answer to your questions before, but it really is awesome. They have a an HA solution called GitLab Geo. It’s primarily meant to speed up GitLab usage for geographically distributed teams, but the other main benefit is that the read-only secondary server can be easily promoted to be the master: $URL$ But to get back to your specific question: no. There is not really a way to get true high availability if you are relying on someone else. You can only ensure true HA if you have control over the infrastructure. This is one of the downfalls of using all SaaS solution. You have to hope thei internal ops teams have a good implementation under the hood. For that reason, I wouldn’t even recommend using something like GitHub or GitLab.com SaaS paid plans. You should look into self hosted options considering this problem has severely affected your team’s ability to reach deadlines. As @Tensibai mentioned, this is achievable with multiple remotes, sync scripts, watch processes, and other “glue code”, but it would be a lot of work and a lot to maintain. Another solution would be to self host a Git solution inside a docker container. You could have the file store backed by something like EFS and the database hosted in Amazon RDS. I’ve not tried this setup myself, but the theory is there: make the application hosting ephemeral, keep the files and datastore in a proven high-available system. Amazon’s uptime numbers are really good, though you still have the issue on relying on others here. The difference here is that the entire system isn’t out of your control. In the event your application Docker instance goes down, simply spin it up somewhere else and change the DNS. That all being said, self-hosted GitLab on the other hand does have the magic bullet Geo solution that I previously mentioned. I’ve used it and it’s really nice 

If you want a beautiful integration between Docker and SCM, GitLab provides it's own built-in Docker registry. This makes publishing a Docker image in the build pipeline a breeze. The other big advantage of GitLab Docker registry is that it supports multiple Docker repositories for each GitLab repo. This allows you to create a new Docker repo for each branch, or each commit, each environment, or anything you need. My company leverages this by pushing our images to repos based on the branch, tagged with their commit. Here is an example: Or if you have multiple Docker images being build for a particular branch (like a front-end Angular app and it's back-end API that it talks to), you can scope it even further, like so: 

You could provide the same status page to your customers. When there is a service disruption, you could manually update the status while providing detailed, customer-facing information. Or you could hook it into your Zabbix monitoring to have the status auto-updated. This also lets your customers see a history of incidents and average uptime of your service. There are also self-hosted, open-source solutions that you could implement: 

Short answer Ideally, you should store secrets as environment variables, and retrieve them from a secrets management system like Hashicorp's Vault or AWS Parameter store. Long answer I saw your questions out of turn, and kinda touched on this in your other question: 

This is a similar problem I've been facing. And we're not the only ones it seems. GitLab just added a very interesting feature in their recent 10.5 release where you can pull external files into the pipeline. In their blog post, they had this to say: 

Yes, you can. is purpose built to balance any TCP or HTTP connection. My old company used to run every component of Kafka with HAproxy. Here is a whole project dedicated to running auto-scaling Kafka in Docker using HAproxy: $URL$ You can even see their HAproxy config here where they specify the broker check (port 9092): $URL$ 

Although, you'll really want to host those outside your existing infrastructure so that your StatusPage can always be reached, even if you're down. If you go the self-hosted route, I recommend hosting the status page at a different provider: Digital Ocean, AWS, etc... 

'Suitable' can honestly be whatever works for you. I personally feel its perfectly reasonable to do a manual install of Chef server. It is the CM tool to help provision the rest of your fleet. You will not be setting up Chef server over and over again. You do it once, and then never again, in theory. That theory breaks down however, in respect to server outages. Eventually, one day, your Chef server will go down. If you are running Chef in HA mode, or you have a very good backup system that allows you to restore the entire server, then you are fine. I don't know what it is for Azure, but with Amazon, you can take nightly snapshots of the entire server and simply restore from the AMI if the server ever goes "hard down". Let's pretend that you don't run Chef in HA, or you aren't very confident in your backup system, or you just want to tinker... my recommendation would be to leverage Terraform w/provisioner scripts, or Packer with Terraform. You can get the basics of Chef server setup with some simple commands that only need to be run once. This is perfect for Terraform and it's provisioner scripts. Obviously, it runs the provisioner scripts only once (during server creation) and that's all you need for Chef. If something happens to your Chef server, Terraform can rebuild it from scratch. Although, you'll have to re-key your local client and tie all your instances to the new Chef server. This is a huge headache and I would not recommend ever doing that if you can avoid it. :P 

I can provide some personal experience here. I worked at a company that had the same need. The product was a BeagleBone (like an Arduino or Raspberry Pi) that ran our software and sent data back to our SaaS. It had to be installed inside customer networks, and be a little black box that they didn't touch. When I walked into this company, management of the systems was a nightmare. Networking was a nightmare. OS updates, keeping them online, shipping new versions of the software, etc... it was a nightmare. The path we ended up choosing ended up working really well: The entire application and supporting packages was shipped as a Docker container. That allowed us to easily ship application code changes and control versions of any supporting software (ffmpg, gcc, etc...). With the application abstracted, that simply left making the OS as minimal as possible. All we needed was a dead simple OS that could run docker. One of our low level engineers actually made our own in-house fork of Debian (if I remember) and we shipped that embedded directly into the devices. We already had a service contract with all of our customers to service these machines yearly, so when our field technician went on-site, the procedure was to replace the entire unit with a fresh hardware that came with the latest embedded OS. We didn't have to make many OS changes just to run Docker, so often times swapping wasn't even necessary. But out of procedure, we always swapped hardware whether it was an emergency service call or the yearly maintenance. I can't say if this would be a standard approach or not, but just that it worked really well for us and was a life saver. It sounds like your idea of shipping the OS as an immutable image is along the same lines. Edit: When I left, we were still using home grown bash scripts to automate the Docker container deployments, but the idea was to move toward a config management tool (Like Chef, Salt, or Ansible). The basic premise is that you have a private Docker registry somewhere, then on the devices you would simply run a couple commands: