There's a lot of moving parts there. But you might be able to make it work for you. I wrote a few related blog posts about TRY...CATCH that may be helpful: The Unfulfilled Promise of TRY...CATCH Enhanced T-SQL Error Handling With Extended Events Part 2: Enhanced T-SQL Error Handling With Extended Events 

I ran into the same issue you did. I would go on to discover that Event Notifications can be used to handle the event. (Note that this event can't be used with a DDL trigger.) I wrote an Event Notifications blog article that just happens to use the event as an example: SQL Server Event Handling: Event Notifications Below is a script that can get you started. 

Although I don't have a local copy of the Stack Overflow database, I was able to try out a couple of queries. My thought was to get a count of users from a system catalog view (as opposed to directly getting a count of rows from the underlying table). Then get a count of rows that do (or maybe do not) match Erik's criteria, and do some simple math. I used the Stack Exchange Data Explorer (Along with and ) to test the queries. For a point of reference, here are some queries and the CPU/IO statistics: QUERY 1 

If the developers have individual logins, you might consider a DDL Trigger. Here's an example for the , , and events. 

I'm having difficulty "optimizing" indexes as a minimally logged operation. Before the index maintenance is performed, the database recovery model is switched from FULL to BULK LOGGED. Depending on the fragmentation percent, each index is the recipient of a REBUILD or a REORGANIZE (or no action is taken). After index maintenance is finished, the recovery model is reverted to FULL. One database in particular is causing me some pain. The datafiles are about 64GB (including any free space). A defrag operation bloated the log file to 38GB, until it filled the logical drive. Then the level-17 alerts started rolling in. I tried duplicating this in a test environment. Numerous attempts were made with different recovery models, index REORG vs REBUILD, different transaction isolation levels, read committed snapshot ON vs OFF, different index fragmentation levels, etc. Index REBUILDs on DBâ€™s in the FULL recovery model always bloated the t-logs. No other testing variations did, however. This was frustrating because I could not duplicate what was happening in production. What am I missing? How can I optimize indexes without bloating the log files? UPDATE 09/23/2015 Not surprisingly, Ola Hallengren's name came up soon after I posted my question. Although I don't use his scripts, I am somewhat familiar with them. For those who are interested, there is a wonderful PASS session video--Ola is the presenter. Near the 48 min mark, an audience member asks a question similar to mine. 

Background: I have numerous databases with a large number of VIEW's, and an extremely large number of SYNONYM's. For example, one db has more than 10k VIEW's and 2+ million SYNONYM's. General Problem: Queries involving (and system tables in general) tend to be slow. Queries involving are glacial. I am wondering what I can do to improve performance. Specific Example This command is run by a third party tool. It is slow in both the app, and in SSMS: 

I'm passing in a non-NULL value for , so I fully understand why I'm getting the error on SQL 2012. Questions: 

You can enhance error handling with Extended Events to overcome the TRY...CATCH shortcoming you are experiencing. The design pattern involves these steps: 

I was trying to create the drive through Server manager, when I create the drive through Drive Management it worked fine. 

This morning i was woken up by a transaction log full alert on one of our database. This server is an alwayson cluster and also a transactional replication subscriber. I checked log_reuse_wait_desc and it showed logbackup. Someone had accidentally disabled the logbackup jobs 4 days earlier, I re-enabled the log backup job and the log got cleared. Since it was 4am I thought I will go to office later that morning and shirnk the log as it has grown to 400GB. 10AM- Im in office and I check the log usage before shrinking and it was around 16%. I was surprised and check the log_reuse_wait_desc, which showed replication. I was confused cause this was a replication subscriber. We then saw that the db was enabled for CDC and thought that might be the cause, so disabled CDC and now the log_reuse_wait_desc shows AVAILABILITY_REPLICA. The log usage meanwhile still steadily growing and its at 17% now. I check the alwayson dashboard and check the sent and redo queue and both are virtually zero. I am not sure why the log reuse is showing as AVAILABILITY_REPLICA and unable to clear the log. Any idea why this is happening? 

I mean SQL know its partitioned by date so I would assume it should just do a , Scan on each partition which would be much more efficient. I remember vaguely reading somewhere that ordered scan is not supported in partitioned tables, is that correct or is it something else that is causing this behavior ? 

I am having a situation with linked server which I am not able to understand. So we have a linked server from a 2008R2 server to a 2014 server. The below sample query is executing from 2008R2 server and it works fine. 

From my testing I do not see an option to encrypt a symmetric key with Database Master key. Isn't the Database master key a Symmetric key. What do they mean by public key of th database master key? 

That is not true. if the procs are running on the secondary replica it will be cached. It could mean either that the stored procs are not running or its getting evicted out of cache and are not in the cache anymore. Off course querying the cache does not give you a complete picture and It might be better off to use profiler or xevents to track all the storedprocs. 

But when I execute the same thing with the it does not return any results. Second case - If I replace the with and no also I dont get any results out. eg 

How is your application connecting to the SQL Server? If it is not a direct connection and involve two hops(for example lets say you have an web tier that goes through an App tier and then to SQL Server) it might be related to kerberos authentication. You can check if if the connection that were coming into the principle server for the application were NTLM or Kerberos. Please check the below link Using Kerberos Authentication with SQL Server If it is, that means that your principle server was configured correctly to facilitate kerberos authentication and you would have to do the same thing for secondary server. Register a Service Principal Name for Kerberos Connections Another not so ideal situation could be that someone has added as login on the primary server, you could technically make it work by adding the same thing to the secondary server, but it is not recommended. 

Most of the good tips has been given so far, but without lots of explanations for the best ones. I will give more details. First, delaying index creation is a good one, with enough details in other responses. I will not come back on it. A larger InnoDB log file will help you a lot (if you are using MySQL 5.6 as it is not possible to increase it in MySQL 5.5). You are inserting 7 GB of data, I would recommend a total log size of at least 8 GB (keep at its default (2) and bump at 4 GB). This 8 GB is not exact: it should be at at least the import size in the REDO log and probably double or quadruple that size. The reasoning behind InnoDB log size increase it that when the log will become almost full, InnoDB will start to flush aggressively its buffer pool to disk to avoid the log of filling up (when the log is full, InnoDB cannot do any database write until some pages of the buffer pool are written to disk). A larger InnoDB log file will help you, but you should also insert in primary key order (sort your file before inserting). If you insert in primary key order, InnoDB will fill one page, and then another one, and so on. If you do not insert in primary key order, your next insert might end up in a page that is full and will incur a "page split". This page split will be expensive for InnoDB and will slow down your import. You already have a buffer pool as large as your RAM allows you and if your table does not fit in it, there is not much you can do except buying more RAM. But it you table fits in the buffer pool but is larger that 75% of your buffer pool, you might try increasing to 85 or 95 during the import (the default value is 75). This configuration parameter tells InnoDB to start aggressively flushing the buffer pool when the percentage of dirty pages reaches this limit. By bumping up this parameter (and if you are lucky on the data size), you might avoid aggressive IO during your import and delay those IO to later. Maybe (and this is a guess) importing your data in many small transactions will help you. I do not know exactly how the REDO log is built, but if it is buffered in RAM (and disk when too much RAM would be needed) while the transaction is making progress, you might end up with unnecessary IOs. You could try this: once your file is sorted, split it in many chunks (try with 16 MB and other sizes) and import them one by one. This would also allow you to control the progress of your import. If you do not want your data to be partially visible to other reader while you do your import, you could import using a different table name, create the indexes later, and then rename the table. About your hybrid SSD/5400RPM disk, I do not know about those and how to optimize this. 5400RPM looks slow for a database, but maybe the SSD is avoiding that. Of maybe you are filling the SSD part of your disk with sequential writes to the REDO log and the SSD is hurting performances. I do not know. A bad tips that you should not try (or be careful with) is the following: do no use multi-thread: it will be very hard to optimize to avoid page splits in InnoDB. If you want to use multi-thread, insert in different tables (or in different partitions of the same table). If you are considering multi-thread, maybe you have a multi-socket (NUMA) computer. In this case, make sure you avoid the The MySQL swap insanity problem. If you are using MySQL 5.5, upgrade to MySQL 5.6: it has the option of increasing the REDO log size and has better buffer pool flushing algorithms. Good luck with your import. 

And now the , Scan is gone and it has to scan the entire table to get that value. Why do you do this SQL Server? 

But for the second query it looks like sql server can sniff the value with option(recompile). I thought SQL Server cannot sniff variables even if we use option recompile? Seek Predicates - Seek Keys1: Prefix: [DB].[dbo].[test].c1 = Scalar Operator((216)) 

I know I can just change the connection string to remove the property, but how do I remove the configuration from within SQL Server? For example, if we query , how do I get an empty result? 

This is a bit of a broad question but I am trying to understand a storage performance behavior with two of our servers. I was following $URL$ On one server I ran dskspd with the below parameters on the same disk as the DB. 

So for the first query the behavior is as expected, the estimated number of rows is calculated from density vector. Seek Predicates - Seek Keys1: Prefix: [db].[dbo].[test].c1 = Scalar Operator([@Min]) 

I am testing -SubscriptionStreams parameter in replication and wanted to make sure it is using multiple streams when applying transactions on subscription. When I check the subscriber there is only one SPID applying the transaction. May be my understanding is wrong, but is it supposed to have multiple SPID's when SubscriptionStreams is enabled or it does it internally. Is there any way to verify that it has picked up the parameter change? Publisher and distributor are SQL 2014, subscriber is SQL 1008R2 

In simple recovery mode the log gets truncated automatically when checkpoint is issued. Now when you run DBCC SQLPERF(LOGSPACE) and your log is more than 70% full then you can assume that your log is not getting truncated because a checkpoint is supposed to be issued when the log grows to 70% in simple recovery. If you suspect that your log is not getting truncated in simple recovery you can use sys.databases, log_reuse_wait_desc columns to find out why the last log truncation did not work. If you wanna dig a bit deeper, you can use dbcc loginfo and check the status column to see which virtual log records are active and in that way figure out if its getting truncated or not 

What I am trying to understand are the key locks on the non-clustered index(indid 2). Why are there two key lock on non-clustered index? If I check dbcc page on page id 248, I could locate the obvious one((1bfceb831cd9)) which is the lock for the entry for the record 6 which got changed to 7. Output of DBCC PAGE below 

I have a transactional replication which was initially synced from backup. Now I need to add a new table which is really big so we have decided to backup and restore a fresh copy of the db to subscriber to re-intializing it. My question is, in this scenario should I be dropping the subscription, backup restore and then re-add the subscription? is that the correct way or is there any other way of going about it? Thanks 

This might be a permission issue. I am assuming that the monitor server is not on the primary and the issue could be that the primary server is not able to update the values in the monitor server. Can you check where your monitor server is and make sure primary server service account has permission to update the monitor server msdb. 

Did you have a look at in ? We had a similar issue and I have asked a similar question. Unable to truncate transaction log, log_reuse_wait_desc - AVAILABILITY_REPLICA