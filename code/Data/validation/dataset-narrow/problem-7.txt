One possible but kind of ugly and very brittle workaround could be to use winrm to launch the vagrant commands on windows from linux 'remotely'. This need a networking setup for the VM so it can talk to the host through tcp/ip (configuration done on the virtualization you use), configuring winrm on the windows host (if your company allows it) and then using something like pywinrm (first found with a quick search) or any other tool in your language of choice. 

Each of them says: To allow an entity to call 'DescribeInstance', grant all of the following required permissions. 

Well that's not exactly about chef, you're basically asking how to use . The command to create an archive of a folder would be 

In my opinion you should treat each pipeline dependency as any other dependency. Set up an artifact repo like nexus or artifactory to be used as source of artifacts. This way you can ask for latest or tagged version of an artifact inside your pipeline and push to this repo your own artifacts when the build succeed. 

So your answer would be : There's not responsibilities for a project manager on a DevOps product because the main idea behind devops is to have a team handling the product on long term and not on a planned project time only. 

I'm using vagrant with vsphere plugin, starting from a 'base' image already present on vsphere. The new machine is then fully configured (inlcluding middleware) by chef after vsphere customization. That's one in many possibilities, unsure this question lead to actionnable answers out of everyone 'workaround'. 

This quote aim at giving emphase on the break between silo teams, one the principle of devops is to avoid silo achieving one task. While the idea here focus on the build and run phases, the important idea is to bring a whole team together, from architecture to exploitation roles. A "Devops Team" would be made of all roles taking part in the software life-cycle, including tester role, with no-one in the team handling a single role. The main change for a tester would be to learn to voice his/her opinions and feedback on the plan phase in an agile team and probably to take part in some operational tasks. But all in all, there's no exact rule, what someone previously in a silo would do in a "Devops team" depends on what he/she is more interested in and comfortable to do. That's one of the challenge of a newly created team, spot the best abilities of everyone to share the load on the most efficient way. 

The problem involved is understanding a VPN is made of a tunel AND routing configuration, this needs some configuration to avoid breaking your connection to the VPN server once the tunnel is established (usual caveat is the vpn client setting up a new default route and thus breaking your workstation ability to talk with the VPN server through local gateway). The VPN client job is to establish the tunnel and then configure your client station routing table to route packets for the destination network through the tunnel interface. Vincenzo's answer to your question give the methodology to start an openVPN server container so I'll let you start from there but I highly encourage you to read more about the subject to avoid a false sense of security. 

As you can notice there's a Scale in step, triggering a Terminating:Wait for the autoscaling group and notifying the instance to be terminated, the instance has now to do it's work and once done signal it is ok to be terminated. If the task can take longer than the autoscaling group HeartbeatTimeout parameter you can reset the timeout with (quoting still from the same page): 

To avoid relying on (out of building new images from a dockerfile which should be ok) I'd look around Jenkins's kubernetes plugin. It obviously needs to bring to life a kubernetes cluster but allow to have a one time jenkins agent to run the test in a clean isolated environment, and then tear it off. The maintenance costs of the kubernetes cluster should not be greater than maintaining your Jenkins slaves. The bonus point is that you can version you slaves with specific environment and don't have to tweak around for multiples version of ruby/java/python and let the task choose the proper "environment" by specifying the agent image to use and have a ready to test environment without any tweak pre-test. I'd head toward kurbenetes as container scheduler because it is relatively lightweight to configure and maintain. 

I think a simple mount should be enough to allow communication from docker client within the container to docker daemon on host but in case you get a write permission error it means you need to run your container as privileged container using a securityContext object like such (just an extract from above to show the addition, values taken from the documentation): 

What you're describing here could be ITIL, which is a management system requiring documentation and you mix it with the fact a DevOps team will usually define the underlying layers as code, as such it gets back to any development documentation with the caveats of Code is Documentation often seen in Scrum methodology for an agile methodology of development (quick and short iterations aiming at minimal working solution at end of the iteration) Disclaimer for the rest of this answer: I know more of chef and inspec and that's why I take them as exemple here; but they are not the only tools existing on the market, I won't open a debate on them as the better one is the one you're more comfortable with. As such the rest of the question is a little biased and I, personally, didn't encounter an organization documenting the layer relation you describe more than the infrastructure as code and configuration management system code documentation. (Again, this doesn't mean no-one does it, I just never heard of it). To illustrate from my company in a chef environment, an application cookbook will declare its dependencies (tomcat, jboss, nginx/php and on which OS, needed mount points for some shared datas and its DB schema name mostly) and expose its services URIs to be consumed by chef for other applications configuration, this sound like defining your 'Finance System' and the documentation for it is on this application cookbook README, with some more files if really needed. Configuration management systems usually have a central reporting place, "chef-server" for datas an "manage UI" for presentation in chef world "ansible tower" for ansible world to name two of them, but they usually aim more at giving an oversight of the overall managed system than graphing the dependencies. That said, for chef, the chef-server also act as a CMDB you can query with various tools (it return JSON datas from HTTP requests), the inter applications dependencies can be expressed in various way and there's no 'out of the box' method, each company will have its own way to declare them in the system for configuration purposes and as such you may leverage this to build your graph, but that's on your side. In an infrastructure as code point of view, the infrastructure needs would be kept with the application, it is still the application who know what it needs under as middle-ware, which OS, with which locale, what are other services dependencies and what services this application offer). Last thing I can think of if you want to manage those dependencies for documentation only are tools like glpi which is mainly a CMDB and a ticketing system, it takes advantage of documenting assets and their relation to be able to tell what is impacted when you open a ticket saying an application is down. coupled with ng-inventory it allows to query system states and as such can fulfill your query for patch needs, but in my opinion this is an audit system task, like could do inspect integrated within a chef run for exemple, as the next phase would be to fix the outdated systems by updating/patching them. 

Will result in being when the script run, hence taking the wrong case as it is not equal to . So the choice has been made to disallow boolean to prevent unwanted behaviors hard to debug when you're not aware of the YAML rule. I see two way to get over the problem: 

You have either a bad understanding or misconception about how a cidr works. 10.105.5.128/21 is just one address in the range 10.105.0.0 - 10.105.7.255. So either your cidr bits are not the one you're expecting, 10.105.5.128/26 would be a valid network definition but that's a /26 subnet within the 10.105.0.0/21, or you're not using the proper network definition. You can check those division on this very helping site (the link already use your networks above. 

What happens here is that when the shell parse the line to give it to input, is not yet exported and available as an environment variable. Step by step this will give: 

If you really want to use a local repo you'll have to set up a gem server (alternatively, some artifact repositories can handle that also) and set-up the sources for gem using chef's in . If you're using a chef client above 13.0 your can use the in client.rb to tweak that: 

Considering your first paragraph, the organization you're describing is a highly siloed org, which is exactly what a DevOps organization tend to avoid. 

Yes, you just need a Read Only Domaine Controler (search for RODC) in the vpc and you'll be able to have a one way replication as the read only controler won't need to send datas back. This bring limitation for clients in the vpc, as they won't be able to reach a writable DC they can't change passwords and you'll need a careful site architecture in your forest. 

This come from a design choice of YAML language about booleans Every unquoted value matching this "regex": 

Usually containers refers to something like docker containers which have popularized the name I quote there from docker definition: 

This should avoid the grub question. Be warned that any other package using ucf will also use the maintainer package version, for a creation from a base ami this should not be a problem, but it worth being noted. 

There's two *src.zip which can be removed, they won't be of use, and as you can see the lib directory is half the size and within it you have a bunch of things you don't really need: 

I don't know the inner of bamboo, but I assume it works like a bash script. What happens is that variables are replaced before execution, in bash that would be and test.sh will only contain "echo" because VAR is replaced by it's value before the command is executed. If you want test.sh to contain you have to tell bash to ignore the replacement on the first call by escaping the $ sign: will give in the file. In the same note, the notation and are the same, using braces is a good practice when you do concatenation like in as without the braces like this bash would try to find a variable named and return an empty value. To address PrestonM different behavior, I assume there's either - the fact running under windows with the powershell interpreter doesn't behave the same - or just that bamboo has a special case for variables prefixed with and as such escape them. If someone want to test you may try: 

No. Immutable is exactly what it means, immutable, no change on configuration or code running or system library or whatever, if a change has to be made, create a new image and deploy it, never change it while running. Source code updates are the least thing to change on a running server, this should not happen on a running server, immutable or not. You should create a versioned release of your code to deploy on servers, once you have this level of fixed versioning you can step ahead to versioning your server image which include this code release. Any change, being a code patch or a system patch or a configuration change should trigger a new release to be deployed. 

When you use replication you usualy let AWS takes care of routing the alias to one region by targetting in your REST request from your servers and let the redirect do it's job. Whenever N.Virginia is down, the magic cease to work and you're out of luck to access your data and have to update your configuration to choose a specific region endpoint. The problem does not come from the DNS (a request to the bucket itself will work) but from S3 clients, which will connect to the S3 API endpoint before accessing the bucket, in this case the dns resolution is done on and this is us-east-1 endpoint. When you use regions alias, you loose the ease of load balancing over regions with the health check from AWS included. If you use DNS cname targeting the regions to switch quickly, you're responsible of your DNS TTL but nothing guarantee cache servers of client ISP will honor your value (one of many cache your client may encounter). And lastly, if you try to load balance yourself you'll probably create the same SPOF than AWS already have with the added burden of maintaining it. AWS is working on it but that's all the information I have at time of writing. 

Very basic recipe exemple to install filebeat on ubuntu (I do use more or less the same approach for elasticsearch, kibana and logstash):