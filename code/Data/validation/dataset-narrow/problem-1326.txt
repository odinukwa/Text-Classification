After a few milliseconds, gnuplot spat out the following adjusted values (which were pretty close to my initial guesses anyway, since I'd eyeballed it in advance): 

In multiplayer games, a very effective "automatic" balancing mechanism is that the weaker players can (and have a strong incentive to) join forces against the strongest one. Some ways to make this work more effectively include: 

Sounds harmless, right? And it is, as long as the software uses only one component with such a license, or maybe two or three. But what if there were, say, 75 of them — as Richard Stallman famously claims to have counted in a old version of NetBSD — each one requiring a different acknowledgement to be displayed? To avoid this issue, I'd recommend making your attribution requirements fairly flexible. For instance, you could follow the example of the Creative Commons Attribution 3.0 license (which would actually seem a fairly good fit for your requirements, except for the fact that it's not really designed for licensing software), which qualifies its attribution clause with the words: 

However, note that, using this rule, the total amount of density contributed by a unit to all cells is not constant, not even if λ = 1. In particular, when the unit is exactly at the midpoint of the edge between two cells, it contributes (1/2)λ units of density to those two cells, and nothing to any other cell. Thus, calling the resulting value a "density" seems a bit misleading. 

Of course, you also need to update the horizontal position and velocity of the player too, but I've left that out for the sake of simplicity. Also, you'll probably want to implement ceilings too. 

Assuming realistic Newtonian gravity (i.e. that the gravitational force between two bodies is inversely proportional to the square of their distance), and that the planets don't interact gravitationally with each other, but only with the sun, then the planets will always follow Kepler orbits (which, if closed, will always be elliptical). Conversely, if the planets do affect each other via gravity, then you're dealing with the full n-body problem, which can have all sorts of crazy solutions like figure of 8 orbits or even planets accelerating to infinite velocities. Admittedly, those particular examples are mainly mathematical curiosities that you're unlikely to see in your game, but still, you should not expect there to be a single, unambiguous definition of an orbit for general n-body dynamics. The best you can do is probably to just pick some arbitrary, pragmatic definition and stick to it. For example, in two dimensions, you could simply declare that a planet has completed a full orbit when its angle, as seen from the sun, equals the angle of its starting position (after actually looping around the sun). You could simply track this angle for each planet, adjusting it incrementally as the planet moves, and declaring a complete orbit when it hits &pm;360°; this way, you won't end up with false positives for planets that e.g. loop around each other and reverse direction without actually going around the sun. If the above seems too easy, you could also require that the planet must return to approximately the same position and velocity as it had before. (Again, you'll probably want to also track the sun angle, to make sure that the planet has actually gone around the sun and not just, say, around another planet.) This probably requires you to track the trajectory of each planet (which, of course, you may want to do anyway for visualization purposes) and to come up with some fairly efficient scheme for finding near matches (e.g. using spatial binning). 

I'm working on converting a dx11 shader from a .fx file for use in Unity3D and I'm a little puzzled by the HLSL Buffer<> type declared in the shader. More specifically, what are these and how can I implement them in Unity? I'm aware of the Structured, Append, and Consume Buffers but those appear to be different then this and the Microsoft documentation wasn't to helpful. Is it just like an array that is populated and sized from code before getting assigned to the shader? Are they read only or writable as well? So far I'm thinking the closest approximation I can use is a StructuredBuffer but the .fx file has its own declaration for that as well so I'm not entirely sure I should go that route. Example: 

} 1.) Consider using Transform.LookAt to simplify the rotation. 2.) In your movement code, myTransform.Up represents the Up vector of the transform in local spcae, so if your model is tipped off center then Up will no longer point Up in world space. Use Vector3.Up as MistaGiggles suggests. 3.) You can make your enemy hover above the player by adding an Offset amount for the Y axis like the sample code above. Additional way to rotate your enemy based on your comment: 1.) Get the normalized vector from enemy to player and assign it to Transform.Right, which would be your X axis. Unity takes care of updating the Transform's rotation internally when you set Up/Right/Forward manually. 

Limit Velocity Over Lifetime->Dampen in the Editor. From Unity Docs: "(0-1) value that controls how much the exceeding velocity should be dampened. For example, a value of 0.5 will dampen exceeding velocity by 50%." For best results set Speed under Limit Velocity Over Lifetime to a Curve that shows how you want the particles velocity to react over its lifetime then use Dampen to fine tune it to your liking. 

I have an app made in Unity3d for android and iOS. In it, the user has the ability to link their accounts with Facebook and Twitter. For Facebook I use their Unity plugin, for Twitter, I open an external browser page that lets them accept or decline Twitter integration that then redirects to a php page the calls the app URI to return focus to Unity. On Android this all works perfectly using an Intent in the Manifest. In IOS I'm running into issues though and I don't really know my way around XCode. I'm using the newest version, XCode 7 Beta 2. The App runs as expected but for both the Facebook and Twitter authentications, upon returning focus to the App from the Browser, it completely restarts the App as if just launched rather then returning from a suspended state. In the Unity build I have "Run in Background" selected. I setup the URL Scheme in XCode. Its a pretty basic app so I don't think its being closed due to Memory usage. Any ideas or any settings I can change in XCode that might be preventing it from resuming? *Note, if I just hit Home and send the App to the background it will properly resume, its only when called by the URI from the browser that it seems to completely restart. 

(Ps. I used instead of just to make sure that and end up in the valid range even in the unlikely case that the background movement speed exceeds one tile width/height per frame. It probably won't make any practical difference, but it's generally better to play it safe anyway. Of course, if you really were expecting such high movement speeds, you should rewrite this to use the operator instead.) 

Note that, if you only record the initial PRNG state, any changes made to the timeline are likely to have very strong global "butterfly effects" as soon as they change the number of calls made to the PRNG, which will cause the sequence of outputs to be shifted. If you find this undesirable, you could mitigate it in various ways, such as: 

node y is the goal node, d is a horizontal move, and either of the cells vertically adjacent to the cell y − d (that is, the cell one step before y when moving in the direction d) is blocked, or d is a vertical move, and there exists a node z which is a jump point from y (by condition 1 or 2) in some horizontal direction d'. 

ImageMagick's montage command can do this. For example, to compile a bunch of irregular-sized sprites into a sheet of 32 × 32 pixel tiles, you can do: 

In general, the thing to realize about security is that it's a mindset. It's not about knowing the latest security buzzwords, or about how the latest attacks work or about trying to anticipate what hackers will do before they do it (although all of those things may be of some use). Actual secure programming is about learning how to use the tools you have the way they were meant to be used, about dividing your code into pieces small enough that you can understand what each of them does (and making sure they do it right, even if other parts are compromised), and about writing your code to expect the unexpected. If you do all that, you won't need to know what hackers might try, because you'll be prepared for anything. 

If you're just looking to make your game compatible with some older devices consider using Quality Settings . Go to Edit->Project Settings->Quality Settings to set up each level and instruct Unity to use full size textures/half size etc.... Then add a Menu Scene that will load first allowing the User to select the settings he wants for his devices. 

1.) You need to update _lookRotation when the mouse is clicked or it never changes so your agent will never look at a new location. Simple Coroutine Example: 

I have a texture shown over the screen using screen space UV's. The Texture is 495x,596x, Screen size is 888,500, but can change. The Texture has a Tile of (3.469775f,1.392644f) to correct for distortion from being stretched to the Screen's size, and has repeat set to Clamped. So basically, I'm showing a Texture of an arbitrary size on screen and using Tile and Offset in the shader to correct for distortion and keep the texture at its original aspect ratio. Now my problem is I want to Zoom in and out by a set pixel amount in Screen space. So lets say: I need the texture to increase in width by 10 pixels in screen space. What is a formula I can use to recalculate the Tile and Offset so that the texture stays at the same aspect it was but increases by the amount I want it zoomed? I've been trying things like: 1 Pixel = (1f / Camera.main.pixelWidth). So to zoom by 10 pixels I would go something like Tile.x += (10f * (1f / Camera.main.pixelWidth)); But this doesn't seem quite right, I'm not seeing it increase the Texture by the right number of pixels... My current function is something like this, but for Zoom = 5f, it's only enlarging by about 2px. AspectRatio = Tiling in the Material and WidthOffset/HeightOffset is just the Offset. 

Try using CacheProceduralProperty to specify at runtime what properties you want to cache and which ones you don't. 

When a Resource is Loaded its not actually Instantiated and added to the Scene so Awake() is never called. Change your code to something like the following: 

But when I change the initial parent transform from to (basically use an identity matrix), the model is displayed in its bind pose: 

A bone's is basically the bind pose (regarding to what the exporter says, it's in object space) of the bone. Now, a contains position and rotation (in quaternion) of a bone in the given frame. The loop at the top of basically checks if in the given frame (in this case, th) there's transformation data for the given bone. After launching the code above, all I get is this: (please ignore the lack of textures for now) 

I think this part is working fine, because when I hardcode an identity matrix in the shader, the model is rendered in its bind pose. Computing bone matrices For now, I just want to display the model in a pose which it would be in, let's say, frame 49: 

The question is, can someone point out what is wrong with the code? Thank you very much. PS. If you need any more information, please let me know in the comments. Edit 1. I've changed the vertex shader code a bit - final vertex is calculated in a different way, but still it does not seem to solve my problem. 

How do I perform range picking in the latest OpenGL version? By range picking I mean selecting all objects which are picked using a selection rectangle, like in an RTS game. For single object picking I'm using the ray picking method, which I guess can be used in this case as well, but I'm not sure how I should go about doing that. Could you give me any pointers? Thank you. 

This time I've decided to ask a question related to my skinning problem. Two earlier ones were not really related to my problem and I've deleted them. Ok, first of all, I'm using OpenGL 3, 3ds max, and Delphi to create a skeletal animation demo. I'm using the IGame interface to export meshes from 3ds max to XML - I use the sample included in the SDK. Now, I'm going to describe as precisely as I can what I do. Passing bone weights to the shader I'll skip parsing geometry, because it works okay. Now, if a mesh is skinned (using the Physique modifier), the resulting XML contains information about bone influences for each vertex, like so: