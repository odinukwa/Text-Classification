I'm using SlimDX 11 in my managed Direct3D application. I would like to implement deferred shading. I'm having trouble when I try to set a depth stencil surface and multiple render targets at the same time. Here is my code: 

I'm working on a radiosity processor. I'm projecting scene geometry onto a hemisphere at a high order of tessellation during a visibility pass onto a 1024x1024 render target. The problem is that the edges of certain triangles are not being rendered to the item buffer( render target )...so when I test certain edges( or pixels during pixel shader ) for visibility during a reconstruction pass, visible edges are not identified and as a result the pixel for that edge is discarded. One solution was to increase the resolution of the item buffer( up to 4096x4096 )...this helped and more edges were visible, however, this was not fullproof. How do I increase visibility? Here is a screenshot of a scene after radiosity is applied: the seams are edges along a triangle face that were not visible due to the resolution of the item buffer... 

What is the proper way to evaluate the transforms for a skinned mesh when converting from FBX to DirectX 9? 

Conservative rasterization is a fix for this problem. Thanks to Nathan Reed's comments, in preparation for the generation of a lightmap, I dilate the uv coordinates and recalculate they're associated positions based on the dilation. For each triangle: 

I had to enable color writes in the BlendStateDescription before instantiating the BlendState. Code: 

I am creating an application using Direct3D 9. I want to implement triple buffering with vsync. I can successfully create a graphics device object with 2 back buffers. How do I know which buffer to draw to? EDIT: Here is how I retrieve a pointer to the back buffer onto which I will draw using a deferred shading technique: 

The 'raw.riProcessMouseMessage() and raw.riProcessKeyboardMessage() functions come from a class I use to keep track of raw input keyboard and mouse states...I found this on a raw input tutorial( that I can't find right now ). Here are the function definitions: 

I'm writing a Direct3D9 application wrapped by SlimDX. I'm trying to take my application full screen but I am having problems. I can reset the device so that it covers the entire screen, however, it seems as though the device window loses focus. The cursor belongs to that of the window beneath my device window. When I click on the screen(while in full screen mode) the window beneath my device window gets focus. This shouldn't happen. In my c# code for managing the device I have converted DXUT(from the DirectX SDK) to c#...There is a lot of code to cover but I was hoping for a theoretical answer. What is happening to the device as I take the app full screen? Why does the window beneath the device window get focus while the device window continues to render and cover the screen? Could my problem have anything to do with the fact that Windows Forms draws with GDI? I found a post here that describes a problem that differs in result but may be along the same lines wrt cause. 

As you can see, the mesh face( green ) extends beyond the pixels rendered during generation of the lightmap. It seems as though those pixels were excluded during rasterization. Most of the problem faces are narrow in dimension. I thought that I could fix this by increasing the size of the lightmap, however, this doesn't always work... Here is the entire lightmap: 

I am building lightmaps for 3D models. My lightmap algorithm needs to determine which pixels( lumels ) within the lightmap texture fall within the boundary of a mesh face( triangle )...this process takes place for each mesh face... I was using barycentric coordinate techniques to accomplish this task. It works for the most part...here is code: 

This function takes as input 3 UV coordinates( mesh face ) and a pixel UV coordinate. The problem with this method is that if the pixel is only partially covered by the face, the function returns false( because the pixel center does not ly within the triangle ). I could do overlap tests for each point on the triangle...I would like to know if there is a more efficient method...as this process takes place many times per generation of a lightmap... ...any ideas? here is a screenshot showing the endpoint of a mesh face after lightmap generation...the corner of this face overlaps a pixel surface but doesn't overlap the pixel center...the algorithm left this pixel black( no light )... 

If you're using win32 then you can handle mouse and keyboard events using RawInput. Info can be found on MSDN. You would handle windows messages for raw input devices in the window process. For example, if handling the input for a keyboard and mouse, register those devices in the WM_CREATE case of your application window process like so: 

I'm finding that the Pos2 value sent to the pixel shader is incorrect. Am I incorrectly converting projected vertex position coordinates to normalized device coordinates? PIX says that values are well above 1 when dubugging the pixel shader, however, the output looks correct when looking at the 'mesh' tab while hiliting the draw call... Here is debug output for a single pixel whose position lies within the clip space: 

You should look into 'impulse force' since you are using physics...Impulse is calculated as force * time...or mass * change in velocity...if your particles are motionless before the explosion, an impulse from the explosion will give your particles dynamic motion( less linear )... impulse link 

I have an application that utilizes Direct3D 9 to render 3D graphics. I'm trying to manually control the number of frames rendered in a given second. Here is pseudocode: 

When using the back buffer at index 1 the frame rate falls to 30fps while using vsync. This produces artifacts and Horizontal Shearing. Using the back buffer at index 0 seems to be correct. When using this buffer the application presents a frame at the refresh rate of the monitor. Therefore in order to correctly implement triple buffering using Direct3D 9 you should draw to the back buffer at index 0. 

The above code is very rough but explains what I'm trying to do. I want to allow time between frames to process work( i.e. visibility, animation, etc. ). The actual code works well except for one problem. Screen anomalies occur due to rendering frames out of sync with the screen's refresh rate. The anomalies are best described as horizontal tearing, an undesirable result. When I activate vsync while using this code the screen anomalies do not appear. However, I can't use vsync because it prevents my game loop from continuing until the frame is rendered and thus consumes the spare time that I would like to set aside for work. How can I get around this problem? I would like to sync with the monitor using my application rather than the Direct3D API. This way I can control the work flow...Any ideas?