After changing to TSM for the VMWare backups, we've started experiencing an odd issue with our Snapshot backups. We take system drive snapshots as a backup solution for the machine states. All the other drives are independent drives, with their data backed up through other means. Our problem is isolated to SQL Servers. For these servers, we use as a backup solution. So the drives that contain the databases don't need to be backed up on a VM level. Since our change from to the snapshots have started failing on SQL Servers. The SQLServer VSS Writer is throwing an error. However, neither the SQL binaries, nor the SQL Server data or log files are included in the backup. The only things that are included in this snapshot that relate to SQL Server are the few dll's that have to remain on the C: drive. Probably related is the fact that the dll for the SQLServer VSS Writer is one of those dll's, located in . We're working around the problem by adding a config file as documented here. But as the problem does not span all SQL Servers, and we don't know the cause for the problem yet, we're still looking for a better solution. How can we find out what the VSS writer is failing on? 

Possibly your problem is with the ports not being open / firewalls blocking the ports. You can easily test this from your VPS to your dedicated machine using nmap: 

You don't actually want to escape your wildcard character. Because then you just end up comparing to the wildcard character, while what you want to do is find out if your string contains an asterisk. The way to do this is by matching on a regular expression. 

UPDATE 2: By request on a different thread I redid the procedure, making sure the prompt was ran as administrator. After running the commands the following events are found in event viewer: 

Since you stated that the application runs correctly from your machine, you could test the following: 

It doesn't matter who you buy your certificate from, as long as it is a from a Certificate Authority that is recognized by your clients. In truth the only difference between your self-signed certificate, a free certificate or one generated by a well known public Certificate Authority lies in how inherently trusted it is by your users. So again, no, shop around for the best deal on a certificate from trusted vendors, you are in no way bound to stay with GoDaddy. 

Blob storage accounts are region locked, just like nearly everything in Azure. There is a comprehensive and up to date list maintained by microsoft. In your case, you want to look at the Blob Storage Accounts tab. There are also ways to check using powershell or other means. 

This smells like a UAC problem. As suggested in some similar technet forums you might be having this issue because the administrators group is the only group that has permissions on the folder of those applications. You could try turning off UAC temporarily, or using the work-around that involves creating a second group, granting it permissions to the folders affected and adding your users to that group. 

What you're looking for isn't what Windows Clustering was designed for. Windows clustering is based on the notion of a shared SAN drive. However you can look into active/passive load balancing, with disk mirroring. ARR is a Microsoft solution for load balancing web applications. As for the mirroring, you could use RoboCopy. However keep in mind, there is a reason that you're coming across shared drives as a requisite for most clustering options. It is easier to set up than a mirror / load balancing solution, and most networks with availability concerns should be using SAN in stead of local drives anyways. 

For a small company that is looking to have a server publicly accessible, it is often not easy to make sure that it is entirely safe. Your first order of business is probably to make sure that if the server is compromised, you limit the amount of risk. 

More than likely your disks are not yet correctly attached to your instance. When you add additional disk space to an EC2 instance, your disk isn't immediately mounted on your Linux. ITKBCentral has a blog post with a tutorial on how to move your running instance to a larger disk. If this isn't your current situation, please provide more information about your EC2 volumes and maybe the output of parted / fdisk -l commands on your linux distro. 

The newly added rule is not there. If I however add the rule with the portal, and try to create it using my powershell command, I receive the following error: 

When using the login prompt. Which means I just type in and it forwards me to my company SSO page. In which I log in successfully. I get the following error: 

This is most likely due to the wrong version of SSMS being installed. If you're absolutely sure you have the correct version of SQL server installed on your server, which you can test by opening services.msc and checking if the SQL Server Agent is present. 

This is because the switch is not a formatting tool, it is intended as a way to receive more information than the default values already included. From the Get-ADOrganizationalUnit article. 

Current situation: Windows server 2008r2 running under VMWare esxi5.5. 3TB drive consisting of 3x 1TB virtual disks under a spanned volume. (D) The drive contains approx 1,9TB of data. D drive hosts 30+ SQL Server database files. Several other instances are hosted on different drives. We want to move this 3TB drive to a new server. The new server will have the same OS version and SQL-server version. It will however have more resources, and be completely dedicated to this one SQL-server instance. What we're considering: 

I've ran into this same problem, and it seems like there is a bug in the way new-adcomputer adds UPN's to the serviceprincipalname field. Most likely it's because it has to translate the backslashes to forward slashes, which it fails to do correctly when using the new-adcomputer. However, if you use a piped set-adcomputer with forward slashed UPN's, it will correctly create your computer. Example of a working piped ServicePrincipalNames ad-computer creation. 

There is a difference between allocated and used space. When you create a protection group, a certain amount of space is allocated to it, which DPM will try and utilise for backups for that specific group. What you're seeing is that DPM is warning you that it thinks it doesn't have enough space unallocated. So it might not have enough space to allocate disk space to a new protection group. However it DOES have enough free space within the allocated space to safely back up your current protection groups. 

First issue: login failed error. Your database is most likely not fully initialized yet when the tests run. You should catch this in your procedure, an easy way to do this is to query the master database to see if the target database is up and running. 

Use multiple media or devices Research optimization options for your specific type of backup. Full and differential backups have different optimizations than Log backups, and yet different optimizations than restoring your backup. Improve read performance on your origin platform Improve write performance on your destination platform 

can provide you with an LDAP solution that can refer to the windows authentication available in your active directory environment, but does not require replication. In other words, there are no schema changes required to install for an LDAP enabled application. As such you can install several connected applications, which each have their own stored locally, with it's own schema without affecting the main directory schema. The advantage being that you don't need to replicate anything to your main schema, and lower replication traffic as a result. Relevant quote from the microsoft documentation: 

You could try querying the ad groups using powershell, then adding the user to group2 if he isn't a member of group1. 

Ask your server administrator directly... Depending on your server and your applications there are several logs that could possibly contain bits of information as to what a person was doing. If you have a SQL server, he might have ran indexing jobs. If you have a Vsphere environment he might have updated some of the templates or looked at the resource and redistributed them. He might have simply reviewed logs, created a history of events for baseline purposes. These are all things you'd need to check separately, as the list of possible activities rises, so does the amount of places you need to check. And this is all assuming the logs still exist, and you know where to find them. If you don't trust your administrators, look for new administrators. They have full control of your system, and if there is any animosity and you don't trust their characters, then you have bigger problems than wasted billing time. 

It works fine for smaller environments, but anything over medium size would do best to use direct attach or SAN-based storage. 

I switched to using en_windows_server_2008_r2_with_sp1_x64_dvd.iso and this version succeded in installing. Perhaps I made a mistake and used my old copy of en_windows_server_2008_r2_standard_enterprise_datacenter_and_web_with_sp1_debug_checked_build_x64_dvd after downloading a fresh ISO. Or something happened on both downloads... Regardless, the issue was with the ISO, and downloading a different one fixed it. 

It needs to be in a domain. DPM 2007 reference. DPM 2012 reference. You can however set up a small domain, only to host DPM, and install the backup agents on other domains / workstation computers. You're only causing yourself more work this way though, it is best to join DPM to your primary domain. 

I don't know of any built in way to find out if a specific UserID exists. However, you can just match the content of the message to find your SiD, as it should be unique: 

After you removed the computer from the domain, the domain credentials were removed from the registry cache. This means you'll now have to use a local user. If you don't have access to a local user, and the data on the server is critical, you can remove the new TEST01 from the domain, and run system restore using a windows disk on the old TEST01, and restore it to a joined state. System Restore and Domain credentials Edit: Comment by TheCleaner is probably a better fit, using a boot cd such as Ultimate Boot CD is an easier solution. 

In Synology terminology a disk group is simply a RAID array that can then be split it up into several volumes. So if you have 4 disks, you can create one single disk group (RAID array) and create two volumes from it. Or you could create 2 disk groups, and create one volume on each. The difference / advantages being that the single array will have more space, while the separate arrays will have better redundancy. There is a nice write-up on the synology forums about this SHR is a separate concept, and is based on the Linux Raid Management system. It's a design that allows for the usage of otherwise unused space on larger disks added to an array limited by a smaller disk. Full disclosure, I use synology at home but have never been involved with our NAS at work 

Oddly enough you're not the first person to do this. Dell even has a guide on how to register your DC back into DNS. You really should review everything though, a domain controller is the key part of your infrastructure that needs to be consistent and reliable. Renaming a DC is something I've avoided at all costs in the past, and reluctantly executed with a lot of mistakes / problems that I wish I never had to resolve. Take system state backups before each action, and verify each action extensively. Don't start deleting stuff at random! 

But as a final recommendation. If your only indication was the Avg. Disk Queue Length, your Sysadmin is probably right. If you're sure it's a disk issue, you can always try creating storage spaces. 

While it is rare to have any averse affects after installing updates but not yet rebooting, it is still an extra degree of unpredictability that you should try to avoid. Most people wait to install upgrades until they know they can safely reboot afterwards. Keep in mind SP or CU installs are much more impact-full, and the reboot should not be postponed on those if you can help it (if even possible). For middle-ware updates This is completely out of your hands. Each piece of middle-ware is different, and it depends on the vendor, as well as the specifications of the update. If you're updating certain middle-ware to a higher step of the same main version most of them don't really require much in terms of reboots. However if you're installing a new major version, some middle-ware will require a reboot, and be inoperative until you've rebooted. The best advice I can probably give you is: 

For exchange 2007 there are some obscure limitations on the server based on tcp port limits. For exchange 2010 there is a limit on the amount of connections a single user can have and the default limit is 10. However it doesn't really matter what kind of connection it is, mobile or otherwise. Exchange can throttle the amount of concurrent client-connections to a mailbox, however this is only for active connections. Meaning your new devices are probably not incurring any kind of limit. Judging by your question, I would not look at the exchange server, but at the client setup. You can use the Microsoft connectivity tester to see what the problem is in more detail. 

You don't actually have a route setup for that address. Which causes it to default to the default route: 

Proxy-Arp is only required when the IP is not on the interface you're configuring. Similar problems: Juniper ForumJuniper Forum Example 2 

Looking at the source code of the app engine, it seems like you're failing an xsrf check. The only thing you yourself can do to try and fix this is by logging out entirely, preferably closing your browser (shouldn't help but you never know) and then starting fresh. If that doesn't help, the best thing you could do is contact google support. 

As you're being somewhat vague (or at least in-precise) on what updates you're going to do, I'll make some broad blanket statements that will hopefully answer your question. For windows updates on Servers You can generally safely postpone the updates indefinitely, not taking into regard that it is not safe to have your system be behind in security, stability updates and bug-fixes etc... As long as you don't reboot, the following is true: 

When running logman parameter /? I came to the conclusion that the only switches that allow for the -u parameter are the update create and delete switches. My worry is that when rolling this out, I'll have problems passing the user credentials when starting querying and stopping the counters. Because remote access is going to be restricted I have to be able to pass a user that was given the correct credentials, without running my powershell session as that user. (I am not joined to the domain) Is there anything I'm missing? Perhaps regarding this conflicting documentation?