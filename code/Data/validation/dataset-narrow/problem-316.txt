I am running a SQL Server 2014 Standard Edition (Cumulative Updates 6) instance on an r3.large Amazon EC2 box. I noticed that the CPU usage of SQL Server skyrockets to 100% while executing pretty much no queries at all. Looking at the SQL Performance Tool, I can see that most of the time is lost in SQL CLR functions. Here's a screenshot of one of such operations: $URL$ Most of those CLR functions are doing basic things. For example the IsNullOrEmpty function is returning the result of String.IsNullOrEmpty() but even those are taking up between 1500 to 7000 ms to execute. If I take the same job and run it on a physical box that we use for development, those calls can't almost be seen as they're way too quick. Is there anyone who faced similar issues? I suppose it's due to EC2 or VMs but I can't really tell why. Thanks! 

Apparently I am the only user connected to that database so I'd say that the file isn't open in exclusive mode by someone else, neither by me as I am accessing it in read-only mode through the Mode=Read parameter in the provider string. What's that workgroup information file that it is referring to? Is that the actual problem or could it be a more broader error message and the problem might lie somewhere else? 

What's wrong with this? Is there any way to debug what's going on between SQL Server and the linked server? Update If I add the Access password information to the linked server logins like this: 

It turns out that the CLR was not enabled and for some reason the exception wasn't being trapped and SQL Server took about 15 seconds just to say that the CLR was not enabled but during that time the instance is being kept at 100% CPU and hogged down. In case anyone happens to have the same problem, you can turn the CLR on by running this: 

I can't answer to question 1 as I do not have enough information about the MySQL instances offered by different hosting providers. To answer the rest of your questions, yes, if you can leverage the features of the RDBMS you're using, you might replace PHP code with SQL scripts that can run faster than your code. Just think about the time you're saving up by not transferring your data back and forth. On the other hand you would probably still end up using PHP code or other programs (crontab or any other kind of scheduler) to run your timed tasks as there's no built-in scheduler in MySQL. 

I've installed the Microsoft.OLEDB.12.0 driver for x64 on the SQL Server box and they work correctly for local Access 2010 files. I added the Access database as a linked server with the following commands: 

Those commands work fine and don'r return any error. The moment I try to test the connection from SQL Server Management Studio, I get the following error: 

You are using the 3rd normal form. A relation in 3NF, must also be in 2NF and 1NF. You are asking if you need tools to validate your design, which is something no one can answer. If you design a car, will you use available tools to validate that it is safe, fast, and reliable? or will you just trust that your original design achieved these goals without testing it with any tools? 

Check out this article. I think it will answer your questions and give you the tools to deal with over-inflated number of auto-statistics. The author suggests dropping ALL auto-created stats occasionally, as the one that are required will be recreated when the first query that needs it is executed. It explains the risks / benefits. 

You concern yourself for nothing. "it's a bottleneck since of each row processing" is an assumption you make, and not necessarily what the optimizer will choose to do, even if it looks to you that you write it that way. SQL is a declarative language, and as such you tell the engine what you want done, and not how you want it done, like you do in imperative languages. The query optimizer, especially in SQL Server, is a mind blowing wizard in moving things around, considering alternatives, and understanding what it is that you want, regardless of how you write it. It doesn't get it 100% right, but it is damn good. Write your query, use several syntax like you did is never a bad idea, and test it against a representative data set. If you see performance challenges, examine the execution plan, or post it here so people can help you. HTH 

In a relational model every relation describes one 'thing' - the 'Fruits' table models fruits, fruits are not blends. Your Fruits table with a single column, is all key - 5NF. If you don't have a Fruits table, how will you insert a new fruit which has no blend yet? What happens to a fruit you have in stock, but its blend is removed? 

You should definitely use only BigFuzzyDB by fad software. There is no other database on the planet, or in the universe, that can process zetabytes of hipo-structured data at near light speed like BigFuzzyDB. Resistance is futile. It uses Heisen-SQL, it's kind of a hybrid of NoSQL and SQL (and none) at the same time, and you can never know which one it is. It has bit-fork-key-pair parallel memory resident indexes, so performance is off the charts. Don't use anything else but BigFuzzyDB! 

The technique you are describing for representing task hierarchy is called 'Adjacency list'. Although it is the most intuitive to humans, it doesn't lend itself to very efficient querying in SQL. Other techniques include path enumeration (aka materialized paths) and nested sets. To learn about some other techniques, read this post or search the web for numerous articles on these techniques. SQL Server offers a native hierarchy representation for path enumeration. This is most likely your best bet... 

My system is able to do the daily processing of QuotationLineItem and PurchaseOrderLineItem fairly okay. The issue is with reporting which leads me to do many joins which is crazy. I have looked at the Kimball book 3rd Ed of Data Warehouse Toolkit: Dimensional Modeling. I am convinced that I need to have a separate database instance which is meant for OLAP situations to satisfy the reporting requirements. Because of this, I need to design dimensional and fact tables. My question is, it appears that I have at least 2 fact tables. and . How do I generate a report like the above? Because from what I understand fact tables are not supposed to have foreign keys to each other else I will get back a snowflake schema. EDIT As requested, these are my source tables which are in a 3NF database schema. Tables include but not limited to: 

both fact tables need to have some common dimensions and the row headers must be from the common dimension table. perform queries on the 2 fact tables separately perform an outer join on the 2 result sets. 

I have a Quotation Management system that I built as a CRUD web app. Over time, I realized I need to improve it so that management can have their reports. Hence I studied the Kimball method of data warehousing. I am now familiar with the concept of Dimensional and Fact tables. On paper and pen, I have identified Quotation Line Item as the Fact table. My issue is sometimes, the Sales Rep and the Customer have frequent back and forth regarding the Line Items so the line items will change quantities frequently or even removed. I know that fact tables not supposed to have zero rows according to the Kimball book. My question is how then do I handle such changes in the fact table? According to a source, I am supposed to continuously update the fact table. Please advise. 

If I don't need to track history then just add/update/delete the fact records as the Quotation Lines change. If I do need to track history then add effective start and end dates to the fact table. See Timespan Accumulating Snapshot Fact Tables Credit goes to Nick White in this thread. 

Then I do a drill across the 2 fact table. Although I am not sure how to do that using MySQL and PHP. Attached is the drill across concept I re-read in the book. 

Situation I have 2 separate instances. Both running on Ubuntu 14.04 server. Both have installed postgres 9.5 the following way: 

So where did I go wrong with getting the standby server to be up? UPDATE As suggested by one of the answers, I have tried adding a into and then starting the postgres as user . Same result. 

The script successfully sends the WAL file every 2 mins to using the and leaves them inside Now I then use to backup the master database at then I stopped the postgres to stop the continuous archiving every 2 mins in I run 

The COUNT technique that was suggested here should work, but the problem is that your table structure and data mess it up. You did not define any keys, and you have duplicates in the demo data - for example, course 101 enrollment by student 4 appears twice, on the same date, with different grades. Proper declaration of keys would have not allowed it. Changing the predicate to would also give you the result that you were looking for, taking into account that there may be multiple registrations. The generalized problem you are facing is called 'relational division'. Search here and you will find many discussions on the topic. Typically, the most efficient way to solve relational division problems is to use a negative formulation of the question. It seems weird at first, but once you get the hang of it, you can't go back... In your case the question is: 

Example 1: You shouldn't care. The process is much more involved than it seems, and I don't think you have any guarantee. The 3 commands go into the queue in order. The first one is pulled and executed, when the second is attempted, the engine may choose to put it back in queue if it waits for more than some threshold, and the third may be pulled and attempted. if it happens that the resources are now free, the 3rd will be executed first. if not, it may go back in queue again. Example 2: I think the above explains it, and the answer to both your questions is no AFAIK, unless you develop an execution buffer of your own. Don't. Rationale: Now you got to the core challenge, which should really have been the title - "How to deal with deadlocks". You assumed that your 'global lock table' solution is the only way to make things right, but I think the price of losing all concurrency is not worth it. It would be easier just to have the procedures executed serially using a scheduler of some sort. You will open a Pandora box with this approach that will be hard to close. Queuing and syncing is a very complicated challenge. If I were in your place, I would first investigate the root cause of the deadlocks. Sometimes, something as simple as adding or modifying an index can solve it. If you post your deadlock graphs here, along with the involved queries with their execution plans, people will help you pin point the root issue. HTH 

Depends on what data you store in it. If you store a null or an empty string, it will take around 2 bytes. If you use all 50 characters, it will take around 102 bytes. See the documentation here. 

UNTESTED - You did not provide any DDL or sample data scripts... IF is a flow control construct that determines which statement block will be executed. What you need are Conditional Expressions. In your case, the function should do the trick, also look at for non null condition predicates.