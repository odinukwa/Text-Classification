The reason it shows the after the is because without the there's no data to - I'm reasonably sure that you can effectively consider them as one operation in this case (i.e. the restriction conditions will be part of each index scan, rather than the index scan copying a whole lump of data and restrict throwing most of it out). 

For Aurora Postgres, there's two relevant cluster-level parameters (note they're not instance-level parameters): and . I haven't tested this myself but you should be able to modify them in the usual way using DB Parameter Groups. 

First of all, if this is homework, please tag it as such. Secondly if it's not homework and you're doing this in a professional environment, get a professional to do it (or at least to thoroughly scrutinize your final design). Schema design underpins your application design, and flows on from clarity in business requirements and how well you understand those requirements. If you don't completely and clearly understand the requirements, your schema is going to be miles off the mark. And, to actually answer the question (at least as best I can with the information given), I'd keep the staff in one table, however you may want to create ancillary tables for each 'type' of staff member (doctor, nurse, janitor, admin, etc) to store data specific to that staff-member-type. To give an answer that's any more specific, the requirements need to be more specific. 

Re: Shrinking. I see so many people getting their claws out at the very mention of 'shrink' and 'database' in the same sentence, so time to clear things up a little. Shrinking data is baaaaad. It literally turns your indexes into quivering shells of their former glory. Don't do it. Shrinking log should not be done routinely, but if you have a ridiculously outsized log (i.e. in one case I saw a 40GB log for a 100MB database, due to whoever set it up putting recovery model to full then never dumping the transaction log), you can shrink the log (after ) to reclaim that space without any ill effects (although, obviously, the will chew up I/O while it's running). PS: Speaking of log files, check your log file size and autogrowth settings. Small autogrowth settings can lead to underperforming log I/O (due to a poorly-explained feature in SQL Server called Virtual Log Files), particularly on batch transactions. 

I'm a huge fan of the event-driven approach for a scenario like yours. Meaning, I don't want to sleuth who did it after the fact, I want SQL to tell me when it happens. For that reason, I'd suggest a DDL Trigger. Here's an example: 

I have also been able to perform index maintenance. This works in my SQL 2012 or newer environments. For instance running identifies the indexes on the table, and from there I create and run these commands: 

You can enhance error handling with Extended Events to overcome the TRY...CATCH shortcoming you are experiencing. The design pattern involves these steps: 

1st Attempt This was slower than all of Erik's queries I listed here...at least in terms of elapsed time. 

Here are two queries I have used to compare permissions between database users. The first shows the database roles (if any) to which the database user has membership. The second shows individual GRANT and DENY permissions. 

Marcello, if you absolutely have to be able to achieve your goal with a single script, you could try iterating through the databases and running TSQL in the context of each database. An example is below with sp_executesql running sp_executesql from/in each database in the cursor loop. Note that @Tsql is static here, so it's declared outside of the cursor loop. However it could just as easily be built dynamically within the loop. This approach isn't easy. It might even seem a little convoluted. I'll let you be the judge. 

There's a lot of moving parts there. But you might be able to make it work for you. I wrote a few related blog posts about TRY...CATCH that may be helpful: The Unfulfilled Promise of TRY...CATCH Enhanced T-SQL Error Handling With Extended Events Part 2: Enhanced T-SQL Error Handling With Extended Events 

Note that if you have a front-end built in Access and just want to shift the database away from JET (Access's internal DB engine) onto a "proper" RDBMS, you can do so by migrating the data across and setting up linked tables inside Access to the new data source. 

You need three tables - , and an associative table for the many-to-many relation between them (a can have many s and an will have many s), call it or something. and will have autoincrement fields as a surrogate primary key, will have FKs to both and , like so: 

If you distribute your application with scripts to build the database (not a database backup or something like that), then in theory it should make no difference which version (of 32-bit and 64-bit) you develop against. Back when 64-bit was new and shiny (which was SQL Server 2000), there were some advanced features that 64-bit didn't support, but in 2005/2008 they should be functionally identical. (Not that it should matter if you're using Compact Edition, but there are some configuration differences between 32-bit and 64-bit once you get to the point where your DB performance is an issue.) 

I didn't mention the PK on , for efficiency it would be a compound PK on , but there's also an argument to be made for a seperate surrogate primary key (which I personally think is a waste of space, unless you need to allow for multiple s between one and one ). For a purely associative table in a many-to-many relationship, though, the compound primary key should work fine. Edit: The complicated part of this isn't in the relational design, it'll be in the application code, because you'll need best-match/partial matching in order to show users the groups that most closely match their interests (rather than having accidental splinter groups all over the place). And if you want to out-Facebook Facebook, you'll need a smoother and better user experience than they offer. 

This could easily be put into a job step for a SQL Agent job. I tested this on SQL Server 2012 w/ SP3. I've also run similar code in SQL 2008 R2 and SQL 2014. I don't know for sure if it works in SQL 2016--please let us know if it does not. Other Notes The commands worked when I was logged in as a [sysadmin]. It also worked if the login was not a member of [sysadmin], but was the database owner. Membership in [db_owner] alone was not sufficient. (Again, this was on SQL 2012 w/ SP3. YMMV.) 

Background: I have numerous databases with a large number of VIEW's, and an extremely large number of SYNONYM's. For example, one db has more than 10k VIEW's and 2+ million SYNONYM's. General Problem: Queries involving (and system tables in general) tend to be slow. Queries involving are glacial. I am wondering what I can do to improve performance. Specific Example This command is run by a third party tool. It is slow in both the app, and in SSMS: 

Here's a blog post about handling events with DDL triggers with some further explanation: SQL Server Event Handling: DDL Events 

I'm having difficulty "optimizing" indexes as a minimally logged operation. Before the index maintenance is performed, the database recovery model is switched from FULL to BULK LOGGED. Depending on the fragmentation percent, each index is the recipient of a REBUILD or a REORGANIZE (or no action is taken). After index maintenance is finished, the recovery model is reverted to FULL. One database in particular is causing me some pain. The datafiles are about 64GB (including any free space). A defrag operation bloated the log file to 38GB, until it filled the logical drive. Then the level-17 alerts started rolling in. I tried duplicating this in a test environment. Numerous attempts were made with different recovery models, index REORG vs REBUILD, different transaction isolation levels, read committed snapshot ON vs OFF, different index fragmentation levels, etc. Index REBUILDs on DBâ€™s in the FULL recovery model always bloated the t-logs. No other testing variations did, however. This was frustrating because I could not duplicate what was happening in production. What am I missing? How can I optimize indexes without bloating the log files? UPDATE 09/23/2015 Not surprisingly, Ola Hallengren's name came up soon after I posted my question. Although I don't use his scripts, I am somewhat familiar with them. For those who are interested, there is a wonderful PASS session video--Ola is the presenter. Near the 48 min mark, an audience member asks a question similar to mine. 

How often you need to run index maintenance/rebuild stats depends on your database load, specifically how often your data is modified (i.e. //). If you're modifying data all over the show (i.e. a staging table for a weekly batch process), you probably want to update stats/reorganize indexes nightly. If your data is rather more static you can probably make it a weekly or fortnightly schedule. 

(change table names as appropriate for Hashtags and Words) Or, slightly more complicated, get the top 10 words for all tweets mentioning a specific hashtag: 

(I'm assuming SQL Server in this answer, please clarify which RDBMS you're using if it's not SQL Server) The index you want is: 

I know I just spent this entire post detailing why EAV is a terrible idea in most cases - but there are a few cases where it's needed/unavoidable. however, most of the time (including the example above), it's going to be far more hassle than it's worth. If you have a requirement for wide support of EAV-type data input, you should look at storing them in a key-value system, e.g. Hadoop/HBase, CouchDB, MongoDB, Cassandra, BerkeleyDB. 

is a session-level command in Sybase ASE, it's not a server-level setting (if it was a server-level setting you'd be able to alter it via ). Can you run wireshark (or something similar) on the packets being sent from the JDBC client to see if it's setting showplan on as part of the session initialization? That said - showing the plan should not affect database CPU or memory usage, the plan is generated by the query optimiser anyway. However, showing the plan will increase network utilization. 

I have been able to update statistics on the underlying system tables. This has worked in my SQL 2008 R2 or newer environments: 

If the developers have individual logins, you might consider a DDL Trigger. Here's an example for the , , and events. 

Here's an example that can be run manually in a single batch from SSMS. Just make sure to replace all occurrences of "2016" in the script with your SPID. 

I have inferred that the Windows OS does not actively "push" the OS level memory availability status to SQL Server. Rather, the SQLOS Resource Monitor must actively ask for it via the Win API function. This would seem to be an asynchronous operation. Am I interpreting this correctly? If my reasoning for #1 is sound, what causes the Resource Monitor to get the OS level memory availability status? Is is performed on a schedule? Event driven? How much time might pass from the moment there is low memory at the OS level to the moment the SQL OS Resource Monitor becomes aware of it? When SQL Server reduces memory usage to free memory back to the OS, just how bad is that? Is this an "expensive" operation that should be avoided at all costs? 

An Event Notification can be used to both monitor for DBCC command events and respond to them. The script below creates all the necessary objects, which I would recommend be created in a separate database. I've added comments here and there where appropriate. Most of the script can probably be left intact, except for the stored proc, which you will want to tailor for your needs. If this solution doesn't fit your needs, you can simply drop the database.