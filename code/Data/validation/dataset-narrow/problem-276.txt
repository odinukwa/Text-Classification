The official documentation covering unusual errors describes DIANA: PL/SQL is based on the programming language Ada. As a result, PL/SQL uses a variant of Descriptive Intermediate Attributed Notation for Ada (DIANA), a tree-structured intermediate language. It is defined using a meta-notation called Interface Definition Language (IDL). DIANA is used internally by compilers and other tools. At compile time, PL/SQL source code is translated into machine-readable m-code. Both the DIANA and m-code for a procedure or package are stored in the database. At run time, they are loaded into the shared memory pool. The DIANA is used to compile dependent procedures; the m-code is simply executed. I see why you are interested. Access to the compiler underlying PL/SQL is tantalizing if you like looking under the hood. Nonetheless I would be reluctant to invest a lot of time when the same goal can be accomplished by parsing with PL/SQL. As someone who has used your site many times I can only thank you for having an inquiring mind. 

If you just want to run the job now then don't add a date/time. You can use this as a test case, run as an anonymous block to verify it works then modify your package. 

When you look at the view the data type is Number with no precision for all of them. This appears to be a consequence of the UNION as just 

Try moving the program to a file path where you have rights to save. I have not needed to run it with admin privileges. I can run the current version, 4, from a networked drive, if I use the sqldeveloperw.exe shortcut which you can find by poking around in the program folder, (possibly the bin)? 

Sounds like a data pump using a network link would do the job. It's always preferable to run it during quieter hours but if your source database is not under powered then you might be able to run it during regular hours without your user's noticing any difference in performance. From the article 

This bat script must be located in the same folder as the dump file which should be on a folder off your main drive for convenience sake. 

Based on Phil's test case I backed up the the production database using exp. I dropped the editioned user, recreated them and imported the data with no issues. The user had views and packages which were versioned but as long as you set the edition to the most current one you are good to go. This query showed that the recreated user was not using editioning 

I implemented what you are trying to do this way: the transformation to a user defined object happens in a packaged procedure that is called by triggers. 

this is a clean design but if each type of thing generates ten or twenty log entries per thing you don't have to do too much to have a huge table for application_logging with a million entries. Users complain it's slow to see the activity log. The real question is: 

There is no in-place upgrade possible but the existing exp utility is still supported for importing dmp files. Some of the issues you need to research are: 

you have not shown the roles TEST_USER is granted. Likely they have been granted the DBA role which allows ALTER USER. see the documentation for details. This query will show the roles a user has 

The answer to your question is, no, you cannot connect to MySQL using the current oracle instant client. 

One solution is to have both databases using the same character set. Considering your destination is a free XE version why not do a new install? Just make sure you use EL8MSWIN1253 character set and the same UTF version. 

Why go to all this extra work? This solution breaks your project into two issues, the upgrade and the character set. Upgrading databases is not to be considered lightly. Many times it is not the upgrade that is the problem but peripheral problems like interactions with applications or other databases. Custom PL/SQL code should have no problems but you never know. If there is a problem people will be looking to you to explain how you were diligent and careful not how you saved a day or two. 

All of these are possible but some are not advisable but they all need more tables and storage. Also keep in mind that you are more likely to hit the limits for memory depending on how many concurrent users and the connection pooling methods used. "Other hardware restrictions remain the same with CPU usage capped at one physical core and memory at 1GB". 

I believe this will do the job but the databases I have access to at the moment do not have text indexes. I will confirm later today. 

Here is more information on how I use AQ between databases. I am not an expert and got most of the code from the Internet. Oracle documentation is lengthy but did not really help me. First create the queue: 

yields NUMBER, NUMBER(1), NUMBER as the datatypes Is there a workaround to force Oracle to derive a precision in a view with a UNION? Edit: @Phil asks the desired result is. He is correct that Entity Framework has to have a precision to work with so the desired datatype from the sample above would be NUMBER(1) Edit: @Phil for this version Oracle Database 11g Enterprise Edition Release 11.2.0.1.0 - Production on Windows 32 bit a view with a union gives a view that has NUMBER with no precision. Entity Framework interprets this as int 64. I am looking for a solution that shows the NUMBERS with a precision that Entity Framework can understand as a 32 bit integer. Any precision from NUMBER(1) to NUMBER (9) does the job. 

The Action is admittedly crude but versatile. The requirements were to funnel changes from multiple tables to another database where further processing was required without touching the application code. A typical call is to a package from a trigger, other procedure or job. 

Either way, do not take anything from my answer or anyone else's answer as valid. Oracle licencing is complex and a final word can only come from them. Contact your representative and ask for clarification. 

This question is caused by the assessment of the team that editioning is not used and does not offer any benefits when we have generous time periods for scheduled downtimed. It prevents any materialized views being created by the editioned user if they select non editioned object from another user. This is required for one project due soon and a data warehouse in the near future. 

Watch out for character set conversions, you do not want Oracle truncating or replacing characters. Set the option to the same character set in the scripts for the export and import. EDIT You cannot use the existing dump file but you do not have to upgrade the older Oracle version. You need to create a dump file compatible with the older Oracle version. 

Yes you do. If you install the Oracle software you need to pay for it. However numerous exceptions exist including 

You did not specify open source or No-SQL or the platform where the database would be installed so your choices are not limited. Most databases will do what you want. I grant that #8 would require some customization so changes by a group can be rolled back. As an example: Oracle 1) key value: storage can be done in a table 2) mostly read only usage does not enforce any choice of database. 3) if latency is important spend some money on solid state drives and good hardware combined with good table design for your application 4) use discs for values: what is driving this? Is there a defined end user requirement that needs this or is this a requirement looking a problem? 5) all databases can do linked or individual transactions 6) Oracle has the flashback query to allow you query the state of your database at a point in time. This can also be accomplished by detailed transaction logging or even restoring from a backup at the specified point in time. 7) all changes attributed to a user. This is implicit in connecting to a database that has detailed transaction logging. If your application is pooling connections and allowing anonymous edits this is not a good practice. 8) rolling back all changes by a group of users: this is harder to do. It can be done by forcing all insert, update, delete transactions to be done through stored procedures which log everything 9) why does it matter whether transaction logs are kept in RAM? You need them, let the database decide where to keep them. 10) I can't think of a commercial database that does not allow online backups. 11) accessible from a JVM. This is possible from Oracle any many other products. I think there is context missing from your question that would explain your use-case better. What did I miss? 

I am using Entity Framework and Oracle. Oracle's ODAC seems to have a known bug where data of type NUMBER is cast to Int64 by Entity Framework. I thought a view would assist me but 

I have a search page where users can select from multiple variables. When the page is submitted to a package in the database a cursor is constructed and opened. a snippet to show what I mean 

If you are using the old imp method the users and tablespaces must be created first. This includes grants, database links and any advanced queues you may be using. A sample script could look like this in Windows imp a_DBA_user/apassword LOG =C:\dump\logs\import.log file =user01.dmp fromuser=user_name touser=user_name The newer datapump method works wonderfully. You don't need to create the user but I would create any custom tablespaces you may have on the original. A sample data pump call requires you to create a database link to the original database and a directory. This creates a directory 

I'm not sure what you really want but the official documentation is a good place to start, all 86 pages of it. Just thinking about Oracle error codes gets me all nostalgic for those cryptic messages 

"Safe" is relative and involves a tradeoff between access and privileges and application and data security. The factors that you would night want to consider are: 

You need to know when the triggers were disabled and re enabled and that all transactions stopped at the start of the migration and only resumed after the migration. Before, during and after the migration you need a log of all the actions that were taken so you can improve the process. A better way to do this would be to add a parameter: 0 or 1 to disable or enable the triggers something like 

Of course the best solution is to move to DBMS_Scheduler but until that is done how can I ensure that the timestamp that is inserted is the local time? 

I work with a database that has this solution that I outline below. I don't like it because the base table has an entry for each type and it ends up being a huge table that is slow to query. --base table 

Looks like there is already a job on the imported database with that job number. Just recreate the job on the import database. The job looks to be a queue so you could, depending on your business procedures, probably run the new job to test that everything is working. 

1) When to be sure that your database design is perfect? Your design is never perfect because the business logic and amount of data is always changing. Perfect is difficult to define I've seen systems that were great on deployment but had poor performance after a few years of data were added. The regrettable trend to treat a database like a black box by some application developers means some databases are deployed with critical tables lacking primary keys or indexes. Perfect to the CIO because they got the application delivered on time and on budget could be a pain in the butt to the developer/ DBA who has to deal with the problems. Here are some of indicators I would look for which indicate your design is ready to go a) Extensive use of primary keys, unique keys, foreign keys, indexes, more so on the larger tables ( I only mention this because I've seen commercial products which lack this) b) Application logic is duplicated as far as is practical in the database with the use of constraints and default values. You cannot capture everything but just knowing that there will always be a value for an entry provides peace of mind. c) Test, test, test: test from the user perspective on data entry, test from the manager perspective who wants an overview, test from the analyst perspective who wants to see trends If you find yourself joining 9 large tables which require full table scans in order to find out what work is assigned to a user then maybe you need to reexamine things. Not everything can be simple but excessive complexity to answer user needs is a hint of trouble to come. d) do some daydreaming on how the database could and could not be extended. If I need to add a new property to a unit of work how hard is it? Can it be done without table changes? If you are asked to add a new type of work how hard is it. (Work could be a product, a transaction, a case) e) people and organizations provide hours of work for me. How easy is it to create, edit, de-duplicate and report on them? f) how many user languages are you supporting? What character sets will be required. If, for example, you intend to support English and Spanish, what happens if the design must be extended to cover French and Italian? 2) Is returning to the data base design to change some issues (like adding new column, delete a column or change data type or add new table or ....) considered as a bad practice or is it normal? I would say it is normal for an application where the business logic changes frequently or the end user requirements are being added to. 

and moving the job to DBMS_SCHEDULER. Now, when it runs the correct (local) time is entered in the logging table.