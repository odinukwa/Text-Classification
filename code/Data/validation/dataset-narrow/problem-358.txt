Before you switch to the bulk-logged recovery model, take a tlog backup Switch the recovery model of the db to bulk-logged Perform your minimally logged operations (e.g. index maintenance, etc.) Switch the database back to the full recovery model Immediately take another tlog backup 

When you restore a database that has encrypted data to a different instance, with a different Service Master Key (SMK), the data won't decrypt properly because of SQL Server's Encryption Hierarchy. The "restore" of the master key you mention is really decrpyting the data using the Database Master Key (DMK) password and then re-ecrypting the data with the new instance's SMK: 

@Scott Hodgin's answer touches on this, but basically the best approach when generating Dynamic SQL Strings that are client/app facing is to utilize sp_executesql. While not entirely foolproof for eliminating SQL Injection attacks, sp_executesql is likely the best you're going to get. The article Scott links to by Aaron Bertrand is pretty straight forward, but to quickly summarize the benefits of sp_executesql over other approaches is that it: 

To summarize what we hashed out in the comments, yes, you will first need to setup a file share with proper permissions as part of the Prerequisites outlined in the Configure Log Shipping Guide provided by Microsoft. Once you have the fileshare created, you should be able to connect to it using a UNC path similar to 

In case this helps anyone else, an alternative approach to the using double single-quotes to represent a single single-quote in a string is . That sentence is hard to follow, so as an example: can be represented using this approach as 

As SqlWorldWide stated in his answer, it's not possible (using out-of-the-box log shipping). A supported workaround is to install a third instance on the server, , and put on that instance with a different schedule. Your server will need to be of sufficient size, but this will allow you to have two different copies of the database on two different recovery schedules. If you can't install a third instance, and you're not afraid to code up some custom routines, there is another option available. Basically, log shipping is a process wrapper that will automagically backup a database, copy the backups to a remote location, and then restore those files to a different database. There's really no mystery about what's going on here, but the routines make this process easy to manage. With sufficient effort, you can customize your own restore routines for the database as the formal Log Shipping jobs for will already have the backups located in a common area for you to reference. You'll need to remove as a log shipping candidate and develop routines that restore the outstanding logs when/how you desire. All backup information will be located in the tables on your primary instance, so you may need to query that over a linked server to generate the restore scripts. This isn't going to be an easy approach, but it's an option if you must have the functionality you want and you've got time to customize some restore routines. Obviously the downside is won't show up in your log shipping monitoring reports, but it sounds like you want this location more as report target instead of a failover candidate in the event of a disaster. 

Regardless what I've tried, any new Control Flow task always defaults this property to False which has caused some false positives when reviewing successful package executions. I'm developing within SSIS 2016, but this behavior doesn't seem to be version specific from what I've seen. 

One possible solution to your issue is to take advantage of Partitioned Views. This functionality has been around forever and is often overlooked, but a Partitioned View will provide you with a way to sidestep your data limit by adding new data to a different table where the column is a datatype. This new table should also have a better partition function/scheme underlying it and hopefully will be maintained a little better going forward. Any queries that are pointing to the old table would then be pointed to the Partitioned View and you'll not run into a data limit issue. Let me explain with an example. The following is a trivial recreation of your current partitioned table, consisting of one partition, as follows: 

I suspect my Google skills are just insufficient today, but I'm looking for a quick TSQL script that will identify all SSIS packages in the Package Store that are using a given Environment Variable Name. For instance, when I Configure a package and look at the Paramaters page, I can choose a given Environment Variable. In the following screen shot, the environment variable I'm concerned with is named . I want to find all packages using this environment variable so I can perform some investigation/testing before making any en masse changes. 

This returns results that show how much size a file is using according to the OS (), how much data is actually contained within the file () and some other information about the data files. The script is ugly, but it works all the way back to SQL 2000. With this output in had, what I would do is look for any files on User Databases where the column is low and the database is not (I'll talk about what to do with tempdb further below), and then run operations against said files. This will give back the OS some space, but by no means does this mean your job is done. Once this is complete, the next step is to identify what data you can purge or which objects you can drop. Purge old data, drop unused indexes, and then rerun the script and see what else can be reduced in size. Again, rebuilding indexes is recommended, but as you've already seen, Index Rebuild operations may cause data files to grow so you may have to weigh if a defragmented index is worth the extra consumption of disk space. Other options available to you will depend on your SQL Server Version and Edition. If you're running Enterprise or SQL 2016 Standard SP1 or later, you can take advantage of Table Compression. This can help reduce the space used within your data files as well. If you see that tempdb is consuming a large amount of space which is relatively unused, reduce the the size of the tempdb files, but make sure that if you have multiple files, you shrink them all to the same size as per MS's recommended guidelines. Another word about tempdb, shrinking it is often an act of futility because there's often activity occurring within the database, so you may have to restart the instance to get these files to shrink down as desired. Here's MS's documentation on the various ways to shrink tempdb. Another possibility is that you have non-database related clutter on your drive(s). In this case, simply deleting these files may be the fix for your issue. I find using WinDirStat helps quickly identify where some of the larger files are. Be sure you know what you're deleting though. Sometimes there are lots of "seemingly worthless" files sitting in hidden directories, such as the WinSxS folder that look like they could be quickly deleted. Don't recklessly delete files; make sure you only purge files you know are safe to remove. Finally, ask for money and buy some additional storage. Your server just may be at capacity and it's time for an upgrade. Hardware isn't always the answer, but sometimes it can be the best answer.