I'm not sure what your dynamic columns are so I cant 100% promise this will work, but if you get any column errors you should just be able to put ts. infront of it to declare its from that table Ste 

to generate your static random generator start point the $usersrandomnumber acts as a starting seed see the mysql documentation here: $URL$ it will involve you storing the user's seed in a table somewhere, or cookie /local/server storage but will get you what you're after 

If you run it'll show you all the connections that are running against your DB. Running can show how many connections are active You may think that a high number of connections would increase the CPU but ultimately it depends what those connections are trying to accomplish, if they aren't doing much but are preventing you're main application from running their connections then the normal processing level will go down while you are not running your normal processes The annoying thing is to be able to run the show processlist command you need to actually be connected to the DB which you wont be able to as you're at max connections. A slight work around is to increase your max connections (if you're able to) and set up some monitoring somewhere that runs every so often (every 5 minutes would have caught that) and when you're over 50 connections, run a command that dumps out all the active connections somewhere so you can review later. 

I would not use GetDate() for this fixed-period scenario as the end date for queries, because the reports are only meningful for specific periods. A better approach is to use a specific run-date for each reporting period. That run-date is not the current date. For example, let's say one of your KPIs is the sales amount for the first 6 months of 2013. Your system should use end of June as the end date not the current run-date regardless of when the report is run. If you do this, you'd not have to touch the data. 

Here the query fails because you are trying to compare a numeric value to an empty string. The data types is not the same. You need to compare values of compatible data types to ensure correct result. I don't fully understand what you mean by Q3, so I will not be able to help with that. Be very careful with Nulls they are tricky. 

Your justification for why you separate the tables is not convincing to me. I suggest you consider table partitioning rather than table splitting into 2 tables. You did not provide enough information, so here is a guess. The reason it is running slow is probably because you don't have the correct indexes defined. You should at least have an index defined on each table for mainDateTime column. Sorting can be provided by adding ORDER BY to the result set. Other reasons for bad performance may be memory, heavy table access by other users, row size, table table fragmantaion, locks, etc. 

It depends on what you need, the basic data for conenctions can all be found in table Main points of reference are 

SQL Server is designed (along with most db engines) with security in mind the main areas you'll look at for viewing users are 

To answer question 2 first 2 A primary Key is NOT always the clustered index, it can be the clustered index and in the majority of cases is the way things are done, but it isn't always the best for your data. The culstered index is the order in which your data is physically stored on the disk whereas the primary key (which can be composite) designates the field to be a unique field and is beneficial for not inserting duplicate values and for foreign key lookups if you wish to do joins (In summary for 2, yes you can add a primary key without a clustered index) NOTE: sometimes if you want an identifiable row you can add a new field to the table to simply act as the primary key (not always advisable, but can sometimes be the a solution to improve performance) 1 Adding a primary key can change performance, however the only true way to know if it's going to improve performance is to test it, if you have a pre-live environment consider adding it to there and running your queries across it. If you have queries that run as joins to this table on ID,AID,BID all together (Sorry I'm not 100% sure how these are all coming together) the Potentially create a composite primary key across all three which means when anything wishes to get data with this table comparing all three of those it can find that row with ease. (Hope this makes sense) 3 Adding a Clustered Index completely depends on your data, once again a pre-live environment would be an ideal situation for testing. A few things to consider when creating a clustered index, what data are you retrieving and what are you inserting (this is a general example)If you're inserting and retrieving data that is the most recent data then a clustered index on the date field sounds the best idea, however if there is a LOT of data going in and out you will have very high contention on the most recent pages in your table, an alternative would be to have the clustered index around a category that those dates are on, eg client, this would mean that the data is grouped by a client which is more likely to have data gathered by, and spreads the read write load across the disk / disks If the data retrieve is very random then a clustered index is quite pointless, if the data you get back has no real order to it then a Heap is completely acceptable. Ultimately there is no be all answer to should I add a clustered index or a primary key because every situation is slightly different and will react in different ways. If you have a pre-live environment (even a cut down version) can help make your decisions. Personally we have tables with primary clustered composite keys, and some tables that are simply heaps. Hope this helps (and makes sense, sometimes I find I ramble) 

To make an efficient compare of all columns between 2 tables, you could use a UNION method with GROUP BY as described in Diff2Tables. Alternatively, if your profile table can have rows deleted from it without cascade delete, you could, drop indexes that are not for PK of the profile, Delete rows having the same key in both tables, and insert the rows from the other table into the profile then re-build the indexes on the profile table again. This may sound bad but in fact it is not that bad. This is a typical scenario in Data Warehousing. 

Do you know of an industry standard that is designed to describe the metadata contents of a relational database (or part of it, such as tables, columns, etc.)? Do you know if a tool exists that does export the metadata of any of the top 5 RDBMS tools in any format such as XML (without having to type in DDL in the syntax of particular database)? Thank you. 

Address modeling is not universal. A universal implementation would be too complex for most applications. The different models vary according to model type (OLTP vs OLAP), country rules, customer type (organization vs. individual), how critical the address data is, etc. As said, you should separate city. Separating City will make the problem of having different city names in the customer table go away. Reasons are: 1 - Separating City names in a separate table allows you to run queries like: give me customer distribution by city and show cities where no customers are there. 2 - Allows you GUI to always refer to the correct list of cities. 3 - Allows you to maintain city information without touching your program code. 4 - Allows duplicate city names (in different states) if you use the diagram below. If you are sure that your application is for 1 country, then don't add the country in. Also, I noticed you don't separate the street address information into 2 columns which is common in North America and that you have no Zip Code. Review the country's postal address requirements from postal authorities to make sure your design conforms to them. Here is a common representation of address in an OLTP application. Here the PK of City is 2 columns, namely, CityID and StateID. A variation on this version would be to use single ID for each table (as a sequence number) and end up with a FK composed of 1 column only at the customer table. It all boils down to business rules and requirements. 

to catch up with the backlog from the night before (when most of our deletes take place) I'm wondering if there's any way to change the default settings from 5 seconds and 10 pages to say every second or run over 20 pages, is there any way of doing that or should I continue just spinning up multiple cleanup procs to clear out the data, or if there's any other actions that can assist with this Re-indexing runs on most effected indexs atleast once a week (most are every other day) SQL Server 2012 Enterprise SP3_CU8 (upgrading to CU9 tomorrow) on AlwaysOn High Availabilty Cluster also with replication (distribution on a separate server) 

you cant have multiple databases 'in use' at one time, as there are a lot of things which change between databases even within the same instance. You can access databases outside of the currently accessed database like so: 

TLDR; Check for conversations being left open completely. In our system we re-use conversations and have a table dedicated to holding these conversations that are usable, however the dev team setup a new service broker without my knowledge ages ago while I was off, didn't set up these conversation points and didn't set any thresholds on the alerting. When the new system has been turned on the conversations are being opened but not closed properly and since there aren't any in the pool it is just creating a new conversation (we reached 7.1 million conversations for one service broker) My steps for fixing was to create and record the 20 conversation handlers that I require for that service broker and record them into our table. This stopped the growth of the tempDB to stop the risk of the DB going down. Then came the long process of closing off all the un-used conversations 

The process step is not meaningful in your case unless a process version is defined. So you could say that process 1 has its steps executed in this order (a, b , d, c) when the process was in version 1, but in version 2 the step execution order changed to be (a,b,c). So I think that a process version is important. The diagram below represents my suggestion. The silly thing about this is that if you change the order of a step, you have to insert all steps again in the new order, but in this case, it won't matter either in space or time. 

I guess that the first option is fastest (although it does not look very slick from programming perspective!). This is because It deals with exactly N rows (where N is the table size) and has to do no search or sort like method 2 or 3. A test with large sample should prove the point. As yet another option to consider (as if you need more!), is to create a materialized view over your table. if your table size is in 100s of thousands or more. This way, the min value is calculated while the row is changed and the entire table would not have to be processed with every query. In SQL Server, materialized views are called Indexed Views 

Gives the list of all the IDs cursor through those values and simply run on each of them. After the process has finished the temp DB space will loosen up (Note it does not do it as you close them, it seems to do it in large chunks when you're no longer working on creating / ending them (I cant guarantee this is how it works, just what I observed after stopping mid process and tempDB getting some space back) 

I may be understanding this slightly wrong so I apologise if I am. If you have two databases that you need to be identical and are on 2014 then use the AlwaysOn High Availability Group. Since you're data centres are at separate locations use the Async mode This will mean the database is kept completely up to date (all be it possibly with a few second delay) and you can have the secondary node as a read-only replica, this means that your alarm system can read into that database run all the checks etc you would normally. the Always on system keeps everything up to date, so if the connection drops, when it comes back online it will merge over all changes It also means that if your main centre goes down you can set it to automatically failover to the secondary, when the main datacentre comes back online it will re-sync with the (now) primary node, at which point you can fail it back over to your main centre. You can run this on multiple databases, so we have our main DB and our Admin DB synced across our nodes, however what runs all the jobs and direct actions on each side is not replicated so can stay independent of each other 

Typically you'd have an Invoice table that contains 1 or more sold line items (as a child of Invoice) and each line item's price (as well as possibly other details) would be looked up from a lookup table (that would act as a parent for the line item and would contain the price of each item). A line item in this case has many parents. The price table plays the role of a reference or lookup table. 

I suggest you follow normalization rules. This will minimize the write and update processing (and errors) and simplifies coding of the logic of write and update routines. You can tune reads in many ways. You could use materialized views or indexed views to keep track of counts without queering the records one by one. Don't sacrifice normalization for no obvious reasons. 

First, I connect 2 tables with 1 to many relationship. Workbench correctly creates FK column in the child table. 1-When I change the name of the PK in the parent, the old FK name remains unchanged in the child. 2-The name generated for the fk is not following the standard pattern set in the options (even when using the defaults). Any suggestions to keep names in sync? (besides me performing the manual change)? Note: I am using Workbench 6.3 Community Edition. 

I cant speak for your compatibility, but general performance and security and longevity is always better with the latest version. (you'll most likely get another 2 years of support for a version 2 years more recent) The best thing I can advise is getting a dev licence (which by comparison cost next to nothing) for 2016 and use that to test your system to see if there are any issues and how easy they are to fix beyond the general these things shouldn't be used any more. There's also the question of time, since you've done a full sweep of 2014 it will be far quicker now to upgrade to that, but there's also you should now know all of the tests that need to be performed, so a second compatability test with a new system should go much smoother (and you've already got the fixed up to 2014 bypassing 2012 which had a lot of changes) so the number of required changes from your prepared system and what you need will be (in theroy) substantially less than the set you have already done. EDIT: SQL Developer edition can now be used for no cost with a visual studio subscription or visual studio dev essentials $URL$ 

Since NULL value is not the same as empty string in SQL, then the result you obtained is correct and makes sense in SQL terms. Think of NULL as "Not Defined Value" and as such it is not same as an empty string (or any non-null value for that mater) which is a defined value. From an English standpoint, null is the same as nothing but this is not the same in SQL. Now what do you expect this to return? 

What you are describing is more like a print log where you insert data about when and whom has requested the report. If this, is so, it is valid. But the part about hiding the data is not clear in your description. The data you are reporting should, generally,not be changed in the database as part of the reporting process. There are situations where you have to mark each printed report as 'printed' and hence update its status but from your description, I can't see that this is the case. 

"integers are cheaper to compare than characters, because character sets and collations (sorting rules) make character comparisons complicated. Integers are very good when using as an index column." - HottestGuide 

A relational database would typically contain several related tables. The relationships between tables use what is called a foreign key column which is basically the primary key of a parent table placed in a child table's list of columns (with other rules not listed here). For simplicity, you can say that relationships in a database depend on Primary keys very very much. Now if a value is changed, the system has to perform so many changes to coupe with this change. As an example, say you have a Social Security Number of a person and you keep track about this person's properties, cars, jobs, wive(s), kids and let's say you change the SSN value. The system will have to change the corresponding value in each of those related (child) tables. This process is not always good to perform on-line. Another reason is business related. Say you are issuing an EmployeeID Cards, if the number on the card changes, then the card is no longer usable and you have to print another one, not only that, but this means that the salary information has to change, the insurance information has to change as well as other information related to the EmployeeID. In business, sometimes not all this information is integrated or even automated, so this change leads to a lot of work and possible inconsistency of information in manual systems. Such changes may not only be localized to one organization and may have to affect other external organizations or systems, which would make life very difficult, because you have to ask your business partners to carry such changes on their end too. Yet, another reason for not changing key values is that when you do so, history tracking becomes difficult. All history logs and documents will not reflect the current value making it difficult to interpret or even find data in manual systems.