The following problem is a special case of k-medians. Is it NP-hard? Is it in P? Input: $n$ points $(x_1,y_1), (x_2,y_2), \ldots, (x_n, y_n)$ with each $y_i \ge 0$, and an integer $k$. Output: a set $S$ containing $k$ of the given points, of minimum cost, defined as $\sum_i d((x_i,y_i), S)$, where $$ d((x_i, y_i), S) = \min\big(y_i, ~\min\{ y_i - y_j : j \in S \,\wedge\, y_j \le y_i \,\wedge\,|x_i - x_j| \le 1 \}\big).$$ I think of this as each point "falling" vertically just until it either (a) reaches the x-axis, or (b) is aligned horizontally with, and at distance at most 1 from, one of the $k$ chosen points. The goal is to choose the $k$ points to minimize the total distance fallen by the remaining points, as shown below. (The input is on the left, a solution with $k=3$ is on the right. The cost of the solution is the total length of the dashed vertical lines.) 

Initialize all $x_s = 0$. Let $N=\log(n)/\varepsilon$. Repeat until $\min_e A_e x \ge N$: 2.1. Choose $s$ maximizing the partial derivative of Lmin$(Ax)$ w.r.t. $x_s$. (Explicitly, choose $s$ maximizing $\sum_{e\in s} \exp(-\sum_{s'\ni e} x_{s'})$.) 2.2. Increase $x_s$ by $\varepsilon$. Return $x/\min_{e} A_e x$. 

$S$ is in the intersection $M_d \cap M_t^*$ of the two matroids; $S$ has at most $d$ edges incident to each vertex in $U$, and the complement of $S$ contains a spanning tree. 

I believe it's NP-hard, by a reduction from min-balanced cut. Given a graph $G=(V,E)$ and integer $\ell$, min-balanced cut asks whether there is a cut that is balanced (has $|V|/2$ vertices on each side), and cuts at most $\ell$ edges. Given $G$ and $\ell$, construct the following instance of your problem. For each vertex $v$, create a set $S_v$ containing the edges incident to $v$. Take $k=|V|/2$. Then the (minimal) $k$-good partitions are partitions of the sets into two equal-size groups, corresponding to the balanced cuts of $G$, and your (unweighted) cost for such a solution equals $|E|$ plus the number of edges cut. Unless I'm mistaken :-). EDIT: Similarly you can reduce the following problem to yours: given a graph $G=(V,E)$ and integer $\ell$, color the vertices of $G$ so that each color class has at most $k$ vertices, minimizing the number of edges whose two endpoints have the same color. (The reduction: make a set $S_v$ for each vertex $v\in V$, containing the pairs $\{v,w\}$ not in $E$.) 

No. For the following graph the expected size of the generated matching is $O(\sqrt n)$, but any maximal matching has size $\Theta(n)$. The graph consists of a $k$-vertex core $C$ and a matching $M_0$ of $k^2$ edges and vertices (disjoint from $C$), plus all edges $(u,w)$ where $u\in C$ and $w\in M_0$. lemma 1. Any maximal matching $M$ has $\Omega(k^2)$ edges Proof. Every edge in $M_0$ is either in $M$ or is touched by an edge in $M$ into $C$, but all at most $k$ edges of $M$ can touch $C$. QED lemma 2. In your random process, the expected number of edges chosen in the random matching is $O(k)$. Proof. Clearly at most $k$ edges are chosen that are not in $M_0$ (as there are only $k$ vertices in $C$). For a given edge $(u,w)$ in $M_0$, the chance that it is chosen is $O(1/k)$, because, given that, say, $u$ is active and $w$ is passive, the chance that $u$ picks $w$ is $O(1/k)$, as $w$ has $k$ other edges (to $C$). QED (One thing you can prove about your process is that, for any graph, in expectation, a constant fraction of the edges will touch the edges in the generated matching. See e.g. here. Roughly, if you direct each edge $(u,w)$ from the lower-degree vertex to the higher-degree vertex, and call a vertex good if more than one-third of its incident edges are directed into it, then at least half the edges must be directed into good vertices (Lemma 6). And each good vertex has constant probability of being matched in your process. So, at least half the edges touch vertices that have a constant probability of being matched. EDIT: So after $O(\log n)$ rounds of your process, it will have a maximal matching with high probability.) 

Luca, since a year has passed, you probably have researched your own answer. I'm answering some of your questions here just for the record. I review some Lagrangian-relaxation algorithms for the problems you mention, and sketch the connection to learning (in particular, following expert advice). I don't comment here on SDP algorithms. Note that the particular algorithms that you mention do not run in nearly linear time. (There is a nearly linear-time algorithm for explicitly given packing or covering problems. See Beating Simplex for Fractional Packing and Covering Linear Programs.) The algorithms you have in mind typically have variants that run in a nearly linear number of iterations, but each iteration typically requires at least linear time as well. I discuss some of these algorithms below. Some useful functions Before we start, here are some functions that we will use in the proof sketches. (If you are interested in the algorithms, but not the proof details, you can skip ahead.) For any vector $y$, define $\mbox{Lmax}(y)$ to be $\ln \sum_i \exp(y_i)$. This function is an upper bound on $\max_i y_i$: $$\max_i y_i ~\le~ \mbox{Lmax}(y) ~\le~ \max_i y_i + \ln m.$$ Analogously, define $\mbox{Lmin}(y)$ to be $-\mbox{Lmax}(-y)$, a lower bound on $\min_i y_i$. For convenience in what follows, we use $g(y)$ to denote the gradient $\nabla \mbox{Lmin}(y)$ of Lmin. We use $G(y)$ to denote the gradient $\nabla \mbox{Lmax}(y)$ of Lmax. Explicitly, $g_i(y)$ is $\exp(-y_i)/\sum_{i'} \exp(-y_{i'})$ while $G_i(y)$ is $\exp(y_i)/\sum_{i'} \exp(y_{i'})$. Lmin and Lmax are smooth in the following sense: for any vectors $d\in[0,\varepsilon]^n$ and $y\in R^n$, $$\mbox{Lmin}(y+d) ~\ge~ \mbox{Lmin}(y) ~+~ (1-O(\varepsilon))\, d \cdot g(y)$$ and $$\mbox{Lmax}(y+d) ~\le~ \mbox{Lmax}(y) ~+~ (1+O(\varepsilon))\, d \cdot G(y).$$ Note that both of the gradients have 1-norm equal to 1: $|G(y)| = |g(y)| = 1$. (Throughout we use $|z|$ to denote the 1-norm.) Note also that, for a matrix $A$, the gradient of the function $x\mapsto \mbox{Lmin}(Ax)$ with respect to $x$ is (by the chain rule) $(g(Ax))^T A$. More explicitly, the partial derivative of the function with respect to $x_j$ is $\sum_i A_{ij} \exp(-A_i x) / \sum_i \exp(-A_i x)$. Likewise, the partial derivative of Lmax$(Ax)$ with respect to $x_j$ is $\sum_i A_{ij} \exp(A_i x)/\sum_i \exp(A_i x)$. Fractional Set Cover Fix a Set-Cover instance. Let $A$ denote the element/set incidence matrix. Thus, $A_{es} = 1$ if $e\in s$, else 0, and $A_e x$ is the extent that fractional cover $x$ covers element $e$. The LP is $\min\{ |x| : A x \ge 1; x \ge 0\}$. Given $\varepsilon\in (0,1)$, the algorithm is