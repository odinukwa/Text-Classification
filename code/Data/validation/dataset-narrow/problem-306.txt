When dealing with Data (Full, file, etc) or differential version of said backups the FirstLSN and LastLSN actually mean something different than is being inferred. FirstLSN when dealing with the differentials means that's the LSN which is needed in order start recovery with that backup (meaning that's where it's going to start the process, at that lsn). LastLSN when dealing with the differentials means that's the LSN at which any subsequent restore operations (in this case, your log backups) need to start. If you wanted to apply any extra logs, for example, the log backup that can first be applied needs to hold the LastLSN value in it in order to be applied. However, this definition changes when dealing with the log backups which would be your original thought on the matter. Edit: Just found this, which may help. Edit: Updated question so updating answer. 

This is not the proper way of measuring synchronous commit overhead. This means we "waited" on synchronous commits for something but can't be resolved to "My overhead is 12 ms". Well then, why? This is because in SQL Server when "something" hits a waiting point it adds to the overall wait time. This works out well when only 1 "thing" is waiting... but what if we had 2 things waiting, each on the same operation to complete... then we have 2*wait_time even though the wait time wasn't doubled. The proper way to measure this is by using either the perfmon counters under SQLServer:Database Replica - and . is how much time, total in milliseconds, it took for all of the mirrored writes to complete in that interval (1 second). is how many synchronous transactions we sent to the secondary per second. If we divide Transaction Delay by Mirrored Writes we get ms per transaction. That's the mostly accurate (without extended events in the debug channel) way of getting the overhead per transaction. 

Extended event catalog views fall under both sys.dm_xe* and sys.server_event_* views. The "active" or "running" event sessions have information stored in sys.dm_xe* (also global package metadata) views and overall server defined metadata stored in sys.server_event_* views regardless of state. Specifically since the question was about information on extended event sessions that aren't currently running, the metadata for that still exists and the basics of it can be found in sys.server_event_sessions catalog view. Joining this with other catalog views such as sys.server_event_session_events, sys.server_event_session_actions, and sys.server_event_session_targets (to name a few) can return most of the information that should be needed. 

Yes and No. Yes, Basic Availability Groups are available but I honestly don't believe it's the right fit for your situation. I actually don't believe Availability Groups in general is a good fit for what you've described. 

They don't have to be. It really depends on what you're trying to solve for. Remember that each secondary will have a copy of the database and will need to have the log blocks be transported to it. This may or may not be doable with the infrastructure available. 

Depending on the settings of the listener in the windows cluster, your clients may need to flush their dns cache (to pickup the new ip if they are legacy clients). Once the first DC comes back online we'll want to make sure to reconfigure and re-evaluate the situation. Resources that will help you: 

The shared network location in that wizard is only used in the wizard and only used as a temporary location to backup the databases to and restore from that location to any of the replicas that could host that database. Apart from that, it has no use and is not used internally anywhere in availability groups. You can easily change the value to whatever new value your storage team has given you, but you'll want to double check that all of the replicas have access to said shared location. 

The pages are asked for from memory like most things in SQL Server. The same process applies for as it does for other read operations in SQL Server. 

Create an extended event session for wait_completed and specify a SPID known to you. Run the query and give us the output or consume it internally. Give us an update on the output from #1. There is no magic number for determining bucket count for hash indexes. Basically as long as the buckets don't get completely full and the row chains stay below 3 or 4, the performance should stay acceptable. This is kind of like asking, "What should I set my log file to?" - it's going to depend per process, per database, per usage type. 

Dan, I'm assuming the account running the agent job is a sysadmin. In this case, the instance will need to have its' service account or service sid for the given access to the folder. SQL Server agent doesn't do the restore, it connects to SQL Server and impersonates the account if non sysadmin and then continues. In this case you're connecting to the database engine and issuing a command which will be processed by the engine. SQL Agent isn't touching the backups, the engine is. Edit: Explaining the posted agent output 

Where to go from here: I'd start by taking a backup with continue_after_error to make sure that you have something of a record. Then restore that backup to an instance on the same patch level as the one it was taken from so that all testing can be completed on a copy of the database and not the database itself. Use the copy and the older backup of the database you have on a second instance to see what data is lost and what may be able to be manually salvageable. This is time consuming and lengthy, but may be needed. If you're really stuck, call in a consultant to help you with this as corruption is a great thing to cut your teeth on and gives you some semblance of liability. Check the objects associated with the corruption. If they are tables that don't have any super important data (say dictionary tables that can be rebuilt or don't change much) you might be able to get away with manually fixing the table by using the older backup to script out data. Check your storage subsystem, make sure you have the latest drivers, firmware, etc. Check for any failed drives in the array/san/nas/etc. Double check Ethernet cables, fibre cables, switches, etc. Find the root cause of the corruption or this may happen again. Run a health check on your storage subsystem/motherboard to make sure something isn't faulty with the hardware or controllers. Lastly, update your resume. 

If SQL Server created the listener for you, I highly doubt it is setup incorrectly... I won't rule out edge cases though. 

Sure, you log it manually when you figure out what the root cause is. There is nothing automatic that will magically give you a root cause for the failure - that's your job. 

I have no sources for this but have backed up my information through screenshots and configuration options that change settings. Everything in this answer can be found by looking through the product itself and knowing how pieces of windows work (ex: filter drivers). 

It may... only if you have an unordinary small workload. Just the worker threads alone would make me assume that you're not going to have a good time. This coupled with having 4 instances per server, that don't communicate with each other, that are all trying to step on each others' toes (so to speak) leaves me thinking this, "I am really glad I won't have to admin this!". My personal take I would step away from thinking of AGs such as this. If you want to consolidate, that's great and I'm all for it! However, it needs to be done in a way that doesn't end up hurting you or your customers. If it were me, and I was tasked with the same thing, I'd immediately push back on "6" servers. I don't yet know how many I'll need... unless of course we're not going to do any scientific research or testing - in which case I'd escalate my concerns. We don't know things such as the number of total databases, how much log generation each database creates, etc., which we'd need to start gaining an understanding of what will be needed. Undoubtedly you can consolidate these. Is it even the right thing to put them all in a single cluster? I wouldn't. Each cluster is a fault domain, and while there are some really awesome features such as distributed availability groups that still ends up being more than a single cluster. Think of it this way, all of your AGs are in a single cluster - what happens if you have an issue with said cluster? Are all of the AGs now down? Maybe. Probably. Either way it's not something I'd want to be dealing with at 5 am. Some of the main things you'll see when you have too many AGs or databases in AGs: 

This means that the client driver is going to be re-pointed to this address; if the address is not reachable then the driver won't be able to connect and you'll have a problem - which you happened to run into. 

We'd need to see the cluster logs to see what it thought happened. Other logs of use would be the output of sp_server_diagnostics in the logs folder, the failover policy level, and potentially the always on health extended events files. There also started to be issues with the hardening of logs: