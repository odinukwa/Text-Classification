I am trying to grasp the idea of how a switch communicates. Does a layer 2 switch require the entire stack to do it's job, same for layer 3? If a switch can support SNMP which sits on top of UDP, it must have a full stack? I believe most switches have some type of network operating system if any at all, so it is hard to understand what a switch actually entails. 

Do standard NICs that support IPv6 have the ability to perform MLD? Essentially I understand MLD, but am struggling to find if it needs to be configured or if it is enabled by default on a host. Just want to understand how a host sends a Multicast Listener Report. Does it happen automatically when a Multicast Listener query is sent by the router/switch? 

The point is that it is impossible to protect against a fake arp/gateway at a public Wifi hotspot, you need to protect the traffic, and ensure you don't have a active MITM, event if the correct GW is used there can be a attack on the traffic stream behind the AP. Some Quotes related to SSL/TLS and newer HSTS protection. 

Do you have any solution for the public IPs? They are sort of scattered throughout the datacenter so there is not a single access point for these machines. It would be very helpful to have a large vlan with a /24 or so that spanned the whole network, see question 

Think of all the ports in the vlan as being one port, traffic between them never hit the vlan-L3 interface. Only traffic flowing between vlan's hit the vlan-32 ACL. Thus traffic from vlan-X to vlan-32-host will be seen as outbound by the vlan-32 ACL. And traffic from a host in vlan32, that hits the GW on it's way somewhere else will be inbound. 

If a layer 2 managed switch supports management functions and entails a "management plan" it must require an OS. If it is a typical network operating system, *nix based, does is have a full file system? Would I be able to write an application and run it on the cpu of the switch? 

How does the /sys/class/net/ethx/carrier field get set? I am trying to understand this field as it determines if a network cable is plugged in, but I am curious how it knows. This is specifically in regards to 10Gbase-KR and 1Gbase-KX. I have a little knowledge in regards to CMSA/CD, but that is only for half duplex and NPL - pulses are for auto negotiation. Is something going across the medium when no traffic is being sent? I believe it may have to do with IDLE symbols, but I do not fully understand them. 

A normal switch works independently of the rest of the network. A OpenFlow/SDN switch, when it receives a packet, that it does not have a flow for (Match + exit port) will contact a SDN controller(Server) and ask what must it do with this packet. The controller can then download a flow to the switch, possibly including some packet manipulation. Once the flow is downloaded to the switch it will switch similar packets at wire-speed. Why is centralizing the decision making so wow ? Having a central server that knows the network layout and can make all the switching decisions and build the paths gives us new capabilities. 

No a POE switch does not deliver enough power to run a POE+ device. Having plugged a Cisco 1500 AP into a normal poe switch and then struggling for a day to get a ptp link-up I can say first-hand that it is a bad idea. The device booted up and I could log in but the Radio was stuck in reset, thinking it was a config or antenna problem, I wasted a lot of time, before finding the small message in the log, that there was not enough power to powerup the wifi module. 

If UDP and/or TCP send packets via IP to an Internet Protocol Address, how can a layer-2 switch forward these? Is this possible or do you need a layer-3 switch? 

As a UDP segment traverses down the stack, each layer adds a header. What occurs for this to happen? Does the kernel do a bunch stuff then add the header? I am particularly interested in the transition from the network to data-link layer on an Ethernet system. Is ARP performed and then the cache searched in order to provide the next hope information? 

If I want to send an IPv6 UDP multicast message, how does L3 to L2 address resolution occur? I think I understand ARP and how this same idea applies to unicast, but I am struggling to grasp the idea of how it works for multicast. I am aware of MLD and NDP, but not sure how they come into play with my question. 

Problem1: You should have consistent ospf cost, the L3 routing is independent of the L2 hsrp gateway redundancy Problem1b: You host vlans hanging off the Nexus devices should be passive for OSPF, why do you want active ospf here ? Problem2: On your vlan2 (192.168.0.0/24) you should not have hsrp, if all the devices are using ospf to interconnect no need for L2 shared ip, only needed if you are doing static routing and need l2 ip failover. Problem3: If you have dual Nexus devices you probably have vPC configured, although you have not attached any config. vPC has specific rules for interconnecting L3 devices and traversing the peer link. see $URL$ Problem3: Recommendation is to use a dedicated L3 link (not Vlan) to connect to other ospf routers. 

Edit: Re-do the answer as the IP's in the question changed. Note1: It is WRONG to route different subnet masks in the same network/vlan this is just a explanations as to why it work's in certain cases for certain specific IP's. Note2: Even if the subnets provided were in different vlans, it would be a Broken configuration as the subnet's overlap IP routing on a host is configured with three pieces of information. 

I am trying to understand the purpose of a VLAN aware NIC on a server or work station? I was under the impression a switch does most of the work when it comes to VLANs. Is this another way of filtering traffic? Or does the NIC receive the traffic regardless? 

If an Ethernet link is up on my end, does it necessarily mean the computer on the other end is alive and I can talk to him? Or does it mean that my PHY is just ready to send data onto the medium? I know there are various ways to check "link state" and that there are operational vs administrative states of an interface. 

What are the trade-offs/major differences of Ethernet Flow Control vs Quality of Service? How to know when to choose one over the other? They seem to both be a solution for congestion. The obvious downfall of flow control seems to be, that it can congest the switch itself. There are no priorities assigned to traffic, therefore the switch buffers can fill in a hurry. 

For my explanation lets assume the mask on both subnet's is /24 255.255.255.0 On any single subnet, all the hosts (including the gateway) must have the same network portion (as defined by the mask), else they are not on the same subnet. 

You are correct the switch only knows/cares about the MAC. When you send a ping, it first generates a arp with a MAC(ffff.ffff.ffff) broadcast that goes to all ports asking for the MAC of a specific ip. Once the destination host receives the broadcast, is will see that it is a ARP for it's own IP, and using the MAC from the received packet, reply back to the original host, using known MAC's as src and dst. Once the host receives the ARP reply it has the MAC of the destination and will cache the mapping of IP-MAC in the ARP table, it then send's the actual ping's using the destination MAC. The switches don't care that it is a ping or any other packet it only works for the MAC. 

I am looking for a common MIB that supports checking of crc errors, does this depend on hardware vendor and a custom MIB like CISCO? 

I have a network with roughly 50 machines all on one LAN, consisting of Linux, Windows, and server machines. I have an untangled u150 firewall/router. I have the ability to add an additional WAN & LAN port to get more bandwidth. What is the best way for all machines internally to stay on the same LAN, yet gain the additional bandwidth? Part I am struggling with. If I assign the LAN ports 10.20.30.1 & and 10.20.30.2, everything will work I believe, but I manually have to then set the default gateway on machines to one or the other. Is this true, or am I not thinking about this correctly? 

Is there an actual difference between a CRC error and a FCS error? After reading a lot, they seem like the same thing, but most switches and readings refer to them as separate entities. Is there an overlap of these counters on a switch interface?