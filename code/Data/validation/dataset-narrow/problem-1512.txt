You are talking about anomaly detection, and there are many approaches. If you can create a training set, one-class SVM is a place to start, but even simple control charts can be useful, particularly with live streaming data. 

Get raw data from SQL query (in R) Build munging routine If possible, re-write SQL query to accomplish munging I accomplished in R 

In the context of sentiment analysis, removing stop words can be problematic if context is affected. For example suppose your stop word corpus includes ‘not’, which is a negation that can alter the valence of the passage. So you have to be cautious of exactly what is being dropped and what consequences it can have. 

A simple way to think about this is to appreciate that you are minimizing an objective function. L2 regularization alters the output of the objective function such that smaller values are favored. So you have this constant 'pressure' on the parameters aiming towards 0. 

You can't assume the loss wouldn't drop with additional cycles, but it's generally safe to assume there are diminishing returns. This is why stopping rules are defined in terms of both number of iterations and change in the loss function. You want to stop when the gains become vanishingly small. At that point, increasing your accuracy from, say, 90.1% to 90.2% probably have little practical benefit, and performance on a hold-out sample probably won't differ much. But, that is an empirical question you can actually test. Run a model to near completion then run another with more stringent stopping criteria and compare on a hold-out. 

Congrats on your career move, but please know that data science is one of the more difficult fields to enter, being a combination of statistics, programming, computer science, mathematics, etc. But, for someone motivated enough, with sharp acumen and intuitive data skills, it's a great field. A commenter recommended Andrew Ng's (pronounced 'ing' ) online course through coursera, and I also agree this is a great foundation. It is dense, but you'll need to master it if you hope to be a quality data scientist. He uses Octave, but I advise that you come up to speed quickly in R or Python. If you can translate Andy's code to either of those platforms, you'll be in good shape to start your practice. 

I assume you are performing an independent samples t-test. That the Ns are different isn't necessarily a problem -- the mean is an unbiased estimator -- but just how different are the sample sizes? As you describe, you are probably violating the homogeneity of variance assumption. T-tests tend to be fairly robust to this violation, but you might consider having some outlier treatment and/or using a nonparametric test. 

"found" is the type of object that it came across, "required" is the type of the object that the function accepts. The types look mostly the same ( org.apache.spark.mllib.linalg.Vector ) but the first is Array[X] and the second is X. Why? Because trainingData.collect() gives you the RDD as an array of objects in the array. What you probably want to do instead is , to turn each vector into a LabeledPoint, instead of trying to turn the whole RDD into a single LabeledPoint. 

I don't think "can" is the right question to ask; it's not going to give you a syntax error. The right question is "what could go wrong?". Any modeling technique will have assumptions that may be broken, and knowing how those assumptions impact the results will help you know what to look for (and how much to care when those assumptions are broken). 

There's also a terminology point here: typically, people talk about the training set and the test set, or the training set, the validation set, and the test set. The point of the validation set is different from that of the test set; the test set is used to determine out-of-sample error (that is, how much the model has overfit and what its real world error is likely to be like) whereas the validation set is used to determine what hyperparameters minimize the expected test error. One could fit ten different models on 80% of the data, determine which one has the lowest error on another 10% of the data, and then finally estimate real-world error on the last 10% of the data. 

To clarify, you have at least one observation in every possible category combination, but you only want to perform analysis on a subset of the total data, and are trying to decide how to choose which points to keep and which points to throw away? I think the right approach here will depend strongly on what your hypothesis is, what sort of statistical tests you want to run, and what your loss function is. If you're trying to answer a question which can be answered by the number of datapoints in each combination, for example, or by the mean and stdev of some continuous variable for each combination, reducing the size of your data by sampling will only hurt your analysis. If you're trying to learn a classifier, for example, a classic question is whether to train on a set with equal numbers of all possible classes or with the underlying class distribution found in the wild. The first will train a "superior" classifier, especially if its prior on class membership is later reset to the actual distribution in the wild, by most reasonable loss functions. But is your loss function one of the ones where this is better? You might also want to look into design of experiments, combinatorial design in particular, which is trying to solve a symmetrical problem--starting with no data but being able to choose the various values, what set of points should we test to get as much information as possible about the underlying functions? 

As is common in R the answer depends and you should always double check to make sure including the target (or other) doesn’t affect the answer. But generally it won’t. Superfluous features are ignored as long as newdata includes all expected predictors present during training. 

Most likely, SGD is not a limiting factor for you. But, have you considered taking a classification rather than regression approach? (It looks like you're predicting real values as opposed to classes). Since you state that the prediction doesn't have to be perfect, why not try grouping your outcome variable into bins, then predict the bins? You'll have a far less granular solution, but you might find it works. 

There might be a more principled way to go about this but I don't know how IP assignment works. That said, All your information content appears to be in the second 2 numbers. The last 0-255 might be assumed and the leading 1 can be dropped. You might have something more complicated but as you lay it out, you can leave them as strings and assign to 3 groups: '0.0', '0.1 or '0.3' '4.0' You may not need to spell out all possible IPs. Define your problem space first. 

By ‘design document’ I assume you are authoring a technical doc in an industry job. I treat this similarly to an academic journal publication but with less formatting and page restrictions. Ultimately, you want to provide sufficient information such that if Apple hired you tomorrow your replacement could reproduce the work. Further, your colleagues should be able to grasp the problem attacked, solution proposed, and evidence of success. I usually put the yawn-inducing deep stuff in appendix, but suggested sections might include: 0) executive summary 1) background/motivation 2) data sources 3) analysis/algorithm/model 4) validation performance 5) [optional] real world test results/pilot 6) [optional] estimated return on investment 7) [appendix] SQL source code, modeling heuristic, outlier treatment, etc. 

I assume you are running classification, and have a binary target variable. If that's the case, it does not make sense to show component ROC curves, because your separation may be based on on combinations of 2, 3, or more predictors that individual ROC curves will not reflect. I would show your overall ROC curve, along with perhaps variable importance measures. If you have a handful of predictors that are clear winners, you could re-run your model including only those, and then show that ROC. Otherwise, I don't see what it buys you. 

In stacked generalization, several algorithms (I use some random trees, booster trees, ...) are first trained and used to make the predictions, which are used as input for another algorithm. However, can I use any kind of algorithms, of is there a preference ? (p.s. I often see people using linear models in this case) 

I'm using some ML algorithms (from sklearn lib) and on most of them there is a parameter n_estimators which is (if I understood well) the number of used trees. Intuitevely I would say that the more trees you use, the more accurate results you get. It turned out to be not exactly true, sometimes, a very few number of trees give much better results, but I can't figure out why ? Edit Some precisions: this is a regression problem, with a dataset containing about 4000 samples and 500 features. I'm using GradientBoostingRegressor, ExtraTreeRegressor, AdaBoost, RandomForest. Edit 2 Additional info: I use a cross-validation with KFold=10 to evaluate the accuracy of the algorithms. The best n_estimators value seems to be 50, which give a R2 score of ~56/57% +- 8% for all above cited algo. When I try to increase it, the score quickly decreases. I tried several values, from 100 to 500, it keeps decreasing even reaching 52%. 

Suppose you have some training dataset that you want to use to train some ML models, where targets are comprised between let's say 1 and 100. However, from the 4000 samples, there are few of them (less than 10) which have values out of the previous range, much higher than 100, let's say 300. Is it reasonable to ignore these samples and remove them from the data set, or should they be kept ? I saw people react differently, some of them say that they may hurt the model, while others say no as these samples give additional information to the model. 

In stacked generalization, if I understood well, we divide the training set into train/test set. We use train set to train M models, and make predictions on test set. Then we use the predictions as input of a new model. Thus, the new training set will have M features corresponding to the M models predictions. Finally, we use the last model to make the final predictions. First, is my understanding correct ? If so, how is it possible to use the last model to make predictions as it has different features. 

First, let's try to get some intuition why this would work. $||d_i||$ seems to serve as a word rarity measure, which seems plausible as something to filter on. If documents use dissimilar numbers of rare words, it'll be difficult for them to line up on the cosine similarity measure. But it seems unlikely to me that this cutoff will only depend on $||d_i||$, rather than also on the structure in the tf or idf weights that go into $||d_i||$. To work through some algebra, let me introduce a few more terms (and rename some to shorter ones): Let $d_1$ be a vector of tf weights $[t_1, t_2, ...]$ element-wise multiplied by a vector of idf weights $[w_1, w_2, ...]$ to get the final weights $[d_1, d_2, ...]$. We know that $0.5\le t_i\le 1$ and $0\le w_i\le 6$ (because of the corpus size and assuming we're using base 10, it doesn't matter if we're not). Let $D_1=||d_1||$. Knowing $d_1$, we want to construct a delta vector $x$, such that $d_1+x$ has the minimal (or maximal) $X$ subject to the constraints that: $X=\sqrt{\sum_i w_i^2 (t_i+x_i)^2}$ $0.6D_1X\le \sum_i w_i^2t_i(t_i+x_i)$ (1) $0.5\le t_i+x_i \le 1$ Because we didn't use the raw tf weight for $x$, $x_i=0\ \forall i$ is in the solution space. I'm also ignoring the more complicated constraint that at least one $d_i+x_i=1$, because we can't express that linearly. We'll leave it as is and hope that the optimizer ends up setting one of them to 1. Intuitively, it seems like the set of possible $x$ should be convex, but even if so we're already in the realm of quadratically constrained programming. Note that we can solve for minimal $X^2$ instead of minimal $X$, because $X>0$, but we probably can't use this methodology to maximize $X$ (i.e. minimize $-X$). But thankfully this'll be easily solvable if $P$ is positive semidefinite. So what is $P$? We need to rewrite (1) in the correct form, which starts by squaring both sides: $0\ge 0.36D_1^2\sum_i w_i^2 (t_i+x_i)^2-\sum_{i,j}w_i^4t_it_j(t_i+x_i)(t_j+x_j)$ We can rewrite this as $0\ge x^TPx+q^Tx+r$ where $P_{i,j}=0.36D_1^2-w_i^2t_it_j$ if $i=j$ and $-w_i^2t_it_j$ otherwise. It's non-obvious to me that $P$ has to be positive semidefinite, but that'll be easy to check for any individual $d_1$. If so, pop this into a QP solver and you'll get a lower bound on $X$. If not, we're in trouble. Can we also get a practical upper bound? I'm not sure. Obviously there's some finite upper bound, since we can calculate the maximum possible $X$ from the idf vector $w$ easily. But the fact that the minimum tf weight is 0.5 instead of 0 is throwing off my intuitions about how to create an adversarial $x$ with maximum $X$, and so the best approach that I'm coming up with is gradient descent, which may or may not find the actual global maximum but will probably be close.