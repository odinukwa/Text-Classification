I've just found out about NME a few days ago in another question and thought it might be worth spreading the word. On the subject of multi-platform support, the frontpage states the following: 

Take your pick, all of these use C#. I recommend the first as I believe it to be the easiest one to get started with, especially for 2D graphics: 

So you're interested in learning the language? Then one of these two books might be a good place to start depending on your current level. Beginner Level (Never used C++ / Never programmed a game) 

Question I'm suspecting that with today's powerful hardware, coupled with the memory requirements of doing this for every level, any benefits this technique once had are now outweighted by simply performing an A* at runtime. I've also heard that nowadays memory lookups might even be slower than general computation, which is why creating sine and cosine look up tables is not as popular anymore. But I must admit I'm not yet too knowledgeable on these matters of low-level hardware efficiency though, so I'm taking this chance to ask the opinion of those more familiar with the subject. On my engine I also needed the ability to dynamically add and remove nodes to the graph at runtime (see this) so the precomputed route only made things more complicated, so I scrapped it (not to mention my runtime A* solution was already running perfectly). Still, I was left wondering... Bottom line, is this technique still relevant nowadays in any scenario? 

Tweak the maximum distance and number of rays for performance. Too little and you'll miss tiles, too much and your performance will suffer. Also, the furthest the rays have to travel, the larger the "error" will get, and the more precision you'll need. Edit Check the following tutorial on raycasting, in particular Step 3 and Step 4, to help you implement the intersection bit of the algorithm: $URL$ 

Solution 3 - Modify the vertices' texture coordinates There's also another alternative which would basically be modifying the texture coordinates of your vertices to match the cropped region. This could either be done on the GPU with a custom vertex shader that would receive the source rectangle and texture size as inputs and perform the cropping internally, or on the CPU by changing them directly on your vertex buffer. This is not too complicated, but since the solutions above are probably enough for you, I'll leave it just at a general description: 

You seem to be under the misconception that you can apply a projection matrix the same way as you apply a world or view matrix, i.e. just by multiplying a position in world or view space by it. That's not how it works! To put it simply, when applying the projection matrix, the component is relevant, so you need to pass it a 4D vector and call instead. Afterwards you also need to homogenize the result (i.e. convert from 4D back to 3D) by dividing every component by . Example 

Step 2 An offline algorithm (e.g. Dijkstra or A*) would calculate the shortest path between each and every pair of nodes, and store the first step of the path in a 2D matrix, indexed in each dimension by the starting and ending node used. E.g. using the walk boxes above: 

I'd suggest letting the framework do all of the work for you. Start by calculating a rotation matrix for your start and end orientations, and convert them both to quaternions. You only do this once at the start of the movement and store the values. 

And finally the most important method, which takes care of converting all the relevant data from the Gleed2D level object into our own. First it discovers how many layers there are in the level, and creates the object. Then it simpy iterates over all s in the level, creates a from them, and adds the sprite to our level. I've commented the code for clarity: 

A Texture2D in XNA can only hold so much. Depending on which profile I use that limit is either 2048 or 4096 pixels wide. As far as I could tell from my research (someone correct me if I'm wrong) GDI Images do not have any limitation other than not exceeding the user's memory. I'm looking for the most efficient way to take a GDI Image and a source rectangle (within XNA's size limitations) and create a new Texture2D from that data at runtime. The process must also take into account XNA 4.0's pre-multiplied alpha! OR... A way to take Texture2D.FromStream and adapt it so that it takes a source rectangle too, as that would be even better for my needs. The requirements are: 

The best resource I've ever read on this subject is the following article written by Amit Patel: $URL$ Here's a 2D picture of this technique: 

I'd like to add that the stability of the sorting algorithm is also important. When I first implemented sprite sorting in C# I used the built-in list sort method. This created a problem where overlapping sprites would flicker randomly when sharing the same Y value. This happened because I used an unstable implementation of Quicksort, and the order in which those sprites were rendered would keep changing from frame to frame. I switched to a stable sorting algorithm and the problem went away. I think I went with a Merge sort at the time, but Philipp makes a fine point about Insertion sort and its best case scenario, so I would look into that instead. 

Add a new private instance to the class. I called it (Line 29). Add a public getter so that you can retrieve it from outside. (Line 35) Initialize in the method, exactly like the existing . (Line 106) On the final call in the method, change to so that the bloomed image is stored in the new render target instead of being rendered to the backbuffer. (Line 188) At the very end of the method add a to pass control back to the backbuffer. (Line 206) (Optional) Rename to . I think it makes the code more symetric as you'll see below. :-) (Line 156) 

You should look into Deferred Shading which is great for rendering scenes with a large amount of dynamic lights (video). 

Well, you're asking a few different questions there. I'll start by answering your main question which is about how to display a debug representation of a object. How to render a BoundingBox The easiest way I can think of is to get the corners of the bounding box and render it using a line list primitive. You can get the corners of the bounding box using the method (documentation). This method returns 8 positions in world space. The first four represent the front face of the bounding box, and the final four represent the back face of the bounding box. Then simply create a vertex for each of the corners, and render lines between them in the correct order. Whether you use indices or not is up to you. No need to implement it though, just download this sample from the education catalog, add the class to your project, it and finally use the provided method. It does pretty much what I described above. Creating BoundingBox for the table 

I found the following resource that might be helpful in implementing the technique described by Twitchy, although it's written for Direct3D. It's taken from the NVIDIA Direct3D SDK 10 Code Samples page. Quoting the page: 

What this does is basically find the smallest AABB that can completely fit the OBB that would be formed by the four corners of your camera. 

Moving the Snake Finally, movement. The first thing you need is to keep track of the the snake is moving. This can be a simple enum: 

Representing the Snake Creating a Snake game is pretty simple. The first thing you need is a way to represent your snake's body. If you consider your snake to be made up of blocks or tiles, your body can simply be a list of these blocks coordinates. In your case, if you intent do a console application, each of these will be a character on the console, and the position would correspond to one line or row of the console output. So you start with this: 

And finally, the length of your data array should obviously correspond to the you specify when creating the heightfield. 

Rendering the Snake For rendering just draw a character at each position on the body list. The example above for instance could be drawn as: 

This should make vertices that are further away from the camera, be rendered more to the right, and up from their original positions. 

I was in the same situation as you in the sense that I was already very familiar with C# and .NET and just wanted to learn the XNA API as quickly as possible. I'll just describe the resources I've used in the beginning. In that situation (and although it's a bit outdated now because the API changed a bit in version 4.0) I found Riemers (check his website too) book "XNA 3.0 Game Programming Recipes A Problem-Solution Approach" to be really useful: 

When drawing the root sprite there's no parent transform, so you pass it . You can create an overload to help with this case: 

In your example, isn't the reason why the button press should take precedence simply because the button is on top? So you don't really need to assign priorities, or sort your list, as you already have such an order in place somewhere else - the draw order. Just handle mouse events in the inverse order that you're drawing your entities. For example, in my 2D engine I have entities placed on a display list, which is divided into layers and sorted in draw order. Then my input manager gets a list of all entities from my display list (but only the ones I mark as ) and events are handled front to back, top layer to bottom layer. At the bottom of everything, if nothing else handled the event, I also have a class called which exists simply to handle events that were not consumed by entities (such as clicking outside of all entities). Entities consume events by default, but if one them should be non-blocking, I simply return a different value from the event handler like you're doing (which is a design pattern called chain of responsibility by the way). 

So for instance if the result of examining an object should make the character say hello, walk a bit and then sit down, I simply spawn the following code: 

Have you checked TexturePacker? It has a considerable list of features and at first sight seems to cover all or most of the features you listed. For instance, it seems to work under linux and has a command line tool that you can plug into your build process. Also seems to be capable of detecting and handling duplicates. As for the price, it states on the front page that most features are available for free. If you need one of the features that aren't available, you can request a free license if you're a blogger or a framework developer. Otherwise the full price at the moment seems to be 25$ for TexturePacker Pro. 

Step 4 - Rendering the Sprite Note: The method takes the parent's global transform as a parameter. There are other ways to propagate this information, but I found this one to be easy to use. 

I've seen several usability guideline documents for mobile devices before, but unfortunately not for games specifically, which are a different beast entirely. For your example, I guess that would have to vary from game to game. If for instance your game used double tapping or sliding as part of the gameplay mechanics, then obviously you would no longer be able to use that motion for pausing, so a standard wouldn't do you much good. I've just checked a dozen of games on my library, and most of them display a little pause button (or just the symbol) on one of the corners of the screen to bring up the menu, so if anything, I'd call that the "default standard". Bottom Line My advice is simple - play a bunch of games on your target platform and see how they are usually done. Assume the majority as being the standard and see if that implementation would also fit your game. If it doesn't, just find your own alternative! And don't be afraid to think outside the box, if it works well. For instance, I used to see lots of action/RPG games with d-pads drawn on the screen and thought that was the obvious solution... Then I played Final Fantasy III on the iOS and there's no d-pad drawn anywhere! You simply hold your finger down anywhere on the screen and as soon as you move it a little, that place becomes your d-pad (and a little d-pad graphic appears beneath your finger). This means I can play holding my device in any way I like and with either hand. And to perform actions such as talking with NPCs, a single tap anywhere does the trick. I found this pretty amazing, that you're able to completely control an RPG of moderate complexity, with a single thumb. 

Go to and make sure that the property is set to , not . Starting from Unity 4.2 you can also set the property to without needing a Pro license, which is supposed to work better with version control too. And here is the gitignore that I use: 

It's automatic as long as you're calling the virtual method through a pointer or reference to your base class - that's the beauty of polymorphism! So to give a typical example using pointers, imagine you had these three classes: 

But that's assuming you're calculating the intersection depth correctly. Your method looks a bit shady though. For instance you're calculating those and variables but you're never using them. Either way, here's a simpler implementation: 

I think you can tell the difference from whether you have 1 or -1 in the Z line (or column depending on the matrix orientation). In particular: 

I admit I'm not aware of any ideal solution to this problem, so I'll describe a workaround that you may or may not be comfortable with: 

That's really as simple as it gets though. The sprites themselves can only be translated, not rotated or scaled, and the hierarchy is flat, meaning that there are no parts inside/attached of/to other parts. Also this simple system only supports translation, rotation and uniform scaling of animation parts, but it's possible to support other types as well, such as mirroring, non-uniform scaling, or skewing. 

There's no distortion but the result looks blurred and it loses most of the highlights colors. In my opinion it breaks the retro look I needed. The second time I tried and the result was this: 

That's a bit subjective. It's going to depend mostly on how familiar you are with each of the technologies. And I think the deciding factor would also be, do you really need the level editor to be available in game? Is the general user supposed to have access to the editor too? If you're the only one that's going to use it, don't think twice, just go with the technology you're more familiar working with, even if it's a standalone editor. Personally, I'd go the standalone editor route, but that's just my preference. With that said, there are some advantages to building the editor in game. For one, you can use the codebase that you alredy built for the game to aid you with the editor. And you can more easily switch between normal gameplay and level editing on the fly. Is it worth the extra effort? That's up to you. 

Unlock that player, lock the scenario! Let the player be the one that moves around (and in both axes). Instead of using some value, just use the directly, minus half of the screen size just to make it centered (and don't forget to negate at the end too). 

I think you kinda just answered you own question. The number needs to appear not only on damage but also on heals, i.e. it appears whenever your health changes. And your component has no connection with heals. So they don't really use the same functionality equally. Because of that I think I would make it a part of your . Make it react automatically to any variation in the health bar, and show that variation as a number in green or red depending on whether it increased or decreased. Besides, the is by nature a GUI component, and so is the damage number you want to show. On the other hand is conceptual. But your seems to be only holding a value at the moment. This would make it harder to know which number to show since there's no absolute range. Because of that I would also change it into a pair instead, and keep it synced with the player's health. Nonetheless, you might still want to generalize your component a bit further too, to enable applying any sort of stat variation like you mentioned. The bottomline is... If and when both options really provide the same amount of coupling, then it doesn't matter where you add the functionality. Just pick the one and move on, since none of them will have a benefit over the other. Try to choose the one at the closest level of abstraction though. 

And for this you can just use the class which already creates the quad for you. Load your shader into an object and pass it to . Also, since the render target is already a texture, you can just use it directly! Something like: