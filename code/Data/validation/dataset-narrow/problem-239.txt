There is also third party monitoring software like Spotlight or Idera that can be configured to capture the deadlock graph and send it to you in an email. Once you have the information about which objects or resources are involved, you then have the challenge of resolving the problem. Since everything in the deadlock graphs and xml are object IDs and hobts, you will need to look them up using object_name(obj_id), selecting from sys.objects or, in the case of a hobt (heap or b-tree) you would select from sys.partitions. That should help you get started. 

When I saw this question, I opened SSMS with the intent to read the help documentation on the "Trust server certificate" option only to discover that this is an undocumented feature with no description of what it does on MSDN. My guess is that it can be used to trust expired certificates or those which can't be validated by walking the chain of trust. Since there is no chain of trust for a self signed certificate, SSMS trusts it when it verifies the signature using the public key. I think your suggestion would make an excellent server configuration option for high security implementations of SQL Server. It seems like you want a configuration option to reject all self-signed certificates, since avoiding those is the only real way to mitigate a man in the middle attack. It is exactly the concern that you raise that causes me to recommend configuring the Certificate tab with a domain or public CA certificate. 

Also, when you create a symmetric key, you can specify the argument key_source, which forms the basis of creating the actual key, but if you don't the database engine will create a random key for you. The symmetric key is protected by the certificate, not a derivative of it. It would be very dangerous if the symmetric key were able to be derived from the certificate or it's private key. The Open Master Key command is redundant since it is already been opened so that the private key from the certificate can be used. I would also highly advise against using the master database for column level encryption for your user data. I hope that the above description was clear because I wanted you to understand why you are having a problem before providing the resolution. The problem is that the Service Master Key on your local SQL server instance can't decrypt the Database Master Key. You can fix this in one of three ways. Back up the SMK from production and restore it on your local SQL Server or backup the DMK for the production database and restore it on the database on your local SQL Server or move the command to open the database master key by password before the open symmetric key command. Backing up the DMK would be the better and less impactful choice because restoring an SMK could be resource intensive. I would advise one of the first two resolutions since you don't want to put passwords in your code for security reasons. 

CONCAT will return NULL if any of the inputs are NULL. This suggests that your is returning NULL in at least one iteration. CONCAT_WS will skip NULLs. Try using that instead and see what it gives you. Note the first argument is the desired separator, so if you want to duplicate CONCAT() but with NULL skipping just set that to an empty string, This should help you debug the problem. Alternately you can also use on your if you want to set a default in cases where the result might be NULL. If you use COALESCE to set a default empty string you shouldn't get NULL back from the subsequent CONCAT. Presuming the is in Thai script, you may very well have collation/character set issues, which could be why it is returning null in the first place, your comparison may be failing. Check that all the columns you are comparing have the same character set/collation. @Rick James is also entirely right that looping in SQL should be avoided, and likely is not necessary in the first place. So ideally rewrite the whole thing without the loop. But the above hopefully will help you get to the bottom of where the problem is. 

What you are talking about here is multi-tenant vs multi-instance architecture. I'm just bringing up these terms as you don't use them in your question but this is what you are discussing is called and if you just plug "multi-tenant architecture" into Google, you will find a wealth of resources and discussion about it, entire books have been written on it. Some good resources regarding SQL Server specifically here: $URL$ $URL$ I would be with other answers, in that I would lean strongly towards multi-tenant as a default, unless you have compelling reasons to favour multi-instance. You don't need to split into thousands of individual client databases to scale, there are many other ways of doing that, that are likely to be preferable. Like clustering, replication, sharding, partitioning etc. Don't reinvent the wheel. There's nothing inherent that says you need to split this yourself manually on an individual customer level and indeed doing so is likely to increase significantly the costs of adding every new customer. You are talking about "millions" of customers, think of any large-scale cloud-based software as a service, Gmail, whatever, you hardly think they create an entirely new database for each new signup, now do you? There can be reasons where you do want to facilitate this, for example, if you are selling your product to a customer that MUST have it hosted in-house on their own infrastructure. But as a general SAAS rule, lean as a default to a multi-tenant architecture. 

The "standard" only returns records that match in both tables. To return records that only exist in the left or right table you need to use an . This can be a or a depending on whether you want to retrieve all the records from the left or right table; by convention LEFT is more frequently used. $URL$ $URL$ In the above you are selecting from as the first table, and then doing one inner and then several LEFT outer joins on the other tables. So you are only going to get results from tbl_rooms, that is the LEFT table, that is how it works. You need to either move to using RIGHT outer joins on the other tables, or, and I recommend as probably less complicated to conceptualize, to start with the table on the top/LEFT that you want to get all the results from. So you need to start with at the top and then do your left joins down to at the end, reverse the existing order. You'll then get all the results from the table you start with, and only the results that match from the table you are LEFT outer joining on. EDIT: Something like this should give you properties with a room count: 

What you are looking for is called Transactional Replication, which is very commonly used and a good solution for avoiding contention on a transactional database. There are three databases in a transactional replication system, which are the publisher, the distributor and the subscriber. The publisher is the source database, the distributor is where all changes to the publisher are sent for distribution to one or more subscribers. The distribution database can be on the same server as the publisher or a different server. When you create a publication, you choose the articles to be included, such as tables, views, procedures, etc, and whenever a change is made to any of the objects and gets recorded in the transaction log, an agent, which is an executable, reads the transaction log, packages the change and sends it to the distribution database. Then a separate agent, or executable, takes the packages from the distribution database and applies them to the subscriber. You will be setting up a publication and subscriber on servers 1,2 and 3 and have all subscriptions point to a database on server 4. There are several articles on MSDN that have good descriptions of transactional replication. 

The proper key hierarchy is the Service Master Key protects the Database Master Key, the Database Master Key protects either an asymmetric key or certificate, the asymmetric key or certificate can be used for encryption or can be used to protect a symmetric key which is used for encryption. The keys are created in that order. The system creates and protects the SMK, and the DMK and subsequent keys are created by the user. The design of the key hierarchy in SQL Server protects both the data and keys from compromise. Remember that with a symmetric key, the same key is used to encrypt and decrypt data, or in this case other keys. The main purpose of introducing an asymmetric key or certificate into the hierarchy protected by the Database Master Key is to prevent an attack on the DMK and SMK from inside of the database. If someone malicious wanted to obtain the DMK unencrypted, then that person would need to do one of two things. Either decrypt the service master key and use it to decrypt the DMK, which is not possible because only the database engine can use the Data Protection API to do this, or attempt to decrypt from the top of the hierarchy down, which would be possible if there were not an asymmetric key or certificate in the way. All of the signed or encrypted bits of the symmetric keys in SQL Server are in a system table named sys.crypt_properties in each database, including the encryption of the Service Master Key in the master database. There is no system table that contains the private key for either of the asymmetric key types. If all keys in a hierarchy were symmetric keys, then the SMK would encrypt the DMK and the DMK would encrypt the symmetric key that would encrypt the data. Because of the way that symmetric keys work, that would also mean that, if someone opens the symmetric key for the data, then it can theoretically decrypt the DMK and the decrypted DMK can be saved by a malicious user or used to decrypt the SMK because the same key is used for encryption and decryption. This is why an asymmetric key or certificate is required to be an integrated part of the encryption hierarchy. 

I have a SQL query that I have spent the past two days trying to optimise using trial-and-error and the execution plan, but to no avail. Please forgive me for doing this but I will post the entire execution plan here. I have made the effort to make the table and column names in the query and execution plan generic both for brevity and to protect my company's IP. The execution plan can be opened with SQL Sentry Plan Explorer. I have done a fair amount of T-SQL, but using execution plans to optimise my query is a new area for me and I have really tried to understand how to do it. So, if anyone could help me with this and explain how this execution plan can be deciphered to find ways in the query to optimise it, I would be eternally grateful. I have many more queries to optimise - I just need a springboard to help me with this first one. This is the query: 

This might seem like a stupid question, but I've been looking into open source solutions for schema migration, namely Liquibase and Flyway. However, my boss told me that SQL Server Data Tools (SSDT) achieves the same job. I'm not sure if agree, but I can find very little on the Internet that directly compares it to Liquibase and/or Flyway. My view is that SSDT is a development, data modelling and design tool for SQL Server and also supports schema comparison (and generating scripts thereof) and source control. It tackles a different problem although there may be some overlap with Liquibase/Flyway in some aspects of schema migration. But as an overall schema migration tool, Liquibase and Flyway are fully dedicated tools whereas SSDT is more for the design and development of a database. Any opinions would be much appreciated even if it's just to say there's no comparison and SSDT is not a schema migration tool per se at all. 

I have very little knowledge of MDX, but through some trial-and-error, I have found that removing the two entries from the makes the query return quickly. Or, removing both the and entries also makes it return quickly. But, obviously, this is changing the query, so is not the solution, but perhaps it provides clues as to where I should look in terms of indexes or suchlike if there is anything like this in SQL Analysis Services (I'm more familiar with SQL Server databases)? One thing I did try, is to put before and . This caused the query to return very quickly with no results. Unfortunately, I don't know if this is correct because the query without this change takes too long to be able to see if there are any results. I don't know what difference is supposed to make, but is this the obvious answer or does it change the query to be different? It is equally possible that the original writer of this query should have used anyway, but never got to test the query with a real data set. Any help would be greatly appreciated.