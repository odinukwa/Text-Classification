This will show latest in-memory Varnish log entries for “Backend fetch failed” (HTTP 503 error). You can also check more commands for troubleshooting the 503 status in Varnish 

Essentially this means that all URLs under will be delivered from root () of backend and all URLs under will be delivered from root () of backend. 

It is possible. You will use bans for that: (example of a ban lurker friendly ban) Since you're able to control your application's code the best way is to setup purge handler in Varnish, then make your application issue request to that (via curl). In fact, your purge handler might use bans as above, or it might use . Bans are normally used to invalidate cache for multiple objects at once, whereas purge invalidates single page. 

Are these local accounts or domain accounts? If these are local accounts, they can't be moved. Are these IIS or windows users? You can review the applicationHost.config (make a backup copy first) and see what configuration is stored in there. I asked a question about moving IIS manager users to another machine, a MS team member mentioned this. We need a bit more information. As far as moving the configuration over. You could try copying the sites configuration specific to the FTP site by copying and pasting the particular configuration. This involves the XML so it's not very straight forward, but is an option. $URL$ 

This can't be controlled as far as I know. First time i've seen this type of question. There is a range of ports Windows uses for dynamic allocation, which is the range you are seeing. You can control dynamic ports but it requires having control of both the client and server to limit the number of dynamic ports. For an internet facing site, this is not fesible. 

It doesn't work for you because uses "client" mode by default. Enable it using switch. Subsequently, this will work: 

The reason for message in case of release packages is simple: it's either already installed or you're using an outdated URL which installed older version of the release package. Release packages contain YUM repository information. You typically install a release package first (using URL), then install the packages of interest from that repository (using package name). So if a *-release package is already installed, then there's really nothing to do and nothing you should do about it. You have to proceed to install the packages from that repository. Complete steps: 

Do not mix installed packages via ones. The proper fix involves removing pip packages and install everything from . I have outlined it here. You do not need newer pyOpenSSL on CentOS 7 to run certbot! 

An app pool recycle should be sufficient per site basis. These are independent processes. Too often articles and processes promote using iisreset. Is stopping / starting the app pool for the one site an option? Is this a single server solution and are you trying to minimze downtime for the site? There is an option to Disable recycling on configuration changes. Then you can manually recycle. When the problem happens, what is listed in the applicationHost.config? 

By default all website folders should only need 'read' unless you adjusted the perms. I'd use NTFS permissions to lockdown to certain folders. I wouldn't recommend having individual files in different locations. I'd lockdown by folder. 

I looked on the vendors website. $URL$ Are you hosting within your own windows service? I found on their workarounds page on regarding using certificates. $URL$ 

First of all, you typically should never specify a DNS hostname in Varnish backend definition. It is resolved only during startup and never updated. Specify IP of remove Nginx server instead. Second, what you're asking is already the default behaviour of Varnish. It is a transparent caching HTTP proxy, and as such - it will pass through whatever header is sent via HTTP by clients. 

You might want to check full explanation which includes additional optimization to query slow backend requests. 

First command adds repository information, can give "nothing to do" in case it's already present (that's OK). The second command installs the actual package from the repository. 

You need to remove first level of URL and pass requests to respective backend by using instead of , like so: 

logparser has the ability to insert into syslog type logparser -h -o:syslog Wrap into a batch file or powershell script. That is one option. Examples: Send error entries in the IIS log to a SYSLOG server: 

by default in IIS 7.5, the applicationpoolidentity (aka i_AppPoolName) is used. I would NOT grant write perms and only grant on the folder(s) needed to support your app. Network service was the default account in IIS 6 and wasn't the best account. MS made a change in 7.x. 'note the default app pool name should be changed to whatever you have. ICACLS C:\inetpub\wwwroot /grant "IIS AppPool\DefaultAppPool":(M) You can do this from the GUI in r2/iis 7.5 The other option is to grant the IIS_IUSRS group modify, the app pool command line above is the most secure. I just wouldn't grant modify on all folders. Personally, I use the IIS_IUSRS perms architecture as my servers are not shared hosting servers. 

The 503 HTTP status typically indicates a problem with the server setup. It's not a block or ban status. Most likely the problem is due to timeouts being hit by Varnish while communicating to Apache. You may troubleshoot this via command line using: 

If you really want s instead (do you really have to?), then don't use REGEX matching () to invalidate specific URLs, since it will more than likely interpret and other query parameters as engine flags. So: 

I think you're looking for instead. It will be more effective, and will invalidate whatever URL you request in PURGE request. In , add: 

Most likely you have /etc/nginx/conf.d/default.conf which is present on rpm based installations. It has default host which listens on port 80. Best practice is to trim that file so that further upgrades will not replace it. 

Can you look in the iis logs and see what the sub-status code is on the 401. Here is a list of codes posted by MS $URL$ The HTTP status code in IIS 7.0, IIS 7.5, and IIS 8.0 $URL$ you could also use failed request tracing to see what is happening. $URL$ 

Try a hosts file entry remotely on a client and see if you things work. Once that is all set, you'll just need the dns A record as mentioned in the previous post. This is something I do all the time to make sure the setup is working both locally as well as remotely. 

Sounds like webgardens (one app pool spawning additional processes). I'd suggest trying to not use web gardens if they are being used. Additionally, if you use SCOM 2012 sp1, I'd suggest deploying APM (application performance monitor) and push a management pack to monitor what's going on. APM doesn't require any code changes and can get down to the function level. The only other option is to use IIS Debug Diag to $URL$ 

So once your app sees a POST request, it will (provided you code necessary logic) issue PURGE request to Varnish with the URL of the resource that need to be invalidated (doesn't have to be the same URL). 

The ads will not be cached, because Google ads are Javascript based. (the actual ads are pulled by Javascript code into your page dynamically, irrespective of Varnish cache). 

Normally what you return in POST request is different to output from GET request. The default Varnish behaviour is that POST requests are not cached. However, to answer your questions: 

You don't really have to use private IP of your instances. You can use your Linode servers public IP addresses in Varnish backend definition. It will work fine. 

first enable sql remote connections $URL$ second open windows fw if you are using it on port 1433 (or whatever port you configured) for sql. 1433 is the default. 

The permissions are based on the IUSR (anonymous by default) or the application pool identity. The best tool to help determine if there are permission issues is using process monitor by sysinternals. Auditing will tell you right away if there are denies. If so, adjust folder security to meet your requirements. $URL$ 

The only other options I can think of 1) create a local user with the same user name and password on each machine 2) Grant permissions on the folder (first try granting the user, if that doesn't work, try users or authenticated users). The right way is to have a trust like it's mentioned although is more involved. 

Varnish is a transparent cache. It will work for multiple websites in a single instance without any issue. If your many websites are using the same platform, i.e. Wordpress, you are just fine with single VCL file configuration. If your many websites are using different CMS, and need to have different handling / cache logic per-site, you have to make use of include files as outlined in this article. 

You should be using if you want to make sure that the custom header goes into the response that clients (browsers) get: 

Unfortunately, it doesn't seem like any version of Varnish supports access for from within . If it would, then something like this would suffice: 

You can put pretty much anything that does HTTPs termination for the purpose. Even Apache. Bust mostly people use nginx for SSL termination (or pound, or hitch) because it's more lightweight. Just because you have less software, i.e. Apache (SSL) -> Varnish -> Apache. It doesn't mean that the request flow would be any faster. It still has to go through 3 layers and travel as HTTP packet. So there's no speed up from using less software. Using Apache as SSL terminator does not make serving static content faster pointless. You can configure it in a way that Apache SSL layer will serve static files directly whereas proxy forward to Varnish for dynamic content. But overall, nginx is just better for serving static files.