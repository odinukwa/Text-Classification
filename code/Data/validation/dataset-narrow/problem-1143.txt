Now if there is a set cover $X$ of size at most $k$, we can get the benefit of at least $nM + m - k$ as follows: 

At first, this sounded like a simple exercise in dynamic programming, but now it seems that some amount of actual thinking is required. Does anyone recognise this problem? Or see a clever algorithm for solving it? 

To make it completely unambiguous, you would state something like "the running time in the RAM model is $O(n)$, where $n$ is the number nodes." 

Just to give some ideas of what is possible (but somewhat non-trivial), here is one example: a distributed algorithm that finds a maximal edge packing on a bounded-degree graph. Problem definition Given a simple undirected graph $G = (V,E)$, an edge packing (or fractional matching) associates a weight $w(e)$ with each edge $e \in E$ such that for each node $v \in V$, the total weight of edges incident to $v$ is at most $1$. A node is saturated if the total weight of incident edges is equal to $1$. An edge packing is maximal if all edges have at least one saturated endpoint (i.e., none of the weights can be greedily extended). Observe that a maximal matching $M \subseteq E$ defines a maximal edge packing (set $w(e) = 1$ iff $e \in M$); hence it is easy to solve in a classical centralised setting (assuming $G$ is finite). Edge packings actually have some applications, at least if one defines an application in the usual TCS sense: the set of saturated nodes forms a $2$-approximation of a minimum vertex cover (of course this makes only sense in the case of a finite $G$). Model of computation We will assume that there is a global constant $\Delta$ such that the degree of any $v \in V$ is at most $\Delta$. To keep this as close to the spirit of the original question, let us define the model of computation as follows. We assume that each node $v \in V$ is a Turing machine, and an edge $\{u,v\} \in E$ is a communication channel between $u$ and $v$. The input tape of $v$ encodes the degree $\deg(v)$ of $v$. For each $v \in V$, the edges incident to $v$ are labelled (in an arbitrary order) with integers $1,2,\dotsc,\deg(v)$; these are called local edge labels (the label of $\{u,v\} \in E$ can be different for $u$ and $v$). The machine has instructions with which it can send and receive messages through each of these edges; a machine can address its neighbours by using the local edge labels. We require that the machines compute a valid edge packing $w$ for $G$. More precisely, each $v \in V$ has to print on its output tape an encoding of $w(e)$ for each edge $e$ incident to $v$, ordered by the local edge labels, and then halt. We say that a distributed algorithm $A$ finds a maximal edge packing in time $T$, if the following holds for any graph $G$ of maximum degree $\Delta$, and for any local edge labelling of $G$: if we replace each node of $G$ with an identical copy of the Turing machine $A$ and start the machines, then after $T$ steps all machines have printed a valid (globally consistent) solution and halted. Infinities Now all of the above makes perfect sense even if the set of nodes $V$ is countably infinite. The problem formulation and the model of computation do not have any references to $|V|$, directly or indirectly. The length of the input for each Turing machine is bounded by a constant. What is known The problem can be solved in finite time even if $G$ is infinite. The problem is non-trivial in the sense that some communication is necessary. Moreover, the running time depends on $\Delta$. However, for any fixed $\Delta$, the problem can be solved in constant time regardless of the size of $G$; in particular, the problem is solvable on infinitely large graphs. I have not checked what is the best known running time in the model defined above (which is not the usual model used in the field). Nevertheless, a running time that is polynomial in $\Delta$ should be fairly easy to achieve, and I think a running time that is sublinear in $\Delta$ is impossible. 

(I am assuming that $N^+$ is some finite set, and I will use the shorthand notation $N = N^+$.) Let us interpret each $i \in N$ as a node and each $(i,j) \in N$ as a directed edge from $i$ to $j$; now $(N,S)$ is the complete bipartite graph. The task is to choose a subset $X \subseteq S$ of edges. If we choose an edge $(i,j)$, we get the "benefit" $c(i)$. The constraints are: 

There are 256 integers of type X + Y + Z. These are precisely those integers of type X that begin with sequence '0000011111...' 

Proof: In a cubic graph, if you sum over all $3n/2$ constraints $x_i + x_j \le 1$, you have $\sum_i 3 x_i \le 3n/2$, and hence the optimum is at most $n/2$. The solution $x_i = 1/2$ for all $i$ is trivially feasible, and hence also optimal. In a bipartite cubic graph, each part has half of the nodes, and the solution $x_i = 1$ in one part is hence also optimal. Any optimal solution must be tight, that is, we must have $\sum_i 3 x_i = 3n/2$ and hence $x_i + x_j = 1$ for each edge $\{i,j\}$. Thus if you have an odd cycle, you must choose $x_i = 1/2$ for each node in the cycle. And then if the graph is connected, this choice gets propagated everywhere. 

Yes. If I understood correctly, the proof of Theorem 1.6 in Khot (2001) establishes that it is NP-hard to distinguish between the following two cases, even if we focus on bounded-degree graphs of sufficiently high degree: 

Background: The motivation for this question is two-fold. First, I would like to get some hard facts to better understand the ongoing conferences vs. journals debate. Second, if this information was somewhere available, I could make a more informed decision when submitting papers for review; I would be happy to favour journals whose editors do a good job at selecting and shepherding referees. 

If you spend five years of your life by studying a purely theoretical concept X (e.g., a certain esoteric model of computation), then eventually X becomes a natural part of your daily life. You will learn to know how X behaves, what it feels like, how it responds to your manipulations, and in what kind of neighbourhood it lives. You will learn who discovered it, when, and why, and what others have done with X, successfully or unsuccessfully. You will know X just like you know any physical object that you encounter every day. Indeed, you may know it much better than those strange, ill-defined, unpredictable, and erratic physical things... But it is a long way, and I don't think there are that many magic shortcuts. 

Non-bipartite connected cubic graphs have the unique optimal solution $x_i = 1/2$; in a bipartite cubic graph you have an integral optimal solution. 

We say that $[i] \in W$ is maximal if $[i] \le' [j]$ implies $[i] = [j]$. We have the following property that is easy to verify: 

Multiplication. Perhaps one of the oldest not-entirely-trivial algorithms, and a problem that is solved more often than FFT. 

Another example is related to compilers and register allocation. Assume that we have implemented an exact algorithm that solves a problem $B$ in polynomial time. The running time of the algorithm depends, in part, on how well the compiler managed to assign variables onto CPU registers – this is our problem $A$. The solution of problem $B$ is correct even if the compiler uses an approximation algorithm to solve problem $A$; however, an approximation factor in problem $A$ affects the running time of algorithm $B$. 

And what was the end result? It looks much better than what I got from the scanner; the noise is less pronounced, and there is exactly nothing to be seen. Nevertheless, the Gaussian noise is there: 

Is this the best possible approximation? If you knew the actual travelling times, could you find a better solution? 

There are also many problems that are hard on weighted stars, by reduction from the subset-sum problem. A natural example is: 

Usually, we can afford to be a bit sloppy. The model is often clear from the context; in the case of algorithms, it is usually the RAM model unless otherwise specified. Moreover, there are various conventions regarding the definition of $n$. For example, in the context of graph algorithms, $n$ almost always denotes the number of nodes (and $m$ is the number of edges) – these are just conventions that you have to learn. If you are really interested in logarithmic factors, then you have to be careful with these details. The same algorithm might be $\Theta(n)$ or $\Theta(n \log n)$ or $\Theta(n/\log n)$ if you slightly vary your model of computation or your definition of $n$. However, if something like "polynomial time" is good enough for you, then you can ignore most of these details. Typical models of computation can simulate each other with a polynomial overhead (time $x$ in one is at most time $\mathrm{poly}(x)$ in the other), and typically an input can be encoded in $\mathrm{poly}(n)$ bits, no matter what happened to be your precise definition of $n$. For example, an $n$-node graph can be encoded as a string of $n^2$ bits. Hence you can usually say "runs in polynomial time" without worrying too much about the details ("polynomial in what?", "polynomial number of what?").