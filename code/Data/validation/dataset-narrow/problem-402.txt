Tune your mdf files for random read/write access in 64 KB chunks. Tune your ldf files for sequential write access in 64 KB chunks. 

I'm having to do a little bit of psychic debugging here, but generally speaking, you should have the subform bound to a query that joins your junction table to the table of related records. The form inside the subform shouldn't know about the table in the left side of the relationship (tbl_apptltrs). So the chain is something like this: 

In this situation, it can be useful if you want to, say, compress historic data that doesn't change, while leaving the current data uncompressed to maximize insert/update performance. I've done this in a couple of places on our server. You also have the option of different backup schedules across the different filegroups. But if you're trying to improve overall performance, there will be much more effective things to try first. 

The / statements are mostly superfluous in this specific case. As for running all queries with isolation level, I'd strongly advise against it, unless you have a specific need, and aren't worried about the implications of dirty reads: 

Using a database snapshot located on your production OLTP server will, in all likelihood, make performance worse. There are two main reasons: 

REBUILD/REORGANIZE indexes, DBCC SHRINKFILE (Okay, this isn't a sensible option, since DBCC SHRINKFILE will fragment the piss out of anything it touches, but I'm including it for completeness.) Create a new database with auto-stats off. Script and recreate all tables from the source database. Use bcp to export/import the data into the new database, in cluster-key order. Script and recreate all indexes. Recalculate all statistics with full scan. Create a new database with auto-stats off. Script and recreate all tables from the source database. Use SSIS or T-SQL to transfer data to the new database. Script and recreate all indexes. Recalculate all statistics with full scan. 

So despite the fact that the function technically returns varbinary(8000), you'll only ever get 16 bytes with MD5. If you are completely certain you only need MD5, then a binary(16) will do it. If you want to play it safe, and allow any possible algorithm (current or future) that HASHBYTES can handle, go with varbinary(8000). I definitely wouldn't recommend a char type, since that will involve string conversion overhead, and unnecessary storage overhead. 

I checked those names on 2008, but I seem to recall they have the same names on 2000 (though the structure may differ). 

If I had to manage this, I'd look at what, if anything, modifies the data in the tables outside of the ETL process. If nothing else besides your ETL modifies the data, I would simply update the ETL process to insert the finished data at both locations (and likewise carry out whatever index maintenance you're doing in both places). If something else updates this data, but only one one server, then transactional replication is probably the most lightweight way to get the data to the secondary server. Even if the data isn't being modified outside of the ETL, then this wouldn't be a terrible alternative to modifying the ETL process to update two targets. It sounds like a relatively small percentage of data that's being inserted daily. If the data is being modified on both servers, then you'll probably want to consider merge replication. The simplicity of this will largely depend on if the table has any identity columns. 

Apart from performance, they all have rather different meanings. will give you the last identity value inserted into any table directly within the current scope (scope = batch, stored procedure, etc. but not within, say, a trigger that was fired by the current scope). will give you the last identity value inserted into a specific table from any scope, by any user. gives you the last identity value generated by the most recent INSERT statement for the current connection, regardless of table or scope. (Side note: Access uses this function, and thus has some issues with triggers that insert values into tables with identity columns.) Using or can give you entirely wrong results if the table has a negative identity step, or has had rows inserted with in play. Here's a script demonstrating all of these: 

The simplest approach I can think of is to have one fact table for order items, and have two price columns in that fact table: original price, and discounted price. Original price would be the price as listed on the line item. Discounted price would be calculated by totaling the price of all line items on the order, calculating the percent difference between that and the total stated on the order header, then scaling down the prices of the line items using that percentage. 

Yes, that's non-normalized, but occasionally non-normalized designs win out for performance reasons. However, I would probably approach it a little differently, for safety reasons. (Disclaimer: I don't currently, nor have I ever worked in the financial sector. I'm just throwing this out there.) Have a table for posted balances on the cards. This would have a row inserted for each account, indicating the posted balance at the close of each period (day, week, month, or whatever is appropriate). Index this table by account number and date. Use another table for holding pending transactions, which are inserted on the fly. At the close of each period, run a routine that adds the unposted transactions to the last closing balance of the account to calculate the new balance. Either mark the pending transactions as posted, or look at the dates to determine what's still pending. This way, you have a means of calculating a card balance on demand, without having to sum up all the account history, and by putting the balance recalculation in a dedicated posting routine, you can ensure that the transaction safety of this recalculation is limited to a single place (and also limit security on the balance table so only the posting routine can write to it). Then just keep as much historic data as necessitated by auditing, customer service, and performance requirements. 

Cut the chatter. Use NOCOUNT. Make sure your trigger can handle multiple rows. Prevent unbounded trigger cascading and recursion. Avoid horrible performance of the INSERTED and DELETED virtual tables. 

It's difficult to say exactly what's causing it without seeing the code for the trigger, but I'd bet some non-zero amount that misuse of the INSERTED and DELETED virtual tables is to blame. The thing about INSERTED and DELETED is that they have NO INDEXES. If you join them directly back to the base table (or to each other), you can end up with some very awful performance. You can generally mitigate this by loading the two into temp tables and creating indexes for however you'll be joining them (usually on the primary key). 

The fact that the backup is 365 MB while the transaction log is almost 30 GB and the database is in FULL recovery, and yet there's never been a transaction log backup is a bit perplexing. There may be something else going on that's allowing for transaction log reuse besides a backup. If you want to double-check whether log backups are happening, you can do this from Management Studio. Right-click the database, and go to Reports, Standard Reports, Backup and Restore Events. Expand "Successful Backup Operations" and see if any log backups are happening. If not, then check whether or not the database is regularly being changed from FULL recovery to SIMPLE and back to FULL again. This will show up in the SQL Server error log as something like . With that out of the way, you can shrink the transaction log to something more sensible - probably around 25-50% of the size of the database - using . You'll want to make a transaction log backup first to make sure active log space is marked for reuse. Next, run to get the name of the transaction log file (the logical name in the column, not the physical filename), then shrink it to an appropriate size, e.g. (the size is given in MB). Note that it may not fully shrink to the target size depending on what portion of the log file is in use. If that's the case, you'll want to run , then run another log backup, and again. Then for ongoing maintenance, I would recommend leaving the database in FULL recovery, and adding a nightly transaction log backup just before the full backup. This way, you'll at least be able to recover the database in the event of a failure of whatever volume the data file is on (assuming the transaction log file is still safe), and also retain the ability to do point-in-time restores. 

My personal preference would be to use triggers to handle at least some part of the synchronization. I don't particularly care for scheduled polling synchronization, as you have to deal with potential conflicts, stale data, performance impact of the repeating job, etc. If they want to do it as aggressively as 1 to 5 minutes, I'm guessing it's to mitigate conflicts and staleness, and the immediacy of a trigger would address that. If it's all happening within the same server, you're probably fine putting the sync code within the triggers, or having the triggers call a stored procedure that synchronizes each affected record. If the synchronization spans servers, or you want to make sure that having one database offline won't prevent the other database from being usable, look into using Service Broker to handle asynchronous updates. I've done this to synchronize sales entry figures with CRM data between two different servers, while allowing for either server to be taken offline without affecting the other application. Once it comes back up, Service Broker delivers the messages and updates the data on the remote server. And there's really nothing inherently bad about triggers, but like most aspects of T-SQL, it's very possible to write code that performs horribly. I wrote an article about the common pitfalls of triggers, if that helps any. Writing Well-Behaved Triggers 

Without reading the transaction log and/or log backups (which may be impossible, if msdb is set to the default simple recovery), I can't think of any good way. Going forward, if you want to log the changes, you could create some log tables, and add logging triggers to these tables in msdb. 

I'm running SQL Server Management Studio 2008 (10.0.5500.0) and was attempting to edit the built-in template "Create T-SQL Trigger (New Menu)". I open the Template Explorer, right-click the template, choose Edit, modify it, then save. But when I try to use the template by right clicking "Triggers" underneath any table and selecting "New Trigger", it still uses the default template. In Process Explorer, I can see it accessing this file when I edit the template: 

Oh, damn it, not 30 seconds after I post this, I stumble across sp_altermessage (naturally, after spending a significant amount of time trying to figure out if I could do this with extended events). So if anybody else is wondering how to do this: 

Thus when it looks for the latest sale date in, say, 2012, it will ignore Feb. 29, and compare to the previous year's sales from Jan. 1 to Feb. 28 instead. I'm still open to suggestions if there's a better way to do this (particularly that awful Filter expression I used). 

And I get 'Can't find any matching row in the user table', for the obvious reason that '%' isn't a host mask I've explicitly allowed a connection from for this particular user. So what's the proper syntax to grant a table privilege to a user from any of its allowed host masks? How is MySQL Workbench getting away with it for schema privileges? I don't have to manually insert rows into mysql.tables_priv, do I? UPDATE: To clarify, here's what the current user/grant tables look like. I've anonymized some names, obviously. Note that the host in the schema privilege table is '%', but there aren't any users with that host. How do I get MySQL to let me do that with schema object grants? Preferably without mucking around directly in mysql.tables_priv, but I'll do it if it comes down to it.