Sorry if this is redundant, but due to the crazy naming of the tools, it's hard to find the answer to the question. Question 1 Will SSIS packages, reports and so forth built with Microsoft SQL Server Data Tools - Business Intelligence (SSDT-BI) for Visual Studio 2013 work on SQL Server 2008 R2? Question 2 I'm currently using SQL Server Business Intelligence Development Studio (BIDS) for Microsoft Visual Studio 2008. I want to potentially upgrade to Data Tools - Business Intelligence for Visual Studio 2013. I assume I would need to A) purchase a new copy of Visual Studio 2013 and then B) download the free SSDT-BI software? That's assuming SSDT-BI for VS2013 works for 2008 R2. 

USQL Just the base USQL job that defines the schema then selects all fields to a new directory. No transformation happening outside of leaving out the headers. The file is CSV, comma delimited with double quotes on strings. Schema is all strings regardless of data type. Extractors tried is TEXT and CSV with both set to be encoded:UTF8 even though both are default to UTF8 according to Azure documentation on the system. Other Notes This same document was uploaded in the past to BLOB storage and imported in the same fashion into Azure Data Warehouse without errors via Polybase. 

I have a bulk loading process that loads millions of records into a few fact tables within a warehouse. Those tables are primarily clustered on time. The non-clustered indexes are in place for how the data is used to speed up performance. I typically drop some of the non-clustered indexes to both speed up inserts and to reduce index fragmentation after large data loads. However, this process of dropping and rebuilding is causing a huge amount of time as the data grows. Example: One table took 2 hours to apply a new non-clustered index on 100m+ rows. Likewise, if I leave the non-clustered indexes in place, they will increase the inserts 3x to 10x in some cases that force your hand to drop and rebuild. While it's awesome to drop and rebuild indexes, they don't really pan out that well as data grows in those tables. What options are available to me? Should I bulk up the server with more memory (currently 32GB) and cpu (4 vCPU)? Should I rethink my indexes? Should I find a balance of keeping some indexes on for reorganization versus dropping and rebuilding? (Note: I don't have enterprise edition.) I'm thinking my only option is enterprise edition with table partitioning where I can rebuild indexes per partition as opposed to the whole table. 

To add a bit to TomTom's comment: The optimizer has full access to meta-data etc regardless of whether the data is in one database or several databases. And the optimizer or execution engine is on constrained to one database. As long at it is in the same instance! If you talk about cross-instance queries (using linked servers, for instance), then you will definitely hurt! 

EMPTYFILE is just there is transfer all the pages to other files, so you subsequently can do ALTER DATABASE ... REMOVE FILE. This doesn't seem to be what we want here. I wouldn't worry about this, just let it sort itself at next re-start. Remember that tempdb is re-created based on what sys.master_files says. This is the "template" for tempdb regarding number, placement, size etc of the files. You adjust the size in sys.master_files using ALTER DATABASE ... MODIFY FILE. If you want help, just use SSMS, tempdb, properties, and adjust the size for any of the tempdb files to a larger size. Then script the command and adjust to your liking. (The reason to not specify a smaller size in the GUI is that it will generate a DBCC SHRINKFILE, which only (tries) to address the current size for tempdb and does nothing for the template. It is perfectly possible to specify a smaller size than current size using ALTER DATABASE for tempdb as it will only affect the "template" and not the current size. 

FWIW, the missing index info is there is is just Profiler that doesn't bother to show it. If you dig into the underlying XML you will see it. Ans since Profiler has been deprecated and MS haven't done any work on it since 2010, we can't expect any change here. If you use Extended Events, the "new" (if we consider 2008 being new) trace engine, then you will find that the missing index is there as well. But the GUI for XE doesn't show the graphical representation for the plan in the first place, only the raw XML. I suggest you follow Erik's recommendation, most probably a more productive path. 

SSIS is not really my forte. I noticed you have a table lock option on OLE DB destinations where the entire table becomes locked in what I assume is during the transaction of inserting data. What happens if you have a flat file data source with a conditional split that is parsing out data into 5 or more OLE DB destinations that are targeting the same table with table lock on? Would each OLE DB destination get blocked by one another in this scenario if data is being fast loaded (inserted) into the destinations themselves? I removed the table lock in my instance and everything seemed fine. It was splitting the data and inserting records at about 1 million records per minute. 

I have a document that was compressed in gunzip from a unknown source system. It was downloaded and decompressed using a 7zip console application. The document is a CSV file that appears to be encoded in UTF-8. It's then uploaded to Azure Data Lake Store right after compression. Then there is a U-SQL job setup to simply copy it from one folder to another folder. This process fails and raises a UTF-8 encoding error for a value: Ã©e Testing I downloaded the document from the store and removed all records but that one with the value flagged by Azure. In Notepad++, it shows the document as UTF-8. I save the document as UTF-8 again and upload it back to the store. I run the process again and the process succeeds with that value as UTF-8 What am I missing here? Is it possible the original document is not truly UTF-8? Is there something else causing a false positive? I'm a bit baffled. Possibilities 

Interesting find today, I have a file group that is used for indexing. It has one file that has a .ldf extension, which as you know for SQL Server, is the extension for the transaction log files. My understanding is the extensions don't really matter. Whatever is first is first and anything else is secondary regardless of the extension. Does that apply to .ldf in this case when clearly it's being used for the clustered indexes? I ask because I would assume SQL Server treats .ldf differently than say, mdf's. (And before you ask, yes, there is already a .ldf assigned for the transaction log on another spindle)