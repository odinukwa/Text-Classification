Currently we use HP servers with ILO boards to give out of band remote access and cold boots. IBMs have the same functionality with RSA boards. APC powerstrips are excellent for remote manageablilty. Throw in an IP KVM and you've got all the remote access you need. 

We have some old servers with 8GB C: drive. I end up deleting stuff like the $NTServicePackUninstall$ folder from c:\windows\system32 which frees up a few hundred meg. Also a server can end up with a lot of admin profiles on it, these can go too (of course they might come back again). What else? Move pagefile.sys to another drive if you can. Move the spool folder to another drive if there is heavy printer use. I use TreeSize Pro because it has a handy "File Ages" bar chart where you can quickly see if any files in a folder are over 6 months old for example. 

If so do they work well? I seem to recall that quotas under W2003 work on a per-volume basis which limits there usefullness. Is W2008 more flexible, maybe allowing per-folder quotas? 

I got my first job in 1990 by reciting the OSI layer model. They were most impressed. I didn't actually understand it but boy could I recite it well. 

I have a large Windows file server with about 2TB of data, about half of that is over 2 years old (based on modification date). What would be the best way of archiving off that old data, using scripts or whatever, but without spending big money on a full archiving (HSM) system? The purpose is to reduce the backup window, because all that old data is backed up every week when really it never changes and can be backed up much less frequently, thus reducing tape requirements. BTW the archive would be on another disk, with read-only permissions. Has anyone implemented something similar? How would users access the archive easily? 

Say I install a driver (FC HBA) that immediately blue screens the server (Windows 2003). If I reboot and select Last Known Good, will that enable me to get to the login screen? I know it loads the previous registry. 

Track KPIs for restores. It should be possible to produce a report showing how many requests for restores have been successful. Anything less than 100% should be investigated thoroughly. Management love reports and this is hard evidence. There should be documented procedures for all backup and restore operations, including all systems and their backup strategy, tape rotations, schedules, escalation paths, test restores etc. Ask to see them. Speak to the manager of the sys admins and voice your concerns. Go armed with proof that restores aren't working. If no joy go higher. 

We used to use IOMeter for this sort of thing. It used to be maintained by Intel but I think they dropped it and it's now opensource. 

The problem now is that DFS-R is broken and not replicating, because it is still referencing the old computer name, server-old. There are some Active Directory attributes related to DFS-R still attached to the old computer account. Am I able to fix DFS-R by associating the old computer account with the new server but keep the original name (server1)? I think this would work as it would fool DFS-R into thinking nothing had changed, and the DfsrPrivate folder still exists. I don't want to have to recreate the replication group as that would mean an initial resync. 

I have a spreadsheet on a Windows 2003 R2 server. The clients are all Windows 7 using Excel 2010. If a remote user opens the spreadsheet first then any other "local" user gets access denied if they try to open the file afterwards. By "remote" I mean across a WAN link, whereas "local" means on a LAN to the server. Is it possible to allow the file to be opened "read-only" instead of getting access denied? The spreadsheet is not configured to be shared within Excel. This is not a permissions issue, everyone can access the file at the NTFS permissions level. I suspect it may be to do with op locks and the fact that the remote user is across a slow link (satellite). 

I've just checked our BladeCenter H AMM options and unfortunately there doesn't seem to be a way to do this. 

There is no quick way to do this. Explorer will happily trundle off for hours (days?) applying the new permission to every file and folder (if inheritance is set). Enabling a share is much easier, the user just needs at least read permissions on the share. The underlying NTFS permissions will determine what the user can actually do. Note that if the share permission is read-only, then that is the maximum access even if the NTFS security is set to modify (r/w). 

Why are you letting them install software themselves? Usually there is a system is place to do this, either with servicedesk (level 1 type support) or software like Altiris which installs apps with admin rights even if the user doesn't. Also, your users would need admin rights on the local PC which is also a bad idea. We have an app share that is locked down by security group of which only a few IT staff are members. Users are not admins and get an error if they try to install anything. Otherwise they would install all sorts of crapware. Even the "skilled" ones. Actually they are usually the worst. In addition if you let users install whatever they want how do you enforce licensing? 

It will copy the entire file each time you make a change to it. What are the file types? Microsoft has made delta sync to SkyDrive (now OneDrive) available on Windows 8 with Office 2013 (for office docs). $URL$ But outside of that, pretty sure you are stuck with uploading the entire file each time. 

If the box is a member server check in the local Administrators group on the server. If it's a Domain Controller check in both Administrators and Domain Admins in Active Directory Users and Computers tool. 

Snapshots are 100% reliable in this scenario. I would be inclined to believe your developer has made a mistake. The snapshot doesn't deal with "files" it deals with low level disk blocks. If there was any problem with the snapshot you would have more serious errors than missing files, more likely you would get partition corruption or disk failure errors (assuming the vmdk could even be mounted). 

I use Quantum DLT-S4 tapes (800/1600GB) in my library, which has two Quantum tape heads in it. I have had quite a few bad tapes i.e. tapes that come up "CRC error" or "cannot read tape" etc and the backup software marks them as bad. I've sent back around 20 to Quantum out of 100 for a refund. The library is only 18 months old, and one of the drives has already been replaced a while back. Still get bad tapes. How many bad tapes do you guys get? 

Yes the Western Digital NAS boxes can do this. Just plug in an ordinary USB external disk. However I think the new disk is only available as a separate share, you can't extend the internal disks to include the external one. 

In my experience what works best is the Master IP Spreadsheet to Rule All Others. That is, there exists one spreadsheet and it is everyone's responsibility to update it. If it ain't in the spreadsheet, it ain't live. No ifs no buts. People soon learn to update it. Also, I wouldn't use DHCP for any servers, even test ones. The last thing you want on a server is the chance that it's IP might change.