(Not sure if worth it to handle edge cases, but I assume joins are not required to simply rebuild each form, if the following tables are strictly dependent. I.e. if rates were separated out, I would already know the form_id for a given or and simply SELECT for it, correct?) : describe calendar info pertinent to each event date. About 80% of events only have one date. Per the above: Should the "default" (i.e. primary) event_id for each series be placed in ? Should it be a boolean (is_primary, is_default) in ? Or should it be a field in that is UNIQUE foreign key ( = form_id), default NULL? (to mark it as the default, or primary record) I want to structure it so that the php can't accidentally cause data integrity issues by setting non-unique default child record ids (primary date, default billing) for each form. : speaker info unique to a given date or form (as submitted). The idea here is that old info from the date-of would be preserved even if the canonical info for a given speaker in the contacts db is updated. the form could AJAX query the contacts db to allow read-only hinting if a speaker appears multiple times, in which case the info listed in the form would be overwritten with the then-existing contact info for the speaker and a reference to the unique key. event_speakers would be many-to-one relational to each event date. If a speaker is overall host, then they would be listed as default for the entire form, but I am not sure where best to put this, as a flag or foreign key (same question as above.) Like primary event date, primary host info would be unique to each form, one-to-one, but would be in the same format / table as all other event_dates or speakers respectively. Perhaps I should separate them into a cloned table for primaries only? The problem with that is, the client may wish to allow a different speaker to be made primary, or allow a different event in a series to be the primary date without having to INSERT/DELETE anything. OK, so here's my main problem: : list all actual dates in which the event occurs, queried by the web calendar. I currently have as listing custom event info only, i.e. dates with something different going on than the primary event_date -- different lineup, different description, or different hours. (Different lineup would simply be a reference to different event_date_id in ). However, this leaves me with that simply repeat at intervals (some ad infinitum). I can put the repeat-interval (or date-mask, e.g. mTW) in , but this only helps to set dates, not retrieve them. And I realized I needed to track dates that are not in the calendar, such as "every second wednesday EXCEPT July 1st. ALSO INCLUDING July 20th." I figure the best way of handling this is to use the info in form_control to set or generate dates but then write a record in ongoing_dates upon-submit for every custom or blackout date, (otherwise scheduled dates for which it is reserved, or dark), with default = reserved. (reserved would be treated as a suggestion if the event is not yet confirmed, i.e. pending). That way, only primary, default, or custom dates (dates for which there is any data other than "it's happening again") would have their own record created. The rest would go in ongoing_dates.its_happening to be simply queried by the web calendar. Possible values for its_happening would be e.g. "it's happening again", "it's not happening this particular interval", or... "it may be happening", which can be represented as a simple TRUE-FALSE-NULL. ongoing_dates would also have a column for: event_start and end_time (using form_control start and end time as default, allowing custom times without creating a record) and, importantly, an optional event_dates foreign key. (i.e. every event_dates record would have a 1-1 relationship with ongoing_dates, but 99% of ongoing dates would not have a separate event_dates record, unless one is created by simply adding the ID.) This way the calendar can add and delete ongoing_dates at will without losing any information (e.g. if the exact same event is made once a month instead of twice a month) My question is, does this approach make sense from a normalization standpoint? Should I separate time and visibility info entirely into ongoing_dates, so that each event_dates record refers to a date that is only listed in ongoing_dates? Or must every date in the calendar have its own full record (with mostly-default information) and the calendar queries only that? Or does it not make sense to have a separate (small) record for potentially indefinite repeating dates? Those seem to to be my three options. I don't want the database to fill up with extraneous info simply because one event happens every day of the week. 

The first will explain the data structure most commonly used by indexes, the B-Tree. The second explains how MySQL uses the B-Tree. The third will tell you about the command , which is how MySQL describes the query plan (it will tell you which (if any) index it is using, if it is doing table scans -- which you must avoid at all costs). To create an optimized index, you should first think about the structure of the query (or queries) you will need. For example, it might be something like: . You should analyze the cardinality of each column (ie, how many different values can the column have). You chose the columns with highest cardinality to be the first in an index, leaving those with lower cardinality to the end. Example: suppose you have 2M rows and is a number between 1 and 1M, randomly distributed, and is the full name of the owner of the file. In such a situation, you want the index , in this very order, since the clause will leave you, in average, with only 2 rows. The cardinality of , on the other hand, is much lower: there are much less then 1M possibilities for people's names. Therefore, if your index is , the query will first look for every row in which begins with (which will be likely tens or hundreds of thousands), and only then it will look for the other condition. Also, note that the index is used from left to right. That is, if you have 3 columns, , and and you index , the index will be mostly useless in a query such as . It will likely be used to find rows matching , but after that every row will be checked for . If most of your queries are like that one (and some might specify B as well), than your index should be . Finally, note that can use an index, while (or ) cannot. This is, again, because the index is read from left to right. To match , it knows where to start looking; to match it must check every single row. All that said about indexes, I'd strongly suggest you to rework your criteria so that you have something more structured. As you said, you could try to precompute something. There are other considerations, such as the size of the content. If you can make it fit in under 8KB (which is like 3000 characters if you use UTF-8), then InnoDB will store the data in the same page as the primary key; otherwise, it will store the data elsewhere. If you query by primary key, in the first case you have a single read operation; if you query by another index, in the second case you have 3 read operations: one to find the primary key of matching row, one to find the row by primary key (to read the address of the data) and one to read the data. Oh, check the amount of RAM of your server. Ideally your data (or at the very least your indexes) should fit in the RAM. By considering all these points, you should have no problem at all: I don't know the hardware of your server, or its load (since you said it is shared), but 800k rows is close to nothing if you fine tune your indexes; I'm very far from expert and, by doing all the above things, I work daily with (very optimized) tables with 10M, some 100M rows, and the queries are ultra-fast. I hope that helps. Once you have your table(s), you could ask another question showing the statement and describing a bit about your data (sizes, cardinalities, etc) and the select queries you will use, so someone could help you to create an optimized index. 

Could this be a restriction on my IP address? If so, why does it allow me to bind? Or, is there a way for me to find out the patch levels of both databases? (It's going to be much easier for me to have the DBAs install a patch if i have a patch number, and a description that clearly states that this is what prevents me from selecting). Edit 3: How it ended Just in case anyone is interested or has a similar problem: I got the DBAs to bind the V10.5 client packages. After that, 

Edit 2: even more information Binding works, but doesn't change the error message when i select from sysibm.sysdummy1. Binding , as suggested by IBM for SQLC2K26 results in missing access rights; since this is a production database, i can't get the DBAs to change anything. Ok, so i tried the test database, which has an identical setup, except the I is changed to J: 

I'm migrating a bunch of perl CGI scripts from an older RHEL5 installation to a RHEL6 server. On the old server, DB2 V9.7 was installed, the new server has V10.5. No databases on the servers themselves, just several connections to remote databases. I can access one of these remote databases, which is running on AIX, without problems, so the installation itself should be ok. I can connect to another of the remote databases, without problems on the old RHEL5 server. On the new server, i can connect ok, but as soon as i try to select something, the connection breaks with the following error: 

worked well against the productive database (DB2I21). My perl program still crashed. Deinstalled DB2 Express C 10.5, installed the 10.5FP3 DS driver. Still the same problem, both databases work from the command line, the test database works with perl, the prod database doesn't. I did a wireshark trace on the perl program against test and prod DBs - up to a certain point, they seem to be exactly identical, except the password exchange. After the client has executed the query and starts retrieving result values, the server just closes the TCP connection, without returning anything, not even an empty or error TCP packet. I gave up at that point and installed the 10.1FP3 DS driver. Everything seems to work well with that one. 

Note that the IP address, 10.199.252.155, is the IP of the database server, not mine, so it seems that the DB2 server detects some error and reports that to me, instead of the client detecting the error itself. On both of my servers, the commands to catalog the remote database (issued 2 years ago on the RHEL5 server, issued a few days ago on RHEL6) were 

(the rest is identical). When googling for my error codes, i found a lot of sites where a) the connection didn't succeed at all, or b) the connection broke for timeout reasons. Both these errors don't seem to apply to me. Also, those entries had some relevant error codes in the protocol specific section, not just "", "", "0" like i have. So the only difference i see right now is the different database release levels. Which means the only problem i can think of is, that the database is still running under something older, and has issues with the V10.5 protocol. Does this mean i have to downgrade my client to 9.7, or is there a way to tell the newer client to use an older protocol level, by changing the database release level in the catalog? If so, what would be the commands to do that? Or is there anything else i should try? Edit: Added info as requested by mustaccio. I wasn't prompted when i didn't pass user and password on the command line, so i had to modify the connect command to include them. Also, i changed username and password in this post - the real username starts with a as well though. Old machine: