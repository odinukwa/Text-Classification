Ensure first there's an index for , or better still, ; If this doesn't preform well enough, you may benefit from creating a temporary table: 

You could reduce that index even further by using a smallint calculated as the difference between a set date (e.g. 1/1/2010 +- 32768 days = Apr-1920 to Sep-2099): 

All of the options above have a significant performance impact. A possibly better performing option could involve a table for this purpose: 

I found that your query had many redundancies in the conditions, and you used cross joins that were good candidates for simple joins. This might confuse the planner. Perhaps you could try the following rearrangement of the query (it is functionally exactly the same but uses joins and removes all the redundant comparisons) to see what the planner comes up with in both production and test? 

The keyword is context sensitive within an statement only (some systems may support it as well within ). You can't use within the context of calling a stored procedure. Also, MySQL does not yet support default values for stored procedure parameters either. An alternative can be: 

If you're able to create a table, and update allows you to use the result of a subselect (all valid SQL92 code), you should be able do perform the following multi-step process: 

I have to investigate a MySQL Production server,While investigating I found a query in which was taking more than 400 second in state, I tried to find the query in slow log but i was unable to find the query in slow log. 

What I understand from your question I am answering on that basis. If you want database dump use client utility.You can Execute it as 

If you want to change some of the global configuration setting of MySQL There are two ways. 1. By Changing variable using option on running MySQL instance,this does not require server restart we can change variables like Ex : I need to set my key buffer to 128 MB ; But using this technique we can not change or configure all variables and once we restart MySQL these GLOBAL settings are lost. 2. By Changing variables in config file (my.cnf) This requires a server restart but changes are permanent. 

This is due to MySQL Master and Slave version mismatch The message below is clear, and confirms the master/slaves run different versions. The variable is not supported with but is supported with 

Instead of creating trigger you can use below code, It will also return you success or failure message. 

First of all, if is the 's Primary Key, then there's no need to specify at all. Second, grouping and then filtering by the grouped key may be very inefficient when the filtering will likely render a small subset: filter first, then group. Third, I assume you don't require casting to varchar when removing the DISTINCT clause. Fourth, no need to include the alias of every selected field when there's only one table in the selection, it just makes reading the query more cumbersome! This query can really benefit from indexes on and . This may perform better, especially if those indexes exist: 

The size of the blob can be queried without having to read the file to memory and calculate the size of the blob, by simply getting the file size from the filesystem: 

With your datasets, MySQL has to obtain those 450,000 records from posts (in 1000 little chunks from each matching source_id), sort it, and then return the top 10. It is a costly exercise. You could resort to using a stored procedure, and accumulate results going back in time, say daily or weekly, looping until obtaining at least 10 records, and then returning the 10 most recent ones. You'll need an index on by . It would return quickly for the most recently active users, but take much longer for users without recent posts. Something like the following: 

this will create the TableName.txt file and TableName.sql files at the location as you specified with --tab option where TableName.txt is your CSV you can rename it as TableName.csv. Note : use the (--tab="PATH") path where the mysql has write permissions. for various other options of mysqldump. please see.. 

I am bit confused between setting the global and session parameters , I am trying to set The default settings are 

but while selecting the result i am getting invalid output. where budgetID is Primay key bigint and StartDate is datetime and modifiedBy bigint. AND 

You have chain replication as Master--> SLAVE A--> SLAVE B (Slave A is Slave of Master and Slave B is slave of Slave A) As you told On Slave Server(A) binary log is enabled. 

If you don't have backup. I think the recommended way to recover that databases is start MySQL with innodb_force_recovery = 4 (or higher values) and dump the databases to a SQL.Then drop it and recover from backup. Instead of dropping the original database I prefer to create the new databases with another name or in another server and check the content first. STEPS TO BE FOLLOWED 

Without knowledge of your schema, query attempted and statistics from explain analyze, any response can only deal in generics. In this sense and in terms of SQL, there are generally two commonly used strategies for dealing with finding missed relations: and . NOT EXISTS: 

You'll speed-up the query significantly if you create an index (ideally a unique index) on and , and another on just : 

The query may still not use the index if the regex comparison is anchored, when it considers that the results would not actually filter much (from collected statistics) and a sequential scan would perform better, e.g. when most of the rows in that table actually start with . Edit - optimising the query Your query is not optimised for large sets. Firstly, most of you filtering is based on a non-indexed comparison (). Secondly, you apply the filtering condition to the results of the join; PostgreSQL is often smart enough to translate the filter to the subquery, but it is best to construct your queries in a way that will ensure that PostgreSQL will the do the right thing. For example, filter by directly from : 

Use Backtick Sign around columns names. As you have a column which is a keyword in MySQL, So you need to put Backtick around column the name 

So I need to delete 611992998 records from biggets table. We have One MySQL Master and 4 MySQL Slaves, We need to delete data from all the servers, What I am thinking is i will delete data in chunks from master so that slaves also didn't lag too much.For that i have created a procedure here is the procedure, I have not yet tested it 

As you need to perform a read,search operation faster you can use the MyISAM as the table engine if the table will not have the High write's in future. When you will use the MyISAM Engine for fast read/search for this table ,you need to set the Key_Buffer_size to some appropriate value depending upon the index size of your MyISAM tables and also the amount of available RAM. As the Table having two columns id and domain name, so you can use the FULL TEXT index on domain name so it will make searching fast on the column domain name. Look the structure which I have Created