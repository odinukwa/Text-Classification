The simplest solution would probably be to buy another Cisco ASA 5505 for the remote office you are connecting from, and do a Site to Site (or EasyVPN) connection for the whole office LAN. Will be a lot easier to support, and more effective use of your bandwidth. 

I have a Sharepoint (OSS 2007) farm running on three virtual machines in VMWare ESX, plus a SQL Server backend on physical hardware. During a recent Business Continuity Planning event I tried restoring the sharepoint farm with only the config and content databases, and failed to get things working. My plan was to build a new sharepoint server, then attach this to a restoration config database and install the Central Management site on this server, then reattach the content databases. This failed at the Central Management part of the plan. So I am back to the drawing board on the best strategy for backup and recovery, with reducing the time and complexity of the restore job the main objective. I haven't been able to find much in the way of discussion of backup/restore strategies for Sharepoint in a VMWare environment, so I figured I'd see if anyone on server fault has any ideas or experience. 

I can't find any reference to this only applying to pages stored in the Pages library in Microsoft's documentation. What Microsoft says is: 

So a total of about 900 computers and an additional 100 network devices. Most of the network devices would be configured to send netflow (or sflow) data to ntop. Additionally, the ntop server would be connected to a mirroring port on our core switching. Is this feasible on a VMware virtual machine running on a five host vSphere cluster (also running about 40 other VMs)? Or is the amount of IO going to require dedicated physical equipment? 

Easiest way is probably to buy in a service like MailChimp. There are a lot of advantages to this - particularly that those companies care to spend the time to get your newsletter through spam filters etc, saving you the hassle. Next simplest is setting up a mailing list server such as Mailman. This will get you double opt in, list management, and sends to recipients one by one, as you request. If you don't already have EXIM or Postfix on your Linux server, you aren't going to find a super simple way of doing this, unless you already understand MTAs in general, and a specific one in detail. 

Where is your web server? If your web server is on the same local network, sitting behind the same firewall then you don't have a problem. If your web server is on the internet, then you would be sending your password in plain text across the internet to the web server - there are plenty of places it could be intercepted in between. One special case here is if you have a VPN connection between you and the web server, in which case the password would be encrypted across the internet, but I imagine you don't have this. Any web host worth its salt should allow you to use SFTP or SCP to transfer files securely. Frankly, any web host who only offers FTP for file transfer is a liability to itself, its customers and the commmunity at large. 

I was advised by the ntop community to update to the SVN version of ntop, and this did indeed start populating the graphs without configuring things differently. However, I have found that after collecting data for a few weeks, I am not seeing useful results. I have read that there are certain limitations with the netflow data from ASAs which may result in this. I think for better analysis I am probably looking at a different collection mechanism for data, and the lack of current and clear documentation for NTOP means I am probably looking elsewhere for collating and interpreting the data too. Back to the drawing board! 

instead of ? If that works, then it is the MIBs - see net-snmp wiki for a tutorial on how to install and use MIBS. Otherwise check your syntax as pointed out by Mike Weber 

So VirtualHosts directives and .htaccess files are out of the question. So the only thing left is to run a different instance of Apache for each subdomain, which would have a completely separate configuration file, which might potentially work with your management software. If none of those work for you, then you have drifted so far from the standard way of doing things that you probably need to write your own fork of Apache to deal with your specific requirements ;-) 

Unfortunately, this is our only site with an 800 series router connecting to the internet, so I am not very familiar with it, and I won't be able to test the configuration out properly until I get there. 

This gets me a bit closer. I'd still really like a script that iterates through all the profile properties and puts them into a CSV or XML file more gracefully. This will do for now. 

Do you have any experience with any of the distro's you list? If so, go with what you know already. If, on the other hand, this is your first foray into Linux servers, then Ubuntu or CentOS are probably better options in my opinion. Why? In my experience the documentation available for those distros was more consistantly approachable than Debian. I started out trying to learn Linux using Debian (about 15 years ago), and I went around in circles for a couple of weeks - I needed to understand x in order to understand y in order to understand z in order to understand x. Things may have changed but since Ubuntu and CentOS are both backed up by large businesses (CentOS being more or less the same as Red Hat Enterprise Linux) there are clear documentation paths, and books you can buy that take you through step by step. Once you've got either of these, you can delve into Debian with confidence. I doubt you'll get any benefit from 64bit unless you have more than 4Gb of RAM on your VPS. I wouldn't pay extra for this. The beauty of a VPS running Linux is that once you have it set up, you can upgrade, or migrate your config and data to a new VPS that is more appropriate. Start small and simple and work your way up. 

By all means virtualise your dev and test systems for flexibility or cost savings, but virtualisation for critical production systems should just be for resilience in my opinion, never for cost saving. If you had a couple of extra servers running Hyper-V that you could cluster, I would say go for it, but if you had the budget for that I guess you wouldn't be asking this question. 

However, once I'd tried this and since it was in maintenance mode anyway, I rebooted the host, and when it came back online, the Host Health was reporting again. 

Probably if you are just testing cables, but if you are trying to do cable mapping on a poorly documented LAN, a Fluke or similar can't be beaten. 

Restarting will achieve a lot of things, most of which can be achieved without restarting. One of the things that restarting will often do is clear the logs in memory, so you won't know anything about the problem after you restarted. So if you are seeing this behaviour frequently, rather than just blindly restarting, take a look at what is going on with the switch (assuming that is possible with the switch - it may not be if the switch is consumer grade, or the problem is really serious). Knowing what the problem actually is will help you fix the problem, rather than just working around it.