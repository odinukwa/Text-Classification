Please note that Pearson correlation (and mutual information) considers the concept and the single feature. There are cases in which a single feature is useless but given more features it becomes important. Consider a concept which is the XOR of some features. Given all the features, the concept is totally predictable. Given one of them, you have 0 MI. A more real life example is of age at death. Birth date and death date give you the age. One of them will have very low correlation (due to increase in life expectancy). 

A term alone appears most times. "and" is more common than "or" so a term with "and" is more common than the term with "or" Note that 

Start with Hierarchical clustering. That will give you an hierarchical grouping of the datapoint, matching each one to the closest to it. One of the benefits of such clustering is that you can decide afterwards what is the number of clusters (k) that you want and then get the actual clusters. In your case there is and additional constraint of the diameter. Hence, you should verify that the maximal distance between the items in the cluster is bellow your bound. Since that in hierarchical clustering we match the closest items first, when you divide into clusters you should check that the distance between the current item and the rest of the items in the clusters is bellow your bound. You can doing that while building the hierarchy, saving computation time. However, you can also do it after the hierarchy was constructed and this way use libraries that already implemented these algorithms and you will only need to add the diameter bound on your own. 

If I understand your goal correctly, you want to build classifier on similar entities taken from different distributions and than compare the classifier/source distributions. Am I right? By the way, if it is indeed so you might find the area of domain adaptation In this case I would go in your direction of splitting a database (since you need the same entities and features). In order the splitting will make sense I recommend doing the splitting by a feature that should change the entities behavior (e.g., country, far periods in time). That dataset you should use must be quite large in order to have large enough partitioned datasets. Specific such dataset that you might find useful are US census 2000 (plenty of options for concept and splitting features). 

The number of results is similar for "burglar AND burglar", "burglar OR burglar" and "burglar burglar" though we see that "And" is more popular than "OR". It seems that Bing treatments is the removal of "And" and "OR", possibly as stop words. Bing documentation suggest the operators "&&" for "and" and "||" for "or". - burglar || burglar 4,400,000 = burglar - burglar && burglar 1,610,000 = burglar burglar These results fit the claim that when a term appears twice in the search query it should appear at least twice in the document too. 

You can have a variety of solutions, starting from very simple to a more complex and beneficial ones. I suggest that you'll start with the simple solutions, jain much of the benefit and continue by need. Note that though the problem can be treated as a supervised learning problem, it is a multi label problem (a customer can be interested in many services). Therefore, decision tree or logistic regression is not suitable (unless you have a handful of services and you try to predict if the user would like a service, for each service). The classical method to provide such recommendations are "People that like X, also liked Y". Though very simple, at time 50% of Amazon's revenues where due to that. You should make the recommandation more sepcific by considering the lift - Harri Potter is very popular and many people like it. However, it isn't the most sutabile recommandation for someone reading browsing books. You can take such a item-to-item recommendation and use all user history by aggregating using Naive Bayes or similar methods. You can take care of temporality and dates by giving more eight to recent data. The next step is moving to recommendation system. You can find on the web some implementation. The winner in the Netflix challenge used matrix factorization algorithm, so this direction is very popular for recommendation systems. 

From my experience, when you ask on a good project why is it good, the project owner can immediately say so. The answer themselves are quite different (e.g, accuracy level, business values, etc). It seems that these project owners had in mind some definition of a good project while working on it, what helped them execute such a project. When very good project owners present a project, they say why it is good without being asked. 

Apparently looking for documents in which the term appears twice. By the way, Google's Search operators documentation claim that "OR" should indeed act as a binary operator. You found a case in which they fail to do so. Note that this behaviour is very specific to search engine. In Bing you get the following results: 

Now for your specific case: I recommend that you'll begin in computing the correlations among the features and the concept. Computing correlations among all features is also informative. Note that there are many types of useful correlations (e.g., Pearson, Mutual information) and many attributes that might effect them (e.g., sparseness, concept imbalance). Examining them instead of blindly go with a feature selection algorithm might save you plenty of time in the future. I don't think that you will have a lot of running time problems with your dataset. However, your samples/features ratio isn't too high so you might benefit from feature selection. Choose a classifier of low complexity(e.g., linear regression, a small decision tree) and use it as a benchmark. Try it on the full data set and on some dataset with a subset of the features. Such a benchmark will guid you in the use of feature selection. You will need such guidance since there are many options (e.g., the number of features to select, the feature selection algorithm) an since the goal is usually the predication and not the feature selection so feedback is at least one step away. 

The problem is moving from estimating a single hypothesis into few ones. One could claim that X and Y are symmetric, if we were willing to examine X, why shouldn't we examine Y? The difference is that since Y wasn't part of the original plan, it is possible that there are many other variables there Y1, Y2, Yn... Consider that we have extra n variables, all purely random. If we have a large enough n, one of them will have observations that seem to be correlated to F. In case that you consider a pair of variables, the number of options you have becomes O(n^2). The more complex hypothesis set you'll have, the more options you will have and more likely you will be to gat a false correlation. It doesn't mean that you should ignore the result regarding Y. Many discoveries were accidentals. As Robert de Graaf suggested, you can do another experiment and check the Y-F relation. You can also check multiple hypothesis techniques in order to evaluate your current results in order to estimate whether the new relation is significant. 

The churn rate is users the ratio of active users that stop being active after a given period. The churn rate for application user is exactly the one used for other cases, like employees. However, when dealing with employees it is clear whether the employee is active, The problem with application users is the identification that they are no longer active. What you can do is to identify a period of in activity after which, the vast majority of these user never return. For example, take the users that were active during January of the previous year and were not active for 7 days since then. If 99% of them were not active for the rest of the year than 7 days are a good indication of being in active. The suitable period is usually determined by the domain (e.g., some applications are used in periods longer than 7 days) and the required confidence level (99%, 90%). Note that you don't want to be too safe because if you'll decide that a user is inactive only after 2 years, this definition will prevent you from identifying users that are practicality lost and take actions to reduce the churn rate. 

Let's say that we have a perfect algorithm for imputing data. You give him a dataset with missing data for some features and it predicts them. I had had such an data imputation algorithm, I would have use it as a classifier. The fact the we can reduce classification to data imputation means that dat imputation is as hard as classification. As Rayn said, you can indeed do so. Imputation algorithm could have been evaluated as classification algorithm but this is not very common. One of the reason is that the rule by which you hide data from the imputation algorithm is subjective and might effect the results significantly. In whatever method you use, you assume that the data behave in someway and missing due to some rule. It is also an important question, since errors in the imputation (you will probably have such), will effect the process down the road. I prefer to avoid imputation and let the prediction algorithm cope with the missing data. There are many such algorithms. A different approach is to use few methods of imputation, choose few simple classifiers (so their complexity won't have a too large influence on the results) and compare the predications. In many cases, there is no big difference. When there is a big difference, of course that you will prefer to proceed into a more detailed analysis using the winner. However, before that try to understand why it has a big advantage. That might lead to interesting insights about your data. 

You might have a domain adaptation problem. The samples are actually taken from two different sources and have different rules governing their behavior. I suggest that you'll try the following method. 

You should note some points: Some association rules with different implication might apply to the same basket. You might have the rules milk -> X and beer -Y and the basket {milk, beer}. If the set {milk, beer} is frequent enough you might have a rule specifically to it. Otherwise you'll need a met rule in order to base by the rules. Such rules might be taking the rule with the highest confidence or the most likely class. Association rules are based only on the existence of item, not on the lack of them. If students never buy diapers, you will not find such association rule. You can add "no diapers" as an item to your data set but that will turn it from a sparse one to a full one and the storage might be unfeasible. The complexity of association rules is exponential. The is simply since the output it self is exponential , as can be noted when you have a Clique in the data. Usually people don't try to find all association rules due to that problem, which is the practical thing to do. However, it means that there might be association rules in your data that you won't find.