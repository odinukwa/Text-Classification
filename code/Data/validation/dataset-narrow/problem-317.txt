Not really. Since Excel isn't a proper client server database, you can't ask it how many rows are there before bringing them in. You could bring them into a staging area and count rows as they come in. If they don't match and you need to handle that with a business process, the destination is untouched. If they meet your validation criteria, do a second transfer to the real destination. You can use a database table for this, or a raw file connection if you'd rather spill to the filesystem. Cache connection managers are useful if your dataset is reasonably sized. 

It's not clear what you're try to do. It might be that you want a read-only copy for reporting; in which case you should probably consider log shipping, db mirroring or the like. If you just want to be able to query data from another server, linked servers are an option. The queries would always return fresh data, but there's a performance hit to consider. SSIS is the correct tool, if you want periodic secondary copies updated batch-style. Service Broker is almost certainly far too complex for your scenario, but addresses asynchronous cases. 

I have a process step where I have an XML document in a SQL Server column and I need to alter the text of a node that could occur multiple times. Modify() with 'replace' DML only operates on single nodes. To complicate matters, the replacement needs to happen based on a lookup table maintained locally. I can use nodes() to get a nodelist and contrive a query to join the data appropriately, but at that point, I no longer have a reference that modify() can act on. It appears that my only choice is to create a CLR procedure or function and use some sort of dotnet native XML iteration. That's less appealing, since the business logic would be locked up in the source code, not visible in the procedure definition. Is there any way to do this transformation using the XML methods exclusively? 

How do I structure a conversation or series of conversations to touch all three systems? Can I have 'A' submit to 'O', which does a lookup into 'C' - all in the context of a single conversation? 

I'm trying to implement a Service Broker architecture for a business process involving three systems and I want to do it in the spirit of using SB as designed. 

Developer Edition was created for this specific reason and can be used perpetually. It was $50 USD last time I looked. Like the Eval, it has all the Enterprise Edition features. If that's not the target for deployment, take care not to use those features. Another thought is that Amazon offers AWS instances that include SQL Server licensing built in. You can test deployment against a Standard Edition Instance and only pay the hourly usage rates. 

If you intend to run the SSIS jobs from the SQL Agent, then they will execute as the user the agent runs as. That's usually the same as the account the instance runs as, which has rights to everything. 

That's a filesystem concern, not an SSIS one. Any file that SSIS creates will be owned by the user SSIS runs as. The best solution would be to set the ACL on the target folder to be what you want; when SSIS creates the file it will get those permissions by default. If you can't do that for some operational reason, you can manipulate the file after creation with a command task. You can use SetACL or any command line tools for this. 

In this example I'm cribbing from a previous question, My source table has a varchar(max) field that I want to split by line into individual elements with FOR XML as I query it. Given this data- 

Is it safe to assume that while a database is in single_user mode, the queues are not available for message delivery? My understanding is that the special tables that the queues represent reside in the database context, but it's not clear if message delivery counts as a 'connection' in the usual sense. 

This isn't a direct answer to your question, since what you want is trend analysis, but you should familiarize yourself with the reports in SSMS at the different node levels. They are much-improved over previous versions. There's one at the database level that shows size of individual tables. Look into the Management Data Warehouse, as well. 

I'm looking into Central Management Server and I'm a bit surprised there's no 'Custom Reports' node in SSMS to add things into. It seems like that's exactly where you would want to place something like that. Failing that, is there any documentation on how to create SSMS add-ins that use the CMS server list? 

What edition and licensing mode are you in? You are probably not using all the cores. See the note on this page - $URL$ "Enterprise Edition with Server + Client Access License (CAL) based licensing is limited to a maximum of 20 cores per SQL Server instance." 

You are almost certainly 'doing it wrong' in one way or the other. If your application truly needs to operate that way, you probably should be using some other sort of datastore. Maybe something like Hadoop or a key-value store. Alternately, you may be doing something that is inherently a batch operation and should be done on the server side rather than remotely. Your description sounds like load-test simulation software designed to find the limits of connection pooling. 

I have few SQLCLR functions implementing business logic that return SqlXml data. I would also like to make the Xml Schema Definitions in the dll available in Sql Server. It's not a 'SqlUserDefinedType', as it is used indirectly by the Xml type. However, the collections node appears under 'types' in SSMS. I would prefer to keep the xsd along with the functions to simplify deployment. Is this possible? 

I'm trying to implement a writeback measure group for required quantities by month and part number. I'm puzzled by the behavior I am seeing in Excel 2013, but I don't know if SSAS or Excel is responsible. If the dimension selection in Excel is not the native granularity of the the underlying Measure Group, 'Publishing' the data distributes the data as equally as it can among the dimension members. Which is not necessarily wrong, but a bit weird and definitely unexpected. e.g. If I publish a value at the year level, it will divide it equally into quarters and months. If I don't include the part dimension it will divide the quantity equally into all part numbers. I need to prevent data from being published anywhere but at the intersection of Part and Month. My only thought so far is to control write access through the Role with an expression that includes the Measure and Dimensions, but I'm not having much luck with that due to my limited MDX skills. Is there any other mechanism to modify writeback behavior? 

As a newly minted DBA, you should understand that, as a matter of policy, SQL server will consume all the memory in the system to the exclusion of all other programs. If you do not want that behavior, you need to set the MAXMEM setting for the instance. SQL server is not just an app, it has low level functionality that overlaps with things you would ordinarily expect the operating system to handle. That's not a problem, it's intended behavior. If you are on 2012, look into "xVelocity memory optimized ColumnStore index". It's a way to get a taste of columnar performance without re-architecting your schema. You could also look into the SSAS Multidimensional and Tabular/PowerPivot/"xVelocity in-memory analytics engine" products,but those would be a significant effort. If you're really forward thinking read up Hekaton. I'd also suggest you start reading Brent Ozar's Accidental DBA 6 month training plan. 

In SSIS(2012 specifically) there is an option in the Flat File Datasource to skip a number of lines at the beginning of a file. The preview appears correctly. However, when trying to execute the data flow, it appears that those rows are still being validated; so that if they contain invalid data(blank rows), the datasource fails. This prevents using a conditional switch to remove rows, since the data flow never starts. Is this accurate description of the normal functioning and is there any way to work around it? Sample data looks like - 

In that unpleasant case, I think the best strategy is to compartmentalize it away from SSIS. Either move that out to a linked server with an altering view or use some other etl tool like a powershell script to get it out of the dancing schema into a table with a fixed one. If you have any history you should be able to guess a reasonable value for the upper size of the field. You could always make it varchar(max). I'm presuming MySql is doing a 'select into' to create the table. In any case MySql is not making any reliable assertions about the data, so you need to do that yourself before SSIS is going to be happy. 

I'm re-designing some SSIS packages for 2012 and some parameter/variable/configuration issues are not clear to me. The result that I am trying to achieve is that the user context's temp folder(stored in the %temp% OS environment variable) is available in my package. Specifically, when I open the package in SSDT, it should expand the environment variable to the current path so that it passes validation. I can get this at run time with script task, but it's ugly working around it at design time. In the SSIS catalog, I can apply an environment variable to a parameter, so that aspect is not an issue. How can I get this to happen when a project is opened? 

That's either a problem with MySql or with the driver you're using. All SSIS knows is that the driver says 'I can't do that'. The place to start is by manually inserting as record of that kind into the destination db and looking for a more meaningful error. 

I have an app to deploy in production that uses 'honor system' security. That is, all users connect to the DB using a SQL user/passwd credential and the app manages permissions itself. The latter part doesn't bother me as much as the fact that the connection object contains embedded credentials and can be copied around freely. I'm try to find some way to limit connections to a more limited set of clients. I can create firewall rules to limit by IP, of course. Is there any way to 'prequalify' SQL logins either by Machine account or domain membership? 

I have done a bit of experimentation with non-relational datastores and found the integration tooling is usually poor or non-existant. It's probably more productive to approach this as a pull from the MarkLogic side than as a push from the SSIS side.