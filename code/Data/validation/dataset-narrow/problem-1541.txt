I stumbled upon some rules of thumb for dataset sizes, but not specific to the ratio of features / samples. Also, it seems easier to guess to the "positive" side, i.e. "might it work?" then "might it fail?". I witnessed both small datasets (<1000) that functioned well with even 20-50 features, and huge datasets (~10M) that functioned well with very few (<5) features. The stage in the processing pipeline to tackle this, is the feature selection one. I'd start with gathering all of your make-sense features and playing with different k-best values for the feature selection object to find. (ranges like: 10, 50, 100, ..) You didn't mention a specific research environment, but I'd like to suggest scikit-learn sklearn.feature_selection.SelectKBest running inside a GridSearchCV pipeline. This way the grid object will help you choose the closest k-best featues out of a given list. Something like: 

To convert from string to float in pandas(assuming you want to convert Employees and you loaded the dataframe with df), you can use 

I used Binary classification for sentiment analysis of texts. I converted sentences into vectors by taking appropriate vectorizer and classified using OneVsRest classifier. On another approach, my words were converted into vectors and there, I used a CNN based approach to classify. Both when tested on my datasets were giving comparable results as of now. If you have vectors, there are already really good approaches available for binary classification which you can try. On Binary Classification with Singleâ€“Layer Convolutional Neural Networks is a good read for you for classification using CNNs for starters. This is one of the first blogs I read to gain more knowledge about this and doesn't require much of pre-requisites to understand(I am assuming you know the basics about convolution and Neural Networks). 

To find similar words in semantic and synthetic, which is used to search and information retrieval. A skip-gram model is useful for modeling those like click-sequence data, which could be used in recommendation 

Suppose, for example, that the first search result on a page of Google search results is swapped with the second result. How much would this change the click-through probabilities of the two results? How much would its click-through probability drop if the fifth search result was swapped with the sixth? Can we say something, with some level of assurance, about how expected click-through probabilities change if we do these types of pairwise swaps within pages of search results? What we seek is a measure of the contribution to click-through rates made specifically by position bias. Likely, how position ranking would affect the sales in Amazon or other online shopping website? If we cast the sales into two parts, the product quality and its ranking effect. 

It looks like you're looking for a smart aggregation function, which aggregate n folded rows instead of the usual 1 (out of the many falling to the group-by criteria). Some of the aggregation function you suggested like min and max can be adjusted as the n-smallest or the n-biggest values. Alternatively, the min/max n-values can be a skip list of the sorted value (ascending for min, descending for max), choosing every n/5-th item value In a wider approach, we can borrow the concept from time-series, where at each time interval a set of parameters are sampled. Instead of the built in time-stamp buckets (say every 1 sec, 10 min, etc..) you can use your images index, thus preserving the order. As with time-series, that can be farther more reduced into smaller time-series (say from a 1 sec interval into 1 day interval), write a custom group-by function that "folds" every n/5 image-rows into 1 row (so that it'll leave you with 5) and apply your min/max/custom aggregation function on every n/5 values of your magic-image-property 

I think that depends on the pattern of your data, its skewness and sparsity. Because it would press these two bias term, which is designed to detect the difference with global mean. For example, you can plot mean and std of the user's bias: ($R_u$ - global_mean) then make a histogram of std. Another way, just make up a rating matrix $R'$ according to the $\mu + B_u + B_v$ you've got. And run the solver again and see what happens. These parts play its own role, even your change on global_mean would make a difference to the final RMSE, I think the role of that latent factors are born to capture the REST non-linear pattern in user & item's interaction. So, as for your question: 

I trained a word2vec from the gensim package. Even though I pass a word in the model.train() method, it doesnt appear in the model's vocab. 

If you look at the Keras documentation, you will observe that for Sequential model's first layers takes the required input. So for example, your first layer is Dense layer with input dimension as 400. Hence each input should be a numpy array of size 400. You can pass a 2D numpy array with size (x,400). (I assume that x is the number of input examples). Your output has 13 dimensions for each output document and hence your final Dense layer has to be instead of model.add(Dense(1)). Coming to how it knows, it takes the first dimension of X as number of training examples and second dimension as the size of each example. Similarly for output layer. 

Sure, as @LaruiK said. You've notice the factor which would change the similarity measure of the 'co-view' data, a simple way it to integrate a time decay function into your model. 

I think your last question is worth discussing, but forgive my careless on skipping the details of the model and just leaving a quick answer here :P Repeating a sentence in your corpus would definitely change the learning result, and strength the relationship of the words in this sentence, because one of the models behind word2vec is , which assume the center word can be used to predict its surroundings. But I have to ask another question coming follows: what is our purpose of using word2vec? 

I think Gephi, an open-source visualization tool, would help you a lot. Actually, as I know, the InMaps and its community detection algorithm are same as the Gephi's.