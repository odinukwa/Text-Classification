Gives the list of all the IDs cursor through those values and simply run on each of them. After the process has finished the temp DB space will loosen up (Note it does not do it as you close them, it seems to do it in large chunks when you're no longer working on creating / ending them (I cant guarantee this is how it works, just what I observed after stopping mid process and tempDB getting some space back) 

you cant have multiple databases 'in use' at one time, as there are a lot of things which change between databases even within the same instance. You can access databases outside of the currently accessed database like so: 

Just putting this point out first and foremost, you should not use select *, you should declare the columns you want, even if you want everything you should have everything listed one by one so any column additions later on don't potentially screw up other functions. Now that's over This will get you the column names, but I can't guarantee the order that you will be pulling the data out, they will be listed here generally in the order that the tables / columns were created, which will be potentially be different to what you are getting in your initial query, 

to catch up with the backlog from the night before (when most of our deletes take place) I'm wondering if there's any way to change the default settings from 5 seconds and 10 pages to say every second or run over 20 pages, is there any way of doing that or should I continue just spinning up multiple cleanup procs to clear out the data, or if there's any other actions that can assist with this Re-indexing runs on most effected indexs atleast once a week (most are every other day) SQL Server 2012 Enterprise SP3_CU8 (upgrading to CU9 tomorrow) on AlwaysOn High Availabilty Cluster also with replication (distribution on a separate server) 

This is a pretty good case for Distributed Partitioned Views. It would be fairly easy to implement, especially if the archives have the same schema. 

Doing it in that order avoids updating rows you've just inserted. operations do the and steps separately in reality, so concurrency issues should be no worse than with . Community wiki answer 

Not clear to me if you are looking for synonyms. If so, and you need mere synonyms in the future, check out the word-choice tag on English Language & Usage Stack Exchange. You might need to be careful with column names like these as they might confuse a user, since they can have a different meaning than row status or record type. 

Opening the office 2010 database in 2016 removed all of the VBA references to 14.0 Object Libraries in the References menu, and replaced them with 16.0, which is not present in Office 2010. Broken references are to be expected when the is opened in an older version. It happens when there are references to Office libraries (Word, Excel, etc). Edit the references (in the VB Editor) in the oldest version (the one that doesn't run the code). 

The GUI is nice for some simple quick tasks but not for partitioning a table with billions of rows. Use T-SQL so that you know exactly what is going on. Large tables are unforgiving. 

and then copy the output to the select statement (remember to remove the final,) which then allows you to pick the exact order you want everything to come out in which is a win under you're not using select * and a win under you get to choose which columns are where 

You could try putting the remoteData into a temp table before doing the merge. a lot of your time will be taken up with comparisons of data not the actual retrieval so if you run the 

The problem you have is that the where clause is executed before the Select clause of a query, so when it hits the where section it has no concept of what the muni_score is There are two simple workarounds (not taking any performance into account here) and I'll note down a slightly more complex one. The easiest is to simply replace the muni_score in the where clause with what makes is up, so: 

The method of insert doesn't appear to be the problem, more the data itself The error you are receiving is because one of the columns you have in your database table is too short (see your column 'wikipedia_link') see what length it is and if you can increase it, note it is quite possible to have more errors on other columns that are similar, judging by the data contained in that field the longest field is 128 characters, so I'd make the field nvarchar(130) as a minimum If you load the csv in excel you can find out the max length of a column by using {=MAX(LEN(Q:Q))} (NOTE to get the {} array calculation after typing the query press ctrl + shift + enter) Check all your fields are long enough and try again 

This a typical use case for a queue. The problem is that you're mixing state and events. It will never work. Separate state (your table) from the event ('needs processing'), store the events into a proper queue table, and dequeue by deleting. 'Consuming' is a dequeue operation. Adding a new record (with nulls) is an enqueue. Dequeue destructively from events, then visit state as many times as you fancy. See Using tables as Queues by Remus Rusanu. 

Community wiki answer: Yes, the query can still be active (and is in this case). However, while DDL autocommits, data inserts don't. So it may get to that step and then die if pgAdmin is not there anymore to commit it. 

If the differential takes a long time (e.g. 3+ hours) it indicates a lot of time was spent undoing the effects of a long-running open transaction. 

Community wiki answer based on a comment originally left by the question author: Finally I solved this issue. The problem is in phpmyadmin: I can see those using MySQL query in command but still not in phpmyadmin. It's a bug in phpmyadmin as mentioned in How to support full Unicode in MySQL databases by Mathias Bynens. 

A separate idea would be to have a table alongside your main one, with one ID per word you are interested in, and an ID related to a specific text, and just populate it as needed. The choice will obviously depend on many other constraints that only you can know. 

TLDR; Check for conversations being left open completely. In our system we re-use conversations and have a table dedicated to holding these conversations that are usable, however the dev team setup a new service broker without my knowledge ages ago while I was off, didn't set up these conversation points and didn't set any thresholds on the alerting. When the new system has been turned on the conversations are being opened but not closed properly and since there aren't any in the pool it is just creating a new conversation (we reached 7.1 million conversations for one service broker) My steps for fixing was to create and record the 20 conversation handlers that I require for that service broker and record them into our table. This stopped the growth of the tempDB to stop the risk of the DB going down. Then came the long process of closing off all the un-used conversations 

I may be understanding this slightly wrong so I apologise if I am. If you have two databases that you need to be identical and are on 2014 then use the AlwaysOn High Availability Group. Since you're data centres are at separate locations use the Async mode This will mean the database is kept completely up to date (all be it possibly with a few second delay) and you can have the secondary node as a read-only replica, this means that your alarm system can read into that database run all the checks etc you would normally. the Always on system keeps everything up to date, so if the connection drops, when it comes back online it will merge over all changes It also means that if your main centre goes down you can set it to automatically failover to the secondary, when the main datacentre comes back online it will re-sync with the (now) primary node, at which point you can fail it back over to your main centre. You can run this on multiple databases, so we have our main DB and our Admin DB synced across our nodes, however what runs all the jobs and direct actions on each side is not replicated so can stay independent of each other 

Community wiki answer: Is it possible that the text appears in deleted rows whose disk space has not yet been "recycled" or "garbage collected". If you vacuum the database and check again with grep, the text should be gone. 

A backup contains data since the last backup. and backups do not truncate the log. From The Transaction Log (SQL Server) 

...you should still be able to use separate and statements, for the same effect. Using the same basic criteria as in the , you should be able to: 

Community wiki answer originally left as an edit to the question by its author: The problem was that the database property was set on. The solution was to set to false. 

Unchecking seemed to fix this immediate issue, but caused a new one. Now the 'me' user cannot expand any server nodes in SSMS. It looks like ultimately some of our logins were orphaned because we created the databases from backups from a different server. Community wiki answer based on comments left by the question author 

As a workaround, if you use date-based partitioning, you can leave the most recent partition uncompressed. Also Clustered columnstore tables do something similar, with initial inserts going to a row store, and later being compressed into Column segments. 

A backup never breaks the log chain. A non- backup just resets the differential base (the reference point for a backup). So, no, the would not prevent you successfully performing the following restore sequence: 

Note, are you wanting those default values in there? would make more sense (in my opinion) to have it as a required field on the execution of the proc so you don't accidentally run it with no values and presume its right 

It depends on what you need, the basic data for conenctions can all be found in table Main points of reference are 

This could be expected behaviour especially if you're deleting based on an index (aka where [primarykeyfield] < targetID) since that delete will take in the ms of time you'll almost always catch the process in the wait time between the two batches Say for example the delete takes 10ms (i imagine it will potentially take less than this but its just an example), the wait time is 2000ms So for every time you run the command there is a 10/2010 chance of hitting that in the period of time the delete is running (less than 0.5%) and a 2000/2010 chance of catching it during the wait for delay. If you want to observe it directly find the spid that its running over and run the profiler over it checking for the search and you'll see the delete occurring every 2s and going into the waitfor (note if you wan to see the start of the waitfor as well check the but the completed one will give you the duration of how long it takes to run each part. 

Community wiki answer based on comments left by the question author: I did as suggested and inserted records from a csv file and then from a command prompt, the sales tax was 0 and the table at a rate of .0925 It turned out was defined as an , the tax data type was . I changed to: 

Community wiki answer When I got back from lunch the backup completed successfully when was not specified. After seeing it fail all morning, and now having one successful, uncompressed backup, it seems as if the failure is happening when it gets to the log file. 

Does seem somewhat contradictory, but I agree it sounds like this is not expected behaviour for that field to be updated if errors were reported. On the other hand, your examples are quite minor issues, easily fixed by running so maybe SQL Server regards that as a successful run. 

Community wiki answer: If track is a serial column it might be easier to use . See Sequence Manipulation Functions in the PostgreSQL documentation. On the other hand, your example seems to be functionally equivalent to: 

Community wiki answer: Parameters and local variables are different beasts. The plan with parameters is generated (if not already cached) using row count estimates gleaned from the actual values supplied and statistic histograms. The plan with local variables is generated based on unknown values so the average overall density () is used to estimate row counts. The plans may differ due to different row count estimates. Related: Understanding Performance Mysteries by Erland Sommarskog 

To answer question 2 first 2 A primary Key is NOT always the clustered index, it can be the clustered index and in the majority of cases is the way things are done, but it isn't always the best for your data. The culstered index is the order in which your data is physically stored on the disk whereas the primary key (which can be composite) designates the field to be a unique field and is beneficial for not inserting duplicate values and for foreign key lookups if you wish to do joins (In summary for 2, yes you can add a primary key without a clustered index) NOTE: sometimes if you want an identifiable row you can add a new field to the table to simply act as the primary key (not always advisable, but can sometimes be the a solution to improve performance) 1 Adding a primary key can change performance, however the only true way to know if it's going to improve performance is to test it, if you have a pre-live environment consider adding it to there and running your queries across it. If you have queries that run as joins to this table on ID,AID,BID all together (Sorry I'm not 100% sure how these are all coming together) the Potentially create a composite primary key across all three which means when anything wishes to get data with this table comparing all three of those it can find that row with ease. (Hope this makes sense) 3 Adding a Clustered Index completely depends on your data, once again a pre-live environment would be an ideal situation for testing. A few things to consider when creating a clustered index, what data are you retrieving and what are you inserting (this is a general example)If you're inserting and retrieving data that is the most recent data then a clustered index on the date field sounds the best idea, however if there is a LOT of data going in and out you will have very high contention on the most recent pages in your table, an alternative would be to have the clustered index around a category that those dates are on, eg client, this would mean that the data is grouped by a client which is more likely to have data gathered by, and spreads the read write load across the disk / disks If the data retrieve is very random then a clustered index is quite pointless, if the data you get back has no real order to it then a Heap is completely acceptable. Ultimately there is no be all answer to should I add a clustered index or a primary key because every situation is slightly different and will react in different ways. If you have a pre-live environment (even a cut down version) can help make your decisions. Personally we have tables with primary clustered composite keys, and some tables that are simply heaps. Hope this helps (and makes sense, sometimes I find I ramble) 

I solved adding a new column only of timestamp, I order the SQL based on timestamp and the results are from 1 Jan to 31 Dec. 

Community wiki answer: Best guess: The plan chosen to update stats is either parallel, or more parallel, on the 2014 box than on the 2008 R2 box. Parallel update stats for has been around since 2005, and for sampled statistics starting with 2016, see Query Optimizer Additions in SQL Server 2016 by Gjorgji Gjeorgjievski on the SQL Server Database Engine Blog. If you have Enterprise Edition, you could use Resource Governor to limit the CPU being used by your maintenance job. Consider also voting for the Connect suggestion Add parameter to Update Stats by Javier Villegas. Related Q & A: Parallel Statistics Update 

Community wiki answer: Error 665 points to a file system limitation as per Microsoft Customer Service and Support (CSS) articles: 

In my particular scenario I'm simply looking to pull out the filegroup allocations of a given partition scheme, regardless of what's been set against the partition. All I needed really was: 

Community wiki answer: The functionality in question should be managed via application program code (it entails computational capabilities), perhaps OOP elements. If, on the other hand, the informational requirements demand handling certain things (let's say, ) that have some properties in common, then modeling them with a conceptual supertype-subtype structure might be called for. If that's the case, you may find the accepted answer to How to model an entity type that can have different sets of attributes? of interest to deal with the structural aspect of the database.