To get location based tweets, you have to specify a location circle with center (lat and long) and radius using reverse_geocode. There is no way to find tweets by setting a polygon or drawing a border. 

Generally, to perform machine learning all data needs to be in a single dataframe. Team name, better yet team id, should be the primary key. Then modeling each year requires selecting the relevant columns (e.g., wins_year_1, loses_year_1, …). Finally, the general model would select all the columns. It is best practice to transform all data before modeling. This avoids making transformation errors on some of the data which can lead to errors in modeling. Pandas support this type of combining through the merge command. Typically, this data is not called sequential data. Sequential data implies a notion of state. The state changes values over time. In most sports, recent performance is not a strong indicator of how likely a team will the next game. This is often referred to as the Hot Hand Fallacy. Thus, current "win/lose" state of a team is not a strong feature in modeling. 

It also depends on what kind of what kind of search is needed. A working definition of "close" vector is needed. The most common definitions of "close" vectors can be found here. Fais is a new library for efficient similarity search of vectors. It is designed for many vectors (> 1,000,000) each being relatively small (10s to 100s of dimensions). It may or may not scale to your problem. If it does not scale because of high dimensionality, you can reduce dimensionality with Principal component analysis (PCA) or t-SNE. 

One option is to strip the output from the .ipynb file. Then the git diff would only track the cell data. One package that strips the output Jupyter Notebook is nbstripout. 

Load all vectors into memory. If you are able to load vectors into memory, then you might be able to search the space with "clever" brute force. One such method is found in this paper. Keep vectors on disk. If you follow this path, then you have to index the vectors. You are basically building a search engine. Common open source search engines are: Apache Solr and Elasticsearch 

In general, this is called feature engineering. Expansion of dataset attributes via attribute transformations is called basis function expansion. These are often nonlinear functions of the original attributes. Popular nonlinear functions are polynomials. 

You could be describing a variation of named-entity recognition (NER). You have labels/ categories for tokens. Given a corpus (resumes), you want a NER tagger to classify tokens as belonging to one of the labels or not. You need to create a training set of ground-truth / "gold" labels of tokens and labels. Since you are only dealing with nouns, you can run a standard Part-Of-Speech (POS) Tagger then only custom tag the noun phrases. It best to take an active learning approach. Active learning makes tagging the training set part of the entire machine learning pipeline, thus greatly reducing the number of annotations. "Deep Active learning for named entity recognition" is the current state-of-the-art. Once you have a set of labels you can train NER classifier. The common options are Stanford Named Entity Recognizer (NER) and spaCy NER. A detailed example for Stanford Core NLP can be found here. 

Applying common categorical labels to words is typically called Named-entity recognition (NER). NER can be done by static rules (e.g., regular expressions) or learned rules (e.g., decision trees). These rules are often brittle and do not generalize. Conditional Random Fields (CRF) are often a better solution because they are able to model the latent states of languages. Current state-of-the-art performance in NER is done with a combination of Deep Learning models. The Stanford Named Entity Recognizer and spaCy are packages to perform NER. 

BLEU (bilingual evaluation understudy) is an algorithm for evaluating the quality of text which has been machine-translated (MT) from one natural language to another. BLUE is typically measured on a 0 to 1 scale, with 1 as the hypothetical “perfect” translation. Google uses the same definition of BLUE but is multiplying the typical score by 100. 

One method is using locality-sensitive hashing (LSH), an approximate nearest neighbor search method. Each document, both resumes and job descriptions, is hashed into the same space. There are several ways to perform the hashing. An older method is shingling. The entire process is outlined in Chapter 3: Finding Similar Items in Mining of Massive Datasets. A newer method is doc2vec. Once all the documents are in the same space, LSH maps similar items to the same “buckets” with high probability. It is a type of hashing where collisions are features, not bugs. The collisions are a variation of clustering. Given a new document, all similar documents are retrieved also. Given a resume, retrieve similar job descriptions. Given a job description, retrieve similar resumes. (After retrieval, filter out documents in the same document class) Since LSH is an approximate algorithm, it scales well to millions of documents and handles noisy data. Resumes and job descriptions are noisy descriptions so it is appropriate to have an algorithm that can handle noise. 

GPS coordinates can be directly converted to a geohash. Geohash divides the Earth into "buckets" of different size based on the number of digits (short Geohash codes create big areas and longer codes for smaller areas). A geohash is a single number that can be used as a feature in a model. Geohash applies only to the entire world, zipcodes do not. 

You are describing one-hot encoding. There is a slot for each element. If the element is present, the slot has a one, and if the element if not present, the slot has a zero. Typically, people will encode orthogonal features in different dimensions. In your case, cities would be one dimension and transportation type would be another dimension. A given data point would be one-hot encoded in a matrix (a 2D collection of vectors). If you want to add people, you would another dimension. That would create a 3D tensor with each person in a row. Another way to compress your data is to encode not the cities (nodes) but the path between the cities (edges). From that encoding, you can create a Laplacian matrix that sets up spectral clustering. Since you have multiple transportation methods for the routes, you can create clusters with multi-dimensional spectral clustering.