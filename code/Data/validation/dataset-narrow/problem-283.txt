SSIS is going to be your best bet in this case I believe. If you already license SQL Server, it should be included as well. An SSIS package has the ability to access and utilize multiple ODBC or OLEDB sources and destinations including MySQL. It can be run using a SQL Agent job or just using DTEXEC from a Windows scheduled job. Also you can easily build logging, error checking and validation. 

As long as it is run as a single transaction, the value will be re-used on each line, not recalculated. I'm not 100% sure about Postgres or SQL Lite but I'm fairly confident (98%) they will work the same. In the past when I was more confused about this question, I would pre-fetch the date value to a variable and then reuse the variable in the where clause. Now I realize that was pointless unless I have more than one place in a longer set of statements where I would like to use the value. As a test, move the calculated date value to the select clause and execute against a large table that takes more than a few seconds to return. You will notice that the first line will have the same date/time value as the last record. 

I think given the requirements and that at some point you may want additional information that wasn't listed here, I would go with a fully normalized approach. 3 Tables: Guests, Episodes, Episode_Guests Then depending on if you want to do this for more than one show, another table for Shows(or series). As Paparazzi mentioned, the Guests table should contain sex. The Episode table should contain a date. Also, if you are going to do this for multiple shows, the Episode table should also have a foreign key back to the Shows table. The Episode_Guests table should record every instance of a Guest appearing on an Episode so all it would need is a foreign key relationship back to guests and another for Episode. 

MySQL calls these section "Groups". The [mysqld] group contains the variables apply to the MySQL Server (the mysqld process). The [mysql] group contains the variables for the client program (mysql). The [client] option group is read by all client programs (but not by mysqld), so "mysql", "mysqldump" etc... None of these groups are mandatory but usually we set at least the [mysqld] because it's where we configure the server. About how MySQL interprets these variables, MySQL Documentation says: 

will replicate statements on ads database (your context should be ads, ) and ignore the statements on other databases: The context is : REPLICATED 

Unfortunately MySQL doesn't support "Functioned Based" index (but you will have noticed that). It is therefore not recommended to use this trick, Peter Zeitsev wrote in a blog post on the ORDER BY thematic : 

It will not answer your question directly, but I wanted to notice you about this ibdata issue in general. Unfortunately, there is no magic solution to decrease the ibdata file size... It is the central point of the InnoDB engine, removing it (even when database is shutdown properly) will cause data inconsistency/corruption. The only way to restart from a healthy state (with a small ibdata) is to reinstall a brand new MySQL instance and load the data from your origin, and PLEASE use innodb_file_per_table to avoid having all you data in one single file (MySQL doc here). To load you current data into your new instance, you can use mysqldump if your dataset is small (few GB) or use "Transporting tablespace" procedures to cleanly import table by table your dataset (example on Percona Blog here). You can also read this article form Percona Blog which explain many things regarding these ibdata issues. Max. 

PMON registers to addresses defined in and parameters. If you have an invalid address defined there, then it will not be able to register the instance in the listener. If you have nothing defined there, then it tries to register on the local machine using the default 1521 port. You could also try to set your as: 

You need to create an external job for that. Techniques for External Jobs First, you need to create a credential, with the OS user and password in whose name the job will run: 

You can delete databases with DBCA which takes care of most of it. Or you can do as below, but this will do the same as removing the datafiles, redo logs, controlfiles manually. 

There are other files that are not critical, but they need space, for example standby redo logs and the block change tracking file: 

TABLE ACCESS FULL and INDEX FAST FULL SCAN use multi-block I/O, other kind of access paths use single-block I/O. 

As you can see, the final form ("Final query") of 2 queries are the same, their execution plans are also the same. The queries were running for 2 minutes because statistics_level was set to ALL in order to gather plan statistics. So knowing that the distinct can be eliminated from the query, look at the text of this query: 

Oracle SQL Reserved Words If you really want to use a reserved word, you can use it between double quotes, but that is where life becomes complicated. 

Performing online redefinition creates materialized view log on the source table, that is where DML changes are tracked during the redefinition process. Finishing the online redefinition gracefully (finish_redef_table or abort_redef_table) removes the materialized view log automatically, otherwise it will stay there. Truncating the table with an existing materialized view works with the following syntax: 

You can also generate this type of script with OS tools (awk or notepad++) You can run this type of script safely on a production master, you DELETE rows from the PK and you limit with LIMIT 1 so you do not risk to have replication lags. Best Regards. Max. 

I dont know about feed_id but your timestamp column by definition have probably many differents values. I will advise you to try to create a simple index on timestamp column. Do you have "disparate" values too on feed_id column? For me you should take your attention on the timestamp column (WHERE + ORDER BY). Maybe it'll not a index problem, maybe your MySQL instance have some difficulties to sort the result (bad temp table management) so you can try without ORDER BY clause to see its impact. Let me know. Max. 

3. ALTER TABLE (ONLINE) The problem of and that it locks table during operation. You can use the Percona tool : pt-online-schema-change (from Percona Toolkit : link ). pt-online-schema... will construct a mecanism with triggers and temp table that you permit original table to be available for read and writes during the operation. I use this tool in production for the big it's pretty cool. Note that you should have referenced to your table, FK and triggers risk to produce a mess. To check this prereqs, query: 

I usually use a MySQL replication. I have a Slave dedicated to the tests & validations of the "SQL Script Releases" (scripts made by the dev teams). For DML (UPDATE, INSERT, DELETE), I run thoses scripts into a transaction and rollbacked it after (InnoDB tables only): 

--replicate-ignore-db applies only to the default database (determined by the USE statement). Because the database was specified explicitly in the statement, the statement has not been replicated. My advice is to change the context with a instead of explicitly specify the database in the statement. Max EDIT (for questions) 1. 

Without merging the view, the cost is 126. So Oracle could have chosen not to transform the query, use the plan with cost 126, but instead of it, it merged the view without examining the cost, and chosen a plan with a cost 10 times higher = 1242. The above quoted sentence from the documentation explains this behavior. If you check the execution plan of the original query, you can see the lack of view merging - VIEW operation in the execution plan and "from$_subquery$_005" in the predicates part: 

No GROUP BY, no COUNT. But we have DISTINCT, ROW_NUMBER(), PARTITION BY, ORDER BY. Is it simpler/better than? 

The technologies that use the wallet are excluded, so the answer is no. Oracle Express Edition does not include the wallet management tools (mkstore, orapki, owm) at all. 

You can make the procedure have its on transaction - called an autonomous transaction, but be careful with this one, you can easily get yourself into deadlocks if you use it carelessly without knowing your transactions. 

By default, an Oracle database does not detect dead clients, you need to enable it explicitly by setting in . SQLNET.EXPIRE_TIME Since the database does not detect dead clients, it maintains its session and locks, so the answer is yes, killed application processes can cause blocking. This parameter is often misinterpreted. Setting this parameter to 10 does not mean dead connections will disappear at most in 10 minutes. This means the database server sends a probe packet every 10 minutes. The time required to detect a dead client depends on the operating system as well. For example on Linux, an already established TCP connection is valuable, so even if there is no answer from the other side, the server keeps trying to reach it. When the database server sends the probe packet and receives no answer, it will try again, 15 times in total (because of the kernel parameter), and the total time needed to detect a dead client can be way more than 10 minutes.