So changes are made to blocks kept in the Database Buffer Cache (DBCache). Once commited, the changes are pushed to the Redo Log Buffer (RLB) which is dumped on a regular basis to the Redo Log Files (RLF) and, eventually to the database storage files (DF). Also on a regular basis and not completely unrelated to commit, the checkpoint process dumps the dirty blocks from the DBCache to permanent storage DF. During the checkpoint process the SCN associated with the latest DB block written to storage is written on the DF headers and the control file. That will be from that moment on the latest consistent estate of the database. 

db_create_file_dest is set and points to ASM (i.e: +DATA): The database will take 'DATA' as an alias and will create ASM file on the destination associated with the 'DATA' alias that will be located at the ASM +DATA root directory. db_create_file_dest is set and points to filesystem: It should be quite obvious that you'll have a new 'DATA' file in the location pointed at the parameter. db_create_file_dest is not set (worst scenario): The database will default to $ORACLE_HOME/dbs and create a datafile there with the name 'DATA'. 

Question: Is overwriting the PFILE with the database ON dangerous? Answer: No. The PFILE is read only when the database starts, and that's if no SPFILE is there or if the PFILE argument is passed when invoking the startup command. Otherwise, when you start the database, the SPFILE is read for initial memory values, control files location and such basic information. After that, everything will remain in memory. Note: If you change the PFILE at the OS level, you need to bounce the database to make the changes take effect. On the other hand, using the ALTER SYSTEM SET ... SCOPE=MEMORY may be used as @ibre5041 suggested and achieve the result you want. Nevertheless, you're "losing" those archives from the original location, which makes me think you could as well add the NOLOGGING clause and avoid the generation of the logs that are filling up your storage. So my personal suggestion would be to use the command 

When I run mysqldump --routines I'm seeing multiple collations for my stored procs. Neither my DB nor columns use utf8mb (I understand the benefits of utf8mb over utf8 but let's leave it for now). 

The way I resolved it is I clicked on Modify in AWS/RDS console and shortened the DB Cluster Identifier so the resulting cluster endpoint fit in 60 characters. 

I have a very simple "inventory management" update that under load causes InnoDB to deadlock and rollback one of the threads. There doesn't appear to be any ordering issue here, so I'm at a loss how to resolve it. MySQL version is 5.6.33-0 

From what I'm reading, the interaction between a shared (S) lock and an exclusive (X) lock causes my concurrent updates to deadlock. Is there anything I can do to avoid this? Update: I believe this is a bug in MySQL. Two concurrent threads acquire Shared-locks and try to acquire eXclusive locks to do the update. They wait for the other to release the Shared-lock first. There is no ordering issue in the way locks are acquired; the mixing of types of locks is the issue. MySQL should be able to "upgrade" the existing shared-lock to eXclusive. If another thread wants to do the same, the first thread should make progress by obtaining eXclusive lock, do the update, then release the lock so the other Shared-lock can be upgraded, and so on. Workaround My UPDATE statement was modified to use the PRIMARY KEY (from previous UNIQUE KEY). In addition, I'm first calling a SELECT matching the condition of the UPDATE and skip the UPDATE entirely if the SELECT comes back empty, thus avoiding the eXclusive lock whenever possible. All of this is in a stored proc so clients aren't affected. 

Just completing the answer from @kubanczyk, which is very good. When you issue the recover database command, the RDBMS will try a complete recovery, meaning that it will look for the archived logs starting at the SCN (thread and sequence) from the restored backup and apply each archived log until it reaches the "active" redo log of the original database. That's why you're getting that error. So your best bet as @kubanczyk posted is to use recover database until cancel which is an incomplete recovery until the most recent (contiguous) archived log found. That should do the trick. Hope this clarifies the restore/recover process a bit. As for the other part of the question 

If there is no backup and no way to restoring the lost datafiles, what you can do is backup any other important datafile/tablespace and recreate the database. I think it will be the less painful way to get a fully working database. 

In Oracle every result set have an implicit row number that you can use for limiting output. If, by any chance, you're using oracle 12c you can use the brand new feature for top n queries 

The SCN (System Change Number) is kept in the control file and the headers of each datafile, uit allows the database to know which datafiles are in sync and where the database writer DBWR has to perform the next writes from the database buffer cache. Each backup also is "tagged" with a SCN (and thread-sequence) to allow the RMAN process know the exact "time" those were taken. Hope this helps. 

The last 48 hours or so I spend on trying to determine the cause for sporadic crashes of my MySQL (Aurora) DB. Turns out they were attributed to my mixing of database, table, column and stored procedure collation/charsets. My bad, or is it? Aurora aside, what were the practical reasons for all the ways to configure charset/collation in MySQL? Why couldn't utf8 just become utf8mb4 as part of a major release? Why changing charset or collation of a database/table doesn't actually change the database or table, only future additions to the schema? The last one provides for an opportunity to have a mix of columns charset/collations in your tables, for instance. What sane person would want to have different charsets/collations within the same DB? And if there is a rationale for all this madness, why not make it the exception rather than the norm when configuring mysqld? In my case, to resolve mysqld crashes I had to ensure all of below return consistent charset/collation values: 

I'm trying to configure external slave connecting to Aurora RDS. To my delight, I run into a silly problem where the Aurora cluster name - assigned by AWS - exceeds permissible MASTER_HOST limit of 60 characters. I want to use the cluster name (not IP) since it can be dynamically changed by Route53 on failover. Can anyone suggest a solution? 

you need to set the search string in ASM. Failing to do so, will result in the gv$asm_diskgroup view to be empty. Tomorrow morning with a clearer head and a desktop to give you exact commands I will edit and extend this answer. 

That will remove the fragmentation and shrink the associated datafiles plus it will set the HWM in the "lowest" possible position. CAUTION: This is a n I/O intensive operation, never do it during business hours or outside a maintenance window for production environments. Another option that can be used is the creation of a "backup as copy" of the fragmented datafiles and then switch the database to point those "image copies". Then drop the original and redo using the image copy as the original and backing them up "as copy" to the original location. It's a little longer and some more complex, but the "downtime" is just a few seconds while the switch is performed. Refer to the Oracle Database 11.2 RMAN Reference Manual for more details on this option. Hope this helps you. If not, please add more detail to the question. Like version of the software, maintenance window time, accepted downtime, etc. 

Prior to resizing, you have to remove fragmentation in the datafile. That means every segment (sets of blocks assigned to each object in that tablespace) needs to bring together all rows inside their db blocks. For this you have several options: If you're on 11gR2 you can use 

I'm aware of the limitation "every unique key on the table must use every column in the table's partitioning expression". So given that my primary key is a UUID, it's going to require a change. Background: I have a transaction history table that grows at about 1.5G per month (more so than I can keep upgrading server memory). The table has a PRIMARY key (uuid_short) and a datetime column, among others. There are two types of queries hitting the table: those that return a list of transactions in a date-range, and those that refer to a specific transaction through its primary key. Question: Since vast majority of queries don't care for transactions more than 18 months old I thought datetime could be a candidate for range partitioning. Does this mean I'm better off dropping the PRIMARY key and converting it to a non-unique key? (dups are not a problem, the app servers guarantee there cannot be any). The system is currently a hybrid of OLTP and OLAP scenarios which is probably why it doesn't scale. Is partitioning going to make a positive impact on memory footprint? If not, then I may be better off reorganizing the system by moving historical transactions to a data warehouse like AWS Redshift. It would be quite a refactoring effort, but if that's the preferred solution then I'd like to plan early. This is MySQL 5.6. 

To complete some of the previous answers: A commit will end the current transaction in the current section. It will guarantee the consistency of the data that was "touched" during the transaction. A checkpoint writes all commited changes to disk up to some SCN that willl be kept in the control file and datafile headers. It guarantees the consistency of the database. 

Actually they don't do the same. TDE encrypts the data in the database, you need to configure the wallet and have it open before you can see/use the data after that. VPD is an Access Control Mechanism, it allows you to define sub-datasets that will be owned by different users, allowing them to see/use only the data that they actually own (even if the same set of tables are shared among different VPDs) To have more detailed information about each tool, please review the documentation on $URL$ 

EDIT 1 As some of the comments point out that the concepts aren't completely clear I will provide some more information here. We have 3 major structures involved in the commit and checkpoint concepts. 

Due to the bad naming convention on your question, I will use ColumnA as A, ColumnB as B and ColumnDate as Date. I am assuming that you require the first item as it's fetched from storage, no particular order. And you have to know that if the row gets updated it can be migrated from the current block and so, the query result will eventually change. This queries are examples only and you have to edit them to get exactly what you want. You may try the following query: