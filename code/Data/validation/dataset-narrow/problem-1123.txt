If we pick a restrictive enough model (say our model can only implement the identity), then the complexity of a string becomes decidable, although we also lose the invariance theorem. Is it possible to have a model strong enough to have complexity equal (upto a constant offset, or even a multiplicative factor) to the Turing-complete model, but weak enough to still allow the complexity of a string to be decidable? Is there a standard name for Kolmogorov complexity with non-Turing complete models of computation? Where could I read more about this? 

The property testing barrier: if all 0-instances are $\epsilon$-far from all 1-instances then the adversary method cannot prove a lower bound better than $\Omega(1/\epsilon)$. The certificate complexity barrier: if $C_b(f)$ is the certificate complexity of $b$-instances then the adversary method cannot prove a lower bound better than $\sqrt{C_0(f)C_1(f)}$ where 

With these assumptions, the Google Doodle Machine is Turing Complete. I will show this by showing how to build the state system for any Turing machine (with a slightly non-standard definition). In each state the TM reads the input ($0$, $1$, or $\epsilon$), moves (left, right, or none) and then writes $0$ or $1$. The $n$th state is the designated halt state. The required Google Doodle Machine (GDM) will need $3(n - 1) + 1$ lines each of length $5n + 1$. 

In other words, you will find a morphism $h: X \rightarrow Z$ such that $gh = f$. For more info, see these slides. 

The general rule of thumb is if you have cloning (and unbounded computation!), then most things get easy. In the case of the qubit-channel communication complexity, Alice can just send her initial state with one qubit. Interpret Alice's string $x$ as an integer, between $1$ and $2^n$. Given a string $x$, Alice prepares the state $|\phi_x\rangle = \sqrt{1 - x/2^n}|0\rangle + \sqrt{x/2^n}|1\rangle$. She sends state $|\phi_x\rangle$ to Bob, Bob makes an exponential number of copies of $|\phi_x\rangle$ and measures each individually in different bases to determine $x$. With $x$ in his hands, Bob can compute any function $f(x,y)$. 

What does a Martin-Löf random sequence look like? Well, take a perfectly balanced coin and start flipping it. At each flip, write a 0 for heads and a 1 for tails. Continue until the end of time. That's what a Martin-Löf sequence looks like :-) Now back to the initial question: is there a computable way to generate a Martin-Löf random sequence? Intuitively the answer should be NO, because if we can use a computable process to generate a sequence $\alpha$, then we can certainly use a computable process to describe the singleton {$\alpha$}, so $\alpha$ is not random. Formally this is done as follows. Suppose a sequence $\alpha$ is computable. Consider the following Martin-Löf test: for all $k$, just output the prefix $a_k$ of $\alpha$ of length $k$, and nothing else. This has measure at most (in fact, exactly) $2^{-k}$, and the intersection of the sets $U_k$ as in the definition is exactly {${\alpha}$}. QED!! In fact a Martin-Löf random sequence $\alpha$ is incomputable in a much stronger sense: if some oracle computation with oracle $\beta$ (which itself is an infinite binary sequence) can compute $\alpha$, then for all $n$, $n-O(1)$ bits of $\beta$ are needed to compute the first $n$ bits of $\alpha$ (this is in fact a characterization of Martin-Löf randomness, which unfortunately is rarely stated as is in the literature). 

I am joining the discussion fairly late, but I will try to address several questions that were asked earlier. First, as observed by Aaron Sterling, it is important to first decide what we mean by "truly random" numbers, and especially if we are looking at things from a computational complexity or computability perspective. Let me argue however that in complexity theory, people are mainly interested in pseudo-randomness, and pseudo-random generators, i.e. functions from strings to strings such that the distribution of the output sequences cannot be told apart from the uniform distribution by some efficient process (where several meanings of efficient can be considered, e.g. polytime computable, polynomial-size circuits etc). It is a beautiful and very active research area, but I think most people would agree that the objects it studies are not truly random, it is enough that they just look random (hence the term "pseudo"). In computability theory, a concensus has emerged to what should be a good notion of "true randomness", and it is indeed the notion of Martin-Löf randomness which prevailed (other ones have been proposed and are interesting to study but do not bare all the nice properties Martin-Löf randomness has). To simplify matters, we will consider randomness for infinite binary sequences (other objects such as functions from strings to strings can easily be encoded by such sequence). An infinite binary sequence $\alpha$ is Martin-Löf random if no computable process (even if we allow this process to be computable in triple exponential time or higher) can detect a randomness flaw. (1) What do we mean by "randomness flaw"? That part is easy: it is a set of measure 0, i.e. a property that almost all sequences do not have (here we talk about Lebesgue measure i.e. the measure where each bit has a $1/2$ probability to be $0$ independently of all the other bits). An example of such a flaw is "having asymptotically 1/3 of zeroes and 2/3 of ones", which violates the law of large numbers. Another example is "for every n, the first 2n bits of $\alpha$ are perfectly distributed (as many zeroes as ones)". In this case the law of large numbers is satified, but not the central limit theorem. Etc etc. (2) How can a computable process test that a sequence does not belong to a particular set of measure 0? In other words, what sets of measure 0 can be computably described? This is precisely what Martin-Löf tests are about. A Martin-Löf test is a computable procedure which, given an input k, computably (i.e., via a Turing machine with input $k$) generates a sequence of strings $w_{k,0}$, $w_{k,1}$, ... such that the set $U_k$ of infinite sequences starting by one of those $w_{k,i}$ has measure at most $2^{-k}$ (if you like topology, notice that this is an open set in the product topology for the set of infinite binary sequences). Then the set $G=\bigcap_k U_k$ has measure $0$ and is referred to as Martin-Löf nullset. We can now define Martin-Löf randomness by saying that an infinite binary sequence $\alpha$ is Martin-Löf random if it does not belong to any Martin-Löf nullset. This definition might seem technical but it is widely accepted as being the right one for several reasons: 

Motivation Classical random walks are useful in algorithm design, and quantum random walks have proven useful for making a number of cool quantum algorithm (sometimes with provable exponential speed-ups). Thus, it is important to understand the difference between quantum and classical random walks. Sometimes, the easiest way to do this is to consider toy models, such as walks on the line. There is a physics motivation as well: it is interesting to know how quantum mechanics scales to classical mechanics. But this is not very relevant to cstheory. My personal motivation is completely orthogonal: I am trying to match some experimental data with a model that transitions smoothly from quantum to classical and is relatively intuitive. Background When considering quantum and classical walks on the integer line, a key difference is that the standard deviation (of the position distribution) of the quantum walk goes as $\Theta(t)$ and classical ones as $\Theta({t^{1/2}})$ where $t$ is the number of steps for a discrete model, or time in a continuous model. Note that this is not restricted to the line, and for many graphs you will see a similar quadratic relationship between the quantum and classical mixing time, I consider the restricted case of the line since I think it is easier to analyze. As we introduce decoherence to a quantum walk (either through measurement or noise) the walk starts to behave more classically. In fact, for most measurements, we just end up with a classical walk that spreads as $\Theta(t^{1/2})$ if viewed from the right timescale. For other forms of decoherence (such as dephasing the coin, or introducing imperfections in the line) there is usually a sharp threshold below which the walk behaves quantumly (spread as $\Theta(t)$) and above which the walk starts to be classical (spread as $\Theta(t^{1/2})$). In fact, this scaling has even been suggested as the definition of a quantum walk. 

Ok, now the "edit" part of Joseph's question: Is it the case that a TM with access to a pure source of randomness (an oracle?), can compute a function that a classical TM cannot? From a computability perspective, the answer is "yes and no". If you are given access to a random source as an oracle (where the output is presented as an infinite binary sequence), with probability 1 you will get a Martin-Löf random oracle, and as we saw earlier Martin-Löf random implies non-computable, so it suffices to output the oracle itself! Or if you want a function $f: \mathbb{N} \rightarrow \mathbb{N}$, you can consider the function $f$ which for all $n$ tells you how many zeroes there are among the first $n$ bits of your oracle. If the oracle is Martin-Löf random, this function will be non-computable. But of course you might argue that this is cheating: indeed, for a different oracle we might get a different function, so there is a non-reproducibility problem. Hence another way to understand your question is the following: is there a function $f$ which is non-computable, but which can be "computed with positive probability", in the sense that there is an Turing machine with access to a random oracle which, with positive probability (over the oracle), computes $f$. The answer is no, due to a theorem of Sacks whose proof is quite simple. Actually it has mainly been answered by Robin Kothari: if the probability for the TM to be correct is greater than 1/2, then one can look for all $n$ at all the possible oracle computations with input $n$ and find the output which gets the "majority vote", i.e. which is produced by a set of oracles of measure more than 1/2 (this can be done effectively). The argument even extend to smaller probabilities: suppose the TM outputs $f$ with probability $\epsilon >0$. By Lebesgue's density theorem, there exists a finite string $\sigma$ such that if we fix the first bits of the oracle to be exactly $\sigma$, and then get the other bits at random, then we compute $f$ with probability at least 0.99. By taking such a $\sigma$, we can apply the above argument again. 

My answer addresses the general theory of disease (and similar) modelling and only briefly touches on the implementations. For simulations (as opposed to analytic work, which usually uses evolutionary game theory or SIR models) the popular paradigm is agent-based modeling (ABM). A good recent book on agent-based modelling from a CS perspective is: 

If you want more exact analytic results on the BA model, than you can't rely on the original heuristic arguments given in the Science paper, and I recommend looking at these lecture notes for a formal treatement. To prove things about the BA model, usually self-loops and multiple edges are allowed. In that case the network can be initialized as one vertex with $m$ self-loops. 

As an example, Shelby Kimmel uses the adversary method in this paper to show that there has to exist $O(1)$ query algorithm for a certain problem for which we do not know a constant query solution. She does this in a particularly slick way by finding the query complexity of the problem composed with itself $d$ times and then finding the query complexity $Q$ of the composted function, and noting that the query complexity of the original function is order $Q^\frac{1}{d}$. 

This is not exactly an answer of an efficient data structure, but rather an elaboration on the comments of @bbejot and @Kaveh giving a hand-waving argument for why given the current question we should not expect something that does a lot better than searching the whole database. The argument is based on a reduction from SAT, the exponential time hypothesis, and a lot of hand-waving. Assume we have upto $n$ different tags, then we can think of each id as being associated with a bitvector $x$ of length $|x| = n$ where $x_j = 1$ if we have the $j$-th tag, and $x_j = 0$ otherwise. Since there is no real restrictions on what the database looks like, I can assume it has ids $1$ through $2^n$ with the $k$-th id having an associated tag vector of $k$ written in binary. The order fields can be arbitrary, since they only make the problem harder. Now, if we are given an arbitrary query of $AND$, $OR$, and $NOT$s then this is just asking a SAT question on $n$ variables. By the exponential time hypothesis, we can't expect this to be much faster than $2^n$... or in other words, we can't expect this to be much faster than searching the whole database. We shouldn't expect efficient search in the length of the query (by reduction to SAT). We also shouldn't expect much better than looking at all the items in the database by the exponential time hypothesis. To hope for an efficient data-structure for this question, we will have to make some serious assumptions on the structure of the database that are not made in this question. For instance, if we assume a special structure on the queries (such as CNF) then we can hope for something more efficient. An alternative assumption is on the structure of the database. We could probably assume that given $n$ tags, only a small fraction of the tags will be present on any given id (say a logarithmicly few $1$s). This is not an unreasonable assumption given the application of tagged questions (what use are tags if almost every single tag is used for a question).