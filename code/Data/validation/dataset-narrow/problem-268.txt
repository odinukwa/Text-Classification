Relational division is the proper way to do this. If you know what attributes your objects can have you can construct a view based on a query. Experience shows this to be dog slow for non-trivial row counts, however. Concatenating the attribute IDs and values, then hashing and storing an object / hash lookup has worked for me. You have to order consistently, of course, choose delimiters carefully and deal with hash collisions. 

The IDs used in From and To says in which direction the money flows. There is no need to have any information about a party in the Payments table, such as a type, other than the two PartyIDs. To get all the payments made to customers (for example) add to the query given above. The Payments_Track table you list has all the attributes I list here (bar a few surrogate IDs) but is normalised. 

Dollars and cents are just different units for the same amount. Whether a charge is ten dollars or one thousand cents, it's the same cost. In the same way the distance from your house to the beach is the same irrespective of being measured in miles, kilometres or stadia. Changing the dollar-number must necessarily change the cent-number. Dimensions have the property of orthogonality. A measure in one dimension can change without affecting the measure in the other dimensions. One can move East or West without altering how far North you are. The latitude and longitude are independent variables. So yes, if your problem domain has the corresponding number of independent variables, and they are of the same primative type that your GIS software supports (float?), then you can use that software's capabilities to process your problem. Be careful of the geometry the software implements, however. Walking in a straight line on a Euclidian plane produces a very different result to walking on a straight line on the surface of a sphere! 

Service Broker can provide this functionality. It uses a store-and-forward messaging model. Your local edition may not have this available, however. 

It will, however, show the tables that definitely are used. I'm afraid the only reliable solution is to examine each application's code. 

The foreign keys are defined on course_no and student_id. Assuming the error is a foreign key violation (big assumption, please post the full error in future questions) it must be one of those two at fault. Other rows reference course EE200 so it's not that; so student 018 doesn't exist in the student table. Similarly MA220 so student 021 is missing. With EE220 / 016 there are no comparators so either or both could be missing. Given your record I'd suggest the student. This is basic stuff and fundamental to the concept of foreign keys. You should read up on the concept and ensure you write data in a sequence that respects the constraints you have defined. Otherwise you are in for a world of pain. 

In July 2017 Microsoft announced Azure Stack, which makes their cloud management software available for on-premises deployment. Included in this is SQL databases. 

If your data needs to be persistent over a failure & restart then a "real" table in your database would be best. You need a mechanism to isolate one user's execution of this from another's and to tidy up nicely. If data is to be disposed of if the SP crashes and will be re-created on a restart then a table-valued variable would be adequate. These are not indexed, however. If the data will be re-read according to different predicates this may be a problem. Temporary tables e.g. will keep one user's data isolated from another users'. The tables will be tidied away at the end of the SP. Indexes and statistics are held for #Tables so these perform better for larger row counts or multiple access paths. 

What you're talking about is "stemming" where a word -- refill -- is related to its "root" or "stem" word -- fill. This is usually implemented through Full Text Search, which I believe SQLite supports, though I've never used it. To implement this yourself in your own tables and code will be a complex task requiring a thorough understanding of the target language(s), how word-forms are constructed and how the grammar is defined, both theoretically and in practice. 

This query will return all current bookings which overlap your proposed booking, or an empty set if none exist. 

The last ELSE clause will fall into the general-purpose SELECT that has so the added cost is incurred as infrequently as possible. You may have to monitor your application to find what combinations are actually submitted by users. Although this makes for long SPs it avoids the run-time overhead of constructing dynamic SQL and allows for better plan caching. Now you have to address parameter sniffing. Do this by having each of the specific queries in its own SP. These will each have their own plan in cache, and each will be smaller than that for a single SP holding all the queries. This way infrequently used search SPs' plans can be evicted from cache while frequently used ones are retained. Also the recompile cost of each is smaller should it be needed. Parameter sniffing issues are limited to one search SP at a time. 

With that many values you are likely to run into estimation errors during query plan generation. This will lead to it choosing worse (slower-performing) plans than it might otherwise. With that many values I would suggest INSERTing them into an actual table, with index(es), which is JOINed to in the query. Logically the two are the same. Practically, aesthetically and performance wise it is likely to be better, even with the additional writes. 

Assuming your DBMS is using BTree indexes, there is a chance that there may be a small speed-up in certain situations. To perform a lookup on a BTree the system starts at the root node, follows links through the intermediate nodes to the leaf node. The fewer columns there are in the index the more index keys will fit in a page and the fewer levels will be needed in the BTree. With fewer levels there will be fewer page reads for each lookup. There are a lot of caveats to this. For example, in most realistic scenarios the fanout in a BTree is such that for medium-large table the index becomes 3 levels deep and stays that way for a very long time, irrespective of the keys in the index. Also it is likely most of your index pages are in memory already so the additional read time is trivial. Against the slim possibility of increased read speed you must balance the absolute certainty of decreased write speed. Each row inserted must now be written to the the table, the {a} index and the {a,b} index. Most optimisers will consider indexes where the leading keys are matched in the query. Say you define and index on {a,b,c,d}. That would be considered for queries that have predicates on a, a and b, a and b and c, or all four columns. (I say "consider" and not "used". Choosing which index to actually use is a whole different discussion.) So for a wide class of queries you get some potential benefit without the addition write overhead. Here's more detail on that. How much storage will an index take? Order-of-magnitude I'd say the size of the columns times the number of rows. So if you have an integer (4bytes) and 1 million rows, the index will take about 4MB. It might be 3MB, it might be 10MB, depending on a lot of things. But it won't be 1KB and it won't be 1GB. As a rule of thumb I would avoid keys with duplicate leading indexes. If you identify a very specific use case, after careful testing, and having considered the load on the system as a whole, then you may think of adding some. Not as a matter of course, however. 

uses a merge join. Unsurprising, since both tables are ordered appropriately for the query. By deleting the primary key on one table, however, and converting it to a heap, the optimiser no longer has a guarantee of ordering and it uses a hash operator: 

Basically what Filip De Vos said but with triggers on the old table to keep the shadow copy in sync while data is transferred. In the past I've used a view to achieve fast switch-over and fail-back from old to new table. In preparation for go-live rename to ; create a view which simply . At go-live re-create this view as . If acceptance testing fails you can switch back to the previous view with little down time. The view can be dropped and the new data table renamed at your liesure, or never. (Sorry, not big on MySQL, not sure how it copes with writing through views). 

Page splitting can occur at all levels of the BTree, including the root level. Due to fan-out, however, the frequency at which intermediate pages split is (typically) an order of magnitude less than that at which leaves split. Also there are many fewer non-leaf pages than leaf pages. Consequently PAD_INDEX is less concerning than FILLFACTOR. Still worth thinking about, especially on randomly-inserted indexes. During writes, pages will be filled up to 100%. Then they will split 50/50, or as close as possible to ensure each row is contained entirely on a single page. I believe the split is always 50/50. Subsequent writes may be skewed, however, so one of the new pages fills much quicker than the other. 

Yes. Normalise your design. breaks first normal form because it holds many values. Create a new table which contains only and . And please, please stop prefixing your table names with 'tbl'. 

I'm assuming that a particular component will only be measured once even if it occurs in two tests. For example, if Test A consists of blood sugar and vitamin A, and Test B consists of blood sugar and iron level, and both Test A and Test B were ordered, then blood sugar would only be measured once from each sample. A way to check you model is to list the questions you want to ask of it. Can your model answer those questions fully and unambiguously? If not you need to talk to your users about how things work in the real world and add this new understanding to the model. Finally, don't forget normalisation. As you add more attributes into the model ensure you are adding them to the correct table. You may have to add new tables and revise the relationships between existing tables. Don't worry, this is normal in database design. I hope I've given you some good pointers on how to proceed. Concentrate on what people do and what they do it to. This will be a good start. Lab software is field in itself and can be complicated. There are a number of commercial packages in existance. 

If your tables have surrogate keys that use , however, you have more work to do. You have to capture these identity values as they are generated in the parent tables and use them in the mapping table. This can be done with the clause: