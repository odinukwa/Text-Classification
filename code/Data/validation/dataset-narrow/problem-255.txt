Have you tested or would it be possible to drop indexes on the destination DB table(s) where you are inserting into, insert those into smaller batched chunks (optimal as indicated above), and then rebuild the indexes on the destination table(s) once all inserts are complete? May be something easy enough to test to confirm. 

Just remove all three of the correlated table data files from the file system and that should ensure all traces are removed as per my understanding of 15.2 The MyISAM Storage Engine Each MyISAM table is stored on disk in three files: 

Memory related system DMVs would probably be a good starting point, but I placed a few examples below with direct links to the MSDN articles that give more detail and T-SQL examples, otherwise, you could and start filtering down from there based on your needs. Additionally, consider having a look at Glenn Barry's Scripts too when you get a chance. He provides some excellent queries specific to each version of SQL Server you need to run it with to help get stats, troubleshoot, and so forth. 

Below are two scripts I've used for this purpose quite a few times in the past; just adjust accordingly for your environment. Basically just plug in the new path locations, the data and log file names, and the logical SQL data and log file names, and then run the first script (see comment above second script below). First Script Plug in your variable accordingly for your environment. 

Afterwards, as timed out with some testing, what you already know about the timing, etc. the LSBackup, LSCopy, and LSRestore jobs should start working again. I don't remember exactly, but you may need to purge the LSBackup and LSCopy TRN files on both primary and secondary after step #1 above to ensure SQL doesn't try to apply the broken chain TRN files to the secondary DB. This can be done and I've done it before. It may not be "best practice" but if there's a business need, that should be enough justification if it works, gets the job done, and allows you to put some automation around it. 

According to Enable Encrypted Connections to the Database Engine (SQL Server Configuration Manager), you do this as such: 

Getting the SID Once a SQL login is created you can get the value of its SID by simply using and pass the SQL login name as an argument, and it will return the SID value for that principal. Example T-SQL 

So do a FULL backup on your primary DB Run the SHRINKFILE on the primary DB of the mirror without ALTERing to SIMPLE RECOVERY mode 

So this means there IS still SQL transaction logging that occurs when the database is in recovery mode but it's minimal logging when compared to recovery mode. 

Make sure you have a full or log backup on your primary DB first, and then do the shrink operation from the primary DB. It'll probably grow again due huge transaction so you may want to see if throwing in a few log backups may help reduce the growth before your full backups occur or else determine what's causing the logs to grow so much bigger than the data file -- typically it's due to poorly written queries or HUGE transactions or maybe not doing enough backups to free up space since your in FULL recovery mode. 

Since you control the logic that creates the dynamic SQL Agent jobs you should be able to test with additional logic that will delete the dynamic SQL Agent job and execute sp_delete_job if the date is equal to a particular date (e.g. day eight or day nine). It may be best to use logic that will execute sp_delete_job if the date is greater than or equal to day eight or nine in case it doesn't run on the exact date due to system issues, etc. with logic such as: 

I also wanted to share a db_DDLAdmin_Restriction role you may want to consider to consider creating otherwise with explicit to restrict what give access to so you could at least create this on the DBs where you grant them this role and set the explicit for the actual object types, etc. you don't want them to have access to. For example, if you know they will definitely be creating stored procedures and functions, you can exclude , , . 

Additional Resources (These articles seem to have some references to potential parameters in SSIS package areas to set for smaller transactions, etc. The titles or the steps may be for a different ultimate goal, but there is potentially applicable content in these for what you may need to change in your SSIS logic to rectify your issue so these may be worth a simple read.) 

2. Restart SQL Server (if you can restart the server OS meaning power cycle it) 3. Run the below once it's back up and confirm . . . TSQL 

Obviously your current process has an issue trying to insert duplicate data on a table with a constraint that doesn't allow such data to be duplicated where the value is per the defined Primary Key. 

A Limit to the number of MySQL Databases a User Account can Create At the MySQL level as per the Limits on Number of Databases and Tables there is no limit on the number of databases MySQL can contain at this level. If you give a user account the global CREATE permission to create new databases, then you give them just that and you cannot restrict the number of databases it can create at this level. Limiting the number of Database a MySQL User Account can Create To control the number of database you allow a user account to create you could just create the databases per an "approved" request and not grant them permission to CREATE databases themselves. You'd grant the user account Database Privileges at this level once created. Additionally, as some third party hosting services utilize, if you only allow access to manage MySQL instances and databases via an application, it is possible to have rules at this level keeping track of the number of databases an account creates, and enforcing rules to set such restrictions. 

Use the keyword with the statement to get the data to load with MySQL without needing to adjust the file parameters 

NOTE: It is worth noting that if you create the login on the secondary instance with everything matching, the security changes will still not be effective until the next round of Transaction Logs you get via SFTP which have these changes in them from primary are actually committed to the secondary DB so timing is something to consider and test before publishing externally for their usage. 

The above TSQL will grant explicit VIEW DEFINITION access to the specific DB object and to the specific security principal as specified in the applicable TSQL logic 

Ensure you're on the 'publication' DB so and then execute the SP. Otherwise, try various combinations such as the below for the variable you're passing to the SP as an argument to the SP in case it's related to it not liking the preceding which indicates that the subsequent string is in Unicode, thereby passing an , or value, as opposed to , or . 

I have to agree with Max's comments about running the stuff that would normally run when the issue occurs as a simple step to confirm whether or not those items/processes are causing the issue -- process of elimination should be simple enough. Since you pretty much have the day and timing down to a science when the issue occurs, you could schedule a SQL profiler trace to run via a SQL agent job and give it a stop time (@stoptime) to stop the trace to see what details the trace will provide. Since you asked about where else you could start to troubleshoot, I think a SQL profiler trace would give you a lot of detail to go by actually. I'll paste what I have on this when I did it on SQL Server 2008 R2 below. SETUP DETAILS Follow the below instructions for scheduling a trace with a specific start time and a specific end time. You'll go to 'SQL Server Profiler' and build your trace criteria as usual but be sure to save to a file somewhere valid on the server itself, check the 'enable file rollover' option, and specify a trace stop time. Once all the criteria is selected and filtered for what you need to capture, run it, stop it, and then navigate. . . FILE | EXPORT | SCRIPT TRACE DEFINITION | 'select' the SQL Server 2005 - 2008 R2 | and then save the file somewhere you can open later. Go to the file, open it and you should get something similar to the below. KEY POINTS 

The issue is either you're simply running out of disk space and the physical file cannot grow to complete the transaction, or you have a limit on the SQL file growth settings and the physical file cannot grow more to complete the transaction (whatever max space available means extactly e.g. per settings or disk capacity). 

In the SQL Agent job that you use for backing up your DBs with backup devices, just have each step move onto the next step whether they fail or succeed. However, have the very last step have the below logic in it and tell it to report failure on failure and to report success on success. This will email you the failure detail from the msdb tables for that job as you'd see in the SQL Agent job history area for the current date records. You could always check the SQL Agent history manually when you get the email notification as well but this works well for the need where I've set it up. I'm sure you could use the logic and make some changes to suite your need further too. In my case, each backup job to backup device is it's own separate step with each moving onto the next step regardless of the results, and the very last step is as I indicated above and is sufficient for failure monitoring. If I'm not in the office or by VPN connection, the emailed HTML table data will give me an idea if this needs attention and so on checking Outlook Web Mail. 

You can script this whole process out with TSQL and SQL Agent jobs depending on the your cleanup process -- I assume you can or already have it scripted but here are the basics how I've dealt with similar issues in environments I maintain and support. 

Press the Test Connection option from the bottom of the Manage Server Connections window after you make the to confirm the error goes away. 

Yes and with the argument it should prompt you for a password to authenticate to perform the correlated command operation. 

I'm not suggesting making a bunch of changes before trying to find and fix the specific problem beforehand but there are many factors to consider I would think and it's just something you have to troubleshoot one step at a time unless there's something obvious causing the issue. In my setup, I'm the domain admin as well as the DBA so I have access to everything and all servers (two domains with two-way trusts) to troubleshoot at all levels whereas you may be a bit more restricted with what you can review, etc. on each server in the loop of the issue. I usually start with the simple items first that don't require changes and see if there are any obvious issues (e.g. event viewers on all servers, DNS testing on all servers, review all server configurations, etc.). This could be an issue at the OS level of the SQL Server for the Windows version it is on. This could also be an issue with the NIC of the SQL Server or a mis-configuration of TCP/IP or DNS settings on the SQL Server or any of your DCs. Gather information on or ensure the SQL Server is fully updated with Windows Updates, and ensure server firmware is up-to-date if applicable with BIOS version as well as any hardware device firmware updates. The domain admins could also do the same on the DCs but use caution with making a bunch of changes and do so one step at a time. I'm not sure how your AD and DCs are configured topology wise, but best practices should be followed as best as possible and a DC should be close to the network where both domains are physically rather than going across a slower network pipe or routers, etc. which adds another level where there could be issues or configurations to review. The DCs should have their TCP/IP settings configured so that their DNS settings point to other DNS servers or follow best practices otherwise as per Microsoft for the DCs configurations applicable in your environment. You'll need to do basic troubleshooting to find the issues to resolve, ask questions, see what all could be updated and patched or changed, and get your domain admins to help you troubleshoot or disclose domain configurations to you if you're not sure to see if everything is configured as best it could be for optimum performance and per best practices. That's not to say there could be some very simple reason this is happening suddenly but if nothing has changed anywhere (i.e. OS on DCs or SQL Server, network configurations, AD or forest functional levels, hardware upgrades, Windows Updates, etc.) then you just have to troubleshoot for the reason why one step at a time. For your #2 question I think the OS has an issue talking with the DC (for whatever reason(s) you determine) and when this happens, it just gets out of sync somehow with DC, AD, etc. and a quick fix is to reboot the SQL Server OS when this occurs and there is no network communication with the DC at the time of the reboot, and all comes up fine with everything back in sync at that point. To accurately answer this, you probably need to find the reason first. 

Switch recovery model to simple on primary DB (TSQL with SQL Agent job step) -- this will break the log shipping chain so time out your LSCopy, LSBackup, and LSRestore jobs around this time when this occurs -- then run your cleanup process on primary DB Switch your primary DB back to full recovery model (TSQL with SQL Agent job step), grow your (or perhaps shrink) your primary log file back to the "usual" size (SQL Agent TSQL again) Run (or TSQL script with SQL Agent job) a FULL backup of the primary DB to the "usual" full backup location Restore the secondary DB with the FULL backup file as in #3 above (TSQL with SQL Agent job step)