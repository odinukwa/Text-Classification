Seems to me that according to the question, simply sorting by EndDate would do the trick... The OP does not specify that the results are to be returned in descending order. In fact, he does not state that the rows need to be ordered by StartDate at all. Since NULL dates will be ordered before dates with values, an fulfills the requirement. Otherwise, should be sufficient. Using @Julien's sample code, the final SELECT could look like this: 

We have a business-critical stored procedure that normally runs daily at 2am from a scheduled job (in an SSIS package) on the production DB server. The same procedure/package is called from a second job, 15 minutes later, from a SQL Agent job running on a different server (as an emergency failover in case anything goes awry with the first job). The procedure is defined WITH RECOMPILE. The procedure normally executes in about 45 seconds. Last Wednesday, and again this morning (also Wednesday, coincidence?!?), the 2am procedure took 90 minutes to execute. While it was executing, the 2:15am job ran and that execution took the usual 45 seconds. I have the execution plans from both situations. There are some table variable processes that should include estimated row counts in the neighborhood of 200K rows. The faulty plan reports these table variables with an estimated 130 billion rows. [Side note: I have already rewritten the code to use temp tables instead of table variables, based on this discussion and will be moving it to production in the near future] Our monitoring software (Solar Winds DPA) reports excessive CXPACKET waits for the 2am execution. This seems to indicate issues with parallelism and is likely related to the table variables being used in the procedure. There is still user activity on the server during this time, and some scheduled jobs, but nothing that I see that would affect this procedure or its execution plan. An index maintenance job is run at 2:30am. I understand the poorly-performing execution plan is related to the temp tables, but why would this same procedure executed 15 minutes later have such a drastically different execution plan (and why does the 2am execution run fine the rest of the week?) Here are links to the .sqlplan files: The Bad Plan and the Good Plan. 

Department - Number, Name Employee - Name, SSN, Address, Salary, Sex, DOB Location - ??? Project - Number, Name Dependent - First Name, Sex, DOB, Relation 

Normalization is the formal process for removing redundancy from relations by taking projections which when joined back form the original relational and thus eliminate some redundancy without data loss. It is the science underlying database design. The first three normal forms, and BCNF, deal specifically with eliminating redundancy due by ensuring that every non-trivial functional dependency is fully dependent only on candidate keys. Higher normal forms deal with other kinds of dependencies to further eliminate redundancies. Even when fully normalized (5NF is generally considered the "final" normal form although there are four others in the literature) redundancy can still remain as not all redundancies can be removed by taking projections. Another tool to address eliminating redundancy is the principle of orthogonal design which states that two distinct relvars cannot have in them a tuple with the property that if it appears in the first relvar it must also appear in the second and vice versa. But this principle only addresses redundancy across relvars whereas normalization addresses redundancy within them so it doesn't help with your example. Ultimately Date contends we just need more science to guide database design as that which we have today as you show isn't quite enough. One practical point to your example is that although there is redundancy, at least it can be controlled redundancy if a table is defined to hold the dancers, all key, with name and birth date. Then, name and birth date become a foreign key to the dances table, and that foreign key can be defined to cascade updates. Then, if a particular dancer's birthdate is found to be in error and corrected, the DBMS will automatically handle updating all the places in the dance table where that dancer was listed. Moving the control of the redundancy from the user to the system is a big step forward that you can get with today's SQL DBMS'. All of this information is paraphrased from Date's excellent book Database Design and Relational Theory which provides a significant amount of thinking and detail around just this issue. It is indeed the case that we stand on the shoulders of giants. 

This appears to be the result of a "bug" in Windows Server firewall. We disabled the firewall service, and after rebooting the production server (yuck) all appears to be working as expected. We have an open MS case for this, but I don't have great confidence that anything will come of it.... If we do come up with something conclusive I will post back here. 

BIG QUESTION: Why are these emails failing? Smaller question: What steps can I take to further troubleshoot this to ultimately answer the big question? 

In SSMS 2014, when I connect to a server in Object Explorer using my network credentials (I am a domain admin), I can do pretty much everything I would expect as a DBA. But I cannot start/stop/restart services. For instance, if I right-click SQL Server Agent, the options to Start, Stop, and Restart are greyed out. Using SSMS 2008R2, from the same client machine using the same credentials connecting to the same server, the options are available and functional when I right-click. This is true for all servers in our environment. What do I need to configure in order to be able to manipulate SQL services in SSMS 2014? 

I've been seeing some SMTP errors in my database mail logs. So I have a set up a test SQL Agent job that runs every ten minutes (it's only action is to SELECT NULL). It is set to send email notifications whenever the job completes. The addresses are internal email addresses. I have configured database mail to retry failed emails up to 4 times. In the last day, almost half the emails encounter an error at least once but eventually get delivered. 15 of the emails failed completely. A little more than half get sent without any error at all. Here's a list of each job execution since midnight, along with the number of errors encountered by each attempt to send mail: $URL$ (note: there are gaps in the mailitem_id sequence because we have other jobs that also send emails). The errors are a mix of the following two messages: 

What I do at this point is list out the entities and relationships identified and place the attributes identified with the entity type it belongs to: 

You have the right idea and have just gone a little wrong with the year column. Background The table given shows that + is the key to the table - meaning that each row must have a unique combination of the value of these two columns. This is indicated by the solid line under these two columns. A table is normalized when that table is a faithful representation of a relation. To approximate a relation, the table must: 

A foreign key can't be made conditional so that is out of the question. The business rule appears to be that an employee can work for one and only one physical store. Given that, the super type of store has two sub-types as you suggested: Physical and Online. Each physical store may be staffed by one or more employees, and each employee must be assigned to one and only one physical store. Physical stores then have two sub-types, Brick and Mortar and Kiosk. Having three direct sub-types - Kiosk, Online, and Brick and Mortar - hides a property that is possessed by every store - whether or not it can be found at a physical location. Now the design relies on a human to understand the semantics inherent in the sub-type names to understand that online stores don't have employees. This is not readily apparent in the declared schema and code in the form of a trigger must be written to express that understanding in a way the DBMS can enforce. Developing, testing, and maintaining a trigger that does not impact performance is a much more difficult solution to implement as is shown in the book Applied Mathematics for Database Professionals. Sub-typing Store first on its kind of location and then on the physical store's kind of structure is a more correct design with respect to the business rules and eliminates the need to write code to enforce the rule. Once the property is clearly included as a store location type which can be used as a discriminator for the sub-types, the relationship can be made between employees and physical stores directly and thus fully implementing the rule just with the foreign key constraint. ere is a data model created with Oracle SQL Developer Data Modeler that shows the super and sub-typing using Barker-Ellis box in box notation for super and sub-types, which I prefer for its elegant presentation. The diagram can now clearly show the rule as well. 

replica-01 and 02 are in on-premise data center replica-03 is on Azure VM (03 is synchronizing, no problems here) When I try to use replica-03 as reporting read-only server, running "use MyDB" statement shows the following 

Found out that all our Azure SQL Databases (pricing tiers S1-S2) have high number of connections/sessions from SA 

I have a production SQL server and a linked server (Azure SQL Database) I join two tables to do update 

I have a non-sysadmin user which must be provided with an ability to view "server audit logs" Tried to grant below permissions to this login / user: 

I even tried to significantly scale up Azure SQL DB (from 20 DTU to 800 DTU), but the speed of query (100 rows update) went from 18 sec to 8 sec which is still not acceptable What am I missing ? Is there any workarounds in this situation ? Regards, 

I have a table on Azure SQL Database that I would like to have replicated/mirrored to our on-premise SQL Server So the on-prem SQL Server would have a copy of table from Azure that is always up-to-date, available for read-only queries Is there a technology for this ? The reason I need this is because I need to join this Azure table to some tables on-premise (on 300K + rows) in a query, and linked server is not working for me very well, despite all the tricks and workarounds I have tried Regards, 

Another benefit of the relational approach is that the performance of the system can be managed to a great extent at the physical level without impacting the logical database schema used by applications to work with the data. With the relational approach you can focus on the correct logical schema first and then in most cases let the DBA implement a correct physical design and infrastructure to best support the read heavy workload. This is not the case in navigational systems where the programmer must specify the access paths directly in programs. 

Yes there are significant benefits to data normalization if you are willing to do the work to achieve and maintain it. The two fundamental benefits to normalization are: 

This says that to convert from Kg to Lb the first step is to multiply Kg by 2.2. There is also a constant if a conversion must also include a constant value, and the ability to create multiple steps. So when converting say Celsius to Fahrenheit you multiply Celsius by 1.8 and then add 32. The key would be the from UOM, the to UOM, and the Calculation Step. That is my 2 cents worth. I hope these references give you some good food for thought should you ever get the chance to do a reboot on the current design. 

I have never used the relationships tool for anything other than understanding the relationships between base tables on the back end database. I could see however where it would be very useful on the front end if you have many users of your application who write a lot of ad-hoc queries and you provide them queries stored on the front end database which they can use as an abstraction layer. On the other hand, if the users only interact with the database via forms I would think the work to add the queries to the relationship tool and connect them up would not be worth it. 

In SQL Server, is there an ability to limit allowed single query duration for certain logins (users) ? I would like to limit some logins so they won't be able to run queries more than 30 seconds. For a second set of logins, I would allow max query execution time of 60 seconds, and so on Regards, 

but in case of sp_spaceused (without any parameters) - can not insert output, since it produces 2 result sets. I know 2016 has nice parameter @oneresultset = 1, but is there any way you can insert 2 or more output result sets into #temp in SQL 2014 or lower ? Is there any tricks ? 

The update of 100 rows takes about 18 seconds! 1000 or more rows take much more time I made sure below is true: 

It is making it very hard to capture the "root cause" error message (example 911) instead of that generic 3013 message Is there any way to suppress message 3013 so SQL Server does not throw it anymore ? 

Tried to exclude MyDB from availability group, set compatibility level to 130, and include back to AG But since database in NORECOVERY mode, it does not allow me to set compatibility level If I restore database with recovery, I can set compatibility level to 130, but can't return database in norecovery mode, and can't add back to AG What do you suggest on above ? I need to keep primary SQL server replica as 2012 for now, and I need to make async replica (SQL server 2016) to be available for read-only access