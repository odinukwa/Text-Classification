It seems that the source and destination of this are reversed. Compare yours to the example from the doc in Continuous archiving: 

with as a placeholder for a numeric value (possibly with a fractional part). The syntax tried in the question is rejected because for the SQL grammar, the entire expression is a constant. It cannot be changed by injecting a placeholder into it, just like we couldn't write and pass as an external parameter to form the PI number. 

This forces plpgsql to avoid pre-processing the query, and it's somehow the generic workaround against this kind of errors. But anyway it looks like pgsql_fdw is quite experimental at the moment, it's not included in the PostgreSQL contrib directory, and its source code is suprisingly not available for download at: $URL$ where we would expect it. Currently, you may have more success with dblink, the "old way". 

The function, as the first choice to format dates, is quite limited with time zones. According to Template Patterns for Date/Time Formatting in the documentation we have and and they'll output a time zone name, not a offset against GMT. You may use to get the offset as a signed number of hours and to get the number of minutes that comes with it. This offset is documented as: 

Although inherent column order does not matter, but since you're looking for 'industry standards', you can search publicly available databases, like Microsoft's AdventureWorks, to see where Foreign Key columns are placed. It's usually at the beginning. This MSSQL query lists all FKs in a database : 

We know that the survivor/victim are SQL sessions (SPID), NOT SQL statements. We also know that the UPDATE statements shown in Frame1 of both sessions are involved in the Deadlock. And How are they involved ? They are both REQUESTORS of the U lock on the Index Key. Am I correct so far.. But which statements are the OWNERS ? And when did they start holding the X lock on the Index Key? One of the standard recommendations for resolving/reducing deadlocks is to Shorten Transactions. But if there are a hundred similar statements, without knowing WHICH statement started holding the lock and WHEN, it's not easy to go about shortening a transaction.. 

We have a main analytics SP that calls 100 other sub SPs, all of which act upon the same set of data (Claims), check it for some business rules, and output discrepancies. The SPs need not all be run sequentially, they can be broken out into sections, which are dependent on previous sections (sequential), but independent within the section (parallel). After looking at many options like Service Broker, Agent Jobs, Batch Files, SSIS etc., I used this CLR code to parallelize the sections, and it gave great performance improvement. However, when I run multiple (5, 10, 15) main SPs concurrently (each of which analyzes different claims) , performance starts to taper as concurrency increases. I guess this is because of the overhead of creating multiple parallel threads through the CLR. I also see lot of XTP_THREAD_POOL sessions idle in sp_who2. Has anyone used CLR for parallelizing Stored Procedures in Critical OLTP Production workloads ? Are there any best practices for performance tuning SQL CLR ? Is there a threshold for number of parallel threads that can be opened before the overhead makes things worse ? If my system has 20 cores, does it mean creating > 20 parallel threads does not help ? 

The TG_* built-in variables are documented in Trigger Procedures and the and plpgsql constructs in Basic Statements. The query above is absurd by itself (it selects results that go nowhere); the intent is to just show the base syntax on which an actual query could be built. 

However this form does not seem to offer any advantage over calling directly , so it's probably not worth creating such functions. 

In Debian/Ubuntu, per policy it's the package that is in charge of handling log rotation and purge for all services, PostgreSQL included. From $URL$ : 

As an alternative to prefixing all objects with the schema-qualifier variable, you may assign , so that it happens automatically. 

TL;DR: forget that, run the SELECT in the same psql session than the INSERT. From the point of view of PostgreSQL, there is no second user, it's the same user on the same connection, which is seeing the effects of its own not-yet-committed INSERTs. Otherwise it would be a dirty read, which PostgreSQL documentation states as "Not possible" in any of the isolation levels. A middleware routing queries from different connections from its clients to the same connection to the backend could produce the effect of a dirty read to its client. In practice, your goal is out of reach with a normal pooler, because these tools actively forbird situations where a client would see uncommitted changes of another client. It's part of the effort to be transparent to users, to whom the results should be the same whether their connection is shared or not. For example PgBouncer when using statement pooling, says: 

Is vSphere Replication safe for SQL databases? We are a virtual environment (Cisco hardware + VMware hosts + HP san). We have a primary and disaster recovery datacenter, with SAN-level replication already setup for every 4-hours. There are several HA/DR solutions for SQL - alwayson, logshipping, clustering, replication etc. But we want the easiest administrative solution that can deliver good RPO & RTO. So we're looking to replicate at the VM-level instead of SQL-level strategies. Apparently, any VM-level snapshot/backup solution needs to be VSS (Microsoft's volume shadow service) aware to ensure highly transactional applications like Exchange or SQL Server comes back up in a consistent state after restore. VMWare's SRM(Site Recovery Manager) gives you the choice of array based replication or vSphere Replication. I read here that vSphere Replication is VSS aware and can offer utmost 15 minute RPO, which is very acceptable for most of our applications. Does anyone else use this for enterprise oltp applications with 15-minute DR RPO (not high-availability) ? Does it freeze IO on SQL server for unusable period of time ? Does it definitely ensure consistency when VM turns back on at the DR site ? 

Which indexes would be considered under-performing, or which ones could be modified for better performance? 

We're looking at converting several normal T-SQL Stored Procedures into Natively Compiled SPs for performance. The SPs use a ton of #tmp tables - select into, delete from, update from, and several unsupported constructs like SUM/MAX/ROW_NUMBER OVER PARTITION BY, WITH, LIKE etc. We have figured workarounds for many of these : Select into - Create table type ahead of time Delete from - use while loop as cursor ... I was able to work around SUM/MAX OVER PARTITION BY.. but don't know how to do it for ROW_NUMBER OVER PARTITION BY. Is there a way ? Also, has anyone done large-scale conversion of SPs to Natively-Compiled and any roadblocks you encountered ? 

it's actually harmless, it's just the way of your to check for a new WAL file that's not necessarily there yet. Presumably this command looks like: 

Here does not matter because the COPY format is independant from it. When this COPY is executed, it will make no difference whether it's on or off. 2. Dump of 8.1 data with 8.1 pg_dump with --inserts option 

Note that the empty select (no column) is a relatively recent addition in PostgreSQL.On older versions, the above query will cause a syntax error. 

where is the size of the query. Be aware that a query just below might require large amounts of memory to be parsed, planned or executed, in addition to that buffer. If you need to push a large series of literals into a query, consider the alternative of creating a temporary table, rows into it and have the main query refer to that temporary table. 

Updating a row in a multiversion model like postgres means creating a second copy of that row with the new contents. Physically, there is no in-place update: UPDATE is similar to DELETE + INSERT the new contents. So in the above loop, the first chunk, instead of being written once, is written N times, the second chunk is written N-1 times, the third N-2 times and so on. To make it worse, all these writes have to go to the WAL files for journaling too, and all the intermediate versions of the rows will need be picked up by the autovacuum process to be eventually discarded. Assuming that these chunks cannot be assembled on the client and must be streamed, it might help to do something like this: 

Our primary OLTP workload is tempdb heavy (both reads & writes) based on virtual_file_stats DMV, and based on SPs all of which heavily use #tmp tables. The server has 400GB of memory, more than enough to fit all tables in databases within in. Tempdb was previously on a local SSD. We recently bought and installed a Micron PCI-E card, and placed SQL (2016 Ent) tempdb data and log files on it. However, I did not see a great improvement in performance, for some workloads it was even worse. The vendor specs lists 4K read-write speeds, whereas SQL uses 64KB (right?). Before I ask the SAN team to run benchmarks on the existing SSD vs new PCI-E disks, would it make sense to format the PCI-E disk as 4KB block size, instead of 64KB.. or does it not make difference for tempdb ? 

But all they have is reads/writes, but not actual number of rows. Is it possible to get this info in SQL 2008 or SQL 2008 R2 ? 

There is a discussion going on in our company on what the ideal CPU count and Max Degree of Parallelism are for a 3rd party database server. The server has 12 CPUs, 32GB RAM and all database sizes add up to < 30GB so they can all fit in memory (I tried to force this by doing a select * from every table). On certain payroll days, the CPU gets maxed out to 100% for a few seconds. MAXDOP was originally set to the default 0. We later changed it to 8 based on several 'best-practices' articles. However the vendor suggests to change it to 1 (no parallelism), while others suggest changing it to 4, so that one run-away query doesn't hog most of the CPUs. I'd like to find out how many CPUs are actually being used by queries. There is a Degree of Parallelism event in Profiler. The BinaryData column says : 

Also, there's something rather unusual in the output of your question: the postgres server is running under a Unix user, whereas generally, the dedicated Unix user is used for that. 

or have to infer the column types for the relation to create. Sometimes the context is not sufficient to guess a datatype, for example when it's just a string literal. In this case it's created as . Example : 

Uninstalling software will solve the problem for good only if that software was responsible for consuming too many simultaneous connections in the first place. Otherwise, getting back some of these resources may happen as a side-effect of say, rebooting, or just waiting a bit so that TCP connections in state disappear, or individually terminating programs that held these connections open. 

VACUUM FULL does not solve that problem, in the sense that if the files you found inside the database directory do not corresponding to anything in the catalog, they will be ignored by and their space will not be reclaimed. Such files are left out when a backend is stopped abrubtly (fast shutdown or crash) during the transaction that creates the objects, and the crash recovery sequence is not able to clean up. In that case, only manual deletion will reclaim that space. Before removing the files you want to be absolutely sure of your query. Right now, it's based on , and altough it gets back to indexes and toast tables indirectly, it seems to miss materialized views or sequences. See How to reclaim space taken by an index that partially built and was terminated by a power outage for an approach that worked. That question was about an index, but it applies to other kind of objects. 

All articles I looked at mention setting ANSI_WARNINGS to ON to get it to work. But I Absolutely do not want to see those Warnings, because I'm sending query results as attachment in sp_send_dbmail which has that Warning message right at the top of attachment. Is it possible to suppress the warnings for heterogenous queries ? 

I'm trying to troubleshoot a complex deadlock issue. There are 2 separate processes (service and agent job) which often concurrently execute the same SP but with different @BatchID parameters. This SP is called within an explicit BEGIN TRAN. There are 30 different Insert/Update statements within the SP. The 2 processes deadlock 5 times a day on the same table/index, around the same time each day, and the service process is always the victim. The table has many unnecessary/redundant indexes and a couple of Insert/Update Triggers. Lock Escalation is Disabled. Deadlock graph: 

What does "parallel plan running in serial" mean ? I see a lot of 0x01000000, and a few 0x08000000's in my trace. How can i determine whether one query is hogging CPUs and if reducing it to 4 will help ? 

We run on Cisco UCS + VMware ESX + HP 3PAR. Host blade config : UCSB-B200-M4 , Xeon E5-2667 v3 3.1GHz , 2 sockets , 8 cores each, Hyper-Threading Active So total 16 physical cores, or 32 logical cores. We have Software Assurance with M$, so all are Enterprise editions, and lot of additional SQL core licenses paid for, so money is not an issue. Our primary single OLTP SQL VM is 'dedicated' to one of the hosts, i.e. no other VMs are allowed to run on it, cos it requires all 16 cores of power. Even with that, CPU regularly runs ~60-80% , so we're planning to upgrade hardware. Questions below : $URL$ 

It's not technically true that is not installed with postgres packaged for Debian/Ubuntu. It's actually installed in , where is the postgres version. This path is intentionally not in any user's default PATH since it should never needed to be invoked directly. The reason for this setup is that the Debian/Ubuntu packaging supports several PostgreSQL instances running in parallel, either identical or different major versions. To learn about that, it's best to read first $URL$ , before the real PostgreSQL manual or any other non-Ubuntu documentation. is a multi-master fork of PostgreSQL that shouldn't be confused with PostgreSQL itself. The hint suggesting to install it to get is an unfortunate nonsense, probably a machine-generated advice based on the contents of . 

An approximation of the size of a row, including the TOAST'ed contents, is easy to get by querying the length of the TEXT representation of the entire row: 

comes from dynamically linking a newer with an older , for instance the one that ships with the system. On Mac OS X, this can be worked around by setting (the equivalent of linux's ) to the path of the newer PostgreSQL installation. Per comments and update, it's confirmed to work when setting this variable globally in 

Let's assume that the target table of the INSERT ("TableD") has a column with a foreign key to "TableC". An INSERT to this table must acquire a shared lock on "TableC" and will keep that lock until the transaction ends. Say two concurrent transactions, T1 and T2, start with such an INSERT. Each takes a shared lock on "TableC". Later on, in order to do a CREATE TABLE involving a foreign key to "TableC", T1 must acquire an access exclusive lock on "TableC". Since T2 has already a shared lock on "TableC", T1 will be put to wait until T2 commits or rollbacks. At this point, if T2 tries to create a table also involving a foreign key to "TableC" (which again implies acquiring an access exclusive lock on "TableC"), it must wait for T1 to commit or rollback, because T1 holds a shared lock on "TableC". So now T1 is waiting for T2 to finish and T2 is waiting for T1 to finish: there's the deadlock. To avoid this situation, an explicit and exclusive lock may be taken at the beginning of the transaction, or at least at the beginning of the sequence of instructions that can't be run safely in parallel by concurrent transactions. 

Full SP here In what order are the locks being taken ? Is there any way to avoid these deadlocks ? Adding CPU, Memory, Indexes, or Locking granularity ? 

I've been performance tuning SQL servers for a few years as a DBA. I'm trying to create a fast-food version of performance metrics that can quickly (in 5 minutes) and accurately (provable) answer a question from Management "Does this server need more/faster _ ?" '_' being one of these 4 possible bottelenecks in an IT-stack bottom up (from server-perspective, without going in to the app/code/ui): 

I need to setup a sql audit to capture all queries running on a server from any unauthorized locations, and one of the columns that is required is Row Count. In sql trace, this is possible. But I can't seem to find corresponding column in extended events. I looked into these 2 DMVs: 

SQL Server Processor NUMA config - all 16 processors are on 1 NUMA node - But I thought since the host has 2 sockets, there would be 2 NUMA nodes ? vCenter Socket/Core config - 16 sockets with 1 core per socket - Is it better to change this to 2 sockets with 8 cores per socket since that matched underlying hardware ? I recently heard at PASS conference, that if an entire VM is being dedicated to a single host, then leave Hyper-Threading disabled. Anybody agree/disagree ? As a basic pen-paper calculation, we need to double our 'compute capacity' from 50 (16 cores x 3.2Ghz) to ~100 GHz. Is it better to go for higher core-count across 2 sockets: E5-4669 : 2 socket x 22 cores x 2.2 Ghz = 96.8 Ghz or higher clock speed across 4 sockets: E5-4627 : 4 socket x 10 cores x 2.6 Ghz = 104 Ghz 

Looking at the WIN1252 character set: $URL$ , there is no character corresponding to that hexadecimal code . The same goes for , , , . That's why it "has not equivalent" in UTF8: that character doesn't exist in the first place in the source map. It doesn't error out when displayed or input in a session configured in encoding. When using the proper code page () with psql.exe on top of cmd.exe, these characters are displayed in my environment as a question mark inside a rectangular box. Anyway, you might want to remove these with statements like (when using the WIN1252 client encoding): 

The "Cannot assign requested address" part in the error message comes from the kernel TCP stack. When encountered intermittently, this typically means that the space of available sockets is exhausted because of too much sockets in wait state (, or less probably or ) The range of socket ports can be output by . The default value on a stock Linux kernel is generally . You may check the result of on the client(s) and on the pgBouncer's host when the system is busy. The flag will show the timeout counters related to wait states. If the total number of TCP sockets is close to then exhaustion of this range is likely your problem. Since a closed socket spends 60 seconds in state in normal condition, if a client host connects more than 28232 times in one minute, new connections will fail with the mentioned error until ports are freed. As a first workaround, the TCP ports range may be extended: