With this setup, you could download something like the latest version of Python onto the server (via logging in with credentials, API, etc.) and have it store Python in a central location for your other servers to download and install. You could store your artifacts in a multitude of ways (artifact repository, cloud storage, file server, etc.), but only that server should be able to upload to your storage. Your IT team could even set alerts when a new artifact is downloaded on this server or uploaded to your storage so that they can inspect it. I think this approach demonstrates a few things to your IT department: 

I am creating a design proposal for a data lake hosted in S3. My goal is to create a flat data lake that will host a variety objects within it. The majority of these objects will be made available to the general public, but some will be for internal use only. My proposal is to have a bucket policy that allows only public access for items with a public=true tag implemented. However, this tag should only be able to be changed by users with the correct privileges. Is this something that is possible in S3 with IAM, and if it isn't, what other options that can be utilized to control private/public objects in an S3 data lake? 

I pulled this from the Firefox Chef cookbook (in particular this .rb file). This an example of the request they use to get the latest version based on OS and language. 

If you are building these artifacts for a release/deploy, you will want them to be in as ready to release of a package as possible. If that is as a .zip, then yes you should create the .zip on the Jenkins server, then upload to Nexus. Your deploy system will then download, decompress, and manipulate the artifact as seen fit. In general, I would only use the node running Nexus to store artifacts and not execute any "build-like" processes. That is exactly what Jenkins was made for :) 

If testing locally isn't an option, then the most straight forward approach would be to use disk volume snapshots/backups to your advantage. These will still cost $$$, but will save you time in the long run. You should then separate your bash script into different working segments/scripts that can be tested individually. Once your server is provisioned, run a script, then take a snapshot. If it was successful, run the next script, take a snapshot, then rinse and repeat. If your script fails, modify the script, revert to the last successful snapshot, then try again. NOTE: I'm not sure if you can take snapshots of virtual machine disks in IBM Cloud/Softlayer, but it looks like you can create a VM image pretty easily. 

Monorepos are nice because it eliminates the technical constraints between multiple projects. This does however open the door to other complications within your repository (naming conventions, cross-team dependencies, merge conflict increases, etc.). I do not have any experience with CircleCI, but I will provide some input based on other CI tools I have used. 

For your own personal use, I think a GitHub repository with all of your bootstrap and installation scripts is a great idea. It serves as a portable way to download and distribute any scripts you need where firewalls and restrictions aren't an issue. As for within your organization, one suggestion that your IT might be open to as a compromise is to create a machine (barebones, VM, etc.) that is solely dedicated to downloading and storing the artifacts that you need. The criteria for this machine would be: 

These artifacts are important enough to you and your teams that you are willing to go out of your way to establish a safer and more traceable method to retrieve them. You have given your IT department an automated way to track who and when an artifact was downloaded in the event an artifact is malicious and needs to be deleted and traced to a source (something they probably didn't have in place before). The method you propose not only gives you more accessibility to the tools you need to be productive, but you are proposing ways to help your IT team be more productive by not having to manage all aspects of downloading artifacts. 

Before every production release, you should run a full suite test on all projects in your infrastructure. This is the only way to be certain that all project dependencies are non-breaking. Tests/provisionings must run in parallel. By doing this, your full repository testing speed will be the length of the longest test/provisioning in your repository. (As a side effect, this puts an emphasis on speeding up your slowest test) 

It has access to download artifacts (files, apps, etc.) from a source (preferably either the open internet or from white-listed addresses). It is able to store the artifacts in a central location. It is not able to access internal infrastructure It is has auditing to track any artifacts that it downloads/stores 

I don't think there is a generic term for promoting to production. I can tell you that within the devops community, terms are frequently interchanged. As long as the point being addressed is well thought out and made clear, the point can be understood. Some of the terms that are used when referring to the movement between environments are deploy, propagate, move, promote, and release. I will say though, whenever I hear the term stage, I usually associate it with a pre-production environment. I think most others would assume the same. Within your organization/groups, I think it is important to try to be consistent with your terminology. This eliminates any confusion and assumptions people can make. An example would be, in my organization, we use the term deploy whenever we are referencing moving between environments except when referring to the production environment which we use the term release. This is to emphasize that it is the final movement in the pipeline/process. 

I searched around for official api documentation, but couldn't find anything. If I do, I will post a link to it in this answer. 

And of course as with all PowerShell remoting issues, you should check that your firewall and network settings on the remote server allow connections from Jenkins over ports 5985 and 5986. 

Moreover, the fact the data will be written really in-place is not guaranteed and you may be writing aside the original data. That said the reason to do this goes back to a point where a data was written on disk on consecutive sectors on the same physical medium, usual storage nowadays use striping technology to spread data on multiple physical medium, each write in the system toward this storage will have an impact on all the stripes this file was part of. Each provider will have its own custom storage backend, but I'm pretty confident the same principles are used. When you get the picture even higher, you're writing a file on a virtual hard disk which is itself a file on a storage pool, probably streched on multiple lun composed of raid array of physical disks. There's so many IOs running on all those strates of striping/aggregation that the probability someone could recover a file by magnetic reading of physical disks (what wipe and shred should protect against) is very very very low as the storage used for this file has probably been reused a bunch of time already. Even on existing ext3/ext4 filesytem, tools like extundelete have a hard time recovering any file if the disk is not unmounted/mounted read only very quickly after the deletion. 

The main idea behind a SCM is to manage your infrastructure as you manage your application, code it, version it, test it and be sure it will always behave the same. If you rely on human procedure for configuration of the middleware/libs/app config you can be sure there will be a step forget during the deploy process once in a while, and spotting this problem can be really hard. With a SCM you prevent this omission. 

In my opinion you are right, Continuous Delivery (CD) is not the control framework of Devops, at least it is not the only possible one. But in the context of the book you're quoting it gets the most used possibility when you start to include security baseline and assessments as part of the product delivered. In a security context, you'll add smoke tests in a post deploy phase to ensure your application is not subject to XSS or Sql injection for example. If any of this tests fail, block any deploy in other environment, mark the deploy as failed and eventually rollback to previous version. This enforce the security rules to be satisfied, and in this way act as a control framework to ensure production deploy are 'safe' for known vulnerabilities. 

Once parsed your config create one and only one pipeline, with various inputs, various filters and various outputs. You have to use some conditional constructs to apply filter and output only to specific messages, usually using a special tag or field set on input. Some exemple are available in logstash documentation: 

In short, Chef may not be the tool to use here. Chef is a convergent model, so you have to write your recipes in an idempotent way such as after some runs it will be at the desired state according to the others nodes around. Your approach about storing the state sounds the way to go, and you'll have to use an external orchestrator to schedule the chef-client runs alternatively on A and B. If you have a chef-server, push jobs could be the way to go sticking in chef ecosystem, main advantage I see with it compared to others orchestrators is a pull model not needing an incoming administrative port nor ssh on the target nodes. 

That's still a XY problem IMHO, why do you want an output in a single line at all ? what's the need for it ? If you can't explain more and wish to footgun yourself by replacing newlines with spaces the use (you'll probably end up loosing the end of the log). If your goal if to parse the last line then will give This select the lines containing and split each field (3 to 6) on the char, thus giving the key as first element of array and the value as the second element. use instead of to get each state on separate lines. 

First of all, I strongly advise against "feature comparisons" blog post for similar software, they're quickly outdated and I'll try to keep this answer generic for a system configuration manager (SCM) and let docker out of the talk for the first part. What it can brings you: 

There's a need to use bash for a one liner to take advantage of command subsitution. You can also do a .sh script like: 

For what it worth you can achieve that pretty simply using your ssh client as a socks server and configuring your clients (RDP/Browse/Ssh client) to use a localhost: as a socks proxy and then access your inner network. A SSH tunnel is a tunnel as secure as a VPN tunnel; main difference with a VPN is there's no routing involved. 

Best setup in my opinion is 2+3 above, to handle any problem of missed dependency while enforcing a communication between separate teams and allowing releases compatible with multiple version of the dependencies. this need to keep trace of this kind of behavior change to remove the unneeded old API handling once it is not on line anymore. 

Outdated but the most efficient in anomaly detectionI know: Shyline + Oculus from Etsy Both tools available from etsy's github as Skyline and oculus, check the network graph for more up to date forks. Best one I found is earthgecko/skyline with its doc Graphite and checking that the Standard deviation fits in the Holtz Winter's Confidence bands ELK stack in version 5 with timelion which, as above, allow to compute a standard deviation and alert if it goes out of confidence bands with the proper license. 

Probably not exactly an answer, but worth giving the alternatives. Chef's habitat has been created with this in mind, creating a package with all needed dependencies without the extraneous distro/base image load you don't wish. Extracts on what matters here, the container size from this blog post with a simple nodejs app: 

I'd go with to store a transient variable in a run and define it in a so it happens at converge time, something like this: