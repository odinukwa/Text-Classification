Does it change the state of your game world? It's logic code. Does it display the state of the game world? It's rendering code. 

There is no one size fits all answer to this, as is so often the case. Generally though, if you can precompute it, do it. If your robot doesn't change all the time, so for example you build it and it stays the same the whole time, then what's the point of calculating all of its parts every frame. But if it does change all the time then obviously you can't get away with making all of it one big object. You are probably aware that there are tons of things that become computationally more expensive the more objects you have, for example collision detection. So you have an interest to keep the number of game objects low(-ish). When it comes to rendering, ideally you would want to have a VBO and IBO for your whole robot, so you can push it over to the GPU once and then just reference it forever. If instead you split your robot up into bits you will have to make a draw call for each of the bits. While the amount of drawing that your GPU ends up doing will be the same, the amount of work for your CPU to translate all of those instructions for the GPU will be much higher. So in most cases treating it as one big object would be better. However there are cases where the other treatment might work better as well. Computing everything you need to treat your robot as one object is relatively computationally intensive and memory heavy. (Compared to just keeping everything as a collection of blocks) So if you are in a garbage collected environment and want to minimize your memory footprint, if each time a block is changed you have to recalculate the big object it could cause a garbage collection every time and make your game lag for a frame. 

Another solution to transitions and other such things is to provide the destination and source state, along with the state machine, which could be linked to the "engine", whatever that may be. The truth is that most state machines are probably going to need to be tailored to the project at hand. One solution might benefit this or that game, other solutions may hinder it. 

Model The first is the world or model matrix. This matrix takes the vertices in an individual model (such as a crate) modelled around the origin (0, 0, 0) and transforms them into world vertex coordinates. This might include scaling the object, rotating it around it's origin and finally translating it across to where it's located in the scene. 

I think your issue is that you pathfind towards the players tile rather than to a tile that he can be attacked from. Since there is no actual path to the player in that case, since diagonal movement isn't allowed, of course they can't find a path. If you just want them to get closer you can put a maximum on the number of tried nodes and always move towards the minimum of your heuristic.(In which case I'd recommend the manhatten metric) Or you could change your metric a bit so that it's 0 when you're at a distance you can attack from. 

There's two slight ugliness's with being very based, though. One is that my dictionary can't serialize as because Type isn't supported by (what I'm using at the moment). So what I'm doing in and is translating to and from a string using (i.e. "Game.Skills.Engineering.LaserWeapons"). And the other is potential performance problems, which is something I should profile. This all works, of course, but it feels like the weak point in an otherwise enum-less, strongly typed skill definition system. I don't really want to use enums and a "list" of Skill structures, or move it to an external data definition file because then that requires more maintenance of created and deleted enum values and such. Do you think this is worth it, or do topics such as performance of Type based operations and other considerations make it worth rethinking? 

You can do any interpolation or tweening or whatever that you want there. As a sidenote, in almost all cases converting away from radians seems like a bad idea. If your question was specifically about how to figure out which way to rotate. 

I can't see anything fundamentally wrong with the shader, but here are a few things I find commonly done wrong with deferred shading that you might be doing. 1: Drawing full screen lights. The beauty of deferred shading is that you can pack your lights into geometry so that you only need to consider a part of the screen when drawing them (Like a cube with 2 times the radius as size). If instead you draw a full screen quad for every light, that will have a serious performance impact. 2: Render target depth. You are moving a lot of data around in deferred shading, so you have an interest in making the footprint of that as small as possible. If you can reduce the number or size of your render targets that can have a pretty big performance impact. 3: Rendering one light at a time rather than batching them. (Edit: You can have a static vertexbuffer and indexbuffer containing vertices for the max number of lights you want to ever render and then just patch the position and color of the lights that are active) And lastly, you can always look at the assembly of your shader and see how many cycles it will take, so that gives you an easy way to see your own performance and compare. GPU Gems has 2 or 3 great articles on deferred shading and the performance and other issues that come with it. 

Projection The final piece of the puzzle is the projection matrix. In a camera-like perspective projection, the matrix transforms the almost screen coordinates to give the illusion of a perspective with a field of view of x degrees. If you think in terms of corners of the screen (for a 640x480 game), this is how projection "aligns" the coordinates along the Z axis within the 2D coordinates of the screen. For orthographic projection, no scaling takes place along the Z axis into the screen. For perspective, the further away the object, the smaller it becomes in relation to the screen's extents. 

To me it looks very much like your issue here is that you are mixing world and viewspace or something similiar. Now your GBuffer normals look like they might be in viewspace in that picture, but the code in your geometry pass definitely doesn't transform them from world to view space. If they are in world space then your NBT matrix is basically oriented incorrectly by exactly the rotation of your view matrix, so your kernel will end up being rotated into or out of the wall depending on where you are looking. And that would give you exactly the result you are seeing. Hope this helps. (Minor sidenote: you are only interested in offset.xy, so after projecting you can just use the two rather than offset.xyz) 

The Matrix. Or rather, lots of matrix math. It's scary stuff for the uninitiated. There's typically three 4x4 matrices involved in turning a bunch of 3D coordinates in space into 2D coordinates on the screen (including the depth into the screen as Z). 3D matrices are a set of 16 floating point values arranged in a 4x4 grid. Algorithms are used to generate the required values and then using matrix-vector multiplication, these numbers transform a 3-dimensional vector (X, Y, Z). 

View The second matrix is the view matrix. This takes the world coordinates and transforms them so that they are within the context of the view. The concept of a camera is typically used to generate this matrix. The camera usually contains a position vector, a direction or target vector, and an up vector. This up vector describes the 'spin' of the camera. 

Fourth there is also something to be said about the effectiveness of lists, especially in a garbage collected environment, but that varies a lot depending on implementation details. Fifth You can handle your collision phase differently if you don't care if objects intersect for a few frames, by just letting them gardually push each other out by applying force. It looks like you are trying to guarantee zero intersect, which you are currently not doing. (An object can be moved out of the first collision and then back into it when resolving the second collision) As a last sidenote I think that simulating n-body attraction is just always going to be a performance nightmare, so in some way you don't really need to worry about this rest, since that's what will most definitely ruin your performance. (Although space partitioning can also aleviate that a bit since you can only check with objects that are close enough to matter) I probably didn't notice everything but I think that's the jist of it. 

Set UI transformation matrices, set U/V/vertex coordinates for quad, draw. Nothing more to it, really. 

I am working on a skill system for use in a game project I'm chipping away at. The project is entirely in C# so I'm using .NET to its advantage. In my preliminary implementation of a Skill Category -> Skill types system I'm using subclasses of ISkill and SkillCategory (base class with some virtuals) and using System.Type based reflection to generate a map of these at runtime (on demand). The great thing about this implementation is that it entirely decouples specific skill types (such as from the SkillLibrary (the container for the generated skill mapping). The goal is to remove the need to explicitly add each skill at some point as they are automatically detected by the library throughout the entire . Only game logic which relies on these skills use concrete types. ISkill 

My best guess is that you have some rounding errors. You aren't actually showing the movement code for the bullets so it's really hard to tell. Edit: Looking at the code, it should(tm) work. It definitely works fine to do things like this in the games I've written with XNA. The thing that bothers me is that your game doesn't seem to be running at all when you set FixedTimeStep to true, so something really weird must be going on somewhere. Could it be that you didn't separate Update and Draw logic properly? Edit2: The TargetElapsedTime is set to 16.666 seconds instead of milliseconds, so that's most likely causing the issues.