I dont think you can ignore the stored proc. This it ultimately query tuning. Queries use indexes. Indexes support queries. They go together. There is a limit to how much indexes can help a bad query. You cant do too much harm adding a few indexes judiciously. However if you add too many indexes you could make performance even worse. I would start by ensuring your current indexes and statistics are maintained and in good shape. If not you should start there. You may get a noticeable improvement just from rebuilding your indexes. Next I would look at the queries in your stored proc and assess if they would benefit by adding or modifying indexes. As mentioned by others I would focus on covering indexes that support joins and filtering. Dont go crazy, just start with a couple, test and review. Look at the execution plan and focus on the tables and functions that have the highest "cost". If there are several queries within the same sp I believe you should consider them together. You may find one covering index may benefit several queries. Keep in mind that different query types may benefit from different types of indexes or indexes on different columns depending upon what they do. For example a query designed to return a single row vs a query attempting to aggregate data. Edit: Your comment about the reccomendation to create 78 statistics makes me suspicious that something isnt right there. This article about auto_create_statistics & auto_update_statistics might be of use to you. 

If the data is a mix of fixed width and and delimited you can use a combination of the steps above. If you have to repeat or automate this process, I recommend you go back to the source of this data and ask them to re-export with a delimiter, grant query access so you can import data directly, or get them to export directly to a database table for you. 

If the field is NULL, Do I need to update the field first to insert a base for the xml i.e before I can insert to it? Whats the name of an XML node that doesn't use an opening and closing pair. Are there other/better methods of inserting nodes into XML using SQL? 

If you don't want to do this, then you will have to find a way to do this periodically as you suggest. You could write a script to up date all statistics, or as you suggested use Ola Hallengren's maintenance scripts to update all stats. 

I agree with Rick and I Michaels answers, and would go further. In my opinion you are talking about the difference between tables and the interface. I would question if you need to have a schedule table at all. I'd store each vehicle's details and service requirements. Personally id have different tables for make/model and the details of the items that need to be serviced and at what distances/intervals. I would have a form that looks up the make/model and works out what needs to be serviced depending upon the mileage. You need the details related to that specific car and component. Why not store them together? Unless you need to store the details of jobs done why bother storing all of the possible iterations of service schedule? Can 1 schedule be reused for many cars? Another thing to consider is where you will get all of this data from. Is it in a format that can be easily imported? Text data? Microfiche? Without data your system isn't going to be much use. And hundreds of hours of data entry is boring and expensive. Do you need to the data in database fields so you can perform calculations and queries on it? Or does the user just need to be able to lookup the details. (Ie pdf/microfiche ) If you haven't done so already i suggest that you look at a few different service manuals for different cars. You may have some different ideas after that. 

If you follow a traditional entity relationship diagram customers and employees would typically be considered as two different entities and therefore would have thier own tables. This would make sense to me in this case if a person could be both an employee and a customer and an employee at the same time. How would you handle that? 

I would highly reccomend migrating the back end to SQL Server. Tables can be attached from access via linked table to SQL Server. The advantage of this approach is that you can scale up to avoid the access file restriction yet still make use of all the work you have done in access. There are even wizards to assist with importing and upscaling from access. Performance will be directly affected by your sql file, table and index design. But roughly speaking should perform as well, if not better than access. Basic tips: 

I'm currently upgrading our Data warehouse from SQL 2012 to SQL 2016. I have both my old and new DW's running side by side in parallel. My ETL process (A framework developed in SSIS by a 3rd party) has run successfully for more than 2 years on 2012 but is failing on 2016. So far the databases and ETL process are identical. Both Servers are Virtual machines running on VMWare. Old Server is Win 2008 with 24Gb of RAM. SQL 2012 Std. Max mem set to 16Gb. New Server is Win 2012 with 64Gb of RAM. SQL 2016 dev. Max mem set to 50Gb. New DW is running v13.0.1601.5 RTM Developer Edition (64-bit). While running my ETL process the load steps that use a SQL Merge into either a dimension or fact table fail with the following error. Full text: 

It is possible to process individual rows and redirect the records that failed. You could even validate records before you try to insert them. How do you want it to work? Unless they specifically need to be processed together as a batch I would be processing individual records and storing failed records for correction and resubmission. 

Why? What am i missing? The inner query has filtered and converted the value I am evaluating. Is there a better way of filtering this list to be used in an update query? 

You will need to write a query or report that joins between the tables so you can select the description. The Image below shows the MS Access query designer. It displays both tables and the join between them. Below that it shows the fields I want to return in my query. 

No auto shrink can't permanently damage a database. It may lead to highly fragmented tables and indexes which may hurt performance. The fragmentation could be difficult to remove completely, but minor fragmentation itself shouldn't be a concern. You need to assess whether this is really an issue for you worth spending time and effort to fix. 

As I review the structure you have now I would change the identity columns from BigInt to Int. I suggest you review how many Students, Questions, Answers and Choices you expect and select data types with appropriate sized ranges. I would also replace nvarchar(max) with a more reasonable sized field. As there are limitations with functions on nvarchar(max). I'd only use it as a last resort. As for the table structure itself I would makes changes to the Student choices table. 

Assuming those are the only 2 status, or that any other status will come after 'completed' alphanumerically, you can use the min() aggregate function. 

I would create 3 tables: Contract: Contract id (pkey) Attrib 1 Attrib 2 etc Annex: Annex id (pkey) Contract id (fkey) Attrib 1 Attrib 2 etc Dynamics: Payment id (pkey) Contract id (fkey) Annex id (fkey) Attrib 1 Attrib 2 etc. As I understand your description this would allow entries in the dynamics table to relate to a contract or an annex or both. 

I would take the lowest level transactions and transfer them to a central location. Load and then batch process. Preferably without triggers. Id have create_dt and change_dt fields which id use to identify deltas that need to be resent. Id then periodically reconcile source and target. Whether you overwrite the changed records or append deltas will depend on what your till captures, your account practices and level of detail required. I.e Cash vs accrual and whether you need to reconstruct how it looked at a point in time. (stationarity of record) I do something similar with my Datawarehouse. We ship low level data using ssis, but also have a step in our etl which performs sums/counts on both systems and stores the result. If it varies we know to reload.