I would look into the Soft Imputation method that has an implementation in R. It uses iterative soft-thresholding to compute missing values. Calculations are done with a matrix class called "Incomplete" to deal large sparse matrices and allows for quick calculation of scaling/centering rows and columns. I've had good success using this completing a 10,000 by 10,000 very sparse matrix so I'd imagine it should do fairly well with your dataset. 

This methodology seems a bit strange and potentially overkill for the problem. I would try having your input just be a 5 dimensional vector that is the difference between racer 1 and racer 2 and have the output just be the result for racer 1 (or what every racer's features are being subtracted from the other racer's features.). From here you can just use a few fully connected layers (which is essentially what you are doing be having kernel's of size 5 anyway. I would also try some more traditional machine learning algorithms with this type of 1D input. EDIT: since signs are important for the difference between your two racer features, do not use a relu as your activation function for this approach. 

You would then turn the text into features using some sklearn pre-processors. Count Vectorizer or TF-IDF Vectorizer are popular choices. You can also create your own features from keywords from the Attribute key and values you already have, such as creating a Brand indicator column if the words Samsung, Nokia or OnePlus appear in the text, but I would never use only manually specified features when modeling with text features. 

If instead you want to create a second model, you can try using any machine learning model suitable for classification. I would probably stay away from larger models like Random Forests, but doing something like using the predictions as features into a Logistic Regression model can work well. But again, I would emphasize only trying to use the predictions from a holdout data set into the logistic regression model to make sure you aren't over fitting. 

However, if you see the performance on model one does better overall, you might shift the average to 

PCA (unsupervised): this creates "new" linear combinations of your data where each proceding component explains as much variance in the data as possible. So the first 7 components (out of 27) should be able to explain a good percentage of the variation in your data. You can then plug these seven components into your logistic regression equation. The disadvantage here is that because the components are combinations of your original variables you lose some interpretability with your regression model. It should however produce very good accuracy. This same technique applied to other dimension reduction methods such as Another common method in regression is forward stepwise where you start with one variable and add on another each step, which is either kept or dropped based on some criteria (usually a BIC or AIC score). Backwards stepwise regression is the same thing but you start with all variables and remove one each time again based on some criteria. Based on a brief search it doesn't seem that python has a stepwise regression but they do a similar feature elimination algorithm described in this Data Science post. Lasso Regression uses an $L_{1}$ penalization norm that shrinks the coefficients of features effectively eliminating some of them.You can include this $L_1$ norm into your logistic regression model. It seems sklearn's LogisticRegression allows you do assign the penalization you want in order to achieve this. Note: Lasso will not explicitly set variable coefficients to zero, but will shrink them allowing you to select the 7 largest coefficients. 

Line search is an optimization method that involves guessing how far along a given direction (i.e., along a line) one should move to best reach the local minimum. 

It simply means that $Y = A X$ where $A$ are the parameters. The variables $X$ might contain nonlinear relationships; e.g., $X = [\alpha\; \alpha \beta\; \beta^2]^T$, yet $Y$ is a linear function of $X$. 

nDCG is a ranking metric and RMSE is not. In the context of recommender systems, you would use a ranking metric when your ratings are implicit (e.g., item skipped vs. item consumed) rather than explicit (the user provides an actual number, a la Netflix). 

The hyperplane is a linear combination of the support vectors. In the soft margin case, there is only a limited amount of slack; every input does not get to be support vector. In the nonlinear case, the separating hypersurface may be embedded in an infinite-dimensional space, making it impossible to store. To borrow from the Wikipedia article, the normal vector $w$ is given by $$w = \sum_i c_i y_i \phi(x_i)$$ where $\phi$ is the feature embedding function, and $c_i$ is a Lagrangian dual variable that is zero for points on the correct side of the margin. Instead, test points are classified through a kernel function $k(x_i,x_j) = \left< \phi(x_i), \phi(x_j) \right>$ like so: $$x \to \mathrm{sgn}(\left<w , \phi(x)\right> + b) \equiv \mathrm{sgn} \left( b+\sum_i c_i y_i k(x_i, x)\right)$$ Notice how we avoided explicitly calculating $w$. 

A random forest will work, however standard regression will also work with categorical variables as predictors. You will have to "one-hot" encode your categorical predictors into 6 "dummy" variables (classes-1 = 7-1 = 6). The first dummy variable will encode 0/1 for whether or not the observation is class A, second dummy variable as 0/1 for class B, etc. You only need 6 dummy variables because if all of them are 0 for a given observation, that means the observation is in group 7 (G). In some languages, such as R, the regression command will automatically do this one-hot conversion for you. For python, the pandas package can do this for you with . 

Your problem is that you are not specifying the axis that you want to convert your tensor into a one hot tensor vector with so it's defaulting to looking at all elements at once, making logic_b of shape (2, 4, 8) when really what you want it to be is of shape (8, 2, 4). See below: 

Your line just returns an array of scores for each run of the cross validation, so the error is telling you that it's just an array of numbers, not a model itself and has no defined method for fitting things. To use predict, you need to call it on a model object, which I'm assuming you're going for the VotingClassifier. Your variable has a fit method, so you want to call 

Given just one line of the data, it's a little hard to go off of, but I'm assuming you're trying to get at the number after each colon, and the number before it refers to the column name? If so, you can use read_csv with a little tweaking: 

The problem type you're dealing with is referred to as multiclass classification. Not all algorithms are suited to handle it, but tree based methods and neural networks are popular choices. If you need it to run quickly and probability calibration isn't too important, Naive Bayes also works quite well for some data sets. To see an example of a dataset of this type, check of the Kaggle Spooky Author Identification competition. The published kernels give some good examples of feature engineering and modeling choices. (I'm assuming the category label is unique. If there can be more than one label per record, it's called multilabel classification, which I would handle by building a separate binary model for each of the 39 labels in your data set. For an example dataset, check out the Kaggle Toxic Comments competition.) As for modeling with your data in specific, the structure you have it in now seems a bit odd to feed to a model. A training data set should have each row represent one record, and each column represent a feature with the value in the column describing the record, whereas in your format, any cell phone should have information regarding each value in the corresponding mobile Attribute_Names. When you get the raw input 'Samsung Galaxy On Nxt 3 GB RAM 16 GB ROM Expandable Upto 256 GB 5.5 inch Full HD Display', how is that transformed into a format that can be fed into the model? Also, where are the Attribute_Names, Attribute_Values coming from, are they manually specified? If so, that is limiting the performance potential from a model since there could be additional words in the data the model could detect if left to generate features on it's own. For a good modeling flow, the training data should contain the inputted raw text, then process the text to generate features, then feed into the actual model to output a label. So one row of raw training data set would be: 

Good question! I've faced this problem before, where the CEO had unrealistic expectations. Unfortunately I can't tell you how we worked it out, because it didn't work out. The good news is that you're not working for them yet, so you can just walk away. At the end of the day, it comes down to trust, and they have to convince you they trust you. Talk to the other employees to find out they are treated; that might tell you something. 

It would make no sense of the word embeddings to change within a document. It would be as if the spellings of the words changed; how would that help? When you use a document embedding, you find its numerical representation in a way that loosely captures its meaning. If you want to capture the meaning of each paragraph separately, find their embeddings separately. If you want to capture meaning of the entire document, feed the entire document. For example, if your document covers a range of topics and you want to allow users to pinpoint where a particular topic is covered, you can find embeddings of each section (paragraph, page, etc.), then find the section that's nearest to your query's embedding. One use case for dynamic word embeddings is to identify temporal dynamics; how the meaning of a word changes over time, as in this paper: The Visualization of Change in Word Meaning over Time using Temporal Word Embeddings. 

Assuming the output size of your model is an integer (i.e. $\in\mathbb Z ^{[0,100]}$) a vector $\in \mathbb R^{100}$ should definitely be the output to your ANN. This is the most common approach to creating output classification for neural networks. The values of this output vector could be all over the place depending on how your weights are initialized/trained so in order to interpret the output as a probability of belonging to one of the given 100 classes (and sum to 1 as you say) you need to feed the output through a soft max function. You can then take the output of this to get your size class prediction and train using the cross entropy. If your output is continuous the easiest approach is just to have one output size, train on MSE, and get your answer by rounding between 0 and 100. 

You're dataset seems fairly small for a recommendation system so I am not sure how these approaches with scale but you are looking to solve a collaborative filtering problem which is essentially a sparse matrix completion. I popular and effective method for doing this is softImpute. For further resources here is a survey paper on Top-N Recommender Systems via Matrix Completion. 

The number of support vectors must be increasing. The prediction time is proportional to that; after all, the kernel classifier is $f(x) = \sum_i \alpha_i k(x, x_i)$, where the summation is over the support vectors. With sklearn you can find out how many you have by inspecting n_support_ 

A simple solution is store the words in a dictionary (since you have to store them in some data structure) whose key is the character distribution; e.g., Counter("hello") = {h: 1, e: 1, l: 2, o: 1}. Querying the dictionary would give you the anagrams. For storing the immutable key (character distribution) can either use a tuple list, incurring a sort, or you can use a vector the length of your alphabet (26). So you can make a speed-space trade-off in the preparation stage; the look-ups are constant time, not counting the time it takes to calculate the character distribution of the query word. If you go the latter, fixed-width route, you can make another trade-off by hashing the key, since you know the cardinality of the input (number of unique words). 

Part of the problem lies in how much data you have. To create a second level of complexity, you ideally want to use a holdout data set to decide the right combination for the model predictions. If you use the training data from the models themselves to also combine the model output, you risk over fitting your final model. If you have a small data set, trying to build an ensemble on top of it might lead to worse performance in deployment since the model is only memorizing the training data. In that case, a simple average might do you better than trying to anything more sophisticated. But, assuming you have more data to use, or if you want to use the data you trained on anyways, you can either design a weighted average or create a second model. If you have an idea of which model performs better, you might want to try manually assigning weights for each model's output. A simple average assigns an equal weight to each output, but you can experiment with shifting the weight around a bit. For an example, given two models, a basic average yields