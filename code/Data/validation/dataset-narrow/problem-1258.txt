I don't think something much better is known. Because for example, if it were possible to derandomize circuits with only a sublinear blowup, then I think it would also be possible to non-trivially (but non-uniformly*) derandomize communication protocols. And I don't believe the latter is known. Adleman's proof gives a linear blowup as you say, so that the derandomization of communication protocols is trivial because it would give a linear blowup in the communication complexity. *: By "non-uniform" in the context of communication protocols, I mean the algorithm for the two parties to compute the next bit to send to the other is not explicit. I recall reading a discussion about this in some paper, but I can't seem to find a reference now... 

There was a blog post by Fortnow on this topic. It's believed that there's no hope of a "dequantization" program, similar to the derandomization one. On the other hand, for some specific non-quantum results that were obtained using quantum methods, it has been possible to remove the quantumness in the proof. For example, Kerenidis and de Wolf (2002) proved the first exponential lower bound for the length of possibly non-linear 2-query locally decodable codes using quantum arguments. Later, Ben-Aroya, Regev and de Wolf (2007) could remove the quantumness of the proof (although the line of argument still modeled the quantum one). Similar situations also arose in proving lower bounds for rigidity of Hadamard matrices, and in showing that PP is closed under intersection (though in reverse chronological order :)). See this survey by Drucker and de Wolf for references and discussion. 

Random graphs: For many graph properties, random graphs are extremal in expectation. For instance, the number of times a given complete bipartite graph occurs as a subgraph is minimized in a random graph. (It's a beautiful conjecture of Erdős-Simonovits and Sidorenko that if $H$ is a bipartite graph, then the random graph with edge density $p$ has in expectation asymptotically the minimum number of copies of $H$ over all graphs of the same order and edge density.) Distributions specified through random graphs are the source of many lower bounds for randomized graph algorithms, through Yao's minimax principle. Structured graphs: This is a rough designation for a class of graphs that are somehow specially structured for the problem at hand. For example, Turán's theorem says that the densest graph on $n$ vertices which is triangle-free is the complete bipartite graph $K_{n/2,n/2}$; this graph is clearly specially built to avoid triangles. "Non-random" graphs: These are intermediate between being completely generic, as in random graphs, and completely specific to the problem, as in structured graphs. For example, such a family could be random subgraphs of structured graphs. Such examples come up often in creating stronger variants of Szemerédi's regularity lemma. One way to produce these examples is to come up with a definition of "pseudorandomness" that models random inputs, so that for pseudorandom inputs, you can show that your algorithm or your conjecture works. Then, you identify obstructions to pseudorandomness, and graphs which have these obstructions can then produce a large collection of non-random graphs which are counterexamples. A more involved discussion of this principle can be found at Terry Tao's ICM talk in 2006. These non-random graphs roughly correspond to the "nilsequences" in some of his works with Ben Green and others. 

The very general intuition is that the larger the symmetry group of a property, the easier it is to analyze its testability. The idea is the following. Suppose you have a property $\mathcal{P}$, defined as a collection of subsets of some domain $D$, and suppose that $\mathcal{P}$ is invariant under a permutation $\pi : D \to D$. In other words, a subset $S \subseteq D$ is in $\mathcal{P}$ iff $\pi(S)$ is also in $\mathcal{P}$. In testing, we want to determine whether a given set $S$ is in $\mathcal{P}$ or "far" from $\mathcal{P}$ by choosing a small number of points $X_1,\dots,X_q \in D$, querying whether each of them is a member of $S$ or not, and then making a decision based on the results of the queries. Suppose that a particular choice of points $z_1,\dots,z_q \in D$ constitutes a valid test. Then, the observation is that $\pi^{-1}(z_1),\dots,\pi^{-1}(z_q)$ also must constitute a valid test. Because using the test $\pi^{-1}(z_1),\dots,\pi^{-1}(z_q)$ on $S$ is the same as using the test $z_1, \dots, z_q$ on $\pi(S)$, and we know that $S \in \mathcal{P}$ iff $\pi(S) \in \mathcal{P}$. So, if $\mathcal{P}$ has a group of symmetry $\Pi \leq \mathsf{Sym}(D)$, then the entire orbit $\{(\pi(z_1),\dots,\pi(z_q)): \pi \in \Pi\}$ consists of valid tests. Thus, the larger the group $\Pi$ is, the more tests can be generated from a single test, and in fact, for many natural invariance groups, all possible tests can be generated from the orbits of a very restricted class of tests. When this is the case, it becomes much easier to give conditions for when a property is testable and when it's not. Now, coming back to graph properties, because every graph property is invariant under relabeling of vertices, Goldreich and Trevisan (2003) formalized this above argument to show that the test for any graph property consists of querying induced subgraphs from the given graph. This result was crucial in a later combinatorial characterization of testability for graph properties. In your question, you seem to be asking what happens if there are even more symmetries known for the graph property at hand. As far as I know, there's no general study of this question, mainly because there's already a precise characterization of testability for general graph properties. It might be interesting to ask which graph properties are "easily testable", meaning the query complexity is polynomial in the proximity parameter, and here there's no general characterization known...so putting additional symmetry restrictions on the property might make answering the question easier. 

Here, a hyperplane is a $(d-1)$-dimensional flat. For $d=2$, this is essentially what you stated. The restriction that $\beta_d \leq 1/2$ is from the example of having two skew lines in $\mathbb{R}^3$ containing $n/2$ points each, where the number of spanned planes is $n$. For more, see: $URL$ 

Given $k$ affine subspaces in $\{0,1\}^n$, consider the problem of testing whether their union covers all of $\{0,1\}^n$. What's the complexity of this problem? P.S.: It seems that this can be reduced to the computation of a characteristic polynomial, which looks like a hard problem. Is there another route? 

In compressed sensing, the goal is to find linear compression schemes for huge input signals that are known to have a sparse representation, so that the input signal can be recovered efficiently from the compression (the "sketch"). More formally, the standard setup is that there is a signal vector $x \in \mathbb{R}^n$ for which $\|x\|_0 < k$, and the compressed representation equals $Ax$ where $A$ is a $R$-by-$n$ real matrix where we want $R \ll n$. The magic of compressed sensing is that one can explicitly construct $A$ such that it allows fast (near-linear time) exact recovery of any $k$-sparse $x$ with $R$ as small as $O(k n^{o(1)})$. I might not have the parameters best known but this is the general idea. My question is: are there similar phenomena in other settings? What I mean is that the input signal could come from some "low complexity family" according to a measure of complexity that is not necessarily sparsity. We then want compression and decompression algorithms, not necessarily linear maps, that are efficient and correct. Are such results known in a different context? What would your guess be for a more "general" theory of compressed sensing? (Of course, in applications of compressed sensing, linearity and sparsity are important issues. The question I ask here is more "philosophical".) 

Another number theory example, similar to the ones above. It's known by Bertrand's postulate that for every positive integer $n$, there's a prime between $n$ and $2n$. But we have no polynomial time algorithm currently to find such a prime, given $n$. (The desired algorithm must run in polylog($n$) time.) One can easily come up with polynomial time randomized algorithms because of the prime number theorem, and one can derandomize them by assuming some standard number theoretic conjectures (such as Cramer's conjecture), but no unconditionally polynomial time deterministic algorithm is known. Related work was recently done in the Polymath4 project; Tao's blog post on the project is a good summary of it. 

PLANAR NAE $k$-SAT is in P for all values of $k$ . Reason is that we can reduce PLANAR NAE $k$-SAT to PLANAR NAE $3$-SAT. Let $\phi$ be an instance of PLANAR NAE $k$-SAT, and suppose $\phi$ contains a clause $C$ with literals $\ell_1, \ell_2, \dots, \ell_k$. Introduce a new variable $v_C$, and replace $C$ with two NAE clauses $C_1$ and $C_2$. $C_1$ contains $3$ literals $\ell_1$, $\ell_2$, and $v_C$, while $C_2$ contains $k-1$ literals $\bar{v}_C, \ell_3, \ell_4, \dots, \ell_k$. It's easy to see that $C$ is satisfiable iff $C_1 \wedge C_2$ is and that the transformation preserves planarity. Now, we can repeatedly apply this procedure on the clauses to eventually obtain an instance $\phi'$ of NAE $3$-SAT as desired. 

This construction does NOT give pairwise independent variables (indeed, $|Y_{i,j}| = |Y_{i,j'}|$ below) as asked by Anindya, but it gives pairwise uncorrelated variables which is enough for getting good concentration bounds for the sum through Chebyshev's inequality (and this is many times the end goal). For each distinct pair $(i,j) \in {[k] \choose 2}$, let $Y_{i,j} = |X_i| \cdot \sigma(X_i X_j)$, where $\sigma(\cdot)$ is the sign function. It's clear that each $Y_{i,j}$ is a normal variable with mean 0 and variance 1. To see that they are orthogonal, for $(i,j) \neq (i',j')$, note that $$\mathbb{E}[Y_{i,j}Y_{i',j'}] = \mathbb{E}[|X_i X_{i'}| \cdot \sigma(X_i X_{i'} X_j X_{j'})]$$ which can be easily checked to equal 0 by looking at the various cases of possible equalities between $i,i',j,j'$. P.S.: A previous version falsely claimed pairwise independence. 

To start off, there is of course Arora and Barak's book Computational Complexity: A Modern Approach. From there, parts 3 and 4 of Jukna's book Boolean Function Complexity: Advances and Frontiers make excellent reading material. Also, Ryan Williams teaches a nice course on circuit complexity whose course notes might hopefully be put up online :) 

The objects you are searching for are called seedless affine dispersers with one output bit. More generally, a seedless disperser with one output bit for a family $\mathcal{F}$ of subsets of $\{0,1\}^n$ is a function $f : \{0,1\}^n \to \{0,1\}$ such that on any subset $S \in \mathcal{F}$, the function $f$ is not constant. Here, you are interested in $\mathcal{F}$ being the family of affine subspaces Ben-Sasson and Kopparty in "Affine Dispersers from Subspace Polynomials" explicitly construct seedless affine dispersers for subspaces of dimension at least $6n^{4/5}$. The full details of the disperser are a bit too complicated to describe here. A simpler case also discussed in the paper is when we want an affine disperser for subspaces of dimension $2n/5+10$. Then, their construction views ${\mathbb{F}}_2^n$ as ${\mathbb{F}}_{2^n}$ and specifies the disperser to be $f(x) = Tr(x^7)$, where $Tr: {\mathbb{F}}_{2^n} \to {\mathbb{F}}_2$ denotes the trace map: $Tr(x) = \sum_{i=0}^{n-1} x^{2^i}$. A key property of the trace map is that $Tr(x+y) = Tr(x) + Tr(y)$. 

Shafi Goldwasser communicated to me that she and coauthors have been investigating exactly such algorithms for number-theoretic problems! The following is known: 

According to me, "computer science" is the "science of all sciences" :) What is "science"? We get data from nature, and we try to construct a model that explains the data. Also, we assume implicitly that nature is not arbitrary. The laws of nature must have a concise expression, the data must satisfy some symmetries, etc. But this is exactly a learning problem! The data is generated by some process that is promised to be of "low complexity", and our task is to reconstruct a description of the process. Our understanding of such problems is at such a primitive level that it's your duty to work on them! :) Even our understanding of the seemingly simpler problem of whether the output of a black-box process is equivalent to some fixed function is far from complete. For example, suppose that we are promised that the black-box is evaluating a function that can be computed by a small-depth arithmetic circuit (this is easy to explain to high-schoolers), and we want to find out whether the box is computing the zero function. We don't know if this can be done in the lifetime of the universe for functions on reasonable sized domains! Cue to start talking about arithmetic complexity theory, the chasm at depth 4, the role of randomness in computation, what's known if we reduce # of multiplication gates, etc. etc. ... 

The notes from Salil Vadhan's class, "Pseudorandomness", are excellent for this purpose. He is writing a textbook based on these. The draft version of the book is available online. 

Your problem is well-understood in the case $S = \{0,1\}$. Here, the function $\phi$ can be viewed as the adjacency matrix of a graph $G_\phi$ on the vertex set $V$. And $\phi$ satisfying a triangular property $P$ just means that $G_\phi$ is free of some collection $C_P$ of induced subgraphs, each on $3$ vertices. Now, it is known [Alon-Fischer-Krivelevich-Szegedy '99] that any such property is testable (in the property testing sense) with query complexity, independent of $n$, bounded from above by a tower of exponentials of height polynomial in $1/\epsilon$. In particular, when $P$ is 1 only when its input is $(1,1,1)$, the property corresponds to triangle-freeness in graphs for which the best upper-bound is the tower of exp's I mentioned and the best lower bound is slightly superpolynomial in $1/\epsilon$ [Alon '01]. If you want better dependence on $1/\epsilon$ at the cost of perhaps having some dependence on $n$, I don't believe anything much better than what you described is known. The test you described is more or less folklore. For arbitrary fixed $S$, I don't know if the testability question has been resolved. I believe the framework of graph limits, especially the paper [Borgs-Chayes-Lovász-Sos-Szegedy-Vesztergombi '06], might show testability for hereditary properties of weighted graphs which would imply testability of your triangle properties. But I don't know this fact for sure. 

For an excellent introduction to forcing in set theory, there's Timothy Chow's famous USENET post "Forcing for dummies" as well as the more formal paper that arose from it, "A beginner's guide to forcing". 

Yes, there is. Ben-Aroya, Regev and de Wolf in "A Hypercontractive Inequality for Matrix-Valued Functions with Applications to Quantum Computing and LDCs" (FOCS '08) provide a non-quantum proof of the exponential lower bound for 2-query LDC's, although there are similarities between the quantum and non-quantum proof techniques. Here's a rough sketch of their proof. Let $C: \{\pm 1\}^m \to \{\pm 1\}^n$ be a $2$-query LDC. Then, for a given $x \in \{0,1\}^m$, set $M(x)$ to be the rank-1 matrix whose $(i,j)$'th entry is $C(x)_i C(x)_j$. $M$ is a matrix-valued function, and we can look at its matrix-valued Fourier coefficients: $\widehat{M}(\alpha) = \sum_{x} M(x) \prod_{i \in \alpha} x_i$. But now, observe that $\widehat{M}(\{i\})$ computes the matrix whose $(j,k)$'th entry is the expected correlation of $x_i$ with $C(x)_j C(x)_k$. Thus, if $C$ is a $2$-query LDC, we know that for every $i \in [m]$, many entries in $\widehat{M}(\{i\})$ are larger than some $\epsilon$. This allows you to lowerbound the $p$-norm of $\widehat{M}(\{i\})$. Finally, you can then apply their matrix-valued hypercontractive inequality, which is the main theorem of the paper, to get a lowerbound on $n$.