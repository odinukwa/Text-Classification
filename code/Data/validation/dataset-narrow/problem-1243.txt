then it can be used to break pseudorandom generators. This is what the natural proofs barrier is, informally. We argued that 1. is very reasonable for many approaches to lower bounds, without 2. the complexity measure seems useless, and 3. is based on the observation that we have been able to turn most combinatorial existence proofs into efficient algorithms, and on the intuition that an inherently non-constructive proof is a hard one to devise. You can make the above more concrete by coming up with very efficient pseudorandom generators. If a function can be computed inside a complexity $C$ that looks pseudorandom to functions from a class $C'$, then a measure computable inside $C'$ is doomed for lowerbounds against $C$. 

If you have a graph with maximum degree $\Delta$, then the greedy algorithm finds a coloring with $\Delta+1$ colors, so for $k = \Delta+1$ the assumption that you are given a proper $k$-coloring does not change the complexity of the problem. It is known that independent set is NP-hard to approximate within factor $\Delta^{1 - o(1)}$, and Unique Games-hard to approximate within factor $\frac{\Delta}{O(\log^2\Delta)}$, which, for $k=\Delta$ is within a $\log \Delta$ factor from the upper bound you cite. (I prefer to write approximation factors as bigger than 1, so I am thinking of the upper bound you cite as $\frac{k}{2}$.) However, it's still interesting to close these logarithmic gaps. This paper by Bansal may contain useful ideas. 

First: all your inputs are either 0 or 180, and the midpoint gate always gives a point on the arc between its two inputs. So any intermediate value stays on the arc between 0 and 180, there is never wrap-around, and we may just assume that each input is a bit and the gates return $(a+b)/2$. Then there is a gate at the top that tests whether its input is in a range $[c, d]$. Notice that any such circuit recognizes a set of the form $S(a, c, d)=\{x: c \leq \langle a, x\rangle \leq d\}$ where $a$ is $n$-dimensional and $c$ and $d$ are scalar constants. The way to see that is that the input to the top gate is a linear function of the input. In fact your setup is significantly more restricted, but nevermind that. The family of sets $S(a, c, d)$ has VC-dimension at most $2n+2$: halfspaces have VC-dimension $n+1$, and taking all pairwise intersections of sets in family can at most double the VC-dimension. Therefore the family of sets computed by your circuits cannot possibly shatter all $2^n$ possible binary inputs, even for $n=4$. There are boolean functions on $4$ bits that a circuit of this type does not compute. 

What about the following. I claim that solving the feasibility problem $\exists? x: Ax \le b$ reduces in strongly polynomial time to finding a linear separator. Then it's easy to reduce linear programming to the feasibility problem. First notice that you can reduce the feasibility problem to the special case $b=0$: just transform the constraints to $$ Ax + tb \le 0, \\ t \le 0 $$ This just adds a row and a column to $A$, and a new variable $t$. Now, after modifying the problem in this way, create a data set which has one point labeled $0$ for every row of $A$, and is equal to that row. It also has the point $0$, labeled $1$. The linear separator, if it exists, will be the solution to your feasibility problem. Another way to say this is that certifying if 0 is in the convex hull of a set of points is equivalent to linear programming, in the sense of a strongly polynomial time reduction. 

One usually thinks about approximating solutions (with guarantees) to NP-hard problems. Is there any research going on in approximating problems already known to be in P? This might be a good idea for several reasons. Off the top of my head, an approximation algorithm may run with a much lower complexity (or even a much smaller constant), might use less space or might be much better parallelizable. Also, schemes that provide time/accuracy tradeoffs (FPTAS's and PTAS's) might be very attractive for problems in P with lower bounds that are unacceptable on large inputs. Three questions: is there anything that I'm missing that makes this obviously a bad idea? Is there research going on in developing a theory of these algorithms? If not, at least, is anyone familiar with individual examples of such algorithms? 

The 3SUM conjecture was proven false here, so it is no longer believed that the problem has a quadratic lower bound (unless it is strictly harder than 3SUM, see next point). Unlike most problems in the 3SUM-conjecture paper, co-linearity was only proven to be at least as hard as 3SUM, a reduction in the other direction was not given. That is, it still may be the case that co-linearity is strictly harder than 3SUM. Therefore, the sub-quadratic algorithm for 3SUM cannot be immediately used for the co-linearity problem. 3SUM references only address the problem of checking whether at least 3 points are co-linear. It is not clear to me how to reduce the problem of counting the maximum number of co-linear points to that without a multiplicative linear factor. 

I can find algorithms that satisfy any two of these but I can't find any that satisfy all three. Does it exist? Is it proven not to exist? 

The problem: given a set of points in the Euclidean plane, find the maximum number of co-linear points. I already know that the problem can be solved in quadratic time using hashing or projective duality (as described here). I also know that checking whether three points are co-linear was proven to be 3SUM-hard under sub-quadratic reductions here. There are a few issues however: 

I am unsure what you mean by "non-trivial", but how about this. $L = \{0^{2^k} | k \geq 0\}$. This language is not regular therefore, any TM deciding it must run in $\Omega(n \log n)$. The simple algorithm (crossing every other 0) is optimal. 

My question is: is there any sub-quadratic algorithm known for the problem? If not, do we have a good reason to believe that such an algorithm does not exist? It is known that finding an $O(n^{2-\epsilon})$ algorithm for 3SUM is still open. So I am, at best, hoping for a logarithmic improvement over the quadratic algorithm. EDIT: I am also (but not exclusively) interested in the case when the coordinates are integral. 

figure which tableau cell it corresponds to if it corresponds to the first $|x|$ entries of the first row or the first entry of the last row of the tableau, output the clause if it corresponds to any other cell, we're dealing with the same constant-size 3CNF, and we only need to compute the variable labels for it, which can be done based only on the index of the tableau cell 

By the Ky Fan minimax theorem, if you can identify the strategy spaces of the two players with convex compact subsets of a locally convex topological vector space, and the payoff function $M(x, y)$ is convex and continuous in $y$ for any $x$, and also concave and continuous in $x$ for any $y$, then the minimax theorem holds. 

In section 2 of $URL$ the authors define a model of MapReduce that is intentionally structured like BSP. They prove simulation theorems as well. May be a good place to start. 

At least among regular bi-partite graphs, Ramanujan graphs provide the optimal approximation of the complete bipartite graph. Let's say that a graph $H$ $C$-approximates a graph $G$ if $tL_H \preceq CL_G$ for $t$ the smallest real number such that $L_G \preceq tL_H$. (I.e. $t := \|L_H^{-1/2} L_G L_H^{-1/2}\|$ where the norm is the operator norm and inverses are taken w.r.t. to the space orthogonal to the all-ones vector). The eigenvalues of $L_{K_{n,n}}$ are $\lambda_1 = 0, \lambda_2 = \ldots \lambda_{2n-1} = n, \lambda_{2n} = 2n$. For any $d$-regular bipartite $H$, the eigenvalues $0=\mu_1 \le \ldots \le \mu_{2n} = 2d$ of $L_H$ come in pairs $d \pm x$. By Alon-Boppana, $\mu_2 \le d-2\sqrt{d-1}$, and, therefore, $\mu_{2d-1} \geq d + 2\sqrt{d-1}$ (ignoring lower-order terms). This implies $C \geq \frac{d + 2\sqrt{d-1}}{d - 2\sqrt{d-1}}$, which is achived by a $d$-regular Ramanujan graph. 

This is equivalent to the biclique partition number of a bipartite graph. You can think of M as representing a bipartite graph $G$ on $[n] \times [m]$ in the natural way: $M_{i,j}$ is 1 if and only if there is an edge $(i,j)$ in G (where $i$ is an element of the left partition, and $j$ an element of the right partition). Then $M$ has binary rank $r$ if and only if the edges of the corresponding bipartite graph $G$ can be partitioned into $r$ complete bipartite subgraphs. To see this, take an optimal factorization $M = UV^\intercal$, denote the columns of $U$ by $u_1, \ldots, u_r$, and the columns of $V$ by $v_1, ..., v_r$. $M=UV^\intercal$ is equivalent to $M = \sum_{i = 1}^r{u_i v_i^\intercal}$, and $u_i v_i^\intercal$ represents a complete bipartite graph on the vertices $S_i \cup T_i$, where $S_i$ is the set of left vertices for which $u_i$ is the indicator vector, and $T_i$ is the set of right vertices for which $v_i$ is the indicator vector. Computing the biclique partition number is NP-hard, and hard to approximate. See these two papers for some results and references: [1], [2]. 

Some classes (e.g. automata) are cryptographically hard to learn. Some classes (e.g. k-term DNF) are NP-hard to properly learn. 

I find the arXiv and the TOC Blog Aggregator too much to follow. If you just read The Complexity Blog, GÃ¶del's Lost Letter, and Shtetl-Optimized, you won't miss much theory news. 

When $f$ is convex, even if it doesn't have a closed form, you can use search methods (on a bounded domain) to find a point as close as you'd like to the local minimum, which will also be the global minimum -- this will work for finding the minimum of the sum, as the sum of convex functions is also convex. There are many other better numerical methods with varying guarantees (depending on the properties of the function) for optimizing convex functions -- this book is a good (and free!) reference. 

These two results seem to imply a hardness result for your problem. On the one hand (1), we can find a large decision tree; on the other hand (2), we shouldn't be able to minimize it to get an equivalent "small" one, of size $s^k$, even when one exists of size $s$. 

One thing to keep in mind is that if you want a job where you actually do research, you pretty much do need a Ph.D. Rarely do big companies these days hire people without a Ph.D. as research scientists. If you decide to do a Ph.D. there are lots of areas where you can possibly focus, and TCS is just one possibility. You can also do a mix of theoretical and applied research or many other options. Whether or not you'll get a job in industry will more depend on their interest in your work (theoretical or not), whether you can sell yourself, the market, etc. Lots of Ph.D.s in TCS work in industrial research, lots work in academia, and lots leave research all together. There aren't nearly enough academic slots for all TCS Ph.D.s to into academia, so an expectation that you do would be unrealistic and generally isn't there (industrial research jobs are competitive too though). Perhaps your advisor might "expect" you do choose one career path over another, but it's your life and you ultimately get to decide what to do with it. 

Let me give a partial answer from a learning theory perspective. As your question isn't well specified, this answer won't be either. In my answer, I'm assuming your question was inspired by your blog post, linked from your profile. Say that you are thinking about programs that are just functions (so they have to halt, etc.). You can ask whether certain classes of such functions can appear randomly by, perhaps, looking at the probability a random program (from some distribution that you think is likely) lands in that class or not, with the hope that probability is polynomially large. I haven't really thought this argument through. You can also ask whether such a class is efficiently evolvable according to Valiant's model of evolution (also in @Artem's pointer in comments): luckily what is efficiently evolvable is known to be the class learnable by correlational statistical queries; taking "crossover" into account, you get parallel correlational statistical queries. One thing to note is that just because evolvability is characterized, it is still a separate and sometimes difficult task to determine whether a particular class is evolvable (learnable with CSQs) or not. If you find a class of "programs" that is neither randomly occurring nor evolvable, perhaps you can conclue it has a "creator/programmer," though that conclusion may still take a leap of faith. 

any set system of $m$ sets has an $\epsilon$-net of size $O(\frac{1}{\epsilon} \log m)$; an $\epsilon/2$ net of an $\epsilon/2$-sample for $\mathcal{S}$ is an $\epsilon$-net for $\mathcal{S}$; 

I have a feeling you're looking for a model that reflects the engineering meaning of "real time", but let me just point out there is something that theorists call real time (and is inspired by engineering "real time"). Look at this post by Lipton and the references there: $URL$ 

Let $w = x - y_i$. Let $v$ be the vertex of $P$ that maximizes $w^Tz$ for $z \in P$ (can be found by LP). Find the point on the line through $y_i$ and $v$ that is closest to $x$ (this is quadratic optimization in a single variable). This point is $y_{i+1}$. 

Just to add to Domotor's answer, any hyperplane whose normal vector has polynomially bounded coefficients (in fact bounded by $2^{o(n)}$) contains an entire face (i.e. subcube) of the cube of dimension $n - o(n)$. This follows from the Sauer-Shelah lemma. To be precise, say each $|a_i| \leq n^c$. Then for any $I \subseteq [n]$, $\sum_{i \in I}{a_i} \in \mathbb{Z} \cap [-n^{c+1}, n^{c+1}]$. So by averaging there exists some integer $b: |b| \leq n^{c+1}$, for which $\mathcal{I}_b = |\{I: \sum_{i \in I}{a_i} = b\}| \geq 2^{n-1} n^{-c - 1} = 2^{n - o(n)}$. Then according to Sauer-Shelah, there exists a set $S \subseteq [n]$ of size $n - o(n)$ shattered by $\mathcal{I}_b$, i.e. for each $T \subseteq S$ there exists $I \in \mathcal{I}_b$ such that $I \cap S = T$. This means that for any $\epsilon \in \{-1, 0, 1\}^S$, there exists a $\delta \in \{-1, 0, 1\}^n$ such that $\sum{\delta_i a_i} = 0$ and $\forall i\in S: \delta_i = \epsilon_i$. In other words, the hyperplane normal to $a_1, \ldots, a_n$ contains the entire subcube corresponding to $S$ (i.e. the projection of the full dimensional cube onto the coordinate subspace corresponding to $S$). 

Here is a simple protocol for Hamming distance that uses $O(\varepsilon^{-2} \log n)$ bits. The protocol is essentially the Alon, Matias, Szegedy second moment sketch. Or you can think of it as a version of the Johnson-Lindenstrauss lemma. I am assuming that Alice has a vector $x \in \{0,1\}^n$ and Bob has a vector $y \in \{0,1\}^n$, and they share randomness. Notice that the Hamming distance between $x$ and $y$ is equal to $\|x-y\|_2^2$. Using the shared randomness Alice and Bob sample a $\varepsilon^{-2} \times n$ matrix $\Sigma$ of IID unbiased $\pm 1$ random variables. The Alice sends $\Sigma x$ and Bob outputs $\varepsilon^{-2}\|\Sigma (x - y)\|_2^2$. For the analysis notice that $\mathbb{E}[\varepsilon^{-2}\|\Sigma (x - y)\|_2^2] = \|x-y\|_2^2$, so the output is an unbiased estimator of $\|x-y\|_2^2$. Because the rows of $\Sigma$ are IID, $$ \mathrm{Var}[\varepsilon^{-2}\|\Sigma(x-y)\|_2^2] = \varepsilon^{-2} \mathrm{Var}\left[\left(\sum_{i=1}^n{\sigma_i (x-y)}\right)^2\right], $$ where $\sigma_1, \ldots, \sigma_n$ are IID unbiased $\pm 1$ random variables. Then, by Khintchine's inequality, for a constant C $$ \begin{align*} \mathrm{Var}\left[\left(\sum_{i=1}^n{\sigma_i (x-y)}\right)^2\right] &\leq \mathbb{E}\left[\left(\sum_{i=1}^n{\sigma_i (x-y)}\right)^4\right] \\ &\leq C \mathbb{E}\left[\left(\sum_{i=1}^n{\sigma_i (x-y)}\right)^2\right]^2 \\ &= C\|x-y\|_2^4. \end{align*} $$ So, by Chebyshev, with constant probability the output of the protocol is a $1+O(\varepsilon)$ approximation to $\|x-y\|_2^2$, which is equal to the Hamming distance.