"integers are cheaper to compare than characters, because character sets and collations (sorting rules) make character comparisons complicated. Integers are very good when using as an index column." - HottestGuide 

Your justification for why you separate the tables is not convincing to me. I suggest you consider table partitioning rather than table splitting into 2 tables. You did not provide enough information, so here is a guess. The reason it is running slow is probably because you don't have the correct indexes defined. You should at least have an index defined on each table for mainDateTime column. Sorting can be provided by adding ORDER BY to the result set. Other reasons for bad performance may be memory, heavy table access by other users, row size, table table fragmantaion, locks, etc. 

For the database to be able to guarantee uniqueness of a row, it needs to search the table to make sure that the new incoming row has a unique key. This can only be practical and efficient, in large tables at least, when there is a structure called an index defined on that table that can be searched quicker than searching each row of the table in question. The same index could be used to locate rows by user queries. The index can be built on 1 or more columns of your choice, even on non-key columns to speed the search or to prevent duplicate rows. Sometimes designers would use artificial keys such as auto-incremented keys of data type int for example, instead of long composite keys containing long strings of data. This increases the search speed but still adds an extra index that could affect the insert/update slightly. The shorter the key, the more keys could be loaded and searched in memory, hence improving the overall response time. In your case, you could, use such an artificial key to enhance performance. To guarantee uniqueness, you will still need a unique index defined on your composite key. 

Nested views are non-deterministic, so we cannot index them Each view references multiple s to build the strings Each UDF contains nested s to get the ISO codes for localized languages Views in the stack are using additional string builders returned from s as predicates Each view stack is treated as a table, meaning that there are / / triggers on each to write to the underlying tables These triggers on the views use that stored procedures which reference more of these string building s. 

Say for example, if a defect is reported in this spaghetti mess, for example a column may not be accurate in certain specific circumstances, what would be the best practices to start troubleshooting? If debugging this was a zen art, how should I prepare my mind? :) Are there any preferred SQL Profilier filter settings that have been found useful in debugging this? Any good tactics, or is it even possible to set breakpoints in nested procs when using debug mode? Tips or suggestions on providing meaningful debug logging when dealing with nested string builders? 

Is there a best practice method to handle localized strings via a view? Which alternatives exist for using a as a stub? (I can write a specific for each schema owner and hard-code the language instead of relying on a variety of stubs.) Can these views be simply made deterministic by fully qualifying the nested s and then schemabinding the view stacks? 

I have a table that stores version information in multi-dotted strings, ie '4.1.345.08', '5.0.1.100', etc. I am looking for the quickest way to find out which is larger. Is there a faster, or easier way on the eyes than what I have come up with below? One caveat is that I am 90% sure that we should ONLY find dotted numeric values here, but I can't guarantee that in the future. 

First, I connect 2 tables with 1 to many relationship. Workbench correctly creates FK column in the child table. 1-When I change the name of the PK in the parent, the old FK name remains unchanged in the child. 2-The name generated for the fk is not following the standard pattern set in the options (even when using the defaults). Any suggestions to keep names in sync? (besides me performing the manual change)? Note: I am using Workbench 6.3 Community Edition. 

I suggest you follow normalization rules. This will minimize the write and update processing (and errors) and simplifies coding of the logic of write and update routines. You can tune reads in many ways. You could use materialized views or indexed views to keep track of counts without queering the records one by one. Don't sacrifice normalization for no obvious reasons. 

Do you know of an industry standard that is designed to describe the metadata contents of a relational database (or part of it, such as tables, columns, etc.)? Do you know if a tool exists that does export the metadata of any of the top 5 RDBMS tools in any format such as XML (without having to type in DDL in the syntax of particular database)? Thank you. 

A relational database would typically contain several related tables. The relationships between tables use what is called a foreign key column which is basically the primary key of a parent table placed in a child table's list of columns (with other rules not listed here). For simplicity, you can say that relationships in a database depend on Primary keys very very much. Now if a value is changed, the system has to perform so many changes to coupe with this change. As an example, say you have a Social Security Number of a person and you keep track about this person's properties, cars, jobs, wive(s), kids and let's say you change the SSN value. The system will have to change the corresponding value in each of those related (child) tables. This process is not always good to perform on-line. Another reason is business related. Say you are issuing an EmployeeID Cards, if the number on the card changes, then the card is no longer usable and you have to print another one, not only that, but this means that the salary information has to change, the insurance information has to change as well as other information related to the EmployeeID. In business, sometimes not all this information is integrated or even automated, so this change leads to a lot of work and possible inconsistency of information in manual systems. Such changes may not only be localized to one organization and may have to affect other external organizations or systems, which would make life very difficult, because you have to ask your business partners to carry such changes on their end too. Yet, another reason for not changing key values is that when you do so, history tracking becomes difficult. All history logs and documents will not reflect the current value making it difficult to interpret or even find data in manual systems. 

It sounds like you need to add another column in your data table to account for the quantity of ingredient. In the example you linked, it appears that the site calculates how many grams each ingredient adds to the total sum of the final product. But it will be up to you to decide how best to generate these. For example, if your site is all about home made juices, perhaps you can come up with a formula to determine how potent or how much flavor a specific ingredient adds to the final product. But I digress. If you have an additional column for grams you could take the weight of one ingredient, calculate the SUM(Weight) of all ingredients in your recipe and divide the two together, like in this Excel example. 

While profiling a database I came across a view that is referencing some non-deterministic functions that get accessed 1000-2500 times per minute for each connection in this application's pool. A simple from the view yields the following execution plan: 

I recently inherited a MESS of a search. It's around 10,000 lines of code in the procs/functions alone. This search is aptly named the "Standard Search." It's a proc that calls about 6 other procs, which are entirely composed of string builders, in which each proc has between 109 and 130 parameters. These procs also call deeply nested functions which generate more strings to be assembled into a final query. Each proc can join up to 10 views, depending on the logged in user, which are abstracted from the table data by between 5 and 12 other views per primary view. So I am left with hundreds of views to comb through. On the plus side, it does have a parameter to PRINT out the final query, (unformatted of course!) It's a hot mess. The string builders also have no formatting, and they don't logically flow. Everything jumps around everywhere. Although I couid do an auto-format, to pretty print the code, that doesn't help with the formatting on the string builders, as that would involve refactoring the content of strings, which is a no-no. 

If there is no backup and no way to restoring the lost datafiles, what you can do is backup any other important datafile/tablespace and recreate the database. I think it will be the less painful way to get a fully working database. 

Question: Is overwriting the PFILE with the database ON dangerous? Answer: No. The PFILE is read only when the database starts, and that's if no SPFILE is there or if the PFILE argument is passed when invoking the startup command. Otherwise, when you start the database, the SPFILE is read for initial memory values, control files location and such basic information. After that, everything will remain in memory. Note: If you change the PFILE at the OS level, you need to bounce the database to make the changes take effect. On the other hand, using the ALTER SYSTEM SET ... SCOPE=MEMORY may be used as @ibre5041 suggested and achieve the result you want. Nevertheless, you're "losing" those archives from the original location, which makes me think you could as well add the NOLOGGING clause and avoid the generation of the logs that are filling up your storage. So my personal suggestion would be to use the command 

To complete some of the previous answers: A commit will end the current transaction in the current section. It will guarantee the consistency of the data that was "touched" during the transaction. A checkpoint writes all commited changes to disk up to some SCN that willl be kept in the control file and datafile headers. It guarantees the consistency of the database.