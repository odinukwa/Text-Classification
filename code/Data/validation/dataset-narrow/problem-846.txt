You're best bet would be to set something up like Squid as a transparent proxy to capture all of this traffic. There are a number of things you can do with pfSense like this to set it up as a sniffer or special purpose appliance if a proxy really just can't be configured in your network. But from the sounds of it you may want to look into tightening down that network a little more with a hardware or software firewall of some sort which will give you some of these functions. I have done this at a couple offices using Squid and it's worked out very well. 

I'm trying to get a little more usefulness out of my MySQL backups. I'm wanting to gzip my backup when it's completed from a cron job. Here's the script I have so far to do the backup. 

I've seen this happen in Vista when the users profile was corrupted causing it to build a temporary profile every login. Just removing the profile folder from the Users folder corrected the issue on the next login when it created a new user profile correctly. Might also be caused by GPO writing the profile to an alternate profile location. Have you tried any troubleshooting on it? 

Not sure if they haev a VB build or not, but I've used YAF (Yet Another Forum) fourm in the past and it has worked out very well. It's also open source and has a very active message board for questions. 

Not knowing your PC's specs it mgiht make a huge difference. I was running Win2003 Standard on a desktop box, Core 2 Duo e6400 (if I remember correctly) with 4gb of memory and a Raptor X 150gb drive. When I moved to 2008 Ent it decrease my boot time by about 20 seconds and overall performance of my app went way, way up due to the increase performance in the way is handles memory. Stability is about the same. I never really had a crash on the 2003 install, and have gone 136 days with no reboot on the 2008 install. Only reason it rebooted was due to a change in roles. I've not gotten the chance to install R2 on it yet, but am sure the new improvements will only add to the stability and responsiveness of the box. 

As an in-house IT person myself I would have to say go for in-house. With that size of a user base it should be enough to keep some one always working. Even if it's putting up keyboard trays... Outsourcing it means you dont always have the same person in most cases. This can lead to increased costs as it will take time for each person to get to know the envrioment and equipment. And at $75 to $200 an hour for outside help that will add up quickly. 

Sucess!!! Since I'm using cheap-o hardware as the modem, I just borrowed another one from another computer in the office! I mean come on, like we still use modems, what is this 1999? Never the less, the new modem works correctly. Looks like it was just a hardware problem, still weird that it was seen in device manager as a working modem though.... 

It's possible to just add two websites to IIS and use different Host Headers as splattne said. This is the easiest way to do it. Then just add the "site name" as an cname record in DNS and point it to your A name. IIS tree: Web Sites -> Server (host header of 'server.mydomain.tld') -> StudentNet (host header of 'studentnet.mydomain.tld') DNS Tree A Host Host '@' Points To '111.222.333.444' CNAME Host 'server' Points To '@' Host 'studentnet' Points To '@' Edit: Not sure what happened to my formatting. It looks so nice in edit mode. 

Since it's on the intranet doing it in IIS would be the only way to do it as far as I'm aware. If it was an internet facing site you could do it at the firewall level and leave the internal port what ever you wanted, just so long as the firewall knew what port it was. Also using host headers can help if you have multiple web sites inside IIS. But so you know, you can only change the port for the entire website in IIS. It's not possible to change the port for just an application or virtual directory under the root. So in other words you can change www.me.com to www.me.com:89. But not www.me.com/you/ to www.me.com:89/you/ and have it not affect your entire website in IIS. 

Personally I have a 12gb (and growing) database that has a nightly backup. I keep 5 days on disk each in their own file and copy them to tape nightly with a month or so retention. I also have transaction logs shipping every hour, each to their own file. I keep 3 days of these around, but never copy them to tape. If I ever upgrade the storage on the tape I might start this as well, though it's most likely not needed (IMO) as you have the nightly backups for in between. As Peter said it's good to keep the backups on disk for at least a few days to make restores quicker and easier, not to mention restores into a secondary database for debug reasons easier. Also good to have a good length of retention for the tape. Not so much an issue for me, but good for if you have delete capabilities in your application and the client has an 'oops I deleted something... a month ago that I need today, can you get it back for me?' type moment. Edit: In answer to the comment and extenting my answer a bit here. I use a second/third SQL Job to manage the files for me. In SQL 2005 I had issues managing the files where it would not delete correctly so I wrote some code (VBScript) to do it for me on a scheduled task as far as deleting it. But in SQL 2008 either they fixed it, or I just did a better job configuring it and it deletes my old backups, old log shipments, and even another one to keep the DB healthy. I then use some VB script to copy my files to "tape" which I really should rewrite in C# the next time I get bored. I'm using the IOMEGA REV drive/disks as my "tape" and they are working out very well. 

You would need some app in IIS to receive the byte stream. You can use web services for this type of file transfer. Then it would be up to your service layer to save them in the desiered location. Also you would need it to set that folder to have write access and most likely be working in Full Trust mode. You could check out this app and see how he's doing it in Silverlight from an asp.net page. $URL$ 

If you are using an Antivirus scanner and trying to transfer large files it may be locking the file. You may want to see if you can exclude that folder from the scan. 

Well if it's just to do testing under IIS 7 you now have the option for using IIS Express and run on XP or better. Just a thought if your doing it for dev reasons. 

I know you stated not putting them in a database, but I'm an advocate for this where appropriate and I'm thinking here it is. Any way you store files you are going to be taking on some amount of overhead, it’s just how much your will to take is the issue. By storing files in the database you can further limit who can see them by completely removing them from the file system. Under the right control and programming you can limit the files to have access by only your application, thus eliminating the need for another server and tons of file system security changes not to mention an additional backup plan for that additional “secure” server. Another added benefit of storing them like this is you can encrypt them and store them, as well as have a simplified backup plan. 

Then just create a new website and point it to the corresponding domain name. And again use host headers in your Bindings to specify the different domain names. There is no need to create a website pointed to the webRoot folder, or even have file in it as it's only used to house your different domain name folders that are application specfic. 

I'd say set up one folder and have all your domains as folders inside, as I think you already have. So your folder structure will look like this 

I've used DoubleTake for the last year or so. This has allowed us to replicate 70 to 90 (count growing monthly) databases to a DR fairly quickly and with a high compression. As Farseeker stated it allows a block level replication which works quite well. It also allows for a queue for those times where there is a heavy transactional load. The only issue I really see for your instance is how heavy are these databases? I have at least 5 databases over the 10gb mark with a high volume of transactional data during peak hours and jobs, around 1 to 3gb per day per database. In all transferring upwards of 800+gb over the wire monthly (compressed is MUCH less of course). Since adding an additional 20+ databases in the past 15 months I've seen excessively high IO related to disk queueing. So it may be an issue trying to use something like this if most of your databases are very busy. You may end up with missing data or the receiving end if you have to much queued up and your link goes down. Of course this would only matter for a DR deployment. But keeping a close eye on it and tuning the bandwidth will help greatly with keeping the queue down. 

Just for clairity I'm wondering what the best practice is for giving the IUSR_ account write access under IIS 6.0 to a folder. I gave searching on Google a try and on here a try but nothings really pointing me in either direction. From my understanind giving that account write access to a folder in an Internet site would be a bad idea. This site would be a mix of Classic ASP and ASP.NET. Any one want to chime in on this?