Another approach This is a fairly poor analogy but here it goes anyway. What if I told you to flip through a deck of cards and tell me if there are any Jokers? Which approach do you think is better? 

Problem Background Essentially this problem is asking you to remove all palindromes of even length from the string as they are first encountered. So becomes and NOT even though both and are even-length palindromes. Naming Unfortunately, I cannot think of a succinct name for what operation this function actually does. You could go with but that is a bit much. I would just stick with and document the function with some commented examples. Method The good news is that you do have the right solution by using a stack. The bad news is that you are using some unnecessary intermediary data types for processing. Note that you do not need to put the data into an actual stack. Instead you can just use the output string as the stack. By eliminating these intermediary data types and processing steps, the code is much more readable. Here is a char array solution: 

So you decide that should save the changes. It might as well have been . Using either one, it's not clear from the code that the other data are saved as well. If you surround the code by a UoW, it's clear that the UoW commits the changes transactionally. I guess this is also the piece of code that raised your question 

Now you get only one record per show in the result set and only the required fields are included in the . You'll still see a number of joins, but they don't cause any widening of the query, as s do. 

First, you can copy an object easily by getting it with and then -ing it to the context. Even child objects will be added: So, leaving the Id values for a moment, the core of the copying code could be nothing but: 

You obviously want to benefit from both strands of data reduction. As for the reduction in number of rows, there's no way to make Entity Framework join with local data other than lists of primitive values. Even then, joining is rather inefficient because EF has to convert the local list into a temporary SQL table (sort of), which requires a considerable amount of code. It's more efficient to use , which translates into an statement: 

The principle of the dynamic range here is that if we have some palindrome \$a_1 \cdot b_1\$ already and we are trying to find another palindrome \$a_2 \cdot b_2\$ such that \$ a_2 \cdot b_2 \gt a_1 \cdot b_1\$ where \$b_2 < b_1\$ then that implies \$a_2 > a_1\$. You can add a break condition as Dennis does however you will find it doesn't have much affect on this algorithm unless the palindromes are sparse. 

This has a major impact on the performance of your original code, but as you will see we can do better. Problematic Optimization Flattening the 2D array to 1D sounds like a good idea until you factor in the cost of post-processing. When the array is processed as 1D the border pixels change values and, in fact, exhibit border-wrap effects. We often fix problems like this by wrapping the matrix inside a false border - i.e. we add a \$1\times1\$ border all around the matrix. Unfortunately this would not work here. Instead, you can fix this by applying the filter and then going back and zeroing out border pixels each iteration. More Optimizations 

To prevent these errors you'd want to ensure that whenever is to be executed is eagerly loaded (using ). Two options: 

You are combining an with a local sequence, . It is possible to do that, but Entity Framework1 needs a lot of code to convert the sequence into something that behaves like a SQL table. I'm sure this is a major performance killer. From what I see, you can do without this local sequence. You know the begin and end date. So it's possible to get the forecasts between those dates. If you group those forecasts by their , you achieve the same grouping: 

Having these filters added to the model builder in , you'll see that whenever you query any of these entities, the filter will be part of the SQL query. Also when using s. If you want deleted items as well, just disable the filters in a context instance: 

In short, this would mean that each step in a workflow would be represented by a service method. The UI just issues commands and reads results. It doesn't contain any business logic whatsoever. 

The next step is to realize that if is our \$m \times (n-1)\$ difference matrix, yields a \$1 \times (n-1)\$ matrix of column means. If we truncate columns of this matrix as we do when we take submatrix it does not affect the means of the columns that are still there. So instead of performing over and over on submatrices, we can again pull it out the loop, perform once on the entire matrix , and take submatrices of this mean matrix. So now the code could look like this: 

Performance Scalability is always what I look for in problems like this. Yes a may not be that big these days but why limit ourselves with inferior algorithms that do not scale well? Consider you have the number $$n = 123456789012345678901234567890...$$ Let's just say this number has \$m\$ digits. You will end up performing \$2m\$ multiplications and \$m-1\$ additions. However, of those multiplications, you can only have \$10\$ unique values since there are only \$10\$ digits. You should therefore pre-compute the powers so that you do not have to recompute them over and over. 

is a materialized list (for example, ). After running , a new enumeration of will produce the same, modified, objects. is an enumerable that produces new objects on each execution. After running , a new enumeration of will produce new objects. The changed objects are out of scope and will soon be garbage collected. 

... you would pull all records from the database before the actual join is made. By returning , this would turn into a SQL query containing a and (obviously) far less traffic. 

Since it may not always be clear if the program control flow will end up calling the method it may seem sensible to load the collection always. But that would require any old time s are loaded, even when they're not needed. Load right before is going to be called (if possible). 

Where's Commit? No, I didn't forget the method. The fact is, repositories shouldn't commit. Maybe this is surprising, but if you think about transaction management it becomes obvious. There may be business transactions in which several repositories are involved. When each repo has the potential to save all changes, it may be very hard to figure out which one is able to save at the right moment. That's the reason why generic repos always come with a Unit-of-Work pattern. This is all explained well enough in the link above. You can see this problem lurking in your code. At the end you have 

Bugs in nonContiguous If your input string contains spaces then your function produces arbitrary output regardless of whether you are correct or not. 

You'll notice that if increases by , these column differences remain the same. The only thing that changes is that we obtain one additional column difference in . Hence you do not need to perform the function multiple times. Simply perform it once on the entire \$m \times n\$ matrix to yield a \$m \times (n-1)\$ matrix of column differences and take submatrices of this difference matrix. So the original code you posted: 

The code runs much slower with these bug fixes. Quick Optimization The algorithm cannot be performed in-place so you are using to store the output of applying your filter to . But then you want the output to be placed back in so you perform a deep copy. However a swap of the pointers would suffice so you could use this instead: 

To accomplish the dynamic range we need to have a strict ordering. Here I will always assume \$b \geq a\$ 

All other pieces of code that wrap this part can be deemed redundant. As the ultimate (over) simplification you could even write this code directly in an MVC controller's action method. No added layers involved and the job is done. Useful layers? Anything added on top of this base line should be carefully considered. Additions should be useful, not restrictive. Your proposed architecture is restrictive because it is "vertical". You seem to have a column of abstractions for each entity: , (as ), with subclasses like . Then there is a class, maybe part of a similar column. This architecture has the same drawbacks as Data Access Object: it will lead to multiple isolated queries and repetitive code. Alternatives? This columns-per-entity setup defeats the purpose of an OR mapper like Entity Framework, which is to work with object graphs that map to a relational data model. When you need orders and their related customers you can get them in one LINQ query. Likewise, when you want to save orders and customers, you can add them to the context and do one call to save everything in one transaction. This has made me move to API-oriented architectures. I usually create services that live for the duration of one web request. Each service has a number of methods that execute some business case, like creating orders. For this, the service has one context instance that can pretty freely be used inside the service methods. This works best in combination with dependency injection (or Inversion of Control, IoC), but that's not a prerequisite. This is, very briefly, what it could look like: