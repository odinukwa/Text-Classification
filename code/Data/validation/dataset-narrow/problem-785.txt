In Sonicwall firewalls, regardless of whether you have multiple internet providers or just one, you have to set at least one interface in the default LB group in , as shown in this screenshot: 

So, If you wan't to gather enough data to troubleshoot blocked/dropped ports issues you will need to setup either a GMS/Analyzer (Which displays lots of information in a graphical console), or your favorite syslog daemon in a server. The procedure to enable a syslog server is the same as adding an GMS/Analyzer appliance: $URL$ Update: To get that level of detail with SonicWALL, you definitely will have to deploy a Syslog server. If you don't want to see anything else besides dropped/blocked packets reports, make sure to go to Log > Categories and uncheck all the fields except Network Access. To have an idea of what kind of information you can expect to find in your syslog server, take a look at this filter: 

The article that you pointed out does not necessarily apply only to domain environments. As noted at the end: 

Besides BIND Views, the new version of Windows Server (Which is in a technical preview currently) also has a feature called DNS Policies which looks very promising for achieving what you are looking for. 

You can accomplish this by editing the registry. In , create a DWORD entry called and set the value to . Also, you can use the URLRewrite module or install URLScan, more about this in the following article: $URL$ 

You can either wait a couple of hours until the new rule takes effect, or force it restarting the Transport Service. 

In your case, it seems like the package installer has already taken care of creating the dedicated user. 

If the virtual machine is not registered on the ESX host and you want to search its configuration file, run this command and press Enter: 

By default, Postfix only listens in the localhost interface. You must edit and change the line to and restart Postfix 

You can use the real time monitor and filter to match your desired interface to view how much bandwidth is using that interface, and which applications are using that bandwidth. 

You can use Nagios plugins to monitor the server via IPMI. Also, Supermicro has it's own monitoring software called SuperDoctor, which also relies on IPMI. More Details: $URL$ $URL$ ftp://ftp.supermicro.com/utility/SuperDoctor_II/Linux/ 

You can now active domain services directly for IaaS VMs using the Azure Active Directory Domain Services (currently in preview). This can potentially replace the need to deploy full ADDS VMs in your Azure virtual network. $URL$ 

Fingerprints (and other biometrics) on Windows are designed to replace the PIN on the machine and are keyed to the machine's TPM individually. The point is that if your fingerprint is stolen, it could not be used to access resources using a different machine. There might be some third party tools that would allow you to do what you want, but that's not by design with Windows/Active Directory. 

You might want to take a look at Azure File storage, which will allow other Azure servers to access files via SMB, without needing a server to host the share - $URL$ 

You might want to look at the features of MS Intune. It does a lot of what you seek, particularly with the remote device management, VPN access and security. If you sync on-prem AD with Azure AD, it will help with the credential management too. 

You have the right idea with your diagram. DCs should point to other DCs for DNS, ideally in the same site (if you have multiple sites). Your workstations should then be pointing to a DC in their site for DNS resolution. You will see slow logons as a symptom of DNS issues. Just about every problem with AD tends to be DNS related, at least in my experience... it's always the first place to check! 

Printers and desktop items are set by the USER policy, not by the MACHINE policy. So make sure that your policies are set to apply to the appropriate scope and have the appropriate permissions. If you have a policy that is set to be processed by machines, but it only change user-specific items, it the policy will apply, but no changes will result. 

I have used Azure-to-Azure site recovery, which is currently unsupported, in test and lab scenarios. It's totally doable, but you would be assuming some risk with the unsupported aspect of it. As of now, you can't "fall back" to the original region, it only replicates in the one direction. 

You can also try resizing the CPU in the portal, which will likely bounce the machine to a new host. That could also resolve your issue. 

It sounds like the tar file does include those files. If you are actually removing the directory, ie after you do the rm then "ls websites" shows that it's been deleted, then the files must be in the tar file. 

Here is a good paper on what ICE is, and what it does. Basically ICE is a inter process communication protocol, with authentication, protocol negotiation and potentially multiplexing built in. It allows two X clients to talk directly to each other, for example, a video player program could potentially talk to a jukebox program to update each other. As Richard Holloway says, the .ICEAuthority file is for authentication. It contains a number of random cookies. If two programs have the same cookie, then they're allowed to talk to each other. In practice this either means that they're reading the same .ICEAuthority file, or the cookies have been added. In a lot of ways it's similar to the xauth program & the .Xauthority file, except that .ICEAuthority is used for client to client, while .Xauthority is for client to server. 

You have a bad installation somehow. It is conventional on Unix that a program exists in /usr/sbin/sendmail$, which acts as an interface to the local MTA. This MTA is rarely sendmail nowadays, but other MTA's have a compatible program which is installed here. sendmail.h would be part of the source code for sendmail, and sendmail.0 and sendmail.8 would be the man pages, so it looks like you've somehow got the source code for the right sendmail in /usr/sbin You need to clean this up, and get a proper sendmail program into /usr/sbin/sendmail to fix php. Since you've not told us the distribution, nor which MTA you're actually using, no-one else can help you. $ = Actually one or more of /usr/lib/sendmail and /usr/bin/sendmail as well as /usr/sbin/sendmail, programs will either search these paths, or have one or more hardcoded into them, or ask at installation time. It looks like your program is using /usr/sbin/sendmail. Regardless of which, the installation for the MTA will do it. 

It depends totally on what you're trying to do. A static website? Probably no problem at all? A website where each page requires a large amount of CPU & memory before it can output the results? You're going to have problems. Is it mainly plain text? Does it output a 1Mb graphic for each user? The best way to know is to use benchmarking software to find out what the site's requirements are, and by extension, if it can handle the load you're expecting. Here is a list of different programs. Loadrunner will without doubt do it for you, but I'm sure there are others which will work just as well. 

When you find out if the space that can be reclaimed is substantial, you are ready for the next step, running Optimization Jobs: In Windows Server 2012 Data Deduplication, Optimzation Jobs perform the work of deduplicating data and optimizing a volume. These jobs can be run on-demand (manually) or on a scheduled basis. You can trigger an optimization job on-demand in Windows PowerShell by using the Start-DedupJob cmdlet. For example: 

The iptables rule is fine, but according to nmap's output I don't think that you have any service running in that port. Confirm that by running the following command: 

You can give selective DNS responses based on location with BIND Views if you are using BIND as your external DNS server. The Technical Preview of the new version of Windows Server also has a feature called DNS Policies which looks very promising. To serve content based on client location and other criteria such as User Agent or schedules, F5 has an appliance called Global Traffic Manager which used in conjuntion with their load balancers achieves what you are looking for. In Cloud environments, Amazon's Route 53 can accomplish the same. In order to keep the data in sync you must have a storage backend capable of do synchoronous replication, or use the replication provided by MySQL, which will keep the replicated data consistent. 

You should reach Machine A's port 5900 when connecting to the server's localhost port 6001 with this command: 

Although some of these options are just security through obscurity, they are still better than nothing, and a nice layer of defense when used all together. 

Assuming that both webservers are in the same network (not distributed across sites/countries), the best solution will be to put a Load Balancer device in front of the web servers, one of the best ones out there is F5 LTM, but as this is an expensive appliance, I would recommend haproxy, which is an open source software package compatible with most *nix based systems. Some useful resources for your particular scenario: Zero-Downtime Restarts with HAProxy Easy web server Load-Balancing with HAProxy Creating a maintenance page for your site So, you can put a Linux server with haproxy in front of those servers to load balance the incoming requests. 

Just because you are logged into the Windows 2008 machine, it doesn't mean that the focus of your ADUC tool is that server. You can be logged into one DC physically, but be connect to the directory located on another. Make sure to confirm which DC you are connected to before attempting the change. 

Under the configuration settings for the AD domain, you need to make sure that it's configured to allow devices to be joined. There is more information here - $URL$ 

It appears you can use Azure Express Route without a VPN, but your access would be limited to the using the public peering routes, not the private peering - $URL$ You can't initiate access from Azure to your on-prem via public peering - only the other way around, which I don't think meets your needs. 

As of March 2016, the ability to protect ARM VMs with Azure Backup is now in public preview. You can find more information at $URL$ 

With the classic portal, only co-admins can access a subscription. The Azure Resource Manager features of the new portal, you'll be able to more finely control the access within a subscription. The official documentation is here - $URL$ 

The ability to protect ARM VMs with Azure Backup is now in public preview, you can find more information at $URL$ 

If you download the file, it just contains some text and dates related to diagnostics updates. I can't find any documentation specifically about what reads or writes to that file and seems to be updated on the fly. It seems totally safe to delete when you are getting rid of VHDs. I delete them myself. 

You are correct, in order to use Azure AD you must become a "tenant" within the system. So a tenant is basically just securing a .onmicrosoft.com sub-domain. At that point you would have one account registered in your Azure AD. From there, you can activate Office365, Intune or any of the Azure services. 

Your spns need to be added the service accounts, not the local administrator. ADSIEdit can be really helpful in figuring out where they go. 

If you are looking to run multiple websites using Azure, it's not necessary to use IaaS. You can run up to 10 web roles for free - $URL$ 

It's confusing to try to understand what your problem is, you don't make it clear if you're checking a file on web1 on web1, or db1 on web1, or whatever. Please give a good description of what's you're actually seeing, without confusing the issue with copying and chmoding. Something simple like I create a file on db1, with permissions x:y and on web1 I see permissions a:b and on web2 I see permissions c:d. First thing, using NFS, any file which is owned by root will usually be shared so that it's owned by nobody. This means that if you have root on the client machine, you effectively don't have root on the server. I think that explains some of what you're seeing. Secondly, if you are running NFS, it's vital that the userid->username mappings are identical on all the servers. Unix filesystems only store a numeric id for userid & groupid, which are then mapped to usernames by programs like ls. Are you sure that they are all in sync? It could be that you've got a mismatch. Finally, tar p is an option for extracting, not creating tars. It's ignored when creating tars, and even when it's used, it's not going to set the ownership to what they originally were. -p basically means, ignore the umask. Tar will create files owned by you only, unless you're root. 

You should give a full path, because you don't know the context that it will be executed in. If your program exits with 67, then this will be bounce the message as unknown user, 0 will drop the message. Anything else will be retried until the message times out and bounces. Be careful of security - you're basically allowing anyone on the Internet to run a program on your system, so don't trust user input, and sanitize it before you use it. 

You can also use the screen program, and pressing ctrl-a h will write the screen to a file 'hardcopy.n'. 

There isn't any way to match upon text, but you can forward all messages to an address to a program. You need to add an alias to your system aliases file, usually /etc/postfix/aliases or to the user's .forward file. The first option has the better flexibility, cause you can have an aliases for an address which doesn't actually map to an account. The alias should be something like