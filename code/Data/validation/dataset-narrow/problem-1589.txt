The downvotes are because of the topic, but I'll attempt to answer your question as best I can since it's here. Data science is a term that is thrown around as loosely as Big Data. Everyone has a rough idea of what they mean by the term, but when you look at the actual work tasks, a data scientist's responsibilities will vary greatly from company to company. Statistical analysis could encompass the entirety of the workload in one job, and not even be a consideration for another. I wouldn't chase after a job title per se. If you are interested in the field, network (like you are doing now) and find a good fit. If you are perusing job ads, just look for the ones that stress statistical and informatics backgrounds. Hadoop and SQL are both easy to become familiar with given the time and motivation, but I would stick with the areas you are strongest in and go from there. 

From our perspective on here, the big benefit of S3 is the ease of accessing the data from within EC2. Google Drive is directly accessible from the Google Cloud platform. There are a host of other differences that might matter depending on your usage requirements, but that's the one that would matter most around here. The only other difference I can think of that would matter to the DS community is that when you are sharing something, you have no control of the address of a given file on google drive. 

Use consecutive search strings from users (short-to-long-tail searches) as training data for a machine-learning algorithm. Run searches that occur > N amount of times a week against that recommendation algorithm. Validate and clean results. Push them out to the general population in rotation/A-B testing. Track click-throughs. Refine results over time. 

There are a couple of open source options I know of - LibOTS - $URL$ DocSum - $URL$ A couple of commercial solutions - Intellix Summarizer Pro - $URL$ Copernic Summarizer - $URL$ And this one is a web service - TextTeaser - $URL$ I'm sure there are plenty of others out there. I have used Copernic a good deal and it's pretty good, but I was hoping it could be automated easily, which it can't - at least it couldn't when I used it. 

This is not an issue. Here, what you'll want to see, is if for instance, for each vector with label x, there is a vector with label y close to it. To do that, you could use the Earth Mover's Distance which gives you a single score when you are trying to see how far two "groups of things" are. 

As far as I understand, it is a simple autoencoder, meaning that all it does is trying to map the input into another space, so no fancy training, just some plain feed-forward and backprop. This is why it's rather fast to train. If you want to use pre-trained embeddings, you can do it that way 

Let's say that I have sparse feature vectors and I'd like to use dimensionality reduction in order to visualize them more easily. What if I have some prior knowledge on the colinearity between my features? As in, I would be able to create an approximate distance matrix between my features, and therefore between my data points. I am aware that if features are actually colinear, methods like PCA will find a way to reduce them, however, I am afraid that I don't have enough data points to infer that colinearity strictly from the data. Let's say that my data looks like something like this: $x_0 = \{1,NaN,-1,NaN\}$ $x_1 = \{1,NaN,NaN,-1\}$ In my case, I know that the 3rd and 4th features are colinear and that the distance between $x_0$ and $x_1$ is close to zero. Therefore, they should be mapped to a very similar data point in a lower dimensional space. Can PCA still do it with so few data points? Are there ways to "force" the known colinearity or distance measures? T-SNE perhaps? 

Let's say that I want to create a pseudorandom number generator, and I'd like to make sure that it's truly close to random. For simplicity, let's assume that I want to output a 0 or a 1 randomly. I have heard of the monobit, runs, poker test etc. but are there "machine learning" ways to evaluate a pseudorandom number generator? As in, one could try to predict the number that will be outputted given the first k numbers that were previously outputted, and the performance of that model would give you how well the pseudorandom generator is performing. It might be way over my head, but could a generative adversarial network learn a truly pseudorandom generator that way? 

Not all government data is listed on data.gov - Sunlight Foundation put together a set of spreadsheets back in February describing sets of available data. 

Validate values (String length tolerance, data type, formatting masks, required field presence, etc.) Range correctness (Does this seemingly correct data fall within expected ranges of values) Preliminary processing (If I attempt to analyze this data, can I perform the basics without running into errors) Preliminary reporting (run a report against a data set and ensure that it passes a sanity test) Defining null vs. empty vs. zero vs. False for any given column of data Identifying data that is out of place (numeric values dramatically different than other values in a data set, string values that look like they might be misspelled, etc.) Eliminating or correcting obviously errant data 

People see something closely related to the Travelling Salesman Problem and think that it can't be solved. A good deal of work has been done on this topic and not all of it indicates that a solution is not available. Depending on the parameters and the desired solution, you may be able to find something that will work. You may want to give a look at the OpenOpt python library. Another resource to look at would be the TSP Solver and Generator. If you are using R, there is a TSP package available. Actually implementing a solution to your problem is a little too much to cover here, but this should provide a good starting point. Within these packages and at the documentation within the links that I provided for you, you will find that there are a fairly wide variety of algorithmic strategies available. You have a small geographic region and a small set of "salespeople", so the computational power needed to calculate a strategy within a reasonable time frame should be available on your desktop. In practical terms, you don't need to find the absolutely most optimal strategy. You just need a very good one. Pick a TSP package that looks the least overwhelming and give it a go. 

Twitter uses Storm for real-time processing of data. Problems can happen with real-time data. Systems might go down. Data might be inadvertently processed twice. Network connections can be lost. A lot can happen in a real-time system. They use hadoop to reliably process historical data. I don't know specifics, but for instance, getting solid information from aggregated logs is probably more reliable than attaching to the stream. If they simply relied on Storm for everything - Storm would have problems due to the nature of providing real-time information at scale. If they relied on hadoop for everything, there's a good deal of latency involved. Combining the two with Summingbird is the next logical step. 

You can do that by using binning. If you want to use K-Means for categorical data, you can use hamming distance instead of Euclidean distance. 

Sometimes your data is "imbalanced" because it doesn't represent the true distribution of the data. In this case, you have to be careful, because you have "too many" examples of one class and "too few" of the other, and therefore, you need to make sure that your model doesn't over-/underfit on one of these classes. This is different than using costs because it might not be the case that one mistake is worse than another. What would happen is that you would be biased and it wouldn't be beneficial for your model if the unseen data doesn't have the same distribution as the data you trained on. Let's say that I give you training data and your goal is to guess if something is red or blue. Whether you mistake blue for red or red for blue doesn't make much of a difference. Your training data has 90% red instances where in real life, they only happen 10% of the time. You would need to deal with that in order to make your model better. 

Let me try to answer your questions point by point. Perhaps you already solved your problem, but your questions are interesting and so perhaps other people can benefit from this discussion. 

Your features could be binary (0 if reason X is not applied, 1 if it is). You could have one for each reason, and even combination of reasons etc. You might run into some issues if you have many possible reasons (your features will be very sparse). If this is the case, I'd look into dimensionality reduction to make your features denser. 

you need to deal with class imbalance if/because it makes your model better (on unseen data). "Better" is something that you have to define yourself. It could be accuracy, it could be a cost, it could be the true positive rate etc. 

This is an interesting problem because obviously, you want to train a model that performs well on unseen data, and therefore you'd like to train it on data resembling the one you'll encounter later on. If you already know the true distribution of your data. I would use what Keith mentioned in the comment. Fit a curve on the histogram of that data, and use the inverse as weight. What you could also do if you know the true mean of your distribution is build some type of "discriminator", which guesses whether a data point will fall on the "left" or "right" of the mean. Since your data is skewed, you could also have a discriminator for the median, which might perform better. Then, you can build models for each "side" of the mean. Since your data is heavily skewed, you will have a lot of data for one, and not so much for the other. Finally, you could have a final model that takes in as input the discriminator(s) (how certain it is that the data point will fall left or right), and the output of both models. That final model should smooth out some of the mistakes done by the discriminator(s) and both models. 

2. Document proposed changes Any changes you want to make to data should be documented so that they can be replicated moving forward. Version control and timestamp the document every time you change it. That will help in troubleshooting at a later date. Be explicit. Do not simply state "correct capitalization problems", state "ensure that the first letter of each city name begins with a capital letter and the remaining letters are lower-case." Update the document with references to any automation routines that have been built to manage data cleansing. 3. Decide on a standard data cleansing technology Whether you use perl, python, java, a particular utility, a manual process or something else is not the issue. The issue is that in the future you want to hand the data cleansing process to somebody else. If they have to know 12 different data cleansing technologies, delegating the cleansing procedure will be very difficult. 4. Standardize the workflow There should be a standard way to handle new data. Ideally, it will be as simple as dropping a file in a specific location and a predictable automated process cleanses it and hands off a cleansed set of data to the next processing step. 5. Make as few changes as is absolutely necessary It's always better to have a fault tolerant analysis than one that makes assumptions about the data. 6. Avoid manual updates It's always tempting, but people are error-prone and again it makes delegation difficult. Notes on manual processing To more completely address the original question as to whether there's a "good" way to do manual processing, I would say no, there is not. My answer is based on experience and is not one that I make arbitrarily. I have had more than one project lose days of time due to a client insisting that a manual data cleansing process was just fine and could be handled internally. You do not want your projects to be dependent on a single individual accomplishing a judgement based task of varying scale. It's much better to have that individual build and document a rule set based on what they would do than to have them manually cleanse data. (And then automating that rule set) If automation fails you in the end or is simply not possible, the ability to delegate that rule set to others without domain specific knowledge is vital. In the end, routines to do something like correct city names can be applied to other data sets. 

The text is obfuscated for you but to the computer, this is as incomprehensible as English for instance. When classifying text, you always first vectorize your input, meaning that your text becomes a vector. This vector typically represents the frequency of the tokens in your vocabulary in that particular string (bag-of-words representation). What you can do is represent each document/string as a vector representing the frequencies of characters, character n-grams, or even words. One you have your input, you can deal with it as any classification task. Naive Bayes has shown to be a good lower bound for these types of tasks. 

Here is my understanding of the difference between a Bernoulli and a Multinomial Naive Bayes: Bernoulli explicitly models the presence/absence of a feature, whereas Multinomial doesn't. 

Due to the curse of dimensionality, less features usually mean high improvement in terms of speed. If speed is not an issue, perhaps don't remove these features right away (see next point) 

Is there a standard way to deal with this? ------------ EDIT: -------------- On Github, I found this: $URL$ This suggests that if a centroid becomes an "orphan", it should be assigned to the point that is the furthest from its centroid. This seems like a sound method, is there any paper or theory supporting this? 

As always, "is A better than B" always depends on what you consider better? Is it accuracy, is it speed etc. The dumb but correct answer to your question is: "try both and see which one is better". Performance of techniques is always to some extent dependent on the data. What you need to remember with word vectors, is that they are learned based on a certain context. If the context that was used by Google's model is similar to yours, you might be better off using their model. But if it's different, you might run into some problems. Just imagine the following case. You have 4 words: King, Man, Queen, Woman. Which pairs of two words would you create? Depending on the context, you could make a case for several 

I am implementing K-Means from scratch and that exercise raised a question. To update my centroids, for each centroid, I have to find the points for which that centroid is the closest. In some cases, especially when the number of centroids is high and the number of instances is low (i.e. k=20 and 100 instances), I find centroids for which no point has them as their closest centroid. In other words, they become "orphans" as no instances are allocated to them.