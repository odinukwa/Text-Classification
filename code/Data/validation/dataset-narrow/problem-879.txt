I went and checked a really old kernel (EL4) and that restriction above was still there. This means its fundamentally not feasible to do what you want to do on your architecture (x86). Given you provided the name of the NAS I did some googling and discovered this: $URL$ Which implies it uses a PPC CPU. 

The important bit is -D which is used for duplicate detection mode. Basically throw that in your startup scripts to check if the address is available before adding it. 

To give you some idea here of what is going on, memory is broken up into zones, this is specifically useful on NUMA systems which RAM is tied to specific CPUs. In these hosts memory locality can be an important factor in performance. If for example memory banks 1 and 2 are assigned to physical CPU 0, CPU 1 can access this but at the cost of locking that RAM from CPU 0, which causes a performance degradation. On linux, the zoning reflects the NUMA layout of the physical machine. Each zone is 16GB in size. What is happening at the moment with zone reclaim on is the kernel is opting to reclaim (write dirty pages to disk, evict file cache, swap out memory) in a full zone (16 GB) rather than permit the process to allocate memory in another zone (which will impact performance on that CPU. This is why you notice swapping after 16GB. If you switch off this value this should alter the behaviour of the kernel not aggressively reclaim zone data and instead allocate from another node. Try switching off by running on your system and then re-running your test. Note, long running high memory processes running on a configuration like this with off will become increasingly expensive over time. If you allow lots of disparate processes running on many different CPUs all using lots of memory to use any node with free pages, you can effectively render the performance of the host to something akin to it only having 1 physical CPU. 

Note: The actual process ID (the main starting thread whose tid == pid) acts as a 'container' for the cumalative total of all threads (existing or no longer existing) CPU times, so gives you a reasonably accurate depiction of a processes entire usage. To explain, if a processes lifetime equals its cpu time it means that for all the time the process has ever lived, its demanded and recieved a CPU to run on. This would equal 100% of cpu utilization. I'm almost certain in reality you're going to find that your processes are going to be using hardly any CPU. So - to reiterate, to perform as efficiently as possible do nothing as your kernel knows how to best prioritize CPU resources to best utilize your system. Anything you could possibly add is in most cases is reducing your overall effectiveness. Unless its your plan to actually cripple the processes in some way (and there are circumstances where you may actually intend to do that) you dont want to use , or to get the best performance possible. 

That will clear out your page cache (which frankly isn't very much), but that might if your lucky clear out the idr_layer_cache slab entries. Storage and page cache nearly always dominates slabs so this is nothing more than a guess to see if that helps really. Then again -- it might not. There is either some internally compiled kernel module, or externally used kernel module using up that memory. If that does work, you can try updating your kernel as the bug may have been resolved at some point in the past (and your kernels future). 

Swapping behaviour and when to swap is determined by a few factors, one being how active the pages are that have been allocated to applications. If they are not very active, they will be swapped in favour of the busier work occurring in the cache. I assume that the pages in your VMs dont get activated very often. 

A page has really nothing to do with this problem. The following command should output a list of pids for httpd sorted by CPU percentage with the current working directory that httpd worker is in. This of course assumes that httpd is the culprit. 

You are going about this in the wrong direction. You can use filesystem semantics to enforce consistency. 

Then, in the access file (default is ) add the three following lines (providing no other lines offer any other security setup). 

Since the strongest one does not fail (hmac sha256) its unlikely you are using the failed ones even if they could be selected. To summarize - 

Based off of the limited data I wonder if you have a firewall between the client and the MySQL server (it may be on the hosts directly) which permits 3306 to the MySQL server but is blocking response packets to the initial SYN request back. You may want to perform a packet dump on both sides to work out what is happening on the TCP side and/or check any firewalls that exists between both sides. 

Never build packages from source and install them over the top of the packages that are there. This breaks your systems package integrity which can lead to strange ABI problems like receiving or messages. It is pretty critical the system maintains a reliable and accurate index as to what software has been deployed on a given system to ensure that it all works with each other properly, this is the reason we use RPMs in the first place. The viable (and Redhat blessed) way to go about resolving this is to use software collections. www.softwarecollections.org It installs is software and its 'new' dependencies in its own root. This can make it slightly harder to apply the package in your environment but does protect your system from weird errors or problems. It also installs the packages in their own namespace, letting you install multiple versions of a package in parallel. The website gives instructions how to install and activate these packages, it contains most of what people miss on older versions of CentOS and Redhat (EL6 in particular). Some things I've used from this website successfully. 

Following on from the link in you're reference, it suggests you are trying to mount with NFS as the root, in which case you should never end up landing in this function at all. In which case: 

Yes, at the cost of maintaining more file descriptors which is probably a very cheap cost in practical terms. Setting up a connection and closing it incurs 5 system calls (open and connect on client, accept on server, close on client/server) which with maintaining the connection is avoided until necessary. 

You can lock the pages into memory in later versions of libvirt:- $URL$ Careful: This doesn't appear when using Fedora 19 as a hypervisor, nevertheless according to the changelog for the latest RPM (I can find) for EL6.5 libvirt this exists; 

The parameter is controlled via the kernel command line, using setterm merely alters the runtime settings. To disable it system wide you can alter your kernel boot command line by appending it with "consoleblank=0" in your boot configuration (grub/lilo). If something during boot setterm's it then it will override the value. 

Commonly known, disk activity can inflate load. The load can be artificially inflated if lots of processes are bound to one CPU which is being waited on. Tasks with a very low priority (niceness) will often wait a long time inflating load by 1 for that particular process. 

Also, you did not setup any routing relationship between the child namespace and the parent namespace so it wouldnt even work pinging the host directly. In general, using the generic broadcast address for anything aside from provision of fundamental network services is not a good practice. You should use the broadcast address of the subnet you are aiming for. I got all of what you mentioned working doing the following for the network namespace to prepare. 

There is no netfilter library I am aware of that helps you manipulate the rules. There is an internally driven library that is used instead though. IPtables inherits a rather archaic method of speaking to userspace -- you open a SOCK_RAW IP socket to communicate with it. This is totally going to be removed (as it makes no sense) with nftables which will speak over netlink to do the same thing. 

I am willing to bet you know that the SAN has shifted the beginning of the physical disk off by a few bytes. I've seen this before. Its a bitch to get your files off of a disk that has done this but it is possible. If you run 'fdisk -l' do you get messages about the starting cylinders on the device not marrying up? Its usually in brackets around each partition declaration. Do you manage to find the LVM groups but not the disk itself? Is the LVM device made up of multiple SAN disks and just one is affected? The following script is going to try to search for the correct offset on /dev/sdb where your lvm partition starts. No guarantees it will find anything. If it does, you might be in a good position to recover your data. 

Lazy Copying This C program demonstrates lazy copying occurring, in this scenario, all processes map to the same region of memory, but the children have overwritten the contents. In the background the kernel has remapped these pages to different locations in real memory, but show the same virtual address space. Now, each instance actually does take up memory but the RSS value remains constant. 

In linux, IP addresses have a notion of 'primary' and 'secondary' addresses. The primary is typically the first address you add to the system. Removing the primary address has the implicit operation of flushing the entire list of secondary addresses also. You can avoid this behaviour by setting the sysctl to 1 like so: 

No, keepalives prevent the remote side timing out. Since the host knows the status of both sides of the connection a keepalive is redundant. 

I would suggest trying the HOME_DIR option first (which is the actual way in policy you should implement it) or alternatively use a local override. 

A fifo may not work so nicely for you. There must be a reader on the other side, else the writer will block. Additionally once the reader is no longer reading the fifo the writer will exit with a broken pipe. can help you mostly, but it comes at the cost of possibly losing some data. 

Well, your load probably hovers around 3 to 4 because half the running processes you have probably dont complete within 5 seconds (load is measured internally every five seconds). 

Again, roughly equivalent. Your speed, in all these cases comes from the fact that the RAID acts like one single disk with ten write heads, so you can send 1/10th of the work in parallel to each disk. Whilst I have no hard numbers to show you, my past experience tells me that increasing your disks performance is not quite so simple as getting more capacity. Despite what the marketing people tell you is innovation, before the start of cheap(er) solid state disks there has been little significant development in the performance of spinning media in the last 20 years, presumably theres only so much you can get out of rust and only so fast we can get our current models of disk heads to go. 

This is because mod_fastcgi uses select() as a multiplexing option. Select is pretty bad for this stuff, the man page specifies for select; 

If it is purely about performance, then you may find it more beneficial in your case to spend the money on more RAM (~16G) and allowing the page cache to save all static data read in from the webserver from disk to ram. It would be a few orders of magnitude faster. 

Personally, I find the current setup somewhat lacking. LXC seems more at the forefront -- certainly more maintained. 

If you are updating it a LOT then the files contents being stale is not likely to be an issue for you. If so, stick it on tmpfs, truncate the file on update and rewrite into it. That would be the cheapest method as it is not likely to use disk at all. The next closest is to truncate/write onto a filesystem that has the noatime mount option set and journalling turned off. But again its risky if you crash out as you might possibly lose the data. After that its noatime again with journalling turned on. Remember, linux buffers writes and syncs to disk at determined intervals so you wont normally 'feel' the impact of a write from an I/O standpoint (unless its very heavy writing but thats tunable). You can alter the conditions to sync to disk too so you can have the write buffer fill up for quie a long time before syncing to disk. If your after doing something really clever.. use fallocate to preallocate space for the file which would be its maximum possible value. Then, mmap open the file to read it directly to memory. Then rewriting will be near instantaneous (but lossy if you had power loss). You can then control when to flush back to the disk with the msync call. 

Whats not so obvious is what happens during register. Another module called also built into the kernel receives notifications of new algorithms being inserted. Once it sees a new registered algorithm it schedules a test of it. This is what produces the output you see on your screen. When the test is finished, the algorithm goes into a TESTED state. If the test fails, I imagine (I couldnt find the bit that produces this behaviour) it isn't selectable for searching if you pass the right flags. Whether or not the test passes is definitely internally stored. In addition to this, calling the psudeo random number generator causes a list of algorithms to be iterated of prngs in order of strength as dictated by this note in