You can do this with a function that runs in the security context of its owner. Function that refreshes the view (create it with the user that owns the MV/table): 

You can do this with a hierarchical recursive CTE. The only problem is that you have a loop in your data, due to there being , and , . 

You have 2 options to fix this. Option 1 - create the intermediate table without a primary key (best option!!): 

Unfortunately the maximum length a or string can be in Oracle is 4000 bytes. The only option available to you if characterset conversion pushes you over this limit is to convert the columns to use the datatype, but we warned - s are difficult to deal with and can present challenges. 

This is a resource issue. The DB server cannot satisfy your queries due to the client host being configured incorrectly as-per the pre-requisites for an Oracle database client installation. Ask your system administrators to verify that they have set the required number of open files, semaphores, shmax etc etc. Link to the Oracle documentation - I assume Linux, but Solaris docs are a link away from that. 

Or update a field like comment_no in shares(posts) table? I have fields like shares_no,comments_no,likes_no. Everytime a user click the likes_no field will be incremented and other fields as I explained. This way I have to update every record each time a user post a comment or shares a post or likes a post. Should I use and so on for getting the record count or just updating a record. There will about 10 to 20 posts each second. Site's traffic will rose. Which approach should I use? Which one is more expensive? EDIT: I've used as below to speed things up: 

I have enabled SSL on with optional SSL connection: . I have used in order to obtain SSL certificates. SSLs works as expected and in file I can see that: 

is just 4,688! It is not much compared to the total data of the collection which is 30M documents. When Mongo gets slow when it has domino effect, CPU usage and Memory is not high. Mongo just uses 40% of the memory. Disk partition is if that helps. Another example of a very slow query in full details: 

connects directly to the database without using networking (TNS. This type of connection is called a bequeath connection. connects through the listener. 

Now, the best way of working out if a row has moved is to look at its . The for a given row is based on the location of the row in a datafile and block. Therefore, if the changes it means the row has moved! Documentation link. So, create a table to hold the rowids of the existing rows in the table, and insert them: 

The grant isn't needed if you create the directory logged in as the streams admin user (usually strmadmin). 

.. creates a functional index on the table, that uses the function on the column . In simple terms, Oracle creates an index that pre-calculates the value of , thus making any lookups faster. The query you have posted: 

Basically, is causing this, because . One of the columns in your self-joined table will be all s. Here's a little test case that shows why this can happen. Naughty equality and the way works, picking column names to join on for you. Setup: 

I want to count correct col upon ct_id(course) col. I queried a lot but gained no success. How to query such a thing? for example in SELECT I should have two columns: -correctNo -incorrectNo for each course(ct_id). Note: The last column(correct) is a derived column: IF(selected_answer=answer,1,0) as correct 

I want to fetch those users that is not in a specific user's circle (Like G+). It means that user has not yet added those people to his/her circle. I tried the below query but it was empty: 

I want to insert about 14000 records in a junction table, but the problem arise when there is a duplicate key for unique(iq_id,q_id)? what do do? 

Why is this happening? P.S.: I should note that Horde_groupware database have innoDB tables, when everything is messed up and I I get the error says bad information in .frm file. 

There is about 100,000 records in ips table. Is there any query to the following scenario: Check if is equal to , if it's equal then add country.coutryid to that record. I couldn't think of any way to do it. Do you have any idea how to do that? 

There may also be tables in the tablespace that can be purged. The most likely culprit is usually old AWR snapshots. You can query the data dictionary view (, filter on ) to get a better idea of why the tablespace is growing. 

(The "*" in the above output denotes the maximum precision). is not a data type that can be used in table columns. It can only be used as a PL/SQL variable datatype. 

In other words, one of your control files might be corrupted. They are supposed to be identical. To fix this, we'll try each control file in turn to see if the database starts OK with it. Follow this step-by-step, to the letter. MAKE SURE YOU BACK THEM UP, AS BELOW. You have been warned. Make a backup of BOTH control files to a location of your choice. EG: 

Another thing to mention; You're going to be limited to the 4000 character limit, but will be OK unless you have tables with hundreds of columns. 

I have DB with about 30M records in a collection with about 100GB collection size (total documents and indexes). I have a compound index that filters data based on user_id and some other fields like: , , etc. With I see slow queries of about 10s, 20s or even 40 seconds! I ran the exact same query and result is fetched less than 500ms (though it may get cached on second try). When I get that ongoing stat, I see the following lock status: 

I want to get one post(e.g p_id=8) and all of its comments. how to do that? How to get count(*) from comment table while your getting your post record? 

I want to fill this table with lots of data. Let's say millions of records. How to do this? I need to produce unique emails. Different data for emails. I know I can use but don't know how to produce random data 

When there is a unique key in a table (not ) and procedure is running, on duplicate key error the whole process will be halted. I want to resume on error and call the procedure again. The procedure: 

The easiest way to do this is to add another step prior to the end of the chain that changes the job to start execution of the chain at . Something like this: 

What you are experiencing is called caching. The database doesn't have to go to disk the 2nd time because it can either get the data from its own buffer cache, or the operating system/disk array can also provide the data faster from its own cache. In order to see whether Oracle fetched the data from disk, or used its cache you can enable autotrace in SQL Developer. You'll get something like the following: 

The encryption process is atomic, and no data will be lost. The only way you'll lose the data is if you lose/destroy the wallet key, or drop the table/column yourself. 

The answer is very simple. The prefix in executes the following text as a command in the local shell. In your case it's executing the Unix command locally. Query to check the name of the machine you're connected to. There are ways to execute local Unix commands via the database, but it's seen as a huge security hole, so I won't document them here. ssh (assuming Unix) into the database host instead. 

I've read in an article about social networking database schema and saw that they've used varchar for those columns that I mentioned in the question title. It's not normalized! Isn't it better to have a table for sports and then put the foreign keys in user's profile table? In this case we'll have much less redundancy? Please correct me if I'm wrong. Which approach is better have another tables for fav_sports,fav_videos,fav_music, etc or put them all in user's table? 

I've seen so many times that we use numbers like 5,6 or 10 at most. What is this concurrency? Doesn't it refer to number of simultaneous users in bench? EDIT I'm not talking about any specific benchmarking tools, I mean in general. In benchmarks for databases OR for apache servers we use low concurrent numbers. why is that the case? 

Problem solved: It should be an unknown option as I've put it below [mysql] not [mysqld]. I put all the parameters below and I went through the process again and now it works just fine. files are in their respective database folder. 

The reason for the truncation is quite simple. Some characters (accented ones, for example) in the WE8ISO8859P1 character set are stored as a single byte, but in AL32UTF8 they end up being stored as multiple bytes. As a result of conversion, a 4000 character string may end up actually requiring more than 4000 bytes. By way of example, this query shows you that the Euro symbol (0x80 in WE8ISO8859P1) becomes 2 bytes in AL32UTF8: 

Next, check to see how much space can be freed from each of the existing tablespace files without moving the current database objects. The following (taken from Ask Tom & assuming a block size of 8k) tells you how much space you can currently free from each datafile, in megabytes: 

Also, in Oracle, the owner of a schema (hence the schema name) is the user name. Obviously, grants can be given to other users to perform DML/DDL on other schemas, but you still have to login with a user with sufficient rights. (Proxy logins exist, as does , but both are beyond the scope of this answer). 

As option separates table files instead of putting all data and indexes of DBs into one ibdata file, is using this option improve speed of alter table? I have a table of 40M rows and when I alter a specific table it takes about 5 to 6 hours. Does this solution help? Is there other ways around to improve alter table speed on heavy tables? 

I changed hostname from to . Moreover SSL keys are generated for . Mongo is now up and running with . 

Why it reports that ? Is there something in between that I have missed? Could someone shed some light on this? 

When we index a timestamp field like timetime. Can we use it for or is it just usable for OR ? Does mySQL use this index when doing vise verse ordering? 

EDIT 2: The output of in order to remove extra information based on read and write: $URL$ Another result which reached around 1000: $URL$