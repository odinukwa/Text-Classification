Finer distinctions between these categories are pointless. Other metrics like the number of issues/articles per year, are utterly irrelevant. A long time to publication is frustrating, but also ultimately irrelevant; this is only an issue of latency, not throughput. Assume it will take a year and plan accordingly. Meanwhile, you're already published your results in a good conference, right? And you've put a full version of your paper on the web, right? Ultimately, your work is not going to be judged on the venue in which it's published, but rather on the impact it has on the research community. There are crap papers in JACM, and there are groundbreaking papers in write-only journals. So where should you aim your paper? Be respectful but brutally honest with yourself. If you really have a once-in-a-lifetime breakthrough result, send it to an elite journal. If you have a good paper that will be interesting to a large segment of some intellectual community, send it to a good journal. If you have some decent results that could be published somewhere but only a handful of people would care about, I suppose you could send it to a write-only journal, but why bother? In other words, stop worrying about how to play the game and just do good science. 

The best published results all appear in a 1997 paper by Jianer Chen, Saroja P. Kanchi, and Arkady Kanevsky. 

There are several data structures that support edge insertions, edge deletions, and connectivity queries (Are these two vertices in the same connected component?) in polylogarithmic time. 

Once you have one such surface map, larger maps with the same girth and degree can be generated by constructing covering spaces. 

Klein's algorithm computes a compressed representation of all shortest-path trees rooted at vertices of the designated face $f$, in $O(n\log n)$ time and space. This representation allows us to retrieve the shortest-path distance from any vertex on $f$ to any other vertex in $O(\log n)$ time. An explicit representation of these shortest-path trees would require $\Theta(kn)$ space, where $k$ is the number of vertices in $f$; in the worst case, this space bound is $\Theta(n^2)$. Klein's observation, at least intuitively, is that the shortest-path trees for two adjacent source vertices are nearly identical; his data structure stores only the differences between adjacent trees. Klein's algorithm works just fine for bidirected and undirected graphs. For purposes of the algorithm, a bidirected graph is just a directed graph where the reversal of every arc is another arc, and an undirected graph is jut a bidirected graph where every arc has the same weight as its reversal. In other words, a bidirected graph is a directed graph whose adjacency matrix is symmetric, and (in this context) an undirected graph is just a directed graph whose weighted adjacency matrix is symmetric. Klein's algorithm requires no modifications whatsoever to handle these special cases. Klein's algorithm does not compute shortest-path distances between arbitrary pairs of vertices; in every pair, the source vertex must lie on the designated face. For a solution to the more general problem, see Sergio Cabello's paper "Many distances in planar graphs" [SODA 2006 and Algorithmica 2012]. You might also check out my upcoming journal paper with Sergio Cabello and Erin Chambers, which describes a different multiple-source shortest-path algorithm, which also works for higher-genus graphs. 

Let $C:\{0,1\}^{n} \to \{0,1\}^{2n}$ be an error correcting code with linear distance. Let $g: \{0,1\}^{n} \times \{0,1\}^{n} \to \{0,1\}$ be a function whose randomized communication complexity is large (say, $\Omega(\sqrt{n})$ or $\Omega(n))$. Define $f: \{0,1\}^{2n} \times \{0,1\}^{2n} \to \{0,1,*\}$ to be the partial function that on codewords of $C$ outputs $f(x,y) = g(C^{-1}(x),C^{-1}(y))$, and it outputs $*$ if at least one of $x,y$ is not in $C$. Clearly, the communication complexity of $f$ is equal to the communication complexity of $g$, and $f$ satisfies the property that for every two different inputs on which $f$ outputs 0 or 1, the distance between them is linear. 

Is it possible that $\overline{SAT} \in NTIME(\exp(n^{0.9}))$ ? Are there interesting consequences of such containment? Would it contradict the Exponential Time Hypothesis? 

Claim: If $\delta$-random restriction of $f$ has decision tree of size $O(1)$ (in expectation), then the total influence of such $f$ is $O(\delta^{-1})$. Proof sketch: By definition of influence we have $Inf(f) = n \cdot \Pr_{x,i}[f(x) \neq f(x+e_i)]$. Let us upper bound $\Pr_{x,i}[f(x) \neq f(x+e_i)]$ by first applying a $\delta$-restriction, then picking $i \in [n]$ among the remaining coordinates, and fixing at random everything except for $x_i$. Now, if $\delta$-restriction reduces the decision tree of $f$ to size $O(1)$, then in particular the $\delta$-restriction of $f$ depends on $r = O(1)$ coordinated. Let us now pick one random unfixed coordinate (among $\delta n$), and fix all others randomly. Since the $\delta$-restriction of $f$ depends on at most $r$ coordinates, we get a function (on one bit) that is not constant with probability at most $\frac{r}{\delta n}$. Therefore $Inf(f) = n \cdot \Pr_{x,i}[f(x) \neq f(x+e_i)] \leq \frac{r}{\delta}$, as required. Remark: The claim above is tight by taking a parity function on $O(1/\delta)$ bits. 

It is a standard fact that if $f:\{-,1,1\}^n \to \{-1,1\}$ is a function of Fourier degree $d$, then its Fourier coefficients are multiples of $2^{-d+1}$. In particular, every non-zero coefficient must be at least $2^{-d+1}$ in absolute value. Therefore, by Parseval, there are at most $2^{2(d-1)}$ non-zero coefficients, and so the spectral norm of $f$ is at most $$\sum_{S}|\hat{f}(S)| \leq \sqrt{2^{2(d-1)}}\sqrt{\sum_{S}\hat{f}(S)^2} = 2^{d-1}$$. This bound is tight. For example the complete binary decision tree of depth $d$ has spectral norm $2^{d-1}$. This can be shown, e.g., by induction on $d$. The address function has also maximal possible spectral norm. 

If I understand it correctly, Gauss's lemma implies that that $P$ and $E$ have a non-trivial common factor over $\mathbb{F}[x,y]$. But in the beginning of the proof of Lemma 8 they assume without loss of generality that $P$ and $E$ do not have a common factor. More specifically, they show that if $P$ and $E$ have a common factor then they can use induction to show that $E$ divides $P$. 

Sorting nuts and bolts The following problem was suggested by Rawlins in 1992: Suppose you are given a collection of n nuts and n bolts. Each bolt fits exactly one nut, and otherwise, the nuts and bolts have distinct sizes. The sizes are too close to allow direct comparison between pairs of bolts or pairs of nuts. However, you can compare any nut to any bolt by trying to screw them together; in constant time, you will discover whether the bolt is too large, too small, or just right for the nut. Your task is to discover which bolt fits each nut, or equivalently, to sort the nuts and bolts by size. A straightforward variant of randomized quicksort solves the problem in $O(n \log n)$ time with high probability. Pick a random bolt; use it to partition the nuts; use the matching nut to partition the bolts; and recurse. However, finding a deterministic algorithm that even runs in $o(n^2)$ is nontrivial. Deterministic $O(n\log n)$-time algorithms were finally found in 1995 by Bradford and independently by Komlós, Ma, and Szemerédi. Under the hood, both algorithms use variants of the AKS parallel sorting network, so the hidden constant in the $O(n\log n)$ time bound is quite large; the hidden constant for the randomized algorithm is 4. 

I use my own lightweight LaTeX environment to typeset pseudocode. (It's just a environment inside an .) Here's my source code for Borůvka's algorithm: 

It is possible to sort obliviously with $O(\sqrt{n}\log n)$ calls to the black box, each applied to a contiguous subarray of the original input. The algorithm never touches the input data except through black-box calls. In particular, the same sequence of black-box calls is only a function of $n$, not the actual input data (hence "oblivious"). Here is a sketch of a simpler algorithm, which uses a black box that sorts subarrays of size $k$ instead of just $\sqrt{n}$. The total number of calls to the black box is roughly $2(n/k)^2$. For notational simplicity, assume that $k$ is even and $2n/k$ is an odd integer. 

Shih, Wu, and Kuo's "unifying" paper reduces the planar maximum cut problem to two related problems in planar graphs: (1) maximum-weight bipartite matching and (2) minimum cycle (aka weighed girth). Both problems are solved in $O(n^{3/2}\log n)$ time using divide-and-conquer algorithms that rely on Lipton and Tarjan's planar separator theorem. After a long series of improvements, the minimum-weight cycle in a weighted planar graph can now be computed in $O(n \log\log n)$ time using a recent algorithm of Łącki and Sankowski (2011). However, I am unaware of any improvement on the time to find maximum-weight matchings, which means Shih, Wu, and Kuo's $O(n^{3/2}\log n)$ time algorithm is still the fastest algorithm known (at least to me) for weighted planar max-cut. 

This can be clearly done with a constant depth circuit. (Computing $z_i$ can be done in depth 1, and computing the last step is done using an $OR$ gate.) It is also easy to see that this circuit indeed computes $\rm Majority$ because $z_i \in Dyck(1)$ if and only if ${\rm weight}(x) = n - i$. 

Obviously this can be done in polynomial time. For the analysis note that if one of the constraints has no satisfying assignments, then clearly the given instance is not satisfiable. Otherwise, if all constraints are satisfiable, then a random assignment satisfies each assignment with probability at least $2^{-q}$, and so, in expectation a random assignment satisfies at least $2^{-q}$ fraction of the constraints. This means that the value of the given CSP instance is at least $2^{-q}$, and by the gap assumption must be 1. 

We have $\Pr[\max(Y_1,Y_2) \leq k] = \Pr[Pois(\lambda) \leq k]^2 = e^{-2\lambda}(\sum_{j=0}^k\frac{\lambda^j}{j!})^2$. Therefore, $\Pr[X \geq \max(Y_1,Y_2)] = \sum_{k \in \mathbb N} \Pr[X = k] \cdot \Pr[\max(Y_1,Y_2) \leq k] = e^{-(2\lambda+\mu)} \bigg (\sum_{k = 0}^\infty\frac{\mu^k}{k!} \cdot (\sum_{j=0}^k \frac{\lambda^j}{j!})^2 \bigg)$. From this, if you have some concrete values of $\lambda$ and $\mu$, you can try to see which terms are important, and which are negligible. I guess the important terms are $k < O(\lambda)$. 

What is known about complexity of NP-hard problems on Cayley graphs? Suppose that the graph is given explicitly as the multiplication table of the group and the list of generators. So the input length is the size of the graph. Can we solve NP-complete problems on such graphs (maximum clique/max-cut) in polynomial time? What about some special cases of groups? For example, $\mathbb{Z}_n$ (a.k.a. circulant graphs) or $\mathbb{Z}_2^{\log(n)}$. That is, the input to the problem is the set of generators (and $1^n$ to represent the size of the graph). 

There is a classical paper of Feige and Killian Zero Knowledge and the Chromatic Number that uses the ideas from Zero Knowledge Proofs in order to construct PCPs with certain "ZKP-type" properties. Using these properties they prove that it is NP-hard to color a $N^{0.01}$-colorable graph with $N^{0.99}$-colors. It should be noted that their result does not rely on any commitment schemes, or any other cryptographic assumptions. The only assumption they make is $NP \not\subseteq ZPP$, that is, their PCP-reduction is randomized, and not deterministic. 

Isn't this graph just a collection of cycles? So we only need to compare that all the lengths in the two graphs match (which can be done by sorting the lengths). 

Is the following claim known? Claim: For any graph $G$ with $n$ vertices there exists a coloring of $G$ such that every independent set is colored by at most $O(\sqrt{n})$ colors. 

Here is an $AC_0$ reduction from $\rm Majority$ to $Dyck(1)$. (This implies that $\rm Majority$ is $AC_0$ reducible to $Dyck(k)$ for all $k \geq 1$.) In order to do it, we construct a poly-size constant depth circuit whose gates are $AND$, $OR$, $NOT$ and $Dyck(1)$. 

The first approach can be formalized as follows. Let $P$ be an arbitrary set of $n$ points on the positive branch of the parabola $y=x^2$; that is, $$ P = \{ (t_1, t_1^2), (t_2, t_2^2), \dots, (t_n, t_n^2) \} $$ for some positive real numbers $t_1, t_2, \dots, t_n$. Without loss of generality, assume these points are indexed in increasing order: $0 < t_1 < t_2 < \cdots < t_n$. Claim: In the Delaunay triangulation of $P$, the leftmost point $(t_1, t_1^2)$ is a neighbor of every other point in $P$. This claim implies that adding a new point $(t_0, t_0^2)$ to $P$ with $0 < t_0 < t_1$ adds $n$ new edges to the Delaunay triangulation. Thus, inductively, if we incrementally contract the Delaunay triangulation of $P$ by inserting the points in right-to-left order, the total number of Delaunay edges created is $\Omega(n^2)$. 

Here's a possible alternative to Bob's reduction, this time from (undirected) Hamiltonian cycle. I'm not 100% confident that the details are correct—I've already found and fixed several issues—but I'm sure it can be massaged into a correct proof. As Bob points out, this reduction has a serious bug; the white king can easily stray from its canonical path through the board. This bug can be fixed by adding Bob's cross-over gadget at appropriate locations (I think), but then it's not significantly different from his reduction. Let $G$ be an undirected graph with $n$ vertices and $m$ edges. Draw $G$ in the plane by placing its vertices at regularly spaced points on a line with slope $-1$, and drawing every edge as a horizontal segment plus a vertical segment, both above the vertex line. Now we reduce this drawing to an $O(n^2)\times O(n^2)$ board (rotated 45 degrees) with $O(n^2+m)$ black checkers and one white king. We need three types of gadgets: corners, splitters, and hordes. A corner contains two black pieces that can only be captured together by changing the white king's direction. A $k$-splitter contains a single piece that must be captured in a particular direction, with $k$ special locations for the capturing king to jump into. Finally, a hoard is a large box full of $hn$ black pieces that must be captured in a particular order, for some large constant $h$. In the figures below, the gray circles are pieces that cannot be captured. 

There is no need for the tradeoff that Yuval suggests. The entire optimal editing sequence can be computed in $O(nm)$ time and $O(n+m)$ space, using a mixture of dynamic programming and divide-and-conquer first described by Dan Hirschberg. (A linear space algorithm for computing maximal common subsequences. Commun. ACM 18(6):341–343, 1975.) Intuitively, Hirschberg's idea is to compute a single editing operation halfway through the optimal edit sequence, and then recursively compute the two halves of the sequence. If we think of the optimal edit sequence as a path from one corner of the memoization table to the other, we need a modified recurrence to record where this path crosses the middle row of the table. One recurrence that works is the following: $$ Half(i,j) = \begin{cases} \infty & \text{if $i<m/2$}\\ j & \text{if $i=m/2$}\\ Half(i-1,j) & \text{if $i>m/2$ and $Edit(i,j) = Edit(i-1,j)+1$}\\ Half(i,j-1) & \text{if $i>m/2$ and $Edit(i,j) = Edit(i,j-1)+1$}\\ Half(i-1,j-1) & \text{otherwise} \end{cases} $$ The values of $Half(i,j)$ can be computed at the same time as the edit distance table $Edit(i,j)$, using $O(mn)$ time. Since each row of the memoization table depends only on the row above it, computing both $Edit(m,n)$ and $Half(m,n)$ requires only $O(m+n)$ space.