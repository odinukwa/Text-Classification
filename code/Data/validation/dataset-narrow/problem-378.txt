Read the article Configuring PostgreSQL to Accept Connections From Computers on Your Network where it is explained with screen shots. 

When you are not connected then the SQL will fail and the other SQL statements will not get executed. 

The script expects just 1 row. Not the 26 you get. You must use a cursor and loop through the rows that you get: 

That might be the reason. The optimizers are cost based and decide what path to choose based on the 'cost' that each execution path has. The 'biggest' cost is getting the data from disk to memory. If the optimizer calculates that it takes more time to read both the index and data then it might decide to skip the index. The bigger the rows are the more disk blocks they take. 

You do not use the in your query. This must be a/the reason why the index is not used. Also, the optimizer decides. It is not because you create an index that it will be used. All depends on the statistics for the table. How many rows? How many different unique values? What is the expected percentage of returned rows? 

In this trigger you make the ID NULL. When the trigger is finished the insert is done and Postgres will provide an ID. I assume that you have defined the ID as . 

I'm looking to set up SQL Server 2012 installation with an Always On Availability Group, where the 'passive' replica will be hosted at another site on the WAN and using synchronous data commit - the idea being that we will have a hot standby with no loss of data in the event of a failure at our primary site. One potential problem that I foresee is that the secondary site has slower storage than our primary site. I don't care about that in the event of a failure, we can live with slow speeds for a period of time until the primary site is restored. My worry is that, because we are using synchronous commit, that the slower disk speed at the secondary site will affect performance at the primary site during normal operation. Is this a valid concern, or is it likely that the slower speed will be offset by, for example, the disk not having much read activity in comparison to the primary site? 

I'm currently doing some work on monitoring SQL Server performance. I've found the following script[1] which calculates 'Page Lookups Percentage' - the suggestion being that a "good" value is something less than 100. 

EDIT #2 I just completed a backup using the GUI database backup in case my script was wrong somehow - This gave the same result of a damaged backup. -- This only started happening last night. The only significant change that I am aware of is that I changed the 'auto-shrink' option of the database (it was originally set to true). Is anybody aware of a problem in this area with SQL Server 2012 (specifically I'm on version 11.0.3128)? It could just be coincidence or related to something else entirely but that seems the most likely cause at the moment. In addition does anybody have any advice on what to do in such a situation? The database is functioning fine (as far as I can determine), but I don't care to be without backups for very long...! 

In your view you do not fetch all the key fields from both tables. Oracle needs that to be able to do the update. In your case you do an insert so I suppose that you do not provide the primary keys for both tables. Since you did not gave the structure of both tables I cannot be more specific. 

If your is immutable then it can serve as primary key. Only if it was to hold long values then you could consider to create a separate primary key field to save space in the tables that have a foreign key constraint with this table. 

Create the new empty table as table1. Make the ID an 'auto-increment'. Insert a union of table1 and table2 with all fields except the ID and insert these in the new table. 

The command gives the CPU usage per core. Since you have 12 I would not create another Linode for that. Check the line with . There you will find the which is a better indication weather or not your server is using it's full CPU capacity. 

Check the documentation for more information or type . There is a parameter (TABLE_EXISTS_ACTION) that defines if you want to overwrite existing objects or if you want to append. 

Why do you do an update on ? Anyway you must add a where clause with the primary key of the row to the update. You are now updating all rows in . Also the insert is inserting for every row already in . Why don't you rewrite the insert into something: 

It's always possible that this table is populated by something external to the database instance that the table is situated on - from an application, via a linked server, from a script on the server. There are probably other possibilities but you get the idea. The best way I can think to track activity on the table is to run SQL Server Profiler (via the Tools menu of SSMS on the 2012 version). I'm not going to write a tutorial on that software here but you essentially want to filter the activity on the server by the name of the table that you are interested in. If anything updates that table then it should appear in the profiler results. 

I have a database that seems to be functioning fine with no apparent errors except that any full backup taken is broken - attempting a restore fails with an error "RESTORE detected an error on page (18469:-1767164485)" DBCC CheckDB on the database completes without errors. EDIT #1 The backup is created with the following command: 

Perhaps the easiest way is to just do a insert...select, re-referencing at the same time by using set identity_insert on and adding a fixed value to the existing identity to ensure no conflict between the data sets ? Something like this: 

The is the name of the user that runs the application. The is the name of the user defined in the database. Suppose that is logged on his desktop as and runs an application to modify the data. When he is asked to connect he enters the credentials of . In the audit you will find as and as . This explains while you find as since this is the user that is used to run Oracle itself. 

I do not think that you want to test Oracle 12c itself but if the applications still function. In that case I propose that the developers (or test team) perform there 'normal' application tests to see if the applications still function correct. The only thing that the DBA might give is information on the differences between Oracle10g and Oracle 12c at the handling of the data at SQL-level. 

Is quite simple. Make sure that oracle can write on a folder on the NAS and run the following RMAN job: 

Replace the with the field names (with alias if you need to change the name) you need. You can put in the other 5 tables as : 

Solution 3 is the best solution provided that you move the from the table into the table. The reason why it is better is that when you need to add a variable to one device you can do this without changing your and tables in solution 2. If you want to query the data in the solution 2 way then you can create views that look like the and tables in solution 2. 

If you only need to know that a row is distinct, and don't need the actual contents of col3, then perhaps returning the hash of col3 would speed up the query? You could even perhaps pre-compute the hash using a calculated column so that you aren't computing the hash on the fly. If you do need the contents of col3, but have a lot of duplicates of col1+col2+col3, then it still may be beneficial to work with the hash to remove duplicates as a sub-query, then only return the col3 contents for the distinct rows. 

Do note though that BufferWithTolerance has a tolerance value (in my example 0.9) that is essentially a trade-off value between speed and accuracy. If you want exact results this probably isn't the method for you. I also seem to recall that STIntersects is an imprecise method but I can't find any reference to back that up at the moment so maybe I am mistaken about that. 

If you've tried everything else then perhaps it would be possible (making sure you have a good backup first!) to detach the database, rename the log file (so SQL Server cannot find it) and then re-attach the database. I believe this will force SQL Server to create a new log file. Whether it will also stop thinking that the database is replicated I have no idea, but it seems at least possible. 

In my opinion option 3 is the best. Option 1 has the problem that you always have to use a like on the tags field to find the articles. Option 2 will cause a rewrite of the tag each time that an article is added or deleted. When there are a lot of articles for the tag then the record becomes long. Both option 1 and 2 will end up with 'big' rows which means less rows in a physical block which means more physical reads and 'slower' result. Option 3 has a simple setup. It is the way to present an M-to-N relation. Put an index on the name of the tag to help to speed up the finding of the tags (will not help much if you have a very limited number of tags). Adding an article or adding a tag to an article will be fast too. 

The command should give an output with its parameters and a list of databases that run on this server. Check the directory on the client machine to check if you have a file that has the information to connect to the database. Check it with the command . 

Normalise the table so there is 1 question per row. Then you are also not limited to a number of questions per survey.