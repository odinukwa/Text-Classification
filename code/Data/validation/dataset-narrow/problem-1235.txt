1 Background 1.1 Asynchronous Shared Memory Model Let's consider a collection of distributed nodes that communicate using shared memory variables. There is an adversary that controls when a node take steps and when to deliver messages. The computation is asynchronous, i.e., the adversary can delay the steps of nodes for any (finite) amount of time. You can think of a step of a node as a state transition of its local automaton (according to the algorithm) where the next state is determined by the current state and the observations of the node since the last step. 1.2 Safety and Liveness When formally reasoning about the properties of an asynchronous algorithm, we distinguish between safety and liveness properties. Informally, a safety property can be interpreted as a guarantee that something "bad" never happens. (E.g., for mutual exclusion, a safety property would be that no two nodes enter the critical section simultaneously.) Liveness, on the other hand, can be interpreted as "something good will eventually happen", e.g.: every node eventually terminates. To formalize safety, we consider the infinite set ${M}$ of all possible executions of all possible algorithms, taking into account all possible choices of the adversary. An execution is an infinite sequence of steps. We can define a metric on $M$ by taking the distance between two distinct runs $\alpha,\beta \in M$ to be $2^{-n}$ where $n$ is the first index where $\alpha$ and $\beta$ differ. A safety property $S$ corresponds to some nonempty set $P\subseteq M$ that is closed in the sense that the limit of an infinite sequence of runs in $P$ cannot be in $M\setminus P$. So once we know that some property is a safety property, it is sufficient to show that this property holds on finite prefixes. 

There are many examples of such reductions in certain distributed computing models, I'll limit my answer to the following 2: Local computation in the (fault-free) synchronous model [2] show reductions between many optimization problems, for example, Minimum Dominating Set, Maximal Independent Set (MIS), Maximal Matching (MM), and Minimum Vertex Cover (MVC): In particular, [2] show a lower bound of $\Omega(\sqrt{\log n} + \log\Delta)$, where $n$ is the network size and $\Delta$ the max degree, for computing a constant approximation of an MVC. From this, the same bound follows for computing an MM, since any maximal matching is also a $2$-approximation of an MVC. From this, you also get a bound on computing an MIS, because an MIS-algorithm $A$ can be used to compute MM by simulating the execution of $A$ on the line graph. Asynchronous Model with Crash Failures Here the most studied problem is fault-tolerant consensus and its many variations, since implementing basic primitives like atomic broadcast and or a synchronizer themselves require consensus. For example, assuming access to failure detectors [3], gives rise to a classification of problems by looking at the power of their respective failure detector. We say that failure detector $S$ is the weakest failure detector for problem $P$, if, given any failure detector $T$ for $P$, you can also implement $S$. Using this relation, we can define problem $P$ as being harder than problem $Q$, if the weakest failure detector for solving $P$ can implement the weakest failure detector for solving $Q$. It has recently been shown that every problem has a weakest failure detector [4]; note however, that [4] is an existential result. For some problems, e.g., consensus, the weakest failure detector is known [5], while for other problems, e.g., $k$-set agreement, the weakest falure detector is still open [6]. 

You might want to check the theorynet mailing list for CS theory related announcements. I believe there are several lists for specific communites, for example, for distributed computing there is the PODC list. 

I'm assuming that you mean the Flood Set algorithm for solving the agreement problem in Section 6.2 of [1]. The Flood Set algorithm proceeds in rounds and requires every process $u$ to forward all received values, which it stores in a set $W$; initially $W$ contains $u$'s input value. We assume that $f$ processes can be faulty. After $f+1$ rounds, every alive process $u$ checks if its set $W$ is a singleton. If so, $u$ decides on the singleton value, otherwise $u$ decides on a default value $x$. It can easily be shown that after $f+1$ rounds, all correct processes have the same set $W$. For a general graph, the answer is obviously no: Suppose that the network is cut by a faulty process $u$ into parts $C_0$ and $C_1$ and processes in $C_0$ start with $0$ whereas processes in $C_1$ start with $1$. If $u$ crashes initially, the following happens in round $f+1$: all processes in $C_0$ will decide on $0$ and all processes in $C_1$ will decide on $1$. Note that this impossibility argument holds for any agreement algorithm. [1] Nancy Lynch. Distributed Computing. Publisher: Morgan Kaufmann 

In every round, each active node $u$ marks itself with probability $1/d_u$ where $d_u>0$ is the degree of $u$; if $d_u=0$, $u$ simply enters the independent set. (Initially, every node is active.) If $u$ is the only marked node in its neighborhood, $u$ enters the independent set, deactivates itself and notifies all of its neighbors to deactivate themselves. The degrees of the remaining active nodes are decreased accordingly, i.e., all edges to deactivated nodes are removed. Otherwise, if there is some neighboring node $v$ that is also marked, the lower degree vertex unmarks itself and remains active. 

There's a recent paper [1] on dense subgraph (i.e. community) detection in the context of distributed computing. In [1], each node of the graph represents an operating entity (i.e. runs an instance of a distributed algorithm) and each node can communicate with its neighboring nodes by sending messages. The graph itself can change (slowly) over time. Note that the connections in social networks evolve over time, so considering a dynamic graph seems like the right model. By running a distributed algorithm, each node can locally determine whether it's part of a dense subgraph. You might want to consult the related work section of [1] to get pointers to other references. [1] Atish Das Sarma, Ashwin Lall, Danupon Nanongkai, Amitabh Trehan: Dense Subgraphs on Dynamic Networks. DISC 2012: 151-165. Full paper available at $URL$ 

[1] Michael Luby: A Simple Parallel Algorithm for the Maximal Independent Set Problem. SIAM J. Comput. 15(4): 1036-1053 (1986) $URL$ [2] Alessandro Panconesi, Aravind Srinivasan: On the Complexity of Distributed Network Decomposition. J. Algorithms 20(2): 356-374 (1996) $URL$ [3] Fabian Kuhn, Thomas Moscibroda, Roger Wattenhofer: Local Computation: Lower and Upper Bounds. CoRR abs/1011.5470 (2010) $URL$ 

Applications One important application of the consensus problem is the election of a coordinator or leader in a fault-tolerant environment for initiating some global action. A consensus algorithm allows you to do this on-the-fly, without fixing a "supernode" in advance (which would introduce a single point of failure). Another application is maintaining consistency in a distributed network: Suppose that you have different sensor nodes monitoring the same environment. In the case where some of these sensor nodes crash (or even start sending corrupted data due to a hardware fault), a consensus protocol ensures robustness against such faults. 

We can define a metric topology on the set $ASYNC$, which is the set of all possible runs of a distributed algorithms. Note that each run $\alpha \in ASYNC$ corresponds to an infinite sequence of state transitions. For $\alpha, \beta \in ASYNC$, $\alpha \ne \beta$, we define $$d(\alpha,\beta) := 2^{-N}$$where $N$ is the earliest index where the state transitions in $\alpha$ and $\beta$ differ; otherwise, if $\alpha = \beta$, we define $d(\alpha,\beta) = 0$. We first argue that $d$ is a metric on $ASYNC$. By definition, $d$ is nonnegative and $\forall \alpha,\beta \in ASYNC$ we have $d(\alpha,\beta)=d(\beta,\alpha)$. For $\alpha,\beta,\gamma \in ASYNC$, the triangle-inequality $d(\alpha,\beta) \le d(\alpha,\gamma) + d(\gamma,\beta)$ trivially holds if $\gamma=\alpha$ or $\gamma=\beta$. Now consider the case that $d(\alpha,\gamma) \ge d(\gamma,\beta) > 0$, i.e., $d(\alpha,\gamma)=2^{-n_1}$ and $d(\gamma,\beta)=2^{-n_2}$, for some indices $n_1\le n_2$. Since $\gamma$ shares a common prefix of length $n_2-1$ with $\beta$ but only a prefix of length $n_1-1$ with $\alpha$, it follows that $\alpha$ and $\beta$ differ at index $n_1$, and thus $d(\alpha,\beta) = d(\alpha,\gamma)$ and the triangle-inequality follows. The case where $0<d(\alpha,\gamma) < d(\gamma,\beta)$ follows analogously. The metric $d$ induces a topology (e.g., page 119 of [1]) where the $\epsilon$-balls $B_\varepsilon(\alpha) = \{ \beta \in ASYNC \mid d(\alpha,\beta) < \varepsilon \}$ are the basic open sets. We will now argue why safety properties correspond to closed sets: If an execution $\alpha$ does not satisfy a safety property $S\subseteq ASYNC$, i.e.\ $\alpha \notin S$, then there is an index $N$ where all runs $\beta$ that share a prefix longer than $N$ with $\alpha$ are not in $S$. This closely matches intuition, since once a safety property is violated in a prefix of an execution, it makes no difference how this prefix is extended! Formally speaking, suppose that $\alpha \notin S$. There exists an $N\geq 0$ such that, if some $\beta \in ASYNC$ has $d(\alpha,\beta) < {2^{-N}}\text{,}$ i.e., $\alpha$ and $\beta$ share a prefix of length $\ge N$, then $\beta \notin S$. Thus, the set of runs $S$ is closed, since its complement is open. [1] James Munkres. Topology. 

Entropy waves, the zig-zag graph product, and new constant-degree expanders conveys a lot of intuition about graph products and expander graphs and the ideas are accessible to anyone with basic knowledge of linear algebra. 

A simple approach to achieve reliability and fast lookup in a "well connected" network $G$ is to replicate each file $f$ on a set $V_f$ of $\Theta(\sqrt{n}\log n)$ nodes and use random walks to efficiently find members of $V_f$. When searching for a file $f$, we start $c\sqrt{n}\in \Theta(\sqrt{n})$ random walks of length $\tau$ from some particular node, where $\tau$ is the mixing time of a random walk on $G$. That is, after taking $\tau$ steps, a random walk has probability in $[1/2n,3/2n]$ of being at any particular node in $G$. Even if some $\epsilon\sqrt{n}\log n$ (for some small $\epsilon>0$) nodes have failed, the probability that at least one of these random walks encounters a node in $V_f$ is at least $$1-\left(1-\frac{\log n}{2\sqrt{n}}\right)^{c\sqrt{n}} \approx 1-\frac{1}{n^{c'}},$$ where $c'$ is a constant $>2$. Of course, this approach only makes sense if $\tau$ is sufficiently small - that's what I meant above when saying "well connected" network. This is the case in networks with sufficiently large (i.e. constant) spectral gap since $\tau \in O(\log n)$. A similar approach is taken in [1]. After some time, most of the $\Theta(\sqrt{n}\log n)$ could have been removed. Thus it is necessary to estimate the current size of $V_f$ periodically and replenish the removed members of $V_f$ by replicating $f$ on new nodes. [1] Ruggero, Bhattacharjee, Srinivasan, Marsh. Efficient lookup on unstructured topologies. 

I'm not aware of a general rule to convert centralized bounds to the distributed setting. In the distributed setting, local computation is given for free but the difficulty lies in breaking the symmetry - a node needs to accumulate enough information about it's neighborhood to make a decision about being part of the output set, e.g. being in the MIS. (Taking this a step further, we can think of an $r$-round algorithm as a function mapping $r$-neighborhoods to a yes/no decision.) For example, for certain $n$-node graphs with max degree $\Delta$ there is an $\Omega(\sqrt{\log n}+\log \Delta)$ lower bound [2] known on the number of distributed rounds, that holds for (the distributed version of) computing a constant approximation of a minimum vertex cover or of a minimum dominating set. While the above problems are known to be NP-complete, the same lower bound also holds for problems for which there are polynomial time centralized algorithms like computing a maximal independent set or finding a maximal matching. For most of these problems the lower bound is almost tight; there are distributed algorithms that run in polylogarithmic rounds (see [2] and references therein). On the other hand, some NP complete problems even allow constant time solutions in the distributed setting! In [1] there's an $O(1)$ round distributed algorithm for $O(n^{1/2+\varepsilon} \chi)$-coloring, which itself is NP complete. [1] Leonid Barenboim. On the Locality of Some NP-Complete Problems $URL$ [2] Fabian Kuhn, Thomas Moscibroda, Roger Wattenhofer. Local Computation: Lower and Upper Bounds. $URL$