Make a mesh that covers the polygon where the nodes are at most 2r apart and connect the nodes with neighbours. Based on the distance and wind/water current between each node, add a difficulty value to progress through the arcs; arcs are bi-directional and don't have the same value (for instance for node A to B -> cost is 10, but for B to A -> cost is 20 because of the wind), you'll get a directed graph. Apply a greedy algorithm to explore all the nodes in the graph. 

Generally, when using a vector2 to represent velocity, the length of the vector is the speed, while the coordinates represent the orientation (the direction of the speed). To extract the length of your vector, use the method of your vector: 

The tool pipeline is the set of tools that an asset has to go through from the original creation to the moment it's used in the game. For example: 

...as opposed to if you reverse, where your reverse speed will need to defined differently. Now it's either going to be a positive speed with a flag that says you're in reverse, or a negative speed; this will depend on the rest of your architecture. 

Today, I have implemented soft particles by rendering them to a separate render target and blending that with the scene, which worked as expected with additive blended particles. In that case I render the particles to a rendertarget while blending them additively, then drawing that rendertarget in fron of the scene while blending additively, works like a charm, I even save some render time if I use a smaller render target. In the case of alpha blended particles, I have a serious issue. First I draw them with a blend state set up for alpha blending: 

If your Health pickup is an object, you could set it to null if you detect a collision. The garbage collector will do the rest for you. But don't try to reference that object after it is null because it will give your a runtime error. 

If you would do hard normals for these boxes it would eliminate your problem. For that you have to have a normal not per vertex, but per face. You could do this by having four vertices with the same normal for each face of the cube, that is 24 vertices. That way, a whole face will receive an even amount of light. 

I have not tested the code, but it's been done with nearly the best of my knowledge. When computing the final position of a node, you really have to keep a wall between the node's data, and what's being applied to the scene graph: when computing the position in the scene graph, it must not change the node's original settings (of course, it's the position w.r.t. the parent, but this does not need to be modified during the update and the calculation of the final location). My coding standards have been applied, please disregard as I'm becoming an old monkey. 

Edit: I'm not sure you'd want to write a whole physics engine from scratch, as it's one of the most complex parts of a game. I would suggest you to learn and use one that is already written for your target platform if you need that. Re-writing the wheel is often a waste of time when it's not required. You can take a look here to find one that seems like a good start. You have only vaguely stated your requirements so the question how do I resolve collisions in a semi-realistic way? is too vague to be given a good answer. 

Which makes an orthographic view of the scene. With this, the distance of the objects to the camera is not visible. It is the equivalent of a parallel projection. You have to make a perspective projection for the objects to be displayed bigger as the get closer, and smaller as they get further away. You could look into to make a perspective matrix. 

I would say that you should try to keep the size of your data (vertex buffers, arrays) at the minimum, especially if you want to be updating the vertex buffers since that is a slow operation even if you created them with dynamic usage flag. More data will slow down that operation significantly. Also I don't know what big of a program you are developing but wasting space is usually a big luxury. :) And since you already have different types of shaders for different layouts I see no need to keep empty data in several of your models. I would recommend however to avoid switching layouts and shaders when unnecessary. For this you could draw everything which uses the first layout first, then draw everything which uses the second layout and so on. 

World position is vertex position multiplied by world matrix, not view matrix. So it should be like this in the vertex shader: 

I wonder if it is a welcome practice to get current render state from the device context context so often, or should I roll my own solution for checking them? Keeping pointers for currently bound states for example and checking with those? I ask this now, because I only started implementing this for some states (PSSetshader and PSSetShaderresources), and I have a lot more, so I would opt-out early if this method could be problematic. 

I'm not completely familiar with it, but I think that for your convenience, I would take a look into the Wavefront .obj file format. As I understand, a lot of 3d software implement this file format as a convenience to transfer from one 3d app to another. Since the format is text based, you could 'code' your first shapes at first by hand, and then use a 3d software like Blender to make more complex objects. You'll have to change the way you "read" your 3d objects, however, but it's for the best :) Another advantage of using an open file format is that you'll already have a bunch of features right away, without having the burden of thinking how to structure them. Be aware that different software use different hand-ness and coordinate system convention, so it may differ a bit from your implementation (for instance, a software might think the axis is up, while another, or your own, thinks that axis is up. 

What I'd do: Have your standard arrow draw from (0, 0, 0) to (1, 0, 0) (using a line) and the tip of the arrow be a triangle at the end (using a triangle, points at (0.95, 0, +/-0.025) and (1, 0, 0)). Then resize your arrow to the size you need with a scale matrix (since it's a unit vector, it's easy to have it the desired size) and then move your arrow to wherever you need it with a translation and a rotation matrix. And you can reuse it wherever you like! Of course, you have to decide where you want it first, and remember that it is pointing in x+ when moving it around..! 

An option would be to render a cylinder. To fade it out, you could use the depth map of the scene, compare it to the current fragment's depth of the cylinder and fade out according to how far is the current depth from the scene depth. Kind of like you would do with a soft particle renderer, but instead "softening" (fading out) the geometry when the two depths are close, fade it out when they become more far away. Also, in your picture, it looks like the fade is non-linear. An other approach for a simpler effect you could also use decals. 

Transforming uv coordinates should be good enough. You could do it while loading your model, or even in a shader. Whether the texture is upside-down in memory is irrelevant for performance when sampling the texture. 

You can't use SV_Position for this in hlsl, you should get the position from an other output slot without the SV_Position semantic instead. 

DXGI_FORMAT_R32G32B32_FLOAT is not 16 bytes, that's the first problem after a quick look. Either declare it 12 in your layout or specify D3D11_APPEND_ALIGNED_ELEMENT instead. Or Make it DXGI_FORMAT_R32G32B32A32_FLOAT instead depending on your matrix. 

These two will do the same, which is not outputting any color to your rendertarget/depthmap. EDIT: You'd have to insert the above code (either the discard or the clip) to your MVSMShadows.hlsl to the beginning of the ShadowCasterFP function. But you also have to sample a texture resource and a sampler there which you must declare and upload to the GPU beforehand. You can declare a texture and a sampler like this: 

From the description of your issue, the speed and drag coefficient of 1. are irrelevant as they'll be the same throughout the whole path-finding process. Note: You will not find the optimal trajectory with this. You will find a good or okay trajectory, but unless you want to throw in optimization and meta-heuristics, that's all you're going to get. 

Usually, (ok, maybe not usually, but at least for ODE (Open Dynamics Engine)) the physics engine does 2 different things: 

They solved this problem quite easily in the latest iterations of the Far Cry series: If you can one-shot an enemy with a silent weapon, you succeeded at a stealth-kill (head-shot with a silenced gun, bow), and no one will know where you are. If you aim badly (e.g. the chest or lower body), or the enemy has too much defense (aim for the head with a silenced small gun on a heavily armoured guy) and you're not able to one-shot him, he'll know for sure where you are and come hunt you (with his friends). Not knowing what is exactly right for you and your game, I suggest to list all the options available, architect your game code around the fact that you'll have to try all those different things, and then test the many things that could make the game fun. 

To get correct screen space coordinates you have to do your w divide in the pixel shader. After that your return should function correctly: 

Draw a shadow sprite like you draw your character before the character (to make it appear behind him). Make sure its X position is updated according to the character, but the Y position stays on the tile. You have got a shadow. 

Maybe this is not the whole problem, but you need to divide by the w component after multiplying with a projection matrix. So after but before the you should do this: 

There are also some advanced techniques (order independent transparency) which are not really used in real time apps because of their performace hungry nature. 

I am rendering shadow maps to cubemaps (just rendering raw depth buffer) then projecting them to the scene to do omni-directional shadow mapping. The cubemap is sampled by the light direction vector on each fragment: 

Keep in mind that this effect only works well if you are facing the light source because essentially it is the lightsource which you have to do a radial blur on. In your scene the light is neither facing the camera nor is the lightsource visible itself (or maybe just out of the view frustum). For the best results you have to render your light volume with colors on and the rest of your scene black so that solid objects will block incoming light (their colour value is not accumulated into the blur). By the way, I use 35,0.90f,0.2f,0.999f,0.2f values for NUM_SAMPLES,Density,Weight,Decay,Exposure parameters with the same effect and it looks quite convincing with the rendering technique described above. 

Have the refuse to destroy a if the ref_count of it's is more than 1. This will oblige your users to take a great care of what happens. This might not be very fun or practical, though. In your architecture, add a listener pattern: when A links to B, tell B that A is linked to it. So when B is destroyed, notify all the other linked s (A) that B has been destroyed and so delete the link. Don't link your s together. Keep some kind of reference to the game object that you need instead, and fetch it each frame from the . If the it's destroyed, the will return , and you'll know that the other object is dead. Your vector could then contain instead of . 

This last idea really comes down to "Why do I have different coloured cubes?" If there is no reason, there is no reason to have different coloured cubes and differentiate them, as this could confuse some players. If you want to have different coloured cubes, find a game-design reason to have different coloured (or whatever you'd like to change their look for). Once you find that reason, this will help you define the cube personalities, and then expand on them, and then further expand on your game's background story. If your game does not have a story/background, this could be a nice reason to develop one (some players like to follow through the story of the game, sometimes more than to play the game per se), and if you already have one, try to make the colour/personalities complement it. 

This one means it will "wrap" your texture around the model and sample from the nearest texel. You should try out the other sampler states too. Mirror will mirror your texture if the texture coordinates are outside the (0,1) bounds. Clamp will not tile your texture. You can also do it in the shader while you are declaring your sampler: 

For which is the best filter there is no universal definition it depends on the art style. For example with pixelated retro graphics you would want to use point filtering, while high resoulution sprites like bilinear better. Also bilinear filtering requires a bit more processing because of the interpolation. There is an other filtering method called anisotropic which makes no difference from linear in 2D graphics. 

The vertex shader will output a full screen quad this way if you call a draw for 4 vertices with a TRIANGLESTRIP topology; Probably you can also save some time if you try out different rendertarget formats and resoulutions. If you can have depth or stencil testing it can also save you some time in some scenarios. 

Don't declare the sampler inside the shader, but create an ID3D11SamplerState* outside and upload it while drawing by ID3D11DeviceContext::PSSetSampler(slot,count,sampler); I had the same problem with DX11. You might create a sampler like this: 

The point you draw is really a spot of the size you put on the screen. It has no volume. From opengl wiki: 

Edit based on comments. Alternatively, you can use Unity's method to have the update done on a more precise basis. (In Unity, menu Edit > Project Settings > Time to set your fixed timestep; thanks @Chris.) You then have 2 options. You can either still use the delta time provided by Unity: 

The images are not saved in the executable. You store the path to the files in the executable. And the files are typically in the folder of the exe. When you use Visual Studio, the exe might be in a folder, while the "environment" folder that the exe will look in to find files could be in another (the "Working Directory"). Your images could be in the "working directory" while the exe is in another directory. This really depends on your setup and your needs (and your Visual Studio configuration). 

This issue will rise if there is not way to produce useful items. This is due to the laws of the market: if there is an increase in the price, more producers will get on the market, and they'll all fight for the best price, which will make the price come down, until it reaches again a "stable" price. So; what is better? It all comes down to the implementation and the design. EVE Online uses local markets but gives users ability to see the prices easily on other local markets via user-developed tools; in EO, you can "easily" produce goods that would net you a profit, you can travel "easily" to other local markets and you can carry stuff "easily" from a local market to another. IMHO, their implementation is GREAT! Rappelz used to have local auction houses and local personal-store, but a high/infinite cost of producing goods, and a high cost of travelling from local market to local market. They eventually changed everything for a global market and it's been soo much of a plain reliever.