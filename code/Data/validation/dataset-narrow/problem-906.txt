If you're getting traffic spikes on a WordPress website, switching to nginx is probably not the first thing you should do. The first thing to do is to install some WordPress-specific caching mechanism. My favorite is the W3 Total Cache plugin. This will help you by creating static cached pages of your site which Apache can serve much faster to anyone who isn't a logged in WordPress user. It also has a lot more features, including memcached support, database and object caching support, CDN support, and many other potentially confusing things, but you don't have to set those up immediately. Just doing the static page caching will help a lot if you aren't already doing it. If you're already caching as much as you can through solutions like this, then it might be appropriate to consider switching to nginx, upgrading your server, etc. 

Consider creating a local private mirror of your favorite Openstack-containing distribution, such as CentOS, Fedora, Ubuntu, etc. Once you have set up the local mirror (on a networked machine) using the distribution's instructions, walk it over the air gap on an external hard drive to the disconnected network for use there. To obtain updates, you can simply update the mirror on the network, and then walk it over the air gap again. 

You disabled NAT Loopback. This needs to be enabled in order to use your external IP address from within your network. 

It appears you're using the ChangeResourceRecordSets API call. Just send both changes at the same time: a CREATE for the A record and a DELETE for the CNAME record. If this doesn't work, yell at Amazon until they fix it. 

You have probably upgraded the system from 9.1 to 9.2 but failed to reboot it. You might also have simply booted a previous kernel, but given the output you've shown, this seems much less likely. One other possibility comes to mind, that the system is actually inside a jail, running on a 9.1 system, so that while the jail is upgraded to 9.2, the host system is still running 9.1. 

All MDM traffic goes over the Apple Push Notification Service. In particular, to get iOS devices (and Macs!) to be able to receive from APN, they must be able to make outgoing connections to TCP port 5223. You can allow outgoing traffic for this destination port to pass without being redirected to the captive portal, and then iOS devices should be able to reach APN and receive your commands. See also: 

Your email was sent out to the Internet via smtp4.fiu.edu (131.94.79.14). However, neither this name nor IP address are in your SPF record as an authorized sender. The SPF record shows only Office 365 as an authorized sender for thewolf.fiu.edu. No other hosts are authorized to send mail, and this correctly failed. Since you are sending mail via other mail servers, you should add their IP addresses to your SPF record. Or, you can alter your own mail servers to redirect outgoing mail via Office 365. 

The documentation states that you need libvirt 0.10.0 for this feature, but Ubuntu 12.04 LTS contains 0.9.8. You will need to upgrade to use this feature. 

Of course you can. OpenStack was designed for this. The purpose of installing it to VMs is primarily for evaluation, testing and learning, rather than production deployment. 

You can't do that. nginx was designed and intended to reverse proxy to upstream app servers on the same machine or nearby on the local network. It assumes that it has a direct connection available to the upstream servers. While you can reverse proxy to another machine across the Internet, and this is occasionally useful in migration scenarios, it was never intended to be a "client" that would have to obey proxy server settings. Thus nginx has no functionality to route upstream requests through yet another proxy server. Some things you can do include: 

RFC 822 actually gives an example of this usage. It required (Section 4.4) that the Sender: header be present when it was used. 

Sorry, this can't be done, and doesn't need to be done anyway. BEAST is no longer considered a serious threat, as client-side patches have been available for affected platforms for years now (even for XP!), and server-side mitigation requires RC4, which is now considered too weak to be secure and only makes things worse. 

If the package was just installed, and hasn't yet been updated, you can rollback the transaction with, e.g. . This won't work if any of the packages has been updated since installation, though, as it matches name, version and release. And if you really just installed the packages, you can skip everything else and run to rollback the most recent transaction. 

nginx already can cache 301 redirects. You can change the amount of time they are cached with the proxy_cache_valid directive: 

The primary difference is that users authenticate to using their own password, whereas with or direct root login the root password is used. This means that you don't have to share the root password with all and sundry, and that if you need to disable root access for one or two users in the future, you can just disable it for them, instead of having to change the root password. is also capable of limiting which commands each user can run as root, so specific users can be given access only to the tasks they need to perform, if they do not require full root access. 

As for curl, I can replicate the problem when using , but works properly (though it sends a 302 redirect to . 

At the time this question was asked, the site in question supported only a small selection of very weak cipher suites and had very little compatibility with modern user agents. Thus it was impossible for most browsers or even robots to negotiate a connection, let alone a secure one. The site has been (mostly) fixed since; while it still has weak cipher suites and other problems, it should at least be functional. 

Surprise! For a TLS negotiation with a mail server, typically the subject name is not checked at all. Much of the encryption between mail servers is entirely opportunistic, using only self-signed certificates, often with a subject name of localhost. Unfortunately, it's not really possible to do much better than this, as it is possible to MitM the connection and prevent the encryption negotiation from ever taking place. And it's not possible to prevent that attack until DNSSEC (or an equivalent) is fully deployed and everyone requires proper certificates of everyone else. This is unlikely to happen before we retire. If your email must be encrypted end to end, you'll need to do it yourself. 

You said that you wanted 3DES for IE8, but you have explicitly disabled it in the cipher spec! Just remove that: 

If a direct kernel boot is present, it will always be used, and the regular boot order will be ignored. The best way to handle this is to not use direct kernel boot for installing the system, but rather pass a where the Linux distribution you are installing lives (which can be on a local disk or a remote web server, FTP or NFS server). This also allows you to inject a local kickstart or preseed file for a completely automated installation. For example: 

But be aware that this will match either sender or recipient addresses, depending on when you call it from . To check the recipient address, add to : 

Why shouldn't you give up delivering email after one day? One good reason is weekends. Email is not now, and never was, particularly reliable. In the early days of the Internet, the 1980s, it was entirely possible for email to take a couple of days just to reach its destination, what with some network links not being 24x7, over expensive long distance dialup calls (back then it cost per minute to call two towns away, nevermind the cost of a call from Sydney to Los Angeles), or even over amateur radio. As a result, it could take a while to deliver email, and the protocols had to cope with unreliable and part-time connections. They do this very well, but even then, mail could get delayed or lost. Certainly today, email has an illusion of reliability, if only because the underlying transports are more reliable, and many uninformed people (like most of our users) have an expectation that it is reliable, but that expectation does not match reality. Without a significant change to email delivery protocols, which will probably never happen, email, like anything built by humans, will always be less than 100% perfect. Sometimes, we sysadmins take advantage of that. For instance, in an office where everyone is only there Monday-Friday, I can have an email outage lasting all weekend if necessary. Of course, it virtually never is necessary to be out that long, but I have had to have email down for over 24 hours in rare cases. In such a case, if you give up after 24 hours, email sent Friday afternoon may not reach its recipient. The sender won't find out until Monday morning, but if you had kept trying, the recipient would have had it by Monday morning. Further, it's very important to set user expectations appropriately. The fact that Internet email is not and never will be 100% reliable needs to be clearly understood, even as we like to think that it is. The RFC says you should keep trying, precisely because things go wrong, and it's intended that the mail be delivered eventually, if possible, but at some point you do have to give up. It might be OK to reduce this to three days. I've always thought five days was too long to wait for delivery for most messages on a 24x7 Internet. 

Since your domain seems to be registered with PCExtreme, you will need to contact them to have this error corrected. 

This is happening because Apache doesn't have permission to search (descend, the permission) one of the parent directories. To resolve the problem, first check to see which parent directory is missing the proper permissions, e.g. with . Find the directory that is missing the permissions for all users, and give the correct permissions with . 

Security groups for regular EC2 instances can only have rules applied for TCP, UDP and ICMP. To resolve the problem, start your instance in a VPC. Security groups for VPC instances can be written for any protocol, though you may need to use the command line tool to create the rule. This should be sufficient to open up GRE for your VPC instances: 

If you want your names to show up with mixed-case, preserving whatever you put in the zone file, you will need to use a different nameserver, such as BIND. It is, after all, the Berkeley Internet Name Daemon... While it may be possible to hack the NSD source code to make it preserve case, you should be wary of doing this, as it may not be possible to predict what bugs or unintended consequences such a change might introduce, and your nameserver is one of the last places you want mysterious bugs showing up. 

getty is the process that manages logins on the console and on serial ports, if any. It's generally not safe to disable this, as console access is what we go to when other means of accessing a server fail. 

Or you can enable but this may carry a performance penalty, since while it works it's not what MultiViews was designed for. 

You want to send 404 errors up to your application? Easy enough. You need to specify an existing file, though. For example: 

You should be able to do this within the confines of Red Hat's network scripts. However, you can't due to a bug in those scripts. 

By default is a tmpfs, and doesn't exist on boot anyway. To get into this situation, your server must already be non-standard in some way: Someone explicitly configured your system to not have emptied on boot. So it is best to fix the problem, by undoing whatever changes were made to cause to not mount as a tmpfs on boot: 

Did you open inbound TCP port 10514 in the rsyslog server's firewall? You have to do that if you want to receive incoming connections. 

Shopify doesn't support IPv6 (which may be a big problem for you later) so just remove the old AAAA record. Your DNS records appear to be hosted by "Web-2u Limited". 

Your consultant is blowing smoke. And probably getting very highly paid to do so. We're all in the wrong business... There are good reasons to limit the Apache modules that you install and use. Security and stability come to mind as the top reasons. But efficiency? Do they mean power savings? Unused code doesn't consume any electricity. I can't imagine what is supposed to be meant by this. You've spent more than a year's power savings from such "efficiency" just typing out your question. As for WordPress, its requirements are pretty minimal. You don't even need Apache... And advise the bean counters that they aren't getting their money's worth from that moron consultant. I'd ballpark the savings from this one at around 3 cents per year per server. You can use that as an example of the quality of recommendations you're receiving. 

To begin, thermal sensor values are normally reported in Celsius, not Fahrenheit, so you seem to have something converting the values for you. 261 F is 127 C. This also happens to be the maximum value of an 8-bit signed integer. Normally, sensor values reported as 127 (or as -128, the minimum value of an 8-bit signed integer) refer to sensors which do not actually exist in the hardware, or sometimes sensors which exist but are not functioning. Indeed, some firmware release notes found on the Internet indicate that these VRD1 and VRD2 sensors were removed in a firmware update, which strongly suggests they never existed. Try updating your IPMI firmware. Or you can just ignore them. 

This is happening because your daemon is only listening on IPv4. IPv6 is the default protocol, so if a given hostname has both IPv4 and IPv6 addresses, the IPv6 address is always tried first. In your case, has the IPv4 address and the IPv6 address . But your daemon is only listening on . So, when tries to connect to it first connects to , finds nothing is listening, and returns . It then tries to connect to and finds your daemon. 

firewalld already allows outbound traffic and blocks all inbound traffic. You only need to add the services or ports you want to open. 

That looks like a terribly complicated setup. My first thought is to advise you to get rid of Apache entirely; nginx + php-fpm is perfectly capable of serving your WordPress blog, and most of the WordPress nginx rules you will find expect nginx to be used as the origin server rather than a reverse proxy to Apache. Now, on with the show... You can't use in the same block where you are passing the request upstream. It would just be ignored since everything is going to the proxy anyway. If you want to make this work, I suggest using a named location. Something like this (off the top of my head):