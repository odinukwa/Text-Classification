The purpose of the proof checker is to minimise the trusted computing base. By having a proof checker, neither the compiler nor the theorem prover need to be correct. The paper makes this point on Page 3: 

They map ("compile") from source languages $A$ and $B$ to target $T$. Typically you want these two translation functions to be at least sound. That means that for all $A$-programs $P, Q$, we have: $trans_A(P) \cong_T trans_A(Q) \Rightarrow P \cong_A Q$, and likewise for $trans_B$. Now you want to argue that $(A, \cong_A)$ and $(B, \cong_B)$ are equivalent as languages. That's what you use the target $(T, \cong_T)$ for. There are many ways that this can be done, details depend on what concept of language equivalence you are interested in. One approach could be to say 

This is a subtle question. TMs are very much a sequential model of computation. So in some sense, TMs cannot (directly) model multiprocessing. However, TMs can do step-by-step simulations of the reductions a multiprocessor is carrying out. So TMs can do a sequential simulation of a parallel computation. Whether this a genuine model of parallel computation is debatable. See also the discussion Applicability of Church-Turing thesis to interactive models of computation. 

My answer is really just an elaboration of Gilles', that I had not read before I wrote mine. Maybe it's nevertheless helpful. Let me begin my attempt at answering your question with a distinction between two dimensions of programming languages work that relate quite differently to programming language theory in general and process calculus in particular. 

[1] K. Honda, M. Tokoro, On Asynchronous Communication Semantics. [2] K. Honda, M. Tokoro, An Object Calculus for Asynchronous Communication. [3] K. Honda, Two bisimilarities in ν-calculus. [4] R. Milner, Functions as Processes. 

Note that my formulation of the problem has been deliberately vague, for it's likely that a lot depends on $\sqsubseteq$ and the typing system. Most orders $\sqsubseteq$ will be too complicated to admit efficient algorithms, I imagine. The orders I have in mind are program size, or weighted program size (some program constructors are 'heavier' than others). 

(As an aside, you can iterate this and have kind-level computation in the same way and so on, but that's not done in $\lambda\underline{\omega}$.) So far, I've said that kinds classify types. That's another way of saying that well-formed programs for type-level computation inhabit kinds. In $\lambda\underline{\omega}$ kinds are given by the grammar $$ \newcommand{\TY}{\mathsf{Ty}} \kappa\quad ::= \quad \TY\ \ |\ \ \kappa \rightarrow \kappa $$ Here $\kappa \rightarrow \kappa'$ is the kind of type-level functions such as $\lambda t^{\kappa}. \mathbb{N} \rightarrow \kappa$. But what is $\TY$? Answer: the base kind. The kind that is inhabited by all types that can potentially be inhabited by terms, e.g. types like $\mathbb{B}$ or $\mathbb{N} \rightarrow \mathbb{B}$. It is the only kind that is inhabited by types. This gives a neat classification of type-level programs into types usable to be inhabited by terms, and type-level programs that are only used as components in the computation of such types. Operators like $Pair$ are not kinded by $\TY$ and so cannot be inhabited by terms. Instead $Pair$ has the kind $\TY$ $\rightarrow$ $\TY$ $\rightarrow$ $\TY$, which, by its very shape, cannot be inhabited by types, it can only be used as a program in a type-level computation. Now $\mathbb{N}$ (integers) and $\mathbb{B}$ (Booleans) both are kinded $\TY$, hence the type level program $Pair \;\mathbb{N} \;\mathbb{B}$ also has kind $\TY$ and can therefore be inhabited by a program. 

Thank you Gopi for this question. I'd like to extend the many interesting answers in another dimension that hasn't been mentioned yet. Research is not the only thing we do at university: if you want to stay in academia, eventually you will have to teach. If you are lucky, you'll have to teach courses that are quite far away from your area of specialisation. Quite likely you'll be assigned courses with a substantial programming component. This is where even a moderate ability to program helps substantially: you'll be much better a teacher if you know how to program. First and foremost, you'll be more comfortable with the material, you'll be able to answer student questions better, and you understand the difficulties that students have with learning to program, as you've experienced this learning process yourself. Moreover, you can produce better teaching material. For instance you can test programming exercises yourself before giving them to students, and fine-tune the level of difficulty. There's an additional pragmatic dimension: teaching involves various repetitive tasks that a skilled programmer can often automate, like quickly making a website that students can use to submit coursework, and have it graded automatically (according to the number of automated tests the code passes). 

Tobias Nipkow has formalised a proof in Isabelle/HOL, have a look here (I didn't check that this is the right proof, don't have time right now and I'm not super familiar with Isabelle/HOL, please feel free to edit if you find a better formalisation). The paper A Logical Analysis of Aliasing in Imperative Higher-Order Functions by N. Yoshida, K. Honda, and myself contains a sketch of a proof for polymorphic quicksort. The sketch assumes that the logic provides positive inductive definitions, which the logic we discuss in the rest of the paper doesn't (to keep the presentation simple). It's easy to add them. 

Finally, even though I find the idea of measuring maintainability via query numbers to reach consensus intriguing, I suggest to be more clear about what kind of queries are relevant here (syntax? semantics?), and how the concept of programmer consensus can be made precise. 

The process $P{\langle 0, c, d\rangle}$ in the context of a recursively defined process like $$ P{\langle count, c, d\rangle} \quad\stackrel{\text{def}}{=}\quad c(v).(\overline{d}\langle count+v \rangle\ |\ P{\langle count+v, c, d\rangle}) $$ does the job, although we might quibble about how exact it models the original program. However, there is no problem with a more explicit modelling of state. Translation from recursion to replication is folklore, easy and already discussed in (1): let's assume you have a process $P$ and it uses recursive definitions $X_1\langle \vec{a}_1\rangle = Q_1$, ..., $X_n\langle \vec{a}_n\rangle = Q_n$, where for simplicity we assume that no $Q_i$ uses recursively defined processes. (This restriction is easy to lift.) Then any use of $X_i\langle \vec{a}\rangle$ in $P$ is replaced by an output $\overline{x_i}\langle \vec{a}\rangle$. Let's call the resulting process $P_{new}$. Now the process $$ (\nu x_1 ... x_n) ( P_{new} \ |\ !x_1(\vec{a}_1).Q_1 \ |\ ... \ |\ !x_n(\vec{a}_n).Q_n ) $$ does without recursion what $P$ does with recursion. We might say that each recursive equation is replaced by a 'recursion server'. I'm glossing over a few details here. For more, see e.g. (2). 

There are not that many books on this subject, as it continues to evolve at a rapid pace. Classic books on process calculi (that don't focus on π-calculus-like mobility) are: 

Given this background, it should not come as a surprise that there is somewhat of a disconnect between pure programming language research, and commercial programming language development. While both aim to make software construction and evolution more efficient, especially for large-scale software, industrial programming language work must be more interested in facilitating quick adoption to reach a critical mass and get the network effect. This leads to a research focus on things that working programmers care about. And that tends to be things like library availability, compiler speed, quality of compiled code, portability and so on. Process calculus as we practise it today is of little use to programmers working on mainstream projects (although I believe that will change in the future). Pure programming language research is quite different. It works with simplified models of programming languages: the $\lambda$-calculus is a massive simplification of functional programming. In the same way the $\pi$-calculus is a massive simplification of concurrent programming. These massive simplifications are the key to successful research. They enable us to focus on core computational mechanisms (e.g. $\beta$-reduction for functional programming, resolution/unification for logic-programming, name-passing for concurrent computation). To understand if a language like Scala can have viable full type-inference, we don't need to worry about the JVM. Indeed thinking about the JVM will detract from a better understanding of type-inference. That's why abstraction of computation into tiny core calculi is vital and powerful. So you can think of programming language research as a massive sandbox where people play with toys, and if they find something interesting when playing with a specific toy, and have investigated the toy thoroughly, then that interesting toy begins its long march towards mainstream industrial acceptance. I say long march because language features first invented by programming language researcher tend to take decades before becoming widely accepted. For example garbage collection was conceived in the 1950s and became widely available with Java in the 1990s. Pattern matching harks back to 1970 and is widely used only since Scala. Process calculus is an especially interesting toy. But it's too new to be investigated thoroughly. That will take another decade of pure research. What currently going in process theory research is to take the single biggest success story of programming language research, the theory of (sequential) types and develop the theory of types for message passing concurrency. Typing systems of moderate expressivity for sequential programming, say Hindley-Milner, are now well-understood, ubiquitous and accepted by working programmers. We'd like to have moderately expressive types for concurrent programming. Research on this started in the 1980s by pioneers like Milner, Sangiorgi, Turner, Kobayashi, Honda, and others, often based, explicitly or implicitly, on the idea of linearity which comes from linear logic. The last few years have seen a major increase in activity and I expect this upwards trajectory to continue for the foreseeable future. I also expect this work to start leaking into product focused R&D, in parts for the pragmatic reason that young researchers who have been trained in process calculus will go and work in industrial R&D labs, but also because of the evolution of CPU and computer architecture away from sequential forms of computation. In summary, I would not worry that you don't find cutting edge programming language theory such as process calculus useful in your own work of building languages. That's simply because cutting edge theory does not address the concerns of current programming languages. It is about future languages. It will take a while for the 'real world' to catch up. The knowledge you use to build languages for today is the programming language theory of the past. I encourage you to learn more about process calculus because it's one of the most exiting areas of all of theoretical computer science. 

Not open source, but the AI engines of some successful computer games are based on modal logic, see e.g. "Using Exclusion Logic to Model Social Practices" and "Introducing Exclusion Logic as a Deontic Logic" by Richard Evans. 

Summary. A logical framework is a meta-language for the formalisation of deductive systems, where deductions become syntactic objects. Of course what counts as a meta-language is quite vague, and it is helpful to understand the historical development of logical frameworks. The first logical framework was de Bruijn's Automath (1), which is based on $\lambda$-calculus. Many of the ideas from the Automath language family have found their way into modern logical frameworks. Martin-Löf's work on constructive type-theories, also based on $\lambda$-calculi, has also been influential. Edinburgh's LF (2) is a very influential logical framework. Edinburgh LF is what you get when you enrich the simply typed $\lambda$-calculus with type-dependency. That's all. In order to make type-dependency precise, one needs to replace the function space operator $A \rightarrow B$ on types with type-abstraction, usually written $\Pi x^A. B$, and introduce the kind of types, as well as kind abstraction. In terms of rules, the key is in the elimiation rule for $A \rightarrow B$, resp. $\Pi x^A. B$: $$ \newcommand{\TYPES}[3]{#1 \vdash #2 : #3} \newcommand{\SUBST}[2]{\{#1/#2\}} \frac{ \TYPES{\Gamma}{\color{red}{M}}{A \rightarrow B} \qquad \TYPES{\Gamma}{\color{red}{N}}{A} }{ \TYPES{\Gamma}{\color{red}{MN}}{B} } \qquad \frac{ \TYPES{\Gamma}{{\color{red}M}}{\Pi x^A. B} \qquad \TYPES{\Gamma}{\color{red}{N}}{A} }{ \TYPES{\Gamma}{{\color{red}{MN}}}{B\SUBST{{\color{red}N}}{x}} } $$ On the left we have the rule for the simply typed $\lambda$-calculus, on the right the rule that generalises the left with type dependency. We see that a value 'flows' into the type in the conclusion on the right. I think the interactive proof assistant Isabelle uses intuitionistic second order logic based on $\lambda$-calculus, without any numbers or recursive data types as logical framework. Various others have been proposed. One advantage of using $\lambda$-calculi as logical framework is that binding constructs like universal quantifiers can be implemented using the framework's $\lambda$-binder. Note that most logical frameworks are expressively weak: the frameworks support object-level reasoning, but are insufficient to perform much meta-theoretic reasoning beyond the fact that a particular object-level statement is a theorem. In fact the metal-logic is usually so weak that even proving the deduction theorem for a Hilbert-style object logic is impossible. Of course nothing prevents you from using more powerful type-theories as a logical framework. For these practical and historical reasons, most logical frameworks in use today are typed $\lambda$-calculi, i.e. type-theories. See (3, 4) for more in-depth discussions of logical frameworks. 

One approach to such questions is via encodings. Say you have a language $L_1$ and a language $L_2$ and you want to show that they are somehow "the same", you can do this by finding an encoding $$ \newcommand{\SEMBTYPE}[1]{\ulcorner #1 \urcorner} \newcommand{\SEMB}[1]{\lbrack\!\lbrack #1 \rbrack\!\rbrack} \SEMB{\cdot} : L_1 \rightarrow L_2 $$ and then show that for all $L_1$ programs $M, N$ the following holds: $$ M \cong_1 N \qquad \text{iff} \qquad \SEMB{M_1} \cong_2 \SEMB{M_2} $$ Here $\cong_i$ is a chosen notion of program equivalence for $L_i$. In order to do this for typed languages, one typically also maps $L_1$-types to $L_2$ by way of a function $\SEMBTYPE{\cdot}$ which is extended to typing environments, such that something like the following holds: $$ \Gamma \vdash_1 M : \alpha \qquad \text{implies} \qquad \SEMBTYPE{\Gamma} \vdash_2 \SEMB{M} : \SEMBTYPE{\alpha} $$ Here $\vdash_i$ is the typing judgement for $L_i$. The whole approach is called full abstraction. In order to avoid the "curse of Church-Turing universality", one typically imposes conditions on $\SEMB{\cdot}$, e.g. that it's compositional, or closed under injective renaming. The more conditions $\SEMB{\cdot}$ meets, the stronger the full abstraction result. This is also Orchard & Yoshida are attempting to do (Theorems 1- 5), although they don't quite achieve it. 

Sometimes it is useful to enumerate in increasing order programs that have a given type. A simple example is test generation for compilers: we want to test a new optimising phase and are interested in generating programs that trigger that phase (we use types for this purpose). Since we want to find 'minimal' examples exhibiting bugs in the new optimising phase, it makes sense to start with the smallest programs as test cases first. Another (somewhat trivial) example is superoptimising compilation where we enumerate all possible assembly programs by increasing length. The typing system is trivial in the sense that all assembly programs are admissible. There is a trivial algorithm solving this problem: lazily enumerate all untyped programs and do type inference on each, reject those that fail to type. The trivial algorithm is unlikely to be efficient since for reasonable typing systems, most programs don't have the target type. Surely one can do better by interspersing typing and enumeration. Let's express this problem in a more abstract setting. 

The work by Régis-Gianas et al mentioned in another answer is similar to the first work above by Honda/Yoshida. This has been extended to effectful ML-style languages: 

Short answer. Formal reasoning about binding and $\alpha$-conversion with nominal approaches is closer to intuitive reasoning than alternative approaches. Longer answer. Binders arise everywhere in mathematics and computer science. Dealing with binders has historically been done in a handwaving way (e.g. assuming Barendregt's variable convention). This works well in informal mathematics, but breaks down in formal reasoning. In implementations of programming languages, this problem was initially handled with some form of renaming using a global operation, often called gensym, that creates globally fresh variables. Already in the implementation of programming languages that has some disadvantages, but the problem comes to the fore when reasoning about formal languages with binders, in particular about certified software. The problem is that in early formal proofs about languages with binders, a disproportional amount of effort was dedicated to proving largely uninteresting lemmas about substitutions and $\alpha$-conversion. The core of the problem is that programming language syntax is usually defined inductively. For example $$ P \quad ::= \quad x = E \ |\ while\ P\ do\ P \ |\ ... $$ Given this genesis of programming language syntax, it is natural to prove program properties by structural induction on program syntax. This works fine for simple languages like that above, but once we add binders $$ P \ ::=\ ... \ |\ \lambda x.P $$ the situation changes. $P$ is not a subterm of $\lambda x.P$. We want to identfy programs up to consistent renaming of bound variables, for example we don't want to distinguish $\lambda x.ax$ from $\lambda y.ay$. In other words, programs are equivalence classes ... and structural induction goes out of the window. Several techniques have been proposed for a better mathematisation of the informal handling of binding.