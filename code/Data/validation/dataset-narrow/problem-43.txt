What you do is shifting the texture coordinates. This will move the texture on your polygon, and depending on your settings it will handle the borders which are now probably wrong. however, this will not move your polygon - if you want to move an entire object, you should not do that within the shader. you do that via modifying your model matrix. I assume you have a rectangle, and you want to move it by along the x axis. Then you would edit the correct element of your model matrix for the rectangle. 

If that is what you wanted, then you need to remove the fragment shader position calculation. On a sidenote: I had problems with using in a shader the other day, so it might just be that this doesn't compile. Try using instead. 

So you start with an offset of , which far exceeds the texture coordinate interval and go up to which again exceeds the interval. This is probably the biggest problem you have in your code, calculation wise. Furthermore, you use a variable to increase your and your variables, but this has never been created. This should not even compile. Since your and are meant for the gaussian blur, I guess a new variable is needed. Your next problem is, that iirc a gaussian blur should offset the texture coordinates, not the color. You multiply the readout color though, with your method, which may be wrong. In any case, you don't even have the method, and it is not a standard glsl method. Again this should not compile. 

Yes, such a thing is possible. There are a few different ways of doing it, but the basic idea is to split up the scene into chunks and assign these chunks to different machines. Each machine then traces rays locally within their assigned chunk. The tricky bit is of course when rays leave a chunk, in which case machines need to synchronize and exchange rays that cross the local chunk boundaries. A recent paper from this year's EGSR, Distributed Out-of-Core Stochastic Progressive Photon Mapping, describes how to do this efficiently for Progressive Photon Mapping, although other light transport methods might be feasible. Although such distributed ray tracing is possible, it is certainly not the most efficient form of parallelization, and non-uniform lighting directly translates into non-uniform distribution of the workload. It is really only beneficial when the size of the scene far exceeds the memory of a single machine. 

The goal of Heitz et al.'s model is pretty much the opposite of subsurface scattering: They only consider surface scattering, i.e. the ray can never enter the material. Because microfacets are statistical in nature, they can recast their problem in such a way that it can be solved by microflakes, which allows them to compute properties such as the mean free path to derive a heightfield sampling procedure. However, even though they use microflake theory, they still solve the same problem of multiple scattering on microfacets, and their result is still a BSDF, not a subsurface scattering model. Because it is a BSDF, the path exits at the same location as it hit the surface. This is also mentioned in the introduction: 

radiant energy $Q$ (in joules, $\left[J\right]$) measures the energy, i.e. the energy of a photon times the number of photons. radiant flux $\Phi$ (in watts, $\left[W]\right]= \left[\frac{J}{s}\right])$ measures the energy per time, e.g. don't just count the number of photons but the number of photons per second. $\Phi = \frac{Q}{t}$ a) irradiance $E$ (in watts per square meter, $\left[\frac{W}{m^2}\right]$) measures the energy per time and surface area, or the flux per surface area. $E = \frac{Q}{t A} = \frac{\Phi}{A}$ b) radiosity $M$ (in some papers also $B$) is the same as irradiance, only it's leaving a surface and not arriving at it radiance $L$ (watts per sqaure meter per steradian, $\left[\frac{W}{m^2 sr}\right]$) is the radiant flux per area and solid angle, or the irradiance per solid angle. $L = \frac{\Phi}{A w}$ 

The idea of calculating the cie coordinate is best described with the mathematical formula: $coord = \sum_{i=0}^{n-1}x_i c_i \Delta\lambda$ where $n$ is the number of samples in the spectrum, $x_i$ is the $i-th$ cie function value for the wavelength of the $i-th$ sample and $c_i$ is the $i-th$ function value of the spectrum and $\Delta\lambda$ is the stride between two samples $\Delta\lambda = \lambda_{i+1} - \lambda_i$ 

You have missunderstood this. $D$ is a Normal Distribution Function (or short NDF), so it doesn't really give you a single normal, but a distribution. In a (specular) BRDF you are always using the normal that is the half vector between the incoming and outgoing light, since via your theory, every microfacet is a perfect mirror and thus every microfacet reflects light exactly along the (micro) surface and that one only. You also must not confuse roughness with microfacet normal. The roughness is a more or less arbitrary value (and a scalar, so a single one at that). The roughness is being used differently, depending on the overall BRDF (e.g. roughness in GGX(/Trowbridge-Reitz) is different from the Roughness in Oren-Nayar) and therefore can have different ranges. Still, if you look at one specific NDF, you will see, how the roughness is used. $D(\omega_m) = \frac{\alpha^2}{\pi((\omega_n \cdot \omega_m)^2 (\alpha^2-1)+1)^2}$ with $\alpha = roughness^2$ (a common remapping), $\omega_m$ is the microfacet normal, $\omega_n$ is the geometry normal. You can see here, that the roughness is being treated like a single value. The higher the roughness is, the more random your microfacets (or normals thereof) are distributed. The more that happens, the less your surface will reflect light concentrated into the same direction (i.e. the less highlights you will have). Thus, your material is more diffuse. Since now your roughness is a single value, the map makes sense to have only values in $\left[0, 1\right]$ and therefore is a gray scale image. 

That by itself is fine, but you need to make sure to multiply the rays by the Lambertian BRDF (i.e. dot(normal, ray)/Pi). Even better is to directly sample from a cosine hemisphere, in which case all factors cancel out. This blog post has all the important info in one place. 

To directly answer the question: Simplex noise is patented, whereas Perlin noise is not. Other than that, Simplex noise has many advantages that are already mentioned in your question, and apart from the slightly increased implementation difficulty, it is the better algorithm of the two. I believe the reason why many people still pick Perlin noise is simply because it's more widely known. It's also worth noting that Perlin noise is very frequently confused with a combination of value noise and Fractal Brownian Motion (FBM). Perlin noise, Simplex noise and value noise are all methods for synthesizing coherent noise. On the other hand, FBM (sometimes called "FBM noise"), is what is used when adding multiple layers of noise on top of each other at different scales to obtain more complex functions. The combination of FBM and value noise is simple to implement and can be very useful for terrain synthesis, procedural clouds and friends, and it is quite popular. However, it tends to be mistakenly labelled Perlin noise, misleadingly adding to its popularity. 

"Physically based" is not a very well defined term, so it's difficult to answer this question exactly. In general, "physically based" refers to the fact that the algorithm in question is derived from physically based principles. It's not physically correct (because we can't afford that) and some approximations usually have to be made, but it's well known what those approximations are and ideally there is some intuition as to what kind of error it introduces. This is in contrast to ad hoc models, which are usually made by an artist who observes an effect in real life and tries to write a shader or similar that somehow mimics the look of it. Usually ad hoc models are simpler and cheaper and tend to be the first solutions to turn up for a given problem, but they don't offer any particular insight into what's actually happening. It's also practically impossible to say how accurately such a model is able to reproduce the effect that it attempts to simulate. In the context of rendering, a "physically based renderer" would therefore be simply something that renders an image using physically based principles, which is a very vague classification. A ray tracer is not inherently physically based, and most early ray tracers in fact used ad hoc models for lighting and similar. From my personal experience, "physically based rendering" used to usually refer to solving the rendering equation. However, it seems that in recent years, many game engines have claimed this term as well to mean "we do energy conservation" or "we no longer use phong". So really, there's no hard classification of what "physically based rendering" means, and using ray tracing by itself does not make a renderer physically based. In offline rendering, this term is still mostly used to refer to renderers that solve the rendering equation, whereas in real-time rendering, it more likely refers to the use of microfacet models or similar. But it's possible that the meaning of this term will change over the years.