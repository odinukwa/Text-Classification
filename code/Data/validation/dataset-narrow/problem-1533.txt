You can easily use get_dummy function in Pandas to convert them to numerical vectors. The idea is that categorical variables do not have a numerical intuition e.g. when it comes to the definition of Distance. But just imagine you have one feature Genre with 3 values Comedy, Romance and Crime. Then you can model them in a 3-dimensional space by saying Comedy = (1,0,0), Crime = (0,1,0) and Romance = (0,0,1). It replaces 1 feature with three but intuitively works well. Update I just understood your question after editing it! It was a bit fuzzy previously. But I keep my initial answer and add an update. In this case use the values of the feature Genre (unique values of union of all genre sets in that column) as new features and determine their presence with 1 and 0 otherwise. Should work. 

If we assume that wrongly telling someone that he got HIV simply leads to another test, we may take much care about not wrongly say someone he is not a carrier as it may result in propagating the virus. Here your algorithm should be sensitive on False Negative and punishes it much more than False Positive i.e. according to the figure above, you may end up with higher rate of False Positive. Same happens when you want to automatically recognize people faces with a camera to let them enter in an ultra-security site. You don't mind if the door is not opened once for someone who has permission (False Negative) but I'm sure you don't want to let a stranger in! (False Positive) Hope it helped. 

Yes There are! Networkx I think 20k-30k node-edge would be OK on Networkx, IF YOU HAVE A GOOD MACHINE! Networkx is a great library in Python particularly for Graph Analysis so you have access to great analysis tools beside visualizing but visualizing 20k vertices needs much RAM and takes long. Igraph Igraph is another great tool for Graph Analysis with APIs in Python, R and C. You can do a lot with Igraph library including nice plotting facilities. SNAP SNAP was developed by Jure Leskovec and his colleagues at Stanford. A large scale graph processing library natively written in C with API in Python. This library can handle huge graphs on a single personal machine. 

So let's start! First of all please have a look at the edit I made to your original question. It was not just an edit but implied important conceptual thing so I need to start with them: 

It's not a problem, it's just your definition of your question. It assumes rate 3 as neutral and blow/above that as negative/positive. One may say I assume every rate is positive so each number is a level of positiveness then rate 3 works for him. The main point is that you get the answer for the question you define so be careful about what you want from your classifier to set up your question correctly. 

Nice question! Some Remarks For imbalanced data you have different approaches. Most well-stablished one is resampling (Oversampling small classes /underssampling large classes). The other one is to make your classification hierarchical i.e. classify large classes against all others and then classify small classes in second step (classifiers are not supposed to be the same. Try model selection strategies to find the best). Practical Answer I have got acceptable results without resampling the data! So try it but later improve it using resampling methods (statistically they are kind of A MUST). TFIDF is good for such problem. Classifiers should be selected through model selection but my experience shows that Logistic Regression and Random Forest work well on this specific problem (however it's just a practical experience). You may follow the code bellow as it worked simply well then you may try modifying it to improve your results: 

Choose top $n_{selection}$ and ignore the rest. If you found an optimal solution (in your case you said output=3 is desired) stop, if not: According to $P_{crossover}$ choose 25% of survived samples and reproduce children by combining them through a breaking point (image from here). 

You may go through two different approaches: Unsupervised Learning (Clustering) You can choose attributes who make someone's profile and try to cluster it (e.g. using k-means). If you look at clusters according to different attributes it may give you some insight about the data. Do not forget to exclude names as they are meaningless for analysis. Supervised Learning (Classification) You can use relevant attributes which may affect an output e.g. age and city may affect gender or Nr of children so if u put these as targets you can classify different users according to their age and city. The point is that your data is not suitable for more sophisticated analysis as your features are not many and too complicated (i.e. answering the questions like age distribution among a certain gender or the relation between cities, age and number of children or something like them are the only questions can be answered) I hope it helped and if there are more questions please comment here. UPDATE 1 You can look at your data from different point of view so first try to choose one. For example clients can be clustered via their transactions (when they bought, what they bought, how expensive they bought, etc) or by their personal info (gender, age, payment method, etc). It gives you a first impression of your client data. Then go through clustering using all those information i.e. put personal and transactional features together. It gives you an overview of customers in general and might result in some significant clusters (categories). After that you can look at the problem from product point of view meaning you look at the transactional features and try to see different distributions and histograms to get an overview of what's happening to the product e.g. you can set time as the x axis and try to extract different time series like how many product are sold at this point of time?, what kind of people (age, gender, etc) bought at this point of time? and so on. For general analysis a dimensionality reduction (e.g. PCA) might reveal some information and give you insight. Please note that for any kind of analysis ONLY use relevant features e.g. IDs are not informative but categories. The most important point for recommending product to a customer (or predicting what they buy) is to use your input/output pair properly so the customer info (age, city, when does he/she usually buy something?, etc) are inputs and products (their categories, their type, whatever u know about them) are outputs. Good Luck! PS1: From experience of doing an industrial version of exactly the same project I would say payment method is not much informative. PS2: StackExchange provides an upvote and an accept button for good answers! 

In practice you may sacrifice an error to optimize the other one. For instance diagnosis of HIV might be a case (I am just mocking-up an example). It is highly imbalanced classification as, of course, the number of people with no HIV is dramatically higher than the ones who are carrier. Now let's look at errors: 

Overfitting meaning your model is learning the noise from the data and its ability to generalize the results is very low. In this case you have a small training error but very large validation error. If you inspect (e.g. by plotting) the evolution of training and validation errors, you see that training error is always going down but validation error is goes up at some point. That is the point you need to stop training to avoid overfitting. I strongly recommend you to read this. So, the 0.98 and 0.95 accuracy that you mentioned could be overfitting and could not! The point is that you also need to check the validation accuracy beside them. If validation accuracy is falling down then you are on overfitting zone! 

using this you will have a huge graph (network) of your object on which you can calculate tones of structural and statistical measures. I assume my thought were naive as your question is not clear and you need to provide more detailed example to get a more accurate answer. Good Luck! 

Nice question. Before specifically taking Naive Bayes into account, it is a general machine learning problem when the population of classes are imbalanced. If this is the case then better to balance them, not only in train/test split but also during train itself as the dominating class will bias your result. If the classes are not that imbalanced then you can split things randomly and it's fine. Regarding Bayes Classifier itself the balance inside training set should be more important as NB learns from the statistics of your training set. On test set, it just uses the already learned statistics so portion of classes should not impact. 

First of all check this carefully. You'll find a simple dataset and some papers to review. BUT as you want to start a simple learning project I recommend to not going through papers (which are obviously not basic) but try to build your own bayesian learner which is not so difficult. I personally suggest Andrew Moore's lecture slides on Probabilistic Graphical Models which are freely available and you can learn from them simply and step by step. If you need more detailed help just comment on this answer and I'll be glad to help :) Enjoy baysian learning! 

Again difficult to understand but anyways. In this case you better gather data from another field. Randomly mine some papers from other fields and construct a new class of NOT THIS FIELD. Then go for a binary classification(Logistic Regression works well on TF-IDF features). It should be pointed that as your data is text you can not cover every text which is not in your main class so limit your question to a specific border in domain. Hope I could help :) PS: The question needs a surgery. I will edit it soon and please check if my edit is conceptually correct. 

This is an interesting but broad question. Imagine PCA. You yse it for exploring the data embedded in lower dimensional space but the first $n$ principale components are also used as the features (after projection of data on them). Or you use correlation analysis and remove (deselect) features with high correlation with an existing feature. You calculate the variance of each feature abd low variances tell you that there is no infirmation in this feature. You inspect feature distributions according to target to determine how much they contribute to the prediction. And of course much more ... 

So, the underfitting means that you still have capacity for improving your learning while overfitting means that you have used a capacity more than needed for learning. Green area is where testing error is rising i.e. you should continue providing capacity (either data points or model complexity) to gain better results. More green line goes, more flat it becomes i.e. you are reaching the point where the provided capacity (which is data) is enough and better to try providing the other type of capacity which is model complexity. If it does not improve your test score or even reduce it that means that the combination of Data-Complexity was somehow optimal and you can stop training. 

For more information read this carefully and in case you need any help (specially for implementation) just drop me a line in the comments :) Good Luck! 

It's usually called "Tick" and as it's for x axis, let's say "xtick". You can label it as xticks and give it two values "evenly spacing" and "actual spacing" (This is just an idea as there is no standard terminology as far as I know). 

This is an optimization problem and optimization is all what Machine Learning does in classic sense (mostly not unsupervised approaches). Your problem can be designed to be solved by Genetic Algorithms. For this you need to make a sequences of ingrediants which is called chromosome (genetic representation of solution) and an objective function which is to be minimized/maximized according to the chromosomes (it's usually called fitness function and is to be maximized then). You have genetic operators to be applied randomly on your chromosomes: 

Conclusion My suggestion is to smooth your signals first and go for a simple clustering mthod (e.g. using GMMs) to find an accurate estimation of the number of segments in signals. Given this information you can start finding changing points constrained by the number of segments you found from previous part. I hope it all helped :) Good Luck! UPDATE Luckily your data is pretty straightforward and clean. I strongly recommend dimensionality reduction algorithms (e.g. simple PCA). I guess it reveals the internal structure of your clusters. Once you apply PCA to the data you can use k-means much much easier and more accurate. A Serious(!) Solution According to your data I see the generative distribution of different segments are different which is a great chance for you to segment your time series. See this which is probably the best and most state-of-the-art solution to your problem. The main idea behind this paper is that if different segments of a time series are generated by different underlying distributions you can find those distributions, set tham as ground truth for your clustering approach and find clusters. For example assume a long video in which the first 10 minutes somebody is biking, in the second 10 mins he is running and in the third he is sitting. you can cluster these three different segments (activities) using this approach. 

First of all, interesting question! Most important thing to recommend you a starting point is to know what kind of data you have in hand. So my answer is regardless of that fact. If you have a road-map of studies of students who got into University XXX then you can use model it with an HMM (Hidden Markov Model). In this case the observations are the sequence of courses one passed and you want to calculate the probability of landing in desired university. Transition matrix is the probability of getting course A after getting course B. Emission matrix might be something like how successful they were in the course. Transition and emission matrices can be scored using facts like dropping a course or failing or the education track they exhibit in. 

Well ... the simplest approach is using Fuzzy String Matching and it will work. Just go through the examples in python implementation of it (fuzzywuzzy) and you will understand how it works. You need to find a threshold by practice to determine if two strings are similar enough to be considered as same concept. If it didn't work please drop a line in the comments so I can propose more sophisticated algorithms. Good luck! 

After these points I get back to your question. Yes, what you are looking for is Pattern Recognition and you can use many Machine Learning approaches to solve your problem. If you already knew what are different classes of patterns, it would be called Classification but now, as I understood, you don't know the classes and you want to find similar patterns. Finding patterns of similarity in data is called Clustering. As I don't know if your data is a time-series (e.g. amount of raining vs time) or just a structured n-dimensional data (e.g. amount of raining vs humidity) I try both. Time-Series When you have different segments of time-series and you need their similarity you can use Correlation Analysis or Dynamic Time Warping. If the time series is high frequency e.g. speech data or EEG, then you better convert data to frequency (or time-frequency) domain and then extract features from those segment and then use those features to determine similarity. Please note that the term similarity is pretty wide and it's defined according to the nature of data and problem. You may search for time-series clustering for more approaches. Non-temporal Data Similarity between the parameters of a regression model fitted to those subsets of data might help. Another approach is to apply clustering algorithms to those segments. These algorithms can be applied to the original data subsets or some features extracted from those. Dimensionality Reduction algorithms like PCA can be used for feature extraction and might be helpful for a better clustering. I wrote my answer in rush so I would appreciate any comment or further questions. Good Luck! 

The question is not defined well. You talk about clustering using target which is actually paradoxical however I understand your point. The problem is that, not caring about this paradox may hurt your analysis which comes in second point. According to 1, you classify your points based on target and try to find dense subgroups there. That will not work as you are including the target in your analysis. This is the confusion made from paradoxical definition I mentioned above. You have your targets, so divide your data according to them and analyze each subset of data separately, having an eye on their interaction.