I would try each of these suggestions in this order, and stop when it is good enough. It sounds like using a better stopwords vocabulary will be good enough for you. EDIT: I forgot to mention, but of course you can add more uninteresting words to the stopwords list, if you have the need to do so. You may also want to first evaluate the impact TF-IDF is having in your counts. I think this will work: 

Dropout is also usually compared with neural networks ensembles. It seems it has some of the performance benefits of training and averaging several neural networks. Dropout is easier to calibrate than regularization. There is only one hyperparameter which is the dropout rate and people widely use 0.5 while training (and then 1.0 on evaluation of course :)), see e.g. this TensorFlow example. 

(Purely illustrative example.) Now adjust as you may. XPath is an incredibly powerful notation of accessing XML nodes. Much more is possible than I am writing here. Take this tutorial to learn the full XPath syntax. Some web designers make it much harder to access whatever you want because they do not properly -ify their HTML objects. In those cases, you may have to access a parent node and ask for their child. Or access a sibling and then get the siblings. Anyway, XPath and Python's package make all this incredibly easy. Any modern browser like Chrome and Firefox also lets you easily explore the DOM of any webpage. Just right-click and press Inspect or go to Developer Tools in the Tools menu or some such. Note: some website like scholar.google.com disallow scrapers and are very good at detecting if that's what you're doing. You can specify an user-agent for urllib, but it might be futile. Even advanced frameworks may not be able to help you there. EDIT: I have made a blog post where I elaborate a little more. 

Since I asked this question, it seems the documentation was expanded: We need to provide a ".group" file. For example, the file could be 

Interesting puzzle indeed. The short answer is that you did not pass to . First things first. The has some stochastic behavior. For instance, the splitter code iterates through the features at random: 

One typical way is to change the input and see the impact on the model performance. In images, you can black out parts of the image, and see which parts contributes most to the accuracy. This is widely used for convolutional neural networks, which are hard to interpret otherwise. For numerical variables, you can zero out or add some noise to each feature and see what the impact of that individual feature has on the result. I have seen these kind of things widely used. 

After a detailed analysis of my data I figured out that I cannot drop these two columns from my dataframe as they are too importanct for prediction. I can hash these two features but there is one more interesting thing. There are only 2,000 types of user_ids and mail_ids. So doing one hot encoding can help a lot. My question is that if I convert this into one hot encoding using 'get_dummies' method in pandas with sparse=True, will it be memory efficient or is there any other efficient way to do it? 

If you don't define the validation set/ validation split for your model, then it has no way to check for it's performance because you have not provided anything to the model on which it can validate its performance. In this case, the model will run through training examples, will learn a way to minimize the cost function as per your code and that's it. You get a model which has learned a hypothesis from your data but how good the model is, it can only be checked by making predictions for the test set. 

As told by @smci, this technique is called Data Imputation. There are several techniques which can be used to deal with the missing data. Some of these are: 

There is no preference for stacked generalization. You can use any algorithm whether it be a , etc. The only thing that you need to take care of is the fact that the algorithm which you are applying to your model is useful to your data or not. Also, model stacked at level 1 may not be a very good model for level 2. For example, suppose at level 1 you stacked an XGB model with , but at second level it might happen that the same depth is just an overkill and hence you should discard using that model at second level. So, in short there is no preference for stacked models, the preference is for the data on which you are going to train your model. 

I have a huge data set with one of the columns named 'mail_id'. The mail_id is given in a very creepy format as shown below: 

Hyperparameters and parameters are often used interchangeably but there is a difference between them. You call something a 'hyperparameter' if it cannot be learned within the estimator directly. However, 'parameters' is more general term. When you say 'passing the parameters to the model', it generally means a combination of hyperparameters along with some other paramaters that are not directly related to your estimator but are required for your model. For example, suppose your are building a SVM classifier in sklearn as shown below: 

I don't know how this is implemented in matlab. I know that some packages use cross validation to decide whether to grow the tree or not. Quite simply, they decide whether to grow based on that evaluation. (Notice what a lot of packages call pruning is: during training, they mark branches with some score criterium and then remove if the user wants afterwards.) Personally, I use sklearn which does not have this feature. So, I just do a grid search for several values of and use whatever maximizes accuracy or whatever score I want. 

I think this last suggestion of mine of using Levenshtein distance to the closest English words is overkill. If your documents involve a lot of urban words or hip terms, you could also filter based on the number of Google results, instead of using a common dictionary. Your word hdhhdhhhhhjjj, for instance, gives me only two Google results, one of which is this stackexchange. Data science is about being creative. :) There probably are heuristics or statistics based on the number of consonants in a word, and letter combinations, which you could use to probabilistic shrink your documents' dictionary, but I wouldn't go there. It will be too much work and very brittle. 

I was right in suspecting that self-training could be used for PU learning. In fact, I found the original paper on PU Learning, and indeed the paper is a variation on self-training. (Oddly enough, the original authors had Positive, Unlabeled and Negative examples!) The authors of this survey identify three families of methods: (i) two-step strategy (identify reliable negative examples in the unlabeled data and then use supervised learning), (ii) weight the positive and unlabeled examples, and estimate the conditional probability of positive label given an example (I believe this is akin to semi-supervised self-training), and (iii) just treat the unlabeled data as highly noisy negative data. There are also some interesting loss functions to be used with neural networks (and I imagine could be adapted for gradient boosting) described in Table 1 of this paper. 

This is not necessarily an answer to your question. Just general thoughts about cross-validating the number of decision trees within a random forest. I see a lot of people in kaggle and stackexchange cross-validating the number of trees in a random forest. I have also asked a couple of colleagues and they tell me it is important to cross-validate them to avoid overfitting. This never made sense to me. Since each decision tree is trained independently, adding more decision trees should just make your ensemble more and more robust. (This is different from gradient boosting trees, which are a particular case of ada boosting, and therefore there is potential for overfitting since each decision tree is trained to weight residuals more heavily.) I did a simple experiment: 

In our company, we have a MongoDB database containing a lot of unstructured data, on which we need to run map-reduce algorithms to generate reports and other analyses. We have two approaches to select from for implementing the required analyses: 

The question is, using the second approach and writing the algorithms for MongoDB, can them be later ported to Hadoop with little needed modification and algorithm redesign? MongoDB just supports JavaScript but programming language differences are easy to handle. However, is there any fundamental differences in the map-reduce model of MongoDB and Hadoop that may force us to redesign algorithms substantially for porting to Hadoop? 

Considering another criteria, I think that in some cases using Python may be much superior to R for Big Data. I know the wide-spread use of R in data science educational materials and the good data analysis libraries available for it, but sometimes it just depend on the team. In my experience, for people already familiar with programming, using Python provides much more flexibility and productivity boost compared to a language like R, which is not as well-designed and powerful compared to Python in terms of a programming language. As an evidence, in a data mining course in my university, the best final project was written in Python, although the others has access to R's rich data analysis library. That is, sometimes the overall productivity (considering learning materials, documentation, etc.) for Python may be better than R even in the lack of special-purpose data analysis libraries for Python. Also, there are some good articles explaining the fast pace of Python in data science: Python Displacing R and Rich Scientific Data Structures in Python that may soon fill the gap of available libraries for R. Another important reason for not using R is when working with real world Big Data problems, contrary to academical only problems, there is much need for other tools and techniques, like data parsing, cleaning, visualization, web scrapping, and a lot of others that are much easier using a general purpose programming language. This may be why the default language used in many Hadoop courses (including the Udacity's online course) is Python. Edit: Recently DARPA has also invested $3 million to help fund Python's data processing and visualization capabilities for big data jobs, which is clearly a sign of Python's future in Big Data. (details) 

One approach is to extract the data from MongoDB to a Hadoop cluster and do the analysis completely in Hadoop platform. However, this requires considerable investment on preparing the platform (software and hardware) and educating the team to work with Hadoop and write map-reduce tasks for it. Another approach is to just put our effort on designing the map-reduce algorithms, and run the algorithms on MongoDB map-reduce functionalities. This way, we can create an initial prototype of final system that can generate the reports. I know that the MongoDB's map-reduce functionalities are much slower compared to Hadoop, but currently the data is not that big that makes this a bottleneck yet, at least not for the next six months. 

Comparing two libraries or tools in terms of these things is somewhat that is opinion dependent. Some people prefer for doing almost all the tasks. has also gained quite reputation. But what is better for you depends on what you want do. In my personal experience, I have found that along with libraries is all that I need to do all the Natural Language Processing tasks. 

So this vector y = [0,0,0,1] is your target to these input types that you feed into the network as labels corresponding to your above input data. 

I will assume that the dataset here is being split into training and validation sets. When you split up your dataset into training and validation sets, you need to take care of that you don't throw away too much data for validation because it is generally seen that with more training data, we get better results in most of the cases. So 50:50 is too bad, 60:40 is fine but not that good. You can make it 80:20 or 75:25 for getting better results. 

There can be many reasons for this thing but in most of the cases I have observed one common reason. When you split your data using train_test_split or any other method, it is important to note that the column on which you are splitting the data is significant for splitting considering both the training and testing set. For example if I have a 'time' field in my data and I have split the data into training and testing sets on this column such that there is no value in this column of the test set which matches with any of the values in the same columns in the train set. 

Welcome to the community!! There can be a lot of answers to this question but I would suggest you the approach I took when I shifted from software development to the data science field. 1) Refresh your statistics and probability concepts. You should not go into too much details but you must understand basic things like Gaussian Distribution, Mean, Variance, Probability,etc. 2) Go through the basics of Machine Learning concepts. I prefer Andrew Ng's machine learning course on Coursera. That will help you build a strong foundation and will give you a great start in the field 3) Choose a particular language Python/R for building models. Though it's totally upto you but I prefer python as it has great libraries for machine learning as well as Deep learning. 4) Take part in competitions. We learn by doing not by taking only lectures. I suggest you should join Kaggle and the slack community out there namely, 'KaggleNoobs'. It's a great community. I learn everyday a new thing from there. P.S: Data Science is a vast field. It demands from you various skill set like Data Analysis, Data Visualisation, Machine Learning,etc. So sometimes it can become frustrating too. But once you start enjoying it, you will become a master eventually.