Non-optimal tricks I might use: 1) Use a force directed graph drawing program, then use the layout coordinates to generate a quadtree(z-curve). 2) Use k-means, with k= 3, and distance being the shortest path length. Then recursively apply the kmeans to each cluster. You will get something close to the notion of a space filling curve. When you subdivide one cell into three, use the distance between the generator for each of the three children and the two generators of the other two parent level cells to determine the ordering. For the highest level ordering just pick something. 

More likely that it is $O(n^{2} log(n^2))$. Having $\omega = 2$ seems fanciful as constant factor book-keeping doesn't seem like it could scale. 

You can do much better than checking all n! permutations when brute forcing a solution, $URL$ The grail is showing that you can't do much better than that, or exploiting the fact that most graphs have no symmetry in them and using this to speed calculation. 

NAUTY can be used as a library to help you build a hashtable for the entire poset of graph minors for small $n$. The key would be the cannonial form given by NAUTY and the value would be a concatenation in sorted order of the cannonical forms of it's direct minors. 

You have an endofunction: genetic :: Population -> Population What are the idempotents? What are the indexes and periods? Convergence means that under iteration most Populations hit upon a high fitness Population, i.e. high fitness Populations form a near dominating set over the rest of the population space if you make a graph that has edges f^i -> f. 

So I am trying to figure out an upper bound on the probability of the following... This is a question related to a problem I am working on (not for a class, just for fun) Let $\Omega=\{X_{1},\dots,X_{n}\}$ where each $X_{i}$ is i.i.d Bernoulli with success probability 1/2. Let $Y=\{Y_{1},\dots,Y_{N}\}$ where $Y_{i}\subset\Omega$ and $|Y_{i}|=t$. Let $Z_{i}$ indicate the event that $\sum_{X\in Y_{i}}X\geq k$, where $k$ is some value close to $t$. Let $Z=\sum_{i=1}^{N}Z_{i}$ Say that $I_{r}= \{\{Y_{i},Y_{j}\}:|Y_{i}\cap Y_{j}|=r\}$, that is, $I_{r}$ is the set of all pairs from $Y$ that share $r$ elements from $X$. I am looking for an upper bound on $Pr[Z\leq\mathbb{E}[Z]-t]$. I've seen Janson's inequality and notice that it looks quite similar to this, but I'm not sure where to go from here. Any help or any references would be greatly appreciated. 

I'm posting a suitable answer that I found for this problem. The approach linked here does not exploit the fact that no $t$-tuples of elements of $\Omega$ are present in too many subsets $Y$ (for arbitrary $t$), but it does end up giving me a good enough bound anyway. 

I am trying to find results on the best known time complexity for finding $(1+\epsilon) \log n$ sized cliques in $G(n,1/2)$. More general results would be great, i.e. if $C_p$ is the constant such that the obvious greedy algorithm finds a clique of size $C_p \log n$ in $G(n,p)$ almost surely, then any results on finding a clique of size $(C_p + \epsilon ) \log n$ would be even better. In particular, I am interested in whether we know how to find a $(C_p +\epsilon)\log n$ clique in $G(n,p)$ using $O(n^{\epsilon^2\log n})$ time and $O(n^{\epsilon^2\log n})$ space. Would a randomized algorithm that finds a $(C_p +\epsilon)\log n$ vertex clique with high probability under these time and space constraints constitute a publishable result? Any pointers to relevant literature would be greatly appreciated. 

A while back I wrote up Ronald Read's tree canonization algorithm and put it on wikipedia. I would make a hashtable for each internal node signature, and label them with a list of pointers back to the subtrees they came from. However, it will only work for treelets with true leaves. 

Yossi Shiloach, Uzi Vishkin. An O(log n) Parallel Connectivity Algorithm. J. Algorithms, 1982: 57~67 -- One of my favorite papers. It would be interesting if you could do it in O((nlogn/k)/p) space with p processors in $k$ rounds where each round each processor is only allowed to read in O(n/p) of the edges. 

Apply parallel prefix. $O(n)$ In parallel subtract the prefix that is $k-1$ before it. $O(n)$. Reduce the array in parallel and get the min $O(n)$. You could alternatively formulate this as a monoid that esentially @RB's solution but made to run in parallel. I did one for arbitrary length roman numerals last year for a Iowa Ruby Brigade kata. 

Double your chances by also applying to their Math department. You are going to have to take their grad level Algebra and Combinatorics classes anyway, also real analysis if you want to do stuff like fractal dimension. 

Read Feynman's letter. Very good advice. Always be curious and solve little problems you are good at. Eventually you will have enough solved for a major publication. 

I recall Jason's thesis had something on this, application was finding differences between versions in a code repository: $URL$ 

I came up with a result the other day that arbitrary length Roman numeral evaluation can be modeled as a monoid: $URL$ 1) Is this a known result? 2) If not, any suggestions of a niche journal that might be interested in such a submission? 3) Any known results on the space complexity of finite monoid elements? I have yet to come across a monoid representation with efficient parallel computation that took more than O(log N) space, with N being the number of elements being "added"/"multiplied". Useful monoid data structures seem to be a constant number of counters or a member of a transformation semigroup of constant size; i.e. a fixed length array of size K with elements in 0...(K-1). 

The Knapsack Problems text by Kellerer et al. cites the two references below as proving that multidimensional knapsack is strongly NP-hard already in the special case of CARDINALITY 2-KP (two-dimensional, with unit values). It also reproduces a proof, reducing from EQUIPARTITION, i.e., the existence of an FPTAS for CARDINALITY 2-KP implies that the EQUIPARTITION decision problem can be solved in polynomial time. I believe the augmented resources version of the (fixed dimension) problem, where you're permitted to exceed the bounds by $\epsilon$ does admit an FPTAS, however. G.Y. Gens and E.V. Levner. Computational complexity of approximation algorithms for combinatorial problems. In Mathematical Foundations o f Computer Science, volume 74 of Lecture Notes in Computer Science, pages 292-300. Springer, 1979. B. Korte and R. Schrader. On the existence of fast approximation schemes. In O.L. Mangasarian, R.R. Meyer, and S.M. Robinson, editors, Nonlinear Programming, volume 4, pages 415-437. Academic Press, 1981. 

Several authors, starting with Slavik, have noted that the classical analysis of the set cover $H_n$ greedy algorithm does not readily extend to the set partial cover problem, where the goal is to pick a minimum-cost family of sets to cover $p \cdot n$ of the $n$ elements, where $0<p<1$ is a constant. But it sure seems to! Greedy: repeatedly choose the most cost-effective set, i.e., one minimizing $c(S) /\min(|S-C|,pn-|C|)$, where $C$ is the set of elements covered so far. That is, the standard set cover greedy's cost-effectiveness definition is modified so that the benefit of a set is the min of # new elements and # of additional elements you still need to get. Then it would seem that you can just say: number the elements $e_1,...,e_{pn}$ in order covered (ignoring any additional ones covered--we'll allocate all the costs to these first $pn$ elements), and argue that at the moment when greedy covers $e_k$, choosing all of $OPT$ would take care of your outstanding $\ge pn-k+1$ element needs, with cost per "satisfied element need" of at most $\alpha = OPT/(pn-k+1)$, so there's got to be a set that's at least that good, so greedy's going to choose one at least that good, which gives us a total bound of $OPT \sum_{i=1}^{pn} 1/(pn-k+1) = H_{pn} OPT$. But apparently this argument is flawed. How so? (Slivak writes in his thesis, "Even though [the algorithms] are quite similar, it turns out that the approach used by Chvatal, Lovasz, or Johnson cannot be used to establish a reasonable bound on the performance [...]. The reasons are that only a fraction of points of the set $U$ are covered and that the part of $U$ covered by the optimum partial cover can be completely different from the part covered by the greedy cover. This makes the analysis of the performance bound [...] quite complicated." $URL$ And Kearns proved a $2H_n+3$ bound, and presumably not because he simply overlooked the obvious approach.) 

I am trying to track down the name of this digraph and some references: You take all members of the transformation semigroup on $n$ elements, $T_{n}$. For two members $x$ ,$y$ ; if $x$ is in the semigroup generated by y then you put an arrow from $y$ -> $x$ . You would read this as "$x$ is in the semigroup generated by $y$". Alternatively $y^k = x$. What is the name of this digraph? References would be great. If you wanted to visualize it, the symmetric group $S_{n}$ would be one of the connected components. The sinks are idemptent transformations like (0,1,2,3), (1,1,1,1), ... 

Every integer has an associated Kolmogorov complexity; the shortest program that prints that integer. There are $\approx {x \over ln(x)}$ primes up to $x$ so primes have lower Kolmogrov complexity than composites on average; $\approx ln({x \over ln(x)})$ vs $\approx ln(x)$. As a side effect you have to have some large gaps between primes; otherwise you could encode every number as the previous prime plus some small number of bits. 

NAUTY "colors" nodes with constant depth neighborhood canonical forms. Babai's new algo does likewise with log size neighborhoods. The kicker is that in a random graph the diameter is about log n, so you end up gobbling the whole thing. Definitely worth doing for sparse graphs, can really cut down the state space you need to search. Also when you have to go brute force, only check repeated prime cycles, not all k! $URL$ 

Joyal's Combinatorial Species, Sedgwick/Falojet's "admissible constructions" of Analytic Combinatorics, and Yorgey's Haskell Species are all good. Power Series Power Serious by McIlroy of UNIX diff fame is also a must read, as is the chapter on corecursion in The Haskell Road to Logic Maths and Programming. The historical works by Buchi edited by Saunders MacLane and Chomsky/SchÃ¼tzenberger make the connection between power series, algebras, trees, and finite state automata. The Transfer Matrix Method described in Stanley shows you how to compute generating functions from weighted automata. I'm still working out the best way to translate between the domains (GF, weighted automata, algebra, tree, recursion) efficiently. Right now I'm shelling out to SymPy since there isn't a good Haskell symbolics package yet. Personally, I've taken the iteration graph of an endofuction then calculated a min dominating set on it to get an exact black box search bound, $URL$ Not sure what types of complexity results you were searching for, but that technique is very powerful in examining any endofuction over a finite set. --Original Oct 2 '14 at 15:37 answer-- Take a look at Brent Yorgey's thesis which follows the paper of his you cited. $URL$ 

I don't think this is true as stated, since if $(u,v)$,$(u,w)$,$(v,w)$ is a triangle in $G$ then clearly there is no way to "two color" the corresponding hyper edges in $H$ no matter how many isomorphic copies of $G$ you make, yet, $G$ may still be $k$-colorable for $k>2$. Here is a different way that may work. Following your same set up, Let $e^{H}_{u,v}= {\{ x_{1,u},x_{1,v},x_{2,u},x_{2,v},\dots ,x_{k,u},x_{k,v} }\}$ and let $$E(H)= (\bigcup_{(u,v)\in E(G)} e^{H}_{u,v}) \cup {\{f_1,\dots ,f_n}\}$$ If $G$ is $k$-colorable, with $c:V \rightarrow [k]$ as the coloring, then for each $j\in[k]$, and for each vertex in $c(j)^{-1}$, color it's copy in $G_j$ green, and the rest of the vertices in that copy red. This is clearly a two coloring of $H$ since we have ensured that for every vertex, it is colored green in at least one copy $G_i$, and for any edge $(u,v)\in E(G)$, the color of $x_{j,u}$ is not the same as the color $x_{j,v}$ for some $j \in [k]$. Note that we didn't need to use the $f_i$'s here. For the other direction, this was the best start I could get... Assume that we have a $2$-coloring of $H$, by having the same element $y$ in each $f_i$ we know that for no two distinct vertices $u$ and $v$ did our $2$ coloring assign green to $x_{j,v}$ and red to $x_{j,u}$ for all $j\in[k]$. This is because we have to color $y$ red or green, and y appears in $f_u$ and $f_v$. This implies that if $x_{j,v}$ is colored say, red, for all $j$ and $x_{j,u}$ is also colored red for all $j$, then $u$ and $v$ are not adjacent (o.w. we couldn't color $e^{H}_{u,v}$). I am stuck here. Somehow you need to construct a $k$-coloring of $G$, but it's not clear how. I hope this helps (I would have left as a comment, but it's too long).