I know that joins of type Sort-Merge don't use nonclustered indexes, because they work by sorting both tables and then joining them. But I've read recommendations to noncluster index all FKs, I believe that's for helping joining. Is there then some type of join that uses nonclustered indexes? 

Is there a tool for quickly backup and restore a table's data, preferably to a csv file? It would be best if I could do it using batch. Run a .cmd file and data is backed up into a file with datetime name. And anytime needed I can just run another .cmd file to restore a file into the table. 

I created a table in a DB that already exists in another DB. It was initially populated with the old DB data. The table's PK had to receive the values that already exist on those records, so it couldn't be autoincrement. Now I need the new table to have its PK as autoincrement. But how can I do that after the PK already exists and have data? 

I have a table with 4 distinct data-type fields. For each record, I need to find the max and min date over them. That's easy to do for a set of records over a unique field. But how to do it in my case, without using cursor? I'd need some sort of function that receives a list of fields, like . 

With this I see those most frequent customerID. But they appear only once on the top of the list. Example: 

I have a CRMOffer table containing customerID and other CRMOffer data. And a Sell table containing customerID and sellValue and sellPK. Sell table has a explicit PK, CRMOffer sadly doesn't have one available. I must join these tables, so that I can relate CRM offers to actual sells. If Sell table doesn't have a specific customerID, sell value will be null. But if it does, and there are multiple CRM offers to the same customer ID, the join will result in duplicated SellPK and sellValue. I must then group by CRMOffer's customerID and sum sell values, but sum only once for each SellPK. A given sell event may be duplicated, in case CRM made multiple offers to a customer that generated a sell. But there may also be multiple distinct sells from the same customer. Sorry I think the explanation is kinda confusing. Basically I must sum values of distinct sells being careful to exclude duplications resulted to multiple CRM offers. I can see if a sell is duplicated by its SellPK. I can use to remove duplications from customerID, but I can't find a way to remove duplications from sell value. 

Community wiki answer: Best guess: The plan chosen to update stats is either parallel, or more parallel, on the 2014 box than on the 2008 R2 box. Parallel update stats for has been around since 2005, and for sampled statistics starting with 2016, see Query Optimizer Additions in SQL Server 2016 by Gjorgji Gjeorgjievski on the SQL Server Database Engine Blog. If you have Enterprise Edition, you could use Resource Governor to limit the CPU being used by your maintenance job. Consider also voting for the Connect suggestion Add parameter to Update Stats by Javier Villegas. Related Q & A: Parallel Statistics Update 

...you should still be able to use separate and statements, for the same effect. Using the same basic criteria as in the , you should be able to: 

If you want to use to sync the table on the remote server, but you can't because it's on the remote server... 

The update to your question shows that the differential backup was indeed compressed. Backup larger than expected That 10GB of data could be spread over a lot of pages, and those pages could total a great deal more than the 10GB you inserted (especially with poor clustered index choices). Note also that differential backups operate at the extent level, so any change to any page within an 8-page extent will result in the whole extent being backed up. Check the after doing log backup or use . If there is an open transaction since yesterday, your full and differential backups will be larger due to the included active log. There may be no open transaction now, but if a long-running transaction was active at the time of differential backup, the backup file size will be more than expected. The fact that your log grew to 500GB (and you take regular log backups) means the log could not be cleared for long time due to something (probably an open transaction). All the log associated with that transaction was added to the differential backup. If you want to check if it's so, try to restore your database into another (new) database, perhaps on another server. This way you will do 2 things: 

Community wiki answer: Please ignore the sleep and queue waits. These should be filtered out as they are always going to be high, by definition. They collect a minute of wait every minute. tells me it can't keep up. That said, I don't see that as your major problem here. For latch contention, you might consider this whitepaper. However, I'd be much more concerned about the and waits. Are you doing a ton of cross-server stuff? Can any of this be consolidated, or are these things communicating with non-SQL Server RDBMS platforms? I don't think 2008 -> 2008 R2 should cause any severe issues, but you may want to be sure that you have updated statistics everywhere (with , preferably during a maintenance window), as this is a common cause for plan/perf degradation after an upgrade or migration. But I would also strongly consider getting on the most recent branch and installing the most recent CU available. You should also keep in mind that mainstream support for 2008R2 ends in two months. And while I hesitate to advocate Thomas' line of thinking that a clustered GUID is always better than an column, given that you have Fusion IO and your latches are still occasionally exceeding 2 seconds, you may need to consider redesign, because obviously throwing fast I/O at the problem isn't solving anything. See this post and this post for some edge case enlightenment (again, please don't consider this an endorsement of "heaps > clustered index" or "GUID > identity"). 

No, is only relevant for IST. GCache is not used for storing incoming transactions to a joiner node during SST. Gcache is used for storing transactions in case another node requests a state transfer. If the GCache is large enough so that it holds all the needed transactions, then this will allow for an IST. Incoming transactions to a joiner node during SST are instead stored in the local receive queue (max size controlled by the variable and current size seen from the status variable ). This queue is independent from . 

That said, there are cases where InnoDB will create on-disk temporary tables even if the query could potentially have been handled in memory. See 8.4.4 Internal Temporary Table Use in MySQL for details. These are cases when a RAM disk could perhaps have been helpful. Here's a blog entry (from 2012) about how to put the MySQL tmpdir on RAM-disk. However, any RAM used for the tmpdir in a RAM-disk solution is RAM that could potentially have been used for the all-important InnoDB buffer pool. So make sure you have a large enough buffer pool for your data working set before you consider using any of your RAM on a RAM disk. Assuming you put the tables' data files on the RAM disk and you plan to snapshot it, then you also need to take steps to ensure you're getting a point-in-time consistent backup. These kinds of steps will make the RAM disk slower. So alternatively, instead of using a RAM disk, you could use Galera as is, but take all possible precautions to avoid the creation of on-disk internal temporary tables. You should obviously also make sure to use SSD instead of spinning disks. Another technology to consider may be MySQL NDB Cluster: 

One factor that can cause the creation of temp disk tables is the inclusion of or columns in the s. To avoid this you can use the function in the to shorten the data as you may not need the full contents of that column in the result set. However, it seems a / query will only create a temporary disk table if other factors in the query cause the creation of temporary (non-disk) tables. Also, is really shorthand for which means the database needs to eliminate duplicates to produce the result set. Use instead of just to avoid what in this case looks like wasted effort. In many cases you will see that the UNION RESULT line disappears from the EXPLAIN because the database no longer needs to create a temporary table. According to the MariaDB Knowledge Base on UNION/UNION ALL: 

Not clear to me if you are looking for synonyms. If so, and you need mere synonyms in the future, check out the word-choice tag on English Language & Usage Stack Exchange. You might need to be careful with column names like these as they might confuse a user, since they can have a different meaning than row status or record type. 

This finally did execute, but took 29 minutes, 16 seconds. The operation itself should be pretty quick (metadata-only) so I imagine that almost all of that time was spent waiting to acquire the necessary (schema modification) lock. With the new field in place, I was able to quickly add the default value to it via the script: 

The problem may have to do with the Microsoft driver itself. Use an IBM provided driver instead. Instructions to install this can be found here. Vendor-supplied drivers are often far superior: more stable and with higher performance, compared with those provided out of the box by Microsoft. 

In my particular scenario I'm simply looking to pull out the filegroup allocations of a given partition scheme, regardless of what's been set against the partition. All I needed really was: 

It's probably not the execution plan that's making it go faster. It's the data. After you run a query, SQL Server may keep the data in cache. Therefore, it doesn't have to read from disk to get the information, instead it can pull it from RAM, which is much faster. While your execution plan may also be stored in the plan cache, I highly doubt compiling an execution plan is increasing your query execution time seven-fold. The plan and data will automatically stay in memory, if you use them frequently enough, and if there isn't any "memory pressure" (caused by too little physical memory) on the server. You cannot really force a plan or data to stay fixed in cache - it will be tossed out if memory is too tight. 

A backup never breaks the log chain. A non- backup just resets the differential base (the reference point for a backup). So, no, the would not prevent you successfully performing the following restore sequence: 

Community Wiki answer created from a comment left on the question: You can change the port for EM Express with and . But EM Express is optional, you can just simply skip it. - Balazs Papp 

Check your config files and and look for the socket parameter. Make sure it's the same for client and server. A good place for the socket parameter is in the [client-server] section in . The socket file is created by the MariaDB server when it starts up. Note that there are other places where MariaDB config files could reside, and these could potentially override your settings in the two files mentioned above. For more details, see this page in the MariaDB KB. Also note that you may be able to connect to MariaDB through TCP rather than through the socket. You can specify that you want to use TCP rather than the socket with . If this is successful, you can then do to find out where the socket file really is (or should be). You can then connect with or edit the config files so that the client reads it correctly from there. In some cases you may also be able to figure out what socket file location is used by listing the processes: which could output something like 

The relationship between and is one-to-one, so if you wanted to simplify your table structure, then these two tables could be merged into one. Are you 100% certain that every question will have only 5 (or less) options? For flexibility reasons, it might make sense to have only one answer option per record. In that case, the table must remain separate from the table, and it becomes a one-to-many relationship. Which is the right answer option? You should probably indicate that somehow. In the table, the is obviously the primary key. The is a foreign key. And you seem to have forgotten a foreign key to , as well as a foreign key to a table to indicate who gave this answer. A minor issue: In the table, the column is a . This is up to 16 million bytes. Do you really need that many? A column can store up to 64K bytes, maybe that would be enough? Also, you would want to use for your columns. The reason for this is that auto incrementing ints start at 1, so by allowing signed s you're effectively wasting a bit and limiting the range of values you can use. There may be other opportunities for improvements, these are just a few I noticed.