You can "link" those types and handlers with a specialized linker. Unlike using open datatypes, you concatenate the list of constructors and the case functions. But the linker must add cases for partial application: Without whole-program analysis, you cannot predict which partial applications can be used for which function, so you add all cases. An $n$-ary function can be partially applied to $i$ arguments (with $0< i < n$) and produce a function of arity $n-i$, which can be partially applied again. 

Background It's well known that, in a bicartesian closed category (BCCC), if the initial and final object coincide (that is, if the category has a zero object) the category collapses (with all types being isomorphic) by $A \cong A \times 1 \cong A \times 0 \cong 0$ for all $A$. This means, for instance, that the category of pointed sets, with its zero object, can't be bicartesian closed. But the category SCpo also has a zero object for the same reason: all objects are structured sets (CPOs) with a bottom element $\bot$, and arrows are strict (and ($\omega$-)continuous) functions, so they preserve $\bot$. Indeed, this is attributed to Smyth and Plotkin (1982), who describe this category as $\mathbf{CPO}_{\bot}$ and state it lacks categorical products; other categories they consider lack other features of a BiCCC (e.g., their $\mathbf{CPO}$ lacks sums). What is not clear to me is whether every way of handling $\bot$ falls into this trap. However, knowledgeable people on Reddit seem to repeat this claim without good sources (Filinski's master thesis was the best reference I got, and it doesn't lay out a generic categorical argument). 

Can the above two statements be enough to make a general statement about the probability of the satisfaction of the property in $G_{n,p}$ Any references would be very helpful? 

I am asking the question on a slightly abstract level and it may depend on the specifics but it would be great to have related references or ideas. Consider the random graph model $G_{n,p}$ where its a random graph on n vertices and each edge is selected with probability $p$. I want to prove that a certain property $P$ is satisfied by graphs in this model with high probability ($\rightarrow 1$ as $n \rightarrow \infty$). I know/can prove the following two facts 

I have seen a bunch of results concerning Matrix Completion, PCA, Compressed Sensing where a common theme has been to relax the Rank constraint/objective by replacing it with Nuclear Norm. I was wondering if there is a survey of some sort which collects these results, compares them and presents the basic underlying technique. I havent read the original papers yet so they might be the best reference but the purpose of the question is to know if there are other easier to understand references to get started on this topic with. Thanks in advance 

I am looking for a way of getting a good estimate of the eigenvalues of random bipartite d-regular graphs. The literature has very precise values the proofs of which are very involved and since I am looking to extend the estimates to a more general scenario I dont want to get into very involved techniques. The kind of bounds I am looking for can have constant (or even log factors) thrown around. One way I have in my mind is to use the matrix Bernstein inequality by expressing a random bipartite d-regular graph as a sum of d independent random matchings and then black box the Matrix Bernstein Inequality result. This gives satisfactory answers for me with the caveat that summing up d random matchings does not necessarily produce simple graphs (edges can get repeated), however I feel that the estimate that we get from Matrix Bernstein should hold for the random regular graph case too. Is there an easy way to get around this difficulty? Thanks in advance 

To the aspiring scientist: Don't assign too much weight to any answer on such matters, and don't assume that a small and highly non-random sample represents the common views among senior (or non-senior) people in the community. In general, think for yourself! See $URL$ for more details... To the senior scientist: Be careful about what you say, since it may be misinterpreted in harmful ways and/or have more impact than what you intend and/or perceive. 

I was just referred to this question by graduate students that, in my opinion, were far too influenced by the answers. So let me start with two generic advises. 

Re the discussion itself, I think that the idea that credits are non-monotone is not only utterly non-intuitive but also utterly wrong, and I am talking as a person who sat on numerous committees that took various career decisions. A person who has $X$ fundamental contributions, $Y$ important contributions, and $Z+1$ nice/legitimate contribution is ranked higher than one who has $(X,Y,Z)$, regardless of the numerical values of $X,Y,Z$ and assuming that quality captured by the $(X,Y,Z)$ triples is exactly the same. Trade-offs between different types is a different question, ditto re how much credit does each increase give... In other words, for any set of works $S$ and any additional work $a$, the credit of $S \cup \{a\}$ is (strictly) bigger than to $S$ [i.e., strict monotonicity]. In my opinion, people who claim the opposite just assume that a larger number $Z$ implies a decrease in what the value of $X$ (or $Y$) could have been. But this assumption may be wrong and more importantly is irrelevant to the comparison at hand. That is, if you compare a case of $(X,Y,Z)$ to one of $(X,Y,Z+1)$, you must rule that the second person (called B) was able to meet the performance of the first (called A) although B also did another work of 3rd type; so B is clearly better. Indeed, you may think that B could have done better investing more energy in Type 1 (which is not always true - see below), but that's a comparison against an imaginary B, not against A. (And when you have a case of $(X,Y,Z)$ against $(X,Y,Z+10)$, the same holds is stronger terms.) In addition, I think there is also a confusion between the works and the publications. If a work already exists in writing, and assuming that it has its merits, then it can only be advantageous to publish it in a adequate venue, where by adequate I mean one that is intended for works of this profile (wrt quality and scope - publication in a too prestigious conference may actually hurt, since it may generate some annoyance and even bad opinions re the author). But if one still has to develop a work from an initial idea (or "only" write it - which always involves some more research...), then one may consider the trade-off between the amount of time required versus the importance of the work. Finally, as I hinted above, it is not clear that one is better off aiming all the time at Type 1 (i.e., fundamental work). Firstly, this is infeasible and thus problematic/harmful. Secondly, and more importantly, one is always better off following the inherent logic of his/her own interests and ideas/feelings, and aiming to do as well as possible. See more in the aforementioned webpage. Oded Goldreich 

Are there results/techniques pertaining to the analysis of squares of random matrices ? More specifically, let $A$ be an $n\times n$ matrix such that each entry is $1$ or $-1$ independently and with equal probability. Now if we want to analyze for any $u,v \in \{-1,1\}^n$, we can make a case for concentration of the value of $u^TAv$ using chernoff bound arguements. However suppose now we want to analyze the value of $u^TA^2v$. This time due to a lot of dependencies among the variables a chernoff type arguement becomes difficult or at least I cannot see it straightaway. Could someone point me to an analysis for this scenario ? 

I wanted to add this as a comment but it was too long. I am not sure if this completely answers the question. For bipartite graphs for instance we can possibly get a simple first cut bound from a simple trace method. Lets look at the adjacency matrix. In your case we want to show that $\lambda_k$ is not close to 0. So consider $Trace(A^2)$. For a d-regular graph this is always $\geq dn$. Therefore $$\sum \lambda_i^2 \geq dn$$ which implies that $$2d^2k + n\lambda_k^2 \geq dn$$ Now for $k << n$ and $d$ a constant one can essentially get a $\Omega(\sqrt{d})$ bound for $\lambda_k$. So indeed they cant be very close to 1 in the normalized laplacian. I am not sure whether the parameters work out in the way you want them to.