Now, this query should produced row-level locks for the sake of having data available for repeatable reads. In other words, this is a transactional query. CAVEAT OPTION #2 has advantages over OPTION #1 

At the very least, you must make sure the packet sizes for both the machine you mysqldump'd from and the machine you are loading are identical. There may be two(2) approaches you can take: APPROACH #1 : Perform the mysqldump using --skip-extended-insert This will make sure the MySQL Packet is not inundated with multiple BLOBs, TEXT fields. That way SQL INSERTs are performed one at a time. The major drawbacks are 

What this will do is create a shadow copy of the table being changed. All reads and writes can go on with the original table. Additional changes made to the table is placed in the shadow copy. Everything is merged and presented at the end. CAVEAT #1 The online schema change feature uses the folder mapped in tmpdir. Make sure the copy of the table can fit entirely in tmpdir. For example, if tmpdir is mapped to and the table you are changing is 7GB, there has to be more than 7GB free in . If is not large enough, please consider mounting on a large volume. CAVEAT #2 Again, from the MySQL 5.6 Documentation for 

With innodb_fast_shutdown enabled, this cause mysqld to purge everything and its grandmother out of the InnoDB plumbing upon shutdown. Next, move the datadir to the readonly media. Then, add this to 

There is no natural way to do this except to use information_schema.tables to record all columns with auto_increment option. You could collect those columns as follows: 

Column a is in front of the index as an eq_ref, but Column b is not. This is a range query. Depending on the cardinality of Column a (cardinality visible in SHOW INDEXES FROM testtable), an index scan if cardinality of a=1 is very low and the total number of rows with Column a=1 is less than 5% of the number of rows in testtable, otherwise a full table scan is chosen by any Query Optimizer (MySQL, Oracle, PostgreSQL, SQL Server, etc). Note this query : 

Even though this is right, it would be illogical to script this convoluted method for all rows, especially since some rows have different formats. Therefore, it is in your best interest to convert and columns into or 

This will give you high availability for writes Scenario #2 : If the Slave goes down, AAAHHHH !!!! No data read !!!! Need I say anything else wrong with this ??? SUGGESTION: Setup Multiple Read Slaves This will facilitate Data Redundancy. Use one of the Slaves to do mysqldump backups (if the database is fairly small) or LVM snapshots (if the database is fairly large) Give it a Try !!! CAVEAT : Make plenty of backups of everything before implementing any of this !!! 

If the values do not come back the same, then something is out-of-sync. If you want to examine a bunch of table in bulk, you can down Percona's MAATKIT. You will need two specific tools (Percona also has the Percona Toolkit that they themselves forked from MAATKIT which is now being promoted more) 

I searched around Google, and I'll admit, it's tough to find any documentation. You can try these links 

Not every user needs this. Giving the SUPER privilege to just anyone can actually hamper a DBA from logging into mysql if max_connections is reached. 

If your range was Aug 13th 5:00PM to Aug 14th 2:00AM, you must span two binary logs. So, you would run the following: 

Galera will still run in a two node setup. However, there is always the threat of a split-brain scenario. For example, suppose you have DB1 and DB2 form a two node Galera Cluster. If DB1 goes down, you need to failover to DB2. While DB1 is having maintenance done to bring it back up, DB2 gets all your changes. By the time DB1 is back up, doing an IST is virtually impossible (or at least too late at this point). You would have to perform a full SST from DB2. Easiest way to do this is to delete the Galera cache files before starting up DB1. During the SST, DB2 is in a read-only state (as a donor) and cannot process any inserts, updates, or deletes. When there is a third node in the Cluster, at least one server can collect inserts, updates, and deletes. You will also find Percona XtraDB Cluster: Failure Scenarios with only 2 nodes more informative. 

Additionally, if you do not need every column from the hits tables, then only include the column you know you will access. The same goes for players, stadiums, and games. In other words, as an example, if you only need the playerName from the player table, then you do not need player.* in the SELECT. You will need just player.playerName. VIEWPOINT #4 : You may need to index the leagueLevel column You will need to do the following to make the needed index: 

You could copy the MyISAM tables straight to the target database From DBServer2, run these Linux commands 

PROBLEM #1 You innodb_thread_concurrency is too small. SUGGESTION: It should either be or . Please read my post from (MariaDB 10.1.22 to use more RAM rather than CPU) on why. PROBLEM #2 I don't see innodb_read_io_threads and innodb_write_io_threads mentioned. By default, both are . SUGGESTION: I would double both to (Percona Server's default). Please see my post from (MySQL using too much CPU). PROBLEM #3 Why is your innodb_log_file_size ? InnoDB Redo Logs are written sequentially. IMHO, writing sequential files on an SSD is not that great (See my post from : MySQL on SSD - what are the disadvantages? and : What is the best storing device for DB transaction log?). SUGGESTION #1: You could configure InnoDB to have the Redo Logs on a separate disk. SUGGESTION #2: Please make innodb_log_file_size for now. Then, go figure out the proper size by reading my post from (MySQL 5.5 - Innodb - innodb_log_file_size higher than 4GB combined?) where I point to two posts from Percona's mysqlperformanceblog: 

facilitate automated logging via triggers You would definitely have a penalty to pay because you would introduce additional disk I/O using triggers to record every login. You could pull off some elaborate infrastructure nuances to alleviate most of that disk I/O as follows : 

Please keep in mind that is a DDL command that tells the InnoDB storage engine to pretend there are no records. According to the MySQL Documentation on TRUNCATE TABLE : 

This will expose all remote servers known by the MySQL Instance, including the password in plain text. To see all the federated tables, run this: 

SHUTDOWN MySQL 5.7 now features the SHUTDOWN command, which requiresthe SHUTDOWN privilege. SHUTDOWN METHOD #1: Within mysql client 

There is actually a clean function for handling the conversion : TIME_TO_SEC For example, let's take your time from the question : 

That way, every manual or automatic flush of binary logs will delete logs older than 2 days. Once expire_logs_days is set, mysqldump will cleanup old logs every time it does --flush-logs. 

In order to see if a set of 17 columns exist you will have to concatenate them and then query the table like this: 

The table has no index on event_time. Your must therefore perform a full table scan every time. This has to be a big contributor to the slowness. SUGGESTION #1 Add an index 

Make your query a subquery. Take the result of the subquery and find minimum id grouped by idLocal,time ordered by id PROPOSED QUERY 

Algorithmically, it is doing what your colleague says. But, do you see what it is doing ??? It is generating 10,000 temp tables each containing 1 row after traversing 317 million rows through in the InnoDB internal index. Each temp table is a complete regeneration of the rowids in sernumbers_results_2009 table along with executing handler_read_prev commands internally to sort the data by an index scan from the back of the internal rowid index. Also, please remember you are dealing with InnoDB. Who knows what Multiversioning (via MVCC) is going on so that the INSERT is completed without interference and with rollback capabilities. Is there any reason why this query wouldn't work for you ??? 

In a Galera cluster, any node can play the part of a Master node. Such a node can execute DML (INSERTs, UPDATEs, and DELETEs). If DML statements are spread across multiple Master nodes and are targeting the same tables, you must expect log conflicts. Why ? Galera wsrep libraries were designed to commit DML changes if all nodes in a cluster are ready to do so. If one node cannot do so, all nodes must abandon a commit. It's basically all-or-nothing commits. If you look at the bottom of the document you referenced, there are suggestions to minimize log conflicts under the heading : 

Why do two sets of heavy lifting ??? Let MongoDB do it. For this example, suppose IP of is with hostname STEP 01 : Enable Replica Set on the PRIMARY only Make the following changes in on the PRIMARY 

Here is the key thing to remember : Any instance of MySQL that works for your query in a specific environment is itself a corner case. Once you change one or more of the twelve(12) evaluation aspects, the corner case is due to break, especially given the first nine(9) aspects. 

You should disable anything related to such keys before dropping them Using the variables FOREIGN_KEY_CHECKS and UNIQUE_CHECKS in your session, run 

You could do SELECT SQL_NO_CACHE. This is used by mysqldump to always dump fresh data to a text file. This may also help flush the innodb buffer pool if the table being SELECTed is InnoDB. If you cannot maniuplate the SELECT statements, then set query_cache_type to 0. That will make all SELECTs behave like SELECT SQL_NO_CACHE. You can do this without restarting mysql: Step 1) Add this to the /etc/my.cnf 

Please note that both of those columns are VARCHAR(255). Together their maximum character length could be 511 (255 characters X 2 + 1 byte for VARCHAR length management). Since UTF8 is a multibyte character set, 511 * 2 = 1022. That's way past InnoDB's limit of 767 bytes. Even if you convert the table to MyISAM, it's still past 1000 bytes. I would suggest drastically reducing the widths of and . 

I would venture to say no up front in terms of the table. However, the order of fields in an index and the number of fields in a WHERE matter a great deal. Example : You have the following table : 

If you are running MySQL 5.6 already on the Windows machine and you are trying to run an additional MySQL 5.6 instance, make sure you change the following under the group header in your 

There is a way to set the maximum number of rows on a table for MyISAM. According to the MySQL Documentation under : 

These and other problems can result and you would not even know it until mysqld restarts and nobody can log in, execute a stored procedure, or schedule a cron'd event. Therefore, with good reason, these 10 tables were left as MyISAM. Again, please don't change the storage engine of anything in the schema. 

If you want to keep another number, change the line to whatever number you need. Give it a Try !!! UPDATE 2012-07-05 17:52 EDT If you want to limit the DELETEs to every 100 INSERTs, here it is: 

The Slow Query Log can be handled for CSV and MyISAM CSV (RDS) For those who used MySQL RDS and enable the slow log, Amazon will use CSV only. The RDS environment can auto-rotate at a specific time every night. If you using MySQL RDS and wish to use slow logs, let RDS do its thing. CSV (Non-RDS) If you wish to manage you own slow log rotations (CSV or MyISAM) Run one of these at 11:59 PM every night Log Rotate One Day 

In the OS, go to that folder, run (or for you people using Windows) and look for any file that begins with the hostname followed by followed by six digits. The only way to disable relay logs is to disable replication completely. Otherwise, MySQL will generate relay logs once is executed by hand or by a mysqld startup. 

This rigorous check is performed quickly because all the grant tables are loaded in RAM upon mysqld's startup. Obviously, a stored procedure with security_type is more strict in allowing an authenticated user access to queries within the code. UPDATE 2013-05-08 11:29 EDT Please look at the EMS output 

If or are 1 in any instance, that range has to be clenaed up. You have to be sure that all netBlocks are properly represented. If you have StartRangeAheadOfEndRange=1, that is a definite problem. I hope these suggestions provide food for thought. 

That way, all the data needed for the view are in the index only. The other two indexes are not enough. Why do I say that? Even though the index was used, the amount has to retrieved from the table. Essentially, the query passes through both the index and the table. 

You will have to restart mysql for this to take effect. Give it a Try !!! CAVEAT : Your data integrity will deteriorate when added new data because many INSERT queries or multiple-row INSERTs will fail because of just one row not having parent keys. You should really consider loading all referenced tables into the slave to have good, clean data. Otherwise, it's not a true copy of the Master. 

into a balanced binary tree, absolutely no tree rebalancing would ever occur. This being the case, entering random data into a balanced binary tree could never be as bad as entering already sort data. On average, random ordering would require less tree rebalancing than sequential ordering. Now, let's get away from balanced binary trees into something more real-world, BTREEs. Treenodes in BTREE indexes usually hold some power of two index page entries + pointers to a lower BTREE node. Page splitting would occur in the event pages become full. Consequently, many early BTREE pages would end up half full due to page splitting (The execption to this would be with bulk inserts and in-memory sorting of keys and writing out more compact index pages with as little fragmentation as possible). Other than that exception, insertion of ordered data in a BTREE page would cause page splits with a very small degree of regularity. Random inserts would at the very least wreak less havoc than loading ordered data. InnoDB In terms of InnoDB, you have the gen_clust_index. It has hooks to another type of index used internally for tracking row IDs. Never think about the gen_clust_index as just any other BTREE. It is what the Oracle world would call an index-organized table. Column ordering for InnoDB is handled so differently from MyISAM that can you detect a notable difference if you attempt to reorder row data in a specific key order. According to pages 148,149 of MySQL Database Design and Tuning