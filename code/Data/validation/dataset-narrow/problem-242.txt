The super quick fix will be to apply to your query. DISTINCT ON Postgres allows the use of the qualifier, but allows it to be applied across a subset of your returned columns from your query. In this case, you could simply rewrite your query as 

OK, good. Back in it's right place in the schema, and the functions are still accessible to all schemas in the database. TL,DR; Just use the default schema for extensions. 

This will almost certainly do what you are asking. Refer to the PostgreSQL Documentation on for more info. 

Then you are going to such that every row entry in is preserved, and will be present for row entries when they weren't found in . You are then filtering to find the s, and then counting. As such, you're counting the number of entries in the table for which there was no match in the table. Are you actually trying to perform this query? 

I'll try my best to answer your question in brief, but since I'm not really aware of your level of comfort with PostgreSQL, and I don't have a lot of time to go into an in-depth explanation anyways, I'll keep the answers simple, and you can ask for clarification if you'd like more info. 1) Why is it faster in batches? Due to the structure of PostgreSQL's write ahead log, the amount of shared buffer space in RAM, and the attempt to perform the entire in a single transaction, my guess is that you simply don't have enough computing resources to efficiently handle the update to nearly a million records in a single transaction. PostgreSQL has a well-built concurrency control system, essentially meaning that it has to keep the old copies of your pre- rows available during your operation. This is so that, in case another client tries to access these rows while you're updating, in case the update fails, or in case you cancel the update, you don't lose the old information. If you perform a large enough , PostgreSQL will load pages into memory and modify them, but will eventually run out of memory to work with, so it is forced to immediately copy these pages temporarily to disk if it wants to be able to load further pages and continue the transaction. Rather than being able to amortize the disk writes over a period of time, you've just forced your database into a bottleneck. 2) Scripting the updates You absolutely can script the updates, by creating a function in PL/pgSQL. There's a lot to learn about PL/pgSQL, including a lot I probably don't know, but generally speaking, you could do something like this 

Essentially, by creating the CTE using the clause, you are asking PostgreSQL to perform a separate optimization on the subquery 

While in a traditional OLTP system you may need to be concerned about normalizing and inheritance, in a typical data warehouse system you will want to denormalize to accelerate query responses. In this case, I would design an table which had all relevant columns about an employee which you might want to query against, along with an additional column which I might call , which I would fill in with entries like , , etc. and then have another column which had either the filled for employees who were a driver, or leave the column as when they aren't drivers. Of course, you can add even further columns to include information for other employee roles. Long story short, if you're building a typical star schema, you can achieve faster query results by using heavy denormalization, at the cost of storage space of course. 

and it's ready to go. Now, once again, I don't fully understand the issue, but it appears you are concerned with the size of the indexed entry, and not necessarily the actual row entry in the heap. For that case, you should use a function call from the extension to query the pages of your BTree index, as 

This may not seem like it is all that helpful at first, but if you have for example thousands of concurrent trips, then they may all be taking measurements once per second, on the second. In that case, you'd have to re-record the time stamp each time for each trip, rather than just using a single entry in the table. Use case: This case will be good if there are many concurrent trips for which you are recording data, and you don't mind accessing all of the measurement types all together. Since Postgres reads by rows, any time you wanted, for example, the measurements over a given time range, you must read the whole row from the table, which will definitely slow down a query, though if the data set you are working with is not too large, then you wouldn't even notice the difference. Splitting Up Your Measured Facts To extend the last section just a bit further, you could break apart your measurements into separate tables, where for example I'll show the tables for speed and distance: 

and then materializing those results. I then, once again use the expression to try to manually manipulate the plan. In these cases, you probably can't trust just the results of , because it already thought that it found the least costly solution. Make sure to run an to find out what it really costs in terms of time and page reads. 

Window Functions, or the OVER() Clause It appears that what you are looking for is a window function or an clause. In your original example, you are trying to use two conditions, which doesn't work, because when you try to then apply , you can have a condition where the max of the first column and the second column aren't in the same row, and so then simply can't be understood. However, if you are looking to extract the max over a grouping, and you can define that grouping, and you want to obtain the 'column-wise' maximum for more than one column for that grouping (as you seem to want to do), then here's the solution. 

Now, this will of course have set your random page cost low, and you may not want to keep it that way. So, you could do it locally for a single transaction as 

Aaron, In my recent work, I've been looking into some similar questions with PostgreSQL. PostgreSQL is almost always pretty good at generating the right query plan, but it isn't always perfect. Some simple suggestions would be to make sure to run an on your table to make sure that you have updated statistics, but this isn't guaranteed to fix your problems! For reasons that are probably too long-winded for this post, I've found some odd behaviors in the statistics gathering of and the query planner that may need to be resolved in the long-term. In the short-term the trick is to rewrite your query to try and hack out the query plan you want. Without having access to your data for testing, I'll make the following two possible suggestions. 1) Use PostgreSQL treats arrays and sets of records differently in its query planner. Sometimes you'll end up with an identical query plan. In this case, as in many of my cases, you don't. In your original query you had: 

My recommendation would be for you to check out Cassandra. In my experience, and based on the requirements you seem to have, it seems like it would be a good fit. It appears to me that you have essentially have these requirements: (1) Key-value storage (look up data based on ID) (2) No need for or sub- (3) Need for a SQL-like language (4) Horizontal scalability As for (1), Cassandra will hash the primary key (in your case you can use a single column as your primary key) and which node in the cluster stores the data, and the Cassandra internals ensure look up by primary key is very fast. Cassandra doesn't support as in (2), but you appear to be OK with that. Since you are essentially doing key-value lookups, I'm actually not sure why you need an SQL language. Perhaps you are wanting to not only look up data by primary key, but also perform aggregations and use ? In this case, Cassandra won't work, but if you only need a simple SQL language to retrieve data as per (3), Cassandra has CQL, which may serve your needs. Horizontal scalability is built right in to Cassandra, covering (4). All that being said, Cassandra isn't a silver bullet. It's a great technology for certain applications, and it sounds like it fits you needs. Still, if you want aggregations, or if you want to query for multiple primary keys in a single query (e.g. ), then you may run in to some issues. Now, if you can let go of constraint (4), I'd 100% be promoting PostgreSQL. You stated the database will be getting hit with thousands of requests per hour, but that's nothing for a database like PostgreSQL, if your queries are only individual primary key lookups, and you aren't trying to transfer large amounts of data in those queries. Best of luck! If you provide more info based on my feedback, perhaps the community here can help you narrow down your selection even more. 

Of course, you can see how this might be extended to the other measurements. Use case: So this won't give you a tremendous speed up for a query, perhaps only a linear increase in speed when you are querying about one measurement type. This is because when you want to look up info about speed, you need only to read rows from the table, rather than all the extra, unneeded info that would be present in a row of the table. So, you need to read huge bulks of data about one measurement type only, you could get some benefit. With your proposed case of 10 hours of data at one second intervals, you'd only be reading 36,000 rows, so you'd never really find a significant benefit from doing this. However, if you were to be looking at speed measurement data for 5,000 trips that were all around 10 hours, now you're looking at reading 180 million rows. A linear increase in speed for such a query could yield some benefit, so long as you only need to access one or two of the measurement types at a time. Arrays/HStore/ & TOAST You probably don't need to worry about this part, but I know of cases where it does matter. If you need to access HUGE amounts of time series data, and you know you need to access all of it in one huge block, you can use a structure which will make use of the TOAST Tables, which essentially stores your data in larger, compressed segments. This leads to quicker access to the data, as long as your goal is to access all of the data. One example implementation could be 

Just trying to brainstorm a couple ideas which might jump-start the query planner into using a better query plan. I can't be sure if they'll work any better because I don't have your data. 

Masi, The PostgreSQL B-Tree index is very strongly based on the implementation by Lehman and Yao, which includes a lot of work oriented around multi-version concurrency control, but there's still great info in this paper. Of course, PostgreSQL doesn't make a 100% accurate replica of the method in the paper, and to find the differences, there will be almost no way to do it other than to (1) find someone who understands the PostgreSQL B-Tree, and has the time to go through the intricate explanation, or (2) dig through the source code yourself. Another possibility is for you to visit Bruce Momjian's excellent reference website, where he discusses PostgreSQL internals in more detail. In this case, however, based on the nature of your questions, I feel like you may have a fundamental misunderstanding about how B-Tree indexes work. In this case, I think a little Google searching, or maybe reading through a portion of a textbook like Fundamentals of Database Systems by Elmasri & Navathe would do you some good. 

Treatment of subcategories can easily be handled by either 1) simply placing a NULL entry for the column in the appropriate row of the table, or 2) adding a subcategory entry where the subcategory field is . Last but not least you would add a foreigh key reference from your table to the appropriate in the table. Does it really need to be so normalized? Well, in my opinion, no it doesn't. I've heard a quote, though I can't remember who spoke it, but it basically goes "Normalize until it hurts, denormailize until it works." Especially in the case where you don't have a lot of categories, the fully normalized design may be a little bit of overkill. What might simply make more sense would be to keep the above mentioned and tables to enumerate your types, but skip making the separate table, and then simply have your table referencing the and tables as 

Note the change of the subquery from to . 2) Use CTEs Another trick is to force separate optimizations, if my first suggestion doesn't work. I know many people use this trick, because queries within a CTE are optimized and materialized separate from the main query. 

Upgrading a PostgreSQL Cluster Postgres official documentation I've only ever migrated using and , which is perfectly acceptable, but the docs also refer you to methods which involve , , or via replication. In general you want to apply one of these approaches since the data format used in successive versions is subject to change. 

is completely redundant. You appear to be re-checking for already checked conditions. For example, try just running 

I'll try to put together a SQLFiddle in a couple minutes here, just to make sure I didn't get some of my query wrong. :P EDIT: I did make a small error. Trying to clean it up in an example as we speak. EDIT 2: The error is now corrected! Unfortunately, SQLFiddle is having a little trouble right now, but HERE is the link to the SQLFiddle for some testing. Please note that I may not have solved this in the most elegant possible way, but it should solve your issues. 

There's really no one 'best way' to store time series data, and it honestly depends on a number of factors. However, I'm going to focus on two factors primarily, with them being: (1) How serious is this project that it deserves your effort to optimize the schema? (2) What are your query access patterns really going to be like? With those questions in mind, let's discuss a few schema options. Flat Table The option to use a flat table has a lot more to do with question (1), where if this isn't a serious or large-scale project, you'll find it much easier not to think too much about the schema, and just use a flat table, as: