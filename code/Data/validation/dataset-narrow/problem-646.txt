You can argue about the readability gain. someone who is familiar with java 8 stream api will know what this is about immediately. Naming Convention 1) a getter returning ? 

I think in these types of questions, where input is known and does not change, arrays are bettter suited then Collections, since they offer more concise syntax: is more clear then naming conventions: variables start with lower case. 

First of all, ? come on... how hard is it to call it or ? (no need for , we can see its type) but what does the line mean? why are you asking about the first character? ok, you want to know if the subsquent String is to be added or substracted, right? so why not make a method that returns an enum that has values and then you can have a separate method that accepts enum and performs the actual operation. The benefit of this design is that you are better prepared for new operation requirment like which will mean... I don't know, something. Now that I think of it, perhaps you can add the identifying character to the enum values and then in you extract the first character and search for the matching enum, which is a behavior of the enum: 

since I know Java :) I have the following comments to add to the previous answer: resources handling Resources, in this context, are the input files you are reading. Their lifecycle is not handled properly. In other words, you do not close the files, leaving OS resources open after the file was read and parsed into memory. This is not a big deal in your program since you only open two files, but it is still a glaring omission. Since proper handling of lifecycle of resources can be tricky, starting with version 7, the Java compiler gives you the feature of try-with-resources, offering automatic closure (and better exception handling) 

Let's review this code purely from a performance angle, without a focus on style or anything else (in addition to optimization suggestions, Peter already mentioned several things in areas other than performance). First, you can play with all the algorithm discussed here in this github repo. I compiled it on Linux but it should approximately work on Windows if you have or - if you add a thunk to adjust the calling convention. If someone really wants it I'll do it. Profiling If you've ever asked for performance help, no doubt someone has told you to profile, profile, then profile some more. So sure, let's start there. I'm going to use Linux's since it is awesome and free and available on Linux, but you can get the same information presented nicely in Windows using VTune or maybe with this stuff. Anyway, let's run on the original algorithm to get a feel for any high-level issues: 

Each bitmap is "normalized" such that the LSB is always 1. Then the loop is very simple: it loops over all 30 primes, and shifts the bitmap by the right amount to "align" it, and ORs all the results together. In this way, the loop handles 64 candidates. The shift amount is simply the amount need to correct stitch the bitmap from the last iteration so that it is periodic. Using a 16-bit example for sanity, the bitmap for in binary is . In the next iteration, you can use the same one since the effective 32-bit bitmap would be . Oops, two adjancent 1s! You just need to shift it over by 1: and now it stitches fine. In general the stitch amounts for any prime have period and take all the values between and inclusive. The last line in the loop calculates them. Let's try this guy: 

So If I understand correctly, clients get an instance of from the factory, call and get an instance of and then call and get an exception (or nothing). so you replaced an block with a one. the client still have to check the return value to figure out what to fix. and if you ask me, it is more clear to ask 

3) Design This last comment falls into "best practice" category: the method does too many unrelated tasks: receive input from user, file handling and main loop. if, for example, you have a separate method that is responsible for communicating with the user, receiving user input and validating it, then perhaps you would be able to check for existence of input file as part of validation... then again, one method for receiving user input and validating it? hmm maybe we can design further break down ... 

Note: the best practice rule dictates that you first open the input handler, then the output one, and closes them in the reverse order. As you can see, proper file handling is a delicate process that requires careful attention to details at different points in the code. Since many errors were made by many a good developers in many production programs, Java 7 added a feature that automatically handles this logic for you. it is called try-with-resources and it means that you declare and open the handlers at the point of the block and the compiler adds the correct clause. I leave to you as study exercise to go though the tutorial and implement the feature into your code. 2) Exception handling Exception handling in is incorrect: 

The inner loop here runs once per prime and processes 64 bytes per iteration (512 odd candidates), in 14 instructions. The the two instructions are doing the heavy lifting of combining the bitmmaks into the two accumulators, and the rest is mostly just managing the indexes. The outer loop runs when the bitmap for all 30 primes have have been accumulated and stores the bitmap into a temporary buffer provided by the caller. We periodically break out of the asm code to examine the generated primes (in this case, simply counting) - see for some details. In a real implementation, you would still want to do the handling periodically, but you might inline it right into the function. Let's time this guy: 

You increased from 2 reads to 6 without adding any index calculation overhead! It's quite similar to the "horizontal unrolling" discussed above, where you just read more consecutive bytes (2 x 32B reads in this example), but that approach nearly doubles the table size (since you need to accommodate a 64B read at all positions), while this approach increases it very little (by 2 bytes, probably, since you just need to accomodate the maximum offset of +2 at every position). The code is different per-prime: the code for adds 4, or else subtracts 1 (they are equivalent, but you need to design your index handling to account for which direction you are going). This approach is promising because it lets you get more reads without greatly increasing the table size. It works best for the smaller primes since the jump amounts (and hence required table padding) are smaller, while the horizontal unrolling described above works best for larger primes since the table increas is relatively less (since it is fixed to the read size and the large primes already have larger lookup tables). Unrolling both the inner and outer loops allows you to even pick and choose different strategies for different primes. Optimize the Lookup Tables I just made the tables 2D arrays, for simplicity, but this wastes a lot of space, since the rows for the smaller primes are often very short (I padded out some of them with to help catch bugs). To optimize this, you'd probably want to first pack the tables more tightly, either as a jagged matrix (i.e., an array of pointers to rows), or just as one large packed 1D array. The latter approach is great if you've done a lot of unrolling as described above since the various instructions can just directly embed the offset into the array in their memory operand "for free". I didn't make any effort to align any of the lookup tables at all (and they won't have any natural alignment since they are all bytes. No doubt about 50% of the 32B loads will be "split loads" that cross a cache line. With a bit of care you can reduce that to 0% for the smaller tables with very little size increase. For the larger primes I think you can reduce it to 0% but at a 100% size increase (just thinking about it, I haven't checked), which may not be worth it. Furthermore, depending on the number of consecutive bytes you are reading (see ) there are opportunities to dramatically reduce the larger tables. For example, if you are doing 64-bit reads like the non-SIMD bitmap algorithms, for any prime >= 67 every 64-bit read returns either an all-zeros value or a value with exactly one bit set. Yet such each such prime is using their own large lookup tables which are mostly zero. To support any possible 64-bit read for all primes >= 67 you need only 8 zero all bytes, and 8 other 15-byte regions with the all bytes zero other than the middle byte which 1 of the possible 8 bits set. You can overlap this all nicely so it takes about 72 bytes. So you can replace all so you can replace the 13 * 134 byte lookup tables for the primes from 67 to 127 by 72 bytes: a reduction of about 25 times! Even better, this scales as you add larger primes: even if you want to add 100 more primes, you don't need any additional lookup tables for the bitmap. For the fully generic algorithm which uses the table on every calculation, this transformation is free. For the unrolled versions that encode prime-specific knowledge into the reads it doesn't work as well. It also doesn't work as well for the larger reads: the version that reads 64B (512 bits) never gets close the "zero or 1 bit" set case for the first 30 primes, so you can't use it there. It would be useful if you wanted to use more primes, however and since this algorithm is so fast, it makes sense to do so. Combine Small Primes Currently every prime is handled separately: although there could be some bitmap sharing for larger primes as described above, each prime still implies at least one to incorporate it. There is nothing particular special about one-prime-per-bitmap, however: why not simply combine several primes together into one pre-calculated bitmap? Instead of having two bitmaps for 3 and 5 like: 

But really, as it was already suggested, you should design a class that holds all the information of a Player and another for team. 

EDIT #2 good pointer to the function. I was able to find the ultimate chain of functions that eliminates all if conditions! 

1.3) Instantiation of Later on I have more (much more) to say about but this comment is relevant for the general case of instantiation through reflection: You make the assumption that the default no args constructor exists, and this is not always true. A better aproach would be for you to accept an instance of factory that produces instances of . 2) Warning 2.1) variable types 

"Main work was the instance-version of parse()" I am guessing that this means the non static method. it contains 4 lines of code. out of 140 lines posted, that is very little "Main work"... like it was said in the comments, it is not clear what code is yours and what was given. So I restricted my comments to the non static method in class: 

Now we come to the question of all those statements. You already have arrays that you assign the variables into so why not use them when you ask about the team players? 

Advantages: 2.1) all the advantages of static constructor 2.2) lazy instantiation (only when you access the value) Disdvantages: 2.1) still cannot receive an argument during initialization. Although an enum can receive an argument, you have to specify it in the enum value decalration. You can use a setter like described, but then again, you can use a setter with the static constructor solution as well. 2.2) cannot have a parameterized method. 

This is just a "saturating" shift, which returns zero if the shift amount is 64 or more3 and which compiles to a branch4. It also turns out that the loop has no less than two very slow division instructions every time around, coming from the two operators in this line: 

We are down to 0.16 cycles per candidate! That's fully 25 times faster than the original algorithm, and if you measure it by cycles per prime, we are finding a prime every 1.44 cycles. Unless you are doing almost "zero work" per found prime, it's very likely that the other work will start to dominate here. Further Optimizations If you are so inclined, this can still be made much faster, probably by a factor of 5 at least. Of course, before you pursue that, you would need to benchmark your full application, since it is highly likely that the unspecified work you do per prime is what is slowing this down now. Minor Optimizations The loop above directly admits some minor optimizations. For example, which counts off the 30 primes could be inverted so that it counts down to zero (or from -29 up to 0) allowing use to remove the check at the end (we use the flag from the prior instead). The could be changed to a 3-argument , avoiding the prior , or this whole calculation could be removed by using the induction counter instead by making the row size of the two involved tables and consistent (right now one has an inner dimension of 128 and the other 190). These may shave another small fraction of a cycle off of the existing time, but the ones below are much bigger. Larger Contiguous Reads The above algorithm reads uses on two consecutive registers worth of data (64 bytes) from the calculated index. It is in fact the slightly bigger brother of the not-shown variant, which only reads one 32B value in the inner loop. That guy ran at 0.27 cycles/candidate, so just doubling the read size in the loop nearly doubled the speed. It's easy to see why: it only took one extra instruction to do that, while the other 12 instructions in the loop are pure index calculation overhead which are now doing double work. So by increasing the loop by one instruction it does double the work. You can just carry this idea to its logical conclusion, reading 4, 8 or however many values per loop. There is no particular reason it has to be a power of two, either. These will give very fast and easy speedups: I guess it is easy to get below 0.1 cycles/candidate using this approach. The larger reads come at a size cost for the - larger reads mean a larger table6. This optimization is probably the best and easiest one if you want performance. The code is already kind of half-generic. I call this "unrolling horizontally" based on my mental model of each prime being a long horizontal bitmap, with primes stacked vertically one above another. So the is accumulating in vertical slices (column-wise) and this unrolling moves in the horizontal direction. Unroll the Inner Loop This is the "usual" unrolling and the counterpart of the horizontal unrolling discussed above. Currently the inner loop iterates over all 30 primes. This loop has a fixed trip-count, and it could be completely unrolled. Several instructions in the inner loop would just disappear, such as all of the loop control, the instructions dealing with and the . This should give a reasonable one-time gain and the loop should still easily fit in the uop cache. It's less appealing than the horizontal unrolling since you can only do it once! Unroll the Outer Loop Once you've unrolled the inner loop, you may want to unroll the outer loop as well. Unrolling this by N would result N copies of the unrolled inner loop so, the code would get big, fast, but I think you could probably unroll it by 3 or 4 and still fit it in the uop cache. This allows some very interesting optimization since by unrolling the inner loop you now have unique sections of code handling each prime. When you unroll the outer loop, you may now be handling several reads for the same prime, in explicit unrolled code. The big win here is that you can directly hardcode the "offset sequence" that normally has to be painstakingly calculated by the generic code. For example here's the start of table for consecutive 64-byte reads: 

in my eyes, in the case of deciding between two options, the short form of the if statement can be used to make it clear: 

all of these do not contribute to the correctness nor clarity of the code. regarding usage of parenthesis for operands of operator, personally I think this is another case of redundancy, but there are other who argue that this is ok. Java has constants to define min/max values for number types. They are defined in the number wrapper classes, like 

AS you can see, this solves ALL three problems mentioned above. 2) Use Class names that reflect the scope of one instance an instance of holds the info of one contact. Same as an instance of so it should be named in singular form. 3) misleading method names / redundant logic The logic of modify and remove is fine but the method names are misleading: there is and which give the false impression that they are doing the same. In fact, is actually doing a search for the contact to be removed. same with and . and while we are on the subject, why do you require both old contant's name AND number to find it? especially since you have a choice to search for a contact based on name only. Now, if we agree on that, you can use when modifying and removing contacts. 

Huh, well that kind of sucked. At 5.2 cycles per candidate it's a bit slower than the algorithm in the OP. For one thing it still has ~9% branch mispredictions. It turns out the main culprit is this line: 

What a difference a removed branch makes! It's about three times faster, at 1.35 cycles per candidate. That's despite the fact that we are executing more instructions: about 280 billion versus 240 million. The upside is all due to removing branch-misses, which are now reported at 0.00%, and the IPC has increased to ~3 instructions per cycle. Loop Splitting Of course in the real world, you don't want to just count the primes, you want to do something with them. That's fine: it's a slight modification to the above to generate a bitmap indicating which values are likely primes, rather than simply counting them, without slowing down much. So to avoid the mis-predictions, you process some fixed number of candidates with the loop above, generating a bitmap, and then you iterate in a branch-prediction aware way over the bitmap (e.g., using and ) to generate the likely prime values for your "secondary processing". I won't actually flesh this out fully for since we are about to move into the fast lane with a different approach entirely (which still ultimately uses the same "bitmap" output format. Bitmaps FTW Let's step back a moment and understand what the core of the algorithm is doing. Basically it implements 30 periodic counters incremented in sync and tries to determine if at least one counter has "wrapped" on every iteration. To do this, it uses 30 byte counters in a . Since AVX2 lets us do 32 byte operations per operation, it means we can do 30 operations on this counter per instruction (and perhaps up to 3*30 = 90 if we use all 3 vector ports fully). We get lucky that the instruction works well to do a 30-way or "wrap" operation! What if instead of using byte counters, we use a series of bitmaps with one bit per candidate, which encode the same periodic behavior as the counters? That is, to replace the counter which goes we use the bitmap with every 3rd bit set? Well now a 256-bit register holds 256 counter values, not 30. Of course, the correspondence isn't exact, since everything is transposed: one register contains a lot of state, but for one prime. You'll need 30 such registers for all the primes. Still, we can ballpark this: first note that combining registers is simply a matter of ing them together - the remaining zeros are the likely primes. So it will take 29 instructions to combine 30 registers, and the result will cover 256 candidate values, so that's ~8.5 candidates per instruction, versus ~1 for the counter approach. Now this is a very rough analysis and leaves out a lot of details like how do you get the bitmaps all aligned properly, but it seems like we may get about an order of magnitude improvement with this approach even over the branchless counter version. A C++ Prototype Let's try to prototype this in C++. Here's the core loop: