If you understand your toolbox (standard library functions) you can make them perform well and not need to reinvent them. This is crazy: 

That's a big IF. Throwing it into a method which is several hundred lines and already has a cyclomatic complexity in the double digits is not appropriate. 

The .NET serial port is notorious for making it easy to write horrible code :( As far as reviewing yours, I can't get past the usage of . It's wrong, irredeemably broken, because the sleep starts when the data is appended to the kernel buffer, not when it gets transmitted. Actual time of transmit is affected by how much data was already in the write buffer, what busses exist between the UART and your CPU (for example, is it connected via USB? Then transfers have to wait for USB timeslices and may get delayed behind bulk transfers to USB mass storage devices), UART flow control, and then there's the actual transmission time based on the number of bits (including start, stop, and potentially parity) divided by the baud rate. So your sleep does not guarantee a period of inactivity on the serial line or give the device a chance to process one command before you send the next. The Windows API provides an event when the kernel write buffer has been completely drained and the serial wire becomes idle. Use it. Even if that means throwing away the C# class. Ultimately, that's the better approach: don't use the horrible wrapper provided by .NET web developers, use the Communications Functions API that was written by people with hardware experience. (This code also makes the mistake of confusing serial data with strings -- serial ports transmit bytes, not characters) 

You added the cast likely because the compiler complained. But the problem was not the missing cast, but that you converted an integer to a pointer, instead of an integer pointer to a void pointer, which can be done without cast. When you store a correct pointer, you can then display it correctly with: 

You have data sets within code. Code should ideally be independent from the data. We can see that target_meter, update_timestamp_col, data_lifespan, move_back_amount, secs_in_res all correlate. I would extract the data from the functions: 

With this change it's a breeze to adapt data and also to add new resolutions without accidently messing up code that worked before. 

The context is passed (as pointer) to each function so it knows the circumstances it operates under. This also groups relevant data together. The second context you operate under is a specific meter. 

This way changes of the data do not change the executing code, also the data is actually grouped together - you have a chance to spot erroneous data just by pattern recognition (or rather that something disturbs the pattern). Then we see that the data also correlates with the input parameter. We can include this, too: 

Here I would establish something that I call context state. Your functions and daemon run in a certain context that is established. 

a) There is one conceptional behavior I personally don't like: The code will leave a partially written file behind in case of an error, instead of removing it. b) 

Equally-sized sequential subsets of primes don't evenly divide the task. The smallest numbers lead to a lot more multiples. Test-before-set might significantly reduce cache contention. Even better, don't start at , start at . 

That even gets rid of the need for , because now the value-setting code is so dirt simple it can safely be repeated. I notice you already did this exact thing with , so why not with the ? is to as is to . 

I believe it will be much faster than yours, because I was able to avoid ever using division ( and ). As a matter of fact, I don't use any multiplies either. (It's about 6x faster with g++ than the code in the question, and the actual factor generation part is marginally faster, about 10%, than lol4t0's. Returning factors in a takes 5x as long as the factor generation.) Test program: $URL$ Original application: 

This is a C++ wrapper for the National Instruments ANSI C library for interfacing with NA-DAQmx devices such as the USB-6008. I wrote it for use in real-time data processing to test algorithms developed in my dissertation research. I'm sure there's lots of room for improvement. Suggestions on style are welcome, in addition to contributions to support more features of the NI hardware (the emphasis of the current code is on near-simultaneous acquisition of a few analog voltage channels) 

As explained in comments (in which I pointed out a few possible edge cases), even though it's not optimized in terms of performance, this is about as clear as it gets. A more clever algorithm - calculating the number of weekdays without iterating through all of them - is probably possible, but I bet it would be much less readable. If performance isn't a concern at this point, I'd leave it. You could maybe replace the loop with a LINQ-based solution. 

So how on earth is it able to correctly detect whether is present in the collection? Standardization Custom collections should support standard .NET interfaces such as IList. Otherwise other code can't really use it (other than as an ) unless specifically refactored to do so. Besides, your implementation doesn't allow inserting an element in between pre-existing elements, which pretty much defeats any major benefits of implementing a collection as a linked list. Generics As @MatsMug remarked already, it's supposed to be generic, yet - same as your previous implementation - it doesn't support , only its non-generic, legacy version. Documentation It's good that you now use documentation comments (at least for a class that's supposed to be of general use). But this "this is" (as in "this is a singly linked list data structure" or "this is the Node class") is unnecessary. It's just fluff. We understand that it's a class, and that the comment must refer to this class, not some other class elsewhere. Typos don't make great impression either ("Implementatoin"). If your class is opinionated about nullability (which is an improvement over your previous submission), this should be described in documentation comments. As of now, your comments are stating the obvious, for example: 

This is a mixture of storing the value and storing the operation. When you need to print a value, you look up the range and print the value. As you see, we need less than 64 bytes for this simple example to store the information how the 7.4 GB data is compromised of, yet we can print the value for each and every array position. Even if we have reduced the data, we still need to optimize the remaining data. We can't just dump it into a linked list - as stated, we can end up with Q+1 entries (200001), we can't afford to search the entries with an effort of O(n), let alone (r-l) * O(n). But if we put the data into a binary search tree, we can find the correct array entry in 18 steps within the 200000 entries. This costs us some more memory, not only due to the overhead, but as we need the array index as key, we can't group all identical values together, just as long as they are one range. So the example above will need 4 nodes in the binary search tree, to separate the value 0 twice. So, when you combine these two approaches, you should be able to get a decent memory and processor time consumption. 

The first question I would have is: What head are you talking about, there is and ? Does the function modify a global variable called head? Not even your unit test creates a variable called head as head of the list ;) Your data is a void pointer and then you use , that's not legal, because the size of a void pointer can be different from the size of an int. It's already strange that you don't store a pointer to an int in data, but the int itself. Is this really what you want? Didn't you want to store the pointer to the int? I assume you actually wanted: 

Code style Magic numbers In "DatabaseManager", I would convert "magic numbers" (connection life time being 5 minutes, command timeout set to 120 etc.) into constants. Fluff / noise 

doesn't make sense to me. When your code is conditional, DRY (Don't Repeat Yourself) and try to extract whatever is common for all execution paths. Eg. 

There's no use in putting "generic" in the name. Any C# programmer knows that indicates generics by itself. Case in point: in .NET we've got and - not . (So "drop the the" ;) ) 

Who would have thunk! Not too informative... In contrast, this code doesn't comment the non-obvious stuff. For instance, now your class has a certain policy regarding nullability - which, as I said, is by itself an improvement over the previous code, where this policy was sort of accidental. But is it obvious that throws an exception whereas is ignored? I wouldn't have guessed that correctly, and yet the documentation doesn't say a word about it. It's too busy telling me that element is element. By the way, it's not just a question of documentation - I'm not sure I like this asymmetry in principle. And it isn't the only inconsistency lurking in the implementation, either. For example won't crash when the list is empty - but will if there are already elements in it. What's the rationale for that? :) That's not predictable behaviour in my book. Speaking of , this comment is just plain wrong: 

For N = 109 you have to store about 7.4 GB data, if you merely use longs with a size of 4 bytes. This is not feasible, which, I assume, is the whole point of the problem. You have 200000 Qs - storing the Qs instead takes up only 11 bytes per Q - one byte to determine the operation, 8 bytes for the two values which can go up to 109and 2 bytes for c, which fits into 16 bits, as it's not higher than 10000. If you store Q, you end up with around 2 MB of data. You could apply the Q operations at runtime to a single long (long) and then print each value individually. This algorithm will be memory-efficient, but very very slow. Looking at the problem, we will have a lot of array members having the same value. There are only 200000 operations but 1000000000 array entries - we can change 200000 different values - if we do this, we still have (109 - 200000) array entries with the very same value. Even if we modify 200000 ranges, this doesn't change the fact, merely the distribution of distinct values. So it's much more efficient to store a value and then for which array range this value is valid. In example: 

Regarding the updated code: I don't like the use of the term in the functions. You have this prototype: