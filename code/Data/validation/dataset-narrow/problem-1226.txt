try Concise encyclopedia of computer science, Wiley. unfortunately a complete/thorough table of contents for this ref does not seem to be available on the web [a somewhat unusual omission nowadays, maybe Wiley could correct this on request] but the complete index appears to be browsable on amazon. it has coverage that is much broader than TCS such as hardware concepts etc, but it appears to cover significant parts of TCS eg: 

this is a new 58 page paper (32 refs) accepted to STOC 2013 that surveys and advances the area of determining the precise k-SAT threshhold, building especially from results borrowed from statistical physics. from the abstract 

closely related is a highly-viewed documentary circulating on a lego turing machine built in honor of the 100th anniversary of Turings birthday, by CWI Phd student & software engr/researcher 

other answers are good, here is one additional angle/reason for consideration that meshes with others yet might be even more definitive, nevertheless may be harder to keep clearly in mind as the old origins are lost somewhat in the sands of time: historical precedence! Lambda calculus was introduced at least as early as 1932 in the following ref: 

here are two other fairly comprehensive surveys to gauge state-of-the-art but maybe more with an empirical slant. 

there is some research into using GAs for wine classification. it accurately classifies the variety of wine and the production place ("origin denomination").[1] this is a subset of use of GAs in Agricultural Systems of which there are many applications. [2] [1] Feature selection algorithms using Chilean wine chromatograms as examples by N.H.Beltran et al [2] State of Art in Genetic Algorithms for Agricultural Systems by Bolboaca et al 

universal graphs are infinite & a generalization of the Rado random graph mentioned by DE. recent research in the area is in the direction of identifying universal graphs for a graph family F: ie, an infinite graph belonging to F that contains all finite graphs in F as induced subgraphs. 

seems to me the AKS P-time primality test is quite beautiful in various senses. a breakthrough at the time, one of the great but rather rare breakthroughs seen in complexity theory in our lifetimes. it solves a problem dating back to greek antiquity & relates to some of the earliest algorithms invented (sieve of eratosthenes), ie identifying primes efficiently. its a constructive proof that primality detection is in P as opposed to many great proofs that are unfortunately nonconstructive. its interconnected to the RSA cryptography algorithm mentioned in another answer because that algorithm needs to find large primes quickly, prior to the AKS algorithm this was only probabilistically possible. its fundamentally connected to number theory & other deep problems eg the Riemann conjecture which in many ways is the original realm of algorithmics. awarded the 2006 Gödel Prize and the 2006 Fulkerson Prize 

the question of whether there are integer solutions to Diophantine (ie simple algebra) equations is a simple mathematical question, aka Hilberts 10th problem. the Greeks pondered variations on diophantine equations two millenia ago [and Egyptians even earlier], also suggesting its a relatively simple concept. the proof that it is Turing complete on the other hand is complex and took ~¾ century to resolve after posed by Hilbert as completed by Matiyasevich (building on earlier work by Davis/Robinson/Putnam). 

a great answer to this question probably does not yet exist because its a relatively young and very active area of research. for example Ingo Wegeners comprehensive book on boolean functions from 1987 has nothing on the subject (except for analyzing the circuit complexity of the DFT). a simple intuition or conjecture is that it appears that large Fourier coefficients of higher order indicate the presence of subfunctions that must take into account many input variables and therefore require many gates. ie the Fourier expansion is apparently a natural way to quantitatively measure the hardness of a boolean function. have not seen this directly proven but think its hinted in many results. eg Khrapchenkos lower bound can be related to Fourier coefficients.[1] another rough analogy can be borrowed from EE or other engineering fields to some degree where Fourier analysis is used extensively. it is often used for EE filters/signal processing. the Fourier coefficients represent a particular "band" of the filter. the story there is also that "noise" seems to manifest in particular ranges of frequencies, eg low or high. in CS an analogy to "noise" is "randomness" but also its clear from much research (reaching a milestone in eg [4]) that randomness is basically the same as complexity. (in some cases "entropy" also shows up in the same context.) Fourier analysis seems to be suited to study "noise" even in CS settings.[2] another intuition or picture comes from voting/choice theory.[2,3] it is helpful to analyze boolean functions as having subcomponents that "vote" and influence the outcome. ie analysis of voting is a sort of decomposition system for functions. this also leverages some voting theory which reached heights of mathematical analysis and which apparently predates the use of much Fourier analysis of boolean functions. also, the concept of symmetry appears to be paramount in Fourier analysis. the more "symmetric" the function, the more that Fourier coefficient cancel out, and also the more "simple" the function is to compute. but also the more "random" and therefore more complex the function, the less the coefficients cancel out. in other words symmetry and simplicity, and conversely asymmetry and complexity in the function seem to be coordinated in a way that Fourier analysis can measure. [1] On the Fourier analysis of boolean functions by Bernasconi, Codenotti, Simon [2] A brief introduction to Fourier analysis on the Boolean cube (2008) by De Wolf [3] Some topics on the analysis of boolean functions by O'Donnell [4] Natural proofs by Razborov & Rudich 

there are several questions on this site related to Cook vs Karp reductions. have not seen a very clear description of this for the neophyte because its somewhat inherently subtle in many ways and its an active/open area of research. here are some refs that may be helpful to address it. as wikipedia summarizes, "Many-one reductions are valuable because most well-studied complexity classes are closed under some type of many-one reducibility, including P, NP, L, NL, co-NP, PSPACE, EXP, and many others. These classes are not closed under arbitrary many-one reductions, however." it seems fair to say that even advanced theorists are actively pondering the exact distinction and differences as in below refs and the full story wont be available unless important open complexity class separations are resolved, ie these questions seem to cut to the center of the known vs unknown. [1] Cook versus Karp-Levin: Separating Completeness Notions If NP Is Not Small (1992) Lutz, Mayordomo [2] Are Cook and Karp Ever the Same? Beigel and Fortnow [3] More NP-Complete Problems (PPT) see slides 9-14 on history & Cook vs Karp reduction distinctions 

[1] How to prove NP different than P Fortnow blog [2] Understanding the Mulmuley-Sohoni Approach to P vs. NP Regan [3] On P vs. NP and Geometric Complexity Theory Mulmuley [4] The GCT program towards the P vs. NP problem Mulmuley 

another answer in the vein of JS's number theory direction. one of the earliest analog-digital computers constructed is the Lehmer sieve dating to ~1932, predating electronic computers by about a decade. it basically computes remainder of division mod $n_i$ for a finite number of $n_i$ and applies the chinese remainder thm. for larger numbers it computes probabilistic answers to number theory questions including factoring. although the terminology at the time may not have referred to "probabilistic algorithms" it was possibly used in this way in some cases. (in this way it also has some similarity to the Fermat probabilistic prime test.) the machine also bears some similarity to the Babbage differential engine (~1830s). its not entirely inconceivable that Babbage or Lovelace may have envisioned something similar to probabilistic algorithms. the machine(s) can certainly be used to implement probabilistic algorithms, borrowing modern theory and superimposing it on the past. [1] Lehmer factoring machine [2] Babbage engine 

this is a pretty good discussion in this ref p123: Google's PageRank and Beyond: The Science of Search Engine Rankings Amy N. Langville & Carl D. Meyer, Princeton University Press. 

surprisingly one of the most obvious answers not added yet. sometimes one works too much with something to see it impartially. the theory of NP completeness launched by Cook/Levin and immediately amplified by Karp who gave an early indication of its ubiquitousness, even more prescient in retrospect. in many ways this is the birth of modern TCS & complexity theory, and its core/key/notorious question P=?NP is still open after four decades of intense study/attack. P=?NP has a $1M Claymath award for its solution. the Cook proof introduced the NDTM which is apparently not at all a mere theoretical curiosity but an almost extremely fundamental part of TCS. launched a thousand ships, so to speak. moreover, it continually resists/defies efforts via one of the other key/powerful TCS techniques mentioned in this list, diagonalization, seen in eg the BGS-75 Oracle/Relativization results-- suggesting that there must be something exotic and different about any possible solution, also further suggested/expanded by the Razborov-Rudich Natural Proofs paper (2007 Godel prize). there are many, many refs on the subj but one more recent with some 1sthand account of the history can be found in The P=?NP Question and Godel's Lost Letter by RJ Lipton 

leading expert on the topic Wigderson at IAS has recently been lecturing/researching/survey/overviewing this important/emerging/crosscutting area. 

a remarkable, even extraordinary or paradigm shifting use of GAs, highly cited in later surveys, was pioneered by Koza to solve a video game "problem"— namely Pac Man for a proof of principle, but the concept can likely be applied to possibly almost any video game, and the results are definitely far from trivial or "toy". that is, he evolved algorithms that implement actual behavior to win at playing the game for extended periods of time. results are on the level of performance of amateur or even advanced human players. a fitness function can be either points scored by the algorithm or length of time played (the later will presumably evolve algorithms that survive without scoring points, such as a classic case of "hunting" spaceships in the game Asteroids). the behavior is implemented with "primitives" (eg sense monsters/act by turning etc) and trees that represent the combinations of primitive strategies. [1] Evolving Diverse Ms. Pac-Man Playing Agents Using Genetic Programming by Atif M. Alhejali and Simon M. Lucas [2] Learning to Play Pac-Man: An Evolutionary, Rule-based Approach by Gallagher and Ryan [3] Learning to Play Using Low-Complexity Rule-Based Policies: Illustrations through Ms. Pac-Man by István Szita András L~orincz 

here is some evidence/ analysis/ results leaning against your conjecture that a Go generalization might be undecidable (aka "Turing Complete"); at least there does not seem to be a well known or commonly accepted case, and a search returns more results on the idea that its ("natural"?) generalizations are decidable. the generalization considered in this set of papers is PSpace complete. however, there are no "consistent" or "inevitable" ways to generalize games and it is conceivable someone could come up with a variant that is undecidable. actually most nontrivial games probably can be modified or generalized in some way to have undecidable variants. (a famous simple game/ example along these lines proved "undecidable" by Conway is Life.) the following references also point to many other references. another line of thought might be that no game can be undecidable if it is winnable, ie undecidability works against the idea of games terminating with a winner in a finite number of moves. in other words maybe games are better/ more naturally analyzed as within the (decidable) complexity hierarchy as is typically the case. 

center for quantum computation, Clarendon Laboratory, university of Oxford, who run www.qubit.org and home of David Deutsch one of the premier & senior researchers in the field. 

full disclosure: Moshe Vardi has studied phase transitions particularly in SAT and has a contrasting more skeptical view in this talk/ video. 

there is a somewhat side/more recently studied field of complexity called graph complexity that studies how larger graphs are built out of smaller graphs using AND and OR operations of edges. Jukna has a nice survey. in particular using units of "star graphs" there is a key theorem, see p20 remark 1.18 (the theorem is technically stronger than below and actually implies $P \ne NP/poly$): 

another angle on this. admittedly this could be regarded as somewhat obscure or fringe by some, but there is some work on parallelizing, in a general way, probabilistic algorithms, which are asserted to be somewhat naturally suited to parallelism. see eg Parallel Probabilistic Computations on a Cluster of Workstations Radenski, Vann, Norris: 

the more general question is the connection between knot theory and graph theory. as one possible place to start there is a connection between the Jones polynomial (used to classify knots) and the Tutte polynomial of planar graphs. ie in knot theory, the Tutte polynomial appears as the Jones polynomial of an alternating knot. (so maybe there is some connection of knot theory to GI on planar graphs.) see thms 7,8 in: Computing the Tutte Polynomial of a Graph and the Jones Polynomial of an Alternating Link of Moderate Size Sekine, Imai, Tani THE JONES POLYNOMIAL AND GRAPHS ON SURFACES OLIVER T. DASBACH, DAVID FUTER, EFSTRATIA KALFAGIANNI, XIAO-SONG LIN, AND NEAL W. STOLTZFUS 

Number theory is generally concerned with integer equations, though note wikipedia states, more broadly, a subbranch of number theory is the approximation of reals by rationals and the relation between them: "One may also study real numbers in relation to rational numbers, e.g., as approximated by the latter (Diophantine approximation)." here are two papers generally along those lines: 

another approach would be to go in a more empirical angle and just generate random instances (presumably around the easy-hard-easy 50% satisfiable transition point) and filter them to fit the criteria stated. one would require implementations of tree resolution/DAG resolution or "stronger systems". 

you mention Cloud computing specifically. there has been within just a few years intense innovation in this area with the Amazon elastic compute cloud, the google app engine & various tools and their associated conceptual parallel processing "models". special open source tools include google's Mapreduce, Apache Hadoop, and NoSQL databases which are emerging as new, strong, widely-adapted standards in parallelization algorithm "best practices" and "design patterns". also memcacheD is increasingly being used as an in-memory distributed database. an example of this is in use at Facebook described in a recent paper [1]. [1] Many core key-value store by Berezecki et al 

yet the metaphor of a quantum computer processing answers in parallel is widespread & a reasonable conceptual simplification of QM computing, and referred to in many QM computing textbooks. there are probably other examples from QM theory/computing. there is a natural tension in TCS and other theoretical research in communicating with the public/media because it sometimes tends to emphasize critical distinctions/concepts as part of rigorous training that are not known or crucial to laymen. in other words in many cases research theory works against various conceptual "big picture" simplifications that are legitimate to laymen.