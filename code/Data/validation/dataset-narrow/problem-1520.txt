Let's start with row-1 where A = 1 , B =0 , C=0 so it means that you are talking about A. So the value of CreditAmount_Numeric belongs to A and the retired binary value also belongs to A. Similarly, you can deduce for the rest records. Conclusion is that you need not do any transformation on your numeric variable. 

Yes, I would also totally agree with Louis T, as he has suggested it is better if you start with 3 hidden layers and see how the model is performing. There are couple of things which I would like to add to his answer: I would recommend you to do feature engg. on data so that you can make better sense out of it, rather then model taking over that task, one of the major reason for that is sometimes machine cannot give some meaningful features with respect to the business and the other important thing would be Cost Function, as the model increases its Complexity the more the cost function would be. So, how do you decide which model would give you better results: For example, there are 2 networks(assuming using NN for Prediction) : 

Remove all the factors with single level Then you need to make sure that data which is partitioned(Spliting between Test and Train) have Same Levels before you predict. For now you din't see the issue but once you predict then you will get this error. 

You can go through this Link for better understanding of mlextend. You can get the Precision and Accuracy values by using the following formulas: $\text{Precision}_{~i} = \cfrac{M_{ii}}{\sum_j M_{kji}}$ $\text{Recall}_{~i} = \cfrac{M_{ii}}{\sum_j M_{ijk}}$ Go through these Link-1,Link-2 for better understanding on how to compute the same, in the Link-3 is GitHub link which explains on how they implemented for a 1-D array, looking at that you can try expanding it for your outcome. 

I think you need to consider these metrics also i.e., Precision and Recall when you decide on some model. As you can in the above outcome that there are Precision and Recall values, you din't explain anything about the problem's business perspective. For example: Consider a data set with 100,000 observations. A data set consist of candidates who applied for Internship in Harvard. Apparently, harvard is well-known for its extremely low acceptance rate. The dependent variable represents if a candidate has been shortlisted (1) or not shortlisted (0). It was found ~ 98% did not get shortlisted and only ~ 2% got lucky. In this scenario you need to understand that precision and recall plays a vital role at the time of selecting model. In the same way if your business problem is inline with the above scenario then you need to select a model with high Precision and Recall. In the above screenshot which you have shared, it think you have good performance scores too(Assuming that your model is not over fitting). So, you can select which ever model suits your business problem. Hope my answer is helpful! 

The reason why you need to use these techniques are, if we don't use then the model accuracy will be very high it will be able to predict with 99% accuracy correctly the cases which are not fraud, which we don't want. if it can predict with same accuracy to find out fraud, then that is a great insight. This can be achived only by using either of the above techniques. Do have a look and Let me know if you have any additional questions. 

Welcome to the site! So far what you have done is good. Before going to modelling you need to take care of couple of things(Exploratory Analysis), Like: 

Data Labeling is a very trivial process as you have mentioned. As far as I know, it falls under Data Understanding(Exploration Analysis). When you don't know anything about data then you do exploratory analysis to understand and derive some insights. If you don't know the target variable then the problem falls under Unsupervised Learning, as you have mentioned in the question that your problem is unsupervised. So, you don't know your Target Variable, you are trying to make new feature/dimension to get some good insight irrespective of the factor whether you derive your Target Variable or any other feature it falls under Data Preparation(any new derived variables), which ever we think are important for our Analysis 

Random Forest(you cannot use this when any category variable has more than 52 categories, in your case it shouldn't be an issue) XGBoost 

Yes, you will have to convert everything to numeric. That requires thinking about what these attributes represent accordingly you can use either the below 3 options. There are three options: 

Here in your Scenario you need to select the one with more Information Gain rather than the least one and the process goes on till you reach the last feature/last node. Go through this Link. I think your doing it vice versa, I agree with Emre. In the above link it was explained with an example to decide whether to play tennis or not. Do let me know if you have any issues. 

Logistic Regression is used when you know that the data is lineraly seperable/classifiable and the outcome is Binary or Dichotomous but it can extended when the dependent has more than 2 categories. Linear Regression is used to find the relation and based on the relation between them you can predict the outcome, the dependent variable should be numeric. What kind of usecases are you expecting? give an example so that we can extend it further. 

PCA Linear discriminant analysis Non-negative Matrix Factorization Generalized discriminant analysis and many more. 

I think your question can be solved using Case Based Reasoning . Basic principle how it works is, you need to train the model using whole lot of different cases which you have. Based on the symptoms which you give the outcome is predicted(which disease). 

You can go through this link for better understanding. Regarding Poisson Distribution, Did you plot and check whether the data is following Poisson Distribution like: 

If you cannot find these books online, do let me know will share the link, I have them on my drive. These books helped me in understanding the basics of stats with examples explained in layman terms. If you are looking for some online courses, let me know can suggest you couple of good courses(most of them are free). 

Generally when ever we are trying to compare between models and to choose the best one, we go for other metrics like AIC, BIC, AUC(this is not applicable as it is used for classification algorithm) etc along with $R^2$. Now why are they important criteria because AIC tries to select the model that most adequately describes an unknown, high dimensional reality. This means that reality is never in the set of candidate models that are being considered. On the contrary, BIC tries to find the TRUE model among the set of candidates. I find it quite odd the assumption that reality is instantiated in one of the model that the researchers built along the way. This is a real issue for BIC. Generally we use both AIC and BIC together. you can got through this Link to understand better on AIC, BIC and which is better but conclusion is both are important. Now when we compare between $R^2$ and AIC value: $R^2$ and AIC are answering two different questions. I want to keep this breezy and non-mathematical, so my statements are non-mathematical. $R^2$ is saying something to the effect of how well your model explains the observed data. If the model is regression and non-adjusted $R^2$ is used, then this is correct on the nose. AIC, on the other hand, is trying to explain how well the model will predict on new data. That is, AIC is a measure of how well the model will fit new data, not the existing data. Lower AIC means that a model should have improved prediction. Frequently, adding more variables decreases predictive accuracy and in that case the model with higher $R^2$ will have a higher (worse) AIC. A nice example of this is in "Introduction to Statistical Learning with R" in the chapter on regression models including 'best subset' and regularization. They do a pretty thorough analysis of the 'hitters' data set. One can also do a thought experiment. Imagine one is trying to predict output on the basis of some known variables. Adding noise variables to the fit will increase $R^2$, but it will also decrease predictive power of the model. Thus the model with noise variables will have higher $R^2$ and higher AIC. To understand more with respect to the above explanation, you can go through this Link 

once these are achieved then look into Modelling(you have done most of the above steps). Now coming to your problem: When you are trying to apply Logistic Regression on data to predict then you need to take care of couple of things: 

Now once the data is ready, you can give them as an input to Boruta, by using this you get the predictor importance graph. For better understanding you can go through this Link or else if you want to learn why, how and when you can go through this Link for different tests for different variables. Boruta does all the above by itself and the outcome is set of important features, With Respect to that you can feed those respective features to your model for getting better results/accuracy. As your problem is Binary Classification, you can use the following Algorithms: 

The outcome of a multi-nomial or binomial is confusion matrix (2*2 for binomial, n*n for multinomial), Interpretation of the confusion matrix is for example: 

Welcome to the site! As Media has mentioned values of A,b,c are passed through those matrices. To understand it better I'm naming the constraints from top to bottom i.e., 1st constraint as constraints-1,......constraint-4. Firstly, lets talk about matrix b which consists of all the values on the right hand side of the constraints i.e., 1,2,0,4. He converted all the constraints to $⩽$ by multiplying with -1 on both sides. So he multiplied with -1 for constraints-2 and 3. Now the constraints-2 has changed from $x_1 + x_2 ⩾ 2$ to $-x_1 - x_2 ⩽ -2$ Values of Matrix b would be 1,-2,0(-0),4 Now lets go through the constraints i.e., 

In my scenario for predicting the sentiment of tweets(real-time tweets) I used 1.5 GB of Tweets which where segregated manually as Positive or Negative. I used that for training my model and I could get an accuracy of 90%(out of 10 tweets 9 were segregated correctly). It was time consuming process but it helped me to get good outcome. 

I think that you can approach this problem in a better way. To do so you need some Customer Demographics. Even if you try doing some recommendation using customer ID it won't give you ideal Basket/Recommendation, because from Customer ID alone you can't decide anything. Is there any possibility to get Transaction Id, so that you can unique baskets to apply Aprori Algo/ any association mining techniques by which you can give good recommendation using Lift and Support. In the time stamp do you have time? As your sample record doesn't show time in it(just for confirmation), Let us consider a scenario you have it. Now ['Celery', 'Beets', 'Cheese'] - transform as Basket-1, ['Celery', 'Bread', 'Cheese'] as Basket-2 and so on... Now you see which Baskets are sold the most WRT timestamp. This is to see if there is any seasonality in the data and you can even find some trends, if there are any. This is just one way of looking at the problem but this is not the concrete solution. 

First thing first, when ever you use Time Series data you call it as Forecasting not Prediction as it is time dependent. To understand why you can go through this link Metrics to compare models When you are trying to compare between models you need to use AIC,BIC, AUC etc values. you can go though this link to understand better Metrics to Access the Model When you are accessing the performance of the model then you need to check for Error Rates(RMSE, MAE, MAPE, MSE etc). Yes, in this case you need to divide the data into Train and Test to access the model. You can go through this link for better understanding Improve the Forecast To take it to the next level, you can use an ensemble to get better the result. This might or might not decrease the error rate. In most of the cases it is helpful. You can combine 2/3 moderately performing hotels outcome to get best results i.e., Ensemble Model. 

Main reason for suggesting Ensemble is Navie Bayes classifier works best with short sentence but for rest that is not the best model. Regarding Sampling, I think to do that you need to understand data clearly as to extract sample you need to make sure that every dimension of the population is covered(Sample should exactly represent the population). So, you should be very careful when implementing Sampling techniques. Choose the best model, you need to try all the methods apply them in all applicable conditions and based on your Business Application and results achieved, you need to decide but generally people implement Ensemble to achieve best results(in most of the cases). If you need more on Ensemble or any other technique let me know, will help you! 

To answer your question, you need not transform the numeric to binary variable(you meant binning right?). I will try to explain it with the above example: 

What you said is right, the above equation is for normalizing the data with-in the range of [0,1] Now, we can generalize using the below equation To normalize in $[-1,1]$ you can use: $$ x'' = 2\frac{x - \min{x}}{\max{x} - \min{x}} - 1 $$ In general, you can always get a new variable $x'''$ in $[a,b]$: $$ x''' = (b-a)\frac{x - \min{x}}{\max{x} - \min{x}} + a $$ 

By this you will get the baskets that is nothing but your desired outcome. Once that is done you can directly apply Association Mining to get the desired recommendations and select the Best Recommendations. Do let me know if you have anymore doubts. 

I think you can perform Predictor Importance test and see which are the variable explaining the most. There is this package named Boruta, you can go through the link for implementation in python. You can eliminate the variables which are highly correlated. For example if you have age as the target variable and you have DOB as a feature then it makes no sense to build a model. So, you need to make sure to eliminate the variable which are highly correlated to the target variable. In my Scenario I had this following visualization As you can see the 2 variables(underlined with red dash) are highly correlated with the target variable, before removing these variables the MAE was 0.9(approx) after removing those features(Backward Stepwise Elimination) and the MAE was 3.5(approx) but that is the actual error. Currently working on getting some external features to explain the data and to improve the accuracy. Every time it is not about accuracy/error rate of the model, it is also about how good our model could be generalized and robust our model should be. To check if the data is overfitting, then I tried testing it by taking those 2 variables and try modelling and the MAE was 1.6(approx) from this we can understand that these 2 variables explain the most. So, try applying and see how the features are correlated with the target variable. One of the methods used to address over-fitting in decision tree is called pruning which is done after the initial training is complete. In pruning, you trim off the branches of the tree, i.e., remove the decision nodes starting from the leaf node such that the overall accuracy is not disturbed. This is done by segregating the actual training set into two sets: training data set, D and validation data set, V. Prepare the decision tree using the segregated training data set, D. Then continue trimming the tree accordingly to optimize the accuracy of the validation data set, V. You can go thorough this link, about how we can avoid over fitting by tuning the parameters.