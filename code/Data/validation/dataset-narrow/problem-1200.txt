Let us fix an encoding of Turing-machines and a universal Turing-machine, U, that on input (T,x) outputs whatever T outputs on input x (possibly both running forever). Define the Kolmogorov complexity of x, K(x), as the length of the shortest program, p, such that U(p)=x. 

So I give a sketch why the problem is NP-complete. It is very sketchy, which you can take as a sign of trust that you're a smart guy, and not at all a sign of laziness on my part. We will reduce a variant of PLANAR-SAT, where we also require that the edges connecting the variable to its negated and unnegated occurrences form adjacent intervals in the rotation of the vertex of the variable; e.g., if each variable occurs at most once negated. The matrix will have a small top-left corner that will contain the important information, and many additional rows and columns to impose a structure on this part. In particular, I claim that with properly chosen additional rows and columns, we can achieve that instead of arbitrary permutations, we can restrict the problem to permutations that do not change the rows and can swap only given pairs of adjacent columns, or otherwise the number of components would be larger than $k$. If we can achieve this, then in the top-left corner we "draw" the graph of our PLANAR-SAT, such that at the heart of each vertex there is a pair of swappable columns. Every other row is constant on these two columns, so only the neighborhood of the vertex is effected. And at this vertex the negated clause-edges come from one side, the unnegated from the other, the swappable column decides which ones are connected to some main component. Therefore, the CNF is satisfiable if and only if all clauses can be connected to the main component. Since I didn't provide any details about the additional part, it is not clear how $k$ depends on $n$. Can $k$ be kept constant? 

After the newer and newer successes of neural networks in playing board games, one feels that the next goal we set could be something more useful than beating humans in Starcraft. More precisely, I wondered whether 

I have serious trouble understanding one step in the paper of Dobkin and Kirkpatrick about the separation of polyhedra. I am trying to understand this version: $URL$ It argues that after we know the best separation of $P_{i}$ and $S$, realized by $r_i$ and $s_i$, we can find the best separation of $P_{i-1}$ and $S$ in $O(1)$ steps. This is done in the following way. We take the plane parallel to $S$ through $r_i$ and cut $P_{i-1}$ into two parts with it. On one side, the closest point to $S$ is $r_i$ and on the other we have an ``elementary'' polyhedron that we can check in $O(1)$ time. My problem is - how do we find this elementary polyhedron? Note that the degree of $r_i$ in $P_{i-1}$ might be unbounded. In the pdf to prove Thm 5.1 from page 9, they use Thm 3.1 from page 4, which makes the whole thing harder to follow. 

Actually, the Damerau–Levenshtein distance is a metric. (See, for example, §11.1 of Encyclopedia of Distances, by Deza & Deza, Springer, 2009.) That is, it does obey the triangle inequality. This can be seen quite easily if you view every possible string as a node, with the edit operations and transpositions between them as edges. The Damerau–Levenshtein distance is then a shortest-path distance between the two strings in question, and such distances (graph geodesics) are always metric (i.e., triangular): You can't take a shortcut (breaking the triangle inequality) without simultaneously discovering a shorter path (a contradiction). The problem is that actually computing the Damerau–Levenshtein distances is tricky, and you may see algorithms that compute something similar (optimal string alignment, for example), but that don't actually find the minimal number of steps. If Damerau–Levenshtein with adjacent transpositions is OK for your application, you can use that. You'll find algorithms for computing it online (even in Wikipedia), and as long as your code computes the minimum number of steps, the triangle inequality is obeyed. (But note that, as stated, algorithms based on finding the Damerau–Levenshtein distance through optimal alignment will not obey the triangle inequality, but that is because they don't really find the true Damerau–Levenshtein distance.) If you want the unrestricted Damerau–Levenshtein distance (without necessarily using adjacent transpositions): Fear not, there are ways of computing that as well! You could have a look at the work of Lowrance & Wagner, for example. If you don't want to get into the details of that, you could simply use a heuristic algorithm such as $A^*$ to compute the distance, with plain Levenshtein distance as the lower-bounding heuristic, for example. Should be efficient enough for reasonably-sized strings, and will give you the corret Damerau–Levenshtein distance. And it will work with your BK-trees. 

Let you look up membership efficiently; Iterate over the members in linear time (as a function of the number of remaining members); Remove members efficiently; and Reset the table efficiently, for running multiple searches. 

My intuition comes from the fact that DP formulations in general are equivalent to path problems (such as shortest/longest path, or the number of paths) in DAGs. I think counting the number of paths from some node $s$ (representing the zero-capacity knapsack and the empty subset of items) that are “long enough,” given some threshold (in your case, $(1-\varepsilon)v$) is quite straightforward as well. You can formulate it recursively, as follows. Let $G=(V,E)$ be your DAG, let $w(u,v)$ be the edge weight function, let $\ell_\delta(v)$ be the number of $s$-paths ending at node $v$ with a length of at least $\delta$. You then have: \begin{equation} \ell_\delta(v) = \begin{cases} 0 & \text{if $v=s \land \delta > 0\,$;} \\ 1 & \text{if $v=s \land \delta \leq 0\,$;} \\ \sum_{u:(u,v)\in E} \ell_{\delta-w(u,v)}(u)& \text{otherwise.} \end{cases} \end{equation} I'm implicitly assuming that all nodes lie on a path from $s$, as they will in your case, but for nodes beside $s$ that don't have predecessors, the sum will simply be an empty sum, which can be assumed to be zero. To implement this, just memoize it or turn it “upside down” and fill the appropriate array. In the knapsack case, you'd only have two predecessors $u$ to sum over, of course (i.e., the ones representing the solutions with and without the object under consideration in node $v$). The solution will simply be $\ell_{(1-\varepsilon)v}(t)$, where $t$ is the final “corner” of your matrix, with full capacity and the full set of items. 

The questions is what you are allowed to do, can you look at the names of all the boxes or you just one to decide for a given ball and box whether the ball is in that box? I suppose y is at most x in the problem. If you can look at only one box, then its name has to describe which balls are in it, so you would need x/y log x bits/boxes. If you can look at all the box names, then the problem becomes to count the number of ways you can evenly distribute x balls into y boxes, I think this will also give x/y log x bits/boxes though I am not 100% sure. 

I don't think that having a non-parsimonious reduction helps in proving that a counting problem is in FP. Eg. if either 1 or 2 instances are mapped to 1, and which of these depends on something NP-hard, then you do not know anything about the complexity of your problem. Of course, you might have a special reduction from which you get some extra information. 

As far as I know, this is still open. A very recent paper that mentions these quantities and some bounds is Aaronson et al: Weak parity (see $URL$ You can also see chapter 14 of Jukna: Boolean funcions and the 1999 (still beats 1998!) survey by Buhrman and de Wolf. Another very recent paper about randomized decision tree complexity is Magniez et al: $URL$ Finally, a short summary I made for myself last month (without defs): R2<=R0<=D<=n D<=N0*N1<=C^2<=R0^2 s<=bs<=C<=s*bs<=bs^2 (new: [Gilmer-Saks-Srinivasan]: there is f s.t. bs^2(f)=O(C(f))) D<=N1*bs<=bs^3<=(3R2)^3 deg<=D<=bs*deg<=deg^3 (new: [Tal]: bs<=deg^2) D<=N1*deg C<=bs*deg^2<=deg^4 Sensitivity conjecture is that s is also polynomially related to other parameters. 

The problem is NP-complete already for $k=3$ (and I guess also for $k=2$). We reduce the following problem to it: For an input $r$-regular graph $G$, decide whether $G$ has an equitable $3$-coloring, i.e., one where the size of each color class is the same. (I couldn't find a reference for this, but one can easily reduce the classical $3$-coloring problem to this by adding a few extra vertices and edges.) The reduction is as follows. For each vertex $v$ of the input graph $G$, define a set $S_v=\{\{u,v\}\mid uv\notin E(G)\}$. For any subset $X$ of the vertices we have $$|\cup_{\{v\in X\}} S_v|=|X|(n-1-r)-|X_2|$$ where $X_2$ is the set of non-adjacent pairs in $X$. If $|X|\ge n/3$, this is at least $$\frac{n(n-1-r)}3-\binom{\frac n3}{2}$$ with equality only if $|X|=n/3$ and all pairs in $X$ are non-adjacent. Since in any grouping of the $S_v$ into three groups we have such an $X$, we are done. 

I’m sure there are several ways of dealing with this, but one I’ve come up with for my current research code lets you do 1., 3. and 4. in constant time with a really small memory overhead. The structure assumes that every member is represented by an integer $0\ldots n-1$, and that you have two tables of size $n$ that can accomodate such integers; let's call these $\pi$ and $\pi^{-1}$. You can then use these to represent a permutation of the members — as well as the inverse permutation. Basically, the inverse permutation $\pi^{-1}$ is simply an array of members (answering the question “Which member is in position $k$?”), while the permutation ($\pi$) gives you the location of any given member. In addition, you store the number of remanining members, $m$. Initially, you need to fill these two arrays so that $\pi = \pi^{-1} = \langle 0, 1, 2, \ldots, n-1\rangle$, as well as set $m=n$. To reset it, though, you only need to set $m=n$ (constant time). Adding and removing elements only requires you to swap the given member into the position just inside/outside the “cutoff point” given by $m$ (updating both $\pi$ and $\pi^{-1}$), and then to increment or decrement $m$, as needed. To make best use of this setup, you would only have to compare $m$ to size of the neighbor array of the current node in your BFS. Iterate over whichever is smaller, and do the lookups in the other one. 

If you don't use the flow per se, but use the Ford-Fulkerson algorithm (or some version, like Edmonds-Karp), you can get both the max-flow and the min-cut directly as a result. When looking for augmenting paths, you do a traversal, in which you use some form of queue of as-yet-unvisited nodes (in the Edmonds-Karp version, you use BFS, which means a FIFO queue). In the last iteration, you can't reach $t$ from $s$ (this is the termination criterion, after all). At this point, the set of nodes you reached forms the $s$-part of the cut, while the nodes you didn't reach form the $t$-part. The leaf nodes of your traversal tree form the “fringe” of the $s$-part, while the nodes in your traversal queue form the fringe of the $t$-part, and what you want is the set of edges from the $s$-fringe to the $t$-fringe. This can also easily be maintained during traversal: Just add an edge to the cut when it is examined, and leads to an unvisited node, and remove it if it is traversed (so its target becomes visited). Then, once Ford-Fulkerson is finished, you'll have your min-cut (or, rather, one of them) right there. The running time will be (asymptotically) identical to Ford-Fulkerson (or Edmonds-Karp or whatever version you're using), which should give you what you were looking for. 

No. There's a huge literature on the topic, called combinatorial search theory, you can read more about these types of questions there. The simplest example that I could think of is the following. Suppose that you want to find an edge of a graph, and you can ask whether a given vertex is incident to the hidden edge, or not. Now take the following bipartite graph. It contains a perfect matching on $100+100$ vertices. Moreover, it also contains $10$ degree $10$ vertices that are connected to $10$ different vertices each, all on the same side. So one side of the graph has $100$ vertices, all degree $2$, the other $110$, of which $100$ have degree $1$, the other $10$ degree $10$. In this case, the greedy algorithm would first query the degree $10$ vertices, and then the rest, taking about $110$ steps. If instead it started with querying the $100$ vertices of degree $2$, it would be done in $101$ steps. 

As I was told by Abhishek Methuku, this has been studied already, in fact several times, see e.g. $URL$ For $k$ large, the answer is close to $n \log_{k!}(n)$ but e.g. $k=3$ seems to be open. 

This is not an answer, just an explanation why for k=2 no such labeling can exist (I am sure this was already known to Alex, so this is just a write-up for other readers like myself...) For k=2 we have $T\ge {n \choose n/2}\ge 1.99^n$. This is because there are ${n \choose n/2}$ subsets of size n/2. If any two get the same label, e.g. A and B, then either the sum of the label of A and its complement is not in S, or the sum of the label of B and the complement of A is in S. This implies $T\ge {n \choose n/2}$ (for large n). For larger k a similar argument shows that all the labels must be different, but this only gives a weaker exponential lower bound. So already k=3 seems to be unknown. 

I think the answer is $\Theta(\log n)$ and the proof is the same as the classic Ramsey-theorem proof. On one hand, you always have a complete or empty subgraph with these many vertices. On the other, a random graph won't have a large induced $C_4$-free subgraph. For this latter, bound the number of induced subgraphs on $t$ vertices by $n^t$ and for each bound the probability of being $C_4$-free by $c^{t^2}$ where $c<1$ is some constant. This we can do because a complete graph on $t$ vertices contains $\Omega(t^2)$ disjoint $K_4$'s. In more detail, divide the $t\choose 2$ possible edges among any $t$ vertices into $\Omega(t^2)$ disjoint cliques of four vertices. In any such clique of four vertices, the probability that the edges among them will not form a $C_4$ is some constant $p<1$. Therefore the probability that there won't be a $C_4$ in any of the cliques is $p^{\Omega(t^2)}$. This is clearly an upper bound for the random graph to be $C_4$-free.