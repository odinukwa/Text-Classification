When 'overtraining' is not a problem (as in it will not diverge if you use more time), just use all your data and the empirically found optimal hyper parameters. In case of neural networks this is not the case (although in my experience, a lot of architectures converge to a specific test error, and take a long time to end up diverging again). I see a few options that you could try: 

It doesn't matter, with or without flattening, a Dense layer takes the whole previous layer as input. The spatial structure information is not used anymore. Some Neural Network implementations might not be able to map a spatial structure directly into a dense layer, which is why you would need the Flatten in between. Mathematically it is exactly the same in this case. EDIT: As I expected, in the source of Keras it mentions that if the rank is higher like with convolutions, they implicitly Flatten it first. $URL$ 

When you get a bit more insight into network topologies these hyperparameters will make more sense, but in general this is just like any other hyperparameter, you will have to test some settings and see what works better. In the case of pooling layers it is actually relatively interpretable. Why do we use pooling? To downsample our feature maps. This is done to decrease the resolution of our processed image but also to increase the receptive fields of our following layers. Because it is a form of downsampling we throw away all the information that was not the maximum in case of max-pooling, or we throw away information about the spread with mean-pooling. By increase our pooling filter size, we decrease our resolution even further and lose more information, but we will also need less parameters in our network if we have a fully connected layer at the end and our next convolutional layers will have a bigger receptive field, but with less attention to detail. 

Yes you turn it into three different variables, however this is not called multivariate regression, that indicates multiple output variables, not inputs. (Thanks to Simon for correcting me) 

Are you getting good results on your training or your test set? If it's only the first then you are overfitting, but if it's the second it will not have to do with the fact that your whole training set fits in your batch, the gradients still have a strong pull towards the dominating class(es). But having the pretrained weights will help a lot if the classes are relatively easy to seperate. It will also help if your dataset is similar to the one it was pretrained on, because the intermediate layers that do feature extractions have learned valuable filters for your problem instance. 

You should always use just your training data and then apply the same transformation on your test data. This is a much fairer representation of how your model will perform when real new unseen samples are fed to your model. The point of a test set is trying to estimate generalization on unseen data, using your test data leaks information into your training set. Plus, if you cannot apply the transformation to your test data, how will you apply it to new data? 

This work very well if you have few categories, however in the case of thousands of IDs this will increase your dimensionality too much. What you can do is collect statistics about the target and other features per group and join these onto your set and then remove your categories. This is what is usually done with a high number of categories. You have to be careful not to leak any information about your target into your features though (problem called label leaking). 

First of all I would clean up INC, LLC, BV etcetera from both the sources. After this there are a few options. Since Levenshtein is a metric you can use metric trees to search your space more efficiently (about O(n*log(m))). This will still be very slow so there are approximations available, for example the cosine similarity on bi-grams of the names. You can do this using matrix multiplication which is both very efficient and easily distributable. Instead of taking the highest similarity you could take the top-n and do further analysis on these, for example the real Levenshtein distance. The fact that you have additional information could be useful, you could add this to your similarity function in some way but this will be guess work. Most of these ideas I got from a PyData meetup that was recorded, a speaker from ING (a big bank) discusses the exact problem you have albeit on a bigger set with less additional information: $URL$ 

It depends a bit on your definition, usually the stochastic part of Stochastic Gradient Descent refers to the fact that you sample mini batches and estimate the true gradient with this sample. Dropout adds stochastic behaviour by sampling masks that comes down to sampling new network architectures every time. You could certainly see this as a form of stochastic gradient descent but whether this is the case or not depends solely on the definition. 

You can parameterize continuous distributions with a number of distributions (normal, Beta, Gamma etc). For discrete distributions you have Bernoulli (for two cases), Poisson (for infinite discrete possibilities, where there is a clear order), softmax for unordered, finite discrete possibilities. Let's say I have a set of ordered, finite possibilities. Let's say 0..10. However there is a clear correlation between the choices, 2 and 3 are closer to 1 and 7, so I want the probabilities to be correlated as well (like with the Poisson). What would be good options? I am looking to estimate these parameters using neural networks (like with a softmax). 

The behaviour you are seeing is not related to not properly resetting models but the stochastic nature of most of these algorithms. By setting a random seed the same random numbers will be generated every time. See: $URL$ However, while this will lead to a reproducible sample, this might still not be fair. If one model randomly gets a good seed and another one a bad one you will unfairly always favor the first one. What you could do is run the models multiple times with the same hyperparameters but with different seed and look for the average performance. This way you get a more fair comparison and the reproducibility. Pick the seeds up front, then you can loop over them. Something like this: 

Other options would be doing something with sine and cosine functions but I feel like the fact that multiple pre-activations map to the same output will also make optimization and generalizations very difficult. 

What you propose is certainly possible, you would just have n nodes in your output layer, with n being the size of your MFCC feature vector, and you need to define a loss function that determines what mistakes it is currently making. If all the features have a similar scale and similar importance you could use the Mean Squared Error but you can get creative here. In principle this is not very different from multiclass classification with a softmax layer at the end. In that case you have c output nodes after a softmax layer and a cross-entropy loss function, in your case you have n nodes in your output layer after for example a fully connected layer with a MSE loss function. Feed it a number of examples and it should do the task reasonable if the data is represented well. Although I'm uncertain why you would want to use a ML approach if this is calculatable, you will lose accuracy and I'm uncertain how much speed you win. A very easy example where TensorFlow is used for a regression task and not a classification task is linear regression with gradient descent, which you can see here: $URL$ 

The way gradient boosting is constructed and trained, there is not an obvious solution for this without just training from scratch. Other models might be more suitable for this (Adding this to a neural network and retraining this will take less time than from scratch I think). Another approach would be to use these time-based lag features as a time series, that way you just input your features and it will learn relationships between the past few weeks instead. 

It's really dependent on the problem you are solving. Neural network layers attempt to get more and more abstract representations of your input, tailored towards your output. So the factors that influence your output need to be able to be represented well at each abstraction level. Each abstraction level represents more and more non-linear interactions between your inputs and hopefully less and less noise. Let's take convolutional neural networks as an example, because it's slightly easier to interpret what is happening. The first levels are looking for small patterns while the later levels are looking for combinations of these smaller patterns that are relevant. If there are a lot of different small patterns that are relevant for your task but the later layers are only looking for a few different combinations of these filters your 3rd example would be appropriate. On the other hand, if you have some edge detectors and then some basic shapes in the first few abstraction layers, but you are trying to distinguish between 1000 classes that are all built up from these basic shapes, then you would need a more extreme variant of the second example. The first example is somewhere in between. 

You need examples if you want to regress ratings. I would get someone with domain knowledge (product owner for example) to take a look at a subset of your data, let them rate those data points and use this subset to train a model on your features and maybe some added artificial features. Then you can use this model to rate the other drivers. 

It depends on the clustering technique you use. Since you tagged this post with I will assume this is what you are using. Cluster centers should already be somewhat informative for laymans, but since you should be/are scaling this can lose some of it's interpretation. What you could do is assign class labels to each sample based on in what cluster they ended up in. Then you could fit a multi-class decision tree to your data and use the decision rules for interpretation, like 60% of cluster 1 has $x_1 < 0.9$. 

With binary classification (your first example) you use a sigmoid function at the end, where the value indicates the probability P(Red), and 1-value equals the probability of P(Blue). Because there are only two classes we can use one value because together they sum to one. By using binary crossentropy loss we can fit our model. When you have to deal with more than two classes having one value is not enough anymore. What you do then is to have n output values that are mapped to a probability distribution using the softmax. This comes down to taking the e-power of all of them and then normalizing them to sum up to 1. By using the multiclass crossentropy loss which is the generalized version of the binary crossentropy loss we can still estimate this. EDIT: Where with binary classification we map using the sigmoid: $$P(Red) = \frac{1}{1+e^{-Wx}}$$ Now we have a probability for all colors individually: $$P(Color = y) = \frac{e^{W_yx}}{\sum_{c\in C}e^{W_cx}}$$ 

Decision Trees cannot deal with strings. To be able to learn from your log lines, you need to represent what is happening at the logs. You can use both categorical features and numerical features. This process is called feature engineering, which is part of preprocessing. I don't know what the number after yoru header represents but this could very well be a feature if it influences if it counts as 'Normal' or 'Abnormal'. I would extract a categorical feature for the type of request, like GET and POST. You could add a granular level that tries to represent what document the client requests. Just feeding strings to Machine Learning algorithms will not work (un)fortunately. 

Given that in your training data this feature has different values and some predictive power, I think not keeping this feature would be a mistake (without looking into overfitting due to having too many features). You cannot just discard the feature from your training set if it does influence the target because then these would be from a different population than your predictions and it will be able to learn from the other features. Extreme example where x_2 will always be 5 in the future: 

Certain models are able to deal with missing values 'naturally', like certain tree based models. Most models however are just a mathematical function which is shaped after the training data. A very easy example would be: $$f(x) = \alpha x_1 + log(2x_2)$$ What do you do if one of them is NaN? The function is undefined and no prediction can be made. By imputing the values you make a reasonable guess where this sample lies on the data manifold.