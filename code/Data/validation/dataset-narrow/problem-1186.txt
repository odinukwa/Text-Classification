Shannon coding always sets the length $ls_i$ of each codeword to a function of how many times $f_i$ it occurs in some text of length $LN$: $ls_i=\lceil -\log (f_i/LN)\rceil$. two-symbol Shannon-Fano coding and Huffman coding: always sets the codeword for one symbol to 0, and the other codeword to 1, which is optimal -- therefore it is always better than Shannon coding in this case (or equal, in the case where both probabilities are 1/2). multi-symbol Shannon-Fano coding and Huffman coding: case (a): sometimes there exists some letter $i$ assigned Fano codeword with a length $lf_i$ 1 bit longer (worse compression) than $ls_i$. multi-symbol Shannon-Fano coding and Huffman coding: case (b): sometimes there exists some letter $j$ assigned Fano codeword with a length $lf_j$ shorter (better compression) than $ls_j$. Whenever case (a) occurs, case (b) also occurs at least as many times. Each letter (if any) $i$ that falls under case (a) can be paired up with some other letter $j$ that not only falls under case (b), but also letter $j$ is more frequent than letter $i$. Therefore, whenever case (b) occurs, the total number of bits needed to store all the letters $i$ and all the letters $j$ with with Shannon-Fano coding is no worse than with Shannon coding: $f_i lf_i + f_j lf_j \leq f_i ls_i + f_j ls_j$. Therefore Shannon-Fano coding is always at least as good as Shannon coding. 

Notice that $f$ can be tight but still improvable. If you have that $g(n)=n-\epsilon\sin((n-1)/100)$ then $f(n)=n^2$ is a tight bound. It is an upper bound for $g$ and they agree for $n=1$. However, that doesn’t mean it cannot be improved. A better bound might be $f_k(n)=n^{1+k^{-1}}$. This family has the property that $g(n)<f_k(n)<f(n)$ for $n>1$ and $g(1)=f_k(1)+f(1)$. I am generally not a fan of the term “optimal bound” because it’s usually inaccurate. The optimal bound for a sequence of integers is simply that sequence of integers, so if you have an “optimal bound” you almost always have an expression for the underlying process. There are contexts in which the previous paragraphs is wrong, however, and you seem to think that you’ve proven that you bound is the best possible. If that’s the case, there aren’t any caveats to that sentence, and what you’ve proven is indeed a bound rather than an expression for the number of vertices in the graph, then “optimal bound” or “best possible bound” is the terminology that I would recommend using. 

As in the above formula, for each set $W_{ss'}$ (and $W_{ss'}^F$) we can decide wether it occurs in the intersection (and hence determines if one complement $A^{\ast}\setminus W_{ss'}$ and $A^{\ast} \setminus W_{ss'}^F$ occurs) we have $2^{n^2}\cdot 2^{n^2}$ options (different then $|Q^{2^{2Q}}|$ which would be given by the identification of the classes with functions $f : Q \to 2^Q \times 2^Q$ in the wikipedia article). 

Let $A$ be some finite alphabet and $\mathcal A = (Q, \delta, q_0)$ be some determinisitic finite automaton. Then $\mathcal A$ accepts infinite words $\xi \in A^{\omega}$ according to the Muller condition iff there exists some $\mathcal F \subseteq \mathcal P(Q)$ (called the table) such that $$ \operatorname{Inf}_{\mathcal A}(\xi) \in \mathcal F $$ where $\operatorname{Inf}_{\mathcal A}$ denotes the states which are traversed infinitely often when $\xi$ is processed. It accepts according to the Rabin chain condition (or parity condition) if we have some $\mathcal C = \{ E_0, F_0, E_1, F_1, \ldots, E_n, F_n \} \subseteq \mathcal P(Q)$ such that $$ E_0 \subseteq F_0 \subseteq E_1 \subseteq F_1 \subseteq \ldots \subseteq E_n \subseteq F_n $$ and there exists some $k \in \{1,\ldots, n \}$ with $\operatorname{Inf}_{\mathcal A}(\xi) \cap E_k = \emptyset$ and $\operatorname{Inf}_{\mathcal A}(\xi) \cap F_k \ne \emptyset$. Now we have that: Theorem: Every $\omega$-language acceptable by some Müller automaton is acceptable by some Rabin chain automaton and vice versa. I have a question on the construction of a Rabin chain automaton for some Müller automaton. The proof uses a construction called the memory extension of a given Müller automaton $\mathcal A = (Q, \delta, q_0)$ with table $\mathcal F$. An arrangement of some symbols is a sequence or word such that every symbol occurs at most once, then let $S = \{ (u,v) \mid uv \mbox{ is an arrangement of } Q\}$ be the arrangements of $Q$ with an additional mark in between, which marks the previous position of the last state. The empty sequence is denotes by $\varepsilon$. Then build the automaton $\mathcal B = (S, \mu, (\varepsilon, i))$ with $$ \mu((u,v), a) := \left\{ \begin{array}{ll} (x, yq) & \mbox{ if } uv = xqy \\ (uv, q) & \mbox{ if } q \notin uv. \end{array}\right. $$ A crucial property of the memory extension is the following: Property: If $\mathcal A$ is an automaton and $\mathcal B$ its memory extension and $c$ is an initial path in $\mathcal A$ and $c'$ its corresponding path in $\mathcal B$. Then $T = \mbox{Inf}_{\mathcal A}(c)$ if and only if all states $(u,v) \in \mbox{Inf}_{\mathcal B}(c')$ satisfy $\underline{v} \subseteq T$ and at least one satisfies $\underline v = T$; where $\underline v$ denotes the symbols occuring in the sequence/word $v$. Now with this, given some Müller automaton $\mathcal A = (Q, \delta, i)$ with table $\mathcal F$, we build its memory extension $\mathcal B = (S, \mu, (\varepsilon, i))$ and define the following chain $E_0 \subseteq F_0 \subseteq \ldots \subseteq E_n \subseteq F_n \ldots$. For $i \ge 0$ let $E_i$ be the set of states $(u,v)$ of $\mathcal B$ such that either $|u| < i$ or $|u| = i$ and $\underline v \notin \mathcal F$. And let $F_i$ be the union of $E_i$ and the set of states $(u,v)$ such that $|u| = i$ and $\underline v \in \mathcal F$. Then with this chain $\mathcal B$ accepts the same $\omega$-language as $\mathcal A$. 

Nice counterexample. A short "proof" that Shannon-Fano coding is always at least as good as Shannon coding over the long term, even though it may be worse for a few specific letters: 

The difficulty of reverse-engineering the algorithm seems to depend on how much of the keyspace has already been expired. Say the algorithm of Mr. X is very restricted, so there are (say) only 10,000 potentially valid keys. If Mr. X only recently started this company, so there are very few expired keys -- and hence few "false negatives" -- then reverse-engineering the algorithm may be relatively easy. If Mr. X has already expired 9,000 of the potentially valid keys, and so we have 9 out of 10 "false negatives", then reverse-engineering the algorithm seems to be much more difficult. And, of course, if Mr. X has already expired every potentially valid key except for the ones that Mr. Y and his collaborators already know, then the "taciturn oracle" will give him zero new information. 

I would agree with the commenter that this should be referred to as a cardinality constraint or a collection of cardinality constraints. I might also call it a "structural constraint," but I would specifically avoid calling it a "group carnality constraint" as "group" already has a meaning. In general, I would advocate for avoiding referring to a collection as a "group," "set," or "class" unless your object actually satisfies the required axioms. "Collection" is a good, general word that doesn't have a mathematical meaning. If you don't want to use the term "carnality constraint" you can always introduce a term to refer to this specific restriction in your context. This can be helpful because it allows you to highlight the context-sensitive meaning of the constraint in a way that a more general term doesn't. For example, I have a paper with a constraint of this form that I refer to as an "isolation condition" because, when $S$ satisfies it, $S$ is isolated from the structure of the rest of the graph in a relevant way. This gives me a simple and intuitive way to refer to the condition, while simultaneously telling the reader what is important about the condition every time it comes up. 

The conclusion that $\delta(p, w^R) = \delta(q, w^R)$ does not seem to hold; but for the argument used in the proof of the paper this is not necessary. More specifically, in the paper it is enough that for every distinct $p,q \in \{1,\ldots, n-2\}$ there is no word $u$ such that $\delta(0, u) = p, \delta(r, u) = q$ for some $r \in \{1,\ldots, n-2\}$. Suppose the above holds for two $p,q \in \{1,\ldots, n-2\}$ distinct, then as written in the post we have some $S$ with $p,q \in S$ and a word $w$ such that $\delta(F, w) = S$, which gives $\delta(p, w^R) \in F, \delta(q, w^R) \in F$. But then $uw^R \in L$ and furthermore as the automaton is minimal, we have some word $v$ such that $\delta(0,v) = r$, hence $vuw^R \in L$, contradicting the suffix-freeness of $L$. 

What is the standard approach on minimizing Büchi-Automata (or also Müller-Automata)? Transfering the usual technique from finite words, i.e. setting two states to be equal if the words "running out" of the states which are accepted are the same, will not work. For example consider the Büchi-Automoton accepting all words with an infinite number of a's consisting of two states, a initial and a final state, and the final state is entered each time an a is read, and the initial state is entered each time a different symbol is read. Both states are considered equal by the above defintion, but collapsing them yields an automata consisting of a single state, and thereby accepting every words. 

Alas, there's a lot of hand-waving in this "proof". I suspect there might be a better proof in the book Yaglom and Yaglom: "Probability and information". p.s.: You might also be interested in yet another algorithm for generating prefix codes, "Polar coding" developed by Andrew Polar. 

You are right that in many real-world applications, we never need to transmit an empty string. You are right that in those applications, we could divide the unit interval into only two pieces, [0,(P(b)−P(a))/(1−P(a)),1] when encoding the first bit, which corresponds to mapping [P(a),P(a)+P(b),1] to fill up the unit interval. However, arithmetic coding already does that, so there is no need to add special case code to handle that, much like there is no need to add special-case hardware to zero the AX register in the special case of XOR AX, AX. details At any time during sequentially encoding or decoding, the P(a) is the probability that the message ends at that time. If you know for a fact that P(a) is zero at one or more times during sequential decoding -- if you know that you will never need to transmit the empty string, or you know for a fact that you will always transmit a multiple of 8 bits -- then the compression implementation and decompression implementation should set P(a) to zero at those one or more times. Then the normal [0,P(a),P(a)+P(b),1] intervals becomes [0, 0, P(b), 1], giving effectively only two intervals of non-zero measure -- [0, P(b), 1]. Your suggestion of [0,(P(b)−P(a))/(1−P(a)),1], when you know P(a) is zero, becomes the same two intervals [0, P(b), 1]. does arithmetic coding assign any string an encoding in the interval [0,P(a))? Yes, the encoder assigns the empty string an encoding in that interval -- but only if that interval exists. However, in the special case where P(a) is zero, the interval [0,P(a)) becomes [0, 0), which is empty (not even a single point). In that special case where you know we will never need to transmit an empty string, then P(a) is zero while encoding that first bit, and so arithmetic coding does not assign any string a finite-length encoding in the interval [0,P(a)). And so, in that special case where P(a) is zero during the first bit, a arithmetic decoder will never emit an empty string, no matter what compressed representation it is given. Even if the range of some symbol did include exactly one single point -- such as, for example, [0,0] -- the decompressor would never emit that symbol. Even the decompressor is given the sequence of decimal digits (or whatever other base the corresponding arithmetic compressor uses) 00000000000.... , each "0" of which narrows the range down more and more but never enough to put the range of that value as exactly equal to or inside the [0,0] range. As more and more "0" symbols in the compressed representation is fed to the decoder, the midpoint of the current range keeps getting closer to, but never actually enters into the single-point range [0,0]. This is related to the zero-frequency problem. 

so that if $(u', v') \in \mbox{Inf}_{\mathcal B}(\xi)$ with $|u'| < i$ we have $|v'| > |v|$ and this contradicts $\underline v' \subseteq \underline v = T$, further if $|u'| = i$ then as $\underline v' \subseteq T$ we have $\underline v' = \underline v$ as $|v'| = |v|$. This gives $E_k \cap \mbox{Inf}_{\mathcal B}(\xi) = \emptyset$. Conversely if there exists some $\xi$ and $k$ with $\mbox{Inf}_{\mathcal B}(\xi) \cap E_k = \emptyset$ and $\mbox{Inf}_{\mathcal B}(\xi) \cap F_k \ne \emptyset$, then we have to show that for $T := \mbox{Inf}_{\mathcal A}(\xi)$ we have $T \in \mathcal F$. By the last equation we have $(u,v) \in \mbox{Inf}_{\mathcal B}(\xi)$ with $|u| = k$ and $\underline v =: S \in \mathcal F$. Further by the property some $(u',v')$ satisfies $\underline v' = T$ and $S \subseteq T$. As $\mbox{Inf}_{\mathcal B}(\xi) \cap E_k = \emptyset$ we have for all $(u'', v'') \in \mbox{Inf}_{\mathcal B}(\xi)$ that $|u''| \ge k$, and if $|u''| = k$ this implies $\underline v'' \in \mathcal F$. As $|uv| = |u'v'|$ this gives directly $S = T$. 

This paper opens with an introductory survey on graph grammars and then advances two new applications. It’s dated (1992) but explains the concepts well enough that it seems like the kind of thing you’re interested in. 

My impression reading this question is that no suitable example of a problem that requires more than just PA (let alone ZF) has been given, and the excellent answer by Timothy Chow explains why it's so hard to find examples. However, there are some examples of TCS extending beyond the realm of arithmetic, so I thought I would give a theorem that requires strictly more than $ZF$. Although it doesn’t require the full axiom of choice, it does require a weaker version. The De Bruijin-Erdos Theorem in graph theory states that the chromatic number of a graph, $G$, is the sup of $\chi(H)$ as $H$ ranges over all finite subgraphs of $G$. Notice that the conclusion is trivially satisfied for finite $G$, so this is an interesting statement about infinite graphs. This theorem has many different proofs, but my favorite is to evoke Tychonov's Theorem. As mentioned in the Wikipedia article I linked to, this theorem really and truly requires more than $ZF$, however it doesn't go as far as requiring the "full axiom of choice." There's a horribly unreadable proof of this on the Wikipedia page, but basically the theorem falls in the Solovay Model due to a clever constructions involving measure theory.