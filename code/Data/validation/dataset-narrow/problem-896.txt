We are trying to eliminate a single point of failure for our files without a SAN. We currently have a single server for file serving (locally), web server, and database (in VM). Looking at using DRBD with GFS2 to create a cluster file system and then run as much as possible from that file system, including the Host OS, VM's, and shared files. We would like to have two servers setup in a master/slave setup running from a shared source; but the servers would have slightly different hardware configurations (drive port assignments, actual brands and models of main board, NICs, etc) so the hardware configuration files would be kept local (fstab and mdadm for example). Ideally, one server would act as primary. Package management would happen by the primary server only on the clustered file system because the slave server should not need to if it is using the same data (data from same block device). The master server would also be the single point of administration for both servers simply because they share the data and configuration files, and the second server simply sits as a redundant set of hardaware to run the software that is configured on server 1 and does so via STONITH, Hearbeat, Pacemaker, etc. But as I read, it seems as though most people administer the servers independently of each other, and try to automate using cron jobs and rsync. It also seems that most of the files are fairly static and user modify; that is the number of files/folders that are actively modified by the OS (logs, swapfile, etc) independent of user input and/or dependent on hardware are very small. We would like to keep the files served by the native OS, (not in a VM). So now the questions. 

Looking at using DRBD or a clustered files system to help with up-time when downtime strikes in a small business environment. We currently use a server box for a file server using Linux and samba, then running the web server and Database in a VM. Was looking at adding a second server and putting the files and the VM onto the distributed file system. The base OS is more static and easily can be managed more manually (copy config files at time of change, copy base OS if needed from full backups, etc) Question is about the fail over scenario if manually done. If server 1 goes down and fail over is manually done, is fail over completed by simply setting the static IP of server 2 to server 1 (again server 1 is down and would be in a state of needing repair), starting Samba, and starting the VM which would have the same static IP's as they had when running on server 1, and starting the backup services? This sounds like a quick and simple process, almost too simple. Am I missing something? This could easily be automated as well through a script or something that someone with little proficiency could be directed to run in the event of a failure. Down time if we have a hardware failure could easily be days without the support of on call IT support and the parts needed without a second server, but with the the second server, down time would be at the maximum a matter of hours (if no one is the office proficient enough to perform such operations, minutes if someone was) 

I have been working at setting this up in much of a way you describe and it works great! (XenServer) I setup an old but capable server as the primary host, this runs a console only VM for DRBD. This VM then serves a "SharedDRBD" SR back to the Xen Host via NFS. The rest of the working VMs providing services run on the SharedDRBD SR. The VM's DRBD dev is on its own VDI on a MDADM RAID 1. This SharedDRBD SR hosts the rest of the VMs for various services with a local larger RAID10 array for bulk filestorage. All MDADM work is done by the host, but one side of the DRBD is in a VM. The DRBD ran in a VM gets synced with a DRBD service running on the file backup server; the file backup server is NOT virtualized purposefully so we have bare metal access to all files given XenServer is the biggest quirk we generally deal with. There is a secondary server that is virtualized but has no local storage except for what is required for the host. This server is part of a Xen pool with the primary server to simplify failover. Failover is currently manual but fast and easy. First, all VMs on the SharedDRBD SR are shutdown while the secondary XenServer host is powered on. The DRBD on the file backup server is made primary and mounted as needed. Then, the SharedDRBD SR is pointed to the file backup server and VMs are started up on the secondary server; XenCenter doesn't even realize it is serving the VMs from a new location because it sees the same SR with the same data. The VMs are fired back up and things are back and running. There is alot more to it in terms of configuration, and arrays, network topology, etc; but the jist is DRBD is served in a VM back to its own host. Overall it is HA enough for our SMB / Home use; down time during a catastrophic failure of the primary server is 10-20 min or less to fully back online and no loss of data; DRBD means the VMs are up to date! Plus, outside of the primary server which is pretty robust, there is a ton of overall redundancy. Most of the primary server is redundant in-and-of-it-self so it pretty much gives us triple redundancy or better for just about every piece of hardware you can think of (PS, RAM, CPU, HDD, Controllers, NICs, etc) besides the motherboard(s) which is only double redundancy (primary/secondary Xen Hosts). And yes, XenCenter is installed on windows sadly, the rest is all Linux. I know, this Q's is 8 years old. 

I am looking to begin a tape backup regimen and am looking to keep data flowing to the tape drive in a sufficient manner (120+MBs target sustained) but cannot figure out how to do so without a dedicated source drive/array that idles when not writing tapes. The documentation for our specific drive mentions no minimum throughput required. Enviroment 

After running this setup in production for 6 months, I can say there does NOT seem to be any stability issues with the VMs running on a SR serviced from DRBD in a VM. The biggest issue is you have two "hosts" to worry about that will affect all other VMs, Dom0 and the DRBD server effectively adding a second point of failure software wise (configuration errors, administration errors, bugs, etc). However this has not proved to be an issue thus far. I have not ran any comparison benchmark for performance of VMs on and off DRBD although we have not had any noticable performance issues and the majority of data served is not served by DRBD; the SR on DRBD only host the VM host disks. TL;DR We had some extra RAM available on the HOST so I did set the DRBD server to use all remaining available RAM so it caches the data it serves. Bringing up the secondary server is relatively easy. It involves setting the backup servers DRBDs service to primary, mounting the DRBD drive, then removing the primary servers SR and re-adding it on the secondary server from the SR on the DRBD drive, then utilizing XenServers builtin backup and restore to reassociate the metadata to the virtual disks. This means the VMs used during an outage on the primary server are not outdated in anyway because they were actively replicated via DRBD vs a script. It is rather critical we keep the metadata up to date to make this work easily. This is used like a RAID and the VMs are still backed up in case of some other failure or corruption. 

This assumes the array can be taken offline which is not always the case. In the end though, some have found the same that building a new array from scratch and transferring data back in one fell swoop is easier and faster, than attempting a full rebuild on a large multi TB array. Further, I suspect that reading the data and writing the data off the array sequentially in a degraded state effectively only once would greatly lower the chances of a second drive failure before the data is duplicated compared to a full thrashing rebuild, although the chance is still there. In the end, its all about risk management which varies on the plethora of specific circumstances. In my particular case, I can usually find time within a 24 hour window to restore my array and thus freshly backing up, rebuilding, and restoring from the fresh backup would be best in my case. 

Is it bad practice to run a 3 copy MD RAID10 array on two drives with a missing drive in place of a 2 copy MD RAID10 array on two drives to allow for the addition of a third drive in the future? We are migrating servers and I will build a new array on the new server. The plan is to build a 3 copy array with a missing drive and leave it this way possibly for months to allow for the addition of a third drive in the future. Is this equivalent to running a 2 copy array or are there caveats to this? Our current array is a 2 copy MD RAID10 array on two drives. We would like to add another level of redundancy but we do not have the third drive yet or the tray for it and nothing is pressing us make the purchase to implement this immediately. 

I've realize UREs are a bit more complex and unknown to most as they relate to array failures.. The conclusion is UREs can cause arrays to fail, but not as often as that math in the articles say. But RAID 5 still is a very failure prone RAID array compared to ALL other RAID levels. So back to basics, what are we mitigating during a RAID 5 rebuild? We are trying to get parity back before a second drive fails. THATs IT! This is a by-any-means-necessary endeavor. This leads me to solidify my list 

(This is not a question of how to run both machines from the same set of data at the same time; they would be effectrively running from the same set of data at different times. The files the servers each need to constantly modify would be on the local file system) We simply want to setup a redundant server and minimize the redundant administration. 

I am assuming a sustained drop in throughput to below 10-20MB/s (or less) on the source during a tape write would be a problem? Do I need to have a source guaranteed to have no backups scheduled to it? Essentially 2 arrays minimum; one for backups and one for archives and tape writing? Is there a QOS for drives/arrays that could prioritize the tape writing over all else? LTO-4 tape drives throttle, so is there a common lower throughput limit to maintain for LTO-4 or does it vary widely per drive? Again, documentation mentions max designed speed and "variable speed transfers", but no mention of how variable. Am I missing something in this source-throughput equation, or have unfounded worries? 

Windows 7 Pro only supports (2) discrete CPU sockets so make sure you have allocated the cores over 2 sockets or less. In my case, I ran quickly through the settings and allocated 4 vCPUs over 4 sockets with 1 core per socket; only 2 of the 4 cores showed up in the VM. When I changed to the cores being in a single socket, all 4 cores were visible and usable in the VM. I know this years later, but I just experienced this and figured I would share. 

Given XenServer (7 currently) is based on CentOS, does that mean it works just like CentOS in terms of updating, CLI, administration (non-Xen specific like mdadm and boot loaders) etc? Basically, if I want to use XenServer, then am I committing to using, learning, and working in the CentOS "way"? We have a new (to us) server on the way and now is the time to switch hypervisors and we are set on using Xen. Our current setup that I am familiar with and can administer efficiently is a Debian host with a couple VMs using Virtual Box which is less than ideal to say the least. Due to this, I am familiar to working in Debian and have made a conscious choice to use Debian for our servers. I administer only our servers for our small business so I do not have the diversity of other setups and other distributions to work from. From my understanding, the way Redhat does things is a bit different from Debian based distributions and would require a learning curve of unknown amount; but a learning curve for sure. So if I use XenServer, am I also committing to the Redhat learning curve? I am aware that I can install Xen with a Debian based Dom0 but the consensus I have read seems to say XenServer works the best overall. However there will be a bit of configuration I will need to do such as getting our local RAID arrays up and running for the Dom0, Xen, and Network Shares, along with getting boot loaders and grub in order. I can do this configuration rather easily in Debian so I am trying to weigh the cost in time of trying to do the same configuration the CentOS way which I am afraid will add a considerable amount of time to get the new server in to Production given IT by myself for our company slowly happens in the afterhours of business; hence the question. 

Update: I decided to tax things minimally with a single I/O stream via a 600GB archive job reading from the array at about 30MB/s sustained while a tar was being written to the tape from a 4 drive RAID 6 with consumer SATA. The tape definitely slowed to a crawl via listening to the drive but did NOT seem to run out of data or shoe shine. This tells me to NOT expect things to keep up during a full scheduled backup for our hardware configuration but it can handle a less taxing I/O job wile writing to tape. As of note, the LOT4 tapes must do 56 end-to-end passes so effectively it writes in ~14GB chunks before it stops for some seconds to slow down and then "go" the other direction. I think this helped keep the drive "fed" with data under lower throughput as I have read ahead and async writes set in the stinit.def. Another note is a read of "dd if=/dev/st0 of=/dev/null" only produced a result of 107MB/s. This, I would assume, is the real-world max effective throughput of this the drive and NOT 120 MB/s. The drive is currently on a dedicated SAS PCIe HBA with no other PCIe cards installed In the meantime, I setup a 1TB RAID0 as a Disk2Tape buffer and had to add another disk to server to make this feasible. I would still love to find away to do some sort of QOS for the tape drive and set writing to tape top priority so we can simplify our arrays and reduce parasitic hardaware costs, but in the mean time, I'm not seeing a way to NOT get around having a dedicated disk2tape buffer if I want to ensure continuous writes no matter what scheduled jobs hit the array. 

If the source array has significant reads/writes (from scheduled backups) during a tape write, throughput to the tape would drop dramatically even if temporarily. So some questions centered around source array/tape write throughput: 

I have not found a way to delete a file from the archive, so if there is a large file I want gone I delete it in the directory structure and if someone tried to recover it for some reason I'm sure it would throw an error when recovering it. Although I only do this for known large temporary or duplicate files so this has not been an issue; namely from users using backed-up locations as a scratch space and the scratch work ends up getting backed up. The biggest danger is if your configuration files get corrupted or go missing you cannot rebuild them; but you still would have your files. Overall if you do not like vendor lock in or proprietary file formats, it is a great solution. I have no affiliation with them, they just provided something that solves a problem for me!