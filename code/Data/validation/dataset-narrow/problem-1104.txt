An upper bound on the running time of $A$ is $O(n)$, since the description of $A$ can just list the whole string $s$ and return $s_i$ on input $i$. The universal Turing machine on inputs $(A,i)$ simply reads the description of $A$ (Which is $O(n)$ in length) and outputs the $i^{th}$ coordinate (which takes time $\log n$) A lower bound on the running time of $A$ can be proved as follows. a. The description length of $A$ must be at least $n - log n - c$ for a constant $c > 0$. This is because the program $B$: "Run $A$ on every input from $1$ to $n$" has description length $DL(B) = DL(A) + log(n) + c$ and generates the string $s$. Thus $DL(B) \geq n$ and $DL(A) \geq n- \log(n) - c$. b. The universal Turing machine $U$ on input $(A,i)$ needs to read both $A$ and $i$, which requires at least $DL(A) + \log n \geq n-c$ operations. 

Does the Johnson-Lindenstrauss Lemma apply to any finite-dimensional Hilbert Space? In particular, I am interested in the space of random variables $X = (X_1,...,X_N)$ over $N$ uncertain states. If $\pi_i$ is the probability of state $i$, then this space has an inner product $\langle X, Y \rangle = E(XY) = \sum_{i=1}^n \pi_i X_i Y_i$ and a norm $\|X\|^2_{\pi} = \langle X, X \rangle = \sum_{i=1}^N \pi_i X_i^2$. The standard JL lemma says that, if $S$ is a set of $m$ points in $\mathbb{R}^n$ and $n > C \frac{ln(m)}{\varepsilon^2}$ then there a (suitably scaled) random orthogonal projection $f:\mathbb{R}^N \to \mathbb{R}^n$ will satisfy $$(1-\varepsilon)\|u-v\|^2_{N} \leq \|f(u) - f(v)\|_{n}^2 \leq (1+\varepsilon)\|u-v\|_{N}^2$$ where I have used $\|\cdot\|_{n},\|\cdot\|_{N}$ to denote the standard euclidean norms in $\mathbb{R}^n$ and $\mathbb{R}^N.$ Does there exist a version of the lemma with the weighted norm $\|X\|^2_{\pi}$? What would the corresponding norm in the lower $n$-dimensional space look like? 

One of my friends asks me the following scheduling problem on tree. I find it is very clean and interesting. Is there any reference for it? Problem: There is a tree $T(V,E)$, each edge has symmetric traveling cost of 1. For each vertex $v_i$, there is a task which needs to be done before its deadline $d_i$. The task is also denoted as $v_i$. Each task has the uniform value 1. The processing time is 0 for each task, i.e., visiting a task before its deadline equals finishing it. Without loss of generality, let $v_0$ denote the root and assuming there is no task located at $v_0$. There is a vehicle at $v_0$ at time 0. Besides, we assume that $d_i \ge dep_i$ for every vertex, $dep_i$ stands for the depth of $v_i$. This is self-evident, the vertex with deadline less than its depth should be taken as outlier. The problem asks to find a scheduling which finishes as many tasks as possible. Progress: 

Given two directed acyclic graphs $G_1$ and $G_2$, is it NP-Complete to find a one-to-one mapping $f:V(G_1) \rightarrow V(G_2)$ such that $(v_i,v_j) \in D(G_1)$ if and only if $(f(v_i),f(v_j)) \in D(G_2)$? $D(G)$ is defined as the set of arcs in $G$. 

Let $U$ be a universal Turing Machine. Suppose I have a Kolmogorov incompressible string $s$ of length $n$. Let $A:\{1,...,n\} \to \{0,1\}$ be an algorithm such that $A(i) = s_i$. I believe that the time it takes for the universal Turing machine to evaluate $A$ on input $i$ should be $\Theta(n)$. I'm not familiar with Kolmogorov complexity, and I wanted to ask if the following intuition is correct 

I'm looking into communication complexity with real numbers. One problem if we want to define this is that one can encode many real numbers $0.a_1a_2a_3... , 0.b_1b_2b_3..., 0.c_1c_2c_3...$ using only one number $0.a_1b_1c_1a_2b_2c_2...$ To get around this problem, existing papers that deal with this issue (such as Abelson (1978), Luo and Tsitiklis, and Chen(1994)) assume that the messages that can be sent between Alice and Bob must be continuously differentiable functions of the inputs $x_1,...,x_n$. Do we need differentiability? Is there any problem if the messages are assumed to be continuous (not necessarily differentiable) functions? I know that there's no continuous bijective function $f: \mathbb{R}^n \to \mathbb{R}$ for $n > 1$, so it seems like just assuming continuity should be enough. Thanks! 

Let $E_r=\{e_0, e_1, e_2\}$, $X_i$ be the indicator random variable that whether color $i$ is assigned to one edge from $E_r$. $X_i=1$ iff the i-th color is not assigned to the edges e_0, e_1, e_2. We calulate the expected number of colors from clolrs $\{1, 2\}$ which are not assinged to any of $\{e_0, e_1, e_2\}$. For the "first algorithm": $E[X_1+X_2] = Pr(X_1=1)+Pr(X_2=1) $ $=Pr(X_1=1)+Pr(X_2=1|X_1=1)\cdot Pr(X_1=1) + Pr(X_2=1|X_1=0)\cdot Pr(X_1=0)$ $=(\frac{2}{3})^3 + (\frac{1}{2})^3 \cdot (\frac{2}{3})^3 + (\frac{1}{2})^2 \cdot (1- (\frac{2}{3})^3 )$ $=\frac{1}{2}$ For the "second algorithm": We list all the $9!$ orderings, and get $E[X_1+X_2]=12/27=0.44444$. The follwoing is responce for @vzn, thanks very much. 

I am reading a paper called "Rational Proof". It mentioned the following one-to-one reduction. I cannot google an introduction of it. An excerpt from the paper. "Recall that a one-to-one reduction from a function $f$ to another function $g$ is a triple of polynomial time computable functions ($\alpha$, $\beta$, $\gamma$) such that: 

what is the worst behavior of greedy coloring on cograph coloring? Is it possible that the greedy coloring needs more than $\lceil \frac{1+\Delta}{2} \rceil$ colors? 

If the tree is restricted to a path, then it is in $\mathsf{P}$ via dynamic programming. If the tree is generalized to a graph, then it is in $\mathsf{NP}$-complete. I have a very simple greedy algorithm which is believed 3-factor apporoximation. I have not proved it completely. Rightnow, I am more interested about the NP-hard results. :-) 

Imagine I have a weighted directed graph $G = (V,E)$ with $n$ vertices and a weighted adjacency matrix $W$. Assume the indegree of vertex $i$ is bounded away from 1 , so that there exists a constant $\lambda$ such that $\sum_{j=1}^{n} W_{ji} < \lambda < 1$. I can compute the adjacency matrix of the transitive closure as $X = (I-W)^{-1} = I + W + W^2 + ...$ and compute the indegree of vertex $i$ in the transitive closure as $d_i = \sum_{j=1}^n X_{ji}$. I now sample the edges of $G$ to form a new graph $G' = (V,E')$ where each edge of $E$ is in $E'$ independently with probability $p > 0$. Let $X'(p)$ be the corresponding transitive closure adjacency matrix, and let $d_i'(p) = \sum_{j=1}^n X_{ji}'(p)$. I'm trying to show that $\lim_{p \to 1} d_i'(p) = d_i$ uniformly over all $i$ and all $n$. That is, for every $\epsilon > 0$, there exists a $\delta$ independent of $n$ and $i$, such that for all $p > 1- \delta$ we have $|d_i'(p) - d_i| < \epsilon$. I know this is true if we allow $\delta$ to depend on $n$, but I'm not sure where I would start thinking about this to prove the uniform limit. 

I am going to edge color an undirected simple graph. The following randomized offline algorithm is showed at Online Algorithms for Edge Coloring. Offline: The potential colors are ordered 1, 2, . . . , 2$\Delta$−1. The edges are ordered at random, and we greedily assign color 1 until a maximal matching is colored 1, then we start over with a random order of the remaining uncolored edges and we greedily assign color 2 until a maximal matching of the remaining edges is colored 2, and so on with the remaining edges and maximal matchings for colors 3, 4, . . . In paper Online Algorithms for Edge Coloring, the authors claimed the offline algorithm is the same as the online algorithm as follows. Online: The edges are ordered at random only once, and each edge in turn is colored with the least valid color out of 1, 2, . . . , 2$\Delta$ − 1. The "same" means, for a given graph. If the first algorithm needs $c$ colors with probability $p$, then the second algorithm uses needs $c$ colors with probability $p$. So there is not clear justification that the given two algorithms are same. The paper claims "For the second algorithm, if we consider sequentially when colors 1, 2, . . . are assigned, the same bound holds.". In fact, they turn out to be different (see the examle below). So my question is: can the offline algorithm be made onlne? I cannot handle the multi-times random ordering ( after each maximal matching is removed, the order of the remaining edges is shuffled). Example. 

This is a question about properties of large directed graphs which are preserved when we randomly sample edges. Imagine I have an infinite sequence of positively weighted directed graphs. The graphs are represented by adjacency matrices $$M_1,M_2,...,M_n,...$$ where each $M_n \in \mathbb{R}^{n \times n}$. The sup-norm $\|M_n\|_{\infty} = \max_{i} \sum_{j=1}^n |M_{n}(i,j)|$ is uniformly bounded by some bound $B$ for all $n \in \mathbb{N}$. Furthermore, for any $\ell < n $ we have $M_{\ell}(i,j) = M_n(i,j)$ as long as $i,j \leq \ell$. Many times we do not observe the full graph. Instead of observing $M_n$, we observe a random variable $\tilde{M_n}$ where $$\tilde{M_n}(i,j) = \begin{cases} M_n(i,j) & \text{ with probability } \pi \\ 0 & \text{ with probability } 1-\pi.\end{cases}$$ Let $\tilde{y_n} = f(\tilde{M_n})$ be a real-valued function of the matrix $\tilde{M_n}$. Using a Chernoff bound, I know that if $f(\tilde{M_{n}}) = \sum_{i,j} a_{ij} \tilde{M_n(i,j)}$ is a linear function, then the value of $f(\tilde{M_{n}})$ is tightly concentrated around its expectation, with the probability of it being ``far away'' form $E[f(\tilde{M_n})]$ decreasing exponentially with $n$. Is this true for non-linear properties? For example, if $f(\tilde{M_n})$ is a convex function, do we have a similar concentration bound? 

The chromatic number of graph, $\chi( G)$ is hard to approximate for general graphs. Are there results of hardness of approximating $\chi(G)$ for triangle-free graphs? 

In fact, the paper was not published as far as I know. I have contacted the first author, he have not respond it in detail. The key is that, it does not "fit" clearly. Why I only concern the first two colors in the example. If $x$ edges from $\{e_0, e_1, e_2\}$ are not assigned to colors in $\{1,2\}$, then the algorithms needs exactly $2+(3-x)=5-x$ colors. Because for the remaining $(3-x)$ edges incident to the root not colored, $(3-x)$ new colors have to be used. While the other edges will be colored in the third color definitely. Put another way, if the two algorithms were the same, all the characterizations of them should be the same? (For example, $E[X_1+X_2]$ here. Am I right? I am not sure.) 

For all $x$ in the domain of $f$ we have $y=f(x)$ if and only if $g(\alpha(x))=\beta(y)$ For all $x$ in the domain of $f$, let $w=\alpha(x)$. Then we have $g(w)=z$ if and only if $f(x)=\gamma(z)$. " 

This is not a homework, though it looks like. Any reference is welcome. :-) Scenario: There are $n$ different balls and $n$ different bins (labled from 1 to $n$, from left to right). Each ball is thrown independently and uniformly into bins. Let $f(i)$ be the number of balls in the $i$~th bin. Let $E_i$ denote the following event.