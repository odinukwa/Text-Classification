I found this simple C program that can be used to capture using different timestamping options for Linux sockets: $URL$ Invoking it with gives a output like . Unfortunately, I apparently don't have hardware available that is capable of hardware timestamps so I don't know for sure whether the test also succeeds for NICs that are hardstamp-capable. 

I want to capture packets using tcpdump and get as precise timestamps as possible. The latest version of tcpdump has the command-line option, which can be used for this. I tested the option inside a virtual machine (which definitely has no hardware timestamps ;-)), but there was no error message. Online research did not yield any keyword or phrase that can be used to check whether some card supports it or not. Typical NIC data sheets don't have anything in them which indicates timestamp precision or general support for such a thing. How can I check whether my actual NIC supports hardware timestamps? 

Basically, DNS is only a name system. So you have FQDN and they translate into something else. This could be for example an IP address (A record), a mailserver (MX record) or another thing. Often times, it is used in a way that subdomains provide hierarchy. So is something belonging to . But if you want, you can do this mapping in any way you want. There is especially no geographic information in domain names. Nevertheless, it makes sense to use the hierarchy and distribute your name servers in a geographically senseful manner. But you are not forced to do so and many organizations don't do so. Take for example content-delivery networks (CDNs), where they are resolving domains based on your location, so that you get the content from a geographically nearby location to reduce latency and load on the network. The actual DNS lookup is also done hierarchically: 

Up to this point, the domain hierarchy and the name servers are usually mapped 1:1, so one part of the domain relates to one name server. Starting from the domain, it can then have an arbitrary structure and is due to the organizations needs. So one server could be responsible for or there can be 3 servers responsible for this. 

Actually, in networking you never know if a packet got lost or not if you are only watching the end-points. This is due to the fact that you cannot distinguish between a lost packet (dropped by router or even corrupted on the link layer) and a extremely delayed transmission (packet is still on the wire or in a queue but is not processed). A TCP duplicate ACK indicates: You received an acknowledgement from the server but it gives the same sequence number that the last ACK had. This means that an intermediate packet is lost (as the receiver ACKed the number twice) or extremely delay. But as you have received another ACK (the duplicate ACK) this means that the network path is not completely congested. Consequently, if you get multiple ACKs in a row, you can assume that the ACKed packet is lost as the other packets come through. Many TCP implementations then do what is called a Fast Retransmit, so that they don't wait for the retransmit timeout but instead resend the first unacked packet. This also has some implications regarding the congestion window. The message "Expert Info (Warn/Sequence): Previous segment not captured (common at capture start)" means the following: On the receiver side you capture an outgoing ACK packet for a sequence number where you haven't seen the respective segment. This is common, as it might be possible that a segment arrived, you started the capture and afterwards your TCP stack replied with an ACK. So there was no way to see the incoming packet. This does not necessarily indicate a loss. 

This is true of oldschool ethernet implementations (10base2, 10base5 et al ... as well as 10baseT and 100baseT IF a non-switching hub was used) that actually used a physically or logically shared medium (cabling plant) to connect more than two transmission capable ports. If two stations accidentally transmitted at the same time, the signal on that shared medium became garbled - network ports were designed to detect such garbled signals and send yet another signal onto the medium, that could still be read correctly over the garbled data and signalled everyone that all their transmissions are invalid and will have to be repeated after a backoff time. The problem with these setups was that if anything loaded the network to its limit with small packets, the achievable bandwidth ended up significantly below the theoretical maximum, since bandwidth got wasted resolving collisions instead of transmitting data. In a fully-switched Ethernet (as you would get when building it with any parts commonly available in a store after ca. 2004. "Fully switched" because hybrid forms exist(ed) where mutiple shared media (with more than 2 ports) were separated by switches), there are always two ports (one is on the switch, the other on a computer or other connected device) sharing a medium. Any "T" type ethernet has separate wire pairs for each direction, so between two ports it will always be possible for both of them to transmit and receive at the same time. A switch will always order packets from multiple senders into one valid sequence and transmit it wherever it decides it needs to send them, unless it is actually overloaded (which is unlikely to occur in a home network situation). For completeness, certain other (nowadays rarely used) non-Ethernet wired LAN systems (eg Token Ring, FDDI) used different methods to discipline medium access, in some cases positively negotiating the privilege to send before anyone got to transmit a payload. WIFI also has to deal with a shared medium problem, and also the fact that two conflicting transmitters might have different visibility to different receivers. An algorithm called CSMA/CA is used to resolve this. 

In practice, existing Cat.5e cabling can, on some or all cabling runs, INCIDENTALLY either pass Cat.6 specifications or at least surpass minimal Cat.5e specifications enough to behave within the tolerance margins of 10GBase-T. The practical limits are dependent on things like when the cable was made - older machinery to make the cable might have been built to only just meet the accuracy needed for Cat.5e, while newer cable might have been produced on the same machinery as the Cat.6a cable and merely labelled differently, either because there was demand for Cat.5e or because cable that marginally fails Cat.6a but meets Cat.5e specs was manufactured due to machine/operator wear/error/maladjustment/contamination... Also, if a given run works or not can depend on the hardware at both ends - one transceiver design (in a NIC or switch) might be able to just flawlessly work with a marginal cable while another will not. Small factors like materials aging, someone bending or rolling up the cable, temperature and humidity, someone placing metallic objects too near it ... could be sufficient to turn a working cable run into a non working one suddenly. Also, the cable could work but create unacceptable EMI problems. 10GBase-T on Cat.5e seems actually common practice - though usually not at maximum length.