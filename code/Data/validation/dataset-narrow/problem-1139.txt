Alon and Naor used Grothendieck's inequality to prove an approximation algorithm on the max-cut problem. I think that there are subsequent works on that topic but I'm not an expert. Interestingly, the same theorem was used by Cleve, Hoyer, Toner and Watrous to analyze quantum XOR games, and Linial and Shraibman used it for quantum communication complexity. Up to my knowledge, the relation between Grothendieck's inequality and the foundation of quantum physics was discovered by Tsirelson in 85, but the two results I mentioned specifically address computer science. 

In the paper Negative weights make adversaries stronger, Lee, Spalek, and Hoyer give a function for which $ADV^{±}(f)= \Omega( ({C_0(f)C_1(f)})^{0.549})$, where $ADV^±$ denotes the general adversary lower bound. This lower bound has since been proved to characterize quantum query complexity. This function is obtained by composing a function on a few bits bits with itself $d$ times. You get in the end $C_0(f)=C_1(f)=3^d$ and thus $C(f) \in o(Q_2(f))$. 

Whenever $f(S)$ counts the number of edges $(u,v)$ satisfying some Boolean predicate defined in terms of $u\in S$ and $v\in S$, then what you wrote is just a Boolean 2-CSP. The objective function asks to maximize the number of satisfied clauses over all assignments to the variables. This is known to be NP-hard and the exact hardness threshold is also known assuming UGC (see Raghavendra'08). There are many natural positive examples when you want to maximize over subsets of edges, e.g, Maximum matching is one example of a polynomial time problem in this case. 

You are right in that the integrality gap of a relaxation has as such nothing to do with any rounding algorithm. These are two different notions. An integrality gap is a property of a particular relaxation. That is, how much larger is the value of that relaxation compared to the optimal integral value? Why do we care about linear/convex relaxations? To efficiently approximate an integral value. Hence, we typicaly talk about relaxations only in cases where the optimal value is hard to compute and we're interested in efficient approximations. Integrality gaps show us the inherent limitations of what can be achieved by such techniques. So, why do we care about rounding algorithms on top of the relaxation? We use rounding algorithms to solve the algorithmic problem of finding a near optimal solution as opposed to just approximating the value of an optimal solution. Moreover, often rounding algorithms are used to bound the integrality gap of a relaxation in the first place. 

As you may know, the problem of taking a weak shared key, about which the adversary knows some information, and extracting from it a key about which the adversary has essentially no information, is often referred to as privacy amplification. Once this is done, we can use the new strong key for authentication, encryption, etc. If interaction is allowed, Alice and Bob can extract a strong key over an inauthentic channel very efficiently, in particular there is no requirement that the min-entropy rate be at least 1/2. See "Privacy Ampliﬁcation with Asymptotically Optimal Entropy Loss" by Chandran, Kanukurthi, Ostrovsky, Reyzin, and references therein. Their introduction mentions that non-interactive solutions (one message from Alice to Bob) exist as well (references 5, 10, 15), but these do require min-entropy rate 1/2. Depending on the application, another possibility for a one-message solution would be to use deterministic (aka seedless) extractors, which work only for certain classes of sources with some restricted structure. For instance, the adversary knows some fraction of the key's bits, but the remainder are uniformly random to him. See for example Kamp and Zuckerman, "Deterministic Extractors for Bit-Fixing Sources and Exposure-Resilient Cryptography" or Kamp, Rao, Vadhan, Zuckerman, "Deterministic Extractors for Small-Space Sources" 

In general, if you have a probabilistic Turing machine $P$ solving some decision problem, you can always simulate it deterministically by running $P$ for every possible value of the randomness and outputting the majority answer of $P$. Therefore, no probabilistic Turing machine can solve an undecidable decision problem. I think this was the meaning of Raphael's comment. 

The incompressibility method is a method based on Kolmogorov complexity to prove lower bounds. One of the first application of this method was to prove that recognizing palindromes on a Turing machine with a single tape requires quadratic time. Loosely speaking, the idea of this method is to describe a procedure to find an input using the information contained in the run of an algorithm solving the problem we consider on this input. The better is the procedure, the higher is the lower bound on the original problem. Of course, full details can be found in the textbook of Li and Vitanyi. 

There is basically an entire course devoted to this question: $URL$ The course is still ongoing. So not all notes are available as of writing this. Also, some examples from the course were already mentioned. 

The standard reference for such a positive result is Piotr Indyk's paper on stable distributions: $URL$ He shows a dimension reduction technique for $\ell_1$ where the distance between any pair of points does not increase (by more than factor $1+\epsilon$) with constant probability and distances do not decrease (by more than factor $1-\epsilon$) with high probability. The dimension of the embedding will be exponential in $1/\epsilon$. There are probably follow up works that I'm not aware of. 

This is a nice question. I don't know of approximation algorithms, but the known hardness results for approximating minimum distortion (and related problems such as metric labeling) should also show that $\epsilon^2$ is hard to approximate. The reason is that they give a reduction from an NP-hard problem such that in the YES case the distortion is $O(1)$ and in the NO case the distortion is $\Omega(k)$ for at least a constant fraction of the edges. Hence, in the YES case $\epsilon^2$ will be a factor $k$ smaller than in the NO case. For details, see for instance the paper by Khot-Saket: www.cs.cmu.edu/~rsaket/pubs/approx.pdf I'm not exactly sure which hardness factor follows from their paper, but it shouldn't be difficult to figure out. (I would guess at least the $\log^c(n)$ factor that you get for metric labeling should follow.) 

Your question suggests the following tentative reduction to obtain a OWF from a secure KE: Given an input, interpret it as the private random coins of two simulated parties to the KE protocol. Based on their private randomness, there will be some sequence of public messages between them, and at some point they will stop and agree on a private shared key (with some high probability). Then the public transcript of their communications is the output of the OWF. You are right to be worried about this approach in the case when Alice and Bob may not agree with probability 1. In fact, this construction is NOT necessarily a OWF in that case. Rather, it is a weaker primitive known as a "distributionally one-way function," which is, informally, a function for which it is hard to generate a uniformly random preimage of $f(x)$, when $x$ is selected at random. It is known (but non-obvious) that one can construct a true one-way function using a distributionally one-way function. See e.g. Exercise 17 on page 96 of Goldreich "Foundations of Cryptography: Basic Tools," which is available for limited viewing on Google preview. To directly see why the above reduction does not suffice, we can use the common "make a stupid modification" technique. Namely, suppose there exists a secure KE protocol where Alice and Bob agree with probability $1 - \epsilon$. Consider a stupidly-modified KE protocol that works as follows: Alice and Bob first each flip 100 coins. For each of Alice, Bob individually: If all 100 coins are zero, their remaining randomness is interpreted as encoding a sequence of messages which they send to the other party, ignoring whatever the other one says; then they just output a random "shared key." (Otherwise they follow the original protocol.) Given any public transcript, it is possible (with negligible probability) that it was generated by an Alice and Bob who BOTH flipped 100 zeroes and happened to have their remaining randomness specify that exact sequence of messages. This is still a secure KE protocol, where Alice and Bob will agree with probability at least $1 - \epsilon - 2^{-100}$. On the other hand, it utterly fails to be a OWF in the hoped-for manner, since an attacker who sees some public transcript can always trivially invert by choosing 100-zeroes randomness for each of Alice and Bob.