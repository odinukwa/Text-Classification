This is the method available in the control panel for selecting what layout ClearType uses. Additionally, it seems that iOS and the Windows modern UI style de-emphasize subpixel antialiasing heavily, due to the prevalence of animations and screen rotations. As a result I expect the OS vendors to not spend a lot of effort trying to figure out the subpixel layout of every screen. 

Yes, lookup textures are still used. For example, pre-integrated BRDFs (for ambient lighting, say), or arbitrarily complicated curves baked down to a 1D texture, or a 3D lookup texture for color grading, or a noise texture instead of a PRNG in the shader. ALU is generally cheaper than a texture sample, true, but you still have a limited amount of ALU per frame. GPUs are good at latency hiding and small lookup textures are likely to be in the cache. If your function is complicated enough, it may still be worth using a lookup texture. 

Here's a chromaticity diagram that includes a projection of the sRGB color space: a triangle whose vertices are red (1,0,0), green (0,1,0), and blue (0,0,1). 

Forward rendering is the process of computing a radiance value for a surface fragment directly from input geometry and lighting information. Deferred rendering splits that process into two steps: first producing a screen-space buffer containing material properties (a geometry buffer, or G-buffer) built by rasterizing the input geometry, and second producing a radiance value for each pixel by combining the G-buffer with lighting information. Deferred rendering is often presented as an optimization of forward rendering. One explanation is that lighting is fairly expensive and if you have any overdraw then you are lighting pixels that will never be seen on screen, whereas if you store material properties into a G-buffer and light afterwards, you are only lighting a pixel that will actually appear on-screen. Is this actually an advantage of deferred, given that you can also do a depth pre-pass and then do a forward rendering pass with depth test set to or or the equivalent? Deferred rendering also has the potential to schedule better on the GPU. Splitting one large warp/wavefront into a smaller geometry wavefront and then smaller lighting wavefronts later improves occupancy (more wavefronts in flight simultaneously). But you also end up with a lot more bandwidth use (writing out a large number of channels to the G-buffer, then reading them back in during lighting). Obviously the specifics here depend a lot on your GPU, but what are the general principles? Are there other practical performance considerations when deciding between forward and deferred rendering? (Assume that we can use variations of each technique if necessary: i.e. we can compare tiled forward to tiled deferred as well.) 

I would like to blur the content of those 4 spheres using the same offset no mater their position. If I apply the same blur on all objects, far object's content will appear more blurry than ones on the foreground and I want to avoid that. I think that the depth make could help but any precision would help me. If I blur the whole image and apply the result on the sphere, the white background will bleed onto the sphere shape and I want to avoid that. I also don't want that the blue (3) and yellow (4) sphere merges with the red (1) and green (2) ones. But I would like that the green and red ones merges. Again This could be done using the depth but if you have more precision about how to do it it would be interesting. 

I think that I got a solution but I would gladly know if there are some optimizations possible. My UVs and local coordinates values are corresponding. I mean that they are in the same range value. That said, I can use my XY vertices values for sampling the occlusion texture. The main problem is that the local coordinates are dependent to the rotation of my object which doesn't solve anything... To solve that I am converting my local coordinates into world space using a objectToWorld matrix (inverse of current world matrix). Then I am converting it back into object space using the inverse TRS matrix but without taking into account the rotation of the object. It's a bit hacky and I think that I could avoid some steps but this is working. Any advice is welcome :) 

You can simply avoid the 3rd dimension if you don't need it. This method comes from the Distance function article from Inigo Quilez. 

This is integrated with the Physically based Refraction and it gives interesting results but I still don't know how to get rid of the high parallax scale issue which appear at grazing angles... Any idea how to get rig of that ? 

I am trying to scale and repeat a Cubemap with Latitude-Longitude mapping layout just as you would do with classical UV mapping but without any interesting result. This should be used as a skybox. This comes from the fact that the coordinates are in 3D space and we can't apply this simple formula How would you handle such features : scaling which involves tiling and offsetting. 

This is clearly wrong. All six shadow maps seem kind of stretched. Without the stretching, the curtain's shadow would still be in the close vicinity of the curtain itself and the leaves' shadow would be much smaller (equal to the leaves' shadow for the spotlight). Furthermore, the circular shadow at the end of the pillar's shadow is associated to the buckets in front of the curtains. Any ideas what goes or could go wrong? For clarity, the following two images show the shadow factor of the cube map face corresponding to the spotlight, for the omni light by adding: 

A Transform structure can now be allocated on the heap with 16 byte alignment, since our custom will now be invoked: 

For completeness (and in addition to Nathan Reed's answer), I explicitly add the inverse projection matrices for perspective and orthographic cameras. Perspective Camera $$\begin{align} \mathrm{T}_{\mathrm{view \rightarrow projection}} &= \begin{bmatrix} \mathrm{T}_{00} &0 &0 &0 \\ 0 &\mathrm{T}_{11} &0 &0 \\ 0 &0 &\mathrm{T}_{22} &1 \\ 0 &0 &\mathrm{T}_{32} &0 \end{bmatrix} \! , \\ \mathrm{T}_{\mathrm{projection \rightarrow view}} &=\begin{bmatrix} 1/\mathrm{T}_{00} &0 &0 &0 \\ 0 &1/\mathrm{T}_{11} &0 &0 \\ 0 &0 &0 &1/\mathrm{T}_{32} \\ 0 &0 &1 &-\mathrm{T}_{22}/\mathrm{T}_{32}\end{bmatrix} \! . \end{align}$$ 

Here, the hit position in light projection space coordinates is calculated as follows from the hit position in camera view space coordinates (shading space): 

Even if I use a value equal to 100000.0, I'll notice lit areas? Depth Biasing I use (to prevent shadow acne; note that I use 16bit depth maps) for all my Rasterizer states. PCF filtering I use the following sampler comparison state for PCF filtering (my shadow maps have no mipmaps) for both spotlights and omni lights. 

In the SIGGRAPH course: BURLEY B.: Physically Based Shading at Disney, SIGGRAPH 2012 Course: Practical Physically Based Shading in Film and Game Production, 2012. it is mentioned that some BRDF models include a diffuse Fresnel factor such as: $$(1-F(\theta_l)) (1-F(\theta_d)).$$ The Disney BRDF itself uses the following diffuse BRDF component (using Sclick's Fresnel approximation): $$f_d = \frac{\textrm{c_base}}{\pi} (1 + (F_{\textrm{D90}} - 1)(1-\cos\theta_l)^5) (1 + (F_{\textrm{D90}} - 1)(1-\cos\theta_v)^5),$$ where $$F_{\textrm{D90}} = 0.5 + 2 \text{roughness} \cos^2\theta_d. $$ Where does this come from? My attempt... If I evaluate $(1-F(\theta_l)) (1-F(\theta_v))$ (instead of $(1-F(\theta_l)) (1-F(\theta_d))$?) with Schlick's approximation, we get: $$\left(1-(F_0 + (1-F_0)(1-\cos\theta_l)^5)\right) \left(1-(F_0 + (1-F_0)(1-\cos\theta_v)^5)\right)$$ $$\left(1-F_0 + (F_0-1)(1-\cos\theta_l)^5\right) \left(1-F_0 + (F_0-1)(1-\cos\theta_v)^5\right)$$ If we substitute $F_{\textrm{D90}} = F_0$, we get: $$\left(1-F_{\textrm{D90}} + (F_{\textrm{D90}}-1)(1-\cos\theta_l)^5\right) \left(1-F_{\textrm{D90}} + (F_{\textrm{D90}}-1)(1-\cos\theta_v)^5\right)$$ This looks similar except for the 2x $-F_{\textrm{D90}}$? Is my reasoning completely wrong or where do I make mistakes? Or am I not aware of some further (common) approximations? 

The displayed region of clip space (that you can think of rasterization as happening in) is a box which has a "near" face. This is associated with the near clip plane. But we can push the far clip plane out to infinity, so can we pull the near clip plane in to zero? Mathematically, you absolutely can! The only point that presents a problem is the eye itself: you can get infinitely close to the eye and still resolve to a single point in clip space. However, if you have a triangle that passes through the eye, you still need to clip it, and to clip it you need an actual plane, not a concept like "infinitely close to the eye." More critically for the current rasterization pipeline, the perspective divide does fun things with numerical precision. See the line at NDC 0 in the diagram above? It's halfway through the viewing volume in clip space, but not at all near halfway through the view frustum in camera space. In fact, its position depends on the near clip plane's distance to the eye! This means that over half of the depth range in clip space is in that region near the eye. As you get the near clip plane closer and closer to the eye, that plane pulls in even closer and you waste more and more depth range. This isn't necessarily fundamental—you don't have to store z/w in the depth buffer—but it is convenient. Here's an article from Nathan Reed about depth precision, and a few notes about why the projected z/w depth is useful from Emil Persson. Finally, I'll plug a post in the excellent "A trip through the Graphics Pipeline" series, which is very interesting and references homogeneous rasterization algorithms which, in theory, could avoid the need to do clipping in projected clip space, which would totally avoid some of these traps. 

You can see that the last two images look sharper, but also have the characteristic color fringing. Here's the downsampling pattern used for the last image: 

Encoding the reflectance of a surface as the color at F0 and getting a value that is outside of the (somewhat arbitrary chosen) sRGB gamut is totally reasonable. It just means that gold is "more red" than sRGB can represent, because it reserves valuable area of its dynamic range for other colors. 

I obtain the following images after visualizing the shadow factor (this is not the light contribution of the omni light) (and some linear fog as well): 

They all seem to be used (in the literature, in the animation industry and in the gaming industry) in the format corresponding to your second option. All the D factors in my enumeration contain an explicit $\frac{1}{\pi \alpha^2} $ with $\alpha \equiv \text{roughness}^2$ (See Equations). Edit 2: A recent presentation deriving and explaining the division by $4$ instead of $\pi$: Earl Hammon: PBR Diffuse Lighting for GGX+Smith Microsurfaces, GDC 2017. To make a long story shorter, option 2 is the only correct specular term (of the three options provided). 

No MakeFile or Visual Studio Solution file is provided. So you will need to do the setup yourself in the Visual Studio IDE. Note that you could just drag and drop the files (.h and .cc are known extensions to Visual Studio) in a new console application if you are not/less familiar with Visual Studio. The code itself is just plain C++ code. No special stuff. So it does not matter which platform or IDE was used during development. In fact the code is pretty self containing except for 

In general, artists like working with a linear roughness value between 0 and 1 (similarly for all other material parameters), since this is easier to work with and to understand compared to directly using the parameters of certain BRDF components as presented in the literature. Disney for instance always uses linear material parameters for their Disney BRDF in the range [0,1] from the perspective of the artists (see their course notes on page 18). Working with linear values in the range [0,1] also simplifies storing and loading these values in RGB or sRGB textures. The actual roughness used in the BRDF equations is non-linear. So one needs to map linear to non-linear roughness in some computationally cheap way that pleases the artists. The most important thing is to be consistent across your renderer and to explicitly specify when a roughness parameter is linear or non-linear. It is worth reading Moving Frostbite to Physically Based Rendering 3.0. These course notes explicitly use the terminology linear roughness and (non-linear) roughness, both in the text and code samples. Furthermore, it is also worth reading The Specular BRDF Reference which defines various BRDF components for the Cook-Torrance BRDF using the same non-linear roughness parameter $\alpha$ (defined as the square of the linear roughness $roughness$). 

I am trying to do simple PCF with Unity but I am facing some issues and I don't know where they come from. If anybody has an idea... Here are two examples 

It seems that the issue comes from the scaling part of the radius and the smoothness and the length call. But I don't see any way to fix it. If you also have some suggestions about some ways to improve the code I would be glad to hear you. Thanks ! 

As I said it works well but when I scale my shape in the X axis the result is not correct. Here is a illustration of the issue. 

I am currently trying to implement a specific directional light type. This light type has been used in the game INSIDE and is called orthogonal spotlight (aka local directional light). I assume that this is a directional light which behaves like a spot light and have a squared or rectangular attenuation but I have some difficulties to integrate it in my deferred pipeline and get the general concept of this light type. Classical directional light : 

I have some classic texture coordinates and as a normal behaviour they follow the mesh's transformations. I am trying to use the same texture coordinates behaviour but without being affected by the mesh rotation transformation. The results would be a sort of texture coordinates projection. I don't know if my explanations are well explained but how could I achieve such effect. Thanks a lot. 

As I am working with Unity, I found an interesting way of calculating the parallax offset inside the engine. It's not physically based but it gives better results than the classic parallax refraction. 

I discovered that some engines use derivative maps instead of tangent space normal maps. After some reading, it seems to be a really awesome way to replace tangent space normals but are there some disadvantage using them ? Why still continue using tangent space normals ? Is it possible to compare both with advantage and disadvantage ? 

I am trying to achieve a special texture stretching effect in my shader. Here is how I define my uv before sampling my texture, nothing really special.