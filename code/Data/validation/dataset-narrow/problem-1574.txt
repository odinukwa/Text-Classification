Therefore how it should increase is totally arbitrary too. I think the best answer to your question is that you need to think about the behavior you want to see out of your Y variable, and then write an algorithm that exhibits that behavior. Do you want the value to suddenly explode as you reach the crisis event? use an exponential. Do you want to gradually approach larger values? Use a linear model. Do you want a function that gets larger until you reach the crisis event and then slowly dissipates? Use a gaussian. 

Also tried changing loss functions, removing pooling layers, etc. I know it is weird. I've never come across anything quite like this before. 

It shouldn't matter if you average all of them or take the average of the average for each person. It turns out that it will give you the same number. This is known as the law of total expectation. Essentially, E(X)=E(E(X|Y)) Here's the wikipedia article for more details: $URL$ 

I think that the way to fix your problem is to use something like SMOTE or one of its variants. (My favorite is SMOTE-ENN, but go with what works the best). Here is an implementation that you can use to fix the class imbalances. I don't know if this will solve your specific problem, but it is worth a shot. 

Caveat: I have a doctorate in economics and that is why I knew how, and where, and when to apply this type of model. Sure, I used a vecm model last year to figure out how many credit cards get compromised per month given how much activity we see. (I work for a financial institution). We were looking for a long-run relationship and a short-run relationship. From this model, a survival analysis, and a couple of other techniques we were able to determine the optimal time to set the expiration date on a card. I applied this technique because I was afraid I was getting the wrong answer, namely because the number of transactions and number of fraud transactions were cointegrated.(All of the models I tried gave similar results which was comforting). Given that we are talking about fraud strategies I don't feel comfortable giving more details than this, hopefully, that gives you a flavor of what I was trying to accomplish. Basically, I was trying to do a robustness check. It isn't easily applied in a business context because this model isn't intuitive, it isn't easily explained, and it takes great skill to interpret it. The "mathematical" answer probably has something to go like this, ask an average data scientist what the impulse-response function for their model looks like and you will get a blank stare. Ask them what the FEVD looks like for their model, and you will get a blank stare. Data scientists tend to be trained in non-linear models (read machine learning), not cointegrated time-series models. It isn't that they aren't capable, they've just never been trained to think in those terms. I know technically not math, and technically that is an opinion but it is probably the truth. Also doing the math, I can probably get a better fit out of a non-linear model like an LSTM network(?). So if I only care about pure prediction, it wouldn't be worth my time. Where VAR and VECM shine is that although complicated, they are essentially linear, and therefore, fairly interpretable. So perhaps what you are looking for is a business setting that requires causally valid, interpretable, multivariate time-series models. If you look hard enough, I'm sure you can come up with a couple of interelated KPIs being tracked in your business where it would make sense. But in my experience, if you need a fast dirty answer, forgive me econometrics teachers everywhere, you can just plug the differenced series into a linear regression. (Warning not best practices, but it does alright). 

LDA is a bayesian model. The equation that you gave is the posterior distribution of the model. The alpha and beta parameters come from the fact that the dirichlet distribution, (a generalization of the beta distribution) takes these as parameters in the prior distribution. So to answer your first question, will the formula above work without the alpha and gamma, yes, you would implicitly be assuming these parameters from the prior distribution are both equal to zero. You will still get a result, but the prior you are using might be problematic on a conceptual level (maybe it would cause some numerical problems too, I'm not sure?) To answer question 2, I want to point out that in LDA we use the conjugate prior to simplify the calculation of the optimal parameter values. That is what you are seeing in the above equation. What you are seeing is the posterior value is proportional to the left hand side. Adding the "c" terms allows you to calculate the posterior. 

So I was training a fairly shallow convnet, because my deepnet based on vgg19 wasn't working. 2 conv layers and 2 flat layers, the second flat layer was the output. It converged quickly to all zeros in the second conv layer. First conv layer wasn't all zeros. First flat layer appears to have learned the ditribution of classes. So it appears that my network's strategy os to ignore the inputs and just predict the class distribution. I tried class weights, same result just a more uniform distribution gets predicted. I changed the learning rate, optimizer, and even tried gradient clipping. I augmented the data, and introduced regularization on the data, and network via dropout layers. No luck. Same result. Why would my network exhibit this behavior and what can be done about it? EDIT: Here is a code snippet that defines the network that I am using: 

It seems like you have a classical bayesian problem. You have some sort of prior distribution, a distribution over years of birth, your prior distribution is bimodal with peaks at the two years, you can probably use a convolution of two normal distributions to model this variable. Then have it spit out a posterior distribution after you feed in some data. The real problem that I have with this analysis is it seems your features aren't particularly good. It is true these vars might have information about birth year, for example for the 20th century the average age of first marriage has steadily been increasing. But I suspect that the signal is going to be fairly weak. Essentially, if I tell you that I got married at age 24, had my first child at 26, and that my older brother is 3 years older than me and my younger sister is 2 years younger than me, can you tell me in what year was I born, 1956 or 1989? I suspect that without additional data this information that I provided would be completely useless, mostly because it is a very noisy signal. That information could apply equally to someone born in 1956 or 1989. It isn't very helpful. Essentially, what I am saying is that when you update your prior, it isn't going to change very much. (Your posterior distribution would look very similar to the prior distribution.) Instead of doing some mustache twirling over what is the right algorithm to crack this problem, I think a much more fruitful exercise would be to think up some better features. 

I'm putting together some tutorials around data wrangling. Maybe my jupyter notebook on github will help. I think that it is the key is modifying the line: 

I believe that the algorithm that you want to use is something called a latent dirichlet allocation (LDA) model. This model is designed to uncover the topics in a corpus of documents. Scikit learn has an implementation. They even have a tutorial which teaches you how to extract topics. The tutorial also describes Non-negative Matrix Factorization (NNMF) as a method to extract the topics. I can't vouch for this algorithm, because I haven't used it personally (as opposed to LDA which I have used before), but from their tutorial NNMF does seem to give reasonable results. Using cosine similarity will help you to group tweets that are most similar, but it wouldn't give you their topics. Which may be what you want? It really is hard to say, because only you would know how you should have the system behave. Unfortunately, that doesn't help you figure out what is trending, and you will need to do some heavy post-processing to make whatever algorithm you use spit out something that is useful to you. Good luck! 

Autoencoder Solution You could try an autoencoder. The autoencoder would take an input vector and it would try to recreate it as an output. So you take your input, and measure the distance between the input and the predicted output variable, using your metric of choice (euclidean should work but try various). Larger distances can be thought of as more abnormal. So you can stack rank your observations from weirdest to most normal. Make sure that you are only training the autoencoder on normal data though. This would of course assume that you have more than the 13 samples that you are looking at. If not this probably isn't going to work very well, just because of the small sample. KDE Solution The idea is to use Kernel Density Estimation to generate a non-parametric joint density of your data set. You then find what the probability of finding a value that extreme would be. Here is some code using python's sklearn package: 

Your model actually looks pretty good. What it sounds like you are asking to do is to overfit your model. I would not recommend that you do that. You can do that by finding more variables that you can input into the model, fitting extra polynomial terms or anything else like that, fitting a neural network will potentially do it for you too. However, you generally want to smooth your predictions out, like what you have. One thing that you could try is to add an autocorrelation term. That might cause your model to behave as you intend. With negative autocorrelation your predicted values will have a tendency to bounce back and forth around the mean. But I wouldn't recommend doing that, your performance will probably suffer, just judging by the graph that you provided. 

In principle no you do not need to check for stationarity nor correct for it when you are using an LSTM. The thing about stationarity is that it makes prediction tasks much more efficient, and stable. Think about stationarity in terms of a target. When what you are trying to predict is not stationary, it is like trying to shoot a moving target. It isn't impossible to do, but there is a better way to do things. If you are trying to predict a stationary variable, then what you are trying to do is hit a non-moving, for lack of a better word, stationary target. It is staying in the same spot relative to you. Correcting for non-stationarities in an ARIMA model is necessary because it is one of the fundamental assumptions that you make about that particular model. LSTMs do not make that assumption. So you don't have to do it with an LSTM. Seems like the answer to your question is the same as what the people you asked originally. Case closed, right? Well... you remember shooting at moving targets is much more difficult than shooting at a stationary target right? You may want to force stationarity on a problem that you are going to run against an LSTM, simply because you may be able to eek out a little bit more performance by making it easier for the neural network to learn. So although you don't need to do it, it may still be a good idea and give you a boost in performance. 

I suggest taking a look at this page for some more ideas: Feature Selection That being said a couple of ideas that come to mind quickly, is to: 

I think that you want this: K-fold If you want say MSE of each check out section 3.1.1 here: cross validated metrics 

I think your confusion is that you only have two stores, but you might have more than 2 data points being generated from those two stores. For example, you might be interested in how much individual customers spend at the two stores. You can look at receipt totals from the stores. Then you would just run an independent t-test on the receipt totals data, because you have essentially two groups. Group A comes from store A's Reciepts and Group B comes from store B's receipts. Using this methodology you could look at any number of response variables. 

This code shows that the observations in the lowest probability density areas are observations 1,2 and 7. Of course this would work better with a larger sample, and you need to fuss with the bandwidth to calibrate it, but that this should do it. 

use a tree based method (like Random Forest) and look at your feature importances. Scikit Learn has a handy class for doing just that see the link above. Use some sort of regularization/penalty like L1 or L2 regularization. That will force non-useful features to have parameters close to zero. Recursively remove variables and see what the resulting output is and cross-validate. Again sklearn has a method for this. 

And here is example output from the first layer for comparison, which I generated with similar code: 

If you have time series for good clients and time series for bad clients, I would suggest using something along the lines of dynamic time warping to get out of a time domain, and into a dissimilarity domain. From there you could just use just about any classifier that you can think of, I have a silly sort of toy example that detects whether or not it is a superbowl sunday based on traffic to a website. $URL$ But the nice thing is that now that you are in dissimilarity space you can classify based on a few exemplars, say two or three columns in dissimilarity matrix, and feed that into your classifier. It should be relatively easy to implement for your particular use case.