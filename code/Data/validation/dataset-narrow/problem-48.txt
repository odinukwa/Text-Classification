In a webgl pixel shader, all functions are inlined as i understand it, however you can have parameters that are marked as in versus being inout meaning that their value can change but the value won't persist outside of the function call. Does this mean that the shader must make a copy of the value for the function to work with when it is an in value? Are shader compilers/optimizers smart enough to know when they don't need to make a copy, or is it best to really just mark up all parameters as inout and make sure and not modify the ones you don't want modified, if performance is the primary concern? Thanks! 

Elaboration on temporal solve: I don't have much concrete info for you, but I'm drawing from the idea of "temporal anti aliasing". Basically, if a camera was stationary, you could average pixel values over the last N frames, possibly using harmonic mean or something else like that to help filter out spikes. The result would be a cleaner, less noisy, more correct image. But not all cameras (or objects!) are stationary, so what then? Well, if you have some way of identifying where a pixel this frame matches a pixel on the previous N frames, you could average them in the same way. If a current pixel has no matching previous pixel (due to something previously occluding becoming visible) you just show the raw current value. Games use this for antialiasing by simulating super sampling over time, but they have the benefit of per pixel motion vectors as well as the current and previous camera matrix, so it's a lot harder in your situation! 

And here is a poisson disc sampling, using Mitchel's best candidate algorithm: More information, including the source code that generated these images can be found here: $URL$ 

The magic is that the mesh is attached to the skeleton. In it's simplest form, this is done by assigning each vertex to a bone. When a vertex is assigned to a bone, that means that it will always keep the same position relative to that bone's position, and orientation (normal, tangent, bitangent aka the bone's local X,Y,Z axis) as the bone moves as directed by the animation data, physics impulses, or whatever else driving it. As you might expect, some triangles will have vertices that belong to different bones. When this happens, since the triangle is made up of those vertices, the triangle will stretch or squish as the bones move. The link you mention explains a way to set up the vertices for this specific type of movement such that the vertices stretch and squish in realistic and well behaved ways. In modern games, vertices are usually attached to multiple bones with a weight per bone so that each vertex is actually affected by the movement of multiple bones. This gives a more natural and organic look to the mesh. This answer only scratches the surface though, and this is a very deep topic which has many sub topics that are active areas of research! If you want to read more about skeletal animation, this link is a good starting point: $URL$ 

Strangely, I never get that "nice concentric rings" DFT frequency amplitude that i'd expect and see in papers regarding blue noise sample patterns. Are these results expected? Manually hunting between 100 and 1000 samples, it seems like 400 is pretty near optimal. Is there some "sweet spot" for the number of candidates to use based on image size and desired sample (black pixel) count to get the best result? 

You say that you don't need the fastest possible algorithm, so instead of solving it analytically, you might try considering the numeric alternatives, which in this case could mean ray marching. How you could do this is take steps down the ray and test if the point is inside or outside of this bezier tube. You would iterate down the ray until you either find an intersection or leave the bounding box of this shape (or maybe when you take a maximum number of steps, or some other fail case). The challenge then only becomes "how can I tell if a point is inside or outside of the shape?" which is a lot simpler. If you had constant thickness you would just find the closest point on the bezier to your point and do a distance comparison. Some details you might be interested in: 

Why is it that it's so common to use monte carlo randomized sample locations, instead of uniform sampling? I'm assuming that taking randomized samples gives some benefit but I don't know what they could be. Is anyone able to explain the advantage of randomized sample locations over uniform sample locations? 

Just accept either the current or last point tested as the intersection point (fastest) Binary search the range to get a better answer If you have a function which gives you an (estimated) signed distance from a point to the surface, instead of just an "inside vs outside" test, you can do a linear interpolation between the last point and the current point to find where the "0" is, and take that as your intersection point. This is what I've had most success with personally. 

I could live without distance if needed but the others are required. I know that I can store just x,y of the normal and calculate z as needed since its always negative (the normal is in screenspace). That still leaves 5 values to store in four 16 bit floats: normal.xy, uv.xy, material index. Does anyone have any good methods for making this happen? It would be nice to have the distance too, otherwise I am limited to directional lighting instead of getting point lights as well. This is in shadertoy if that context changes any answers. I'll be sampling this buffer with nearest neighbor sampling, so you don't have to worry about it being stored in a way that interpolates well. Thanks! 

Whenever there's too much churn of things being created and destroyed, regardless of the specifics of the situation, a common way to dealing with this is to make free object pools. How these work is when you are done using something, instead of actually freeing or destroying it, you put it back into a list or array of "free objects". When you need a new object of a specific type, you check the object pool to see if there's a free one lying around. If there isn't one there, you create a new one the "expensive" way and use it. When you are done with it, you put it in the free pool for use in the future. A possible issue with free object pools is that you may have a spike in objects for some reason and then you are left with a ton of unused objects sitting in a free pool that will never be needed again. In situations like this, you can keep track of how long (in number of frames for instance) since an object was last used, and if they get over a certain unused age, free them from the object pool. PS - make sure and profile your code to find what the ACTUAL performance bottlenecks are before attempting to solve problems. Optimizing without profiling before (to find the problem) and after (to verify you improved things, and knowing by how much) won't get you very far (: 

Let's say that I have a raytracer that is rendering a scene that includes a refractive object. When a ray hits the refractive object, I know that I can use the fresnel equation (Schlick's approximation in my case) as well as the object's base reflectivity amount to figure out how much light is reflected or transmitted at that point on the surface. At this point, I can get the reflected color using a reflection ray (possibly recursively), an environment map, and also calculate a specular highlight for the light source(s) I have. I sum whatever reflection colors I have, multiplied by the amount of reflection from the fresnel equation. Next, I calculate what amount of light comes in due to refraction by following the refracted ray (recursively) and multiplying that value by 1.0-reflectionAmount. That makes sense to me so far, but where I get a little confused is for the refraction rays. When I follow the refracted ray into the object and hit the back side of the surface the first time, i once again use the fresnel equations to calculate the reflected (internally) and refracted multipliers for the light, taking into account the possibility of total internal refraction (assuming here that the inside of the object has a higher IOR than outside of the object, an ok assumption for my case). But, at this point, do i apply a specular highlight to the surface that i hit from the inside? On one hand it's a surface so i feel like i should, but which ray should i use for the lighting - the refracted ray or the reflected ray, or both? If using the refracted ray, it feels wrong to use the direction to the light source for the specular light, and feels like i should shoot the refracted ray to the surface of the object and see what the specular highlight would look like from there and use that - but do so recursively, using the fresnel equation again at each step. Can anyone explain a decent way to handle a specular highlight in a ray tracer that supports refraction and total internal reflection? Since this is a ray tracer, not a path tracer, instead of being limited to physically accurate solutions, I'm looking for any plausible solutions, but of course, physically inspired solutions are likely to be better! Thanks!