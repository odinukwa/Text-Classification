Is an effectively nested Xen Server storage repository a problem for stability? Consider the graphic for the data path of the web server to a physical drive: (Starting on the right) 

I know this is old, but extremely relevant as duplicating 100's of GB's to Tb's of uneeded files often is problematic for anyone budget conscious. I have been using Areca Backup. and it seems to keep track of duplicate files well. I recently moved 300GB one day and it only backup up 8GB actual data meaning it did not recopy the files and just referenced them My manual file mirror, versions, and deletions had bloated 350GB (300 noted above) in 3 months where ArecaBackup only bloated 20GB over the same time frame. I appreciate it's ability to backup the files in such a way that you can access the actual file in a directory tree on the backend; that is files can be stored in a directory tree in their original format instead of a proprietary format to fight possibility of corruption of a backup. Although typically you would browse through the GUI. 

What we want to achieve We now want to show (or notify - whatever) each java web application user within the java web application that an email which he/she wanted to send actually failed to sent. (Remember: The bounce email are currently sent back to support@example.com - not to the web application users email address - so these users have no clue their emails did not get send) The basic (and simple) idea we now have: Make exim write the of each failed message (which in the above case is 484648301.663.1401636847496.JavaMail.webapp@node1) into a local running PostgreSQL database as soon as possible we know the message delivery failed (or: as soon as possible the corresponding bounce message was generated and/or sent). If this is possible, we could easily read these "failing" s from the database within our java web application, and - voil√†! - we can show users that their messages have been bounced. Simple idea, isn't it? Just for a better understanding: The mentioned (unique) s already get generated for each message within the java web application before the JavaMail API sends the message to exim - so we can be sure our java web application knows each for each message (and of course also knows which user sent out a specific message). Possible Solutions? I did some research on how this could be solved and this is with what I came up with by now (Please correct my if I am wrong - these are just my thoughts right now): Approach 1: Pipe the failed message and it's to a bash script. Such a bash script could then easily store the in the local PostgreSQL database. Is this possible and how could this be achieved? The problem I see is that as soon a message failed, no other exim router gets executed - which means I can not write a router which handles a failed message - the failed message would never reach the router. Or am I wrong? Approach 2: Another possibility I was thinking about is to not have a look at the original message which failed, but the newly generated bounce message. As far as I understand this newly bounce message is also going through all routers until one accepts it. So maybe we could write a router which checks if a message is a bounce message and then pipe it to the bash script? But how do I know that if a message is a bounce message? Does it have a special header or something else significant? (Remember: As we only use exim for outgoing emails we can be sure this would always be a bounce message "generated" by our java web application users). Approach 3: Similiar to approach 2, but instead of writing a router we could make use of the system filter, which looks after a bounce message and then executes a command / pipe... Approach 4: Would it be possible to configure or even to not be an email address but a pipe to shell script (which of course received the id and/or whole failed message)? Two more things: It seems exim can also talk directly to databases (I read this somewhere) - but I have no idea how I could make use of this for my described problem. Plus: However this is solved, I do not want to modify any configuration files which gets overriden when exim gets updated via (I do not want to re-configure exim after each update). You help is needed How would you solve this problem (saving the failed message s into the database)? Please correct me If I was wrong somewhere - I am not that deep into the exim configuration. Also, as you can see, the solutions I think about are more theoretical - it would be nice I someone could show me real world configurations and where to put this configs to make them work. Thank you very very much for any help! 

In the graphic Xen Server and the VM DRBD/NFS server is hosted on an internal USB flash drive. Hard drives are a local MDRAID10 array. The other VMs would be stored on a storage repo via NFS on a DRBD via a VDI attached as a block device on the MDRAID array. VMS are typically Debian. We DO NOT have a dedicated shared storage device (NAS, SAN, etc.). If performance is not a concern (5-10 low use users for sporadic web access to a company site from the field), will there be stability concerns based on the data path being a storage repository served by another storage repository via NFS on DRBD. TL;DR In lieu of dual SANs with supporting networking, we are trying to use DRBD on two servers with local storage to allow for easy manual failover during an outage on the primary server. During an outage the secondary server would become the primary and the VMs could (theoretically) be instantly be fired up with very little configuration. The servers are same RAM and CPU generations even. Xen seems to have its quirks and I forsee Xen Server having a problem that wipes out the entire server until it gets "fixed" given Xen is running EVERYTHING. I doubt we would have permanent data loss, but storage repositories disappearing happens more often than I would think based on the reading; and unless all your disaster recovery ducks are in a perfect little row, it could take a bit of time to bring things back up correctly if we had to reinstall Xen from scratch while carefully filling in the holes we were missing in our documentation as we went. With DRBD thought, in the event of a problem we could be running on our backup server very quickly with active file and VM mirrors. Then worst case we could easily just start from scratch on the primary server if need be and not have to worry about "fixing" anything. What is not shown are a couple more VMS to serve files, but the data would be housed on another SR, so only the VM data itself which is rather static would be served by the same path depicted in the graphic. 

Others have said online that it works for them, but so far it hasn't for me. It doesn't give an error, it accepts the command just fine, but when I look in ADUC, the groups are still there for the user. Any suggestions as to what I may be doing wrong? Executing from Windows 7 with domain admin rights, Exchange cmdlets and Quest snapin loaded. Thanks! 

Thanks for your help above. What you suggested worked fine for me in part, but I had some other formatting issues that I ended up figuring out as well. Just to help the next person looking I'm posting my end result/configuration as well. It may not be as efficient as it could be but it seems to work for what I wanted! Thanks again! 

This may be a somewhat simplified question, but I've read online quite a bit and have found very little on the comparison. We are a smaller shop, approximately 200 users. Currently our Exchange mailbox quotas are set at 400MB. For as long as I can remember we've had users archive email as they fill up their mailbox, so we have more .psts floating around than we would like. I've looked into the archiving feature of Exchange, and it looks nice, but I'm having a hard time determining why I would go that route (for the cost, would be about 12K for licensing) versus just having more hard drive space on the server and increasing everyone's mailbox limit to 10GB (for example). I would imagine having their primary mail on one server and archive on another would be ideal for balancing the load/stress on the system, but with our small shop we only have the one server/database and had planned on putting the archive database on the same server anyway. We plan to deploy DAG with a DR site in the near future as well. So, other than being able to give our legal department more capabilities with the discovery process, is there any benefits I'm not seeing here? Also, we are currently on Office 2007, from what I read a lot of features aren't usable unless you are running 2010, so that would be something else to take into consideration I suppose. But anyway, I would appreciate some thoughts on this from those who have used both ways and are more "in the know" than I am with regards to Exchange/archiving. :) Thanks for your input! 

Temporarily duplicate data from array, tape is cheapest if array is large and HDD space is not available. Remove and replace failed drive. Build new array with new drive from scratch. Reload files to new array from step 1. 

Virtualizing will help with your needs tremndously. We have a small business and virtualizing allows us to consolidate hardware, increase segregation of services to help with security, and helps with uptime because we can migrate VMs between hosts (hypervisors) very easily; something that is very difficult with baremetal. We use dated enterprise hardware in pairs with backup parts kept on site (fans, drives, etc) but have both a primary and secondary host; Our host runs VMs serving DRBD, Apache, MYSQL, Samba, NFS, Reslio Sync, Dropbox, etc. We let our host manage RAID arrays using MDADM. Utilizing DRBD the VMs are kept in sync on a backup server so downtime is almost a non-issue even with a catastrophic hardware failure on the primary host. But being a small business it simplifies hardware management, allows us to run less hardware which has far reaching implications on budget and IT resources, and consolidates the management of our services because it is natural to administer all the VMs from a single console on a workstation; for us through Xen Center as we use XenServer. Further it allows us to segregate things so things like cloud services can be virtually segregated from internal services providing a high degree of security. For example we serve two separate cloud file services in two separate VMs; one for field personel accessible via mobile devices and one for office personel accessible via the internal network. As a note, our backup server (not secondary host) is not virtualized purposefully so we have baremetal access to our files in case of a software or configuration failure with our host. That is if our host corrupts our VMs or data stores somehow we have baremetal access to the files and VMs still. In the end we can provide our company with enterprise grade file, backup, web, cloud, and other services all in house for minimal cost and maximum uptime. It also allows us to expand as we can integrate other services; planned in the near future are VPN services for remote book keepers and Android form services for field personel that needs a windows software intermediary to interface with MySql. Without virutalizing we would need to buy, run, and administer more hardware; virtualizing has eliminated the problem of hardware all together when adding such services and we can simply focus on integration of the software/service which can be daunting enough. 

I've testd them manually and they work just fine to remove the entries and allow the machine to boot up without a prompt. The thing is, I can't find the exact place to place them in the task sequence. I want them to run before the first login, so I placed them In Postinstall, before the Restart Computer task. That didn't work, gave an error. I tried right before that, before Next Phase, same issue. I try putting them after Restart Computer, but before State Restore and they don't error out, but they don't seem to run until I hit "OK" through the one prompt, then it runs and never prompts again. So it's progress as before it would prompt me to hit "OK" through the disclaimer after every reboot, now I only have to hit it that one time and it removes the line from the cached registry and never prompts again. I just can't figure out how to get it to run before it gets to that first prompt so that it doesn't prompt at all. It's so close to being fully automated... My only other thought is to re-do my capture and run that line first, but I would love to avoid that unless it's my only option left. Thanks for any help or advice you can give. 

The Problem Recently we are receiving bounce emails because users of our java web application tried to send out emails to non-existing email addresses. Lets have a look on such a case in the (anonymised) log file: 

The Setup We are running exim version 4.72 on Debian Squeeze 6 (Squeeze) (LTS). Exim is used for outgoing emails only - it does not handle emails from other servers/domains - it only handles (outgoing) message from a local running java (web) application. For anonymization, lets assume the domain we use for our company email addresses, as well for the domain our java web application is running on, is called example.com. The Background We have a web application written in a java framework which allows it's users to send out emails. The address of these emails belongs to our domain: support@example.com. Technically these emails are created and get send from the java web application (we use the JavaMail API) locally to exim, which then eventually sends them out to their destination address. Incoming emails to our domain are handled by Google Apps/Gmail (We also set up SPF records, etc.) This setup is working fine without any problems so far. For completion, here is our (anonymised) : 

I've never attempted to restore a deleted individual mailbox and figure it's a good thing to have tried for when/if it's actually needed. I have a full backup from Monday (today is Wednesday), i have the GRT options checked in the backup for Exchange, I can browse through the backup job and find a particular employee's mailbox that I removed on Tuesday. My question is, is it as simple as selecting that checkbox and submitting the job? His account no longer exists on the network because he is no longer employed with us, i figured it was a good person to try this on. I see an option in Backup Exec that says "automatically recreate user accounts and mailboxes", so i assume i would select that as well. I guess it just seems too easy, and since this is a production server i wanted to ask around before attempting to restore this individual mailbox back. Anyone have any experience with this? Thanks. 

On the phone screen I click Next. It says USB connection is detected, I click Skip (only option is Skip or Menu). It asks me for my extension or phone number. I put the number in that is assigned to me on the Lync Server (I set it to 5551212 and verified in AD that the number I put in is the same as the msRTCSIP-Line field. I click Next, then enter my PIN (I also reset my PIN to make sure it was valid). I click Sign in, it says "Contacting time server" for a bit, then "Connecting to Lync Server" for just a split second, then quickly an error flashes that says "Account used is not authorized, please contact your support team". Afterward it goes to the screen saying "An account matching this phone number cannot be found. Please contact your support team." I see looking at the menu/system information on the phone that it grabbed an IP address from the DHCP server (and verify it is shown in leases), the mask and GW/DNS are all set correctly. I did notice VLAN ID is set to 1...which we don't use, but I don't see a way to change that to the VLAN ID we do use on our network (if that is even relevant?). I can't access the phone via the web or ping it, but my computer can use the built-in switch to work through it just fine. On the Lync server I have ran the test command -