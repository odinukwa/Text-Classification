When designing an algorithm for a new problem, if I can't find a polynomial time algorithm after a while, I might try to prove it is NP-hard instead. If I succeed, I've explained why I couldn't find the polynomial time algorithm. It's not that I know for sure that P != NP, it's just that this is the best that can be done with current knowledge, and indeed the consensus is that P != NP. Similarly, say I've found a polynomial-time solution for some problem, but the running time is $O(n^2)$. After a lot of effort, I make no progress in improving this. So instead, I might try to prove that it is 3SUM-hard instead. This is usually a satisfactory state of affairs, not because of my supreme belief that 3SUM does indeed require $\Theta(n^2)$ time, but because this is the current state of the art, and a lot of smart people have tried to improve it, and have failed. So it's not my fault that it's the best I can do. In such cases, the best we can do is a hardness result, in lieu of an actual lower bound, since we don't have any super-linear lower bounds for Turing Machines for problems in NP. Is there a uniform set of problems that can be used for all polynomial running times? For example, if I want to prove that it is unlikely that some problem has an algorithm better than $O(n^7)$, is there some problem X such that I can show it is X-hard and leave it at that? Update: This question originally asked for families of problems. Since there aren't that many families of problems, and this question has already received excellent examples of individual hard problems, I'm relaxing the question to any problem that can be used for polynomial-time hardness results. I'm also adding a bounty to this question to encourage more answers. 

I think all the other answers are very good; I'll try to give a different perspective on the issue. I don't know how well P models "efficient" computation in the real world, but we like the class because of its nice closure properties and other mathematical reasons. Similarly, L is also a nice class due to some of the aforementioned reasons. However, as you commented, if we relax our definition of "efficient" to quasi-polynomial time, then PolyL is also efficient. We could discuss complexity theory where we allow classes defined with a logarithmic bound on some resource to use polylog resources instead. Correspondingly, we would also relax our definitions of NC, NL, etc. to allow quasi-polynomial size circuits instead. If we do this, NC1, L, NL and NC all coincide with the class PolyL. In this sense PolyL is a robust class since many natural classes coincide with it. For more information on complexity theory with log -> polylog and polynomial -> quasi-polynomial, see Quasipolynomial size circuit classes by Barrington. Another reason to study polyL or similar classes like quasi-AC0 is that while an oracle separation between (say) ParityP and PH implies that PARITY is not contained in AC0, the reverse implication is not known to be true. On the other hand, PARITY is not contained in quasi-AC0 if and only if there is an oracle separation between ParityP and PH. Similarly, the classes quasi-TC0 and quasi-AC0 are different if and only if there is an oracle separation between CH and PH. So the usual complexity classes like PH, ModPH, CH, etc. when scaled down by an exponential to prove oracle results turn into quasi-polynomial versions of the usual classes AC0, ACC0 and TC0 respectively. Similarly, the argument used in Toda's theorem (PH is contained in PPP) can be used to show that quasi-AC0 is contained in depth-3 quasi-TC0. (I don't know if the same conclusion is known for the usual versions of these classes. I have seen this listed as an open problem in some papers.) 

They mention "the problem of the approximate degree of AC0" in their acknowledgements too. So I assume there has been some work on this problem before? Can someone point me to a paper that talks about the problem? And what are the best known upper and lower bounds? What I know about the problem (This section was added in v2 of the question) The best known upper bound on $\widetilde{\textrm{deg}}_{1/3}(\textrm{AC}^0)$ that is know is the trivial upper bound $n$. The best lower bound I know comes from Aaronson and Shi's lower bound for the collision and element distinctness problems, which gives a lower bound of $\tilde{\Omega}(n^{2/3})$. (For severely restricted versions of $\textrm{AC}^0$, like formulas with $o(n^2)$ formula size, or depth-2 circuits with $o(n^2)$ gates, we can prove a $o(n)$ upper bound using quantum query complexity.) Related: threshold degree (Added in v3) As Tsuyoshi points out in the comments, this problem is related to the problem of determining the threshold degree of $\textrm{AC}^0$. The threshold degree of a function $f$ is the minimum degree of a polynomial $p$ such that $f(x)=1 \implies p(x)>0$ and $f(x)=0 \implies p(x)<0$. Lower bounds for the threshold degree of $\mathrm{AC}^0$ have now been improved by Sherstov. He exhibits a family of constant-depth read-once formulas on $n$ variables whose threshold degree approaches $\Omega(\sqrt{n})$ as the depth goes to infinity, which is almost tight since read-once formulas have threshold (and even approximate) degree $O(\sqrt{n})$. See $URL$ (Jan, 2014) 

If we restrict attention to graph properties, then we can prove slightly improved bounds compared to the general bounds you mention: 

This question has been resolved! A few days ago Andris Ambainis, Kaspars Balodis, Aleksandrs Belovs, Troy Lee, Miklos Santha, and Juris Smotrovs uploaded a preprint showing the existence of a total function $f$ which satisfies 

What you call the "revealing complexity" is sometimes called the (deterministic) query complexity of exact learning with membership queries. Exact learning refers to the fact that you have to exactly identify $x\in C$ at the end, as opposed to being (say) probably approximately correct (as in PAC learning). Membership queries refers to the fact that you can query the individual bits of $x$ (as opposed to being given a random sample according to some distribution, as in PAC learning). See this related question where I explain some of what's known about this complexity measure. For example, we know that the deterministic query complexity of exactly learning a concept class $C\subseteq \{0,1\}^n$ with membership queries is characterized up to a factor of $\log |C|$ by the complexity measure defined in this paper by Servedio and Gortler or the extended teaching dimension of Hegedus. Both measures are combinatorial in nature. Whether there is a combinatorial measure that captures this complexity without a $\log|C|$ factor is unknown to me (and probably an open problem). 

(I've seen Ryan's answer, but I just wanted to provide another perspective, which was too long to fit into a comment.) In the $L = P \Rightarrow PSPACE = EXP$ proof, all that you need to know about L, informally, is that when blown up by an exponential, L becomes PSPACE. The same proof goes through for NL, because NL blown up by an exponential also becomes PSPACE. Similarly, when NC is blown up by an exponential, you do get PSPACE. I like to see this in terms of circuits: NC is the class of polynomial size circuits with polylog depth. When blown up, this becomes exponential size circuits with polynomial depth. One can show that this is exactly PSPACE, once the appropriate uniformity conditions are added in. I guess if NC is defined with L-uniformity, then this will get PSPACE-uniformity. The proof should be easy. In one direction, take a PSPACE-complete problem like TQBF and express the quantifiers using AND and OR gates of exponential size. In the other direction, try traversing the polynomial depth circuit recursively. The stack size will be polynomial, so this can be done in PSPACE. Finally, I came up with this argument when I saw the question (and before reading Ryan's answer), so there might be bugs. Please point them out. 

Schoening's algorithm is a probabilistic algorithm for k-SAT with running time $O(a^n)$, where $a = 2(k-1)/k$. This results in an $O(1.33334^n)$ algorithm for 3SAT, an $O(1.5^n)$ algorithm for 4SAT, etc. The algorithm has also been (almost completely) derandomized by Moser and Scheder, who give a deterministic algorithm for solving kSAT running time $O((a+\epsilon)^n)$ where $a$ is the same constant as before, and $\epsilon>0$ can be made arbitrarily small. Note: In this answer the big Oh notation hides poly(n) factors. I wanted to use the $O^*$ notation, but it isn't rendering properly. 

Diagonal Hamiltonians are easy to simulate with 2 queries. It's very well explained (with a circuit diagram!) in Andrew Childs' PhD thesis on page 18, Rule 1.6. The main idea is to observe that if H is the Hamiltonian with diagonal entries d(j), then simulating H for time t maps the basis state $|j\rangle$ to $e^{-id(j)t}|j\rangle$. This is performed as follows: $|j, 0\rangle \rightarrow |j, d(j)\rangle$ (this uses 1 query), then $|j, d(j)\rangle \rightarrow e^{-id(j)t}|j, d(j)\rangle$ (this is a controlled phase flip on the first register controlled by the second), and then $e^{-id(j)t}|j, d(j)\rangle \rightarrow e^{-id(j)t}|j, 0\rangle$ (this uncomputes the answer using another query). Indeed, all 1-sparse Hamiltonians can be simulated with 2 queries. This is also explained in Andrew's thesis, towards the end of page 20. It might be a bit hard to understand since it's explained in the context of simulating a d-sparse Hamiltonian using edge coloring. But the essential idea is to reduce a d-sparse Hamiltonian to a sum of 1-sparse Hamiltonians and then simulate the 1-sparse Hamiltonian. 

One easy way to come up with such a model is to first create a restricted model of quantum computation that can still do something non-classical, and then just give it classical computation for free. An examples of this strategy is the one clean qubit model (along with a BPP machine). Some references: On the Power of One Bit of Quantum Information, Computation with Unitaries and One Pure Qubit and Estimating Jones polynomials is a complete problem for one clean qubit. Another example would be to have a log-depth (or polylog depth) quantum circuit with access to a classical computer. This will yield something like $BPP^{BQNC}$. 

Newer, simpler question: This question is a relaxation of the original one where I don't insist that the resultant circuit be constant depth. As explained above, there is a way to convert an AC0 circuit with depth k, size S into a circuit with size $O(S^k)$ such that the new circuit has fanout = 1 for all gates. Is there a better construction? 

If you don't mind artificial languages, we can construct such problems using pretty much any number k whose value is unknown to mathematicians. For example, we don't know the value of R(5,5) (the fifth Ramsey number), or the size of the largest excluded minor of the family of knotless graphs (this number is finite due to the Robertson-Seymour theorem), or the value of BB(10), where BB() stands for the Busy Beaver function. Let k equal any of these numbers. We know that k is finite, but we don't know the value of k. Now construct some problem in NP where the witness is of size $O(n^k)$. Off the top of my head I can't think of a nice way of doing this, but here's one way. Let the input be a succinct description of a graph. Since the description size is n, the graph is on exponentially many vertices. (For example, maybe the input is a circuit that accepts two inputs x and y and tells you if (x,y) is an edge in the graph.) The question is to determine if the graph contains a path of length $n^k$. This problem is in NP because the prover can send the list of vertices on the path in order, which the verifier can check. The size of the witness is $n^k$. 

The Fourier Hierarchy as defined in "Yaoyun Shi, Quantum and classical tradeoffs." From the complexity zoo: 

Efficient algorithms for all quantum systems defined by "local" Hamiltonians. (Lloyd also explains that any system consistent with special and general relativity evolves according to local interactions.) 

Copying my comment: If you meant to ask for a problem analogous to GI, then perhaps you're asking for a problem that's not in PH and not PSPACE-complete. Problems complete for any class not known to be contained in PH, but contained in PSPACE, will work as an example. So take any problem complete for BQP, QMA, PP, etc. 

Turan's theorem exactly characterizes the densest graphs on n vertices with no k clique. The Turan graphs are the maximal graphs, and they're very easy to construct. Is this what you want?