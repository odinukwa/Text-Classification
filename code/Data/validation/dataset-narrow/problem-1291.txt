Step 4 - Use the SoundEffect as normal Your OGG file is now loaded and can be used like any other loaded through the content pipeline: 

I basically created a simple to hold my view and projection transformations, and passed two vertices (storing position and color) to the method to be rendered as a. Of course there are many ways to optimize it, most of which involve the creation of a to store all of the vertices, and batching as many lines as possible into a single Draw call but that's irrelevant to the question. 

I have never used Corona before but reading through the documentation I believe you can't easily retrieve these coordinates back from the generated display object. From what I read, returns a vector display object to which you can as many vertices as you want, but there's no way to access those values. I guess you could use to get back the size of the rectangle that encompasses the line, but this wouldn't tell you which diagonal to pick. What I would suggest is to encapsulate this behavior in your own separate class (if that's possible in Corona) which stores the vertices, provides access to them, and generates the display object for you. Since I'm not familiar with Corona I'll use another language, but try to think of it as pseudocode. I hope you can understand the general idea and can adapt it to some Corona equivalent. Let me know if there's something you don't understand: 

The answer given by Byte56 describes one fundamental mistake in your code that you must fix. But afterwards you'll still want to factor in the delta time into your movement otherwise it is likely to occur too fast and depend on the framerate of the game. In that regard, I think you're trying to implement euler integration (although if you read that article you will see that he recommends using a more complicated method). It's usually implemented like this: 

Step 3 Revert back to the old rotation if any of the corners lies outside of the camera's bounds. Example: 

Draw the track; for simplicity your track can be one complete image and you simply draw it on the background. A 2D camera that changes your point of view of the scene. Basically it creates a View Matrix for you to pass OpenGL when drawing. A player/car object with X,Y position in the track. Draw the player on top of the track at that position. Fix the 2D camera to your player (e.g. ). 

I'll try to give a quick answer, assuming that you want to keep using the same technique I suggested on the other post (which by the way is not perfect, but in practice I haven't run into any noticeable problems yet in the context of a game). Basically you changed the most important part of the code when you applied the velocity all at once to the position. What you need to do after calculating , is to apply it in two steps to your such as: 

This method returns the current mouse position relative to the control bounds. Put that inside your control class and see how it works for you. This solved a few problems I was having when I tried using the XNA Mouse API for reading the mouse position, in particular when placing my XNA rendering control inside of another control. Your problem seems to be a different one, but I've just tried using Fraps on my application which uses the method described above, and it's working correctly, so I think it should probably solve your problem too. 

The reason for this behavior is simply because transformations are always applied using the "origin" of the current space. Translation remains the same regardless of the origin, since it's just a displacement and does not need a point of reference. But scaling and rotation vary greatly as you can see from the picture. Applied to your example of shadows if you don't keep in mind the correct order you'll get a shadow that's glued to your figure! 

The alternative, I think, would be to create classes that wrap the sprites, make the model observable, and make the view react to changes on the model. This seems like a lot of extra work and boilerplate code, and I'm not seeing the benefits if I'm just going to have one view per controller. Example 2 

I've actually implemented this just the other day. I started from the XNA Platformer Sample but noticed it had a few problems on corner cases. This was especially noticeable when using it for Zelda-like movement, because the character would get stuck between tiles when hugging the walls in a certain direction. I tried several changes to the algorithm, but there would always be one direction where the problem would appear. I looked around a bit, and although I was skeptic, ended up trying a simple solution that I read in one forum - and it worked! So, instead of moving the player all at once and then resolving each collision along the smallest axis of intersection depth, the trick was to update and resolve on the X and Y axes separately. The process is still very similar to before, except for that difference. In other words: 

Tip 2 - Registering states and addressing them by ID I've seen some state manager implementations that expect you to pass the actual state object when pushing a new state. This means that you will either be using it as and creating a new screen object everytime, or you will have to create and store references to your screens and reuse those references. Usually I don't want to recreate the same state everytime, but I don't want to have to manage the references myself either, so I always add a a method such as to my state manager which stores the states in a and then I do all push or switch operations using the state name instead (e.g. instead of ). I tend to create and register all my states at once when the application starts, although most of them hold little state until they are actually added to the stack. Tip 3 - Transition callbacks It's also useful to add a few callbacks to your states to be called whenever the state is pushed or popped. For instance, I use these four callbacks: 

Problem #1 - Collision Detection Under the assumption that you will use Gleed2D mostly for sprite based level editing, i.e. by placing, rotating and scaling a set of rectangular sprites in the world, I would suggest one of the following approaches to collision detection depending on your needs (read to the end even if the first suggestions don't apply to your case): 

The backgrounds in Jamestown are not looping bitmaps, because they are always changing from the start of the level to the end. But if you only need a looping bitmap, and you only need it to loop vertically, the easiest way I know is the following (cross reference with the image below): 

So in the end, that entire processing might even be more expensive than using the stencil buffer (haven't tested though). So my advice for now would be to try using the stencil buffer first, and if it turns out to work well enough, don't give it a second thought. 

Afterwards, just tweak your initial velocity and acceleration values to change the duration of the jump. By the way I think would make more sense being called and defined as a vector pointing donwards, so that you can get rid of the minus sign. And you might as well use everywhere to avoid the conversion at the end and simplify everything. 

On this example I'm also rotating the rectangle around its center, but I tried with the origin set to zero and it didn't cause any problems for the shadow. And to show you why order of multiplication matters when applying transformations check this image: 

I've got this sort of info spread over many books on my bibliography, but I'm currently away from them. But from what I could gather up from memory and browsing through the table of contents online, I recall the following books: 3D Math Primer for Graphics and Game Development 1st Edition or Mathematics for 3D Game Programming & Computer Graphics or Chapter 16 (Visibility Determination) of 3D Math Primer for Graphics and Game Development 1st Edition (strangely the authors seem to have removed this section from the second edition of the book) covers the most common techniques (i.e. grid system, quadtree and octree, bsp trees, portal occlusion). The book is really good, although perhaps not the best of its kind. I've seen Mathematics for 3D Game Programming & Computer Graphics being mentioned very often, but unfortunately haven't gotten my hands on it yet. From the table of contents, it seems to cover space partitioning algorithms too. Not sure how they compare against each other. Naturally, the focus of these books is on the math. And although the title says 3D, they are also quite relevant for 2D programming. Real-time Rendering 3rd Edition 

Mixing Everything Together - An Example Here's how you can use these processes. Let's say for instance that you would like to play animation A, wait 2 seconds, play animation B, and finally write a message to the debugger. By using the processes above you can implement this entire sequence as four lines of code: 

Ktodisco already mentioned most of what there is to know, but to add to what has already been mentioned I'd like to mention a few more ideas. I've also found this resource to be of great help to me on this subject. Environment Mapping Add an environment map to your scene, such as a cube map. A cube map is basically just a collection made up by six textures. Each of the textures represents one of the faces of a cube that describes the "environment". The most common example is a sky box. Given a direction vector from the center of the cube, your CubeMap class should be able to to tell you the color of the pixel that the vector is "pointing to". Once you have that class, it's pretty trivial to add it to your raytracer. Just redirect any rays that don't intersect anything in the scene, to your cube map, and use the color it returns. From experience this will improve the look of your scene by a large margin. Also, since it doesn't involve casting any additional rays, it won't slow your performance down by much. Bump Mapping If you want to add a bit of roughness to your surfaces, one hack I've found was pretty easy to implement was to add a little bit of displacement or randomization to the normals of your surface before calculating your lighting. Using some sort of noise texture (e.g. perlin noise) as a basis for that displacement will also allow you more control over the frequency and depth of the bumps. Chromatic Dispersion Haven't tried this on a raytracer yet, but it worked nicely on a glass shader I've created before. Basically the idea is that different light wavelenghts are refracted by different amounts, so whenever you're calculating a refraction, instead of casting one ray, you cast three separate rays with different indices of refraction, and in the end combine the results with a tint of R, G and B for each of the rays. You can also combine this with the fresnel effect which varies the reflectiveness of the surface based on the angle at which you're seeing it. More information here. 

Option 1 One possibility is to create a 2D array of instead of . This way each position in the array can store more than one if needed. So you would create the data structure like this (I think, it's been many years since I've used Java): 

The previous books were mostly theoretical though. For more specific and practical advice, I've read several articles on the subject scattered throughout the Game Programming Gems series. Some that come to mind: 

I created a simple scripting language to use on a Unity3D game. My parser generates an AST with nodes for every type of operation, and to run the script all I need to do is call Execute() on the root of the tree, which propagates recursively to every node in it. To run functions from my script, I first create a dictionary linking function names to delegates, and the function node simply needs to look it up run it. But I have ran into a problem now... Some of the functions I need to call from the script are coroutines that take time to execute, such as WalkTo or WaitNSeconds, and for those coroutines I need to halt the script execution until they are done. I'm stuck on figuring a way to halt the script execution, as once I call on the root node, the script gets executed recursively all the way. Just to provide some more context, here are some examples of a few possible types of nodes and how they operate (not the actual nodes from my language, but close enough). Some nodes simply call Execute() on their children and return nothing. Some nodes simply return values directly. Other nodes get values from their children and modify it in some way. 

These values will be based off a center point which may or may not be the player's position. The shape of the workable area depends on where you place this point. 

Which would be wrapped and compiled at runtime inside a method such as (notice the use of the dynamic keyword to disable static checking): 

The main advantage is that besides being simple, you have complete control over the construction order, and most importantly, you can just look at the code and see in which order things are being created. 

I am guessing that maybe your content writer is still doing a split by " " and then writing it in that way to the stream, but since you're now separating your values by "," it's not written correctly. So when deserializing, the ReadObject method is expecting a list of rectangles, but finds something else and fails. Add your content reader and writer to your post if you need help figuring that out, but it really should be just a matter of keeping both with sync with each other and with the file format. 

Now let's create a class that serves as a process queue. You add processes to this queue, and they get executed in turn. This is where my implementation differs from the one in the book above. In the book every process is updated every frame, but in this case I'll make them sequential instead, so that the second process only starts when the first one ends. Once again it's a simple class: 

Rendering in most games takes place within a loop (the game loop) and in each iteration of the game loop the entire backbuffer is cleared (in this case your canvas) and redrawn from scratch. The term dirty rendering is referring to a technique where instead of clearing the entire canvas every frame, you only clear it on demand (i.e. only when something changed in the scene) and possibly only a portion of it (i.e. only the portion where something changed). This process of marking only a portion of your canvas to be redrawn is what is what dirty is referring to (i.e. that portion of the canvas is dirty and needs to be redrawn but everything else is still clean and shouldn't be touched). I have no idea if there's some special way for you to implement this in HTML5, but I found this resource which might give you some ideas. 

Depth testing happens in what is known as the merging stage after the pixel shader (also check this link). There's however an optimization technique called the "Early-Z Test" which is available in some graphic cards, and it consists of moving the check before the pixel shader so that it can be skipped completely for occluded fragments. As for whether you can modify the depth value of a fragment in the pixel shader, yes you can and this source confirms it. although I'm not sure how to use it correctly. Quoting: 

Theory Since you didn't specify in what platform you're implementing this, I'll give a description of the algorithm in a language-agnostic way: 

Yep. First you move your character. Then you have the collision detection phase, and finally the collision response phase. That's essentially what you'll be doing by moving the character up outside the tile. Separate axis handling