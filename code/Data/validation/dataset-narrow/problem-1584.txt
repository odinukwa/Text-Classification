Interesting ideas on finger printing, but that isnt going to work here, as you don't have a definitive sound you are working with. To me, this seems a bit more like an image processing problem. So instead of thinking how you could reduce dimensionality, could could try and combine your data together into "images" that digitally represent the sounds you are looking at. Then using standard image processing techniques. I say this waving my hands around vaguely, as I don't know of specific ones, but have enough understanding of edge detection, feature detection and applying map/reduce type methodologies which might be able to give templates for what you are after. 

This will allow you to be able to do the query of pages with the maximum number of steps, or pages most frequently linked to, etc. 

Just to throw an extra option in, Microsoft Azure's Stream Analytics. As far as a comparison of the different technologies and their relative performance, those stats seem difficult to get, but there is a useful comparison of a few technologies in this article, though doesn't mention Azure Stream Analytics. I think the difficulty is that the architectures are so different, that is makes simple metrics hard to produce. What is needed is some more robust measurement tools that can be applied to give a few metrics for various operations. I would think that Gartner would have something to say, but the nearest I could find was their BI and Analytics quadrant, which only obliquely address stream analytics. 

I have one dataset, and decided to use XGBClassifier to get a variable importance plot from it. I was a little surprised that some features that I assumed were insightful had no appreciable value. With this in mind, I used an sklearn RandomForestClassifier to create another variable importance plot. To my surprise the two were very different! Many variables that appear on one don't appear on the other. I used as similar parameters as I could, between the two models. Here's what they look like to give a general idea: 

Clearly the input layer is a vector with 3 components. Each of the three components is propagated to the hidden layer. Each neuron in the hidden layer is seeing the same vector with 3 components -- all neurons see the same data. So we are at the hidden layer now. From what I read, this layer is all just (usually) ReLus or sigmoids. Correct me if I'm wrong, but a ReLu is a ReLu. Why would you need 4 of the exact same function, all seeing the exact same data? What makes the red neurons in the hidden layer different from each other? Are they supposed to be different? I haven't read anything about tuning or setting parameters or perturbing different neurons to have them be different. But if they aren't different...then what's the point? Text under the image above says, "A neural network is really just a composition of perceptrons, connected in different ways." They all look connected in the exact same way to me. 

Okay, think of it like this. In machine learning algirithms, such as linear regression or random forest you give the algorithms a set of features and the target and then it tries to minimize the cost function, so no it doesnt learn any new features, it just learns the weights. Now when you come to deep learning, you have atleast one, (almost always more) hidden layer with a set number of units, these are the features that are being talked about. So a deep learning algorithm doesnt just learn the sets of weights, in that process it also learns the values for hidden units which are complex high level features of the trivial data that you have given. Hence while practicing vanilla machine learning a lot of expertise lies in your ability to engineer features because the algorithm isnt learning any by itself. I hope I answered your question. 

Yes, it might not be exactly natural Language Understanding but CRF is an excellent algorithm to train Named Entity Recognition tasks and is the stamdard model used by Stanford NLP group. You can try out their NER tagger here. If you want something that accounts for language understanding then there are certain papers who have trained recurrent neural network architectures such as LSTM and Bi-directional RNNs. Look at this paper. I must tell you that Named Entity Recognition is an incredibly hard problem and if you want to use Deep Learning architectures, it will require tremendous amount of data. I will suggest try out the Stanford NER tagger, since your data doesn't have much of a sequential nature, I am certain it will perform well. In the end here is part 1 of an excellent blog post that goes in detail about training your own NER model using the Stanford NER tool. 

I'm not sure if this is the type of analysis you are after, but you mention that the visual side is restricted in STATA. A colleague wrote a blog that utilised neo4j to read web data into a graph database, and d3js to display the data graphically. I realise you don't have web data as such, but your data can be stored in a graph database, but I guess when I was asking about what types of analysis you were planning on doing, I was asking were you needing a qualitative or quantitative direction. But it seems like you are still in the process of working that out. The nice thing with neo4j is that you can pull the data into R and do any sort of analytics you want on it. 

You could you both forecast and actual weather, then see which provide better features for forecasting? Obviously there will be a lot of correlation between the forecast and the actual weather, but you could use feature selection to choose the most important? 

I learned that Keras doesn't have a built-in way to set a threshold for precision and accuracy when building a classifier. Courtesy of a solution here, I wanted to see what would happen when I fit a simple 3 layer Multilayer Perceptron with different classification thresholds for these metrics. So I went ahead and fit a model with as such: 

Interestingly, if you look at the final Precision and Recall scores, I notice that they didn't change, even with the extreme parameters I set. Basically the same confusion matrix as well. I repeated this for a large set of different settings and got generally the same final Precision and Recall scores. I could past them in here but it would take up a lot of space. One more detail then I'll switch to my actual question. My model is based on the business case where a large precision value is very desirable and recall is not relevant. My question is this -- given that I desire large precision -- is tuning the specific precision and recall thresholds for the model valuable at all, if the final Precision and Recall scores are left fundamentally unchanged across all configurations of the thresholds? I hope this makes sense. Thanks for reading this. 

Java I'd have to disagree with the other posters on the java question. There are certain noSQL databases (like hadoop) that one needs to write mapreduce jobs in java. Now you can use HIVE to achieve much the same result. Python The python / R debate continues. Both are extensible languages, so potentially both could have the same ability to process. I only know R and my python knowledge is quite superficial. Speaking as a small business owner, you want to not have too many tools in your business otherwise there will be a general lack of depth in them, and difficulty supporting them. I think it will come down to depth of tool knowledge in the team. If the team is focused on python, then hiring another python data scientist is going to make sense as they can engage with the existing code base and historic experiment code. 

I would suggest using a combination of rvest and rselenium, depending on the way the web page is set up. 

This seems pretty reasonable so far. Then I switched up the thresholds and looked at what outcomes resulted. The table below shows what I got when I changed the precision threshold to 0.1 and the recall threshold to 0.9: 

Say I've built a (completely unrealistic) classification model in Keras that gives me 1.00 accuracy. And next, I would like to use my model on some new, unseen data, and use to get a probability that the observation belongs to class "A". Say this returns to me a 0.75. Am I interpreting this correctly in English: "100 percent of the time, the model is confident that this new observation is 75 percent likely to be class A" ? If this is correct, then let's consider if my model was not totally perfect, like in real life, and instead it gave me a 0.40 accuracy. Say my is still 0.75. Then, is this correct: "40 percent of the time, the model is confident that this new observation is 75 percent likely to be class A." ? If so...this makes it seem like is not tell a complete story. I could mislead someone (say a journalist...or a judge, whoever) by saying, "There's a 75 percent chance this unseen observation belongs to class A"...and that might sound great, if I fail to reveal that this statment was based on a model that had a low accuracy like 0.40. Am I stating this correctly, and does my apprehension have validity? 

When I run with the stackexchange URL, suitably modified code for the different URL, would give a vector of data. The reading I've done about google stopping screen scrapers, suggests that they only stop people who abuse it and usually will do it with a Captcha or similar. Any thoughts on a solution? 

I would set up a sensor as idle, purposefully to determine the decay in the signal. Then try an ARIMA or similar model on the data collected to find out what an approximation of the decay function is. Further to this you should determine if the model still fits if they decay is put into a "stop/start" mode. Afterwards you can apply a corrective function to remove or take into account the decay on your other data. 

Tufte has a wealth of examples on good and bad visualisations. McCandless has some lovely, or even beautiful graphics and ways to display data. 

can be experienced by the network as: Notice how each image has extracted a different edge of the original 7. This is all great, but then, say the next layer in your network is a Max Pooling layer. My question is, generally, doesn't this seem a little bit like overkill? We just were very careful and deliberate with identifying edges using filters -- now, we no longer care about any of that, since we've blasted the hell out of the pixel values! Please correct me if I'm wrong, but we went from 25 X 25 to 2 X 2! Why not just go straight to Max Pooling then, won't we end up with basically the same thing? As an extension the my question, I can't help but wonder what would happen if, coincidentally, each of the 4 squares all just happen to have a pixel with the same max value. Surely this isn't a rare case, right? Suddenly all your training images look the exact same. 

Yes, you are exactly right. 0.5 is just a heuristic, ROC curve and precision-recall curve give a much better idea of what the cut off should be. You can then use predict_proba, extract the probabilities and do the classification based on the cut-off you have inferred from ROC curve and. precision-recall curve 

Your reviews column is a column of lists, and not text. Tfidf Vectorizer works on text. I see that your reviews column is just a list of relevant polarity defining adjectives. A simple workaround is: 

A context window applies to the number of words you will use to determine the context of each word. Like of your statement is "the quick brown fox" a context window of two would mean your samples are like (the,quick) and ( the, brown). Then you slide one word and your samples become (quick, the ), (quick, brown) and (quick fox) and so on. I would suggest reading up this word 2vec tutorial to understand the training method and terminology.