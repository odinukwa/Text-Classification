If you are browsing in Excel and want to be able to see dimension attribute members that do not have corresponding rows in a fact table (measure returns nothing), there is a pivot table setting you can use. Access the PivotTable Options dialog box by right clicking on the pivot table and choosing PivotTable Options. On the Display tab, check the box next to Show items with no data on rows. This will show all values of whatever hierarchy is in your rows regardless of if the measure in the Values section of your pivot table returns a non-blank result. You can do the same thing for columns by checking the item underneath the one indicated in the image. This option must be set for each pivot table and will apply to all fields in the rows (or columns) when you check the box. 

Since I can't see your table structure and I don't know the size of your data, here's something that will return the right answer but may or may not need to be tweaked for performance. You can write a query that groups both created and closed tickets together by date. Although you could just write two queries and do a full outer join, I would guess you want SSRS to accurately represent days with no ticket activity in addition to filtering the data for the date range at the source instead of in the presentation layer, so I would suggest creating a date table/CTE/query. If you already have a date table, use it. If not, here is a way to create what you need for the last 30 days. You could also make it a stored procedure and parameterize it to choose the number of days. 

Can (and do) people successfully deliver projects that do this sort of thing? Unfortunately, yes, they do so reasonably often. Is this a good approach? No, it's not. You're basically taking your relatively expensive database and turning it into a relatively slow file system. If you really want to build a system that saves its state by serializing and de-serializing objects, you may as well use a file system rather than using a database. If you build systems that store data by serializing objects into the database, you won't make friends with your DBA. You'll end up storing redundant data. You'll end up with terribly inconsistent data-- any time shared data is updated, some objects will end up with the new values and some objects will end up with the old values. You'll make it impossible to do any sort of reporting on the data-- everything that anyone wants to do with the data is going to require someone to write additional code. That's a huge, huge issue in most enterprises because they want to do things like extracting data from one system to load into another system or to have a reporting system that can deliver reports from multiple front-end applications. Plus, as you point out, you're constantly going to have to deal with issues when you're evolving your data model. Are there advantages to this approach? I guess you can argue that it's pretty easy to implement the first version of the app. And it lets the developer completely ignore anything related to properly interacting with a database. I'm hard-pressed to imagine many cases where these advantages outweigh the numerous downsides to the approach. As for how you should deal with this particular professor, that's a separate issue (and one that's probably out of scope of this forum). If your professor is actively developing projects in the real world, he's probably not going to be terribly receptive to any argument from a student that his approach is fundamentally wrong (even if the approach really is fundamentally wrong). You may be better served doing your project the way the professor wants and learning the proper way to save data on your own (or in a different course). 

You can put your cube formula in a separate cell and reference the cell where users type in the name they want. For example, if you have users put the branch name in cell B2, you can put this formula in another cell. 

This will give you the desired answer based upon the example data you provided. You didn't answer my question about the number of levels, so this assumes there are no more than 3 levels. 

Date dimensions are pretty standard in a data warehouse, and are highly recommended by Kimball as most facts tie to a date. Typically, the key is an integer. It can be a meaningless surrogate key, or it can be a "smart" key where the integer is in the form yyyymmdd; e.g., the key for August 2, 2014 would be 20140802. Date dimensions provide a set of contiguous dates in multiple formats and allow you to do date calculations once rather than in each query. They make it very easy to do time period comparisons. You can add other fields that could be analytically relevant such as holidays, indicators of work days, fiscal calendars (where different from standard calendar years). There are lots of scripts available online to create and populate a date dimension. Many tools will create the date dimension for you. I'm not sure what you are using for your underlying data source, so here are a couple of examples of date dimensions. Hopefully you can convert these to the appropriate format for your needs. 

If you have a simple b-tree index on , then yes, you would need to avoid calling functions on that column in order to be able to use the index to filter rows. In this case, it would seem to make much more sense to convert your numeric literals to timestamps than to do the reverse 

In general, you shouldn't rely on the to compare two different queries. is only intended to be used internally by the optimizer in ranking different plans for the same query. It is entirely possible that one query with a cost of 26 is more efficient than a different query with a cost of 15. 

Deleting data isn't expected to change the size of the table segment. It will create free space in blocks that are part of the table segment and it will probably create at least a few empty blocks. That free space will be made available for subsequent operations on the table (assuming you're doing conventional-path inserts) as well as operations that cause the size of rows to increase. If you want to actually shrink the size of the table segment (generally not a good idea if you're just going to insert more data into this table in the future), depending on the Oracle version, the type of tablespace, and whether you've enabled row movement you can probably shrink the segment 

Normally, you'd specify an or that was just before the first erroneous command. In general, though, you would try to avoid using RMAN to recover from human errors. Generally, doing a full database restore is a last resort and generally there are other transactions going on that you don't want to lose. If a DBA inadvertently drops a table, for example, you don't want to restore the database and lose all the transactions that other people had made after the table was dropped. You would generally do a 

The validity depends on how users want to look at the data. You are looking at it as just a transaction fact. Other types of fact tables include periodic snapshots and accumulating snapshots. If you want to see all the times that someone corrected a row to help decrease erroneous entries, the effective dates may be appropriate so it's clear that the transaction was updated. This creates a fact table that is somewhat similar to a type 2 SCD. The Kimball Group has an article that directly addresses your question. Here's a Kimball Group design tip that talks about effective dated accumulating snpashot fact tables. You may be correct that you should just add transactions that reverse the original row. That sounds like it could be a valid solution if you just need to see the transactions and sum them up. This is how I've seen most accounting data work. Kimball says the effective dated fact tables may be useful for quickly calculating account balances at a paritcular point in time, especially for tracking slowly changing balances. But this is a fairly rare case. I think your concern that it will be confusing for users is also valid. You have to decide if that can be overcome by education and if its worth it for the added analytical capabilites in the data. I haven't had to do this much in my data warehousing experience because most of my facts were simple transactions or periodic snapshots. But I have created several effective dated bridge tables for many-to-many relationships. 

It depends on your data (and this assumes that you don't care about the different behaviors of the two data types). You can put together a test with sample data that is similar to what you're actually storing to find out what approach will use less space. I'll create a table with two different columns, one declared as a and one as a . 

Are you trying to test the database? Or the application? Assuming that the Oracle database is configured correctly (i.e. it is in ARCHIVELOG mode, backups are done regularly, DataGuard is in place depending on your recovery requirements, etc.), by far the most common source of problems in a failure is that the application itself has not defined its transaction boundaries correctly. The classic scenario here is a banking application that wants to make a $50 payment from account A to account B debits A by $50 in one transaction and credits B that $50 in a separate transaction. Unless you happen to be able to test what happens when system fails after the first transaction commits and before the second transaction commits, you won't see that the transaction boundaries are incorrect and the application might inadvertently lose $50. 

Oracle data dictionary views that have the prefix provide information about objects that your user has access to. Views that have the prefix provide information about all objects in the database. And views that have the prefix provide information about objects that you own. In this case, does not and can not have privileges on the indexes owned by because there are no privileges one can grant on individual indexes so it makes sense that would not see the indexes in . If were granted access to or (either individually or via the privilege), you could query (or ) to get a list of all the indexes owned by .