Reading the article "Dynamic Search Conditions in Tâ€‘SQL" I saw the following statement in the "Using sp_executesql" section. 

Would this be the correct approach to doing this or is there a better way to both return the resultset (without included) and update the 2nd table based on the returned rows? 

(Plan XML here) Have I stumbled upon some kind of bug or am I doing something wrong on my end that I should be doing differently? (P.S. I know what the warning means and how to fix it, I am more interested in the warning showing up in one place but not another.) EDIT: Here is the version information for my SSMS from the "About" help page. 

From what I have gathered the 256 limit is a hard limit for "supported configurations", once you go over 256 some features may fail to work. For example I have a database with 629 merge articles and it works fine, but if I add a filtered article to it it will blow up with a error similar to this error "Message: Too many table names in the query. The maximum allowable is 256." when you try to build the snapshot. 

When working with virtual machines often it can be very useful to have a base read-only VM then have several smaller VM's that use that larger VM as a base and write their changes to their own writeable copy. Is something similar available in Microsoft Sql Server? The situation I am in is we host a demo copy of our product that our clients can connect to for about 30 days or so. When we create a demo account we have to create a new copy of the base database the program uses and have the software point at that. Each image is a little over 2GB, but on average the demo users will only change about maybe 100MB worth of data in the database. What I would like to do is have a read-only database that acts as a base then have the demo databases create a "differencing database" and write out it's data and log information to that differencing database. Is this possible? I have been searching through the MSDN documentation but I have yet to find anything and unfortunately google searching has been fairly useless as searching for gets polluted with all the pages about doing differential backups (doing did not return any useful results either, all I got was information about differential deployment scripts). 

The problem is from in the first outer join. can contain text in the ENTRY_CODE column, however all records that have set to the line number from where will always be numbers only. I perform the cast as may or may not have leading 0's and spaces so I am trying to get them to be well formed. What appears to me to be happening is adding to the outer query causes the clause not to be evaluated before the cast to int in the inner query. I have tried things like adding to both the inner and outer queries but it has no affect. 

making those 3 columns I am interested in the clustered key. However this violates the "Keep the clustered key narrow" and the "Keep the clustered key sequential" guidelines I have read everywhere. Which of these approaches is the correct way to implement this? If neither are correct please tell me what the correct way to approach this is. 

My Question: How do I, from the publisher, mark a pull merge replication publication as having a invalid snapshot such that if I did the column would return 0. Doing will cause the subscriber to re-initialize but it does not mark the snapshot as invalid. I know I could change a publication or article property then change it back and cause to invalidation to happen that way but I would really like to invalidate as the "primary action" instead of having the invalidation be a side effect of some other action. I am looking for something similar to transactional replication's procedure which has a parameter, but I could not find the equivalent for merge replication. Is there any way to invalidate a merge replication snapshot only without making some other kind of change that has snapshot invalidation as a side effect? 

Yes, however this depends on the page count. If the page count is too low then the index fragmentation will always be on a higher side and it wont effect the performance. In our environment we consider Page count > 500 during the index fragmentation analysis. Re-organize when Index fragmentation >5% and <30% Rebuild when Index Fragmentation > 30% The above numbers (%) is not a standard but yes its widely preferred. 

Index maintenance is very vital when it comes to query performance. So it is very important to have it properly planned and implemented. Along with Index maintenance comes the ballooned log file which is the result of Index rebuild and re-organize. Now planning for Index maintenance is important as it might just trigger another issue of 'Transaction full' unless proper precautions are taken. It depends on you how you want to do the index maintenance, however the solution from Ola hellengren is the best and freely available. Link below : $URL$ With above solution you get multiple options and all can found in the website. Coming down to your queries: 

With the points mentioned , it seems your database is already connected by some login and since it is in Single-User mode, you are not able to access it. 

Azure SQL (PaaS) is not yet fully mature and there are few features in the On-Premises that will not be supported. IaaS will be much suitable for existing application for migration as you will have full control on moving the existing functionalities. Here are 2 links which give a better comparison of the two offerings: 1.Microsoft Documentation 2.MSDN Blog 

Do not use Index rebuild ALL but individually.Index maintenance has to be followed-up by regular T-log backups. This helps in controlling the log growth. Have a rough estimate of log growth and the log drive ready with sufficient space. Follow the best practices for transaction log management and Index maintenance will never be a problem. 

For excluding a database from the backup script in Ola Hellengren's solution, you will need to add it like the below: 

This is how SQL Server works and there are limitations to make it perform better as the Page limit is 8KB for all versions of sql server. Pasting something from msdn that will be helpful : $URL$ Row-Overflow Data Exceeding 8 KB A table can contain a maximum of 8,060 bytes per row. In SQL Server 2008, this restriction is relaxed for tables that contain varchar, nvarchar, varbinary, sql_variant, or CLR user-defined type columns. The length of each one of these columns must still fall within the limit of 8,000 bytes; however, their combined widths can exceed the 8,060-byte limit. This applies to varchar, nvarchar, varbinary, sql_variant, or CLR user-defined type columns when they are created and modified, and also to when data is updated or inserted. 

In SQL Server restricting to view just one database is not possible. Either you see everything or you do not see anything (Except master and tempdb).Hiding all databases can be achieved by below steps : 

Since the recent patch where TLS 1.0 was disabled and 1.1, 1.2 Enabled , we are having issues where the SSRS in the server cannot make connections to the Database server. 

Patch the passive node first. Move the resources to the passive node.(This will become active now) Patch the current passive node.(It was originally active). 

We use the entire Maintenance solution provided by Ola Hellengren i.e.Backup, Database Integrity and Index Optimization. What we do is create a new database for DBAs and then use it deploy the maintenance solution. This keeps the system dbs separate from any other user /DBA created objects. Just need to change the script a bit for that (Use databasename). 1.Important parameter Values which I need to modify? For Index Optimize check the SP and configure it according to your need for Index fragmentation in the script itself. 2.Where will these commands create the SP and related Command Execute and Command Log, all in Master Database or MSDB? Yes, if you use the script as it is it will use master to create all objects. This can however be customized as per your requirement. We do use a different database to get this deployed. 3.Series of code execution - Which code I need to execute first? Once the solution is deployed ,it will create jobs and then you can schedule it from there. This is the best available solution with lots of freedom to customize. Once you start using it, it will reduce a lot of burden from your daily activity.