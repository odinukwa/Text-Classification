Please be more specific about what you're trying to do. The WAF is a feature that protects internal web servers. As such, you MUST set it up correctly in order to gain access to your internal server. This usually requires that you do the following: 

VPN stand for Virtual Private Network. It's a way to connect remote users and networks to an internal network in a safe way. It usually includes authentication as well as connection encryption. There are two main types of VPNs: site to site and remote access. Site to site is used to connect whole networks while remote access is typically used for, well, connecting remote users (i.e. single machines). So yes, VPN is the best way to got for what you want to do. In practice, what you should do is setup a VPN endpoint at the edge of your network (most firewalls have an option for this) or inside you DMZ (if you have a larger infrastructure) and have users connect to that VPN system. From there, you can setup what they can have access to and how. More sophisticated system also uses SSL connection for tunneling RDP or other remote control system. They work in a similar manner although the details will vary: users connect to a (usually web-based) external interface and then tunnel through that server to reach the internal machine. In all cases, you should setup the VPN endpoint directly on your production server but use an different machine for that. From there, it all depends on how secure you need your system to be. It's not uncommon to require users of higher-security systems to connect to an SSL VPN server, use it to connect to a "jump" server and then RDP from there to the final production machine. If you want specific examples of products that can help you set this kind of things up, I can give you some references of what I used. Edit: I know of 3 products that willl do what you specifically want well: Citrix Metaframe (expensive and suiited for large deployment, probably not what you're looking for), MS's own remoteApp system (not vers well suited to secure deployment and frankly probably too complex to setup for you need) and Sophos UMT. I suggest you use the last one: it's a specialized Linux firewall distro that includes all you need. There is a "free for home users" version that has all the features you need. For commercial use, however, you'll need to purchase a license. If you feel like investing more time into the project, you can also do is yourself using OpenVPN server on any Linux system for free. 

Edit: actually, there is an even better way: First, you need to isolate the app in its own apppool. Then you just issue the following command: %windir%\System32\inetsrv\appcmd.exe recycle apppool "MyApplicationPool" (use %windir%\System32\inetsrv\appcmd.exe list apppool to get the name if necessary) That should recycle the app gracefully (unless it's really stuck). 

A certificate will only be valid for the exact host name it was created for. That's the "CN" (Common Name) part of the "Subject" field in the certificate. The typical way to deal with what you're doing is to buy a certificate with another field called "Subject Alternate Names" where you can list other hosts name that should be considered valid by the client. These certificates typically are a bit more expensive (although not always). 

Looks like you're having a problem with HTTP compression: it doesn't seem to be working correctly. Try going into the IIS web site and disable compression for both static and dynamic content. 

At the command prompt, start the system configuration tool (sconfig) and check if you haven't switched the server to "core" mode. If so, you can re-enabled it. Alternatively, you can start powershell and run: 

First, check that you have properly registered the new FQDN in the DNS: does it resolve to an IP address that is assigned to your server ? Now, assuming you're sticking with the default port 80 for the binding and if DNS is working correctly, then did you assign a different IP address to each web site ? If not, then you will have to setup the "hostname" property of the HTTP binding of each web site to the FQDN that you want it to use. 

It really depends. You didn't specify what OS you're talking about but, under Windows (and I suppose, Linux as well), the TCP window size is typically handled by the TCP stack. However, it is perfectly possible for some non-OS component to affect that as well. For instance: - An application can influence the behavior of the TCP stack by specifying a non-default buffer size for the socket (SO_RCVBUF). - An application could be using raw sockets and reimplement TCP in user mode (not sure why you'd want to do that, but it's possible). Finally, you should know that getting a windows size of 0 is actually a normal condition: it indicates that one of the party already have a full data buffer and that it needs more time to process it. 

FTP uses more than a single port. TCP 21 is for the command channel only. When you do a directory listing, you're transferring the result through the data channel. Since you're using passive mode, the server will open a random high port (> 1024) for the data channel and the client must open a second TCP connection to that port. Typically, right after the PASV command, the server should answer with 

I have an application in a Citrix/2008R2 that is acting as a user starting point for other apps. I have recently had to modify this application to integrate it more deeply with active directory. As part of this process the application sometimes has to run the user login script directly (instead of counting on the login process to do so). That script is written in powershell and, among other things, maps the users drives based on what OU the account object is in. I had to modify the script to take into account users who had to chose their work environment when they log in and therefore cannot use the standard login script. Anyway: my issue is that, when the script is started from a DFS UNC path from my application, it fails to run, asking the user to confirm its execution. I checked the execution policies in both 32 and 64 bits powershell and they are like this: 

How do you plan on accessing the table data ? If you're using something a bit flexible (your own code or simply SQl queries) then an easy way out is to copy the needed table to another database and the detach it from the server. Afterward, truncate the table (or delete the unwanted rows, although you might want to check your T-Log size of you do that). When you'll need it, you can always remount the relevant database from storage in read only mode. If you need to run queries on multiple backups at once, you can create a view regrouping all the data. If you're using a software that won't allow you to customize how that table is accessed, you can still copy the table to another database on its own but restoring that backup can be problematic because you'll need to merge the existing context (the other table states) with the old data. Plus, the data structure might have changed making it next to impossible to restore. So in that case, I would suggest that you simply take a backup of the full database and store that (preferably along with a version of the software that accesses it). 

I would do two things to try to fix the problem: First, remove the AV. Completely. Don't just disable one part or another, uninstall it. Second, assuming it still fails from time to time, change the NIC and cables. 

Frankly, I don't think you'll find any product that does what you request, mostly because the best way to handle bandwidth management is at the network layer. In your case, I would setup a QOS rule at the perimeter and define bandwidth reservation for the specific path you want to prioritize. 

Sounds like you have a hardware issue that causes the machine to fail to load the BIOS. The "blue flashing light" sounds like the server identification light. It's just there so you can easily identify the physical machine when you're on the OOB management board or RDP (or when you're in the front and want to be sure you have the right server from the back). Advice: Call dell support. 

AFAIK, it's not possible natively with TS and RDP. Writing a virtual channel handler for doing so shouldn't be too complex, though. Do you know any programing ? 

It looks like the client certificate you receive does not have the expected properties. Specifically, it looks like it's subject canonical name field isn't matching the expected "proxy.salesforce.com" In your situation, I would setup a tcpdump on the external interface of your reverse proxy waiting for a connection from 96.43.148.8. I'd then feed the result of that trace into wireshark so that it would parse the SSL handshake and allow you to grab the subject.cn of the certificate used for SSL client authentication. That should give you a good indication of what is failing. 

First, try issuing a "truncate table log" command and check again. While you're at it, run "select * from sys.all_objects where type = 'U'" to make sure you really have a single table there. Finally, make sure your table has a primary key: tables without primary keys have the bad tendency to grow without limit even if you delete elements (they just mark on ones as deleted and add new ones at the end). 

I have made sure that the group policy applied to both the user and the computer should allow all script to run: For now, I have resorted to making a copy of the script to the temp folder and running it from there (it works fine) but it is nagging me: I can't see any logical reason why powershell would refuse the execution of that script. Anyone has an idea what is happening here ? edit To clarify, I am experiencing the common powershell security warning here: 

Dealing with these types of problems can be difficult because the logs usually tell you only part of what is happening. Your best bet at this point is to use a packet capture and analysis software (like the most excellent - and free - wireshark) to see what is really going on the wire. Make sure you capture all the traffic between your machine and the remote system and try to see what difference you can spot between a working connection and your code. That might give you an indication where the problem lies. Do pay special attention to the way the TLS connection is negociated because it might point out to the actual issue (there is a pretty nice and simple to understand document that can be found at IBM's). For instance: 

First, I suggest you verify that the application behavior matches the documentation: type in an elevated command-line and you'll get a list of open ports with the associated executable (if possible). Once you have that, make sure your rules matches the actual listening pattern of your application. An alternative would be to open the ports not based on port number but based on the executable itself. That might make it easier to manage. 

It should bring the DB online. Read the integrated help on the restore command. It really contains information you cannot ignore. 

Design a way of generating and maintening your certificates. Typically, that means installing a CA and using it for signing user-generated certificates. It's a rather complex topic but you might want to start by installing and understanding the "Certificate Authority" role in Windows. Do NOT install that on the same machine as your web server, though. Implementing a Client certificate authentication rule in IIS 7.5. That's actually not too complex to do. 

(alternatively, you can just download and use AutoLogon) I'm sure you can see the potential issue here so I won't elaborate. Your second request is pretty easy to do with a batch. Here is a sample that uses blat: 

(where 1.2.3.4 is actually the IP address of your destination machine) Save the file, and you're done. Note, however, that this will just change the IP address of the machine that you'll connect to when using the dns host name. It will not change anything in the content of these connections so if you're using a protocol like HTTP (or SSL) where the host name is embedded into the data, the target machine might not respond properly. 

You can use the administrative share to access the C drive directly ( \\xxx.xxx.xxx.xxx\c$ ) but if your problem is low memory, deleting file will not help: you'll need to reboot or terminate some processes in order to free up memory. 

There is a known bug with task scheduler that will cause it to run the same task twice from time to time. It happens when the previous scheduled task terminates exactly when the new task is supposed to start: the scheduler will then run the same task twice. There is a hotfix available there: $URL$ When we ran into that problem, we fixed it simply by making the task run less frequently. 

Your firewall is most likely a transparent SMTP proxy for filtering spam and viruses. Add a PTR record for the firewall's external IP. While you're at it, make sure you're updating your SPF record (if any).