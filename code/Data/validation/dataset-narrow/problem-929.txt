I have a rather odd request. We have an Amazon VPC that is the primary network that my company controls. I have a client running a physical computer on a separate network in another part of the country. We do not control that physical network, so at this time I cannot install a site2site VPN over there for this project alone. That remote client computer is running OpenVPN Client and connecting successfully into our VPC. However, here is the problem, I need to send packets to the client since it's running a hardware dongle that (obviously) cannot run in AWS since it's hardware. It's essentially running a single service that needs to be talked to but I cannot initiate a connection into a client. I do realize that Amazon has direct connect and VPN services to connect a physical LAN to a VPC, however that isn't really possible in the scope of this project. I can easily create a new VPN server in the VPC if that's a requirement OpenVPN's client-to-client is the closes thing I can see, but that routes within the VPN server, so I don't think that will help because the machine initiating the request isn't a client of OpenVPN, it just sits on our VPC. One thing to note is that the connection to this machine is only lightweight traffic, no heavy bandwidth is needed. Is there some sort of tunnel that I can create? We would have to jump through some serious hoops in order to get the Gateway/Firewalls/Routers reconfigured at the remote site so that is an absolute last resort. EDIT: I wanted to add this for clarity: $URL$ The image on that page pretty much sums up what I need to do, however the PC on the right side isn't connected via OpenVPN, it's already on the same LAN/subnet as the OpenVPN server. 

If you provided your server config it would really help out. Summary: Ensure your server.conf is configured to assign addresses properly, and it's configured to push your desired routes. 

Route 53 is not required to host a website in AWS nor is it required to have name resolution to a service in AWS Wherever you currently manage the DNS records for your domain will be fine. Just point them to the IP or host address of the AWS site/service you're connecting to. 

Now creating the routes is easy, what I don't understand is the DNS settings I need to create in order to allow the machines to properly access the KMS activation server. Any help is appreciated, thanks. 

Would like to forcefully kick a specific user connection. OS is Ubuntu 16, OVPN Server is OpenVPN 2.3.10 I can see them connected in which lists out current client status. And their persistent connection pool is saved to I can kick all users by simply cycling the openvpn daemon however I want to kick ONE single user. I've tried and as well as searched google but not seeing anything. OS is Ubuntu 16, Server is OpenVPN 2.3.10 

Docker service kicking you off SSH doesn't really make any sense. Do you have any iptables rules configured? Any other steps you're taking after provisioning the system? There has to be more to this story. Anyways -- You mentioned you tried on Ubuntu 16.04. Here is direct out of my notes and has worked perfectly for me several times on Ubuntu 16.04: 

API returns Run garbage collection manually if you don't want to wait for its next scheduled run: Example if running registry as a container: Garbage collector deletes the associated blobs and manifests from disk for you. 

In the server config file you utilize the item to push out routes to the clients. Ensure you have the routes defined in your server.conf. Here is an example: 

I'm following this guide: $URL$ Which works great! My backups are saving to s3 without issue. However I would love to put an S3 lifecycle policy to purge backups older than 14 days. This is easy enough to do, but there are some other items in the bucket related to gitlab backups (restore instructions, some configs, etc) that I never want to purge. These can be easily skipped by only including backup files via S3 lifecycle policy "prefix" rule which only purge files with a certain prefix in their filename. Great, however the prefix of the gitlab backups using have a dynamically changing prefix. My question: Is there a way to change the file name format of the backups created with ? Or is there a way to alter the Lifecycle policy to take a suffix rather than a prefix? Any suggested alternatives to meet the goal is appreciated as well. 

This question is about KMS activation over a different network from the one my workstation is connected to. The software I am dealing with is Windows 7 Enterprise and MS Office 2013 Scenario I manage a LAN for my company. We have our own internal AD and DNS/DHCP servers. However, Our physical workstations are owned and managed by a Corporation and they handle licensing of the workstation software and manage the KMS servers. I can physically plug the workstations into the CORP LAN and Domain and force activation, but this is basically the worst case scenario. My Goal: My Goal is to create a constant static route to kms.corp.com through a router that is connected to both networks. I want the workstations to then have DNS records that automatically point to the proper host for KMS activation. I drew a diagram of the scenario I would like to have: 

At this point I can confirm the blobs are completely deleted from disk and I can no longer call image details (like in step 1 above) so I thought I was done. However, when running: my associated repo still lists (even though there are no images within it)! Obviously it cannot pull or be used, but how can I fully remove that repo from that list now that it has no images associated with it? 

^^ this works, but NO, not granting full su permissions to this user. Since it works when globally allowing I'm assuming it's somewhere in my syntax. Edit - In order to get around this issue I just put the command I want to run in a script and granted sudo to the script. However this is not ideal and adds an extra step in this workflow. Would much rather be able to run the command directly with sudo. 

EDIT - I wanted to add some more info on how the registry looks before and after the above deletion takes place. Before the Delete Operation Above: 

If ran as an active check: FAIL So, everything works perfectly from the command line, however, in Nagios console, it fails! 

Since you mentioned Python, I assume you're using BOTO. Check the boto documentation, you can even filter on certain tags: $URL$ Noted on that doc page: 

Lots of experimentation and trial and error. However here is a tested working solution when restoring my Ubuntu 14.04 MBR image to a 4TB disk: This should also work if you're just trying to convert a bootable Ubuntu OS disk to GPT (just skip step 1), however in that case ensure you have a backup!!!! 

1 AD domain. 3 total Domain controllers, all on 2008 R2 The primary DC (with FSMO roles) is being moved to a new office and it will be offline for a maximum of 3 days. This DC also serves DNS but the other DC also has that role. I am planning to migrate the FSMO roles to secondary server, and then just simply power this one down and boot it back up at the new site. Currently not planning on taking any other steps to prep for this server move. Once the new office is ready and the DC is booted back up, I plan to allow synchronization from the other controllers, and then re-apply the FSMO roles back to it. My question is -- Is my method ok? Is there any other steps I need to take to plan for this? 

I'm trying to find the history of container restarts. Of course the field on a will show the current uptime. However if I have a container with a restart policy such as and it's gone through several restarts - How can I check that restart/uptime history? If the docker engine doesn't natively track this - is there a known good method to handle this? 

This means that the packet sent to NRPE would have a source address of 172.20.0.2 (which is the Docker container IP, within the docker bridge network). If so, how would it make it through the firewall?! This doesn't quite make sense, and I'm a bit stumped Of course, by setting in the NRPE config gets around the issue, but that's not persistent and doesn't truly solve the issue here. Does Nagios send what it 'thinks' is the "source" IP in the NRPE packet, and that's what NRPE judges the "source" address from? If so, how can that be altered? What am I missing here? My goal is to put the Docker host as the allowed_host as I know that's static and won't change. 

when developing Bash scripts, sometimes I'll run it in a Docker container, or on a VM to test it, but the ideal place for me to build out scripts is just on my local MacOS workstation. When it comes to Bash scripts, this has never been a problem so far. However today I noticed that the command behaves differently between Linux and MacOS. Example scenario, adding days to a date object: Linux: 

Graylog v2.3.2 My goal is to have a condition raise an alert, and the alert remain open until it's marked resolved or a defined resolved condition applies. I have an alert setup and here is the condition configuration: 

In the above example, the Subnet for clients will be 10.10.18.0/24 It looks like, from your log you are pushing a subnet (192.168.5.6/255.255.248.0) so that may already be configured properly. Routes: The errors are specifically complaining about the routes. Your log shows 0 out of 0 total routes were successful, so it sounds like it doesn't even have any routes to push too. 

Firstly, I am using these as a base guide: $URL$ $URL$ Secondly, our region doesn't support the AWS directory service Scenario I am looking to create a "base AMI" for a set of application servers we have. I may need to deploy 1 or more new servers based off of this image. The instances are originially created from EC2 Windows Server 2008 R2 Datacenter base AMI (Created by Amazon) The applications themselves are static, licensing is completely redistributable, and the config doesn't need to change on one machine vs another. The ONLY thing that is different from one machine to the other is the hostname and network config. Network config is handled by EC2, so the hostname is really the only thing that needs to change from once instance to another The machines are part of Active Directory, and have specific OU Group Policy rules applied to the machines. They will all join the same OU. Goal: My goal is to have a base AMI. When this AMI is launched it auto-joins the domain OR is already joined to the domain. The applications that are ran from the machine REQUIRE domain accounts to run the Windows Services. So I can't have an image that's not joined to the domain. An idea I had (Will test this tomorrow):