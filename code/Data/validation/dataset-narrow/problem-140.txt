If your ISP routes packets but you have a dynamic IP address, perhaps all you need is any one of the dynamic DNS services, whereby your systems register the current address. If the incoming packets aren't delivered because of the ISP's policies or its NAT configuration, you might care to search for "vpn services static ip address", and you'll find many companies sell this exact service you describe. Be aware that some jurisdictions have considerable restrictions on what's legal. 

Certificate login You must have a certificate-based login available on your router, as you want the configuration to be entirely automatic. (Doc) 

I think the short answer is that usage varies and is really a matter of emphasis. RFC 1122 Requirements for internet hosts says "A host computer, or simply "host," is the ultimate consumer of communication services. A host generally executes application programs on behalf of user(s), employing network and/or Internet communication services in support of this function. An Internet host corresponds to the concept of an "End-System" used in the OSI protocol suite." The corresponding RFC 1009 Requirements for internet gateways gives this definition: "A gateway is connected to two or more networks, appearing to each of these networks as a connected host. Thus, it has a physical interface and an IP address on each of the connected networks." The general idea of host as being an actual target of communications seems the most usual, but purposive, description. For a network engineer, a router or switch or UPS might well be a host -- a thing to be talked to. For a person using web banking, routers are just plumbing. Kind regards Jonathan. 

Regarding hubs: when we say "devices who simply repeat the input signal to every output port", it's important to realise it's done essentially with a transistor: it copies voltages from one place to another. Pretty much everything else -- bridges, switches, routers -- have a CPU inside them which gets bits, interprets them in various ways as addresses, and decides where to send them out. The obvious technique is to put the ethernet frame into a memory queue, figure out what to do, then emit it in the right way. There is therefore a measurable delay between the input and the output. In contrast, the output signal on a hub is following the input with sub-bit-time delay. Clever switches can start forwarding before the input has finished; stupid ones are store-and-forward. You normally have to put an oscilloscope on the signals to find out exactly what a given switch does. 

This big-small-big-small is characteristic of fragmented fragments. Normally reassembly is only done at the final host, or a host doing security or NAT. (Note that the fragements can overwrite other fragments, which has had some security implications. I believe most current IP stacks will discard such funny-fragment packets.) The IP layer discards the bad packet, perhaps sending ICMP problem report. It never gets to the UDP-handling layer. I do recommend reading RFC 791 on fragmentation and reassembly. $URL$ PS. Your example table has some assumptions: 20-byte headers, and that all these packets had the same destination address. 

We see 60 bytes for the exact reason you said: minimum ethernet frame of 64 bytes with the 4-byte FCS removed. Jonathan. 

Obviously you could do any configuration you like with this method. You might also consider source-based routing into a blackhole rather than straight blocking. 

The usual way would be to use two edgerouter sockets as a switch and connect the two switches directly. If that's not practical for wiring reasons, then you chain the switches together as has been already suggested. I'm guessing there are no significant performance considerations. If you're after resilience of cable/switch breakages, read up on spanning tree. 

As a given host has a maximum of 2^16 ports (in IPv4), it can have at most 65,536 simultaneous connections to a given web server on the normal web port (minus some ports which have special meanings). That's not a problem for an individual's computer, but might be for a large web proxy. (And a given operating system might well have other much smaller limits: the limit I'm speaking of is inherent in the protocol limit.) Hope that helps clarify somewhat. 

In a small room, say 3 m wide, with radio travelling very close to 300,000,000 m/s, it takes about 10 ns to cross the room. Each time it bounces off the wall it will be very much attenuated. Perhaps it takes five bounces before it's entirely insignificant, so that's about 50 ns. It's pretty brief. 

You can also consider a second email server, which accepts mail and forwards it to the primary. It makes the routing problem trivial and the mail problem a little harder: depends which is easier for you and your operating system, and where you see your risks that you want to minimise. Addressing sending: easiest is again on the server, by having a script check reachability and manipulating the server's routes. Of course, the proper internet design is to do this at the ISP's end, but you normally can't get rerouting done except for large address blocks. If both routers are connected to the same ISP they might have a way to help you; but you don't get much resilience if they both are on the same ISP. Hope that helps Jonathan. 

The traffic is surely the netflow export UDP packets you have configured. It would normally come from the inside interface, hence the inside address. Use to change the source interface. Yes, allow traffic from the inside of the router to the server, otherwise your server will not send packets to and from the router, such as logging, configuration, NTP, whatever. Unless of course your intention is to really separate this server somehow. 

gives the static route and the two interface routes marked for connected. shows the local addresses. 

I've no experience of them, but searching for "vsd file converter online" and "vsd to svg" yields a few promising results including 

The main reason for putting your equipment in another company's data centre is to take advantage of all the infrastructure: power, cooling, networking, physical security. Secondarily it can be advantageous if it's near something it has to communicate with a lot, such as web servers near database servers and web servers close to customers. The trade-offs between owning equipment in colocation or renting by the hour in the cloud can be complex, but I'm sure you've considered them. These are all server-related issues best dealt with elsewhere than this forum. The main reason for having your own ASN and doing the appropriate border routing is if you have a large amount of network, with multiple peering to other networks. None of which is necessary for a few servers, and if you're new to it, it's a lot of learning and work for no particular gain. It might be helpful to read a peering guide from an exchange in Amsterdam and a peering tutorial. I'd suggest you consider just using the networking provided by your datacentre, getting multiple redundant ethernet connections, and have your autonomy at the DNS layer. To directly answer your question, you'd need a router, not a switch, at the border. 

A well-known search engine suggests it's a quote from RIPE describing the network design for Atlas, its internet monitoring network, which has lots of probes out in the network. It has this diagram (copied per RIPE public-non-profit permissions) 

A property of UDP which is frequently overlooked is that fact that it can work with entirely one-way traffic. Consider two systems T and R. T wants to transmit something, R receives it. Under normal behaviour (with a few preconditions eg no ARP, no ICMP unreachables) T will not receive a single packet from R. Appropriate filtering can then prevent all traffic from R to T, which means that faults/intrusions on R can never affect T in any way. This is an extremely important property in complex systems, which can be used to overcome unforeseen boot-order dependencies, for example. I've seen this used in various vision systems. I've also seen UDP "syslog sinks", where log data is sent to multiple inward-only systems to create multiple cross-referable logs. In some sites you can't even ssh to them, you have to go to the machine room and use the console. It is frequently stated that UDP is unreliable, which makes it sound as though it has inherent non-delivery. This is false: what it lacks is an inbuilt retry mechanism, leaving it to the upstairs protocols (compare DNS and streaming for approaches). Further, under many real-world conditions (decent switches, non-saturated network), you will find 100% UDP delivery. Under some very nasty real-world conditions with very high packet loss, you also find that UDP gets through where TCP often doesn't. For example when packet loss is say 90%, it's basically certain to break a TCP connection, whereas at least some of the UDP packets would get through. Protocols such as NTP and DNS will still work, syslog and SNMP are likely to work well enough to find the cause. 

Routes are done statically at the sites, and by a 20-line shell script in on the hub machine. If the centre was a router, I'd do it with a pile of static routes. 

If you ever get your oscilloscope on the wire you'll quickly find out that the symbol rate can be different. 100baseT for example is 125 Mbaud, because it transmits 5 bits on the wire for 4 bits of data. In summary, the terms used often vary depending on what you're emphasising: 

In essence the IEEE is just being a source for unique numbers. Mostly people use them for MAC addresses, but you could use them for serial numbers of cars, or museum object IDs. For analogy, in internet addressing, we no longer speak of "subnet mask", we say "netmask" because everything is now CIDR. We no longer speak of "Class B", we say "/16". On the equipment, there's still 255.255.0.0 somewhere, but we think of it very slightly differently. 

The only place I've seen actual classful behaviour in recent years is in the point-to-point tunnelling protocol. PPTP Many would consider this to be obsolete in itself, but there is certainly a lot of it still in use. When the client connects to a server, the tunnel gets either a default route or a route to the classful network of the server. $URL$ Had a few networks where this was actually a problem, as recently as 2016. I believe there are workarounds with DHCP and various add-on scripts, and indeed for routes in the other direction. If at all possible use a different tunnelling protocol which has better support for routes. Kind regards, Jonathan.