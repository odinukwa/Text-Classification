Other comments You should keep your in format for accuracy. Also you can move the variable into the scope of the where it is initialized and used. 

See if the contains \$c=-a_k -a_i\$. If it does, add \$-a_k -a_i\$, \$a_k\$ and \$a_i\$ as a tuple to the list of found tuples. If there are \$i\$ left to try, go to step 5.1, otherwise set \$k=k+1\$ and go to step 4. 

and you would have all the private members from your class as globals in the file. This is the exactly the same thing as what you're doing. You're abusing a as a . While I do not condone this particular design for the problem at hand, the namespace approach would have placed you a bit higher up in the pile of applicants, because at least you're not abusing object orientation and I can accept the namespace approach as a convenient design decision. Had you instead properly used the singleton pattern where it is suitable, like this: 

I realized that in addition to my previous answer it is possible to do this in an even faster \$\mathcal{O}(n)\$ time complexity. Instead of making my previous answer even longer than it already is, I'll provide the code here as a reference with a caveat: It will not return a correct result for the special cases where all points form a vertical or horizontal line with thickness 1. It's a matter of definition if one wants these to count as rectangles or not. The handling of those two special cases (xmin==xmax and ymin==ymax) is left to the reader. Here is the implementation: 

The algorithm is essentially the same with one minor difference, the approach with the may not terminate early on finding the first element with exactly 'k' occurrences as the array based algorithm can. This means that the version must visit exactly all 'M<1000' unique values in 'vec' including some overhead from traversing a sparse hash-map, while the array version on average only needs to visit '1000/2' of the possible values. So they have different cases where they are faster. For example a degenerate case such as K=3 , {999,999,999} is faster with unordered_map as you only visit one unique element but with the array solution you need to (very quickly) scan through the 998 elements in the array. But on average on random sets the array solution is expected to be slightly faster by a constant factor as both have the same big O time. 

is executed \$w \cdot h \cdot \frac{w\cdot h}{5000} = \frac{(w\cdot h)^2}{5000} = 169869312\$ times for 1280x720 (which I gather from your screenshot). Notice the square on \$w\cdot h\$? That's your problem. How to solve it? Well you can use a for , this reduces to \$\mathcal{O}(1)\$ amortized. But you're still left with . You need to come up with a smarter algorithm that doesn't have to iterate over all the spots for all pixels. Cache Efficiency The CPU has a little thing called a "pre-fetcher". Simply put when you ask for memory address it will also make sure that and are in the CPU cache. This means that linear access of memory is much faster than any other way of addressing memory. Particularly, "randomly" accessing memory is bound to give you a metric f-ton of cache misses and your performance will be a proper charlie-foxtrot. I am not aware of any graphics library or hardware that stores images in column-major mode. This means that you should always process images, row-by-row. Meaning that the coordinate should always be the inner most loop. I.e. this: 

Note that by using we get access to which in turn allows lazy garbage collection of the subscribers. In turn this means that a subscriber doesn't have to unregister if all shared pointers to it just simply die. The above code is untested pseudocode. Some assembly may be required :) 

This also goes for your visited list. For the love of ${DEITY}, don't encode coordinates into strings Instead of simply use . Much faster, especially since you don't have to parse out the x and y values when you use it. Use the correct data structure and algorithm for unvisited nodes You are storing your unvisited nodes in a list and doing a linear search through it for the next unvisited node. This is what is ruining your algorithm performance of Dijkstra, going from to and why it takes 30 seconds for your machine. First off you only need to store the currently accessible, unvisited nodes. This alone would significantly speed up your search for the unvisited node with smallest cost. You can do this by adding the neighbours of the current node to the unvisited nodes set only if they don't already exist in the set. (You can keep an additional matrix with nodes that have been added or you can query the set if it contains the node provided you have lookup). Next as you will be taking the smallest element as the next node, it makes sense to use an ordered data structure such as a heap or also known as a PriorityQueue in Java. Note that if you do use the priority queue you need to remove and re-add elements as you change their cost so that they get re-sorted in the queue. Use an Object Oriented Solution You have many matrixes (arrays) of the same size that represent different properties of each node. It makes sense to just consolidate these arrays into one array of nodes where each node has properties. Like this: 

Addendum/Edit Seeing as this answer has gotten quite some attention I've gone ahead and dug deeper. Using godbolt compiler explorer I compiled () and disassembled my trivial loop implementation. Here is the disassembly on GCC 6.2: 

And I'm sorry for being unable to provide a runnable example, but I feel like that would be too much work for the reviewer. If you do wish to run the code you can check it out from the github repository at the top and run the gradle setup script to get the dependencies.