At first I'd suggest to have group_id and visit_id in INT or at least BIGINT. If that table has only 3 columns it might worth to create not-unique clustered index by group_id and unique constraint on visit_id: 

In your sample case SQL Server made very good choice. Here is what it was actually doing: - In the first set of data it extracts 10 rows from table based on the column. Then it extracts 10 corresponding IDs from table using kind of operation for each ID. - In the second scenario, when there are 100 matching rows in table SQL decided not to do operation, but use instead, which is much cheaper from the I/O perspective. Yes, you can use hints for your queries ($URL$ 

Can see it only on my SQL2014 on Win8.1 VM. Error persists even when run query locally as local admin 

However, there are plenty of objects in the database, which were created with credentials of that account. I've tried to run one of these procedures and it was executed successfully. When I've tried to recreate that scenario in my test database it returned me an error: 

What is a good way to manage developers' data changes? I am using RedGate SQL Source Control to monitor changes to tables and stored procecures, so that when we do a production build, I can include all the changes in a SQL update script. My question is: what is a good way to consolidate and monitor changes made to the data? (As this is something RedGate Source Control cannot track) For example, new records included in lookup tables? And new records included in tables used for routing procedure calls. Thank you! 

Does anyone have preferences or biases? For myself, I use most (4): I create a new empty database using SSMS, then populate the schema of new database using RedGate. Perhaps, it makes no difference. But I would like to know from the more experienced DBAs what their opinions are. Thank you. 

I currently have Mirroring set up between two production SQL Servers. One of these servers is a hardware SQL server (principal) and the mirror is a VM (on different server). I need to migrate these servers to Availability Groups (HA cluster NOT using shared storage). None of this is my choice. I would personally prefer two VMs, or completely identical systems. My question is: has anyone come across issues for AG/clustering between a hardware SQL Server and a VM SQL Server? Or is there any issues that you can imagine with that setup? I have had no issues with Mirroring between a hardware & VM. As long as the servers are very similar (ie: same version of SQL Server, same RAM given, data files are named & stored in the same locations, etc). My concern is AG utilizes windows failover clustering - an unfamiliar territory for me - so I'd like to know if there's anything additional to look out for. 

Yes you can. Use SQL $URL$ By doing that you'll have a full control behind the curtains, but please fully document it and be consistent. It is a nightmare to troubleshoot databases with synonyms. As an alternative you can use dynamic SQL, it might be more transparent. 

The problem occurs ONLY when I switch default port to another value. When return its value back to 1433 everything goes back to normal. During the execution could capture following waits: 

Get the same error message. In addition to SQL 2014 installed SQL 2016. With SS2016 behavior is little different: It produces an error in ANY case, even if I set SQL port to 1433. I guess because it is "Named" instance. So far see only that extra evidence within the file that SQL Server loads during the request (I assume it is normal): 

Finally found an answer: You can create database user, which won't have a SQL Login and won't be able to authenticate, but will have permissions and can be specified in clause for stored procedures and functions. The simple way to create such a user is just to use clause during user creation: 

I am attempting to setup a Linked Server from MS SQL Server 2012 to PostgreSQL 9.3 via Linked Servers & ODBC driver from PostgreSQL. Everything works, until a given query invokes MSDTC, at which point I get an error like this on the SQL Server machine, and the query utterly fails: 

The DLL is in fact that location, so the registry seems to be pointing to the right file. The ODBC driver is 64bit and so is my OS. "File=%2" is pointing to something on the d drive, which doesn't make sense to me, since d drive is a DVD. MSDTC is running... what am I missing? I have toggled Linked Server Properties "Enable Promotion of Distributed Transactions for RPC" to both "True" and "False" and this doesn't change the issue and does not produce a different error. Otherwise, scouring the Internet has brought me nothing. Last thing to point out, my query isn't actually doing any updating - it is just pulling data. So I'm not sure why MSDTC get's invoked in the first place... 

Your provides 2 sets of the data: - DB Size + Unallocated space - These numbers include BOTH: Data and log file; - Total statistics of RESERVED space for all objects within the database; I bet your 36 GB of free space are in the Log file. For real numbers use following query: 

You create clustered index on only one column. Technically, you can do it on multiple, but the main goal to have it as short as possible. If you created clustered index on you DO NOT NEED any other indexes on that column. I can guess that your performance improved because before there was no Clustered index and indexes were so bad that SQL decided to do full table scan instead. Suggestion: read a book about Indexes, their differences and how they work. 

You have to be very careful. Assuming your query is not restarting and you have a for that SPID. If you restart SQL Server it won't help, because transaction still would have to be rolled back. The problem with that is following: When you run a transaction in multi-CPU environment with not restricted degree of parallelism it most probably will generate parallel execution plan. Would say your transaction run for 10 minutes on 8 CPUs. When you Killed it, will be processed ONLY by ONE CPU. That means it might take up to 8 times longer to recover. 

I am changing a database's compatibility mode from 90 (SQL Server 2005) to 110 (SQL 2012) and I am wondering at what point would I see errors from breaking changes if there are any. I ran Upgrade Advisor 2012 which spotted several Stored Procedures that needed updating. An example was: "In SQL Server 2005 or later, column aliases in the ORDER BY clause cannot be prefixed by the table alias." However, when I run this stored procedure in SQL 2012 (without having made changes) it doesn't show any errors. Also, when I changed the database compatibility level from 90 to 110 there were also no errors. Nor when I restored the database. If I am not seeing errors anywhere, I am hoping that Upgrade Advisor caught everything. Is there any other ways I should check for compatibility errors? Also, how is it possible that this stored procedure ran successfully even though Upgrade Advisor told me it would fail? Thanks :) 

Might happen you still have some opened transactions, which hold you from shrinking individual files. See who hols them, close them. Reboot server if necessary. Shrink individual files while nobody accessing the database. switch it temporarily to Single User if necessary: 

I'm not a replication guru, but record may be marked as deleted only in case when an update is generating Page Split. For the case of replication you can test it by looking at DBCC PAGE and look through transaction log. 

So, the question is: How it is possible that procedure created with an option can be successfully executed while SQL Server login does not exist for that user? 

Items with big values in and are red flags, such as Table scans and Key Lookups. There can be because of bad indexing, statistics, etc. Wild guess: when you use Temp Table Variable SQL comes up with better query plan. 

After enabling TDE Encryption my database backup file .bak is 3x larger than before. Is there a way to improve this? Is this due to compression not being as efficient on an encrypted file? 

I fixed the problem by changing folder permissions on the Postgresql ODBC driver folder and giving read/execute access to Network Service. Because MS DTC runs under network service, which I confirmed by looking at the service properties. That got rid of the error! 

I am using Availability Groups (database level) SQL Server 2017: I have set my availability group to "prefer secondary". I have allowed read access on replica. I have created a maintenance plan with full backups on all databases on replica using "copy only" and then also on primary. But now I'm wondering does AG actually make any decisions? Both Mainenance plans will run and from what I can tell won't change "copy only" to primary's maintenance plan if there's a failover. Besides just preventing me from running backups if I change the setting..... besides enforcing rules for what [another] dba may or may not be able to do: what is the point of this? Or, did I setup my backups totally wrong? 

There are 2 options: 1. Create clustered index by account_id. Delete all other indexes. 2. Add new column and create clustered index on that column. Create another non-clustered index on ONLY one column . The second option is more preferable because it is much faster. You join only by 4 bytes. While your current queries join by 40 bytes. That means your current operation is 10 times more expensive. Also, ask yourself couple of questions: - Do you really need have Account_ID as Unicode? - Can you convert Account_ID to INT or BIGINT? Hopefully, you've got my point. ADDITION: 

Try both of them for both of your data sets to see the difference in query cost and amount of I/O using . For instance, when you force for the second data set I/O for table jumps 24 to 215 reads. So, be VERY CAREFUL using any kind of these hints.