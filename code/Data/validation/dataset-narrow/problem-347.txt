I'm troubleshooting a SSRS security issue where one user is being denied access, while another user with apparently identical settings is working as expected. I'm seeing "The permissions granted to user 'mydomain\myAccount' are insufficient for performing this operation" in the browser, but curiously NOT seeing the (rsAccessDenied) at the end. Is this indicative of something that can help me troubleshoot the issue? 

Do maintenance plans provide any functionality beyond their component parts? It would seem that the only value added is the convenience of grouping subplans into logical bundles, and the simplified GUI for creation. All the actual work is being done by the underlying SSIS tasks which are being scheduled as agent jobs. Is there anything more to it than that? 

After investigating a production issue, I have found a number of references to SSIS packages getting stuck in 'validation' when run with DTEXEC or through as SQL Agent job steps. Specifics of my situation aside, is there a technique to make this more robust so that in a production environment, I am notified if a job starts, but does not complete? Is it possible to have a timeout period on validation so that the execution explicitly fails? Or have a way to specify number of retries? 

is returned. Is there any way to achieve this behavior? To clarify, the behavior I would expect to see is demonstrated in this example 

This query works in the simplest case, but does not allow adding attributes to the root element, which is required in my case. 

There's no answer that applies in all cases. In general, however... If the lookup list is small and you can cache it(or use a cache data source), there's not much performance penalty to doing it in SSIS. If you want crossref a list of 50 location codes to names of cities, go for it. It's nice to see all the process on-screen in one place, rather than buried in sql statements. TSQL will be better-performing in most cases, since it knows the most about the data and the query optimizer is always going to be smarter than you. If all the data is in one DB, you can hide a lot of complexity in a sql query source. If the data is spread out across different systems, the middle ground is to to do an SSIS merge join from each system. Trying to do that at the RDBMS level is madness. Always do the sorting in the source query, though. SSIS Sorting is almost always a bad idea. 

I'm trying to make a multi-statement table valued function and failing due to what I think is a limitation of functions and CTEs. The intended function defines an input parameter(@Param) and and output table(@ResultTable). It then executes a complex insert statement involving a CTE into that table variable which is (of necessity) terminated by a semicolon. I then attempt another complex update statement to that table variable involving a CTE and receive an error saying that "Must declare the scalar variable "@ResultTable". Apparently it has dropped out of scope somehow. I have used this sort of pattern in the past, so my only thought is that the CTEs seem to limit the scope in some way. Is this a known limitation? 

In my specific case, this was a result of cell security. The role did not have access to all the fields returned in the drillthrough action. I missed this because there are two very similarly-named fields and my eye did not notice. It's still odd that a security denial creates a query timeout, but at least this note is here for the next person. 

I'm starting a few new database projects and I'm attempting to create them at Data Tier Applications. There are two items I'm not able to find documentation for. I would like to set the db owner to SA and set the initial filesize and growth rate. Even if those items are outside the scope of the app, I would expect that there would some way to specify that at publish time, either in SSDT or SSMS. I can find no documentation either way. Is this the case? 

I have a server with a working installation of the Sql Server 2012 SSIS Catalog. I need to set up an additional instance including the SSIS Package Store service as an interim step while the packages are being re-written. The Package Store is a per-server feature, not a per-instance feature. Can these two features operate side-by-side? 

I have a cmdexec step in a sql agent job that includes a redirection into a file at the end. It works as expected from a CMD shell running in the context of SQL Agent service account and produces a file in the desired location. If I run it as an agent job, however, the step 'succeeds' but never produces the output file. In both cases, the service account obviously has filesystem permissions and system rights sufficient to perform this action. Is the cmdexec environment more restrictive in someway than just running cmd.exe? 

You need to work your way up the networking stack to determine where the issue is. Can you ping the destination from the source? Can you telnet to the postgres port(5432 by default) from the source? Can you connect to the postgres service with a management tool(pgadmin or psql) from the source? 

Import it as a string and do a derived column to convert it to a float. Whatever rows fail conversion will be re-directed to the error output and you can at least see what the problem is. Flat File connections fail completely when they encounter problems. It's generally more productive to bring them in as loosely as possible and adding structure inside SSIS. You almost certainly don't want to be using floats anyway. Use 'numeric' once they're inside the package. 

This is almost exactly the example used for 'factless fact tables' in analysis services, which illustrates the point. The thing represented by the table is a 'booking', which has no natural primary key. The true key would be a composite key of location-timeslot. You need to create an artificial primary key if you don't want to do that or the DBMS does not support that. You need at least these tables- Location(room #, Capacity) Timeslot(day,starttime), module(code,name). It's not clear if the person is tied to the module or tied to the booking. It's not clear if the extension is tied to the person or the location. 

I'm trying to perform the same scenario as in the following link; Create a SSIS Script Component as a Data Source that uses a pre-existing HTTP Connection Manager to retreive a page with GET and emit rows into the Data Flow pipeline. $URL$ My target platform is SQL Server 2008 and therefore C#. The MSDN documentation gives examples of File and SQL Connection Managers but not HTTP ones. $URL$ The specific problem is that I can NOT figure out why there's no HttpClientConnection constructor in my current context. The MSDN documentation of that class does not seem to apply in the case of Script Components and translating this to something useful is apparently beyond me. $URL$ My non-working code looks like this - 

This is a pretty convoluted way to do it and I am sure there is likely to better ways, but it can be adapted to handle more columns. 

The answer is yes it is possible to get performance with filters over multiple columns including spatial columns. I've tried to replicate your tables and closely as I can to your brief description using RAND to create prices, location points and link them to events. But I can't duplicate you performance problem. Have I got the basic table details right? 

This is generally the way that I tackle rebuilding spatial indexes in a consistent way. This assumes that your geometries are all in the same projection and have already been indexed. It will exclude the usual ESRI Geodatabase System tables. I avoid having ArcGIS create or maintain indexes. They never seem to get nice extents on the projections that I use. 

This is my interpretation of your requirement.As a simple query, this will return what you want, however you will probably want to try other options to make it perform. I don't use MySQL, but in SQL Server I would look at trying CROSS APPLIES or some sort of grouping option. This query will return all the dates on which the minimum or maximum occurred. You can of course then filter that to suit what you require. 

I get 93ms, 4ms and 93ms respectively, removing the TOP 2000 from the queries causes the price filtered query to blow out to a minute or so. Are you able to give more details (execution plans, DDL, etc) to help us replicate your issue. My initial thought was that the was hiding the underlying problem. I think that may still be the case. 

A couple of things that have caught you out here. In the SQL Server version using the Point, the order of the coordinate is Lat Lon, eg Y X. The OGC Point construct is X Y or Lon Lat. The next is that the PostGIS query you've posted actually returns which while appearing similar to the SQL Server result, is quite a lot smaller. The last issue is that the PostGIS query is treating the objects as Geometries and doing a cartesian distance on them. You need to explicitly make them Geographies. Here's a few ways to do the PostGIS query. I've swapped the coordinate order 

Also note SQL Server let me build a spatial index over the geometry column that contains mixed SRIDs, but it is of limited use. Even if there was a lot more data, the index would be of limited use. I would suggest to make life easier that you pick a single projection to use across all you data and transform geometries to that. This can be done alongside the current geometries or replace them. Also if you choose a Lat/Lon projection, use the Geography data type. There is an open source project, SQL Server Spatial Tools, that has some tools that will allow you to do projections. You will however have to know the parameters for the projections you are using as it does not appear to have a list of projections to work with. I can't advise how good these are, as I haven't used them. 

I've done this using a CTE query as I find it easier to follow and explain. The first CTE is just your data. The second CTE ranks each row of the data using based on the ordering of the . I have include the in the order so that if there are duplicate the first occurrence will be picked. In the final query we are using statements to pivot the data and aggregating the result for each with to create a single row for each . This query can be run over multiple 's 

The SRID does not appear to be an integral part of the spatial index nor can it be added. It is integral to the geometry even though there are no real tools for manipulating geometries based on the SRIDs in SQL Server. It does have a few rules about SRIDs. 

I have struck similar issues with Geometry and Geography data types before. I think the issue is to do with where (and how) the optimizer decides to build the geometry in the plan. I was having an issue with insert statement that was basically 

You are correct in your interpretation of a SRID. It identifies the projection used to locate the Geometry or Geography. Even if the database in question (I'm assuming SQL Server) doesn't make use of the SRID directly, the point of it is to identify how to work with the coordinates in relationship to others. I suppose you could look at it like having a country code on a phone number. Not always necessary, but important none the less. Your understanding of Geometry vs Geography isn't correct. Geometry is used to store objects which are projected to a Cartesian or Planar coordinate system. They are not restricted to just 2 dimensions. They use distance units to (metres, feet, etc) to represent points. Geography is used to store objects that are projected on a Spherical coordinate system. These systems use degrees (Latitudes and Longitudes) to represent points. It is important to use the correct data type for your information. If your objects have Latitude and Longitude coordinates, store them in a Geography. This means when you use distance and area functions you will get a sensible result. While some systems (SQL Server) mostly ignore the SRID in geometries, it is still important metadata and should be set correctly. Other databases and software can and do use the SRID to determine how to treat the coordinates in the object, allowing for the coordinates to be reprojected to other coordinate systems and compared correctly to other objects that are projected differently. 

The first query determines distances between points. It will only return a distance between the 3rd and 4th point as they are the only ones with the same SRID. The other distances will be NULL. The second query is for selecting geometries inside a buffer. Only a single row is returned since the query buffer was created with SRID 2. 

Then to query for all points within a specified distance you can do a query similar to the following 

Then to incorporate the scaling factor into the sum you can use a case statement in a subquery, similar to the following 

Seriously though, I would put some effort into validating your data and reviewing your database design. 

The other common query for this type of thing is to return the n closest points, otherwise known as a nearest neighbour or knn query. 

As Ypercube said in his comment you seem to have an uncorrelated subquery. This means that for each way you are trying to build a line containing 1,000,000 odd nodes. Also it it would be best to put a Geography constructor around the result of the subquery. The tutorial's update statement that you referenced has got a correlated subquery, because the clause is Changing your update query to this should fix the issue.