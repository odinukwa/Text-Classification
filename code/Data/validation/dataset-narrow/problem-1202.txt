edit: (adding relation with 'classical' computers) Classical computers are still really efficient in what they do best, and so the vision is that quantum computers will be used to off-load certain algorithms, analogous to how current computer offloads graphics to a GPU. As you have seen above, the CPU would control the quantum computer by sending it an instruction stream and read back the measurement results from the boolean 'signals'. This way you have a strict separation of classical control by the CPU and quantum state and effects on the quantum computer. For example, I'm going to use my quantum co-processor to calculate a random boolean or cointoss. Classical computers are deterministic, so its bad at returning a good random number. Quantum computers are inherently probabilistic though, all I have to do to get a random 0 or 1 is to measure out a equally-balanced qubit. The communication between the CPU and 'QPU' would look something like this: 

The above program thus creates an ancilla, entangles it with the input qubit, measures the input and depending on the measurement outcome performs an operation on the ancilla. The result is that qubit 2 now contains the state of qubit 1 after Hadamard operation. The above is naturally at such low level that you wouldn't want to hand-code it. The benefit of the measurement calculus is that it introduces 'patterns', some sort of composable macros that allow you to compose larger algorithms as you would with subroutines. You start off with 1-instruction patterns and grow larger patterns from there. Instead of an assembler-like instruction sequence, it is also common to write the program down as a graph: 

Given a DAG, which can represent a partial order and has at least one topological sort. For example the graph 

Put the all the graph minima (nodes with no input edges) in set #i Remove the graph minima from the graph. repeat step 1. with as long as the graph is non-empty 

$X \vdash b$ implies $\{a\} \vdash b$, for some $a \in X$, and the deductive closures $\overline{X} = \{b \mid X \vdash b\}$ are finite. 

If $A$ is a (prime) information system, write $Pt(A)$ for the points (also "elements", "ideals") of $A$, that is, for the usual consistent and deductively closed (possibly infinite) sets of tokens in $A$; for the finite ones write $Pt_f(A)$. Then, a stable approximable mapping between two prime information systems with carriers $A$ and $B$, according to Zhang, is a relation $r \subseteq Pt_f(A) \times B$, such that 

I'm currently looking into the representation theory of Scott domains. In his paper "dI-Domains as prime information systems" (1992), Guo-Qiang Zhang uses prime information systems to represent dI-domains. A prime information system is an information system where additionally we have linearity and finiteness of entailment, in the following sense: 

if $(a_i,p_i) \in r$, for $i = 1, \ldots, m$ and $\bigcup_i a_i \in Con_A$, then $\{p_1, \ldots, p_m\} \in Con_B$, if $a \cup b \in Pt_f(A)$, and $(a, p) \in r$, $(b, p) \in r$, then $a=b$, and if $(a, p) \in r$ and $\{p\} \vdash_B q$, then $(b, q) \in r$, for some $b \subseteq a$. 

This is admittedly a rather naively put and vague question, and I'm not sure how much more specific I want or can make it, but I'll try. By "practice" I mean surely in actual programming practice (of which I embarrassingly don't know much), but also in mathematical practice, whenever certain higher-type objects are employed as examples or counterexamples within arguments. By the "height" of a type, I don't mean to include the obvious and natural arbitrariness involved in objects like, say, the fixpoint functional (in fact, in the spirit of the question, I would prefer to understand this as a type $2$ object "up to parameter types", so to speak). A better example of what I mean would be the well-known (from mathematical practice, at least) fan functional, of type $((\mathbb{N} \to \mathbb{B}) \to \mathbb{N}) \to \mathbb{N}$, given by $$ \lambda f. \mu m. \forall_{\alpha, \beta} \left( \forall_{n < m} \alpha(n) = \beta(n) \to f(\alpha) = f(\beta) \right) \ , $$ where $f : (\mathbb{N} \to \mathbb{B}) \to \mathbb{N}$ and $\alpha, \beta : \mathbb{N} \to \mathbb{B}$. My questions: Are there any objects of yet higher type than $3$ (possibly "up to parameter types") that are naturally used in the literature? In practice? 

Since you are interested in the structure of tree rotations, I think you may also be interested in the following paper, which shows that the rotation graph of binary trees of a fixed number of nodes has a Hamiltonian cycle. In fact Lucas showed that the cycle can be traversed with constant delay per tree. (A rotation can be performed in $O(1)$ time of course, but it is not obvious a priori that we should be able to decide which rotation to perform along the Hamiltonian cycle in $O(1)$ time.) Naturally, you may also be interested in the references within. Joan M. Lucas, The rotation graph of binary trees is Hamiltonian, Journal of Algorithms, Volume 8, Issue 4, December 1987, Pages 503-535, ISSN 0196-6774, DOI: 10.1016/0196-6774(87)90048-4. A simpler proof, also constructive, of the simpler fact that a Hamiltonian path exists in the rotation graph can be found in this later paper coauthored by Lucas and her collaborators. Lucas J. M., Vanbaronaigien D. R., Ruskey F., On Rotations and the Generation of Binary Trees, Journal of Algorithms, Volume 15, Issue 3, November 1993, Pages 343-366, ISSN 0196-6774, DOI: 10.1006/jagm.1993.1045. 

As suggested by Matt Groff, you may be interested to look into the practice community for inspirations (or if $n$ in your situation is within the bit width of a current CPU). Indeed, the problem of integer multiplication by a constant has been considered by many compiler writers and circuit designers, although they are usually interested in "multipler-less multipler" (multiply using shift, add, and subtract). One of the early references I am aware of is (I learned this from Hacker's Delight section 8.4.): Bernstein, R. (1986), Multiplication by integer constants. Software: Practice and Experience, 16: 641–652. doi: 10.1002/spe.4380160704 More modern work by Vincent Lefèvre can be found here (be sure to see follow-up works to his), and he also notes a CMU project on efficient circuit synthesis (see the references there). The latter project even considers simultaneous multiplication by a set of constants. P.S. I encourage you to consider changing your username to something recognizable.