Unfortunately this is an "it depends" type of question. We could theoretically give you answers from our own experiences, but you will need to test. If you do bump into issues they will not necessarily be impossible issues that you cannot fix, they will require testing and tweaking. 

Glenn Berry has written some great scripts to help you find your missing indexes. I suggest using his scripts which take some of the guess work out of the task for you. Those scripts aren't just looking for null or 0 user lookups/scans/seeks but also looking for indexes that have a large skew between read activity and write activity, possibly still resulting in better overall performance by dropping. I'd check his scripts out - you can get a start on this post of his. I wouldn't be worried about the system activity. That isn't something that will get worse if you remove the indexes, in fact it could be activity that only happens on that index because it exists. The main thing you care about is user read activity and user write activity and balancing that out. 

In FULL SAFETY mode you should be good to do this without data loss if there are no transactions in flight. Are you waiting for the state to change from Synchronizing to Synchronized after that restart? My guess is application users are in there when you are doing the failover, though. If the applications are still connecting and data is still being modified then there are transactions in flight and that is the price you have to pay. Best to get a window and stop access. Shut down the site or apps accessing SQL, etc. 

Agree with StrayCatDBA and their answer. Schema Stability locks are one of those necessary parts of SQL Server. While you can absorb a dirty read (or any of the problems that go with uncommitted reads), you really can't absorb the impact of an underlying schema changing on you mid query. Really, SQL couldn't handle it either. So that is a lock you can't get rid of. I feel, though, like there may be a deeper underlying question here. Why do I say this? Because in working with SQL Server for 20 years now, I've been frustrated at times by locking involving SCH-S or SCH-M locks but I've never found myself saying, "Oh! If only I could remove these pesky schema stability or schema modification locks, all would be solved!!" Two selects each holding an SCH-S lock will not hurt you queries. An update won't block a select with because it is holding an SCH-S lock, it would block it if you were in read committed (the default) isolation level and the locks needed at the table or page level are incompatible with the ones for the update, for instance. So I would turn the question back on you and ask what you are doing to be burned by the SCH-S locks. Your SCH-S lock isn't normally causing a problem unless something else is trying to truncate a table, alter a table, alter a column, etc. Or during index rebuilding operations. So you could either have a situation where you need to (1) analyze DML happening against your table during production workloads; (2) or a situation where another lock type is the real issue; (3)or a situation where maybe you are truly doing something that somehow makes a block along this lock type make sense. For those I would probably start down one of the paths: 

Log the sp_whoisactive data to a table (Kendra Little has an excellent blog post about this) (I wouldn't do this terribly frequently, and I would monitor your performance while it runs. The tool is well written but it isn't "free" to run from system resourecs perspective. If you were running this every 1 second I'd potentially be concerned. Every 15? 30? 60? It depends - you need to test. Periodically query this table in a SQL Agent job. Use the information presented in the query (Duration, query text, application, user) to determine if it is "alert worthy" and if it is alert worthy (Duration > set amount, user not in an "acceptable" list, text not in an acceptable list, etc.) either raise an error that an alert can fire off of, or generate an e-mail. Remember to keep that table pruned and really monitor the approach - it may be that your environment can't handle the load of doing this frequently enough. You can look at some of the tables underneath this procedure and capture less data. 

You could shrink it in smaller "chunks" using . Pros: No development really required, potentially less downtime than migration. Cons: You are still shrinking a database, you still have to deal with fragmented indexes, but you should do some index rebuilds anyway because you've removed a lot of data. You could build a new database and select data into it. Pros: You aren't shrinking. You are starting "fresh" and clean. You can use this as a time for other cleanup potentially. Cons: Probably more effort than just shrinking in chunks, may require a larger down window depending on how complex the environment is. Potential risk for missing something in the scripting. If you chose the shrink route, remember to rebuild your indexes and deal with the resulting fragmentation. 

Short Answer: From when the backup was taken I actually wasn't sure of the answer, so I just made a database, put it in full recovery model, took a full backup, did some work (create a couple tables named after the time I created them) and then started restores. Restored the full and then attempted to apply the log backups. When I did that I had to specify the time zone from when the changes were made - from when the backup was taken. If I tried to use the new time zone's setting, it errored - bad timing. So the answer to your question in my experience with SQL Server 2012 and 2008R2 - appears to be "The local time from when the backup was taken" This backs up my expectation before testing. The way the log records are written and the way the backups are taken - that makes sense. That said - I can't imagine a ton of situations where the time zone is changing with the need to worry about point in time recovery? 

Kevin's answer describes what events to capture in SQL Trace/SQL Profiler. To expand on that answer a bit - will show you each statement within a stored procedure being completed, as it sounds. Also if you are on a busy system and trying to diagnose a performance issue you should be careful with SQL Profiler. SQL Profiler is much slower than tracing to a file or using Extended Events. This blog post by Jonathan Kehayias shows about a 90% overhead on a system's performance from using SQL Profiler and about a 10% overhead from tracing to file. Less for Extended Events. That is why it is typically recommended to not run SQL Profiler itself whil While this information is available through Extended Events, I would suggest still using SQL Trace (the technology behind SQL Profiler) but tracing to a file instead (if you wanted to invest in learning and using Extended Events this would be the way to go, in a future version of SQL Server SQL Trace will be gone and all we'll have is Extended Events) . I would also suggest you filter through the Column Filters button out as much background noise as possible to make sure you are only capturing what is necessary. You can setup your trace with the Profiler tool using the steps Kevin describes in his good answer and then add a filter from that same GUI. Then you can export the trace as a script and run that script on SQL Server tracing to a file on a folder that doesn't contain database or transaction log files. To export, you would simply setup your trace, run it for a few seconds to just make sure you are capturing what you want, stop it and then go to the menu bar and -> -> and save the file. Then open that file in a new query window on the server you intend to trace. You can see more about the options and definitions of this script you created by looking at the help articles for the various stored procedures used in that script you just created by starting here. If you have time and want to learn, you can also read some articles on Extended Events and see how to capture the information as well. Jonathan Kehayias is a great resource for blog posts when you are ready to start there. 

I think you could run a query from SQL Server Management Studio from the machine that you can connect from and run the query with client statistics. That won't be exactly the same as a ping but you can see the server execution time (time from request sent from client to time server starts to send response back) and the client processing time (time from when the server starts to send to when the client receives it). That is why it isn't a ping because client execution time includes that network time and the time on the client to process. I'd run a fairly simple query with a handful or so of rows returned. And maybe a slightly more complex query with more rows. You should also run each of these a few times in a row to eliminate any other slowdowns with the query. Again, this won't really be a "ping" but it can at least show you some information and you can use this to trend information over time as well by running the same queries and looking at the info. To look at client statistics - from within the query window of SSMS, go to and then select . You'll see the details in the tab of the results. If you could also run SSMS queries on the actual server itself and run the same queries you can look at the difference in these times for the same queries and that should give a rough approximation of the impact of network latency perhaps. 

On the maintenance plans I typically let my maintenance run as the agent account and I normally keep them owned by SA so when I quit or became a consultant things still run. When I help a client with maintenance/management I do the same thing. You see to run maintenance you need more than just CRUD permissions. You are doing backups (backup operator), index rebuild and statistics updates (DDL Admin, Alter or DBO), potentially error log recycles (not sure here perhaps setupadmin, definitely SA), etc. For maintenance, it is a process you own, you setup and you trust. Like a monitoring tool, I see no issue with maintenance running as sysadmin. I would also suggest you check out Ola Hallengren's maintenance solution. Maintenance plans are okay and serve a purpose, but he's put a lot of work into his maintenance solution and it works well. On: 

Short Answer: This is probably not the best counter for you to do what you are looking for. In fact you should consider writing your own monitoring process or utilizing a third party monitoring product, rather than a SQL Agent alert. Longer Answer/Background: As has been established by the original answer from Jonysuise - this is looking at transactions that are long running. Not every query gets a transaction. If it is an update or an insert? Yes. If it is a select query? No, unless someone explicitly wraps it in a transaction. This is important if you are looking to capture long running selects, which in my experience, tend to be the killers in many environments. Either way here I am not sure that this is the best way to accomplish what you are looking for. I'm not a fan of rolling my own approach to something that a tool already has - and I like SQL Sentry for this reason - among other monitoring needs. So that is a good approach - then it is someone else supporting the long term development to keep up with versions, someone else implementing other alerts - and they have a tried and trued harness for things like this, especially the Top SQL running long. But I don't want this to sound like a commercial for them (I get nothing from them for talking about them positively). There are plenty of other tools also. So with that out of the way.. Something of an answer: 1.) One problem here is that the polling interval for the SQL Server Agent alerts is not constant - in 2000 it was definitely every 20 seconds, it may be more frequent in later versions, but it is not continuous, that would be too much overhead. So something has to be above the defined threshold for more than 20 seconds. If you have a spike and it is done when the 3 times a minute (20 second) polling runs, you'l miss it. This is just one reason why the Agent Alert is probably not the right tool for this job. If you increase your occurrence count and make sure the SQL SErver agent is running, you'll see an occurrence for this in the history. 2.) Again - I don't love this approach. You miss select queries not explicitly defined as transactions. And even if you could there is no way for you to filter these out. So I would suggest a monitoring tool here or perhaps trying something like I explain at the end here. I would also, though, suggest asking yourself what the goal is. Do you want to know anytime a query runs over a certain length of time? That is good information, and sometimes you want that depending on your system. Sometimes you care more about the overall health, or perhaps a certain query. But assuming you want to do this and don't want to buy a tool, there are plenty of guides out there on rolling your own monitoring solutions in SQL Server. One approach I've seen used involves the sp_whoisactive script that Adam Machanic wrote. This is a well accepted, widely adopted free tool that combines several key DMVs and system information to show you what is happening on your SQL Server instance. One approach (in pseudo code/explanation style) could be: