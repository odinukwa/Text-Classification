All 5 FSMO roles need to be migrated to your new DCs, including the Schema Master, however you only need to move each role once. 

If you are looking to run multiple websites using Azure, it's not necessary to use IaaS. You can run up to 10 web roles for free - $URL$ 

You are correct, in order to use Azure AD you must become a "tenant" within the system. So a tenant is basically just securing a .onmicrosoft.com sub-domain. At that point you would have one account registered in your Azure AD. From there, you can activate Office365, Intune or any of the Azure services. 

I wrote custom file-counting program for this StackOverflow question: $URL$ You can find the GitHub repo here if you'd like to browse, download, or contribute: $URL$ 

I'd like to switch to the newer kernel and I'm not sure how to do that. If this were bare hardware, I'd make sure that knew about the new kernel, make it the default, and (if necessary) install the bootloader onto the boot sector of the primary disk. Since this is EC2, I'm not sure what the procedure should be. The file exists, and contains: 

Fingerprints (and other biometrics) on Windows are designed to replace the PIN on the machine and are keyed to the machine's TPM individually. The point is that if your fingerprint is stolen, it could not be used to access resources using a different machine. There might be some third party tools that would allow you to do what you want, but that's not by design with Windows/Active Directory. 

You might want to take a look at Azure File storage, which will allow other Azure servers to access files via SMB, without needing a server to host the share - $URL$ 

It looks like is trying to verify the certificate against a CA, rather than directly-trusting the certificate that is in my file. Any suggestions? UPDATE If I leave in the configuration file but place the client certificate's parent certificate (the one from StartCom) into the file, I can connect using my test certificate. If I change to using with both the test cert and its signer in the file, I again cannot connect. Is there a way to give the exact certificate(s) I am willing to accept from clients? The documentation suggests that using or is the way to do that, but I can't seem to make it work in the way I think I should be able to. 

Have you changed the DNS for the Azure network to use your first DC as DNS, instead of Azure DNS (default). The 2nd DC wouldn't be able to find the first DC without that being listed in the settings for the Azure network. 

You have the right idea with your diagram. DCs should point to other DCs for DNS, ideally in the same site (if you have multiple sites). Your workstations should then be pointing to a DC in their site for DNS resolution. You will see slow logons as a symptom of DNS issues. Just about every problem with AD tends to be DNS related, at least in my experience... it's always the first place to check! 

We have 3 AWS-hosted web servers all configured (as far as I can tell) identically. Each server has a copy of a web feedback form that submits to a PHP script that ultimately calls the php function to send an email to us. This works fine on 2 of the 3 web servers. On the third, the php function returns (indicating that the mail was sent), on the web server shows the message as accepted for delivery, and then transferred-on to Google's MX which also accepts the message. Example mail log for delivery to gmail: 

You can now active domain services directly for IaaS VMs using the Azure Active Directory Domain Services (currently in preview). This can potentially replace the need to deploy full ADDS VMs in your Azure virtual network. $URL$ 

I don't know if this will be entirely helpful but here is good list of ADFS resources - $URL$ In many cases, organizations are using ADFS because they have a requirement to authenticate to cloud services using an "on-prem" domain controller (or a DC entirely under their control). Azure AD can handle all cloud related authentication and sync with local DCs, but Azure AD would be using the cloud-based directory and on-prem authentications would happen with your DCs. You can run DCs and ADFS servers on Azure VMs and treat those DCs as being located in a different "site" (under AD Sites & Services) to manage replication much like you would do with multiple business locations. This post is dated, but might be a good start for migration ideas - $URL$ If you are using ADFS to meet a B2B need - that is currently in preview for Azure - $URL$ 

When you do this, MySQL lo longer uses rDNS to resolve -> and, since all my s are for , the user isn't allowed to connect from host . Two solutions exist for this particular problem: 

I can't seem to connect using . (Note that I'm cheating a bit -- using my TLS server's certificate as the client cert as well, but the cert says it has an extension for , so I figure it should work.) 

Since my certificate for testing appears in that list, I expect that it should, well, be acceptable. I'm not sure what I'm missing. In , is logging this: 

With the classic portal, only co-admins can access a subscription. The Azure Resource Manager features of the new portal, you'll be able to more finely control the access within a subscription. The official documentation is here - $URL$ 

Printers and desktop items are set by the USER policy, not by the MACHINE policy. So make sure that your policies are set to apply to the appropriate scope and have the appropriate permissions. If you have a policy that is set to be processed by machines, but it only change user-specific items, it the policy will apply, but no changes will result. 

I recently broke a working installation where most clients are Java-based. The CLI tools would work, but the Java clients all stopped dead in their tracks. In my case, the culprit was a new setting that I had enabled "to improve performance": 

But, when I make requests to either of those ports, I can't see the Proxy Protocol header when using tcpdump. I can make requests successfully through the back-end servers with both HTTP and HTTPS but I just don't seem to get the expected PROXY header. I'm not using any other kind of proxy between my clients (openssl s_client, Firefox) and the backend web server (where tcpdump is observing the connection). The listeners are TCP:80 -> TCP:8080 and TCP:443 -> TCP:8443. Do I have to do anything else to get the Proxy Protocol enabled on my ELB?