Sonatype's Nexus 3 Pro supports High Availability through a couple of mechanisms that are collectively known as Component Fabric: 

We exist in an increasingly complex ecosystem of Free and Open Source Software, FOSS, and it's dependencies. Having done a bit of analysis on one medium size project there are over 1,500 dependent software packages, not counting different versions of the same package or any packages developed internally for reuse. How do we manage this complexity in an enterprise that needs to be able to answer the following two questions categorically 

Conceptual Consistency I advise that you don't use too many tools, try and standardise on a single tool rather than changing tools for each project, typically you see sets of tools being used across an organization: 

With the assumption that the Operations role doesn't exist as a separate entity within a DevOps enabled organisation, how is an organisation collectively or severally meant to maintain supplier/vendor relationships so that when a vulnerability is publicised the teams can respond by reaching out to the vendor to understand remediation and mitigation actions. Conceivably we could: 

The desire is to store keys in Azure Key Vault as much as possible rather than rely on them being stored in images, and ideally never stored on disk at all. 

Sputnik: Trying chat out to see how it works. Mercury: Starting to move to chat. Gemini: Chat as the primary source of communication, experimenting with bots. Apollo: Chat as the nexus of communication and work, including bots. Elon Musk: Chat is the primary centre of mission critical work. 

All three are patterns of sorts, it isn't is a case of picking and choosing which to use in any specific circumstance but a case of knowing when to recognise the patterns that can help or hurt you. Snowflake Server A Snowflake Server is very much an anti-pattern representing the case when a server evolves in an uncontrolled manner to the point when it cannot be easily reproduced. I have had numerous run-ins with this kind of server in production, they are fairly easy to spot as there is usually a large number of failed changes and comments such as "it [the change] worked in Development/Test/UAT/Staging". Phoenix Servier A Phoenix Server is more of a principal than a pattern as Martin Fowler puts it: 

Agile Development Teams - Agile teams writing code. Systems Administrations Teams - Building infrastructure to run the software. Operations Teams - Supporting applications and infrastructure in Production/Live. 

It's not really in Sonatype's interests to support HA for the community project as it would cannibalize some of the enterprise customers from their paid product. 

Adding DevOps practices to a single development team is a relatively simple proposition as it usually comes down to introducing either people to the team to build up a DevOps, Release Engineering or Platform Engineering capability within an organisation. 

The major drawback of AppVeyor and it's cousins TravisCI and CircleCI is they don't play particularly nicely with on-premise source code management solutions, you do really need to be using GitHub or BitBucket. 

Don't forget this is a Hypermedia API so wherever possible have your code follow the links in the API rather than trying to "guess them". 

Fundamentally, what Bias and Baker are trying to convey is there has to be a transition from how we treat servers from being "Unique Snowflakes" with names and emotional attachments, to a model whereby if we have a problem with the server we create a replacement and destroy the problematic server. Finally, it is probably worth mentioning that in regulated environments taking a server out the back and shooting it may not be optimal. In these cases it is often advantageous to "freeze" the server, for example using to freeze a container. This can then be used to perform a Root Cause Analysis as part of the Incident or Problem Management Process. 

Large enterprises will commonly adopt a delivery framework, or operating model, that has been built to support large enterprises. From an Agile/DevOps perspective, which is my area of expertise there are three frameworks of interest: 

Debois' proposal was to unify the three ways of working together, specifically moving Systems Administration teams and Operations teams from a Waterfall Model to an Agile Model. To that end, Debois setup DevOpsDays 2009 in Ghent, Belgium inadvertently coining the phrase DevOps. The idea of DevOps resonated with the Authors of the VisibleOps series of books: Gene Kim, George Spafford and Kevin Behr; who went on to write The Phoenix Project and The DevOps Handbook. Both books explore how Agile and Lean can positively impact Systems Administration and Operations teams. 

Storage is, in general, cheep. Therefore, only ever use the Hot Storage Tier. Managed Disks are, in general, easier to use. Therefore, you are limited to only using Locally Redundant Storage. 

StackOverflow Close Vote Reviewers We have some chatbots running in the SOCVR channels that help us to: 

What other books and resources are available that describe the people, process and tools used to implement ChatOps within an organisation. 

I've had some very interesting conversations today about the role of Environment Management within teams that follow DevOps practices. The traditional role of an Environment Manager was someone who: 

Randy Bias chronicles the history of the term stating that it probably originated in 2011 or 2012 when Bill Baker used the analogy when describing "scale-up" vs. "scale-out" architectural strategies. Bias adopted this into his presentations about cloud architectural patterns: 

Given these definitions, you might consider ways of improving the capability of the organisation as a whole to be resilient against failure: 

Considering that virtual hard drives are generally formatted using something like that abstracts away from the underlying block device, which in and of itself is an abstraction over the top of a physical and network topology. Is it possible to use something like or against a single file or an entire device to mitigate the risk of data being recovered at a later date if that machine was compromised? 

The three major cloud platforms you mentioned have either free trials or a free tier; these all include spending and resource caps which prevent you from spending more than your free trial allows: Azure 

Azure offer £150/$200/€170 for the first one month of usage, in addition many of Azure's services have a free or low cost tier: 

The above statement points to the fact that VMSSs are just an orchestration layer over the top of Virtual Machines and as you quite rightly pointed out results in an availability for a VMSS of 99.95%. In terms of the storage calculation, there are several parameters here: 

In some organisations it may help to prevent all pushes to the central GitHub repository while a Hotfix is pending post-approval. 

If you are not using AWS Autoscaling Groups you can still achieve the desired result using tags on your EC2 Instances: 

Whether you are using Hot or Cold storage, the availability for Hot storage is 10 times that of cold storage. Whether your application is read or write heavy, what you need to consider here is that there are separate SLAs for reading and writing. 

With all of that said I strongly recommend using Consul by HashiCorp as your service discovery mechanism, in the long run, you get significantly more flexibility in terms of decoupling your parts of your system from each other. 

With the above two assumptions your storage SLA is 99.9%. Multiplying these two figures together you get, 99.985%, this is still kind of low, but higher than your 98% figure above. You can increase this number by hosting Service Fabric across multiple regions: 

However none of these questions or their answers describe the differences between a Systems Administrator and a Site Reliability Engineer. In broader terms: what are the key differences between Google's practice of Site Reliability Engineering and the traditional separated Development and Operations functions within a business. 

Single Pane of Glass In the last three years, various products have gone to market providing a "Single Pane of Glass". The advantages of these products are: 

With each of these solutions, testing is key to ensuring that nothing slips through the cracks. Logging and Event Management must be considered to be a first class Non-functional Requirement with the same level of development rigour applied to a solution - including peer review and tests to ensure that data is properly categorised and partitioned in the tool of choice. 

There are two ways of configuring AppVeyor, either through the web-based user interface or via the file checked into the root of the git repository. I would strongly recommend the latter, you are welcome to start from this template: 

The term "treat your servers like cattle not pets" has proliferated in recent years, particularly when applied to Docker containers and Virtual Machines 

Given this architecture, is it possible to configure Java to perform cryptographic operations using Azure Key Vault rather than Java Key Store or JCEKS stores: 

The strict answer is no, in that there is no direct equivalent to a . However, there are Build Process Templates and you can use the Psake Build Automation tool in combination with templates to reduce the barriers to entry for creating a new build, test and deployment build definitions. It is nowhere near as fluid as Jenkins' Pipelines as Code, AppVeyor's , CircleCI's and Travis' . There is a wealth of information on the ALM Rangers Library of tooling and guidance solutions.