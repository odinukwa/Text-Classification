Suppose we want to multiply $n \times n$ matrices. The slow matrix multiplication algorithm runs in time $O(n^3)$ and uses $O(n^2)$ memory. The fastest matrix multiplication runs in time $n^{\omega + o(1)}$, where $\omega$ is the linear algebra constant, but what is known about its memory complexity? It seems that it may be possible a priori that fast matrix multiplication consumes $n^{\omega}$ memory. Is there are any guarantee that it can be done in $O(n^2)$ memory? Is it the case that the currently-known matrix multiplication algorithms use $O(n^2)$ memory? (I am actually interested in rectangular matrix multiplication, but I assume that the answer would be the same in that case as for the square case, and the square case is better studied.) 

You are looking at this the wrong way. Almost any object can be encoded into a natural number, so it would be possible to have a programming language with a single type. But the goal is to have more types. You want to distinguish semantically distinct operations, even if they are encoded the same way as integers. For example, you might have Boolean conjunction and integer multiplication. They encode in the same way. However, multiplying a boolean by an integer is a type error. By forcing distinct encodings, this type error is harmlessly detected at compile-time. By merging into the same encoding, this error leads to undefined semantics (even if, by the good fortune of the encoding scheme, it will be interpreted harmlessly). 

I am surprised no one has mentioned primitive recursion. By restricting to bounded loops (i.e. the number of iterations for each loop must be calculated before the loop commences) the resulting program is primitive recursive, and hence total. Douglas Hofstadter proposed a programming language, BLOOP, that allowed all and only primitive recursive functions. 

If the graph is a clique (or a set of disconnected cliques), then Luby's algorithm succeeds on the first iteration. 

I would like to add to Dave Clarke's answer. The binary relations are sufficient to express higher-arity relations and functions (by adding new non-logical symbols if necessary). Hence, if you allow multiple directed edges, which can take on different colors, then this is already sufficient to encode any $L$-structure. Furthermore, this encoding can be described in a first-order way. 

Suppose we are given $m$ vectors $v_1, \dots, v_m$ in $n$-dimensional space $\mathbf R^n$ (or perhaps they are specified up to $b$ bits of precision). I would like to find an orthonormal basis for the subspace perpendicular to $v_1, \dots, v_m$. I could do this by Gram-Schmidt but this would appear to take roughly $(m+n)^3$ time in serial, and cannot be done at all in parallel. Maybe we can assume $m = O(n)$ if this simplifies things. Are there better algorithms for this? This seems closely related to matrix inversion so one should expect algorithms running in time $n^{\omega}$ (serial) or NC algorithms in time $\log^2 n$ (in parallel). Any references or key words would be greatly appreciated. 

Colloquially, the definition of the matrix-multiplication exponent $\omega$ is the smallest value for which there is a known $n^{\omega}$ matrix-multiplication algorithm. This is not acceptable as a formal mathematical definition, so I guess the technical definition is something like the infimum over all $t$ such that there exists a matrix-multiplication algorithm in $n^t$. In this case, we cannot say there is an algorithm for matrix-multiplication in $n^{\omega}$ or even $n^{\omega + o(1)}$, merely that for all $\epsilon > 0$ there exists an algorithm in $n^{\omega + \epsilon}$. Often, however, papers and results which use matrix-multiplication will report their cost as simply $O(n^{\omega})$. Is there some alternate definition of $\omega$ that permits this usage? Are there any results that guarantee that an algorithm of time $n^{\omega}$ or $n^{\omega + o(1)}$ must exist? Or is the usage $O(n^{\omega})$ simply sloppy? 

Suppose that, for each $\epsilon > 0$, there is an Turing machine $M_{\epsilon}$ that decides a language $L$ in time $O(n^{a + \epsilon})$. Is there a single algorithm deciding $L$ in time $O(n^{a + o(1)})$? (Here, the $o(1)$ term is measured in terms of $n$, the input length.) Does it make a difference if the algorithms $M_{\epsilon}$ are computable, or efficiently computable, in terms of $\epsilon$? Motivation: in many proofs, it is easier to construct algorithms of time $O(n^{a + \epsilon})$ than the limiting algorithm $O(n^{a + o(1)})$. In particular, you need to bound the constant term in $O(n^{a + \epsilon})$ to pass to the limit $O(n^{a+o(1)})$. It would be nice if there is some general result you can invoke to pass to the limit directly. 

Testing whether two circuits of size $\leq N$ circuits are equivalent. To determine if $C_1 \sim C_2$ you only need to evaluate at the $2^n$ input points. To determine a class representative, one would probably have to test all $2^{\Omega(N \log N)}$ possible circuits. For $N$ sufficiently large this is exponentially harder than testing circuit equivalence. 

Kolmogorov complexity might fall into this category. One can show that there are certain strings, which cannot be compressed by any Turing machine. These strings behave "generically" so you can study the random properties of certain information and computational tasks by studying the behavior with respect to a (non-random) incompressible string. 

Dealing with problems with continuous variables is very annoying with exact algorithms. For example, what does it mean to specify the edge-weights of a TSP instance with exact real numbers? When we allow FPTAS algorithms for these problems, we can quantize these parameters to integers. This makes the problem much better behaved (can use standard Turing machines), but incurs a small error. 

I have often heard it said that you cannot write a program to catch bugs in a web browser, or word-processor, or operating system, because of Rice's Theorem: any semantical property for a Turing-complete language is undecidable. However, I am not sure to what extent this applies to real-world program likes operating systems. Do these types of programs need the full strength of Turing completeness? Are there simpler models of computation (such as PR) in which these applications could be written? If so, to what extent does this allow decidability of program correctness? 

The probability of error when sampling $f(x)$ is $\delta$ when $x$ is chosen at random. Using self-correction, the probability of error is $2Î´$ for all $x$ (not only random $x$) 

Suppose you have two vectors of real numbers $\langle a_1, \dots, a_n \rangle, \langle b_1, \dots, b_n \rangle$, with $a_i, b_i \geq 0$, and wish to compute the convolution $$ c_i = \sum_{j \leq i} a_j b_{i-j} $$ There is an obvious algorithm to compute this in time $O(n^2)$. This obvious algorithm basically involves a sum of many positive summands, so there is no numerical cancellation. Hence the obvious algorithm is numerically stable. The other obvious algorithm is to use a Fast Fourier transform to compute $\hat a, \hat b$, obtain $\hat c = \hat a \hat b$, and then use an inverse transform to obtain $c$. This algorithm is much faster, like $O(n \log n)$ time. Unfortunately, the Fourier transforms require adding many terms with different complex phases, so there is catastrophic cancellation. So this algorithm is numerically unstable. Is there any algorithm which is both fast (near-linear time) and numerically stable? 

In standard complexity theory terminology, one wants to find Turing machines which decide membership in a language. For any given string, there is only one right answer --- either "yes" or "no." But suppose we want a Turing machine $f$, which computes a function of its input $x$. We want to ensure that, for any $x$, we satisfy $A(x, f(x))$ where $A$ is some predicate. For example, given a graph $G$, we want to find some proper vertex-coloring of $G$. We don't care which one. Many of the standard complexity theory notions have analogues in this model. For example, we may want $f$ to be a deterministic polytime algorithm succeeding with certainty (corresponding to complexity class $P$), we may want $f$ to run in expected polynomial time and succeed with probability $1$ (corresponding to class $ZPP$), we may want $f$ to run polynomial time and succeed with high probability (corresponding to $BPP$), or we may want $f$ to run in deterministic polylog time on polynomial-many parallel processors (corresponding to $NC$). And so forth. What is the terminology for this type of algorithm? In many cases, we know that there exists some $y$ satisfying $A(x,y)$, so $f$ can't really be said to decide anything. I could just refer to saying that $f$ is an "NC algorithm", which is really an analogy and not proper nomenclature. Thanks! 

It is standard to express the running time of linear-time graph algorithms as $O(m+n)$ (such as depth-first-search, etc.). For nearly all such algorithms, vertices of degree zero have no effect on the underlying graph problem. So a solution to the graph problem can usually treat all these unsupported vertices jointly, and does not need to process them individually. For example, in depth-first-search, it suffices to list the dfs-tree for the supported vertices, along with an auxiliary record that all other vertices are unsupported. If the graph is presented with any kind of intelligent data structure, it should be possible to process in time $O(m)$, and to generate a data structure that encodes the solution in time $O(m)$. This applies even in the case that $n \gg m$. My question is, what is the reason for the notation $O(m+n)$? Are there really algorithms which require this running time $\Omega(n)$ in the regime $n \gg m$? Is this basically for pedagogical purposes only? 

Over all possible $\pi$, each $\rho(x)$ is uniformly distributed in $[0,1]$ $\rho(\pi(1)) < \rho(\pi(2)) < \dots < \rho(\pi(n))$ (Optional) $\rho(\pi(i))$ depends only only $\pi(1), \dots, \pi(i)$ 

Suppose I want to find the rank of an $m \times n$ matrix $A$ over $GF(2)$, where $m \ll n$. The algorithms for rank in the literature seem to be focused on the case when $m = n$, giving a time complexity of $O(log^2 n)$ and $n^{4.5}$ processors. Are there better algorithms known when $m$ and $n$ are very different? I am thinking that $m \approx \text{polylog}(n)$. In the case when $m = c \log n$, then one can simply exhaust over all $2^m$ subsets of the rows of $A$, and this is actually more efficient than Gauss elimination. 

Feige's well known result (and more recent results) show that set cover cannot be approximated within a factor of $(1 - o(1)) \ln n$, where $n$ is the number of variables. What if we want an approximation ratio in terms of $\log m$? Apparently, there is a folklore result that the approximation ratio must be of the form $\Omega(\log m)$. (The reason for this is that one can assume that the set cover instances which witness the hardness result of Feige, satisfy $m = \text{poly}(n)$). Does anyone know an actual reference for this result? All the sources I read merely state that this is "well-known." Is there anything more specific known about the approximation ratio here? For instance, is it known what is the optimal constant coefficient of $\log m$ in the approximation ratio? Any sources/references would be greatly appreciated