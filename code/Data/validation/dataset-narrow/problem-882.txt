Is the SSID configured on PC(s) that access(ed) the router? Log in to the router over wired Ethernet using telnet (or HTTP if supported) and look at the configuration. If the router is broadcasting its SSID then it should be visible as an available wireless network with a high signal strength. Reset the router configuration (the manual will advise on how this is done) and re-configure it. It may be possible to sniff the router -- but if it is not broadcasting its SSID and no devices are connecting to it (see 1) you may have a long wait. 

Don't make assumptions about log files. Field formats need to be checked. For example: are dates dd/mm/yy or mm/dd/yy?; are numeric fields decimal, hex, octal or something else? Are timestamps consistent (others have mentioned the importance of syncing time between devices: check it has been sync'd or work out what the source of a timestamp would be and correct it)? Are all devices/processes logging at the same log level and to where you would expect them to? Is logging consistent between different revisions of the same software? (checking that log outputs are consistent with previous versions and with the documentation should be on the list for testing new software revisions but can be overlooked) 

A selection (not all of then all the time ) and many are as much for routine hardware removal/insertion and changing ... and don't use them unless you are competent to tackle what you're about to do and (where appropriate) trained, qualified, authorised and have the time and conditions to do the work safely: Small Phillips head and flat head screwdrivers 'pearl catcher' for retrieving the small screw when I drop it (I find the '4-wire' version easier than the '3-wire' for small objects) spanner and large screwdriver for equipment rack bolts anti-static wriststrap Ethernet cable tester to check wiring (e.g. normal, xover, faulty) multimeter (continuity tests, low tension voltage checks) thermocouple or (preferably) laser/infrared temperature measurement device room thermometer / humidity meter radio clock (for accurately setting and checking time) pen torch pen and paper back in the days of serial connections: break-out boxes and oscilloscope cutters for removing cable ties tweezers/forceps for setting DIP switches and handling header plugs spare screws, bolts, thumbscrews for computer cases, cable ties, DIP headers temporary cable labels tape measure (metric, imperial, U) bags to hold removed bolts, screws &c air spray small cleaning kit 

You could set up and additional VirtualHost with the same DocumentRoot and related parameters as your main site but with a different ServerName. To work, the internal links in your own site would need to be local. You can extract the related log entries from a single set of log files or set up different ErrorLog and CustomLog for your new VirtualHost. 

It all depends. For example, Ubuntu (since Hardy Heron) has had a user option to interrupt (and postpone) a routine disk check if it comes up during boot. I make use of this for ext3 partitions if I need a quick boot-up (see $URL$ I would not personally interrupt an fsck following a dirty umount or had any reason to think that a filesystem might be compromised: not least bnecause I wouldn't want to save new data to such a file system and potentially put that data at risk as well as cause further damage to existing data or metadata. 

Size is often not a reliable measure of how well they will provide, develop and support the software you need. No matter the size, companies may change direction, fail, change product ranges, be taken over, replace a product with one inimical to your needs. A very small company may be more vulnerable than a large one to loss of a key member of staff or irrecoverable failure from flood, fire, ... but large companies may lose, or be willing to sacrifice, a small unit for 'the greater good' ... and this may not equate to your best interests. Where possible an exit strategy, including an alternative that can be rolled out in time to keep your business afloat, can make life more comfortable. If you don't have that then the unexpected problem can rapidly become a catastrophe (just one recent example: $URL$ 

seems to support this view but I expect you are probably more familiar with the code than I am and there will be folk on the mailing list who know it in detail and could be more definitive. 

I don't use Microsoft windows so can't advise directly on reliable sources for tools on that platform (I use dig and nslookup under GNU/Linux) but there are plenty of online gateways to dig and other tools, including nslookup, that may be sufficient if you are looking at Internet-based problems. A Google search will turn up lots. I just tried $URL$ and $URL$ and both seemed to work well. Additionally, ISPs often provide these tools on their websites for customers who don't have native tools on their own computers. 

Some techniques to get more information. Write down what you know about the network settings on each device, draw a diagram of how things connect. As you check things and run tests consider whether this information needs adding to or if any of it is wrong. With two DHCP servers on one subnet how do you know that all the devices will get the settings you expect? For example, if the allocation is by MAC address, check the the DHCP server settings carefully and check that PC0 and PC1 consistently get the details you planned for. To distinguish between a DNS and a routing problem, try pinging between PC0 and PC1 and between each of these and each of the DNS servers and routers by IP address using ping -n , or its equivalent in your OS. [ping -n tells ping not to look up the names of hosts when displaying its output, by using this and pinging to an IP address we can avoid the need for DNS in this test]. Some of the responses may not happen: this could be security features on the device or security constraints working correctly or a problem: write down what you tested and what the result was and then think about it. Check the setup of PC0 and PC1 including which nameserver(s) should they use (these must be specified as IP addresses, not as names) and domain names (if the two subnets have different domain names then these need to be specified explicitly or be in the domain search list in the client DNS setup). Check that there are no settings in the hosts files on the PCs that could clash with names that should be resolved using DNS. Try pinging between devices by name: even if the ping fails it will report whether it could resolve the name to an IP address. Use dig or nslookup to query the two DNS servers to see whether they can both respond with resolutions for the names of PC0 and PC1. These queries should be done to both nameservers from devices on both subnets (for example dig @ will attempt to use to resolve ). Check the routing tables of PC0, PC1, the two DNS servers and the two routers. See if all have routes to the 172.16.2.x and 172.16.1.x subnets. I assume, but you should check, that these are all /24 networks (subnet mask 255.255.255.0) but you need to check that all the devices have the same subnet mask setting and that this is reflected in the routing tables. traceroute between PC0 and PC1 (both ways) will confirm that traffic is taking the route you would expect. Somewhere in here I would personally be using a sniffer. You might not be able to interpret the output from that yet and, hopefully, some of these simpler checks and tests will be helpful. This should all contribute to a clearer picture of what does work, what doesn't work, whether these states are stable (i.e. it either works or doesn't rather than works sometimes) and, probably, why. 

Real This was certainly possible, and used, on at least some makes of older mainframes (1970s, early 1980s). The console (a Teletype, if you have ever seen such a thing) had a small speaker and a volume control. When this was turned up the audio output was indicative of the work the CPU was doing. Much of the work was off-line batch work and with practice it was possible to recognise certain distinctive 'signatures' of some jobs in normal mode and the shrill whine of a job in a loop. As the console would become sluggish when the machine was very busy the main use of the speaker was to allow the operator to check whether a job was progressing (the sound would usually vary) or stuck. On the lighter side, it also allowed jobs to be written to create certain signatures, for example a, somewhat atonal but recognisable, "Jingle Bells". 

If you want to refer to part of the hierarchy of one site as another, this is possible in the same way, e.g. the second entry above could be 

I use Webalizer and AWStats. Both will analyse the W3C Extended Log Format, both are licenced under the GNU General Public License (GPL). There are differences between the two in the format of displayed results and if you are producing stats for many sites some users may prefer one to the other. I have also used analog in the past but its main advantage for me was raw speed and I don't now have log files big enough to make it worthwhile using a third analyser. There are differences between the three in the format and extensiveness of displayed results and if you are producing statistics and graphs for many sites some users may prefer one to the other. Many web hosting companies provide a couple of these for online use by customers. The platform you are using it on can be an issue. All three (and more!) are available in GNU/Linux distribution repositories, e.g. Ubuntu 9.04, and are usually ready to run when installed. It may require more work to get the one you want working on other platforms (e.g. I think awstats needs perl; webalizer comes from the author as C source code or as a Linux-x86 or solaris executable). To make a choice between the many analysers available you need to decide which reports you need, which you would like and which you don't want and compare that list to what the various tools offer. Tailoring the output to what you want makes it easier to get out the information you need. You may want to consider several runs through the data to provide different reports, possibly using different tools. For example, a fast run through just to identify unexpected errors (missing files or graphics (404) referenced by other pages on the site and unexpected error codes) can be helpful to the site administrator. Data providers are less interested in those reports but may want to know which pages are most popular, search strings used and numbers of visitors. Network administrators may want to know average and peak total load and which pages generate most load so that they can ask when they have been optimised correctly. Eventually people start to ask questions that none of the tools answers well but experience with several different analysers may postpone that day for a while. Not analysing the logs from the server but relevant to the area, Google provide Webmasters tools that give information about the site from Google's perspective, gained from site crawls. As well as showing the sites ranking within Google on certain search terms and which other sites link to yours it has other information such as which pages Google is not indexing (e.g. because of robots.txt restrictions) and which pages it cannot find. These are a useful adjunct to the log file analysis on the server when looking for errors and missing material on the site.