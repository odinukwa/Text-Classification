The primitive recursive functions are defined over the natural numbers. However, it seems as if the concept should generalise to other data types, allowing one to talk about primitive recursive functions that map lists to binary trees, for example. By analogy, partial recursive functions over the natural numbers generalise nicely to computable functions on any data type, and I'd like to understand how to make the same kind of generalisation for primitive recursive functions. Intuitively, if I were to define a simple imperative language that allowed basic operations on, say lists (such as concatenation, taking the head and tail, comparison of elements) and a form of iteration that requires knowing in advance how many iterations will occur (such as iterating over the elements in an immutable list), then such a language should at most be able to compute the primitive recursive functions over lists. But how can I understand this formally, and more specifically, how would I go about proving that my language computes all primitive recursive functions over lists and not just a subset of them? To be clear, I'm interested in understanding primitive recursive functions as a well-defined class of functions (if indeed they are), rather than just in the operation of primitive recursion itself, which seems straightforward. I'd be interested in pointers to anything that's been written on primitive recursion over general data structures, or indeed in any context other than the natural numbers. update: I may have found an answer, in a 1996 paper called Walther Recursion, by McAllester and Arkoudas. (It's not clear where it was published.) This seems to contain a generalised version of primitive recursion as well as the more powerful Walther recursion. I intend to write a self-answer once I've digested this, but in the meantime this note might be helpful to others with the same question. 

I have a slightly crazy idea that would require me to play around with an actual implementation of such a language - I want to be able to generate arbitrary (self-terminating) binary strings, have them interpreted as programs, and get binary strings as output. Of course, in principle I could just invent some kind of trivial prefix-free encoding of ASCII, run it through an interpreter for my favourite language, and convert STDOUT to binary. However, if I do this, the chances of it doing anything other than terminate immediately (with a syntax error) are vanishingly small. Thus, I also have the following requirement: 

Consider a function $f:\mathbb{N} \to \{0,1\}$ whose is defined in terms of some universal Turing machine $U$. If $U$ halts when given $x$ as input then $f(x)=1$, otherwise $f(x)=0$. Clearly the function $f$ is undecidable. Now consider a new function $g:\mathbb{N}\to \{0,1\}$, defined by $$ g(x) = \begin{cases} f(x/10^{6}) & \text{if $x \mathop{\%} 10^{6}=0$} \\ 0 & \text{otherwise.} \end{cases} $$ It's equally clear that $g$ is undecidable. However, informally it seems that there should be some sense in which $g$ is less undecidable than $f$, since, informally speaking, it looks like the value of $g$ can be determined for $99.999\%$ of its inputs just by checking if they're divisible by a million. My first question is whether there's any sense in which the intuitive claim that $f$ is more undecidable than $g$ can be made into a rigorous one. If there is, I'm interested in whether there are any functions that are particularly "densely" undecidable. I have in mind something like a universal Turing machine that not only halts for about 50% of possible inputs, but also when it fails to halt it does so for a wide variety of different non-trivial reasons. (If this were not the case one could generate a second Turing machine to spot the majority of non-halting cases, so it wouldn't be densely undecidable.) To ask more or less the same question in a different way: the vast majority of randomly-generated strings will halt immediately (with a compile error) if fed to a C compiler. The probability that randomly generated C code will fail to halt is very low, and the probability that it will fail to halt for some non-trivial reason is much lower still. I'm interested in whether one can define a language (or model of computation) that, when given random input, is quite likely to do something strange and non-trivial that makes its output difficult to predict. 

Repeating the self-referential construction above only yields that the time complexity of $s$ has to be at least $f^{-1}$ 

It is not hard to see that a positive answer would imply $\exists A \in \mathsf{TALLY}:\mathsf{R} \subset \mathsf{E}^A$ ($A$ is s.t. $U$ is Cook-reducible to $A$ and we have $\mathsf{R} \subset \mathsf{E}^A$ since it is possible to apply $U$ to the sequence $\chi_L(n)$ to decide $L$ in exponential time). This implication seems surprising but I don't see why it's necessarily false EDIT: Actually such $A$ exists: $\lbrace w | \exists i,j \in \mathbb{N} : i \in L_j$ and $|w| = f(i,j) \rbrace$ Here $f : \mathbb{N}^2 \rightarrow \mathbb{N}$ is a bijection computable in polynomial time and $L_j$ is the $j$-th recursive language (I'm using an arbitrary enumeration of the recursive languages) However a universal predictor in $\mathsf{P/poly}$ implies a stronger statement, namely, that there is $A \in \mathsf{TALLY}$ and some fixed polynomial $p(n)$ such that all languages in $\mathsf{TALLY} \cap \mathsf{R}$ can be computed in time $p(n)$ given an oracle for $A$ EDIT: The answer to the first question is positive. Showing this is equivalent to constructing a Turing machine with a special tape on which some fixed (input-independent) infinite string is written in the initial state s.t. this machine computes a universal predictor in polynomial time. To do this, imagine the special tape to be 2-dimensional. In row $i$ of the tape write infinite computable sequence number $i$ (i.e. computed by program $i$). The predictor then works as follows. Given an input of length $n$ it compares it to the first $n$ infinite computable sequences. The first sequence which matches the input is used for the prediction. If no sequence yields a match the predictor outputs $0$. 

Consider any language $L$. Define $s(L) \in {\lbrace 0, 1 \rbrace}^\omega$ (an infinite sequence of bits) by the recursive formula $$s(L)_n=\chi_L(s(L)_{<n})$$ Here $\chi_L$ is the characteristic function of $L$ i.e. $\chi_L(w)=1$ for $w \in L$, $\chi_L(w)=0$ for $w \notin L$ A language $U$ is called a "universal predictor" when $$\forall L \in \mathsf{R} \, \forall n>>0:s(L)_n=\chi_U(s(L)_{<n})$$ A famous example is Solomonoff induction It is easy to see $U \notin \mathsf{R}$ by considering $L = U^c$. The question is 

To understand the motivation for the easiness condition, note that if $f$ is s.t. the formulas it produces only have existential quantifiers (which implies $f^{-1}(\textsf{TQBF}) \in \textsf{NP}$) then the condition requires that not only $f^{-1}(\textsf{TQBF}) \in \textsf{P}$ but that the $\textsf{NP}$-witnesses can be produced in polynomial time (in fact it is even stronger: we should be able to complete any prefix to an $\textsf{NP}$-witness whenever possible). 

Observations Reductions Let's call a Karp-reduction $f$ of $(L, X_n)$ to $(M, Y_n)$ semi-invertible when there exists $g: \lbrace 0, 1 \rbrace^* \rightarrow \lbrace 0, 1 \rbrace^* \cup \lbrace \bot \rbrace$ computable in polynomial time s.t. $$\forall x: g(f(x))=x$$ $$\forall y: g(y) \ne \bot \rightarrow f(g(y))=y$$ Given $A$ optimal for $(M, Y_n)$ and $f$ a semi-invertible reduction of $(L, X_n)$ to $(M, Y_n)$, $A \circ f$ is optimal for $(L, X_n)$. In particular, if $(M, Y_n)$ is $\mathsf{sampNP}$-complete s.t. the reductions can be chosen semi-invertible (such $(M, Y_n)$ exists since examining the construction of a complete problem in Ben-David et al we see the reductions are semi-invertible), we get optimal algorithms for all problems in $\mathsf{sampNP}$. In particular, $A$ yields an average-case polynomial time solution for any problem formulated as a problem in $\mathsf{sampNP}$ which is "secretly" in $\mathsf{sampP}$. This is similar to the properties of Levin search, but Levin search only solves instances with a positive answer. Uniqueness Consider $A$, $B$ estimators for $(L, X_n)$. Define the "metric" $$\delta_n(A, B; L) := E_{X_n}[\chi_L |\ln{A} - \ln{B}| + (1-\chi_L) |\ln{(1-A)} - \ln{(1-B)}|]$$ If $\delta_n(A, B; L)$ vanishes for large $n$ with superpolynomial speed, then $\epsilon_n(A; L) - \epsilon_n(B; L)$ also vanished for large $n$ with superpolynomial speed. Suppose $A$ and $B$ are both optimal. Then it can be seen that $\delta_n(A, B; L)$ vanishes for large $n$ with superpolynomial speed (via considering $\frac{A+B}{2}$ and using the convexity of the logarithm). So optimal estimators are unique up to "small" deviations. 

Not sure if its immediately related but putting this out here : Is there any obvious reason why this very famous paper $URL$ does not deal with Max-Clique? Isn't it true that the complement of the Max-Independent-Set integrality gap instance does not give the integrality gap instance of Max-Clique? 

Are there complexity theoretic results about recoverability or learnability of the marginals (of the source vertices) and the conditionals (along each of the edges) of a Bayesian network from having access to some subset of the vertices? Are there at least known conditions when this is provably impossible or when some hardness result has been proven? 

Is it possible that the different approximation hardness behaviour of the different NP-complete problems is somehow related to this issue of their optimization versions not having an uniform representation as polynomials (one for each) to be optimized over the hypercube? 

In the other situation when in your notation $k>>d$, in one of our papers we did analyze one such autoencoder in details to be able to say something about what you are asking. Does our Theorem 3.2 here, $URL$ help? It took us about 15 pages of laborious calculation to get this - hopefully its helpful! :D The "??" in the statement is a LaTeX error. It was meant to refer to the previous Theorem 3.1. 

Is there any sense in which one can call Ramanujan graphs to be the "optimal" spectral sparsifiers? (Reference : $URL$ ) (..At least w.r.t $K_n$ can one say that given any $d$ and $n$, any $d-$regular Ramanujan graph on $n$ vertices, say $R_{(d,n)}$, gives the smallest $a$ s.t $x^T L_{K_{n}}x \leq \frac{n}{d} x^TL_{R_{(d,n)}}x \leq a x^TL_{K_{n} }x , \forall x \in \mathbb{R}^n$?..or some such similar inequality?.. ) Does the usual Cheeger's inequality become sharper if restricted to Ramanujan graphs? Or do we otherwise know of graphs which either saturate the Cheeger's inequality and/or maximize any of the combinatorial notions of expansion? The only kind of connection I know of between the spectral gap of an expander to its combinatorial expansion are statements like Theorem 4 and 6 here in these notes, $URL$ But even this is a lower bound and not an upperbound. One can put in $\lambda = 2\sqrt{d-1}$ in these two theorems but that somehow looks too weak and naive. I wonder if something sharper can be said about the combinatorial expansion properties of a Ramanujan expander : may be some sense in which one can justify calling them optimal from this point of view too? 

The closest thing I have found in this regard are things like Theorem 1 (page 12) , Theorem 2 and Theorem 3 (page 14) in this paper, $URL$ But then I am not understanding as to why these would be called "sample complexity" because these theorems don't seem to tell me any lower bounds on the number of samples needed to learn the tensors they are looking at. What one can do at best is to take the RHS of the equations proved in these three theorems and ask for that to be upper-bounded by some error tolerance and hence invert that to get a lower-bound on the number of samples needed. But this is a-priori not the same thing as finding out the minimal number of samples needed to get that accuracy. Am I missing something?