Yes there are benefits in entity complexity, yada yada yada, but the real benefit of an entity component system comes not from the ease of creating new entities, or the ease of changing entire sub-systems; it comes from the ability to leverage cache-effecient algorithms. With components, for example, your physics code only needs to know about the object's position, velocity, and collision volume. It doesn't care about the model being displayed, or that it happens to be tinted green, and it doesn't have to waste space in the cache for the entire entity. Unfortunately, these benefits aren't nearly as great in C#. Short answer : Yes, but don't half-ass it. Make your components data-only, and enforce this rigorously. Define behavior in separate modules that update all like components at once, and separate disparate data into separate components as much as possible. For reference, $URL$ 

1) Friction is the correct solution. It's what keeps you on a moving thing in real life. Once the person has accelerated to the same speed as the platform, inertia will keep them on it. If a platform moves down and sideways at the same time, a player might still appear to 'float off' if they don't fall fast enough to keep up with the platform. 1.5 and 2) The platform should (probably) be a kinematic body. As one, it won't respond to collisions, so it will never rotate, and the player won't be able to push it to the ground. If your physics library doesn't support kinematic bodies, it might have angular inertia, which can be set to an absurdly high value. 3) I've never done this with tiles, but I have with worlds that had little corners the player could get stuck on. My solution was to make the player out of two shapes, a circular 'foot' and a rectangular body. The round foot slid easily over small corners and up slopes. 4) You're probably better off with fewer, larger shapes. Depending on the library, it may perform better with slightly more convex shapes rather than fewer concave ones. A torus is impossible if you use only convex shapes. 5) This sounds like a value you'll have to experiment with. Not knowing specifics of the library, I would guess that a smaller value trades performance for accuracy. 

If it's in the vertex shader's input structure the shader needs it in the vertex stream, whether you use it or not. XNA is enforcing that. You're implementing optional texturing improperly. If you do it this way, you'll need to supply texture coordinates always. Also, branching is still to be avoided. Instead, have two techniques, referencing two different vertex shaders, with two different input structures. One input structure has texture coordinates, the other does not. Then, choose the correct technique for drawing. You don't need to have a 'texture enabled' constant in your shader at all. 

This is very possible. You can use a custom shader with spritebatch. The source to the default spritebatch shader is available from the XNA 'education catalog'. What you will need to do is get this shader and change the texture filtering in the shader from linear to point filtering, and then use your new version of the shader with spritebatch. To use it with spritebatch, call spritebatch.Begin, but set the sprite sort mode to immediate (This effectively means no batching). Then, apply the correct technique on your new effect. Then draw your sprites. Spritebatch will draw them immediately with whatever effect is currently applied. IIRC using a custom shader with spritebatch is far simpler in XNA 4.0. 

Sprite batch is implemented with 'camera-facing polygons with Z set to zero'. IIRC it even has rotation, and if it doesn't, you can supply your own transformation matrix. Your choices are equivalent. Ask yourself how much work you want to do duplicating spritebatch instead. 

This will call the action passed in for every block coordinate that intersects the ray. You should be able to figure out how to make it choose the block closest to the camera. Another solution is to use a raycasting algorithm, but I'll let someone else write that one up. 

To do this in XNA, you will need to use a shader. You will need to render your scene to a texture (Or, render just the portion the cursor covers), and then draw the cursor using that texture and a shader that inverts the color. Unless you've already got a deferred renderer, this will probably be more hassle than it's worth. If you do, you can use the existing depth/color buffers, and re-shade the pixels covered by the cursor in your color-inverting shader. 

If the entire model uses only one texture, theoretically you can set that texture and draw the model's vertex buffer and index buffer in one call, since the entire model is stored in a single vertex and index buffer. Except, when you load the model the indexes for various mesh parts will not be relative the vertex buffer as a whole, but rather the mesh part's vertex offset, making that impossible for no good reason. So to draw the entire model in one draw call you will have to create a new index buffer with corrected indexes. 

There's a better and simpler way to find the closest block under the 'mouse'. XNA provides a ray and an AABB type, and the function Ray.Intersects. Intersects returns the distance to the intersection or null, if there was none. You can test the blocks that may be under the 'mouse/reticle' and pick whichever block is closest. Obviously, checking every block would be terribly in-efficient. You can use a simple tree of bounding boxes to narrow the search down to individual blocks. Here is some code I ripped out a project of mine that does exactly this. In this example, imagine that 'prism' represents a rectangular prism of blocks, and Prism.Split splits the prism on it's longest axis, unless it's smaller than a threshold, in which case it returns false. 

Not exactly: texture arrays are declared in HLSL as for Texture2D and not as an array of texture, so it is quite different. They are almost acting as a 3D texture, where the z is a slice of the 2D Texture (in terms of uv, it is the w dimension). The difference with 3D texture is that they are not supporting trilinear interpolation between the 2D slices but you can still select dynamically a Texture2D slice with a z/w component (unlike an array of texture), the z/w component is rounded to the nearest integer to select the z/w slice. Concerning the memory, I believe this is equivalent, for performance, not sure they give a huge boost even accessing a set of Texture2D compare to an array of texture (but they support dynamic indexing). Using is also easier as you only need to bind it to a single slot. 

As catflier mentionned, moving from a "high level" framework like XNA to a low level Direct3D11 API would require quite some work in order to achieve the same results. But there are now some options that you could also consider: 

Minor issue: don't perform any GPU interaction in the update method but only in Draw (in fixed time step, this method can be called several times per frame). The correct way to implement the micro-synthetic test is to do it like this: 

Instancing requires to change the InputLayout of vertex buffers and pass an instancing buffer along the mesh vertices. Unfortunately, there is nothing automatic to do this with Toolkit models, though possible but would require to dig into the internals to do this yourself. Before even trying to do some instancing with models, you should start with a basic instancing sample with your own raw vertex buffers/index buffer and effect. You will see exactly what needs to be changed and how to setup/use instancing. With this proof of concept working and with the source code of the Toolkit available, you should be able to figure out how to use existing toolkit Model data to turn it into instancing friendly. 

The main reason Direct3D10 Map methods were moved to Direct3D11 DeviceContext is to support multithreading. They were previously attached to each resource (thus implicitly, a single device), but with Direct3D11, It is now possible to update the same resource from different deffered context. Concerning your issue with MapSubresource, you need to check this documentation on Resource Usage. You will see that it is not possible to use Map method with Usage.Default, as it is only working with dynamic texture. Usage.Default is suitable if you are only using UpdateSubresource. The correct way to use Map is to declare the texture with Usage.Dynamic and Map with WriteDiscard. It is not possible to keep the content of the dynamic texture (update partially) as a race condition between the GPU and CPU would arise. 

If your is pointing to the same texture, you should have the same performance, but if not, then you are just hitting a design restriction of SpriteBatch. Most implems of SpriteBatch I'm aware of (at least, XNA, SharpDX, Paradox, DirectXTk... though, don't know for sure about MonoGame...) are trying to batch draws with the same consecutive texture (see for example here, DirectXtk the C++ equivalent of SpriteBatch is assuming the same here - look at method ). If you switch between textures, the code path is much slower so this is not recommended to use SpriteBatch in this way. As suggested by @Shiro, SpriteBatch is usually more efficient if you have packed all your sprites in a sprite sheet and use this same texture in your whole batch between Begin/End. 

Don't know much about internals of MonoGame, but texture are definitely not stored in a ConstantBuffer. The texture is not a constant buffer, it is a texture which has its own slot and is bounded completely separetly from the ConstantBuffer. To get the slot of the texture, this is done through the ShaderReflection object and method reflect.GetResourceBindingDescription(i);. 

Using this format when declaring a depth stencil buffer, you should be able to copy the depth buffer to another R16_FLOAT/R32_FLOAT...etc. texture. On a side note, it is often not recommended to read back data on the CPU because of the latency that will be introduced. Current techniques - on Windows Direct3D11 - tend to perform typical CPU computation on the GPU with DirectCompute. 

When using an Effect in the Toolkit (which is similar to the XNA Effect or the native D3D11 Effect API), you are not suppose to access directly the constant buffer but instead setting parameters directly on the effect: 

If you are creating a Texture2D with an initial DataRectangle, the Pitch must be set to the number of bytes per row and is theoretically equal to TextureWidth * SharpDX.DXGI.FormatHelper.SizeOfInBytes(Format.XXX), unless you are laying out your data differently in memory. If your are using DeviceContext.MapSubresource, you can't determine in advance what would be the stride and you need to use the Pitch returned by the DataBox.RowPitch. The stride could be hardware dependent, depending on the layout on the GPU memory. 

Yes, this behavior changed from DX9 to DX10+, don't know much about the real story behind it, but I suspect that DX9 had to remap registers between VS and PS at runtime anyway (when linking shaders) which is inefficient. In DX10+, when you compile a VertexShader and a PixelShader, you can see which register will be affected to a particular semantic (see output of fx.exe compiler), and these registers (which will almost map to an hardware register) should match between the output of a stage to the input of the next stage. Note that the input of a stage could have less semantics mapped, as long as the order is the same and there is no gap in the mapping. If you are using some legacy FX file (techniques/passes) with DX10+, the compiler will check input/output signatures for you. But if you are compiling separately VS and PS, you will have to double check any signature mismatch (or you can use D3DCompiler API to verify this yourself). There is one exception for the input of the VertexShader that is coming from the VertexBuffer and the InputLayout. The InputLayout is able to specify the byte offsets of each vertex elements and how it maps to the semantic in the VS shader, so you can have a "sparsed" mapping, the input layout can have more vertex elements/semantic bindings than the VS input, as long as all semantics from the VS are covered by the InputLayout declaration. This is handy when you have a model that has several vertex elements, and want to use the same vertex buffers for several vertex shaders that are expecting different input layouts: you can map the layout of the vertex buffer to the layout of the vertex shaders (as long as the mapping is covering all the input of the VS shader).