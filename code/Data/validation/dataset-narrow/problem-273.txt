To migrate data between different storage engines in MongoDB 3.0 you will need to perform an initial sync to the new replica set node or use to seed a new replica set with your data. If you want to add a node with a new storage engine to a live production replica set, is the correct approach. To ensure a smooth upgrade I would also upgrade all existing nodes to the latest version of MongoDB 3.0.x with MMAP storage (the default) before adding your new WiredTiger node(s). The key information in your question (and comments) is: 

The problem is that your query object uses the same key () twice, so the second value is overwriting the first. Most languages do not support duplicate keys in a standard object / hash / dictionary representation. The outcome is more evident if you evaluate your query object in the shell: 

If the reason for your performance challenge is a lack of resources, you should be adding dedicated resources (such as more RAM or faster storage) rather than sharing existing resources. Note that even with replica set members on multiple servers, secondary reads are not a panacea. For some considerations, see: Can I use more replica nodes to scale?. Before adding additional server resources, I recommend starting with optimising indexes to support your common queries and aggregations. See: Index Strategies in the MongoDB documentation. 

The tool is only intended for importing simple data in text formats (JSON, CSV, TSV). To backup and restore a full database (including index definitions & collection options) typical approaches are to use either: 

A standalone server limits your backup options if you also want to keep your deployment available while taking a backup. Here are a few suggested approaches in order of most to least recommended: Approach #1: Use a cloud backup service For the easiest short term solution, I would consider using a commercial cloud backup service like MongoDB Cloud Manager. MongoDB Cloud Manager provides continuous backup with scheduled snapshots and a retention policy (see Backup Preparations for more info). A cloud service also avoids you having to deploy any extra servers/infrastructure, so even if you plan to do so in future this is a helpful short-term solution. The general approach would be: 

As at MongoDB 3.4, the source IPs that are able to establish a connection are determined by your firewall settings. There is also a setting for MongoDB which determines which network interface(s) listens to. Once a connection is established, access control and authentication is managed by your MongoDB configuration. In your example, you are able to connect via the shell because the source IP is not blocked by firewall configuration. Assuming access control is enabled and authentication has been properly configured, establishing a connection to MongoDB does not provide any access to data. There is a relevant feature suggestion in the MongoDB issue tracker you can watch/upvote: SERVER-22084: Add IP-based login restrictions for users/roles. For more information on best practices, see the MongoDB Security Checklist. 

The error message in your screenshot indicates you are trying to add identical members to your replica set under different names: 

MongoDB 3.4 Another option worth mentioning for future consideration (although generally not the fastest path to enabling authentication on an existing deployment) would be upgrading to MongoDB 3.4. MongoDB 3.4 includes a new option that enables a rolling upgrade to authentication; see: Enforce Keyfile Access Control in a Replica Set without Downtime. 

A MongoDB arbiter cannot automatically become a secondary or a primary node, as it does not have a copy of the data set. If you try to manually reconfigure the arbiter as a regular node via you should get an exception similar to: 

I believe your specific chunk map error is likely due to the hashed index and sharded Map/Reduce both assuming they should create the initial chunks for an empty collection. When you create a Shard a Collection Using a Hashed Shard Key: 

The setting controls the frequency where the in-memory view is synced with the on-disk view of data (aka the "background flush interval"). By default, the is 100ms and the (background flush) happens every 60 seconds. If you adjust the too low, you can end up creating a lot of additional I/O (and reduced performance) because writes to dirty pages can no longer be effectively batched together and the same page will be re-written multiple times. If you adjust the higher, you can create I/O spikes (and reduced performance) with a large volume of changes being committed in a single batch. The and parameters are available as tuneable settings because in some cases it may make sense to adjust these (for example, to try to temporarily help an under-provisioned I/O system by reducing write spikes). In a healthy production system, it would be best to leave these settings as-is. For more information, How MongoDBâ€™s Journaling Works is a helpful read. 

I haven't personally tried Hydra (and there is currently a caveat in the Github repo that the authors only tested with MongoDB 2.2.3 on Ubuntu 12.04), however it seems like a promising approach. 

If you've restored a sharded cluster from a backup, the shards will already be configured. If host names have changed from the original deployment, you will need to update the information via a rather than adding new shards (and then restart the instances in your sharded cluster to ensure the new names are used). See the MongoDB 3.0 Restore a Sharded Cluster tutorial for more specific details. 

There isn't an absolute number. General factors include resource challenges such as the size of your data relative to RAM, available network bandwidth, and how quickly your data changes. Typically if you have sufficient data or workload to warrant sharding, you've also outgrown as a backup approach. is going to read all data into memory which will have significant impact on working sets for shards if your data is much larger than available RAM. You also need to have enough disk space to save a complete backup (or compressed backup for MongoDB 3.2+) of the data dumped via a single , enough network bandwidth to cope with the increased traffic, etc. For your specific use case definitely isn't a recommendable strategy for several strong reasons: 

The MongoDB Extended JSON format allows representation of additional BSON data types that are not part of standard JSON. There is a subtle distinction between: 

Depending on your version of MongoDB server there are a few ways to approach this. In MongoDB 3.6+ you can use the expression (which is only when the first array given is a subset of the second) via the operator: 

I've listed some alternatives for connection management below, in order of most to least recommended. Increase the connections allowed on the server The total incoming connection limit on the server is determined by the lesser of the limits imposed by the operating system or (aka in MongoDB 2.4 and earlier). Typically Linux distributions limit file descriptors per process to 1024, of which MongoDB will use 80% for incoming connections (leaving about 819 available connections). You can check current and available connections in the shell via: 

As per the definitions you've quoted in your question description, assertions are logical tests that trigger exceptions on failure. These are used within most automated testing suites as well as at runtime. In test suites assertions confirm that the results of a function call or data update are as expected and are often set up as macros indicating the comparison type (eg. , , ). In the runtime context assertions are safety checks that cause the current operation or process to abort when it is unsafe or impossible to continue. The usage and behaviour of assertions will vary by product. For the specific case of the MongoDB codebase, there are several types of runtime assertions used. Quoting from the MongoDB Server Exception Architecture documentation these are (as at MongoDB 3.4): 

Your chosen shard key for a collection defines the granularity of possible chunk splits. Chunks represent a range of values for the shard key. With a single field in your shard key (), that means the smallest possible range is a single value such as . The chunk size setting determines when MongoDB will try to split chunk ranges in order to help distribute data in a sharded cluster, but does not put an upper bound on the size of the data in a chunk range. When the maximum chunk size is reached for a given value, that chunk will be marked as an indivisible chunk and the balancer will no longer attempt to split or migrate that chunk. New data will continue to be added to that chunk leading to an imbalance in data distribution as you observed in your testing. 

Add any other parameters needed, eg if you need to auth against another database. If you have users in the database on your config server, you will also want to dump that as well. (optional) Remove files from your dump except for the user/role information. If you are certain nothing has changed since you did the original migration from mmap to WiredTiger you could skip this step, however it would be safer to not overwrite any existing data. Preview the files to remove: 

When you add a new shard using you are providing a seed list of members which will be used to discover the current replica set configuration. You can specify as few as one member of the replica set or as many as all, and you do not need to specifically include the arbiter. FYI, the shell helper wraps the underlying command (which has a few additional options not currently exposed via the helper). 

An important caveat is "since the last background flush". Multiple updates affecting the same pages within a given sync interval will effectively be batched. If you're trying to get to the bottom of performance issues then consistently high background flush times (particularly as a large or increasing percentage of the default 60s flush interval) are definitely of concern, but should be reviewed in the context of other metrics such as page faults, I/O stats, and lock percentage. I would also review the MongoDB Production Notes for general tips and upgrade to the latest MongoDB production release for your major version (i.e. latest 2.6.x or 3.0.x if there's a newer than your current version). 

Validate output details will vary based on storage engine, MongoDB server version, and configuration. The common and storage-engine specific validate field output is better described in the MongoDB 3.4 documentation but hasn't been updated for 3.2/3.0 yet (I created DOCS-9766 to do so). I can see (which is a common field) in your sample output. The extent and deleted/freelist details are specific to the MMAPv1 storage engine, so aren't included since you are using WiredTiger. 

Your best option would be to restore from your most recent MongoDB backup. In a standalone MMAP deployment without backups, the only other possibility would be writing custom data recovery scripts to try to identify and salvage deleted documents. This involves some significant guesswork and manual effort to clean up data. Deleted records will be missing some essential information such as the size of the associated BSON document, and there may be variations in internal storage depending on the provenance of your data (for example, if all data was created in a specific version of MongoDB versus data files that have been upgraded through successive major releases). See StackOverflow: Is there any way to recover deleted documents? for some possible approaches. 

My recommendation would be to set up a single MongoDB deployment (ideally starting with a replica set or hosted service with high availability & data redundancy): 

A configuration with three voting nodes in different data centres plus an additional non-voting delayed secondary is definitely supported. This is actually a reasonable configuration in terms of satisying core availability requirements using the distributed voting nodes with additional hidden/non-voting nodes for special use cases. A delayed secondary should be always configured as and priority 0, and may have 0 or 1 vote. However, there is one important caveat with your deployment: you have three data-bearing nodes but one of those is delayed. This will be problematic if you want to use any write concern higher than the usual default ( aka acknowledgement by the primary). In the event one of your non-delayed secondaries is unavailable, a higher write concern (eg or ) could still be satisfied but the writes would have to wait for acknowledgement from a second data-bearing node which would have to be the delayed secondary. A more robust configuration would be to replace the arbiter with a data-bearing secondary so that unavailability of any of the three voting nodes would still allow for acknowledgement of majority writes. 

The only applicable step in this list is "Restore backup to primary". With you are exporting data and index definitions, and must into the primary to rebuild the data files. Your secondaries must be online while you are restoring and be able to keep up via replication. You can use the option to wait for a writes to be acknowledged by a majority of the replica set and ensure your restore doesn't outpace replication.