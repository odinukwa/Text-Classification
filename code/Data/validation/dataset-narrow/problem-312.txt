You can use instead of . With a severity of 0 the two are equivalent but there is some primitive string formatting functionality available as below. 

And has a very high datatype precedence (only beaten by user-defined data types and they would prevent the method from working anyway). Personally I don't find this any more understandable then just finding the conjunction of the individual results though. 

You might also be encountering some degree of blocking. When I test a on a table with pages pulled from cache it seems to be much more CPU bound than your results show (more like 90% CPU time than 50%). You could try with and see if that improves matters. If it does you will need to determine whether the returned is likely to be sufficiently accurate for your purposes. 

If you were to change to and actually store dates before 1753 you would need to change the lower bound condition to . This also won't bring back dates matching the higher bound of - but in the real world this is unlikely to cause a problem. 

You don't need 30 join conditions for a here. You can just Full Outer Join on the PK, preserve rows with at least one difference with and use to unpivot out both sides of the ed rows into individual rows. 

You can also do it within the same scope and in the same transaction if the table does not yet exist. 

Ranges in the pattern syntax use the sorting rules of your collation. Use a binary collate clause so the range is ordered by character code. (I also changed it to as I find that more obvious than ) 

(From previous experimentation I'd found the highlighted bytes were the only ones that changed between runs so these are definitely the right ones). One surprising aspect is that = . This is exactly 3600 (one hour) less than reported by . I believe this to be some attempt to disfavour pages being kept in cache by calling itself. For a "normal" page select this one hour adjustment does not occur. After running 

The difference being that the first one calculates concatenated strings for all rows in and then removes duplicates for . The second one finds distinct first and then just performs the string concatenation work on those distinct values. So in your example data instead of performing the work 5 times (twice for test and 3 times for Nadal) then throwing away three of them it will just perform the work 2 times, once for each. 

One other alternative would be to block simple parameterisation by adding a redundant as below. But I don't recommend this as then you will get plans compiled and cached for every different literal value that you pass. 

But in practice it does seem the garbage from rollbacks is dispensed with quicker than for commits so perhaps the aborting thread does clean up its own garbage at least some of the time. To see this I created the following proc (in a memory optimised database) 

No. At least in my testing both of them have identical execution plans and identical locking behaviour. Both place an lock on and a schema stability lock on straight away and then follow the same behaviour in doing the clustered index scan on taking locks on the page, locks on the , then in the event of a match converting the page lock to then the key lock to prior to deleting the row. This is exactly the same pattern as if the hint on is removed entirely so it is pointless in this case. My original results had some differences between the two queries in that the Join version has a series of apparently aquiring and immediately releasing schema stability locks on that didn't show up in the version. That seems to be related to whether the "Include Actual Execution Plan" option is enabled in SSMS. Today if I have this option enabled this series appears in the locking info for both of them so not sure why originally it only appeared in one. Maybe some timing issue. You can see this locking information using as below (with "Include Actual Execution Plan" option disabled). Code 

In addition to JOINs another benefit of indexing Foreign Key columns is that it can speed up enforcement of the Foreign Key constraint for some DML operations. If you delete a row from an table then the RDBMS would need to ensure that this would not leave an orphaned row in . Obviously this is easier if it can be verified with the use of an index. 

The recorded in the transaction log does not contain any unique identifier related to stored procedure. If you remove the from then the result of the final select changes to showing that the executed from just rolls back to the most recent (unrolled back) savepoint of that name and takes no account of stored procedure nesting. 

The read operation read the uncommitted value and the final result was used in the even though the that was read was eventually rolled back. 

For me about 50% of the time this second query returns a row with different values as the value changes between the two seeks on . 

No. There isn't even something that returns the names of all of the available built in functions (such as itself). These property values are documented for even though they do also appear to work for and probably should be documented there (the return type is so there is no reason that they shouldn't be supported there and it is more convenient to work with that function without needing a cast from sql_variant.) For entirely undocumented property values (such as for ) I'm not aware of any way of determining these. 

For the first query you can see different ordering if SQL Server uses an allocation order scan. So make sure the table is at least 64 pages in size and that the allocation order isn't the same as key order then run the query at read uncommitted isolation level. 

The etlUser should not have permissions to the table (and certainly not to the column) so these are explicitly denied above. The ETL process truncates so is given table permissions on that. This is flagged during a security audit. How dangerous is this scenario? 

Given that the table only contains 3 rows it is easy (if we assume no foreign keys) to contrive a situation where the statistics are created and then the data is altered in a way that dramatically effects the actual number of rows returned but is insufficient to trip the auto update of stats and recompile threshold. As SQL Server gets the number of rows coming out of that join so wrong all the other row estimates in the join plan are massively underestimated. As well as meaning that you get a serial plan the query also gets an insufficient memory grant and the sorts and hash joins spill to . One possible scenario that reproduces the actual vs estimated rows shown in your plan is below. 

I would probably abandon the views here and use the new views (as opposed to the backward compatibility ones) instead, or at least materialize the results of the into an indexed table first. Recursive CTEs always get the same basic plan in SQL Server where each row is added to a stack spool and processed one by one. This means that the join between will happen as many times as the result of the following query . Which I assume in your case (from the 11 mins execution time) is probably many thousands of times so you need the recursive part to be as efficient as possible. Your query will be performing the following type of thing thousands of times. 

You would need to write triggers that route the Inserts/Updates/Deletes to the appropriate table. This could initially be based on whether Id was <= 2147483647 or not but that isn't going to work if you try and migrate rows in the background from the legacy table to the new one so probably best to do the following. Delete trigger Apply deletes against both tables by joining on id Update trigger Apply updates to both tables by joining on id Insert trigger Route all inserts into the new "YourTableBigInt" table. It shouldn't be possible for an insert through the view to enter an explicit identity that might clash with anything in the original table as any attempt to will fail now that is actually a view. You could then have a background process that deletes batches of rows from and s them into . Once the original table is empty you can drop it and the view and rename YourTableBigInt to YourTable. 

I'd hope that the predicate on gets pushed up into the selects from the source tables. Check the execution plans to be sure. 

Just to add to the existing answers. The SQL Server 2008 Internals Book (pp 175-177) implies that detaching the database, deleting the log file and reattaching the mdf file ought to be quite safe as it says. 

So it is clear that the number of waits is much higher in the table case. I'm not aware of any way of adding the wait resource to the extended events trace so to investigate this further I ran 

This is somewhat subjective but I'm not at all a fan of anyway and normally replace it with an explicit and as the datatypes, column names, and nullability can then be seen much more explicitly (and both can be minimally logged). In this case if you were to create the temp table with an unnamed primary key constraint on the column up front 

How valuable is the lost data, how many schema changes have there been in the last three months and what exactly was the nature of the corruption? Some of it may be recoverable if you are prepared to spend sufficient time on the project. You could download the source code for OrcaMDF as a starting point. 

Computed columns that are only d columns are persisted to the NCI leaf page and do not have the requirement that they also be persisted in the data page. 

From your previous questions you use SQL Server. So you can use the & operator. e.g. to see if the bit for 4 is on (and assuming should return ) 

To give your desired results (online demo). In the event of tied Year,Reference,Int2 this will arbitrarily assign a sequential numbering between the tied rows. 

As alluded to by @Souplex in the comments one possible explanation might be if this column is the first -able column in the non clustered index it participates in. For the following setup 

Before the transaction is committed the log must be physically written to disc up to that point but those writes don't get included in the write stats reported Profiler. For many inserts no physical writes may occur before the statement completes as the page is modified in the buffer cache and not written out to disc until later (e.g. by the checkpoint or lazy writer process). For bulk inserts there is an eager writer so the pages may be written out to disc before . See Writing Pages in BOL for more. However the above doesn't seem relevant as even when no pages are written to disc the physical writes can be reported as non zero. In the test below Profiler reports 5 writes for the insert but monitoring the file writes with process monitor and shows that none actually occurred. From which I conclude that the physical writes column actually shows the number of pages made dirty (on the grounds that these will need to be written out to disc later) 

Well perhaps you have good reasons. As you aren't going to share them it isn't possible for us to comment on them. 

The above gathers the statements waiting on locks for the threshold amount of time but doesn't give the specific lock resource. I have never used this event and have no insight into how much overhead this session would cause on your production server. I found this video on the topic. That does strongly recommend filtering on to reduce the number of events collected and I have done so above. It also mentions an old legacy undocumented command 

Just as an addition to Brent's answer when using a non covering index to avoid a sort there is a potential issue with later page numbers that can be seen from running the below 

The value for is stored on the database boot page. (page 9 in the primary data file). This is not written to every time a timestamp value is allocated. Instead SQL Server reserves a few thousand at a time. For example if is and the is also then SQL Server updates the value written in the boot page to the next time it needs a timestamp value. The values from are allocated in memory and "lost" if the database is set offline and then online again. The backup contains the copy of the boot page that has been updated to . So upon restoring it the timestamp values will start from this number. It knows nothing about any lost intermediate values. To see this 

There is a single seek predicate on . Meaning that SQL Server can perform an equality seek on the first two columns and then begin a range seek starting at and ordered (as the index is defined with ) The operator will stop requesting more rows from the seek after the first row is emitted. When I create the partition scheme and function 

Both of these pages being latched belong to (different) non clustered indexes on the base table named and . Querying during the runs indicates that the number of log records added by the first execution of each stored procedure was somewhat variable but for subsequent executions the number added by each iteration was very consistent and predictable. Once the procedure plans are cached the number of log entries are about half those needed for the version.