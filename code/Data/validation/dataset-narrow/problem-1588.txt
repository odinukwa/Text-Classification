You can certainly use an LSTM for this approach, as well as VAR, or potentially a more 'traditional' model (random forest). I have implemented multivariate LSTM for t+x prediction in Keras. Feature engineering may definitely help improve results. I have also used VAR and was surprised at the effectiveness of this when compared against the LSTM. There are packages in Python and R for VAR. Alternatively, if your features are effectively describing a pattern that is not necessarily dependent on the time component, a more traditional method such as random forest could be used. Probably not as effective as VAR or LSTM, but it's fast. 

Referring to $URL$ you are using teh predict method to derive the binary response. If you call predict_proba, you will get an array of probabilities corresponding to the binary response. 

I think a random forest would be appropriate in this case. If you have annotated historical data (records of all the variables you described as well as if the tube passed or failed), you could build a random forest, determine variable importance, and link both the categorical data (human operator) and continuous data (temperature, pressure...) to the tube outcome. A benefit of random forests is that they can be more easily described in terms of a decision tree. R has some nice visualization tools for decision trees, and in my experience, management has an easier time accepting decision trees then other methods. Let me know if you need more detail and I can edit this response to include some sources on random forest, or more in depth explanation. 

There are two steps to this problem: feature selection/ dimensionality reduction and selecting the predictive model. Selecting the 'best' features to use in the model will often improve the accuracy, and there are quite a few methods you can use. 

One option would be copy the source code and create a function similar to pairplot, but with the ability to plot boxplots. 

I have worked on similar projects (using medical images such as PET to predict outcomes). A method being used more and more for predicting cancer treatment outcomes is texture analysis: $URL$ Another method of texture analysis uses wavelet transforms: $URL$ After deriving the texture features, you could then use those in a ML model for making predictions. A benefit to texture analysis is that it is mostly independent of the size of the target. It is however, more dependent on resolution. If your images are produced on different scanners, you may need to resample in order to normalize the images. There are fewer studies about deep learning for medical imaging, but it definitely has some exciting potential. The question is whether or not a CNN can pick up subtle differences. From what I've seen (I'll look for a reference), CNN hasn't outperformed any other method yet. Maybe add a few more details so we can get a better idea of what your objective is? 

This wouldn't be possible in seaborn without changing the source code ($URL$ pairplot is designed with only two options for the non-diagonal plots: scatter or reg. This can be seen in the source code: 

Overfitting is generally a result of the data and structure of your model. The 'advanced' algorithms you mention have specific uses that may or may not out perform other methods depending on your objectives and your data. Here is a source for some further reading: $URL$ 

This was going to be a comment but it grew to an answer. I think there should be some clarification because the question itself is not specifically about visualization but checking the "integrity" of the features. PCA will work for make a generalization of the dataset as a whole. It is a very standard starting point for exploring data. If the first 2 components do show a clear separation, this is a pretty solid indication that at least some projection of your data can well represent your classes. So, short answer is yes. If the first 2 components don't show separation, that does not mean that the features are necessarily bad, it just means that the first two components do not explain the majority of the variability in the dataset. Feature selection is used to check the integrity/importance/value of individual features and how they affect the model. You can use random forest gini importance to rank your features or lasso regularization to with cross-validation to find out how individual features are weighted in a log. reg. model (this does require a little more work as the weightings are not necessarily an exact measurement of variable importance). Feature selection and cross-validation are the most direct ways of determining feature integrity. PCA is mostly a good first pass and helpful visualization. 

One way is to approach as a traditional probability problem: probability of simultaneous events. $URL$ This may not apply because this is more focused on probabilities of events occurring simultaneously and it sounds like you want the probability of an event occurring (purchase of a product) given another event occurring (not independent events). A method of handling this could therefore be categorical clustering. This post has some very good explanations of k-modes clustering: K-Means clustering for mixed numeric and categorical data MATLAB also has an implementation of this: $URL$ 

Sounds like you are on the right track. There are many ways to attack this problem. If you want to visualize the clustering, it would help to reduce the data down to two components. This could be done via PCA or manifold learning if you want to go non-linear ($URL$ In terms of clustering, there are many different methods to do this. Here are some comparisons of different methods: $URL$ 

Without putting in the time to look through Azure's documentation, my guess is that their PCA method is really just a way to do a feature reduction, then use some algorithm they have to classify. Best thing to do is try both methods and then CV and compare performances. gallery.cortanaintelligence.com/Experiment/ 

Part of the problem with answering this question is there are actually two questions. The first: Are there any image classification algorithms which are not neural networks? Yes, lots. But now the actually question: Is there any paper which tries an approach which does not use neural networks in LSVRC? In your question, you rule out methods such as kNN and SVM because of speed. Bag of Words is one method used to solve this problem. MATLAB has a good demonstration ($URL$ But BoW incorporates k-means clustering, so that may not fit your needs. There are some other interesting image classification methods such as texture analysis. TA is being researched as a way to classify malevolence of disease in medical images (such as tumors). Here is a commonly referenced paper: $URL$ Here's an overview of image classification: $URL$ 

I would highly recommend doing some research into the architecture of random forests. There are many sites that provide in depth tutorials on RFs (Implementation in Python). Quick explanation: take your dataset, bootstrap the samples and apply a decision tree. Within your trees, you want to randomly sample the features at each split. You should not have to build your own RF using fitctree however. You don't want to control each individual tree in the forest. This introduces bias, and the point of the RF is that by bagging many trees, you remove the risk of overfitting. Define your hyperparameters and let the algorithm do it's thing. Carefully cross-validate to ensure you are not under-fitting. 

My background is Physics undergrad, Physics grad, now in industry. Some advice off the top of my head: 

Yes, F1 matters. It is telling you that your classifier is basically a coin toss for true positives. AUC and accuracy are high because of all the negatives. So, how can you improve your score? Try this: $URL$ 

The simplest, and likely best, way is to just rebuild the model every time you add more data. If you want to build on existing models, one way is to average multiple random forest models. So you build a model on 20k, test it, then decide to build on it by building another random forest model on a separate set of 20k and averaging the model parameter results with the previous model. This is basically a poorly described ensemble method. Check out this: $URL$ 

One possible reason is that when you use one-hot-encoding for categorical data, you should set the intercept property in the function to be False: 

seasonal_decompose returns an 'object with seasonal, trend, and resid attributes.' We can access the data by calling the object: 

Look into this package for Python: $URL$ You can use this to generate a numeric value representing the similarity between word. Here is a similar post that should help: $URL$ Additionally, a level up in complexity would be to use t-SNE on an array generated using word2vec (this is word embedding). Examples and resources for this are: $URL$ $URL$ 

Check out this article: $URL$ Looks like the answer is yes, your DWH can be defined as datalake because you are adding unstructured data. Here is an excerpt: 

After you have selected the best features, you want to choose the right model for binary classification. The go-to model in this case is logistic regression. There are multiple hyperparameters in sklearn.linear_model.LogisticRegression and in order to get the best results, you may have to perform some grid searches to find the right parameters. Here are some additional resources that may prove helpful: 

Switching to Python from Matlab is going to be somewhat dependent on the field you are in (for example, if you are in image processing/ imaging, Matlab is pretty well solid and so swithcing will be more difficult) and the stubbornness of your coworkers/boss. While documentation for Matlab is easy to find, it's also all you get, whereas Python packages generally have decent documentation as well as countless posts on SE in which someone has likely already answered your question. One of the greatest aspects of Python is that it is open source, but it can make finding the right tool slightly more time-consuming. Your example is about resampling. Pandas is designed for time-series (see pandas doc). All the arguments are described, but I can also look at the source code if I really need to understand anything more in depth. Or I search for some examples online: $URL$ But it seems like you are actually interested in interpolation functions. If we search for "Python interpolation" we will discover a few methods. Looks like Scipy has an extensive package for interpolating: $URL$ It takes some time to figure out how to search for what you need in Python, but once you get the hang of it, there are a lot of great examples online of all sorts of application. Plus, plotting in Python (and R) is way beyond Matlab (c'mon mathworks). In the end, you can do what I did which is flat out refuse to use Matlab unless it is actually necessary and hope they don't fire you! 

Try generating a dictionary of patterns you want to identify. You can then use convolutions/ cross-correlations to identify where these patterns appear in your data. $URL$ $URL$ $URL$ This method is also called 'matched filter'. 

I added the last link, discussing bias vs. variance tradeoff, because I think it is important to understand when testing your models. 

This will avoid the dummy variable trap: $URL$ You could also use dummy encoding to avoid this problem: $URL$ 

Here is a pretty solid notebook for you to compare with (their random forest yielded 0.98): $URL$ Have you tried adjusting the number of estimators in your random forest model? It looks like you are using 10, but more should help improve the model. Try 100. 

This line derives the indices in order to split the data into the train/validation data sets. Now that we have the indices, we use those indices to actually split the data. It is interesting that they wrote next(iter(kf)) and then input the indices into the dataset when they could use sklearn.cross_validation.train_test_split. train_test_split is just a wrapper for next(iter(kf)), but it is more readable and it's already a function in sklearn. Hope this helps! 

A KFold split will take the data and split it however many times you designate. StratifiedKFold is used in order to ensure that your training and validation datasets each contain the same percentage of classes (see sklearn documentation for more). The function StratifiedKFold takes two arguments, the array of labels (for binary classification this would be an array of 1's and 0's) and the number of folds. They have designated the number of folds as 1./eval_size where eval_size = 0.10. So this is a 10-fold validation.