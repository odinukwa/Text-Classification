You could also use window functions which are usually faster,to get OrderNumbers that contain two rows, meaning contain both 'ghi' & 'abc' 

There is a workaround for that issue. It will require an additional table,a trigger, and an agent job to be ran before transaction log backup job. So if you think its worth going through that than bare with me: You should create a table where you should add Database name,whether database was updated or not (you will need this for an agent), and some other info needed for backups such as : 

When you use the nodes() method with the XML column from the table, you must use the APPLY operator. Example: 

You can also use extended events and capture high resource consuming queries, with included user names 

Like i said there are other ways, including roles but these two could get you a job done. Just remember you cannot track a user that executed procedure by specifying 'execute as'(login is possible tho). Also creating a certificate/database keys can be a headache when you`re migrating DB, or simply restoring it somewhere else. 

After delete trigger is executed after the record has been removed from the table. Joining a table Emp will yield no result because record with that ID does not exist in that table. Also note that inserted table will always be empty in after delete trigger. 

In your config.ini file, check that you haven't specified the host that is allowed to connect to this ndbapi slot - check the 'hostname' parameter in your config.ini file or run "ndb_mgm -e show". If you leave out the hostname parameter then you can connect the data node from any host Firewall. Temporarily turn off your firewall and see if that fixes it. If you need to lock down the ports used by the data nodes then set the ServerPort parameter in the [ndbd] sections or the [ndbd default] section of the config.ini file. The firewall also needs to allow port 1186 (by default) for the management node. You've filled up all of the API slots - run "ndb_mgm -e show" to check. If needed, add extra [mysqld] or [ndbapi] sections to your config.ini file and restart your nodes. 

It's dependent on the connector that you're using. For example, if using Connector/J then you can provide multiple server addresses in your connect-string... $URL$ Andrew. 

I agree that it's worth turning off the firewall temporarily to see if that fixes the issue. If it doesn't, a couple of other things you could try... - running with a single replica is very unusual, try setting it to 2 and starting a second ndbd process - try including the connect-string on the command-line when starting the ndbd(s) I've never had to alter selinux to get Cluster to work. Something else you could do is try running all of the processes on a single host - this is stepped through in the MySQL Cluster quick-start guides. For multiple hosts see this blog post on running MySQL Cluster over multiple Linux hosts. 

As specified ,returns table therefore SQL server checks the underlying structure of the table because it needs to know what kind of table is begin returned. How many columns it has,if any of the columns are without names(in aggregate cases) and inform you about it. You can test this by running the query above without referencing the column name( 'as user name' ) in case username table does not have a specified column name. Note: I had to tweak this two functions a little bit, to make them work. Hope this clarifies what is going under the hood for you 

See if you can find SOS_SCHEDULER_YIELD & CXPACKET waits. If SOS_SCHEDULER_YIELD waits are high you might have some very CPU extensive queries, which you should pay attention to. This: 

Next step is to create a job that will check (before tran log backup job): Step 1. Whether there is a database with value of 0 (meaning not backed up) Step 2. Back it up taking values from the table (DB name,BackupName) Step 3. Update column to 1. Optional: Send an email informing DBAs about newly backed up database. 

Just to expand on previous answer that was posted. Just like George said, shrinking in general is not something that should be part of maintenance job. Rather log size miscalculations, some unexpected scenarios (such as uncommitted transaction, large and intensive DMLs etc etc) or insufficient amount of log backups can cause excessive log growths. If your log size does not seem large enough, you should monitor it during busy hours, or during night time ETLs(If you have some) to see the average log size and change it if needed. Also make sure to set log size auto growth in specific MB size, which will mostly depend on your initial log size. More info could be found here Database log VLFs Now to answer your questions: 1) No in general. But in scenarios i mentioned above, it could be helpful. Which is the only time when it should be used - out of ordinary situations. 2) If you determined you want to shrink your log file, you should be aware that the log file is made out of VLFs(Virtual log files), which are gradually filled one at the time. Once all of them are filled,if you reached your log maximum size, log auto growth will happen and depending on size will grow in 4/8/16 VLFs. Once the log is backed up, these VLFs will become empty again (you will always have some in use, so it can track current LSN). To keep it short, once you backup you log, you can check used and unused VLFs using command 

everything is in order!   The data nodes don't have individual files for each table and so you won't see the table names there. What you're seeing with the MySQL Server directories is not the files containing the contents of the tables but a copy of the data dictionary/schema definitions for those tables (e.g. if you add more rows to these tables then the files you see with the data nodes should increase in size but the ones with the MySQL Servers should not). Regards, Andrew. 

To follow up on JD's response, you can view a worked example of using the Memcached API to MySQL Cluster at $URL$ You can choose whether certain key-value pairs are stored in Memcached, MySQL Cluster or both by setting up meta-data in the database. 

Make sure that you have at least 2 unused API slots ([mysqld] sections) in your configi.ini file (and perform a rolling restart of your Cluster after adding them). These slots will then be used by memcached. Note that you can choose to run mysqld processes on the host as well and have them part of the cluster (can access the same data through SQL and NoSQL) - you just need to make sure that you include enough [mysqld] sections in your config.ini file so that there are a couple left for memcached. 

Depending on transaction level, those queries will block writes. Serializable & Repetable Read transaction isolation level will hold S locks (for the whole duration of the transaction), which are incompatible with X locks that are required for writes(inserts/updates) . And yes it makes no difference, if you check execution plan you will see that they are exactly the same. So in order to prevent locking and blocking implementing different kind of objects wont give you no results, but performance boost(if used stored proc). Instead you should change isolation level 

There are couple ways to achieve what you are after. I will show you two, but there are other ways as well. The first one is signing a procedure, and allowing other users to use it with specifying only. First of all you need a master key, which i assume you already have, and you can check it using DMV, and looking for record that has '##' as a prefix. If not simply create a DB master key 

I used examples on value and node, because you provided the code with those two functions only, if you want to read more about it please visit this Hope this simple examples give you an idea of how to query xml types 

As Kris mentioned, you could create a stored procedure and run it as a job Here is a sample script that will accept Table name and Threshold(in KB) and send an email if table size exceeds its threshold 

To answer @a_vlad, the environment hosts customer dbs for a SaaS. Here is how we dealt with it: Load occurs on some customer DBs around the clock. We ended up writing a script to detect usage, if no usage occurred during our lighter times, we did a mysql dump and import for that particular organization on the master, forcing a full sync to the slave for that database. We then ran this overnight for a month and were able to catch most of the organizations on that particular database master in a usage lull. 

I'm planning an upgrade to MySQL where DATETIME, TIMESTAMP, TIME and YEAR are potentially a problem. I'd like to query the database to find all uses of these datatypes. How can I accomplish this? 

I have a MySQL 5.6 server that feeds to 6 web servers and a slave read-only mysql 5.6 server. These are the only clients with the exception of a monitoring software. If any were failing to work, I'd expect my users to notify me immediately or get a report from my monitoring software, instead I am seeing performance issues under heavy load that I'd like to improve upon. All the web servers and the db servers are on the same subnet in a VMWare cluster with SAN storage behind. I'm not seeing any performance/latency/service-affecting issues on the hardware or storage networks on other VMs hosted on this infrastructure so I'm not seeing a reason to blame hardware. In looking into this, I've noticed high connection problems on both the mysql master and slave servers. Details below are from the master server, but look similar on both. 

When you are all set and done, you can transfer those views from ViewSchema to dbo or some other more meaningful schema with command: 

Havent really tried to debug the script, but i can see just from the parameters what it essentially does. There are many scripts online that you can use to do just that The one that i use pretty often is this: 

Green rectangles are tables, red eclipses are attributes, blue triangles are tables that connect tables and a relationship descriptors so you could understand their relationship. Do not name triangle like tables as referred on the diagram, such as 'contains' , give them meaningful names. Purple eclipses are additional attributes of tables that connect other tables Numbers and letters next to triangles are telling you what kind of relationship it is Whether is : One to many - example Table Account has 1 and table Character has N which means one account can have many characters , but character can have only 1 account Many to many - example Character can "Ran into" many creeps and many creeps can ran into many characters What does it means? It means that depending on a relationship type, you will create foreign keys and constrains accordingly. If you are still in doubt what the foreign keys are, i suggest you to read some books/articles before you actually start designing database. Databases are base for any project, and it is not something to be taken lightly