I am building a deep neural network based binary classifier, with single output. The loss function I actually want to minimize is $$ \mathcal L(\hat y,y) = \begin{cases} 0, & \text{if $\hat y$ = 0} \\ 1, & \text{if $\hat y$ = 1 & $y$ = 0} \\ \gamma \approx -0.7 , & \text{if $\hat y$ = 1 & $y$ = 1} \end{cases} $$ where $y \in \{0;1\}$ is sample's label, $\hat y \in \{0;1\}$ - classifier's output and $\gamma$ - a hyperparameter. I think this is a case of assymmetric loss. (One can see it as betting: no reward when not betting, stake 1\$, payout 1.7\$ if bet wins) From what I know by now, this loss function is probably not suited well for backpropagation and gradient descent. Question: Is there a better-suited formulation? The often used cross-entropy loss doesn't allow for trade-off tuning between precision and recall. LINEX and LINLIN are assymetric by design purpose, but I coundn't find an example of a deep NN trained with them. An alternative could be leaving the loss function as it is and resorting to SPSA, but I'd like to keep it simple if possible. Edit: I came up with $$ \mathcal L(\hat y,y) = - (\gamma \hat y)^{y}(-\hat y)^{1-y}$$ At the moment, I have no clue if it is going to work for NN learning. I'm concerned (maybe unnecessarily) that, without logarithm, it is not convex regarding NN weights. (Last layer has sigmoid activation.) Here is the log loss shown for comparison. $$ \mathcal L(\hat y,y) = - (y\log \hat y + (1-y)\log(1-\hat y)) = -\log[\hat y^y (1-\hat y)^{1-y}]$$ 

Simply, one common approach is to increase the complexity of the model, making it simple, and most probably underfitting at first, and increasing the complexity of the model until early signs of overfitting are witnessed using a resampling technique such as cross validation, bootstrap, etc. You increase the complexity either by adding parameters (number of hidden neurons for artificial neural networks, number of trees in a random forest) or by relaxing the regularization (often named lambda, or C for support vector machines) term in your model. 

The number of data in the class is sometimes referred to as the of the classifier. It tells how much you can trust your result, like a p-value would allow you to trust or distrust some test. One approach you can use is to compute several classifier performance measures, not only precision and recall, but also true positive rate, false positive rate, specificity, sensitivity, positive likelihood, negative likelihood, etc. and see whether they are consistent with one another. If one of the measure maxes out (100%) and the other do not, it is often, in my experience, indicative of something went wrong (e.g. poor support, trivial classifier, biased classifier, etc.). See this for a list of classifier performance measures. 

As many have answered, it is always best to avoid anything done manually as it is less reproducible/documentable. Your point about the overhead of writing a script vs. opening and fixing the file is valid, though. Best practice is often to 

What techniques are currently available for e.g. locating the ZIP code on the envelope and cutting it to single digits prior to feeding them to digit recognition algorithm (trained on MNIST dataset)? Or imagine a task of automatic recognition if a phone call recording contains a sound of a train passing a track switch (just yes or no). If present at all, it can be gentle or loud, depending if a window was open. It can be slow or fast, depending how quick the train was going through the switch. I found so far Spatial Transformations Networks mentioned in this comment 

I got a similar task. Maybe you can tune your trade-off by designing an appropriate cost function. Or, see F Score on wikipedia. Reinforcement learning could also have tools you are looking for, namely, means of rewarding 'acting only if sure'. See e.g. the intro in Sutton, Barto Or, try searching so called Conformal Prediction. Venn Prediction is an extension of the original Conformal Prediction framework... 

Imagine a [neural network] binary classifier. Presented person's photo, it should output "positive" if person is wearing dark eyeglasses and "negative" otherwise, including 'beeing unsure'. Tricky part: there is a positive reward for correct classification and a negative reward for a wrong one. The purpose is to maximize the reward-to-variability ratio Now imagine two possible applications. First one: 

Intuitively, I'd say that these two applications are quite different and, due to the purpose stated above, the classifier has to be tuned differently, too. In the second case it should be more inclined to 'cherry-picking'. Question: Are there ways to encount for costs of type I and type II errors and to maximize an arbitrary performance measure when learning a classifier? Is it possible to incorporate the abovementioned performance measure into the cost function, while keeping NN training computationally realistic? Reinforcement learning sounds like an option, too (agent has possible actions "do nothing" and "classify" and seeks to maximize his total reward), but I wonder if I can solve this just by means of supervised learing. I also thought that unequal reward magnitudes for true and false positives call for boosting correspondingly. Is this viable and described somewhere? 

I am trying to do this in R. I tried the below function, but my R session is not producing any result and it is terminating. 

I have a data set, in excel format, with account names, reported symptoms, a determined root cause and a date in month year format for each row. I am trying to implement a mahout like system with a purpose of determining the likelihood symptoms an account can report by doing a user based similarity kind of a thing. Technically, I am just hoping to tweak the recommendation system into a deterministic system to spot out the probable symptoms an account can report on. Instead of ratings, I can get the frequency of symptoms by accounts. Is it possible to use a programming language or any other software to build such system? Here is an example: Account : X Symptoms : AB, AD, AB, AB Account : Y Symptoms : AE, AE, AB, AB, EA For the sake of this example, let's assume that all the dates are this month. O/P: Account : X Symptom: AE Here both of them have reported AB 2 or more times. I could fix such number as a threshold to look for probable symptoms. 

I have provided a sample data, but mine has thousands of records distributed in a similar way. Here, Col1, Col2, Col3, Col4 are my features and Col5 is target variable. Hence prediction should be 1,2,3 or 4 as these are my values for target variable. I have tried using algorithms such as random forest, decision tree etc. for predictions. Here if you see, values 1 and 3 are occurring more times as compared to 2 and 4. Hence while predicting, my model is more biased towards 1 and 3 whereas I am getting only less number of predictions for 2 and 4 (Got only 1 predicted for policy4 out of thousands of records when I saw the confusion matrix). I am planning to do feature extractions and try to see how my model behaves, but is there any way to make my model predict in a generalized way where it can generalize some values 2 and 4? I just read of undersampling and oversampling concepts, but I am beginner here, is there any good approach I could try? Should I split the training dataset based on Col5? I am implementing this in python using pandas.