For software in the web app category, depending on you infra/hosting provider such decoupling might be possible to switch incoming traffic across (or split it between) different deployed versions of the sw, practically covering any of the changes you mention: bugfixes, visuals, etc. Such support would typically not require feature toggles. And it might be applicable regardless of the app being monolithic or split in microservices. For example Google's App Engine Paas offering has support for traffic splitting and migration. From Splitting Traffic: 

Note: this answer is based solely on documentation, I didn't try it. From External Data Source (emphasis mine): 

Personally I don't think using CasC itself will have any negative scalability impact. Fundamentally CasC means the actual config files are not hand-maintaned, instead they're auto-generated following version-controlled rules - which can be a lot more reliable. But once the config files are generated - the service using them functions just as it dit with manually-crafted config files. If anything - using CasC should make such large nagios system more reliable - if done correctly CasC should reduce/eliminate the risk of human error in modifying the config files. 

The complication reside in the definition of the health status (and its usefulness). You mentioned the definition being and for that indeed checking the homepage should suffice. However that doesn't mean the service is properly configured for and correctly performing all the jobs that it's supposed to be doing. Without a consistent, universally acceptable definition of a health status there is no point of providing such information as a single, one-stop location to check. But Jenkins offers via its Remote access API various checking points that users can combine to derive their interpretation of the overall service health status. For example maybe checking the status of all the configured jobs or at least that of the critical ones would be more useful, which could be done using the or checkpoints. The JenkinsAPI could also be of interest (better docs at least), found via Where can I find jenkins restful api reference? 

Your difficulty stems from encouraging potentially long-lived branches, which is deviating from the CI methodology - which is supposedly why you'd be using jenkins - a CI tool - in the first place. Normally you shouldn't be running the tests for those branches in the Jenkins pipeline for the trunk - you want to measure the quality of the integrated work, not that of the work done in the silos. I do understand the desire to run such tests before merging into trunk, so maybe it's OK to use jenkins to do it if you want to, but with separate jobs/pipelines, not tied into the trunk pipeline. And keep in mind that the results of such tests aren't necessarily conclusive: just because they pass over there doesn't mean the same tests will also pass in the trunk, you'll need very strict conditions to make that always true. Personally I'd go for a fully automated patcing/merging solution based on pre-commit verifications which can enforce the strict conditions necessary to prevent post-merge trunk quality regressions (but I'm biased, I built such solutions). See for example How to ensure that git subtrees are kept up to date? Anyways, back to the question. Since the code for the various pipeline segments that you seek to perform in parallel comes from different branches you won't be able to use the same workspace, you have to pull separate workspaces from the respective, different branches. Which makes the question kinda moot. Final note: assuming the same workspace could be used in multiple builds, your #2 approach requires a pristine build structure. It's not uncommon for less-than-perfect builds running in parallel in the same workspace to interfere with each-other: race conditions creating/deleting directories, overwriting artifacts, too high peaks of resource usage, etc - typically hard to repro intermittent failures. These could easily happen if, for example, the builds would be for the same product but for different CPU architectures, or builds including code shared across multiple products. Just be aware of the possibility. 

When jenkins processing reaches a deployment stage for a particular site it creates a corresponding request file with the necessary information in it. Each site user periodically (cron-driven, for example) checks for request files pertaining to their site, handle the request as appropriate, following their own site policies and provide status updates in the respective response files, which the jenkins user checks periodically. When a request handling is completed the jenkins user removes the request file, signalling that it received the "message" and then the site user periodical job can remove the corresponding response file. The names of the request and response files can be used to encode the particular site and request identification, so that the periodical checks don't have to fumble through multiple files. The scheme can easily work across machines (if, for example, some of the sites are migrated to other servers) simply by placing the "mailbox" on a shared filesystem accessible from all those machines. OK, an example, as requested. Just a basic skeleton, in python, hopefully self-documenting. Prerequisites: 

It might be possible, depending on your version control system. In Git, for example, for a dir structure like this: 

Speaking primarily from the Google App Engine (GAE) perspective. My preference is for environment-driven project split. This might be of interest: Advantages of implementing CI/CD environments at GAE project/app level vs service/module level? I wouldn't go for architecturally-driven project split unless there are serious reasons for doing so (if you feel you have such reasons you should ask with details): 

There are no fundamental technical issues with running multiple jenkins slaves on the same machine. In fact Running Multiple Slaves on the Same Machine lists several good reasons for doing it: 

Sometimes the cost of such development is higher overall than that of developing a custom system. Othertimes the functionality actually needed from the third party tools is a relatively small percent of its capability set and the overhead for learning/installing/operating the tool is simply not worthy. You guessed it - I wrote my own monitoring tool :) 

Each CI/CD system is different when it comes to performing the job(s) that the execution pipeline consists of. In most (if not all) cases the details are part of a configuration which is specific to the particular CI/CD system used. As you mentioned Jenkins uses a Jenkinsfile, GitLabCI uses , others use "live"/GUI-based configurations, etc. So I don't think you can find a common solution directly pluggable in an arbitrary CI/CD system. But you can get close to that: fundamentally each of these configurations are simply adaptation layers between the CI/CD system and the automated task(s) to be performed. So if you can create a (set of) standalone script(s) to perform your automated step(s) in as portable way you need as possible you should be able to integrate with in almost any CI/CD system without too much trouble: any decent CI/CD system should support a custom script invocation mode to cover for whatever special case that doesn't properly fit into its pre-defined cases. Using python/bash is generally quite portable, up to you to decide if that indeed covers all possible execution environments you'd be willing to consider. I'd recommend structuring such script itself as a wrapper around smaller scripts implementing its logical/functional steps - some CI systems may require individual control of the steps or may offer advantages if such control is provided. In your case I'd have separate scripts for: 

There is no right answer, in a sense you're facing a requirement conflict. You can't eat the cake and have it, too ;) If a particular QA verification (any, really, not just performance) is important enough and you want to catch any regression right away - you have to insert it in the CI execution. But, unless you manage to somehow execute it in parallel with other existing jobs in the CI pipeline, it will extend the duration of the CI execution. You could, if you want, perform it in a "slower" CI pipeline, executed less often than your regular CI pipeline. For example just nightly instead of after every commit. The drawback is that when a regression is detected you'll have to sift through a bigger pile of commits to identify the culprit. Which can be done either manually, though human analysis, or using automated bisections executing the affected QA verifications (or even a mix of both). But that takes time and resources. If it's a rare event it could be an acceptable compromise, if not then you have to decrease the time between the "slower" CI pipeline executions. You have to decide what's more important: the CI execution speed or catching regressions faster. This decision can also depend on the development stage context. For example: 

A not very devopsy, but quite simple and effective solution with no security implications (no credential exchange or authentication required) would be to establish a file-based "message" exchange scheme, in a "mailbox" - a well-known filesystem location (setup once and owned by ) with 2 directories: 

The walking skeleton reflects the rough structure/shape of the product, but it may be entirely stubbed out (initially), meaning it may not be actually functional. The skeleton cannot move without the muscles. The MVP however must meet a certain minimum functionality level to be considered viable. As development progresses the walking skeleton typically morphs into an MVP and eventually into a final product. 

I'd approach this a bit differently to avoid these issues. I don't even see a need for separate services for this particular job: 

The simplest/cleanest branch strategy is IMHO the one used in continuous deployment: a single/main integration branch which is also your release branch. From What is Your Branching Model?: 

If the different CDNs are hosted at different IP addresses one possibility might be to configure the local table for the machines in a certain environment in such way as to overwrite the DNS resolution for the CDNs which shouldn't be accessible and pointing them to either: 

Indeed, releases don't appear to have been registered on GitHub, the list returned by the REST is empty, most likely explaining the 404 returned for : 

Building on Xiong Chiamiov's answer, which correctly identified the root cause of the problem - the reference by relative path when attempting to empty or delete that directory depends on the working directory at the time, which was not correctly set in the cases mentioned in the OP. So there are 2 solutions available: 

I'm not familiar with MS tools, but if MSTest doesn't offer good integration with the other tools in your workflow you could build custom wrappers to implement a typical submit/execute API with a pass/fail result (or set of results) which most tools can integrate with. 

I highlighted as it is not . Nothing in the above description actually requires an agile development methodology. But I suspect often transitions towards DevOps often incorporate insertion of agile methodologies in the development process as they really fit well together. 

You would first need to create a trigger for that pipeline. Then you can activate that trigger (from your jenkins job in this case): 

Depending on the GCP products used there may be sufficient support for architectural splits inside the same project. At least GAE does IMHO. My GAE apps are all standalone and do not use inter-networking at all, so no costs from this prospective. There are still networking (bandwidth) costs for accessing various GCP-provided infra/services, but those are irellevant/unavoidable from the project split perspective. 

Of course, you also need to configure your docker daemon to use your local cache (but I presume that may be already done from the context of the question): 

In such context the typical advice should be immediately applicable: use the right tool for the job. But then you also cannot ignore nowadays the almost virulent tendency of software tools to extend functionality into more or less related fields and actually become toolsets for various reasons: cool feature(s) to have, expand customer base, amass more revenue, etc. For example many file management tools include image viewing features and many image processing tools include file management features. You can move files around and you can view images with either of the tools, often equally well. Because of this it's quite possible to have entire portions of the software development process covered/overlapped by multiple tool(sets) even if their main feature/capability differs. So it really boils down to the exact functionality you want to achieve in your particular process and how well one tool or the other does the job in your context. Subjectivity/preferences/convenience included. Making this question primarily opinion-based ;) 

Stages group jobs inside plans. So technically they are cloned when the respective plans are cloned. Which means that missing the ability to clone a specific stage should be just a UX/usability/functionality issue, not a conceptual one. There might be a non-trivial implementation details when cloning stages - they may carry inside stage sequencing information, as in "pointers" to the stage objects preceeding and/or following them inside the plan "pipeline", which might be a bit more difficult to handle when cloning just one stage in the sequence. These would be easier to handle when cloning entire plans as the entire sequence of stages is cloned, there will always be a 1:1 relationship between the stages. 

Such high-performance logging services can also offer alternatives to logging to files, management of logfiles can thus be avoided altogether if that is of interest: 

Since you're talking about making delivery/deployment decisions based on the results provided by your script(s) you need to communicate with the team(s) responsible with maintaining the official CI/CD pipeline implementation for your organisation. If they're using Jenkins and a particular pipeline implementation (multiple technological approaches are possible) compatible with the way you hooked up your script into Jenkins locally they may be able to directly use your approach. Even if that's not directly possible, there probably still are ways of including your tests in the CI/CD pipeline, only with a more or less different implementation. Note that there may be related technical and/or logistic questions/requirements that might need to be addressed depending on your organisation particularities, for example: ownership, development/maintenance process and policies, SLAs for the tests, script(s) and/or the resources for their execution. Even if you don't get your tests integrated at first, you can still run them for every repository commit using your private Jenkins setup and make the results available for everyone to see. In time, if those results prove useful to others they may support your initiative and help with a reconsideration of the initial decision. 

IMHO the comment is a bit misleading, in the sense that mixes up and/or attempts to compare the unique leader/chef role (in French, literally means chief/boss/leader) with the expertise required to fill that role. By contrast, the is just an expertise, it is not a unique role. If I'm not mistaken to become a chef one has to have experience in every other position in the kitchen. Similarly, a full-stack developer's expertise covers frontend, backend, etc - every other more specialized expertise required for the team's product. Which is probably the starting point for the comparison in the comment. As I mentioned in my answer to that post, scale matters. In a large team, the requirement for the leadership position isn't necessarily the technical expertise. Which is why often in software development the team leadership's role is a management position, not a technical one. Clear labour division example, if you want. It's not impossible to even have multiple persons qualified to fill the chef role in the same kitchen, but only one can actually fill that position at any time, in this context really refers to the role, not to the expertise. Also - there may be many types of chefs in a kitchen, see, for example Chef Jobs on Cruise Liners :) But in a small resource-starved team where the team leader has to be selected from between the only full-stack developer and one or more less-experienced ones - probably the full-stack developer will be the leader, just like the only person qualified to be a chef in a small kitchen.