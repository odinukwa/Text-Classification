I dont think there are any examples of such things. Except for linear programming, semi-definite programming, complex numbers, large fractions of machine learning, etc. The real question is $URL$ 

Computing the probability that a point is inside a random convex-hull is quite interesting, and got some attention recently (under a slightly different sampling model). The exact probability can be computed (but not too quickly). Good estimates follow by looking on the Tukey depth of the points of interest. See the following recent paper, and references therein. $URL$ 

If I may quote Sarah Palin on this issue: "All of them". More seriously, I think most papers should not be read in the original. As time passes people figure out better way of understanding and presenting the original problem/solution. Except for the Turing original paper, which is of historical importance, I would not recommend reading most original papers if there is followup work that cleaned it up. In particular, of a lot of stuff is presented much better in books than in the original. 

Well, if you dont mind some approximation than regular dimension reduction techniques would give you some speedup as they preserve (approximately) dot product. Otherwise, you can think about each vector as being a CNF clause. If you just have to decide if the dot product is not zero for all vectors, then what you are asking for is the shortest equivalent DNF to the given CNF (or to be more precise some kind of minimal computation that compute the given CNF). This however is easier than the problem of deciding if the given CNF is never satisfiable, which is co-NP complete. In short, this problem should be computationally hard. I think there was quite a bit of work on stuff like that in the learning community, especially in the dinosaurs age ;) (80s and earlier?). 

Since you have to compare a sum of square roots, it is not clear/unknown if this problem is in NP. Otherwise, if you assume you can compute sum of square roots and compare them, then I am unaware of an algorithm that works in faster than quadratic time. For approximation, you can just approximate the 1-median using known algorithms in linear time, and then just take the closest point to this center. This would give you a constant approximation. A better approximation can be had by working somewhat harder. 

From a memorable talk of a person that implemented an algorithm that solves an undecidable problem: "It takes 2-3 seconds for all the inputs I tried". 

$URL$ (Or open any standard book on Computational Geometry). For practical implemented algorithms, see CGAL $URL$ If your main problem is numerical issues, then this can be challenging in practice. A lot of practical work on this problem was done by Dan Halperin in Tel-Aviv University. 

Well. If you can do it with linked list, then just modify it into skip-list. Just make every node in higher level list point to its original node in the bottom list. Viola, you have what you want... If you are still not convinced - read a good description of a sweeping algorithm (the book by Mark de Berg, Otfried Cheong, Marc van Kreveld and Mark Overmars is a good starting point). Then go back to your problem - and everything should be clear. It is really no more than a slightly modified implementation of a sweeping algorithm. 

One natural thing would be to try a separator kind of trick. If the Voronoi cells are fat, and of similar size then a randomly shifted grid of the right side would do reasonably well. in particular, there is a paper by Miller and Thurston (and some other people - their names escape me) about finding a separator if you have fat regions covering space, and every point is covered only a constant number of times. It is essentially an extension of their proof of the existence of a planar separator to these more general settings. 

The best heuristics are really approximation algorithms. The most beautiful approximation algorithms are just "stupid" heuristics that work. For example, local search for clustering, greedy clustering (Gonzalez), one for the price of two, various greedy algorithms, etc, etc, etc. So studying approximation algorithms is really about understanding what heuristics are guaranteed approximation algorithms. The hope is that research on approximation algorithms creates two kinds of cross-fertilization: 

If I rmemeber correctly, you have to approximate the number of clients connected to each gate. Otherwise, you would immediately get something like $O(n^{O(g)})$, where $g$ is the number of gates in a subproblem. By approximating this number up to a facotr of $(1+\varepsilon/\log n)$ throughout the dynamic programming one can get a $(1+\varepsilon)$ error in the end. That would yield running times similar to what you stated above. 

The approximate Caratheodory theorem goes back to the 60s, and probably way earlier than that (it follows for example from the mistake bound of the preceptron algorithm analysis). As for the dimensionality reduction, the answer is no - the supporting subsets are different subsets, and their number is too large. In particular, the number of possible subsets is $n^{O(1/\epsilon^2)}$ - but there is some connection... Specifically, let $P$ be a set of $n$ points in high dimenionsional Euclidean space of diameter $1$. Let $Q$ be an $\epsilon$-net in the convex-hull of $P$. That is, any pair of points of $Q$ is in distance at least $\epsilon$ from each other, and every point of $CH(P)$ is in distance at most $\epsilon$ from some point of $Q$. Now, by the approximate Caratheodory theorem, we know that $|Q| = n^{O(1/\epsilon^2)}$. Now, imagine that you do some experiment, and with probability half the experiment succeeds for half the points of $Q$ (or, more formally, half the pairs of $Q \times Q$ -- since we look on vectors formed by differences of points of $Q$). How many times do you have to repeat the experiment till all the points are served? Well, roughly $\log_2 |Q| = O(\epsilon^{-2} \log n)$, which is, surprise surprise, the target dimension in the JL lemma. This is of course, does not imply the JL lemma - it is somewhat of a "coincidence" - a wrong calculation that gives the right bounds... There is a useful lesson here however - a set of $n$ points in high dimensions, induces roughly $n^{O(1/\epsilon^2)}$ points that are $\epsilon$-distinct, independent of the ambient dimension.