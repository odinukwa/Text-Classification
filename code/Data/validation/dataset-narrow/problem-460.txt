As you are using universal references this means that will bind to anything and retain it's qualifiers. Even r-value references... which means that the following will compile and run doing some manner of undefined behaviour: 

and in the fxml you would have an attribute saying . To link the button action to the method in your controller. This here will get you started with FXML. Use JavaFX controls are built on the powerful concept of observables, properties and bindings. An observable is a class that wraps an object that allows you to listen to changes in the object. A property is a value that is observable. A binding is a way to create an expression that is bound to the values of properties, using the fact that they are observable. So if a property changes, so does all bindings that depend on it. And finally, you can bind the value of a property to a binding so you can effectively link properties to each other and create arbitrary expressions of your properties and bind those expressions to other properties. More info here. For example you could do the following: 

If multiple threads access this instance simultaneously before it is created, you have a data race and may end up be being constructed multiple times or other kinds of undefined behaviour. You need to add mutex locks around the block or use which is preferred. As it is supposed to be a singleton you're not supposed to be able to create more instances as the meaning of a singleton is to just have one instance but it appears that it is fully possible to construct multiple instances of simply by creating them as local variables. So this is a design flaw in your template. A much better way of creating a singleton is to rely on C++11 Magic Statics (N2660). And simply do this: 

Do not repeat yourself, your copy constructor and assignment operator share a lot of code. You can implement copy constructor like this: 

and change the last: to . It is always good to eliminate variables if you can. Don't modify parameters/arguments It is confusing when you change the value of the input parameters for example with . This makes the code hard to follow. Instead choose as . This makes your intent much clearer. Avoid when possible Some times it is impossible to avoid an infinite loop such as this is not the case. You should avoid formulating infinite loops like this because it makes it harder to analyze the termination condition of the loop to verify that it will terminate. 

I don't know what framework you are using for your tests but gtest has type parametrised tests that you can use to test all the implementationsâ€‹ with the same tests. By using this feature you wouldn't even need the code to begin with, which in my book is the best code: the code you never wrote :) 

Okay so I'm kind of gleaning from your code and description that you want to visit all numbers in the range \$[0,n]\$ in a (pseudo) random order without visiting any number twice. I'm saying pseudo random here because you are using which is just a pseudo random generator. You can use something called a Linear Congruential Generator (LCG). This is a type of well known pseudo random number generator, in fact is an LCG. Funny thing about LCGs is that they have a period and if you choose your constants suitably you can pick that period and guarantee that you will not have any repeats in the period, the PRNG is said to have a full period. The general form of the algorithm is: $$x_{k+1} = (a*x_k+c) \%n$$ where \$0<n\$ and \$0<a,c<n\$. The value of \$x_0\$ is simply chosen as a seed value. If \$n\$ and \$c\$ are co-prime, \$a-1\$ is evenly divisible by all prime factors of \$n\$ and \$a-1\$ is evenly divisible by 4 if \$n\$ is divisible by 4. Then the LCG will have a full period. I did a test here. I was a bit sneaky: to make sure \$c\$ and \$n\$ are coprime, I simply picked \$c\$ to be a very large prime. In fact if you pick \$c\$ as a prime number such that \$c\$ > \$\text{INT_MAX}/2\$ then \$c\$ and \$n\$ will always be co-prime unless \$c=n\$ in which case simply have two such \$c\$ values that you pick between. To make \$a-1\$ evenly divisible by all the prime factors of \$n\$ and 4 if \$n\$ is divisible by 4, I simply pick \$a=n+1\$. Note that these values do not fulfill the conditions as listed on Wikipedia: \$0<a,c<n\$. However they seem to work just fine. So consider it a word of caution to either check up the maths yourself to see if this is OK or to use a more robust way of selecting \$a\$ and \$c\$. At any rate using an LCG will be the most efficient in terms of memory and computation provided that you can choose the coefficients easily. 

can and should easily be reworked into members of . Try it and see how much less typing of it will result in. This method: 

I'll start with giving a bit of background as there are many classes involved and I can't attach all of them because that would make this question wildly too long. For the curios they can be found at my github repo. I have an immutable structure called a which consists of 8 immutable s. Each is named by a such as or . A is a particular configuration of s that are equipped on a . For each on the , the has a matching that contains the s equipped on that component of the . In addition to this, each can have a number of fixed s and some are kind of fixed but can be toggled on and off (not my decision, that's the game rules). So the toggle state is a part of the which is why has a method which will return the truly fixed items + any items which have their toggle state set to enabled. In many places in the code base I need to iterate over all items on the (fixed and removable). Even more frequently I'm only interested in certain types of items on the loadout. For example those that are a subclass of which is an or even those items that implement a particular interface not necessarily inheriting from . In other words I may want to filter on a class that is not descendant from . However all items on the loadout have as a base class. To this end I decided upon letting have methods to return an of approriate type which I can use to iterate over of a particular type. Then I implement a form of MultiIterator that can iterate over all the s on the Loadout. Relevant excerpt from : 

I'm going to assume your target machine is a super scalar CPU with some variation of Tomasulo architecture. I'm also going to assume that there is no significant cache miss ratio at cause for your performance (you should verify this). When compiling optimised code, the compiler will rearrange operations to reduce stalls due to availability of input data to the instructions. Hence you cannot completely trust the line numbers in the profiler. As the CPU is super scalar some instructions may execute and complete out of order (but are committed in order). I see that you are computing the reciprocal of and to save yourself two divisions. It is possible (but doubtful) that this could cause a delay stage as the two dependent operations could end up stalling until the result is available instead of executing in parallel on different execution units. Contrived example: 

In addition to Pimgd's excellent review: Use to build strings String concatenation by operator causes many object allocations unnecessarily, you can use a string builder like this to avoid having to return a new every time too: 

Hmm interesting problem, I think I have an solution that might be faster (but I'm not able to test at the moment). You said that was the bottle neck and I find this reasonable. So lets work around . Recall that base 10 numbers are represented as \$x=\sum_{i=0}^\infty a_i10^i\$ so a sum of numbers \$X=x_0,...,x_n=\sum_{j=0}^nx_j=\sum_{i=0}^\infty (\sum_{j=0}^n a_{ij})10^i\$. Which means that if we maintain an array of for the sum of each \$a_i\$ we can reduce the cost to effectively one summed atoi. Something like this should work: 

I believe that you are expending way too much energy trying to solve something that should be a non-problem. Simply always let your containers be zero based, if you have for example 1-based indexes as input, simply subtract 1 from them as soon as you get the input and convert back right before output. Always use zero-based indexes internally and only convert as close to input/output as possible. (too large for comment really) 

Number 1 and 2 are bad in my humble opinion. You might want to do something else with the compressed data at some point, like encode it in base64 and transmit to a webpage in text mode. Number 3 is so-so depending on how you use the Huffman coder. I disagree with the accepted answer that the code is clear. There is a limit when small functions become too many and I believe you have crossed that line. It is difficult to follow the algorithm in order to verify correct implementation (I understand Huffman it's just difficult to follow with jumping across all functions). Test that your compressed data is smaller than the source data Your tests should really be unit tests as palacsint says in their answer. Also the probably most important test that you should do is to see that the compressed data is smaller than the original data. Specifically, any correctly implemented Huffman coder should perform very closely to Shannon's Source Coding Theorem. For sufficiently many samples you should be within 1% of the limit in that link. Other comments You are generating your prefix codes as strings and by doing string addition. I find this kind of code difficult to follow as you are covering up the fact that you are doing bit arithmetic. I would use a natural ordering for the nodes instead of implementing a comparator class. I do not see any mention of a stop symbol which prevents accidental decoding of junk when your output bitstream is not an even multiple of 8 bits. And this: 

As you have not provided any profiling data I can only speculate in what I see in the code you have provided. Guesstimating expected performance First of lets consider some numbers, you say millions of points so I'm going to assume 10 million points. You say 2GB file then for 10 million points, that's around 200 bytes per point sounds reasonable. Lets assume a typical HDD with read speed somewhere around 80MB/s, then reading 2GB of data should take 2000/80 = 25 seconds or so. Lets assume a modern 3GHz CPU with average instructions per cycle (IPC) of 5 (which is a bit pessimistic). That means that you can do around 3000*5/10 = 1500 instructions per point, per second per core. I would reasonably expect to be able to build the tree (ignoring the delay to read from disk) in a few, maybe tens of seconds. All in all to read the entire thing into memory and build a quadtree, I would expect around a minute. You say 5 minutes which is a bit slower than what should be possible. Looking at the code I see no major blunders that would be an obvious culprit but I do see a LOT of branching on data. One reason CPUs can have such high IPC is that they do something called "Speculative Execution". When the CPU encounters a branch (if-else) it will take an educated guess using sophisticated algorithms to try to predict which branch will be taken and start executing that branch before it has actually computed the branching condition. Pretty neat stuff. As long as this branch prediction goes well (and it does so quite frequently) you gain a lot of performance, but if the CPU on the other hand miss-predicted the branch, it has to back out of the work it already did and then go back and execute the correct branch. This is called, "branch prediction failure" and is very well illustrated in this SO question. In that question the OP has an array of random characters and sums all values larger than 128 (i.e. 50% of the values). Then the op does additional work and sorts the array first, and then runs the exactly same code. Intuitively you would expect it to be slower because you have done an additional sort before hand. But this is not the case, doing the extra work of the sorting actually makes the entire code 5x faster. This 5x speedup is entirely due to avoiding branch miss-predictions as the data is now sorted and the branch predictor in the CPU can correctly predict almost all branches while the random array is impossible to predict. The 5x speedup factor uncannily matches my guesstimate above (1min vs 5min) and your code appears to be branching on random data (as the points are not sorted geometrically, whatever that means). I believe that you can gain some significant performance by rethinking how you branch in your insert method. For example here: