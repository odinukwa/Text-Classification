Concerning the size of the data - have you tried compressing the files using the Python tarfile library? You could compress it in chunks and after each successful compression operation, and keep a sql db with the metadata in it. That way when you do your sample selection you can perform the selection on the db and pull only the files you need. I would go for it and split it up. As far as a selection method I would kick it old school and do a random selection with replacement. 

Would I have to add the new feature retroactively to the training data and retrain the model eventhough it will never occur? Wouldn't this introduce 'look-ahead-bias'? How do I account for the added feature in the encoder going forward? 

A few things, 1) Have you determined whether the relationship between hr_of_day and power_used is statistically significant? I recommend doing a kendall's tau correlation if you haven't already. I like kendall's tau because it handles non-linear relationships and can be considered as the probability that the probability that the two are related. 2) Also, I would check whether temperature and hour are related. If there is multicollinearity you might need to reexamine the factors applied. 3) If you know that the independent fits a bimodal distribution then run some P-P plots on known bimodal distributions (such as 'beta distributions'). It would also be interesting to extract the potential gaussian distributions underlying the population - running two OLS models sequentially. In the end, you may be better off working from time-series analysis where the function is expressed in terms of hour. But I wouldn't go adding your own factors most regression analyses will "tell" you that something is missing. The way to tell is if the model coefficients are not statistically significant or the amount of explained variance is small. But I reiterate that coercing the hour to a cosine or sine function implies a relationship between the dependent and independent variable that may not be real. It seems like you might want to go back and do some (more) due diligence before throwing it into an ML model. 

Every Blob in caffe can be assigned a nonzero loss weight. And you can have an arbitrary number of outputs. This means you can just learn n different networks on the same data with different targets and assign every loss function it's own weight. Caffe takes care of adding up all the loss. 

The necessary information can be found on wikipedia: $URL$ "when we use more technical features such as colour histograms" Judging from this sentence I guess you need to understand the "Codebook" generation. First step is to extract features of patches in the image. For efficiency you only want to take patches which are interesting and calculate discriminative features on them. SIFT is one Method which performs both steps for you. It takes care of finding good spots and it calculates features on this spot. Now you can generate your codebook. A codebook will map every possible featurevector ( after all they're just numeric vectors ) to a certain output codeword. One possibility to do this, is to use k-means for codebook generation. After you built your codebook, a vector is mapped to a code by finding the minimal distance to all the entries ( since you used k-means you can use euclidean distance ). Now you have a complete realization of the bag of words model. You can now dive into using it for classification. The required algorithms can be implemented using the libraries you mentioned. 

In my opinion you're looking into the wrong methods to solve your problem. You have strictly no numeric data. Statistics based machine learning has a very hard time with such problems. Your problem sounds more like one of the problems which should be solved with rule based systems. My first instinct would be to try to understand the rules and put them into code leading to a classification. There are however methods for learning such rule based systems based on logic. They are only quite unfashionable for machine learning today. $URL$ 

Still, it is a good idea to think of using a spell checker, such as Aspell, since it gives you the ability to deal with no-so-common misspellings (of course, at the cost of false positives). A good starting point is Peter Norvig's essay on creating a spelling corrector. 

In sentiment analysis you may want to combine a number of classifiers. Let's say: a separate classifier for emoticons, another one for emotionally loaded terms, another one for some special linguistic patterns and - let's say - yet another one to detect and filter out spam messages. It's all up to you. You can use either SVM, Naive Bayes, or anything else that best suits your problem. You may use majority voting, weights (for example based on cross validation results), or any other more advanced technique to decide which class is the most appropriate one. Also, googling for hybrid sentiment returns tons of papers containing answers to the questions that you have stated. Please, don't ask us to rewrite this papers here. 

I would go for dimensionality reduction. You can start with SVD (should be available in Weka). If SVD is too slow / too memory consuming, then there are still some options: 

That's the tip of an iceberg. More approaches are there in the wild. The problem is that I doubt any of these solutions come with Weka (please, correct me if I am wrong on this). I would search for a usable Java implementation of any of these algorithms and try to port it to work with Weka's arff files. 

That's at least where the numbers come from. I share your doubts about the maximum value of this metric, and whether it should sum to 1, but now that you know where these numbers come from you can think how to normalize them. 

In your case, you're trying to classify an instance where and are unknown: . Since the first known attribute is , in your decision tree you're only interested in the edge that comes out of node on the right-hand side. There are five instances: three labeled , one labeled and one labeled . Instances with labels and split on the attribute. There's two of them, so the weight for both of them is . Now, we split on the attribute. There are 3 instances with value larger than 20, and two with value no larger than 20. Here's the trick: we already know that the weights for two of these were altered from to each. So, now we have three instances weighted each, and 2 instances weighted each. So the total value is . Now, we can count the weights for attribute: 

I have no experience in hmmlearn. But if you want to receive the labels for the underlying states of the process you have to do 'decoding' by using the 'Viterbi' algorithm. It calculates the most probable state sequence for your data. So if you only have two hidden states it calculates a 0/1 label for each of your time-stamps. If instead you're trying to discern between two different processes and recognize them, you have to train two different HMMs for the two processes and calculate the foward probabilty up to time t. Then you can choose the process with the higher probability. 

I don't feel secure enough to give a definitive answer but this described situation arises in phoneme classification when the data is split up in arbitray small parts. Here even though it is the same problem it does not cause any problems I know of. So I would just give it a try violating this assumption and just see if it works. This approach is often used in machine learning. For example Naive Bayes is sometimes used when the training data does not behave like a diagonal covariance gaussian etc. 

It's quite hard to understand what your actual goal is. I understood you want to calculate optimal fruit bundles from a daily given set of fruits which may not have any fruit in common. The easiest way to go about would be to just accumulate data how often each fruit is bought and take this as a measure of value for the customer. Then you could assume the bundling does not matter. From here you can just split up the fruit baskets by ranking the fruits according to their value and assign every odd element to basket one and every even element to basket two. If you are really sure the composition of the basket matters because there are interactions between fruits you have to do a different strategy. If you can accumulate vast amounts of data, you could just sell every possible combination of baskets and again take the sales as a value measure. 

Once you have your own lists of named entities, and you're only interested in extracting the relations, I believe there are simpler solutions (although I never tried relation extraction in NLTK, so I might be wrong): 

There are too many too general problems in your post. We're definitely in an AI summer era right now (as opposed to AI winter), and the research on Semantic Web receives less attention. Still, there are many projects related to building ontologies. Google has Knowledge Graph and Knowledge Vault. Both of these are using Freebase (among other sources). There are dozens of links I can give you to answer some of your questions, but the best thing you can do is to browse W3C Semantic Web pages. Take a look at RDF, SPARQL, OWL, Virtuoso, Protege - these are de facto standards. In terms of extracting ontologies from textual corpora - there are various tools out there. Neither of them is perfect, so you really have to do some research and find something that suits your needs. For example, there's the OntoLearn Reloaded (this paper is relatively new, so you can check out the bibliography to seek out for other approaches). 

All weights are ascribed. Now we can multiply the weights by the "frequencies" of instances (remembering that for and the "frequency" is now ): 

Looks good to me. This derivative is also presented in the paper (equations 56-58). The paper you're linking to is the most advanced attempt - at least to best of my knowledge - to explain how word2vec works, but there is also a lot of other resources on the topic (just search for word2vec on arxiv.org). If you're interested in word2vec, you may find GloVe interesting too (see: Linking GloVe with word2vec). 

Your question has nothing to do with NLP or text-mining (as you claim in the attached tags), or data science in general. It's a pure programming question best suitable for StackOverflow. Moreover, you don't really need any libraries to do, what you want to do. A simple function will do. NOTE: I am using and functions on purpose to include at least a little bit of data science-related stuff (WINK):