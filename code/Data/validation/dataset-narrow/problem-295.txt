You could just use trigger to track changes in whatever way you wish, you don't have to use the built in CDC. In a past job, we used CodeSmith to generate all our change checking triggers. 

So that may decide for you if any of your customers will not have the enterprise editions, or you don't yet know you will be using the enterprise editions. (As the spec includes "multiple future applications" this may be an real issue for you) Unlike triggers it is not real time, this is both an advantage and a disadvantage. Using triggers always slow down an update. I worked on one system when we used triggers (generated by CodeSmith), as well as tracking all the changes to the records, we also linked the changes together to a “history” table that included the module of the application that made the change, and the UI item the user used to make the change. However you may be best solving this at the application level, by say writing all update to a message queue that is then replayed to create a database at any given point of time, see Temporal Patterns on Martin Flowler blog for a good overview of options. 

There are many reasons way the database can’t safely do this, including the fact that add/removing Foreign Keys will change the meaning of pre-written queries including queries in the source code of the application. Most database also don’t have a good set of Foreign Keys that cover all possible joins you are likely to want do. Also for better or for worth, Foreign Keys often get removed to speed up systems and can't be used on tables that are loaded in the "wrong" order from file. However there is no reason why a query design tool or the text editor can’t auto complete a join with the help of Foreign Keys in the same way as they give you intellisense on column name. You can the edit the query if the tool got it wrong and save a completely defined query. Such a tool could also usefully make use of the convention of naming Foreign Keys columns by the “parent” table name and columns with the same name in both the parent/child table etc. (My wife still can’t understand the difference between Management Studio and Sql Server and talks about starting sql server when she starts up management studio!) 

I found no other relevant log entries in the Error Log or Event Viewer. The closest error that happens in the Event Viewer is: 

This error happened about ~18 minutes before the database start the recovery process, and repeated sometimes during the beginning of the recovery. It is somewhat related with the DBA user, but I really don't know what it is (I had no time to ask for the DBA yet). 

I am analyzing the size of some tables in Oracle, and I noticed that the primary key occupies almost the same size of its related table. To be clear, my statistics are the following (approx.): 

I am facing a situation that it is being somewhat hard to address. I need help to understand what is happening. TL;DR: Every time the Transaction Log gets full in SQL Server it needs to shutdown the database to enter in Recovery Mode and rollback the offending transactions? Is this always done by design or this only happens when something bad happens? 

I am currently experiencing a situation where an application is sending several identical queries at once (I found this in SQL Profiler). This application is buggy, it is probably in loop somewhere and the responsible team is working to solve this problem. But the same application also provides other services that are essential to the company, so I can't just disable it. I have to live with it, until the team solves the problem. So far so good. But this SQL Server instance also maintains other applications, whose performance are being penalized due this occurrence, some of them timeout their operation which makes this situation a DoS, in practice. My question is: what can I do, in the DB side, to minimize the impact to the other applications? I don't care if the buggy app gets penalized, but I would not like to penalize the other ones. (I can't just disable it, but I can slow down their queries results, if possible.) 

I think it automatically depend on the installation path of the scripts already. Note that you do not need to modify the systemd unit files in the library directory but you can amend them in the 

When using Oracle BasicFile and SecureFile one property of the implementation is that each LOB occupies multiples of the chunk size (and in case of SecureFile the smallest chunk size is the block size). This does luckily not happen for data which fits inline in the row, however for that data I could use a normal type. So my concern is more with data stored in the lobsegments - there the overhead can be up to a whole database block (per row) if the LOBs have unlucky sizes. Does this change when compression is turned on? I mean if I have a blob which is 7k and compresses to 5k It would still occupy 8k (if this is the DB blocksize). It would be somewhat unfortunate to have a tablespace with extra small blocks for (extra large) LOB segments. (But I understand that SecureFile with dynamic chunk sizes deals much better with this) 

In a scenario where Oracle Golden Gate is used to replicate a primary site with an Oracle RAC database to a secondary site (and active/active back) we suspect unexpected changes from the unused secondary site. The issue is a bit hard to debug as we do not have direct DBA access. I wonder is there an easy way with unprivileged SQL access on the primary side to see if any changes are received from the other database? Can I see counters or timestamps of OGG activity which helps me to track down DML made? As I understand it I could see changes from the OGG user when setting up triggers or auditing - however both is not available in this situation. 

The scenario: One of our heavy used production databases, which runs several ETL jobs, and long running table batches, entered in Recovery Mode and became inaccessible for some time. This happened three times this week (this server is on for ~2 years, and we didn't notice this issue in the past). Looking into the errors logs what happened was clear: the Transaction Log was full, the database needed to rollback the transaction, the rollback failed, the database shutdown, and started in recovery mode. The DBA defends this as normal behavior of SQL Server. That is, according to him, every time the transaction log gets full and a transaction needs to rollback the database will enter in Recovery Mode due to the lack of log space. After the rollback (that can only be done in Recovery Mode according to him), the database will become available again. I found no reference for this info. So I strongly disagree. I would really appreciate if someone convince me that I am wrong. My point: As far of my knowledge, a DBMS is built to manage/run queries. If it lacks space, the query will fail. Simple as it is. And I am not talking about performance of anything else, but availability only. It makes no sense for me to accept that a DBMS needs by design to shutdown itself to rollback any transaction. In my understanding, it does not matter if I am running tons of queries or if the queries are bad designed. The bad queries should fail and life continues. Doesn't it? My guess is that something else is making it fail, and I need to track what is happening. Is my understanding wrong or this is really how SQL Server is designed to work? Supposing I am not wrong, what else can I do to track the source of this issue?