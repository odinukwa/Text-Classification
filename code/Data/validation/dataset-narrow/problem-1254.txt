I have a transitive DAG G which is a subgraph of an unknown DAG R. (The nodes are the same in G and R, but R may have edges not in G.) I can determine the presence of a given edge in R by an oracle query. Many natural questions come to mind. I've found it's often more useful to give several than just one—I wouldn't want to reject the answer I don't yet know is the right one by being over-specific upfront. 

Remove all $n_j$ which are multiples of $n_i$, including $n_i$ itself. Compute the new density $D_\text{new}$. The density is $D = 1 - (1-D_\text{old})(1-1/n_i).$ 

The algorithms used may be adaptive. Here's (transitive reduction of) an example of a graph of the sort I'd test. As a simpler version of #1 or #2, what's the complexity for this graph? sample graph $URL$ 

Valiant's theorem says that computing the permanent of an $n\times n$ matrix is #P-hard. Is the problem of determining if a permanent is 0 any easier? This arises in the context of sequence A006063 in the OEIS, where membership is determined by the sign (0 or nonzero) of a certain permanent. (A solution to the restricted problem would be enough for me.) 

The conclusion that Th($U$) is at least exponential time. They don't provide a proof for this (they say they will in a subsequent paper, which I believe has never appeared), but they say that their proof for the theory of the real numbers can be adapted by using $ku$ as a representation for $k$. Is there a more detailed exposition of a proof of this theorem? The following points are not clear to me: 

I've seen pseudorandom generators defined for nonuniform efficient adversaries, or uniform probabilistic efficient adversaries. (For example, a monograph Pseudorandomness by Vadhan (here's its draft does that.) I believe that it's natural to think about pseudorandom generators indistinguishable by uniform deterministic efficient adversaries. Has that notion studied before? Does it have any significance to some degree? EDIT: I was mistaken in that Vadhan defined pseudorandomness against uniform probabilistic adversaries: he did define indistinguishability by uniform probabilistic algorithms, but he neither defined, nor stated facts about, the corresponding notion of pseudorandomness. My interest mostly concerned with uniform adversaries and my original question was about deterministic vs. probabilistic. 

This can be generalized to composite $n_i$ if all terms are either divisible by $n_i$ or else coprime to $n_i$. This expands the size we can approach with the naive lcm algorithm, but it doesn't fundamentally alter it. 

Given a set $S$, its multiplicative closure is the set $$ \mathcal{M}(S) = \{s_1s_2\cdots s_k: k\in\mathbb{N},s_i\in S\} $$ of products of zero or more elements of $S$. So the multiplicative closure of $\{2,3\}$ is the set of integers of the form $2^m3^n,$ for example. I'm looking for an efficient algorithm to compute the number of elements of $\mathcal{M}(S)$ up to a given bound $x,$ given a finite set $S$ of positive integers. You may assume (it is true in my case, and simplifies the problem) that each element of $\mathcal{M}(S)$ has a unique factorization in $S$. In my case $S$ is small (150-250 elements) and $x$ is large ($10^{35}$ to $10^{50}$ or more). A naive algorithm would construct all of the numbers and count them. This is slow, and more importantly consumes a lot of memory. A better approach (dynamic programming) is to split the set roughly in half, let's say into $S_1$ and $S_2$, and construct the multiplicative closures of both. Then each element of $\mathcal{M}(S)$ is a product of an element of $\mathcal{M}(S_1)$ and an element of $\mathcal{M}(S_2)$, and by the special property no other product is equal to this value. So you can iterate through one of these, let's say $\mathcal{M}(S_1)$, and for each $m$ in that set the number of products which are at most $x$ is the number of elements in $\mathcal{M}(S_2)$ which are at most $x/m.$ This can be determined with a binary search. Is there a way to push this method further? Is there a completely different approach which is more efficient? Is there literature I can read? 

Fischer and Rabin's Super-Exponential Complexity of Presburger Arithmetic (1974) has the following theorem. 

Whether or not $\mathbf{BPP} = \mathbf{EXP}^{\mathbf{NP}}$ is an open problem, although we believe the former is strictly contained in the other. I guess, from the absence of the proof of the separation, that there should have been a relativized result on this problem, namely, the existence of an oracle relative to which $\mathbf{BPP} = \mathbf{EXP}^{\mathbf{NP}}$. Is there such a result in the literature indeed? 

A priority argument, an important proof technique in recursion theory, was introduced by Friedberg and Muchnik, to solve Post's Problem, i.e., the existence of two r.e. sets that do not Turing reduce to each other. My question is whether or not there has been an attempt to formalize priority arguments in some proof system adopted by proof checkers or proof checkers. I believe this is an interesting problem, since: 

The exact hypothesis of this theorem is unclear to me. On one hand, they say "additive", use the symbol $+$, and list commutative monoids or expansions thereof as examples after stating this theorem. On the other hand, they do not explicitly say "commutative" or "abelian"; it may be sufficient to assume associativity if we are applying the operation to $u, u+u, \dots$ to represent natural numbers because commutativity is not an issue as long as these elements are concerned. Presumably, one needs to say in the language $\{+\}$ that $u, \dots ku$ are distinct in order to use them as representation for natural numbers up to $k$. One also needs very large $k$ to simulate Turing machines. How can one ensure that $u$ has the desired property with a relatively short logical formula? 

No, the string need not be normal. Take any uncomputable sequence and add two 0s between each term; now there are too many 0s for the sequence to be normal but it's still uncomputable. 

I have a finite set $S$, a function $f:S\to S$, and a total order $<$ on $S$. I want to find the number of distinct cycles in $S$. For a given element $s\in S$ I can use Floyd's algorithm (or Brent's, etc.) to find the length of the cycle that repeated applications of $f$ sends $s$ to; with a bit more effort I can identify this cycle (e.g. by its $<$-minimal element). A bad method for solving the problem would be to repeat this each element, sort the resulting minimal elements discarding duplicates, and return the count. But this potentially involves many passes over the same elements and large space requirements. What methods have better time and space performance? I'm not even sure what's the best way to measure the space needed—if $f$ is the identity function then any method that stores all cycles will use $\Omega(n)$ space. 

Let $F$ and $G$ be endofunctors over categories $C$ and $ D$, respectively. Suppose that there is a forgetful functor $C \to D$ that has a left adjoint. Can we infer properties of $F$-coalgebras from those of $G$-algebras, or vice versa, by exploiting the connection between the two categories? Presumably the situation here is too vague. Is there some special case of this scenario in which my question make sense and answerable? What are the relevant literature here? (The application I had in mind is this: suppose that I have a coalgebraic modal logic $L$ for coalgebras in $D$, and that $L$ as it is does not generalize to other categories than $D$ since it relies on some special properties of $D$. Can I still use $L$ to describe coalgebras in $C$, at least in some limited cases, by using the adjunction?) 

Let $L$ be the set of sentences in some logic. I am interested in cases where $L$ is the set of sentences in monadic second-order logic, or it is its $\Pi^1_1$ fragment. Let $K$ be a class of finite structures. The theory $\mathrm{Th}_L(K)$ of $K$ is the set of sentences $\phi$ such that $\forall M \in K$ $M \models \phi$. I am interested in decidability of $\mathrm{Th}_L(K)$. For what kind of class $K$ is this known? What are important / deep techniques here? What is a good source to learn the results and the techniques? 

Here's an example of a simplification we can make to the problem. If one of the $n_i$ is prime, then you can 

Given a fixed regular language R, what is the complexity of generating all members of R with length at most $n$? Suppose some reasonable model (RAM with $n$-bit words?) and a write-only output tape. The list should be in length-lexicographical order. I'm interested in the answer in terms of the output as well as the input. (If R = Σ* then you can't improve on $O(|\Sigma|^n)$ but it only takes time linear in the output.) 

I'm considering the problem of recognizing a language (over alphabet 0-9 and space) containing strings like "1 2 3 4 5 6" and "14 15 16 17" but not "1 3". This came up while working on a common parsing task where elements needed to be in an ordered list. It struck me that while parsing the rest of that language was regular, this part was clearly irregular -- it can recognize, for example, the language A1A2 where A is an arbitrary string 0-9. In fact it seems to be content-sensitive (and not context-free by the pumping lemma). My first question: is there a (reasonably well-known, i.e. not defined just for this problem) class of languages between context-sensitive and context-free that describes its expressive power better? I've read about Aho's indexed languages, but it's not obvious (to me!) that these are even in that class, powerful though it is. My second question is informal. It seems that this language is easy to parse, and yet it is very high on the hierarchy. Is it common to come across similar examples and is there a standard way of dealing with them? Is there an alternate grouping of classes of languages that is incompatible with inclusion on the 'usual' ones? My reason for thinking this is easy: the language can be parsed deterministically, by reading until you get to the end of the first number, checking if the next number follows, and so forth. In particular it can be parsed in O(n) time with O(n) space; the space can be reduced to $O(\sqrt n)$ without too much trouble, I think. But it's hard enough to get that kind of performance with regular languages, let alone context-free.