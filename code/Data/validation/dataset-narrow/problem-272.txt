The solution was to change the processing in Visual Studio (VS) from Default to Full. The issue is described in detail here: $URL$ (see Cathy Dumas's reply under "Posted by Microsoft on 8/25/2011 at 2:02 PM") 

To my surprise, I can create tables (!?!) I can also truncate them and drop them (?!?) Not sure what the heck is going on here. Am I missing something? I need the user to be contained in this database and not to exist outside of it. 

I am experiencing an issue with an SSIS job that connects to a Firebird DB using their ODBC driver (version 2.0.4.155). 

Is anyone aware of a SSAS (tabular) 2016 in which the number of records in a table are not fully shown in Excel? 

If the value of the source varchar field is blank, then it will be set as NULL in the target tinyint field. For that I would normally use NULLIF. If the field contains anything that could not be cast as a tinyint value (e.g. punctuation, letters, or a value higher than 255) then the value will be set as 255 in the target tinyint field. The rationale behind this is that the tinyint field would never have a 255 value unless there is an error -- and it wouldn't be appropriate to have errors set as NULL or 0 because these are perfectly valid values and we wouldn't be able to differentiate between a valid entry and a mistake. 

I have an SSIS job that gets data from a SharePoint 2010 list that runs perfectly from Visual Studio and also if I trigger it manually from the SSIS store, but not when scheduled as a job in the SQL Server DB Engine instance. I am doing everything using just one super user account called "spadmin". 

Connecting to the database via SSMS using my admin account. Right click the database in question and select new query. 

I have a date dimension table in which I need to add a new column in which I define the iteration of the day of the week within the month (2 for the second Mon/Tue/Wed/Thu/Fri/Sat/Sun etc). Is it possible to do this be making calculations solely on the date column of the table, which is of type 'date'? 

Log in with the user that has the User DSN entries that you with to convert to System DSN Open the Registry Editor and navigate to HKEY_LOCAL_CURRENT\SOFTWARE\Microsoft\ODBC\ Right-click ODBC.INI and select export to save it as a file on the desktop (or anywhere else you fancy) Open the .reg file with a text editor such as Notepad Replace the text HKEY_CURRENT_USER with HKEY_LOCAL_MACHINE. Save your changes Double click the .reg file and proceed to import it into the registry. 

The only thing I can recommend is to put your 2000 database on a server with SQL Server 2005 or greater (a development or test server) installed, find the unused indexes with the help of the new DMVs and once you have a list of changes apply them back to the production SQL Server 2000 instance. Please be aware that SQL Server 2000 is no longer supported and you should move your production processes, if possible, to a later version of SQL Server. An example query in SQL Server 2012 using the sys.dm_db_index_usage_stats DMV: 

The short answer is yes. You can read more about the Merge command syntax at the Microsoft MSDN site. 

How to find a list of available tables using the sys.tables that have a name with documents in the name: 

There is a limitation of this query that it will only search in one database and you have to keep on changing the target database manually until you find the missing table. This script, which looks into all the databases on the server, will provide the database name, schema name and tables contained on the target server. 

You should read Brent Ozar's take on memory. He has some fairly standard answers on why you should be looking memory and why more memory equals better performance. Generally speaking 4 GB or 10% reserved for OS. 

You should look at the Execution Plan and see where most of the cost is for the query. Without the results of the Execution Plan it is difficult to offer advice. I suspect the query optimizer is using the best process possible for the query as it is structured. You might try breaking the query into smaller parts, like query the email field to get the results into a temp table first, then run your main query using the temp table of emails. The might not make the query run faster, but this might expose information in the Execution Plan that will help you troubleshoot. Another option is to use include to add columns to your non-clustered index to return all the values in the index scan to maybe speed up the process. 

If you have your servers running in a virtual environment, you can perform a snapshot on a clone and then apply the in-place upgrade and test the instance to verify the upgrade was successful. If it works, you can apply the snapshot and make the clone the production server. If it goes poorly, you can delete the snapshot and go back to the pre-upgrade image to try again, or delete the clone and to a full migration. 

This immediately made me think that a ODBC driver mismatch might be the issue, where SSIS is looking for the 32-bit drivers. So here is what I did: 

I am inserting values form a staging table in SQL Azure into a production table. Values in the staging table are varchar, while the target value in the production table is tinyint (it is an age field for humans, so I reckon that up to 255 will suffice ;) ) Problem is that due to a bug in the source system, we tend to get some values as -1 or -2 when they should be 0. This causes issues, since TINYINT only supports values form 0 to 255 (and also because this is factually wrong). I was wondering if there is some sort of case statement that I could use to convert values to 0 if they fall under 0, or perhaps if they fall outside the 0-255 range? 

This only seems to happen with ONE table (all other tables are fine). So we tried recreating the table, but the same problem persists. Any idea of what might be causing this issue? 

As soon as I logged off from that SQL server, the behavior occurred again. This was hard to troubleshoot at first because the drive installs without throwing any errors and it even works at first. But I am confident now that this has to do with permissions. Any ideas what I might be missing? 

I am trying to create a stored procedure that will allow us to duplicate values from the previous month into the current month -- BUT ONLY if there isn't a value for the current month already. 

At this point I am just trying to select the rows based on the @backDays variable, but I cannot concatenate the @query string with the variable. Not sure of what I am doing wrong here. I am fairly new to dynamic queries, though. 

I have a table in my db that stored both successful and failed log-in attempts. I am creating a stored procedure that allow us to delete records which are older than X days old. So far so good, but I want to raise it a notch (or two) and allow us to specify whether to delete records on whether the [Success] column is true, false or both. I am having some issues with the concatenation of the script that needs to be executed, though. Here is what I did so far: 

This works fine but of course I don't like to have plaintext PWD in scripts. At a minimum could chmod 500 on it but still feels sub-optimal to have plain text pwd in script. What are the best alternatives? I'm running Mongo 3.4.4 on RHEL 7.3. It's the non-Enterprise version of Mongo so no LDAP/Kerberos. we are not using x509 security either and I'm looking for alternatives that don't involve upgrading to Enterprise or going down x509 route, although I'd be fine with hearing why doing those things really is the right thing to do and also without doing those things what the gaps are that get left open. 

The chunks collections can ONLY be sharded on files_id or files_id,n. So, rather than accepting the default ObjectId() _id given to a file uploaded into fs.files (which is used as the files_id "parent" value in fs.chunks), I have used the MongoGridFSCreateOptions object to set the _id/files_id to a value which is composed of the value I want to shard on plus a GUID. Now I can shard fs.chunks just fine. 

The system is highly write intensive, with very few reads. Thus, overall I believe the above setup is optimal, since writers get fast "local" writes, while readers get convenience of single query to read across the shard, albeit slightly slow [and just to underline, for reading, I value convenience over speed]. OK so that's the background. But then suddenly I realized: I'm also using GridFS to store files, and I want the files to get stored in same way (i.e. write to local region). Hopefully in the context of the above, my original question makes sense: How to define shardKey on GridFS collections (fs.files/fs.chunks) to achieve Location/Data Centre affinity? This is what I've tried: I've noticed that I can add meta-data in during the upload operation, such that the fs.FILES collection has the region in as an accessible field that could be used as part of the sharding key for fs.files (i.e. sharding key REGION + "FILE_ID"). However, what about fs.chunks? will they follow around their "parent" fs.file record and get routed to the same shard as the "parent" fs.file? Thanks in advance! 

We would like to do nightly purging of old data out of our mongo cluster. First thought is to write shell script which connects to one of the mongos and issues a remove instruction. 

3 shards, one per global region APAC/EMEA/AMER (in different data centres) Shard Key is REGION + 'MONOTONICALLY INCREASING PRIMARY KEY' The "writing" clients for the cluster [typically] connect to the mongos in their region, and will be writing documents with their region's region. The "reading" clients for the cluster [typically] connect to the mongos in their region, and will be reading documents from across multiple regions (broadcast query). 

I am trying to create a contained user for a database in SQL Azure that only as read-only access. What I am doing is: 

Here is the answer. Even though the job was configured to run via a PROXY Account, the SQL Server Agent is still responsible for the job. I had a look and the SQL Server agent was configured to run under the Local System Account on that server. So what I did is to put the agent to run under the superuser admin account and it worked as expected. Now in this case the fact that the job no longer needs a proxy since the Server Agent itself is running under the ultimate account. However I appreciate that this is not the right way moving forward (even though this isn't my server and I hope I never get to touch it again!) I will be advising the customer to reconfigure SQL so every service runs under a dedicated domain account (i.e. created solely for this purpose), which is the way it should be! Now what I would love to understand is why the job would run as long as the proxy account used for scheduling the job was logged into the SQL server! 

This is as far as I managed to get unfortunately. My skill is limited where it gets to do an INSERT with a condition with the NOT EXIST condition. How can I make sure that I only insert rows in the xxx.MyTable where rows with the same primary key (period AND genusId AND subjectId AND waitingStageId) does not exist already? 

I will be able to execute the package again as long as I am still logged in to the SQL server (I logged in to install the Firebird drivers). I can execute it even from a remove SQL Server Management Studio connection -- but as long as I do not log off from the server. If I log off from the server, then the SSIs job will no longer work (same error as before). This led me to think that this is not in fact a 32/64 bit mismatch, but perhaps some registry or environment variable is not being committed after I log off from thee server due to insufficient permissions (even though I was meant to be admin on that server). So for my next test: 

The process outlined above basically copies all User DSN entries as System DSN entries. If you to have just one entry copied over, then you will need to delete their corresponding keys under .\ODBC.INI as well as their corresponding entries in .\ODBC.INI\ODBC Data Sources.