I'm implementing a lock-free, multiple consumer, multiple producer FIFO queue/pipe as an exercise in thinking about atomicity in operations. My main concern is correctness of operation, my second concern is good practices around atomics and general C++11. Performance is interesting but not important for this exercise. Without futher ado, here's the code: 

If executed as is this would require two execution stages. The two multiplications would have to wait until the division is completed before they can start. The total delay is 1 div + 1 mul (the two mul can execute in parallel on the cpu). Compare this to: 

The optimization gives you maybe 10% speed gain which isn't much. And I say maybe, it depends on if your CPU correctly predicts the branch so it's probably a bit less than 10%. These kind of problems very rarely depend on how you optimize your code, but rather that you have the correct time complexity, see the above. 

So from these results we can draw the conclusion that any differences between the two methods is minimal and I would say it is within measurement error because these measurements are done on an otherwise active system. Benchmark code below for reference 

Again it is not bad design but use the correct pattern for it. And yes, statics tend to complicate things when you start multithreading but in your case it is no less complicated if using the singleton pattern. 

will not bind to temporaries because it must be able to take the address of the argument. Other than that it will bind to reference to because will be deduced to . So this constructor covers all legal use cases that the one with universal reference does, so you can remove either one. I suggest you nuke the universal one because it is slightly harder to understand. Stream operators Your two stream operators: 

The singleton pattern as I implemented it has well defined construction and destruction which means that all code paths that exit the program normally will invoke the destructor and perform clean up. You do not need to call explicitly to clean up. The standard guarantees that memory for members will be laid out contiguously in memory which is good for your cache hit/miss ratio. Granted that the compiler may likely place your statics contiguously as well, this is not guaranteed though. If you at some point decide to allow multiple game instances, say you make a multiplayer server, object oriented code is much easier to adapt. 

Okay I understand this is a programming challenge so I won't comment on programming style or variable naming as one does not usually care in this case. But it would be nice if you'd tidy it up enough for us to see what you're doing. I can't quite make out what is supposed to be for example. In any case I'd bet your performance issue comes from these two for loops: 

Conceptual Problems You have hard coded the animation FPS as . This is a big no-no! You are assuming that your code will be called exactly with exactly intervals. But you are using which is: 

The class seems like a work around for a problem with passing integers as arguments, I would structure my code so that this is not needed. 

and your code would be easier to follow and would generate the same code. And allow the above usage syntax. Reading into a temporary?? This tickled my funny bone: 

Your card drawing is assuming that there are an infinite number of decks shuffled together, making it possible (albeit not very likely) to for example draw 20 aces in a row. This is not comparable to how you would play at a real black jack table (you can't count cards for instance). You'd need to decide a number of decks you want, allocate a buffer with all card values and shuffle it (for example using Fisher-Yates) then have a pointer into the buffer for the next card to draw and increment it. Once you reach the end, you shuffle the buffer again and reset the pointer to the start. 

Note that this has the same asymptotic time complexity as OP's own answer \$\mathcal{O}(whr^2)\$. But this computes the reciprocal of the hypotenuse only \$\pi r^2\$ times while OPs answer does it \$\frac{wh}{5000}\pi r^2\$ times. The calculation of the reciprocal of the hypotenuse is the most time consuming part in this algorithm and should dominate the execution time. Note: The observant reader will have noticed that the grid is symmetrical in all quadrants. This means you can reduce the number of calculations of the reciprocal of the hypotenuse to \$\frac{\pi r^2}{4}\$ but that's left as an exercise to the reader. 

you are passing it "by value" which means that the variable will be a copy of the value the caller passed in. When the type of the parameter is a complex type or managing dynamic memory (like ) is doing then this copy time will become non-trivial. Unless you actually need the copy (because you're storing it somewhere or you want to modify it without affecting the caller) then you should pass all non-POD types by reference to const. Like so: 

I do not do C# but I can offer another way of approaching the problem: As our range of numbers is \$[100\cdot100, 999\cdot999] = [10000, 998001]\$, assume a palindromic number on the form \$abccba\$. The largest \$abccba < 998001\$ is \$k = abccba = 997799\$. We need to determine if \$k\$ can be written as the product of two 3-digit numbers. Using a prime table (or generate one with a sieve at program start) see if any prime number \$p\$ where \$\min\left(\frac{k}{100}, 999\right) \lt p\le k\$ evenly divides \$k\$. If you can find such a prime then we know that \$\frac{k}{p} \lt 100 \$ and any remaining factors are too small or that \$p \gt 999\$ and we have a factor that is too large. So we can discard \$k\$ and decrement \$c\$ by one to form the next smaller palindrome, eg. \$k=996699\$, and keep trying smaller palindromes. If you reach \$p\lt \min\left(\frac{k}{100}, 999\right) \$ without finding any prime that evenly divides \$k\$ we need to convince ourselves that the remaining factors can be multiplied into two factors so that both factors are three digits. There are a few ways to convince ourselves of the above. One way is trial division by simply trying all divisors \$d\$ such that \$100\le d \le \max\left(\frac{k}{100}, 999\right)\$ and checking what the other factor is. Another more elegant way is to use the fact that we know that there is no prime factor larger than \$p\le\min\left(\frac{k}{100}, 999\right)\$ and find the largest prime factor \$p_{max}\le\min\left(\frac{k}{100}, 999\right)\$ and let \$r_i = \frac{k}{p_{max}}\$. Let the prime factors of \$r_i\$ be \$p_i\$ and let \$Q=\prod_{\forall i:p_i>\frac{999}{p_{max}}}p_i\$. At this point you have the largest prime factor of \$k\$ as \$p_{max}\$ and all prime factors of \$r_i=\frac{k}{p_{max}}\$ that are too big to be together with \$p_{max}\$ multiplied together as \$Q\$. Now, factor \$\frac{r_i}{Q}\$ into primes \$P_k\$. Each of the \$P_k\$ primes has to be multiplied into either \$Q\$ or \$p_{max}\$. Iteratively try all combinations (and exit early for impossible branches) until you find one that is satisfactory or retry next smallest palindrome if no combination was found. Note: This may well be slower than trial multiplication but it's another way to approach the problem and it may give birth to other ideas. 

Matrix multiplication, while it seems trivial to implement from the definition, the naive implementation you are using is actually slow for anything but small matrices. You really should look into more efficient algorithms for matrix multiplication, a good place to start is the Wikipedia page here: Matrix multiplication algorithm. 

Set speculation hat: On The JIT should inline expand the function calls to and , it should also not do anything with the stack variables (not even initializing). So the remaining difference is and the book keeping associated with the variable which could make or break here as you're only doing a few instructions to fetch the data every time. Adding a few book keeping instructions on the variable could have a big impact in your case. I would investigate if I could get rid of it somehow, maybe encapsulate it somehow so the same instance is used in all calls. Now if you doubt your JVM is good, you can try to inline the two function calls manually and restructure the thread safe variant like this: 

Not mentioned elsewhere: Source and destination can be pairwise unaligned Which would force at least one unaligned write or unaligned read per word. Depending on platform this can cause a cpu exception, poor performance or be hardly noticeable. Let the compiler do the job A good optimising compiler will detect a memory copy and generate a fast sequence of instructions to do the copy. Just write a byte by byte copy and let the compiler worry about the details. 

Don't load the same resource repeatedly I don't have much time to give a full review but I can point out one big thing that I saw while looking through the code. 

Let me stop you right there buddy! Just because you are using a C++ compiler, doesn't mean your code is object oriented. Lots of C, and procedural code compiles perfectly fine under C++. What the language is and how you use it are two entirely different things. Just because you can use a wrench to hammer a nail, doesn't make it a hammer. 

You should ideally prefix the include guard by something that is unlikely to cause a collision with any one using your library. As it stands now it is at risk of collision if some one else unluckily choose to use as their include guard. Hide implementation details To me it looks like the class is not directly intended to be used by the user. In that case you should wrap it in an internal namespace so that the primary namespace that the user sees isn't cluttered with internal functions. Naming I think that the name is kind of mixing two different case standards. To my knowledge there are kind of two major standards emerging, one is to use CameCase in which you would name your class and the other one is to use inderscore instead of space and all lower case so you would have . If you're going to allow upper case in names, then I would at least suggest that you use leading upper case like so . Any of the above are easier on my eyes than the current name. Also I think that the functions would be better named : 

How large buffer size are you using? What is the hit/miss frequency on your buffer in ? If the hit frequency is less than 90% and you're doing as you say 80E6 per second that means you have 8 million buffer allocations per second which is going to cause the GC significant head aches. AFAIR the JVM can stall all threads while doing GC under certain circumstances (I had a similar problem developing a high-performance parallel java application). So I would change: 

Always measure! :) I took the liberty of taking the original code and the proposed algorithms and did a benchmark. I also threw in my own algorithm in the mix. First off, the results. I generate a random string with 10k vowels and 90k non-vowels and run each algorithm through it for 1000 passes and take the average: 

is telling you that you won't get any non-latin letters so the ASCII assumption should hold here. If you can't assume ASCII then use a bigger table or a . As for other aspects of the code, those have already been covered by others. 

Lets take a look at the function too, adapting it to use a list of lists for chaining and cleaning it up a bit: 

Generate shuffled arrays in a vector (). Start timing For each of the shuffled arrays, sort it in place. Stop timing Write all the arrays somewhere (cout or file) to prevent removal by as-if. 

Usage remarks If you're a single developer using this in your own project then I do not see any problems with the usage. After all it's a convenience for you. I can not comment on the design on the macros as I find them harder to read than what is worth spending my free time on (this is not specific to your macros, but to complex macros in general). However if you are working in an environment where there are more developers than yourself, then I believe that this type of convenience macro is rather an inconvenience. In general I find that the further one goes to try to make C++ look like something it isn't by "inventing/implementing new syntax" using various macros, the more difficult it becomes to maintain over time. Developers come and go and the closer you stick to standard approaches to solving problems the better you will be able to handle new programmers on your project. As Michael Urman said in a comment on OP: 

In a merge sort (and quick sort) you need to stop your recursion when there is a handful of iterations left (say 32 elements) and then use an in-place sort (some even use bubblesort shrugs) to sort the last elements in that range. The reason is that the recursion and splitting overhead becomes too large when the number of elements are small. 

Also add some checking to see if you have enough "free" numbers of the type in the hashmap. The time complexity is amortized \$\mathcal{O}(n^2)\$ as lookup in is amortized \$\mathcal{O}(1)\$. 

There are a few problems namely that you the value of to . If the universal reference of has bound to an r-value reference then will have the same effect as and thus is not required to contain a valid task for and if it is added to a queue and subsequently executed you will have undefined behaviour. You must only a universal reference exactly zero or one times. Closing remarks In the current implementation I cannot see how this multi-queue could be more effective than a correctly implemented single queue. But I would love to be proven wrong by benchmarks with source code. 

Use properties of prime numbers The only prime number that is even is 2, for the simple reason that if the number is even, it is evenly divisible by 2 and is thus not a prime. So after checking if 2 is a factor and removing it, simply start with and in the update do . And you're twice as fast! But there is more, consider \$x=6k+l\$ where \$l\in\left[0,5\right]\in \mathcal{Z}\$ and \$k\ge 0\$. It's obvious \$x\$ can represent any positive integer. Assume for a second that \$k>0\$ and lets look a bit closer: