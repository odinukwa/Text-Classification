Even if you don't have ratings or reviews, you can use the customer purchases to help creating your model and selecting the most appropriate one. If the customer has bought the product, you can suppose he has liked it. If he hasn't, we can suppose he has disliked it. This is an approach which is mainly used by e-shops. You can find more on recommendation system selection and validation below. It is discussed how to choose the best recommender with both offline and online approaches. $URL$ 

Also, here is a playground to test gradient boosting algorithms including on XOR dataset. It is really interesting ! You can click on "rotate trees" to activate rotation. Gradient boosting playground 

The reason is your learning rate alpha is too large for this optimization problem. Start with a really small value (< 0.000001) and you will observe a decrease in your cost function. Keep in mind that when the learning rate is too large, the gradient descent algorithm will miss the global minimum (global because MSE cost function is convex) and will diverge. This is why you have observed still increasing cost function values with alpha = 0.001. 

I have taken a look on your code. You obtain same errors results for each alpha value because your regularization strength is too small. Replacing : 

In case you have more than 20-25% missing data, it will be really hard to impute values. I think in this situation you should consider creating a new model such as : 

I think second job will benefit more from spark than the first one. The reason is machine learning and predictive models often run multiple iterations on data. As you have mentioned, spark is able to keep data in memory between two iterations while Hadoop MapReduce has to write and read data to file system. Here is a good comparison of the two frameworks : $URL$ 

It depends of the algorithm you are using. For a linear model (linear / logistic regression, SVM...), you need to create dummy variables meaning features "Sex_M" and "Sex_F" as you noticed. However, if you are using tree-based technics, create an integer typed column with Sex in [0, 1, 2] should be sufficient for these algorithms. Reason is, contrary to linear model, tree-based technics are non-linear and will evaluate all possible splits to partition your observations. However, the way you map your categorical variale into integers can lead to different trees structures. Below is an example. Suppose you want to predict y variable using indexed feature "Sex". On left chart, there is a slight difference between category 2 and categories 0 and 1 because categories are not oriented. It will results into two consecutive splits and should happen late in the tree building. However, on the right chart, the means difference between categories is larger that on left chart. So the split should happen earlier in the tree building. Moreover, you will only need 1 split to make the difference. To conclude, I think simple indexing can be used with tree-based technics. I would also recommend you to sort your categorical features in an orderly way to easy tree learning. 

You have instantiated a sparkContext object with "local" mode configuration. It means you have allocated ressources for a single multi-core Java Virtual Machine on your computer. In this configuration, you can't reach better performs than with python. Because : 

I agree with most of the answer. However, I think you are missing some points including the cross-validation step. I try below to provide an overview of a common machine learning project. I assume a common project is a supervised machine learning problem (like iris dataset). 1. You defining the 'scope' or aim of the project : 

Keep in mind that the number of executors is independent of the number of partitions of your dataframe. You set the number of executors when creating SparkConf() object. $URL$ To retrieve the number of executors from code, it depends of the API you use. For python, this is not currently implemented. For Scala, you can. Here is a good stack post : $URL$ Number of partitions refers to the number of parts your dataframe is composed of. With pyspark API, you can get it by transform your dataframe to a rdd and then use method .getNumPartitions() : 

I think the second method will yield less correlated models than the first method. It is particularly true with decision trees which tend to quickly overfitting in the bottom nodes. It will help reduce the variance. However, by using the second approach, you will end with 10 smaller datasets and so you risk to introduce a variance error due to the too small number of observations. Discussing about decision trees again, it would mean that your trees algorithms will tend to overfitting upper in the tree. And so you will increase your variance error. In my opinion, for most of datasets, it is still better to use the first approach than the second. I think that very low correlated estimators won't bring a better improvement that the first method. We can also observe that differences in the two approaches depend also of the number of observations, the number of features, the kind of estimators you are using.. A benchmark would be really interesting ! 

The purpose of the Word2vec is it will itself learn a hidden structure in your text data. If "recovery" comes after rap and music is because Eminem is more frequent with these context words than with recovery. You can't simply change that. You should avoid making a manual intervention on your model. However, here are 2 ideas you could try if you really want "recovery" to be closer of Eminem word : 

Why pruning is not currently supported in scikit-learn? How can we tune the decision trees to make a workaround? 

Spark benefits arise when you use Spark on multiple nodes. In this configuration, spark master is yarn, mesos or standalone. In that case, a spark job would be separated in multiple tasks and each node would be dedicated to different tasks. For example, if you have 3 nodes and 6 tasks, each node can handle 2 tasks. If you have 6 nodes and 6 tasks, each node will handle a task. With simple python, think that you have only 1 node for "6 tasks". So, for large tasks (big enough datasets), Spark will results in better latencies than simple python. 

will yields different errors values for alpha values large enough. Are you sure about figure alpha values? Do you have a link to this hands-on? Also, I would like to highlight the fact that you have to standardize your features before using regularization. Reason is, by creating polynomial features, polynomial features will have different magnitudes. Therefore when fitting model, coefficients to be estimated won't have the same magnitudes neither and so regularization will highly penalize coefficients with large values. Standardization / Normalization is a strong prerequisite to regularization. 

I guess differences in accuracies between class 0 and class 1 come from the class_weight parameter you have used. Class 1 will benefit from this overweighting towards class 0. You could try to play on this parameter to re-balance your results in class 0 and class 1. An other idea could be to play on probabilities outputs and decision boundary threshold. Remember than when calling for method .predict(), sklearn decision tree will compare outputed probability with threshold 0.5. If it is greater than 0.5, then it assign class 1. On the contrary, if it is less than 0.5, it will assign class 0. You could try to play on this threshold by outputing probabilities first with .predict_proba()[:,1] and then test results for different thresholds decision boundaries. You can operate such as below : 

Here is just a guess, but according to me, the linearSVC might perfoms better than SVM with linear kernel because of regularization. Because linearSVC is based on liblinear rather than libsvm, it has more flexibility and it gives you the possibility to use regularization with your SVM (default is L2-Ridge regularization). Because you have more features than observations, it exists multiple solutions to your classification model. Some of them are more "robust" than other ones. L2 regularization will help you reduce the ampltitude of your model coefficients and maybe lead to a more stable solution. More on LinearSVC in the documentation : $URL$ More on Ridge regression benefits when working with a lot of features (and so multicollinearity) : $URL$ 

Simple decision trees have some limitations listed below. Fortunately, some of these can be fixed used ensemble learning techniques (think bagging, boosting...). Concerning limitations : 

Bagging (and features sampling) aim to reduce variance by providing low-correlated trees. Estimators can then be aggregated together to reduce variance. Reason is simple decision trees tend to quickly overfit in the bottom nodes. Bagging and features sampling only adress high-variance (overfitting) problems. On the other hand, boosting is a meta-algorithm that aims to reduce both bias and variance by training estimators in a sequential way. Each predictor tries to reduce residuals from previous estimators. In my opinion, the two meta-algorithms can't be easily compared that way. They will give you different results depending of data distributions, potential outliers, usecase... Potentially, boosting is more sensitive to outliers than bagging for example. I think trying the 2 methods is your best chance and you don't have to restrict to only one model. 

If less than 20-25% of behavioral data is missing, maybe you could try to impute missing data using one of the following solutions : 

We can now run the backward pass which is about computing the derivative of the cost function with regards to any weight / bias coefficient in the logistic regression / neural network. We can the update coefficients such as : 

Usually you need to generate the ROC curve and choose the threshold within the training data. Then, with the selected threshold, you have the possibility to report accuracy, sensitivity, recall results you reach on your validation set. 

Below is an example of XOR dataset for classification. As you can see, decision trees perform pretty poorly on this dataset. Reason is decision trees splits space into rectangular regions. Therefore they are not pretty good with this kind of distributions. If you really want to use trees in that sort of situation, it is interesting to use so-called rotation trees. Rotation is about performing PCA (principal components analysis) on input features while learning trees. Using it, decision trees can then build non-rectangular regions. 

This is because you have an imbalanced dataset towards class 0. I have taken a look on the logistic regression coefficient you get. On the below chart 1, I have plotted the decision boundary you get with your logistic regression. On the second chart, I have plotted what you have expected. So why this difference and why does your logistic regression yield chart 1? The reason is even if it is a machine learning algorithm, it is not really smart. The logistic regression algorithm wants to minimize its cost fucntion (cross-entropy). Cross-entropy can be defined in a really simple way as the distance between your points and the decision boundary. But, in your training case, you have two class-1 observations for only one class_0 observations. Thus the machine learning algorithm will see that in order to reach the lowest cost function as possible, the best choice is to promote the two class-1 observations than the only one class-0 observation. The cost function value is larger on chart 2 than on chart 1, this is why it has yielded chart 1 result. Majority wins ! To avoid this problem, here are three ideas you can do :