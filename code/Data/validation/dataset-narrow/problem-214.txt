You mentioned keeping the original which is doable if you have an ORDERS_HISTORY or ORDERS_VERSION table. Basically, whenever the status of an order changes, update the main table but keep a snapshot of what it used to look like in the history table. This would let you pull up old versions of the quote/order to quote against. 

Edit: I missed that you consider duplicates to restart after another row shows up. We could likely solve the problem via analytic functions but it would be helpful to have example data in a table. Please put a sample on up SQL Fiddle or similar site. 

Make a prize_money column with the proper currency/float/decimal type and a prize_other column that is text/varchar. Depending on the type of prize, use one or the other. 

This should be doable with a SUM in a recursive query. See: $URL$ That is tough when it can roll all the way up. You would need a lock on all the parents and it may be quicker to lock the whole table. This is a downfall of the design of having an unknown number of parent records affecting the balance. How would this work in practice if I sent only one message? Take a fraction of an SMS from each parent? 

In your example, maybe the system isn't supposed to allow certain types of records to be deleted. If the application tries that, it logs an error. 

The second part you could the ARRAY function to transform rows for all values that aren't equal to the one above into one long array, though that is honestly a little tough to do in one SQL statement. Does it really have to be one long array like that? What are you going to do with it? There is an example of ARRAY() here: $URL$ 

Do you have to follow this architecture? What if instead of unlimited layers of resellers, you had buckets that resellers were assigned to and could transfer to/from? 

Will you ever need to show a user's likes of different types all at once? If so, I would keep it in one table. If you run into scale issues, you can always start partitioning by the like type column and basically have separate tables on the back end. 

I wouldn't say you did anything wrong. Humorously, I just suggested to someone to do the same thing you did for a similar problem: How should I model a database structure to retain calculation results? You might want to explore a semi-structured table format where instead of your "data" table having so many rows, you add a column to the "main" table that stores all the answers in a JSON object that gets parsed at query time. This is relatively new in MySQL but PostgreSQL and SQL Server are a little farther along. In other words, after a survey on your favorite fruit and favorite color, your main table would have an answer column that looked like this: 

With a super-user, do a one-time GRANT to both logins and add to the end. It sounds like the users have the ability to query their own schemas but are not allowed to grant it to others. $URL$ 

Entered - data being typed in, just started Quoted - quote provided to customer Ordered - customer has accepted the quote and the quote converted to an order 

How about combine them? Keep the idea of an order table as the base, and add a status field with values: 

You may be able to code something like that into a stored procedure but it is not straightforward. MySQL does not have a PIVOT operator that dynamically generates columns, so the closest you could get is something like this Q&A below. Basically, you create CASE statements that extract each part of your string. $URL$ 

and you would parse it at query time into two columns called f1 and f2. There is some obvious overhead associated with parsing the JSON, but on a VM running on my laptop it was about .5 secs for 100K rows. 

If you use partitioning on your table, queries can be sub-second. Edit: There is an open retail data model that handles multiple stores that you can see here: $URL$ 

If those orphaned records are a problem, you would normally force a cascading delete on the FK. If you're still using MyISAM (InnoDB has been default for awhile already), then your garbage collection idea is workable. Benchmark how long it takes on a test system first and run it off hours if possible. 

Use the random function to generate a number. "Number" your locations with slots equal to the number of positions they need. For example, if location A needs 15 and location B 16 then request a random number from 1 to 31 and: 

Use something like rsync with compression to get your endpoint files up to a centralized, reliable server. Once you believe the files are good, do a bulk load from that centralized location to the DB. You could then use the COPY command, a flat file foreign table wrapper, or get fancier with an ETL tool. 

I don't see why the problems you gave for the original design are insurmountable. If you need to add a column to an existing table, you analyze the impact that will have. Either it is so important you should populate it retroactively, or it isn't important and you can leave it alone. If you are looking for a compromise solution, you could have a "flexible" schema stored in each row. For example, PostgreSQL and SQL Server support a JSON column type. Based on the fields in your form, you would stuff that JSON with the relevant data for that record at that time. Using this solution, each individual row is still somewhat legible to a human, but you have the overhead of parsing the JSON to find out what data elements are in it. 

CREATE TABLE table_temp as select * from table where ID IN (records to keep) Alter the current table to be table_old Alter table_temp to be table Drop table_old 

TRY an insert into table CATCH duplicate PK error update row with that PK with same data that was going to be inserted 

If you put a primary key on an InnoDB table, it acts like an index organized table in SQL Server. In other words, the index and actual data gets stored together and generally in the order of the index. It would be like creating a phonebook in alphabetical order by last name, and then later deciding you wanted to "index" it by first name - you'd have to resort the whole thing. This has a lot more detail: $URL$ 

If you want to go really cheap, Microsoft's Tableau competitor (Power BI) has basic ETL functionality built into it, can source directly from flat files, can call R scripts for clean-up before presenting data, and can store a limited amount of data internally. 

a trigger function that basically just does lower() on the new column value. a trigger on that table on INSERT or UPDATE that runs the trigger function.