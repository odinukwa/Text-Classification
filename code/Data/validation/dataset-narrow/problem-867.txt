Lack of Manageability: Extranet or Extra Headache? The Extranet computers are on what is more or less an air-gapped DMZ, they are not managed with Active Directory and get their file, printer and WSUS services from a standalone rackmount server. This limits the manageability of their computers. Restriction #2 requires us to go to some contortions to perform backups and general administrative work. This was all well and good when that division had IT staff but now they do not (YAY! Budget Cuts!). My supervisor and I agree the best way to move forward would be to move them over to our existing platform/s wherever possible so we are only maintaining one system and not two or three. There is an additional Disaster Recovery and Business Continuity concern. The standing plan is essentially to take a LTO tape containing backups and go somewhere and then recover the data and away we go. My supervisor and I both agree there is some detail that is missing from this plan before it is workable. It would be nice to address this concern along with the file services at the same time. Last but not least... financial stuff is time sensitive and important. As in millions of dollars worth of important. Standardization, reliability and security of their computers and the Extranet LAN is a requirement, even more so now that we do not have IT staff that can be on-site during the initial hours of their shift who could immediately respond to an issue. The technical requirements of our Extranet users is pretty mundane: Windows 7 workstations, file, print and update services, internet access and a few 3rd party applications provided by our financial partner. 

The preferred way to do this is via GPO. For Example: Computer Configuration -> Administrative Templates -> Windows Components -> Internet Explorer -> Internet Control Panel - Security Page, right-click Intranet Sites: and select Enable. 

You should push back against this decision. There is really no excuse for purchasing a new application that is tied to a depreciated product that no longer receives security updates. You're brand new application already comes with substantial technical debt instead of eliminating existing debt. Please do yourself and your organization a favor and lobby hard for another application or push the company for an update of the application that works with Java 7. If you must use this application you can find Java 6 SE in the Java Archive. Be FOREWARNED, Oracle will not provide any updates for Java 6 unless you pay for extended support. If your organization goes forward with this purchase, make sure you include the cost of Oracle's extended support in the capital item for the application as it is a dependency that needs security patching and support along with the application it supports. 

It's difficult to comment on what ports are open on a "typical" installation, because due to the differences between distributions and myriad of situations Linux can be used in there's really no golden standard here other than run only as many programs as you absolutely have too. I suggest you start with Fedora Core 14's Security document to help make that determination yourself. 

Is this possible? Probably. Is it a good idea? Nope. It looks like kind of like you're trying to reinvent the wheel. Apache supports a pretty wide variety of authentication backends and since it looks like your organization already has an LDAP implementation in the form of Active Directory I would recommend you take a look at mod_authnz_ldap. In the long run, I'm willing to bet the time it takes to implement, maintain and secure your text file based system will be significantly more than the time it takes to work with your Directory Services team. 

Can anyone shed some light on (A) why this broke, I don't think you can just execute modprobe from . You need likely need to use the directive: From the interfaces(5) manpage: 

If your goal is to build a Collection based on specific usernames, I think the easiest and maintainable way to do this is add those users to a security group and create a User Collection. For Example: We have a User Collection for a specific group of contractors. 

This will immediately return all Error events from the System log that occurred this day. Want to look two days back? 

I used wbemtest to connect to the namespace on our SCCM server. From there I used the [Enum_Instances...] button and search for as the superclass. I deleted the entry for the site code. I then did the same Enum_Instances for the as the superclass and deleted the old site code from that namespace. Re-running the Automatic Deployment Rules and reviewing the showed the successful downloading of each Windows Update. 

Well there's your problem! It is easy enough to distribute certificates using GPOs. Why is this not an option in your case? Baring that, do your own star certificate (that is signed by a Root CA), you could sign your RADIUS server's certificate with? 

You will need to reapply your pf rules using but that's about all you have to do to account for aliased IP addresses assuming your macros and rules are correct. Additionally, you can ignore all aliases on an interface in pf by listing it . 

You need to distribute your RADIUS server's certificate (if it was self-signed) or the certificate of the Certificate Authority that signed it to your clients. Right now you are telling your clients (or supplicants in 802.1X-ese) to verify the the trust path of your RADIUS server's certificate. I don't know how you generated your public and private key-pair for your RADIUS server but generally speaking it will either be self-signed or signed by a certificate authority. In turn the signing certificate authority's public key will be distributed to clients, either through GPOs, Active Directory Certificate Services or it was included by Microsoft in the Trusted Root Certification Authority repository. 

So what am I doing wrong here? Theoretically by setting the and values for WAN2 so high, that unless a packet is already part of an established flow started by inbound traffic on WAN1, nine times out of ten it should be sent out WAN2. In fact, this is the behavior we want out of the OptiQroute; if a packet's not already part of established inbound flow, send it out WAN2. WAN1 should essentially be used just for serving our /24 public network segment. How can I configure the OptiQroute to get this behavior? 

Can someone explain how NetApp Snapshots can be considered backups? I'm looking for Good Subjective answers so please support your position with facts, references and experience. If my understanding the underlying technology is incorrect, please explain where and why that changes my conclusion. If your shop relies on NetApp Snapshots as backups, please include enough contextual information so that people can get a sense of what kind of recovery policy you have to meet. 

What about a machine that has the same Maintenance Windows that is working as expected (i.e., the Maintenance Window is starting at 20:00): 

Or you can go the other way. If you're wondering what package required the package you can use this handy little perl script to return a list of reverse dependencies that are also installed. 

This typically caused by the Certificate Authority for your domain's Active Directory Certificate Services being unavailable. Try looking into why your Domain Controller cannot participate in auto-enrollment. 

And sure enough if I look at the WMI namespace the Software Updates Patch Downloader is connecting to, it doesn't look quite right: 

First off. Sometimes when people talk about libraries they are talking about .deb packages that provide libraries to other packages. We'll deal with that case first. The other context you hear the term library used in is the traditional shared object sense. We'll deal with that second. will return a list of packages that are dependencies for . Packages are not necessarily congruous with libraries (i.e., libraries in the sense of linkable files), but in Debian and Ubuntu libraries are generally packaged as . If you do a you can find which package containing which libraries is installed. 

I have a fresh Debian 6.0.0 (Squeeze) server running on Hyper-V and I'm trying to get the synthetic drivers loaded into the kernel (2.6.32-5-amd64). The research I've done says that the Linux Integration Drivers should have been mereged into the .32 and greater kernel tree but I can't seem to find them in the normal /lib/modules/2.6.32.5-amd64/kernel/ directory. A modprobe also fails to locate and load the modules into the kernelspace. I'm looking for the following kernel modules: