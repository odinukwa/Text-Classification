Fully Homomorphic Encryption without Modulus Switching from Classical GapSVP Fully Homomorphic Encryption with Polylog Overhead Somewhat Practical Fully Homomorphic Encryption 

The state of the art here is: We can decide primality in polynomial time, but the fastest, general-purpose algorithm to $\underline{\rm find}$ the factors of an n-bit composite integer takes time $\approx 2^{n^{1/3}\log^{2/3}n}$. More to your question, a primality test is the same thing as a compositeness test. Therefore, we can easily implement the 'primality decision oracle' required by any (supposed) Cook reduction from factoring to itself. The reason, then, that factoring is commonly presented as an example of a problem that is "unlikely to be downward self-reducible" is because of the possibility that such a reduction identifies prime factors of the original integer. This can be phrased in many ways, but as Joshua points out in comments above, a good example is a reduction that takes $(n, k)$ (is there a factor of $n$ that is at most $k$?) to $(m, k')$ where $m|n$. A reduction of this particular type would have the consequence of breaking the plethora of cryptosystems based on the hardness of factoring (without the need for a quantum computer), e.g. RSA. 

$\underline{\bf Background}$ In 2005, Regev [1] introduced the Learning with Errors (LWE) problem, a generalization of the Learning Parity with Error problem. The assumption of this problem's hardness for certain parameter choices now underlies the security proofs for a host of post-quantum cryptosystems in the field of lattice-based cryptography. The "canonical" versions of LWE are described below. Preliminaries: Let $\mathbb{T} = \mathbb{R}/\mathbb{Z}$ be the additive group of reals modulo 1, i.e. taking values in $[0, 1)$. For positive integers $n$ and $2 \le q \le poly(n)$, a "secret" vector ${\bf s} \in \mathbb{Z}_q^n$, a probability distribution $\phi$ on $\mathbb{R}$, let $A_{{\bf s}, \phi}$ be the distribution on $\mathbb{Z}_q^n \times \mathbb{T}$ obtained by choosing ${\bf a} \in \mathbb{Z}_q^n$ uniformly at random, drawing an error term $x \leftarrow \phi$, and outputting $({\bf a}, b' = \langle{\bf a}, s\rangle/q + x) \in \mathbb{Z}_q^n \times \mathbb{T}$. Let $A_{{\bf s}, \overline{\phi}}$ be the "discretization" of $A_{{\bf s}, \phi}$. That is, we first draw a sample $({\bf a}, b')$ from $A_{{\bf s}, \phi}$ and then output $({\bf a}, b) = ({\bf a}, \lfloor b'\cdot q\rceil) \in \mathbb{Z}_q^n \times \mathbb{Z}_q$. Here $\lfloor\circ\rceil$ denotes rounding $\circ$ to the nearest integral value, so we can view $({\bf a}, b)$ as $({\bf a}, b= \langle {\bf a}, {\bf s} \rangle + \lfloor q\cdot x\rceil)$. In the canonical setting, we take the error distribution $\phi$ to be a Gaussian. For any $\alpha > 0$, the density function of a 1-dimensional Gaussian probability distribution over $\mathbb{R}$ is given by $D_{\alpha}(x)=e^{-\pi(x/\alpha)^2}/\alpha$. We write $A_{{\bf s}, \alpha}$ as shorthand for the discretization of $A_{{\bf s}, D_\alpha}$ LWE Definition: In the search version $LWE_{n, q, \alpha}$ we are given $N = poly(n)$ samples from $A_{{\bf s}, \alpha}$, which we can view as "noisy" linear equations (Note: ${\bf a}_i, {\bf s} \in \mathbb{Z}_q^n, b_i \in \mathbb{Z}_q$): $$\langle{\bf a}_1, {\bf s}\rangle \approx_\chi b_1\mod q$$ $$\vdots$$ $$\langle{\bf a}_N, {\bf s}\rangle \approx_\chi b_N\mod q$$ where the error in each equation is independently drawn from a (centered) discrete Gaussian of width $\alpha q$. Our goal is to recover ${\bf s}$. (Observe that, with no error, we can solve this with Gaussian elimination, but in the presence of this error, Gaussian elimination fails dramatically.) In the decision version $DLWE_{n, q, \alpha}$, we are given access to an oracle $\mathcal{O}_{\bf s}$ that returns samples $({\bf a}, b)$ when queried. We are promised that the samples either all come from $A_{{\bf s}, \alpha}$ or from the uniform distribution $U(\mathbb{Z}_q^n)\times U(\mathbb{Z}_q)$. Our goal is to distinguish which is the case. Both problems are believed to be $hard$ when $\alpha q > 2\sqrt n$. Connection to Complexity Theory: It is known (see [1], [2] for details) that LWE corresponds to solving a Bounded Distance Decoding (BDD) problem on the dual lattice of a GapSVP instance. A polynomial time algorithm for LWE would imply a polynomial time algorithm to approximate certain lattice problems such as SIVP and SVP within $\tilde O(n/\alpha)$ where $1/\alpha$ is a small polynomial factor (say, $n^2$). Current Algorithmic Limits When $\alpha q \le n^\epsilon$ for $\epsilon$ strictly less than 1/2, Arora and Ge [3] give a subexponential-time algorithm for LWE. The idea is that, from well-known properties of the Gaussian, drawing error terms this small fits into a "structured noise" setting except with exponentially low probability. Intuitively in this setting, every time we would have received 1 sample, we receive a block of $m$ samples with a promise that no more than some constant fraction contain error. They use this observation to "linearize" the problem, and enumerate over the error space. $\underline{\bf Question}$ Suppose we are, instead, given access to an oracle $\mathcal{O}_{\bf s}^+$. When queried, $\mathcal{O}_{\bf s}^+$ first queries $\mathcal{O}_{\bf s}$ to obtain a sample $({\bf a}, b)$. If $({\bf a}, b)$ was drawn from $A_{{\bf s}, \alpha}$, then $\mathcal{O}_{\bf s}^+$ returns a sample $({\bf a}, b, d) \in \mathbb{Z}_q^n \times \mathbb{Z}_q \times \mathbb{Z}_2$ where $d$ represents the "direction" (or $\pm$-valued "sign") of the error term. If $({\bf a}, b)$ was drawn at random, then $\mathcal{O}_{\bf s}^+$ returns $({\bf a}, b, d) \leftarrow U(\mathbb{Z}_q^n)\times U(\mathbb{Z}_q)\times U(\mathbb{Z}_2)$. (Alternatively, we could consider the case when the bit $d$ is chosen adversarially when $b$ is drawn uniformly at random.) Let $n, q, \alpha$ be as before, except that now $\alpha q > c\sqrt n$ for a sufficiently large constant $c$, say. (This is to ensure that the absolute error in each equation remains unaffected.) Define the Learning with Signed Error (LWSE) problems $LWSE_{n, q, \alpha}$ and $DLWSE_{n, q, \alpha}$ as before, except that now we have the additional bit of advice for each error term's sign. 

Some background: I'm interested in finding "lesser-known" lower bounds (or hardness results) for the Learning with Errors (LWE) problem, and generalizations thereof like Learning with Errors over Rings. For specific definitions, etc., here is a nice survey by Regev: $URL$ The standard type of (R)LWE-style assumption is via (perhaps, quantum) reduction to the Shortest Vector Problem on (perhaps, ideal) lattices. The usual formulation of SVP is known to be NP-hard, and it's BELIEVED to be hard to approximate to small polynomial factors. (Related: It's hard to approximate CVP to within /almost-polynomial/ factors: $URL$ ) I've also heard it mentioned that (in terms of quantum algorithms) approximating certain lattice problems (like SVP) to small polynomial approximation factors is related to the non-Abelian hidden subgroup problem (which is believed to be hard for its own reasons), though I've never seen an explicit, formal source for this. I'm more interested, however, in hardness results (of any type) that come as a result of the Noisy Parity problem from Learning Theory. These could be complexity class hardness results, concrete algorithmic lower bounds, sample complexity bounds, or even proof size lower bounds (e.g. Resolution). It is known (perhaps, obvious) that LWE can be viewed as a generalization of the Noisy Parity/Learning Parity with Noise (LPN) problem, which (from Googling) appears to have been used in hardness reductions in areas like coding theory and PAC learning. From looking around myself, I've only found (mildly subexponential) UPPER BOUNDS on the LPN problem, e.g. $URL$ Question: I know LPN is BELIEVED HARD in the learning community. My question is: Why? Is it because everyone tried really hard, but no one's found a good algorithm yet? Are there known lower bounds of the italicized variety above (or others I left out)? If the answer is very clear-cut, a succinct summary of what's known and/or references to surveys/lecture notes would be great. If much is unknown, the more "state-of-the-art" papers, the better. :) (Thanks ahead of time!) 

Factor $q(x)$ as $q(x)=\prod_{i=0}^5 (x_i-r'_i)$. Store this as a list $L$ of distinct roots $r'_j$ and their respective multiplicities $m'_j$. While $L$ is not empty, remove the next root/multiplicity from $L$ and any like terms in $T$. Read off $p(x)\bmod{q(x)}$ from the modified table $T$ and output. 

A common claim in lattice-based cryptography is that cryptosystems based on the Learning with Errors ($\mathsf{LWE}$) problem are hard to break (for a per-system definition of "break") for quantum attackers. For instance, the standard paper on $\mathsf{LWE}$, Regev2005, makes multiple references to the security of $\mathsf{LWE}$-type systems based on the quantum hardness of solving, say, $\mathsf{GapSVP}_\alpha$ for polynomial approximation ratios $\alpha$. But, as far as I see, there is no mention here as to why a critic might be convinced that such quantum hardness holds. Across the host of lattice-based cryptography, the reference is generally made back to Regev's first paper, with no more explanation. And this association is quite important to the broader theory community, because likely tens of millions (or more) dollars are handed out in cryptography grant money based (in part) on this high-level claim. Searching the CSTheory site also turns up no answer to this question. Perhaps there is a simple place to look up this information online.. in which case, consider this question the "CSTheory catalogue" of where to find this information! 

(In 2-space,) a Delaunay triangulation is a planar graph. All planar graphs have average degree at most 6. So, many (all?) operations that depend on vertex degree of a Delaunay triangulation will run in $O(1)$ expected time. 

A few more details for intuition about what multilinear maps are, if they're new to you: BGN uses groups $(\mathbb{G}, \mathbb{G}_T)$ and a map $e: \mathbb{G}\times\mathbb{G}\rightarrow\mathbb{G}_T$. IIRC, the GGH multilinear map with, say, $\kappa = 2$ can be seen as reducing (in the simplest case) to groups $(\mathbb{G}_0, \mathbb{G}_1, \mathbb{G}_2)$ and a set of bilinear maps, written generally as $e = \{e_i\}_{i\in [0,...,\kappa-1]}$ where for all $i$, $e_i : \mathbb{G}_i\times\mathbb{G}_i\rightarrow\mathbb{G}_{i+1}$. 

No to the first question. Assuming that $\mathsf{EXP}$ - $\mathsf{PSPACE}$ is not empty, since $\mathsf{PSPACE} \subseteq \mathsf{EXP}$, all languages in $\mathsf{EXP}$ - $\mathsf{PSPACE}$ are $\mathsf{PSPACE}$-hard. 

Another Thought: In case the word "efficiently" in the title was intended to be interpreted as "not FHE," here's an independent observation of mine that might be useful for situations like you ran into just now -- i.e. "I think I need more multiplications than the bilinear map in BGN allows, but I don't want to take the huge hit of FHE..." If you implement a BGN-like scheme, but substitute a multilinear map for the bilinear map.. See: Candidate Multilinear Maps from Ideal Lattices ..then you should be allowed up to $\kappa$ multiplications, for multilinearity parameter $\kappa$. The complexity of the GGH multilinear map depends on $\kappa$, but so long as $\kappa=O(1)$, I have a feeling there wouldn't be much difference between the resulting scheme and the BGN scheme in terms of concrete efficiency. (In fact, it's an interesting question on its own!) In any case, suppose you ran into a situation where you needed.... THREE multiplications per plaintext... Think about using a multilinear map. 

I'm not an expert in graph theory, but I'll give it a shot (so please read this with your critical glasses on). IIRC, a strongly connected tournament is a set of vertices such that for any pair of vertices $p$ and $q$, there is a path from $p$ to $q$. Thus to make a strongly connected tournament a non-strongly connected tournament (i.e. a decisive tournament), you need to cut the graph (i.e., remove a subset of edges s.t. you remove all paths from at least one $p$ to one $q$). Obviously there are many ways to cut the graph, but you want to choose the one that corresponds to the minimum number of vertices removed. Let all edges have capacity $1$. Consider any two vertices $p_0$ and $q_0$, you can separate all paths between them by finding the max flow (which corresponds to min cut). Then, choose the set of vertices adjacent to the cut that is minimal (there are $O(|V|)$ of them); this is guaranteed to remove the edges of the min cut for $p_0$ and $q_0$. For $p_0$ and $q_0$, this is the $V1$ you were looking for. Now, loop over all pairs of vertices (there are $|V|^2$ many of them) proceeding as above, to produce a set of candidate-$V1$ sets. Out of the $|V|^2$ candidate-$V1$ sets, choose the smallest, and output. Naive runtime: $O(|V|^2 \cdot |V| \cdot MF)$ where $MF$ is the runtime of any max-flow algorithm, e.g. Ford-Fulkerson. Again, I don't spend a lot of time thinking about problems like this, so it would be very good for someone to try to poke holes in it. 

I'm especially interested in the combination of values $\ell = k$; e.g., the predicate Not-all-equal($x_1, \dots, x_k$) for $x_1 \dots, x_k \in GF(k)$. 

Sylvain's already mentioned as much, but I think it's worth emphasizing how important hard work and persistence are. Also, I can add: 

I am not an expert in lattice problems, but I do know the exact-case problem, Shortest Vector Problem (SVP), is NP-hard. In this paper, Schnorr appears to reduce integer factorization to some form of the approximation version of Closest Vector Problem ($\gamma$-CVP), where CVP is a generalization of SVP. However, I do not believe there are known polynomial time algorithms for this. Some known facts about $\gamma$-CVP: Arora, et al (PDF), show that approximating the closest vector within any constant is NP-hard. Also, they show that, for $\epsilon > 0$, if you can approximate the closest vector within a factor of $2^{\log^{\frac{1}{2} - \epsilon}{n}}$ in polynomial time, then any problem in NP can be solved in quasi-polynomial time. Dinur, et al (ACM Citation), later strengthened the inapproximability result to: For $\epsilon > 0$, approximately finding the closest vector within a factor $n^{\frac{\epsilon}{\log\log{n}}}$ is NP-hard. Although I'm unfamiliar with Schnorr's work, what we know of lattice problems would lead me to believe that this is not intended to lead to a polynomial-time algorithm directly. Rather, Schnorr spends some deal of time talking about actual implementations (e.g. running this program on such-and-such computer takes approximately so many weeks/months/years/eons). P.S. As Suresh points out, it appears to be an effort to get "quick enough" or "quicker" run times for integer factorization, despite the complexity. P.P.S. And if I can make a further conjecture: Given that Schnorr's paper pre-dates the work on hardness of approximating lattice problems, it's likely that there was some original hope that it might have led to a polynomial-time algorithm for integer factorization. In light of Arora et al and Dinur et al, however, it's clear that there's not a solution (or at least, a straightforward solution) along that route, however. 

There is a series of papers on steganography and covert computation (beginning here) that fundamentally require error-correcting codes. They model failed oracle calls to draw from an arbitrary distribution as noise in a channel. 

Of course, these are all cases where the decision version I've mentioned isn't very interesting (because it's trivially the case). One problem that's not quite as trivial: 

Then we say the width of $(T, (B_t)_{t \in V(T)}, (\gamma_t)_{t \in V(T)})$ is $\max${weight$(\gamma_t), t \in V(T)$}. Finally, the fractional hypertree width of $H$, fhw($H$), is the minimum of the widths over all possible fractional hypertree decompositions of $H$. Question As stated above, if the fractional hypertree width of the underlying graph of a CSP is bounded by a constant, then there is a polynomial time algorithm to solve the CSP. However, it was left as an open problem at the end of the linked paper whether there were any polynomial-time solvable families of CSP instances having unbounded hypertree width. (I should also point out, this question is completely resolved in the case of bounded vs. unbounded treewidth (ACM citation) under the assumption that $FPT \ne W[1]$.) Since there's been some time since the first-linked paper, plus I'm relatively unaware of the general state of this sub-field, my question is: