To use an example of managing environment variables as Key Value pairs in a tool like consul, if you are storing configuration files in git then tools like git2consul with handle getting that configuration into the environment when it's updated. If you have an app that is expecting that there will be config available as a configuration file then you can avoid shipping multiple copies of the configuration file with the app by building a deploy process with something like consul-template which has the capability to turn your consul values back into a file. 

Your probably looking more at the calibre of your platform engineers. You don't want to make developers jobs hardware nor do you want your software engineers to need to handle more complexity, it's simply contrary to your goal. Instead it sounds like you are looking for a platform engineer who is able to wrangle the relevant technologies together so that they become transparent to the user's (devs) as the devs becomes more comfortable using them they will be able to stretch out and use some of the more unique features, but until then, providing some abstraction from the underlying complexity is probably a good idea. 

Having been working with ansible vault quite a bit recently (specifically regarding what the encrypt and how to encrypt those things without making the code unreadable) I've found very little incentive to change my habits in 2.3. If I'm going to have a series of encrypted variables, or files, keep them separate (I preffix them all with vault_) and just leave it at that. A few times I have used the !vault feature but am left distinctly unamazed as it seems easier to just be very explicit about what is and is not coming from a secrets file. That way no one editing my plays makes assumptions about what is and isn't sensitive data. 

recommendation: split your infrastructure into as small a unit as is logical. If you can only afford to buy a single large bit of kit then make sure you split it into logically separate VMs or containers so that if you need to pop a bit of your infrastructure onto another server, or you need to scale stuff at different rates you can easily expand out but investing in a smaller upgrade. Obviously the realities of software licencing, especially for windows OS's might make this cost prohibitive but, while I haven't caught up on windows licensing models in a while, I'm sure there are development centric licensing models that can suit your specific needs. 

When you say that the data is mounted into a docker container, would it not be more correct to say that the "database" is mounted into the docker container? If your are persisting your data outside the container then you are doing the "correct" thing of not putting your database in a container. Sure, go to town putting a DBMS in a container a letting it manage data that you store outside, personally I think that's just good design because it keeps a clean separation between logic and data. But once you put your data into a container them you're potentially playing with fire. Although container storage drivers have come a long way, I'm personally not yet willing to dive in and leave my data tangled up in a container. 

I think you would be hard pressed to get a straight definite definition of a Platform Engineer. Where I'm currently positioned everyone on the team starts out with their own special role but very quickly the end-game seems to be to feel comfortable enough to be called, or to call ones self, a platform engineer. When I was first told that officially a platform engineer the brief explanation of what that meant, for me at least, was that I was going to need to do a whole lot more defending of my platform. I wasn't quite so focused on helping our developers directly (although that's still the goal obviously) but as the platform owners our jobs was mostly to make sure that our developers where being good citizens by making sure we designed the platform so that we could show them what they did wrong if something happened and we could give them all the information they needed to fix their code. The obvious implication of this is that our platform is perfect, which it obviously isn't. But at least we can support the developers in a way that gets them working on a solution on their end while we're fixing up our side of the agreement. 

It depends on what you're trying to avoid. If you are trying to avoid any service interuption of something which is a genuinely critical service (I'm thinking in terms of "people will die if my API call is not appropriately served") the you need to just budget for the huge inefficiencies that come from vastly over provisioning dedicated resources. And yes they have to be dedicated, none of this allowing for traffic spikes stuff, multiple services spiking would thus cause an outage. In the far more likely scenario that you're service going down would be inconvenient you can tackle the problem both from the client and server sides. Although it's worth noting that it's logically impossible to actually solve the problem of to much traffic because without processing the traffic (which consumes resources) you can't know if it's a retry, if it's a retry for a request that was successful but incorrectly handled by the client, if it's a DDOS, etc. But you can mitigate impact. In the client code write sensible retry logic which has an upper limit and a mechanism for gracefully failing. That way you don't stick your users in an infinite loop of failing requests and you just give them an error telling them to try whatever they just did in little while. For your server side infrastructure the simplest solution is to throttle. Hard limits on requests, especially if you can try and spread them logically based on your specific use case (ie. If you have a centralised service make some hard decisions, do you want to start blocking geographically distant requests which might be resulting in threads hanging server side? Or do you want to distribute your inevitable yet minor outage evenly? etc) It basically boils down to the fact that returning a 503 intentionally from a gateway is a hell of a lot cheaper than letting the request go through and sending a 504 anyway. Basically force clients to behave based on what you can currently provide and provide the correct responses so that clients can react appropriately. 

I deploy may application into an environment by going through X change control process and doing Y reviews with Z tool, whatever. I deploy my environment configuration into an environment by going through A change control process and doing B reviews with C tool, same process, different outcome. 

Who said that properties files and environment variables where mutually exclusive? There is a distinction to be made between "where do I store my app configuration?" And "where does my app source it's configuration? " The most likely outcome is that everyone probably should just keep doing what they are doing with configuration files as a storage mechanism (think long term, persistent state for as long as the environment exists). However, rather than dropping that configuration file into the application context and letting it run the application should be able to just expect those variables to already be available in the environment when it starts up. This means you need to have two deployment work flows - 

A single host going down shouldn't impact your entire stack. Trust me this will hurt you badly as soon as you need downtime for an upgrade. The cost to scale any individual component increases drastically if you want to try and scale on multiple similar servers (which will save you management dollars by reducing complexity) or it will cost you to scale out adhoc hardware as needed because you then need to deal with many different-sized hosts that are a pain to manage and require anyone managing them to know the platform intimately. 

A good rule of thumb is to try and avoid using one bit of logical kit (be it a single VM or a single physical host) for more than one thing. If a physical server is going to be a VM host then that's all it should be. If a VM or a physical host is going to be a source code repository then that's all it should be etc. You can squeeze a bit extra out of your infrastructure if you consider everything to be a VM host and you then partition up those VMs to be single purpose hosts themselves. The rationale behind this is that 

I don't see the relation between packer and ansible/chef/puppet the way you do. Packer might give you a means to declare that you are building an image but if you aren't going to use something like ansible/chef/etc to load up the stuff you want in that image then how do you propose to get the stuff into the image? Usually the default answer to this would be "well we would script it" but she'll scripts are just the glue that can stick anything together, they aren't designed specifically to handle infrastructure and configuration. I've been working for a little while on a project where we have made the concious decision to try and write no shell scripts, at all, even where it's a little less convenient we write everything in a clearly defined, preferably declarative, language that someone else is maintaining standards for (ie. We don't need to make sure that ansible can do the things we need it to do, we let the developers do that, as opposed to scripting everything and needing to write all the detailed actions outselves). If you consider your configuration management tools as just a nice way to declare what is actually in he base image then your off to a pretty good start if you want developers to trust the image their code will be running on. 

I think you have fundamentally misunderstood the relationship between plans, stages and jobs in bamboo. A good rule of thumb is to have a single repository of code, which is linked to a single build plan, which produces a single independent module. You may initially start out creating a single stage with a single job inside each single plan where you just run a single maven command and the unit tests built into your maven build may be perfectly sufficient to exercise your modules completely in this extremely simple use case you probably don't need a tool like bamboo or Jenkins to deal with your builds. However, very quickly your conceptualisation of a "build" gets very complicated. As you mentioned in your question you might quickly amass multiple modules that are interrelated, or you might need to support your code on multiple platforms or in multiple browsers, and if you get all the way to the CD end of the work flow you might even need to deploy binaries into environments as the result of a code change. This is where you would want to use the extra orchestration capabilities of your CI tool. For example, if you are developing a module that you expect to load in a browser then your might have a build pipeline that looks like: Check in Code -> Build/Compile/Unit Test -> Deploy to a CI environment -> Stand up a half dozen browsers and verify that they all still work correctly. In the above work flow you would have a single stage to handle your compilation and unit testing which would spit out a compiled or built artifact. Then you have a stage that does whatever needs doing to make your module useful, ie it might create your final shippable product by combining your module with other modules built from other plans, or it might package some software into an RPM, a DEB and so on, depends what you're actually building. If you need to ship to multiple platforms then this is your first stage where parallelism becomes meaningful. Alternatively, if you are building for a single platform but for multiple consumers you might follow up this build of your component with a bunch of headless browsers kicking off test cases to validate that your changes didn't break anything. These can all happen in parallel on different agents running in different OS's, depends how you have bamboo configured. Then finally, assuming you are shopping something more complex than a single module, you will want to do a bit of integration testing with all the other components that you need to interact with. This step will usually be separated into a build plan of its own, triggered by the successful completion of the plans which are spitting out individual modules. Here you get a chance to run all the same tests but with your latest version running along side everyone else's module so you can be sure that there are no breakages caused by dependencies. So at any one time you only have a single repo triggering a single plan. If you have multiple repositories feeding into the same plan and they are completely unrelated but use the same build pipeline then I'd recommend taking advantage of bamboos plan branches so that you can reuse the same build process but with multiple different code bases (if that fits your use case)