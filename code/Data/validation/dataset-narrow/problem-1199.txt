Let us call a logic "symmetric" where a $-A$ ("not A") assumption means the same as proving $A$ and a proof of $-A$ means the same as an assumption of $A$. Classical logic and linear logic are symmetric in this sense. Intuitionistic logic is not. Girard noticed that natural deduction is asymmetric in exactly this way. That is why it matches up with intuitionistic logic. Proof nets represent an attempt by Girard to invent a symmetric form of natural deduction. The best introduction to these ideas is in Girard's "Proofs and Types". If you work through the natural deduction and sequent calculus system for the $\land\to$ fragment of intuitionistic logic, and closely read the sections 5.3 and 5.4 which establish a homomorphism from sequent calculus to natural deduction, you get an appreciation of what natural deduction is all about. Then Lafont's Appendix introduces proof nets in the same spirit. It is more or less straightforward to extend the homomorphism of sections 5.3-4 to one between linear logic sequent calculus and linear logic proof nets (at least for the multiplicative fragment). One thing that is perhaps needlessly confusing about Girard's treatment is that he dispenses with two-sided sequents and uses one-sided sequents in the interest of economy. For sequent calculus this works more or less fine. But, when the same economy is applied to natural deduction, things look strange. A proof net is therefore a "natural deduction proof" of a disjunction of formulas, without any assumptions. A deduction of type $\Gamma \vdash A$ is turned into a proof net of type $\vdash \Gamma^\perp, A$. If this confuses you then you might want to write down for yourself a two sided sequent calculus and an assumption-conclusion form of proof nets. That might clarify things. 

I think this is tricky territory. The author of any creative work legally owns the copyright for that work unless he/she cedes it to another party. Notwithstanding the legal issues, any researcher would naturally feel that they have ownership of their work. So, publishing it (and I believe putting it on arXiv is a method of publication) without their permission would be unseemly. Putting it on one's own web page would be less confrontational. It is perfectly acceptable, however, to cite a draft paper that sits on somebody's web space. It does not need to be on arXiv to be cited. 

In my view, this is an extremely satisfying explanation of why logical relations are "logical". Lambda calculus is logical and, so, the functions defined using it will be uniform with respect to the base types. They can't "see" the permutations we might do to the values of the base types. Viewed in this way, what Gordon and Plotkin meant by "logical" is essentially the same as what Reynolds calls "parametric". However, the term "logical relation" doesn't appear in the published version of the paper. It is possible that the referees might have objected that the term was confusing and Plotkin might have decided it best to avoid the term. But, Statman went back to the old terminology and the term has come back into popular parlance. 

Closures form an important concept that I try to teach in my undergraduate Programming Languages class. You can find my lecture notes online. The Handout 8 shows big step semantics using environments and closures. The Handout 9 shows my flavour of the SECD machine. (The latter didn't work all that well for teaching purposes because the students got lost in detail and didn't develop good intuitions. So I switched to big-step semantics.) Neither of these is what you are asking for, but perhaps they could form a starting point? 

Normal research papers are written primarily for other researchers, typically people in the same field, who are also at the cutting edge of research themselves. It is not surprising that they are hard to understand for normal professionals. Some top researchers do know that a wide readership is a prize to be sought and develop skills to write in such a way that they can reach it as well as impress their colleagues. But that is hard to do. A typical research author would write whatever he/she can get accepted by the referees. So they are our prime audience, perhaps not even all the conference attendees. But conference talks are generally more accessible and, these days, we put the slides on the web so that everybody can benefit from them. Perhaps there is room for a journal (or journals) that invite papers that are understandable to professionals, even if they are rewrites of earlier research papers. It is worth a thought. 

[One more answer. It is probably uncool to pile up several answers. But, hey, this is a deep issue.] I said that I agreed with Andrej's answer, but it seems that I don't agree entirely. There is a difference. I said that a denotational semantics has to say "the meaning of this notation is that". What I meant is that notations must be assigned meanings, which are some form of conceptual entities, not some other notations. In contrast, Andrej also required, paraphrasing Scott, that the meanings must be "mathematical" objects. I don't believe that the mathematical bit is necessary. For example, it would be perfectly fine, from my point of view, for meanings of notations to be physical processes. After all computer programs put on brakes in your car, fly airplanes, drop bombs, and what not. These are physical processes, not elements in some mathematical space. You can't drop a bomb, see if it kills somebody, and take it back if it doesn't. Computer programs can't do that. But mathematical functions can. (They are called "snapback" operations.) So, it is not at all clear that mathematical functions will make good meanings for computer programs. On the other hand, we really don't know yet how to talk about physical processes abstractly. So, we might indeed use some mathematical description of processes in order to articulate our ideas. But these mathematical descriptions would be just that, "descriptions". They are not meanings. The real meanings would be just the physical processes that we imagine conceptually. In his acceptance speech for the SIGPLAN award (which should be on youtube sometime soon), Hoare said that ACP used an "algebraic approach", CSP used a "denotational approach" and CCS used an "operational approach" to describe processes. Ohad and I were sitting together in the session, we looked at each other and said "that is really interesting". So, there is a lot of conceptual space here that is being explored. I think that a lot of Scott's later work, on neighbourhood systems and information systems etc., was indeed an effort to explain functions as "processes" of some form. Girard's geometry of interaction and the later games semantics are also efforts to explain functions as processes. I would say that developing a solid theory of processes could be the big contribution that Computer Science could make to the 21st century mathematics. I wouldn't accept a belief that mathematics has all the answers and we should attempt to reduce computational phenomena to mathematical concepts in order to understand them. What amazes me is how beautifully information hiding works in stateful computations (imperative programming as well as process calculi), whereas it is clumsy and complicated in mathematical/functional formalisms. Yes, we have relational parametricity, and it allows us to get around the limitations of mathematical formalisms very nicely. But it does not match the simplicity and elegance of imperative programming. So, I don't believe that the mathematical formalisms are the right answer, though I would admit that they are the best answer we have at the moment. But we should keep looking. There is a nice theory of processes out there which will beat traditional mathematics hands down. 

Ownership types and Separation Logic seem to have similar goals, control over ownership and aliasing. Perhaps, I should also add: the ability to write modular specifications. What is known about the relationship between ownership types and Separation Logic? 

If you enlarge the idea of "programs" to include Turing machines, then the original Turing's description of their semantics was as transition systems. So, I think transition systems have always been with us! 

The best books on foundational aspects are Gunter's (which I regard as a graduate text book), and Mitchell's (which is good reference book to have on your bookshelf because it is quite comprehensive). 

The answer to your question is really there in Reynolds's fable (Section 1). Let me try and interpret it for you. In a language or formalism in which types are treated as abstractions, a type variable can stand for any abstract concept whatsoever. We don't assume that types are generated via some syntax of type terms, or some fixed collection of type operators, or that we can test two types for equality etc. In such a language, if a function involves a type variable then the only thing the function can do to values of that type is to shuffle around the values it has been given. It cannot invent new values of that type, because it doesn't "know" what that type is! That is the intuitive idea of parametricity. Then Reynolds thought about how to capture this intuitive idea mathematically, and noticed the following principle. Suppose we instantiate the type variable, say $t$, to two different concrete types, say $A$ and $A'$, in separate instantiations, and keep in our mind some correspondence $R : A \leftrightarrow A'$ between the two concrete types. Then we can imagine that, in one instance, we provide a value $x \in A$ to the function and, in the other instance, a corresponding value $x' \in A'$ (where "corresponding" means that $x$ and $x'$ are related by $R$). Then, since the function knows nothing about the types we are supplying for $t$ or the values of that type, it has to treat $x$ and $x'$ in exactly the same way. So, the results we get from the function should again correspond by the relation $R$ we have kept in our mind, i.e., wherever the element $x$ appears in the result of one instance, the element $x'$ must appear in the other instance. Thus, a parametrically polymorphic function should preserve all possible relational correspondences between possible instantiations of type variables. This idea of preservation of correspondences is not new. Mathematicians have known about it for a long time. In the first instance, they thought that polymorphic functions should preserve isomorphisms between type instantiations. Note that isomorphism means some idea of a one-to-one correspondence. Apparently, isomorphisms were originally called "homomorphisms". Then they realized that what we now call "homomorphisms", i.e., some idea of many-to-one correspondences, would be preserved too. Such preservation goes by the name of natural transformation in category theory. But, if we think about it keenly, we realize that preservation of homomorphisms is utterly unsatisfying. The types $A$ and $A'$ we mentioned are completely arbitrary. If we pick $A$ as $A'$ and $A'$ as $A$, we should get the same property. So, why should "many-to-one correspondence", an asymmetric concept, play a role in formulating a symmetric property? Thus, Reynolds took the big step of generalizing from homomorphisms to logical relations, which are many-to-many correspondences. The full impact of this generalization is not yet fully understood. But the underlying intuition is fairly clear. There is one further subtlety here. Whereas the instantiations of type variables can be arbitrarily varied, constant types should stay fixed. So, when we formulate the relational correspondence for a type expression with both variable types and constant types, we should use the chosen relation $R$ wherever the type variable appears and the identity relation $I_K$ wherever a constant type $K$ appears. For instance, the relation expression for the type $t \times Int \to Int \times t$ would be $R \times I_{Int} \to I_{Int} \times R$. So, if $f$ is a function of this type, it should map a pair $(x,n)$ and a related $(x',n)$ to some pair $(m,x)$ and related $(m,x')$. Note that we are obliged to test the function by putting the same values for constant types in the two cases, and we are guaranteed to get the same values for constant types in the outputs. So, in formulating relational correspondences for type expressions, we should make sure that, by plugging in identity relations (representing the idea that those types are going to be consant), we get back identity relations, i.e., $F(I_{A_1},\ldots,I_{A_n}) = I_{F(A_1,\ldots,A_n)}$. This is the crucial identity extension property. To understand parametricity intuitively, all you need to do is to pick some samplee function types, think of what functions can be expressed of those types, and think about how those functions behave if you plug in different instantiations of type variables and different values of those instantiation types. Let me suggest a few function types to get you started: $t \to t$, $t \to Int$, $Int \to t$, $t \times t \to t \times t$, $(t \to t) \to t$, $(t \to t) \to (t \to t)$. 

Here we see the authors struggling with the two notions of "operational", one the technical notion - behaviour expressed using syntactic manipulations, and the other, the conceptual notion - being low-level and detailed. The credit largely goes to Plotkin and Milner for rehabilitating "operational" semantics, making it as high-level as possible and showing that it could be elegant and insightful. Despite all this, the operational notion of process is still quite different from the denotational notion of process, the latter of which was developed by both de Bakker and Hoare and their teams. And, I think there is a lot that is mysterious and beautiful about the denotational process concept which is still to be understood. 

For a quick starting point, you might read the lecture Handout on "Specification and Verification of Functional Programs" of my Lecture Notes on Principles of Programming Languages. 

Are there any good reasons why PL theorists prefer naturals instead of integers? There are some, but in a text book on programming language semantics, I think there is no technical reason why they need to. I can't think of any place other than dependent type systems, where induction on data is important in PL theory. Other text books by Mike Gordon, David Schmidt, Bob Tennent and John Reynolds don't do it. (And, those books would probably be a lot more suitable for teaching people that care about general-purpose industrial PLs!) So, there, you have the proof that it is not necessary. In fact, I would claim that a good PL theory text book should be parametric in the primitive types of the programming language, and it is misleading to suggest otherwise. 

You say that "the true end goal would be to use the theory to actually build a PL." So, you presumably admit that there are other goals? From my point of view, the No. 1 purpose of theory is to provide understanding, which can be in reasoning about existing programming languages as well as the programs written in them. In my spare time, I maintain a large piece of software, an email client, written ages ago in Lisp. All the PL theory I know such as Hoare logic, Separation Logic, data abstraction, relational parametricity and contextual equivalence etc. does come in handy in daily work. For instance, if I am extending the software with a new feature, I know that it still has to preserve the original functionality, which means that it should behave the same way under all the old contexts even though it is going to do something new in new contexts. If I didn't know anything about contextual equivalence, I probably wouldn't even be able to frame the issue in that way. Coming to your question about pi-calculus, I think pi-calculus is still a bit too new to be finding applications in language design. The wikipedia page on pi-calculus does mention BPML and occam-pi as language designs using pi-calculus. But you might also look at the pages of its predecessor CCS, and other process calculi such as CSP, join calculus and others, which have been used in many programming language designs. You might also look at the "Objects and pi-calculus" section of the Sangiorgi and Walker book to see how pi-calculus relates to existing programming languages.