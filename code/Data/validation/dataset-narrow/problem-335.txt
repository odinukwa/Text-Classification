And all that was only the first cumulative update. Want your query results to be accurate? Get your patching on. 

Yes, any corruption alerts can make it to the end user. To see it yourself, grab one of the corrupt databases from Steve Stedman's Corruption Challenge. Attach the corrupt database to one of your development environments, and run the query that Steve describes for that database. You'll see corruption errors - just as your end users would. Your application (C#, Java, whatever) can be coded to trap incoming errors and display different things to the users, but that's an exercise left for the reader. 

It's reporting CPU usage and where I found it in the DMVs - the ring buffer. The ring buffer isn't CAUSING the high CPU use. To find queries doing high CPU use, use sp_BlitzCache @SortOrder = 'cpu'. For more details, check out this post from Microsoft's Premier Field Engineering team on using ring buffers to track CPU usage inside SQL Server. 

You've written a lot of stuff, but here's your main question: Why are my TPC benchmark numbers not what I expect? To find the answer for that, check your SQL Server's wait stats while your workload is running. They're stored in sys.dm_os_wait_stats, but only as a cumulative number, so I wrote sp_BlitzFirst to do differentials for you during heavy workloads. Run it like this: 

Queries are sorted from longest-running to shortest. You can click on the sql_text or query_plan to see that stuff (note that the latter is only included when you run it with @get_plans = 1). It'll also show you who's blocking who, what machine the queries are coming from, and much more. It's free, and it works with all currently supported versions of SQL Server. It's used by thousands of database professionals all over the world. I've used it myself in production all over the place, and never had a problem with it. sp_BlitzWho Erik Darling wrote this because he's really into query tuning, and he wanted to see more details than sp_WhoIsActive provides. Starting with SQL Server 2012, Microsoft has added a ton of cool DMV info like memory grant utilization, and Adam hasn't been updating sp_WhoIsActive to leverage it. You can get sp_BlitzWho from the public Github repo or from our First Responder Kit. Here's what it looks like: 

No, you don't need to rebuild indexes or update statistics. You can't do any writes to a readable replica at all. 

The Compute Scalars are related to the computed field LineTotal. Script out the table, and you'll see that field defined as: 

There's different things you can do with certificates: you can use them to encrypt endpoint communications (like mirroring or AlwaysOn Availability Groups), or you can use them to encrypt databases with Transparent Data Encryption (TDE). If you're using them to encrypt endpoint communications, just back up the certificates once and keep them in a safe place. I wouldn't expect shops to routinely rotate those certificates. If you're using them with Transparent Data Encryption, back up both the certificate and the key as described in Books Online: $URL$ With TDE, though, you should routinely rotate the keys. Otherwise, when anybody steals the certificate and key, then they can decrypt all of the backups going forward. When you rotate the key, you'll want to immediately back that up again. You can't run backups while the database is re-encrypting with the new key, but back it up again as soon as the re-encryption process is done. 

Typically this is caused when the user's default database is set to a database they don't have permissions in, or it's offline. For example, say I set you up a login in the Sales database, and then I right-click on your login in SSMS and change your default database to master - but you don't have any permissions in Master. When you try to connect, even though you want the Sales database, you'll go into Master by default, and get this error. This can also happen if someone takes away your permissions in Sales and doesn't change your default database to something else. This is why I love setting all users' default databases to TempDB. User databases can come and go, but TempDB will always be there. Plus if somebody accidentally creates an object, at least it's in a throwaway database instead of something like Master or Sales. 

If this results from you passing data INTO SQL Server, log your parameters before you issue the query. If this results from you getting data OUT of SQL Server, remove your cast/convert functions and look at the data. Odds are you're casting/converting a field that isn't guaranteed to be a date. 

My preferred solution here is to write the backups to a file share managed by a separate Windows file server (likely an EC2 VM). This way, if your SQL Server goes down, the backups are instantly already accessible on that EC2 VM (as opposed to doing backups locally on the SQL Server, which are inaccessible when the SQL Server goes down.) Then, on that EC2 VM file server, use an S3 folder sync tool that automatically uploads your local backup folder up to an S3 folder. On the subscriber side (which can be in a different AZ or region), use another EC2 VM file server with the same cloud file sync tool, which then makes the files accessible via UNC path for your subscribing SQL Servers. Use the native log shipping jobs, but don't use the file-copy job (since that's handled by the two EC2 VMs syncing to S3.) The two log shipping jobs will have different targets, and that's fine. 

When we renamed it sp_BlitzFirst and went open source, we made several breaking changes, one of which was changing the history table's DATETIME field to be DATETIMEOFFSET instead: $URL$ I would use a new history table name, and then it'll get automatically created with the right datatype. 

SQL Server Management Studio sends a request to FORNSQPRD02\NAV01, who has the data, and sends it back to SQL Server Management Studio 

Q: What are the high availability challenges involved? Amazon RDS for SQL Server uses database mirroring for high availability. In your own EC2 instances, you can choose whether you'd like a similar level of protection with mirroring or Always On Availability Groups, or if you'd like to save money and get less protection. You didn't ask about patching, but I'd bundle that in here: the burdens of patching will now be on you rather than Amazon. Q: Can I make my migration restores go faster? Right now, Amazon RDS only gives you access to full backups. Your backup/restore migration timeline is: 

We actually ran into a similar issue at StackOverflow and Kyle blogged about it: $URL$ The problem can be a number of things: 

Express Edition doesn't include database mirroring, but you can do something similar with log shipping. Log shipping is a technique, not a feature - it's just a matter of frequently taking backups on your primary server, and then restoring them on the secondary. The normal way of doing this requires SQL Server Agent to run the backup & restore jobs - but again, that's not something Express Edition includes. You'll need some kind of job scheduler. Kathrine Villyard has a post on doing this with PowerShell. When it comes to failing over, the basic concepts are the same as failing over Standard Edition's log shipping. They'll be manual steps on Express Edition - but they're also manual on the big brother, too, so don't feel bad. 

But as you've discovered ANSI_PADDING is also involved - both the incoming & outgoing tables have to have the same setting. There is no way to fix this after the data is loaded. 

Disk queue length isn't a problem, by itself: SQL Server will batch off IO requests. It's a question of whether your storage subsystem can deliver data fast enough to keep up with your queries. While the report runs, gather wait statistics - the list of bottlenecks on the server. My favorite way to grab it is with sp_BlitzFirst in the open source First Responder Kit (disclaimer: I'm one of the authors). To gather wait stats for a period of time, run: 

Moral of the story: never, ever doubt anything you see on BrentOzar.com. Just kidding. But believe what you're seeing right here, and get to work digging into those stats. 

When someone does a BEGIN TRAN, inserts a row, locks their computer, and goes home for the weekend, your log file will grow and grow and grow. That would be a Bad Thingâ„¢. Rather than facing an outage, you're better off letting the log file grow, and using your monitoring tool to get alerts that there's a problem. 

The full text catalog will only reflect the contents of a table's data. Since it sounds like you're not using full text search yet, and you just want to start, you basically have 2 ways to do this: Option 1: add columns with the de-obfuscated data, and use full text search on those. Say you have a column dbo.Customers.SavedText that is obfuscated - you add a dbo.Customers.SavedTextClear column with the original text. You can populate it and keep it up to date with triggers on insert and update. (Triggers usually have a bad reputation - rightfully so, you wanna minimize the work performed in them - but you asked for a solution that didn't require modifying the original queries inserting/updating into the table, and this is what it looks like. When you tie my hands, my artwork isn't quite as good, ha ha ho ho.) Option 2: do full text search in something like Elasticsearch. Build an ETL process that exports the fields you want to search, de-obfuscates them, and puts them in a database that's tuned for full text search, like Elastic. You can read about how Stack Overflow approached this problem, although when you read it, I bet modifying your original queries is probably going to sound like a better solution. The only reason I mention it is that if you really need to scale out, this can be a less expensive approach long term. 

Rather than maintaining two separate code bases, the easier way to do this is by aiming for the lowest common denominator. Try to only use features in the database that are supported in Azure SQL DB. That way you'll have a single code base that works for both, and you'll have much easier upgrades, too. For box-product customers that truly need SQL Server features that aren't available in Azure SQL DB, build a separate set of scripts that are deployed after the database is created. If there are specific features you need in SQL 2012 that aren't available in Azure SQL DB, try posting separate questions about each of those (or list 'em in the question itself) so we can get a better feel for why you really need two separate code bases. 

First, if you're getting corruption issues in TempDB, I'd start by making sure I've got DBCC CHECKDB reporting clean results across my user databases. I'd also restore those backups on another server and run DBCC CHECKDB there just to make sure the problems are only confined to TempDB. Then I'd remove any filter drivers on the servers - antivirus, disk defragmenting, storage mirroring, etc - and try to run multiple load-intensive queries against TempDB to see if I can reproduce the issue. Do simple select-intos, dumping Cartesian joins of your user databases into TempDB. Finally, if I can get an outage window, I'd try SQLIO - not SQLIOSIM - hammering the TempDB drives. SQLIOSIM is interesting if you want to reproduce a vague similarity of a benchmark, but SQLIO is better if you just want to repeatedly punch your storage in the junk. Plus, it's really good about bubbling failures up quickly. You also asked: 

Yes, but not directly. You would have to build dynamic SQL strings, one row for every join set (pair of Docs and Tags rows), and then execute those strings. That's probably not practical. In the FROM query, if you had 1,000 records in Docs and 1,000 records in Tags, the result set would have 1,000,000 rows. I'm assuming you knew that and just dummied up some quick T-SQL - obviously this wouldn't work in production. 

Identity fields match all 4 (SUN-E), while GUID fields only match the SU portions. One could argue that a single GUID is kinda narrow compared to, say, multiple GUIDs chained together or an NVARCHAR(250), but it's got nothin' on the narrowness of an INT. Q: Do data modeling guidelines change in IaaS? Infrastructure-as-a-Service providers like Amazon EC2, Google Compute Engine, and Microsoft Azure VMs are just VMs running on someone else's computer. Data modeling rules do not change here. Q: Do data modeling guidelines change in PaaS? Platform-as-a-Service providers are a little different. Amazon RDS is just SQL Server hosted and managed by someone else, so no data modeling changes required there. Microsoft Azure SQL DB isn't exactly "just SQL Server hosted and managed by someone else" - it has some strengths and weaknesses that differ from the traditional boxed product. However, if you're only building your app for a single database in Azure SQL DB (not scaling out to multiple databases), then the conventional rules apply. Q: When does "the cloud" affect data modeling guidelines? When you want to scale a single application across multiple databases and/or servers. This design pattern is called sharding, splitting a single identical table (or sets of tables) across hosts. To be clear, you can do this same design pattern on-premises - it's just more trendy these days in the cloud because it's so much easier to spin up multiple servers or databases. You noted that your database only uses 70GB per year - that's well below the numbers where I'd even remotely consider sharding. Q: When sharding, what data modeling concerns will I have? First, your application has to know which shard to hit in order to get the relevant data. (You don't want the app hitting the database to discover this data for every query - after all, you'd be right back up against the scalability limits of a single database server.) You could shard by customer location - like one shard for North America, another for Europe, another for Asia. However, your customers may move between locations, or they may have locations spread across contents. You could shard by customer name - like A, B, C, D - but you'll have some shards that are much hotter than others due to load. (Say one of your customers runs a World Cup sale - that server might fall over while the rest are idle.) And of course, customers can change names. You could shard by activity date - like the date of sale - but of course today's shard will be ridiculously hot compared to the other shards. By now, you're starting to see now why your question is so broad, and so difficult to answer. Your best bet to get an actionable, useful answer will be to provide as many specifics as you can. However, since you asked for a design for 70GB of data per year, keep it simple: stick with the SUN-E principles. 

No. I feel really guilty just typing that, but no, sadly. That's the first time I've heard of this use case, and it makes perfect sense. Best to submit a request for it on $URL$ and your grandchildren will be able to do it. ;-) 

Don't go off troubleshooting CPU threads - that's not related. Focus on your primary wait type and things that would cause that wait type. To troubleshoot this further, run sp_WhoIsActive or sp_BlitzFirst (disclaimer: I'm one of the authors of that) - both of which will list the queries that are running currently. Look at the wait info column, find the queries waiting for ASYNC_NETWORK_IO, and look at the apps & servers they're running from. From there, you can try: 

These are big decisions that involve a lot of architecture work. For more detailed specifics, include the above details, and we'll be able to tell you more about how to configure it. Q: Isn't it just a matter of checking the box for Always On? Nope. 

One missing link is the size of the index. If you're talking about an object with less than, say, 1000 pages, then index rebuilds aren't all that critical. Another missing link would be the churn of the index. Typically I see filtered indexes used when they're a very, very small subset of the entire table, and the subset changes fast. Guessing by the name of your filtering field (OfferStatus = 0), it sounds like you're indexing just the rows where you haven't made an offer yet, and then you're going to immediately turn around and make an offer. In situations like that, the data's changing so fast that index rebuilds usually don't make sense. 

You can have rows in sys.stats without a corresponding row in sys.tables. Classic example: an indexed view. Watch this: 

Sadly, sys.dm_hadr_availability_replica states is not a reliable indicator of replica health. Here's the Connect item on one of the bugs we've run into where that DMV stops refreshing - note in the comments that log_send_queue_size in the DMV sys.dm_hadr_database_replica_states shows 0 even when there's log data to be sent. Note that the Connect item is marked as Won't Fix. Sad trombone.