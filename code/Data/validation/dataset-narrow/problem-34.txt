For the GLSL test I'm using the Book of Shaders Editor with the following code (can also be seen interactively here): 

I'm trying to figure out the correct math to rotate and translate a curve displayed in a fragment shader. What I try to accomplish is to define a curve, for example a sine curve, in a local coordinate system, rotate it then translate it. Something like this: 

I'm not sure what language/library you are using, but create a translation matrix by yourself or using a built in fuction. Above dx, dy and dz are the translations in x,y and z. Multiply your positions v, v2 and v3 by the translation matrix. Google tranlsation and rotation matrix and you will understand better! Good luck! 

So as you can see, I create a new xz-coordinates and indices buffer for each VAO. I doubt this is the most effective way to do it. How should I do this? 

If I want one VAO for each of the terrain deffinitions, how would that look if the VBO for xz coordinates and indices are the same for each? What I am doing now looks like this: Setting up the buffers: 

Obviously I'm misunderstanding how this should be done in a fragment shader. How should I correctly define a transformed curve in GLSL fragment shader? 

Your triangle's coordinates: (-0.1f,0.7f) (0.0f,0.8f) (0.1f,0.7f) are defined with the origin at the center of the screen. Multiplying a rotation matrix by a vertex position will rotate around the point (0,0,0). In your case, that is the center of the screen. Possible solution: 

I recently replaced the Lambertian BRDF in my path-tracer with Oren-Nayar, under the assumption that I could adjust it to use the GGX distribution model with appropriate masking/shadowing. PBR suggests this is't viable, though - Oren-Nayar is formulated without any clear $D$/$F$/$G$ parameters, and a note under the Torrance-Sparrow model states that "one of the nice things about the Torrance-Sparrow model is that the derivation doesn't depend on the particular microfacet distribution being used" (implying this isn't the case for Oren-Nayar). If Oren-Nayar is extensible, how would I do that? I suspect I could replace my $\sigma$ values with an NDF taking some $\alpha$ parameter (effectively convolving against the implicit Gaussian in the Oren-Nayar definition), and multiply the evaluated Oren-Nayar function against $G$ to capture masked segments of $dA$, but won't this clash with assumptions made by the derivation? PBR states that the function natively accounts for Gaussian-distributed masking, so applying another function over the top should result in over-darkening... If it isn't extensible, can I adjust Torrance-Sparrow instead? Locking $F(\omega_o)$ to $1$ should remove Fresnel effects, and the specular assumption can be negated by extracting the $d\omega_h = \frac{d\omega_o}{4\cos\theta_o}$ relationship to create the modified BRDF $f(\omega_o, \omega_i) = \frac{D(\omega_h)G(\omega_o,\ \omega_i)d\omega_h}{d\omega_o\cos\theta_o}$ 

The Lambertian BRDF is defined as $R / \pi$, where the division by $\pi$ is there to maintain conservation of energy. The derivation is provided here. The maths makes sense, and I can understand where the BRDF comes from in a mechanical sense. But I'm struggling to understand what it represents; how does division by $\pi$ relate to reflectance within the unit hemisphere? 

In the code above, there are two modes which can be toggled with set to or . First mode attempts to plot using the functions ) & . Second mode sets a color to a fragment based on the distance from the calculated curve coordinates. Result of first mode with set to : 

In my project, for convenience I would like to use many buffers. Many buffers in my case means 50-100 terrain patches represented by buffers with vertex coordinates, normals, indices and maybe color. The magnitude of data would be, let's say 10^4 floats per buffer. Some of this data can be shared between each terrain patch, f.ex. xz-coordinates and indices. During the rendering loop, some terrain patches will be updated. Which means that for certain buffers I call for the whole buffer. My question is; are there any pros/cons, performance wise between these two methods: 1) Controlling my data in many buffers (50-100), thus letting me call on a complete buffer when needed. 2) Controlling my data in fewer (5-20) buffers, with more data in each. But then having to set up a system where I need to call glBufferSubData on smaller portion of a buffer. (Which leads to a more complex design in my case). 

Let's say I have a have 10 different terrains. Each terrain have the same x and z coordinates, but they have different y coordinate, normals and colors. The indices used for element rendering is also the same. The data looks like this: 

So, ideally an app-measured latency figure should include some corrections to at least estimate the above factors. Or accept that you're not really measuring the full motion-to-photon latency, but just the app-query-to-flip latency or some such. 

The image above, from panohelp.com, shows the basic idea. For each point on the image, there are multiple rays arriving at that image point, via every point on the 2D lens surface. Therefore, generating an image like this using Monte Carlo will require picking, for each ray, both a 2D sample point on the image plane and an independent 2D sample point on the lens surface. The user-facing parameters to set will be the lens radius (as a physical radius in scene units), which controls how shallow the focus range is (larger lens = shallower focus range), and the distance at which you want objects to be in focus. To generate eye rays into the scene, you can calculate the position and direction of rays leaving the lens surface; in this model there's no need to explicitly simulate the image plane and the refraction through the lens. Basically, think of the lens as centered at the camera position and oriented to face the camera direction. Based on the image location, construct a ray from the camera position (lens center) into the scene, just as you would in the pinhole model; then find its intersection with the focal plane. That's where all the rays from that image location should converge. Now you can offset the starting point of the ray to a randomly chosen point on the lens, and set its direction to be toward the convergence point. You can generalize this a bit by allowing the focal plane to be something other than a plane, or the lens to be something other than a circular disc, and following the same process. That can produce some interesting if not-quite-physical effects. It's also possible to go beyond this simple model with a more physically realistic simulation of a camera's lens elementsâ€”but that's beyond my expertise. 

I recently implemented Mitchell-Netravali filtering in my path-tracer with 16x (4x4) temporal anti-aliasing. PBR's demonstration image shows it should give significantly better results than the box filter, but my test pictures barely show any improvement at all (and the sharpening in Mitchell-Netravali actually highlights jaggies that would have been blurred out by the box filter). Is this filter designed for a higher sampling rate? My samples are spread over an even sub-pixel grid, and I'm using $B$ and $C$ constants on the line $B + 2C = 1$. I'm also using the implementation recommended by PBR. Function for $f(x, y)$: 

Multiple Importance Sampling (MIS) is a technique used by Veach's VCM technique to balance responses of different surfaces to different types of importance sampling for direct illumination. My understanding is that specular surfaces are best lit without sampling the light at all; instead, you pull a direction from the BRDF and hope that it hits the light source. Diffuse surfaces "spread" light more broadly over their domain, so firing rays towards random points on the area covered by the light produces good results without the wastage produced by sampling over the BRDF. I'm still unsure about a few things, though: 

I'm using ray-marching to render simple shapes and test different mapping functions; rotating the shapes along a path lets me easily see all of each surface without clicking/dragging. Rendering boxes with rotation paths that pass through offset vectors (like or ) makes all the rays intersect almost immediately, even the ones going nowhere near the box. Why? I'm using quaternions for rotation and a classic cuboid distance function for the surface. [main]: 

If the downsampling pass is properly designed, it will effectively perform low-pass filtering as part of the downsampling. There is no need for a separate low-pass filter operation. Essentially, when you downsample, you are performing a filter over the source (high-res) image pixels, but only evaluating it at the locations of the destination (low-res) pixels. The footprint of this filter needs to be approximately the spacing between destination pixels, to avoid missing information by skipping over in-between source pixels. But that means the filter footprint will be several source pixels wide, so it will effectively low-pass the source. For example, let's suppose you downsample an image by exactly 10x on each axis. With a box filter (for example's sake), you would set each destination pixel to the average of a 10x10 box of source pixels. That would wipe out any features smaller than 10px, so it's effectively a low-pass filter. You mention bicubic interpolation; we have to make a distinction between filtering and interpolation here. Interpolation is appropriate for upsampling, not downsampling. Bicubic interpolation works by fitting a bicubic spline patch to a 4x4 neighborhood of pixels, then evaluating the patch at interpolated points. While it may work well enough for downsampling images by a small factor (up to 2x or so), it will fail if you go much further than that. For instance, if downsampling by 10x as in the previous example, you can see that bicubic will miss the majority of the source pixels, and the result may be quite aliased. On the other hand, bicubic filtering is just standard filtering, using a kernel that's a bicubic function (as opposed to a box, triangle, Gaussian, Lanczos, etc. kernel). The Mitchell-Netravali kernel is the classic example of this type. If used for downsampling, the kernel should be sized appropriately for the destination pixel spacing as discussed earlier, and you would sum over all the pixels in the footprint, not just a 4x4 or other fixed-size neighborhood.