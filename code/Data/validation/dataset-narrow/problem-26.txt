Mip selection is pretty well standardized across devices today—with the exception of some of the nitty-gritty details of anisotropic filtering, which is still up to the individual GPU manufacturers to define (and its precise details are generally not publicly documented). A good place to read about mip selection in detail is in the OpenGL spec, section 8.14, "Texture Minification". I'd assume it works the same way in Metal. (Apple could have changed something, considering they make both the hardware and the API...but I doubt they have.) I'll summarize it here. The default mip selection (not using any of the modifiers) uses the screen-space gradients of the texture coordinates to pick the mip levels. Essentially, it tries to pick the mip levels that produce as close as possible to a 1:1 mapping of texels to pixels. For example, if the gradients have a length of 4 texels per pixel, it would pick mip level 2 (which is 1/4th the size of level 0 and therefore gives you 1 mipped texel per pixel). With trilinear filtering, since you usually don't land on an exact 1:1 mapping, it picks the two nearest levels and linearly interpolates between them, so that you have a smooth transition between mip levels as the camera or objects in your scene move around. To be mathematically precise: you take the screen-space gradients of the texture coordinates, scale them by the texture size (to get them in units of "texels per pixel"), take their lengths, take the longer of the two (X and Y) gradients, and calculate its logarithm base 2. This quantity is called $\lambda$ in the OpenGL spec; it's also commonly known as the texture LOD (level of detail). It's simply a continuous version of the integer mip level index. So, the mip levels used are the two integers nearest to $\lambda$, and the fractional part of $\lambda$ is used to blend between them. For example if $\lambda = 2.8$, the GPU will sample mip levels 2 and 3 of the texture, then blend to 80% level 3 and 20% level 2. If anisotropic filtering is turned on, then instead of simply using the longer of the two gradients, you use the ratio of them to set the number of aniso samples. For example, if the X gradient is 4 times longer than the Y gradient, you'd use 4 aniso samples, with their positions spaced out along the X gradient. Each one would be a trilinear sample, using a $\lambda$ corresponding to 1/4th the length of the X gradient (i.e. two mip levels lower, since $\log_2(1/4) = -2$). Now, as for the modifier options: 

Given that you want to test for intersection against rays with many different starting points and directions, it's worth investigating raytracing-style acceleration structures, such as the bounding volume hierarchy (BVH). In 2D, this would look like a tree of axis-aligned bounding boxes that divide up the space. Each leaf node of this tree would store a reference to a small list of nearby segments of the polyline, as well as a bounding box that contains them. Higher nodes would store a bounding box that contains all their children. There's a ton of material on BVH construction and traversal out there, so I'd recommend doing a few searches and reading up. It's not too difficult to implement a top-down construction method, by starting with the bounding box of the whole polyline and recursive splitting. Then, to use it for acceleration, when you do a ray query you can find which BVH nodes the ray touches and test only the segments in those nodes for intersections. However, all that being said, I'd like to propose a different approach to your original problem of working with contour lines. It seems to me that this is a place where Delaunay triangulation can probably be very helpful. It basically constructs a triangle mesh that connects a set of input vertices and edges. For example, here is a contour map I found on the internet: 

The notation $\delta_{i,j}$ is the Kronecker delta, a notation commonly used in physics. It's defined as: $$\delta_{i,j} \equiv \begin{cases}1,&i=j\\0,&i\neq j\end{cases}$$ So, as you suspected, it's essentially a shorthand for the identity matrix. The notation $\mathbf{S}_i \in \mathrm{R}^s$ means that each $\mathbf{S}_i$ is an $s$-dimensional vector, so the index $j$ labels the components of this vector. The term "indicator" probably refers to the concept of an indicator function, which is a function that labels some set or point with the value 1 while mapping everything else to 0. So, the "$i$-th spring indicator", in this context, is a vector that has a 1 in the $i$-th component and 0 everywhere else. That's exactly the Kronecker delta, i.e. the identity matrix. The way this "indicator vector" gets used in equation 12 in the paper is in the form of an outer product with another vector, $\mathbf{A}_i \in \mathrm{R}^m$. The two vectors are multiplied together in the form $\mathbf{A}_i \mathbf{S}_i^{\mathrm{T}}$, which is an outer product. The result is an $m \times s$ matrix with the contents of $\mathbf{A}_i$ in the $i$-th column, and zeroes everywhere else. 

One good way you can arrange for a circle (or other shape) to be drawn for each vertex in a mesh is to use geometry instancing. This is a GPU feature that allows multiple instances (copies) of a mesh to be drawn at once, with the vertices/indices of the mesh given by one set of buffers, and another vertex buffer that can give additional data per-instance. The vertex shader can be used to combine the data from both buffers in whatever way you choose. To be concrete, in your case you could create a mesh representing a circle of the desired radius, with its vertices specified directly in screen-space coordinates and centered at the origin. Then you'd use instancing to make the GPU render a new copy of the circle mesh for every vertex of your original point-mesh. In the vertex shader, you'd calculate the screen-space position of the point (using the world-view-projection transforms as usual), then translate the circle mesh to be centered at that position. As for the second part of your question, on how to have the circles influence each other's color: it depends on what you want to do, specifically. Hardware blending can be used to handle simple cases, such as adding or multiplying the colors, or alpha-blending. If you want something more complicated than that, then it might be possible to do it with a multi-pass algorithm, or (on the latest GPUs) programmable blending. 

If depth testing is disabled, the triangles get drawn in the order they're submitted, i.e. the order they appear in the index buffer or vertex buffer. Later ones will overwrite earlier ones, or blend over them if blending is enabled. This ordering is guaranteed by all GPUs and APIs as far as I know. 

Any techniques that involve raytracing in the fragment shader might want to write Z in order that the depth buffer contain an accurate representation of the raytraced surface. For example: 

The other currently common meaning of "light probe" is an environment cube-map whose mip levels have been pre-blurred to different extends so it can be used for specular lighting with varying levels of roughness. This image from Seb Lagarde's blog shows the basic idea: 

It depends. There are a few competing factors at work here. First, consolidating terrain data into fewer buffers (or one) may allow you to combine multiple terrain patches together in a single draw call—assuming there aren't other state changes between patches that would prevent this. So, you may be able to reduce draw-call overhead this way, or at least reduce the state-change overhead between draws. That being said, 50–100 draw calls is not that many, so this may not be a real performance concern. In addition, do you anticipate needing to use the terrain data for other shader effects? For example, maybe a shader procedurally generates grass growing on the terrain, or you use height above the terrain to influence lighting or AO or some such. In that case having all the terrain data in one big buffer that you can pass to those shaders may simplify things. On the other hand, if you're having the terrain deformed/edited in real-time (which it sounds like you are), then it may be faster to update the entirety of a small buffer than to update a small portion of a large buffer. The problem is when the CPU and GPU are working asynchronously, and you update a buffer from the CPU, the GPU may still be using the previous version of that buffer. To fix this, the driver internally keeps 2–3 instances of the buffer (much like double/triple buffering the screen). If you always update the entire buffer at once, the driver can just put the updated data into the "next" instance of that buffer. But if you update only part of the buffer, the driver must copy all the rest of the data from the current instance to the next instance. This extra copy will consume some performance either on the CPU or on the GPU. So, if the terrain data was entirely static I'd say it's better to put it all in one big buffer. However, if you're going to be updating portions of the data in real-time, it may be better to keep it in smaller buffers so that you can update exactly the ones that have changed while leaving others alone. In the end, you'll have to do some profiling to see which approach is better. 

See the ImageMagick docs for more info about its command lines are structured...it's a powerful piece of software, but it takes a bit of work to get the hang of how to use it. 

and that's all! Only 12 bytes per pixel. Alternatively, you could use an RG16 buffer for the normals, and move roughness + AO into a separate 8-bit buffer. That would give you some room to grow should you eventually need more G-buffer components of either 8-bit or 16-bit sizes.