If I understand your question correctly, you're looking for ways to generate lots of non-trivial uniquely satisfiable 3SAT instances. There's a tension between your desire to find a way to generate all uniquely satisfiable instances and your desire to find a way to generate hard uniquely satisfiable instances. For the moment let's ignore your uniqueness condition. Then I know there has been some work on this problem, e.g., see this paper by Achlioptas et al. You might think that they proceed by generating random clauses and discarding the clauses that are inconsistent with a randomly chosen but fixed assignment. But although this method will in principle generate every satisfiable instance (with some probability), it tends to produce instances that are easy to solve. To get hard instances, Achlioptas et al. adopt the approach of picking some known hard problem and generating instances based on that. I suspect that a similar dilemma will arise for uniquely satisfiable instances. My guess is that a randomized method that generates every uniquely satisfiable instance with some probability will tend to produce easily solved instances. If you want hard instances, your best bet is probably to pick some cryptographic problem (where the unique solution is the cryptographic secret, e.g., a factorization) and generate instances based on that. 

The popular press often gives the impression that the primary, if not the only, reason that computers are succeeding at more and more tasks (beating Kasparov at chess, beating Jennings at Jeopardy, etc.) is increased raw processing power. Algorithmic advances are typically not given that much credit. However, I'm ambivalent about whether insisting that algorithmic advances be given more weight is "pedantry." On the one hand, I think that those of us who are more theoretically inclined can sometimes overstate the importance of algorithmic advances and only grudgingly admit the importance of increased processing power. On the other hand, I do think the public should be better informed about the role of theoretical advances in solving practical problems. 

There's a subtle point that I rarely see mentioned in these kinds of discussions and that I think deserves more attention. Suppose, as Andrej suggests, someone builds a device which reliably computes a function $f$ that cannot be computed by any Turing machine. How would we know that the machine is in fact computing $f$? Obviously, no finite number of input/output values would suffice to demonstrate that the machine is computing $f$ as opposed to some other Turing-computable function that agrees with $f$ on that finite set. Therefore, our belief that the machine is computing $f$ would have to be based on our physical theories of how the machine is operating. If you look at some of the concrete proposals for hypercomputers, you will find that, sure enough, what they do is to take some fancy cutting-edge physical theory and extrapolate that theory to infinity. O.K., fine, but now suppose we build the hypercomputer and ask it whether a Turing machine that searches for a contradiction in ZFC will ever halt. Suppose further that the hypercomputer replies, "No." What do we conclude? Do we conclude that the hypercomputer has "computed" the consistency of ZFC? How can we rule out the possibility that ZFC is actually inconsistent and we have just performed an experiment that has falsified our physical theory? A crucial feature of Turing's definition is that its philosophical assumptions are very weak. It assumes, as of course it must, certain simple features of our everyday experience, such as the basic stability of the physical world, and the ability to perform finite operations in a reliable, repeatable, and verifiable manner. These things everyone accepts (outside of a philosophy classroom, that is!). Acceptance of a hypercomputer, however, seems to require us to accept an infinite extrapolation of a physical theory, and all our experience with physics has taught us not to be dogmatic about the validity of a theory in a regime that is far beyond what we can experimentally verify. For this reason, it seems highly unlikely to me that any kind of overwhelming consensus will ever develop that any specific hypercomputer is simply computing as opposed to hypercomputing, i.e., doing something that can be called "computing" only if you accept some controversial philosophical or physical assumptions about infinite extrapolations. Another way to put it is that disproving the Church-Turing thesis would require not only building the device that Andrej describes, but also proving to everybody's satisfaction that the device is performing as advertised. While not inconceivable, this is a tall order. For today's computers, the finitary nature of computation means that if I don't believe the result of a particular computer's "computation," I can in principle carry out a finite sequence of steps in some totally different manner to check the result. This kind of "fallback" to common sense and finite verification is not available if we have doubts about a hypercomputer. 

Survey propagation is another algorithm that has been used with success on some kinds of SAT problems, notably random SAT instances. Like WalkSAT, it cannot be used to prove unsatisfiability, but it is based on very different ideas (message-passing algorithms) from WalkSAT. 

Complexity theory is a strong secondary interest of mine but it's not my primary research interest, so there is no hope for me to attend all the conferences, read all the blogs, and ensure that the "in" crowd cc: me on every bit of hot news. I try to do some of this but I am wondering what methods will give me the most bang for the buck (or rather time, since time is more of a limiting factor than money in this context). Some methods I have attempted include: 

The most common way in which oracles occur in complexity theory is as follows: A fixed oracle is made available to, say, a Turing machine with certain limited resources, and one studies how the oracle increases the computational power of the machine. There is, however, another way in which oracles sometimes occur: as part of the input. For example, suppose I want to study algorithms for computing the volume of a given high-dimensional polytope. Classically, the polytope would need to be specified by providing a list of its facets or some other explicit representation. However, we can also pose the problem of computing the volume of a polytope that is specified by a volume oracle, that takes the coordinates of a point in space as input and outputs "yes" if and only if the given point lies inside the polytope. Then we can ask what computational resources are needed to compute the volume of a polytope that is specified in this manner. In this particular case we have the very nice polynomial time approximation scheme of Dyer, Frieze, and Kannan and, interestingly from the complexity theory point of view, a proof that randomness helps in an essential way for this problem, in that no deterministic algorithm can perform as well as the Dyer-Frieze-Kannan algorithm. Is there a systematic way to study the complexity theory of problems in which oracles are provided as part of the input? Does it somehow reduce to the usual theory of complexity classes with oracles? My guess is no, and that because there are too many different ways that an oracle could be supplied as part of the input, every problem of this sort has to be handled in an ad hoc manner. However, I would be happy to be proved wrong on this point. 

If you require only that the growth rate be a polynomial in $n$ for fixed $k$, then you get the definition of the parameterized complexity class XP, which is certainly an object of interest, so there's nothing wrong with considering it. You get the definition of FPT if you further impose the condition that the degree of the polynomial in $n$ remains fixed as the parameter increases. FPT turns out to be a particularly tractable subclass of XP, and intuitively, the reason is that an expression such as $2^k n^2$ doesn't explode as quickly as an expression such as $k^2 n^k$, if $k$ and $n$ are both increasing. This intuition is supported both in practice and in theory; i.e., FPT problems tend to be noticeably more tractable in practice than arbitrary XP problems, and one can also get a nice theoretical picture of the structure of XP by starting with FPT at the bottom and constructing hierarchies of other subclasses of XP (such as the W hierarchy) above it. 

There is a polynomial-time reduction from one problem to the other, as explained on Wikipedia among other places. 

Your question might better be phrased, "How would complexity theory be affected by the discovery of a proof that P = NP is formally independent of some strong axiomatic system?" It's a little hard to answer this question in the abstract, i.e., in the absence of seeing the details of the proof. As Aaronson mentions in his paper, proving the independence of P = NP would require radically new ideas, not just about complexity theory, but about how to prove independence statements. How can we predict the consequences of a radical breakthrough whose shape we currently can't even guess at? Still, there are a couple of observations we can make. In the wake of the proof of the independence of the continuum hypothesis from ZFC (and later from ZFC + large cardinals), a sizable number of people have come around to the point of view that the continuum hypothesis is neither true nor false. We could ask whether people will similarly come to the conclusion that P = NP is "neither true nor false" in the wake of an independence proof (for the sake of argument, let's suppose that P = NP is proved independent of ZFC + any large cardinal axiom). My guess is not. Aaronson basically says that he wouldn't. Goedel's 2nd incompleteness theorem hasn't led anyone that I know of to argue that "ZFC is consistent" is neither true nor false. P = NP is essentially an arithmetical statement, and most people have strong intuitions that arithmetical statements—or at least arithmetical statements as simple as "P = NP" is—must be either true or false. An independence proof would just be interpreted as saying that we have no way of determining which of P = NP and P $\ne$ NP is the case. One can also ask whether people would interpret this state of affairs as telling us that there is something "wrong" with our definitions of P and NP. Perhaps we should then redo the foundations of complexity theory with new definitions that are more tractable to work with? At this point I think we are in the realm of wild and unfruitful speculation, where we're trying to cross bridges that we haven't gotten to and trying to fix things that ain't broke yet. Furthermore, it's not even clear that anything would be "broken" in this scenario. Set theorists are perfectly happy assuming any large cardinal axioms that they find convenient. Similarly, complexity theorists might also, in this hypothetical future world, be perfectly happy assuming any separation axioms that they believe are true, even though they're provably unprovable. In short, nothing much follows logically from an independence proof of P = NP. The face of complexity theory might change radically in the light of such a fantastic breakthrough, but we'll just have to wait and see what the breakthrough looks like. 

Any arithmetical statement provable in ZFC is provable in ZF, and hence does not "need" the axiom of choice. By an "arithmetical" statement I mean a statement in the first-order language of arithmetic, meaning that it can be stated using only quantifiers over natural numbers ("for all natural numbers x" or "there exists a natural number x"), without quantifying over sets of natural numbers. At first glance it might seem very restrictive to forbid quantification over sets of integers; however, finite sets of integers can be "encoded" using a single integer, so it's O.K. to quantify over finite sets of integers. Virtually any statement of interest in TCS can, with perhaps a bit of finagling, be phrased as an arithmetical statement, and so doesn't need the axiom of choice. For example, $P\ne NP$ looks at first glance like an assertion about infinite sets of integers, but can be rephrased as, "for every polynomial-time Turing machine, there exists a SAT instance that it gets wrong," which is an arithmetical statement. Thus my answer to Ryan's question is, "There aren't any that I know of." But wait, you may say, what about arithmetical statements whose proof requires something like Koenig's lemma or Kruskal's tree theorem? Don't these require a weak form of the axiom of choice? The answer is that it depends on exactly how you state the result in question. For example, if you state the graph minor theorem in the form, "given any infinite set of unlabeled graphs, there must exist two of them such that one is a minor of the other," then some amount of choice is needed to march through your infinite set of data, picking out vertices, subgraphs, etc. However, if instead you write down a particular encoding by natural numbers of the minor relation on labeled finite graphs, and phrase the graph minor theorem as a statement about this particular partial order, then the statement becomes arithmetical and doesn't require AC in the proof. Most people feel that the "combinatorial essence" of the graph minor theorem is already captured by the version that fixes a particular encoding, and that the need to invoke AC to label everything, in the event that you're presented with the general set-theoretic version of the problem, is sort of an irrelevant artifact of a decision to use set theory rather than arithmetic as one's logical foundation. If you feel the same way, then the graph minor theorem doesn't require AC. (See also this post by Ali Enayat to the Foundations of Mathematics mailing list, written in response to a similar question that I once had.) The example of the chromatic number of the plane is similarly a matter of interpretation. There are various questions you can ask that turn out to be equivalent if you assume AC, but which are distinct questions if you don't assume AC. From a TCS point of view, the combinatorial heart of the question is the colorability of finite subgraphs of the plane, and the fact that you can then (if you want) use a compactness argument (this is where AC comes in) to conclude something about the chromatic number of the whole plane is amusing, but of somewhat tangential interest. So I don't think this is a really good example. I think ultimately you may have more luck asking whether there are any TCS questions that require large cardinal axioms for their resolution (rather than AC). Work of Harvey Friedman has shown that certain finitary statements in graph theory can require large cardinal axioms (or at least the 1-consistency of such axioms). Friedman's examples so far are slightly artificial, but I wouldn't be surprised to see similar examples cropping up "naturally" in TCS within our lifetimes.