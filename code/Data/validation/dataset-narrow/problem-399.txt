The main reason is someone had a bad experience with them (or, conversely, no experience with them) and has completely ridden them off... forever. This is otherwise known as personal preference. Now, there are some reasons that you couldn't use them. The number one reason I can think of is that a 3rd party driver or application/tool (think filter driver, disk replication, etc.) does not support it. A quick example of this is a block level disk replication tool that did not support anything other than NTFS, with only specific cluster sizes and couldn't go above 2 TB for any specific volume. 

It would restore ALL of the extents that were changed since the last full backup. These may or may not reflect actual changes. For example, a large insert statement that put 1000 rows into an empty table that was subsequently rolled back. The extents allocated to that table, while nothing would be in there since it was rolled back, will still be apart of the differential as they were changed. All the differential does (well, not all but mostly) is look a the DCM and backup those extents, figure out how much log it needs and presto you have a differential backup. So it won't apply the transactions (as in replaying them) but it will hold whatever their consequences are. 

This happens at the WSFC level. A failure is detected or SQL Server initiates a failover which moves the resource group (and all contained resources) logically to the new node (where ever and which ever that may be). The resources are then attempted to come online. Part of the process to come on-line for a network name (listener) is to have at least one dependent IP address online and register with DNS among other items. This may or may not be the same IP Address based on how your networking is setup and the location of the new node owning the resources. 

One of the use cases for Distributed Availability Groups is for extremely low downtime cross cluster migrations, you're correct. If you're using Windows Server 2012R2, you can do rolling cluster upgrades [NOT available in 2012, must be R2] without needing to do Distributed Availability Groups. 

Model is set to autogrow by 1 MB, so it's 2.2 MB right now... I'm going to guess at 2 MB it grew by 1 MB. The size on disk and the metadata sizes aren't going to match up unless you have a database that is completely and utterly devoid of free space. The log has a size it can't be smaller than, which you're at right now. 

Without going into a large amount of technical detail, you should choose whatever the storage behind the virtualization layer likes as it's preferred or most optimized transfer size, that's what I'd use for my allocation unit size. This is because it matches what the storage layer likes the most and will be the most optimized. If you want to get a little more technical, uniform extents are 8 contiguous pages which just happens to be 64k. This happens to nicely line up with a 64k allocation unit size. The largest possible log block can be 60k, which again fits nicely... but generally is much less, which results in space waste, and could not be the optimized size that the storage layer likes. The allocation unit size is just the smallest possible fragment given to a file. Thus an AU of 64k means a file, holding just a single bit would still have an on disk size of 64k. Normally, larger AU sizes result in lower on disk fragmentation which may or may not matter, depending on the storage array's physical storage medium. In an example of this, SSDs and NVME hold internal translation tables for pages (not the same as DB pages) for wear leveling and write amplification. In this case, there is no idea of where the physical data will be saved, and thus may or may not be physically sequential. Thus, again, I'd go with the value specified by the storage layer vendor. If you don't happen to know what that is, 4k and 64k are both well supported (though larger AU sizes may not support certain NTFS technologies, which shouldn't be used with SQL Server anyway - but for completeness I added it). Really, what will make the largest difference is whether the storage layer has mixed 512 and 512e/h storage representations (which is the physical block size). This will cause more issues that allocation unit size of the filesystem. 

Interestingly I went to find the MSDN BOL page to show this... and there isn't one. That definitely needs to be corrected and I'll follow up on that. 

The connection string is not setting something called which handles the way a connection is deemed to have timed out or failed. This is in seconds. The default is 15 seconds. Knowing this... Assume SQL Server failed and we needed to fail over. The fastest this application would connect is the time it takes to failover. This is a combination of a few things: 

Quite literally... anywhere. The witness may act as a voter (if it has a vote, depends on if dynamic quorum is turned on, the current state of nodes and votes, etc.) and will be used for arbitration. Doing a manual failover involves having quorum and a synchronized secondary. Nothing in there requires a witness. Let's dig a little deeper. The point of having a witness is to help with having enough votes to keep quorum and for arbitration so that you don't split brain. Since you only have the single vote (I'm assuming defaults, here) in San Jose - it's a moot point. If you truly had a DR issue you'd have (from the point of SJC) a single vote out of a possible of 3, which isn't enough to have quorum and thus the cluster service would shut down. It doesn't matter what the AG settings are, that's how WSFC works. You'd have to force quorum, regardless of if you can see to witness or not. We're not going to get into everything with WSFC but that's the gist of your current setup. Thus, I'd put the witness where it will do the most good, which would be at site DC. 

That depends on what is wrong and where it happened. The action to be taken first is to make sure your entire restore sequence is still valid and can be actioned. The second is to figure out what caused the corruption and potentially move to different storage, drivers, etc. In this case, chances aren't that great that it will happen very often if at all. 

You can add more, but for each possible issue there will be a different amount of time it takes. The amount of time is roughly: total = Detection + Mitigation + Redo Queue drain + Undo Each of the above will have different detection and mitigation times, but the redo and undo should be roughly the same for each test. So, at the end of the day, really you're testing detection times. If you only care about how long it takes to do a failover, then run: on the new primary. Time it. That's how long it takes. 

Depends but most likely you won't directly need it unless there is something really, really bad that happens. Then you'll need to restore the DMK over the other one but that's not ideal, fun, or easy, either. If you have a backup of the master database then it already holds the DMK, additionally you will need a safe place to store the password. Since master is a special database, the use cases of this are much smaller than a user database. 

SQL Browser isn't running SQL Browser ports are blocked (1434 UDP) Incorrect DNS entries SQL TCP Port is blocked (custom port number for named instance) SQL TCP Port is blocked (Dynamic ports) TCP is not an enabled protocol 

This is more flexible as a corrupt backup file (say a log file) could be spanned by a differential or a corrupt full could be spanned by differentials, etc. I would not change how your strategy is currently, but I would make sure it meets your RPO and RTO requirements. 

In this case, not a whole lot. The SSMS cert is still a self signed cert. Self signing just means you're the only authority and no one else can claim you are who and or what you say you are which in this case, doesn't matter. The private and private keys will still be used to encrypt and decrypt regardless where the certificate was made, it just needs to be a valid cert. You may or may not be bound by using an PKI ( Public Key Infrastructure) you have at your company or using HSM (Hardware Security Module) to store the keys versus just generating a local certificate on your machine for use in other places. 

This means there is (theoretically) 14.93-10.67 = 4.26 GB/Sec difference. Unfortunately that's just the memory part. Continuing our way down the pipeline, we're going to hit our memory controller, of which almost all are integrated into the processor. Thus we'll have to look at the processor (and motherboard layout) to see the number of possible memory channels and throughput of the controller. You can continue to do this for your specific hardware and figure out the theoretical difference. Since you've specifically asked about SQL Server In Memory technology, having the faster memory will almost-always be better. That's not the whole case, though, since the way IMOLTP works is by using a specific assembly instruction (cmpxchg16b), thus having faster memory may or may not help if the bottleneck is the CPU speed (think 2.1 GHz [high density VM cpu] vs 3.2 GHz [low density high throughput cpu]). Assuming the same processor and that the CPU is not a bottleneck, there is dual or triple channel memory, all memory slots are balanced, etc., etc., then yes having the faster memory would generally be better.