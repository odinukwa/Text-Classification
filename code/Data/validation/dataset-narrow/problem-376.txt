A last point is members of db_owner can drop your database. It is usually good practice to grant the minimum permissions a user requires to perform their role and nothing more. 

You can drop all views on the instance via the following PowerShell script. Edit YourInstanceName before running. This will drop all views with DoNotNeed_ in the name from all databases on the instance. This script could easily be extended to do the same against hundreds of instances by adding another level of iteration. 

Have you looked at a third party tool such as SQL Compare from Redgate? This is designed to compare schemas and ease the process of deploying changes. I don't think SSIS is the most appropriate tool for this task. 

An alternative would be to use virtualisation and maintain separate instances on the same physical host. The benefit in doing this is each instance maintains its own dedicated allocation of CPU and memory that can be scaled up or down as necessary. If you merge all instances as you describe, then all databases will be competing for the same CPU and memory. In addition, a shared plan cache will now mean plans from one database can age out plans from another. If you maintain separate instances in VM, then you can retain the same linked servers and not have to make any database or software changes. Performance monitoring is also far easier with multiple instances. It is simple to see which VM are contributing to peaks in resource usage. In order to use maximum virtualisation described, each core of the physical host must be licensed for SQL Server enterprise edition. Valid software assurance is also a condition of this licensing model. 

That way I can have the same SP installed on each server, any bug/feature fixes doesn't need me to manually type up 10 different SPs to install. In the real world I'm also limited by SQL2000 however I'd be interested to hear in ideas using SQL2000 and/or SQL2008R2. The servers are a mixture of both. As I understand it Synonyms wouldn't help as on the instance with 3 DBs I'd still end up with each DB having it's own copy of the SP with it's own hard coded definition of which named synonym to use. I also don't feel dynamic SQL statements would be a good fit. There's more to it than the example snippet above and I use table variables to marshal all the work to be done- so that would be out of scope for all the other statements I need to work with. 

One of our developers created a view whereby one of the select columns was wrapped in an RTRIM function. That same column on the underlying table has a non-clustered index on it. Now queries using this view and a where statement on this column are failing to use the index due to the RTRIM. I need the view's output to be consistent as unfortunately this developer is no longer with us and I can't read his code. What are some options for optimisation? I've got one, but I'm hoping there is better. 

I created procedures to encrypt, decrypt the column and everything worked OK. I then took the database (SQL Server 2005, Mixed Mode) and restored it to a new PC with an SQL Server 2008 Express. WITHOUT restoring KEY and Certificate, I launched my application which use the stored procedures to encrypt/decrypt and SURPRISINGLY everything worked in the new database!! I could decrypt previously encrypted data successfully. Is this normal?? What is the use of backing-up the Key and the Certificates then? Thanx in advance 

Something tells me that it was not normal. Let me take it from the begining. In an SQL Server Express 2005 I needed to encrypt one column. I did the following. 

Object ID 0?? index ID -1 ?? partition ID 0?? I cannot make out anything out of it The database is in Simple Recovery mode (unfortunately) and I have a month old backup. I must have it up an running because it contains vital data. I would appreciate any help you could give me. Thanx in advance! EDIT1:Unfortunately Red Gate's Data Compare cannot register the database while it is scanning the tables... :( 

I have added all available actions for the login event (SELECT * FROM sys.dm_xe_objects WHERE package_guid = '655FD93F-3364-40D5-B2BA-330F7FFB6491' AND object_type = 'action' ORDER BY name) but none appear to give the network protocol. It may be of course that Login is not the correct event to give this information, but I can't see a connection event (or similar) within the XE DMV. To confirm, I want an extended event session to expose the net_transport information that is returned by dm_exec_connections: - 

If using SQL Server 2016, use the STRING_SPLIT function. If using a version prior to 2016, this great article has all the information you need $URL$ 

You can create a SQL Server Extended Events Session. Monitor the sql_batch_completed and sql_statement_completed events, add a filter for your stored procedure name and include client_app_name in your global fields to return. You can also collect a host of other information that may be relevant for your needs such as nt_username. 

You should create a SQL Agent job. This can poll your column and fire your stored procedure when the datetime exceeds a specified value. Alternatively, if your stored procedure needs to run at exactly the specified time, schedule a SQL Agent job to fire at that exact time and don't poll. You can do this programmatically via the sp_add_jobschedule system stored procedure, or manually depending on your requirements. Using the latter method you could remove the logic that writes the desired time to a table. If that table value is still required however, you could create a trigger on that table that invokes sp_add_jobschedule to create a job schedule. 

It is acceptable for the last person aspect of the answer to be fuzzy or loose. i.e. as long as it's someone who went near it sometime vaguely recently that is "OK". 

I recently got my first taste of the error "Heterogeneous queries require the ANSI_NULLS and ANSI_WARNINGS options to be set for the connection." which I hadn't come across. Nearly every resource I found on this site and elsewhere dealt with this occurring in a stored procedure. My specific scenario is a Vendor supplied app and DB, I can change the DB but not the app. I created a view on the DB to another server (via a linked server) but when I attempted to use this in the app I got the aforementioned error. I used SQL Profiler to prove the app opens its connections with ANSI NULLs and WARNINGS OFF. I used SSMS to prove the link works with the default values there. But unlike an SP I can not set ANSI WARNINGS on a view. I set ANSI WARNINGS on the DB as a test, the app now worked, but I couldn't afford a full regression test of the app so this solution does not work. I also tried many different ways of implementing the linked server including created an ODBC connection on the server and then using that. But it's like the app is explicitly setting ANSI WARNINGS to off. Not sure if matters but the source server is SQL2000 and the remote server was SQL2008R2. 

I don't believe the issue is concerning the encryption. You are doing two sub-queries either of which could contain multiple rows, hence the error. If both source tables are the same, you can re-write as: - 

Is your database online during your release or do you have a maintenance window? If you have a maintenance window and are running Enterprise Edition you could create a database snapshot pre-deployment. Then you can easily revert to the snapshot if your release process does not execute properly. 

In an OLTP system, they should be kept as two separate tables with appropriate indexing on the ID columns. In a data warehouse star schema design it would be perfectly legitimate to denormalise Color into the Car dimension. I am assuming here that Car would be a dimension, supprting facts such as Journey or CarSale etc. The denormalisation would be handled by your ETL process during import to the warehouse from your OLTP system. This would usually be done via intermediate staging tables. 

Take a look at tSQLt ($URL$ which is a great free tool to create unit tests in SQL Server. You can fake tables (by executing tSQLt.FakeTable) within your tests and then create mock data as you describe. Tests are themselves stored procedures and any changes made as part of your testing suite are rolled back post test. You run your tests by executing the tSQLt.RunAll stored procedure and it gives you visual feedback summary of your test results. To get started, navigate to $URL$ and download. Running Example.sql will create an example database called [tSQLt_Example], create the testing framework and some example tests to see what is possible. Also available is the Redgate paid product SQL Test ($URL$ This is a GUI wrapper around tSQLt but is not required to use tSQLt. 

I have encounter some database corruptions in the past but I knew which object has hit the road. In this situation I run a DBCC CHECKDB against a problematic SQL Server 2005 database and I get the following errors 

I have an issue with one database. The issue is as follows: The database is being running on SQL Server 2000 Standard for the last 6 years in Full Recovery mode. In the beginning of this summer the database was about 5GB. Since then, the only thing we did beyond normal usage is some extensive deletions. This week, there was a problem with the PC and I was forced to do a backup and restore it on an SQL Server 2008R2 Express and set the database in 2008 mode. The backup file was about 1GB. When I restored it, the MDF was 9GB of size! I checked the old MDF and it was of the same size. I checked the size of the tables and they cannot reach the 9GB reported! I did a shrink but the size did not change. Any clues or where to check? Is there a chance that the Full Recovery, can affect the size of the MDF files? I am thinking of setinng the recovery model to Simple, back-it-up and restore it. Is it going to make a difference? Can I do it on a live database? Thanx in advance! UPDATE:The initial size of the database is 1306 MB UPDATE2 sp_spaceused: Database Size=8646.88 MB Unallocated Space= 0.00 MB reserved=1336984 KB data=1020376 KB index_size=210408 KB unused=106200 KB