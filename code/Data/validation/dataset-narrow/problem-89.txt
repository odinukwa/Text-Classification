When providers lay fiber they don't just lay one fiber. Over land and on short underwater hops they lay a large bundle because fiber is cheap. Transoceanic fibers need integrated optical amplifiers which reduces the practical fiber count. Your long distance link will run mostly over existing infrastructure with possiblly a small ammount of new fiber for the final connections. There are various levels at which you can buy into the fiber infrastructure depending on your bandwidth/latency/jitter requirements. You can buy a dedicated fiber path, this is often known as "dark fiber" because the customer typically supplies the equipment to "light" the fiber. This is most common on shorter links where the cost of the fiber is relatively small and so the costs of WDM or electronic muxing are not justified. Those with very high bandwidth needs may also buy dark fiber to run their own WDM system over. You can buy a wavelength in a WDM system. This is a common option for high bandwdith links over long distances. The provider will generally operate the specialist WDM transcivers and if needed optical amplifiers and use a non-wdm fiber for the final link to the customer. You can buy a line that is muxed electronically. This is common for relatively slow (sub 10Gbps) links. There are various systems for doing this including ATM, PDH, SONET, MPLS etc. Some systems only allow fixed bandwidths while others allow for a burstable service. The final link to the customer may be either copper or fiber depending on bandwidth requirements and what is available locally. 

If you have line of sight then it should be possible to set up a wireless link over a few kilometers. You would want high gain antennas on both ends, you could either (ab)use wi-fi gear or look for gear designed for the purpose. Getting the antennas as high off the ground as possible is nearly always a good idea to ensure you get the line of sight path and to reduce the impact of ground reflections. Be aware that simply slapping high gain antennas on wifi gear may exceed effective radiated power limitations in radio regulations. If you care about being legal you would need to check your local laws and quite probablly turn down the transmit power. Having the high gain antennas still helps even with reduced transmit power because high gain antennas help both transmission and reception. If you want to connect devices wirelessly at the endpoint you would use a seperate wifi network with an omnidirectional anntena for that. 

SINR and received power are in different units, so it doesn't make sense to say one is less than the other. 

A host or router that wants to send a packet first looks up the destination in it's routing table. This returns two peices of information. 

Two bits are special, one indicates whether an address is "local" or "global", the other indicates whether it is unicast or multicast. MAC addresses were traditionally allocated to a vendor in blocks of 224. Most vendors will never use that many leading to lots of wasted addresses. Smaller blocks are now available but only in a very limited range of sizes, so there is still a lot of wastage. 

Normally IP addresses are given out by providers to their customers. Small hosting customers will get IPv4 IPs allocated one at a time to their servers. Some organsiations will run their own neworks and have their own blocks of IP addresses either allocated by their provider or in the case of larger organisations direct from a RIR (google for example have their own IP space, I dunno about stackexchange). IPv6 IPs being so plentiful are usually allocated in blocks even when a customer only buys a single server. IP addresses are allocated to networks by RIRs (ARIN, RIPE, APNIC etc). However the regular "free pool" of IPv4 addresses at most RIRs has now run out (there are some IP addresses held back by the RIRs for specific purpose allocations). So companies wanting more IPv4 adresses now have to buy them on the market and get the RIR to transfer them. The customer then sets up their DNS records to point their domain name(s) to the IP address(es) that are allocated to their server(s). 

Yes and no. You need a route in your internet gateway to bring the traffic back to the L3 switch. However there is no need for the internet gateway to know about all of your individual subnets individually. Assuming all your internal networks are in 192.168.0.0/16 a single route will suffice. As Ron points out if you do this you should also add a null route for 192.168.0.0/16 on the L3 switch so that if a packet addressed to a nonexistent subnet somehow enters the network it doesn't bounce back and forth between the L3 switch and Internet gateway. 

The Wikipedia answer Teun posted is correct but is pretty dense and hard to follow for someone who is not up on EE terminology. Here is my attempt at a less jargon heavy version. Signals are transmitted as a voltage difference between the two wires in a pair while power is transmitted as a voltage difference between two pairs. Ethernet is transformer coupled so the transmit and receive electronics only see (in principle, nothing is perfect) the voltage between the two wires of a pair. The power connections are taken from a center tap on the transformers so they only see the voltage between the two pairs. The large difference in frequency also helps keep the signal and power from interfering with each other but it's not the primary separation mechanism (unlike with POTS). 

The translation information used by linux nat is available in /proc/net/ip_conntrack Alternatively rather than mess arround with nat you might want to look into the queue target in iptables along with libipq. This will let you mess with the packets in userland without having to mess around with NAT. $URL$ 

There is a chicken and egg problem. The application layer headers are not sent until after the TCP connection is established but the process of establishing the connection establishes things like the initial sequence numbers so once the TCP connection is established it can't be diverted using NAT. So instead an application level proxy is needed which can accept the TCP connection from the client, inspect and possibly modify* the application headers and establish a separate TCP connection to the back-end server. Note: if your clients support SNI (most modern web browsers do) it is not necessary to decrypt tls traffic when proxying it in this way. * For example to pass the IP address of the client to the backend server for logging and abuse control. 

You can host multiple hostnames on the same IP address through a process known of as name based virtual hosting. A complication in the past was that you could only have one ssl/tls certificate per IP address and using the same certificate for multiple independent sites was tricky. In principle one certificate can cover multiple hostnames but the certificate would need to be reissued each time a hostname (or wildcard group of hostnames) is added and if the hostnames belong to different domains convincing the CA that you have legitimate control of all of them may be difficult. Recent versions of tls have an extension called server name indication which allow multiple certificates per IP. Unfortunately it has taken a long time for client support to become ubiquitous. Internet Explorer on Windows XP and the default browser on android 2.x being the main browsers that don't support it. Both are in decline but that decline has been much slower than people hoped. If you have to serve non-browser clients then the picture can be less rosy. Also each IP address has to be assigned to a machine. So if you want to host sites with the same public IPv4 address on different machines you will need to use some sort of reverse proxy service (with the backend links to the servers going over IPv6 or private IPv4). 

Yes. There are three scenarios. The best case is if your ISP is highly cooperative. You can have a single pool of IP addresses with the ISP load balancing on their end and your router load balancing on your end. If your ISP doesn't explicity cooperate but also isn't too careful about ingress filtering you may be able to treat the IPs are one pool on your end even though the ISP is not and load-balance all outgoing traffic. If your ISP doesn't cooperate and is doing careful ingress filtering then you will need to route traffic to the correct ISP based on it's source address. This can be done using policy routing. You can then give your webserver IP addresses from both blocks and use a DNS round robin to load balance them, 

Partly. Cable networks were primerally designed for broadcasting TV. Coax has the ability to support wide signal bandwidths and high signal powers making it suitable for broadcasting TV to lots of clients from one headend. It's relatively expensive though, so passive splitters close to the clients are the order of the day. The phone network was/is a system designed for carrying phone calls. Twisted pair cable is relatively cheap allowing each client to be given their own pair back to the telephone exchange, however it has poor high-freqency/high power characteristics. Neither network is ideal for carrying high-speed internet service, but few providers want to rip up their networks and start from scratch. On the phone network side DSL uses has to use clever modulation tricks to correct for the crappiness of the cable and extend the usable bandwidth but the throughput is still poor, especially if the cable run is long. On the cable network side you have a few challanges to data service. Firstly a significant portion of the available bandwidth is taken up by TV services. Secondly what is left for internet service is shared. Thirdly the splitters make the signal path very lossy, this is not so much of a problem in the downstream direction where you have a big amplifier at the transmitter and little interference at the receiver but is much more of a problem in the upstream direction where you have a small amplifier at the transmitter and lots of interference at the receiver. Combining the crappy high-frequency behaviour of twisted pair cable with the splitting losses and shared nature of a cable TV like network layout would result in terrible performance. Optical fiber offers both very high bandwidths and cheap cable. The downside is that it requires more expensive tooling and more careful handling. Optical is the obvious choice for newcomers to the market but existing providers with a large deployed base of copper and a large staff used to dealing with copper are more reluctant to switch. Many existing providers have moved towards hybrid systems. Fiber is run to street cabinet and then VDSL or cable is run to the final customer site. Performance is improved over traditional systems because the length of the copper run is shortened and in the case of cable the level of sharing may also be reduced. This hybrid approach can deliver performance gains to an existing network at a lower initial cost than a complete move to fiber. However it means that active equipment must be maintained in the field. 

You can run anything in a network namespace with "ip netns exec". So it should just be a matter of writing some scripts to launch your routing daemon in different network namespaces and with different configuration/data files. 

Running your internal network on ISP-assigned addresses is generally a bad idea. The reasonable choices for internal traffic are unique local addresses or provider-independent addresses. 

To interface between different speeds requires a device that can accept complete frames into a buffer and retransmit them at the changed speed. That pretty much means a switch. Custom designing a switch in the form factor you want will not be a cheap option. EEs at the skill level to successfully build a reliable gigabit Ethernet implementation don't come cheap. I think your best option is probably to change to a different wireless access point. 

The abstraction model for an application using TCP is a stream of bytes. Bytes will be delivered to the receiving application in the same order they were sent by the sending application but often not in the same sized chunks. If an application wants to send messages over a TCP connection it will need to provide it's own message framing. If a packet is lost and needs to be retransmitted or if the network reorders packets then the receiving TCP implementation will need to hold the data after the gap back until it can deliver the data to the receiving appliation in order. 

It all depends on the setup. Assuming a simple setup with a single router/firewall and a number of Ethernet switches on a flat ethernet network traffic to the private IP will go directly while traffic to the public IP will have to harpin through the router/firewall. That will add latency to the path, how much depends on how highly loaded the router/firewall is, how highly loaded the network in general is, how fast it can process packets, where the router sits on the network relative to the client and server and so-on. In a more complex network you would have to look at the overall toplogy of the network to determine what impact using the public IP would have on the path and whether the path going through the translation would be longer. Asside from performance two other issues to bear in mind. 

Theres a few ways to do it. One easy way is just to build a virtual ethernet link. The VPN software creates a virtual ethernet interface on both ends and forwards frames between them. You can then use bridging to connect the virtual ethernet interfaces to real ones. I have successfully done this using both vtun and openvpn. I preffer openvpn because it has built-in bridge functionality that allowsone openvpn instance to serve multiple clients. There are also similar solotions from the proffesional networking world but I don't have personal experiance with them. The upside of this approach is that pretty much everything (unless it's very latency sensitive) that works on a LAN just works. Broadcasts get to where they need to go, hosts can seamlessly move from one site to another without changing their addresses. The downside of this approach is that the overhead can be relatively high. Firstly the growth in packet size from the extra headers means that full size packets from your LAN won't fit in a normal internet MTU. So either they have to be fragmented somehow or the network you run your tunnel over needs to support jumbo frames. Secondly a load of pointless broadcast chatter can end up going down the tunnel. Thirdly it can be difficult to keep local traffic local and avoid triangle routing. There are fancier systems too that emulate some aspects of an ethernet network while trying to avoid some of the above problems but again I have no personal experiance with them. 

Multiple requests happening at the same time are distinguished by TCP port numbers. This allows the server to keep track of seperate requests. The NAT will ensure that the TCP port numbers seen by the server for simultansious TCP connections are distinct even if the ports used locally by the client machines were the same. However TCP connections do not help much in determining whether two requests came from the same machine. Obviously if two requests use the same TCP connection they came from the same machine but it is normal for requests from the same machine to use many different TCP connections. User sessions on the web are usually tracked using http cookies. Note though that these are per-browser not per machine. There is no robust way for a website to tell the difference between two different browsers running on the same client machine and two different client machines behind the same NAT.