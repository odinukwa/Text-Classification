This is due to relation attributes (defined in and , or defined dynamically from a statement) supporting modifiers (via ), whilst function parameters do not. Modifiers are lost when processed through functions, and since all operators are handled via functions, modifiers are lost when processed by operators as well. Functions with output values, or that return sets of record, or the equivalent are also unable to retain any modifiers included in the definition. However, tables that will retain (actually, probably typecast to) any modifiers defined for in . 

Your design seems almost correct. I can pinpoint the following problems though: You're using the table for two purposes: 

To check if an index is working, use rather than , otherwise you're not only measuring index performance but also the time it takes to transfer all the data across your client application, which in this case is a whooping 50,000 rows! You can significantly simplify your query and index by date datatype instead of timestamp, which is smaller and therefore it should perform better: 

This was hard! I don't know if this is simpler, but at least it doesn't use window function nor produce rows that require being filtered out. 

There are many ways for doing this. When measurements can be trusted to be sequential (), then one can use a simple self left join, which should perform very fast: 

This is an impressive ERD. Are you sure you want to have every table inherit from ? The tables and are almost the same, differing only by identifier. I would have the field be optional (NULLable) and merge both tables. You'll know if one record is one type or the other if that field is NULL or NOT NULL. The orders/bookings seem alright. 

If you're selecting from tables that include any single-value key (indexed) that is unique for the result (e.g. ), perhaps the following will perform better for your data: 

Your query has many issues that difficult optimisation. You use in several tables where should suffice (e.g. all cases of and ). You include tables that you later never really require (e.g. all cases of ), so removing all of them wouldn't impact the result (replace all instances of from to in the where clause). You left join the table four times, when one should suffice with inner join: 

You're migrating SQL code intended for SQLServer. MySQL uses backticks instead of square brackets, and does not have user. The following code would be suitable for MySQL: 

MariaDB encryption wouldn't be able effectively deal with either, because stealing the drives would effectively provide root access to the system, including access to the encryption keys and complete unrestricted access to the database. This is also the case with cloud technicians, who can access backup copies of the system or peer into the filesystem by analysing the underlying datastore. MariaDB encryption can minimise the risk of intrusion, however in order to access the drive or mysql files, the attacker would have to have either root or at least mysql user privileges, and would access not only the files but also the encryption keys in that case. A good way to ensure encryption at rest is using dm-crypt (or equivalent) to have the OS encrypting the partition where the database files are located, and entering the password manually by an authorised operator at system or RDBMS startup time (i.e. no keys stored in the system at all). This system is not particularly effective against server breaches, but it makes the data completely secure in case of physical storage theft, or access to system backups or underlying datastores. 

Your strategy for getting information from can be useful for a one-off, but for ongoing queries to it, especially over millions of records and expecting quick results, it is far from optimal. Considering the sheer size of your tables, you'll probably benefit from datawarehousing. If your tables are constantly updated, you'll need a trigger to keep your datawarehouse updated. The following is not exactly datawarehousing, but it follows the principle of creating the most lightweight extract of the data you'll need for future queries: 

@Serg's answer is effective and fast for a small set of attributes to compare, but it is limited to 30, i.e. hitting the maximum 61 tables that can be referenced in a query (MySQL 5.x). The following is another equivalent but arguably more costly plan, however this one is not limited in the number of attribute tests (you would build dynamically the table of UNION ALL rows, and leave the rest of the query as is): 

You'll have your reasons for going this route, where it seems you're duplicating information (since is implied from ). However, unless what you want to achieve is having with an optional product OR variant (i.e. a CHECK constraint ensuring one and only one of these IDs is not null), you must also ensure that you don't get into an invalid combination in in order to ensure you don't break referential integrity. For this, you need a UNIQUE key on and a FOREIGN KEY on . 

With the query above, all phone numbers will show as although each area has specific formatting rules, which vary greatly, so that formatting will look funny for certain regions, such as Australia where formatting is . 

The following solution allows for storing/retrieving blobs externally, avoiding any duplications, stored in a tree of folders and files named after the blobs' own md5 hashes, in such a way the can be queried and manipulated as if they were simple database bytea columns. All files are stored in the folder . Documents are saved to , where is the current database name, is a specific separate folder (optional), are the first X digits of the blob's md5 hash, and is the blob's md5 hash. The higher the value of X, the smaller the number of files per folder, i.e. for X=1, all files are distributed in 16 folders (named to ), for X=2, all files are distributed in 256 folders (named to ), and so on. There is a chance that two different files will produce the same MD5 hash (and therefore one would overwrite the other), however the chance is indeed very small, 1 in 2^128: not impossible, but incredibly unlikely. I've created all of the following functions in a separate schema, called : 

I use gdrive (follow the installation and set-up guidelines from the link). Once gdrive is installed, I dump my own databases to a folder daily, and then upload my files to Google Drive's PGBAK folder using gdrive. I use the following script in /etc/cron.daily/: 

You could play with the intervals, reducing them to 1 day (more cycles with faster queries). Ensure you have an index on . 

The schema is for PostgreSQL, you would use the equivalent of for the auto_incremental implementation of your RDBMS of choice. 

The above requires you modify to include a , since you could have more than one person with the same name and surname. 

You could implement an ON INSERT trigger and delete, for the same user, any rows with a timestamp older than the oldest (MIN) of a subselect ordered by timestamp DESC limit 10. Edit, using an array in users: An alternative is using an array straight in the users table: Add a type and a new array in users: 

You can use a deferred constraint trigger, or (what I'd do in this case) replace assignment table with an array column in and a normal before update/insert per row trigger to sort the array, and and using a unique key on the array to ensure uniqueness. 

This will really depend on many factors, beginning with your choice of RDBMS, and followed by your application language and framework. The choice will be very different if you use some form of RAILS and mysql (where database design is driven by class design in your application), than PHP and PostgreSQL (where you can choose to use create a CONTACT composite type, and keep contacts as an array of contacts in a column on each of your databases, without a separate table for it). 

You could create a query that first lists or groups elements as in your first example. Then select on that (as a subquery) grouping by basket and crating a string per basket through GROUP_CONCAT(). Then you can use those results (perhaps as another subquery) grouping by the group_concat string and having count(*) > 1 to find baskets with equal contents. 

You're mixing grouping criteria twice while creating junk_test, first in the GROUP BY subselect by having fewer conditions in the WHERE, and then in the PARTITION BY by having one extra partitioning field (). If you can assume that older ids are older jobs, then you can identify your duplicates by joining grouped table with itself, like this: 

If I execute via an TFDQuery component, the function itself fails (FireDAC executes it as a cursor), raising the same error stated above. After tearing my hear out debugging, I realised that the only way to avoid the error was avoid creating any temporary tables inside the function. Whereas creating the function as or as the error was raised. Without , no errors raised. Any clues will be highly appreciated! 

Your relationships are correct, however, they would allow for a customer to have records of products they've never bought. Is this something you want? Otherwise, you'd be better off establishing the relation between Review and OrderDetails (the intermediate table for Order-Product which in your ERD doesn't have a name), rather than to Product directly. That being said, your ERD would probably be clearer if it'd be , where OrderDetail includes quantity, price used, discount, special notes, etc. Also, Rate should perhaps not be a separate entity, but rather be Review's attributes. 

That's because your query is filtered by a single supplier, then by a range of parts, and finally the results are grouped together by year. In my personal experience, using explicit joins often helps with identifying these sorts of things, and also in reducing the chances of missing a joining condition and causing a cross table join as a result. 

For checking of initials, I often use casting to (with the double quotes). It's not portable, but very fast. Internally, it simply detoasts the text and returns the first character, and "char" comparison operations are very fast because the type is 1-byte fixed length: 

Testing This alternative seems to be less efficient for very small sets, but more or less linear in performance, whilst the original seems to be exponentially more sluggish. 

You have two options: either you loop results from PHP, or you store all the custom fields and values in one single JSON datatype field (stored as a json object, requires MySQL 5.7) in the users table. 

That will show you where the environment variable is pointing to when you start the service normally via , and all the parameters used for starting and stopping the service. 

Your query doesn't loop 10483 times, it loops once (loop=1) for each loop, but it has to sort 10483 rows first prior to discarding all but the first 5 rows. Your last query was almost correct, except for the fact that you filter user_account by the users with greatest follower_count regardless of being involved in the discussion or not. Also, isn't this query returning the same user accounts? Because you're not using DISTINCT nor GROUP BY, and user_account has a one-to-many relationship to user_post_comment. Perhaps this will return more accurate results? 

Response to the sequence question in comments: To create a sequence over a partition, your best bet is self-join with a grouped-and-sorted: 

Your queries are not the same. The first one is required to perform a complete left join and then sort, in case the results of the left join render fewer records than the limit (in which case it would includes as many records from as needed, in no particular order). Your second query effectively cancels the left join and turns it into the equivalent of an inner join, which in this case is completely pointless because you will obtain the exact same result as: 

You have an extra unnecessary subquery check, and your subquery seems to only return a rowset if there is a single value in it, so you may want to use instead of . You can achieve the same by doing this: 

There are several ways for determining if the trigger is recursing. Checking the trigger depth () is not one of them, because you don't know if the trigger itself is recursing or is being fired from another trigger. The only direct way to know analysing the execution stack during an exception: 

A variant of this is comparing a list of IDs rather than the texts, which would perform better for large sets: 

Instead of , perhaps you should use . Instead of , if the parent exists in or any of the others, use that link instead of the constant. You could also use a single IN, and a UNION of both queries. Finally, if you use a filter condition in WHERE on a LEFT JOINed table, that defeats the purpose of a LEFT JOIN and might result in bugs. Either use JOIN or move the condition to the ON part of that LEFT JOIN. In this case, the condition in the LEFT JOIN was different than the one in WHERE for the same field (t2.id), so it could never work. 

JSON documents are ideal for this purpose. MySQL can store JSON documents via its JSON datatype, and it has a variety of function for extracting information in such documents. However, MySQL doesn't currently have an effective native way of indexing JSON documents for fast search. Googling for "mysql json index" will render several strategies for doing so, some using triggers and external tables, others using indexes on computed columns. PostgreSQL, however, has very good support for JSON datatypes including indexing, opening up documents as recordsets (using LEFT JOIN LATERAL in combination with is very powerful), and GIN indexes that support JSON documents (for both tags and contents). Unless you're heavily invested in MySQL already, PostgreSQL would be a great option for your application. Alternatively, you may want to consider PostgreSQL instead of MongoDB if you'd use it for this purpose alone. Additionally, PostgreSQL can easily access MySQL tables with via Foreign Data Wrappers, it has good support for regex text manipulation, and you can do all sorts of magic on JSON documents through standard perl libraries, using pl/perl stored procedures. 

You can also use . Ensure that your local user can ssh passwordlessly to your remote host () and that its public key () is included in the remote hosts authorized_keys files (). Once you (as user) can ssh to the remote host, update the following in your : 

"Millions of columns"? Or millions of rows? For single user applications and just a few megabytes of data, sqlite can be perfectly sufficient, so it will really depend on how many lines of code/rows of data you'll be dealing with, and the complexity of the queries that you'll require. You may want to consider postgresql or mysql from the get go, to avoid getting bogged down or finding sqlite is too limited when you've invested a lot of code and time on it. 

This can be added to the function that handles the logon (replace and with point and user in logon function parameters) 

I resort to maintenance scripts like this all the time (in PostgreSQL). What you can do is using where supported (e.g. ), and conditionals otherwise. MySQL doesn't support for , so you can resort to a conditional. In your specific example, your script could be: 

PostgreSQL will optimise both cases with different internal strategies according to relationships, available indexes, and collected statistics, so which strategy is best will depend on your particular scenario. 

The two analyses differ in the text comparison, the first shows , i.e. case insensitive (ILIKE) whilst the second shows only , i.e. case sensitive (check if you didn't run LIKE by mistake the second time). 

beging the many-to-many relationship between and . These two foreign keys would ensure that the price refers to a store that is intended to have such product. Then you can create a query to find out which prices are missing: 

You could use to get a list of all equipment names sorted alphabetically for each growth space, and compare it with another sorted list of elements: 

PostgreSQL can't do this natively, but you can convert the CSV output to an XLSX file with a simple perl script. : For this script to work, install and perl modules, either through apt-get/yum, or through and . 

Any user (ROLE) can be a member of one or more user groups (which are also ROLES). According to the documentation for GRANT, "Any particular role will have the sum of privileges granted directly to it, privileges granted to any role it is presently a member of, and privileges granted to PUBLIC.". So you can have all of your personnel belonging to a company group (ROLE) with basic privileges, some in another group with elevated privileges, and a few with superuser privileges. 

What is possible is what's specified in the documentation for GRANT ($URL$ To make modifications on tables, a user must own the object. When a user owns an object, a user can do whatever they want with it (including restricting themselves any kind of access, along with the ability to grant themselves any access again). For your limited DBA, there are non-standard ways in which your requirements can be implemented, such as a DBA creating a function with that validates the executing user, and the command to execute, as something allowed, like the following example: