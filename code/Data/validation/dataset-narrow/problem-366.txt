I partially solved it: PHONE is actually a VARCHAR2 column. However, this doesn't at all explain why comparing in one direction versus another works or doesn't work, when I make that mistake. So I'm still very curious. 

I want a PL/SQL procedure to give me progress updates as it runs. However, DBMS_OUTPUT seems only to give me the output when the whole procedure is done. Is there a way to make it give me updates during runtime? Thanks! 

I'm using UTL_MATCH's Jaro Winkler similarity function and it seems to be performing well. However, I would like to adjust the prefix scale according to the situation. Is this possible? Is it possible to see what the default prefix scale is? I could not find any documentation on this, but it seems that in order to be a J-W distance, it must use a prefix scale. 

Will this trigger propagate? Like, if it updates a ROOT_ID for another record, will that trigger its own trigger? Further, if it does trigger that, will it use the new ROOT_ID? I want the ROOT_ID to propagate down the tree I've built. Edit: How this works is that each record has a unique ID, a parent ID, and a root ID. I basically have a tree, each member of that tree has a root_ID pointing at the unique ID of the root and a parent ID pointing at the one above it. The root's root and parent IDs are its own unique ID. in the case that a user manually changes a record to point at a new root and parent, I want all the children of that node to have the new root ID. Is there a better way to do this? 

I've got some resource leaking and I'm trying to find what's causing them. I think I have some statements or resultsets that aren't being closed properly. Is there any way to see all of the cursors that my JDBC connection currently has open? I am using Eclipse. 

Turning the original select into a table is not an option. Ideally for readability, I'd like to not copy/paste the select; I'd rather alias it somehow but I feel like my syntax for that isn't quite right. Here's my actual code. It is a horrible mess (actually if you have any suggestions for making it less of a mess that'd be great). It gets on the top layer - the layer below that works fine (producing the first table above, essentially) 

Otherwise it would be really useful to see disk activity from AWS, amount of data server currently writes (you can calculate it using delta between two consecutive runs) There are other ways too, but for that, I'd need a lot more details on the actual workload. 

Increase innodb log files (innodb_log_file_size) - you can go up to 4GB or so for your logs (esimating based on zero information given on the amount of writes), Increase pIOPS capacity for the data drive Increase innodb_io_capacity (up to ~70-80% of pIOPS capacity) 

It just works! Seriously though, if you got a master with two replicas environment, you probably want to do occasional consistency checks with pt-table-checksum. Replication does not guarantee consistency (and writes can happen on replicas too). Otherwise: 

ANSWER 1: Internally, in case of InnoDB, full table scan is the same as using primary key because InnoDB clusters data by primary key. And it doesn't use any other index because there's no WHERE condition on the first table. ANSWER 2: In reality, adding won't make any difference because the execution plan will essentially remain the same (and results returned will be the same too). That's again due to the fact that InnoDB clusters data by Primary Key i.e. rows are always internally sorted by primary key. So if it makes you feel better to not see those queries in the slow query log, do add the order by. Finally, I know you didn't ask this, but - I really would recommend against using as it makes you focus you on the wrong metrics. Here's my recommended way to go about query analysis: Advanced MySQL slow query logging video tutorial. Good luck! 

There are many other things you can do, but I try to live by the rule: don't fix what ain't broken. Good luck! 

It's a fairly safe bet that some of the tables have (including system privilege tables) have structure that MySQL 5.5 can't recognize. MySQL 5.5 and 5.6 is a major version difference (regardless it looks minor) and they are not backwards compatible. Try adding to to see if the problem is only with system tables (when you add , privilege tables won't be used at all and anyone will be allowed to connect from anywhere with any password). If MySQL would start, that means you have a broken privileges tables and should help you load the blank ones. If it doesn't help, you may have InnoDB structure changes that MySQL 5.5 can't recognize. In that case, you'll have to use logical backup (mysqldump or mydumper) for such downgrade. BTW, you can use any of the working slaves for it. BTW, if you're using on the master, there's no need NOT to use it on slave. In fact, it doesn't make much sense to. 

So, I have an insert statement in a Java program I'm writing. Under some conditions, I want it to insert some values as null. However, before I can execute the statement, I have to set all of the tokens to values. How can I make it set something to null instead? Edit: Example: 

I've been using sysdate to time procedures, but a lot of my procedures are very small procedures that get called thousands of times. Is there a way to get milliseconds elapsed instead of seconds? I know it has something to do with timestamps, but I can't find any exact way to do it. 

Sorry if the way I phrased that is confusing. I'm writing a program that acts kind of like a filter - it retrieves an answer to a query, counts the rows, then I want to do a slightly more narrow query. I want to do those queries until I get a count of 0, which means the query just before it is the narrowest I can get. I then use a similar version of that query, except instead of counting I actually retrieve the records. The best implementation I know of is to just write all of the different queries, put them into different statements and feed those statements into different result statements, and retrieve the counts from there. Then, I use the last one that counted > 0 and use that main query without a count, and use those results. This seems like a horrible implementation. It seems like I'm just going to be bombing the database over and over. I'd prefer to hit it once, and then narrow the resultset on the client. Any ideas? 

I want to determine if a resultset exists very quickly. At the moment, I'm doing a count - this is taking roughly 55ms, which is unfeasable.The table has ~100k records - I don't care if it has 2, 5, 100k rows that fit a query; I care if it has 0 or 1. Maybe 2 in certain situations. Is there a way to do this? Would limiting a count using ROWCOUNT (so it only counts about the first 2 rows it finds) speed up the count at all? 

You'll need to put a trigger on the siteArea table so that whenever it's updated or inserted, it'll call a stored procedure with an argument representing the new site area and that procedure will calculate the needed values and insert them in a table. Your question is too ambiguous for me to describe further. 

About a month ago, we migrated a production database and logins to a new instance. The client is currently troubleshooting an issue and wants to confirm that the login permissions in the new instance match exactly what they were in the old instance. We have long since deleted the old logins as well as the database that was migrated from the old instance. However we have backups of the master database. I though I could restore the master as a regular user database and query the views to peek at the server and database principals as they existed before the migration, but the views appear to be returning current information rather than historical. So for one, there's a fundamental misunderstanding on my part regarding what is contained in the master database (or at least how the views work). If anyone can shed some light on this, that would be great. But mostly, does anyone know if what I'm trying to do is possible? Edit: As a test, I restored the same backup of the master database onto a completely different instance, naming the restored database: master_temp. Then I tried the following: 

The results that were returned only contained information from the instance the database was restored onto. Not a single user login from the instance the backup was taken from was returned in the result set. 

I have a configuration where one database is log shipping to three different servers hosted in a disaster recovery site. These three disaster recovery servers are joined to the same AlwaysOn availability group (AG). In the event of failover, we recover the database on the server acting as the primary replica of the AG and then add the database to the AG using the 'Join only' synchronization option. Since the databases on the secondary replicas are already in a non-recovered state, the operation succeeds and we end up with a database synchronized across the AG. This is 100% great. New problem: Our monitoring software does not like it when databases are not in a readable state. So while we are in our primary site and are log shipping to our NORECOVERY secondaries at the disaster recovery site, our monitoring software opens high-priority tickets because it thinks the secondary databases are down (because it can't read them). Making the secondaries readable by switching them from NORECOVERY to STANDBY solves this issue, but creates a new one. When we failover to the disaster recovery site and try to add the database to the AG (as outlined above), it fails because the databases on the secondary replicas need to be in NORECOVERY in order to successfully join the AG. If we switch these databases from STANDBY to NORECOVERY before attempting to add the database to the AG, we receive a message saying the databases on the secondary replicas are not restored far enough in order to be joined to the AG and the join fails. If at this point, we take a transaction log backup of the database on the primary replica and apply it to the secondaries with NORECOVERY, we can re-initiate the join procedure successfully. It would seem that changing the secondaries from STANDBY to NORECOVERY is causing the engine to determine the databases are no longer in sync but I can't for the life of me figure out why. Anyone have any ideas? The only thing I can think of is that the act of recovering the primary database itself was enough to bring them out of sync, but if this were true, shouldn't it also be the case when we simply leave the secondaries in NORECOVERY to begin with (like our original plan)?