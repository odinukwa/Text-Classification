Oracle 'takes care' of the conversion between different NLS settings. As long as both NLS settings can store the same characters the there should be no problem. It is the same as the difference between the database and a client. If for example the database of the customer is in UTF-8 and you create a database in ISO8859-P1 then you will loose (some) characters with accents and from non-western alphabets. The other way around gives no problem. 

If your is immutable then it can serve as primary key. Only if it was to hold long values then you could consider to create a separate primary key field to save space in the tables that have a foreign key constraint with this table. 

In my opinion you should use the only if you do not need the result of a function that forces the usage of . For example if you have a table with and and you only want to know which s are present and you are not interested in how many (most of the times a sub-select). If is there to 'fix' a problem then you are likely to get a bad performance in return. 

In general, the optimizer is much better at figuring out whether a range scan would be more efficient than a skip scan (or vice versa). If the optimizer is making the wrong choice, the problem almost always comes down to the human not providing the optimizer with the proper information in the form of accurate statistics on the table and on the indexes (including things like histograms on columns that contain skewed data). For any given query, the only way to know with certainty which of two plans is more efficient is to try it out with your data on your hardware with your query. You can try adding the hint to force the plan to change and you can then measure which approach is actually more efficient. If the plan the optimizer is choosing isn't the most efficient, I would tend to suspect that the root cause is that some aspect of the statistics are not correct but that is something that you'd need to look at a 10053 trace to get to the bottom of in most cases. Rarely, there are cases where the optimizer is wrong because the optimizer isn't able to take into account some aspect of the data (for example, until recently, it was unable to deal with situations where the data in multiple columns was not independent such as a car table with make, model, and year columns where knowing one value constrained the set of valid values in the other columns). 

On the machine where you installed your database you should have SQL*Plus installed. For your question. Create a database link from to and use your Read Data user. Now you can query the data. 

The is not correct. You must create 2 of them. One the table for the field and one on the table for the field. 

But even then it will not give a usable result since there is no relation between the , , and . Who's result it is? Who gave the result? 

Why do you do an update on ? Anyway you must add a where clause with the primary key of the row to the update. You are now updating all rows in . Also the insert is inserting for every row already in . Why don't you rewrite the insert into something: 

The second option should be the fastest. It was made for this. Also it should have no bugs since it is used a lot and already for a long time. If you go for this then you do not even need to use a composite primary key. In my opinion the only reason to use the first option is if you need a numbering starting from 1 per client. 

The "format picture" refers to the format mask. If you are doing an explicit conversion using the function, the format picture is the second argument 

Are you trying to test the database? Or the application? Assuming that the Oracle database is configured correctly (i.e. it is in ARCHIVELOG mode, backups are done regularly, DataGuard is in place depending on your recovery requirements, etc.), by far the most common source of problems in a failure is that the application itself has not defined its transaction boundaries correctly. The classic scenario here is a banking application that wants to make a $50 payment from account A to account B debits A by $50 in one transaction and credits B that $50 in a separate transaction. Unless you happen to be able to test what happens when system fails after the first transaction commits and before the second transaction commits, you won't see that the transaction boundaries are incorrect and the application might inadvertently lose $50. 

UPDATED ANSWER BASED ON CLARIFICATIONS IN QUESTION I think because of the requirement that you can't use a constant default value for the state name, but must get the value from the States table, the solution would necessarily require triggers. So: Create your tables like this: 

So you can store the result from in the column, and then retrieve it using . The above code is also compatible with MariaDB (at least 10.2). MySQL additionally supports an init string to the and functions, see the example at the link to above. MySQL also offers InnoDB tablespace encryption, but that will encrypt the whole table, not just individual columns. 

NDB Cluster really is quite fast, with 200 million (NoSQL) QPS reported back in Feb 2015, see MySQL Cluster Benchmarks. What worries me about it is that they're still using those benchmarks now in 2018, as if no progress has been made since 2015. I also get the sense that, for whatever reason, NDB Cluster's popularity is fading compared to Galera and other solutions. (See e.g. the stats for the various tags here on DBA.SE.) 

They are different. In the first option you get 2 times into your query. Once as and once as . Both have a different content and you must put them somehow back together. To me this is more an OR instead of an AND. In the second option you get only the rows that meet both criteria. Suppose you have with the following content: 

The script expects just 1 row. Not the 26 you get. You must use a cursor and loop through the rows that you get: 

If all columns except the ID must be unique and the ID does not need to be preserved the way to go could be: 

Normalise the table so there is 1 question per row. Then you are also not limited to a number of questions per survey. 

Create an SQL that does the same as the would do. Remove the rows from the corresponding ~50 tables in the right order before you remove the users. This is some work but it keeps your data safe from an application error that does an accidental remove of a user with (lots of) messages.