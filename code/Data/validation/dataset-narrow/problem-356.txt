Then I wish you the best in your trial and error to figure out what works and what doesn't work for each specific type of interaction of DML, row store, etc. The only other recourse you may have is talking with Microsoft if you're a partner and attempting to obtain help with this endeavor. Disclaimer: I work for Microsoft. 

What are acceptable SLAs? If an order takes 50 ms to place, is that bad? Is 2.5 seconds good? How many orders per hour must be accepted, minimally? Are there stored procedures associated with these orders that should be watched and have certain thresholds put on them? Putting all of this information together will allow for the forecasting of expected volume, current volume, and potential needs in order to meet your SLAs for the business. How many places do I know that actually do this? You'd be surprised. It's taken a few of them years to put it in place but many have these statistics readily viewable and available at a moments notice. Is it easy to do? Yeah, it's just time consuming. If you don't want to put in the time, just buy a bigger/better hosted solution - if you want to create a repeatable diagnostic infrastructure in order to provide this information then be willing to customize it to your environment. Summary I can't speak for RDS (amazon) and in all transparency, I work for Microsoft - but there are other hosted solutions such as Azure that already monitor many of these systems based items (CPU, Memory, IO, etc) for you. This puts the onus on you to grab the information and create your own application based metrics. Each environment is different and while the system based attributes are a great start for monitoring, the whole picture is needed in order to make appropriate business decisions. 

I don't understand the question. If it's specific to Ola's script then I do not know, if it's a SQL Server question then you must take at least one full backup and one log backup before adding the database to the AG, so the answer would be no it isn't true. 

This will give you everything your session does, however, you can filter on the object ids for the table and indexes to make it more readable. I included acquired, released, timeout, and escalation as I don't know your environment. If you want to look at the captured data, there are two built in ways without using outside programs. 

I'm going to be brutally honest here. Availability Groups are not a silver bullet, fix everything HA & DR. It seems that the technology is being used, but not too much is known about how it works or why it does these things on both an AG level or a WSFC level, so I completely agree it may not be setup properly... however, it is working as it is setup - I can't fault the cluster for doing what it is supposed to be doing. 

You can use sys.databases which has a column called group_database_id. If this column is null then the database is not participating in an availability group. To only get user databases, restrict the query to database_id > 4. 

If you're using an HSM, there would be no point to export the key in a situation like this. The main point of an HSM is to keep the keys safe, among various other things - so you really don't want to export the key. First, I'm going to assume that on the server you want to restore this database to you've created the credential used to access the HSM and I'm going to assume you've allowed the addresses of the new server to talk to the HSM, and all of the other assumptions such as load the EKM DLL, etc. In that case, rather than specifying you'll want to use which is part of the CREATE ASYMMETRIC KEY T-SQL. 

Failover of the Availability Group (not an individual database like Database Mirroring) will happen when the health checks either timeout, fail, or the lease fails to be obtained or renewed. When and under what circumstances is entirely dependent upon the flexible failover policy chosen and the type of issue the instance encounters. 

New in SQL Server 2012 and also in 2014 are two new encryption functions that will return a binary stream as you are describing. CREATE CERTIFICATE was also updated to take BINARY input for public and private keys. CERTENCODED will return the public key portion of the certificate in BINARY form. CERTPRIVATEKEY will return the private key portion of the certificate in BINARY form. this is encrypted so the password to decrypt it would also be needed. The above could be saved in a binary form and written to a disk, or stored in textual representation of the binary form and written to disk. Should you need to create it again, use the BINARY options of the CREATE CERTIFICATE function. Edit: There is nothing in 2008R2 that can do the same that I know of through system functions. 

You've specifically removed the vote from CO and only had 1 other voter left then had an unexpected crash. Sounds like it was working as it should. 

While this post won't be a complete answer due to lacking information, it should be able to point you in the proper direction or otherwise gain insight which you can later share with the community. 

Correct. Since you're using named instances, the installation directory names will be different which is why you're receiving the error. In order for the databases to seed they must have identical folder structures. 

There are a number of different factors involved but it is entirely possible to have a replica that is synchronous be faster or slower than an asynchronous one. To list a few: 

The no listener part is going to severely impact transparent DR, but this setup is possible although not wise. Is it reasonable to use? Sure, just not in its current form - which is mostly due to the connecting through the FCI name and not a listener. Everything else is fine. 

This is because there is no database on the secondary server, which completely falls inline with what you've posted. The "database" shows up under the Availability Groups folder on the secondary because that's the metadata of the Availability Group, not because the database is on the secondary. The reason it "works" with automatic seeding is because automatic seeding creates the database and seeds it on the secondary for you versus what you're doing now which is creating the AG but not preparing the database on the secondary. Thus, no database on secondary. 

Make sure your group name returned matches (identically) to your actual workload group name. Some times, I've found, it's helpful to reconfigure even if it doesn't say it needs to be done. 

File copies and crystal disk mark are using multiple threads to read and write data combined with buffered I/O. They use completely different flags when opening/writing to files on disk, among even more differences. What's applicable to these tests isn't applicable to other applications or services. It's not that one is good or bad, just they behave completely different. How can you speed this up? 

They are equivalent, I wouldn't get hung up on this. Most likely the way the GUI is coded, honestly not even worth making a connect item about. 

It does not do this by default, you have to CHOOSE to enable the share. This is done via SQL Server Configuration Manager. If you deselect the the share will be removed. 

If this query runs quickly, I don't see why it should error out like that as I doubt your low watermark is changing that rapidly. Thus, I'm going to assume this takes some time to run. Having the appropriate indexes, not using a long transaction, and up-to-date statistics will help with this query. Additionally, the NOLOCK hint is completely ignored on the readable secondary, you may already know that and it may just be code re-use which is fine but I wanted to point it out anyway. 

Then you're going to need to fundamentally change the way your application behaves in terms of security and auditing. You'll also need to setup some sort of auditing, either in the application or database. 

Yes in terms of what you can read at any given point as the log file can be thought of as a giant ring buffer that writes over the older inactive portions. It's circular and will continue re-using inactive portions unless it fills up and must grow because all of the current space is active. 

If you have a copy of the master database which holds the then you'll want to restore that and grab the copy of the certificate, then import it into your current instance for use. 

Shared or replicated disks, yes. This is because the Disks "move" between servers. Each potential failover target has SQL Server installed locally and the service is started when the local node owns the resources and is given the online command. The local service starts and mounts the database files which are on the shared or replicated disks. This is why you don't need to change or replicate any SQL Server based items (logins, jobs, etc.). 

Make sure there are no other logreader agents running for the same publishing database. Second part: 

It's not a requirement. If you can do testing ahead of time and figure out the sweet spot for your hardware and workload, then that would be the best. Most of the time, the performance difference is negligible unless there truly is a bad configuration for the hardware or workload. 

NEVER check or uncheck the preferred or possible owners for AG resources. When using SQL Server 2016 the preferred owners list can be moved up and down to control preferred failover targets given the multiple failover targets change but do not check or uncheck any boxes, ever, with Availability Groups. Period. If you check and uncheck things like this, you're AG isn't going to work properly. When I teach the Availability Group class, I always show how this works, why it works, and why we NEVER want to do this (spoiler alert: The AG will most likely fail, hard). Let me explain this, though, to make it a little clearer. Preferred and Possible Owners These values are set by SQL Server based on the settings for the Availability Group. The cluster has its own copy of the metadata of how things should work and SQL Server also has a copy of how it believes things should work. This is great most of the time, when no one changes anything and both SQL Server and the cluster continue to hum along. However, SQL Server knows what it believes should and shouldn't be owners of the resources based on the AG settings and reflects this by calling cluster APIs to set the expected behavior. You'll see this reflected in the wait type. Three Async Replicas Let's look at SQL Server with an AG setup so that there are three Async commit replicas. If we do this, our AG and role resource should look as such: 

The Service Master Key (SMK) protects the database master key (DMK). When we say "protects" we mean encrypts. This could be said as, "The SMK encrypts the DMK." The DMK in the master database "protects" (Encrypts) the Server Certificate. The Server Certificate "protects" (Encrypts) the Database Encryption Key (DEK). 

Depends, does you application require a specific collation or options? The advantage is for the application to sort and display data properly. 

That'll give you a start on how the pieces will need to fit together. There needs to be much more diving into how this will turn out, but hopefully this will point you in the right direction. 

I'm not sure why you're switching the database into single_user except that you want no changes to be made. If you add the server as a node as I've set forth, this won't be an issue. Otherwise, yes, you'll need downtime to migrate the database. Your other option would be to set the recovery model to full, get it in full, setup mirroring to the new instance on the cluster and then cut over during a downtime which would be much less time. That's entirely possible to do, but in your initial question you said you didn't want to do this. You must restore at least one log or differential to bring the lsn gap as close as possible. I would not use the wizard, but the join only means you've staged a database that is currently in norecovery and it only needs to join the AOAG. The wizard will want to pre-stage the databases for you and ask you for shared locations, etc. Like I said it's best to pre-stage and not use the wizard for the best flexibility. 

The SQL Server Browser Service does many different things... However the main purpose of it is to look up the port number associated with a given instance of SQL Server and giving that information back to the client. Any instance of SQL Server (assuming tcp/ip) can be identified by using the computer name or ip address and the port number. If you know both of these things (or SQL Server is listening on the default port of 1433) then you'll be able to connect without using the browser service as an intermediary. To do so, you can connect using the name or IP and the port: 

If that's the case, and you are either running 2017 or can upgrade to 2017, there are read scale availability groups where you don't need a cluster. 

Don't have just a single file in the filegroup that has any free space. Set your files to the same size, grow them at the same time to the same sizes, and let the round robin with proportional fill algorithm do its' thing. 

In Availability Group environments, please do not use enabled and disabled jobs. The proper way to handle this is to check if the instance is currently the primary replica. If it is, continue with the job. If it isn't, complete with success. Here is a small example using your current T-SQL Jobstep: 

See above. Many of the companies I know have an alias policy for that reason. It's also how AGs work for the listener as it is a network layer indirection. 

Standards - each database is setup the same way with the same standard names and tables. Flexibility - a new customer needs their language to be finnish and a different collation to be used, easy enough since it's in it's own database. Try dealing with that with a single table and everyone's information in it. Making a change for a request or feature but only for a single customer, you'll start to like a database for each customer. Security - depending on needs, a different login and user for each database to keep security tight. Automation - since each database is an exact copy (or base tables are exact copies at least), this makes it extremely easy to automate tasks. Upgrades/Changes - changes and updates can be rolled out for each customer on their specific schedules and needs. If a single upgrade or schema change goes wrong, only a single customer is down and not all customers. Granularity - ever need to restore older data that may have been accidentally deleted? Yep, much easier to do this when segregated by database rather tan all jumbled together in the same table or restoring a huge database for a few records in a different schema. Locking and Blocking - if all of the records are in the same table, some type of optimistic locking will need to be used if you don't want too many performance problems of all of the customers attempting to read/write their data at the same time. This is going to lead to more tempdb utilization and make a process much heavier in terms of resources than it should be.