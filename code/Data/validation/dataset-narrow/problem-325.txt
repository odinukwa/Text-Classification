I am using SQL Server 2012 and I would like to keep my pre-existing data but to cause the ID (identity) column to skip by 2000 rows. Why is this happening? I have a scenario where I may have transactions happening on a standby server which I will need to copy to current server. I want the IDs to match. Therefore I do not want any new transactions on this server that would have the same ID as my standby server. Hopefully this makes sense!! Thank you 

I am migrating from Mirroring to AGs on SQL Server 2014 / Windows 2012 R2. I will have a 2 node cluster. I need this cluster to be online at all times for a production environment. I am reading Microsoft documentation for Availability Groups but I cannot find anything along lines of "Best Practices" for having a 2 node AG/WSFC, except that not having a witness is NOT "Best Practice". I am simply exploding with questions. If I implement a File Share Witness What is the best place to put my (file share) witness? Is putting it on a DC a good idea or a security risk? Why does it have to be Windows Server as opposed to (say) a linux/NAS? How do I prevent someone from simply deleting the file share because they don't know what it is or what its for, while not locking myself/all sysadmin out of it? If I don't Implement a File Share Witness Could I just remove dynamic quorum and exclude witness altogether? Is that even possible? Is it better to just add another SQL node to my cluster, if I have one, to act as a 3rd voting party but not include any of its databases? Is that perhaps more durable? 

Using SQL 2012 Enterprise edition Following instructions here for restoring a backup: $URL$ I need to be able to rename the database because it is the second copy on the instance (testing purposes). But I cannot rename it nor change the folders that it will restore the MDF/LDF files to. For whatever reason the "Relocate all files to folder" checkbox is not available. Restoring form a SQL 2005 backup. 

For some reason my mirrored servers triggered an automatic failover. I have 2 databases that both think they are the principal server, one is "Principal, Synchronized/In Recovery" and the other is "Principal, Synchronizing" I figure this is bad. My plan is to rebuild mirroring by dropping the database on one server and restoring it again based on the backup of the other. I have removed mirroring on one of the servers. And took a backup. However, I cannot drop the database on the other server. I have tried single user mode, detaching and taking database offline. I have tried to kill process using this database but it doesn't work. The command it is on is "KILLED/ROLLBACK" but nothing changes here. 

I found a solution - the issue is I tried running perfom not locally to the server but from another server. Locally it works. (sigh) 

I have a backup running on both my master and slave servers using pg_dump, piped into "gzip -c" command. The result file sizes are quite different. My slave backup is 1/3 size of the master backup of the same database. Replication seems to be running OK. I ran a query of table list with rowcounts and the two databases seem to be the same. As far as I understand a pg_dump is a text file with copy commands and schema defines. They should be the same size. Am I correct? PostgreSQL 9.5 on Ubuntu 16.04 The backup file is too big to eyeball 

I an new to Postgres (9.3 on Ubuntu 14.04) monitoring and I have been using Zabbix/PG_monz. I am seeing errors that my database has "too many temp bytes" and that my database is "too large". My actual backup files are 400 MB daily. I am wondering if anyone is familiar with PG_Monz or just what this error might be about. Thank you in advance! 

It seems AdeptSQL Diff is not compatible with SQL Server 2012 (at this time). And it is my preferred tool for deploying changes. I realize this is a shopping list but I hope SE won't mind. I am wondering what are the best alternatives out there and wanted to query the DBAs. Currently I am evaluating RedGate SQL Compare. It doesn't include data comparison (separate product) though it does generate deployment scripts which is nice. Just want something quick and painless for launching schema changes to production servers. What do you recommend. Thanks! 

I want to monitor CPU usage before implementing TDE on several production SQL Servers. My plan was to use user defined performance counters, over a period of a couple of days, counters as defined here: $URL$ I can view data on these counters when just looking at the Performance Monitor graph but the Data Collector sets just show datetime stamps and no counter data. I'm on a time sensitive project and so I am looking for an alternative to perfmon.... and any advice. I suppose the cause of the empty counters belongs on serverfault - but I would appreciate a solution if anyone ran into this issue and solved it. 

Everything else is pretty much default. And yet I have 1600+ files in the archive directory! Because of space constraints I have dared to delete some of them. I would like to figure out what is causing this. I also see that there are only very old files on the slave for the same directory. The only thing I can think of is that I recently did a restore on a database. I dropped the database, created it and did a backup restore. I did this several times. The slave seems to have caught up with the changes. But I have a ton of WAL files in the archive folder for each of the backup restores. Would appreciate some insight into what could be happening. 

I would like to add non-clustered indexes to my Publication for Transactional Replication. I am aware that replication works by reading the transaction log for the published database and sends the updates to the subscribers. My question is: does transaction log include updates to indexes and therefore would this double the updates sent to the subscriber? Will it send updates to subscriber when I run Index maintenance ie: Rebuilding/Reorganizing indexes? If so, then perhaps creating non-clustered indexes as part of post snapshot script application is a better way. If not, then I prefer to keep it as part of Publication. 

I am new to postgres, I have a 20 million row table that is on a live server - I need to remove most of the rows but not all. I want to do this without impact to other read/write processes accessing this table (very frequently). I have a way to delete in about 100-400K row chunks at a time. Between each delete, I want to make the query sleep - so that other operations can get a chance to access this table. I have the code, but I believe in this version, it locks the table the entire time the query runs (with all the sleeps). How can I actually release the table while the process sleeps? Thank you!! My code so far: 

I have two Postgresql 9.5 servers configured in streaming replication. There are no errors in log files and the servers seem to be in sync. MASTER postgresl.config 

I have shut down SQL Server services and renamed the files to something different and restarted the SQL Server. I was then able to take the database offline. And then I was able to drop it. I restored the database from Primary and was subsequently able to set up mirroring. I still do not know why Error 3456 happened during OS losing network connectivity. If I find out (which I hope I do) I will update this answer. Thanks everyone who tried to help. 

I am wondering what are preferred methods to copy a new (Development) database into a new environment. The database will be empty, so essentially I am copying only the schema of this database and not the data. Options include: 1) SSMS (Backup/Restore). I do not use this one because it will copy test data into Production. 2) SSMS (Generate Scripts) 3) Writing T-SQL myself 4) RedGate (or other 3rd party tools out there)