The means all the columns of the target dataset. The target dataset in your case is a CTE which, apart from normal columns, returns a calculated column. The syntax includes the calculated column too but referencing a calculated column is not allowed in that context. Listing all the required columns explicitly resolves the issue. And you can use the filter on that UPDATE (but not inside the CTE). The reference will be valid everywhere apart from the OUTPUT clause and on the left side of an assignment in the SET clause. Here is a demo, which shamelessly borrows ypercubeᵀᴹ's setup as well as his UPDATE statement. 

The parameter list in can have names matching the names of the parameters used by the procedure itself, like or , for instance. There would need to be a way, then, to determine whether a parameter pertains to the system SP or to the user script. Making it a rule that be always specified last in the list of the parameters pertaining to and all the parameters specified after be considered pertaining to the user script is one easy way to implement the distinction. So, if a parameter like is specified after , it will be interpreted as a script parameter and not the SP's parameter. And I believe that is what is happening in the cases where you are specifying or after . Those parameters simply must go before to avoid potential ambiguity. Granted, there is not a word about this in the manual, which, however, is a problem with the manual more than a problem with . 

It is fairly straightforward with scalar functions, although your description is very confusing and hard to follow with regard to function names and parameter names. Anyway, when you have a scalar function and a table of values that you want to pass as arguments to the function one row at a time, you can go like this: 

The LEFT() trick simply cuts off the bit from each name, leaving just the verb. Another way would be like this: 

An EXISTS predicate is often faster than an equivalent join, though, so you might prefer the former option, but test in your environment to see which is actually better. If the user is specified by ID instead of a name, the query becomes simpler in each case. The EXISTS predicate would look like this: 

causes SQL Server to stop executing the subsequent statements until the end of the script or until is encountered, the latter setting the execution back on. 

I have replaced with as that seemed an obvious mistake (and may have been the cause of an error message very similar to the one you have posted). On a different note, the way you are calling the stored procedure: 

Another way would be to insert the input values into an actual table and use that table in the query. Whatever way you represent the input, the next step will be to anti-join the to the input table. There are various ways of implementing an anti-join. One very common method is using NOT EXISTS: 

As a possible workaround, if there was a rule with your company to always prefix column names with table aliases, a query like yours would never compile: 

As you can see, the first common table expression (CTE) merely returns all rows and adds a calculated column , the one subsequently used for grouping/partitioning. So it essentially just assigns a name to the grouping expression, and that is done for better readability/maintainability of the entire query, as the column is later referenced more than once. This is what the first CTE produces: 

You can see that the first 'in' range repeats three times. That is because the first 'in' row lends its quantity to three 'out' rows. You can also see that the last 'out' range is repeated too, meaning it borrows from both 'in' rows. Now that ins and outs are successfully matched, the big issue is to correctly determine the quantity that an 'out' row borrows from an 'in' row in case it matches more than one 'in'. There may be variations on the logic to use, here is one: 

You could also add a filter to the SELECT to ensure that NULLs are skipped (if you want them skipped, that is): 

Based on your joins, it appears that DB rows are going to be unique, so you probably do not need in that specific instance. If the output should reflect the number of actual databases, counting distinct names can give you a skewed result, since different instances might have databases with identical names and would see that as a single item. On the other hand, there would probably be no issue if you counted IDs rather than names: 

– All right, prepared statements can be used in stored procedures. Can we create a stored procedure that would use a prepared statement to do the job, and call the procedure from the trigger? Nope. A stored procedure cannot reference NEW and OLD columns – those namespaces are available only in triggers. The only option appears to be an statement comparing against a fixed set of names and assigning values to corresponding columns. 

And it is indeed possible to do so in SQL Server, because the result of the above SELECT query can be used as the target of your UPDATE statement. You can use it as a derived table: 

So, again, if you want the output to show same rows grouped together, add the column as the first sorting criterion to the final ORDER BY as well: 

PIVOT implies grouping, which is why you can see the MAX() aggregate function applied to rather than simply using . The implicit grouping is also the reason you often need to use a derived table before applying the PIVOT operator: all columns not mentioned in the PIVOT clause actually participate in grouping, so you need to exclude those that are not supposed to be grouping criteria. For your specific example you do need a derived table, because you need to exclude . Option 2 As an alternative to the PIVOT operator, you could try the old-school conditional aggregation method: 

In order to suppress repetition of the same long expression, you can use CROSS APPLY to calculate the month date once and reference it both in GROUP BY and in SELECT, like this: 

The exclusion part may seem counter-intuitive – after all, you are intending to get monthly data. However, you should keep in mind that in SQL you are grouping rows. In your case, one row is one client – therefore, grouping should be by client only. You could say that monthly grouping is implicit, as it is implemented through conditional aggregation only. Anyway, what about the last row? The last row is special, and not just because it is a rollup row, thus representing aggregated data over the entire set. In my view, it is special more because it contains entirely different data: counts instead of concatenated strings. Based on that fact, it seems natural to me to consider a distinct logical step – a separate SELECT – for getting the last row's results. The two result sets would then be combined into one with the help of a UNION ALL operator. This approach would make the logic clear, in my opinion: different kinds of data in the output would be accounted for by different legs of the query. And clear logic ultimately means ease of maintenance. Solution So, taking into account all of the above, the complete query could look like this: 

That would still be aggregating across multiple tables, but using only columns from the same table as grouping criteria might make the resulting query more efficient. All three options are available in a live demo at dbfiddle.uk. 

And the output should be the list of all the articles belonging to the given group provided either of the following is true: 

It appears you want , which gives you the count of unique values in a column – seems to be exactly what you want. 

Alternatively you could be returning just the word and just a single row (using ) in each case, substituting a for a possible null result with IFNULL or COALESCE: 

They could have a custom-made sequence implementation in the form of a dedicated table with an integer column to store the current sequence value. Their inserts could be wrapped into a transaction and go like this: 

You could also try replacing the predicate with a disjunction to see if it can result in a more efficient execution plan for the entire query: 

The in the above script is the input string passed as a parameter. It would probably be best to implement the script as a stored procedure. An example of such a procedure can be found and tested on SQL Fiddle. 

You could try adapting Aaron's SP by getting rid of its dynamic part. The dynamic part is supposed to build a query reading just the database names from based on the arguments supplied. The dynamic SQL is chosen to make the query most efficient – as well as maintainable. Taking into account your specific needs, some sacrifices might be in order. I would argue, though, that the performance might not suffer much from the rewriting I am offering below, as the system view usually does not have very many rows, but in any event you could add at the end. However slow it may be, though, it is likely to end up rather ugly, that I can promise. The method of rewriting is as follows. Aaron's procedure is building the query using a repeating pattern where a parameter value is checked and, based on the result, an additional query is added to the dynamic query, i.e. like this: 

where in table would be the primary key of that table and same-named column in tables and would serve as a reference (foreign key) to the corresponding row in . This way, when querying against individual items of either list as belonging to that particular row in , you would probably use a join. Your typical filter conditions would be as simple as or – not or anything of the kind. The conditions would be simpler because each column would now contain one value per row. That will lend well to further optimisation of performance, because when you query often against a column value (without applying a function to it first, that is), you can make such queries faster by adding an index on the column – something that would be pointless for lists stored as a single value. More can be said on this topic, which would, however, be beyond the scope of your question. I suggest you look up the terms I highlighted in bold for more information. 

Finally, it is also possible to move the calculation into the same scope where , etc. are calculated. You can use CROSS APPLY for that and thus avoid the need for nesting a query entirely – in other words, this way you can get rid of the CTE as well. The entire query would look like this: 

Same continues for the rest of the joins. With your example, however, there will be no more rows, as the hierarchy in the table doesn't go deeper than two levels. Consequently, the other two columns in Chris's original query, and , will return NULLs: 

If you want athletes who have a Negative test coming after at least one Positive test (not necessarily the last one), you could try a different approach: 

And this is a variation that uses CTEs, if you are using a SQLite version that supports Common Table Expressions: 

You can see that the list only has the column names. To make this easier, I would further suggest you use short aliases for tables, like this: 

The values and are calculated for every row but used only when the description matches any of the patterns specified. The is the position two characters after the position of the in the token, and is the first slash encountered starting from – in other words, the slash immediately after the token. In case the description does not have an token, the values of and will make no sense. However, as already explained, they will not be used in that case but the ELSE clause will work instead. 

The rows may be returned in a different order if you omit an ORDER BY on the SELECT statement itself but the numbers will still match the respective period lengths. If you prefer proper labels, like instead of just , you can further format the output of , for instance like this: 

I am not entirely sure but if the aggregated results are supposed to be per date and the date is , then perhaps this: 

As you may have noticed, the difference between the two ranking numbers is constant throughout the particular group of consecutive event occurrences of the same kind ("attended" or "missed") and is also unique for that group within its partition. Therefore, every such group can be identified by , and the just mentioned difference. And it now remains simply to filter the events leaving just the missed ones, group the rows and get the necessary aggregated data, like the number of rows and, possibly, as in the query above, the dates of the first and the last event occurrence in the group. The number of rows is also used in an additional filter condition to omit groups with fewer rows than required. 

What you are trying to do looks very much like a pivot operation. You could probably achieve the result you are looking for without a derived table, using conditional aggregation: 

For the sake of completeness, you can also query sys.foreign_keys instead of sys.foreign_key_columns to find tables that are both unreferenced and not referencing others: 

If by distinct combinations you mean distinct sets of numbers, regardless of their order (in fact, sets are unordered, according to the algebra of sets), then the only fix your script requires is to replace each with either or , by which I mean that it should be either all or all . After you switch the comparison symbol, some comparisons will become redundant, and so your query can be simplified. Here is one way to rewrite it: 

Note how , , , and – the columns that you really need for the final set – are simply repeated across all rows that belong to the same group. That means that you can get rid of and , rename to , slightly rearrange the columns, apply DISTINCT to the resulting set – and you have eliminated the last SELECT: 

Here is how the expression works. If the current row's is equal to , the condition evaluates to and the other, , of course, to . In the context of an arithmetical operation (), is implicitly converted to 1 and to 0. So, the expression becomes equivalent to this: 

You can separately count items with and without discount per SKU, then compare the two results and return the rows where the counts are greater than zero. If you wanted just the SKUs and their prices (provided that the price is the same for the same SKU in this table), you could do all the job in one go like this: 

In this case the action needed is raising an exception, but the message returned with the exception needs to depend on what condition was checked first. The assignment statement uses a CASE expression to choose which message to store in the variable. You can also see that the error is raised only conditionally – only if the variable actually contains a message to show. If the value is an empty string or a null, the script will just continue without interruption.