That diagram includes a CPU block titled "wait for GPU". I do not see the part of your code that includes an equivalent command. Furthermore, even if it did include //etc, there is no guarantee that the CPU time in any way relates to how much time the GPU spent processing it. might block on a mutex, thus ending the thread's timeslice. Which means that the thread will start up sometime after that mutex is released. How long after? Who knows, but your call will include that time. If you want to measure GPU time, you should use tools that actually measure GPU time. For example, timer queries. You bracket some commands with and . Then you can read the result after some time has elapsed. Like maybe next frame. And unlike , this will not absolutely murder your performance ;) 

It means that multisampling is resolved. A multisample framebuffer contains multiple samples for each pixel. The combination of samples represents the color of that pixel. The process of converting from multiple samples to a single color is multisample resolving. By blitting from a multisampled buffer to a non-multisampled buffer, you are telling OpenGL to resolve each pixel's worth of samples into a single sample. Typically, this is done by adding all the sample values up and dividing by the number of samples, but implementations can take weighted averages or something else, depending on the precise nature of each sample. Generally speaking, it's best that you let the implementation do its job. 

I've been trying to build a simple DirectX renderer from the ground up (was an OpenGL guy before but i figure it's good to know different APIs). I've been following the guide at rastertek.com to learn how to get the basics up and running. I got as far as this lesson, following the guide verbatim except for variable names, and ran into a really weird bug. The renderer class contains 3 XMMatrix instances, world, perspective and orthographic, but trying to initialize them would crash my application - it ran fine once i commented out these lines : 

The information you're looking for is defined in the 'accessors' and 'bufferViews' near the top of the source file you linked. Bufferviews simply divide the buffer up into sub ranges and define broadly what kind of data lives there using some obscure shortcodes. In this case, target 34963 means index data and 34962 means vertex data. So from the other parameters in the bufferviews you can see that the first 72 bytes are indices and the remaining 576 bytes are vertex data. The accessors are what actually define the format of the data and again use weird codes for "componentType" - 5123 is an unsigned short (2 bytes) and 5126 is single precision float (4 bytes). They also define whether they should be read singly ("SCALAR") or in vector groups (e.g. "VEC3"), and also a starting offset into the bufferview and "stride" between the start of data points like regular OpenGL vertex buffer bindings. Putting it all together, the first 72 bytes are 36 shorts representing the indices (3 * 12 triangles). The next 576 bytes are two consecutive sets of 24 vec3 (4 per square face, 12 bytes per vec3) representing vertex data. Since the first set are clamped to + or - 1/2 and the second set to + or - 1, I'm guessing first set are positions, second set are normals. The base 64 encoding just takes the raw bit stream, splits it into groups of 6 rather than 8, and maps each unique 6-bit combination onto a character. You can see this is quite wasteful as each character takes at least 8 bits to store, quite possibly 16 or more depending on the encoding used. 

BRDFs are functions the define the distribution of the way light reflects off of a surface. They effectively answer the question, "given a light source here, how much light is reflected to me?" Well, that question has the two directions right there: the direction from the surface point to the light source, and the direction from the surface point to "me": the viewer. BSDFs and BTDFs work in the same way, just asking slightly different questions. But the givens to those questions are the same: a light source and a viewer. And thus, they use the same directions. 

Redraw everything with different FBOs. You can't just use different settings, because you also need to change the depth buffers between the two draws. Use layered rendering. Your two textures would instead be one texture object with two array layers (the same for the depth texture). There are multiple ways of going about this: 

Note two changes. First, the internal format is , which specifies a 32-bit-per-channel signed integer format. The second is the use of for the pixel transfer format. Even though you're not actually performing a pixel transfer (since you're passing NULL), you still must use an appropriate format and type. And the formats must be used if you're transferring to/from a texture with an integer format. When you do this, you must of course use the and types in your shaders. 

So which is it, or is there another possibility I've missed? If it's option 2, is there an option to pass textures 'verbatim' if you already have a Morton-coded version on CPU-side? Or would you have to de-swizzle it before loading to GPU? 

Promoting my comment into an answer since it identified the problem... The issue is confusion between degrees and radians for the angle parameter. GLM, up to version 0.9.5, was very inconsistent in how it dealt with angles, accepting degrees for some functions and radians for others. This behaviour could be overridden with before including the header. In version 0.9.6, radians everywhere became the default. Unfortunately, if you Google 'glm rotate' at the time of writing, the top result is the 0.9.4 doc page, which still describes the parameters as taking degrees, so you need to be aware of which version you are using and its default behaviour in this regard. If you still want to use degrees for your angle variables, you can convert them with before passing them as arguments. 

HDR rendering helps a lot, but unfortunately our project is on mobile and using LDR. I'm making a car paint shader with a clear coat, so the base coat will likely be semi-rough and semi-metallic in a lot of cases which seems to be a worst-case-scenario for this BRDF. Can anyone suggest a way to get saner specular highlights? Either some kind of rolloff for out-of-range specular values, or a different BRDF altogether? 

Because the fundamental foundation of OpenGL makes multi-CPU-core submission impossible. OpenGL, at its core, is a synchronous API. Every OpenGL command behaves "as if" it were executed and completed immediately. Sure, and are provided, so that implementations can have some asynchronous execution of commands. But these are essentially fig leaves over the definition of OpenGL as synchronous. You can attach a texture to an FBO, bind that FBO, render to it, unbind that FBO, bind the texture as a source, and read from it. You can do all of this without any thought as to the fact that you've issued two sets of commands where there can be zero overlap between them. This is because OpenGL implementations are required to issue whatever synchronization mechanisms are needed to make this work. You as the user can be completely oblivious to this.1 Because of that, consider what would happen if you could issue that render-to-texture in one thread and the read-from-texture in another. How exactly would that even work? You would need some way to say "don't execute the read until the write is done". But since the two sets of commands are on different threads, how do you say that? OpenGL has no mechanism for doing so; the closest it gets would be some form of fence sync. And even that would require that the render-to-texture thread has already finished sending those commands, since issues the fence actual fence (you cannot create a fence that has not yet been submitted). Which means that the render-to-texture thread may as well send the read-from-texture commands too. To make OpenGL work asynchronously means making lots of adjustments to its memory model. You'll have to add lots of commands for making things visible to other threads. For allowing thread A to not have its commands execute until thread B is finished. You'd need to make command buffers and queues first-class constructs of the API. And so forth. And you have to do it in a way that's backwards compatible with the existing memory model. Unless you want to go through another core/compatibility split again. NV_command_list shows the challenges of trying to build this sort of thing into OpenGL. They force you to use ARB_bindless_texture. They had to invent bindless buffers, so that you can provide vertex arrays, UBOs, and SSBOs. And even then, it's not something that AMD could even implement. Why? Because of image layouts. NVIDIA's hardware has fewer layouts and handles transitions automatically. AMD's hardware requires explicit control of layouts. Once you break the synchronous memory model, that implementation detail of what layout a texture is in becomes really important. You have to have control over it. You have to explicitly state it in various API, and you have to know to change it when you use a texture in a different way. This is also almost certainly why AMD doesn't implement D3D11 deferred contexts. Or you can do what Khronos did: make an API that naturally handles all of this stuff. A clean break is ultimately best for everyone. It allows us to jettison other garbage that OpenGL had, so that we can have a nice, clean, lower-level API. 1: While you can be oblivious, obviously you shouldn't be oblivious if you care about things like performance. You should put some work between the render-to-texture and the read from that texture, if at all possible. That's one of the biggest problems with OpenGL's synchronous-by-default API: it makes it easy to do things the slow way, since the API itself doesn't force you to issue that synchronization. In Vulkan, you cannot be ignorant of that synchronization. Or at least, not without your code invoking UB. 

This method will work but will involve a huge amount of redundant computation generating, checking, and discarding duplicates, especially in symmetry groups like the 120-cell / 600-cell which contain thousands of permutations. Does anybody know of a more elegant method of creating the full set? 

My best guess is that this is some form of the Mach effect, and our retinas are overshooting where the brightness plateaus. Does anyone know any tricks to mitigate this effect? EDIT FOR CLARITY: Here are two more images to illustrate the effect better - one with my shader, and one with Unity's diffuse shader, with the lights at 90 degrees and all specular and ambient contributions removed. 

Likely a gamma correction issue. Tertiary colours are some permutation of (1, 0.5, 0) in RGB, right? So the issue arises because without applying gamma, 0.5 appears less than half as bright as 1, so the blended colour skews towards the primary (1,0,0) rather than the secondary (1,1,0). The image below has linear colour on top, gamma corrected on the bottom, you can see the gamma corrected colours appear much closer to a halfway tone. 

works! The right-handed version also fails. Also, the failure only happens in release mode, not in debug. I'm using Visual Studio 2015 community and the SDKs that installed with it. Has anyone else encountered this bug, or have any idea what's going on here? 

There is a difference between "can" and "should". You "should" not use to change the size of an existing non-immutable buffer. You can still do so, but don't expect this to be advantageous. You "can" not change the size of an immutable buffer with any function. 

How could it? At no point do you inform OpenGL that you only want 900 invocations. You have exactly 2 mechanisms to control the number of invocations: the work group size in the shader, and the work group count in the dispatch call. That determines how many invocations there are. If you want a variable number of invocations with a fixed work group size, then your shader has to be given the actual maximum number of operations, and you have to explicitly not do anything for invocations beyond that size. 

You can map buffers for reading; invalidation is negatively useful for that. Also, you can map a buffer and only overwrite part of it. Invalidation is negatively useful for that too. has to be able to work outside of the narrow use case of mapping for overwriting an entire range. 

There is nothing in Vulkan which guarantees or requires this. So even if it were a safe assumption today (if it were enforced by Windows in some way), all it would take would be one Windows update to break you. It's best to work with what you're given, even if what you're given may be more than you would prefer. 

This structure allows you to extract all sorts of connectivity information from a mesh, such as which edges or polygons lie around a particular vertex, simply by traversing the half-edges. There's a good article explaining it here. 

I can see a few things wrong here. First, you're setting your GL buffer size to This looks wrong. What is vertices? A std::array of vec4? Then you want , or just Second, as wandering-warrior said, you want to be binding to before calling Third, I don't know what you're doing with in the second part of the code. This is a valid approach if the data is laid out sequentially in one large buffer (vertices then normals then tangents etc. etc.). But in your case you're using a separate buffer for each data set - so each attribute should start reading from the beginning of its own buffer, i.e. should always be 0. Fourth, you don't seem to be actually using your compute shader anywhere. It's not enough to just call , you need to call after you bind your buffers to actually run it. Also, it's not common practice to store binormals in an array, since they can be reconstructed in the fragment shader from the tangent and normal using a cross product. I would do it something like this (using OpenGL 4.5 with the new Direct State Access functions to save all those buffer bindings). 

There is nothing in the OpenGL specification that requires you to have enabled attribute arrays in the bound VAO when you render. All of the user-defined vertex attributes will have undefined values, but since you're not reading any of them, that won't be a problem. What may be a problem is driver bugs. Rendering with an empty VAO is not exactly the most common thing in the world, so it's possible that some drivers won't do the right thing. But the implementation should allow this. 

What's curious about this is the "all memory accesses" part. This specific wording is only used in two other bits: shader storage and shader image. Other barriers for coherent operations like query buffers and transform feedbacks tend to use the term "all shader writes". So... I would say that there is confusion at the specification level. 

Render the scene twice, using a variable of some kind to determine which layer gets rendered to. This means having to read and execute the VS on all of the vertex data twice. Render the scene once with Instanced Rendering, using to determine which layer that particular rendering gets rendered to. This means having to read and execute the VS on all of the vertex data twice. Render the scene with a Geometry Shader that spits out two triangles for each triangle it gets, one for each layer. This only reads the vertex data and executes the VS on it once. Render the scene with Geometry Shader instancing. The GS will be executed twice for each primitive, with a different value specifying which instance the GS is being executed for. You use that ID to determine which layer to send the primitive to, as well as how to transform it. This only reads the vertex data and executes the VS on it once. 

Your GPU does support OpenGL 4.5, basically. Mine shows exactly the same result, all features supported except the GLSL version. This seems to be fairly common, I'm not sure what the delay is in updating the shader language support, or what GLSL 4.5 brings to the table. In order to say a GPU fully supports a GL version, it has to support all the features specified by the ARB for that version number, which is why your driver and mine show up as only supporting 4.4 despite having the full 4.5 feature set on the CPU side. In practice, vendors can add part of a feature level to their driver without fully supporting the version number. It's all pretty messy, which is why we need things like GL3W or GLAD in the first place. 

Spherical harmonics are generally used for dynamic objects in your scene, while fully-baked lighting is used for static objects. A typical game engine will use both. During the GI pass, all surfaces marked as static will have their global lighting fully baked - generally into a lightmap texture rather than per-vertex. At the same time, a bunch of light probes distributed throughout your scene will receive omnidirectional lighting, and this gets converted into SH. Since SH coefficients interpolate nicely, a mesh moving through your scene can pick up the nearest few light probes (generally 4), and use the weighted average to decode a good approximation of the ambient lighting at its current location. Another use for SH is to render less-important dynamic lights in a scene with many lights in it. The most important light or two can be rendered fully dynamically per-pixel, then for each renderer, the SH coefficients of the less-important lights are calculated and added to the light probe data. This allows the lights to change in real-time, and be rendered effectively for free per-vertex in the shader, with a small CPU overhead.