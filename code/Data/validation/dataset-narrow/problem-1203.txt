Suppose I have a DAG with non-negative edge lengths. The problem is to compute the minimum number of edges which disconnects all paths of length L or less. We call this L-length bounded minimum cardinality cut problem on DAGs. It can naturally be extended to all directed graphs. There can be a further generalization where removing an edge has some associated cost. The L-length bounded min cut problem is finding the set of edges with minimum total weight that gives a L-length bounded min-cut. The L-length bounded min-cut problem is known to be NP hard. In fact for a given budget it is NP hard to decide if there exists a length bounded cut which does not exceed the budget for series parallel graphs [1]. So it holds for DAGs. In this paper [1] it is showed that for directed graphs L-length bounded minimum cardinality cut itself is NP hard to have an 1.1377 approximation. 1) Does there exist NP hardness result for L-length bounded minimum cardinality cut on DAGs? Or, 2) Is there any polynomial time, w.r.t. the number of edges and vertices, algorithm to construct the L-length bounded minimum cardinality cut on DAGs? References: 1: Baier, Georg, et al. "Length-bounded cuts and flows." Automata, languages and programming. Springer Berlin Heidelberg, 2006. 679-690. 

Consider a the splittable minimum cost flow problem on network $G(V,E,W,C)$ and a set of commodities $(s_i,t_i)$ with demand $d_i$ for $i=1,2,\dots,k$. Here, $w_e$ and $c_e$ is the weight of edge $e$ and the capacity of edge $e$, respectively. My question is what happens if we scale the demands and the capacities linearly but decrease the weights inversely. Consider the same problem on $G(V,E,W/\lambda, \lambda C)$ for the commodities $(s_i,t_i)$ with demand $\lambda d_i$ for $i=1,2,\dots,k$, where $\lambda$ is a constant. For each $\lambda$ we get a solution $OPT(\lambda)$. I am interested in the minimization of $OPT(\lambda)$ over $\lambda> 0$. If someone can point me to the existing literature it will be helpful. 

Consider the second problem. Note that this set is of the form: (all nodes of left subtree in some order) (all nodes of right subtree in some order) - i.e. the subtree ranges don't intersect. So we only have to find the boundary, which, given it's postorder traversal, should be the root of the left subtree (it's the last thing processed from the left subtree; all that follows is the right subtree). But note that the root of the left subtree is the item with the smallest preorder index. So this means that in "3 4 2 7 8 6 9 5", the left subtree is rooted at 2, and the problem splits into "3 4 2" and "7 8 6 9 5". Recursively, we get a tree: 1 { left = 2 { left = 3, right = 4}, right = 5 {left = 6 {left = 7, right = 8}, right = 9}}. P.S. Note that in this form the algorithm is O(n log n), but it's easy to make it O(n) - just remember in a lookup table (obviously in O(n)) for each element "x" of the array, the position of x+1 - then, whenever you pick x as a root, the left subtree ends at the position of x+1. 

Well, here's the algorithm: Consider the postorder traversal and replace each node with its index in the preorder traversal. For example, let's say you get: 3 4 2 7 8 6 9 5 1. From here you can see: 

There are also some interesting ways of implementing already known datastructures, such as using "nested types" or "generalized algebraic datatypes" to ensure tree invariants. Which other new ideas have appeared since 1998 in this area? 

(eg is there a complete problem related to that, or does KSS result relate to factorization over finite fields etc?) 

another area that seems loosely similar is the use of artificial neural networks to solve SAT. again there is a concept of an algorithm with continuous variables that converges to a solution under update rules expressed as differential equations. see eg 

the following problem/question seems fundamental/hard. it appears in some circuit theory proofs, graph theory, and maybe elsewhere. looking for any nontrivial insight. will add various known/nearby diverse refs if there is sufficient interest. consider the problem of monotone k-CNF ←→ k-DNF conversion by minimizing errors where $k$ is the maximum width of clauses (number of variables in the clause). let $f(x)$ be a k-CNF formula where $x$ is a bitvector (denoting the boolean values of all the variables). there exists a k-DNF "approximation" formula $g(x)$ that "approximates $f(x)$" by minimizing total errors under the following metric. evaluate $f(x)$ and $g(x)$ on every $x$. if $g(x)=1$ while $f(x)=0$ then count one "false positive". if $g(x)=0$ while $f(x)=1$ then count one "false negative". the total errors are the sum of "false positives" and "false negatives". another way of looking at this "natural" error metric is that its the Hamming distance between the two bitvectors of size $2^n, n=|x|$ composed of all "corresponding values" of $f(x)$ and $g(x)$ (eg function value column for both functions evaluated on the same truth table). there is a dual version k-DNF → k-CNF approximation in the opposite direction that has mirrored theory. 

an inexactly phrased but reasonable general question that can be studied in several particular technical ways. there are many "small" machines measured by states/symbols where halting is unknown but no "smallest" machine is possible unless one comes up with some justifiable/quantifiable metric of the complexity of a TM that takes into account both states and symbols (apparently nobody has proposed one so far). actually research into this problem related to Busy Beavers suggests that there are are many such "small" machines lying on a hyperbolic curve where $x \times y$, $x$ states and $y$ symbols, is small. in fact it appears to be a general phase transition/boundary between decidable and undecidable problems. this new paper Problems in number theory from busy beaver competition 2013 by Michel a leading authority exhibits many such cases for low $x,y$ and shows the connection to general number theoretic sequences similar to the Collatz conjecture. 

See "interval labeling" and "2-hop labeling" which are apparently quite efficient in practice, both in time and space, and may give you what you want. In general there's quite a bunch of "reachability indexing" schemes for DAGs. 

Perhaps "parallel or" (given two functions returning a boolean, tell whether one of them returns true, given that any of them, but not both, might fail to terminate) might be what you're talking about: you can't compute it with 1 processor, but can compute with 2. However, this much depends on which computational model you'll be using, whether you're given the processes as black boxes or as their description which you can interpret yourself, etc. 

I recently learnt about the concept of dominator trees and was fascinated by it. I was wondering how the problem extends to computing dominators from multiple sources, or even from all vertices in the graph as sources: can all-sources dominators be represented more compactly than in O(n^2) space and computed more efficiently than running the classic dominator tree algorithm from every vertex? I tried searching for keywords like "multiple sources dominators" etc, but only found about "multiple-vertex dominators", which are a different thing (dominating sets of size > 1) Does any of this become simpler if the graph is acyclic? (in fact, I work at Google and I think it'd be fun to apply this to the huge dependency graph of Google's build targets) 

What useful algorithms do there exist that work on huge data streams and also their results are fairly small and one can compute the result for a mixture of two streams by somehow merging their results? I can name a few: 

Suppose we're sampling a discrete random variable from a distribution f, n times. Is there a simple analytical formulation for the expected number of unique items we obtain, or for the distribution of this number? Is there a name for this characteristic? This question came up in a practical task: I have a stream of items arriving, every item having several Zipf-distributed parameters, and I'd like to answer queries like "how many items had a parameter equal to x" - I was wondering how feasible it is to just count each parameter value's occurrences (obviously, the space required is exactly what I'm asking about). 

Lance Fortnow recently expanded and published his already-comprehensive column from CACM (mentioned in MA's other answer) into a full length popular-science level book, The Golden Ticket: P, NP, and the Search for the Impossible. it was reviewed in the New Yorker, "A most profound math problem", by Nazaryan. (publishers page, Princeton University Press) 

there are many answers to this cited elsewhere, here is one additional one that maybe captures the boundary between P/NP in a natural way and fittingly along the lines requested. a distinctive/ interesting model is the $(2+p)$-SAT model for $0 \leq p \leq 1$ inspired by research relating to phase transitions and statistical mechanics. the idea is that instances have a count of 2-width and 3-width clauses in a mixture wrt the $p$ ratio. two papers: 

as far as I can tell after looking into this somewhat, there is a misunderstanding of the paper in the question. the question states that some of the algorithms are offline vs some online. but the algorithms in the paper are all online it appears (but agree the authors are not clear on this point). that is what is accomplished by having random edge orderings-- the random edge ordering reflects edges coming "online" in whatever order, right? the authors seem to make no reference to "offline" algorithms in the paper and they refer to "online" only in the introduction that I see. they dont state this but the 1st algorithm seems to allow edges to come "online" in "batches" (ie random reordering in stages) whereas the 2nd algorithm does not. while you dont state it exactly you appear to be checking the math in proof for theorem 2.3 in the paper (p3) which establishes a lower bound on the number of colors in a tree and the authors assert it works for both algorithm 1 and 2 ("for the 2nd algorithm.. the same bound holds"). the proof is that no less than $1.23\Delta$ colors ($\Delta$=graph degree) are sufficient for all trees. it seems if this theorem is wrong as you are suggesting then one should be able to exhibit a tree that can be colored with fewer than $1.23\Delta$ colors using those algorithms. from what I can tell your math is off because you are not expressing it using the same formulas in the proof eg you do not calculate the exponential formula in the proof. a way to empirically verify this lower bound would be to generate many random trees, or some sample from some distribution, run the two algorithms and verify that they always use at least $1.23\Delta$ colors.