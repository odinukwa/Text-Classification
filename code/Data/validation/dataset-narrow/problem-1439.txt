I'm trying to make a cinematic platformer, in the veins of the first two Oddworld games, Flashback, Prince of Persia, Blackthorne and so on. This article describes very well how this kind of movement should work: 

Alright, after more digging I finally came across a more definitive solution. I posted the solution in my forum post on the issue on cocos2d-x's forum, but I'll post it here too just in case. So the real issue is that on iOS devices, device orientation isn't reported correctly, and on Landscape mode, the screen coordinates will have the width and height values swapped. To fix that, I basically used the solution described by Tim Closs in this forum post for rotating the projection matrix, with minor tweaks: New method in AppDelegate: 

The only thing that should be noted with this approach is that ideally, you need a fixed timestep in your updates. I think the "Constant Game Speed with Maximum FPS" solution in this article would work pretty well. 

One funny thing is that I tried to deploy the TestCpp sample project to some iOS devices and it worked reasonably well on the iPhone, but on the iPad the application was only being drawn on a small portion of the screen - just like what happened on the iPhone when I tried using the ShowAll policy. 

Fourth there is also something to be said about the effectiveness of lists, especially in a garbage collected environment, but that varies a lot depending on implementation details. Fifth You can handle your collision phase differently if you don't care if objects intersect for a few frames, by just letting them gardually push each other out by applying force. It looks like you are trying to guarantee zero intersect, which you are currently not doing. (An object can be moved out of the first collision and then back into it when resolving the second collision) As a last sidenote I think that simulating n-body attraction is just always going to be a performance nightmare, so in some way you don't really need to worry about this rest, since that's what will most definitely ruin your performance. (Although space partitioning can also aleviate that a bit since you can only check with objects that are close enough to matter) I probably didn't notice everything but I think that's the jist of it. 

The short answer is yes. The long answer is that the specifics of how to do it massively depends on how you handle drawing your characters. If you use skeletal animations (like when using spine), then you can easily exchange bits of your character by just replacing the texture of it. (Since in skeletal animation systems your character is separated into parts anyway). You can do the same thing when you use sprite sheets too, but it is slightly more complicated. You already had the right general idea in your post. You split the animation up into the parts that you want to combine and you need to remember a draw order for all those parts for each animation. But then you can draw each frame of an animation easily by just combining the parts together. The drawback is that you need to make a full set of animations for each part, since every hat for example has to move the same in every animation, otherwise some combinations will just look weird. tl;dr: It is definitely possible and it is much easier if you use skeletal animation. 

There is another option. You could frame for an intermediate ratio, and do both cropping and expanding (or bordering) on both normal resolutions. I believe TV filming is framed in this way, such that widescreen looks ok (everyone isn't crowded into the middle of the scene), whereas cropping for 4:3 isn't quite as drastic and there's minimal panning. 

To convert a cursor position to a ray, you can back-project the 2D co-ordinates onto two planes parallel to the view plane. Typically you might use the near and far planes. Two points define a ray. This is simply a matter of doing the maths used to project a 3D point onto the screen, in reverse. Typically from world space you would multiply by a view and projection matrix to get into clip-space, divide by Z to do the actual projection, and then scale the resulting values into pixel-values. So in reverse you would scale the screen-coordinates into clip space, multiply by Z (taken from the plane you're interested in), and transform by the inverse of the view-projection matrix to get back into world space. However as you've noticed, most engines and libraries can do this for you... 

Limiting the number of bone influences is common, yes. You can either 0-weight unused influences, or have a loop/early-out mechanism to skip. As for whether it works for an RTS, I don't have a reference for you, but I imagine you're going to need LOD if working with a large number of characters on-screen, and also if those characters are small. LODing skeletal characters is much the same as LODing anything else, except you'll probably want to LOD the bone influences and skeleton as well as the mesh. For example, a low-ish level of detail might only use a single bone with the highest influence per-vertex (also known as "hard skinning"). You would probably also limit the number of bones in the skeleton for a low LOD model. Finally - consider whether you ever need to render the characters close-up. You probably only want to model, skin and animate the characters for the closest view distance... certainly you don't want to be storing all the runtime data at a resolution far higher than you'll ever render. You might find that you just don't need more than a very basic skeleton and a couple of influences per-vertex for your situation. 

I can't see anything fundamentally wrong with the shader, but here are a few things I find commonly done wrong with deferred shading that you might be doing. 1: Drawing full screen lights. The beauty of deferred shading is that you can pack your lights into geometry so that you only need to consider a part of the screen when drawing them (Like a cube with 2 times the radius as size). If instead you draw a full screen quad for every light, that will have a serious performance impact. 2: Render target depth. You are moving a lot of data around in deferred shading, so you have an interest in making the footprint of that as small as possible. If you can reduce the number or size of your render targets that can have a pretty big performance impact. 3: Rendering one light at a time rather than batching them. (Edit: You can have a static vertexbuffer and indexbuffer containing vertices for the max number of lights you want to ever render and then just patch the position and color of the lights that are active) And lastly, you can always look at the assembly of your shader and see how many cycles it will take, so that gives you an easy way to see your own performance and compare. GPU Gems has 2 or 3 great articles on deferred shading and the performance and other issues that come with it. 

Eventhough the idea seems very clear to me, the actual implementation of it raises a lot of questions in my mind. For some of them I feel I have very complicated solutions, while I have no idea how to solve others. For starters, the basic mechanism for moving: if the player presses right, the character will enter a "walking right". While on that state, the character will move from the starting tile until the next one, even if the player releases the right key; if the player is still pressing the key when the character reaches the next tile, the same thing will happen. However how exactly should that movement inbetween tiles work? Should it be just a regular speed being applied to the character's x position? What should that speed be in order to assure that the character will stop in the precise location in the next tile? And should that speed work with the update's delta time? Another thing that has me at a loss is how much of the sprite's positioning has to be done by the actual x,y positioning and how much should be done inside the sprite. For example, in the case of a running jump, which is, as noted, a special case of linear movement, should there just be empty space below the character (and position the sprite by its bottom) or should the y position be adjusted accordingly? I'd really appreciate some help with implementing this kind of platformer. That article above is the only thing I've found that sheds any light on how to do it, and I'm struggling to figured out the rest on my own. 

You could search in both directions - from the current tile to the nearest neighbour, but also looking to see if another tile has the current one as nearest in the opposite direction. Move to the closest of those two if different. 

You don't need collision detection, but you will need to give your enemies a bit of intelligence, and have them avoid running in to each other. Collision detection without that will just make your enemies look stupid anyway - people avoid each other, they don't generally collide. Look up "flocking" for some simple behaviours. The basic idea is that things should head towards the player, but away from a close obstacle (each other, walls, that kind of thing). 

You could try some kind of simple distance function in the pixel shader. Perhaps something like barycentric coordinates (store different RGB at the vertices and the rasteriser will do most of the work for you). Then your pixel shader will have the information it needs about where on a polygon the current pixel is - near a vertex, near an edge, or in the middle... I suspect in practice it'd be easier to texture it though. 

Interpolating the matrices like that isn't going to give a useful result, especially for anything involving a rotation. Ideally you want to decompose your animation into some other format (quaternion for rotation, for example) and then interpolate in that form before building the matrix. 

I'm using cocos2d-x alongside with Marmalade and running some tests and tutorials before starting an actual project with them. So far things are working reasonably well on the windows simulator, Android and even on Blackberry's Playbook, but on iOS devices (iPhone and iPad) the positioning seems to be off. To make things clearer, I put together a scene that just draws an image in the middle of the screen. It worked as expected on everything else, but this is the result I got on an iPhone: 

And regarding movement, I was thinking maybe interpolating the player's position between one tile to another, but he had this to say: 

(These articles originally have a linearity to them, and there's more articles in between those listed above in the series, but those that I listed are the ones that relate to the problem at hand.) The solution above deals with the scenario of balls colliding, so it isn't exactly perfect for cars, but I've found a decent way around that: I store the resulting velocity from the collision in a different vector than the velocity vector from the car. Then, the car will follow the direction of the of both the collision vector and the velocity vector. Just so there's some actual change to the velocity vector after the collision, I use the projection of the collision vector onto the velocity vector as the resulting velocity of the car. Here's the bulk of the resulting code: 

If you're dealing with polygonal models, you may well have to deal with non-manifold geometry, which means the question of what is "inside" and what is "outside" is not defined. It's tricky to perform a boolean operation if you don't know if you have a 0 or a 1. You also have to deal with fringe cases such as co-planar polygons, polygons which intersect edges, vertices which lie on edges and/or faces, and things of that nature. None of which is impossible, you just need a very robust way of representing your mesh data, and a tight definition of what you expect to happen in those cases. 

My generally feeling is always that learning to use something in the wrong context is not a valuable exercise. 

Well as a simple hack on your current technique, you could choose between two up-vectors based on whether your view vector is tending towards parallel with one. It'd just be an abs, compare, and select in shader terms, so not much overhead - especially considering the sampling and blending... 

As you highlight yourself, deferred rendering isn't a silver-bullet which solves all problems, and it also introduces new issues. So no, it's probably not the future, nor will it be used in all engines. Instead I think you're going to see a mix of technologies, including forward and deferred rendering used for some time to come. 

My best guess is that you have some rounding errors. You aren't actually showing the movement code for the bullets so it's really hard to tell. Edit: Looking at the code, it should(tm) work. It definitely works fine to do things like this in the games I've written with XNA. The thing that bothers me is that your game doesn't seem to be running at all when you set FixedTimeStep to true, so something really weird must be going on somewhere. Could it be that you didn't separate Update and Draw logic properly? Edit2: The TargetElapsedTime is set to 16.666 seconds instead of milliseconds, so that's most likely causing the issues. 

Genres are a way to group games together that share similiar elements, wether they are game mechanics or specific story elements or just perspective or setting. In some genres games have to have specific mechanics. An FPS has to be first person and has to have a shooting mechanic, hence the name. Mechanics are basically actions that the player can take and their consequences. So if you match 3 and they disappear, that's a game mechanic, if you jump on an enemies head and the enemy takes damage, that's a game mechanic. So to answer your question, genre can describe many things, it doesn't have to be the setting (see FPS). So it doesn't directly relate to game mechanics. A genre can imply certain game mechanics. But other than that there isn't really a connection. Hope that explains it well enough. 

The result will be that the object's vertices will first be scaled (along the local XYZ axes), then rotated (around the local origin), and then translated into "world" space. Then the world-space co-ordinates will be translated such that the camera is at the origin, and finally everything will be rotated around to the correct view direction. This is probably what you want for a basic scene. As Richard Fabian says, you generally want to consider the camera transforms as an inverse, though you might equally replace the camera transforms with a lookAt() function or similar, to directly construct an appropriate view matrix. If you push the matrix stack after setting up the camera, you can pop/push for each object without having to set the camera up again. 

I find it useful to build profiling in. Even if you're not actively optimising it's good to have an idea on what is limiting your performance at any given time. Many games have some kind of overlayable HUD which displays a simple graphical chart (usually just a coloured bar) showing how long various parts of the game loop are taking each frame. It would be a bad idea to leave performance analysis and optimisation to too late a late stage. If you've already built the game and you're 200% over your CPU budget and you can't find that through optimisation, you're screwed. You need to know what the budgets are for graphics, physics, etc., as you write. You can't do that if you have no idea what your performance is going to be, and you can't guess at that without know both what your performance is, and how much slack there might be. So build in some performance stats from day one. As to when to tackle stuff - again, probably best not to leave it too late, lest you have to refactor half your engine. On the other hand, don't get too wrapped up in optimising stuff to squeeze out every cycle if you think you might change the algorithm entirely tomorrow, or if you haven't put real game data through it. Pick off the low hanging fruit as you go along, tackle the big stuff periodically, and you should be fine.