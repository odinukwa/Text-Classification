as AS suggests there may be other more natural ways to study something like the question posed. here is another somewhat similar way to study growth of a regular language based on number of words of size $n$ which does have some loose relation to the question eg 

two examples based on a more broad interpretation of your question & the terms in it which are not strictly defined in the question: 

here is a similar/ more recent ref (2014) using the technique on probabilistic CFGs. (but they seem to skip over details of the SAT solver...?) Grammatical Inference of some Probabilistic Context-Free Grammars from Positive Data using Minimum Satisfiability / Scicluna, Higuera 

the Rosetta Code repository has been used for a new comprehensive scientific/ academic study of language succinctness among other comparisons of properties. announced at "Analyzing Programming Languages using Rosetta Code" 

not exactly what is desired yet similar to your example, consider CS G399: Gems of Theoretical Computer Science; Spring 2009 lecture notes by Viola. its more a proof-centric perspective however most are essentially advanced algorithms in key frontier research areas. (also note lower bounds proofs can be regarded as compression algorithms.) 

the phrasing of this question is slightly problematic because a Turing machine with a finite tape is arguably not much related to a Turing machine and closer to/ essentially a finite state machine. similarly with all other "restrictions" on Turing machines, almost any restriction seems to be a totally different phenomenon (ie other than Turing completeness with completely different properties). in fact some papers now call out/ study this boundary in detail and it may have some rough similarity with another famous computing boundary ie NP complete phase transitions. and its somewhat counterintuitive that "computationally simpler/ fully decidable" FSM theory emerged long after the invention of the Turing machine, presumably somewhat loosely inspired by it. so maybe one way to rephrase it is to ask for "most sophisticated decidable models" of computation or "study of the boundary between undecidable and decidable computing models". so anyway then slightly reformulated in this way, a reasonable answer/ theory/ research program not yet listed is the now significantly developed and actively researched/ advancing theory of timed automata which just won a Church prize for Alur / Dill. heres an example of a paper on timed automata and the study of the computation model (un)decidability boundary and there are many others in this vein. 

a slightly similar mapping issue arises in CA/Cellular Automata research where there are ideas about using infinite periodic patterns on the CAs as "starting patterns" to prove TM equivalence/completeness. so in general your question is not strictly defined until you clarify better (ie more formally/mathematically define) what you mean by "in a game with doors and pressure plates" and in a way that even the paper does not apparently strictly define, esp wrt to ideas about level design, unlimited size levels, etcetera. but notice that the "games" defined with these features then have been abstracted away from the actual/real video games in a very significant way. so in short I think this is interesting/worthwhile research, even though starting out as somewhat informal, and deserves further advancement, but to some degree its formalization must be made more strict esp in basic definitions if it is to advance further. it must make a more strict/formal/transparent distinction between the implementations and the abstractions. 

less complex examples: there are some theorem-like properties that show that greedy algorithms for some problems are optimal. its not so obvious to the uninitiated a minimum spanning tree can be found by a greedy algorithm. somewhat similar conceptually is Dijkstra's algorithm to find a shortest path in a graph. actually in both cases the associated "theorems" are nearly the same as the algorithms. 

from some quick search it looks like the online version is an area of active research. you dont mention the application area which might help to narrow down the literature search. one option is to look for an application area where theres the most or latest innovation. hence there is some application of incremental max flow in vision systems & some algorithms for it there; try Maximum Flows by Incremental Breadth-First Search at microsoft research labs. paraphrasing the intro to this paper, apparently for vision instances the Boykov and Kolmogorov algorithm does well & there are no known exponential time counterexamples although outside of the vision applications it might perform poorly. so it might be worth trying the B&K algorithm on your data & seeing how it performs & also the microsoft algorithm. you seem to be saying that an incremental algorithm that is linear in the number of graph edges is not sufficient speed? but isnt that fairly high efficiency? how many edges are you dealing with? maybe the approach might be to decrease cost of traversing the graph if that is expensive or a significant factor (eg graph stored in db vs graph stored in memory) here is an interesting paper that argues that while the nonincremental algorithm for max flow is in P the incremental version is NP complete. "To the best of our knowledge our results are the first to find a P-time problem whose incremental version is NP complete." Incremental flow by Hartline, Sharp 

study of the mandelbrot set & fractals via visual/graphical explorations. the simple formula $z \leftarrow z^2 + c$ can be understood by kids who have learned complex numbers or even by kids who havent by replacing it with the formula written with reals only. also another case of complex or emergent phenomena arising from simple equations. 

the conjecture that (roughly) "every NP complete function could somehow be used to create one way functions" has not been disproven so there is not really a shortage of "candidates" in at least that sense. an interesting recent formulation of this is by Cook et al, On the One-Way Function Candidate Proposed by Goldreich where they analyze the complexity of Goldreich's function by reducing it to SAT and doing empirical SAT experiments which give circumstantial evidence of its hardness (along with theoretical evidence also). also existence of one way functions is tightly coupled to core long open problems in complexity theory eg the P=?NP problem. which is in turn tightly coupled with proving lower bounds in complexity theory, referred to by Arora/Barak in Complexity theory: a modern approach as complexity theory's Waterloo. here is one ref by leading researchers on this angle that may be helpful: 

this seems an interesting FSM optimization problem; have not seen it studied, wondering if it has been and/ or looking for other insight. 

havent heard of this problem exactly in literature [maybe someone else has] however as a "nearby problem" it seems to me the minimum spanning tree would have useful properties to solve your problem. for example maybe generating two minimum spanning trees starting from the source vertex & the sync vertex, and propagating them outward until they touch, etc. might solve the problem or give a close answer. before anyone dings me on this here plz understand I am extending the idea of the MST somewhat to be generated starting from a given vertex [normally it starts from shortest edge in the entire graph]. if it doesnt work Id be curious for the reason.