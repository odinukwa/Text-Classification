These can also be had in retail or wholesale quantities at monoprice, cablewholesale, etc... There are definitely non auto-MDI GE ports out there still, especially on network hardware. Keeping a dozen straight and another dozen or so crossover (in different colors!!) makes a lot of sense. What -also- makes a lot of sense is keeping a handful of 6" patch cables handy to both make using these guys easier as well as clearly showing where they are in the line for the next person coming along trying to figure out how X connects to Y. 

Both are actually desktop form factors. The pizza box happens to be ~1U in height, but is -not- the same form factor as a a modern 1U server. The lunchbox is, as mentioned above, akin to the shape of a child's metal lunchbox (but does not share its dimensions). These are the classic examples of both form factors - Pizza Box Lunch Box 

if applied, the and may also give useful information. It's less likely as an issue, but the various fabric stats can also provide some info. Go for , among others. 

This is a well-documented set of processes and some perusal of Ubuntu's own website will define what you need to do. In short, you want to centralize the administration of user ID's, passwords, etc in an LDAP directory and set up autofs to allow a given user's directory to be mounted from a central file server whenever they log into a given client machine. The client machine setup is laid out here while the server setup is here. The overall Ubuntu server administration guide is also excellent, as found here. This last resource covers not only network logins and such but also a huge number of other common tasks. It's a good reference and the Ubuntu people put a lot of work into it. 

I'll second the requirement for management of DDoS upstream, but as an intermediate measure you might want to employ some kind of policing or shaping of connections at your own router or switch a hop or so before the servers in question. The best way to bounce back from the crash is not to crash in the first place. The router/firewall/switch isn't actually terminating the packets and is (hopefully) designed to run at a much higher rate. 

It's likely that the default timeout is too long and that apps are breaking as a result. Keep in mind that the resolver will go start with the first entry in /etc/resolv.conf -every- time it's called (notwithstanding cached entries). Try adding something like "options timeout:.5" or similar (see the man page - $URL$ to let the local resolver try alternate name servers sooner. Be careful of making this value too low, as some recursive lookups can legitimately take quite a while. 

The key point in all of this is that all of the hosts - and the routers - on a given L2 subnet (i.e. a VLAN) are configured with the same MTU and that this MTU is greater than or equal to this value. A mismatch between hosts on the same VLAN can be a real mess. Also - the primary mechanism that hosts with different MTU's have to allow for proper communication is PMTU discovery, which assumes both L3 separation and that ICMP unreachables are allowed. 

Setting up Netflow would fit the bill. While normally generated by network equipment there are open source packages ($URL$ to generate Netflow based on libpcap. This data can then be exported to a number of both freely available and commercial collectors and analyzers (either on the same host or separate) that can then provide detailed information about individual hosts, protocols, etc. 

It's normal. IOS translates hostnames at the time of configuration and stores the resulting IP address. This is also true for setting ntp servers, aaa, etc. 

The way that dedicated management ports are usually configured in Cisco equipment (routers and switches, at least) is generally placing said port into its own vrf and then applying ACL's and such as appropriate to limit access. The vrf serves as a completely separate routing table - which speaks to both requirement for potentially overlapping IP's as well as assuring that traffic cannot be forwarded through the isolated port. You can also associate various services (snmp, aaa, etc) with the vrf to follow its particular routing requirement. 

Since there's no guarantee that a given packet to the anycast IP will reach the same physical host, this approach really only maps to connectionless protocols. The reliability of the solution is only as good as the logic tying the correct operation of the service to the advertisement of the route. If the service dies and the route continues to be advertised then there will be a potentially significant black hole. Getting the anycast route advertisements well- and properly- distributed across the public Internet is not trivial. It's very easy to create hot-spots: a particular instance of an anycast route that happens to be preferable to most clients. This is still a potentially decent HA solution (for the easier types of failures) but it doesn't speak to load balancing. 

It's not entirely clear what you're trying to do with your configuration - what's happening with the upstream switch as far as any interconnection amongst the six VLAN's coming into eth0? The other thing that pops out is that you've both got an IP address on eth0 as well as VLAN interfaces. Put the IP on the bridge interface if you're trying to present the same subnet to the various VLAN's (...not that this configuration entirely makes sense). Again, some additional background on what you're trying to achieve would be helpful. For what it's worth, the usual configuration is a single VLAN in a bridge with multiple VM interfaces (i.e. multiple VM's on a given subnet) rather than the inverse. 

for specifics about the command run from a shell. Edit: Forgot to add - use the option to have it show progress rather sending the migration process to the background. 

The larger shops I've seen have often gone in the direction of setting up a single common polling infrastructure. A set of dedicated pollers pulls the data and then makes it available for various consumer apps. This scales a lot better and ends up leading to a lot less traffic and reduction in control plane usage on monitored devices. This sounds like what you're after. There's actually an SNMP proxy MIB out there that's built into Net-SNMP. You may be able to set up a couple of machines in such a way, although it will likely require some customization. There are also commercial packages like this that may provide additional value in terms of protocol translation, support, etc. 

Try putting it in brackets [2001:470:...] or ipv6:[]. A great many parsers can't differentiate between a text entry and a v6 address. 

If you're configuring router on a stick then why do you have L3 interfaces defined on the switch? If you're just looking for the 2950 to provide L2, then the configurations would properly look something like: 

The only connotation I can think of where this would be useful would be in L2 subnets where clients talk to well-known servers but are otherwise not supposed to directly communicate. In this case the various clients wouldn't be able to track on one another based on previously observed hardware addresses. At best it's pretty much a corner case, though, as there are better external solutions to accomplish that kind of separation (i.e. PVLAN). Your theory about commonality of address modes seems like the most likely. 

The comparison to 14.4k modems isn't apt. In the case of an access server with a bunch of modems attached, each client has a dedicated connection and its own share of bandwidth. In the case of wireless all clients are sharing the same radio frequency space. As the client density on a given AP increases, the likelihood of contention for this available resource also increases. The 40-some-odd client is a guideline and can vary based on client activity (among other things). This issue is independent of what actually tended to be an issue with access servers, which is oversubscription of uplink bandwidth. Several hundred 14.4 users being directed out of a single T1 meant an often untenable degree of resource contention but that contention wasn't between the client and the access server but rather between the access server and the rest of the world (vs the wireless case where contention occurs between client and access point).