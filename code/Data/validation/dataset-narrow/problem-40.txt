When you use linewidth or line antialiasing or pointwidth or pointsprites, OpenGL creates for you a small rectangle instead of the line or point, with texture coordinates. Nowadays you can even program this yourself using geometry shaders ot even the tesselator. A totally different approach is to use defered shading, using one geometric pass just to store information in the RGBAZ buffer, then a second pass than you run on all the pixels of the screen to do some process. (to act on all pixels, simply draw one full-screen rectangle). Nowadays you can even do the first pass as one or several "render to texture", then MIPmap these textures, so that the final pass can easily access to less local values. 

the arbitrary line can be represented as $P = (x,y) = P_0 + \lambda. \vec{dir}$ (works in n dimensions, no special case). If your other line is $x=x_1$ simply inject this in to solve for $\lambda$ and get $y$: $y=y_0+(x_1-x_0).\frac{{dir}_y}{{dir}_x}$. 

No, because the underlying physics is not the same, nor the lobe shape - not to speak of their parameters such as color and Fresnel term. Specular is really true surface interaction with the interface material/air, so it has Fresnel modulation and the internal medium has no influence on colors. But the surface condition strongly influence the reflectance, of course. Think of it as the reflections on ocean surface. Diffuse is due to the subsurface scattering, light entering the medium, and thus gains the color characteristic and loose directionality. Think of it as the green color of the water turbidity. If depth of penetration is less than pixel size CG people don't call it "subsurface" but the physics is the same. Of course diffuse input is what specular let pass, which is angle (and polarization) dependant. Beside, there exist transparent medium with internal interfaces (e.g. thin shells), so case exists were light goes inside but still acts specularely (or even wavely) and not diffusely. Oppositely, most plastics and paints/varnishs and biological medium are intrinsically transparent, but contain pigments (often based on a metal atom) that cause the opacity (diffusion or specular on these "objects in the object"). Think of them as the fishes in clear water :-) . 

You can use this formula on each edge where one point is < isovalue and the other is > . Then you link together the points belonging to the same square to form the edges of your iso-mesh. Then you make polygons by linking edges sharing a vertex. In case of more than 3 sides (could be up to 6), it's up to you to prefer spliting the polygon into triangles. 

Volume rendering is not like ray-tracing, it is like "scene rendering". i.e. there exists several algorithms to render volumes. One close to ray-tracing is ray-marching, and has may variants. The simplest: for every pixel in the screen plane, trace a ray starting from the eye point to the screen pixel location, and advance along the ray by constant steps. At each voxel calculate the pixel color and transparency, blend it to the pixel value, and so on. Blend = 

I know that by now it might be considered as a kind of sad recurrent joke, but by chance, does anyone here has the least information about the colorspace of RGB values in the MERL BRDF measurement database ? 

The first things I would try is to see if smartphone pixels are visible (if the photo is high res) or if strange aliasing occurs (your example image shows both). Another classical test (but not adapted to any scenes) is to detect perspective bias: if the camera is not exactly parallel to the smartphone screen, then 3D objects projected on the image on screen are projected again to the camera captor through a slightly different perspective. If the scene contains spheres, discs (physical or as reflects, for instance), perpendicular angles, or any similar invariant, then you can detect this. (this kind of trick is also used to detect images photoshoped by composition of several images). 

Difficulty depends on what you really want to classify, i.e. what exactly your data base of images is made ok. For simple hand-drawn or computer art, I would think of just looking the color histogram (but for complex art with gradients; but still these gradients are likely to induce a very smooth colormap). At the other extreme, distinguishing realistic computer graphics image from reality is probably impossible. 

Nothing else besides the gray rectangle is displayed. In summary, for someone who has something like this working on an ATI card, are these the OpenGL calls that you would use to get it drawing on an ATI card or is there some obvious feature I'm using that's not supported by ATI that's causing the whole thing to not draw. I'm looking for possible differences between how ATI and Nvidia would handle this geometry. P.S. I added this note to answer some of the comments that suggest I should go back to a single triangle and debug from scratch on my own. The reason I'm asking this question is that I don't currently have access to ATI hardware. I'm in South America until May and hardware availability is basically nil where I am. I have several workstations that I brought with me for work, but they all have Nvidia hardware. I have to do a full build and then ship the product to someone in a different time zone each time I attempt to correct the issue. I need to be more purposeful in my fixes than going back to drawing a triangle and working up from scratch. I would love to debug it myself on the affected hardware step by step. I just can't right now. I need to get the code closer to a likely solution before sending another build, so I'm looking for some educated guesses from people with direct experience with this type of geometry on ATI hardware. I would greatly appreciate more understanding than "That's what debug is for. Go back to a triangle.", because those comments really don't help me or anyone else reading this later. This problem seems like something that someone must have encountered before because it seems like a pretty standard way to draw a heightmap. I think that this question will help anyone else who has this problem in the future. I've lost several days to this, and there is no published answer to anything similar that I could find. If this remains unanswered until May, when I return home, I will write the answer myself and remove this note, but I hope it doesn't come to that. 

Here are the shaders for what they're worth even though reducing these to #version 130 doesn't help and it doesn't work with or without the commented code on ATI. Nvidia runs all variations. The shaders themselves don't appear to have any impact on the issue. vertex: 

The geometry itself is correctly formed in to the best of my knowledge because I've inspected it piece by piece in a debugger and because the strips come out correctly on Nvidia cards. I am assuming there are no fundamental differences in how works between Nvidia and ATI. If there is a difference then that's exactly the type of thing this question is trying to figure out. The description of triangle strips in the OpenGL spec doesn't seem to leave much room for differences here if any, so it seems likely that if the strips themselves are formed correctly according to the spec they should work everywhere or nowhere. The geometry is a basic hexagonally arranged triangle grid with degenerates in between the strips. I'd like to figure out what features are causing this to output nothing except the on ATI cards so I can get people with different machines looking at the application. Here is an example of what it outputs on all the Nvidia machines across various windows and OSX flavors, in case that helps 

I have a rendering system that I use for drawing height maps. The system uses triangle strips of indexed verts in a vao. The system has worked fine on the 3 Nvidia cards I have tried it on across several versions of windows and OSX. I don't own any Systems with an ATI card but I sent the application to a colleague to look at for me, and on his machine no output is produced in the 3d viewport. He has a Radeon HD 7800 and is using windows 10 which is one of the operating systems I have tried it on with an Nvidia card. Running OpenGL extensions viewer indicates that his card supports a modern range of capabilities up to GL 4.5. I attached an opengl debug callback. The shader program compiles fine and the vao is created without errors. There are no errors on draw either. The shaders use #version 330 core but I tried dropping down to #version 130 and simplifying the shaders to basically output the vertex position and a flat color, and it still works fine on nvidia but outputs nothing on ATI, so I doubt the shaders are the issue. I suspect it has something to do with the way the vao is set up or possibly some feature that is missing in ATI drivers or hardware. The following two blocks of code are in Kotlin using LWJGL3. The relevant GL calls are all the same as from Java though. They are a very tiny wrapper to the calls. The vao gets set up like this: 

Moreover, in the above example, the length of the cross product is proportional to the area inside abc. So the smoothed normal at a vertex shared by several triangles can be computed by summing up the cross products and normalizing as a last step, thus weighting each triangle by its area. 

While browsing to properly write my question, I actually found the answer, which happens to be very simple. Another Fresnel term is also going to weight in as the photons make their way out of the material (so being refracted into the air) and become the diffuse term. Thus the correct factor for the diffuse term would be: $$(1 - F_{in}) * (1 - F_{out})$$ 

You cannot do something like that. is a 3x3 matrix: without a 4th column and a non-zero homogeneous coordinate, you can only transform directions, not positions. You need to first get the camera direction relative to the vertex, then transform that direction. 

The well known Schlick approximation of the Fresnel coefficient gives the equation: $F=F_0+(1 - F_0)(1 - cos(\theta))^5$ And $cos(\theta)$ is equal to the dot product of the surface normal vector and the view vector. It is still unclear to me though if we should use the actual surface normal $N$ or the half vector $H$. Which should be used in a physically based BRDF and why? Moreover, as far as I understand the Fresnel coefficient gives the probability of a given ray to either get reflected or refracted. So I have trouble seeing why we can still use that formula in a BRDF, which is supposed to approximate the integral over all the hemisphere. This observation would tend to make me think this is where $H$ would come, but it is not obvious to me that the Fresnel of a representative normal is equivalent to integrating the Fresnel of all the actual normals. 

A diffuse shading with a sharp transition between lit a unlit, that happens somewhere below 0 so the light leaks behind a little. The rim lighting that, as you mention, has an almost constant screenspace width. From the look of it and some of the artifacts, my best guess is that it's actually some edge detection filter based on depth, which is then used in combination with the diffuse lighting so the rim lighting doesn't affect the unlit parts. 

So in the case #2, the diffuse is left out, which is equivalent to a 0%, pure black, diffuse color. In case you haven't checked it already, this presentation is an excellent code explanation of SmallPT, dense but thorough: smallpt: Global Illumination in 99 lines of C++. 

From my understanding, the specular color usually refers to the amount of light that is reflected when the surface is lit at normal incidence, and is noted $F_0$ or $R_0$. Moreover, for non metal materials, this value is calculated from the index of refraction of the material $n$ with the formula deduced from the Fresnel equations (in which 1 is the index of refraction of air or void): $$F_0 = \frac{(n - 1)^2}{(n + 1)^2} $$ According to this list of refractive indices on Wikipedia: 

I assume a similar rule will apply to the OpenGL equivalent, Uniform Buffer Objects, since they map to the same hardware feature. What about vanilla uniforms though? What are the rules that apply when declaring uniforms? 

A note first From the look of your screen capture, I suspect there might still be a bug in your code. Noise is to be expected with only 16 spp, but your picture still looks surprisingly dark to me. For comparison, here is what my implementation of SmallPT looks like with 16 spp, 15 bounces, and no next event prediction: 

To be able to give a good answer, we need to know what is the compiler error you are referring to. At first sight your shader looks ok, although: 

Reduce shading when possible Lens distortion Part of the NVidia VRWorks SDK is a feature, Multi-Res Shading, that allow to reduce the amount of shading, to take into account the fact that some pixels contribute less to the final image due to lens distortion. In Alex Vlachos' GDC 2015 presentation, he also mentions the idea of rendering the image in two parts of different resolution, to achieve the same goal. Foveated rendering Finally, like your question mentions, foveated rendering aims at scaling the resolution to take into account the fact that eyes have more precision in the fovea and less in the periphery. There is some research on the topic, but this requires eye tracking like in the Fove HMD.