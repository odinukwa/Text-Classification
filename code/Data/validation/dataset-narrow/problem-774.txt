sql server and vmware tools fight for memory space in a vm. This is probably compounded by running terminal services and vs. If you haven't already, go into the vm's settings and create a full 4Gb memory reservation under the resources tab. This'll prevent the baloon driver attempting to reclaim memory within the vm. You may also want to configure the options within sql server to ensure it only allocates part of the system's ram. Maybe 2gb max. 

I think you're in for a world of pain trying to get LDS working in place of AD when dealing with SharePoint. It's not really a lightweight version of AD even though its name implies so. Think of it this way: AD LDS is like SQL server with no databases or tables defined. AD itself is like SQL server with a full schema, databases, set of tables, triggers and stored procedures all defined and ready to go. 

Typically directory listings are generated by the web server itself, and that feature may be disabled on your host (and it's often good that it's disabled, for security reasons). If you have access to the control panel for the web server, you may be able to enable the setting yourself. Unlikely as you indicated you have been provided with hosted space. You can work around it by creating a PHP file which will list out the contents of your chosen directory. See the instructions for this here. 

If you mean incredibly long as in 2 or 3 minutes before you reach the OS loading, this is normal. If you mean over 10 minutes before the OS begins to load, there's either a hardware problem, or some diagnostics have been enabled and not disabled again. What information is coming up on the screen during these 'tests'? Can you reset the BIOS to clear it back to how it should operate? 

During the migration did you migrate shares from the old server to the new one, or does the new server have the old server's name and share configuration? Quite often there are references left behind that refer back to the old server name (in office file metadata, and offline files to name a couple). If that network name is not available, lookups can stall for a couple of seconds. You can test this quite quickly by adding a hosts file entry for the old server's name, pointing to the new server's IP address. If this resolves the issue, you know where the problem lies and can start to dig out those references. 

Not feasible. HTTP and HTTPS are two distinct services which behave in markedly different ways (despite the naming and payload being similar). Unique services must bind to unique ports to enable client and server to communicate in a language both understand. To accomplish this you would need to have a client that would connect to the port, negotiate which type of connection is to be made (HTTP or HTTPS), then switch to the appropriate communication mode. That negotiation logic doesn't exist on the clients, so there's no way for you to implement this server-side. 

You could educate users to use the username@domainname format instead. If this also happens to be the users actual email address, success rates tend to be much higher. 

From windows nbtstat -a xx.xx.6.6 will give you the netbios name of the remote machine. arp -a will give you the MAC address. 

I deliberately didn't mention build solutions (Altiris/RIS/MoM etc) because if you do all the stuff above you may find it's just not necessary for a 150-person company. 

You're going to need to do a proper cost/benefit analysis on this, tailored to your specific scenario. The main advantage of implementing a VDI solution is for situations where you do precisely the same thing on a lot of workstations for a lot of users, and can connect them all back to a single central system. In your case, I'd look very closely at: 

You really need to be going Disk-to-Disk with this, not Disk-to-Tape. So a few TBs of storage on your backup host is definitely recommended. Without this you just won't see the throughput. Your physical backup hosts needs to be plumbed into your SAN and be able to see all your LUNs in order to back them up. In practical terms this means you usually have a Windows box with an HBA and a ton of 'unidentified volumes' in Disk Management. Which it wants to initialize for you every time you peek in there. If you do, it'll trash your VMFS volumes. Kinda suck. The Change-Block tracking feature seems to go a bit wonky if you, for whatever reason, don't get a backup for a few days in a row. As mentioned briefly you will probably want agents for SQL, Exchange and AD hosts even if they're VMs, to give you granular recovery. Your ESX or ESXi hosts must be licensed. ESXi Free doesn't enable the vStorage stuff. Having a VirtualCenter is also advisable but not, I believe, required. 

Best advice I can give you is to run domain controllers as very discrete entities wherever possible i.e. load no services onto a domain controller that is not essential to the operation of the domain controller. This is commonly overlooked with very small shops and especially Small Business Server for practical/cost reasons, but once you scale beyond that you ideally want to be heading towards a point where DCs are JUST DCs, and you only run as many DCs as you realistically need for adequate replication and fault tolerance. 

I'm a graduate with no certs, someone I work with on the same level as contract engineers has certifications and no degree. He's very capable and (I hope) so am I... we both have similar mindsets despite having followed very different paths in education. It's quite telling though, that I feel I will need to start earning some solid certifications within the next 12 months, just as he feels he has to catch up with the latest certs and keep current. As for the HR problem directly yes, they're often focused on the hard qualifications without any real understanding of what they mean. This tends to be more of a case in larger organisations where HR will process you without much regard for what you really do... it's a square-hole job position and the question is 'are you the right shape block?'. Smaller organisations are where you're more likely to get instant face-time with the IT management, who will be close enough to their own staff that they can competently talk shop. Final thought: I don't mean to be negative, but the assertion that extensive kernel/C++ programming ability would make you a valuable asset for an IT Administration role doesn't really scan with me. I've experienced extremely skilled coders that would throw a wobbly if they had to answer a straightforward server 2008 troubleshooting helpdesk call, and I've seen very skilled 3rd level IT administrators that don't understand operator overloading. It's good to have knowledge of both areas, but they're different disciplines. 

The gateway can now be anywhere in the range: 192.168.x.x Option 2 Add a gateway to each networks Pro: Seperation of subnets and broadcast domain Con: More networking configuration required. May need to re-patch equipment Seperate the subnets physically using seperate switches, or virtually using vLANs. Connect both networks via seperate connections to your router/firewall. Configure the router as a gateway for each interface and configure the appropriate traffic rules between the subnets to allow users access to the server resources they require. e.g. 

Broadly speaking if the virtualisation platform you currently run fully supports the guest OS you're intending to run, virtualisation is a good move. There are some use-cases that warrant more careful inspection: 

This error is a bit misleading, it (usually) refers to a problem with the virtual devices you've assigned to the machine. When you run through the converter wizard, how far are you getting before it fails? I suspect you've got all the way through the wizard, can inspect the source, can browse the target, and are back at the job screen when it tanks? Try disabling any kind of customization of the VM, in the wizard, on the last page. Check the devices you have assigned to the virtual machine, and remove everything but the bare min. Check that your defined VLAN networks are available on both ESXi machines and are labelled the same. Are you using the latest version of Converter? (4.0.1) 

I've encountered numerous problems with timekeeping in VMs, and across several different virtualization platforms, I've also had problems with the native Host-VM time sync tools. My advice is to have an authoritative source of time that is based on a physical piece of hardware (in this case, your KVM host server) and sync your VMs using the standard OS time sync methods (in your case, ntpd) back to that server. Often a shorter resync interval is necessary if the vm guests are drifting excessively. In short, I think you're on the right track with ntpd. As an example on VMWare, we found that the VMWare Tools time sync will only correct time if it drifts backwards on the VMs. It wouldn't correct the time if a VM got ahead of its host. 

I think you'd need to identify specific reasons to choose hibernation over a regular system shutdown before it would make sense. Hibernation was introduced primarily for laptops, where being able to power down yet keep the user session state has significant value. But ideally servers should be able to shut down fully, then start up on a fresh boot without having lost anything of value. An alternative approach to conserving power is the power efficiency of your servers. There are big potential gains here. Servers made by Dell, IBM and HP in the last 2 years give vastly greater power efficiency over their predecessors, because of a general shift to 'green' datacenter technologies. One of our HP G5 servers (2007) draws 400W under regular load to run a single 2008 instance, whereas one of our G6 servers (2009) is running 70 virtual machines and barely drawing 180W. Another Alternative - If you're virtualized, you can also benefit from cluster power management solutions like Insight Power Manager. There are plugins available that can dynamically power on/off VM host servers to meet demand, so when you systems go quiet in the evening, a bunch of the servers that run them will shut down. When things pick up in the morning the servers will come back online. This is a much more elegant solution if you're looking to save power across more than a handful of machines. On the subject of whether daily power cycles of your servers might be bad for the hardware - I couldn't find too much data on the subject but there are good reasons to suspect you will have problems. Disks tend to fail during spin-up, and it seems to be due to the material it's made of expanding and contracting under changing temperatures (powered on vs powered off). Repeat this expansion and contraction enough times and the drive fails. I suspect this is why laptop drives fail most regularly, not because they're moved around, but because they're frequently power-cycled, and tend to run closer to hot components. The reliability issue is another reason why the virtualization solution may be a better fit - host servers can boot from an SD card and have no disks, therefore won't suffer from this problem. Here's the G6 server I mentioned: