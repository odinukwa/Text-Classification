Linux, without docker In Linux, you can create arbitrarily many IP address for each interface. A comment in $URL$ reports success with 2000, and hearsay of 5000 successful IPs. So. Pick any Linux box on your intranet, which could of course be a VM, and create as many IPs on its single ethernet interface (MAC address) as you wish. Say you're using the 10.0.0.0/8 private network, then you can execute lots of these: 

I am in a similar pair of shoes as you - I am currently, slowly, getting into the DevOps topics after having many, many years of conventional development (and close connections to conventional ops) behind me. You seem to have read a few articles, or heard a few things. That is good. Let me suggest a practical approach how to go on: 

Yes, of course it will. Everything dies. I have no idea how the IT world will look in 20 years, but one certainty is that it will not look like today. 

Yes. But even a stateless app handles connections from the outside (or it would do nothing, really!). That is the issue here. If your app, say, some HTTP server that only serves static files, and is thus stateless, were to be routinely killed with -9, then ever so often, you would kill it with an active connection. So your customer would get some HTTP error code. In the best case, it is an unimportant small image and he doesn't even notice; in the worst case, it is your main or file and he will indeed notice badly. So, by using a weaker signal, you give your stateless app the chance to finish the current request before shutting down. Obviously, you can finish it off with -9 if it does not go away after a second or two (or however long you wish to give it time to end its business). 

That's it for separating your code and data. The next step obviously is to create "mycode2" as a clone of "mycode", and from then on you simply need to switch the synonyms in "myapp" between those two. You can have as many code schemas as you want, and switch between them freely with some rather trivial tooling. (Setting the proper options, grants, making sure that the PL/SQL uses the tables of the caller instead of the compile unit left as an exercise to the reader... - feel free to ask if anything is really unclear.) Practical hints: It is true that you have to change 5000 synonyms (well, hopefully you do not have 5000 PL/SQL packages :) ), but that is an uncritical operation insofar as there is nothing that can go wrong (both and can not go wrong for any reason I am aware of; unlike e.g. which can lock if the table is in use). You will obviously script those drops; and you will also probably query the synonyms you have to set by or something like that to automatically catch all of them. 

The more automation, the merrier. Automation not only means shaving off a few minutes here and there (but this factor also pays of hugely surprisingly fast), but also that everything that is automated, is codified in some script or tool; which means you are much less dependent on individual persons (who may become ill, overloaded, or leave the company) and scale much better if your team grows. 

Yes, it is possible. While Docker is often used for microservice-oriented images (i.e., one single service per image), nothing keeps you from converting a monolithic old application to Docker by building a large image, and running arbitrarily many processes inside. While it is true that a Docker image is not a standalone VM, it still contains everything an equivalent VM would contain - excluding the kernel. I.e., all OS libraries (libc, everything else...) are in there anyways. 

I'm neither asking for a list of any private schemes somebody may fancy, nor about opinions on which is the most beautiful. I'm strictly asking about whether there exist any that either by purpose are industry standards or have turned out to just be used in many places by chance/convention. The purpose of using such a scheme would be to have something that would instantly be recognized by fellow DevOps people; or which could easily be shown in contexts where it is not appropriate to look into Dockerfiles or other Infrastructure-as-code sources (e.g., when scribbling on a board, or holding a more abstract presentation). 

As far as I can tell, reading the OpenShift documentation, including $URL$ the officially published "standard" way to manage OpenShift configuration (about their objects like services, build config etc.) is to simply use or the web console and "do it". They do not seem to document/suggest a standard way to keep this configuration, i.e. the YAML or JSON representation that you see in the web console or in in, say, a git repository. Would you say that most projects (that you witnessed or know about) work this way, i.e., just keeping the config inside OpenShift? Or are there established standards, to have this outside of OpenShift itself? I noticed that there is a 3rd party OpenShift ansible module on github, but am not quite sure how proven/established that is. 

It is always ethically acceptable to fork any code on Github (or Sourceforge or wherever you got it from) and do whatever you want with it, within the terms allowed by its license! This is what open source and permissive licenses are all about. The original author wanted the world to contribute to this piece of software, and you will be doing exactly that. Fork it, respect the license requirements, and go on with it. It does not matter whether it is abandoned (like your example, since the last change was 3 years ago) or current. If you do feel that you did some good improvements, send pull requests back to your upstream, as usual. If the original author never comes back, then so be it. If he does, he can integrate them (or not if he so chooses). But this is strictly optional, you are neither legally nor ethically forced to do so. About uploading it to some other locations (Ansible Galaxy or whatever - say Docker Hub for docker images and so on): the issue is probably mostly to avoid irritations due to the name being "replaced" (but I'd assume if the original author already has uploaded it to A.G., you can't just replace it, anyway). So, in the latter case, just upload it with another name, which is still easily identifyable as related to the original - say, append some suffix which is not too obnoxious. If the author, in your case, should later re-appear, and you had uploaded the software to the Ansible Galaxy, and he wants to take over, then you two will simply communicate and resolve the issue somehow (i.e., merge the two forks again and decide who keeps updating Ansible Galaxy). Chances are that he will be more than glad to leave the work to you. Oh, and to be sure; this piece of software is under the MIT license: 

This should take you, if you are an experienced developer and have done these things "dry" already, not much longer than a few days. Docker itself is a pretty simple affair, there is not much more to it. One further part that makes sense to add to the puzzle is Ansible, which makes it pretty easy to manage another machine remotely (i.e. for installing packages, deploying stuff, starting services and whatnot). If you have Docker and Ansible down, just get your CI/CD pipeline to work. Google advanced, but necessary stuff like security and such. After that, you should be experienced enough to decide in which direction to go next (i.e., larger stuff like Openshift/Kubernetes, or just whatever AWS/Azure or whatever have to offer, etc.). There are a myriad of different options out there, and much of the choice seems to be pretty opinion-based. If you get into High Availability, swarms and such, it will also pretty soon get more non-trivial. But starting out with Docker should be fun and relatively straightforward. 

The one (starting a small 1-2 person company and growing as money comes in) has nothing to do with the other (using a DevOps philosophy). Even a 1-man business can be DevOps based. You can start implementing your Great Idea (tm) with a functioning fully containerized CI/CD pipeline, in fact there is no better time than at the beginning, when there is no cruft to take care of, yet, and before you hit a significant number of users. It's not going to take months of work; you can hack together a basic Jenkins/Docker pipeline in a few hours (if you know a lot already) or maybe days (but then the days will have been productively spent learning that stuff as you go). Starting out with the CD part in a greenfield project is much easier than adding CD to an existing application which used manual deploys before. You can start out with full test coverage at the start (there is no better time, for obvious reasons), including tools that verifies test coverage as part of the CI/CD. You can easily start out with some small security measures, like checking your test suite for superfluous open ports or whatever. You can, and should start out with cattle-not-pet right away (meaning IaC, IaaS, PaaS and all that good stuff, hosted on one of the pretty streamlined "big three" providers). Start small, scale a needed. When you add more personel, they will start out right in that fruitful athmosphere, and hopefully many of their cheaper errors will immediately be caught by the CI/CD. 

With cloud solutions like AWS, this is not so straightforward to figure out like with "classic", always-on servers that wholly belong to you, and where you are paying fixed monthly prices for power, network, backup and rack-space, with maybe a little bonus on-top if you exceed some bandwidth limits. So, in your case, have a good look at what "sizes" of VMs AWS has to offer. According to Wikipedia, in 2016, prices on AWS ranged from $0.0065/hour to $6.82/hour, depending on performance and other factors, with similar ranges for storage, and then further complicated by other factors of your plan (i.e., 3-years prepaid or whatever). Secondly, you have to factor in when and how long you actually need your application to be available. If none of your users are working before 7am or after 9pm, then you can have the application running only in this time, not paying anything in the "off" time. Thirdly, there are even more options. For example, maybe you can use lambdas for some functionality of your application (even more finegrained control over costs) and so on. Lastly, there are things like spot prices, where you bid for unused resources at drastically lower prices. You might become creative here and run your background batch processes on such instances, for example. More variations are mentioned in the comments. So, you see, we cannot really predict what your application will cost. It would certainly be possible to give you a ballpark figure in the case that you would run your application 24/7 just like a classic VM, but that would be rather useless. You need to sit down, read some more about the AWS pricing structure (they do have an online calculator where you can plop a few values in). As you are not doing a play or for-fun project, it would also be more than worthwhile to actually experiment, get an AWS account, develop a small prototype and do some performance tests so you see what you actually do need. Check out their calculator at $URL$ if you didn't already, but be prepared that their breakdown of on-premise costs is, in my opinion, not very useful (you'd better figure that out yourself, if on-premise is at all an option for you). 

"Enforcing" means lawyers going after possible culprits. Frankly, in my opinion (and those of some folks around me which are more a mix between IT-guy and lawyer), nobody knows how that will work out exactly. Those iLawyers expect a lot of very interesting court cases in the first months to tests out the water, and see what precedence cases are going to come up. There will probably be plenty of law firms fishing for nice fat fish to fry. For me personally, I don't really see that much impact specifically regarding DevOps. The applications themselves need to be adapted as necessary, obviously. Also, the actual data storage, be it RDBMS, elastic search indexes or whatever, will have to be looked at. Aside from that, one benefit of DevOps, specifically if you work with some kind of container system and infrastructure-as-code is that you should generally know exactly where your data, log files etc. is stored, and what kind of them you have; much more so than in classical servers, where your data and especially logs might be strewn all over the place. It should also be very easy to regularly roll your data as applicable, i.e., get lost of old stuff. So it can help to, for example, take your catalogue of i-a-c files and walk through them diligently - you can then be pretty sure to have found everything you need to be aware of. 

You need to figure that out for yourself as it depends very much on the persons, processes and software you are building/deploying. At first, just build the tools, work with them, and see what happens. If everything works splendidly, continue along the path you laid out yourself; if you hit problems, solve them as they come up. 

As far as I can read between the lines, you want your upper management to support you in introducing the 12-factor approach to your company's software development. In that respect, you do not want to explain each of the 12 factors to them. If they really were interested in that, then you could just send them the 12-factor website you have linked in your question - it already has 1-sentence summaries. I'll answer the question you wrote in the title: 

Your question is quite vague/generic. People store their source code (or generally, files) in git; let Jenkins run jobs (like CI/CD pipelines), use Docker to put their software into well-defined containers, Nagios to monitor their systems, ELK to collect, store and visualize (mostly logging, event) data, AWS to host their stuff, and Chef to manage their configurations. If you are studying that and learning about it, then you should have gotten that kind of information, really. The scenario in real world companies is simply to use those tools for what they are made, in different combinations as needed. If you really have just gotten some deficient information from your college or wherever you studied those things, then get a book like The DevOps Handbook and give it a read. It should clear up how all of this would work together in the real world.