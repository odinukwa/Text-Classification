The computing ecosystem keeps making smaller and smaller things. Imperative programming is far better matched to Arduinos and other small cheap computers that a k12 and earlier student is likely to tinker with (as opposed to just be a user of). 

I can copy the contents ("value") of what's on one of the sheets of paper, and hand the copy to you, and let you read and write on your copy. If you do write on your copy, note that the sheet on the wall remains unchanged by you. I can tell you the number of the selected sheet on the wall, and let you go to the wall, turn the sheet over so you can see the contents, and read and write on that sheet of paper. The number of the selected sheet of paper is its "reference". I get to see your changes (whether I want them or not!). Note that if there is a ton of stuff on the sheet of paper, the reference is a lot shorter and easier to give you than redrawing all the stuff onto a copy. 

The difference may be partially related to whether or not the student has a mental model of the machine (as in simple, possibly physically mechanical, computing hardware), rather than more abstract symbology (algebra). Changing the bits on a Turing machine tape, or the wheels on a Babbage engine, or the display of a pocket calculator, or the position of the beads on an abacus, as one carries out a sequence of steps, clearly shows how and when state changes, and the visible difference at different steps (program counter values) in the algorithm or program. Algebraic notation hides the turning of the gears, or the ticking of the (computer's) clock, and thus hides imperative state changes to variable values. Possibly for this reason, I've seen simplified assembly language (CARDIAC cardboard computer, with 10 instructions and 10 memory locations) taught near the beginning of introductory CS for non-majors courses, before delving into higher level programming language constructs (Lisp/Scheme, et.al.) 

The problem with teaching the trajectory of computing technology is with basic numeracy. The orders of magnitude are just too big for many students to comprehend without more numeracy and feeling for log() scales. Many of the examples here would just go over their heads as random magical numbers. I would teach this by not talking about computers, at first, but start with one of those science picture books that shows a proton, then orders magnitudes to the height of a humun, then orders of magnitudes to the size of the known universe. Also one of those books that show all of human history as the last fraction of a second on a 24 hour clock mapped to the age of the Earth. Then, after students go wow over these scales, maybe an appreciation for them, only then overlay computing technology: gears to relays to tubes to discrete transistors to ICs to VLSI to 10 nm finfets SOCs in their iPhones, and put each on top of the one of pages of those order-of-magnitude books. Give them a better sense of Moore's law scaling as it relates to the universe of scaling. Then they might be impressed that you started out in the Paleolithic period of computing. Or as a protozoa equivalent. 

Bonus points for pop quiz questions about something you just explained from the front of the room. Randomly done. 

The 32-bit subsets of ARM assembly language are fairly clean, orthogonal and RISC-like, as well as being a reasonably nice target for a simple compiler. Inexpensive Raspberry Pi's run 32-bit ARM code, as well as natively supporting a complete compiler development tool set (lex, yak, bison, et.al.) Almost every student is likely to possess a mobile phone that will also run ARM machine code, and both Apple and Google provide free development tools that allow coding portions of a mobile app (iOS or Android) in ARM assembly language. The 6502 and 8051 ISAs were not designed for modern compiler output, even though otherwise suitable for teaching hand-coded assembly language and running it on inexpensive (vintage or microcontroller) hardware. 

I would teach the use of arrays and array indices first (with pictures of a long row of physical mailboxes, or school lockers, etc.) Then later explain that all of a computer's and/or C program's (process) memory can be thought of as one big array with a pointer being a form of index (locker number) into that (virtual) memory array. You can then draw a picture of how arrays of char (bytes) might be allocated in memory. And then show why pointers might be necessary in order to tell a standard string function how to find (or modify) a substring in an arbitrary C string array. 

Item #2, a form of code re-use, especially if it is tested known working lines of code, is usually considered to be a good software engineering practice. Copying a line or two from the official documentation might in some cases be considered the proper way to learn or to code. Some professional IDEs do this automagically in the auto-complete hints. Copying from a web search or stackoverflow requires supervision or auditing. The instructor needs to confirm that the student understands (what it does, the algorithm, etc.) and has checked what they are copying against more official (for the language, library, etc.) documentation. 

Except for the latest iPad Pro and a few other specialized displays (vector, etc.), most laptop and mobile devices displays still update a 2D bitmap raster or composited texture quad at some fixed refresh rate, just like a CRT (but with much higher bandwidth these days). 

Randomly be "too busy with something" for a few minutes, but say you'll get back to them. Then set a timer, and then check with them some number of minutes later. Reward them (somehow, depending on age group, etc.), publicly if possible, if they figured out all or part of the solution themselves. This allows you to provide help (for issues where you really are the expert "oracle") while also giving random intermittent reinforcement (the best kind) for them figuring out the problem themselves. (Note: this works for co-workers as well as students.) 

It may be easier to transition students who have gained proficiency in using a a command-line environment to switch to using an GUI IDE, than vice versa. If so, starting with an editor+terminal will allow more flexibility in your later curriculum design. 

I would start by showing them a few natural language examples, where changing one letter in a sentence completely changes the meaning, perhaps into nonsense. Most computer languages respond similarly to almost any syntax change. Then show students program lines with correct and incorrect syntax (of typical mistakes, perhaps taken from previous sessions of the same course), and the corresponding error message(s), and/or incorrect/unanticipated run results. Quiz questions to test this might be, given a line of incorrect syntax, what is the corresponding error message (or bogus result) and a possible correction. The important idea is to help students form a mental connection between changes in syntax and changes in (IDE/compiler/interpreter/runtime/etc.) behavior. 

The HS curriculum of PLtW appears to use the Python programming language. Python can be learned and used from a command-line plus text editor, with zero graphics, which is good for screen readers. 

If you don't know where to search, it may already be too late. Part of learning at a university is not just completing the coursework, but in learning how to learn, learning where to look. One gets there by having spent significant time reading/studying related areas/technologies/problems just outside the assigned projects. The farther outside (but hopefully turning out to be still related), the better problems solver one might become. The more one might know which paths possibly lead to wherever. And if you don't find the function or conclude the existing solution, but know how to keep hunting for ideas, that's how you end up being an inventor (patents, etc.) of perhaps a better way to solve the holistic problem. You might find out that your friend's "solution" wasn't the best. 

I am of the school of thinking that starting at an abstract level causes too many students to consider computation as inscrutable magic. And that can lead to broken mental models and buggy ideas about how to code stuff. A 1970's vintage UC Berkeley Intro to Computing course (for non-EECS majors) used a cardboard computer with pencil-and-eraser-mutable memory cells ( $URL$ ). The professor also talked about physical adding machines (maybe Pascaline or abacus equivalents) before diving into coding in an HLL. Programming became something that controlled a visualizable machine, not math that became transmuted by magic incantations. I would start there (at a very non-abstract level), and then show how modern HLL's make it so much easier to solve higher level problems. 

In creating a general introductory course on computers or computing (programming and/or CS), does one mostly present high level concepts (recursion, computational complexity, concurrency, etc.), practical concepts (write some code in language X that accomplishes Y), or low level concepts (bits and bytes, logic gates, wire and blink some LEDs)? If more than one conceptual level (which I assume is necessary to some degree), in what mix or proportion? And what order might be best for what purposes or goals of the course? How does one get students started and intested? This earlier question is about the order of teaching, but before ordering, it helps to clarify the content of that which is to be ordered. Also, have instructors found benefit including any other semi-related material, such as was discussed in this previous question on the history of computing? 

Algorithmic thinking can perhaps best be taught completely apart from coding in the classes given programming language(s). I might have one student tell another student how to solve a fairly easy problem on a whiteboard, but only communicating one step at a time using a constrained set of commands, for instance only one input, one arithmetic operation step, or one saved result at a time. Maybe have the algorithm developer write out the steps while the operator student is out of the room, them have the operator come back into the room and execute the written algorithm on the whiteboard. One example might be averaging N numbers, where student A writes an algorithm, student B later provides the numbers one at a time, then student C uses the algorithm to compute a result, while the class watches. 

I have N sheets of paper taped to the wall, labeled #1, #2, #3, etc. to N. Each sheet of paper has different words or animal pictures (etc.) on it, but is turned over so you can't see it. To let you read what's on one of these sheets of paper, I can do two things: 

A touch of some minimal explanation of logic gates, binary numbers, state machines and the processor-memory divide helps make computing seem less like magic, and more like technology (something of which they could become designers in the future). One doesn't need to teach actually using assembly language to explain how a basic fetch execute cycle works. Etc. 

Programmers who learn only how to use libraries, without having even an inkling of what’s inside them or how they might be constructed, often become apprentice magicians throwing together opaque incantations. The habit of throwing together sequences of magic incantations to solve problems often has negative consequences (mobile and data center power consumption, cache thrashing, demons stealing one’s firstborn child, and etc.) 

A CS graduate with good writing and people skills might want to look into entry level product or project management positions. Adding a few business management courses might help with that. 

Pseudo-code allows expressing that an algorithm to solve some problem is not the same as the syntax (formatting, punctuation abuse, etc.) required to make any particular compiler or interpreter happy. Pseudo-code is also suitable for algorithms using data types that are not directly expressible in a standard programming language, such as real physical objects with the kids in the room acting as the operators and sequencers, etc. It also requires less writing (characters, punctuation) on a chalkboard/whiteboard. 

The fall in CS degrees seems to match a rise in women getting degrees in business, psychology, science, and biology, which might be due to a higher proportional interest in those fields. Easier to use personal computers, which arrived in 1984 to '86 timeframe, and which could help solve problems in those other fields might have made those other fields more attractive. That would also be about the time the tail end of the post-war "baby boom" would be graduating. Perhaps the youngest siblings in that demographic, or later those from smaller families, have different personalities leading to different interests (in careers, etc.) than first borns and older siblings in larger families raised during that boom. 

Best practices are sometimes best taught in conjunction with or after one teaches a problem or solution area where lack of best practices can cause bad things to happen. Show them, or better yet let them experience, the bad things that can happen first, after violating a certain practice. Make them value the practice. Otherwise you might be creating blind followers of unsupported dogma or the latest methodology fads. In engineering labs, we often broke things (with a loud bang) in conjunction with the learning theory on how to design things that didn't fall down. 

IIRC, my shop and chemistry classes included mandatory safety briefings. Many educational institutions have a code-of-conduct with which students are supposed to know and comply. The same should happen and apply if any computing activity might affect student safety, or potentially violate the conduct code. 

Teach the command line as part of a brief history of computing. Let them know how good they have it, not having to punch cards and wait overnight, or use a command line on a clanking teletype over a remote 110 baud link. 

Why do some instructors delay teaching mutation due to considering it to be a more difficult concept? (than functional or recursive concepts, etc.) It is very likely that, back in the 8-bit PC days, many thousands of 8 to 12 year olds learned to code in Tiny Basic or other "street" BASIC implementations (where there weren't even local variables or other easy recursion or functional semantic support). How could these kids do so if variable mutation was a difficult concept? Or is the spaghetti code they often produce a result of not really understanding the concept itself? (rather than just the software engineering downsides)