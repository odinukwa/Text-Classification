These steps handle the creation of the temp table when restarting mysqld. STORED PROCEDURE You need to retrieve the Connection ID using the CONNECTION_ID() function: 

Now you have a temporary column at your disposal You could also build it as a separate table called mytablemyid 

Your First Query Since is not indexed your first update failed. MySQL (eh Oracle) should have caught this problem. So, it is a bug. Your Second Query The update works because you accessed the Clustered Index indirectly by trying to manually update the column. Your Third Query It worked because you accessed no indexes at all, neither on nor on . In that instance, a full table scan would easily fulfill this request. Epilogue If I were you, I would report this as a bug ASAP because MySQL 5.6's FULLTEXT indexing for InnoDB is what people want these days and query plans should be accommodating it with WHERE clauses whether accessing with the Clustered index or not. 

Both approved and sid should be together. IF they are not together, the MySQL Query Optimizer may decide to perform an internal index merge of the primary key and an index where approved is the first (or only) column. In fact, your new query should be refactored as follows: 

If the output gives you settings beyond 80% of the DB Server's Installed RAM, then use 80% of whatever the installed RAM is as the innodb_buffer_pool_size. Give it a Try !!! UPDATE 2013-04-15 12:43EDT Let's look at the definition of 

You are not given SUPER privilege and there is no direct access to my.cnf. In light of this, in order to change my.cnf options for startup, you must first create a MySQL-based DB Parameter Option List and use the RDS CLI (Command Line Interface) to change the desired Options. Then, you must do this to import the new options: 

Based on this, you should not be able to write to the Slave because SUPER is a system level privilege (SUPER only appears in mysql.user as Super_priv) and not a DB level privilege (SUPER does not appear in mysql.db). The first GRANT lacks SUPER privilege. The second GRANT does not have a SUPER privilege context at all. Having SUPER privilege has a lot of firepower because you can run the following 

Download that ZIP and try running that . You could then collect all the files with something like this: 

only gives you back whatever you are connected as, which was . and and completely different users. Just do 

Therefore, adding the PRIMARY KEY to a secondary index is definitely redundant. Your index entry would like . This would also needlessly bloat the secondary index by having 2 copies of the . For the index with , if you were to run a query like this 

It will recreate the table with no fragmentation and compute fresh index statistics. If that does not work then try: 

rsync is just a brute force byte-by-byte copy of data from place to another. You would have perform multiple rsyncs until the last rsync, which is very quick. Then, you would do a full shutdown of MySQL and perform one more rsync. XtraBackup is a comprehensive tool that behaves like rsync but has directed purpose in life. It can start off copying all InnoDB data and tablespaces. It has the ability to create checkpoints internally and perform in-place an InnoDB crash recovery to help get perfect point-in-time backup. XtraBackup also has an extra feature which allows for the creation of incremental backups. Another additional feature is the creation of InnoDB log files, also built by an in-place crash recovery. There is also some wrapper software to provide frozen copying of MyISAM tables as well. Both methods are great. XtraBackup simply implements InnoDB transactional features into most of the initial copying. Xtrabackup creates files that can dropped into an established MySQL infrastructure. Fair to say, XtraBackup provides instrumentational backup and materialized, very usable InnoDB files. Using rsync forces you to manage the copy process multiple times and cap it off with a mysql shutdown to require one intervention of rsync. Using one over the other may be a personal preference. One thing must be acknowledged: XtraBackup makes backups that are somewhat bigger that the data that was copied. I leave the choice of method to the reader of this question. I prefer rsync because it has simplicity of use, I can determine the specific point in time before the process begins, I also have total control as to locking mysqld or shutting it down, when such control can be performed, and in whatever order I designate. Both backups styles have one thing in common : With XtraBackup, the actual point-in-time is a moving target until the backup process is complete and you have to trust XtraBackup (hey 200,000 downloads thus far. Facebook is one of the biggest users of it. It has earned a lot of trust). In other words, if I start XtraBackup at midnight and the backup lasts till 2:20 AM, then the backup's actual point-in-time is 2:20 AM. Using rsync has the same moving target problem because you have to do multiple rsyncs manually and then determine when to issue a 'service mysql stop' prior to the final rsync. These methods differ in that mysqld must be shutdown using the rsync method, whereas XtraBackup acts independently of mysqld. In fact, XtraBackup does not interact with mysqld very much at all, except when dealing with MyISAM tables via the wrapper software. XtraBackup tries to build fully functional InnoDB data and log files using InnoDB storage engine operations separate and distinct from mysqld. Shutting down mysql is very necessary for the rsync method for a very big reason: The InnoDB Buffer Pool can hold "dirty pages", which are data and index pages (1 Page = 16KB) that have not been written back to disk. Using rsync does not catch the data changes pending in the InnoDB Buffer Pool. To speed up the shutdown of mysqld for the final rsync, please run this command: 

From this point forward, if the binary logs ever change in size, this quicky tells you that someone or something ran locally. Next, you run 

The WHERE is impossible to fulfill because default_wall_posts.userid not be 1 and 2 and 3 at the same time. Try these changes 

CAVEAT #2 The next time you get , please do not automatically assume that the table reached its internal limit. Always check the to make sure the disk itself is not full. 

That way, you only have inserts to deal with. Using will insert one row at a time. This help deal with duplicate issues, but you will have import like this: 

YOUR QUESTION Changing innodb_file_per_table to 0 does not make currently created tables just migrate into ibdata1. You have to manually move those tables into . This will not alleviate the issues. Why ? Regardless of storage engine, mysqld always checks for the existence of tables by first checking for the tables file. Then, the mysqld process will reference the table's intergrity based on the storage engine. In the case of InnoDB, the data dictionary of the InnoDB table is then checked. Two file handles are opened 

SUGGESTION #2 : Use tail command (if expecting one row of output) Run the command to a text file. Use tail to print everything but the first line: 

are only visible from a textual viewpoint in You can learn more about it from the MySQL 5.1 Documentation MySQL 5.5+ offers some metadata for live transactions 

ANSWER #1 This feels clumsy doing this in Windows but here it goes. If you are able to login to mysql, then run this script 

The PRIMARY KEY, or any index for that matter, would be accessed much faster if the length of the PRIMARY KEY was smaller. It is easier to put a 4-byte integer as a unique identified for a fullpath image name than the fullpath filename (of various and ridiculous lengths). Think of the Clustered Index, where the PRIMARY KEY would reside. Row data will occupy a Clustered Index. In MySQL, the Clustered Key would be coupled with other columns in a nonunique index. Wouldn't a smaller datatype (4 bytes) just make more sense? Otherwise, indexes can blow up at a rate of O(n log n). To create a unique number for each image, you need a table that resembles something like this: 

SUGGESTION #2 (Optional) To accommodate more than 255 swipe stations, perhaps should be SUGGESTION #3 (Optional) Some Transit Systems allow multiple swipes (up to 4 for PATH TransHudson) in one station. You could give some additional thought on this should you have to allow multiple swipes. UPDATE 2012-10-24 17:30 EDT If you are trying to limit within a time range, I have another viewpoint for you Here is the subquery within my answer: 

You had setup MySQL Replication, you could achieve the following: SLAVE You could do the following on the Slave in Session #1 

That's it. This is the same paradigm followed by Percona XtraDB Cluster (PXC). When it comes to PXC, introducing a New Slave in PXC is as simple as adding the MasterIP to my.cnf and starting MySQL. All of the above steps are executed internally by PXC using Quorom Selection to choose which Slave becomes the Donor (a.k.a. Volunteer to be Cloned) as well as one of three methods for copying data (xtrabackup, rsync, mysqldump) This copying method is known as (State Snapshot Transfer). If all of the Application-Level Data are stored in InnoDB only, you should look into using PXC. If you have a mix of InnoDB/MyISAM or all MyISAM, the above 9 steps are to be scripted by you. 

OBSERVATION #1 Since your buffer pool is 6G (6144M), the innodb_log_file_size should be 1536M (25% of 6G) OBSERVATION #2 You have sync_binlog set to 1. This provides the safest ACID compliant setup. It can also slow things down dramatically. You say . That's the case because each completed DML (INSERT, UPDATE, DELETE) and DDL (ALTER TABLE) statement gets written to the binary logs. The default for sync_binlog is 0. That let's the OS be responsible for flush binary log changes to disk. OBSERVATION #3 You have innodb_io_capacity set at 10000. That's really 10000 IOPs you are expecting of mysql. Try lowering it. There are some things to do in this respect 

This will create the table to have the exact same indexes and storage engine as the original table . OPTION #2 : Try creating the table with the same storage engine only, but no indexes. 

This could result in a slower load and a potential lopsiding of the index pages for nonunique indexes. CAVEAT #3 You can dump the actual InnoDB tablespaces (MySQL 5.5.12) 

This should replicate quickly except for the last statement. Give it a Try !!! If you have MySQL 5.1.16+, requires DROP privilege. My answer performs what now does. If you have MySQL 5.1.15 and back, you need DELETE privilege, which my answer covers. 

CAVEAT Make sure the network connection is clear (with no dropped packets) between Master and Slave. Give it a Try !!! 

CAVEAT Looking at the InnoDB/XtraDB Architecture, it says that the Undo Logs can grow unllimitedly. That is the source of uncontrolled growth in the system tablespace. Your Actual Question 

SUGGESTION #2 You are using MySQL 5.5 but I do not see any options to make InnoDB access multiple cores. Please add the following to my.cnf to give InnoDB some Enhanced I/O Performance 

See how mysql decided to create an ENUM based on current values? You definitely need to use the enum type or tinyint. Personally, I hate having to deal with bit field types. Personal feelings aside, since you want to use bit fields, let's transform all strings that are 16 characters to 1 and all others to 0. Here is what you can do to transform the chdata to a bit field 

That way, only and can do within the localhost. I would definitely have the on hand because there are times when mysqld loses connection to the file and prevents from connecting to issue a shutdown. With the , you can issue this: 

Here is something interesting you may want to consider If this table is very small, you can run this: 

You definitely have to script this via MySQL Stored Procedure Language Here is a Stored Function called to Retrieve a ParentID given an ID to Search For 

Checking time differences already calculated is better for performance than calculating it for every row. Give it a Try !!! 

and never disturb the Master. CAVEAT Since setting up Replication requires a stable mysqldump, you may need downtime for the one-time setup. My answer would be a lot different if your MySQL was in Linux. 

STEP 12 : Decommission ServerA1, ServerA2, ServerA3 CAVEAT : Please try all this out on Staging Servers Before Doing So on Production GIVE IT A TRY !!! 

All of these approaches show that you gave these things a lot of thought. You are worried about any pending changes when running . Think about this: When you issue , how is replication affected? Recall that replication has two threads 

DISCLAIMER : Very Rookie MongoDBA My guess would be the Document Size According to "MongoDB Limits and Thresholds" 

ASPECT #2 Since the table is MyISAM, why make mysqld jump through hoops to regenerate the table ??? If the table is and your datadir is , copy these 

You would need to run these queries against each database and the results. If you want to harness the INFORMATION_SCHEMA database then here is a wild guess: 

MySQL default for binlog_format in 5.6 is and in 5.7. You will have to set that in the DB Parameter Group. You may need to convert the binlog_format to on RDS Master, RDS Slave, and the External Slave. Only the can you configure a trigger on the External Slave. Interestingly, MariaDB has in its Documentation . You may have to switch to MariaDB. 

One sure way to speed up an ALTER TABLE is to remove unnecessary indexes Here are the initial steps to load a new version of the table 

UPDATE 2011-10-24 17:37 EDT I created a new table called observations and populated your sample data. I changed the stored procedures to use observations instead of pctable. Here is your output: 

The rows you are deleting is being written into the undo logs. The file ibdata1 should be growing right now for the duration of the delete. According to mysqlperformanceblog.com's : 

CAVEAT : root@'%' is normanlly not recommended. Maybe try root@'10.%' or any other netblock for root. Give it a Try !!! 

If you really need to look at all the records, you should take the strain off of MySQLWorkbench. It already has enough on its plate micromanaging MySQL. SUGGESTION #1 : Use MySQLWorkbench from Another Workstation Install MySQLWorkbench on another server that is not running MySQL. Remotely connect to MySQL from the other Workstation. By divorcing MySQLWorkbench from MySQL, MySQLWorkbench can focus on what does best, micromanage the data and display at the user's whim. MySQL can focus on doing key caching, query caching, InnoDB, DB Connections, read/writing threading, and all other memory-consuming tasks. This will end the tug-of-war for RAM by both products. Even the OS will reap more of the spoils of war for RAM. SUGGESTION #2 : Create a Text File and View it in some Editor Install mysql on Another Workstation. Do not run the server on it, but use the mysql client program. To get data from the table mydb.mytable, from the second server, run this: 

There are two parameters you should try extended-insert Someone wrote this post in the MySQL Documentation 

The only immediate harm I can think of is dealing with contention from the . Normal , , and commands execute a full table lock. Doing ON should work the same. In addition, what if the row's size changes to the point that the row needs to be bigger that the row's original allocation because of increasing the length of a column? That would require writing the row to a new part of the MyISAM table. The only exception to this concern is when the MyISAM table 

What password functions return What passwords are stored in What authentication style is the client launching 

You may have to resort to this if you cannot otherwise figure this out, UPDATE 2012-01-23 11:43 EDT Since you mentioned a certain table has the wrong storage engine, here is what you do: Look above at Step 03. Go find the table in the file. Look for in the file and change it to . Then, load the schema file first and the data file afterwards. UPDATE 2012-01-23 11:52 EDT You should add this to to /etc/my.cnf