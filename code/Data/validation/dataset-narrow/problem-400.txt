Here's an example that can be run manually in a single batch from SSMS. Just make sure to replace all occurrences of "2016" in the script with your SPID. 

I'm passing in a non-NULL value for , so I fully understand why I'm getting the error on SQL 2012. Questions: 

Create and start an Extended Events Session: it will capture events, filtered primarily by SPID. Execute a statement in a block. Within (or after) the block, read the XEvents session data. Use the available data to respond to the error(s) as appropriate. Stop and drop the XEvents session. 

I ran into the same issue you did. I would go on to discover that Event Notifications can be used to handle the event. (Note that this event can't be used with a DDL trigger.) I wrote an Event Notifications blog article that just happens to use the event as an example: SQL Server Event Handling: Event Notifications Below is a script that can get you started. 

More explanation: What you have tried with is the wrong approach. Let me explain with a small example. I'm creating the and are the temporary table for the example 

Looks like you are sorting by column in all the tables in order then . Instead of this approach all the tables and, in the final result you can the as . The working query will be: 

More explanation: this statement will return for the is and for the remaining entries it return only . So the also returns only. So inserting the value into the value causing the error. 

Some time we are providing same laptop for 4-5 employees in our office for some testing purpose (since a costly licensed tool is exist in that laptop only), also asked them to do not click the Remember password checkbox in SSMS to store the credentials. But few of them accidentally click that checkbox, so each time we need to clear the to solve the problem. Instead of that it is nice to disable the Remember password option in the SSMS in that particular laptop. Is it possible to disable the remember password checkbox in SSMS? The expecting behavior is as below: 

I have now sorted out the disk space issue and have re-run the full backup successfully, but I remain concerned about the log item above which references F:\sql\log because the machine does not have an F: drive at all, just a C: and an M: drive. It does appear that the 'Could not generate mail report' issue is related but again this error is a bit odd; I am receiving emails when the various jobs complete and I was alerted to the disk space error via an email, so I wonder why an error is logged if the email was sent successfully? I've dug through the SQL Server, SQL Agent, Database Mail and the maintenance plan configurations but I can find nothing which is set to F:\sql\log and so I have no idea why the maintenance plan is trying to write to that path, or why it is logging an error when email is sent successfully. Can anyone help me to work out how to resolve this? Thanks for any assistance. 

I'm having difficulty "optimizing" indexes as a minimally logged operation. Before the index maintenance is performed, the database recovery model is switched from FULL to BULK LOGGED. Depending on the fragmentation percent, each index is the recipient of a REBUILD or a REORGANIZE (or no action is taken). After index maintenance is finished, the recovery model is reverted to FULL. One database in particular is causing me some pain. The datafiles are about 64GB (including any free space). A defrag operation bloated the log file to 38GB, until it filled the logical drive. Then the level-17 alerts started rolling in. I tried duplicating this in a test environment. Numerous attempts were made with different recovery models, index REORG vs REBUILD, different transaction isolation levels, read committed snapshot ON vs OFF, different index fragmentation levels, etc. Index REBUILDs on DB’s in the FULL recovery model always bloated the t-logs. No other testing variations did, however. This was frustrating because I could not duplicate what was happening in production. What am I missing? How can I optimize indexes without bloating the log files? UPDATE 09/23/2015 Not surprisingly, Ola Hallengren's name came up soon after I posted my question. Although I don't use his scripts, I am somewhat familiar with them. For those who are interested, there is a wonderful PASS session video--Ola is the presenter. Near the 48 min mark, an audience member asks a question similar to mine. 

Each character required 4 bytes to store. That is the reason it showing as 4 times. So requires 120 bytes to store in the database. So in the describe command it showing the memory allocation sizes. 

As you mentioned the is , in your statement currently you are handling only for the value and not for the remaining values. If you add the in the case statement it will solve the issue. It will select for the entries other than So the working code will be: 

Working fiddle: $URL$ Answer for Why on Earth does this happen? Update: In your actual code you declared the as and used in the block, here the value is only. So the query is execute as that's why you are getting as result. 

The replace should be nested on other, not separate by semi colon. More over in clause instead of the you can use 

You need to use by instead of the . Using there is no need of the sub-query. Also before SELECT is not required after the . So the working query is: 

I have recently started using the dba_indexDefrag_sp stored procedure from SQL Fool ($URL$ against my production databases running on SQL Server 2012 (Standard 64-bit). When I execute the procedure I use all of the default settings, the only parameter I provide is the @database, which I use to pick out the active databases which need maintenance - for example: 

I moved dba_indexDefrag_sp and its associated objects into a 'HouseKeeper' database, rather than putting them in the Master database. (I note Michelle Ufford recommends this, saying: 'It’s up to you where you create it. You could technically create it in the MASTER database, but I recommend creating a utility database for your DBA administrative tasks.') I created a new 'SQLJobRunner' Windows login, then added it to SQL Server, setting it as db_owner on the dbs I'm defragmenting. I also applied to same to the HouseKeeper db, to give the login full access to that database. I changed the defrag SQL Agent job to run using the 'SQLJobRunner' login. 

2nd Attempt Here I opted for a variable to store the total number of users (instead of a sub-query). The scan count increased from 1 to 17 compared to the 1st attempt. Logical reads stayed the same. However, elapsed time dropped considerably. 

Although I don't have a local copy of the Stack Overflow database, I was able to try out a couple of queries. My thought was to get a count of users from a system catalog view (as opposed to directly getting a count of rows from the underlying table). Then get a count of rows that do (or maybe do not) match Erik's criteria, and do some simple math. I used the Stack Exchange Data Explorer (Along with and ) to test the queries. For a point of reference, here are some queries and the CPU/IO statistics: QUERY 1 

There's a lot of moving parts there. But you might be able to make it work for you. I wrote a few related blog posts about TRY...CATCH that may be helpful: The Unfulfilled Promise of TRY...CATCH Enhanced T-SQL Error Handling With Extended Events Part 2: Enhanced T-SQL Error Handling With Extended Events 

I don't understand why it fails as a SQL Agent job yet succeeds 'interactively'. Is there something about that INSERT statement which needs special attention? I would appreciate some help to allow me to resolve this problem. Thanks. 

and then it worked, which deals with my issue. It is notable that I have the same security setup against my own login and that still doesn't work. Odd... but that isn't really a problem for me now - I don't know why it that doesn't work though. Note that I'm aware that giving the login db_owner is a bit of a sledgehammer approach, but now that it works I'm able to make the security a bit more fine-grained. Thanks! 

If I run this from SQL Management Studio, on the server, the procedure executes to completion, doing its work and recording its activity, taking about an hour and a half. If, however, I create a SQL Agent job for it, using the same login account (my own) to run the job then I see the job execute successfully BUT I see that the job completed in 0 seconds and the job history indicates an issue: 

1st Attempt This was slower than all of Erik's queries I listed here...at least in terms of elapsed time. 

This is a sad commentary on the state of software development and deployments that rely on an RDBMS such as SQL Server. The development and/or DevOps teams should know what level of authorization is needed by applications, both at runtime and at deployment time. It's very important to make the distinction between these two. Unfortunately, development and/or DevOps teams never figure this out because {reasons}. So here you are... You've mentioned that you are familiar with fixed server roles and fixed database roles. I don't want to insult your intelligence, but I think it's worth mentioning for those that are not familiar. At the server/instance level, the CONTROL SERVER permission is a much better option than membership in . At the database level, I would never let a non-dba own a database. They connect as -- DENY permissions have no effect. You can deny permissions to a database user that is a member of -- this is also a better option than membership in . But it is still usually overkill. I generally prefer to give a database user membership in , , and . Additionally, I'll GRANT EXECUTE, CREATE SCHEMA, and VIEW DEFINITION. Based on your needs, your mileage may vary. Another thing to note: you cannot DENY permission on DBCC commands. Members of can do a lot with DBCC, and to a lesser extent, members of can too. After all of that, I've still had to give authorization to non-DBA types on a regular basis for deployments. In those scenarios, the requestor had to submit their request to a Change Advisory Board for approval. Membership in was temporary and I always set up a SQL Agent job to remove group membership after an agreed upon amount of time (usually one week). I did this enough times that I wrote re-usable code to quickly create the necessary job, job steps, schedule, etc. Moving on... Whether it's temporary or permanent, you still have to deal with non-DBAs that have elevated permissions. SQL Server has some built-in features that can help you handle events that you might not want to happen: DDL Triggers: these are a great way to handle certain SQL Server events synchronously. Maybe you just want to know an event happened. Grab the and send an alert or log the info to a table for later inspection. If you're more hands on, you can inspect the as events occur and optionally ROLLBACK the bad stuff. It's really powerful. I kept an eye on ALTER_INSTANCE, ALTER_DATABASE, DROP_DATABASE, and a raft of authorization-related events. (Note: ROLLBACK doesn't work with the ALTER_DATABASE event.) EVENT NOTIFICATIONS: some events can't be handled synchronously with DDL triggers, but they can be handled with event notifications. ROLLBACK is not an option, but you can at least grab the for post-mortem analysis. This option involves Service Broker, and the learning curve to get started is much higher than working with DDL triggers. I think it's worth it, though. There's nothing to prevent a member of from "sidestepping" a DDL trigger or Event Notification (other than their own ignorance). But that's a severe offense IMO, worthy of disciplinary action. Another feature you may want to look at is Policy Management. I've not used it myself, but it appears to be quite powerful and flexible. It may also be a bit easier to use as compared to DDL triggers and Event Notifications. I'll let you be the judge of that.