Just because if you're getting started with transaction troubleshooting, I'd rather give you something easier to use than DBCC commands. 

There are four tough problems to solve when trying to anonymize data. I'm going to summarize an old blog post of mine about it: 1: Anonymized data may grow in size. If you have peoples' names, for example, you don't really want all of the anonymous ones to have exactly the same length as the original data. If you change the data as it goes out, that means the solution needs to understand the original field name's length to avoid growing data out to something too large to fit. 2: Anonymized data has different statistics distribution. Say you've got a date-of-birth field - you could completely randomize the dates, but then it'll be much less meaningful on the other side, especially if you're trying to do performance testing. Plus... 3: Anonymized data breaks referential integrity. I know it's a bad idea, but sometimes applications join on fields that need to be anonymized. I hate that I've seen this, but I've seen folks using email address in multiple tables, and then expecting to join on those, or find related orders. 4: Anonymizing slows down the export. You'll need to run some kind of business logic to determine which columns need to be anonymized, and then generate the new data in its place. I used to say that if you could solve this problem, you'd be rich - but then you've got some competition. Delphix does data masking for DB2, Oracle, SQL Server, etc, but I don't think they've got Postgres or Heroku covered - yet. 

As well as the two main data tables, there are several other tables containing lists of suppliers, project codes etc., which are used as suggestions in the drop-down lists on the forms. At first, I had these as linked tables as well, but performance was slow. Each drop-down box would take a few seconds to load. Now, these tables are embedded into the front-end. My idea was that people who need to edit these tables could do so within the master copy of the front end. I have built forms to add / remove codes and suppliers from the lists. However, this solution is not going to work as-is. The master file is a pre-compiled .accde file, so when I need to create a new version (from my master .accdb file) any changes made to the lists will be lost. Current idea: To have two copies of the 'auxiliary' tables in the database - one linked locally ("C:\Databases") and one to a master copy on the server. The forms would populate from the local copy. The editing section would alter the server copy. On login, the server auxiliary tables would be pushed to everyone's local drives along with the app. This seems awfully complex for what isn't a complex situation. It also brings about strange consequences for the user; editing the tables doesn't actually do anything to the application until they next login (and sync the tables). 

When a drop-table is waiting for more than a second or two, run Adam Machanic's sp_WhoIsActive. It's a great tool to find out why any query is running slow. In the waits column, you'll see the waits for the specific task, and if it's stuck waiting on CPU (which won't show up in sys.dm_os_wait_stats) you'll see that too. Also, I'd be remiss if I didn't point out that your build is out of support. As a consultant, I tell my clients, "I would hate to do hours of troubleshooting only to find that it's been fixed in a service pack - and end up billing you thousands of dollars for something you could have fixed for free. Plus you're out of support anyway." You can find the most recent SQL Server service packs & cumulative updates at $URL$ Build 3042 is way out of date - the supported version is SP4. 

In order to answer that question, the first thing we would need is the actual execution plan for the query. In SQL Server Management Studio, you can get that by clicking Query, Include Actual Execution Plan. Run the query, click on the Execution Plan tab, and right-click anywhere in the whitespace to click Save Plan As. Save that, and post it somewhere for people to download and examine. The next thing we would need is the output from DBCC SHOW_STATISTICS for the stats on that table. You've hinted at the output, and that's a good start, but the raw output will help us understand exactly what's going on. 

I'm considering SQL, NoSQL and RRD to save the data. I put RRD here because it's easy to implement the insert of the data into statds+graphite. I'm concerned if I take this approach, the querying (although provided by graphite) will not be indexed and will probably have to count all the data whenever I ask for a window/user (no indexing). Another problem is that when querying all the data, all users info will be needed, resulting in reading all the files concurrently which I'm not sure is a good thing. SQL - Very easy implementation when inserting the data and querying. Easy to index, order and group by. However I'm not sure it's easy if I'm anticipating high traffic. Also, I'm not sure how effective is the count() of sql (haven't used SQL in the last few years) after group by. Can it offer parallel computation? NoSQL - Is there a solution out there that is the right fit for this type of scenario (perhaps a Map/Reduce algorithm to fast generation of counts in a time window?) Thanks for helping me model 

innodb_buffer_pool_size is 5.7GB. innodb_log_buffer_size is 8M. max_heap_table_size is 256MB. Is the performance ok? for selecting distinct over ~500k records? Should I be considering something else? EDIT create statement: 

With careful index design and partitioning, you can get away with minimal disruption to other queries. Discussing how to implement partitioning is a little beyond what we can talk through here - check out Louis Davidson's book on SQL 2008 database design. It covers concurrency pretty well. 

And if you're like Evil Admin Brent, change the SA account's password while you're at it. You should probably be rotating it periodically - especially if it's been the same for years, and you've had turnover in the team. If the backup app uses that login, presto, it will stop working. (Of course, so will your backups, so hopefully you have another solution for backups like natives.) 

At the bottom of the page, in the examples, there's an example to back up all user databases with compression, encryption, and a server certificate: 

I do know a consultant who does this kind of work, but it's nowhere near cheap - think mid-five-figures to start the project, paid in advance. It's not for the faint of heart and you only want to go that route as a last resort. 

For me, SARGable means that SQL Server can perform an index seek using your search predicates. You can't just say the DBMS can "take advantage" of an index, because with a non-sargable predicate, SQL Server may end up scanning a nonclustered index. 

I have a query that produces graph data from SQL. The div is to group the numbers into fix dates so it will draw a nice graph (the const is determined beforehand to try and divide the data into ~100 points in time). This query isn't running fast enough for tables with >1M records. The first optimization I thought of was to save some predefined DIVs in each row (using trigger), index them and group by the closest one needed (determined in app). Any thoughts? 

I'm trying to understand whether I'm doing something wrong, or expecting too much. I have an InnoDB table on a db.m1.large RDS (MySQL 5.6.13. 7.5 GB RAM) with ~1M rows. The primary key is a composite one. Having 1 TIMESTAMP and 3 BIGINTs. pk1 is the TIMESTAMP and pk2 is a BIGINT. I don't have any other indices, only the primary key. Both of these queries run for ~1.2 seconds. 

I have an app where user is known (user_id) and he can do several actions (action_id). Every time he makes an action I need to save the fact that he made it for reports/analytics. I guess it is similar to other analytic solutions and their db design. Once I have the data, provided with a time window (minutes resolution) I need to count for each user (all or some) the number of times he did actions and which actions he did. (sum all data grouped by action_id). Some assumptions: 

Why would Microsoft build it? Well, if they want to let someone get a file list without running a RESTORE command. Could be a separation-of-duties thing, or could be making the automation plumbing easier for Azure Managed Instances. 

SQL Server's still scanning the whole table to build the sorted list just to give you your rows 100-200, and the cost is still around 30k. Even worse, this whole list will be rebuilt every time your query runs (because after all, someone might have changed their DisplayName.) To make it go faster, we can create an nonclustered index on DisplayName, which is a copy of our table, sorted by that specific field: 

The query would come back with no rows - because there wasn't a Ferrari in the garage. (At least, there weren't any rows found in my own garage.) Query 2 is different: 

Whenever you've got a performance problem, start by asking, "What's my top wait type?" Wait stats tell you what SQL Server is waiting on. I like measuring waits with sp_BlitzFirst (disclaimer: I wrote it.) It's a free, open source, MIT licensed script that you can call to take a sample of your waits, like this: 

Stop people from breaking the server When they do break the server, be able to identify exactly who did it 

That is, if I look at the history of vial #7 in the above image, I want to see the history of vial #7, (reverse chronologically, for example), followed by the histories of vials #6, #3 and #8. I'm struggling to find the best way to record this data; currently I have one main table (plus several lookup tables) which has one line for each 'node'. A node represents a vial being in a location; so each line has an AutoNumber primary key, a 'parent' (the primary key of the previous node - either the previous location of the vial, or the parent vial if it was aliquoted), a 'location' field, the batch number etc. There is already redundancy here because the batch number is on each node whereas it really only needs to be on the first node after manufacture (it clearly doesn't change when the batch is aliquoted). So the data structure is the first issue here. Once I've got the data structure, does anyone have any ideas for how best to show historical data for a vial? Many thanks 

Am I going about this the right way? Surely there is a better way of handling this sort of requirement? 

I'm designing a database in MS Access which is used for tracking the locations of a variety of chemical solutions, made in batches. Each batch can be split (aliquoted) practically indefinitely. That is, we may manufacture 100g of a product, which is aliquoted into two 50g vials. One is kept in storage, the other is shipped somewhere else and further split into 50 1g vials. These vials may be transported at any time, and may be aliquoted further. I would like to be able to track the history of any particular vial. Where it has been (and when), including the history of its 'parent' vial and the parents of that. 

Glad you liked the post - it was by Jeremiah Peschka though, not me. (I know because I use it all the time myself!) Make sure to read the entire article, though, which includes the answer you seek: 

Think of the "Maximum server memory" setting in your right screenshot as a limit setting. You can configure SQL Server so that it won't use any more than a certain amount. By default, this number is effectively unlimited. (That's what your server is set to.) It's kinda like setting your own personal financial budget - by default, many of us are willing to spend an unlimited amount of money on things that we should probably avoid. Generally, you want to cap SQL Server's memory consumption, and there are varying recommendations out there. In our SQL Server Setup Checklist, we recommend leaving 10% free for the OS as a starting point. That means you'd set SQL Server's maximum server memory to 512 * 90% = 461GB, so you'd put 461000 in the max server memory box. (It's measured in MB, not GB.) It might seem odd to leave over 50GB of memory free, but I've seen what you people do on your SQL Servers. You remote desktop in, you run SSMS, SSIS, SSRS, SSAS, and Outlook from the SQL Server. If you run a tight ship, and you never remote desktop in, then you can go higher on max memory. (Although since you're remote desktopping in to run this stuff, you should probably leave memory free. And also, stop remote desktopping in.)