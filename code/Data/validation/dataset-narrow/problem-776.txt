Is mod_status enabled? $URL$ is an example output, and it has requests/s since startup and grand totals. 

Can you connect with telnet from the machine running varnish to the IP/port apache is running on, when the 503s appear? Did the varnish machine or process maybe run out of filedescriptors? Did apache maybe hit the MaxClients? What are the values of backend_unhealthy, backend_busy, backend_fail (see $URL$ as well). Do you use health probes with varnish? 

Are you sure it's not cached at your client? Have you checked with a different browser or a cleared cache? I've found that at least firefox is pretty stubborn in caching the cpanel default redirect. 

It's likely because you are not benchmarking varnish at all, but are benchmarking for instance the CPU, the scheduler or threading model of your operating system, or your virtualisation platform when several machines are using a lot of CPU. If run at pretty much the same hardware, ab and varnish might be fighting over the same resources. One of the authors of varnish has a nice writeup about benchmarking varnish, and ab is most likely not a useful tool: $URL$ 

Your server most likely did not freeze, it was unreachable. The default rule for ipfw is to deny everything. You can recompile the kernel with " options IPFIREWALL_DEFAULT_TO_ACCEPT" set, or add ";ipfw add allow all " to your command (or build a script that flushes and adds your rules at once). 

Ubuntu-1004-lucid-64-minimal is not a valid global name, tell postfix to use your "real" domain name, using the myhostname setting: $URL$ 

Which hostname(s) did the puppetmaster generate its certificate for? The puppet client expects the certificate to be valid for "puppetmaster", but it doesn't seem to be issued for this hostname. I think "puppet" might be the default CN on the puppetmaster, or else the hostname of the server. You can check it by running "openssl x509 -text -in cert.pem" on the certificate of the server, or connect to with a browser, and see which domains are in the CN and dns_alt_names of the certificate. 

In the end i couldn't find a way to map the mfiutil to a device. I'm sure there is a way but it escapes me. In the end i rebooted into the raid controller bios and luckily as the drive was completely dead it was showing in the controller. I think if i had studied megacli i could have realised this without rebooting, which would have been better. But in the end mfisyspd5 actually mapped to E1:S9, serial 1EJ49HWH But overall, if you want to run ZFS, don't use a RAID controller even in passthrough, just get a HBA. Will save you hassle in the long run. 

What's next? Updates: Further reading shows that this problem would be simple if the drives were not attached to a raid controller (all be it in passthrough mode), as then smartctl would provide the information required. Using mfiutil: 

The above triggered the mentioned symptoms almost immediately. RAID shows healthy, however I'm unsure if I have the optimum caching configuration enabled: 

I've followed the Mitaka setup guide for my first OpenStack cloud. This all went ok (2nd time around!), however i'm now having issues with networking. The instance launches ok, and it's assigned an IP via DHCP, but it won't ping. I don't know if my network is setup right, so i've provided the appropriate outputs below. I setup Mitaka with option 2 - 'self service networks', but for simplicity i've only created a Flat DHCP 'provider' network just to get a feel for things. On my test setup I only have 1 working public IP address, x.x.x.111 

I have a failed drive in a FreeNas server hosted at OVH. I need to get the drive swapped, but i'm extremely conscious of them pulling the wrong drive. FreeNas isn't reporting any serial numbers in it's GUI. I have done the below so far, I don't know how to either get the drive serial, or better yet blink the LED? 

This will not reorder them if they're in the wrong order, but it'll at least add them in the correct order right below the search statement (possibly listing 4 name servers which is more than 3 (MAXNS in resolv.h), but with just file_line resources avoiding this might be hard or impossible). Also, the parameter is specific to file_line resources, hinting where to insert the line, and is a general resource parameter talking about resource ordering. 

Did the varnish process maybe restart? There's an uptime counter in varnishstat. Under certain circumstances the varnish worker thread can die, but it gets restarted immediately. When everything is working fine, this might go unnoticed, but with (planned) backend down time it can be quite inconvient. 

In addition to the comment of BÃ²ss King, you can also simply specify several addresses seperated with a comma: 

EDIT You have a certificate only for "master", but your client connects to "puppetmaster". So either the client needs to expect "master", or you need a certificate for "puppetmaster" on your master. A "certname=puppetmaster" in the [master] block in puppet.conf will change the CN on the server ($URL$ You may need to remove the old certificates, but I am not sure about this. Or, you can have the client connect to "master", either by adding it to /etc/hosts, or to your DNS zone if you're running one. 

Putty uses its own format for the key files, not a standard format. ssh can not use ppk files. I believe puttygen should be able to export your key to something more usable for ssh. 

It's not a DNS issue, browsers tend to cache the default redirect from cpanel quite long and persistently (sadly it's a html refresh, ). Did you also clear the regular browser cache from chrome, or tried with a private browsing window? "example.com/path/to/file.php" works because you probably didn't visit that URL before the site was properly set up in cpanel. 

To streamline the process we would like to install the driver while the machine is online. Unfortunately dism is requiring the disk to be offline, returning the error "This command can only be used with an offline image". We have tried pnputil to install the driver but this does not work, we believe this is because pnputil is for plugged in hardware with no driver currently. Is there a way to install a driver into an online image? 

I believe this is simply a case of a bad idea using RAID 5 in this situation, but I would like to see if I have missed something else. Hardware: 

Also, should i look to make the conntrack settings on the DNS server more aggressive to close connections faster, these are the current settings on the DNS server: 

Ubuntu default guided partition layout, non LVM, completely unmodified. Confirmation of partitions, including ESP is present. 

In Grub, from my understanding, the ESP should be mounted at /boot/efi. However, Grub shows errors as below, but can also list / normally, however anything inside / such as /etc has the error, including /boot: 

I'm looking at moving our XenServer cluster (150 VMs) to OpenStack on KVM. After extensive reading it looks like virt-v2v will do this. However I'm confused about it's usage. I was going to copy the VHD file and then run virt-v2v on this, then import into Glance, and start an instance. However it appears this isn't the process virt-v2v uses. Could someone explain the overall process, how to use virt-v2v or any other tool(s) that I will need to convert VMs from XenServer to KVM and import into OpenStack. The two 'clouds' are separate hardware, over the internet - so i would like to avoid shared storage between them if that's possible, however if it makes it too complex we can sling up a VPN between. 

If your out-of-band management does not allow you to flash the indicator, you could try ethtool if you have a spare/empty network interface 

Then you can go search the server with an empty but blinking network interface. This should also work for interfaces that are up and running (if they're all connected), but then you'd have to distinguish between the ethtool regular interval and the normal blinking that shows the interface activity :) 

$URL$ has an explanation. This command (re)sets the argument variables ($1, $2, $3, $4 and $5 in this case). gives you a line with some numbers, feeding the output to maps those numbers to the $N-variables. See for instance: 

Do you absolutely need to use varnishncsa? Varnishlog (with some extra command line options or shell piping magic) can give you the cookies (and their contents), and the XID, easily. I doubt the full content of the POST requests is available to log. 

NFS is from 1 client to 1 server, so the overall performance is limited by the performance of that 1 server. Adding more servers does not help. Lustre splits the data, the data gets requested from 1 server, but can be sent from one or more other servers. So adding more servers does help (which is why "Lustre scales"). This is an important bit from your first link: 

Both Ubuntu ($URL$ and Debian ($URL$ support upgrading. For Ubuntu, my personal preference is to upgrade from LTS to LTS every 2 years on servers, and if time permits upgrade to every new release on a desktop. People have mixed experiences upgrading both these systems, but with reading the release notes and planning carefully, upgrading never broke anything beyond repair for me (and often, nothing ended up broken at all).