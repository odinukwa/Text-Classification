TO move data, depending on the amount/size of data and n/w bandwidth, Linked server will kill the performance. 

There is no way to clear/reset statistics for , , and just like . Refer to Clearing “missing index” suggestions for a single table 

One option would be if you had a running while that incident happened or you have to dig into the transaction logs using (this is undocumented and unsupported). Below query might help if the data is still persistent in DMV: 

Which is most robust? - Test it out what suits your requirement. Do not drop the entire db or table - what is the point to drop and recreate .. when you are reusing it anyway. Recently I had a requirement to import 3+ GB csv files into sql server. Below is what I did to achieve 100K rows/sec (YMMV due to hardware and environment difference) on a physical machine (was a little less on a VM ) using SSDs and 16 cores and 256GB RAM. 

Other thoughts: Since you are already using Enterprise Edition, why not use Database Snapshots - with caution !!. From Looking at Database Snapshot Performance 

If you want to disable check constraint, then when the wizard asks you to save the package, save it and then edit the connection manager as below : 

A readonly database is "READ_ONLY". No writes are allowed - the application will get an error stating that the database is readonly. You can run a server side trace (since you are stuck with sql server 2005) and look for and filter your database by ID. 

You should use powershell for automation and backup restore for a guranteed way of migrating your databases. For powershell, use dbatools --> or for migrating entire server with logins, jobs etc use 

You can schedule the script to run at specific time or just run it on demand. Refer to the links that I have suggested above. 

A rare scenario, but its good to have it backed up as part of best practice. A SMK () is the root of encryption and is generated automatically the first time it is needed to encrypt another key. If SMK is corrupted, then you might need to restore it. For 3 and 4 .. You can, but why would you do it ? You have to turn off encryption, create a new cert with new Private key and then turn on encryption. Depending on your database size, this can take some time. Remember that a DEK is the one that does the encryption and decryption of the database. As a side note, refer to this script for info on the databases, encryption status along with other useful information. 

I had issues setting the database in single user e.g. a sql agent connection might make it first and then you will struggle to take that database out of single_use. Instead, I prefer OFFLINE. 

You can download SSMS 2012 or 2014 which is a full seperate download and then use it to access tables, etc link: ssms 2012 can't comment on what/ how your plugin is. 

There are couple of things to consider : backup and restore is the safest option that you can use for migration from SQL 2005 to 2012. Refer here for more details of what you are experiencing. Full text has undergone major chance in 2008 and up. from BOL, Attaching a SQL Server 2005 Database to SQL Server 2012 In SQL Server 2008 and later versions, a full-text catalog is a logical concept that refers to a group of full-text indexes. The full-text catalog is a virtual object that does not belong to any filegroup. However, when you attach a SQL Server 2005 database that contains full-text catalog files onto a SQL Server 2012 server instance, the catalog files are attached from their previous location along with the other database files, the same as in SQL Server 2005. The state of each attached full-text catalog on SQL Server 2012 is the same as when the database was detached from SQL Server 2005. If any full-text index population was suspended by the detach operation, the population is resumed on SQL Server 2012, and the full-text index becomes available for full-text search. If SQL Server 2012 cannot find a full-text catalog file or if the full-text file was moved during the attach operation without specifying a new location, the behavior depends on the selected full-text upgrade option. If the full-text upgrade option is Import or Rebuild, the attached full-text catalog is rebuilt. If the full-text upgrade option is Reset, the attached full-text catalog is reset. EDIT: You can attach the detached database along with the fulltext catalog using Create database ... FOR ATTACH. 

Another area to look into is the Default trace which is ON by default. The default trace logs DBCC Events --> Audit DBCC event. So the event gets fired when a DBCC command is ran. Remember that default trace will have 5 max files with 20MB size limit. So if your server is busy, you might miss the event. Alternatively, you can have a WMI alert triggering the load of the trace file into a SQL Server table when a rollover happens. 

Logshipping is your best option for Disaster Recovery since you are on standard edition. Make sure you test the disaster recovery - simulate it so that you are confident when a real disaster strikes. Note: In my company, we use LogShipping as a primary DR solution involving 2500 databases globally and it works perfectly fine and fulfills our business continuity goal (it worked during Hurricane Sandy as well). We automate most of the things and test(simulate) every quarter per individual database and yearly the entire data center. THis gives us a huge confidence when doing an actual failover during a disaster. Check out the things to keep in mind when going for Logshipping : 

What you can do is to have below script stored on your server or make it as a stored procedure : use below with sqlcmd 

This is one of the Myths that Paul Randal clearly debunks in his famous series - A SQL Server DBA myth a day 

This is your disk subsystem causing issues. Check your \MSSQL\LOG\sqldump folder. You will have stack dumps created. You can analyze it as per SQL Server Latch & Debugging latch time out or open a case with Microsoft. Again, check your disk subsystem. 

Since you are running on sql server 2012, you can use my script to find out what trace flags are set as well. 

I would not recommend using service broker for this scenario. Transactional Replicaiton is the best option. It even has the ability to filter only required tables, columns, etc as per your need. T-Rep is asynchronous and since it is the same server, there would be near to zero latency. Out of Curiosity: what is the purpose of sending same data to 2 other databases on the same instance ? Reference: Stairway to SQL Server Replication 

There are many ways of how you can do database capacity planning. msdb backup history if gets regular trimmed, you wont be having much data left for analysis As Mark pointed out, it can be done using the method described by Erin - trending database growth from backup. You can even use PIVOT to find out the database growth over a period of 12 months from the backup history as below : 

For the copying backups from one site to another, it depends on your network bandwidth and the size of the backups. Highly recommend to use robocopy as it is n/w resilient - has retry option along with logging and many more. As you are using web edition, you wont be able to use backup compression unless you use custom compression techniques or redgate's sql backup. 

This is very common for VLDBs. You can be a bit smatter and follow Paul Randal's Quick list of VLDB maintenance best practices. You can rebuild indexes ONLINE in Enterprise edition. In your case since you are using Ola's Index Optimization scripts, you are running into endless index defragmentation which you can solve by being analyzing table I have given few more thoughts on Is a weekly rebuild of indexes a good idea? covering maxdop option. I would say, if your databases are extremely big (TB size), atleast do a reorg instead of rebuild and manually update stats. This will put you in a good reasonable state since a reorg will start from where it got killed. For running in parallel threads, you can drive of table and use powershell runspace (be careful of not spawing many threads - see Notes on usage of using runspace).