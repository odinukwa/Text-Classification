The second line projects the xy normal components from view space. Basically you'll end up with a vector describing "how much" the normal point to right and up direction relatively to the screen. 

The Color retrieved with getPixel is different from the Color set before. I initially thought about float approximation, but when inspectin col the value stored are correct, so can't be that reason. It sounds weird even a sampling error because the getValue returns a value really similar that not seems to be interpolated with anything else. Anyway I tried even to add these lines after building the texture but nothing change: 

Polling code looks ugly Performance: you are doing a lot of unuseful polling checks at every frame, in order to catch an event that is some way occasional (sword hit enemy) 

EDIT For repeat the texture you have to modify the scale not the offset. If you set for example a scale of 2 along an uv axis you will obtain to repeat the texture twice along it. 

I have several scripts attached to static and non static models of my scene. All models are instantiated at run-time (and must be instantiated at run-time because I'm procedural building the scene). I'd like to add a or to my models at runtime. With non animated models it works simply requiring component from the script attached to my . is created of the right dimension. Something like: 

are initialized with their default value. Btw in the most common scenarios when you hide them is because you are setting them explicitly somewhere else, so it's up to your custom code to make the relevant checks. In any case unexpected behaviors are potentially the same of a normal serialized property. 

So in the first function you determine if the d3d9 device will work for your program or not, and return true if it does. In the second you tell it what to do if it loses the device (if someone minimizes the screen for instance). In the third you allow the user to change the device settings, and.... I'm actually not too sure about the fourth, I use DXUTOnFrameRender instead. It's annoying that Microsoft took down the documentation for the DXUT library. If you Google any of those function names you'll find the Japanese version of MSDN, which somehow survived the purge. If you really want a complete example of the DXUT library in use, try the Game Coding Complete code: $URL$ The file GameCode4.cpp is the initializer, where they use these function declarations just like you typed them in. The bodies of the user-defined functions are in gamecode.cpp and gamecode.h I believe the DXUT tutorials are also in the DirectX SDK for DX9 

When programming Windows games, do you use multibyte character settings or Unicode character settings? I'm under the impression that professional companies today use the Unicode setting to make future localization and translations as easy as possible. Is using wchar's a good practice for modern games? While I'm thinking about the subject, what is the difference between multibyte character sets and Unicode sets in Windows? Is it UTF-8 vs. UTF-16? 

From here, you can use the GetNodeLocalTransform() function in KFbxAnimEvaluator to get the transform matrix at specific times. It will give you a final answer matrix that you can pull the TRS info out of. I know that's vague, but my specific code is not on this computer. Hopefully it's high-level enough to help though. 

If you extend AssetPostprocessor you can receive a callback(OnPostProcessTexture) when a texture import is completed. You can use that entry point to automatically create a new material and assign the relative texture to it. 

It's slow because there's no caching of in the Mono side of the scripts. Every is forwarded to the native code which iterates through all components attached to a given . More objects, more calls. More components more thing to iterate through. Definitely an overkill in this specific context. 

The first simple approach is to use a rigidbody with a collider attached to the player (let's say it could be even a shere collider) and use a collider for each object in the scene (coins, obstacles, bonuses,...) . AFAIK, using hundreds of colliders could have a serious impact on performances,specially on mobile devices, am I right? So here's some questions: 

For more details on coroutines have a look here. EDIT Sorry for the mistake. StartCoroutine isn't a GameObject method but belongs to MonoBehavior. So for what concern your question on extension methods: in order to use StartCoroutine method you need a reference to a MonoBehavior. I don't think use an extension method is a particularly good idea here, btw you can't add an extension method to GameObject because it's not a behavior itself. If you want to extend MonoBehavior: 

You have to use StartCoroutine (probably not in the Update) in order to start its execution. Otherwise it's only a method call: 

After having used it a little bit, I think it's wrong. If I supply the same vector as input (beginDir equal to endDir), the cross product is zero, but the dot product is a little bit more than zero. I think that to fix that I can simply check if the cross product is zero, means that the 2 vectors are parallel, but my code doesn't work. I tried the following solution: 

I'm reading a fantastic article about game timer precision and here is a quote about 2/3 of the way into the article: 

$URL$ Here's a link to a few tutorials that show how to use OpenGL in LISP. Beyond that, so long as you are practicing, you should be able to structure some samples and eventually some games once you learn how to use OpenGL properly. Also, though they are old, the old NeHe tutorials at $URL$ have some LISP translation of at least their earliest tutorials. 

Let's say you wanted it to spin 360 degrees in one second. Let's also say that iTime determines how many seconds since the function was called. So if iTime was .01 then 1/100 of a second had passed since the last time the function was called. Then you would use iTime / 360 to figure out how much rotation you'd need to apply. 

Your question is general enough that there's probably no definitive answer to it. I think you could find the answer in the book Game Coding Complete though. It describes a process manager that allows you to chain processes together, so that once you achieve a step in a mission the next one would automatically activate. They discuss how you could even make a C# tool that would let you drag and drop to create mission trees in this manner. Might be the kind of architecture you're looking for. 

The DXUT Framework requires you to define a number of functions in order to plug into it. Each of those names - IsD3D9DeviceAcceptable, OnD3D9LostDevice, ModifyDeviceSettings, and OnFrameMove - are functions that YOU need to define in your code. The signatures are as follows: 

Lots of answers here, but I want to chime in with my personal experience. I'm in the grad program at DePaul University in Chicago. They have a number of degrees, but one of them is Computer Game Development. In my personal experience, this program is spectacular and I can personally vouch for it to anyone who is considering DePaul. The main reason is that the degree is teaching game programming from a low level. It teaches the ins and outs of C++, to the point that I'm referring to the spec rather than online tutorials. It teaches OpenGL, both fixed and programmable pipeline. The goal of the program is to make you an excellent problem solver in the framework of video games. The people who leave this school can dig into call stacks, disassembly, and really get down into the nitty gritty. I would be hesitant to recommend anyone get a game degree that does not teach low level programming like this. On the other hand, I think anyone who wants to get into the game industry would do extremely well for themselves if they looked for any kind of computer science degree that digs into low level stuff like that. When you go to apply to game programs, showing an incredible knowledge of C++, and how the code you write in it translates into assembly, will really impress them. They're looking for engineers and problem solvers, not just people who have a degree that says 'Game Programming' on it. 

You can move a GameObject along the curve you can varying parameter inside your Update function. For example: 

Groups the objects on a per-room basis. Each room has a root that you should enable only when you enter in it (or are next to it), and disable the room you were coming from. Maybe it's easier method, but you will load in memory all the room even if only the active one is rendered (if you haven't too much rooms it should be acceptable). Use occlusion culling to limit the rendering to non-occludee objects. You still have all objects load into memory, but unity takes care to decide what to render and what not, based on its visibility. Use a different scene for each room, and load(load additive)/unload scene(rooms) when you no longer need them. This generally should reduce the memory footprint, but can create a bit of garbage to be collected, so it may be a little more tricky do handle properly. 

Get a reference to the the specific instance you need of is attached to. Get a reference to the component attached to the Access the public field 

I think it depends on how many per pixel lights are illuminating your objects. Unity, at least in forward rendering path, uses several additional shading passes while rendering your objects. It renders before the lights marked as (or and promoted as important by the engine itself). This kind of lights are rendered per pixel. The maximum number of those lights is set in Quality Settings-> Pixel Light Count (the engine allows up to 4 per pixel light). The other lights are rendered per vertex. Spotlights need to be rendered per pixel to actually see the spot (well..it depends on the geometry of the lighted object, but for example a spotlight can't light correctly with a per vertex shading on a quad). So if your lights are popping on/off this might due to: 

Allegro 5 is quite adept - I like it personally. The initialization code is simple to use, and easy to stuff away in a class or an Init() function. Really, if you write a wrapper function or class, you won't even have to directly call anything from any of these libraries. Are you interested in game graphics or game development? Because if you want to make games, you should just pick one of these libraries and run with it. I'd only sweat these small steps if you're particularly interested in learning the nitty gritty of graphics development. 

The FBX documentation is painful at times, and this is definitely one of them. There are two ways I've used to access animation data. The first is used in the ImportScene sample that comes with the SDK, and it's the way you seem to be trying to do things. In your sample, now that you have a valid lAnimCurve, you would need to query the number of keyframes that are stored in that curve, and then access them one at a time. It's a fairly complex process, I would refer you to the DisplayAnimation.cxx file in the ImportScene sample to see how they do it. The problem with this is that you then will need to go through and determine any missing information. In the animation I'm working with, I have keyframe data for frame 0, 35, and 70. So now I would need to determine if the interpolation is linear or cubic, which involves more complicated and error-prone programming. I also don't know at this point how fast the animation runs - 30fps, 24fps, 60fps, or some custom number - so even though I know I have 70 frames, I don't actually know how long the animation is supposed to last. Instead, I would suggest digging into the KFbxAnimEvaluator class. This makes life much easier, because it will figure out all sorts of animation stuff for you. Really, that's the SDK page to keep an eye on. So here's some sample code for how I break the times down. Assume lAnimStackCount is the number of animation stacks my file has. 

When you have specific objects pair that you don't want to collide, using layers is definitively the best way to go. 

You can simply create an AnimationCurve, sample it overtime and use the value to modify an object parameter, such as scale: 

Player's code will be more clean and efficient. It's really simple to enable/disable the hit handling, simply removing the delegate associated with the event. If you have more classes interested to listen to the hit event, it's easy to broadcast this event to all listeners (es. audio manager that plays an effect on sword hit) 

EDIT I'm not sure for what concern , but for Unity uses an additional render pass () if you want to render them per pixel. So for example if you try to access _LightMatrix0 or _WorldSpaceLightPos0 in the pass the values aren't always consistent. So I guess that if you want to display point or spot light positions of an important light per pixel you should do that inside a second pass in the shader with the following tag: . (Or alternatively manually settings uniforms values: I'm doing this for rendering both lightmaps and spotlights in a single pass shader). 

it also fails because magnitude is slightly more than zero. So my question is: given 2 Vector3 in Unity, how to compare them? I need the elegant equivalent version of this: 

I'm looking for a graph library to be used inside script. I'm not looking for pathfinding libraries (I know there are good one available). I could consider using a path finding library only if it gives me direct access to underlying graph classes (I need nodes and edges, and classic graph algorithms) The only product I've seen that seems intersting is QuickGraph. I have the following question: