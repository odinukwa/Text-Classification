Now what are the ways to include 'relationship data' along with 'ratings data' to produce better recommendations ? example : user_1 might like the movies which user_2 likes since they are co_workers 

Given some dataset for prediction, for eg say I have different housing price prediction dataset: dataset 1 : 100 training and 100 testing sample, 50 feature dataset 2 : 100 training and 100 testing sample, 120 feature dataset 3 : 1000 training and 1000 testing sample, 50 feature dataset 4 : 1000 training and 1000 testing sample, 5000 feature how should I choose the best methods for estimating the unknown parameters ( predict price) in a linear regression model from the following for each of these dataset? 

and I am using this data to recommend new movie to user (collaborative filtering). I also have another data set which show relationships between user like 

What is the best approcah to create a model for this case ? Shall I create different models for each of this class (sports, politics, cricket, international, positive, negative etc) ? If yes then is it possible to create a pipeline of each class prediction ? Or is it advisable to trian a single model per each output like sports_cricket_positive,s ports_cricket_negative etc 

try cross-validation on training data, cross-validation combines (averages) measures of fit (prediction error) to correct for the optimistic nature of training error and derive a more accurate estimate of model prediction performance (eg : k-fold), check accuracy (Precision/recall/f measure) and then use this classifier on test data. 

There is a free online course on Reinforcement Learning by Udacity. Check : Machine Learning: Reinforcement Learning 

Is it possible to get recommendation on similar product using Mahout ? eg : I have data set of movies with following attributes Movie_name, Actor_1, Actor_2, Actress_1, Actress_2, Director, Theme, Language Now given a Movie_name the system should recommend top 3 similar movies based on the attributes . Can this be done using Mahout. If yes how ? 

You can try Graph databases (Neo4j/Orient DB etc). Store location and connection between the location as nodes and edges. Then do analysis over graph data. Based on your need, you can use additional attributes (like count) and assign weights for edges etc. Neo4j supports collaborative filtering also. 

Have a look on conditional random field. Conditional random fields are a probabilistic framework for labeling and segmenting structured data, such as sequences, trees and lattices. The underlying idea is that of defining a conditional probability distribution over label sequences given a particular observation sequence, rather than a joint distribution over both label and observation sequences. The primary advantage of CRFs is their conditional nature, resulting in the relaxation of the independence assumptions. Additionally, CRFs avoid the label bias problem. It is often used for labeling or parsing of sequential data. CRF++ is a simple, customizable, and open source implementation of Conditional Random Fields (CRFs) 

A standard computer vision and deep learning dataset for this problem was developed by the Canadian Institute for Advanced Research (CIFAR). The CIFAR-10 dataset consists of 60,000 photos divided into 10 classes (hence the name CIFAR-10). Classes include common objects such as airplanes, automobiles, birds, cats and so on. The dataset is split in a standard way, where 50,000 images are used for training a model and the remaining 10,000 for evaluating its performance. Other datasets are MNIST, CIFAR-100 STL-10, SVHN, ILSVRC2012 etc State of the art results are achieved using very large Convolutional Neural networks. You can learn about state of the are results on CIFAR-10 on Rodrigo Benensonâ€™s webpage. Model performance is reported in classification accuracy, with very good performance above 90%. Here is a sample tutorial on convolutional neural network with caffe and python 

Have a look on conditional random field. Conditional random fields are a probabilistic framework for labeling and segmenting structured data, such as sequences, trees and lattices. The underlying idea is that of defining a conditional probability distribution over label sequences given a particular observation sequence, rather than a joint distribution over both label and observation sequences. The primary advantage of CRFs is their conditional nature, resulting in the relaxation of the independence assumptions. Additionally, CRFs avoid the label bias problem. It is often used for labeling or parsing of sequential data. 

But my results are bad and there are lot of overlapping salaries. Is there anything wrong in the code ? How to improve the results ? Or whether clustering is not a right approach here ? Then how can I cluster users only based on salary ? 

I am working on named entity recognition project and find GATE very useful. But I couldn't find a Python connector for GATE yet. Is it possible or is there a way to integrate GATE work flows into Python ? Or how can I write JAPE rules for extracting entities in python ? 

What are the advantages and disadvantages of using Collaborative filtering based recommendation using machine learning approach and graph based approach ? Say I have user purchase data (user_name, user_location, user_company_name, product_name, product_price, product_ingredients) and would like to recommend product for user based on what other user from the same location, company are buying, based on product price, ingredients etc. How to decide on which of them is suitable for a given use case? I would like to evaluate Neo4j (Graph database) and Mahout (Machine learning) for Collaborative filtering. 

Which all are the equivalent or advanced libraries in Python for building recommendation systems like Mahout for Collaborative Filtering and Content Based Filtering ? Also is there a way to integrate Mahout with Python? 

A decision tree is a graph that uses a branching method to illustrate every possible outcome of a decision. Here each internal node represents a "test" on an attribute (e.g. whether a coin flip comes up heads or tails), each branch represents the outcome of the test and each leaf node represents a class label (decision taken after computing all attributes). The paths from root to leaf represents classification rules. This can be represented as graph. Now you can parse through the possible nodes and edges as per the condition rules (if condition1 and condition2 and condition3 then outcome.) Wikipedia link itself has some great examples :$URL$ Do check these links also : $URL$ $URL$ 

I had this basic query on ML and would like to get basic ideas on modelling prediction models using ML and Python. Say I have a training data of 1000 items as Item_name, Attrib_1, Attrib_2, Attrib_3,....Attrib_N, Cost And my aim is to create a model to predict cost for a new item given the attributes. So where should I start and what are the different ways to prediction and solve this problem ? Also how to evaluate different methods ? 

Should I experiment with each of these one by one and compare the results or is there any rule of thump on when to use each of them based on the dataset ? Please help 

I think you can better go through various university thesis reports and data science related journal papers to get more details on "Including hypothesis, testing, reports, conclusions" of the above mentioned Data science related problems. Check these links from Stanford university : $URL$ $URL$ $URL$ 

I need to perform clustering in a given dataset. There are atrributes with numerical as well as categorical values. What is the best way to convert categorical to numeric value ? as an example one field is colour and the values are red, green, blue so can I assign a mapping like: red : 1, green : 2, blue : 3 or red : 11, green : 25, blue : 30 and if I provide a mapping like this will this affect the Euclidian distance for clustering ? or is there any other way ? 

Other-than Ontologies do check word2vec, this tool provides an efficient implementation of the continuous bag-of-words and skip-gram architectures for computing vector representations of words and can be used to find the closest words for a user-specified word 

1 Is there a way to handle data imbalance? ie if data in each class for training is not balanced, say some classes have 50 documents some other have 200 documents. How to handle this? 2 How to handle the classification problem with a large number of classes? I have around 50+ classes (may increase once more data is available) to learn. I am trying out different algorithms and features. Is there any way to handle classification problem with a large number of classes? 

Try Decision Trees,Support Vector Machines or Naive-Bayes method with TF-IDF weighting for creating document vectors and check the Precision/Recall/F Measure scores. But this will not deal with unknown/new organizations, they will get classified to any other one. One way would be remodel/train again when there are so many unknown/new organization and the Precision/Recall/F Measure score too low. 

The root-mean-square error (RMSE) is a frequently used measure of the differences between values (sample and population values) predicted by a model and the values actually observed. The RMSE for your training and your test sets should be very similar if you have built a good model. If the RMSE for the test set is much higher than that of the training set, it is likely that you've badly over fit the data. 

It depends on the usage of various packages (Numpy/Scipy etc), which are written in C and be incredibly fast also Python can be complied using JIT. Here is an excellent comparison between R and Python : $URL$ 

Since CV data is mostly structured you can use Stanford NER for named entity recognition or use CRF to train and model your own named entity recognizer. 

I am building a document classifier using Naive Bayes. There are 10 classes. My question is that : 1 Should each class contain the same number of documents for training? What if the number of training example in each class is different? 2 Does the number of classes and classification algorithm have any relation? say is there any thump rule like if there are 100 classes shall algorithm 'X' has better performance than 'Y' 

Is there a way or tools available to generate or retrieve cypher query from a Neo4j database ? Should we need to store cypher quries along with the graph data for regeneration ? 

Have a look on HMM (Hidden Markov Model) also. A concrete example Of HMM is available in wikipedia. Decision tree is better in generalising and applying learned data in another context and a Markov-model is better to recall the exact learned machine state. 

I am interested in the state of the art approaches for information retrieval (IR) tasks, where you have a single query and a set of documents and the IR model will give you the best matched document. I have worked on vector space models (tfidf-cosine similarirty ) and LSA. I have also tried Wordnet, NER, fuzzy matching etc for improving the accuracy. Now I would like to know how to improve accuracy of IR tasks by applying nueral networks, word embeddingstopic models etc by capturing more context/sematics information 

Refer Aws documentation : $URL$ there are libraries available for most of the programing languages. So you can create a bucket and configure in your code to fetch data from url and write to this bucket in s3 for eg in python : 

Conditional Random Fields (CRFs) can be used for segmenting/labeling sequential problems. Try CRF++: Yet Another CRF toolkit, a simple, customizable, and open source implementation of Conditional Random Fields (CRFs) You can label and create a tagged training corpus and use CRF++ You also need to create a feature template Refer : $URL$ for more details. Check the example from data for CoNLL shared task (PoS tagging). 

check the following links for Spanish sentiment analysis related links : Working Model :$URL$ Data : $URL$ Api : $URL$ 

The ratio between no of samples and features should be more than 10:1 to get sufficiently decent results. But this ratio also varies with applications. If training data is very less : make sure that dataset is balanced, try ensemble methods, careful feature selection, perform cross validation 

I have a set of news paper report. I need to classify them as Sports, Politics related at first. Once the given document is in Sports category, then I need to furthur classify them as Football, Cricket, Tennis etc. For Politics it's International, National, Local etc. Again I would like assign a positive, negative class on this sub category result. So for a given document ouput will be something like : 

SVM is Parametric. Parametric models are something with fixed finite number of parameters independent of dataset size. Anything which is not parametric model is non-parametric model. ANN is non parametric. Also ANN has 'deep architectures" which can represent "intelligent" behaviour/functions etc more efficiently than "shallow architectures" like SVMs. ANN may have any number of outputs, while support vector machines have only one.