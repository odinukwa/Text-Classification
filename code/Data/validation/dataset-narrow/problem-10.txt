We just updated our Jenkins to v2.207.1 and our build estimation times on pipeline jobs have stopped showing up. Rather than a blue bar that when hovered over shows an estimate, we are now seeing the blue-and-white striped barber pole and an estimation of N/A. I believe this behavior might be only on pipeline jobs - we have so few freestyle jobs that I haven't seen them run yet. What logic does Jenkins use to calculate those estimations? How many builds will we need before those times fill back in? 

We're just starting to push for CI-CD and as a baby step we're going to try updating a stack with the latest green develop once every couple of hours. I am fairly new to Git/Bitbucket, and can't figure out how to ensure the checkout that Jenkins makes gets the last commit to have been marked green by Jenkins, rather than just "the last commit" as a blanket statement. We have the Bitbucket Build Status Notifier plugin installed, so Bitbucket does track which commits are green after our unit tests run. Is there a way to leverage this info to make sure the right commit is picked? 

Alice rename DatabaseClient to DatabaseClient_v1 and create a delegate class called DatabaseClient that uses an object DatabaseClient_v1 and implements an interface called DatabaseClientInterface. (If possible, this DatabaseClientInterface should be a code artefact but duck-typed languages not always support this.) Bob reviews changes made by Alice in 1 and is aware that his maintenance job should happen on DatabaseClient_v1. Alice introduces a new configuration flag in the application that governs the behaviour of the DatabaseClient delegate and implements a DatabaseClient_v2 placeholder, a class implementing the DatabaseClientInterface whose methods all throw a “Not implemented” exception. 

After this, Alice and Bob can collaborate without explicit synchronisation, because code written in their respective iterations is subject to the DatabaseClientInterface. This minimises the risk of a conflict resulting from their concurrent work. Iterations from Alice can be very short, like implementing a test, implementing a method, or even partially doing so, because in production, the code is not selected for use and does not need to be fully functional. The automated testsuite should be configured so that the DatabaseClientInterface always uses DatabaseClient_v1 while Alice can easily toggle to DatabaseClient_v2 when running the testsuite locally – or in a custom CI setup. Once everything is ready, a single commit can perform the change, by updating the configuration value governing the DatabaseClient delegate. 

Our front-ends will feed from a series of microservices (I use that term lightly as they are 'Domain' level and rather large), which will be broken into Read Services and Write Services in each domain. Both will be scalable and load balanced through Kubernetes. Each will also have a read-only copy of their database attached to them within their container, with a single master instance of the db available for writes that will push out updates to those read only copies. 

(Sorry for the poor quality images, I'm redoing them from memory since, naturally, there's no actual documentation for this stuff except in the architect's head) Service to Service communication will happen through a message queue that each service will listen to, and process any relevant messages. The primary use of this will be for email generation, as there isn't anything else we've identified that requires service to service communication for information yet. Anything "business logic" related that would require multiple services being involved would likely flow from the front-ends, where the front-ends would call each service individually and deal with atomicity. From my perspective, the thing that rubs me the wrong way is the read-only db instances spinning up inside the docker containers for the services. The service itself and the db would have drastically different demands in terms of load, so it would make a lot more sense if we could load balance them separately. I believe MYSQL has a way of doing that with master/slave configurations, where new slaves can be spun up whenever load gets high. Especially while we have our system in the cloud and are paying for each instance, spinning up a new instance of the whole service when we only need another db instance seems wasteful (as does the opposite, spinning up a new db copy when we really just need a web service instance). However, I don't know the limitations of MS SQL Server for this. My largest concern is around the MS SQL Server implementation. Coupling the read only instances so tightly to the services feels wrong. Is there a better way to do this? NOTE: I asked this over on software engineering and they pointed me here. Sorry if this is not the appropriate SE. Also there's no MS SQL Server tag 

It is a common misbelief that “DevOps” is a role in a company. The word merely denotes an organisational shift in software companies, this shift can be shortly described by the “You build it, you run it!” coined by Werner Vogels: 

From the operational perspective, this breakdown of the parametrisation matches the natural degrees of freedom of the deployment problem – aside from the credentials that could be bundled with the runtime configuration, but it is better to separate them to avoid spreading them carelessly. 

each having three scripts called , and . Now that the organisation of automation items has somehow been clarified, let's turn our attention to configuration. The main conditions and requirements about the configuration organisation are set by the verb when applied on a service-like artefact. The verb should have the following parameters: 

You are running into the limitations of managed services. Many managed services are great to start experimenting with ideas and can show a great value for the casual user. But they trade ease of use for flexibility and functionality, therefore as soon as we start to have serious needs and are ready to spent a significant amount of effort building up on these ideas, the first move we should do is to opt for a flexible a fully functional solution. In my judgement, you are hitting the line where you need to look for something else – and you will notice that you need to learn a different tool, because that's not free software and you cannot run it yourself. Here we are speaking about Circle CI, but the following limitations are common to many managed services: 

Is it possible to launch a jenkins agent from within a script in another job? I'm open to either shell or pipeline options. To be clear I don't mean the agent the job is running on itself. I'm talking about spinning up a completely separate agent. Sometimes our master gets slammed with unit test requests and will make jobs wait in the queue for 15-20+ minutes instead of just spinning up more agents. I want to see if there's a way to spin an agent up intentionally so I can tie it into a job that polls the queue for wait times. 

Run a curl command on the Jenkins REST API to get the info on the last successful build and store it in a temp file Use jq to parse said info and extract the timestamp field, which is in Epoch time. This is stored to a file. The file is archived to the job and then read into a variable. This timestamp is compared to the current time, (gotten at the top of the job), and if the break between them is less than eight hours the job is "failed" out with a success and error message. If the time is over eight hours, the job continues on to build the downstream job. 

whose complexity in comparison to the Ansible variant is probably tolerable: it just uses the plain, regular, boring constructs from the language. Random documentation step 3: Testing strategies Last, we meet what turns out to be the first actually interesting feature of Ansible: “Ansible resources are models of desired-state. As such, it should not be necessary to test that services are started, packages are installed, or other such things. Ansible is the system that will ensure these things are declaratively true. Instead, assert these things in your playbooks.” Now it starts to be a bit interesting, but: 

A file added by some docker instruction and removed by some later instruction is not present in the resulting filesystem but it is mentioned two times in the docker layers constituting the docker image in construction. Once, with name and full content in the layer resulting from the instruction adding it, and once as a deletion notice in the layer resulting from the instruction removing it. For instance, assume we temporarily need a C compiler and some image and consider the 

Due to a combination of business/enterprise requirements and our architect's preferences we have arrived at a particular architecture that seems a bit off to me, but I have very limited architectural knowledge and even less cloud knowledge, so I would love a sanity check to see if there's improvement that can be made here: Background: We are developing a replacement for an existing system that is a complete rewrite from the ground up. This requires us to source data from an SAP instance through BAPI/SOAP Web Services, as well as use some databases of our own for data not in SAP. Currently all of the data that we will be managing exists in local DBs on a distributed application, or in a MySQL database that will need to be migrated off of. We will need to create a handful of web applications that replicate the functionality of the existing distributed app, as well as providing admin related functionality over the data we control. Business/Enterprise Requirements: 

The reason for this, is that all the previous steps will be cached and the corresponding layers will not need to be downloaded over and over again. This means faster builds and faster releases, which is probably what you want. Interestingly enough, it is surprisingly hard to make optimal use of the docker cache. My second advice is less important but I find it very useful from a maintenance view point: 

First, while Docker is sometimes seen and used as a ad hoc packaging system, it actually solves a totally different problem: Docker is about running programs. The Docker system allows to describe services, that can be scaled at will and to control swarms of containers. Debian packages are for installing programs and they are able to handle dependencies between software versions. Docker certainly don't qualify as a descent packaging system: each “package” can only have one dependency, the system has no “recursive build” option and does not support complex version constraints! A possible answer would be that, if you are willing to write a Debian package for your application, you can also use Docker to deploy your application. This can be achieved with a configuration script which would look like