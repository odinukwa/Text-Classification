The performances of an algorithm are always analyzed in the context of a well defined computational model. Traditional sequential models, e.g. the RAM model, assume that: 

For the equations you are reporting, $J(S_m) = I(S_{m-1}; x_m)$, i.e. $J(S_m)$ is the mutual information between $S_{m-1}$ and $x_m$. Since for any two random variables $X$ and $Y$ $I(X; Y) = H(X) + H(Y) - H(X; Y)$ the equation follows directly from the definition of mutual information (the chain rule does not apply here, since there is no need to deal with conditional entropy): $\begin{gathered} H({S_{m - 1}},{x_m}) = H({S_m}) \\ = \sum\limits_{i = 1}^m {H({x_i})} - J({S_m}) \\ = \sum\limits_{i = 1}^m {H({x_i})} - I({S_{m - 1}};{x_m}) \\ = \sum\limits_{i = 1}^m {H({x_i})} - (H({S_{m - 1}}) + H({x_i}) - H({S_{m - 1}},{x_m})) \\ = \sum\limits_{i = 1}^m {H({x_i})} - (\sum\limits_{i = 1}^m {H({x_i})} - H({S_{m - 1}},{x_m})) \\ = H({S_{m - 1}},{x_m}) \\ \end{gathered}$ 

Sorry, no pseudocode, but a recent paper with a fast parallel algorithm, to appear in ACM Transactions on Mathematical Software: @Article{Gustavson:2011:PCE, author = "Fred Gustavson and Lars Karlsson and Bo K{\aa}gstr{\"o}m", title = "Parallel and Cache-Efficient In-Place Matrix Storage Format Conversion", journal = "{ACM} Transactions on Mathematical Software", accepted = "8 July 2010", upcoming = "true", abstract = " Techniques and algorithms for efficient in-place conversion to and from standard and blocked matrix storage formats are described. Such functionality is required by numerical libraries that use different data layouts internally. Parallel algorithms and a software package for in-place matrix storage format conversion based on in-place matrix transposition are presented and evaluated. A new algorithm for in-place transposition which efficiently determines the structure of the transposition permutation a priori is one of the key ingredients. It enables effective load balancing in a parallel environment.", } Here is the journal page, and here is a pdf version of the paper. I hope this can be useful. 

Ok, I have found how to do this by reading Introduction to Mathematical Optimization: From Linear Programming to Metaheuristics. Chapters 7 and 10 clearly explain the possibilities, essentially the penalty method and the use of Lagrange multipliers coupled with Kuhn-Tucker conditions if necessary. 

Assuming you have already selected the Ph.D. program you are interested to, my advice is to talk directly with the person that could be your future advisor. He/she knows exactly the theoretical background and practical stuff required for each of the research topics available, so that it will be easier to decide what to do next. Basically, knowledge of the research topics and their prerequisite skills will allow you to prioritize the material you need to cover. I would cover first algorithms, and then databases, operating systems and programming in one or more languages. This is because you are likely to need algorithms in every research topic, but not necessarily OS, DB and programming unless the topics are strictly focused on these or require an implementation. Again, talking with your future advisor is the key to understand this. As stated by @Dave Clarke, if you prefer to delay enrolling in the Ph.D. program, this will not hurt: additional experience is never a disadvantage. If you, instead, prefer to enroll immediately, just be prepared for a different kind of full-time studies, taking into account that you do not know something that may be required but you do know better mathematics. 

Since in MapReduce there is a simple and structured graph underlying the computation, this can IMHO classified as a data-flow model. 

To the best of my knowledge, parallelization of this problem is not done as you suggested, i.e., searching in parallel the whole space to determine extremal points. A much faster parallelization method for multi-variate polynomials (when appropriate) may be based on the use of parallel automatic differentiation, the (parallel) solution of the resulting system of equations and finally the study of the related Hessian. However, you may consider non trivial the paper "Polynomial Optimization via Recursions and Parallel Computing", which deals with the problem of finding the global minimum for a special class of dominated real valued polynomials. "A Parallel-Computing Solution for Optimization of Polynomials" provides a parallel algorithm for determining polynomial positivity. 

Since you know that you must process each element and tasks are independent, i.e., the algorithm is embarrassingly parallel, in the Dynamic Multithreading model you should simply use a parallel for instead of spawning threads as you do, and let the compiler implement this optimally. The work (sequential time) is $T_1 = O(nk)$ because you need to process each element and each element requires O(k) time. The span is $T_\infty = O(log n + k)$, and the parallelism is $T_1 / T_\infty = nk / (log n + k)$. In general, you know that, regarding $T_p$, we can only say that: $T_1/p \leq T_p \leq T_1/p + T_\infty$ $T_\infty \leq T_p \leq T_1/p + T_\infty$ This is because $T_p$ is a function of the work, span, size $n$ and scheduler behavior. And $T_\infty$ actually means for your algorithm $p = n$. Now, let's analyze what happens to the speedup by using the Generalized Amdahl's law (derived by Lee). This law describes the speedup of parallel applications in which the computation is made of phases and in each phase a different number of processors is actually used. Think about each level of the directed acyclic graph corresponding to the computation as a phase. Let $q_i$ be the percentage of a parallel application executed simultaneously by $i$ processors; then, we have: ${T_1} = \sum\limits_{i = 1}^p {{q_i}} {T_1}$ ${T_p} = \sum\limits_{i = 1}^p {\frac{{{q_i}}}{i}} {T_1}$ Therefore, the speedup is $\frac{{{T_1}}}{{{T_p}}} = \frac{{\sum\limits_{i = 1}^p {{q_i}} {T_1}}}{{\sum\limits_{i = 1}^p {\frac{{{q_i}}}{i}} {T_1}}} = \frac{{\sum\limits_{i = 1}^p {{q_i}} }}{{\sum\limits_{i = 1}^p {\frac{{{q_i}}}{i}} }} = \frac{1}{{\sum\limits_{i = 1}^p {\frac{{{q_i}}}{i}} }}$ Why is this useful? Because in general it allows you to derive useful bounds. For instance, assuming $q_i = 1/p$ then we can derive an upper bound for the speedup as follows: $\frac{{{T_1}}}{{{T_p}}} = \frac{1}{{\sum\limits_{i = 1}^p {\frac{{{q_i}}}{i}} }} = \frac{1}{{\frac{1}{p}\sum\limits_{i = 1}^p {\frac{1}{i}} }} = \frac{p}{{\sum\limits_{i = 1}^p {\frac{1}{i}} }} \leqslant \frac{p}{{\log p}}$ In your algorithm, $q_i$ is different from zero only for values of $p$ which are powers of 2; i.e., $q_i = 2^i, i = 0,...,\log p$. In the corresponding summation $q_i$ will be divided by $2^i$, giving rise to a summation in which each term is 1. As an example, for $p=16$ the nonzero $q_i$ values are 1,2,4,8,16 and these values must be divided respectively by 1,2,4,8,16 so the sum is 5 and the speedup is 1/5. In general, the speedup of your algorithm is: $\frac{{{T_1}}}{{{T_p}}} = \frac{1}{{\sum\limits_{i = 1}^p {\frac{{{q_i}}}{i}} }} = \frac{1}{{\sum\limits_{j = 0}^{\log p} {\frac{{{2^i}}}{{{2^i}}}} }} = \frac{1}{{(\log p) + 1}}$ Now compare this with the performances you can obtain in the message passing model, in which your algorithm is executed on $p$ processors. Since the algorithm is embarrassingly parallel, $T_p = O(\frac{n}{p} k)$ (domain decomposition is trivial and requires O(1) time) and the speedup is (as expected for an embarrassingly parallel algorithm) $T_1/T_p = p.$ 

Call your sets A and B respectively, and assume wlog that Card(A) = n < Card(B) = m. If you are allowed to choose a data structure, you may want to use a hash table H as follows. Store all of the elements of set B in H. Then iterate through the elements in A. For each element e, check if e is in H. If yes, output e in the intersection set, otherwise in the difference set. Since looking up an element in the hash table is O(1) (on average), the algorithm requires O(n) and is linear in the cardinality of A. If Card(A) = Card(B), a fast intersection algorithm is to use a Bloom filter to store the sets, and then intersection is implemented with bitwise AND operations. 

Do not use a single stream of random numbers generated in one thread (or process) and consumed in other threads (or processes). In general, you must instead use several random number streams for your calculations, one for each thread/process. It is extremely important for these streams to be uncorrelated, in order for the pseudorandom numbers to be effective at the desired variance reduction, which is, of course, the point of large Monte Carlo calculations. You may want to take advantage of the Scalable Parallel Random Number Generators Library (SPRNG), which is a parallel library (but you may also use it sequentially as well) providing these and other additional properties; it is particularly well suited for parallel Monte Carlo simulations. 

Your problem is similar to the data mining problem of finding frequent itemsets, also known as Association rule learning. If I understood correctly, your problem may be reduced to finding frequent itemsets of cardinality 1 (i.e., singletons) with support >= k. Of course, the available algorithms (such as Apriori, Eclat, D-CLUB etc) for the problem also allow determining frequent itemsets of cardinality > 1. 

Finally, you can use an advanced technique known as Adaptive Mesh refinement (AMR): grids are broken into blocks of ﬁxed extents, when needed blocks are reﬁned into children with same extents. Block size are chosen to optimize balance of cache efﬁciency (optimization possible since loops operate on ﬁxed size array), communication, and load balance. Often balancing blocks per processor sufﬁces; if communication is excessive then use a space-ﬁlling curve. Rebalancing requires only simple collective communication operations to decide where blocks go. The technique can be used for arbitrary dimensions. If block size is 1, adaptive blocks are quad or oct-trees. See the next picture for an example. 

One thing that proves to be counterintuitive for CS undergraduates, is the fact that one can select the $i$-th order statistics from an unsorted array of $n$ elements in $O(n)$ time. All of the students think they must first necessarily sort the array (in $O(n~lg ~n)$ time). 

It depends. If you write a paper and exhaustively include all of the relevant materials, then you should submit the paper to a journal (if you think the journal you choose is more valuable then a conference on the basis of impact factor, reputation and other metrics). If you only describe part of the work, then it may be better to submit to a conference and later, when you have new/updated results, then you may consider submitting again to a journal. What constitutes a significant difference is of course highly variable and depends on your particular work. A very rough rule of thumb is that journal versions differ for at least 30% from corresponding conference versions. However, there are exceptions. And, finally, you should carefully decide the venue. Especially in TCS, a conference may be extremely valuable, in some cases even more than a journal. I do not think that the review process applied in some highly valued conference is worse than the average review process applied in journals. This is strictly dependent on the particular conference or journal, and on the reviewers an editor finds available. 

Distributed supercomputing applications require multiple supercomputers to solve problems otherwise too large or whose execution is divided on different components that can benefit from execution on different architectures. This class of applications present a number of challenges to be faced, like resource discovery and scheduling, coordinated startup, configuration at multiple sites, wide area message passing and fault tolerance. An example of such application is SF-Express, a distributed interactive simulation of a military battle. On demand computing refers to the possibility of dynamically acquiring online instruments (e.g. microscopes, satellite sensors and telescopes) connected by high-speed networks to gather and process the data generated. An experiment of microtomography at photon sources in 1999 demonstrated the feasibility of on demand computing. The aim of high throughput computing (HTC) is to schedule many independent jobs for parametric studies or data analysis; in this case a measure of efficiency is the number of jobs processed per unit of time. The two most important tools for HTC, namely Condor and Nimrod are now grid-aware. Data-intensive applications extract new knowledge from geographically distributed data archives or digital libraries; issues related to this class of applications include scheduling and configuration of multiple data flows through several hierarchy levels. Finally cluster computing refers to the use of a cluster, which is a parallel computer, for running parallel scientific simulations. Therefore, when using just a single cluster you are doing parallel computing; when using more than one simultaneously but for the same application, may be with different architecture, you are doing distributed supercomputing, so this is grid computing. Platform computing, the company producing the LSF scheduler, promotes its LSF multi-cluster capabilities as grid computing. 

The grid computing paradigm emerged as a new field distinguished from traditional distributed computing because of its focus on large-scale resource sharing and innovative high-performance applications such as: 

Unfortunately, there are many possible levels of description for a problem; choosing one among them is strictly dependent on your aims and on the tools available to you. In general, there are three possible approaches: analytical, numerical and observational. Each one is characterized by pro and cons. When you need to build a model, there are many possible decisions that must be made. For instance, you may decide among: 

You may find of interest the following recent handbooks. The range of topics covered goes well beyond CLRS, and the material is well suited for graduate and Ph.D. students, even though you may choose a few selected topics for advanced undergraduate students. Algorithms and Theory of Computation Handbook Second Edition (Special Topics and Techniques) Handbook of Applied Algorithms Solving Scientific, Engineering and Practical problems Handbook of Approximation Algorithms and Metaheuristics ￼￼￼￼￼ 

An application of fractals in document analysis has been proposed in Tang, Y.Y.; Hong Ma; Xiaogang Mao; Dan Liu; Suen, C.Y., "A new approach to document analysis based on modified fractal signature," Document Analysis and Recognition, 1995., Proceedings of the Third International Conference on , vol.2, no., pp.567,570 vol.2, 14-16 Aug 1995 doi: 10.1109/ICDAR.1995.601960 Here is the abstract: The proposed approach is based on modified fractal signature. Instead of the time-consuming traditional approaches (top-down and bottom-up approaches) where iterative operations are necessary to break a document into blocks to extract its geometric (layout) structure, this new approach can divide a document into blocks in only one step. This approach can be used to process documents with high geometrical complexity. Experiments have been conducted to prove the proposed new approach for document processing Two years later, they published an extended journal version: Yuan Y. Tang, Hong Ma, Dihua Xi, Xiaogang Mao, Ching Y. Suen, "Modified Fractal Signature (MFS): A New Approach to Document Analysis for Automatic Knowledge Acquisition," IEEE Transactions on Knowledge and Data Engineering, vol. 9, no. 5, pp. 747-762, September-October, 1997 Here is the latter paper. 

here are my answers to your questions. 1) In general, you can not come up with efficient schemes for scaling down the number of processor from $n$ to $p < n$. Why ? Because, even without taking into account additional issues such as caches etc, the problem is strictly related to how you map tasks to processors, which is known to be an NP-complete problem. In particular, this is almost always application dependent, so you have to carefully evaluate this each time you want to scale down your parallel algorithm. The good news here is that, starting from a work-efficient algorithm you can obtain a scaled-down work-efficient algorithm, thus preserving its efficiency. 2) To the best of my knowledge there are no strong connections with parameterized complexity, at least I am not aware of any significant published paper discussing this relation. I may be wrong of course. 

Finding a good tradeoff may require experimenting running the application and measuring how parallel time is affected by different scattering strategies. Here is an example. The first picture show an example image that must be processed. Say it contains sea water and an island, and you may want to process the vegetation on the island. The image is partitioned using a very simple domain decomposition technique. The second picture shows an example of scattered decomposition. 

The Handbook of Data Structures and Applications (Chapman & Hall/CRC Computer & Information Science Series) is mostly devoted to elementary data structures, but it also contains a few advanced materials that you may find useful for teaching a graduate level course. Given the huge size (1392 pages), this book may be classified as an encyclopedic handbook of data structures, even though certainly it is not an handbook focusing primarily on advanced data structures. 

You problem may also be amenable to a solution using what is known as a scattered decomposition. Used when there is a structured domain space (e.g., an image) and the processing requirements are clustered, such as modeling a crash or processing an image with only a few items of interest. Suppose there are P processors. Cover the problem domain with non-overlapping copies of a grid of size P and assign each processor a cell in each of the grids. If you use more pieces: 

You may be interested to this survey done by a Ph.D. student, which is updated to 2009 and presents classical work on parallel clustering. The survey is full of references you may then read to delve into the gory details. 

The idea behind quad-trees is the following one. Partition all of space into quadrants (octants in 3-space). If any piece contains too much work, partition it into quadrants, and so on. See the next picture for an example. 

You can construct selection networks of depth O(log n) using graph expanders or using variants of the AKS sorting network. S. Jimbo and A. Maruoka, A Method of Constructing Selection Networks with $O(\log n)$ Depth,SIAM J. Comput. 25, pp. 709-739, $URL$