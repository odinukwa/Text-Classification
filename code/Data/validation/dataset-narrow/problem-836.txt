The orange led doesn't mean that the drive is broken: it means that it's marked as failed. Recently I had the same issue on a ServeRaid 6i: two drives disappeared. The raid was level 5. I put one of the two online and I rebuilt the second. At the end of the process I got my array rebuild. Of course it was not a broken disk but a weird bug into the controller or into the disks. Some disks seem to have broken firmware that cause the disk to deattach from the array randomly. 

I just purchased a license of safelyremove. It has command line support. It's very nice. There is a full trial on the website $URL$ 

Is there anyone aware of someone seriously working on thunderdbird to get write support on ldap shared addressbook? 

I have a X3400 with 8 x (43w7598 250GB SATA 3GB/S HDD) set in the frontal bay and connected to ServeRaid 8K controller. The raid array works well but, even if I set the controller PHY to 3.0 (rather then Auto or 1.5), all the disks negotiate 1.5Gb/s with the controller. This sounds really weird to me because the disk are WD Caviar Black Sata2 disks (bought from IBM). The firmware is the latest available from IBM (accordingly to the IBM ServeRaid matrix). Any tip? 

What CRC does exactly? Accordingly to wikipedia it should be an integrity check but how does it work? I discovered that setting this parameter to false my disks are finally recognized as sata2 rather than sata1 and speed are really increased. Why? I found this IBM paper in which they say: 'CRC Checkingâ€”(Default: No) Determines whether the controller verifies the accuracy of data transfer on the Serial bus. CRC Checking should be disabled on the controller and all devices if any device supported by the controller does not support CRC Checking." How do I discover if a hdd supports CRC? If CRC is disabled and a breaking event occurs, is there a risk? 

We are working on a project which involves different hardware all hosted in a single rack. The machines are mainly IBM servers: 2 x206 (scsi), 1 x226(scsi), 2 x3400(sata) and another assembled machine with sata controllers. We are using several raid controller. Some machines have only one Serveraid controller, others have one or more controllers not always Adaptec ones. All the firmwares and bios are updated. All the servers and connected devices are under ups. Over the last 4 months we experienced several strange behaviours in our hardware. Suddenly and randomly we loose 2 or 3 drives and the raid volumes stop to work. It can happen once a week but never at the same time of the day or week. Most of the times a rebuild process fixes the problem, sometimes we loose the data. Very often we just need to unplug the raid controllers, restart the server and the problem is fixed. At the beginning we thought it was due to firmware bugs but we performed an accurate update for every machine and raid controller and there is nothing else we can do on the hardware. We have really no hint on what's causing all these troubles. We are starting to think that it's an environmental problem but we don't know if there could be something interfering with our hardware. Have you ever heard of something like that? Do you have any idea on how to investigate the problem? 

the process gets stuck on the installation of libssl because it requires the user input to restart some services (ssh ntp exim4) 

The title should be self explanatory but more in detail I'm looking for a way to protect the ldap from LAN brute force attacks. It would be fine to prevent password guessing by locking a password for a specified period of time after repeated authentication failures. It doesn't matter if this can be turned into a DOS. Unfortunately I can't find a way to do this and the documents I've found are really confused. 

I never had experience on that precise model but generally the answer to all your questions is YES for every nas like that. Even the last question should be yes because the nas should be capable to rebuilt the array from the meta information stored in the disk and also because a mirror raid is not that hard to be rebuild. If you extract a running disk you can mount it on another host (the filesystem used by Iomega should be XFS, so every linux can mount it). Bonus question: If I understand well what you mean, the action is SWAP. Hot swap is when you can exctract a disk without turning off the device. This Iomega is not hot swap. Apart this in the recent past (last 3 years) we sold 12 Iomega Storcentre nas. We had 100% warranty emergencies on them: broken disks, bugs in the GUI, broken power units. We stopped to sell Iomega for this reason. I don't mean that this model is affected too but I would suggest you Buffalo $URL$ 

Unfortunately this is shown inside the terminal and I couldn't find any way to hit "ok" and then, of course, I get this message. 

I'm looking for a 4 ports pci or pci-express (not pci-x) sata controller with these features: - real hot swap support - no RAID support, or with RAID support that can be disabled through the controller bios interface or reflashing the bios My need is to hot plug/unplug one or more different sata1 and/or sata2 disks from different brands size and speed (even at the same time), coming from my customers computers to perform lab activities on them, like backup. I tried several Silicon Image controllers (3112,3114,3124) and Promise TX4. Everyone has issues: some disks are not seen or are dropped during the backup process while the Promise even hangs the host pc with some hard-drives. Adaptec raid (Serveraid 8s) controllers aren't transparent to the operating system and it seems there is noway to disable the raid. What would you suggest? Thank you! 

This is because you have your identity (public key) added as authorized key on serv2 in . See section for more information on this file. Your default identity are stored in files and similar files ending with .pub storing respective public keys. When you try to log to any remote ssh without specifying '-i' option, these are offered to remote server. If the remote server has any of these stored under their then you can log in with that key (i.e. without password). Because password based and public-key based are two different methods of authorizing remote user. When you are specifying '-i' option, you are just presenting a different public key, stored in that file. Also you can always run ssh with '-v' option to get more details about which key method/public key was accepted by the remote. Like in this example: 

As I can see now, from your detailed update, most likely it seems that you don't have those IP's assigned on your local network interface. You've only added 111.222.62.130 on your server. In order for linux S(ource)NAT to work, you must have every IP address you're NAT'ing to (--to-source) to be actually added on your network interface. Check on how to assign additional IPs to an interface. You must issue for every IP you want to have SNAT working for. It will be convenient to write a script to fill those values. Be aware, that once you'll add those IPs on your local interface, you will "expose" your server via them to the Internet. You might use iptables INPUT table to block unwanted local connections to those IPs. By local I mean only to your server. Those coming through NAT are subject to FORWARD table only. PS: If that it's not the case, also please check and post your . 

Your www-data user most likely doesn't has any password set. This disallows logging in with and that user. Your www-data user shell are most likely set to something like which makes sense as this user aren't supposed to log into the system as real user. 

If you are running a 3 or more node cluster with cman/corosync and you have some sort of shared block storage (SAN) connected only to some nodes in the cluster how can you enable CLVM on that storage device? In my example I'm running a 3 node cluster, there 2 nodes are "main workload" and 1 node are used for backups & archives. Main nodes are connected with FC HBA with multipath to a SAN. Everything works fine, I was able to initialize PV on that device and can see it on both nodes: 

I don't really understand if you need to connect to 1.2.3.4 via VPN or directly. If you need to connect via VPN when I'm afraid it might be not possible if you don't have access to configure the VPN server and/or 1.2.3.4 server. Because you need to set up the reverse routing for this to work. Both VPN and 1.2.3.4 server need to know how to reach you back through VPN link. If your routes aren't symmetric most likely the packets are lost, because 1.2.3.4 doesn't knows how to route to 10.10.10.0/24 network. But if you are fine with direct connection (without VPN and encryption) to 1.2.3.4 you can add a route to connect directly via your Internet connection without VPN. Make sure to record your default gateway before connecting to VPN with command. Like this for example: 

Now, to create a clustered VG, I've enabled fencing on all 3 nodes in my cluster (with IPMI if that matters). 

Probably the best practice (if possible) would be to move your service from "Alpha" to another, separate network. And put 2 firewalls between this and Alpha & Gamma. This is a concept similar to DMZ. Consider a worst case scenario - what if your service gets compromised? That way a malicious user from Gamma network gained access to your whole Alpha network if you have service there. Even if you proxy it via HAProxy, he can still execute arbitrary commands on your server on Alpha network via HTTP(S) doorway. If you put the service in separate network even if it's compromised he still needs to penetrate your firewalls to reach other services. 

By considering random VPS vs dedicated server with NVMe storage, you are comparing a ride on a bus with buying a sportcar. And no, Virtuozzo, implementing container virtualization here aren't the bottleneck. Instead containers usually introduce less overhead over full virtualization technologies. But as you are renting the VPS your don't really have control over the physical node. And can't even know how much it's subsystems like I/O and CPU are loaded. Overselling VPS are pretty common practice, especially with cheap VPS. I mean the things inside your VPS/container might be optimized but you are getting the latency because other customers on the same physical node are overloading it's I/O or CPU. But it has nothing to do with Virtuaozzo itself. Contrary to that a dedicated server will give you full control over the resources there. So you will exactly know your resource usage. But something like wordpress blog with 500 visitors/day shouldn't require a dedicated server to run smoothly. You can check & test other VPS providers. Just make to sure to rent from a reliable provider who won't oversell their nodes.