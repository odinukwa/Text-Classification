You can do reverse lookups on each IP in the range and search through the results for the domain you're looking for. However, this will only work if the person/s responsible for that domain created a PTR record for the IP. This is a quick and dirty working example: 

Have you presented this to the dev mailing list? Very interesting for sure. Have you tried enabling full debugging; you might have had to enable it during compile. Once enabled do one request to the canonical and one to another. Perhaps the debug logs will shed some light. Have you tried removing the second server_name domain(somesite-live..)? Good luck, let us know if you figure it out! 

As far as I know that will just allow snmpget requests for this data. How do I enable traps based off this info? Thanks! 

Khaled's answer is probably your best option. You can also use expect for this, good for instances where you are not allowed to install keys for one reason or another. 

Not always the best solution but remote desktop allows you to use your local cd/dvd drives (and virtual cd drives) as if they were local to the remote server. Check your remote desktop preferences to enable this. 

At this point restart mysql and let it run for a few hours; so as to have representative performance values available. Then, as was recommended previously, download and run mysqltuner. Beyond that you may want to configure graphing of mysql performance in Munin or another graphing solution. Hard to know if any anomalies or long term trends in mysql performance exist without something like this. 

I second mysqltuner! You might want to start by using one of the example config files in /usr/share/mysql. There are several to chose from depending on the system resources available. Integrate any already customized options from your existing my.cnf into the example config. Turn off storage engines that you aren't using as well: 

If you truly want to learn about web administration you will stay away from CPanel and other web based administration tools. However, if you aren't familiar with web terminology or the core fundamentals then temporarily using CPanel or Plesk or the like may help you become comfortable with DNS, SSL, Virtual Hosts, etc. You should look at installing linux in vmware server or parallels on your local workstation. Learn how to install and configure an http server, etc. Since it's only for the sake of learning there's really no need to pay just yet. If you feel that you must have a server out on the tubes then check out VPS hosting from companies like Linode and Slicehost. They will allow you to reinstall or restore the OS from backups if you screw anything up (I use Linode & they're awesome). Try to become familiar with the core of the OS before you get too deep into "web administration". Knowing the fundamental tools (cat,sed,awk,cut,xargs,find,vi,locate,which,man,STDIN,STDOUT,etc) will likely make any troubleshooting much easier. 

So everyone who uses CentOS knows the default repositories have their limits. I've been using rpmforge for years as a secondary repo. I usually use yum priorities to make sure rpmforge never installs something the base repo can take care of. Recently I came across a need to install EPEL as a tertiary repo. Is EPEL kept more up to date than rpmforge or in any way better? Should I be using that instead of rpmforge? 

Warner's links are exactly what you need. Though they should be your comprehensive reference they might be a little dry for an introduction to the concept of mod_auth_*. Try googling for "apache htpasswd" or something similar. Here is a decent tutorial to get you started: $URL$ Note: Works the same for subdomains as it does for subdirectories. 

Well, obviously symlinks aren't going to work and to the best of my knowledge you can't have two completely seperate chroot environments available at the same time. So... if you want to chroot them to their home folders plus the foos group folder then just have all users dropped (chrooted) into /home. Then mount /srv/foos to /home/foos instead. Make sure each users own homedir has proper permissions so that only that user has access to it. In this setup each user will be able to access the foos group folder and their own home folder. They could see that other users home dirs exist, but will be unable to access them. Perhaps someone else has a better idea? 

Where is this LAN that the web server resides on? If it is at your home then you can configure your personal soho router to forward all requests for incoming http(80) to a specific IP address on the local LAN. This is achieved via the administrative web interface of the router in most instances accessible via the gateway IP of the device. You will want to make sure apache is configured to listen on the computers LAN IP and not just the loopback interface: 

Test to make sure the locations block is being called by placing an additional directive in there..something like: 

What browser? Are you attempting to use a proxy? A little more detail would be helpful. Also, unless you are configuring a proxy this post shouldn't be on serverfault.com. 

If the backspace was working okay initially and no longer is then try using the "reset" command. Look around putty's options menu for a "send hard reset" option as well. If it was never working then try what tylerl suggested. 

I'd like to centrally remove all non-active snapshots on a vMA host before performing central backups of VMs. I am hoping such a script exists already??? Anyone know of one? Edit: Here is a script using basic vMA tools to do the job: 

This means that you CAN just change the progname and it will look for a pam file of the new name. Not a good security practice and I am kinda surprised by this. Maybe someone knows something I don't..since the OpenBSD guys are a much smarter bunch than myself. :p UPDATE 2: Verified that PAM servicename is set to the basename by doing the following from the console: cp sshd to sshd2: 

I've tried Zabbix but haven't tried BixData. I would recommend trying Zenoss as well, better than zabbix if you ask me. 

It sounds like you don't so much as want a full list each time the script is run but rather a list of any new/uknown files that are set suid/guid. If that's so then: Get a list of current results: 

How can I monitor the write speed to a tmpfs partition. It does not appear in the output of vmstat or iostat. I am running several python processes that are writing heavily to tmpfs and contributing to load. The load is high but the CPU, memory, disk IO, etc is all nominal. The load seems to be taking this heavy tmpfs IO into account indirectly somehow. I'd love to know the write speed to have an idea of upper limits per host. I'm running blind any help would be appreciated. 

The best method for reducing apache's memory usage is to move away from mod_php and to something like fastcgi. Each of your apache processes is 15mb or more because of mod_php overhead (most likely). Having php requests handed off to fastcgi will reduce the average apache process size to approx 1mb or so depending on the apache configuration. Since php is now centralized using fastcgi it's memory usage is more efficient and the total amount of memory used by the system should decrease slightly. Another approach would be to place an http server that's more memory efficient in front of apache and have it server static content directly and proxy non-static requests to apache. Nginx would be great for this. As a temporary fix you can also look into decreasing the MaxRequestsPerChild to 1000 or something more aggressive. Since apache processes tend to grow in size as they serve requests this will limit their size by killing them off and spawning new ones. 

If this rails info is in a mysql dbase you can configure pam_mysql. There are pam modules for just about everything. Here's one for ftp that I have with mysql: 

Nginx or lighttpd are sure bets. You can configure Apache to be more competitive by using the mpm-worker module in place of prefork. However, this requires configuring php to use fastcgi or the like; that's probably a smart idea in and of itself. I personally run wordpress using nginx with php-fpm and it's fast as hell. There are some optimizations you can make regardless of which web server you choose. 

Sorry it's kind of obvious, it's late give me a break. dig XX.XX.XX.in-addr.arpa. will reply with the SOA if one has been assigned. 

Does anyone know how to setup the CentOS repositories to work under RHEL 5.6? Please be specific if possible. 

Just as the title reads: How do I discover what nameserver is the authoritative nameserver for PTR records for a specific IP? 

You will want to parse the web server log files. The log format will depend on the web server and how logging is configured. There are modules for most languages that make parsing the logs relatively trivial. No reason to resort to writing something from scratch in bash or awk. If you dont' want to write any code then I suggest checking out splunk. There is a free version that lets you parse up to 500MB of log data daily, that's a lot of log data so you should be fine with the free/community version. Splunk can also export results for you. 

Every publicly accessible web server receives requests like this all day long. They're just blindly attempting known exploits against your server. What I often do is configure the web server to display a blank page when it receives requests to it's IP (i.e. $URL$ I only allow the sites to appear when the correct virtual host domain is requested. See what site appears when you access the web server by IP instead of by domain name. Most of the exploit scripts crawling the netter-tubes aren't performing valid virtual host requests(proper virtual host headers). You can also look into the various utilities that will automatically block IP addresses that attempt nefarious requests. 

There are performance monitoring tools like munin, cacti, zenoss (prf mon is just 1 feature) that will do what your looking for and much more. Although from your wording I am unclear whether you are looking for something like that or something more "realtime" with no historical information per se. In which case I got nothin for ya, sorry! 

fsck should still work as the cable medium is inconsequential in this case. I'd be skeptical of any other tool that repairs ext3. It might just be a wrapper for fsck anyway. 

Make sure the mail server is listening on more than just the loopback interface. It most likely isn't by default, for security reasons. 

Please, I am not looking for a rehash of what's stated in RedHat's documentation regarding emergency mode. I would like to know what steps are involved from the time grub hands off to the kernel to the time you get a emergency mode login prompt. I imagine /sbin/init is completely bypassed and therefore rc.sysinit bypassed as well. I don't however know what isn't bypassed or how emergency mode differs (intimately) with init=/bin/sh. What sort of steps does the kernel take when given the emergency argument at boot time? Thanks! 

clear your cache in both and make sure it's still working in one and not the other. Attempt to load the css url in the browser directly i.e. $URL$ View the source and verify the src addresses are correct. Check the sites error logs for any access denied or "not found" lines. 

This will allow each client page request to use one apache process to handle all it's requests where it would otherwise use multiple apache processes. This will cut down on the amount of apache processes running at any given time. For optimal memory and requests per sec: Move away from mod_php and use fastcgi, or another app server, instead. Apache processes consume a negligible amount of memory when php pages are served by fastcgi. Not to mention fastcgi can keep persisten connections to your dbase server amongst other things. 

There may be functionality differences between the libraries so I would make every effort to install the additional older libraries first(apt-file search libboost1.35). If that fails you can attempt to symlink the existing libboost libs to the older names and rebuild your ldcache and re-run ./configure. 

I never really pay block size much attention but obviously there can be benefits to choosing something other than the default. I am looking for a good "best practices" paper on choosing block size. Also, when used on top of LVM is it's performance payoffs or importance negated in any way? TIA 

The nginx plugin may rely on Nginx being compiled with certain modules or log output in a certain format. Is there any documentation page for the plugins? 

So all the tutorials and documentation for the Linux quota system has left me confused. For each filesystem with quotas enabled/on where is the actual quota information stored? Is it filesystem metadata or is it in a file? Say user foo creates a new file on /home. How does the kernel determine whether user foo is below their hard limit? Does the kernel have to tally up quota information on that filesystem each time or is it in the superblock or somewhere else? As far as I understand, the kernel consults the aquota.user file for the actual rules, but where is the current quota usage data stored? Can this be viewed with any tools outside repquota and the like? TIA!! Update: Thanks for the help. I had already read that mini-HOWTO. I am pretty clear on the usage of the user space tools. What I was unclear on is whether the usage data was ALSO in the file that stored per-user limits and you answered this with a yes. From what I can tell, rc.sysinit runs quotacheck and quotaon on startup. The quotacheck program analyzes the filesystem, updates the aquota.* files. It then makes use of quota.h and the quotactl() syscall to inform the kernel of quota info. From this point forward the kernel hashes that information and increments/decrements quota stats as changes occur. Upon shutdown, the init.d/halt script runs the quotaoff command RIGHT before the filesystems are unmounted. The quotaoff command does not appear to update the aquota.* files with the information the kernel has in memory. I say this because the {a,c,m}times for the aquota.user file are only updated upon a reboot of the system or by manual running the quotacheck command. It appears - as far as I can tell - that the kernel just drops it's up-to-date usage data on the floor at shutdown. This information is never used to update the aquota.* files. They are updated during startup by quotacheck(rc.sysinit). Seems silly to me since that updated info had already been collected by the kernel. So...in conclusion I am still not entirely clear on the methods. ;)