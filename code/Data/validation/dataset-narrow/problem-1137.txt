The first book mentioned is one of the most widely known resources on algorithms, suitable as an introductory resource. The second book is little bit older and more theoretical, however imho still valuable. The third book is (in my opinion) an excellent and readable introduction to various parts of discrete mathematics. Later, I would recommend to focus on formal languages and automata theory. Here, my choice is the following book: 

Some basics of optimization (especially linear programming) are a must for computer scientists. Here, I would recommend the following gentle introduction: 

My question is rather general, but I have not been able to find a satisfactory answer for quite a long time, and I wonder if there has been a research concerning these issues. I have come over a relatively large number of optimization problems, that can be expressed as a Linear Programming problem, or as an Integer Linear Programming problem. Also decision problems could be expressed as LP or ILP, with a constant objective function (i.e., concerning only (in)feasibility). If a problem can be expressed as LP, then it is certainly in $\mathbf{P}$, since Linear Programming problems can be solved in polynomial time. On the other hand, problems that cannot be expressed as LP, but only as ILP, usually are $\mathbf{NP}$-complete, despite the fact that this is not a valid proof of $\mathbf{NP}$-completeness (there could be a polynomial algorithm not based on LP solving the problem). So my question is: is there any known relation between the complexity of a problem and it's expressibility in the form of (integer) Linear Programming (except the obvious facts stated in the previous paragraph)? Moreover, I believe that there are inherently nonlinear problems that cannot be expressed in the form of LP nor ILP. Is there some known relation between their complexity and their expressibility in the form of some type of Nonlinear Programming problems? Are there any references dealing with similar problems? Thank you in advance. 

Of course, plenty of introductory resources are available and it is impossible to be objective here. So what follows is merely my biased choice. I would definitely start with reading something on algorithms and on discrete mathematics (possibly in parallel). Here, I would recommend the following books: 

You might also like to consult the Sipser's book mentioned in one of the comments. Of course, if you have some background, there may not be a need for you to thoroughly study these textbooks. However, it is imho necessary to have a stable grounding in the above mentioned subjects before proceeding any further. Thus, it may be sensible for you to consult the above mentioned resources and focus on the material that you find new and/or interesting. After mastering these preliminaries, there are plenty of choices where to continue. In my opinion, you should definitely study some complexity theory. Here, I would recommend the following book: 

As pointed out by others, the obvious definition of an "adequate" coding is that it is equitranslatable with any standard one. The question is therefore to characterize such codings in terms of more elementary properties. (Historical note. Smullyan studied this question in the context of combinatory logic. When I was a student, Henk Barendregt suggested Smullyan's conjectures to me as a research problem -- which led to my first scientific publication.) See $URL$ $\newcommand{\code}[1]{\overline{#1}}$ To summarize, given a map $\code{\cdot} : \Lambda \to \Lambda$, we consider whether there exist combinators satisfying certain properties. The most significant ones are: 

It is easy to see that any adequate coding has these properties. The main result of the paper (Corollary 14) is that, for an adequate coding, one of the following suffices: 

There are several aspects to this very nice question, so I will structure this answer accordingly. $\newcommand{\setof}[1]{\{#1\}}$ $\newcommand{\thra}{\twoheadrightarrow}$ $\newcommand{\codeof}[1]{\lceil #1 \rceil}$ 1. The answer to the boxed question is no. The term $\Omega_3 = (\lambda x.xxx)(\lambda x.xxx)$ suggested by your friend is indeed a counterexample. It was earlier noticed in the comments that one has counterexamples like the "ogre" $K^\infty=Y K$, until the question is restricted to terms without weak head normal form. Such terms are known as zero terms. These are terms which never reduce to a lambda, under any substitution. For any fixed point combinator (fpc) $Y$, $Y I$ is a so-called mute (AKA "root-active") term: every reduct of it reduces further to a redex. $K^\infty$ is not mute; neither is $\Omega_3$ $-$ as is manifest by inspecting its set of reducts, which is $$\setof{\Omega_3 \underbrace{(\lambda x. xxx) \cdots (\lambda x. xxx)}_k \mid k \in \mathbb{N}}$$ Rather than give a precise argument why $Y I$ is mute for all fpcs $Y$ (indeed, for any looping combinator) $-$ which may be laborious yet hopefully clear enough $-$ I will treat the obvious generalization of your question, restricting to mute terms as well. Mute terms are a subclass of zero terms which are a subclass of unsolvable terms. Together these are perhaps the most popular choices for the concept of "meaningless" or "undefined" in the lambda calculus, corresponding to the trivial Berarducci, Levy-Longo, and B\"ohm trees, respectively. The lattice of notions of meaningless terms has been analyzed in detail by Paula Severi and Fer-Jan de Vries. [1] The mute terms constitute the bottom element in this lattice, i.e., the most restrictive notion of "undefined". 2. Let $M$ be a mute term, and $Y$ be a looping combinator with the property that $YI = M$. First we argue that, for a fresh variable $z$, $Yz$ actually looks a lot like the $Y_M$ you described, obtained by "sprinkling $z$ around" some reduct of $M$. By Church-Rosser, $YI$ and $M$ have a common reduct, $M'$. Take a standard reduction $R : YI \thra_s M'$. Every subterm of $M'$ corresponds to a unique subterm of $YI\equiv Yz[z:=I]$ under this reduction. For any subterm $C[N]=M'$, $R$ factors as $YI \thra C[N_0] \thra_{wh} C[N_1] \thra_i C[N]$, where the middle leg is a weak head reduction (and final leg is internal). $N$ is "guarded" by a $z$ iff this second leg contracts some redex $I P$, with $I$ a descendant of the substitution $[z:=I]$. Obviously, $Y$ has to guard some subterms of $M$, for otherwise it would be mute as well. On the other hand, it must be careful not to guard those subterms which are needed for non-termination, for otherwise it could not develop the infinite B\"ohm tree of a looping combinator. It thus suffices to find a mute term in which every subterm, of every reduct, is needed for non-normalization, in the sense that putting a variable in front of that subterm yields a normalizing term. Consider $\Psi = W W$, where $W = \lambda w. w I w w$. This is like $\Omega$, but at every iteration, we check that the occurrence of $W$ in the argument position is not "blocked" by a head variable, by feeding it an identity. Putting a $z$ in front of any subterm will eventually yield a normal form of shape $zP_1\cdots P_k$, where each $P_i$ is either $I$, $W$ or a "$z$-sprinkling" of these. So $\Psi$ is a counterexample to the generalized question. THEOREM. There is no looping combinator $Y$ such that $YI = \Psi$. PROOF. The set of all reducts of $\Psi$ is $\setof{WW,WIWW,IIIIWW,IIIWW,IIWW,IWW}$. In order to be convertible with $\Psi$, $YI$ must reduce to one of these. The argument is identical in all cases; for concreteness, suppose that $YI \thra IIWW$. Any standard reduction $YI \thra_s IIWW$ can be factored as \begin{align*} YI \thra_w P N_4, P \thra_w Q N_3, Q \thra_w N_1 N_2, \text{thus } YI \thra_w N_1 N_2 N_3 N_4\\ N_1 \thra I, N_2 \thra I, N_3 \thra W, N_4 \thra W \end{align*} Let us refer to the reduction $YI \thra_w N_1 N_2 N_3 N_4$ as $R_0$, and the reductions starting from $N_i$ as $R_i$. These reductions can be lifted over the substitution $[z:=I]$ to yield \begin{align*} R^z_0 : Yz \thra z^k(M_1 M_2 M_3 M_4)\\ N_i \equiv M_i[z:=I] \end{align*} so that $R_0$ is the composition $YI \stackrel{R^z_0[z:=I]}{\thra} I^k(N_1 \cdots N_4) \thra^k_w N_1 \cdots N_4$. Similarly, we can lift each $R_i : N_i \thra N \in \setof{I,W}$ as \begin{align*} R^z_i : M_i \thra N^z_i\\ R_i : N_i \stackrel{R^z_i[z:=I]}{\thra} N^z_i[z:=I] \thra_I N \end{align*} The second leg of this factorization of $R_i$ consists precisely of contracting those $I$-redexes which are created by the substitution $N^z_i[z:=I]$. (In particular, since $N$ is a normal form, so is $N^z_i$.) $N^z_i$ is what we called a "$z$-sprinkling of $N$", obtained by placing any number of $z$s around any number of subterms of $N$. Since $N \in \setof{I,W}$, the shape of $N^z_i$ will be one of \begin{align*} &z^{k_1}(\lambda x. z^{k_2}(x))\\ &z^{k_1}(\lambda w. z^{k_2}( z^{k_3}( z^{k_5}( z^{k_7}(w) z^{k_8}(\lambda x. z^{k_9}(x)) ) z^{k_6}(w) ) z^{k_4}(w) )) \end{align*} So $M_1 M_2 M_3 M_4 \thra N^z_1 N^z_2 N^z_3 N^z_4$, with $N^z_i$ a $z$-sprinkling of $I$ for $i =1,2$ and of $W$ for $i=3,4$. At the same time, the term $N^z_1 N^z_2 N^z_3 N^z_4$ should yet reduce to yield the infinite fpc Bohm tree $z(z(z(\cdots)))$. So there must exist a "sprinkle" $z^{k_j}$ in one of the $N^z_i$ which comes infinitely often to the head of the term, yet does not block further reductions of it. And now we are done. By inspecting each $N^z_i$, for $i \le 4$, and each possible value of $k_j$, for $j \le 2+7\lfloor \frac{i-1}{2} \rfloor$, we find that no such sprinkling exists. For example, if we modify the last $W$ in $IIWW$ as $W^z = \lambda w. z(wIww)$, then we get the normalizing reduction $$ IIWW^z \to I W W^z \to WW^z \to W^z I W^z W^z \to z (I I I I) W^z W^z \thra z I W^z W^z $$ (Notice that $\Omega$ admits such a sprinkling precisely because a certain subterm of it can be "guarded" without affecting non-normalization. The variable comes in head position, but enough redexes remain below.) 3. The "sprinkling transformation" has other uses. For example, by placing $z$ in front of every redex in $M$, we obtain a term $N = \lambda z. M_z$ which is a normal form, yet satisfies the equation $N I = M$. This was used by Statman in [2], for example. 4. Alternatively, if you relax the requirement that $Y I = M$, you can find various (weak) fpcs $Y$ which simulate the reduction of $M$, while outputting a chain of $z$s along the way. I am not sure this would answer your general question, but there are certainly a number of (computable) transformations $M \mapsto Y_M$ which output looping combinators for every mute $M$, in such a way that the reduction graph of $Y_M$ is structurally similar to that of $M$. For example, one can write $$ Y \codeof{M} z = \begin{cases} z (Y \codeof{P[x:=Q]} z) &M\equiv (\lambda x.P)Q\\ Y \codeof{N} z &M \text{ is not a redex and }M \to_{wh} N \end{cases}$$ [1] Severi P., de Vries FJ. (2011) Decomposing the Lattice of Meaningless Sets in the Infinitary Lambda Calculus. In: Beklemishev L.D., de Queiroz R. (eds) Logic, Language, Information and Computation. WoLLIC 2011. Lecture Notes in Computer Science, vol 6642. [2] Richard Statman. There is no hyperrecurrent S,K combinator. Research Report 91â€“133, Department of Mathematics, Carnegie Mellon University, Pittsburgh, PA, 1991. 

The use of complex analysis (and other "continuous" math) to attack "traditional" graph separator problems was memorable and is the main reason this paper stuck in my head even though it is completely unrelated to my research. 

Jon Kelner won the STOC Best Student Paper Award in 2004 for his paper "Spectral partitioning, eigenvalue bounds, and circle packings for graphs of bounded genus" I'll just quote from the abstract: 

My impression is that admissions committees are generally looking for "strong" students where "strong" is largely defined via prior research experience. i.e. they especially want to know if you've had the experience of doing research, were successful at it and have a good idea if this is something you really want. The letters are important as experienced researchers' evaluation of your research ability and potential and grades give some indication of overall academic aptitude. So I would claim that the specific area you're interested in is not AS important as having these indications of being a strong researcher through prior research, recommendations and grades (especially in relevant courses). After all it is entirely expected that you might do something different than what you first intended and change directions during grad school. But obviously the research area is taken into account, particularly when it comes to matching students with faculty (a department would not want to admit 10 theory students if there is only one theory faculty currently taking students). Also I would think it helps if you can show even some small project in the intended area, even if it's an unpublished ArXiv paper, just to show seriousness about the new area. I personally know someone who was admitted as a networking student and within one year switched and ended up as a very successful theory researcher. So in short I don't think the change of area will significantly hurt the chances of admission as long as you are an otherwise strong candidate. (NOTE: This is mostly based on what I saw of the CMU admission process) 

Here is a well argued article by David Donoho and Jonathan Buckheit that I read in grad school which touches on exactly this topic from the point of view of wavelet researchers: "WaveLab and Reproducible Research" Their idea was even more ambitious, to provide code for reproducing all the figures in their papers in a convenient Matlab package. I really like their idea but I think the issues are obvious. (1) It is extra work (cleaning up the code, making at least a rudimentary user interface, writing some documentation, providing some support when people inevitably run into problems) (2) It is not really required/expected by most conferences/reviewers But I can't help but feel that the CS research community would benefit if there was an expectation of making the code and data used in any publication publicly available in a usable format. I admit that I haven't done so myself even when the amount of work involved would have been manageable. I think it is just hard to make yourself put in the extra effort when there is no external push.