I am not aware of a paper concerned with the comparison between symbolic execution and abstract interpretation. Nor do I think one is needed. Reading the original descriptions of these two techniques should be enough. 

Another nice book that weaves type theory with programming in a functional language is Didier Remy's Using, Understanding, and Unraveling the OCaml Language. 

The paper [Chlebikova, Approximating the maximally balanced connected partition problem in graphs, 1996] studies a related problem: They have $k=2$, and they maximize $\min(|V_1|,|V_2|)$. It should be a good place to start searching for related results, by following the citation graph. 

Random observations, from foggy memory. A. The recursion for minimum vertex covers is similar to the recursion for minimum decision trees: $$\begin{align} {\it VC}(G) &= \min_{x \in G} \bigl( 1 + {\it VC}(G -x- N(x)) \bigr)\\ {\it DT}(G) &= \min_{x\in G} \bigl( 1 + {\it DT}(G-x-N(x)) + N(x) + {\it DT}(G-x)\bigr) \end{align}$$ Here, $N(x)$ is the set of vertices that are adjacent to $x$. (The recursion for decision trees is not completely obvious. One needs to show that in the true branch of the decision tree it's OK to first greedily check all the neighbors of the root $x$.) B. If $G$ is a tree, then one can build a minimum decision tree by using an algorithm similar to that for finding vertex covers of trees: always label the root of the decision tree by a vertex that has a neighbor of degree 1. C. The size ${\it DT}(G)$ of a minimum decision tree can be exponential in the size of $G$. For an example, consider path graphs. D. Consider two graphs $G_1$ and $G_2$ that share no vertex. Somewhat surprisingly, there seems to be no simple way of obtaining a minimum decision tree for $G_1 \cup G_2$ by combining two given minimum decision trees for $G_1$ and $G_2$. For an example, let each of $G_1$ and $G_2$ be path graphs on 4 vertices. Each minimum decision tree of $G_1$ has 5 decision nodes; thus, there are 6 leaves. It turns out that out of these leaves, 3 are labeled by true and 3 are labeled by false. Since $G_2$ is isomorphic to $G_1$, its minimum decision trees have similar shapes. Composing a minimum decision tree for $G_1$ with a minimum decision tree for $G_2$ (using the obvious algorithm for disjunction between decision trees) gives a decision tree with $5 + 3\times 5=20$ decision nodes. However, there exists a decision tree with only 19 decision nodes. 

There is a three-way trade-off between space, query time and approximation in the distance oracle problem. One can trivially answer each query exactly (approximation = $1$) in $O(1)$ time by storing the all-pair distance matrix; or in $O(m + n\log n)$ time by running a shortest path algorithm. For massive graphs, these two solutions may require prohibitively large space (to store the matrix) or query time (to run a shortest path algorithm). Hence, we allow approximation. For general graphs, the state-of-the-art is the distance oracle of Thorup and Zwick, which for any given approximation $k$, requires optimal space. It also gives you a nice space-approximation trade-off. For sparse graphs, a more general space-approximation-time trade-off can be shown. 

The above implies that given a graph with average degree $\mu$, we can replace it with a graph with maximum degree $\mu + 2$, and build the distance oracle on this new graph instead of the original graph. 

In their paper Approximate Distance Oracles, Thorup and Zwick showed that for any weighted undirected graph, it is possible to construct a data structure of size $O(k n^{1+1/k})$ that can return a $(2k-1)$-approximate distance between any pair of vertices in the graph. At a fundamental level, this construction achieves a space-approximation trade-off --- one can reduce the space requirements at the cost of a lower "quality" of the solution. 

I'm interested in an computational geometry problem that's sensibly expressed as an infinite dimensional 0-1 integer program. I'm not worried about finding an actual minimum for the objective function, any solution with isn't stupidly big will do. It thus seems natural to apply an approximation algorithm that starts by running simplex or similar restricted to $[0,1]$. I'd expect the solutions usually require only a few hundred dimensions, but any naive restriction of the problem space yields millions of dimensions. As I understand it, good implementations of a linear program solver should be polynomial time in both the dimension and constraints on average cases, but nevertheless this problem chokes GLPK. Should GLPK really choke on a million dimensions? I've therefore started looking for less naive restrictions of the problem space, which lead me to LP-type problems. In particular, there is a claim that Clarkson's algorithm applied to linear programs are equivalent to running the simplex algorithm on the dual problem. In what sense is this true? I find this claim highly dubious with respect to complexity for several reasons. First, Clarkson's algorithm does not exploit any $[0,1]$ solutions with fast average case solutions, but merely randomly chooses pivots. Second, Clarkson's algorithm has running time worse than exponential in the dimension $O(dn + d! d^k \log n)$, which doesn't rule out polynomial time for average cases, but I haven't found that fact yet. As an aside, any nice examples of improving a restriction of an infinite dimensional linear program over time? 

Background. I bumped into this conjecture while trying to prove a lower bound for a problem discussed in a program analysis paper, [Yang, Grigore, Abstraction Refinement Guided by a Learnt Probabilistic Model, 2016]. 

This is not an answer, but too long for a comment. I'm trying to explain why the question, as posed, may be hard to understand. There are two ways to define computational complexity for a device X. The first and most natural way is intrinsic. One needs to say how the device X uses the input, so that we may later look at how the size n of the input affects the run time of the device. One also needs to say what counts as an operation (or step). Then we simply let the device run on the input and count operations. The second is extrinsic. We define computational complexity for another device Y and then we program Y to act as a simulator for X. Since there may be multiple ways for Y to simulate X, we need to add that we are supposed to use the best one. Let me say the same with other words: We say that X takes $O(f(n))$ time on an input of size n if there exists a simulator of X implemented on machine Y that takes $f(n)$ time. For example, an intrinsic definition for NFA says that it takes n steps to process a string of length n; an extrinsic definition that uses a RAM machine as device Y says that the best known upper bound is probably what David Eppstein answered. (Otherwise it would be strange that (1) the best practical implementation pointed in the other answer does not use the better alternative and (2) no one here indicated a better alternative.) Note also that strictly speaking your device X is the regular expression, but since the NFA has the same size it is safe to take it as being the device X you are looking at. Now, when you use the second kind of definition it makes little sense to ask how restricting the features of device X affects the running time. It does however make sense to ask how restricting the features of device Y affects the running time. Obviously, allowing more powerful machines Y might allow us to simulate X faster. So, if we assume one of the most powerful machines that could be implemented (this rules out nondeterministic machines, for example) and come up with a lower bound $\Omega(f(n))$, then we know that no less powerful machine could do better. So, in a sense, the best answer you could hope for is a proof in something like the cell probe model that simulating an NFA needs a certain amount of time. (Note that if you take into account the conversion NFA to DFA you need time to write down the big DFA, so memory isn't the only issue there.) 

I do not work in theory, but my work requires reading (and understanding) theory papers every once in a while. Once I understand a (set of) results, I discuss these results with people I work with, most of whom do not work in theory as well. During one of such discussions, the following question came up: When does one say that two given algorithms are "similar"? What do I mean by "similar"? Let us say that two algorithms are said to be similar if you can make either of the following claims in a paper without confusing/annoying any reviewer (better definitions welcomed): Claim 1. "Algorithm $A$, which is similar to algorithm $B$, also solves problem $X$" Claim 2. "Our algorithm is similar to Algorithm $C$" Let me make it slightly more specific. Suppose we are working with graph algorithms. First some necessary conditions for the two algorithms to be similar: 

Among many possible notions of the sparsity, I am particularly interested in the one that relies on edge density (or alternatively, average degree). 

There is a sound theory of overloading operators and functions realized by type classes in Haskell, and to rougher extent by traits in Rust, etc. In mathematics however, there are many situations where one set carries the same structure in multiple ways, like the integers being a monoid under both addition and multiplication. We normally just give these different names, but potentially one wants to abstract some tougher mathematical function, or a proof done in the type system. In principle, one could do this the way mathematicians do it, by applying a type class to a type that associates the operations to the base type, as opposed to just the set itself with a fixed association. Is that the "right" or "only" way to gain this flexibility? Or is there something else? In particular, there are a bunch of languages like Scala that do overloading of overloading rules in a rather dangerously complex ways, well even incoherent instances in Haskell probably. It'd be interesting if there were clearer "more parametric" way to achieve the ends that motivates those design decisions.