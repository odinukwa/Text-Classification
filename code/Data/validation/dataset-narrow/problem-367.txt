innodb_log_buffer_size depends mostly on the size of your transactions. If you don't have big blobs in large transactions somewhere around 16M to 64M should be sufficient. See what works best for your workload. Start from a low size and keep an eye on innodb_log_waits status: 

More on this and how to set it up with Nginx: Typeahead and autosuggest with pure Solr and Nginx Option #3 You can use also redis for prefix match using ZRANGEBYLEX. That is quite limited compared to the previous options but also a way to implement autosuggest. 

Because you set as the primary key, the referencing column (pid) is not pointing to a unique item. From postgresql doc: 

tl;dr No. The binlogs contain a command before every transaction exactly to prevent this situation. This will only affect the session scope of the SQL thread so running queries meanwhile won't report date 600 seconds in the past. To take a look you can run: 

You didn't attach the table schemas so I just assume this would work. You might need to fully qualify the field names (using tablename.fieldname). Also feeds_processing_data could benefit from a composite index covering the involved fields. 

By having more tables you're certainly not going to increase the memory allocation for MySQL. Number of tables has no influence on memory usage. Discussing memory allocation is out of scope now but this may be useful to take a look: MySQL memory calculator. 

There is an alternative method of splitting where your main table contains only the small, filtering and sorting columns (including these columns too) and "data" is stored separately. It's always best to try which works best for your dataset and query patterns. I did a benchmark about a year ago comparing one big table with two splitting strategy that can help you get started: $URL$ 

I'm sorry, I don't have a corrupt table to help perform any verifications. My answers here must, by necessity, be heavily based on speculation. With the limited information we have, I am guessing that you are attempting to do a full database , along the lines of 

Nick, based on the information we've gathered, I'm going to venture a guess, but if I'm wrong, come back and let us know and there may still be something we can do. I wouldn't say I'm confident about the internals of auto-vacuum processes on hot standby servers. Streaming Replication, aka Log-Shipping Streaming replication, hot standby, log shipping...there's a lot of different names that people use, but the general idea is that you replicate your back up server by implementing a system where the transaction logs from your primary server are sent and re-implmented on a secondary server to keep the two identical. Now, you've stated that when you your table on the primary, you have significant lag on your standby, and also that your configuration for is set to -1, which disables logging of autovacuum actions. As a potential remedy, you could set this value to 0, indicating you wish to log all autovacuum operations. My guess is that, while autovacuuming is occurring periodically on your master, since it isn't being logged, it isn't replicated on the standby. When you run a manual , only then does the logged become replicated, which may be what's eating into your performance on the standby. How are things if you run a on the master, followed immediately by another on master? This will give us some indication of if I'm right or if I'm full of hot air. 

SQL alchemy returns a list of tuples. This is the standard behaviour. A row in PostgreSQL is represented as a tuple. Even if you query a single columns it's going to be a tuple with one element. Hence the representation. If you need a list of the first column you can easily transform your resultset in python: 

It actually doesn't accept NULL values it considers it as empty string. That's because you have your server in non-strict mode. That controls how MySQL handles invalid or missing values in inserts and updates. You can read more about modes here: $URL$ 

To keep all the state information with the current row to avoid conditions you can so something like: 

Note that the reltuples is an estimate of the rows not the actual count at the moment but generally it's quite accurate (depends on how up to date your statistics are). 

MySQL is completely capable of serving as your full text search engine. InnoDB FTS indexes are reasonably good. With the size you described it should all fit in memory but of course that depends on the other tenants on the shared hosting. If it's not available (you mentioned shared hosting) you can implement your own pretty easily. Full text search is just basically inverted indexing. This way you can also have any custom trickery over it you want in your application which might not be possible with InnoDB FTS. You can see a basic implementation of this and performance + overhead comparison here: $URL$ 

Finally, I simply join this result to itself, comparing one row to the next by its row number, and looking for the text entry. Here's a SQL Fiddle for testing. For your perusal: Common Table Expressions Window Functions Edit If you want to return all rows as column pairs, then use the following approach: 

I know that I should test my SQL script as well, but I don't have time to make a test before leaving for work! I can make a SQL Fiddle later, but for now it seems that no one has quite answered the question correctly. 

where in this case is simply the result of the above . The second argument indicated to that the items are comma-separated. This will yield a field containing your desired entries. 

I think you should really look into using inheritance to manage this particular issue. Inheritance Table inheritance does many things in PostgreSQL, and I'd highly recommend you refer to the online documentation about inheritance to get a feel for what it does and how it might work for you. Since you stated that your are all structurally similar, then you can make a parent table as 

So, in brief, the function takes three input arguments, and uses these to generate a dynamic SQL statement, which it then executes. Inputs - Exactly what you think it is. The json entry you want to update with. - The target table you want to update. - As I mentioned above, since I made the assumption based on your description that you had pre-established predicates, that entry goes here. Operation The function searches for any columns in the table , searching for any columns whose names match keys in the field, by performing the sub-select . For any json key which matches a column name, that column name is returned by the outer select . The loop iterates over each of these matching column names, adding on to the dynamic SQL statement the necessary info for updating the relevant column data. NOTE that 'extraneous' fields have been ignored. That is, if there is a json key for which there is not a matching column name, or there is a column name which is not in the input, these fields are ignored. Calling the function The function can be invoked by simply calling 

Always test the config parameters and see what works best for your workload. Most of them are dynamic so easy too change in runtime, test, analyze and repeat until you find the best. 

Relational DB seems a perfect fit for your needs. MySQL will probably give you some hard time on some of the queries you want to run or you need to off-load that to the application side but that doesn't mean it cannot be done. Depends on the size of your tables too. Databases with window function capabilities (practically anything else than MySQL) will help you with queries like "time between commits". I'd personally recommend PostgreSQL which is great for analytical queries as well. Document stores and key-value stores are not meant to be backends for serving analytical data retrieval. You might gain some performance on read/write operations but since your application has to do the heavy lifting instead of the database the number of operations increases significantly. So with those it might even be slower at the end. How you represent that data at the end is more of a business questions than technical. What your users want. Do they prefer excel sheets over for example an internal websites where they can check? Google Sheets might be a middle ground there. Just an idea. 

If you want to directly pipe it to mail you can A) use the flag. The help is a bit confusing by saying: 

One remark on : in most cases you will find that it is quite expensive. Although very useful for OLAP type scenarios it might not be very well suited for OLTP because the operation can take significant time of your query and as you can see sometimes the suboptimal execution plan is actually faster. Fortunately MySQL provides switches for optimizer so you can customize it as you wish. For all the option you can run: 

I can't build a full fledged test just right now, because I've gotta hit the road. Just note that this code is as of right now untested, but you can easily give it a shot since you already have the data tables. ;) Using JSON + + CTEs It looks like to me, from your description, that a entry is an array of JSON. I'm just making this clear because you earlier stated that is a JSON, but your data sample seems to conflict. In this case, why not use a simple Common Table Expression (the keyword) along with to extract the relevant JSONs as rows, and then perform your query? 

There aren't many cases where I'd recommend this course, only if this is a tiny project which doesn't warrant much of your time. Dimensions and Facts So, if you've cleared the hurdle of question (1), and you want a more performance schema, this is one of the first options to consider. It includes some basic normailization, but extracting the 'dimensional' quantities from the measured 'fact' quantities. Essentially, you'll want a table to record info about the trips, 

I'll venture a guess at what you're looking for. I've made the assumption that the table is called , the column which contains the entries for , , etc. is called , and of course you have your other columns. So, with a simplified table and some data defined as 

To me, it looks like you're on the right track, if I'm understanding your question clearly (which I'm not sure that I am. :P ) To me, it looks like you simply need a block, where you are declaring variable values which will persist throughout the function block. Add 

(These are only the very basics there are much more your can do here) You should worry about IO usage first because CPU and Memory are less likely to become the bottleneck. Keep an eye on IO utilization to identify when you're likely to hit the ceiling of what your system can do. If your writes are competing for locks it's possible you will experience high CPU usage but that's usually coming from mutex contention not "actual" CPU work. These can be remedied designing your tables and queries in such a way that they don't lock out each other. 

The records are stored in a B+Tree on the Primary Key Every single secondary indexes will have your primary key appended to them 

Since you have queries where you only have one of the column or these rows will be close(r) to each other so InnoDB needs to read less pages. Which can significantly speed up multiple rows resulting queries. Cons: 

InnoDb ruby is a great tool which does exactly what you want. Troubleshoot If you don't use this can also be normal data growth so confirm this first. If you use then it is most likely indeed the undo history. You can confirm that by running: 

Workload Assuming InnoDB engine you want primary key to be a continuously incrementing id so INSERT won't be random causing too much page split and fragment the tablespace. Benefits of : 

2) Populate your data and keep it up to date This can be done from your application, cronjob or trigger in mysql. Whichever you feel safe with. 3) Change your query to something like this 

Your issue appears to be that you are applying the same (named ) for both your and your . When you use a which contains an clause, and you then apply certain aggregations such as or , it applies the aggregation continuously across the ordering, which is why your and are identical. If you modify your query have multiple windows as 

You've got the casting right, but if you want to loop through them easily in the function, you just need to do it upfront with a block. We can leave the debate for another time, however, if this is the best approach for achieving the inserts you are hoping to do... 

Again, as expected, the inclusion of reveals to us some very important information. In the low case, we see that there are rows removed by the index recheck, and that we have heap blocks. Conclusion? (or lack thereof) Unfortunately, it looks like on its own is not sufficient to know whether an index recheck will actually be necessary because some of the row id's are being dropped in favor of retaining pages during the bitmap heap scan. Using is fine for diagnosing the issues with moderate length queries, but in the case that a query is taking an extremely long time to complete, then running to discover that your bitmap index is converting to lossy due to insufficient is still a difficult constraint. I wish there was a way to have estimate the likelihood of this occurrence from the table statistics. 

Timestamp would be a great candidate for PK if you can keep it since it's most likely will participate in most of your queries and most like as a range. (PK is at the end of every secondary key). You could also experiment with the an instead. Use whichever gives better performance. Size Try to keep the row size as small as possible to avoid sharding/partitioning/archiving as long as possible (eventually you will need to but MySQL is quite good with tens of millions of rows even). To achieve this use only IDs in this table and have lookup tables where necessary. For example: instead of use . is 1 byte whereas action will be always at least 2 assuming , or similar small character set but longer the string becomes the more byte it will use obviously. can use up to 4 bytes / character. Only put index on columns where you really need to. Daily, monthly, etc aggregation tables will serve you better than heavy indexes on this table. A possible schema 

The behaviour you're describing is called MVCC (Multi version concurrency control). Strictly saying it's not delete + insert. It is more like: 

Use Innodb unless you have a specific reason not to. If your coming from Oracle you will find its internal behaviour familiar therefore learning curve is shorter. For what you described you probably will want to enable (check which format suits your use case better: , or ). For hot backups I recommend to use percona-xtrabackup. It can stream data directly to a remote server so you don't even have to have double the free space available on the backup host. The documentation is very good and detailed and you can also find many source online. I hope this helps getting you started. Good luck!