Name your relationships. For example the line from Airport to Airline - is that "based out of", "offers service to" or "banned from landing in"? Any or all may be valid in your problem space but without good names the subtle differences will not become apparent. And be honest with yourself. Using names like "has" and "relates to" is a cop out. You relate an Airport to one City. What about Dallas/Fort Worth? Have a think about the output you will want from this system. Work through the corresponding queries in your mind and verify the ERD satisfies them. If not, you have to refine your rules and model them appropriately. 

Run the query in SSMS and look at the actual execution plan. Highlight the Compute Scalar and look at its properties (F4). Look at the Defined Values property. On my system I got this: 

Large, general-purpose search queries often produce an inefficient plan. This can be ameliorated by adding to the query to invoke parameter embedding. Now your problem is that the general-purpose query is recompiled with every execution, which is itself an overhead. Avoid this by pulling out the common sets of parameters and have specific queries for each: 

My understanding is that functions, such as , are executed once per column reference to in SQL. Thus any query which scans the whole table will produce the same "random" value for every row. I think you're going to have to iterate through your data setting random numbers one at a time, RBAR-stylie. 

I suspect you would have no concerns about using text? Just think of them this way, but with the A, B and C removed. Or think of your alphabet as consisting of 36 characters, a to z and 0 to 9. 

This is possible using piecemeal restore. The source database's objects and filegroups have to be organised in a way that supports this. The RESTORE requires additional specific keywords. While the concept may take a little bit of getting used to, the amount of scripting to implement is no more than that for other solutions suggested. 

The database is an encoding of the business rules. Without understanding the rules there is no way of making sense of the database. Take it piece-by-piece, focusing on one area until you have a reasonable understanding of it. Even if documentation exists I like to draw ER diagrams from scratch as it helps me remember the connections. It also allows me to compare how I would have done it to how it is currently implemented. The differences are an opportunity for further learning. Reading the code will bring out further "virtual" FKs and alternate keys that are not declared explicitly. 

This eliminates all of the second-order dataset alignment on derived aggregate values required above. It is much easier to read and understand, and hence to maintain. 

A full answer to this would be huge. Off the top of my head, some of the things a RDBMS will give you are 

The two examples are not equivalent and interchangable implementations; they embody different semantics. In the first, the three-way table imposes no conditions on the participants. The example shows this. Alex is enrolled in Biology, Emma is assigned to Alex and Emma knows how to tutor Physics. The tutor's subject is Physics but has a student who's enrolled in Biology and there's nothing in the model to stop them discussing, say, Economics. In the second, the tutor_id can only be associated with a pre-existing student_course_id. So Alex has to be enrolled in Biology before Emma can become his tutor. Since there's only one course_id (through the associated student_course) we can assume this is what they will discuss in their tutorials. However, there is still no way of asserting that Emma is part of Alex's faculty. If you're still working out the logical data model I would suggest you skip the surrogate keys and use only the natural keys for now i.e. drop the various "id" columns and only use course.name, student.name and tutor.name. Surrogate keys are great performance enhancers in real DBMS implementation but are not required when understanding and documenting the problem. They can be substituted in later when you're confident you have solved your problems. Next you need to understand the constraints on the data and the questions the DB has to answer. For example a constraint may be that a tutor can only work in the subject which she is employed. A question may be "who's available to tutor a Chemistry student?" Once you have these, the tables to enforce and answer them will emerge, as will the foreign key constraints. You may be content with any tutor teaching any subject to any student. That's OK if it is so, I can't tell form the question. Or you may choose to enforce this outside of the database - in the application, say, or through a written policy enforced by management. The important thing is to understand the rules as they are, and how they're implemented, so that when they change the appropriate adjustments can be made. Do not be afraid of having lots of tables with a few rows if this is what the data demands. Would you bolt on a new clickstream half way through an existing web site just because you "were going to have it anyway"? 

If you really want id_Products in Options for performance reasons (you do not need it there to ensure data integrity; indeed it confuses that argument) you can do so. Make the foreign key in Options a multi-column key. Make it point to the corresponding columns in Variants. Even if Variants.Id is unique across all products there is no risk to the data by including is_Product in the primary key. Usually this is a waste of time. id_Products will not be meaningful to a human so all queries will join to Products to get the semantic value. You can join Products to Options without having to have an explicit FK constraint in place. 

The same argument holds for Relationship 2 and 3. has only two columns - the ID and the number. I'd suggest quite strongly that it would be an error to have two rows in this table with the same number. So a unique contraint on the number would be appropriate. The ID column, which is the primary key, will have a unique constraint, too. So we have a table where each row consists of a unique number (ID) paired with a unique number (the phone number). It seems to me one of these is redundant. You may choose to retain the ID as a surrogate key since an integer (the most likely data type) will be shorter than a full international telephone number, especially with formatting characters embedded. Adopting this would give the table 

The sub-select will return more than one column but it is in the SELECT section of the outer query where only one column is allowed. Move this nested select into the WHERE clause as a join. 

A row here records the movement of a given quantity of a particular product from one recipient to another. You know the participant and the product from the corresponding account. From the product you can determine if the Quantity represents a kilogram or an item. For ease of processing you'll probably want to duplicate this under each account. This will need another table: 

I would go for separate tables for each type. With so few rows performance will not be a consideration either way. Individual tables will only have a page or two each. Similarly for the combined table. Either way you will have to join a "type" table to complete your views. Inserts are infrequent at this level of abstraction so locking is unlikely to be an issue either way. For me, though, having different tables keeps logically distinct items separated, which is a cleaner design. There will be many different tables which will have a relationship to a type table of some sort. If there are many different type tables the foreign key relationships are self-documenting and enforcable through DRI. With one combined table it is difficult to use DRI to say "Venue.TypeID comes from Type.TypeID but only if the Type.Category = 'V'", for example. The complementary side of this relationship - "Type.TypeID with Type.Category = 'V' can only be used in the Venue table" - is also not supported in any RDBMS's DRI that I know of. 

In the above replace with whatever Access delivers from your search field when you don't type anything there explicitly. To force the user to enter at least one search field add another AND to the query (again, pseudocode): 

Sum the AccountTransaction tables for your participant ID - these accounts record the units assigned to you and, presumably, sitting in your warehouse. 

Starting at the end - if you have, say, 20 large tables of 10 millions rows each at 200 bytes per row that works out at just under 40GB. Add twice as much for indexes and your entire database will fit into memory. With sensibile indexes in place you really shouldn't worry too much about joining multiple tables. Joins are what relational databases do, it's what their code is optimised for. Don't be frightend of using your new software for its intended purpose. Have you considered having different views for each specific use-case? These will be easier to write and maintain. The optimiser will likely do a better job, too, if it has fewer objects to consider in each view. If you want an alternative, however, you could try modeling your data in a star schema, like a data warehouse. will take on the "fact" role and the others taking on the "dimension" roles. You'll still have multiple joins but the opportunity for successive calculations on statistics producing silly answers is much reduced. I would not try to shoehorn all the task-related values into one generic mega-table. Think what the SQL would look like if you implemented . Does that really look better than joins on normalised tables? 

If you wrote the query statically with all the passed values in the WHERE, and added , this will cause the optimiser to do parameter embedding. The redundant predicates will be stripped. Only the needed query operators will be in the plan. You are probably suffering the cost of a statement compilation with each invocation anyway. The statement will be much easier to write correctly and maintain. 

Deletes could be trickier, depending on how the chains interact. If they are all independent (A->B->C and X->Y->Z) then it is easy - simply remove the row and close up the pointers. If the chains merge (A->B->C->P and X->Y->Z->P) it gets more complicated. In extremis, they can be re-calculated from scratch, of course. Without pre-calculation a lazy evaluation may be in order. Once a path has been enumerated, it can be stored against the starting point (and all intermediate nodes) so the expensive path walk need not be performed again. Whether or not this is cost-effective will depend on the read/ write ratio. I will note in passing that this is textbook material for graph processing. If a move to another tool were possible, or when SQL Server's graph processing matures to a sufficient level, I would recommend considering that approach. 

No, you should not have multiple State tables. Having a single table with relationships to the others is the right way to approach this. The perception that this somehow implies that the other tables are linked, other than they share a State, is purely in your head - such an interpretation would not be common practice. You should distinguish between your logical data model and your physical database design. I would recommend adding the relationships to State into the logical model. You may choose to omit the foreign key constraints from your database for performance or other reasons. There's a post on Dell's Toad site which I can't find just now. It's about a 200 table DB, every one of which has a column and corresponding FK relationship to the table. The plan for a statement is a thing of beauty - a perfect left-deep triangle of 200 joins! They would have been better without the constraint. Edit: here's the link. I mis-remembered the details but the principle's the same.