The hard part be will choosing a value for . You can initially try constants -- such as or . This makes the velocity directly proportional to the start and end distance. If that feels good, great. If it doesn't you may need to try more complex scaling factors to achieve more of an easing curve where the velocity for small movements is low but increases greater than linearly as the movement delta becomes larger. For example, you might compute as: 

So basically, For iOS the answer is definitely no. For Android in the Play store, the answer is also no. In both cases, you can't charge users directly via your own payment portal. You'll have to use the in-ecosystem IAP mechanisms available to you and convert that into appropriate tokens for representing purchases on your back-end. I'm having a harder time finding the relevant guidelines for other potentially-applicable Android stores, such as the Kindle store (the guidelines are here, but they don't seem to explicitly call out that you must use the Amazon API for purchasing -- only that Google's API will not work). 

Some people certainly do organize different subsytems, such as physics and rendering, into distinct DLLs. There's no reason you have to do this, though, and as above it just creates extra complexity. You don't have a reason to be doing so at this point in your project, it sounds like, so I wouldn't worry about it. Generally you want to factor subsystems out into DLLs when you expect them to be versioned independant, shared by multiple projects, or swapped out for alternative implementations. All of those are the domain of very moderately complex projects. 

You may also run into a technique called color key transparency. This is a similar technique, often used when for whatever reason alpha channels aren't available or are undesired. It's less common these days, it was used more often before hardware accelerated rendering was so common and we would blit our sprites by hand. It works, essentially, by selecting a color meant to be transparent (usually something like a hot pink or full blue, something unlikely to be used for "real" pixels) and simply not copying those pixels when blitting an image. These days it is often easier to simply use the alpha-based methods, or at least translate color-keyed sprites into ones with alpha by the time you get to rendering via the graphics card. 

For older shader models, "global" variables are just mapped to GPU registers. They may be faster at a micro-timing level than texture lookups, but you probably won't have enough of them to sufficiently represent the data you want. Updating each constant variable would also have some overhead, but since you said updates will be quite rare, that may not be a problem. Textures are probably want you want if you are using an old model. On newer shader models (4 and up) "global" variables (shader constants) are stored in constant buffers or texture buffers. You'd want to use whichever is more appropriate for your data and update/access patterns. 

There is no need to declare the game instance as a global, fortunately. Windows allows you to store extra "user data" alongside each window instance you create, and you can subsequently access that data any time you have a handle to that window (which you do in your window procedure). The functions you'll want to use are SetWindowLongPtr and GetWindowLongPtr. These functions retrieve a pointer-sized chunk of data associated with the window -- you'll probably want to use them with the constant to access the specific pointer-sized chunk of memory reserved by the window system for your use. If you need to store more than a pointer-sized chunk of data, you can request extra storage bytes during the creation of your window's window class by filling out the parameter in the class definition structure when you call RegisterClassEx. However, it sounds like will be sufficient for your needs. So, in summary: 

For Android, it may matter which store you sell through; you didn't specify. Google Play's content policy says, of in-app purchases: 

There appears to be nothing built-in to the API for this (most similar APIs have cancellation support, but libgdx's doesn't appear to). You can implement it yourself. A simple approach is to use an appropriate thread synchronization primitive to control or act as signal to your asynchronous task that it should terminate stop working and early-out. Properly guarded or marked up, a simple Boolean can suffice. Typically this means you will need to either check if the Boolean has been set to signal cancellation at the top of your tasks main loop and/or periodically throughout the execution of your task. You can wrap this behavior up in your own utility class to avoid having to repeat it for every async operation you want to build, but even if you remove that boilerplate work you'll still have the issue where the cancellation is not immediate, it only happens at the level of granularity you perform the cancellation check. This is actually bad (because it means you could have a long wait between requesting a cancel and the actual cancel happening) and good (because it means you can make sure it's not possible to abort the async task while it's in the middle of transitioning some data between two valid states, and that way you ensure class invariants remain in place). 

The ES3 compatibility extension aggregates those (and a few others). You can query for these extensions to check support, presumably. 

There's certainly no "right" way to approach this problem, but there are better ways based on software engineering guidelines and principles. From what you've described, I'd approach the problem this way: A level contains a set of agent objects that exist within the level. Agents could be any kind of character or other entity, player-controlled or not. The player object deals with the actual human player sitting at the keyboard, things like score, name, lives left, et cetera. It might also (directly or indirectly via another interface) manage the transformation of keyboard or controller input into in-game responses, and it can accomplish this by maintaining a reference to the currently-controlled agent in a level. In this fashion the level object and the player object don't actually know about each other directly at all; the level just knows it deals with a bunch of agents, and the player knows it deals with one particular agent that is the one the player is directly controlling right now. There are three key principles at work here: 

The transforms are stored in the bone transformation matrix, and you should be able to extract them via: 

Hull & Domains The hull and domain shader stages are part of the tessellation pipeline of the GPU. They're generally used to compute highly-detailed surface geometry based on lower-detail input surface geometry, which is defined as triangles or quads (et cetera). The lower-detail input primitives are called "patches," and it's important to note that they may not represent actual geometry that will eventually exist (although they could). Think of more like the control points of a bezier curve, except for a surface. The hull shader takes an input patch and produces an output patch (or patches; this is where subdivision of the patch would generally occur). Constant metadata about the patch can also be computed within the hull shader and output for processed by later stages of the pipeline. The output of the hull shader runs through a (fixed function) tessellation stage which produces tiled, normalized domains of the appropriate type (e.g., quads or triangles). The domain shader is executed against these domains in order to compute the actual vertex position of any given point in a domain that resulted from the aforementioned tessellation. The domain shader thus outputs a vertex position. The tessellation phase occurs after the vertex shader stage in the pipeline. Geometry Shaders Geometry shaders are like simplified hull/domain shaders, in a way. They simply take input vertices and produce output vertices. For a given input vertex, many output vertices can be produced, so they can be used to "generate geometry." The geometry shader stage occurs after the vertex shader, and after the tessellation stage. Uses The geometry shader can write to stream output buffers instead of being fed directly into the rasterization and fragment shading phase of the pipeline; this effectively means you can re-run the geometry produced by a combination of a vertex/hull/domain/geometry shader iteration back through the pipeline, to perform additional work in another vertex shader stage or whatever. What you can use these for is a rather broad, effectively-unlimited topic, so I won't really attempt to address that. But as for some motivating reasons to consider using them... The big thing about these shader stages is that they let you get a potentially quite a lot of extra detail without paying the memory or bandwidth cost for all of it, all the time. And also to move processing from the CPU to GPU. Terrain is a good example of where you might want to use some of this technology, as you generally need to see it both very close (as your character is standing on it) and very far away (the mountains in the distance) and being able to control where and how much detail you put into the terrain geometry "on the fly" via these shader stages is very powerful. The alternatives have historically been either paying a constant average cost for the terrain at all times (a lowest-common-denominator approach) or manually paging chunks of geometry for different levels-of-detail in and out of GPU memory, which is tedious and expensive. Any similar situation where you might have a really broad range of levels-of-detail you need to support for some mesh or model that is also reasonably subdividable may be a candidate for doing something clever with these shaders as well. Not everything translates well to subdivision surface style optimization, though. You could probably use them for cloth and hair as well. For further reading, including vastly more detail than I can reasonably remember or go into here: 

Through the public interface of the component base class. Don't over-think it -- that's a trap lots of people fall into with components. Designing them is no different than designing any other subsystem. If the interface of your item components is thus 

By giving them actual pros and cons. Damage and attack speed are one way to provide a pro/con relationship -- high damage/slow rate of fire versus low damage/high rate of fire -- for example. But that relationship can be a mathematical no-op if the resulting DPS is the same. If you put weapons out of alignment, such that the DPS is not always the same, then as you said you have an obviously better weapon. So, consider adding further attributes or effects to weapon types, or to the combat system in general. For example, if your game supports the idea of "% chance to do X on hit" effects, that can allow you to drop the overall DPS of "fast" weapons, since "fast" weapons would make up for that by affording more opportunities for the "% chance" roll to trigger. Thus, the player has a choice between a weapon that offers a higher pure DPS or one that has a lower DPS but a higher overall chance of triggering on-hit effects. You can also apply certain kinds of effects exclusively to certain kinds of attacks. For example, in Skyrim, each flavor of magic (fire, ice, lightning) has a unique side-effect. Fire magic causes additional damage over time via burning, whereas cold magic slows the target and lightning magic reduces the target's own mana pool. This provides a choice for players, even though the overall DPS (ignoring resistances) may be the same: they get to choose a style that complements how they want to play, either a brute force approach via fire damage or a more subtle one via cold spells. Consider allowing certain kinds of skills only with certain kinds of weapons; relate those skills to the nature of the weapon in real-life. Daggers, for example, could feasibly be thrown and might have a few range-related skills (but you're not likely to be hurling your giant two-handed sword around; perhaps it would have instead skills for rendering foes immobile by damaging or destroying their limbs). And then there's damage type you could model: piercing versus slashing versus bludgeoning damage. In D&D, certain creatures could only be hurt with certain kinds of damage. While that might be too draconian for modern games, you could incorporate some amount of that flavor. What you really need to do is force players to make an interesting choice; games are all about interesting choices. 

"Best" is pretty subjective -- it's going to involve weighing what you need out of the algorithm versus it's inputs, runtime complexity and other properties. That said, both your approach and the linked FlipCode approach are reasonable in terms of producing usable results (you could quite simply copy your 'face normals' to each vertex, if you are not sharing actual vertex instances between triangles, which I'm unclear on from your code). Other techniques include weighting the contribution of each face normal by the size of the angle made with each face shared by the vertex. You are correct in that the FlipCode approach appears suboptimal as written, but it could just be poorly specified: it's a little unclear whether or not it intends to suggest the second loop traverse all faces in the model, versus the handful of faces that share the vertex in question. If you have the adjacency information to reduce the search space of the second loop, it becomes less of a concern. Of course you may not have that adjacency information -- this is sort of what I mean by considering what inputs you have available to your algorithm. If you don't just want to copy the face normal into the vertices, or you are sharing vertices and don't want to split them, you could: 

The leaderboard documentation doesn't explicitly say (as of this writing). However if you look at the documentation for the C++ API's object, you can see that the value of the score is exposed as a , which means you have 64 bits of storage for your scores (as integers, times, or currencies). As an integer this value is 18,446,744,073,709,551,615. Note that you can also set explicit upper and lower limits on scores allowed on the leaderboard via the API. 

is an alias for the type , which is a pointer to a Direct3D 9 device object. You are using D3D11, which means you have a completely incompatible device interface. Your device initialization is (probably) fine, you will just not be able to use because it's only supported on an older version of Direct3D. D3DX was deprecated after D3D10. The functionality you are looking for is no longer available as part of the core SDK, you will need to find it elsewhere. You can either write your own, for any number of file formats including ( is popular and Google can help you find tutorials) or utilize a 3rd party library such as Assimp. 

That said, I would suggest instead that you don't transmit ASCII data as your packet payloads, but instead serialize them to an efficient binary representation (this is not the same as encrypting them). You'll have far fewer character-escape issues this way, because the structure of your chat packet (for example) will be something like four bytes to indicate a message length, followed by n bytes of character data that you know is the entire message. (You can, of course, use the length-prefixing technique in an ASCII-format payload as well, as suggested by Byte56 in his answer.) Alternatively, if you're going to use ASCII payload data for diagnostics and visibility, choose a format that is already known so you can leverage existing specifications for character escapes (or even better, existing 3rd party libraries). I really don't recommend ASCII transports for data that goes across the public internet, but if you are transmitting only in a more controlled environment (LAN or intranet where the bandwidth issue isn't as severe, nor are the visibility concerns) it can be a reasonable approach.