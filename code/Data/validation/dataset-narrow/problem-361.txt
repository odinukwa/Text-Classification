Based on the description, this is related to Amazon Aurora product, and not PostgreSQL by itself. See $URL$ 

Indeed, you should not have uninstalled it and re-installed it, as PostgreSQL starts a new database store from scratch, hence empty. BTW it may happen that your data is still there, but in another old directory. See for example this article ($URL$ that shows an upgrade between 9.4 and 9.5 but it may apply in the same way in your case. If you look at the PostgreSQL formula you can see: 

There is no absolute right or wrong way to do it, because it depends on how you use this data afterwards, that is what kind of SELECT queries will you need on it. So your schema could be good enough without anything to improve. Here is my alternate take that could give you other ideas: 

So Amazon added some settings. You will need to contact them directly and ask since their documentation does not provide a result when doing a search, $URL$ gives Your search for "shared_heap_size" returned no results. 

How can I mix boolean logic with bitwise and/or operators in SQL Server, specifically SQL Azure Database and/or SQL Server 2016? To demonstrate what I'm trying to do, consider the following script: 

As expected, this index gets fragmented pretty quickly so we have to be on the ball with keeping it rebuilt. However, this is where we have a problem. My maintenance script that goes through and rebuilds all indexes over x% fragmented with always hangs forever on these indexes. I've tried to let one run for over 3 hours without success when the database was mostly to completely idle. Just for kicks, I even put the DB into single user mode and nothing changed. I also tried with , again with no change. However, I am able to simply drop/create the index and it completes within ~15 seconds! Just to make sure there's nothing funny with my maintenance script, I've manually tried rebuilding these indexes with no success. The script I use is: and it seems like it will never succeed. Given that I can drop and recreate it in only 15 seconds, 3 hours is WAY more than plenty to see this succeed on an idle database. For now, I've altered my maintenance script to filter out indexes with "GlobalID" in the name and then have a follow-up script that drops/creates these indexes. This gets us by until we start having naming variations on these types of columns (i.e. we need a uniqueidentifier for some other purpose). Any idea why rebuilding such indexes would never finish within a reasonable amount of time? I see this happen across ~12 tables, all with essentially the same column/index on them. 

Is it possible to set up an alert in SQL Server 2008 that will send an e-mail anytime a job in a specific category fails? I am wondering because I would like to set up an e-mail anytime an SSRS subscription fails - and all of these subscriptions are jobs in the category Report Server. EDIT - it turns out that when an SSRS subscription fails, the job itself does not fail, so my question will not apply to the SSRS subscription monitoring use. However I would still like to know for other jobs that we run in our environment 

I have set up a predefined replication alert (as outlined here) in SQL Server 2008 and the alert for Replication Warning: Transactional replication latency does not appear to work. This is triggered by error 14161. I found a number of posts around the web that indicated this was a bug, but the posts were so old, I'm not sure that it is still the case. Is this still a broken feature? If so, can anyone suggest a work-around? EDIT/ADDITIONAL INFO: I see there are a number of scripts that have been highlighted in similar questions. To refine my question, I'd like to confirm this is a bug and I am looking for a work around that is rather out-of-the-box... that is, just another way to write the SQL Server Agent alert to get it to work... 

so should be called only if directory is not already populated, so in your case the database just got initialized in a new directory, empty, while your data is still in another directory. But things have changed recently. Homebrew 1.5, released on January 19th 2018 has this in its changelog: 

step 4 : Compare sort keys: The second value is enough to sort them all, and it is in fact already in increasing order, so the final order is indeed: 

It is a very broad question, even more so as you give absolutely no details about your setup and kind of database (volume, type of queries, active connections, size of RAM, dedicated server or not, etc.) You can start by enabling PostgreSQL to log slow queries, see the in the configuration. That will give you historical data that you would then be able to analyze and maybe correlate with other things (like from the list of @VÃ©race) You will then have various tools to help, as described on $URL$ : 

The BlitzIndex tool that @JMarx suggested is working great! However, I'm also finding this additional script to make some good suggestions as well. Not necessarily using all or even most of its suggestions, but cherry-picking from the top is proving very useful! 

That table originally had only 2 indexes: the Clustered PK index (that this shows it doing the Key Lookup on) and another FK index on a column not referenced here. Given that this heavy query always needs these add'l columns (DateForValue [datetime], CurveValue [float], BTUFactor [float], and FuelShrink [float]), I thought a covering index was the obvious solution here to remove the (slow) Key Lookup being performed here. So I added the following covering index: 

Does this sound right to you guys? I expected it to drop most of my indexes but then to create a ton of new indexes. Also, if it takes 4 hours to analyze 9k queries, is it even feasible for me to get this to consider a normal day's worth of usage? Compared to most large databases, ours is fairly light on consumption (~50 users total). I think I'm either misunderstanding something or am simply doing something wrong. 

The sum of the days the subscription is set to run is the number in this column. Thus subscriptions that run Monday through Friday have a value in this column of . The end result I want is a view that derives T/F flags for each day based on this number so that I have a column for each day. The method I am currently exploring to get me there is converting this integer to base-2 so then I can convert that to a varchar and parse out the days. In this example, the result would be 111110. The final twist - I do not have the ability to create functions or stored procedures in this database, so my strong preference is to solve this within a statement... (If push comes to shove, I will move the raw data and use a function in a separate database - and have found a number of those online.) 

I have a number of Oracle stored procedures whose activity I want to be able to audit, so I am having them INSERT some data into an auditing table. It would be swell if I could list the variables and their values as well. Is it possible to get this list in an automated way? I am already using to get my procedure name. 

It depends if you need something generic or if it can work with a "small" number of columns. In your specific example, this query: 

step 4, Compare sort keys (simple binary comparison of each value one by one): The fourth value is enough to sort them all, so the final order becomes: 

In that way you do not "pollute" your table with input by users. It has the same "likes" as your own (you will just need an extra or similar) but not the dislikes. As the dislikes, it is another extra table, that has to be taken into account in your requests. You could even "merge" the 2 tables with just: if you specify in a constraint that either or are NOT NULL (but not both at the same time). You will then obviously need to accept having a lot of NULL values. But again, I recommend you to think about how you will query this data afterwards to find the model that most suits your needs. Also how often the "other" case could happen? It is more like 1%? 10%? 90%? The design and optimisations would depend on this too... 

I want to know if it is possible to create a group "internal" to SSRS without having to add the group to the ReportServer box itself. I do not have permissions to the box to create groups and/or add users to existing groups. I know that this is a way (the only way?) to have groups that can be assigned permissions in the SSRS interface. I do have all the permissions I need within the SSRS interface. I have the System Administrator System Role on the ReportServer instance, as well as full sysadmin rights to the ReportServer database. I am trying to simplify our security structure and it would be peachy if I could create a group, assign users to that group, and then permissions to the group. But all I seem to be able to do is assign permissions to the groups set up on the box. Any ideas? EDIT: Active Directory groups are not an option for me. :( 

Try using a bind variable. You can declare it as a number and that should prevent a damaging SQL injection. ADDITION: bind variables also increase performance and scalability because the query plan is compiled and stored for re-use. Just something else to add to your argument. :) 

We're working on migrating our database (~400 tables) from SQL 2008 R2 to SQL Azure. We're currently in the proof-of-concept stage, figuring out the process we'll be following. We have a process that we think is solid but I'd like to perform some validation diffs on both schema and data to confirm that we have a successful migration. I've never done this before with SQL Azure and am looking for a good way to do this. How can I perform this verification effort on both the schema and data? Ultimately, this is a one-time migration (we'll do it a few times but the real migration will only be done once). 

The script works fine as-is but the moment you uncomment out that second , all hell breaks loose. I understand this is invalid syntax because I'm mixing boolean logic with bitwise operators. But what is the right way to get this logic in there? Without going crazy with statements, is there some way to do this? 

It only tuned ~9k out of ~530k queries It recommended I drop a ton of indexes (in fact, most of them) It recommended I create 0 indexes