Small Business Server 2011 is based on Windows Server 2008 R2. According to the certification matrix at Oracle support, the following versions of Oracle are certified for Windows Server 2008 R2 x64: 

relies on your session's NLS settings to convert between datatypes. You should use with an explicit number format model, something like: 

Your needs to be or at the very least the IP address of the interface you will be connecting to from your remote machine. If you set it to only loopback connections from the server can be made. 

The column should be constrained to only allow the values and or, if you prefer, and -- the latter having the advantage of a simpler possibility to do the of the likes and dislikes to get an overall vote. 

Configure your database in archive log mode, without it you will lose data. Not a question of if, but when. Archived log files go to the flash recovery area. Configure rman retention policy according to how far back in time you wish to be able to go. You need this to recover from user errors that are detected some time after the fact. Size your flash recovery area to be able to keep all of this data. rsync your flash recovery area to another machine, preferable off site. 

Primary and unique keys in all(?) RDBMSes use indexes in order to quickly be able to determine whether a newly inserted value is indeed unique. The side effect of this is that queries via primary and unique keys are usually "fast". Now if you haven't defined primary or unique keys on your tables, 

Then you should never have the problem again that you describe in your question. And you never need to login as a superuser -- unless you need to create new schemas or users. 

Why not use a Common Table Expression aka clause? It's designed for exactly this purpose (among others). 

No, it can't be queried. These values are stored in sqlnet.ora on either or both of the client and the server. In either case, none of the contents of the network configuration files sqlnet.ora, tnsnames.ora, listener.ora etc (eg protocol.ora, snmp.ora, cman.ora) can be queried. If you really need to read the contents of these files and can't solve this problem in some other way (programatically via some OS level program or even administratively), you could read the file via external tables or via a Java stored procedure. I would prefer to solve this outside of the database. 

While I'm not sure if it's the easiest way, you can use an "after servererror on database" trigger to log all errors to a table. See $URL$ See also $URL$ The documentation for system event triggers is here: $URL$ 

Read the Oracle Concepts manual from (virtual) cover to (virtual) cover. The latest version is at $URL$ If you know and understand all of this you'll be in the top 1% of all DBAs. 

You don't have to write twice if you don't need to retrieve it; if you're only interested in the s having a then the following is perfectly valid: 

If you remove all newlines and whitespace from the output before grepping, it should work for you. Use 

large offset: PG will need to scan through the provided offset number of rows and will prefer an index to do this above a table scan. how to index the data: I think a partial index might help you here. Can you try create index xxx_idx on latest_channel_snapshots(view_count DESC NULLS LAST) where network_id is null; See $URL$ for more information on partial indexes. In particular, example 11-2 on unbilled orders seems to apply to your case. 

Assuming you store all revisions of the document in a table, an approach would be to not store the revision number but calculate it based on the number of revisions stored in the table. It is, essentially, a derived value, not something that you need to store. A window function can be used to calculate the revision number, something like 

The other thing to remember is that you can generate your queries by querying the metadata for a list of schema and table names. 

In PostgreSQL 8.4+ or any other DBMS that supports windowing functions, the following approach will also work, and will probably be faster on large tables because the database can answer the query with a single scan of the main table (rather than aggregating on the first scan and then querying the main table again for matching rows): 

Then just try them in order and use the first folder that exists as a physical directory. If in your current model is an attribute, then make the table a child table of that table; ie, if your current model is: 

should be lightning fast -- it's always basically two index lookups and nothing more. Having written all this, it should be possible to use a window function to do just one index scan for the two candidate values on either side of the "measurement" value, but the above approach doesn't require window functions and should work on any DBMS which can "walk" an index instead of performing an . 

In general, for many DBMSes to solve the function using an index, a descending order index on is needed: this index has the property that the MAX value of x is the first value in the index. PostgreSQL supports a "backwards index scan" which, as its name suggests, scans the index in reverse order, so that the MAX value is the first value it finds in an index in normal (ascending) order. Then to find the MAX(x) for a key value, eg will require a composite index: 

Yes, you will need Oracle Advanced Security to be installed. See $URL$ However a license for Oracle Advanced Security is no longer needed with 11g Release 2, but you will apparently need a license for "Oracle Identity Management Directory Services Plus". See PDF page 10 of $URL$ 

The subqueries in your query are al counting all rows of the table. They should probably only be counting for the "shyft" from the main table; something like: 

You'll have to check the execution plans to see if the optimizer is smart enough to push the where clause into the subquery for each table. 

Oracle is compiling all objects, so as far as Oracle is concerned all code in all objects is new code. As a consequence, Oracle has no way of knowing that your function still returns the same values that were used when the index was created. Instead you should only recompile invalid objects, 

Do you know all possible metadata "keys" in advance? If so, then I would go down a different route entirely and model this is a single table -- or one per subsystem/area -- with each key as a column. This is the approach I use when developing database-centric applications. 

I think the answer to your question is "Yes". The Oracle optimizer understands reference partitioning. See the Oracle documentation at $URL$ Also see $URL$ 

You need to choose "Custom Database" in the "Select Template" screen of the Database Creation Assistant, and not one of the templates that includes datafiles as in those the block size is fixed and cannot be changed. 

One approach would be to use optimistic locking. Before displaying the update form, you retrieve all the original values from the database. Or, at a minimum, a column that is guaranteed to change on update, such as or similar. When updating you check that all values are still the same in your clause. If the update fails to update any rows you know that the values have been changed in between and inform the user that this has happened. 

I think the way you'll have to do it is a Jokke suggest: instead of defining the DT datatype as , rather define it as where I think that 29 characters should be enough but you could define it wider. Then when you extract data from the XML you'll have to convert it to . However, you could also define the column as an "xsd:dateTime". See $URL$ and $URL$ 

I think it's easier to keep track of the dependencies by using a purely pull-approach. In a way it's similar to the whole concept of queries on tables or queries on views on tables. It's all "pull", not "push". Most DBMSes have better control of transactions etc in the current database vs the remote database when running code in stored procedures. For example, in Oracle, you can't truncate a table on the other side of database-link. So you'd require a helper-package on the other side to perform these actions. Much better to have the stored procedure execute the truncate locally and pull the data to populate the table. I think "pull" is a better fit to the requirements of a reporting data-warehouse / reporting system. These tend to run at scheduled times. If you have an architecture that does incremental updates of the warehouse or reports, then "push" is a better fit. I don't like to mix the flows. Talk to your admins / ops team. If I was the DBA, I would prefer to admin systems that "pull" there data from others. I think it's easier to manage the cascading effects of eg down time in this way. 

The idea that an application should be written with just standard SQL comes from application developers who think that changing a DBMS is a worthy design goal. I -- and many other DBAs with me -- don't agree. While it holds true for basic features, your application is going to perform best if it uses the features that your particular DBMS provides. This almost always means using DBMS-specific versions of SQL and DBMS-specific features. Your employer/client has paid good money for a license for their DBMS, and often gone through a selection process to do so. Not using the DBMS's features is a waste of money. I like to say it's like buying a Ferrari and always sticking to the speed limit... On the other hand, if you're just learning SQL, and T-SQL just happens to be the dialect you have at your disposal, then, yes, try to learn standard SQL as your knowledge will be more broadly applicable. But don't discount learning your particular DBMS's features if you are in a commercial (or non-academic) environment. 

The bad news for you is that substituting this type of index in the database will mean you also need to change the queries on this column/table. This may or may not also have implications for the user interface -- it could be that end users are currently able to enter search terms that are understood by the Oracle full text index searching implementation. 

You'll need to execute your SQL statement. Use for this. And because you're selecting a value you'll need to it, something like this (replacing with your own where condition: 

I hope I'm not missing something obvious but the way I would query this so that it would scale for a very large lookup table is by observing the following: It's possible to get a competent DBMS (I know PostgreSQL can do this) to use an index to 

Another approach that seems not to have been mentioned yet is to have Hazards and Tasks use the same ID space. If a Hazard has a Task, it will have the same ID. If a Task is for a Hazard it will have the same ID. You would use a sequence rather than identity columns to populate these IDs. Queries on this type of data model would use (full) outer joins to retrieve their results. This approach is very similar to @AndriyM's answer except his answer allows for the IDs being different, and a table to store this relationship. I'm not sure you want to use this approach for a two-table scenario, but it works well when the number of tables involved increases. 

Why not just swap the IDs instead of the value(s)? Or do the other columns have to remain as they were? Anyway, in case someone else reads this question and is looking for this, it's just: 

I'm going to attempt to answer this question for Oracle. Historically Oracle has never supported read-only databases. It's possible to make a tablespace read-only: $URL$ Individual tables can be made read-only: $URL$ Or it's possible to make a select-only role and assign that to the user who logs in rather than the normal readwrite-role. 

I would just . Even if I had to only delete if there were say exactly 2 records, then I would add this condition to the where clause, like 

If you need an Oracle proxy server, then you should investigate Oracle Connection Manager, which is exactly that. See $URL$ for the documentation on 11gR1's version of it. 

Oracle does not support this concept of partial indexes. This syntax is valid for PostgreSQL versions from 8.0 and higher but not for Oracle. Oracle does however support indexes on partitions. If you have a license for the partitioning option, you can create a partition for the values of isdefault = 'Y' and index only that partition. I'm not sure which versions of Oracle support this; it could possibly be only 12c and higher. 

No, it's not true. Many DBMSes today (eg Microsoft SQL Server, PostgreSQL, Oracle, MySQL) use file storage by default, although some (eg Oracle) can also use block storage. 

What I would do is maintain a SQL script for the indexes that you've added to the "virgin" database. Store this script in a version control system, as you'll be adding indexes to fix performance problems. I would employ a naming strategy to make it easy to identify which indexes are "yours" and not part of the database as delivered. I'm not sure what features MySQL has, but in Oracle for example you can store these indexes in a different tablespace making it very easy to identify them as "added". When your vendor supplies an upgrade, first remove your added indexes, then apply your vendor's upgrade and afterwards execute your script to add your indexes. Of course, you should also have a QA environment where you can test all of this before applying it to your production system. You'll need to do this to discover if any indexes will conflict with indexes that the vendor may create in a future version. 

My answer doesn't answer your question directly, but addresses your method of operation. You shouldn't be logging in as , just like you shouldn't be logging in as on your servers. On your servers you should be logging in as real users and be using for the odd occasion that you need superuser privileges. And preferably rather than just doing everything directly using . In PostgreSQL we can implement more fine-grained policies. Create a role structure something like: 

Be careful that this type of query only works properly if you're looking for different records in the sub-query; if it is at all possible to "hit" the same record, you need to exclude that in the subquery's where clause, by doing something like: 

According to $URL$ and $URL$ the only way to do this is to use the parameter to to create a script. Then replace with and then execute the script. 

You don't have a relational table but you have junk (OK, this is a contentious opinion, but a relational model needs keys on all tables). queries on this table (in the absence of any other indexes) will become slower as more data is inserted into the table. 

Yes! You can make a new table which has foreign keys to both and . But be careful either in your model or in your code that you don't associate patient-records from a different patient with the patient associated with the treatment... using natural keys is one way to solve this: 

If you just want to forcefully close the file, you could use a tool which lets you close open files. Googling I found OpenFilesView. See $URL$ 

If this updates 0 rows, then there has been intermediate update and you need to inform the user of this. 

A PostgreSQL server connection dropping after 10-15 minutes is almost certainly being caused by a state-tracking firewall (possibly using Network Address Translation (NAT)) between the client and the server. Many such firewalls have default timeouts of 15 minutes (900 seconds). The three server-side parameters, , , are designed to help in these situations. See the documentation located here: $URL$ There are also client-side parameters for this: , , , , which you can set on connection. See the documentation located here: $URL$ 

How many different combinations of columns to update do you have? How many rows of the entire table will be updated? Are indexes in place for fast access to rows to update? Depending on the answers to these questions you may be able to execute multiple update statements, one for each column that you wish to update and place the condition on that column's value in the where clause of the update so that zero rows are updated if that column has the wrong value. Try and think set-based, don't assume that update needs to update a single row found by the primary key. 

Historically, DBMSes had trouble with . But most DBMSes today can handle it without problem. Not all DBMSes support this syntax with multiple columns (SQL Server doesn't for example), but many do (PostgreSQL, Oracle, MySQL). This should work for those (but untested):