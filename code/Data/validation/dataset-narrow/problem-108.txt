Small packets aimed at hammering the application of the server. Large packets aimed at exhausting available bandwidth. 

This is basic subnetting/binary logic. There's 2 /25s in a /24, 2 /26s in a /25 (ergo 4 /26s in a /24) and so forth. The closest match to the requirements is to use which will leave you with one spare subnet. You cannot by definition have an odd number of subnets exist from any given subnet size. Either your teacher actually intended this and the 7 was a red herring, or somebody failed hard at math. 

There is of course also the possibility of using large packets if the attacker knows that a well-crafted packet can cause additional processing time on the server in excess of the additional bandwidth cost. Simply dropping UDP packets below a certain size is far too blunt of an instrument to really be commendable, not least because this may well just result in your attacker changing strategy. For example, if the focus was to hammer your servers, the attacker could switch to an DNS amplification attack that aims to exhaust your available bandwidth. 

Generally it comes down to interoperability - if you can guarantee that using a /31 is fine for both hosts on the link, and you are 100% sure that if one of the hosts fail, you will not be replacing it with something that can't handle a /31, you should absolutely just use a /31. In reality, the prime candidate for issues is going to be a routing protocol like OSPF, but as usual, this is implementation dependent - the vast majority of routers out there will behave properly when using a /31 - the adage "test your equipment" holds true. 

At layer 2, all load balancing is, at best, done by an XOR or hash of the source and destination MAC, and if you're lucky, it may even read into layer 3 and hash that data too. At layer 3, however, where we're basically talking about multiple gateways (so, effectively, two physical links with a unique next-hop across each) you can max out the bandwidth across the links IF you're prepared to do per-packet balancing. Before I go on, per-packet balancing is generally a bad thing due to the fact that it can result in out-of-order packet delivery, this can be especially terrible with TCP connections, but that of course comes down to the implementation and most modern stacks can tolerate this relatively well. In order to do per-packet balancing, obviously one requirement is that the source and destination IP addresses are not at all on-link to the devices that have the multiple paths since they need to be routed in order for balancing to be possible. Redundancy can be achieved via a routing protocol such as BGP, OSPF, ISIS, RIP, or alternatively, BFD or simple link-state detection. Finally, there is of course a transport layer solution - protocols like SCTP support connecting to multiple endpoints, and TCP already has drafts in the making that will add options to do similar things. Or... you can just make your application open multiple sockets. 

Using a /127 isn't terrible, but letting it go into your backbone as a /127 is. The reason for this is that, essentially, most modern router TCAMs can typically only handle up to 64 bits of address width at a time - this means that if you're in a situation where all routes are /64 or shorter, lookups can occur in a single cycle. Anything longer and it has to perform another lookup operation. Even on a TCAM that only has 32 or 48 bit width, going beyond /64 is obviously still significant. So, my personal recommendation is to allocate a /64 for every P2P link even if you only use a /127 on the wire - that way, when you bring up your routing protocol, you can then aggregate the /127 to a /64. My personal favourite, however is to allocate a reasonable chunk of your IPv6 space purely for facilitating P2P links (in my case, I reserved a /48) - this /48 is then blocked on all network edge interfaces at ingress as a destination. In this way, you're free to just go ahead and use a /64 on your P2P links and still have traceroutes, ICMP errors et. al work, but you are not vulnerable to NDP attacks from outside. Obviously not everyone is going to care about this and if the additional cost of using longer prefixes is acceptable to you (or you have super-duper 128 bit TCAMs) then you can of course ignore everything above. How scalable do you want your network to be? 

Larger images will consume more RAM - it does not impact the CPU once loaded although it will also incur a longer boot time since it of course has to be decompressed into memory and that takes time. On more modern platforms with speedy Intel Xeons, the decompression difference isn't very significant. On older MIPS or PowerPC processors, it is. 

Backpressure refers to what is essentially concentration of traffic. E.g. I can have 10 x 1Gbit links internally that are all feeding into a 1Gbit link that provides me with Internet transit. at saturation point, the router can store packets in its buffer and/or drop them - with no particular configuration, a router will generally fill its buffers and then tail drop, this gives rise to two problems: buffer-bloat and tcp global synchronisation. The first refers to a case where the buffer is constantly filled due to constantly saturated link utilisation. The second refers to the issue of hosts re-transmitting dropped packets all at the same time, thereby causing a burst of traffic and thus, more drops, more retransmits, ad nauseum. RED was conceived a long time ago as a means to deal with this issue; namely by randomly selecting packets to drop during times of congestion. This however required careful tuning according to the properties and expected behaviour of the link. Fortunately things have moved on and AQM (Active Queue Management) is now the cutting-edge of the industry. A top-notch example of AQM is CoDeL - this is an algorithm that focuses purely upon the sojorn of a packet through the system and aims to ensure packets are passed within a specific time rather than caring about whether or not a certain amount of bandwidth/buffer is being utilised. 

Determining a loop really depends on the brand of switch that you have. For example, on an Extreme switch, I can run elrp-client on a VLAN and the switch will basically send out a broadcast frame on all ports for that VLAN and see if it returns by any of them, if so, it tells me which port(s) the frame was received back on, thereby revealing the loop candidates. On a Cisco, you can enable storm control, which is a bit more of a blunt instrument since it will basically block the port for a period of time until the status clears (or you clear the errdisable state) - generally speaking however, this sort of thing is only relevant when you're using Cisco switches in a mixed topology of devices that do not do spanning tree nor forward BPDUs. 

Depending on how the service is delivered by the SP will dictate how you can separate the services on your end. Typical methods are either a port per service or a VLAN tag per service. If the SP is tagging the traffic you can just set up your switch to trunk to the SP and then separate the traffic into two access ports (one to FW and one to LAN). If it's a port per service then just create two VLANs with the services in different VLANs for isolation. 

Working this out in my head I believe it should react as follows: When blue link dies the CE will stop sending/receiving BPDUs from the blue interface. Default RSTP hello timer is 2 seconds. It waits for three missed hellos before calling that link "dead". Once three hellos (6 seconds) have been missed it will then re-establish the STP tree and age the MAC addresses. This is basically option 1 you've stated above except the way I've read the comments and your original post it sounds like you want the PE to participate in STP. I'm suggesting allowing the customer to build its own tree between all CEs. Your network should fail over smoothly, and the client network would follow suit a few seconds later. This feels too simple to be the answer...but that's what I can see based on your write up. 

Oh the 6500. I run a small service provider network and run the 6500 as a PE router. Worst decision of my life. (That was an embellished statement, but you get my point.) I run full BGP routes in a VRF and have experienced a lot of problem surrounding this. You're example is not very surprising. As Daniel said in his post there is an LFIB entry for each VRF prefix as well as a VPNv4 entry. This can be changed by adding the command as was stated; however, this does not get you out of the woods. If you change to per VRF prefixes it does remove the LFIB entry (yay!) but adds an entry for every single prefix into the Adjacency table (wait, what?!). Since the 6500 forwarding hardware is shared between L2 and L3 forwarding this doesn't change your hardware memory usage at all. If anything it makes the problem harder to find. If you look at your usage once you've changed to per VRF usage (using ) it looks as though you've fixed the issue. However if you use the command it reveals the problem has just moved to a different location. Below are the outputs from one of my 6500's resource-level and adjacency usage. Outlining what I'm talking about. Resource-Level 

The best way to do this would be through redistribution. If the non-OSPF devices do support RIP or EIGRP you could create a neighborship with the OSPF device and non-OSPF device and redistribute the routes into OSPF - and the rest of your network. This is relatively safe if the non-OSPF device is single-homed. If dual-homed then you'll need to beware of the possibility of routing loops due to lost metric when redistributing. If running a different IGP on the non-OSPF device is not an option then I'm afraid you'll have to use static routes - be it manually put in or automated with some sort of script. 

Edit: From a wireless perspective if you have all 200 devices trying to access resources at the same time you might find yourself in a bit of a jam if you've only got a handful of APs handling the traffic. I would recommend you keep a close eye on your usage when you finish your deployment and see if you need to add more density to your wireless infrastructure. Now only having 18 employees and mobile devices it would be hard to have them all push enough traffic to matter, but as you grow I would keep an eye on it so you don't run into any problems. Only one client can talk at a time on a wireless network (per AP/Frequency). So ensuring you've got enough available bandwidth is of the utmost importance. 

Security: Security isn't itself achieved by creating a VLAN; however, how you connect that VLAN to other subnets could allow you to filter/block access to that subnet. For instance if you have an office building that has 50 computers and 5 servers you could create a VLAN for the server and a VLAN for the computers. For computers to communicate with the servers you could use a firewall to route and filter that traffic. This would then allow you to apply IPS/IDS,ACLs,Etc. to the connection between the servers and computers. Link Utilization: (Edit)I can't believe I left this out the first time. Brain fart I guess. Link utilization is another big reason to use VLANs. Spanning tree by function builds a single path through your layer 2 network to prevent loops (Oh, my!). If you have multiple redundant links to your aggregating devices then some of these links will go unused. To get around this you can build multiple STP topology with different VLANs. This is accomplished with Cisco Proprietary PVST, RPVST, or standards based MST. This allows you to have multiple STP typologies you can play with to utilize your previously unused links. In example if I had 50 desktops I could place 25 of them in VLAN 10, and 25 of them in VLAN 20. I could then have VLAN 10 take the "left" side of the network and the remaining 25 in VLAN 20 would take the "right" side of the network. Service Separation: This one is pretty straight forward. If you have IP security cameras, IP Phones, and Desktops all connecting into the same switch it might be easier to separate these services out into their own subnet. This would also allow you to apply QOS markings to these services based on VLAN instead of some higher layer service (Ex: NBAR). You can also apply ACLs on the device performing L3 routing to prevent communication between VLANs that might not be desired. For instance I can prevent the desktops from accessing the phones/security cameras directly. Service Isolation: If you have a pair of TOR switches in a single rack that has a few VMWare hosts and a SAN you could create a iSCSI VLAN that remains unrouted. This would allow you to have an entirely isolated iSCSI network so that no other device could attempt to access the SAN or disrupt communication between the hosts and the SAN. This is simply one example of service isolation. Subnet Size: As stated before if a single site becomes too large you can break that site down into different VLANs which will reduce the number of hosts that see need to process each broadcast. There are certainly more ways VLANs are useful (I can think of several that I use specifically as an Internet Service Provider), but I feel these are the most common and should give you a good idea on how/why we use them. There are also Private VLANs that have specific use cases and are worth mentioning here.