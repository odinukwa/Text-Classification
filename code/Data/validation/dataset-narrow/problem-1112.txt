If you measure approximation as a ratio between the number of constraints satisfied by your algorithm divided by the total number of constraints, then trivially all constraint satisfaction problems are unconditionally approximation-resistant. By definition, a problem is approximation resistant if the (worst-case) approximation ratio of a random solution is (up to an additive $o(1)$ term) best possible among the (worst-case) approximation ratios achieved by all polynomial time algorithms. With your definition of approximation ratio, you get approximation resistance of Max Cut (and all constraint satisfaction problems) just because you can always construct instances for which the ratio given (on average) by a random assignment is more or less (up to a $o(1)$ term) the same as the ratio given by an optimal assignment. For example, in the Max Cut problem, a clique on $n$ vertices is a graph that has ${n \choose 2} = n\cdot (n-1)/2$ edges, and in which an optimal cut cuts $n^2/4$ edges. This means that every algorithm, and in particular every polynomial time algorithm, has a worst-case approximation ratio (according to your definition) that is at most $1/2 + O(1/n)$ on $n$-vertex graphs. The random assignment has ratio $1/2$ on all instances, and so no algorithm can do better than the random algorithm, and the problem is approximation-resistant. 

It can be seen that this property is preserved by some natural operations: parallel composition, adjonction of a minimal or maximal element. This immediately implies the Knuth hook formula for forests, although it has a number of different proofs (including some of algorithmic nature). I'd like to know if there are examples of other operations preserving/breaking the existence of a hook length formula, in particular is it possible to define a gluing operation extending the above adjonction operation? 

A partial semigroup (or PSG) consists of a set $X$ and of a partial composition law $*$ defined over $X$, that is to say: (1) $x*y$ is not always defined, (2) if $(x*y)*z$ is defined, so is $x*(y*z)$, and the two values are equal, (3) if $x*(y*z)$ is defined, so is $(x*y)*z$, and the two values are equal. Let $\mathbb{R}$ be a ring. We may then define a convolution product over $\mathbb{R}^X$ : given two functions $f,g$, we define a function $f*g$ such that 

As far as I know, the parameterized complexity of the counting problem is still open. It is known that ILP solving is fixed-parameter tractable in the number of variables ('Integer programming with a fixed number of variables', H.W.Lenstra Jr), although ILP solution counting is expected to be $\# W[1]$-hard. The question was first asked by N. Betzler if my memory is correct. 

As currently asked, the answer to the question is yes, assuming that there are problems in NE (non-deterministic time $2^{O(n)}$) that require doubly-exponential time to be solved. Under the standard assumption that, $NE\neq E$, there is a turing machine $M$ such that computation of $f(M,n)$ given $n$ cannot be done in time polynomial in $n$. In general, suppose that there is a language $L$ that is in NE, and that cannot be solved in deterministic time $t(2^n)$ (plausibly, there could be such a language with $t(n) = 2^{n^\epsilon}$). Let $L'$ be the language that contains only the strings of the form $1^N$ such that the $N$-th string in lexicographic order is in $L$. Thus $L'$ is in NP and such that if $1^N \in L'$ then it has a certificate of length $\leq N$, but such that $L'$ cannot be solved in deterministic time less than $t(N)$. Define the polynomial-time turing machine $M$ so that, on input $x$ of length $N$, it decides whether $x$ is a witness for $1^N$. If it is a witness, then $M$ accepts in time, say $N^2$, otherwise it keeps going some more, and rejects after $N^3$ steps. So we see that $f(M,N)= N^2$ if $1^N \in L'$, and $f(M,N) = N^3$ otherwise, and so computing $f(M,N)$ cannot be done in time less than $t(N)$ 

I update my reply in light of Aravind's comment. The claim in my previous answer was clearly incorrect but, in my defence, I never had a good intuition about graphs. By adapting Aravind's argument you can show an upper bound of $O(\sqrt{n})$, as every $n$-vertex perfect graph has an independent set or a clique of size $\sqrt{n}$. It should be possible to show the tightness of the bound by reasoning on the tree-representation of the cograph. Consider for instance a complete binary tree $T$ with $N = 2^n$ vertices, and label its nodes as series/parallel in an alternating fashion. It represents a cograph $G$, and any $P_3$-free subgraph $H$ of $G$ must span a subtree of $T$ that does not contain three alternations along a root-leaf path; it should imply that this graph may have at most $O(\sqrt{N})$ vertices, but I leave the details to you. A related question would be to look for a partition in bicluster graphs, as they are exactly the bipartite $P_4$-free graphs. Observe that this problem can be solved in polynomial time by computing the chromatic number of the graph: an arbitrary pairing between color classes then gives the optimal partition. 

(Note: I am aware that a mapping with distortion $(1+\epsilon)$ can be found with high probability in time polynomial in $n$ and $1/\epsilon$ by projecting on $O(\epsilon^{-2} \cdot \log n)$ random lines, but I am not sure if the number of dimensions can be constructively reduced to $n\choose 2$ or even $O(n^2)$ when $1/\epsilon$ is much larger than $n$, and I don't know if there is a polynomial time method to handle the case in which $1/\epsilon$ is exponential in $n$.) 

I would like to understand how the Arora-Kale SDP solver approximates the Goemans-Williamson relaxation in nearly linear time, how the Plotkin-Shmoys-Tardos solver approximates fractional "packing" and "covering" problems in nearly linear time, and how the algorithms are instantiations of the "learning from experts" abstract framework. Kale's thesis has an excellent presentation, but I find it very difficult to directly jump into the abstract framework, and I would prefer to start from an example of a simple problem for which it's absolutely obvious what to do, and then move to more general problems, progressively adding "features" to the algorithm and to its analysis. For example: 

An OH relation is then a conjunction of OH clauses. The authors state in Theorem 5 of the paper that the satisfiability of an OH relation can be decided in polynomial time, but the algorithm doesn't seem 'self-contained' as it relies on a generic algorithm for propositional Horn theories. In relation to a problem I'm currently studying, such a self-contained algorithm seems desirable but it's unclear to me whether it is possible. To tell the truth, I have an algorithm for a subclass called Restricted Ordered Horn (containing the relations expressible using clauses without $\neq$ in the rhs) but unfortunately it can't be adapted to the full $OH$ class. 

Are there linear-time recognition algorithms for these classes? By the previous remark, an algorithm for class 2 would immediately yield an algorithm for class 1, although I suspect a more direct algorithm to exist in that case. What is the complexity of the subpattern problem for two skew-merged or two vexillary permutations? The subpattern problem is known to be polynomial for separable and 2-increasing permutations, and these classes seem the next to study. 

This is a family of expander graphs: let $p$ be a prime, let the vertex set be $\{ 0,\ldots,p-1\}$, connect vertex $x$ to $x+ 1 \bmod p$, to $x-1 \bmod p$ and to $x^{-1} \bmod p$. Each such graph is a cycle, plus a matching over $p-3$ vertices. (The multiplicative inverse is undefined for $0$, and is the element itself for $p-1$ and $1$.) This is a picture of what it looks like for $p=59$ 

If $X$ is a linear subspace of ${\mathbb R}^n$, $X$ is high-dimensional, and for every $x\in X$ we have $(1-\epsilon) \sqrt n ||x||_2 \leq ||x||_1 \leq \sqrt n ||x||_2$ for some small $\epsilon >0$, then we say that $X$ is an almost-Euclidean section of $\ell_1^n$, and (the matrix whose image is) X is useful in compressed sensing. A random subspace works excellently, and there is a huge research program devoted to the explicit construction of such spaces. Is it known what is the complexity of approximating the "Euclidan-sectionness" of $X$? That is, given a subspace $X$, say presented via a basis, consider the problem of finding the unit (in $\ell_2$ norm) vector in $X$ of smallest $\ell_1$ norm. 

I'm interested in the possibility of fast algorithms for the following two problems on permutations. 1) Given a permutation $\pi$ and an integer $k$, compute a pair $(\mathcal{C},\mathcal{A})$ where $\mathcal{C}$ is a maximum $k$-chain and $\mathcal{A}$ is a corresponding $l$-antichain certifying the optimality of $\mathcal{C}$. If you only care about $\mathcal{C}$, this can be done in $O(k n \log n)$ by an involved algorithm ('Maximum k-Chains in Planar Point Sets: Combinatorial Structure and Algorithms' by S. Felsner and L. Wernisch). It seems possible to obtain a simpler algorithm that would also provide a certificate. 2) Given a permutation $\pi$, compute the pair of Young tableaux $(P,Q)$ associated to $\pi$ by the Robinson-Schensted bijection. Is it possible to do it in subquadratic time? Note: if we only care about the first $k$ rows of each tableau this can be done in $O(k n \log n)$ time in the RAM model; by Greene theorem, the cumulated size of the first $k$ rows gives the cost of an optimum solution for 1), without providing an explicit solution. Thus question 1) seems to require a different approach. 

(Computing the largest eigenvalue of the Laplacian is equivalent to the problem of solving a weaker version of the Goemans-Williamson SDP relaxation of Max Cut, in which, instead of requiring each vector to be of length one, you want the sum of the squares of the norms to be |V|.) 

Suppose that coauthors from two or more different institutions are writing a paper in latex, and would like to do better than repeatedly emailing drafts back and forth. They realize they can open for free a dropbox account, share the password, and synch the version of the paper on their computer with the one on dropbox. If two people are simultaneously editing the same section, however, they'll overwrite each other changes. They have also heard that version-control systems like SVN and Git have tools to merge concurrent changes, which work reasonably well. The documentation of these products, however, is rather hard to read, and it is more focused on how to undo changes and how to manage different "branches" rather than on the basic needs of coauthors writing a paper. Is there a simple step-by-step exposition on how to use a version-control system in this setup: 

I'm not sure this question is appropriate for this site, but it might have some connections with computational algebra. Consider a fixed "category" $\sf{Cat}$ (in the sense of category theory, but the precise notion is not important), e.g. groups or fields. Given an object $\mathbb{S}$ of $\sf{Cat}$, we say that a "composition series" for $\mathbb{S}$ is a chain of objects $C = (\mathbb{S}_{\alpha})_{\alpha}$ indexed by some ordinal $\lambda$, such that $\mathbb{S}_0$ is an initial object and for every $\alpha < \beta$, $\mathbb{S}_{\alpha}$ is a subobject of $\mathbb{S}_{\beta}$. We may then define the length of $C$ as the ordinal $\lambda$. We say that the structure $\mathbb{S}$ is isogenic if all maximal composition series have the same length. For instance, this is true for finite groups ordered by the normal subgroup relation (according to the Jordan-Hölder theorem). My question is: what are the structures known to have this property? 

Let $w_r$ be the random word assigned to the root at the end of this process. Can we devise a polynomial-time strategy to maximize $Var(w_r) = \sum_{i \in [n]} Var(w_r[i])$? This could be done either 'globally' (by constructing the tree at once) or 'online' (by selecting a 'locally optimal' matching for each generation). The intuition is that a node $u$ represents a 'genetic group' which profiles is described by the distribution of $w_u$, and that we would try to mix groups at each generation in order to maximize the genetic diversity. This is probably NOT desirable in practice due to the possible effects of dominant/recessive mutations.