Required Metadata to Extract Sprites As you correctly say, you need some metadata that describes the structure of the sprite sheet, in order to extract individual sprites from the sheet. I tend to use the following list of metadata myself (you can use this as an example, and add or remove metadata attributes as needed): 

On Homogeneous Coordinates I'll quickly go over the w coordinate you are mentioning, so you understand what is happening. The w-coordinate is used to represent a 3D point in a 4D homogeneous representation. This representation is used to make matrix math more simple. Specifically, it lets you use a 4x4 matrix to perform translations, something which you can not do with a 3x3 matrix and 3D point representations. As you are using a 4D representation for a 3D point, many combinations of 4 coordinates will describe the same 3D point. The actual position of your point can be obtained by dividing all components of the 4D representation by w. So for example, point [x,y,z,w] is located at the following 3D coordinates: [x/w, y/w, z/w]. This explains why you observed clipping at z > w. As in fact, the z-coordinate becomes z/w. Knowing that clipping occurs at z-coordinate > 1, you'll find that this is at: 

1. Model Transformation (Model coordinates to World coordinates) This tranformation is applied to the model itself, disregarding anything else in the scene. It is used to scale, rotate (around its own center axis) and finally translate the object to its desired location. The resulting transformation will move the model to its desired location in the world. Therefore the coordinates are called world coordinates. 

Solution 3: Z-buffering method to achieve your effect There is also a trick you could use to achieve the effect you describe. Z-buffering is a technique used in 3D rendering, that makes sure only the front most fragments are rendered/visible for each pixel. This works by writing the depth (= distance to the camera) of a fragment as it is drawn. Whenever a new fragment is drawn, the renderer checks whether a closer fragment was rendered before, by checking the depth buffer. You can achieve the described effect by: 

Solution 2: Calculate collision positions and normals The behavior you are trying to achieve seems to be "wall-sliding". This means, if you walk into a wall, you will slide along it depending on your movement direction. The way this is achieved in commercial games, is by calculating more information on the collision. In your case, you just "calculate" the answer to the question "Am I colliding or not?". Information that is required to solve your problem in a more elegant manner is "At what position did I collide?" and "What is the normal of the surface I collided with?". This information can be obtained for example by raycasting your object's bounding box against collision geometry (e.g. by line-raycasting the configuration space obstacle (CSO)). Using this information, you can resolve you collisions in the following way: 

The ground + sky: the backmost layer, as the player is always in front of these The backmost church: an inbetween layer, so that the player can walk both behind and in front of it The frontmost church: a more nearby layer, so that the player can walk inbetween both churches, as well as in front of the front-most one 

From this, I assume that at least the vertex buffer bindings and the vertex attribute specifications are stored in the VAO. I'm unsure however how this usage pattern extends to situations where (multiple) textures and (multiple) shader programs come into play. Is the active shader program stored in the VAO? And are the texture bindings (with their sampling/ wrapping settings) stored in the VAO as well? Ditto for uniforms? 

Solution 2: Pre-render to an intermediate texture This is probably the cleanest method, that will work for any type of background. Rather than rendering to the screen instantly, render to an intermediate texture first. 

As in the first solution, you can now draw your objects on top of this setup. Fragments of objects that should appear behind background depth planes will be occluded correctly thanks to automated depth testing with the z-buffer! 

How to find the bounds of your ortho view From the above, you can see that deciding what volume is viewed, happens in the projection transformation. Depending on how you specified your camera, you can retrieve the bounds by looking at the Camera coordinates. Assuming your camera looks along the positive x-axis, simply use the minimum and maximum y and z values of the bounding box in camera coordinates. To construct your view matrix, note that it is a concatenation of rotation and translation transforms. You are trying to calculate the transform that would place your camera in the origin, looking along the positive x-axis (can be another axis if you desire so). 

The color pass would be a simple color image. It should obviously be rendered with a camera identical to the one used in your game. The depth map is a grayscale image that represents the distance of pixels to the camera. White pixels are very close to the camera's near plane, whereas black pixels are very far away from the camera (close to the far plane). Again, this map should be rendered with a camera identical to the one used in your game. Pay special attention to the near and far plane settings here! (You can do a Google search for Depth Map to see an example of these maps) 

Initialize the intermediate texture to be filled with a black color. Then draw your spheres from left to right with 100% source pixel blending. This will produce a texture that looks like the bottom image in your example, but with a black background rather than a blue one. Next, render the intermediate texture to the screen using an additive blend mode. This will produce additive blending, while preserving the disc shapes as shown in your example. It will work with any background as well. 

In summary, you can back multiple sprite into one sprite sheet easily by aligning them on a uniform grid. As you said, you will need to metadata that describes the layout of the sheet. The above list contains some suggested attributes that will give you a lot of flexibility in the type of sheets you can employ in your game. Obviously, you can leave out or add new parameters as needed in your use-case. 

You can solve this by looking at the typical procedure followed when rendering a game world. When you render a 3D object, you typically apply three transformations: 

Z-buffering in a non-prerendered scene I will use the term pre-rendered background for what you describe as a 2.5D scene, as I assume this is what you are looking for. Ideally, you would want your character to be able to walk around the environment, as if it were not rendered as a flat image. Your character should be able to move behind and in front of objects as he moves away from or towards the camera. The way "drawing only the front-most objects" works is by using a Z-buffer or depth buffer. When rendering a fragment, the depth of the fragment is written to this buffer. When a new fragment is drawn on the same onscreen position, the depth buffer is consulted to see whether this previous fragment was in front of the new one. If so, the new fragment is not drawn. 

I was wondering what state is stored in an OpenGL VAO. I've understood that a VAO contains state related to the vertex specifications of buffered vertices (what attributes are in the buffers, and what buffers are bound, ... ). To better understand the correct usage of VAO's, I'd like to know exactly what state they hold. 

As nikoliazekter explained, you'd have to calculate the integral over the surface of polygon P. To solve this integral, you'd probably use a numerical approximation. I don't think it is realistic to do this in a real-time application, due to the complexity of this calculation. While solutions might exist for some specific shapes, they don't for polygons P of any arbitrary shape. If you still want to use area lights, I think you can do either of two things. Approach 1: Raytracing You could opt for a raytracing renderer rather than a standard rasterizer. A raytracer works by "shooting" rays from the camera and checking where these rays end up. They might hit a diffuse surface, bounce of a reflective surface, hit a light source or something else. You might shoot several rays per pixel, to obtain multiple samples to determine the pixel's color. A raytracer is a lot more flexible than a standard rasterizer, but this comes at the cost of complexity. This option might not be suitable for real-time applications. Pathtracing is a specific type of raytracing where the full path of a ray is calculated, instead of only calculating direct light bounces (ie. bounces directed towards light sources) when the rays from the camera hit a surface. This technique is a global lighting solution. When calculating the color of a pixel, the entire scene is considered as a whole, as light rays can bounce between objects. Thus, effects such as shadow casting, color bleeding etc will be simulated. 

In the first case, you will be checking all buttons every frame. Let's say you have 50 objects in your gameworld, and they need to respond to five different keys on average. If you are running at 60 frames per second, you'll be launching 50 * 5 * 60 = 15000 polling function calls per second. In the second case, the input device (or some manager component inbetween) takes the initiative to contact all objects that are interested in listening to keyboard event. Let's say you still have 50 objects in your gameworld, they still respond to five different keys on average, and you are still running at 60 frames per second. The important thing to note is, that object code is only called when an event happens. Let's say you press 10 keys every second (which would be a lot). This means the number of calls made will be 50 * 10 = 500. For active polling, the number of calls relates to the the amount of buttons that an object responds to, the amount of objects in your gameworld and the framerate of your game. For events, the number of calls relates to the amount of actual, physical button presses and the amount of listeners. Events are a great choice for checking input, as input events occur sporadically (aka. relatively far and few inbetween). An additional advantage of events is: 

Solution 2: Splitting of your pre-rendered background into depth planes If direct write access to the z-buffer is impossible, you'll need to fill it with the correct depth data "the right way" (i.e. by rendering polygons). A solution would be to split your backgrounds up into multiple layers of different depths. Within a single layer all objects in your pre-rendered background will be considered to be at the same depth plane. For example, if you need to render a scene with a floor, a sky and two church buildings, you would use separate layers for: 

While this solution is more complicated, it will resolve the two primary problems with the simple approach (objects leaving a gap inbetween them upon colliding and not being able to cope with non-rectangular bounding boxes). 

Game technology Game technology refers to the development of supporting technologies for games, independant of a specific game design. It focusses on the design of high-performace algorithms of core game-related functionality. Think of it as low-level tech that belongs in your game engine. Topics you would label as game technology include: rendering algorithms, input polling from devices, collision checking and physics, audio processing, timing, file handling, memory handling and more. Game technology expertise is important if you are developing a custom game engine or engine component, as well as low level mechanisms to support game development decisions. Additionally, you can pursue game tech as an academic career as constitutes a big body of academic research, being an optimization/ performance-related field. As user Stephan notes, you could see game technology as a form of high-performance computing, specialized in game-related algorithms. 

In summary, I do not believe it is possible to achieve the described effect by using an XNA blend mode. Out of the proposed solutions, think the second approach using an intermediate texture will produce the cleanest result. It is the most robust method as well, working on any background. The Z-buffering method will allow you to render to the screen instantly (not requiring an intermediate texture), but is more involved and only works for simple shapes such as circles and rectangles (as you need to construct polygonal meshes for the shapes to render on). 

Solution 1: Closest Hex Center by Comparing Center Points The clue of this, is that your hex grid is a Voronoi pattern. This means: to figure out which hex (Voronoi cell) a point belongs to, you only need to consider the distance of the point to the hex center (Voronoi cell center). A brute force solution would be to iterate through all your hex-centers (ie. horizontal lines of points, with odd rows shifted by 50% of the tile width) and just find the one with the closest distance to your point.