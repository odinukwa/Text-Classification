However you need them to be used. There is no single correct way to build a component-based entity system (not all of them even include the concept of a "system"), and even among implementations that use similar terminology, not all the details are alike. You need to focus on building the system that accomplishes the goals you have set for your project. 

No feedback is feedback; it could mean that your testers simply don't find the game engaging enough to feel like investing time in crafting reasonable feedback for you. Obviously it will be tricky to figure out why, but it's possible the people you are putting the game in front of are not the ideal demographic (or are not ideal testers). Or it's possible the game just isn't fun. You should try to make providing feedback as smooth as possible, ideally requiring no work from the user at all except an opt-in. For example, gathering automatic metrics like heat maps (this video discusses these sorts of things, somewhere around the 40 minute mark, I think) has for player death and/or time-to-complete levels and such can tell you things about your game you might not realize without any actual action on the part of your testers at all (other than them agreeing to allow your game to send you statistics about their play sessions). You might find that your average game play session lasts five minutes and 95% of the users die or get stuck early in level one, for example. That might suggest why nobody bothers providing much feedback -- they simply get discouraged too early and don't bother. Even getting written feedback should be optimized -- provide an in-game means (for example, a command) to pop up a UI that a tester can write up a bug or some bit of information and send you. People are lazy. The more friction that is involved in testers sending you information, even as little as opening up their email client themselves, the less likely they are to actually do it. 

This will find all connected objects, and might seem like the final solution. But there is a caveat: consider a graph of connected objects like () or, on a grid, a 2x2 block of objects of the same type. Because the above solution does not consider if an object has already been seen, it can recurse forever in such a case. So you need to track that: 

The GPU doesn't, the programmer does. The initialization of the graphics API will typically involve optional specification of which display device to use; not explicitly specifying this generally results in the default of whatever the OS thinks the "primary monitor" is. OpenGL and D3D handle this decision differently. D3D has the concept of "adapters" modeled in to its API, and you can query the available number and figure out the the properties of each and then automatically decide or present a choice to the user. OpenGL doesn't actually concern itself with formally specifying the creation of its context object for every OS; the OS itself handles that. You generally need to use the OS-level display-enumeration features ( or on OS X) to find the set of available displays and give them to the appropriate OS-level functions used to create the OpenGL context. Usually this only matters if you're trying to use exclusive full-screen on a specific monitor, or render to a device that the system doesn't necessarily normally consider a "screen" (in that case getting the appropriate handle may involve the use of APIs specific to that hardware; this may be the case with the Rift, I'm not sure). Generally if you're just doing windowed rendering, you don't need to worry about which monitor you are on. 

mental ray is a stand-alone 3D renderer. It's primarily application domain is film and TV. As a renderer, it supports a concept of "shaders" as functions that compute lighting effects. However, these are not mechanically the same shaders that you'd use in OpenGL or D3D. They are built in their own language, tied to the mental ray lighting and renderer engine. Further, the renderer isn't designed for real-time use. Consequently it is not possible to use the shaders directly in your own game or application. You will need to manually re-implement the effects you are after (or find somebody else who has and use their implementation), probably making tweaks and adjustments for performance. 

The details of the content of the SDK are probably covered by NDA. That said, since SDKs generally contain compilers and other related tools and libraries. Access to the development hardware is often a separate issue, although to most developers they will appear to be part of the same package. You can use any text editor to write code, and just about any IDE that can work with external compilers (which is any decent one) can be made to incorporate the tools from a console SDK. 

As APIs, SlimDX and SharpDX are almost exactly as difficult to use as the native D3D API or any other APIs they encapsulate. With a few minor exceptions, any ease-of-use benefit you get from either API comes primarily from the fact that you're using a higher level language (C#). This is because, as Martins points out in the comments on your question, they're both lightweight wrappers. 

Almost any container can; it matters more what you store rather than what you store it in. If you take advantage of polymorphism, you can implement a base class or interface 

The benefit of organizing data in the fashion described (storing things that will be frequently used or accessed together next to each other in memory) is not zero cache misses. It is fewer cache misses. That's all. You're not wrong: if you represent your entity as an index, and use that index to refer to separate arrays for each of position, texture coordinates, mesh or shader references, et cetera, then yes, that's not very cache-efficient at all. Instead, don't break your entity down into such pointless granular components. If you need position/texture coordinate/resource references all at the same time when processing that component (such as during rendering) store them all in a single record: 

While you might be able to come up with a method that is simpler in code (in particular by avoiding the second step by simply placing a bunch of pre-defined geometry objects, like cubes or 2D rectangles adjusted by depth to fake perspective), you are likely to run into scale problems much sooner given the overhead of each point in such an approach. Stuffing everything into a single object eliminates much of that overhead for improved scaling and performance. Plus, it doesn't require you to recompute every point when the camera rotates as you would in the projected 2D case (instead, you adjust the object's 3D transform property). If you are okay with a solution that uses external dependencies, the Helios toolkit provides a bunch of simple 3D primitives and utilities that will likely take a large chunk of the grunt work out of the above implementation. 

For projectiles that travel faster than the eye can see, raycasting is often employed - a ray from the muzzle is computed with the appropriate direction and is tested against potential target objects to determine what was hit. This can be complicated with multiple rays and some extra computation if you want to simulate things like bullet drop and such. You can also give the projects a speed and use that to add some additional realism to the computation (so bullets aren't instantaneous). 

Some of this information is optional, so if you don't have a clue what you should be using (for example, for the leaderboard ID), it's probably because your game does not make use of that feature and you can omit the data. Your speculation about the use of the share message is correct. 

GML is a distinct language developed for Game Maker. It bears some similarity to C and C++ on the surface, in terms of its syntax (keywords, grammar and structure), but it is otherwise unrelated to C++ -- it certainly is not a sub- or super-set of the C or C++ languages. GML differs from C(++) in a lot of ways... for example it does not require variable declaration, has a much simpler memory allocation model, and several additional extra language constructs (such as the "with" block). You could write your GML code within Visual Studio, but it wouldn't work well. You might get syntax hilighting, but VS could not compile GML in general and using Game Maker itself to author GML is a far better idea since the tool and the language are tightly integrated. 

Fundamentally these are very similar approaches, differing mainly in the interface they present over the multiple-integer abstraction. A given game may choose one over the other depending on it's needs. For example, in a space game like No Man's Sky, you might only be able to "warp" between star systems. In this case there is a distinct transition phase that the game can use to smoothly transition from one coordinate space to another. However, you might be able to smoothly and arbitrarily fly from space on to a planet surface, in which case you might have a harder time masking the coordinate system transition (it's doable but might be annoying depending on the rest of your game's implementation and how you deal with straddling cells or something). In that case you might prefer to represent coordinates using a "big number" class that simply let's you pretend you have a larger coordinate space to work in, at the minor cost of some performance now and then. 

Constantly saving all character data immediately for all players isn't particularly feasible. Fortunately it isn't particularly necessary. This is really a scenario where you want to avoid crashing in the game server as much as possible, because that far more serious long-term implications for your game than just a few minutes of progress being reset. In general, you can get by with a system that keeps character/account data resident in the server's memory and saves either periodically or at certain "critical" moments (such as a level up, or the completion of a quest). It's true that if the game server crashes, you could lose some amount of non-critical progress; this is generally considered acceptable (for example, Guild Wars 2 works this way; this is the phenomenon people often incorrectly call a "server rollback"). You can increase the frequency with which you can make saves by building database cache servers in front of the actual database server. Your game servers enqueue save requests to the cache servers, which distribute the pressure on the game server over time. This also allows you to tune the rate at which the caches flush to the final server without having to directly manipulate the game servers, as you'll likely have many games servers and fewer cache servers. Try to keep the data you store as simple as possible for the database to consume; if you do anything but store binary blobs (which is what Guild Wars 1 did), design your table schema in such a way as to minimize relationships between tables that might cause locks that slow down an update. Consider your indices carefully, because every index you add can increase the complexity of inserting into a table. Et cetera. You also need to consider the hardware/machine configuration options for dealing with the problem: if your entire database cannot be kept in memory on the server, you'll want to have very fast disks. Other than distributing the load as much as possible and finding the appropriate amount of throttling you can make, this is primarily a database optimization problem. There are whole books written on SQL server optimization (and lots of lore on the SQL-alternatives, should you choose to elect to explore one of those as well). I'd recommend going in search of that material. For most hobby "MMOs" though, this kind of scalability won't be needed. 

essentially due to the fact that the planes of the frustum are the planes of a "unit cube" and thus have trivial equations. This means clipping against that frustum is also extremely easy. Clipping is an important part of the graphics pipeline, because it allows you to ensure the set of triangles to be rasterized is entirely within view, and thus avoid expensive checks and breaks in the tight loop of the rasterization routine. At the same time, clipping is a thing you need to test every triangle in the scene for, so arranging your pipeline to make it fast is important. Computer vision is generally unconcerned with these details of rasterization, and thus doesn't need to take this detour. 

you can see that the angles between the tile borders (X and Y axes) and the flagpole (Z axis) don't quite form even 120 increments (they're somewhere between an isometric projection and an oblique one, it looks like). But it's close enough for government work. 

Now, all that said, I think it's useful to support both text and binary serialization for assets. During development, keep all your data in a human-readable format based on XML or JSON. This can increase your iteration ability a lot because you don't need to build such complex tools around editing the data. You can return to being able to make simple quick tweaks by hand. Second, assuming you even still want a binary format for shipping the game (which can improve file size or file IO times, so it's a valid desire), design your serialization and deserialization APIs to handle versioning. Versioning is still useful in a shipping context, because as some point you may want to ship updates or bug fixes. There are some documents describing the versioning abilities of .NET serialization and Boost's serialization that you may find interesting. If you are going to support both text and binary formats, make sure you test them occasionally (or build automated tests to do so, even better). 

It definitely did not. In my youth, I had a CD with a bunch of Mac game demos on it (came from an old MacAddict magazine issue, if I remember correctly). One of those demos was for Crystal Caliburn, a pinball game that supported a multi-ball mode that behaves exactly like Badland's swarm: each ball is interchangeable and has no more relevance than any other, and the players control mechanism and authority over the swarm is not changed. I'm pretty sure the multi-ball mechanic is older than that game, of course, it was probably present on real pinball tables. I doubt it's possible to trace the origin of this mechanic, but I'm positive it's been around for longer than Badland. 

All your does is bring the contents of those namespaces into scope. It sounds like you probably didn't actually request an appropriately-versioned OpenGL context (that is, one with 3.2 support). To do so, you must provide context attributes requesting the desired version when you call