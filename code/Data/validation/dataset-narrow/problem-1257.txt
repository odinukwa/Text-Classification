Dror Bar-Natan's paper "Lie Algebras and the Four Color Theorem" (Combinatorica 17-1 (1997) 43-52, last updated October 1999, arXiv:q-alg/9606016) contains an appealing statement about Lie algebras that is equivalent to the Four Color Theorem. The notions appearing in the statement also appear in the theory of finite-type invariants of knots (Vassiliev invariants) and 3-manifolds. 

I came accross this result by Feige and Lund which shows that unless the polynomial hierarchy collapses it is hard to guess even very partial information about the permanent of a random matrix. Uriel Feige and Carsten Lund, On the Hardness of Computing the Permanent of Random Matrices. Computational Complexity 6 (1996/1997) 101-132. Let me also mention two additional relevant results brought to my attention from Uri Feige: The following two papers apply this in the context of kernelization (fixed parameter tractable algorithms). Hans L. Bodlaender, Rodney G. Downey, Michael R. Fellows, Danny Hermelin: On problems without polynomial kernels. J. Comput. Syst. Sci. 75(8): 423-434 (2009) Lance Fortnow, Rahul Santhanam: Infeasibility of instance compression and succinct PCPs for NP. J. Comput. Syst. Sci. 77(1): 91-106 (2011) 

I can see three related ways to understand the question: 1) Can we we regard $NP \ne P$ as a fundamental principle of computational complexity theory, even before we can prove it? 2) Does the $NP \ne P$ principle extends beyond its narrow mathematical meaning? 3) Does the $NP \ne P$ principle can be regarded as a physical law. I think that there are good reasons to answer 'yes' or 'qualified yes' for all these three questions. 

Polya's counting theorem leads to an algorithm for counting (precisely) the number of isomorphism types of graphs with $n$ vertices in $\exp (\sqrt n )$ steps. From Polya theorem you get a formula where you need to run over all conjugacy classes of the symmetric group $S_n$ and the number of conjugacy classes in $S_n$ is the number of partitions of the number $n$ known to behave like $\exp (\sqrt n )$. (There is a famous approximation for the partition function by Hardy and Ramanujan and subsequent expansions.) Question 1: Is there a better algorithm? Can you improve the number of steps below $\exp (\sqrt n)$? (Actually, I would be interested also by a different algorithm even if not better.) While at it: Question 2 (updated): What is the smallest number of arithmetic operations for an algorithm to count the number of partitions of an integer $n$? (I asked originally about a better than $\exp (\sqrt n)$ time algorithm for counting the number of partitions of an integer n, but Sasho indicated an $O(n^{3/2}$ algorithm.) Question 3: Since Questions 1 and 2 are questions with a "tiny input" (a single integer $n$) is computational complexity theory adequate to ask and offer useful answers (reductions etc.) to these (and similar) question? Question 4: How much better can you do if you want only to approximate the number of isomorphism types by Polya theory or by other means? Question 5: What is the complexity of sampling precisely or approximately isomorphism types of graphs with $n$ vertices? Question 6: What is the complexity of questions 1,4,5 if instead of dealing with all graphs on $n$ vertices we deal with subgraphs of an input graph $G$ with $n$ vertices. (This makes these problems ordinary "big input" problems. Here, I am not aware of a $\exp (\sqrt n$) algorithm. Question 7: Are these problems related in some way to the graph isomorphism problem? 

The questions touches on some very interesting issues regarding quantum computation (and randomness). BQP is the class of decision problems that can be solved efficiently (in polynomial time) but it is not clear that referring just to decision problems suffices to do justice to the power of quantum computing. If we let QSAMPLING describes the probability distributions that can be sampled efficiently by quantum computers then there are reasons to think that QSAMPLING represents computational power of quantum computers (having to do with randomness) that goes beyond BQP. For example, it is plausible that a BPP computer (which under your assumption, as well as standard thinking of CC that derandomization is possible, is equivalent (polynomially) to deterministic computer) cannot perform QSAMPLING even if it is equipped by a BQP subroutine. Thus, it is not known that BQP=P implies that QSAMPLING can be performed by a classical computer. The question about the meaning of randomness you referred to is fairly profound issue in the foundation of probability. (And quantum mechanics does not change the matters drastically.) Computation complexity says quite a bit about randomness but randomness has also various other aspects. 

Lenstra's integer programming in bounded dimension, the Lenstra-Lenstra-Lovasz algorithm, and related subsequent algorithms - Barvinok's algorithm for the number of integer solutions to an IP problem in bounded dimension and Kannan's P-algorithm for the Frobenius/Sylvester problem can be added as a special category. A notable open problem here is to find a P-algorithm for higher order problems in the Presburger Hierarchy. Another class of P-algorithm worth mentioning are those P-algorithm given to object proved to exist by randomized proofs. Examples: algorithms for applications of Lovasz-Local Lemma; algorimic versions of Spencer discrepency result; (of slightly different flavour) algorithmic versions of Szemeredi regularity lemma. 

Fresh from the oven: A Polynomial Time Algorithm for Lossy Population Recovery By: Ankur Moitra, Michael Saks Quoting from the paper: "Here we will prove the uncertainty principle stated in the previous section using tools from complex analysis. Perhaps one of the most useful theorems in understanding the rate of growth of holomorphic functions in the complex plane is Hadamardâ€™s Three Circle Theorem..." 

Moving from the inequalities to the points looks to me somewhat similar to a "transform". Moreover, I would be happy to see a quantum algorithm even if you modify the distribution, e.g. consider product of Gaussian distribution described by the hyperplanes of the polyhedron or some other things. A few remarks: Dyer, Frieze and Kannan found a famous classical polynomial time algorithm to approximately sample and approximately compute the volume of a polyhedron. The algorithm is based on random walks and rapid mixing. So we want to find a different quantum algorithm for the same purpose. (OK, we can hope that a quantum algorithm may lead also to things in this context we do not know to do classically. But to start, all we want is a different algorithm, this must be possible.) Second, we dont even insist on approximately sampling the uniform distribution. We will be happy to approximately sample some other nice distribution which is roughly supported on our polyhedron. There is an argument by Santosh Vampala (and also by me in another context) leading from sampling to optimization: if you want to optimize f(x) sample to find a point y where f(x) is typical. Add the constraint {f(x)>= f(y)} and repeat. 

In error correcting codes, it is possible that there is a fundamental difference between binary codes and codes over larger alphabets in that the Gilbert Varshamov examples for codes which correct a fraction of errors (which are essentially greedy or random examples) are believed by some to be tight in the binary case and are known to be not tight over a large alphabet via algebraic-geometry codes. This led some to speculate that the standard definition of error correcting codes for a large alphabet is not the right analog of binary error correcting codes. 

Two related questions about bounded depth computing: 1) Suppose that you start with n bits, and to start with bit i can be 0 or 1 with some probability p(i), independently. (If it makes the problem simpler we can assume that all p(i)s are 0,1, or 1/2. or even that all of them are 1/2.) Now you make a bounded number of computation round. In each round you apply reversible classical gates on disjoint sets of bits. (Fix your favorite set of universal classical reversible gates.) At the end you get a probability distribution on strings on n bits. Are there results on restriction of such distributions? I am looking to something analogous to Hastad switching lemme, Boppana result that the total influence is small or LMN theorem. 2) The same question as 1) but with bounded depth quantum circuits. 

Let $f$ be a Boolean function of $n$ Boolean variables. Let $g(x)=T_\epsilon (f) (x)$ be the expected value of $f(y)$ when $y$ is obtained from $x$ by flipping each coordinate with probability $\epsilon/2$. I am interested in cases where it is computationally hard to approximate $g$. Let me fixed a notion of "approximation" (but there may be others): A Boolean function $h$ approximates $g$ if $h(x)=1$ when $g(x)\ge 0.9$ and $h(x)=0 $ when $g(x)\le 0.1$.A counting argument (based on the existence of positive rate error correcting codes) seem to give that there are Boolean functions for which any such approximation requires an exponential size circuit. But the question is what happens when $f$ to start with is in NP or in its neighborhood. Q1: Is there an example of $f$ described by NP circuit (or P-space) so that every $h$ is NP hard, or hard in some weaker sense. To see that $h$ might not always be easy (I thank Johan Hastad for useful discussion about it) we can consider the property of graphs of having a clique of size $n^{1/4}$, for random input, it is conceivable that it is hard to detect if there is a large clique but this is manifested by having more than expected cliques of size log n in the noisy graph. In this case any $h$ will be likely-hard (but not provably, and not terribly hard as quasi-polynomial circuits will be telling). Q2: What is the situation if $f$ to start with is low complexity. ($AC^0$, monotone $TC^0$, $ACC$ etc.) Q3: What is the situation for some basic examples of Boolean functions. (The question can be extended also to real-valued function.) Q4: Can the above question be asked formally for the uniform (Turing-machine) model of computation? Update: In view of Andy's answer (Hi there, Andy) I think that the most interesting question is to understand the situation for various specific functions. Update Another question Q5 [Q1 for monotone functions] (also in view of Andy's answer). What is the situation if $f$ is monotone? Can we still encode robustly an NP complete questions> 

This result extends to high dimensions. A triangulation of a d-dimensional sphere so that every vertex has an even degree is (d+1) colorable. See, for example this paper: Jacob E. Goodman and Hironori Onishi, Even triangulations of $S^3$ and the coloring of graphs, Trans. Am. Math. Soc. 246 (1978), 501â€“510. 

Quantum computers are very good for sampling distributions that we dont know how to sample using classical computers. For example if f is a Boolean function (from $\{-1,1\}^n$ to ${-1,1}$) that can be computed in polynomial time then with quantum computers we can efficiently sample according to the distribution described by the Fourier expansion of f. (We do not know how to do it with classical computers.) 

Lovasz found topological obstructions for k-colorability and used his theory to solve Knaser's conjecture. His theorem is the following. Let G be a connected graph, and let N(G) be a simplicial complex whose faces are subsets of V that have a common neighbors. Then if N(K) is k-connected (namely, all its reduced homology groups are 0 up to dimension k-1) then the number of colors needed to color G is at least k+3. 

(Sorry if this is well known.) I would like to give some item to one of $k$ agents, so that agent $j$ will get the item with probability $p_i$. Is there a cryptographic (or other) tool so that every agent (and even every observer) will be able to be convinced that the random drawing was indeed fair? 

Not having a large independent set can be as important as having a large clique. An important obstruction for a graph to be non k-colorable is that the maximum size of an independent set is smaller than n/k, where n is the number of vertices. This is a very important obstraction. For example it implies that a random graph in G(n,1/2) has chromatic number at least n/log n. A more refine obstruction is that for every assignment of nonnegative weights for the vertices there is no independent set that capture a fraction 1/5 (or more) of the total weight. Note that this also include the "no clique obstructions." LP-duality tells you that this obstruction is equivalent to the "fractional chromatic number" of G being larger than k. There are also obstructions for k-colorability of a different nature that sometimes go beyond the fractional chromatic number barrier. I will devote a separate asnwer to them. 

The property MAXIMAL=MAXIMUM fo independent sets in graphs and more general combinatorial structures is important. It will be interesting to understand graphs where this property holds for all induced subgraphs. One general abstract case where we have MAXIMUM=MAXIMAL is when there is an underlying matroid structure, but there are many other cases, like the case of maximal planar graphs mentioned in the question. Here is a related example: Consider n points in the plane in convex position and let k be an integer. Consider graphs whose vertices are line segments between these points where two vertices are adjacent if the line segments do not cross. Dress proved that for this graph MAXIMIM=MAXIMAL for independent sets. 

Question 1: Is it true that for every polynomial $p(n)$ and $\epsilon >0$ there is a polynomial $q(n)$ such that every monotone Boolean function on $n$ variables that can be expressed by a Boolean circuit of size $p(n)$ can be $\epsilon$-approximated by a monotone Boolean circuit of size at most $q(n)$. A function $f$ is $\epsilon$-approximated by a function $g$ if they agree on at least $(1-\epsilon)$ fraction of inputs. We can ask a slightly more general question. Let $\mu_p$ denote the Bernoulli measure on $\{0,1\}^n$ where every variable is '1' with probability $p$. For a (non-constant) monotone function $f$ let $p_c(f)$ be the value such that $\mu_{p_c}(\{x: f(x)=1\})=1/2$. Question 2: Is it true that for every polynomial $p(n)$ and $\epsilon >0$ there is a polynomial $q(n)$ such that every monotone Boolean function $f$ on $n$ variables, there is a Boolean function $g$ that can be expressed by a monotone Boolean circuit of size $q(n)$ such that for $p=p_c(f)$, $\mu_p(\{x: f(x) \ne g(x) \}) \le \epsilon$. Of course, the instinctive thought is that the answers for both these question is NO. Is it known? Razborov famously proved that matching cannot be expressed by a monotone Boolean circuit of polynomial size. But matching can be approximated in its critical probability by the property that there are no isolated vertices and this property admits a monotone (low depth) circuit. I asked the analog questions (Problems 1,2,3) for $AC^0$ and $TC^0$ in this blog post.