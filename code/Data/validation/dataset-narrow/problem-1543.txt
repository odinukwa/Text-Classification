For your particular problem I'm not sure that using a supervised Logistic Regression approach is ideal, but I suppose that is a different and larger topic. To answer your question, yes you can use a "bag of words" representation of your text. Python's library offers this functionality via both and . This will result in a sparse matrix representation of n-grams and occurrences of those n-grams in your corpus. From here you basically have 2 options, (1) training your supervised model directly on the sparse matrix or (2) reducing the dimension of your sparse matrix so it be represented as a dense matrix. Luckily sklearn offers functionality for both these, their class supports sparse matrices and their implementation of PCA/LSA supports sparse matrices as well. That should give what you need from a technical perspective to build a model, but I think the real question is what feature engineering you may be able to do in addition to simply training on the bag of words representation of the text. 

Perform dimensionality reduction (such as LSA via ) on your sparse data to make it dense and combine the features into a single dense matrix to train your model(s). Add your few dense features to your sparse matrix using something like scipy's into a single sparse matrix to train your model(s). Create a model using only your sparse text data and then combine its predictions (probabilities if it's classification) as a dense feature with your other dense features to create a model (ie: ensembling via stacking). If you go this route remember to only use CV predictions as features to train your model otherwise you'll likely overfit quite badly (you can make a quite class to do this all within a single if desired). 

I read all answers, I think the simplest answer to this question is based on the understanding of the train - test strategy. There is NO exact or correct answer to this question. Any split that can guarantee I am not under fitting or over fitting the data is a good a split. under fitting and over fitting are two different problems and are directly connected to the bias error and the variance error. You are highly recommended to read these two tutorials: $URL$ $URL$ 

Best by cross validation The whole data can be divided into training and testing. You can not touch the testing data set for any kind of training. Keep it away!! For the training data you can use it freely to specify the best classification model. But how to know if the current setting is the best one, you do cross validation by using only training data set, dividing the training data set into k folds. Based on this cross validation result you select the setting that gives the best result. Best by cross validation 

I have an answer now for my question. I will share briefly the main steps / technologies I used to deploy the model in production. I am using Python programming language. After training and generating valid models I wrote a restful api using Python programming language and flask. Using flask you can write a restful api. Three important points: 1- It is very important to give attention to where you will define the model architecture/initialize the parameters/ define the session. Avoid doing this each time you call the restful api. this will be very expensive. 2- Flask provide a very good mechanism to run servers in production environment. Read about flask + wcgi Avoid runing the server code (the resful api) directly, in this case you will not have direct and full control. 3- Watch the memory and the cpu usage, make sure to limit the maximum number of instances that can run in parallel. Remember these models can take a lot from the memory. unfortunately, I can not share codes with public. But hopefully my answer can give an idea about how to do it in production. 

Imagine that the hyperparameter is a L2 penalty or a dropout rate -- something that we think that should have a single sweet spot -- too high and you're underfit and too low and you're overfit. I keep getting nonconvex plots like the one above when doing cross-validation. I guess this just points to a lot of noise during training -- I've got a lot of variables for a modest sample size, and I need to regularize heavily to get a good model. But still I'm a little bit unsure whether this sort of thing might point to a bug in some aspect of my implementation. Has anyone come across this sort of thing before? And did you just shrug off the nonconvexity and go with the model that minimized the prediction error? If so, that begs the question: why not just compute a prediction error at each update during training, saving any set of weights that minimizes prediction error -- even if the model is nowhere near converged. Basically letting the noise work in your favor. This seems appealing, because sometimes I get really low prediction errors early on, only to have them evaporate as the loss function declines. This seems horribly unprincipled, but I ask myself "why should I care if it is"? And "is it unprincipled anyway?" 

Self-driving cars, arms that can learn ow to pick up objects, machines that can have conversations, etc. I understand how neural nets work. But I really don't know much about how neural nets are combined to form intelligent systems. Is it all rule-based above a few base nets for tasks like e.g. image recognition? Or are there meta-nets that do deep learning on the outputs from base nets? Things to read/references would be super-helpful! 

It depends on the expected size of neural network you are thinking about. If the DNN consists multiple layers with large default input and many fully connected layers then you need a GPU with large memory. Memory is the most important factor , without enough memory, you GPU is useless. To get an idea about the size of any DNN try to think about the number and size of fully connected layers. Of course the number of cores is important but again memory is more important 

You should use the testing set without any change, as answered by others. But it is very important to understand the difference between average accuracy and overall accuracy. In overall accuracy you find ( number of samples predicted correctly/ total number of samples) in average accuracy, you find the overall accuracy per class and then you find the average of these overall accuracies. When you know that you are working with imbalanced database, where all classes are important, you should use the average accuracy To understand what this means: imagine you have two classes, class A and class B , and the ratio is 90 to 10 . If you are sampling randomly for the training and testing, then the ratio is still 90:10 in the testing set. If your model is very biased , that predicts all the samples to be class A , then: Overall accuracy = 90% Average accuracy = 50 % ( 100% for class A + 0% for class B) / 2 The overall accuracy is really high but it does not reflect the actual quality of the model. The average accuracy gives you a better indication of the quality 

I'm implementing batch normalization for the first time, and I've figured out a tentative solution for the gradients. Given the gradients, how should I do the updates? Does it make sense to use parameter-adaptive update rules like ADAM and RMSprop with batchnorm parameters? Their value is intuitive with regular layer weights, but less so with $\gamma$ and $\beta$. What is commonly done, and why? 

(The lower right plots just give the last 100 iterations, and the x axis is always the iteration.) Once it gets close to the bottom, the net starts overshooting and then correcting itself -- eventually working its way to the bottom (or a bottom) of the loss surface). This is annoying -- I'd like it to go faster and not dither. Features of the net: 

Take the following tensor: $$ \left[\begin{array}{cc} a & b & c\\ d & e & f\\ g & h & i\\ \end{array}\right] $$ $$ \left[\begin{array}{cc} j & k & l\\ o & n & m\\ p & q & r\\ \end{array}\right] $$ Where each matrix represents a channel. This could be reshaped fairly easily into a vector: $$ [a,d,g,b,e,h,c,f,i,j,o,p,k,n,q,l,m,r] $$ And then concatenated row-wise with other vectorized tensors to form a typical flat-file dataset of dimension $N \times P$, where $N$ is the number of training samples and $P$ is the product of the tensor dimensions. Rather than futzing with a convolutional layer, one could simply constrain ones weights to be zero in the subsequent layer. If $X$ is a flat $N\times P$ dataset of concatenated vectorized tensors, then the convolutional weights would form a sparse matrix, with the first two columns of a $P \times 4$ convolutional "layer" being $2\times 2\times 2$ filter being $$ \left[\begin{array}{c} w 0\\ w0\\ 00\\ ww\\ ww\\ 00\\ 0w\\0w\\00\\w0\\ w0\\00\\ww\\ww\\0w\\0w\\0w\\00 \end{array}\right] $$ This seems to me more intuitive than the tensor formulation, and could be computed fairly simply using sparse matrix packages. Perhaps it is partly matter of taste. But I'm curious: is there anything special about the tensor paradigm -- either mathematically or computationally -- that is superior to the flattened representation? I understand that computers convert matrix algebra to for-loops "under the hood", but doesn't the advent of the GPU make such explicit looping irrelevant? 

And clearly if you wanted to selected based on some other criteria than "top k features" then you can just adjust the functions accordingly. 

All three approaches are valid and have their own pros and cons. Personally, I find (1) to typically be the worst because it is, relatively speaking, extremely slow. I also find (3) to usually be the best, being both sufficiently fast and resulting in very good predictions. You can obviously do a combination of them as well if you're willing to do some more extensive ensembling. As for the algorithms you use, they can essentially all fit within that framework. Logistic regression performs surprisingly well most of the time, but others may do better depending on the problem at hand and how well you tune them. I'm partial to GBMs myself, but the bottom line is that you can try as many algorithms as you would like and even doing simple weighted ensembles of their predictions will almost always lead to a better overall solution. 

Standardizing data is recommended because otherwise the range of values in each feature will act as a weight when determining how to cluster data, which is typically undesired. For example consider the standard metric for most clustering algorithms (including DBSCAN in sci-kit learn) -- , otherwise known as the L2 norm. If one of your features has a range of values much larger than the others, clustering will be completely dominated by that one feature. To illustrate this look at the simple example below: 

I like to start with an overfitting set of hyperparameters, train to convergence - usually zero training error - then start increasing the regularization until the test error starts declining. Time is saved by the fact that I never re-initialize, but jack up the regularization parameters in an already-fit network. My regularization parameters are dropout rate and weight decay. I usually leave one of them fixed and move along a grid of the other. When trying for a given set of hyperparameters, I usually start with large batches (faster) and then move to small batches (deeper). I also rely on ensembling, combining the prediction of 4 - 64 models trained analogously, maybe with different architectures Ymmv, perhaps. 

When fitting neural nets and getting close to the bottom, I consistently get a very distinct pattern in the loss function (and the mse). See below: 

N = about 7000, about 2000 input variables, 10 layers of 100 nodes each The loss is L2-penalized squared error. This one is only very lightly penalized (and thus will almost certainly end up overfitting given the architecture) This particular net is using full-batch gradient descent, no dropout, and full-connections. But I see the same thing when using minibatch, and either dropout or convolutional architectures. The learning rate gets multiplied by a constant every time the loss goes down, and divided by the same constant squared when the loss goes up. Activation is the leaky ReLU No batch normalization RMSprop for adaptive learning rate Fit with this package (I'm the author) 

When use any sampling technique ( specifically synthetic) you divide your data first and then apply synthetic sampling on the training data only. After you train you use the testing set ( which contains only original samples) to evaluate. The risk if you use your strategy is to have the original sample in training ( testing) and the synthetic sample ( that was created based on this original sample) in the testing ( training) set. 

I am not sure if the validation set is balanced or not. You have a severe data imbalance problem. If you sample equally and randomly from each class to train your network, and then a percentage of what you sampled is used to validate your network , this means that you train and validate using balanced data set. In the testing you used imbalanced database. This means that your validation and testing sets are not equivalent. In such case you may have high validation accuracy and low testing accuracy. Please find this reference that talks mainly about data imbalance problem for DNN , you can check how they sample to do the training, validation and testing $URL$ 

Data Imbalance Data Imbalance + Very few number of samples (minority class) Severe Data Imbalance + Very few number of samples (minority class) 

You need to understand to which of these cases your problem belong. if you have very severe data imbalance + very few number of samples + wide variation within the majority class and similarities between different classes. regular oversampling or down sampling techniques will not help you as well as most of the synthetic oversampling techniques designed specifically to deal with the data imbalance but the assumption is to have enough number of samples. Try to focus more on ensemble techniques that designed mainly to deal with data imbalance. SMOTE-Boost RUSBoost SMOTEBagging IIIVote EasyEnsemble 

Interesting question. I haven't worked with this sort of data much, but it seems to me that the bulk of the job is likely to be feature engineering. Every "supervised" statistical method that I know of requires that you shoe-horn your data into "outcomes" and "covariates." $\mathbf{Y}$ and $\mathbf{X}$. Once your data is in this form you simply find an appropriate algorithm that can estimate $$ \mathbf{Y} = f(\mathbf{X}) $$ But taking data on how people play a video game and turning it into a 2-dimensional matrix isn't necessarily obvious. One of the most difficult things is that you'll need to make observations of different users comparable. Say you're playing quake 3 and you're trying to predict whether the user is going to go for the BFG. How do you define $\mathbf{Y}$? Is it ? Or should it be ? (maybe people messed up the rocket jump, for example). What if they got fragged on the way to the BFG? The point is that defining the outcome involves choices. Likewise what are the covariates? Say I get spawned in some location, and I know the map, and immediately move toward the BFG. I'll have a different set of , , , and keystrokes than someone who got spawned elsewhere on the map. So is it n-grams that you want to look at? Maybe rather it is change in euclidian distance between spawning location and BFG location over some time interval. if so, what is the appropriate time interval? In general, what you're doing is taking raw data and turning it into abstractions of that data. This is actually what a neural net does. It takes raw data $\mathbf{X}$ and forms a lower-dimensional representation of that data, $\mathbf{V}$, where $\mathbf{V}$ has fewer columns than $\mathbf{X}$. When neural nets work well, those columns of $\mathbf{V}$ can be uncannily similar to things that a human would pick out of a stream of raw data. Akin to this: $URL$ Those $\mathbf{V}$ are then just related to your outcome $\mathbf{Y}$ in a linear model or a logistic regression. But you don't turn raw keystrokes into , and without a lot of data and long training times. It'll go faster if you can prespecify functions of your raw data, which requires domain knowledge.