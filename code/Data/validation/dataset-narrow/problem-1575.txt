I have read the docs here and I understand the general idea. I am able to visualize the weights of the intermediate layers. However, I'm having trouble visualize the activations. Here's what I have: I trained my model and saved the weights in a file called . Thanks to this jupyter notebook, I got the values of the weights. First I defined my model: 

I'm having some trouble interpreting what's going on in the training and validation loss, sensitivity, and specificity for my model. My validation sensitivity and specificity and loss are NaN, and I'm trying to diagnose why. My training set has 50 examples of time series with 24 time steps each, and 500 binary labels (shape: (50, 24, 500)). My validation set has shape (12, 24, 500). Of course, I expect a neural network to overfit massively. Because I was interested in a stateful LSTM, I followed philipperemy's advice and used with batch_size = 1. I had multiple inputs: one called that is a time series, and one called that is not a time series (and so is concatenated in to the model after the LSTM but before the classification step). As my classes are highly imbalanced, I also used Keras's function. To make this function work in a multi-label setting, I concatenated two columns to the front of my responses (one of all 0s and one of all 1s), such that the final shape of the response is (50, 502). 

I'm requesting data from a government body, and they asked me what format I want to receive the data in. This will be a table of about 400,000 rows and about 10 columns. My options are: "comma or tab delimited ASCII, Microsoft Access database, Microsoft Excel file etc" They also want to know the media by which I want to receive data. My options are: "encrypted CD, DLT Tape, etc." I'm used to working with JSON, and doing data analysis via Python and R. These data will have to remain confidential. Any pointers? 

The input at time $t$ is $x_t$ and $h_{t-1}$. These get concatenated and fed into a nonlinear function (in this case a sigmoid). This sigmoid function is called the 'input gate', because it acts as a stopgap for the input. It decides stochastically which values we're going to update at this timestep, based on the current input. That is, (following your example), if we have an input vector $x_t = [1, 2, 3]$ and a previous hidden state $h_t = [4, 5, 6]$, then the input gate does the following: a) Concatenate $x_t$ and $h_{t-1}$ to give us $[1, 2, 3, 4, 5, 6]$ b) Compute $W_i$ times the concatenated vector and add the bias (in math: $W_i \cdot [x_t, h_{t-1}] + b_i$, where $W_i$ is the weight matrix from the input vector to the nonlinearity; $b_i$ is the input bias). Let's assume we're going from a six-dimensional input (the length of the concatenated input vector) to a three-dimensional decision on what states to update. That means we need a 3x6 weight matrix and a 3x1 bias vector. Let's give those some values: $W_i = \begin{bmatrix} 1 & 1 & 1 & 1 & 1 & 1 \\ 2 & 2 & 2 & 2 & 2 & 2 \\ 3 & 3 & 3 & 3 & 3 & 3\end{bmatrix}$ $b_i = \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix}$ The computation would be: $\begin{bmatrix} 1 & 1 & 1 & 1 & 1 & 1 \\ 2 & 2 & 2 & 2 & 2 & 2 \\ 3 & 3 & 3 & 3 & 3 & 3\end{bmatrix} \cdot \begin{bmatrix} 1 \\ 2 \\ 3 \\ 4 \\5 \\6 \end{bmatrix} + \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix} = \begin{bmatrix} 22 \\ 42 \\ 62 \end{bmatrix}$ c) Feed that previous computation into a nonlinearity: $i_t = \sigma (W_i \cdot [x_t, h_{t-1}] + b_i)$ $\sigma(x) = \frac{1}{1 + exp(-x)}$ (we apply this elementwise to the values in the vector $x$) $\sigma(\begin{bmatrix} 22 \\ 42 \\ 62 \end{bmatrix}) = [\frac{1}{1 + exp(-22)}, \frac{1}{1 + exp(-42)}, \frac{1}{1 + exp(-62)}] = [1, 1, 1]$ In English, that means we're going to update all of our states. The input gate has a second part: d) $\tilde{C_t} = tanh(W_C[x_t, h_{t-1}] + b_C)$ The point of this part is to compute how we would update the state, if we were to do so. It's the contribution from the new input at this time step to the cell state. The computation follows the same procedure illustrated above, but with a tanh unit instead of a sigmoid unit. The output $\tilde{C_t}$ is multiplied by that binary vector $i_t$, but we'll cover that when we get to the cell update. Together, $i_t$ tells us which states we want to update, and $\tilde{C_t}$ tells us how we want to update them. It tells us what new information we want to add to our representation so far. Then comes the forget gate, which was the crux of your question. The forget gate 

where TP = 'true positives'; FP = 'false positives'; FN = 'false negatives'; TN = 'true negatives'. You can read more here: $URL$ By taking TP+TN and dividing by TP+FP+FN+TN, you can get the classification accuracy of your model. In your case, that means (9779+148)/(9779+107+2227+148) = about 81% More details: This type of confusion matrix is used for binary classification. 

I'm currently using an LSTM with online learning, and it's taking about 10 seconds per training example. I'm looking to speed this up. Other related answers: One answer to this question implies that minibatch learning will not be helpful when there are millions of features or millions of observations (I think they're just talking about the memory limitations). Another question is related to what I'm asking, but on the implementation side of things. I've read this question and the answers, and it's very good but doesn't quite answer my question. 

In addition to what was suggested by @Media, you may consider adding a softmax layer to your model right above your input layer. This is a way of visualizing which features are strongly associated with each response. Of course as you say, an F-test will not really be appropriate. This is an active area of research as far as I know. 

Then we can expand that function to: $log(Y) = \beta_0 + \beta_1wx$, if $|wx| > 0$ $log(Y) = \beta_0$ otherwise. Which means that if $|wx| > 0$, then the hidden unit is giving us a factor by which to multiply the slope of the line separating the two classes. We can say that a one-unit increase in $\beta_1 w$ implies a one-unit increase in the log-odds of success. If $|wx| <= 0$, then the best separating line between the two classes has 0 slope, regardless of the magnitude of the weight (I think this is related to the dying relu problem). If we expand this to include a bias on the output, we have: $log(Y) = \beta_0 + \beta_1(wx + b)$, if $|wx + b| > 0$ $log(Y) = \beta_0$ otherwise. Through arithmetic, that first part can be expressed as: $log(Y) = \beta_0 + \beta_1b + \beta_1wx$, if $|wx + b| > 0$ which means to me that the weight $w$ from the input to the hidden unit tells us both how to modify the y-intercept (which is now $\beta_0 + \beta_1b$) and how to modify the slope. Does that make sense? 

The -1 values indicate that this position of the numpy array should be ignored (no weight calculation, no associated gradient update, no contribution to the cost function, etc). The positions of the -1 values are the same in each 3x3 numpy array. The 0/1 values are the labels for each position. How can I tell Keras to ignore the values -1 in my input? I have tried one-hot encoding each 0/1/-1 value and using a mask that way (since masking does not accept 1D input). I also saw that masking is applied to dimension 1 of the (num_samples, num_timesteps, num_features) input shape, which is not what I want: I want it applied to the features (the 3x3 numpy arrays). Does the below code do what I want it to do? 

You could try a clustering algorithm on the vector representation of your text data to see what clusters exist. Then once you have discovered the unique clusters, assign a cluster identity (for example, a single digit value indicating cluster identity (1, 2, 3, 4 etc)). Then you can use Keras's function to convert the cluster identity to categorical values. 

The training loss should (roughly) be decreasing per epoch, as should the validation loss. The training sensitivity and specificity are 92% and 97.5%, respectively (another possible sign of overfitting). My questions are: 

I'm reading a paper by Bergstra and Bengio (2012) on random search for hyperparameter optimization. I'm confused by their graph and explanation in Section 2.2: "Figure 2 illustrates the results of a random experiment: an experiment of 256 trials training neural networks to classify the rectangles data set. Since the trials of a random experiment are i.i.d., a random search experiment involving $S$ i.i.d. trials can also be interpreted as N independent experiments of $s$ trials, as long as $sN \le S$." That part I think I understand. Here's my take on it: If I draw from a uniform(0, 1) random variable 100 times, then I have 100 i,.i.d. samples. That's the same as if I had drawn 10 times each from 10 separate uniform(0, 1) random variables. In their figure (below), they say they had 256 trials. I think they chose 256 possible values of a hyperparameter and trained a neural network for each value. 

You have very sparse data. Are you storing these data optimally in a sparse object, for example a csr sparse matrix if you're using Keras? This is more likely to affect training time than accuracy, but is something to think about. 5000 training examples and 500 testing examples sounds alright to me, except you may have too few data to fit the type of model you have (you may be severely underfitting). Try a simpler model and see if you can improve results with that (try something stupidly simple like an MLP with one layer and see what happens). This seems the most likely problem. My first guess is this has to do with the unbalanced classes. If you're using a metric like accuracy to evaluate training at each epoch, I'd recommend instead using something like or or , all available through scikit-learn. If you're using Keras, try using as well, which will weight underrepresented classes higher in the loss function, essentially biasing the algorithm to consider underrepresented classes on an equal playing field. If you're not using Keras, try implementing some similar class weighting scheme. Batch normalization is less popular now than it used to be. It's worth trying the architecture with and without batch normalization to see if it provides a clear benefit. Yes, if your data are purely random then you will not detect a signal at all. This goes back to point number 2 (try a simpler model and see if you can detect a pattern). You can also try visualizing some of your samples to see if you can visually see a difference between the classes. 

You're going to have to do some experimenting to figure out what is 'best', but I would recommend starting with a convolutional neural network. Since you're only detecting a very small difference, though, the pixel values themselves should give you a good indication of where the colour is, and whether there is colour at all. I'm a bit surprised that a simpler architecture hasn't given you any luck. 

Now we have all we need to update the cell state. We take a combination of the information from the input and the forget gates: $C_t = f_t \circ C_{t-1} + i_t \circ \tilde{C_t}$ Now, this is going to be a little odd. Instead of multiplying like we've done before, here $\circ$ indicates the Hadamard product, which is an entry-wise product. Aside: Hadamard product For example, if we had two vectors $x_1 = [1, 2, 3]$ and $x_2 = [3, 2, 1]$ and we wanted to take the Hadamard product, we'd do this: $x_1 \circ x_2 = [(1 \cdot 3), (2 \cdot 2), (3 \cdot 1)] = [3, 4, 3]$ End Aside. In this way, we combine what we want to add to the cell state (input) with what we want to take away from the cell state (forget). The result is the new cell state. The output gate 

This will give us the new hidden state. Essentially the point of the output gate is to decide what information we want the next part of the model to take into account when updating the subsequent cell state. The example in the blog is again, language: if the noun is plural, the verb conjugation in the next step will change. In a disease model, if the susceptibility of individuals in a particular area is different than in another area, then the probability of acquiring an infection may change. The output layer takes the same input again, but then considers the updated cell state: $o_t = \sigma(W_o [x_t, h_{t-1}] + b_o)$ Again, this gives us a vector of probabilities. Then we compute: $h_t = o_t \circ tanh(C_t)$ So the current cell state and the output gate must agree on what to output. That is, if the result of $tanh(C_t)$ is $[0, 1, 1]$ after the stochastic decision has been made as to whether each unit is on or off, and the result of $o_t$ is $[0, 0, 1]$, then when we take the Hadamard product, we're going to get $[0, 0, 1]$, and only the units that were turned on by both the output gate and in the cell state will be part of the final output. [EDIT: There's a comment on the blog that says the $h_t$ is transformed again to an actual output by $y_t = \sigma(W \cdot h_t)$, meaning that the actual output to the screen (assuming you have some) is the result of another nonlinear transformation.] The diagram shows that $h_t$ goes to two places: the next cell, and to the 'output' - to the screen. I think that second part is optional. There are a lot of variants on LSTMs, but that covers the essentials! 

Have you tried using instead of ? For example, let's assume you have two data.frames: train_data and test_data In my example, the last column is the class (and is a variable), and all other variables (all other columns) are . For example, one training example might look like: 4 5 10 12 1 0 where 0 is the class label. You can do the following to run a random forest with 50 trees: 

Following up on the comment about deep learning, with high dimensional time series data you would be much better served with a recurrent-type of deep model. For example, an LSTM is a very good starting point with high-dimensional data. This may be a good place to start: Sequence Classification with LSTM Recurrent Neural Networks in Python with Keras Although CNNs are very useful for high-dimensional data, when you have a time series, it's best to start with a model that is designed for a time series. A CNN may do well, and you should compare your results to a CNN, but it is not a time series model. 

With regard to Xavier initialization, I don't think that's the likely cause of the effect you're seeing. The type of initialization can affect results, of course, but from what you're describing, I strongly suspect this is due to too little signal from the data or the unbalanced classes problem. Hope that helps!