Machine learning (perceptrons or not) is all about automatically finding generic but correct rules, be it in the form of If-Else-Rules, encoded formulas, closest occurrences, or others. The ML algorithm is just a way to (automatically) find this knowledge, whatever the representation may be. In another words, you use it to find the of your example, based on your data. You don't need ML if you can represent such knowledge yourself. 

This way of putting it is valuable to some techniques, notably "one-class classification" and "PU learning" (learning from positive and unlabeled examples). These techniques are very relevant when you want to know "does my item follow a given distribution D?", which is exactly what you're looking for. Still, if you have (or can collect) a lot of data that is neither A nor B, you may simply label it as a garbage class C and use a common classifier - however, it may actually be harmful, because you're likely assuming wrong things about class C [Li, Liu and Ng, "Negative Training Data can be Harmful to Text Classification", 2010]. It could work and is much simpler to develop, may be good enough depending on your case. 

The actual gradient descent implementation seems ok to me, but because there are errors in the gradient and cost function implementations, it fails to deliver :/ Hope this will help you get on track. 

There are several issues I see with the implementation. Some are just unnecessarily complicated ways of doing it, but some are genuine errors. Primary takeaways A: Try to start from the math behind the model. The logistic regression is a relatively simple one. Find the two equations you need and stick to them, replicate them letter by letter. B: Vectorize. It will save you a lot of unnecessary steps and computations, if you step back for a bit and think of the best vectorized implementation. C: Write more comments in the code. It will help those trying to help you. It will also help you understand each part better and maybe uncover errors yourself. Now let's go over the code step by step. 1. The sigmoid function Is there a reason for such a complicate implementation in ? Assuming that is a vector (a numpy array), then all you really need is: 

In decision trees, the (Shannon) entropy is not calculated on the actual attributes, but on the class label. If you wanted to find the entropy of a continuous variable, you could use Differential entropy metrics such as KL divergence, but that's not the point about decision trees. When finding the entropy for a splitting decision in a decision tree, you find a threshold (such as midpoint or anything you come up with), and count the amount of each class label on each size of the threshold. For example: 

When you build a tree, you need to define some criteria for splitting nodes. These include metrics like Information Gain and Gini Index. Those are heuristic approaches, they are not guaranteed to give the best possible split. Weight in the fact some attributes are less relevant and/or more noisy, and many other problems that happen in real data. In short, you cannot build a perfect tree in a decent computational time (you could of course build all possible trees and test the best, but then you'd have to wait some years for training even in a medium-sized dataset). Since we cannot have the best tree, we have approximations. One approximation is to build many trees (using different data partitions or attribute partitions), since we expect most trees to be somewhat correct, and consider their classifications in a voting system; this should deal with most noise, the vertical partition can deal with irrelevant attributes, the heuristic has less importance, and maybe other advantages. 

3. Gradient Again, let's first go over the formula for the gradient of the logistic loss, again, vectorized: $\frac{1}{m} ((\phi(X\theta) - y)^TX)^T + \frac{\lambda}{m}\theta^1$. This will return a vector of derivatives (i.e. gradient) for all parameters, regularized properly (without the bias term regularized). Here again, you've multiplied by $y$ way too soon: . In fact, you shouldn't have multiplied by $y$ in gradient at all. This is what I would do for the gradient: 

As operates element-wise over arrays. Ideally, I'd implement it as a function that can also return its derivative (not necessary here, but might be handy if you try to implement a basic neural net with sigmoid activations): 

2. Cost function Usually, the logistic cost function is defined as a log cost in the following way (vectorized): $ \frac{1}{m} (-(y^T \log{(\phi(X\theta))})-(1-y^T)(\log{(1 - \phi(X\theta))}) + \frac{\lambda}{2m} \theta^{1T}\theta $ where $\phi(z)$ is the logistic (sigmoid) function, $\theta$ is the full parameter vector (including bias weight), $\theta^1$ is parameter vector with $\theta_1=0$ (by convention, bias is not regularized) and $\lambda$ is the regularization parameter. What I really don't understand is the part where you multiply . Assuming is your label vector $y$, why are you multiplying it with your before applying the sigmoid function? And why do you need to split the cost function into zeros and ones and calculate losses for either sample separately? I think the problem in your code really lies in this part: you must be erroneously multiplying $y$ with $X\theta$ before applying $\phi(.)$. Also, this bit here: . So is your bias parameter, right? Why are you adding it to every element of $X\theta$? It shouldn't be added - it should be the first element of the vector $X\theta$. Yes, you don't regularize it, but you need to use in the "prediction" part of the loss function. In your code, I also see the cost function as being overly complicated. Here's what I would try: 

Machine learning is the process of automatically discovering (inductively) the formula/rule that models a certain random variable. If you can discover the rule by yourself, you don't need the machine to aid you. You (or your boss), as the domain expert, has written a rule to model the target class. As long as this rule is deemed good enough for the business, there's no need to spend resources to generate another model. 

Note the true negatives are useless - we don't care if the state stays in the same spot (below/above threshold). Therefore, the accuracy is NOT important for this system. Do not try to minimize the wrong metric. This extends to "do not use ROC/AUC" (in your problem). You can use a PR curve, but be careful with those (no interpolation, no AUC PR, these are wrong/useless). Basic metrics that you could use are F-score, precision and recall, or a re-weighting of those (if the false alarm is less important than a missed alarm, for example). We also would like to deal with a huge "lag", trying to predict many time periods before (predicting just before the change is less useful). For this reason, I'd suggest investigating models other than MLP, notably those based on recurrent neural networks, such as LSTM. Also consider using time series prediction instead of classification - the literature on the subject is extensive and matches your problem really well. 

I've recently started learning to work with and have just come across this peculiar result. I used the dataset available in to try different models and estimation methods. When I tested a Support Vector Machine model on the data, I found out there are two different classes in for SVM classification: and , where the former uses one-against-one approach and the other uses one-against-rest approach. I didn't know what effect that could have on the results, so I tried both. I did a Monte Carlo-style estimation where I ran both models 500 times, each time splitting the sample randomly into 60% training and 40% test and calculating the error of the prediction on the test set. The regular SVC estimator produced the following histogram of errors: 

What could account for such a stark difference? Why does the linear model have such higher accuracy most of the time? And, relatedly, what could be causing the stark polarization in the results? Either an accuracy close to 1 or an accuracy close to 0, nothing in between. For comparison, a decision tree classification produced a much more normally distributed error rate with an accuracy of around .85. 

What you have here is some data that's too specific. I don't know if there's a widely accepted name for that, but I've seen it be called "leaking" or "cheating" data. Using this kind of data is potentially dangerous for the algorithm, as you have seen in your experiments. Here's a quick example: I want to predict whether a costumer prefers to wear white socks or black socks. I have a lot of information from this costumer, like the city he lives or how many TVs he owns (let's pretend there's some relation with preferred sock color). For some reason, the people handling the interview I use as data sorted the 200 interview responses by class - the first 120 responses are for white socks, and the next 80 are for black socks. Each response has an identifier from 1 to 200. Now I run this data through a decision tree learner. It comes up with a stump (a tree with only a single node) containing the rule: