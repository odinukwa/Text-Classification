Every rule, every process, every pattern that is taught in programming courses is an effort to try to "institutionalize" common sense. If all of your developers have perfect common sense at all times and are clear-headed and insightful, then you don't need to follow anybody's rules, processes or patterns. However, as the saying goes: "Common sense ain't" - Meaning that people in fact often don't have perfect common sense and won't naturally do everything that they need to do to keep themselves out of trouble both now, and even more especially into the future. Normalizing your database is very much a common practice in organizations with professional data modellers and developers who understand the importance of getting your database schema right. It does not take a lot of time to normalize your database. In fact, most experienced data modellers do it naturally as they build out their models while they're still in the whiteboard phase. Taking the (very little) time it takes to ensure that your logical schema is normalized before you start building, and taking the time to understand precisely why you are denormalizing in your physical schema, and what the impact of that redundancy will be on your code and business is an important part of a professional development process. 

One of the realities of a solution like this is that you will have to come back to build new code-based rules from time to time as your organization's payroll policies change. Don't be discouraged by this. Even if you had a fully table-driven rules engine, changing payroll policies would entail making changes to the rules data. Business changes entail system changes - that's a fact of life. 

Essentially every time there is a write into the main table, the new main table values are also recorded in the audit table, with the extra columns filled out. In the case of a delete, use the last known values. Sometimes the audit record is written automatically by a trigger, sometimes people write application logic to record this information. 

The optimal way to handle it is to table-drive the things. If you have 300 of them now, what are the chances that you're going to have 301 tomorrow? Instead of making frequent schema changes, just keep a table of and use a many-to-many relationship to track which things your objects own. Here are both the conceptual view, and the logical view, which uses an intersection table to persist the many-to-many relationship values. 

If you have something based on these columns then you have to have all the combinations of from / to units of measure in both directions in order to look up a conversion factor. You could also, alternatively, only have from non-default units of measure to the standard unit of measure for each . This would mean less data in the table, but you'd have to have conditional logic to go from and / or to non-standard units of measure. In the worst case scenario, you might have to convert from units in A (non-standard) to B (standard) and again from B (standard) to C (non-standard). Which way you go depends on your relative tolerance for more data to maintain or more code complexity. 

For practical purposes, if you're implementing a many-to-many relationship with an intersection table - which is by far the most common way to do it, then you will in effect have a "zero or more to zero or more" relationship. Relational databases don't have declarative referential constraints that enforce the child end of a one to many relationship. In other words, your table can't have a constraint (imposed by DRI) that enforces 0, 1 or many owners. DRI works on the table with the foreign key, not the parent (referenced) table. This means that unless you implement some additional procedural logic in your application, you wouldn't be able to enforce 1 or more, so you in effect get 0, 1 or more to each of and from the intersection (). Implementing a many-to-many relationship in this way is consistent with correct normalization. 

Notice in this second alternative is recognized as an entity type. It is an intersection between and the generic line item type: . A SKU is a supertype entity that could be a part, a consumable or some kind of labour. You don't need to have a logical supertype for service line item types, but a lot of systems would be modeled this way because it makes the transactional detail much simpler. This second model introduces abstract entities over and above the concrete entities of the first model. Introduction of abstractions like this is one of the things that tends to happen as you move from an initial logical model, based mostly on tangible things to a physical model. As you gain experience with data modeling, you'll get good instincts for moving past the conceptual/logical model stage directly to a well structured physical model. 

Would it not solve your problem to redefine the table such that it contains either the grade of a course or the level of the course (which ever applies)? The rows in could contain values such as: 

Note that the has been normalised up to the level, but has been left at the item level, since you might want to allow a single order to contain a mixture of rental periods. This design allows a customer to hire a piece of equipment more than once on a given date but it prevents hiring the same equipment more than once on the same order - which is what you would expect. 

The short answer is "no." Entity relationship diagrams can be used to show constraints based on the relationships between entities. However, the particular constraint you want to show (that bookings do not overlap in time) is a constraint between rows in the same table, not between two or more tables. The only single table constraint that can be illustrated in most common entity relationship diagrams is uniqueness, since ER diagrams can illustrate a primary key. 

Taking your form snapshot as a guide, I've divided up the elements of your form into entities which I've colour coded: 

You could simplify your design and make it more powerful by keeping all of the opening times in a single table which includes a date range and a priority code for resolving conflicting records. It would look something like this: 

What you're thinking about is either "row modelling" or "entity-attribute-value" (EAV) - See Wikipedia - either of which is a common subject for religious wars. I won't get too far into the pros vs. cons. If you read up on EAV and row modeling you'll see lots of discussion on that. What I will say is that you should model all of the common predicates using properly normalized tables and rely on EAV (or row modelling) only for the customizable columns (if at all). This is how many highly customizable packaged solutions deal with allowing users to add their own attributes to existing objects. It isn't clean, easy or efficient, but it is probably the best compromise for allowing this particular type of customization when you have no way of reasonably predicting what the customized columns might be at design time. 

An intersection table such as you've described is the standard way to implement a many to many relationship in a physical relational database. The intersection table has foreign keys to each of the parent tables that it's joining. These may be simple or compound foreign keys. That doesn't matter. The primary key of the intersection table is the combination of the two foreign keys to the parent tables. For this reason, the primary key of an intersection table is always a compound foreign key. 

Do not store dates in a separate table unless there is something about a particular date that has additional information that you need to store. In general the date is a simple attribute of some other entity, it isn't an entity in itself unless you have a very peculiar application. One exception to this might be a star schema data warehouse where there is a time dimension table that might use date as the primary key and have attributes that describe the way that date might roll up - assuming that some roll ups might not be trivial to calculate (like fiscal reporting period or some such). You can index a date column just as easily as an integer column, so there is nothing to be gained by abstracting out a date so that you can give it an integer alias value. You can also put a date column in a WHERE clause so searching for records by date is not at all onerous to do. 

To achieve multi-tennancy you should limit the number of relationships you have to the factor that determines the tennant. For example, each of , and have a foreign key to . However, these child tables are also interrelated. Is it necessary for to have a ? You can get to the school a staff member works for by following the foreign key to . Similarly, you have grand-child tables ( and ) which are relating tables in curious ways. What has "months indicating duration of the course" to do with which staff member is teaching the course? Is it really necessary for to relate to ? Also, you appear to have repeating groups defined in your and tables. I believe you may have a normalization issue. By over-relating tables you create cascade cycles. Consider the following ERD as an alternative: