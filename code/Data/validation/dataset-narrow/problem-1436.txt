This is why I make components pure data and constrain logic to systems. Basically, each system has a "mask" that states its required component types. For example, the rendering system requires both a position component and a sprite component. Each entity is also associated with a mask that states the component types it has. When the rendering system is invoked, it only loops through the entities that have masks that are compatible with its mask. 

Just give the sprite component a depth field. There should be a central renderer that knows how to render a sprite with a given depth, and the rendering systems should just issue commands to the central renderer, not straight to your graphics library. 

I'm making a tank game that will have multiple tanks. I want to be able to define the weapon placements using bones that I can add right inside the modelling program (Blender to be exact). All tanks will have a bone called Body and a bone called Turret, and then names like Cannon0 and PickupGun for where the shots will be fired from that are attached to the Turret bone. Is there some way to find the absolute end position of a bone that I choose by name? 

A cubic Bezier surface, then, would be defined as a 16-point patch. Note that only 2 of the control points for our Bezier curve patch will coincide with the actual geometry, and in the same way only 4 of the control points of the Bezier surface patch will. In terms of draw call submission, the only difference is that, for example, in OpenGL one must specify the number of control points per patch and also draw with the mode. 

The edges of a graph are implicit; they are defined by each node's adjacency list. If node A has both node B and C registered as adjacent nodes, but only node B has A registered as an adjacent node, then the edge between nodes A and C is implicitly directed towards C. 

Depending on what you want, drawing both backfaces and frontfaces might be overkill, but it is useful for effects like opacity depending on thickness, refraction, and subsurface scattering. You can safely draw all the fully opaque objects in one pass, because the depth buffer will take care of any overlap. To sort the convex parts, you can use the GJK distance algorithm to calculate the distance from the part to the near camera plane. This is embarrassingly parallel, so you can easily exploit multiple threads to do it quickly. 

When distributing my game, I am going to compile all the resources and level data into a single binary package file, which is loaded from by the game. Do I have to worry about my machine being a different endian-ness than the end-user's, or are discrepancies here obsolete? If this is going to be a problem, what is the best way to circumvent or solve it? 

UPDATE: After reading a bit into asynchronous buffer transfers, I get the idea that the way the graphics driver can automatically page memory in and out is suboptimal. Does this also hold true for CPU resources; i.e., is it better to always manually manage when a given resource is to be loaded and unloaded even in the presence of virtual memory? 

It depends on what kind of terrain you have. Normally, terrain is stored as a grid of height values, which eliminates the possibility of having caves, overhangs, or vertical cliffs. This terrain is usually stored as a height map, which is an image that contains the height data, instead of as a model. Collision detection on a heightmap is usually done by calculating the height (Y) at a certain position (XZ) using interpolation and comparing that to the height of an object or point. If your terrain is just another model, you'll have to decompose it into a series of convex hulls and use a 3D collision algorithm like Minkowski subtraction or the separating-axis theorem, exactly the same way you would handle any other 3D collisions. For simpler games with only basic collision detection needs, I'm sure you can get away with raycasting, which is a relatively simple operation, to check for collisions in a given direction. 

My favourite toolkit so far is GLFW. It's simple to use while fully featured and integrates well with OpenGL. GLEW is a must for anything complex, as it will allow you to easily use extensions. I'm not sure if this is a proper question, but I'll answer it anyways :) 

The tessellation evaluation shader ends up being yet another shader that can access any control point within the current patch. It is also where we do our vertex transformation and finally output the to be rasterized. Geometry shaders and fragment shaders are agnostic of whether tessellation has been used, so I won't go over examples for them. Otherwise, the tessellation pipeline is now complete. The built-in variable probably demands some explanation. Basically, it provides the coordinates in what I call the "tessellation basis space". This is different for each type of tessellation. With isolines, the component specifies the distance along the line from 0-1 and the component identifies which line it is. With quads, the and components specify where on the quad the vertex is, similar to UV coordinates, again within the 0-1 range for both. With triangles, the , , and components are the barycentric coordinates of the vertex. All that means is that if you multiply each component by its corresponding control point and add them together and divide by the sum of the coordinates, you will get the tessellated vertex. A great property of barycentric coordinates is that they work well with homogeneous coordinates; you do not have to then divide by their sum as the component will take care of that. In the case of evaluating a Bezier (or any other parametric) curve or surface, the components are used as the parameters. In the case of doing displacement mapping, the components are used for sampling the displacement map along with the UV coordinates. Typically these two techniques are used together along with dynamic tessellation levels to provide a very rich automatic geometry LOD system. 

To put it simply - winding order. Winding order is the way you define which side of a triangle is the front and which side is the back. For example, OpenGL's default winding order is counter-clockwise, which means the triangle on the left is pointed out of the screen and the triangle on the right is pointed into the screen. If you had back-face culling turned on you would not be able to see the right-hand triangle. 

Probably the most efficient method would be to draw all the sprites using the same texture at once. You can also save some time transferring memory to the GPU by using a geometry shader or instancing that only takes inputs like position, scale, and orientation, and builds the sprite triangles/quads from that. You can preserve sprite layering by using depth buffering, which is arguably a better method all-around than temporal layering. 

A minimal color scheme can look great, and your color choices themselves are perfectly fine. However, you should make the graphics look sharper and cleaner. They're upscaled, which looks pretty bad. Create your graphics at a higher resolution than you need and then shrink them based on the device resolution. In terms of making a UI more interesting, you should try and juice it up. Juice is anything that you add to a game to make it look or sound more fun while never changing the core gameplay. To explain better, watch a quick presentation on juice. Some easy ways of doing this are adding sound effects, screen shaking, and particles when you tap something. You'll make a user feel like they are a god in your little world by turning their finger into a ten ton pillar. 

You can add this kind of behavior to any path-finding algorithm simply by adjusting the heuristic function. A typical heuristic for path-finding is just based on distance to the goal, however, you can factor in an estimate of how dangerous a given spot is: 

I actually wrote an article on a simple algorithm I designed a little while ago to handle this. $URL$ Basically it's a quicksort (or mergesort) that uses a collision detection as its comparison function. If a convex object A's volume created by sweeping it away from the camera plane intersects another convex object B, A is in front of B (and thus the comparison function returns that A is greater than B). In terms of implementation, I'd use a GJK raycast test. 

The thing to realize when trying to design a rendering architecture is that OpenGL (and GPU programming in general) is all about registers and state. You have to bind buffers and textures and then issue draw calls, not issue draw calls that come with data. Although this seems limiting, it's very efficient. Because GPUs often offload operations like texture filtering to dedicated silicon (like texture units), these operations are very fast. The downside is that there are a limited number of specialized units, and they are unable to access just anything in memory. Similar to CPUs, GPUs work best when their memory is in a quick register or cache, and binding operations allow the programmer to guarantee that a variable, buffer, or texture is loaded into a register or assigned to a piece of dedicated silicon. Many operations that deal with binding are very expensive. They may need to flush the pipeline and DMA data from system memory to video memory. In particular, texture and shader binding are very expensive operations. To make sure that you aren't wasting too much time, you want to ensure that you are binding resources as little as possible. The best way to do this is to define a high-level rendering system that transforms a list of renderables into a sequence of commands (that are implemented as OpenGL function calls). A renderable associates various pieces of rendering data to represent elements such as objects and lights, while a command maps to one or more related OpenGL function calls. Internally, this system is essentially just a big sorting algorithm. Renderables are sorted according to their most "expensive" pieces of rendering data. Primarily, renderables would be sorted by texture first and shader next, while uniform changing is not a big priority for sorting. You can think of it as organizing the renderables into a tree, where the leaves are draw calls and the nodes are bindings. You can then walk the "tree" and produce the commands. As an example, here's an unsorted list of renderables: 

I'm using OpenGL in both my Core and Game parts, but as soon as I try to call an OpenGL function in my Core library the application crashes. I assume this is because the static library is not being linked with OpenGL. The build process goes like this: 

If you want to do it with a single texture, the easiest thing to do is UV map your sky cube model to that texture layout. Then, you can use a normal texturing shader. 

This concept is reified in Jason Gregory's Game Engine Architecture under Section 7.4. if you're interested in reading into it further. 

OpenGL and Direct3D are what you can call low-level graphics libraries. They provide a very thin layer over manually sending data to the GPU, and are the basis of almost all other 3D graphics libraries. Libraries that make use of OpenGL and Direct3D are more commonly considered to be "rendering engines", as they abstract away much of the code that interfaces with the GPU. Some notable examples of these rendering engines are: 

The only tricky part of using a directional light is that it usually cannot shadow the whole scene at once. Instead, you have to use techniques such as cascaded shadow maps (CSM). 

Yes, you should use for things like camera movement. Why? Because it doesn't matter what the mouse's factory DPI is. Pretty much all games give the user a "mouse sensitivity" setting, where they can tweak a custom DPI value (they don't need to know it's the DPI!) to fit their preferences. When it comes to GUI, you should still use the data. This will let the user use an in-game cursor that acts in the exact same way to their cursor outside of the game. As far as supporting multiple resolutions goes, you can just divide the cursor position by the real screen resolution (and maybe some more transformations) to get normalized coordinates that work regardless of resolution. A good, adaptive UI should already be designed using normalized coordinates so that any resolution is supported flawlessly. 

Calculating the trajectory of a projectile can be done in two ways: analytically and numerically. Analytical calculation is where you integrate position with respect to time and get equations such as . You can use these equations to solve for time of impact, distance traveled, maximum height, etc. or just find the displacement at any given time. Numerical calculation is much better suited for games. Your standard simple little physics simulation using Euler is an example of a numerical integrator: