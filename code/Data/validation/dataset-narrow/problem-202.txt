Adding to @Surfer513 4) Policy-Based Management policy either to enforce the simple recovery model, or at most let you know when a DB is not Although I favor setting model to simple this does not prevent the T-SQL command from being used and setting it to something else. You can use a policy to evaluate if the recovery model is not Simple and opt to have the policy change it for you. This MSSQLTip.com article is on checking for Full, but you can easily just have yours check for Simple. You can also throw in a check to see if a backup has ever occured on the database too. 

So I go into the text box properties and find the Fill page, I will set the "Fill Color" to this expression: 

The previous setting is retained during upgrade as stated above. Now I would like to point out the fact that other links provided by folks state that SQL Server 7.0 is when torn page detection was available. Which as stated in those articles is true, however it is proven many times over that Microsoft documentation should not be held as truth in all circumstances. There are many where they are wrong. So with that said how can you determine which answer is acceptable? We all provided documentation by Microsoft to support our answer. As well note that torn page detection is on the depreciation list as of SQL Server 2012, so what is the concern with how it was set on your databases to begin with. If I saw it set to anything other than CHECKSUM I immediately change it and move on to other more important task. I have no concern on how a bad configuration was put in place it is more important to correct it and then ensure those who have permissions to change it are informed of why that configuration item should not be changed to anything else. Just my $0.02 

If you happen to be remote you can try to use this command but it will not work if proper access is not granted: To connect to the instances you find with SSMS if you see the services listed as this is a default instance and if you are on the physical server you can use the server name, simply put a period (.), or . If you find a named instance it will be noted as , as on my machine I have a named instance of so I would put since I am on my machine. If I had this on a server and was connecting remotely it would be the actual server name instead of . 

I would first question why they need direct access to the database. You might ask your manager or legal department if the security policy for the company allows granting this type of access. Is it really needed if they are just going to execute a stored procedure on a regular basis. As you stated this is going to be on a regular basis I would setup an SSIS package to export the information to a file. Then either have the package email the file (if not to large) or put it on a UNC directory for them to come and download themselves. If they want to determine when the data is pulled down setup a script for them to execute outside of SQL Server that simply runs the package. 

I will go ahead and let you know something that I found out the hard way. Using SMO you can connect to SQL Server 2000 up to SQL Server 2014, however, each version of SQL Server can vary on where particular properties are located. I had a learning conversation with Jonathan Kehayias on Twitter one afternoon learning this but I cannot recall what property I was looking for. In these situations the .NET class library on MSDN for SMO is your friend because it can help guide you to properties you are looking for. 

You can use variables/parameters in your SSIS package to dynamically build your queries, probably the easiest method. It might be tedious to get setup based on how many queries you have used everywhere in your package. Just have something like an "User::Environment" variable that holds either: , or . You then join that into your variable holding your query and then as you change that value it would query the appropriate table. You could base it on the server you are connecting to as well, but would be the same premise of having to configure your queries dynamically in a variable + expression setup. I would probably use above method over validating against server I'm connecting to, just to have more precise control. 

Expressions: Subject - MessageSource - (line breaks added for readability) Now what occurred after I forced an error: 

Adventureworks database is already setup with flaws that provide good practice. There is also a BI database offered as well by MS, search for "Contoso BI demo". This database is only offered as a backup file which provides you practice for one vital task that every DBA should know: restore. There are a ton of data sets on the world wide web that offer practice on building your own "small" database. I am doing this now to practice design, scripting, and working with SSRS. 

This is a not very intuitive and I was never able to find anything concrete on the explanation [e.g. no exact BOL or white paper was found]. The syntax error in the SQL Agent job is a T-SQL syntax error based on User Tokens. So that basically means that a PowerShell Sub-Expression Operator is treated as a Token to SQL Server Agent. So in PowerShell this appears to be treated as reserved character sequence for SQL Server Agent. So SQL Agent would be looking for something like this T-SQL example from the BOL article referenced: . So in my script when it reached , the "$c.DriveLetter" is not one of the tokens allowed. The workaround to this is simply to not use sub-expression statements in PowerShell. I would make the adjustments in my script so this: 

You can find a more information on Kerberos from the CSS team here. As well Microsoft released a tool that can help you identify Kerberos issues, download that here. To troubleshoot, check your error log that shows the startup of your instance you changed the service account to that new domain. In a perfect world you will see a message like this: 

A full backup is everything... SQL Server backups will only contain the actual data in the database. The size of the physical file is the allocated size that SQL Server has for the file, including the free space. It does however also keep up with the initial size of your files for the database in the backup as well. So when you restore the database the files will be created and attempt to allocate the same amount of space. 

The expression for your and are a bit more lengthy since you have to check each value for each category, but it should look something like: 

Create a Scheduled Task on that server that contains a PowerShell script. The first part of that script contains a that would iterate over each instance and call the backup job on each instance. Use a would allow all the jobs to start at one time. You would then need to script out a while loop to wait and see that all jobs have completed. Then the end of that PowerShell script just call the code you need to run. Pick an instance and use it as a "central" location to actually run the backups against each instance. You could utilize linked servers to remotely call Ola' scripts. This would offer a bit more control and ease of management because you would know once the command finished the backup for that instance are complete. Your last step would just be your PowerShell script. Leave each backup job on the given instance. Add a scheduled task that calls your PowerShell script, but at the start of that script check to make sure a recent backup has been done. If not, just stop the script. Schedule that task to run between an the average end time of the backup jobs until a given a few hours later. This would ensure at some point it will find that the backups on all instances are current, then execute your code. 

Microsoft provided, for free, SQL Server Management Studio (SSMS) starting with SQL Server 2005. Is it not exactly like SqlYog but there are similarities. With regards to table space I would direct you to the Standard Reports that were added to SSMS after SQL 2005 SP2. They provided a more eye friendly look at allocation of space, index usage, performance information, etc. 

You can use the Import/Export wizard with SQL Server. I came across a very good write-up (nice pretty pictures) here on it. 

You might be able to pull off setting up granular control by granting access to the stored procedures used to create maintenance plans (at the bottom of the article I linked above): 

This is fin as long as you are doing regular full and log backups to keep the log file size under control. There are plenty of articles and questions on here regarding log management. 

You can utilize and the property to find the exact server name. It has the same effect as doing the search with but is less typing. 

EDIT: One additional requirement that might affect design: this package could will be called at minimum every 15 minutes. Example: 

Do your onsite backups to disk. Setup blob storage on Azure (or S3 with Amazon) and then script copying that file to that storage. You would then have Veritas backup that file as well from the local disk to tape. If you happen to be on SQL Server 2012 or higher you can backup directly to Azure storage. The Azure storage, you can select geo-redundant service and this will allow your backup to be duplicated between Microsoft data centers. You could consider actually dropping doing backups to Veritas with this option, but an extra copy doesn't hurt to have locally. 

The closest I can find on the RM is from the PSS team here: How It Works: What are the RING_BUFFER_RESOURCE_MONITOR telling me? Which specific to your question on there is no specific documentation that answers this...that I can find. The article references actually shows that there is a startup trace flag that can be used () to narrow down what is triggering RM. I think the main bit of text from the article above would be this: 

This is considering when you run a command in PowerShell will you process any amount of data in PowerShell or do it all in SQL Server. If you do any kind of piping with the data consider how it will be processed. Certain things (piping to an executable) can require all the data to be cached in memory before it moves down the pipe to the next command. 

The is to use an unattended installation script that simply is called from each server, where the media for the installation is on a central directory on your network, that is accessible from each server. You will have to run the script in an elevated mode, which this is fairly easy in PowerShell. You will need to use the CLI to extract the hotfix, I found that here. Then the CLI options for the SQL Server patch can be found here. The following commands are what I used to apply the CU6 patch to the local SQL Server 2012 SP2 instance on my laptop. I changed to the directory I downloaded the hotfix to: 

In SQL Server 2012 the event XML output of Extended Events changed dramatically; enough where it broke any code used for SQL Server 2008. A write of this can be found by Jonathan Kehayias: Extended Events Changes in SQL Server 2012 – Event XML for XML data elements You will need to change the CTE for to the following: 

The database on the end of log shipping has to be in to accept the log backups being shipped over. A database involved in AlwaysOn Availability Group has to be removed from the group in order to be restored or set to any restoring state. So setting up log shipping to that database would require it to no longer being replicated. I would go back to the stockholders and determine their full needs for HA as it seems they left stuff out of their initial request. 

So you basically will see every query plan currently in cache get marked for recompile, this would include stored procedures as noted above. So your initial performance improvement is only being seen because the plan for that stored procedure was recompiled. Then after it ran for some time the plan was not sufficient. Instead of going back and forth between compatibility levels you should try optimizing the procedure so the query plan generated is more optimal for the range of parameter values being passed. Quick alternative, if code changes cannot be done immediately, might be to just schedule a job during the heavy use to initially mark the procedure for recompile. 

I did not actually perform this install, nor am I responsible for fixing issues on this particular cluster. I just happen to be checking it and found some things that puzzle me. My point in this is trying to understand why it is working, at least to some respect. OS: Window Server 2003 Enterprise (64-bit) SQL: SQL Server 2005 Enterprise (64-bit) Started with a 2 node active/active cluster: Server1 running Instance1, then Server2 running Instance2. Instance 1 and Instance2 are at the service pack 3 build number for SQL Server 2005 (+ a few hotfixes). I believe it is 9.00.4053. Two new servers are built to replace Server1 and Server2. So the servers were built and added into the cluster, Server3 and Server4. Since I did not do the installation I am assuming to a point that the person followed the steps to add a new SQL Server node as described here in BOL: $URL$ I found that Instance1 and Instance2 are now both running on Server3. However the build number of the instances now show to be 9.00.1399, which is an installation unpatched. The instances are active and still running on this RTM build of SQL Server within the cluster. ????? My thought process here is you cannot take a backup of master database and restore it to an instance that is at a lower build number than what the backup was taken from. So if you just go on that point, how can a cluster instance failover to a node that is not at the same build number? Why would SQL Server (or Microsoft) even allow it to do this? Also the last step in the BOL article linked above has "All nodes of a failover cluster instance must be at the same version level." I cannot find anything that states what happens if you are not. The only thing I have found on Instance2 is the SQL Agent jobs don't seem to be working anymore and when the databases are being brought online there is a stack dump from something like the IO listener (I don't recall the exact message showing). I have also seen a few messages referencing IO write issues. The databases themselves are online and appears the applications are functioning as desired. They have been running in this manner for the past week or so. Any thoughts? 

Well for security reason you should not be letting anyone use the "sa" account. If you do then obviously changing that password will break whatever is using it. I try to set this password to a random password, rename it (only SQL 2005 and above allow this), and then disable it. I have not found anything so far that explicitly requires the "sa" account to function, my own admin account suffices. With regards to the service account this should not cause any problems. Again as long as no other resource or service is using the same account. The only backup plan I can suggest is having the old password handy to reset the accounts back to in the event something breaks. 

You brought up the above statement in your comments to the other answers. This is something you have to handle on your firewall. The client chooses the port it wants to be communicated back on. As stated later down in the article above your firewall rules have to allow the dynamic allocation to occur. I don't work with the application side but there might be something you do to force the port it wants to talk on. 

However also noting that the error message , the 5 normally indicates permissions issue with the service account running the database engine. If the full message was provided you would see something like