There may be other parts worth attention but these three were ones that stood out for me. In conclusion, I would just like to add that the rewrite in the first bullet point is definitely going to be an improvement to your query as posted in your question. The other two I am less certain about, so I would advise you to try/test them independently. 

At the outermost level, you are returning the average values along with . If you want averages per , then you need to group your rows by . You are doing that in the nested queries but not in the main one. So, after just add . But if you want the average value across the entire row set, remove from the (main) select list. 

While is a variable, it is a table variable, not an array. You access data stored in a table variable the same way you would access data in a conventional table, i.e. using a query. Now when you want to retrieve data of a table's specific row, you need a way of referencing that row. Often you use some sort of an ID column. In this case you could add such column in your declaration: 

For each row the OUTER APPLY subquery returns from the preceding row (or NULL if it is the first row). The WHERE clause simply filters out the rows where the match has a different name or is a NULL. 

that is, you need to concatenate the name of , not the value. That way the dynamic query will be storing the result of into . Note, though, that you may also need to declare the variable explicitly before the dynamic query. Otherwise the dynamic query will probably implicitly declare the variable at its own, nested, level, and the variable will go out of scope once the query is completed, and so the reference further in your query will still evaluate to null. 

The CASE expression is the column of your query's derived table. It is used in the COUNT function as well as in the WHERE clause (the IN predicate). Usually such repetition of code is eliminated by nesting. But nesting cannot be used here because of the MySQL limitation mentioned at the beginning of this post. So, repetition of code is the price you have to pay to work around it. Luckily, there is not much of it in this specific case. 

Remember to double each quotation mark (apostrophe) inside the script. One other important change you will likely need to make is to add a WITH RESULT SETS clause to the EXECUTE statement to describe the result set, so that OPENQUERY can process the output correctly for you. When describing the result set, you will likely just repeat the same type for and as defined for them in the metadata table. For the example below I am assuming the type to be in both cases. And as for the column, I believe would work well there. So, the modified EXECUTE statement would look like this: 

So much for the current time in milliseconds. As for adding 5 minutes to it, you can add 300,000 to the last result: 

The fundamental tip on how to go about this: split this into several smaller problems. Assignment questions are certainly meant to be complex, because the intention is to test your knowledge on a number of issues. As a learner, however, you may find them much easier to tackle if you take on each issue separately. And I suggest you consider the following sub-issues in your complex problem. Inserted is a row set Some database products allow you to define per-row and per-statement triggers. SQL Server supports only per-statement ones. The implication is that the inserted pseudo-table can have more than one row (because it is possible to insert more than one row with a single INSERT statement). So, normally you will not use a scalar variable like @MemberId to pull a single value out of inserted and, probably, use it further in another query. That is iterative thinking. Instead, you should think in sets – whatever you meant to do with that single member ID should be done to every row at once, in a single statement. = rows are already in the table …but the transaction is not yet committed. The Transact-SQL syntax is a synonym for the standard SQL (which is accepted in Transact-SQL as well). The words "after insert" imply that at the time of the trigger's execution the inserted rows are already in the table on which the trigger is defined (in your case, dbo.Rental), even though the transaction is not committed yet. That is an important point to consider given the fact that one of your trigger's tasks is to check the number of books already being rented – and it is also a crucial point to keep in mind with regard to cancelling the insert. Acting on finding matches in a trigger Now that you know that inserted is a set and needs to be processed as such and that the new rows are already in the table as well as in inserted, how to put the two facts together? How to apply conditions to all rows at once and then use the results? One of the most common patterns in triggers is this: 

Note that if you are using SQL Server 2012 or later version, you can avoid the self-join entirely by using the LEAD function: 

The script may actually be further improved to allow you to pass the list without enclosing the items in apostrophes at all. The enclosing can again be done with the help of REPLACE: 

On the other hand, since the logic for cases when a value increases only slightly (not exceeding the reference value) is not defined, it is not clear whether either variation would produce the expected output for you. You may want to elaborate on that in your question so that you can get more options to choose from. 

Now in order to split the into three values based on the value of , you check that value when counting – like this: 

Populate it with pre-defined IDs for each type. For the purpose of this answer, let them match RDFozz's example: 1 for plants, 2 for animals, 3 for bacteria. Add a column to and make it non-nullable and a foreign key. 

I take this to mean that each company's nulls should be populated based on the values taken from the one populated entry that, as you say elsewhere in your post, each company has. So, in pseudo-code, your UPDATE statement would need to look something like this: 

For the DELETE query, take the final draft of the INSERT statement's SELECT query keeping only in the SELECT clause and removing the HAVING clause: 

The Common Table Expression returns rows supplied with row counts per every partition of . The main query only needs to filter on the condition that a row belongs to a specific category and that the corresponding row count is 1. Note also that if your design allows duplicate entries of , you'll need to replace with . (That may accordingly result in more than one row per in the output.) 

That will duplicate each source row but from each duplicate pair you will select only the entry where user_id is not the one specified as the argument: 

And if you decide to go with either Erwin's or Evan's solution, I believe a similar change will need to be added to it as well. 

I don't think you question is really about running totals. In this case you could just use an outer join between your "reference table of sequential dates" (called in the query below) and the table, group the result set by and count matching rows: 

Since the FIRST_VALUE function would return the result for every row of your table, it would return identical values for entries created in the same hour, thus giving you a lot of duplicates. It is to suppress those duplicates that the above query is using DISTINCT. 

The original row set is cross joined with two inline views, one that represents the required prefixes () and one that specifies how many spaces there may be between the prefix and the subsequent number (). This way the query will probe for all combinations of prefixes and numbers of delimiting spaces (, , , ). The string is searched for each combination, one after one, to calculate the starting position of the prefix. It uses PATINDEX, which, for each prefix and space number, builds its own search pattern. As per the assumptions above, PATINDEX expects a space before the prefix and at least one digit after the prefix and, possibly, a space. So, if, for instance, the current prefix is and the current expected number of spaces after it is 1, the pattern would be . To make sure there is a space before the prefix, a space is added at the beginning of . That trick accomplishes two things: it helps to find the prefix if it is at the beginning of the string and it makes the resulting starting position accurate. The latter is important because technically the returned position would match not the prefix itself but the preceding space. So, to get the actual position of the prefix, we would need to increment the result by 1. However, because we had added a space at the beginning of the source, all the positions were shifted forward already. So, in the end, the result points exactly at the beginning of the prefix. One final touch here is that if the returned position is 0, it is transformed into NULL by NULLIF. That helps to avoid errors about negative length values passed to SUBSTRING later. The found position is then used in CHARINDEX to find the first comma in found after that position. Again, to make sure there is a comma, the character is appended to . The result constitutes the ending position of the sought item. Using the obtained starting and ending positions, the item is extracted from using the SUBSTRING function. Finally, in the SELECT clause, before actually returning the item, the query additionally checks if there is a space after the number, just in case the item was not properly delimited by a comma. By appending a space to the item, it uses the same trick as when searching for the comma position, also taking into account the length of the prefix and the expected number of spaces after the prefix to make sure the found space character will be the one after the number. In cases where the prefix is not found in the string, returns NULL. Such rows, however, are excluded from the output by the WHERE clause, which specifies that the starting position be not NULL. 

These five correlated subqueries in the SELECT clause are retrieving data from the same table based on the same condition: 

This is almost like a cross join with a single row, except in this case the final output will always have at least one row. If the subquery does not return any rows, its columns will be null in the output. 

The result will be a with the scale of 3. You can further cast it to if you need the result to be an actual integer type: 

In the line above, the result of is assigned to , then the entire variable assignment expression evaluates to the same value, which is eventually stored into the column. The variable assigned this way can then be used in other expressions that follow. Your update statement, therefore, could be rewritten e.g. like this: 

Within each group, use to put the master row after all its duplicates, and then to determine the order of the duplicate rows (assuming there can be more than one duplicate of the same master row). Those last two criteria are the same as in Kondybas's answer, by the way. The entire clause, therefore, would look like this: 

The total number of rows per is easy, you just need to use . As for the other column, then, assuming cannot be null, you need to count distinct values and compare the result to 1. To explain: if all names are identical, will return 1, otherwise it will return a different number. Thus, by comparing the result to 1 you will determine whether all names are unique or not. This is how I would implement it in SQL: 

If both tables consistently have null in and when contains a , then you can match the two columns sets using EXISTS and INTERSECT: 

Copy the contents of the output column and paste it into the query window where you are writing your UPDATE statement, at the appropriate position. You will only need to make minimal syntactical adjustments, like removing the comma after the final assignment. Even if I knew I might need to do this kind of data movement again in the future, I would still consider this approach first before actually going the dynamic SQL route. If you firmly believe you need a more convenient tool for this problem, then you can use the above statement as a starting point for your script. Browse the dynamic-sql questions on this site or on Stack Overflow to find more examples to help you with the final solution. 

Yes, it is possible to return both results without repeating the subquery by rewriting your query using an outer join. The following will return same results as your version: 

When a date is represented as 8 numerals without any delimiters, SQL Server will always try to interpret it as . That format has deliberately been chosen as locale- or culture-independent (being also one of ISO 8601 standard date formats, as a matter of fact), and there is no way to have the server interpret it as or in any other way. So, either rearrange the date parts of your string to make it : 

The above trigger will work exactly the same. On a different note, your UPDATE statement may have a flaw: as currently written, it will update the column in every row with the same row count – that of the category matching the inserted row. If the table has multiple rows, each for a different category, you probably need to introduce a filter on to your UPDATE statement, similar to the filter in the subquery, in order to update only the corresponding value: 

No WHERE clause here – it is replaced by the join to the table of queried attributes, and the total number of attributes required to match is derived from the same table instead of being hard-coded. This kind of problem is commonly known as relational division. It is discussed in detail in this article by Joe Celko: 

If it is required to store rows with the same set of FKs consecutively on disk (which is not unreasonable), you would probably prefer to do the conversion in a single statement sorting all the rows by the FKs. You can do that using a numbers table: