The mentioned block graphs in the question are distance-hereditary. A linear time algorithm for computing the diameter for distance-hereditary graphs is given in [1] (see Theorem 5). 

[1] Dragan, Feodor F. Dominating cliques in distance-hereditary graphs. Springer Berlin Heidelberg, 1994. 

In the edge-disjoint paths (EDP) problem, we are given an undirected graph $G$, and a set $\{ (s_i,t_i) \mid 1 \leq i \leq k \}$ of $k$ source-sink pairs. The objective is to maximize the number of pairs connected via edge-disjoint paths. In the bounded length edge disjoint paths (BLEDP) problem, we wish to route the maximum number of source-sink pairs such that each of the paths used is of length at most $L$, for some length bounded $L$ that is part of the input. Guruswami et al. [1] showed that BLEDP can be approximated in polynomial time within a factor of $O(\sqrt{m})$. They also showed this is optimal with a matching hardness result of $m^{1/2-\epsilon}$, for any $\epsilon > 0$. Do we have algorithms achieving better approximation ratios for BLEDP for some restricted graphs? For example, how about (undirected) planar graphs? 

The $k$-center problem is a clustering problem, in which we are given a complete undirected graph $G = (V,E)$ with a distance $d_{ij} \geq 0$ between each pair of vertices $i,j \in V$. The distances obey the triangle inequality and model similarity. We are also given an integer $k$. In the problem, we have to find $k$ clusters that group together the vertices that are most similar into clusters together. We choose a set $S \subseteq V, |S| = k$ of $k$ cluster centers. Each vertex will assign itself to the closest cluster center grouping the vertices into $k$ different clusters. The objective is to minimize the maximum distance of a vertex to its cluster center. So geometrically, we want to find the centers of $k$ different balls of the same radius $r$ that cover all points so that $r$ is as small as possible. The optimal algorithm is greedy, and also very simple and intuitive. We first pick a vertex $i \in V$ arbitrarily and put it in our set of $S$ cluster centers. We then pick the next cluster center such that it is as far away as possible from all the other cluster centers. So while $|S| < k$, we repeatedly find a vertex $j \in V$ for which the distance $d(j,S)$ is maximized and add it to $S$. Once $|S| = k$ we are done. The described algorithm is a $2$-approximation algorithm for the $k$-center problem. In fact, if there exists a $\rho$-approximation algorithm for the problem with $\rho < 2$ then $P = NP$. This can be shown easily with a reduction from the NP-complete dominating set problem by showing we can find a dominating set of size at most $k$ iff an instance of the $k$-center problem in which all distances are either 1 or 2 has optimal value 1. The algorithm and analysis is given by Gonzales, Clustering to minimize the maximum intercluster distance, 1985. Another variant of a $2$-approximation is given by Hochbaum and Shmoys, A best possible heuristic for the k-center problem, 1985. 

This reads like a claim that $f$ is a homogeneous polynomial iff $f=0$ is a union of hypersurfaces (through the origin). But $f(x,y,z)=x^2+y^2-z^2$ is a homogeneous polynomial for which $f=0$ is not a finite union of hypersurfaces. 

Yes, there is a known relativization barrier. It's given by $A:=TQBF$, because Emil Jeřábek (see comments) is right: the statement that $TQBF$ is $PSPACE$-complete under logspace many-one reductions is well known. 

This question is not research level, even so showing the equivalence of closure under kleene-star to the well known open problem L=NL was a nice challenge. Obviously $S\cap T$ and $S.T$ are in DLOGSPACE, if $S$ and $T$ are in DLOGSPACE. 

Before this question, my opinion was that Graph Isomorphism might be in P, i.e. that there is no evidence to believe that GI is not in P. So I asked myself what would count as evidence for me: If there were mature algorithms for $p$-group isomorphism that fully exploited the available structure of $p$-groups and still would have no hope to achieve polynomial runtime, then I would agree that GI is probably not in P. There are known algorithms that exploit the available structure like Isomorphism testing for $p$-groups. by O'Brien (1994), but I haven't read it in sufficient detail to judge whether it fully exploits the available structure, or whether there is any hope to improve this algorithm (without exploiting additional non-obvious structure of $p$-groups) to achieve polynomial runtime. But I knew that Dick Lipton called for action near the end of 2011 to clarify the computational complexity of the group isomorphism problem in general, and of the $p$-group isomorphism problem specifically. So I googled for 

Say we have a graph property which can be checked in nondeterministic polynomial time, and a proof in a weak formal system (say RCA0) that the property is minor closed. Can we say anything about the strength of a formal system, which is able to prove that a given finite set of excluded-minors characterises the given graph property? 

I don't see how you can say that SCP or any other problem has "worst-case asymptotic complexity", but surely an algorithm solving it can have. However, the problem is NP-complete, and the optimization version is NP-hard. For an exact algorithm, this is clearly a "theoretical limit". For an exact algorithm, see for example E. Balas and M. C. Carrera, A Dynamic Subgradient-Based Branch-and-Bound Procedure for Set Covering, Operations Research 44, 875-890, 1996. According to A. Caprara et al., Algorithms for the Set Covering Problem, 2000, this algorithm of Balas and Carrera is the fastest known exact algorithm. The result is a bit old already, so I'm not sure if there are newer and faster exact algorithms. There is a simple greedy polynomial time approximation algorithm with an approximation ratio of $H_n$, where $H_n$ is the $n$th harmonic number. Under reasonable assumptions, this is the best one can do, since if there exists a $c \ln n$-approximation algorithm for the (unweighted) set cover problem for some constant $c < 1$, then there is an $O(n^{O(\log \log n)})$-time deterministic algorithm for each NP-complete problem. I think the Wikipedia page gives the essential references. 

Kumar, P. S., & Madhavan, C. E. (1998). Minimal vertex separators of chordal graphs. Discrete Applied Mathematics, 89(1), 155-168 define chordal graphs where every minimal separator is of size exactly $k$ as $k$-separator-chordal graphs. They generalize $k$-trees. In a later paper, Kumar, P. S., & Madhavan, C. E. (2002). Clique tree generalization and new subclasses of chordal graphs. Discrete Applied Mathematics, 117(1), 109-131 the same authors discuss $k$-separator-chordal graphs more in a slightly different context. See Section 6.2 in the paper. 

Colin Cooper and Alan Frieze have a set of results in the context of random digraphs that might be of interest. They study the properties of a simple random walk on the random directed graph $D_{n,p}$ when $np=d \log n, d>1$. They have proved that: 

The wikipedia article states that the Logarithmic time hierarchy (LH) is equal to FO-uniform AC0. But since the natural way to think about LH is in terms of its levels (with DLogTime at the zeroth level, NLogTime and coNLogTime at the first level, and the log-time machines with (k-1)-alternations at the k-th level), it is natural to wonder whether there is a fine grained relation between the classes from the individual levels like (coNLogTime) with specific subclasses of AC0. 

It might be possible to keep the "$\mathbb{N} \to \mathbb{N}$" part, and replace the "(partial) functions" part with something else (I am thinking here of an analogy Fermions <-> "(partial) functions", Bosons <-> "something else"), but the Church-Turing thesis would probably still hold in such modified settings. 

If a massively online collaboration is set up, then it should try to focus on problems with a reasonable chance of success. The three classical construction problems of antiquity are known as "squaring the circle", "trisecting an angle", and "doubling a cube". Modern mathematics resolved all three, but much more important was the earlier Descartes revolution, which enabled mathematics to free itself from the mental prison of compass and straightedge constructions. Notice that the Greeks used compass and straightedge as a practical computational device, as witnessed by the efficient epicycle approximation scheme for celestial mechanics computations. Many conjectures and generalizations of solved conjectures from graph theory should be amenable to solutions by collaboration. However, typical experience with collaborations suggest that teams of 2-4 members are far more effective than significantly larger teams. An example of a very successful team in this area are N. Robertson, P.D. Seymour and R. Thomas, which attacked problems like the strong perfect graph conjecture, generalizations of the four color theorem, and graph minor related conjectures. The elapsed time between announcement of new results and their actual publication has been notoriously long, also for other teams of researchers in the same area, indicating that pure workload volume here is slowing things down, so that collaboration (which already happens) could be beneficial to speed things up. (I'm still curious when some of the announced generalizations of graph minor related results for directed graphs will be published, and whether the people who will finally publish them will be the ones which initially announced the results.) I currently try to understand the role of completeness of intuitionistic logic in practical applications of computer assisted proof refutation. But if you really plan to do proofs by massively online collaborations, then having a solid computer assisted proof refutation system in place might really be important. After all, if you don't know your collaborators sufficiently well, how will you be able to judge whether you can trust their contributions, without wasting a significant amount of time checking everything they did? (I have the impression that mathematicians are more used to proof refutation and enjoy its positive sides like direct personal feedback, while computer scientists show less routine with this sort of feedback.) Anyway, establishing that proof refutation is a natural part of collaboration when tackling difficult problems seems like a good idea to me. 

Output: The encoding of some kind of automaton (say, a Turing machine) which enumerates this set. If you want the shortest machine, then this is a witness variant of determining the Kolmogorov complexity of the input. This is intractable of course. But now suppose I make the following changes/relaxations to the problem: 

Consider the following problem: Input: An integer $n$ and a subset $S \subseteq \{0...n-1\}$ in some representation. 

Much ink has been spilled studying the theory surrounding computation by combinatorial circuits operating on bits or boolean values - with AND, OR and NOT gates (as those are enough to implement any boolean function). One can consider families of these for different input sizes as opposed to general Turing machines, and consider the languages they accept or functions they compute (if they have multiple outputs); one can reason regarding their size and depth as a function of the input length; define the classes of languages acceptable by them with certain restrictions; study their composition; etc. I've also been introduced to the Blum-Shub-Smale computational model, where the constituent operations of a program have elements of a field (e.g. the Rationals or the Reals) as inputs and outputs; and such a machine can be likened to a circuit if it has no loops or similar control structures. It's been the subject of much less theoretical literature, but I can still find tens of thousands of papers referring to the "BSS Model". And there are arithmetic circuits (with only + and * gates) which I vaguely recall from my Complexity Theory class as an undergrad (proofs about #P and PSPACE where boolean circuits get "arithmetized" - AND into * and OR into +, with the inputs still being boolean). But - what about circuits which operate on more complex units of data? That is, arrays of unbounded finite length - with the individual elements either all of some single type or from some vocabulary of types? Obviously, you might want to constrain the nodes in such circuits, as otherwise they would just be Turing machine or circuit families in themselves, but this is still a relevant kind of automaton to study these days - with programming models such as CUDA and OpenCL which apply these kinds of operations on very large amounts of data, controlled at the level of the entire kernels and their dependencies. So - have circuits-operating-on-array seen any theoretical study? I've failed to find any such work so far. Note: RAM/register machines in which registers or individual memory cells contain arrays would be a next best thing. 

In the connected dominating set problem (CDS) we are given an $n$-vertex undirected graph, and asked to find the smallest connected subset $S$ of vertices such that each vertex not in $S$ is adjacent to some vertex in $S$. If we drop the connectedness requirement (the usual dominating set problem (DS)), we have exact algorithms that are much faster than the naive brute force method. The trivial algorithm for CDS runs in $\Omega(2^n)$ time and enumerates all the subsets of vertices. To the best of my knowledge, the fastest known exact algorithm for CDS is the one given by Fomin, Grandoni and Kratsch running in $O(1.9407^n)$ time. It seems both intuitively true, and also generally accepted in the literature, that non-local problems (such as CDS) are harder to solve than local problems (such as DS). The reason seems to be that often exact algorithms are exploiting the local structure of the problem whereas say connectivity is a global property. Another non-local problem is TSP, for which as far as I know, the fastest known algorithm dates back to the sixties and runs in $\Omega(2^n)$ time. Say for TSP, we do have faster algorithms for special graph classes of course (cubic, bounded-degree, and so on). Also, for many non-local problems on graphs of bounded treewidth we have (I guess possibly even optimal under SETH) algorithms (see the work by Cygan et al). Similarly, I think we can get $O(c^{\sqrt{n}})$ algorithms for non-local problems on planar graphs using a result of Fomin and Thilikos bounding the treewidth of a planar graph, and showing how a tree decomposition of such width can be found in polynomial time. But what about general (undirected) graphs? It indeed appears that for many non-local problems the best known algorithms are still trivial. For Steiner tree (i.e. find a minimum size subtree of a given graph spanning a given subset of $k$ nodes), we have a $O(1.4143^n)$ time algorithm obtained by combining the $O((2+\epsilon)^kn^{O(1)}$ DP algorithm (for small $k$) with the trivial $O(2^{n-k}n^{O(1)})$ enumeration of Steiner nodes (for large $k$). To the best of my knowledge, finding a polynomial space algorithm faster than $2^n$ is still open.