possibly somewhat related is finding/adding a few edges to yield "good" hubs (hub sets that are small relative to the vertices they "reach"). 

("thinking outside the box"...) this is a somewhat contrived problem involving DFAs (have not seen it studied elsewhere) but manifests a theme in TCS that even many apparently "simple" computational objects (like DFAs) can have complex properties, also an aspect/theme embodied in Rices theorem. (in some ways the ultimate "complexity" is "undecidability" aka Turing completeness.) consider a family of DFAs and an "exponentiation" operator "↑". this operator takes a regular expression (RE) "x↑n" and repeats it $n$ times. ie for any RE $x$ and finite $n$, $x↑n$ is a RE (and therefore also a DFA). this operator is considered in various contexts and in some important problems eg one of the first problems proven EXPSPACE-complete.[1][2] now consider a family of DFAs $DFA_n$ which is built from a single RL expression $DFA'$ (not exactly/strictly a DFA) with embedded instances of the exponentiation operator in the form "↑n" where here "↑n" is a symbol (but otherwise $DFA'$ is a RL/DFA). then for every finite $n$, $DFA_n$, constructed by replacing/substituting the instances of "x↑n" in $DFA'$ by n repeated instances of x, is also a RL (and a DFA). (now as usual let $\Sigma$ be the language symbol set/alphabet.) claim: 

I'm posting a suitable answer that I found for this problem. The approach linked here does not exploit the fact that no $t$-tuples of elements of $\Omega$ are present in too many subsets $Y$ (for arbitrary $t$), but it does end up giving me a good enough bound anyway. 

So I am trying to figure out an upper bound on the probability of the following... This is a question related to a problem I am working on (not for a class, just for fun) Let $\Omega=\{X_{1},\dots,X_{n}\}$ where each $X_{i}$ is i.i.d Bernoulli with success probability 1/2. Let $Y=\{Y_{1},\dots,Y_{N}\}$ where $Y_{i}\subset\Omega$ and $|Y_{i}|=t$. Let $Z_{i}$ indicate the event that $\sum_{X\in Y_{i}}X\geq k$, where $k$ is some value close to $t$. Let $Z=\sum_{i=1}^{N}Z_{i}$ Say that $I_{r}= \{\{Y_{i},Y_{j}\}:|Y_{i}\cap Y_{j}|=r\}$, that is, $I_{r}$ is the set of all pairs from $Y$ that share $r$ elements from $X$. I am looking for an upper bound on $Pr[Z\leq\mathbb{E}[Z]-t]$. I've seen Janson's inequality and notice that it looks quite similar to this, but I'm not sure where to go from here. Any help or any references would be greatly appreciated. 

I don't think this is true as stated, since if $(u,v)$,$(u,w)$,$(v,w)$ is a triangle in $G$ then clearly there is no way to "two color" the corresponding hyper edges in $H$ no matter how many isomorphic copies of $G$ you make, yet, $G$ may still be $k$-colorable for $k>2$. Here is a different way that may work. Following your same set up, Let $e^{H}_{u,v}= {\{ x_{1,u},x_{1,v},x_{2,u},x_{2,v},\dots ,x_{k,u},x_{k,v} }\}$ and let $$E(H)= (\bigcup_{(u,v)\in E(G)} e^{H}_{u,v}) \cup {\{f_1,\dots ,f_n}\}$$ If $G$ is $k$-colorable, with $c:V \rightarrow [k]$ as the coloring, then for each $j\in[k]$, and for each vertex in $c(j)^{-1}$, color it's copy in $G_j$ green, and the rest of the vertices in that copy red. This is clearly a two coloring of $H$ since we have ensured that for every vertex, it is colored green in at least one copy $G_i$, and for any edge $(u,v)\in E(G)$, the color of $x_{j,u}$ is not the same as the color $x_{j,v}$ for some $j \in [k]$. Note that we didn't need to use the $f_i$'s here. For the other direction, this was the best start I could get... Assume that we have a $2$-coloring of $H$, by having the same element $y$ in each $f_i$ we know that for no two distinct vertices $u$ and $v$ did our $2$ coloring assign green to $x_{j,v}$ and red to $x_{j,u}$ for all $j\in[k]$. This is because we have to color $y$ red or green, and y appears in $f_u$ and $f_v$. This implies that if $x_{j,v}$ is colored say, red, for all $j$ and $x_{j,u}$ is also colored red for all $j$, then $u$ and $v$ are not adjacent (o.w. we couldn't color $e^{H}_{u,v}$). I am stuck here. Somehow you need to construct a $k$-coloring of $G$, but it's not clear how. I hope this helps (I would have left as a comment, but it's too long). 

there is some rough connection of this eg implications of worst case bounds in this paper A simple proof that AND-compression of NP-complete problems is hard (Dell), but am looking for more direct algorithm(s) and possibly more empirical approaches/ analysis, eg somewhat similar to strategies/ heuristics used in SAT solver implementations. (this problem has applications in eg automated theorem proving.) 

heres another angle based on some more search. a principle historical/emerging nexus/intersection between economics and computer science/complexity theory is computing Nash equilibria which are central to various economics models, where Daskalakis (collaborating with Papadimitriou) is a leading figure.[1][2][5] this overlap generally occurs through the field of game theory where [3] is a survey published in ACM and which serves as another key bridge between the fields. also Shoham cites [4] as a survey focusing on Nash equilibria and "Geared mostly towards economists, it includes ample background material on relevant concepts from complexity theory." [1] The Complexity of Computing a Nash Equilibrium Constantinos Daskalakis, Paul W. Goldberg, Christos H. Papadimitriou [2] What computer science can teach economics MIT News [3] Computer Science and Game Theory Shoham [4] T. Roughgarden. Computing equilibria: A computational complexity perspective. Economic Theory, 2008 [5] Nash Equilibria: Complexity, Symmetries, and Approximation Daskalakis 

this following paper seems close to what is requested in some ways (if it does not fit maybe JJ can clarify why). the question wants to rule out PHP (pigeonhole) instances based on the lack of both true/false formulas, but (as cited in the other answers) PHP is one of the most well-studied cases/instance generators from the theory side and has always been a generator for both satisfiable/unsatisfiable formulas although the satisfiable formulas are less emphasized/studied. PHP${^m_n}$ where there are $m$ "pigeons" and $n$ "holes" is unsatisfiable (false) if $m>n$ and satisfiable if $m \leq n$. am not immediately familiar with papers that study both cases but think they exist (would like to hear of any). and some results/analyses that study only the unsatisfiable cases are naturally extended to include the satisfiable case. 

Does there exist any constructive (indeed, computable) proof of the Halting Problem? All the ones I have encountered make use of proof by contradiction. As an aside, some proofs I have encountered make use of Cantor's diagonal argument, but this seems to be purely extraneous (e.g. the proof in Sipser). 

I have some experience in scientific computing, and have extensively used kd-trees for BSP (binary space partitioning) applications. I have recently become rather more familiar with octrees, a similar data structure for partitioning 3-D Euclidean spaces, but one that works at fixed regular intervals, from what I gather. A bit of independence research seems to indicate that kd-trees are typically superior in performance for most datasets -- quicker to construct and to query. My question is, what are the advantages of octrees in spatial/temporal performance or otherwise, and in what situations are they most applicable (I've heard 3D graphics programming)? A summary of the advantages and problems of both types would me most appreciated. As an extra, if anyone could elaborate on the usage of the R-tree data structure and its advantages, I would be grateful for that too. R-trees (more so than octrees) seem to be applied quite similarly to kd-trees for k-nearest-neighbour or range searches. 

I have recently been reading up on some of the ideas and history of the ground-breaking work done by various logicians and mathematicians regarding computability. While the individual concepts are fairly clear to me, I'm am trying to get a firm grasp of there inter-relations and the abstract level at which they are all linked. We know that Church's theorem (or rather, the independent proofs of Hilbert's Entscheidungsproblem by Alonzo Church and Alan Turing) proved that in general we cannot calculate whether a given mathematical statement in a formal system is true or false. As I understand, the Church-Turing thesis provides a pretty clear description of the equivalence (isomorphism) between Church's lambda calculus and Turing machines, hence we effectively have a unified model for computability. (Note: As far as I know, Turing's proof makes use of the fact that the halting problem is undecidable. Correct me if I'm wrong.) Now, Gödel's first incompleteness theorem states that not all statements in a consistent formal system with sufficient arithmetic power may be proven or disproven (decided) within this system. In many ways, this appears to me to be saying exactly the same thing to me as Church's theorems, considering lambda calculus and Turning machines are both effectively formal systems of sorts! This is however my holistic interpretation, and I was hoping someone could shed some light on the details. Are these two theorems effectively equivalent? Are there any subtleties to be observed? If these theories are essentially looking at the same universal truth in different ways, why were they approached from such different angles? (There were more or less 6 years between Godel's proof and Church's). Finally, can we essential say that the concept of provability in a formal system (proof calculus) is identical to the concept of computability in recursion theory (Turing machines/lambda calculus)?