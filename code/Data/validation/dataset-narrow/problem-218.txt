As you say IaaS gives you more control over the instance. The corollary is that you have to provide that control - and the maintenance, patching, HR, backups, DR etc... For that extra effort you get an environment exactly like the on-premise one you're familiar with, with someone else looking after the hardware. With PaaS you lose some access (no more SYSADMIN!) but don't have to deal with many of the admin tasks either. Backups, HA & fail-over are provided to you. The experience is more like being in ops or a highly privileged developer rather than an up-to-the-elbows production DBA. Some of the things to be aware of with PaaS are 

So you have several tables each of which holds an address. You're wondering if these common address-related columns should be in their own table. The typical answer is "yes" - a separate would be a good idea. There are any number of posts on this subject on this site. If you do make a common address table will you compare user input to existing rows so you can re-use an existing address_id in one of your tables? If the answer is "no", or you don't have a plan to handle small variations in how the user spells or formats the address, then why bother? 

Perhaps you will need to record who ordered the test and when. You can add to this table as required. Knowing the we can use to find out what is to be measured. Then we need somewhere to hold the actual measurements. This will depend on the component and the sample. 

Add a second column which will hold a hash of the text value. Create the index on the hash. Even if there is a hash collision there will be only a few rows to read and perform a full comparison on the text vlaues. 

will, at worst, hold a few bytes of status values. No big deal. Most programs will do actual work within the transaction and this is another matter. The point of a transaction is so you can be sure that several facts within the database are true simultaneously, despite there being other users writing to the same database concurrently. Take the cannonical example of transferring money between bank accounts. The system must ensure that the source account exists, has sufficient funds, the destination account exists, and that both debit and credit happen or neither happens. It must guarantee this while other transactions happen, perhaps even between these two accounts. The system ensures this by taking locks on the tables concerned. What locks are taken, and how much of other peoples' work you see, is controlled by the transaction isolation level. So if you do a lot of work there is a good chance other transactions will be queued waiting for the objects on which you hold locks. This will reduce the system's overall throughput. Eventually they will hit timeout limits and fail, which is a problem for overall system behaviour. If you use an optimistic isolation level your transaction may fail when you try a commit because of other's work. Holding locks takes system resources. This is memory which the system cannot use to process other requests, reducing throughput. If a lot of work has been performed the system may choose to perform lock escalation. Instead of locking individual rows the entire table will be locked. Then more concurrent users will be affected, system throughput will drop further and the application impact will be greater. Data changes are written to the log file, as are the locks which protect them. These cannot be cleared from the log until the transaction commits. Hence very long transaction may cause log file bloat with its associated problems. If the current work uses tempdb, which is likely for large workloads, the resources there may be tied up until the end of the transaction. In extreme cases this can cause other tasks to fail because there is no longer enough room for them. I have had cases where a poorly coded UPDATE filled tempdb so there was insufficient disk left for a report's SORT and the report failed. If you choose to ROLLBACK the transaction, or the system fails and recovers, the time taken for the system to become available again will depend on how much work was performed. Simply having a transaction open will not affect the recovery time, it is how much work was performed. If the transaction was open but idle for an hour recovery will be almost instantaneous. If it was writing constantly for that hour the rule of thumb is that recovery time also will be about an hour. As you can see long transaction can be problematic. For OLTP systems best practice is to have one database transaction per business transaction. For batch work process input in blocks, with frequent commits, and restart logic coded. Typically several thousand records can be processed inside a single DB transaction, but this should be tested for concurrency and resoruce consumption. Do not be tempted to go to the other extreme and avoid transactions and locks entirely. If you need to maintain consistency within your data (and why else would you be using a database?) isolation levels and transactions serve a very important purpose. Learn about your options and decide what balance of concurrency and correctness you are prepared to live with for each part of your application. 

MySQL supports columns of type bit. These only use one bit each, in sets of 8, as you would expect. This will give the same disk and memory density as you're currently getting, but with much easier query writing. 

The comparison values can be held in a separate table and joined to the data table. This assumes the comparision clauses do not overlap, as you sample shows. In other words, each data row will match at most one filter condition. This is the data: 

If you want the longest substring that means there is no other which is longer. A predicate will give this. 

The object-holding column will be of fixed length to avoid splits and row moves. Size this column to accommodate your objects and to efficiently fill a page on disk. Pre-fill your table with as many rows as you will need and a few more. When an object is to be written find a row with Used = False and UPDATE that row. When an object is to be destroyed, set it's Used to "False". There is no garbage created and hence no garbage collection. Of course there are many, many exception conditions to handle (row overflow, table overflow, race conditions on ID usage etc.) but none are insurmountable. 

The row with had gone from to and that is returned in the table variable. With this trivial data the rows are processed in the sequence they were created. In a more complex environment the optimizer may choose any row that matches the where clause. If you have a requirement to process rows in, say, time sequence further qualification will be required. I haven't thought it all the way through but I believe his will work at any isolation level with or without explicit transactions. It is of course possible that there are no rows with at the time the statement runs. In this case the table variable will have zero rows. To fully isolate each transaction from the other you could try . This will add overhead and reduce concurrency. You have to be careful to release the app lock as soon as you can in both success and failure scenarios. 

The DDL to make the change is just a text string. The necessary meta data is in the relational database already. This meta data can be queried by SQL. SQL can produce a string as output. Put this together and you get a meta-SQL-generator ... thing. Something like this: 

To answer the actual question, the built-in function LEN() will give the total number of characters in a string and CHARINDEX() will return the position of any given substring. In your case, with a required format of three-dot-six, LEN() must return "10" and CHARINDEX('.', value) must return "4" to have a valid input. Putting it together you get 

Alternatively, on the job step's general page, click the "Type" drop-down and choose "Powershell". Then you can use http calls or whatever else the SQL service account has rights to perform; the world is your lobster. 

The rows just happen to be coming out in this sequence. There are many scenarios where the optimiser may choose to return them in a different sequence, even if the table remains the same. The only way to be sure the client receives results in a particular sequence is by writing an ORDER BY clause. 

Overnight maintenance creates a new data table, drops the old ones and re-generates the definition of the view. Read processes never have to change. Write processes may be trickier, depending on MySQL's abilities (I'm SQL Server centric). Option 1 - dynamic SQL to always write to the current table. 2 - constant name for the current table, and rename during maintenance. 3 - write to the view and let the RDBMS sort it out. 

There is an art and a science to database design. The science is normalisation. If you're intent on learning about databases you will need a solid understanding of it. The art is in deciding just what exactly the "things" are that are going into your database. Defining them in a why that is comprehensive and precise is a skill to learn. In your particular case, storing multiple card ids in a single column prima face breaks first normal form. This is where the "art" bit comes in. If you only ever consider these cards all together - say, as a "hand" or a "pack" - and never as individual items then it would be legitimate to model them as a single column. If you ever want to consider each card individually ("How many times has the Queen of hearts been delt?") then you would be better of modelling a seperate table with one row per card. A second consideration is performance. It will always be faster to insert or delete one row than 52. It will almost always be slower to pull a substring out of a long varchar than to pick a single row from a large, well-indexed table. These considerations conflict. On balance, I would always recommend the one-row-per-card approach until it was proven otherwise. You are best placed to judge the usage and volumes of your application and, hence, resolve the conflicts. 

In our experience peer to peer replication is simpler and more robust than earlier forms. A solution based on Service Broker would enable asynchronous distribution. The application would have to be resilient to the various combinations of CAP scenarios. Master Data Services supports publishing consistent sets of tables. The cadence may not suit, though, as it in essence publishes in batch mode. 

With that many rows, moving the results from the server to the client will be a significant part of the elapsed time. Eliminate variablity in that part by wrapping your query in some way such as: 

It is OK to have many columns in one table. Indeed, this is the normal situation. It is not OK, however, to mix different types of information in a single table. Each table models a single class of "thing" in the real world. The rules of normalisation guide a database designer to separate classes from one another. Denormalisation (storing several things in one table) is a legitimate technique but should be used sparingly, with caution, and proper risk mitigation in place. In your specific case you will have to consider what is your business domain. Are your users interested solely in the balance sheet bottom-line numbers for several corporations over several years (e.g. investment and trading point of view), or do they need the detail of how those numbers are calculated (e.g. an accountancy / book-keeping point of view)? For the former the table you mentions would be OK in my opinion. For the latter you need to model individual transactions, double-entry ledgers etc., and calculate your bottom-line numbers from these as required. 

Normalisation talks about the atomicity of attributes within a relation and the dependency of attributes on candidate keys. Since there is no mention of non-key attributes in the question it is not possible to comment on the normalisation or otherwise of this design. That said, I would like to comment on the semantics of this design with regards to the business requirement to move accounts between organisations. In this design you cannot move an account to another organisation. Primary keys, by definition, distinguish one item from another. The account's key includes the organisation id. Therefore an account for one organisation is a different thing to an account with a different organisation, even if the two accounts have the same values in all attributes. By extension, moving an account would not cause Invoice to be updated. Rather, new rows would be created in Invoice, with values copied from the existing (Organisation, Account, Invoice) rows. The old Invoice rows could be deleted or not, it doesn't matter to the argument. Although an insert followed by a delete looks to an outside observer like an update, we are talking about primary keys here, which define the existence and distinguishability of objects. If I were to replace your Ford by a Ferrari while your eyes were closed would you, on opening them, think "Wow, my car has spontaneously morphed!" No, one thing has gone and another, functionally similar thing arrived. If the problem domain requires accounts to be movable yet still recognised as one continuing object you could A) key the account off account id solely, and have organisation id as a foreign key called "current owner" B) introduce a new table called "account transfer" with attributes From owner From account To owner To account Start date End date 

The program which performs log shipping runs under a specific UserID. Assuming you are using SQL Server Agent to perform log shipping on a schedule, log shipping will happen using SQL Server Agent's credentials (but see below for proxy accounts). You can find this by starting SQL Server Configuration, highlighting SQL Server Services and right-clicking on SQL Server Agent. The account listed there must have write permission on NAS device. Importantly, the source Agent and the destination device must share a source of authentication. If they are on different network domains, say, you may have to get your LAN administrators involved to establish the trust between the two devices. Proxies: it is possible to establish additional credentials which SQL Agent will use to run particular jobs. See here for information. If your site is using proxies to run log shipping then it is the proxy account which will need access to the NAS. To test, I would suggest you create a new job with a single step that copies a small text file from the DB server to the NAS. Use the same credentials for this as you do for log shipping. Run this and adjust permissions until it works. Then duplicate those permission for the actual log shipping job and re-test. 

Most RDBMS mostly use BTrees for indexes. These hold no summary information internally about the fan-out or total number of descendents. So the only way to get to a desired offset within the data is to start at the beginning a step through. If you were able to explicitly define a sequencing column within your table the RDBMS may be able to use an index on it to more quickly get to your offset (no guarantees; which indexes the optimiser chooses to use is a complex topic). The query would then become 

My first thought was to restore the backup. At 1TB this becomes impractical, however. Log shipping would not allow you to write to the pre-prod instance either. I think it should be possible to engineer something around table partitioning - assign each partition to its own filegroup, perform backups after the nightly batch and restore that to pre-prod in piecemeal fashion. I have not tried this; I'm just hypothesising. Replication will add more load onto the produciton server to read changed values and publish them. Having the pre-prod copy writable makes things a little more complicated here. Instead, for the one-off load, I'd suggest you restore a backup of prod. It may take a while but it will be complete and simple to script with minimal load on the prod box. For daily updates I'd go with copying the staging table(s) to pre-prod and re-running the load job there. There will be no additional strain on the production DB. You can roll back pre-prod and re-run a batch as often as you like, for performance tuning or debugging. If the staging DB starts each batch empty it would be simple to script a full backup - copy - restore - rerun sequence. This will be a good way to test future changes to the loader. We've had good results with change tracking, using it to copy hundreds of thousands of rows per day. If your staging DB isn't rebuit each day, this may be a good way forward. It will put a modest run-time load on the instance hosting the staging DB.