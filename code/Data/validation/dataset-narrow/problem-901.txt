It sounds like your email sending function is outside the loop, and is followed by either an exit statement or the end of the script. Can you just put the email sending function within the loop? You'll probably need to add a counter that will establish a time or repetition-based hold so that it doesn't immediately get in a loop of re-emailing you, but that's not too hard. If you do that, then the script won't exit the loop at all: it'll send and email and go back to checking for the file. 

On your local machine, (The "admin" box that has the external hd plugged in) share a folder on the external hd with "everyone", full rights. On the remote server, start:run, type \adminbox\newsharename Once you can browse that unc from your rdp session on the remote db server, you'll be able to back up to it using sql studio. 

I can't speak to "5-6 TB of data", but I currently have 1700 full time fat-client users (Application built in .NET) hammering against a 1.5 TB database using SQL 64bit Itanium. It performs fine. I think the scaling question is not so much the size of the DB, as it is the number of users and transactions per second. Oracle can do clustering to expand capacity as far as transactions/second (in certain circumstances) but I wouldn't necessarily prefer either one regarding raw DB size. 

Have you checked the case differences between the requested file and the file on disk? Don't forget that under most file systems used by Linux, Hello.doc does not equal hello.doc. 

I have a web application (IIS 8) on one server (Windows Server 2012) connecting to SQL Server Reporting Services 2012 on another server (Windows Server 2008) that until recently was working fine. About a week ago, it stopped working, and since then, I have not been able to get the system working again. The premise is that my code calls to SSRS to retrieve a report, then serves it to the user (so that the user never has view or access to the report directly for security reasons). The error message in my logs states "The request was aborted: Could not create SSL/TLS secure channel." The web server is reporting "A fatal alert was generated and sent to the remote endpoint. This may result in termination of the connection. The TLS protocol defined fatal error code is 70. The Windows SChannel error state is 105." which says that it's trying to negotiate an unsupported protocol. Fixes tried (and failed): Updated code to force TLS 1.2 Fully patched both servers, and software that doesn't automatically get patched Checked firewall on both servers (it is disabled) Examined and updated the allowed protocols, etc. with IIS Crypto to disable insecure options on both servers. And out of desperation (because it came up in search results and Windows Update notes): Added the LdapEnforceChannelBinding registry entry per MS at $URL$ Followed the directions at $URL$ re security check exemptions Removed the last set of patches from Windows Update 

The local machine (the source) is accessible from the outside, but only with the IP. The version of the local mongo is 2.0.4, while the version of mongo on the dev server is 2.2.2. 

I had to add a rule to the .htaccess file of existing site, which redirect some old images urls to the new ones. So, directly after the 

I have a collection in a mongoDB, on a project that runs locally, and have to copy the collection to the dev server. Can you tell me how to do this, considering that: 

Locally and on the dev server - all is fine. But when I deployed it on the live site - it's not working :( ... Locally I'm on OpenSUSE, the dev server is on Ubuntu Server, and the live server is FreeBSD. I ran phpinfo() and it showed that on the live server (shared hosting) I have mod_rewrite, so I don't know what could the problem be. Tried to add some "Options" to the .htaccess file, tried with flag for redirect in the end of the new rule ... but nothing changed. All the other rules are working. When I break something in the .htaccess file - it gives me "Internal server error", and if I write "Deny from all" - it gives me "Forbidden" - as it should. Can you help me? ... Thanks in advance :) On another forum got some advices and tried: 

So far nothing has worked, and the only other ideas that I have (but am not sure I want to try except at last resort) are 

Splunk looked pretty good, but we couldn't justify the price compared to a roll-your-own, so I had to roll up a basic logging server using a UDP connection. Fortunately, a 64k size limit gives me the ability to send 99%+ of my log messages through with just one UDP datagram. 

We have a utility scheduled in Task Scheduler on Server 2008 R2 that just ran into an issue I haven't dealt with before. The utility hung up (no activity for a day), and selecting end did nothing to end it, so I had to manually kill the task through task manager. After I did, I tried running the task in debug mode from my machine, and it went through until a dialog box popped up. Once I cleared the dialog box, the utility completed its run and exited cleanly. The utility doesn't have any dialog boxes of its own, as it is designed to run under task scheduler, so the dialog box was a surprise to me. It came out of an API we are using, and I took care of the issue that it presented to me, but now I want to know if it is possible to have task scheduler detect and handle these dialog boxes, or if I need to add some extra code to handle the possibility of these dialogs appearing. 

It only took me an hour or so of Googling and trial n error to get a fully functioning ADITO solution running. Another couple hrs to write my own user modules. For 10 or so users, it's ideal. Free. 

Yes. You want option B, so that the OS is referencing 2 different disk files. One DB can be optimized for writing, the other for reading. Or you could accomplish the same in a single db, if you split your tables into separate filegroups/separate files. 

I'm going to venture outside the "standard" reply and guess that what you need may be less a "centralized infrastructure" (single domain) and more of a centralized management tool. You might have a look at something like Italc. $URL$ It provides you a central command/control console, a singular view of your PC fleet. You'll still have to treat each PC individually, but you can do it all from one place both in the geographical and logical sense. And it's free... 

If you're truly asking, "can users run an interactive program without being logged into Windows" then the answer is "No." But if you mean, "Can users run an interactive program without having to know or enter their Windows credentials" then the answer is yes: you simply set up an auto-login process, and as soon as the PC boots, the user is ready to run an interactive session. You can set particular programs up to autorun, so that the user doesn't even need to launch them. 

In our organization, we have two test machines running Windows XP. While attempting to test a roll-my-own UDP message server, I found that both could receive small messages (under 2k) just fine. However, when I test sending large packets to both of these machines, one receives them fine, while the other can't receive them at all. Both machines have SP3 and both have their Windows Firewall shut off, but one still isn't working. Can anyone tell me where to look for anything that might be blocking or limiting the packet size on a Windows Machine? Thanks. 

Before I take any of these steps, does anyone have any ideas on anything else I can try to get these two servers talking again? 

A number of Windows services are not starting. No or half height taskbar. No start button, can't resize. Upon trying to open Internet Explorer we get the message "Windows cannot find '(null)'. Make sure you typed the name correctly..." Opening a document in Word results in the message "This document could not be registered. It will not be possibe to create links from other documents to this document." Opening excel results in two messages: "Cannot use object linking and embedding" then "An error occured initializing the VB libraries (14)" 

So, the virtual hosts - local and on dev - are pointing to the "root", while on the live site the domain appears to point to the "public" folder. And so, when I put the rules on the "public" .htaccess file - then it all worked on the live site. (And what was confusing for me was that when I break the "root" .htaccess file - the live site gives Error 500 ... and that's why I thought the "root" .htaccess file is the one that I should use ...) 

I installed and setup with virtual users, and each one of them has folder in . The ownership of is . When I point a subdomain to open with from one of those folders - I get . What is the best way to solve this? 

And then pass this new array of JSONs to the insert method of the new collection: (copied the array in the clipboard, directly from the text editor I used to make it) + Shift+Insert + And voila :) ... all your data is transferred :) ...