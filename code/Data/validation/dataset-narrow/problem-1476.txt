I'm sure we can agree that there are some parts of theoretical CS that are unlikely to change - topics such as computation theory (Turing machines, automata) will be useful and applicable for many years to come, [or at least until quantum computing becomes hugely pervasive]! I think a strong theoretical base is extremely important and I greatly enjoyed the modules I took at university which covered this sort of theory. Most courses do include a practical aspect, though. One of my frustrations as a student who started at university with experince in professional software engineering in industry, was that a lot of the more practical content was incredibly outdated, not in line with industry best practices or just downright irrelevant. As a concrete example, I took a web development module in 2015 which focussed on PHP 4, taught us to write code riddled with XSS, CSRF and SQLi vulnerabilities and didn't place any emphasis on architecture design. Some parts of the course were useful (such as the primer on how HTTP works at a protocol level) but it was obvious the same slide deck had been reused year after year and I came away feeling like I'd wasted my time. Some of these details seem to be out of the lecturer's hands and determined by the department, but the end result is still the same. I do realise web is perhaps one of the worst areas for this, but it's equally applicable when e.g. introducing Java and not covering the "new" generics support, or the more functional features in Java 8. What can educators do to try and prevent this from becoming too big an issue? I think trying to keep up with each new shiny JavaScript framework that becomes momentarily popular is unrealistic and useful for neither student nor educator - but there must be some middle ground. Should these sort of modules be entirely theoretical to avoid this problem, or perhaps collaboration with industry could be sought out to explain how things really work in practice? 

Because the standards are explicit, it is clear that it's nothing personal if we say a student cannot continue in the major. If the student really is interested in computer science, I tell her this might not be the right time in her life for her to do it (not that she's incapable of ever doing it) and advise her on retaking courses, with any appropriate accommodations. I might also advise her on related majors in which she might be more successful. We have no way of stopping a student from continuing to take CS courses (as long as she has passed the prerequisites), but the policy sends a strong message that they should choose a different major. 

Another fun exercise is implementing and using only gates (or gates). This proves that is universal. You can demonstrate that is universal then ask them to show that is (or vice versa). 

One reason infinite loops are used could be so the problem can't be solved by letting one thread complete the code before the other thread starts it. With infinite loops, no thread ever completes the code. Of course, using a bounded buffer has the same effect. 

I was used to mathematics, and a calculator which used fixed point arithmetic - an environment in which the answers were always "right" (to a given number of decimal places)! The way most programming languages handled decimal numbers felt broken and almost certainly useless for storing monetary values or for use in scientific code. I didn't appreciate the trade-off being made in terms of speed and space by taking the approach IEEE 754 did until much later on when I looked at the specification in detail and the analogy with standard scientific form was brought up. At what stage should students be introduced to how floating point numbers are stored, and (more importantly) how can the motivations for this approach be properly conveyed? 

A number of the assignments I've completed during the course of my undergraduate degree have been assessed partially using automated tests. These have never been available to students prior to the assignment submission, but I know this isn't the case at some universities. It would seem difficult to me to get less than 100% if all the test cases used for grading were released prior to the assignment deadline (provided the student gave enough time to the assignment). In some cases, the feedback I've received has indicated that my submission failed a number of automated tests - but the test cases were never made available to students and I was left unsure as to what I had done wrong in my work and how I could improve next time. In other cases, the test cases were made available after the assignment grades were released. I much preferred this approach, because I could see exactly where I'd gone wrong and the whole process felt fairer. Should automated test cases be released and if so, at what point should they be released? 

How do you encourage students to benefit from code reviews you give them, rather than being discouraged or responding defensively? 

In Programming Languages, I tell students that, in Fortran 77, the default type of a variable starting with , , , , , or is integer, while variables starting with any other character are reals. In either case, the default can be overridden with an explicit declaration. That sets up the joke: "Fortran is of great theological importance, because GOD is real (unless declared integer)." I recently learned about the deprecated Smurf Naming Convention. 

Don't assume that students who have seen other programming languages will be bored. Creating phone apps is hard, so the ability to do that may be of interest to them. If they are extremely advanced, you can encourage them to contribute to App Inventor, which is open source. Disclaimer: I was part of the App Inventor team and am co-author of a book about App Inventor. I have taught App Inventor at the college-level (to non-majors). 

I'd like to create a college-level AI course based on the famous AI MOOC created by Peter Norvig and Sebastian Thrun. My reasoning is that I wouldn't be able to create lectures as good as theirs, especially because I'm not particularly knowledgeable about AI. (The last AI course I took was in the early 90s.) I would teach a "flipped" course in which the students watched lectures on their own, and we used class time to: 

There is a really good teaching point here. Ideas often arise repeatedly in different contexts. That includes the ideas behind Functional Programming. We also all know how fast computing changes. Your job is to teach them computing, and its up to them to choose how to use it and the specialist areas they need most, which they probably don't yet know as 90% of what they will each specifically use, and several key paradigms they will come to rely on in their future careers spanning maybe 40+ years, hasn't yet been invented or become the main paradigm. Given that you are preparing them for many years ahead, and a broad topic, it is reasonable to include FP and expect it to be taken as serious and useful. Like machine code, firmware, and logic gates, few of your students will directly use it, but many will indirectly use it and all will use tools and techniques which began in it. As even the protesting students can't tell whether they'll find it useful in future, its got a strong ground for being taught, so they understand the concepts and history behind it, and behind what they now do. Also so its there if they do come in contact with FP work in future. 

When I first started working with floating point arithmetic, issues like the one illustrated in the sample below both frustrated and confused me: 

Computer Science lends itself to a variety of different assessment methods - from the more practical assignment based programming problems, to the more formal final exams that cover the theoretical content. More formative assessments give students and educators the ability to improve learning and teaching whilst it is ongoing, but there is usually some concept of a "high-stakes" summative exam in most courses. I'm sure we can all agree that portions of one module (e.g. on data structures) will be exceptionally useful in later modules, and so it makes sense to do as much as possible to help students master these concepts. Often (from my own experience as a student), the "feedback" from the summative exam is a simple number or grade, which isn't broken down by assessment component. Further, there is usually no way to see the exam script or any form of mark scheme or question breakdown, which makes it difficult to improve in the general context of the university degree. The summative exam does nothing other than provide a coarse measure of the candidate's performance during the exam. Is this sort of status quo widespread throughout undergraduate level CS, and what can be done to give students the opportunity to learn from and reflect on these sort of assessments? Should a more formative approach be taken for the majority of each module? 

I would mention it in any class in which it is relevant, such as User Experience or where A/B testing might be used to show different users different versions of a website and measure their behavior. Because such research is ubiquitous (consider Google's testing different shades of blue), Facebook was unprepared for the backlash against their emotional manipulation study. 

As you know, the local variables and do not get changed, because C uses call by value. At that point, I show and discuss this cartoon: 

I teach at Mills, a women's college near Silicon Valley with a high number of students of color. We do a great job of preparing students technically -- they do very well on the job market and then in their jobs. What I am unsure of is how to prepare students for the transition from a nurturing, feminist liberal arts environment to industry, which is not only more rough-and-tumble for everyone but can include bias based on sex, race, age, etc., which may cause them to leave the tech industry. How can I prepare my students to not have their careers derailed by mistreatment in industry? I don't want to just scare them out of the major or make them fearful by mentioning discrimination without providing a solution. 

I would generalise this, because other symbols and terminology are also affected (">" or textual statements such as commands). Start by explaining that symbols and terms in each language have meanings specific to that language. They may overlap or have similarities, but never assume that just because a symbol or word has some meaning in one language or scenario, that it has the same meaning in another. Then use "=" as an example of this general rule. This changes "=" from "a problem affecting one situation" into "a classic example of a really important rule that applies to all of computer science" and into a memorable learning point. You can then pick a few other examples if the students need it. This will prepare them for the idea that different languages often also have different philosophies, approaches to, and ways of representing even superficially similar programming structures. 

Providing the class with pre-chosen code, for the specific purpose of teaching quality control skills and methods (of which review is a component) will help to ensure the code is suitable for teaching the points required, and does actually contain review points where you can provide notes and grades fairly. Asking students to peer review may mean some get easier or harder code to review than others, and some get code poorly suited to review lessons. Teaching the class the importance and focus on QC and quality production methods, puts review in a context - they will need this knowledge professionally as almost all software businesses do operate some form of QC and/or review. Simple as that. By giving them code you have chosen, you can be sure that whether individually or in groups, they have a target number of important and secondary review points to identify, which may motivate them to look harder. The rationale is, if you have to find and review 10 major points in a block of code, as part of a review, you might spent extra time taxing yourself to find the last 3 items, compared to an open-ended count. You'll probably remember them better as well.