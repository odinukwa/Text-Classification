I don't know of any consequences of this particular theorem, but normalization proofs of lambda calculi like the calculus of inductive constructions rely on large cardinal axioms -- even though the set of lambda-terms is as countable as you could want. I think the best way to understand the computational significance of set-theoretic axioms asserting the existence of large cardinals is to think of set theory as a way of phrasing the theory of graphs. That is, a model of a set is a collection of elements equipped with a binary relation used to interpret membership. Then, the axioms of set theory tell you properties of the membership relation, including how you can form new sets from old. In particular, the axiom of foundation means that the membership relation is well-founded (ie, it has no infinite descending chains). This well-foundedness in turn means that if you can line up the execution states of a program with the transitive membership of the elements of a set, then you have a termination proof. So an assertion that a "big" set exists has a computational payoff as a claim that a certain class of loops in a general recursive programming language terminate. This interpretation works uniformly, all the way from the plain old axiom of infinity (which justifies natural number iteration) all the way up to large cardinal axioms. Are these axioms true? Well, if the axiom is false you can find a program in one of these classes which doesn't terminate. But if it's true, we will never be sure, thanks to the Halting theorem. Everything from natural number induction on is a matter of scientific induction, which may always be falsified by experiment -- Edward Nelson has famously hoped to prove exponentiation is a partial function! 

The formulas are formulas of Abadi-Plotkin logic, which they describe in their paper A Logic for Parametric Polymorphism. The semantics of System F that Abadi and Plotkin used to interpret their logic can be found in Bainbridge, Freyd, Scedrov, Scott's paper Functorial Polymorphism. 

Assuming your functional language $L$ lets you encode arithmetic operations, there is one program that you can count on not being expressible if it is total: namely, an interpreter for $L$, written in $L$. 

Scott-continuity rules out the lookup table approach. For a concrete counterexample, let's start with the Sierpinksi domain $O = \{\bot, \top\}$, with order $\bot \sqsubseteq \top$. Intuitively, think of $\bot$ as nontermination, and $\top$ as termination. In Haskell terms, think of this as the domain interpreting expressions of type $()$ -- i.e., you either get back a unit or the expression fails to terminate. We'll also use $2$, the booleans, and $2_\bot$, the pointed booleans. Now, suppose you have $f : O \to 2_\bot$. You can't continuously complete this function, even though there are only a finite number of possible $f$'s! The intuitive reason is that when you run $f(\bot)$, there's no finite waiting period sufficiently long enough to decide that its return value is bottom -- if there were, then I could give you a function that spins for one second longer, and then returns the opposite of whatever you completed the function to. (Prove that any continuous $h : (O \to 2_\bot) \to (O \to 2)$ must return a constant function independent of its argument $f$.) Don't think of bottom as an extra element of the domain of values. This will lead you astray. Instead, think of elements of the domain as observations of a computation. Really do think of sitting in front of a terminal, waiting for your program to print something. Bottom represents your observation when the program is still running and hasn't printed anything to the screen -- it represents observing nothing. The best book I know for developing the right intuitions is Steve Vickers' Topology via Logic. It's a short and easy read, but your domain-fu will go way up for reading it. 

If you look at the recursive combinators in the untyped lambda-calculus, such as the Y combinator or the omega combinator: $$ \begin{array}{lcl} \omega & = & (\lambda x.\,x\;x)\;(\lambda x.\,x\;x)\\ Y & = & \lambda f.\,(\lambda x.\,f\;(x\;x))\; (\lambda x.\,f\;(x\;x)) \\ \end{array} $$ It's clear that all of these combinators end up duplicating a variable somewhere in their definition. Furthermore, all of these combinators are typeable in the simply-typed lambda calculus, if you extend it with recursive types $\mu\alpha.\,A(\alpha)$, where $\alpha$ is allowed to occur negatively in the recursive type. However, what happens if you add full (negative-occurence) recursive types to the exponential-free fragment of linear logic (i.e., MALL)? Then you don't have an exponential $!A$ to give you contraction. You can encode the type of exponentials using something like $$ !A \triangleq \mu\alpha.\;I \;\&\; A \;\&\; (\alpha \otimes \alpha) $$ but I don't see how to define the introduction rule for it, since that seems to require a fixed point combinator to define. And I was trying to define exponentials, to get contraction, to get a fixed-point combinator! Is it the case that MALL plus unrestricted recursive types is still normalizingâ€½ 

You certainly can write programs which catch bugs -- there is a large and active community of people who write programs to do exactly that. However, what Rice's theorem prevents you from doing is to write bug-catchers which are both sound and complete (i.e., catch all bugs of a certain class with no false positives). That said, naive restrictions on the model of computation don't actually help you very much in improving the practicality of program analysis. The reason is that you can get programs which do "almost the same thing" by turning while loops 

The reason for the ban on negative occurrences can be understood by analogy with the Knaster-Tarski theorem. This theorem says that 

What this does is take decrement a counter, and rotate its arguments until it reaches 0. This program will compile to the following machine code: 

Here's the thirty second explanation for why subtraction "should be" saturating. First, suppose that we have $a, b, s \in \mathbb{N}$. 

No, in this case, predicativity and monotonicity are not closely related. The positivity check in Coq/Adga serves to ensure that you are taking the least fixed point of a monotonic thing, roughly. Here's how to think of inductive types in terms of lattices and monotone operators. Recall that the Knaster-Tarski theorem says that on a complete lattice $L$, every monotone operator $f : L \to L$ has a least fixed point $\mu(f)$. Next, we can think of the types in a type theory as forming a lattice under provability. That is, type $S$ is below $T$ if the truth of $S$ entails that of $T$. Now, what we would like to do is to take a monotone operator $F$ on types, and use Knaster-Tarski to get out an interpretation of the least fixed point of this operator $\mu(F)$. However, types in type theory aren't just a lattice: they form a category. That is, given two types $S$ and $T$, there are potentially many ways for $S$ to be below $T$, with one way for each proof $e : S \to T$. So a type operator $F$ also has to do something sensible on these proofs. The appropriate generalization of monotonicity is functoriality. That is, we want $F$ to have an operator on types, and also to have an action on proofs, such that if $e : S \to T$, then $F(e) : F(S) \to F(T)$. Now, functoriality is preserved by sums and products (ie., if $F$ and $G$ are endofunctors on types, then $F+G$ and $F\times G$ (acting pointwise) are also functors on types (assuming we have sums and products in our algebra of types). However, it is not preserved by the function space, since the exponential bifunctor $F \to G$ is contravariant in its left argument. So when you write an inductive type definition, you are defining a functor to take a least fixed point of. To ensure that it is indeed a functor, you need to rule out occurences of the recursive parameter on the left-hand-side of function spaces --- hence the positivity check. Impredicativity (in the sense of System F) is generally avoided, because it is a principle that forces you to choose between classical logic and set-theoretic models. You can't interpret types as sets in classical set theory if you have F-style indexing. (See Reynolds' famous "Polymorphism is Not Set-Theoretic".) Categorically, F-style impredicativity says that the category of types and terms forms a small complete category (that is, homs and objects are both sets, and limits of all small diagrams exist). Classically this forces a category to be a poset. Many constructivists are constructive because they want their theorems to hold in more systems than just classical logic, and so they don't want to prove anything that would be classically false. Hence they are leery of impredicative polymorphism. However, polymorphism lets you say many conditions that are classically "large" internally to your type theory -- and positivity is one of them! A type operator $F$ is functorial, if you can produce a polymorphic term: $$ \mathsf{Fmap} : \forall \alpha, \beta.\; (\alpha \to \beta) \to (F(\alpha) \to F(\beta)) $$ See how this corresponds to functoriality? IMO, this would be a very nice option to have in Coq, since it would let you do generic programming much more easily. The syntactic nature of the positivity check is a big hindrance to generic programming, and I would be happy to trade the possibility of classical axioms for more flexible functional programs. EDIT: The question you are asking about the difference between Prop and Set arises from the fact that the Coq developers want to permit you think about Coq theorems in naive set-theoretic terms if you want, without forcing you to do so. Technically, they split Prop and Set, and then prohibit sets from depending on the computational content of Prop. So you can interpret Prop as truth values in ZFC, which are the booleans true and false. In this world, all proofs of propositions are equal, and so obviously you should not be able to branch on the proof of a proposition. So the prohibition on sets depending on the computational content of proofs of Prop is totally sensible. Furthermore, the 2-element boolean lattice is obviously a complete lattice, so it should support impredicative indexing, since arbitrary set-valued meets exist. The predicativity restriction on Sets arises from the fact (mentioned above) that F-style indexing is degenerate in classical set-theoretic models. Coq has other models (it's constructive logic!) but the point is that off the shelf it will never prove anything that a classical mathematician would be puzzled by. 

Show that there are no fixed points $\mu P$ such that $P(\mu P) = \mu P$. As a result, you can conclude that $\mu X.\;P(X)$ cannot be well-defined. Prove the Knaster-Tarksi theorem for yourself. That is, if you have a complete lattice $L$, and a monotone function $f : L \to L$, then the set of fixed points of $f$ forms a complete lattice. (As a consequence, $f$ has a least and greatest fixed point.) This proof is very short, but it's a bit of a head-scratcher the first time you see it, and the monotonicity of $f$ is critical to the argument. Prove for yourself that any operator defined by an expression with a free variable $X$ which occurs only positively is monotone. So positive occurence is a syntactic condition which is sufficient to enforce monotonicity. 

As you can see, it is not pushing anything on the stack (the contents of ), but it does need to shuffle the contents of the stack around before making the tail call (). (As an aside, decrementing the variable is accomplished by subtracting 2 from , since Ocaml uses a tagged representation of integers.) I seem to recall that the problem of putting variables into slots so as to minimize the amount of stack/register shuffling you need to do is NP-complete, and that consequently most compilers rely on heuristics to figure out what to do. (P.S. I don't know what's wrong with the indentation -- it's fine in the preview....) 

Does anyone direct me to a paper detailing a cut-elimination theorem for propositional intuitionistic logic, including an inductive datatype such as the natural numbers (lists or trees would be fine, too)? An example of the kind of system I am interested in is Godel's T, which has types given by the grammar $A ::= \mathbb{N} \;\;|\;\; A \to A'$. I am not very interested in quantifiers over natural numbers or predicates indexed by natural numbers. I know how to prove beta-normalization for natural deduction version of these systems using a logical relations argument (or related techniques such as NbE), but would like to know if there are standard references on how to adapt these methods to sequent calculi. The reason I ask is that I am studying adding fixed point operators for guarded recursion to a language. The denotational idea is a rather old one -- interpret types as ultrametric spaces and fixed points via Banach's theorem -- but the purely syntactic techniques I know for proving cut-elimination don't seem to adapt that well. 

I've been playing around with resumptions lately, mostly from Abramsky's classic paper Retracing Some Paths in Process Algebra. They are quite slick (basically solutions to the domain equation $R = I \to (O \times R)$), and very reminiscent of Kahn networks. Of course, this observation is not original to me --- they form a traced monoidal category, and this fact was used by Abramsky and Jagadeesan to give semantics to linear logic. At any rate, note that if you feed a resumption $r$ an input of type $I$, you get an output of type $O$ and an updated resumption $r'$, which is what lets you model the fact that a dataflow node can change as it sees inputs come in. As a result, it seems like they could give a nice API for building I/O transducers in a higher-order language like ML or Haskell, but I can't seem to find any papers describing such a thing. But they've been around for decades, and Gordon Plotkin invented them, so it's not like they've languished in obscurity. So I was wondering if anyone had seen them put to such use. 

This is not a complete answer; it is a comment that got too large. If you extend typed lambda calculus with products with projective eliminators (ie, product eliminators and ), there are no basically issues whatsoever. The reason it took so long to figure out is because it turns out to be more natural to do eta expansions rather than eta reductions. See Barry Jay's The Virtues of Eta Expansion. If you want products to have a pattern-style eliminator