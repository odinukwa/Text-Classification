2) We are given an input string, output string, and context-free grammar, and asked if the grammar generates the output string from the input string in a parse tree of $k$ or fewer nodes. In this case, the decision problem of whether any derivation exists can be solved in polynomial time but I am not sure about the problem of smallest derivation. 

Well, at least $\#\mathsf{P}$-hard. Given a SAT formula, construct a graph with two vertices, $v_x$ and $v_x'$, for every possible assignment of variables $\vec{x}$. If $x$ is a satisfying assignment for the formula, draw an edge between $v_x$ and $v_x'$; these are the only edges. It is easy to construct the circuit for this graph from the SAT formula, and the number of odd vertices is exactly twice the number of satisfying assignments. 

I can attempt a long-winded version of Thomas' comment and others can correct me where needed. Algorithms have been around a long time (e.g. Euclidean algorithm for GCD circa 300 BC), and even computational machines are somewhat old (Babbage's design for the analytical engine in the 1830s). However, most wouldn't consider theoretical CS to have started until we had the first formal, mathematical definitions of what an algorithm can be. Algorithms were formalized in three main ways, to my understanding, in the early 1900s. There were the $\mu$-recursive functions, which Godel used in his incompleteness work in 1930. However, these were thought of as mathematical constructs and not necessarily mechanical or computational. I would guess that the same is true for Alonzo Church's lambda calculus (early 1930s), the second way. Although one could view them as being closer to computation, they were mainly mathematical and I don't think there was a clear connection to the capabilities of machines and "programming" (using modern terminology). The third way that algorithms were formalized was of course Turing's Machines in 1937. Most would say that this paper ("On Computable Numbers") founded the field of CS. The reason is that his formalization of algorithms was mechanical. He described how to build "dumb" machines that implemented these algorithms. Recursive functions and the lambda calculus did not so immediately relate to machines (though it was soon shown that all of these are equivalent). The paper was, however, primarily theoretical and mathematical and he proved some interesting/foundational theorems already in the first paper. So again, the key innovation was to define a model instantly recognizable as "mechanical" or dumb (implementable as a machine), yet capable of expressing anything we would consider to be an algorithm; and then proving theorems about it. Many other researchers were involved around that time and soon afterward (I haven't mentioned Kleene or Post yet for instance). But for this reason I think most would point to Turing and his 1937 paper as the start of the field. 

I corresponded with Joel Friedman about 3 years ago on this topic. At the time he said that his approach had not led to any significant new insights into complexity theory, though he still thought it was a promising tack. Basically, Friedman tries to rephrase the problems of circuit complexity in the language of sheaves on a Grothendieck topology. The hope is that this process will allow geometric intuition to be applied to the problem of finding circuit lower bounds. While it's certainly worth checking to see if this path leads anywhere, there are heuristic reasons to be skeptical. Geometric intuition works best in the context of smooth varieties, or things that are sufficiently similar to smooth varieties that the intuition doesn't totally break down. In other words, you need some structure in order for geometric intuition to gain a foothold. But circuit lower bounds by their very nature must confront arbitrary computations, which are difficult to analyze precisely because they seem to be so structureless. Friedman admits right up front that the Grothendieck topologies he considers are highly combinatorial, and far removed from the usual objects of study in algebraic geometry. As a side comment, I'd say that it's important not to get too excited about an idea just because it uses unfamiliar, high-powered machinery. The machinery might be very effective at solving the problems that it was designed for, but for it to be useful for attacking a known hard problem in another domain, there needs to be some compelling argument why the foreign machinery is well adapted to address the fundamental obstacle in the problem of interest. 

The correctness and confidence bounds ($1-e^{-\Omega(M)}$) depend on the following lemma which says that all of the deviation in $L_2$ distance comes from points whose probabilities differ by $\Omega(\epsilon^2)$. 

I will attempt to atone for my previous error by showing something opposite -- that $\tilde{\Theta}\left(\frac{1}{\epsilon^2}\right)$ samples are sufficient (the lower bound of $1/\epsilon^2$ is almost tight)! See what you think.... The key intuition starts from two observations. First, in order for distributions to have an $L_2$ distance of $\epsilon$, there must be points with high probability ($\Omega(\epsilon^2)$). For example, if we had $1/\epsilon^3$ points of probability $\epsilon^3$, we'd have $\|D_1 - D_2\|_2 \leq \sqrt{\frac{1}{\epsilon^3} (\epsilon^3)^2} = \epsilon^{3/2} < \epsilon$. Second, consider uniform distributions with an $L_2$ distance of $\epsilon$. If we had $O(1)$ points of probability $O(1)$, then they would each differ by $O(\epsilon)$ and $1/\epsilon^2$ samples would suffice. On the other hand, if we had $O(1/\epsilon^2)$ points, they would each need to differ by $O(\epsilon^2)$ and again $O(1/\epsilon^2)$ samples (a constant number per point) suffices. So we might hope that, among the high-probability points mentioned earlier, there is always some point differing "enough" that $O(1/\epsilon^2)$ draws distinguishes it. 

While this has been bumped anyway, maybe I can have the hubris to mention a heuristic that comes to mind. An NP-complete problem is, given a circuit, is there an input that evaluates to True? 

This depends on what you mean by "applying calculus to computer science." In your comment to Quaternary's answer, you make a distinction between "direct" and "indirect" application, but it's not clear to me exactly what distinction you're making. Following are some areas of computer science where calculus/analysis is applicable. 

Such graphs are called well-covered graphs. Here is a recent paper on the subject that lists several useful references. As Suresh mentioned, the recognition problem is co-NP-complete. Note that the independent sets of a graph form an abstract simplicial complex. Simplicial complexes that arise in this way are called "independence complexes" or "flag complexes." A simplicial complex is said to be pure if every maximal simplex has the same cardinality. So you may find some relevant papers by searching for "pure independence complex" or "pure flag complex." 

Tim Gowers is a fan of this kind of thing. See specifically his exposition of Razborov's method of approximations. In his introduction, Gowers references my expository article on forcing, which is a (not entirely successful) attempt to do the same thing for forcing. Forcing is normally thought of as a technique in logic and set theory, but it has found its way into TCS occasionally. It comes up in the study of bounded arithmetic and propositional proof complexity (Krajíček and Takeuti are two researchers who have pursued this connection), and the concept of a generic oracle is related to the concept of a generic filter. 

Same question as (2), but relating to the actual equilibria computed by algorithms. I guess probably we will either get algorithmic/constructive answers or none at all, so the distinction doesn't matter much. 

Proof. In the bound of the "theorem", for small enough $p$, the upper bound is $\log\frac{1}{p} - \frac{1}{p}\Theta(H(X|Y))$, which approaches $-\infty$ as $p \to 0$ for all fixed $H(X|Y)$. $\square$ 

If you don't mind randomization, then standard online learning algorithms in the "online convex optimization framework" give you essentially what you ask for, in expectation. The reason is that these algorithms are required to output a distribution $w \in \Delta([n])$ on experts at each time step, suffering an expected loss equal to the expectation of picking an expert from this distribution. And they have low expected regret compared to the best distribution on experts, i.e. $O(\sqrt{\ln n / T})$. For example, you can take the classic multiplicative weights algorithm, which is just weighted majority but picking an expert to follow with probability proportional to its "weight". This is mentioned in Arora's survey (Theorem 6): $URL$ 

For convenience let $H(X|Y) = \log(n)$, then $$ -\infty ~~\leq~~ H(X|Y) - H(X|Y,X\neq Y) ~~\leq~~ \log\left(\frac{n}{n-1}\right) $$ and both sides have tight examples (i.e. as $p\to 0$ it can be arbitrarily negative, and your example matches the upper bound). More specifically, if $p = \Pr[X \neq Y]$, then: $$ H(X|Y) - H(X|Y,X\neq Y) ~~ \geq ~~ -\frac{(1-p)H(X|Y)}{p} $$ and $$ H(X|Y) - H(X|Y,X\neq Y) ~~\leq~~ \log\frac{1}{p} + \frac{1-p}{p}\left[\log\frac{1}{1-p} ~-~ H(X|Y)\right] $$ and we have tight examples for the second, and arbitrarily close to tight examples for the first, for every $p,H(X|Y)$. There will be two steps to the proof: (1) prove an upper bound and matching examples for $H(X|Y,Z)$ where $Z$ is an indicator; (2) convert these to your quantity of interest $H(X|Y,X\neq Y)$. Upper Bound Step 1. 

I think that the friendship theorem (see also Huneke's paper) is a good example even though strictly speaking there now exist proofs of the friendship theorem that avoid eigenvalues. The proofs that avoid eigenvalues entirely are a lot messier than the spectral proof. (The friendship theorem states that if in a room of people, every pair of people has exactly one common friend, then there is someone who knows everyone else.) 

As others have pointed out, there are certain technical difficulties with the statement of your question. To straighten them out, let's start by avoiding the use of the term "unprovable" without qualification, and be explicit about which set of axioms your statement T is supposed to be unprovable from. For instance, let's suppose that we're interested in statements T that are unprovable from PA, the axioms of first-order Peano arithmetic. The first annoyance is that "T is true" is not expressible in the first-order language of arithmetic, by Tarski's theorem. We could get around this by working in a metatheory that is powerful enough to define the truth of an arithmetical statement, but I think for your purposes this is an unnecessarily complicated route to take. I think you're not so interested in truth per se but in provability. That is, I suspect you'd be satisfied with defining T to be Godel_0 if T is true but unprovable in PA, and defining T to be Godel_1 if T is unprovable in PA but "T is unprovable in PA" is unprovable in PA, and defining T to be Godel_2 if T is unprovable in PA and "T is unprovable in PA" is unprovable in PA but "‘T is unprovable in PA’ is unprovable in PA" is unprovable in PA, etc. That way we don't have to fuss with truth predicates. This suffices to make your question precise, but unfortunately there is then a rather trivial solution. Take T = "PA is consistent." Then T is true because PA is consistent, and T is unprovable in PA by Goedel's 2nd incompleteness theorem. Furthermore, "T is unprovable in PA" is also unprovable in PA for a somewhat silly reason: any statement of the form "X is unprovable in PA" is unprovable in PA because "X is unprovable in PA" trivially implies "PA is consistent" (since inconsistent systems prove everything). So T is Godel_n for all n, but I don't this really gets at your intended question. We could try to "patch" your question to avoid such trivialities, but instead let me try to address what I think is your intended question. Tacitly, I believe you are conflating the logical strength needed to prove a theorem with the psychological difficulty of proving it. That is, you interpret a result of the form "T is unprovable in X" as saying that T is somehow beyond our ability to understand. There are these monstrous conjectures out there, and we puny humans crack PA-whips or ZFC-whips or what have you at those ferocious beasts, trying to tame them. But I don't think that "T is unprovable in X" should be interpreted as meaning "T is impossible to reason about." Rather, it's just measuring a particular technical property about T, namely its logical strength. So if you're trying to come up with the über-monster, I don't think that finding something that is not only unprovable, but whose unprovability is unprovable, etc., is the right direction to go. Finally, regarding your question about whether unprovability seems at all related to separability of complexity classes, there are some connections between computational intractability and unprovability in certain systems of bounded arithmetic. Some of this is mentioned in the paper by Aaronson that you cite; see also Cook and Nguyen's book Logical Foundations of Proof Complexity. 

Proof. Again we have $p\cdot H(X|Y,X\neq Y) = H(X|Y,Z)$, so by the previous claim, $$ H(X|Y) - p\cdot H(X|Y,X\neq Y) \geq 0 . $$ Again let $H(X|Y) = p\cdot H(X|Y) + (1-p)H(X|Y)$ and rearrange. By the previous claim, we have arbitrarily close examples (since we have only rearranged the inequality). $\square$ 

Given an algorithm running in time $t(n)$, we can convert it into a "trivial" uniform circuit family for the same problem of size at most $\approx t(n)\log t(n)$. On the other hand, it might be that we have much smaller uniform circuits for that problem, even if $t(n)$ is an optimal running time. The circuits may take longer than $t(n)$ to generate, but they are small. But do we actually know how to build such things? I think the initial question to ask is 

I've tried to comb the literature and seen a lot of references to results that almost but don't quite seem to address this. 

is false. P=BQP only implies that if a machine with randomness can usually output the correct bit, then there is a machine with no randomness that always ouptuts the correct bit. 

EDIT: this is incorrect! See the discussion in the comments -- I will point out the flaw below. I think we can say that $\frac{1}{\epsilon^4}$ are required. Set $n = \Theta\left(\frac{1}{\epsilon^2}\right)$. Let $D_1$ be the uniform distribution (probability of each point $= \Theta(\epsilon^2)$) and let $D_2$ differ from uniform by an additive amount $\pm \Theta\left(\epsilon^2\right)$ at each point. Check that the $L_2$ distance is $\epsilon$. So we have to distinguish an $n$-sided fair coin from an $n$-sided $\Theta(\epsilon^2)$-biased coin. I think this should be at least as hard as telling a $2$-sided fair coin from a $2$-sided $\Theta(\epsilon^2)$-biased coin, which would require $\Theta\left(\frac{1}{(\epsilon^2)^2}\right) = \Theta\left(\frac{1}{\epsilon^4}\right)$ samples. Edit: this is incorrect! The coin is additively $\epsilon^2$-biased, but it is biased multiplicatively by a constant factor. As D.W. points out, that means that a constant number of samples per point distinguishes $D_1$ from $D_2$. 

The title question arose in the course of discussing a question on MathOverflow. Obviously, from the space hierarchy theorem we know that not only is it false that $\mathrm{DSPACE}(n^b) \subseteq \mathrm{DSPACE}(n^{b/2})$, but there is a proper inclusion in the other direction. But once we limit the time budget of the left-hand side to $n^a$, I'm no longer sure what we can say. If the question can't be answered unconditionally, then can we answer it conditional on some standard hypothesis or relative to some oracle? 

Let me respond to your suggestion with a counter-suggestion: Why don't you try setting up a business, acting as a middleman between amateurs and experts? Amateurs pay to have their proofs evaluated. You find an expert and pay the expert to evaluate the proof, taking a cut of the money for your middleman role. Trying to run such a business is the most reliable way of finding out whether your idea is a feasible one. 

To phrase the question another way, what are some exceptions to the heuristic that if can't figure out contradictory relativizations then it is easy to resolve the equality question outright? 

Scientific computing. Computer algebra systems that compute integrals and derivatives directly, either symbolically or numerically, are the most blatant examples here, but in addition, any software that simulates a physical system that is based on continuous differential equations (e.g., computational fluid dynamics) necessarily involves computing derivatives and integrals. Design and analysis of algorithms. The behavior of a combinatorial algorithm on very large instances is often most easily analyzed using calculus. This is especially true for randomized algorithms; modern probability theory is heavily analytic. In the other direction, sometimes one can design an algorithm for a discrete problem by considering a continuous analogue, using calculus to solve the continuous problem, and then discretizing to obtain an algorithm for the original problem. The simplest example of this might be finding an approximate root of a polynomial equation; using calculus, one can formulate Newton's method, and then discretize it. Asymptotic enumeration. Sometimes the only way to get a handle on an enumeration problem is to form a generating function and use analytic methods to estimate its asymptotic behavior. See the book Analytic Combinatorics by Flajolet and Sedgewick. 

Here's a heuristic argument to say that, if each vertex has an expected number of edges of at least $\Omega(\log d)$, then you can get such a bound, and furthermore it depends mainly on this expected number of edges per vertex (not much on $n$ or $d$). (Edit: As Aaron points out in comments, though, a bound for $H_d$ does not look like it will help us bound $Z_d$!) Let's write $H_d$ as $H_d = \sum_{i_2\neq 1}X_{1i_2} \left(1 + \sum_{i_3 \neq 1} X_{i_2 i_3} \left(1 + \cdots \sum_{i_{d-2}\neq 1}X_{i_{d-2}i_{d-1}} \left(1 + \sum_{i_d \neq 1} X_{i_{d-1}i_d}\right)\cdots \right) \right)$. Now use Chernoff from the inside and move outward. For each index $i_k$, let $\mu_k = \sum_{j \neq 0} p_{i_k j}$. $\Pr\left[\sum_{i_d \neq 0} X_{i_{d-1}i_d} > (1 + \delta)\mu_d\right] \leq \approx e^{-\delta^2 \mu_d}$. So replace this innermost sum with the constant $(1+\delta)\mu_d$ and add $e^{-\delta^2 \mu_d}$ onto the probability we need. Now expand outwards, adding the probabilities (union-bounding). For example, at the next step we say $\Pr\left[\sum_{i_{d-1}\neq 0} X_{i_{d-2}i_{d-1}} \left(1 + (1+\delta)\mu_d\right) \geq (1+\delta)(\mu_{d-1} + (1+\delta)\mu_d)\right] \leq \approx e^{-\delta^2 \mu_{d-1}}$. At the end we get $\Pr\left[H_d > (1+\delta)\mu_1 + (1+\delta)^2 \mu_1 \mu_2 + ~\cdots~ + (1+\delta)^d \mu_1 \cdots \mu_d \right] \leq \approx de^{-\delta^2 \mu_{min}}$ where $\mu_{min}$ is the smallest $\mu_k$. If the number of edges per vertex were independent across vertices (not true, but maybe "close" if $\mu = \omega(1)$), then we could say $\Pr\left[H_d > (1+\delta)^d E[H_d] \right] \leq \approx de^{-\delta^2 \mu_{min}}$. So based on this it seems like a sufficient (and hopefully not too loose) condition should be that for every vertex $k$, the expected number of neighbors is $\mu_k = \Omega(\log d)$. 

While it's not inconceivable that the technical obstacles you mention could be overcome somehow, I think that there is very little motivation at present to do so, for the simple reason that (at least as far as I am aware) the difficulty of NP-hard problems in practice seems, empirically, to have little to do with their closeness to the 3-SAT phase transition. Contrast this with some other ways to rank NP-hard problems in terms of difficulty: There is some empirical correlation between NP-hard problems that are easy in practice and NP-hard problems that are easy to approximate, or that are fixed-parameter tractable (in the sense of parameterized complexity). Appropriate notions of reduction have been developed in these cases that partially explain the empirical observations. However, there currently seems to be no empirical indication that most NP-hard problems that are difficult in practice are difficult because of their close relationship to 3-SAT instances near the phase transition. So it does not make too much sense to develop a theory to "explain" something that does not appear to be true in practice. 

I don't know about the terms "efficient" and "feasible." Since these terms even today have no precise technical meaning, I suspect that the history of their usage will turn out to be murky, just as the history of most words in most languages is murky. "Computational complexity" is a more interesting term. With the help of MathSciNet, I find that Juris Hartmanis seems to have been the first to popularize it. The famous 1965 paper by Hartmanis and Stearns uses the term in the title, but even before that, Hartmanis's Mathematical Review of Michael Rabin's paper "Real time computation" (Israel J. Math. 1 (1963), 203–211) says: 

I'm both curious about specific useful applications, and any more broad complexity implications (for instance, would this get us any closer to $P=BPP$?). Edit: by the way, this problem was (is?) the subject of the Polymath4 project, which produced this writeup, but I do not see anywhere that they discuss implications of truth of the conjecture. 

For a rough answer, if you are new to TCS you can probably think of it as a sub-area of mathematics: Theoretical computer science consists mainly of proving theorems. If you want a contrast with standard "mathematics", TCS is (I think) primarily algorithmic: Focusing on the design, analysis, and theoretical capabilities/limitations of algorithms. Computer science can also be experimental, i.e. a natural science like physics or biology, but this tends to more often fall outside the realm of "theory". 

The main difference, in my view, is that DP solves subproblems optimally, then makes the optimal current decision given those sub-solutions. Greedy makes the "optimal" current decision given a local or immediate measure of what's best. Greedy doesn't reason about the subsequent choices to be made, so its measure of what's best is shortsighted and might be wrong. For example, a greedy pathfinding algorithm might always advance directly toward the target, since at each step this decreases the distance left to be traveled the most. But then it might run into a barrier and have to travel all the way around, resulting in a bad solution.