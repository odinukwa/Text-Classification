Claim. Within any ring of (outer) radius $d$, the bug travels a total of at most $10 d$ units. Proof sketch. WIthout loss of generality (by scaling) assume the outer radius is 1. Assume for contradiction that the bug spends more than 10 seconds in this ring before moving into the next ring (of outer radius 0.99). At any time $t$, consider the angle $\alpha(t)$ formed by the following two vectors: the vector pointing from the bug in the direction the bug is traveling, and the vector pointing from the bug to the origin. The bug is always moving towards the nearest point in the intersection of the discs placed so far, and that intersection is convex and contains the origin. Hence, the angle $\alpha(t)$ is always strictly less than ninety degrees, and the distance from the bug to the origin is strictly decreasing. Whenever the angle $\alpha(t)$ is, say, less than eighty-nine degrees, the distance to the origin is decreasing at rate at least $1/100$. But, during the entire time in the ring, this distance decreases by less than $1/100$, so the total amount of time spent in the ring when $\alpha(t) < 89$ is at most 1 second. Thus, at least $9$ seconds are spent with the angle $\alpha(t)$ being at least 89 degrees and at most 90 degrees. Now consider any such time $t$. Since $\alpha(t) \in [89,90]$, and the ring has width $1/100$, the bug is traveling either distinctly clockwise around the ring, or distinctly counter-clockwise. Let $p$ denote the point that the bug is moving towards (the closest point in the intersection of the discs placed so far). As the bug moves towards $p$, consider the line through the bug and perpendicular to the direction of motion of the bug. This line separates the plane into two half-planes, one "ahead" of the bug (containing $p$ and the intersection of the discs), and the other "behind" the bug. Mark the points in the half-plane behind the bug dead --- the bug can never return to any point once it is marked dead (because the point is not in the intersection of the discs). Since $\alpha(t) \in [89,90]$, and the ring has radius 1 and width $1/100$, almost half of the points in the ring are behind the bug and are dead, including the points immediately behind the bug. The bug cannot return to those points, so, if the bug is initially traveling, say, clockwise, then the bug cannot "turn around" and start traveling counter-clockwise (for more than say, $1$ second). Thus, of the $10$ seconds, the bug would have to spend at least $8$ seconds traveling clockwise. But the circumference of the ring is $2\pi< 7$, half of the ring is dead as soon as the bug starts, and the bug cannot return to any dead point, so this is impossible. This proves the claim (more or less; maybe somebody can give a more precise argument). 

Fix any $n$ and any arbitrarily small but constant $\epsilon>0$. Consider just the $n!$ "permutation" input instances $(x_1,x_2,\ldots,x_n)$ that are permutations of $[n]$. The optimum solution for any such instance has cost $n-1$. Define the cost of a permutation $\pi$ to be $c(\pi) = \sum_i |\pi(i+1) - \pi(i)|$. Model the algorithm as taking as input a permutation $\pi$, outputting a permutation $\pi'$, and paying cost $d(\pi,\pi') = c(\pi'\circ \pi)$. Define $C$ to be the minimum number of comparisons for any comparison-based algorithm to achieve competitive ratio $n^{1-\epsilon}$ on these instances. Since opt is $n-1$, the algorithm must guarantee cost at most $n^{2-\epsilon}$. We will show $C\ge \Omega(\epsilon n\log n)$. Define $P$ to be, for any possible output $\pi'$, the fraction of possible inputs for which output $\pi'$ would achieve cost at most $n^{2-\epsilon}$. This fraction is independent of $\pi'$. $P$ also equals the probability that, for a random permutation $\pi$, its cost $c(\pi)$ is at most $n^{2-\epsilon}$. (To see why, take $\pi'$ to be the identity permutation $I$. Then $P$ is the fraction of inputs for which $d(\pi,I)$ at most $n^{2-\epsilon}$, but $d(\pi,I) = c(\pi)$.) Lemma 1. $C \ge \log_2 1/P$. Proof. Fix any algorithm that always uses less than $\log_2 1/P$ comparisons. The decision tree for the algorithm has depth less than $\log_2 1/P$, so there are at less than $1/P$ leaves, and, for some output permutation $\pi'$, the algorithm gives $\pi'$ as output for more than a $P$ fraction of the inputs. By definition of $P$, for at least one such input, the output $\pi'$ gives cost more than $n^{2-\epsilon}$. QED Lemma 2. $P \le \exp(-\Omega(\epsilon n\log n))$. Before we give the proof of Lemma 2, note that the two lemmas together give the claim: $$C ~\ge~ \log_2 \frac{1}{P} ~=~ \log_2 \exp(\Omega(\epsilon n\log n)) ~=~ \Omega(\epsilon n\log n).$$ 

The distance $|ab|+|bc|$ the bug travels is $\Theta(\epsilon^2)$. The distance from the bug to the common point $p$ decreases from $\epsilon$ to $\epsilon-\Theta(\epsilon^3)$. 

Copying my comment on that from here: There exist published algorithms that support sampling from discrete probability distributions in O(1) time, AND modifying the distribution in O(1) time per update: 

FWIW, it seems likely to me there is a PTAS for the problem, following the basic idea in this paper. (This doesn't exactly answer your question, but I'll still describe the PTAS here in the answer section as it is too long to fit in a comment.) Fix any constant $\epsilon>0$. Let $p$ be an an instance of the problem, i.e., a probability distribution on $[n]$. Say that a code (a set of codewords) is $K$-fix-free if no codeword in the code that has length $K$ or less is a prefix or suffix of another codeword. Fix $K=\lceil 1/\epsilon^2\rceil $. Compute a min-cost $K$-fix-free code for $p$, in time polynomial in $n$, as follows. For each of the (constantly many) subsets $S$ of strings of length at most $K$, consider the $K$-fix-free code $C(S)$ formed by assigning, to the $|S|$ largest probabilities in $p$, codewords from $S$ (matching smaller codewords to larger probabilities), then enumerating (in order of increasing length) the $n-|S|$ strings of length larger than $K$ that have no prefix or suffix in $S$, and assigning these $n-|S|$ strings as the codewords for the remaining $n-|S|$ probabilities (in order of decreasing probability). Each subset $S$ gives a code $C(S)$; take $C_0$ to be one of minimum cost (by enumerating all choices for $S$). $C_0$ is a minimum-cost $K$-fix-free code for $p$. Note that the cost of $C_0$ is a lower bound on the cost of the optimal fix-free code for $p$, since the optimal fix-free code is also a $K$-fix-free code. Next, convert $C_0$ into a fix-free code, without increasing its cost by more than a $(1+O(\epsilon))$ factor, as follows. Within every codeword in $C_0$, insert an extra '1' into every (maximal) group of consecutive '1's of length $K' = \lceil 1/\epsilon \rceil$ or more. (This increases the cost by at most a $(1+\epsilon)$ factor, and the resulting code will still be $K$-fix free, and no maximal group of consecutive '1's in any codeword has length $K$.) Then, for every codeword in $C_0$ of length more than $K$, prepend $K'$ '1's followed by a '0', and append $K'$ '1's preceded by a '0'. (This modification uniquely marks the start and end of each codeword, making the code completely fix-free. The modification increases the cost by at most a $1+O(\epsilon)$ factor overall.) Take the resulting fix-free code $C_1$ as the solution. Since $C_1$ costs at most $(1+O(\epsilon))$ times $C_0$, and the cost of $C_0$ is a lower bound on the cost of the optimal fix-free code, the fix-free code $C_1$ has cost at most $(1+O(\epsilon))$ times the cost of the optimal fix-free code.