I'd like to slowdown music playback, say at half speed. I'm currently using SDLmixer. I assume I'd need to tell SDL that I have a 44KHz music and want to stream it at 22KHz, but when I pass in 22Khz it seems it re-samples my audio and plays at normal speed. Does anyone know how to do that, or any lib that supports it? I'm looking for free libs that allow me a commercial license (so no FMOD) Thanks in advance. 

I'm writing a MIDI file loader. Everything is going fine until at some track I get a failbit exception while trying to read from file. I can't figure out why, I've checked the file size and it's ok too. Upon checking "errno" and it returns "0". Any ideas? Thanks. The snippet follows: 

Answer by OFE in StackOverFlow: You will need to perform transformations to the texture co-ordinates yourself, you could do this in one of four places: -Apply the transformation to your raw model data. -Apply the transformation in the CPU (not recommended unless you have good reason as this is what vertex shaders are for). -Apply the transformation in the vertex shader (recommended). -Apply the transformation in the fragment shader. If you are going to apply a translation to the texture coordinates the most flexible way will be to use your maths library to create a translation matrix and pass the new matrix to your vertex shader as a uniform (the same way you pass the mMVPMatrix and mLightModelMatrix). You can then multiply the translation matrix by the texture coordinate in the vertex shader and output the result as a varying vector. Vertex Shader: 

Check this link for more info on ETC1 and alpha: $URL$ 2. Blending You'll also need to set blending. I don't know how to do that in Cocos2D, but in OpenGL it would be something like this: 

[EDIT: FIXED - Calling setting texture max level fixed that] I'm optimizing my game and I've just implemented compressed (DXTn) texture loading in OpenGL. I've worked my way removing bugs but I can't figure out this one: objects w/ DXTn + mipmapped textures are not being rendered. It's not like they are appearing with a flat color, they just don't appear at all. DXTn textured objs render and mipmapped non-compressed textures render just fine. The texture in question is 256x256 I generate the mips all the way down 4x4, i.e 1 block. I've checked on gDebugger and it display all the levels (7) just fine. I'm using GL_LINEAR_MIPMAP_NEAREST for min filter and GL_LINEAR for mag one. The texture is being compressed and mipmaps being created offline with Paint.NET tool using super sampling method. (I also tried bilinear just in case) Source follow: [SNIPPET 1: Loading DDS into sys memory + Initializing Object] 

Is there other,or correct way? In case of use vertex shader, how I change the color of enemy without losing all details(Something like a filter)? shaders 

I am new in shader concepts and I am trying to implement a sprite of 8x8 in OpenGL ES. I want to move the texture in the vertex shader but I cant figure out how to this, my code may be wrong, feel free to correct me If I change this line in the vertex shader, the texture scale but I want to move not scale!: 

I am trying to implement a sprite of 8 columns and 8 rows in OpenGL ES 2.0 I made appear the first imagen but I cant figure out how to translate the Texture matrix in OpenGL ES 2.0 , the equivalent of the code in OpenGL 1.0 that I am looking is 

Well, after a lot of more tests I realized that the problem was either in the way I was measuring time, which I find really unlikely due to the fact that I've tested many timers, or something in the graphics card that was preventing it from rendering to screen, like it was skipping frames. I found the exact term for that and it made a lot easier to search for the solution after. It's called (macro/micro) stuttering. It can be caused by a whole variety of this, not related to your game (yay! the art of PC game programming). It could be due to an anti-virus running, instant message programs interference, multi-GPU sincronization (micro stuttering), laptop clock variety due to power consumption settings, intel processors can give errors due to "intel speed stepping" that changes cpu clock based on cpu usage (that would probably mess up with my QFC timer) $URL$ it could be a driver problem (nvidia as of today 07/12/12 has a driver bug that causes micro-stuttering in single GPU for GTX 6 card family): $URL$ Well in my case I realized another particularity, as stated before that enabling vsync fixes the issue. What is probably happening is a tearing effect that gives the illusion of a frame skip, since the camera is fixed I don't see the "tear line", only notes scrolling weirdly. I can't really tell if a tearing is happening because it scrolls so fast, but I'm accepting it for now as vsync fixes the issue. I hope it helps someone out there struggling with this problem. 

I am new to shaders, and I am trying to tint a texture to mark status of a enemy. I am implementing shaders on OpenGL ES 2.0 (Mobile) The problem is that with this shaders, I see the enemy without change (even if I assign de fragcolor to (1.0, 0 , 0 ,1.0), the real code is commented). Vertex shader 

Please note: Your GLES 1.0 code does not actually perform a translation as you surrounded it with a push and pop. 

I think that you shoud name the Leaderboard something like "Number of puzzles needed", and if you want to show/track his own progress you can include average "puzzles needed" Something like 

I have 2 images that are .png and they mix together, The problem is that one overwrite the other. I know that the draw order matters, but In that case I should redraw the icons 2 times ( there is a lot of them) the black is in fact transparent and drawing a star on top of the other "cuts" a part of the underlying one, In other words, it overwrites with transparency How I see it now 

I'd like to know what are my options for high resolution timer in Window. I want a timer with at least 1ms precision since I need it for a rhythm game. I'm using QueryPerformanceCounter now, but I read there are many problems I can have with it (variable clock due to laptop power consuption, intel speed stepping, multi-core sync gives negative times, etc). I tried SDL_GetTicks() but I realized it is implemented using QPC in Windows. What other options do I have? Thanks. 

Now, if you want a texture compression that supports alpha you can use ETC2 (all opengl es 3 devices support it), and for better performance and quality ASTC. Not many devices support it now but the quality difference from ASTC to ETC1 is brutal, plus ASTC supports alpha.