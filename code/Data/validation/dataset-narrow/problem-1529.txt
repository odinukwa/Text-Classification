Stop using SMOTE. Ensure the training set has the same distribution as the test set. SMOTE might actually be unnecessary in your situation. Continue to augment the training set using SMOTE as you're currently doing, and compensate for the train/test mismatch by shifting the threshold for classification. Logistic regression produces an estimated probability that a particular instance is from the positive class. Typically, you then compare that probability to the threshold 0.5 and use that to classify it as positive or negative. You can adjust the threshold to correct for that: replace $0.5$ with $0.5/k$, where $k$ is the ratio of positives in your training set after augmentation to positive before (e.g., if augmentation shifted the training set from 0.5%/99.5% to 10%/90%, then $k=10/0.5=20$); or you can use cross-validation to find a suitable threshold that maximizes the F1 score (or some other metric). 

The answer depends on the task. Template matching can work for some tasks but not for all. CNNs potentially have the ability to generalize to unseen inputs that don't match any of your templates, so can potentially generalize better. But whether CNNs will beat template matching will depend on the specific task and what specifically you're trying to achieve. This is an empirical science; ultimately, the way you find out which works better is to try them both -- or learn from others who have tried them (e.g., by reading the literature). I don't think you're going to find some theory or taxonomy that is going to substitute for empirical evaluation on real-world data. 

You should start by trying OCR. I am not sure why you are rejecting it without trying it. Start with Tesseract (free but not very good) and then try a commercial OCR as well (Abbyy is well regarded, but you could also try Adobe Acrobat). Also, research document structure extraction. That idea that you are going to re-invent a machine learning model that does OCR better than existing OCR solutions seems ... not very realistic. Current OCR tools have years or decades of engineering put into them. There is no way you are going to be able to afford to put that much effort into your own custom tool. Image quality on those example images is not wonderful, and you might have to accept that accuracy will be less than 100%. 

All four hypotheses are equally consistent with the data. The data set is useless at helping you distinguish between them. Yet each hypothesis leads to a different prediction about which cats to feed: the first two say it doesn't matter, the third says you should feed the ones your friend likes, and the fourth says you should feed the ones your friend dislikes. No amount of statistics or machine learning is going to enable you to make a decision based solely on the data. To distinguish between these possibilities, you either need some kind of prior that will give you a basis for choosing among the models that are consistent with the data (perhaps based on your domain knowledge), or you need to conduct a controlled experiment (where the choice of whether to feed a cat is randomized, rather than being selected based on whether your friend likes the cat). 

It could be a bug in your code, problems with your training set (maybe you don't have the file format quite right), or some other implementation issue. Are you sure you want to use a sigmoid activation function in your last layer? I would have expected that the normal approach would be to use a softmax as the last layer (so that you can treat the outputs as the probability of each class, i.e., so that they're normalized to sum to 1). You might try that. Alternatively, this might be a 'class imbalance' problem. Do some searching on that term and you'll find a bunch of standard methods for dealing with it. You can balance the training set, or use 'weights' on the instances, or adjust the threshold based on the priors. However, as others have pointed out, the imbalance is not severe enough that I would have expected it to cause this strong of an bias. It's also possible that your features are useless and don't help predict the output (e.g., they are not related to or correlated to the output). That would also be consistent with what you are seeing. 

You can test each model with default parameters, and keep the model that seems best. Or, for each model, you can tune the parameters for that model, to see what is the best that's achievable for each model (by separately tuning its parameters) -- then keep the best model. This takes more computation power but might be slightly better. Yes, parameter tuning depends on the model. Each model may have a very different set of parameters, with entirely different meaning. You can't take parameters for a random forest and then try to use them as parameters for a SVM (for instance); that doesn't even make sense, as they have different parameters. Yes, there are many possibilities on what order you do feature selection vs choosing a model vs parameter tuning. I don't know what the "feature weights of FeatureUnion" are, so I can't answer that part. 

This led to the Siku software package. There's probably lots of other work out there; those are just two papers I happen to be familiar with. The actual machine learning needed is not too sophisticated. This is a basic object detection task. If you put together a good training set and train a convolutional neural network, I would imagine that the result should be pretty effective. 

If you feed the output of the LSTM directly into a softmax, you probably won't get good results. If you use a softmax layer after a tanh layer, bad stuff happens. As you say, the confidence will never get near 100%. For instance, if there are two classes, you can never get above about 88% confidence. If there are $k$ classes, you can never get confidence above $e/(e + (k-1)/e)) = e^2/(e^2 + k-1)$. So, rather than directly feeding the output of the LSTM directly into softmax, you can instead use the output of the LSTM as the input to one or more (fully-connected) layers of neural network. 

You'll have to decide how you want to assign the grades. You might want to think about what properties do you think should correspond to higher grades. This doesn't seem like a machine learning problem in its current form, it's just a matter of choosing an evaluation metric. If you had labelled data (for some mailing lists you had grades assigned) then you consider consider applying supervised learning methods. Without that, there's nothing for a ML algorithm to get a foothold with. 

It seems likely that the live data is different somehow from your other data. Cross-validation shows a 84% accuracy, and accuracy on the held-out set is 86%, which is pretty consistent and does not indicate overfitting. Accuracy on the live data is 70%, which is significantly different. That suggests that live data is somehow different in ways that are important to the classifier. Perhaps concept drift has occurred. 

No. Classification requires labelled data. Without labelled data there is no way to solve this. How would you do anything at all, if you don't know which of the products in the training set are good and which are bad? There's no basis for making a decision of any sort. 

Yes, it's possible to use this confidence data. However, I wouldn't recommend the approach you mention. Instead, let me suggest a different approach. Actually, I'll suggest two. The first one is conceptually clean; the second is probably easier to implement; and they'll probably be approximately equivalent in practice. Adjust the loss function You can adjust the loss function to reflect the confidence scores you have on the training data. In particular, if you are using the cross-entropy loss, there's a particularly clean way to do this. Let me explain some background on the cross-entropy loss, then explain how. We think of the label on the training instance as a probability distribution on labels. In binary classification, such a distribution can be represented as a vector $(p_0,p_1)$ where $p_0$ represents the probability that the label is 0 and $p_1$ the probability that the label is 1. Normally, we are given "hard labels": if we know that the correct label on instance $x$ is 0, then that corresponds to the probability distribution $(1,0)$; if the correct label is 1, that's the distribution $(0,1)$. The cross-entropy loss then compares the prediction from the classifier to this distribution. The nice thing about the cross-entropy loss is that it generates readily to comparing any two distributions. Thus, if you have a confidence $0.8$ that the correct label for instance $x$ is 0, then that corresponds to a probability distribution $(0.8,0.2)$. Now, you can compute the cross-entropy of the classifier's prediction with respect to the distribution $(0.8,0.2)$, and that is the contribution to the loss from training instance $x$. Sum this over all instances in the training set, and you get an adjusted loss function. Now you can train a classifier by minimizing this adjusted loss function, and that will directly incorporate all of the information in your confidence scores. Use weights Alternatively, you can use weights to reflect the confidence information. Some classifiers allow you to specify a weight for each instance in the training set. The idea is that a misprediction for a particular instance is penalized proportionality to its weight, so instances with a high weight are more important to get right and instances with a low weight are less important. Or, equivalently, the training procedure tries harder to avoid errors on instances with a high weight. You can use weights to reflect confidence information. Suppose you have an instanced $x$ in the training set that you think should have label 0, with confidence $0.8$. You would add one copy of $(x,0)$ to the training set with weight $0.8$ (i.e., instance is $x$ and label is 0), and add one copy of $(x,1)$ to the training set with weight $0.2$ (i.e., instance is $x$ and label is 1). Build up the training set in this way. This doubles the size of your training set. Now train a classifier, using these weights. For classifiers that support weights, this should be easy to implement. One can also show that it is effective and reasonable. For instance, when using the cross-entropy loss to train a classifier, this approach using weights is equivalent to adjusting the loss function as highlighted above. So, in that particular context, the two approaches are actually equivalent. 

Imputation and dealing with missing data a broad subject; you should start by researching standard material on this subject. The first question to figure out is Why is some data missing? and What is the process that causes data to be missing? It's important to understand how this happens, because this will affect what solution is appropriate. Randomly missing data If data is missing totally at random (whether a value is missing does not depend on any of the feature values of that item), then imputation can be appropriate. It should not create bias, if you do it appropriately. There are many techniques for imputation. You don't mention what you tried or why you think it will bias your results, but in general, if you use an appropriate method of imputation, there is no reason why it needs to bias your data. Alternatively, you can use a classifier that can tolerate missing data. Some classifiers are designed to handle missing data and can tolerate it. However, I don't know of any reason to use them over imputation. Non-randomly missing data In contrast, if the chance for data to go missing for some object depends on the value of the features of that object, then you have a bigger problem. In that case imputation can create bias -- as can any other method. Your best hope is to understand in greater depth the random process that causes data to be missing and the probability distribution (probability that data goes missing, as a function of feature values), and try to design a procedure that is appropriate for that process. Your specific situation: all features missing Your specific situation is especially weird: it appears in your case either all features are missing, or none are. That's a weird one. For instances where the features are missing, you have absolutely no information about those instances. So, the best classification decision in that case is probably a very simple rule: take whichever class appears most frequently in your training set (or, most frequently among instances with missing data). Run the classifier on the remaining instances, i.e., the instances with no missing data. But in real life this situation is pretty rare. It's more typical that some features are missing and others are present, and that requires more work to handle. 

Use a framework for ranking that is designed to supporting ranking, such as the Bradley-Terry model. See also Elo rankings and all the general statistical theory on pairwise comparison.