Source differences only, all the where in static code the 32 bit address part is enough, have to change to full 64 bit in PIE, i.e.: 

One more note about the rest of code: I don't like how you use plenty of additional temporary vectors. When you want to enjoy C++ performance boost, you have to be a bit more aware of data structure, as that's the major advantage of C++ over other high-level languages. Usually the sorting algorithms are implemented to either work above the initial container memory without any temporary, or when temporary is required, only single secondary vector of full size is created. Then internal calls pass the first/last iterators to point to the parts of the vector memory, which should be processed in the particular inner call. If you need temporary vector, one of full size should provide enough temporary space for the operation, being properly partitioned by first/last iterators. I'm sorry to not provide the example, but I believe you can find some merge sort implementations on the Internet, probably showing operation on 1-2 vector's only without the copying of content between internal calls. 

one way is class holding the actual string, responsible for manipulation with it. other way is helper utility class providing only functions, not holding any string at all. 

Hmmm... anyway, I really like what you wrote here: "I'm noticing that reading doesn't really help much as much as writing programs and running into issues and solving them." This is absolutely spot on. All this general babbling around usually makes little sense when hit by real world problem. Also refactor a lot. Not just the code, and implementations, but as learning is your goal, dive deep into finished project and go trough all the API, all the abstraction, and try to imagine something better, easier to use, easier to understand it's responsibilities, easier to read and get the idea and eventually easier to implement (although sometimes to get better API and abstraction the implementation can be more bloated, but the cost of bloat has to be clearly justified by the cleanliness of resulting API/abstraction). 

I am doing interview studying and would like to have another look at my implementation of Floyd's cycle detection algorithm. I've implemented a simple linked list which can be fluently configured and works generally for any type on stack. The cycle detection method serves: 

A cycle should be found in worst-case O(n) time complexity and O(1) space consumption. The same applies for retrieving the cycle start node. Apart from correctness, I've strived for client simplicity, null-safety and clarity - would like to hear thoughts on how to improve this code and/or add more test cases. Here is the data structure: 

Is there a lingering quadratic behavior in my implementation? Is there a better way to test the than relying on String comparison as I have done? I could compute a hash and compare that but that is just a variation on the same theme. I am trying to hide as much implementation details as possible - so any Node leaking, I would dislike. Is such a data structure ever useful? Save for perhaps cryptography, I can't imagine a use case for it - but surely, there are far better crypto-algorithms available. 

Create additional identical list during cloning ('hinging' the cloned one off of the original list) and through clever pointer massaging, split the two and return one of these as the cloned list (O(n) in time, O(1) in space) Use a HashMap and two passes over the list during cloning to first clone the list and then assign the random pointers using the (O(n) in time and space). 

I found both approaches a bit confusing - it feels like is too involved, and especially in 1), prone to nasty pointer-related bugs. On the other hand, I've strived for clean, compact, fast and easy-to-read and operations without pointer mess or two passes over the same data structure - at the expense of extra memory and sacrificing concurrency (I am maintaining a static ). More precisely, is O(1) and shoud be exactly O(n). Discussion While the implementation runs correctly for lists of about <= 100 elements and while I am reasonably confident the clone operation is indeed linear time, cloning scales really poorly (and varies wildly in execution time) and I am not sure why that is so. After running dozens of experiments and tests, empirical evidence suggests after 115 elements or so, cloning takes upward of 30 seconds (when I usually stop the test). For comparison, cloning 10 elements takes the order of 10-100 ms and cloning 100 elements anywhere from 500-4000 ms. Unless I am not seeing a glaring super-quadratic algorithm, the only explanation I can think of is that larger element sets tend to form cycles more quickly and randomly, so the recursive gets stuck in loop - but this is just a conjecture which I am not sure how to prove (except brute force). There are ways to detect cycles in linked lists (e.g. Floyd's algorithm variations) but the assignment doesn't even mention these and I certainly don't think implementing such cycle detection would be in the spirit of this exercise. I am also aware that my recursive would fail with SOE for large collections with languages without tail-recursion optimisations (i.e. Java) but rewriting the method to iterative approach doesn't seem difficult. Questions 

You don't need to check size() for even/odd value, the integer division will work in your favour in this case, so 3/2 = 1. And random_access/bidirectional/forward iterators have overloaded operator, so you can add the result directly to . The would be helpful when you would use container which has only available (can increment, but only by single step). Then it can be still somewhat simplified to: 

(the exact value stored in is , encoded as 64 bit integer value) So you are just destroying precision bits of the original value, but the resulting value can still contain decimals with precision you don't expect. For some more information study how the floating point numbers are designed, maybe this may be of help: $URL$ 

The initial comment: not clear what is entering (deducting it's about byte ) and I would rather use "arguments" or "input" word. Not clear the description is about bits. Not all arguments are described (the code does use also address for ). Typos. Modified registers are incomplete too (again ). Etc.. 

The Model/View relation - how I like it - can be demonstrated on this classic: Consider having blog application, so for each article you have date+time of publishing the article. Then Model should contain UTC timestamp value (can be unix timestamp, if you need only dates since 1970 onward). And View will do all the formatting magic, ie showing "5 seconds ago" for fresh article, or "previous millennium" for some really old article, also converting the date/time to local time zone of user (source data stored on server being in UTC, time zone agnostic). 

I found another bug in your source: will be always false. Unrelated to your bug, I have read an advice somewhere to always use only "<", "<=", "==" and "!=" in comparisons. It felt strange for few weeks, but once you get used to it, it really makes easier to read sources, as you know the values should only increase from left to right, if the expression is true. So I would write your expression as: .