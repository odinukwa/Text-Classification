They approximate indirect lighting in a local illumination light model. In other words, the interaction between lights bouncing from one surface to another is not a part of local illumination; we call the light model where bounces are considered global illumination. Ambient lighting and occlusion simulate accumulated light (or find areas where indirect light has difficulty accumulating in the case of occlusion) without actually doing the complicated work of bouncing photons around the scene. This is a necessary hack for lighting in general purpose real-time rendering. 

I think you may be interested in . That affects the mapping from NDC [-1.0,1.0] to window-space Z (the depth buffer stores window-space Z) during the viewport transformation. Effectively the nearplane becomes and the farplane becomes . You can fool with this to bias and/or rescale your assigned depth values, but GL clamps these values to [0.0,1.0]. Since all testing is done in window-space, you can simply modify the mapping between passes to make depth tests more "permissive." If you are clever, you can even reverse the mapping and the direction of the test to alter precision distribution. 

Find the difference between two angles and . Fix any angle differences that are greater than one full rotation. If angle exceeds 180 degrees, compare in the other direction. 

It is not in perspective because does not divide by . Given a typical perspective projection matrix, the component of the last row/column is usually +/- 1.0 and everything else in the row/column is 0.0. OpenGL Perspective Projection Matrix (right-handed eye-space --> left-handed clip-space):       $URL$ This produces in the transformed vertex (depending on whether the projection matrix is setup to flip the Z-axis - GL flips the Z-axis, D3D does not). Thus the homogeneous divide will actually scale geometry based on distance down the Z-axis and this is why it is sometimes called the perspective divide. 

But be aware that you have to create and destroy a render context just to load and use this extension. It is rather convoluted considering the whole point of this extension is to create a render context. That's just how WGL works unfortunately. 

You can amortize some things over multiple frames. I am sure consists of multiple potentially independent tasks, that you could easily queue up. Rather than completely deferring distant/unimportant unit processing and then having the unpleasant requirement of completing the entire update in the span of a single future frame, you could simply do half of their processing on one frame and the other half the next and only commit the end result when all sub-tasks are done. The net result is the same as if you only updated them once every other frame, but you will have less trouble meeting deadlines in some cases if you split the work that way. I realize that    realtime in this question is not so much about real-time computing :P But the whole thought process behind meeting deadlines is still useful... you can prioritize certain tasks such as position update so that they appear more responsive to the player. Even if a unit is not completely finished thinking, if it knows where it should be spatially before the next frame is drawn, go ahead and put the unit there. You can vary the priority of each sub-task inside of your according to distance, age, rank, or any number of different metrics. In fact, rather than calling an function at all per-entity, you might consider a (priority) queue of tasks that need to be done. Your entities can add their tasks to this queue every time they need to think, and doing it this way makes distributing the workload across threads a lot easier. The bottom line is that generally not everything needs to be finished every frame. If your requirement is that a unit has to completely update itself (complete every one of its update tasks) in the period of one frame, then that can create some awkward situations where framerates start to vary wildly even if you do only process 1/N-many units per-frame. If, instead, you prioritize certain aspects of unit updates you can do the important things more frequently and even tolerate situations where some update tasks take an unpredictable amount of time to complete. 

I would add to this that using client memory to store your vertices is potentially more detrimental to performance than the number of draw calls. The reason for this is that if GL does not have full control over the vertex memory it has to either make a copy of your vertex array every time you draw or synchronize the pipeline (e.g. will not return until GL finishes using the data). When you use VBOs, GL manages the memory and it knows when the data changes so can safely be implemented asynchronously / without making an immediate data copy. Hundreds of draw calls using VBO-managed memory can be quicker than a handful of client memory draw calls as a result. For this reason, immediate mode (which creates new vertex data each time it draws) can actually be more efficient in certain circumstances than using vertex arrays. Granted, using client memory to store vertex arrays has been deprecated for many years as has immediate mode, but since you brought up the fixed-function pipeline it was worth mentioning. 

Clip-space does not have the range [-1,1] that you are describing. That is NDC space, which is achieved by dividing the clip-space coordinate by its W component. What is more, depending on the API, NDC may not be [-1,1] in all directions. This is the case in OpenGL, but in D3D the Z component ranges from [0,1] in NDC while X and Y range from [-1,1]. I mention this because unless you have an orthogonal projection (and even then, W is not guaranteed to be 1.0), W is going to vary by distance in clip-space in order to produce perspective and therefore the actual frustum shape. Now, this frustum shape only applies in other coordinate spaces. Once you get into NDC space, the viewing volume is a cube (or in the case of D3D, half of a cube) and all of the geometry has been warped for perspective. At this point all you need to do is test your object's bounding volume (sphere in your case) against this cube. In short, frustum culling does not apply if you transform your coordinates as far as you seem to be proposing. The shape of the viewing volume is no longer a frustum. The general idea behind frustum culling is to avoid transforming the input objects that far and cull them in a more natural space such as world, where the shape of the viewing volume is actually a frustum. 

Translate after rotation. If you rotate while the object is still in object-space, it will rotate the object around its center. If you rotate after it's been translated into world-space you're in for a real nasty surprise. More often than not, the object will rotate its way off-screen. In short, if you want to rotate an object around the object's center, then the object has to be centered at the origin. This is a very common problem, I would suggest you lookup rotation around an arbitrary point for a more thorough understanding. Also, why are you loading the transpose matrix? Is your matrix math library row-major for some reason? I'd suggest you use at that point... 

If you want on-demand loading of "mipmap layers" there is actually a new hardware feature in DX 11.2 class hardware for this exact purpose. AMD introduced it first to OpenGL as sparse textures, it virtualizes the memory used by textures and allows individual LODs to be demand-paged the first time they are actually referenced in a fragment/pixel shader. D3D 11.2 standardizes this feature and calls it "tiled resources." This is honestly the only time you will ever (portably) have fine-grained knowledge of how the driver is managing your texture's memory at the LOD level. Using this hardware-level feature, you can even tell inside your shader, whether an LOD is resident or not. This is very handy because demand-loading any resource introduces quite a bit of latency, and you want to be able to fallback to a lower LOD while you load the LOD into memory rather than suddenly increasing the time taken to render 1 frame by a factor of 10. 

I do not think this answers your question in the general case, this is more specific to a frustum, and assumes that you know the 8 vertices formed by the intersecting planes ahead of time. Effectively, you need to know the distance from the center of your frustum to the 8 points where 3 planes intersect. What you do not need is to know the Euclidean distance for all 8; squared distance will work just fine. On the range [0,oo): 

You have to define that function pointer yourself. It doesn't exist in the OpenGL implementation that ships with Windows and this is why you have a problem on that line labeled . At the very least, you need something to the effect: 

It all comes down to memory bandwidth with proper anti-aliasing techniques (e.g. MSAA, SSAA, CSAA). While 8x SSAA and 8x MSAA have identical storage requirements (8x), the workload between the two algorithms is quite different. Multisample anti-aliasing adds some intelligence to the rasterization stage to reduce the number of fragments that have to be shaded, thus reducing the computational expense of anti-aliasing. However, the covered samples generated during rasterization still have to be written and averaged and this eats through a tremendous amount of memory bandwidth. Newer image processing techniques (e.g. FXAA, MLAA) approach the problem from an entirely different perspective. They do not seek to solve the fundamental problem that leads to aliasing in the first place (inadequate sample frequency), instead they keep initial rendering as simple as possible and "fix it in post" using a complicated shader. Given the trend in hardware to increase compute power quicker than memory bandwidth and advances in display resolution, these compute-intensive approaches definitely have sticking power. Nevertheless, I have a hard time calling them anti-aliasing, as all they really do is mask aliasing after the fact. If you are speaking of an anti-aliasing approach whose quality is measured in terms of sample rate, then the algorithm is of the memory hungry variety first discussed. The newer shader-based techniques have a myriad of implementation-specific properties that you can tune, and this is why a game that uses something like FXAA will sometimes offer vague "Low", "Medium", "High", etc. quality settings but nothing quantitative.