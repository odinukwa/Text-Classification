Two integers $x,y$ mod $n$ are equivalent if $x^{2} \equiv y^{2}$ mod $n$. If one could easily compute a class representative for this function, then factoring can be done in probabilistic polynomial time. In general, such an example would imply that $P \neq NP$. Suppose $R$ is an equivalence relation that is decidable in polynomial time. Then by lexicographic search using an $NP$ oracle, one can find the lexicographically least element in the equivalence class of any string. If $P=NP$, this becomes polynomial time, so you can use the lexicographically least equivalent string as a class representative. This observation is originally due to Blass and Gurevich [1]. Such an example would also imply $UP \not\subseteq BQP$ (and hence, in particluar, $P \neq UP$). The question you've asked is exactly what we denoted $PEq =? Ker(FP)$ in our paper with Lance Fortnow [2]. That paper also includes the results I've stated here, as well as the example of hash functions pointed out by Peter Shor, a few other possible examples, and related results and questions. [1] Blass, A. and Gurevich, Y. Equivalence relations, invariants, and normal forms. SIAM J. Comput. 13(4):682-689, 1984. [2] Fortnow, L. and Grochow, J. A. Complexity classes of equivalence problems revisited. Inform. and Comput. 209(4):748-763, 2011. Also available on the arxiv. 

[GRW] show that computing the minimum number of mismatches is $\mathsf{NP}$-hard even on trees. They also show that in the weighted case, it is $\mathsf{NP}$-hard if either weighted adjacency matrix has unbounded rank or negative eigenvalues (so the easy cases are a subset of PSD matrices of bounded rank). This problem is also sometimes called "graph similarity" (which I consider a much more general term) or "graph matching" (which you'll have a hard time Googling because you'll get a lot of stuff about matchings), and has apparentely been studied more in the ML and data mining literature. 

[Note: I believe this question in no way hinges on the correctness or incorrectness of Deolalikar's paper.] On Scott Aaronson's blog Shtetl Optimized, in the discussion about Deolalikar's recent attempt on P vs NP, Leonid Gurvits made the following comment: 

I'd recommend the results in The Complexity Theory Companion by Hemaspaandra and Ogihara. It's organized around techniques rather than results, though often the technique was developed for a particular result, and it covers several seminal results and important proof techniques. 

Even higher-dimensional WL is known not to work in poly time on graphs of degree 4 (Cai-Furer-Immerman). I do not know if higher-dimensional WL might work on graphs of degree 3, but I also don't know of a result ruling this out either. Aside from Babai's general algorithm (which is quasi-polynomial, not polynomial), I do not know another algorithm for degree 3 other than the one you mention. 

In a different but somewhat related vein, Smale used topology in a pretty interesting manner (in particular, cohomology of the braid group) to lower bound the complexity of root-finding in the Blum-Shub-Smale model: 

Yes, but most of the work so far (except very recently, see below) has focused on turning irreversible computations into reversible ones, thereby hoping to avoid any entropy generation. (Note: there is an important difference between energy needed to make a computation run, and entropy generated by the computation and put out into the environment, typically in the form of heat.) Based on Landauer and Bennett's original analysis that erasing a bit "must" generate $kT \ln(2)$ entropy (see below for why there are scare-quotes), several researchers have pursued various questions along these lines. One line of research was to simulate irreversible Turing machines by reversible ones, which, it was suggested, would generate no entropy. There are several works showing space-time tradeoffs for how to simulate irreversible TMs by reversible ones, e.g.: 

As mentioned by @SureshVenkat, you can kind of view Geometric Complexity Theory in the light you're talking about. However, the algebraic objects used there - namely representations - are more akin to general properties of a language than to numerical properties per se, but at least they are properties of an algebraic flavor. 

Here is an example where the complexity of function classes and their associated language classes seem to differ: $\mathsf{P}^{\mathsf{NP}[\log ]} = \mathsf{P}^{\mathsf{NP}}_{tt}$ [Wagner 1987 "Log Query Classes", Hemaspaandra's 1987 thesis, Buss & Hay 1991], but if $\mathsf{FP}^{\mathsf{NP}[\log ]} = \mathsf{FP}^{\mathsf{NP}}_{tt}$ then $\mathsf{NP} = \mathsf{RP}$ and $\mathsf{P} = \mathsf{UP}$ [Selman 1994]. (Here the oracle $^{\mathsf{NP}[\log]}$ means the machine gets to make $O(\log n)$ queries to any problem in $\mathsf{NP}$ (say, SAT). The notation $\mathsf{P}^{\mathsf{NP}}_{tt}$ means $\mathsf{P}$ with an $\mathsf{NP}$ oracle, but in which the oracle queries are non-adaptive: on input $x$, the if $y_i$ is the $i$-th string queried to the oracle, then $y_i$ does not depend on the answers to any previous oracle calls. Equivalently, on input $x$ the machine builds a list $y_1, \dotsc, y_m$ without querying the oracle, queries the oracle about all of the $y_i$ and gets the answers, then proceeds to compute without querying the oracle again.) 

As to why GCT is considered plausibly capable of showing $P \neq NP$ many answers have already been given elsewhere and in the comments above, though I think no one has yet mentioned that it appears to avoid the known barriers (relativization, algebrization, natural proofs). As to its value - I think even if it takes us 100 years, we will learn something new about complexity along the way by studying it from this angle. 

Well, here's a not-horrible upper bound over $\mathbb{C}$: $\mathsf{PSPACE}$, or assuming the Riemann Hypothesis, $\mathsf{AM}$. This is because for any given patterns of zeros for $A,B$, checking whether one can make $AB=I_n$ is checking whether a certain system of $n^2$ integer polynomial equations has a solution in $\mathbb{C}$, and this can be done in these upper bounds, by Koiran. Another approach is to try to leverage the fact that this is in fact a system of bilinear equations. Solving bilinear equations is equivalent to finding "rank 1" solutions to linear equations. I've been trying to determine if there are better upper bounds for solving bilinear systems in general, but with no luck so far. It's also possible that one could leverage the particular structure of these bilinear equations to get something better than what's known in general... 

PH has complete problems if and only if it collapses: if it has a complete problem $L$, then $L \in \Sigma_k P$ for some $k$, so $PH = \Sigma_k P$. Conversely, if $PH$ is finite, then $PH = \Sigma_k P$ for some $k$, and $\Sigma_k SAT$ is then PH-complete. As pointed out by Srikanth, there are oracles relative to which PH is infinite. (In fact, finding such oracles was part of the reason people started looking at PARITY not in $AC^0$ in the first place.) Using similar circuit-based techniques, there is also, for every $k$, an oracle that collapses $PH$ to exactly $\Sigma_k P$ (Ker-I Ko, SICOMP 18(2), 1989). For those who are interested, I recommend Ker-I Ko's survey. 

This paper shows (though I have not verified it) that 3-sphere recognition* is in coNP assuming GRH: 

This is perhaps a nearly trivial observation, but I couldn't think of another general property just of the automorphisms that would ensure the limiting distribution is uniform. If the automorphism group of the corresponding weighted directed graph is vertex-transitive, then the limiting distribution must be uniform, since then no vertex can be distinguished from any other. 

One-way functions also featured in some discussions around the Berman-Hartmanis isomorphism conjecture. Joseph and Young conjectured that if one-way functions existed then the isomorphism conjecture fails (one-way against deterministic adversaries, not probabilistic ones, but hopefully that's close enough for the purposes of this question). John Rogers gave a relativized world where the Joseph-Young conjecture failed (that is, where one-way functions exist but the isomorphism conjecture holds). But as far as I know the JY conjecture is still one of the main pieces of technical evidence that lead people to think the Isomorphism Conjecture is false (if they do think that). The essence of the idea of Joseph and Young is that if $f$ is a one-way function, then $f(SAT)$ is $NP$-complete but "shouldn't" be isomorphic to SAT. 

There are several notions of randomness in computability theory (/the arithmetic hierarchy; lookup "Martin-Lof randomness", "Kurtz random", "Schnorr random", ...), but I think the ones that are analogous to $\mathsf{BPP}$ become trivial in the setting of the arithmetic hierarchy. The reason is essentially that a randomized Turing machine with bounded error can be simulated by a deterministic one: the deterministic one simulates the random one with all settings of the randomness and then takes the majority vote. If the original machine took time $t(n)$ then the new machine takes time $2^{t(n)}$, which is why this trick tends not to work in the resource-bounded case. (Though of course it works for any deterministic time-bounded class where if time bound $t(n)$ is allowed in the class then so is $2^{t(n)}$. In particular, this trick works in $\mathsf{DTIME}(\mathcal{E}^4)$, where $\mathcal{E}^4$ is the fourth level of the Grzegorczyk hierarchy of primitive recursive functions. But that's still a pretty big class.) 

Now, if $s$ is computable, then so is $D$, and furthermore, $D$ is in $P^{(2)}_1$, so there is some index $(a_d,b_d)$ for $D$. Then consider $D((a_d,b_d))$: if $s((a_d,b_d))=1$ then $D((a_d,b_d)) = \psi_{(a_0,b_0)} \notin S$, and if $s((a_d,b_d))=0$ then $D((a_d,b_d)) = \psi_{(a_1,b_1)} \in S$, so either way $s$ gets it wrong. (1b) The above relied on the fact that although we don't have a total computable compiler from $\varphi$ to $\psi$, we at least had a partial one which covered a nontrivial set of functions. I wouldn't be surprised if you can cook up a realizable numbering where there's essentially no such partial compiler. (2) Since a $\psi$-Rice's Theorem fails for $\{f\}$, $\psi$ also must not satisfy the S-m-n Theorem (currying). In particular, there is some $n$ such that $\psi_n(x,y) = \varphi_x(y)$ (i.e., a universal TM). But then there is no computable program transformation $t$ such that $\psi_{t(m,x)}(y) = \psi_m(x,y)$ for all $m,x,y$, for if there were, $t(n,x)$ would enable you to decide whether $\varphi_x = f$, which is undecidable by the usual Rice's Theorem. Currying is a pretty basic feature for a programming environment to lack. I think one can prove a similar thing not just for currying, but for infinitely many program transformations, probably whenever the transformation is not finitely different from the identity map and has $f$ in its image. 

One thing they show is that one can test (with high probability) if an $n \times n$ matrix has rank $\leq r$ or requires an $\varepsilon$ fraction of its entries to be changed in order to get rank $\leq r$ by querying only $O(r^2 / \varepsilon)$ entries of the matrix. 

$\mathsf{L}=\mathsf{NL}$. Yes, there is an analog of Mahaney's Theorem for log-space. If there is a sparse set that is hard for $\mathsf{NL}$ under logspace reductions, then $\mathsf{L}=\mathsf{NL}$: J.-Y. Cai and D. Sivakumar. Resolution of Hartmanis’ conjecture for NL-hard sparse sets. Theoret. Comp. Sci. 240(2):257–269, 2000. van Melkebeek extends this to logspace bounded truth-table reductions, which is quite close to the state of the art for extensions of Mahaney's Theorem in the polytime world as well: D. van Melkebeek. Deterministic and Randomized Bounded Truth-Table Reductions of P, NL, and L to Sparse Sets. J. Comput. Syst. Sci. 57(2):213-232, 1998. (I think the reversed dates are simply because these are the journal publications rather than the corresponding conference publications.) 

Some observations: 1) For any NPI set $L$, there is an isomorphic set $L'$ and a set $S$ such that $L' \cup S$ is NP-complete. Let $L' = \{0x : x \in L\}$ and let $S = \{ 1x : x \in SAT \}$. This observation applies in a more direct manner to some of the natural suspected NPI sets (such as $GA$ -- see my comments on turkistany's answer). It will also apply to the padded version of any set in $NEXP \backslash EXP$ (assuming $NEXP \neq EXP$, such a set is in NPI), for most reasonable ways of padding sets. This rasises a potentially interesting question: 

$\mathsf{NL} \neq \mathsf{coNL}$. Prior to the result that these two were equal, I think it was widely believed that they were distinct, by analogy with the belief that $\mathsf{NP} \neq \mathsf{coNP}$ (i.e. the general principle that "nondeterministism and co-nondeterminism are different"; this turned out to be false under space complexity bounds that were at least logarithmic). 

My favorite example to use with non-CS friends is this one: Abraham, A. Blum, Sandholm. Clearing algorithms for barter exchange markets: enabling nationwide kidney exchanges. EC07. Kidney exchange markets are essentially a restricted form of cycle cover. I like this example because a) it's easy to explain the gist(if you leave out some of the more technical details) and b) it's one of the few instances I know of where better algorithms can literally save lives! My second favorite example is the hospitals-and-residents problem (aka the college admissions problem). Each hospital ranks all residents (graduating medical students) and residents rank hospitals. Each hospital has a certain number of slots. From there it's a stable matching problem and can be solved in polynomial time. But in reality, couples can enter the system (yes, there is indeed a system) together, so that the system won't, for example, split up married couples who are both applying for residency. The addition of couples makes the problem NP-complete. In addition to being easy to explain, this nicely demonstrates how the introduction of long-range connections can induce NP-completeness. 

Therefore, deciding the existence of integer roots or of rational roots with odd denominators for polynomials of the form $t(f)$ is $\mathsf{NP}$-complete. This leaves us with the question of how hard it is to decide existence of a rational root of $t(f)$ in general (that is, without restriction on the denominators). I do not know anything more about this question than about Hilbert's Tenth Problem over $\mathbb{Q}$ (addressed in Q1). 

Compute all strings in $K$ of length strictly less than $|x|$. This can be done in polynomial time because all such strings have length at most $\log \log \log |x|$, and we just need to test the computation of $U(d)$ on even smaller strings $d$, for amounts of time that are still very small compared to $|x|$. Run $M(x)$, simulating oracle queries to smaller strings with the results of (1). If $M(x)$ ever queries a string of length $|x|$, simulate that query with a "NO" answer. 

I think this situation is too general to draw many conclusions, but here goes... If $A$ is closed under Cook-like reductions, then (2) would imply $B \subseteq A$, contradicting (1), so it tells you that $A$ is not closed under Cook-like reductions. One way to paraphrase the original statement is: to make the classes equal in power requires more than a single query. One can then ask about other intermediate types of reductions to get a better sense of just how many queries are needed and in what way e.g. are the classes equivalent under truth-table (nonadaptive) reductions? What's the best bound we can put on the number of queries ($2$, $O(1)$, $O(\log n)$?) Since these are counting classes, one could also ask about parsimonious reductions. 

For the more algebraic side of proof complexity I recommend starting with Pitassi's 1996 survey paper: 

Your problem reduces to graph isomorphism, so is not $\mathsf{NP}$-complete unless the polynomial hierarchy collapses. In fact, you've essentially already given the reduction in your question: given a 3-uniform (=every hyperedge has size 3) hypergraph $G$, construct from it the graph $f(G)$ which has the same vertex set, and such that $\{u,v\} \in E(f(G))$ iff there exists a $z \in V$ such that $\{u,v,z\} \in E(G)$. (When $G$ is a hypergraph, I use $E(G)$ to mean the set of hyperedges.) Then $G_1$ and $G_2$ are isomorphic-in-your-sense iff $f(G_1) \cong f(G_2)$ as graphs. It is not hard to check that this reduction only takes polynomial time (note that $|E(f(G))| \leq |V(G)| |E(G)|$). It is also probably worth mentioning that there is already a well-established notion of hypergraph isomorphism that is different from your notion. If we consider a hypergraph $G$ as a vertex set $V$ together with a collection of hyperedges $E \subseteq P(V)$ (each hyperedge is a subset of $V$), then two such hypergraphs are isomorphic if there is a bijection $\pi\colon V_1 \to V_2$ such that $E_2 = \pi(E_1)$. (Here I am abusing notation twice: by "$\pi(E_1)$" I mean $\{\pi(e) : e \in E_1\}$ and by $\pi(e)$ for $e \subseteq V$ I mean $\{\pi(v) : v \in e\}$.) Hypergraph isomorphism in this sense is Karp-equivalent to graph isomorphism, but by a slightly less immediate reduction than from your problem. However, fast (in theory) algorithms for hypergraph isomorphism are often more difficult than those for graph isomorphism (see, e.g. Babai and Codenotti, FOCS 2008).