Sure, you can use one or more supervised learning techniques to train a model here. You have features, a target variable and ground truth for that variable. In addition to applying ML you have learned, all you need to do to test your application fairly is reserve some of the data you have with expert predictions for comparison as test data (i.e. do not train using it). I would caveat that with some additional thoughts: 

Precision Your step change in precision looks to be almost entirely explained by the change in positive class frequency. It is reasonable to expect the proportion of false positives to increase when increasing the proportion of negative examples. Even if you assume your cv results were perfect, then you would see some increase. As an example, assume you have cv results representative of test results - which means same distribution before random under-sampling, and no over-fit to the cv set. Say you measured precision at 0.97 with a t:f ratio of 1:2, and for the sake of simplicity that this was due to the following confusion table: 

There are a few common options available for regression, such as mean square error (MSE) i.e. $\frac{1}{2N}\sum_{i=1}^{N}(\hat{y}_i - y_i)^2$ where $\hat{y}_i$ is your prediction and $y_i$ is the ground truth for each example. If you use this loss function, and want to predict values outside of range 0-1, remember to use a linear output layer (i.e. no activation function after the last layer), so that the network can actually output close to the values you need - that's about the only difference in network architecture you need to care about. In the more general case of game-playing bots, it is usual (but not required) to calculate a predicted "return" or "utility" which is the sum of all rewards that will be gained by continuing to act in a certain way. MSE loss is a good choice for that. Although for zero-sum two player games where the reward is simply win/lose, you can use a sigmoid output layer (predicting chance of a win) and cross-entropy loss, much like a classifier. For your specific case, you can treat your initial policy network as a classifier. This immediately gives you some probability weightings for the predicted move, which you can use to pick the predicted best play, or maybe to guide Monte Carlo Tree Search. The kind of network that predicts utility or return from a position (or combination of position and action) is called a value network (or an action-value network) in reinforcement learning . If you have both a policy network and a value network, then you are on the way to creating an actor-critic algorithm. 

The standard approach with policy gradients for continuous action spaces is to output a vector of parameters to a probability distribution. To resolve the policy into an action for the agent, you then sample from the distribution. In policy gradients with discrete action spaces, this is actually already the case - the softmax layer is providing a discrete distribution for you, that you must sample from to choose the action. The general rule is that your probability distribution function needs to be differentiable. A common choice is the Normal distribution, and for the output vector to be the mean and standard deviation. This adds an extra "interpretation layer" to the agent's model in addition to the NN, which needs to be included in the gradient calculation. Your idea: 

I am researching to implement RMSProp in a neural network project I am writing. I have not found any published paper to refer for a canonical version - I first stumbled across the idea from a Coursera class presented by Geoffrey Hinton (lecture 6 I think). I don't think the approach has ever been formally published, despite many gradient-descent optimisation libraries having an option called "RMSProp". In addition, my searches are showing up a few variations of the original idea, and it is not clear why they differ, or whether there is a clear reason to use one version over another. The general idea behind RMSProp is to scale learning rates by a moving average of current gradient magnitude. On each update step, the existing squared gradients are averaged into a running average (which is "decayed" by a factor) and when the network weight params are updated, the updates are divided by the square roots of these averaged squared gradients. This seems to work by stochastically "feeling out" the second order derivatives of the cost function. Naively, I would implement this as follows: Params: 

The above written in January 2017. In the intervening time (it is now October 2017) the DeepMind team have been working on the efficiency of their model, and it is now much faster and sounds even better. This is close to becoming a solved problem, albeit with some proprietary ownership. Give it a little while longer though and this breakthrough will allow real-time and natural-sounding parametric voice models in many applications. 

The value of $inv(X)$ or $inv(X^T)$ cannot be calculated for non-square matrices, and usually your number of samples and features is different, so your simplification is not generic. However, you can simplify the functional notation in a very similar way to your suggestion using a pseudo-inverse - note that the link here is direct to the part of the article that describes this inverse as a direct solution to the linear least squares problem. Noting the pseudo inverse as $pinv(X)$ then: $$\theta = pinv(X)y$$ However, this is simply hiding the more basic derivation behind a complex function. The computation required is essentially the same. In practice if you are using matrix library that supports inverses and pseudo-inverses, you can choose to use a longer formulation using inverses or the pseudo-inverse version and it will give near identical results. 

The code is not doing that. The mixing is occurring as you say. But the line you show is not part of the content loss. The mixed image is the initial solution, and iterations will turn it into the stylised image. 

I am studying reinforcement learning and I am working methodically through Sutton and Barto's book plus David Silver's lectures. I have noticed a minor difference in how the Markov Decision Processes (MDPs) are defined in those two sources, that affects the formulation of the Bellman equations, and I wonder about the reasoning behind the differences and when I might choose one or the other. In Sutton and Barto, the expected reward function is written $R^a_{ss'}$, whilst in David Silver's lectures it is written $R^a_{s}$. In turn this leads to slightly different formulations of all the Bellman equations. For instance, in Sutton and Barto, the equation for policy evaluation is given by: $v_{\pi}(s) = \sum_a \pi(a|s) \sum_{s'} P_{ss'}^a(R_{ss'}^a + \gamma v_{\pi}(s'))$ Whilst David Silver's lectures show: $v_{\pi}(s) = \sum_a \pi(a|s)(R_{s}^a + \gamma \sum_{s'} P_{ss'}^a v_{\pi}(s'))$ In both cases: 

No it is not possible to learn those meta parameters from a data set using a learning algorithm. There is no way to calculate a gradient of any objective function with respect to pool_size and stride params. Even if there were, they are typically discrete values and relatively small integers, so cannot be updated in the same way as e.g. the weights are. What you can do is search through different values looking for optimal ones, using cross-validation to pick the best. The size and stride meta parameters can also have a large effect on memory use and speed of learning, so it may be the case you are looking to find a good compromise. With stride in lower layers for instance, it may be more effective (in terms of training time required for similar level of accuracy) to have a higher stride value but increase the overall training set size by adding random x/y translations to the training set images. 

The theory of backpropagation should work with complex numbers, it doesn't make any assumptions about all numbers being real. However, most NN libraries would not be able to calculate with them. Your weights and biases would also need to be complex. I have no idea if you could reasonably expect convergence using gradient descent - many activation functions have periodic output and numerically unstable regions when calculating with complex numbers, which makes me think it would be risky. The simplest thing to do would be to split each complex number up into its components at the input, representing as a 2-dimensional vector of real numbers - either using cartesian or polar approach. Neural networks should cope just fine with splitting features up like this. 

Normalise using your training data statistics. Save the values used (e.g. mean and sd per feature), treating them as part of your model. Once you have used these values to transform input, they become fixed translate/scale factors in the model. Use the same values to normalise test data or new inputs as required. You do not need to calculate new normalisation constants for new data. In fact doing so will most likely reduce the effectiveness of your model. The same principle applies to interpreting output values if you need to scale those into range that your model produces. Scale according to your training data. 

The two are equivalent for a scenario with two mutually exclusive classes - e.g. a "positive" and "negative" class - where softmax would have two outputs summing to 1, and logistic regression would have one output giving probability of the "positive" class. I am not certain of the results comparing logistic regression one-vs-all (taking max output) with softmax regression on the same multi-class problem. I would expect the performance to be quite similar. Neither model copes well with non-linear relationships between input and target classes. 

Some low-level ones do (e.g. TensorFlow). However, this is not an architecture that has proven itself useful, so authors of higher-level libraries (e.g. Keras) have no reason to implement or support it. There is the possibility that this situation is an oversight, and the idea is generally useful. In which case, once someone can show this you will find it would get support across actively-developed libraries pretty quickly since it seems straightforward to implement. 

One additional thing you are likely to have problems with: Your choice of state representation does not have good linear relationship with the true value function. The linear estimator is quite limited in what it can do. A better estimator here would be a neural network with at least one hidden layer. However, you can probably tweak the representation slightly to get something that will work a little bit with a linear estimator - the trick is to have the rocks part of the state represented relative to the agent - i.e. don't have a grid of absolute positions of rocks, but make the grid relative to the agent. That way, a rock directly above the agent will always contribute the same to the current state value, which is important. Without this tweak, and using a linear approximator, your agent will not learn optimal control, it will instead learn a kind of fatalistic "with these many rocks, I have roughly this long to live" value function and probably just take random actions (if the distribution of rocks is not even it might learn to move to a particular column . . .) 

It is an order 5 tensor, and the dimensions are: $\text{BatchSize} \times \text{Depth} \times \text{Height} \times \text{Width} \times \text{Channels}$ You could in theory use this for your GAN, but you would need to add (a probably useless) depth dimension to the shape.