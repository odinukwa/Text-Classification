If you have more than one data center why not just copy the data over the network to a different data center each night? If bandwidth is expensive, look into a private link between all of your facilities. Or use the area under your 95th percentile in your trough. :) 

Yes. Your key was loaded into your ssh-agent when you first used it (if it has a passphrase a dialog box was displayed.) If you log out the agent will stop. 

If you ignore dependencies or just try to satisfy with random packages you find, you'll just find yourself in 'RPM hell.' Just because it's an RPM doesn't mean it works with your distribution. Percona does have a yum repository. I suggest you follow those directions, assuming that you're using either CentOS or RedHat. Then installing percona will be as easy as . 

Also enable the server-status module and visit that to find out what's happening. You might be swapping. Have you checked out vmstat while this is happening? 2GB of RAM for 80 MaxClients is only 25 MB for each (assuming the box isn't doing anything else.) Your MaxClients might be too high. The solution for this is obvious: add more RAM or lower MaxClients. If the command line is slow to respond when you restart apache, that's one indication of this situation. It's also possible that you're spoon feeding some mobile clients (or other clients on slow connections) with 'large' files thereby consuming all of your available apache slots. Maybe you have too few MaxClients. Checking out the server-status will tell you what each of those clients are doing at that time. One solution for this situation is to increase MaxClients (but that might also turn into the situation above.) A better solution for this is to install an HTTP accelerator in front of apache (one free option is perlbal.) If your command line is normal speed when you restart apache, that's one indication of this situation. 

Create a NAT interface using powershell (there is apparently a limit of 1 such interface), using the instructions in the link above. This will give you a new vSwitch, and assign it an IP on the host. In Hyper-V, edit the VM's settings, and provide add a new network interface connected to the NAT switch interface created in step 1. On the VM, assign the new nic a static IP on that NAT subnet (e.g. ), and configure the default gateway to be the vSwitch IP, e.g. () Add any nameserver to /etc/resolv.conf () 

The availability of a feature cannot be determined from within the configuration file as far as I can tell. If one wishes to test if a change to the configuration would work under the current settings, one can use the configtest command: (or ) before changes go live. It's not ideal. nginx supports dynamic module loading since version 1.9.11 (feb 2016 announcement). Still, one cannot provide alternate configs based on presence or absence of modules. One shouldn't conflate the ability to load modules dynamically or statically, with the ability to test whether the modules in question are enabled, using directives in a config file. Regardless of the way a module is added to the binary, being able to test whether modular features are available would be a reasonable feature to support. That feature just isn't there yet, with the exception perhaps of a few modules which have perceptible side effects in the config (adding a custom response header for instance would be visible to a later step in the request processing). If "scripting glue" is an option in the deployment to test for features, on the commandline, one can do: to see the list of known modules at the time of compilation. The output will have a mix of and flags. 

Canonical back ports these fixes for Ubuntu. The incident that you found was fixed in 5.3.10-1ubuntu3.2. It appears that the latest version is 5.3.10-1ubuntu3.9. You can go back and look at the change logs for the various releases. I suggest that you keep your distro up to date. These commands should do it. 

This will move all of the files and subdirs from /mnt/disk2/home to /mnt/disk1/home. The result will be: 

The number of machines you can get in a rack will usually be constrained by the power they consume (and the associated cooling) not the physical space. Too many people don't bother to check how much power their servers use then they get surprised when their rack is labelled 'full' when it doesn't look full. 

Almost sounds like the two ends aren't passing terminal parameters properly. Here are a few things to try. Set your terminal window to 80x24. This is the historical standard (TN3270) size of a terminal window. And/or reset the expectations on the size of the terminal on the remote side. There are various ways to accomplish this, such as or (where X and Y is the width and height of your terminal) 

The problem with a site like that is: I don't trust Joe Average to 'know' data centers as well as I do. I also hugely distrust data center sales people. I had one tell me that they were 'practically Tier 4!' Yet I found many disqualifications to Tier 2 status on the tour. Some of those that buy data centers (or data center space) don't really know what makes a DC. Not to mention that we all have different qualification for a 'good' data center. It depends on what you value. Do you highly value technical remote hands? Do you highly value the robust power infrastructure? Do you highly value a robust cooling infrastructure? For me, I want a really robust DC and external remote hands. If the remote hands don't live up to my expectations, I can swap those out over the term. 

A bit late on this but you don't really want to have memcache nodes autoscaling. Hashing will drift so you will be losing large percentage of your cache everytime there is a scale up or scale down event. 

We are currently preparing for a move of a reasonably high traffic web site to the cloud. We are thinking of using scalr to help us manage the whole setup especially since we dont have experience with amazon. We are unsure about whether we should use Scalr's MySQL functionality that relies on EBS backed EC2 instances or whether we should be making use of RDS or even xeround and enjoy much easier maintenance and management. Our dataset is about 40GB and we consume a bandwidth of 4000 GB per month between the application server and the database server. any experiences on similar setups? thanks in advance 

Regarding Scalr's open-source vs hosted version, GUI is the same - I am just not 100% sure if the open source version is 100% up to date. We are using the hosted version. Scalr itself (if you chose to host it yourself) needs to run separately in order to manage your EC2 instances. This will give you a nice overview: $URL$ In theory you can migrate your current instances to scalr but I am sure devil is in the details. Some careful planning would be required in order to complete the migration: $URL$ If you use standard components, you might prefer to use Scalr's ready server templates (Roles in scalr terminology) those as starting points and modify as needed. In general I can highly recommend Scalr as a value for money alternative to Rightscale. You will surely appreciate the automation and configuration capabilities vs managing things manually via the aws interface. 

I am currently having an m1.large instance acting as a memcache server. I would like to replace that with 2 m1.small nodes for High Availability more or less for the same cost. But I am concerned about the impact of the moderate IO performance of m1.small. I read the following as if moderate IO would affect network system as well. Not sure if the fact that there would be more nodes counterbalance the more limited IO capacity of the small instances. Anyone had experience with memcache on these instance types that can comment? Many thanks in advance 

Whichever drives activity lights do NOT come on are likely bad (and hopefully it's just one.) Note that if you have hot-spares configured, those won't light up either. 

Yes, they certainly can. From $URL$ Run yes in an xterm, and you will see a lot of "y" lines swooshing past your eyes. Naturally, the yes process is able to generate "y" lines much faster than the xterm application is able to parse them, update its frame buffer, communicate with the X server in order to scroll the window and so on. How is it possible for these programs to cooperate? The answer lies in blocking I/O. The pseudo terminal can only keep a certain amount of data inside its kernel buffer, and when that buffer is full and yes tries to call write(2), then write(2) will block, moving the yes process into the interruptible sleep state where it remains until the xterm process has had a chance to read off some of the buffered bytes. The same thing happens if the TTY is connected to a serial port. yes would be able to transmit data at a much higher rate than, say, 9600 baud, but if the serial port is limited to that speed, the kernel buffer soon fills up and any subsequent write(2) calls block the process (or fail with the error code EAGAIN if the process has requested non-blocking I/O). 

I doubt this would work. It will be as difficult to get setup as faxing over VoIP. Then there's the problem of finding your modem equivalent. Asterisk comes to mind, that's ... weird. Analog to digital to analog to digital? If you do get it setup, let us know how you did it. 

Are you able to ping the IPMI IP (192.168.3.164)? Is there a dedicated IPMI port? If the ethernet cable is not connected to the mgmt port when the machine first powers on it's likely that the IPMI board will 'share' the eth0 port. If you issue this command: it will renegotiate and choose the proper port. 

I've managed to get something working on Windows 10, by creating a HyperV NAT interface. I believe you'll need windows 10 pro for that. Win10 home doesn't have Hyper-V (yet). This has worked quite well: $URL$ 

The effect of using the NAT interface is that the traffic coming out of any VM using the NAT vSwitch will be masqueraded behind the gateway IP ( in my example), and will be re-routed internally using the routing table of the windows host. So, when I turn on the VPN, this will re-route all the Natted traffic to the VPN too. There are many cases where I want to access the VM from the host directly, which the NAT makes difficult (everything will be behind ). Rather than doing a bunch of dynamic port forwarding (which is also possible too with powershell), I've simply created another internal network between the host and the VM that is not NAtted (e.g. . 

I have a windows 10 pro host with multiple adapters on multiple subnets used by VMs and I would like to re-route incoming traffic from one of the subnets into the host's routing table. My hyper-V VMs are on 192.168.4.0/24, and my windows 10 host has a virtual interface on 192.168.4.215/24 which I would like to use as a gateway for the VMs traffic. I would like traffic from the VMs to go through a VPN interface. When I connect to my VPN connection (pulse secure), another virtual interface shows up on the host, at 10.0.0.100/32, and a default route is automatically added to the host's routing table to point to 10.0.0.100 as the default gateway. I would like to configure the host to re-route/forward all incoming IP traffic from 192.168.4.0/24 onto the VPN interface, possibly NATed. Normally I would bridge the interfaces I want to connect together, but this particular vpn adapter will refuse to be bridged to anything. Is there a way to achieve this in win10 pro, or do I need windows server? This is annoying also because this particular vpn software isn't well supported in Linux.