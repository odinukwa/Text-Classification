This is kind of tough if the only data you have is login/log off (bytes sent/received by time would be better). You surmise that logoffs can't be trusted, because not everyone logs off, and logged in users may equally be someone who stays 10 minutes or several hours. You might consider running some scenarios to get best/worst case estimates. For instance, each login could be considered a singular event convolved with a probability density kernel (positively skewed with some decay rate perhaps), which then gives you 'occupancy over time'. You can then try different parameters to ascertain best case/worst case scenarios. 

I assume you are running classification, and have a binary target variable. If that's the case, it does not make sense to show component ROC curves, because your separation may be based on on combinations of 2, 3, or more predictors that individual ROC curves will not reflect. I would show your overall ROC curve, along with perhaps variable importance measures. If you have a handful of predictors that are clear winners, you could re-run your model including only those, and then show that ROC. Otherwise, I don't see what it buys you. 

I recommend you to use tensorflow which is under strong development and supports deep learning. You can use the high-level neural networks API Keras that runs on top of tensorflow and is very simply to use, just try a tutorial and you are going to love it. 

I am trying to select the best scipy sparse matrix type to use in my algorithm. In that, I should initialize data in a vij way, then I should use it to perform matrix vector multiplication. Eventually I have to add rows and cols. Trying to select the best for my problem, I want to understand which are the best cases to use each of this types: lil_matrix, coo_matrix, csr_matrix, csc_matrix, dok_matrix. Can someone explain me? Its not necessary to show examples of all the types in the same answer. 

Since you know the quantity of labels (6) you can use k-means algorithm to cluster your data into 6 groups. I recommend you to represent each using the tfidf method. You can implement your code using sklearn functions. 

Here's an idea that just popped out of the blue – what if you make use of Random Subspace Sampling (as in fact Sean Owen already suggested) to train a bunch of new classifiers every time a new feature appears (using a random feature subset, including the new set of features). You could train those models on a subset of samples as well to save some training time. This way you can have new classifiers possibly taking on both new and old features, and at the same time keeping your old classifiers. You might even, perhaps using a cross validation technique to measure each classifier's performance, be able to kill-off the worst performing ones after a while, to avoid a bloated model. 

Your description of the model functioning is correct. Structurally, both models are representatives of the so called kernel methods. As such they are very similar, the same in many cases. What is completely different between the two methods is the way the kernel centers and linear coefficients are derived. My experience with SVM is limited, so I cannot go into the details of the training method, but in summary is consists of solving the constrained optimization problem of finding the best approximation subject to maximum coefficients. This is achieved by linear programming or through a method like Sequential Minimal Optimization (SMO). Training RBFNs, on the other hand, is a different story. Identifying the kernels is done in one of two ways usually; either by using a clustering algorithm or by using Orthogonal Least Squares (OLS). In either case, the linear coefficients are identified as a second step through least squares (LS). The differences in training mean that, even through in structure the resulting models may be the same, in functioning they will most probably be completely different, due to the different fitting procedures. Some references: J. C. Platt, “Fast Training of Support Vector Machines Using Sequential Minimal Optimization,” Adv. kernel methods, pp. 185–208, 1998. J. Moody and C. J. Darken, “Fast Learning in Networks of Locally-Tuned Processing Units,” Neural Comput., vol. 1, no. 2, pp. 281–294, Jun. 1989. S. Chen, C. F. N. Cowan, and P. M. Grant, “Orthogonal least squares learning algorithm for radial basis function networks,” IEEE Trans. Neural Networks, vol. 2, no. 2, pp. 302–309, Mar. 1991. 

You have to use unsupervised learning. After that, in order to measure accuracy of your model, you should use cluster quality intrinsic and extrinsic metrics. Compute the similarity of the data in each cluster (intrinsic metric), and the dissimilarity between the data of different clusters (extrinsic metric). A good clustering throws data with great similarity in each cluster and great dissimilarity between clusters. 

b) Sparse types that support efficient access, arithmetic operations, column or row slicing, and matrix-vector products: 

This is exactly why. If you use your test dataset to select the best architecture that can predict, then you are not using it as a test data, you are using your test data as a validation data. And, in the end, you run out of data to test. To make it clear: 

Ok, I was looking for an answer and now I have it clearer: Scipy documentation does not elaborate too much on the explanation, buy wikipedia article is much more clear. For those who are looking for an answer, there are two major groups of sparse matrices: a) Sparse types used to construct the matrices: 

By ‘design document’ I assume you are authoring a technical doc in an industry job. I treat this similarly to an academic journal publication but with less formatting and page restrictions. Ultimately, you want to provide sufficient information such that if Apple hired you tomorrow your replacement could reproduce the work. Further, your colleagues should be able to grasp the problem attacked, solution proposed, and evidence of success. I usually put the yawn-inducing deep stuff in appendix, but suggested sections might include: 0) executive summary 1) background/motivation 2) data sources 3) analysis/algorithm/model 4) validation performance 5) [optional] real world test results/pilot 6) [optional] estimated return on investment 7) [appendix] SQL source code, modeling heuristic, outlier treatment, etc. 

Feature engineering refers to creating new information that was not there previously, often by using domain specific knowledge or by creating new features that are transformations of others you already have, such as adding interaction terms or as you state, moving averages. A model generally cannot 'pick up' on information it doesn't have, and that is where finesse and creativity comes into play. Whether you should one-hot or leave a feature as categorical depends on the modeling approach. Some, like randomForest will do fine with categorical predictors; others prefer recoding. Intuition on these questions comes with practice and experience. There's no substitute for trying out and comparing toy examples to see how your choices affect outcomes. You should take the time to do that, and intuition will follow. 

Once the matrices are build using one of the a) types, to perform manipulations such as multiplication or inversion, we should convert the matrix to either CSC or CSR format. 

At first sight, the total acumulated energy consumption seems to have a linear relation with time, so I suggest to try a linear regression at first. There are several libraries you can use to code it. I recommend you do it with pandas and sklearn, here is an answer related to this: answer. If the relation is not linear, so I could try with a more complex model (but I suggest to keep simplicity at first). Since you are trying to predict a temporal serie, I would try with an LSTM model. Here is a tutorial to implement an LSTM neural network with keras. 

Have you found a good approach? I am envolved in the same work right now. My approach is the following: 1) Make a vector respresentation of all texts in the data set, for example with tfidf technique. 2) Take the first vector and put it in a pile. 3) Enter in the following loop: 3a) take the next vector and compute the cosine similarity between this vector and the centroid of each built pile. 3b) if one of this cosine similarity falls below a predefined threshold, stack this document representation in the corresponding pile. Another case, build a new pile with this vector. 3c) recompute the centroid of each modified pile. This algorithm is going to find similar tweets, which we suppose that are related with same topic. 

There's this misunderstanding that deep learning is generally suitable if you have loads of data, which, judging from your comment, is your case. This is generally an inaccurate belief. Deep learning (including CNN and RNN), are complex models with thousands of parameters that are able to model complex relationships. Such relationships are generally "hidden" in vast amounts of data, but this is not always the case. You may have at hand data that are generated from a simple distribution, and as such it will not need a complex model to approximate, even if your sample size is huge. Here's a fabricated example: Let's say that you have the function . This function entails linear relationships between all independent variables and the dependent variable, y. You could sample this function a million times and still you would have at hand data that could be approximated through linear regression. Coming to your case: to identify what kind of algorithm you would need, you first need to look at your data, perhaps through performing a statistical analysis. Then I would suggest to start simple. Try logistic regression, for starters. Is the model satisfactory in validation? If not, try perhaps decision trees. Or a shallow neural net. Always validate (you have loads of data so validation should be easy!). My (admittedly wild) guess is that your classification problem could be addressed with much simpler algorithms than DNN. But YMMV, of course. FWIW, here's another answer similar to your case. And another one. 

Most likely, SGD is not a limiting factor for you. But, have you considered taking a classification rather than regression approach? (It looks like you're predicting real values as opposed to classes). Since you state that the prediction doesn't have to be perfect, why not try grouping your outcome variable into bins, then predict the bins? You'll have a far less granular solution, but you might find it works. 

If you are new to data science and data munging, this could be kind of a tricky task, but a good one to get your feet wet. Many programming languages have the capability to do this (R, Python, Matlab, etc.). I use R primarily, so I'll give you a brief heuristic for how I'd approach the task in R. Perhaps looking into these steps will get you started. 

Congrats on your career move, but please know that data science is one of the more difficult fields to enter, being a combination of statistics, programming, computer science, mathematics, etc. But, for someone motivated enough, with sharp acumen and intuitive data skills, it's a great field. A commenter recommended Andrew Ng's (pronounced 'ing' ) online course through coursera, and I also agree this is a great foundation. It is dense, but you'll need to master it if you hope to be a quality data scientist. He uses Octave, but I advise that you come up to speed quickly in R or Python. If you can translate Andy's code to either of those platforms, you'll be in good shape to start your practice. 

You can also choose the method used to calculate the correlation between this: -pearson -kendall -spearman 

Using text documents in different languages you are going to have different vector representations, unless you translate the documents previously. For example, house and maison are going to be related to different features. So a cluster algorithm is not going to recognize them as synonymous. You should try a previous translation of your reviews. The quality of that translation is going to affect the clustering algorithm depending on the algorithms you are using. If you tell me the steps you are performing in your cluster I could help you better. 

If you have a hugh dataset you can cluster it (for example using KMeans from scikit learn) after obtaining the representation, and before predicting on new data. This code perform all these steps. You can check it on my github repo. 

I have a text dataset which I vectorize using a tfidf technique and now in order to make a cluster analysis I am measuring distances between these vector representations. I have found that a common technique is to measure distance using cosine similarity, and when I ask why euclidean distance is not used, the common answer is that cosine similarity works better when vectors have different magnitude. Since my text vectorized representation is normalized I wonder which is the advantage of using cosine similarity over euclidean distance in order to cluster my data? 

I assume you are performing an independent samples t-test. That the Ns are different isn't necessarily a problem -- the mean is an unbiased estimator -- but just how different are the sample sizes? As you describe, you are probably violating the homogeneity of variance assumption. T-tests tend to be fairly robust to this violation, but you might consider having some outlier treatment and/or using a nonparametric test. 

Logistic regression isn’t trying to find a class boundary per se as linear SVMs do. LR attempts to model the logit-transformed y scores using predictors. To use a silly analogy , LR tries to put the function ‘through the points’ while SVMs attempt to put support vectors ‘between the points’ 

There might be a more principled way to go about this but I don't know how IP assignment works. That said, All your information content appears to be in the second 2 numbers. The last 0-255 might be assumed and the leading 1 can be dropped. You might have something more complicated but as you lay it out, you can leave them as strings and assign to 3 groups: '0.0', '0.1 or '0.3' '4.0' You may not need to spell out all possible IPs. Define your problem space first. 

Is this correct? I'll risk an answer anyways: For the first part, I believe that there is a slew of literature out there. It's not my expertise, but first thing that comes to mind is measuring the distance in feature space between your shape and each template, and picking the closest one, it the distance is below a threshold that you set. "Feature" here would be either some low-level polygon property, e.g. x and y coordinates of vertices, or an abstraction, e.g. perimeter, area, no. vertices, mean side length/side length variance, etc. For the second part, it really depends on the nature of your constraints/objective functions. Are they convex? Uni- or multi-modal? Single or multi-objective? Do you want to incorporate some domain knowledge (i.e. knowledge about what "good" transformations would be?)? One can really not tell without further details. Evolutionary algorithms are quite versatile but expensive methods (although some argue on that). If you can spare the possibly large amount of function evaluations, you could try EAs as a first step, and then refine your approach. Finally, while not exactly related to what you describe in your process, I believe you may benefit by taking a look into auto-associative networks (and models in general); these are models that are able to perform constraint-satisfaction on their input, effectively enforcing learned relationships on input values. I could see this being used in your case by inputing a shape, and having a transformed shape as an output, which would be "legal", i.e. satisfying the constraints learned by the auto associative model. Thus, you would eliminate the need for a template matching + optimization altogether.