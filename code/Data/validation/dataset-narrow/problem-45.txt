GL and GLES were designed many years ago, when GPUs worked quite differently. (The most obvious difference being immediate-mode draw calls vs tiling and command queues.) GL encourages you to think in an immediate-mode style, and has a lot of legacy cruft. Vulkan offers programming models that are much closer to how contemporary GPUs work, so if you learn Vulkan, you'll have a better understanding of how the technology really works, and of what is efficient and what is inefficient. I see lots of people who've started with GL or GLES and immediately get into bad habits like issuing separate draw calls per-object instead of using VBOs, or even worse, using display lists. It's hard for GL programmers to find out what is no longer encouraged. It's much easier to move from Vulkan to GL or GLES than vice-versa. Vulkan makes explicit a lot of things that were hidden or unpredictable in GL, such as concurrency control, sharing, and rendering state. It pushes a lot of complexity up from the driver to the application: but by doing so, it gives control to the application, and makes it simpler to get predictable performance and compatibility between different GPU vendors. If you have some code that works in Vulkan, it's quite easy to port that to GL or GLES instead, and you end up with something that uses good GL/GLES habits. If you have code that works in GL or GLES, you almost have to start again to make it work efficiently in Vulkan: especially if it was written in a legacy style (see point 1). 

To answer this, you'll need to consider how this affects the surface area, and how this changes each score in turn. 

Make a 'fake' display driver. My display driver would take the place of the HMD's driver, and it would act like a proxy. When the SteamVR compositor tries to swap buffers, my driver would do the processing on the backbuffer before poking the real HMD driver. It seems like the main difficulties in this would be (a) getting Windows to accept my fake driver, (b) getting SteamVR to accept my fake driver, and (c) letting my driver use the real HMD driver. Use normal video capture software (such as OBS) in display capture mode to capture the frames before they get to the display driver, and then use an overlay technique (like the Steam overlay) to draw over the frame completely with my processed frame. This sounds simple, but I know there's a lot of work in making either of the two halves of the system. Hook into the SteamVR compositor itself. It supports third-party HMD integrations, and I'd hoped I'd be able to trick it into using my shader as a chromatic aberration correction shader, but it turns out that it doesn't let the HMD integration run a shader directly, it just sets the parameters for the built-in shader. This seems like it should be the easiest way, if I can just find a way to make the compositor run my shader. (I'm not averse to hacking the compositor if there's a way.) 

Torus A torus is defined by two parameters: the major radius, and the minor radius. The major radius () is the radius of the big ring (in red in the diagram), and the minor radius () is the radius of the circular cross-section. The and here are just indices into the vector: they're unrelated to the x and y axes. The left side of the diagram shows the xz plane, and the right side shows a cross-section with y going horizontally. 

Like in any other kind of signal processing, the relationship is Nyquist's theorem. An image is a discrete sequence of samples of a continuous signal. If the original signal has frequency components higher than half the sampling rate, then there will be aliasing. To put that another way, if you look at the real-world size of a pixel, any details smaller than two pixels wide will be aliased. This applies to synthetic images as well as to photographs. If you define a procedural texture via a mathematical function, the function is continuous but you only evaluate it at certain points. The problem is, of course, if you only have the sampled image and not the original signal, the high-frequency components have already been aliased, so you can't measure them directly. You have to use statistical techniques to guess which details in your aliased image were originally higher-frequency signals. 

You can't see it in the image you uploaded because it's a JPEG, but PNG files which have a full alpha channel can do the trick you mentioned. It works with websites or apps where the "thumbnail" image is displayed on a white background and the "full" image is displayed on a black background. In the thumbnail display, white pixels and completely transparent pixels look the same, because of the white background; in the full image, black pixels and completely transparent pixels look the same. Thus, you can have a white-on-black picture and a black-on-white picture in the same image, and the background hides one of the two images. You can easily see what's going on if you look at the image against a background of some other colour. 

Jittering and dithering are both techniques of adding noise to reduce visible artefacts (such as banding) in an image. They solve different kinds of artefacts so they are used in different situations. Jittering moves sample positions in space to reduce artefacts caused by regular sampling. Dithering changes the way colours are rounded (when reducing precision), replacing banding with noise. When is jittering used? A use case for jittering is when generating eye rays in a ray-tracer. Without jittering, you might generate one ray in the centre of each pixel. But then if you trace those at a regular pattern, such as a grille of bars each one pixel wide, you might find that every ray hits a bar, making the grille solid; or that every ray hits a gap, and the grille isn't seen at all. Instead, you can add a small stochastic displacement to each ray direction, so that each ray goes through a different part of its pixel. That way, some rays will hit the bars of the grille and some will hit the gaps, making a noisy image instead of a completely wrong image. When is dithering used? The normal use of dithering is when rounding colours (intensity values) from a high-precision format (such as floating-point numbers) to a low-precision format (such as 8-bit integers). Say you have a smooth gradient you want to represent in an 8-bit image. If you round each number separately, then starting from the black end of the gradient, you get a lot of black pixels until they get light enough to round to 1/256; then these all have the same value until you get to 2/256; and so on. You end up with regular bands where all the pixels have the same value. Instead, when you round the first number down, you can let it "donate" the lost intensity to another nearby pixel. That way, then at the black end of the gradient, maybe every tenth pixel is 1/256 and the other nine are 0. The ratio of black to nearly-black pixels increases gradually, until eventually all the pixels are 1/256, and then every tenth is 2/256, and so on. Instead of visible bands, the overall intensity increases smoothly along the gradient - but the price of this is that pixels which should be the same are slightly different, adding noise. Dithering is used a lot in printing, where the colour resolution is quite poor (i.e. the steps between colours are big) but the spatial resolution (how small you can make each dot of colour) is very fine. It used to be used in the days of 256-colour displays for making colours which couldn't otherwise be represented, and it's still used to avoid banding when displaying (for example) a 16-bit-per-channel image on an 8-bit display. Is dithering the only way? Because traditional dithering algorithms involve giving information from each pixel to its neighbours, they don't parallelise well, so they're not suited to modern hardware. Another popular technique is to deliberately add noise to the source image. The range of the noise is less than one step of the destination format (e.g. the noise ranges from -1/512 to 1/512), so it can't be seen directly, but it means that each pixel will be rounded differently. Like true dithering, it replaces any banding with subtle noise in the output image. Can we combine them? There's no reason you can't use both in the same image, but as they solve different problems, that's not really "combining" them. It's like asking if we can combine hoses and axes. A fire truck has both, and the firefighters might use both at the same incident, but one is for extinguishing fires and the other is for freeing trapped people. An example of when you might use both is if you ray-trace an image into a floating-point framebuffer, using jittered ray positions to avoid sampling artefacts, and then reduce that to an 8-bit image using dithering to better preserve the colours. 

It entirely depends what hardware and OS you're on. It's up to the integration between the windowing system and GL. In the simple case where one application is rendering full-screen directly, the framebuffer is in memory that's accessible by both the GPU and the display controller. After the GPU is finished, and when the display controller gets to a vsync (the time to start drawing a new frame to the screen), the display controller just reads out the framebuffer. In a compositing system like modern MS Windows or Android, your app's framebuffer is not used by the display controller directly. After you finish drawing your frame, the compositor runs. On the GPU, it reads in your framebuffer, puts it in the right place on the desktop, and writes the composited desktop into the real framebuffer. The display controller then reads out the desktop framebuffer. There's also an optimization possible using overlays, which we have another question about. Copying completed frames into CPU memory is to be avoided. If you had to do this every frame, your application probably couldn't run at a reasonable frame-rate. Even reading back a single pixel (with ) is a big slow-down, because the CPU has to wait for the GPU to finish, and they have to invalidate the cache of any shared memory. Reading back a million pixels is extra slow on desktop because the CPU and GPU have separate memory and the path between them is relatively slow. It's very rare that it should be necessary: mainly it's needed for screenshots, or for debugging. 

Both of your two proposed solutions are valid, with different properties. Which you choose is purely a design issue. The difference is that your probabilistic rays/particles (1) don't need to store their energy: they all have the same amount of energy, so you can just count particles to sum the energy. Your partial particles (2) need to store an energy so you can split them up. The advantage of 1 is that each ray is smaller. Also, it's easy to keep track of where the energy ended up, because you can use the intersections where the ray was dropped. Because you're only reflecting or discarding rays, the number of in-flight rays will decrease with each bounce, so you might find it harder to keep batches of rays together for better SIMD or parallel performance. Also, because you're probabilistically discarding rays, you'll get a lot more noise in your results for the same number of rays. The second solution is closer to a traditional ray-tracer, so you may find it easier to use existing algorithms and code. You can use fewer primary rays, and the number of rays in each batch is constant at first (but gets worse when the brightnesses start to reach zero). It's also a lot more efficient to track multiple frequencies this way. If you give each ray a spectrum or RGB energies, the material can have a different effect on different colours without you having to fire multiple batches of rays (duplicating the ray intersection work). On the other hand, you'll need a second data structure to count up where energy was absorbed, if you want to track that. In your place, I'd probably use the second solution, but it depends on what other software you're trying to integrate with e.g. an existing ray-tracing library or photon representation, and what you need to do with the output/results. 

If you have GL (or equivalent) available, the easiest way is probably to set up your projection matrix so that the plane of the polygon is the near clipping plane, draw the polygon into the stencil buffer, and then draw the polyhedron such that inside faces output a 1 fragment. (You could do this by flipping the winding of the polyhedron and turning on back-face culling, or you could check the normal in the fragment shader.) Then the sum of the white pixels in your framebuffer is the intersection area. 

The Mesa3D project is a software implementation of OpenGL; that is, an implementation that does all the rendering on the CPU with no GPU involvement at all. It's designed in two layers: a backend which renders each frame from some internal state, and a frontend which implements the OpenGL API and sets up the state for the backend to use. Most OpenGL implementations have a similar two-layer architecture, so that the same backend can be used by an OpenGL frontend, an OpenCL frontend, a Direct3D frontend, etc. The open-source NVidia drivers (and most open-source drivers) use Mesa's frontend to provide the OpenGL API and a backend specific to that GPU. The OpenGL API is quite complicated to implement, so it makes sense for vendors to share an existing front-end in this way. However, closed-source drivers do not use Mesa's frontend and have their own closed-source frontend. If you install and use a closed-source GPU driver, it will include its own libgl, which doesn't use Mesa at all. You don't need to install anything else to run OpenGL applications. (You might need an extra "-dev" package to get the header files needed to compile OpenGL applications.) If you install and use an open-source GPU driver, it will use Mesa, but the packages in your distribution should have the dependencies set up correctly, so that if an external Mesa package is needed, it will be installed automatically. Either way, you shouldn't have to worry about whether the OpenGL implementation you're using is based on Mesa or not.