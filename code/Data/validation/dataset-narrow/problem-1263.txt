A book source for information about problems of this type is: Graph Classes: A Survey, A. Brandstadt, Le, and Spinrad, SIAM, 1999. 

Questions of this kind are especially interesting for graphs which are 3-polytopal (edge-vertex graphs of 3-dimensional convex polytopes) and for the case of the graph being bipartite. (Also for higher-dimensional polytopes.) The concept of "shortness exponent" has emerged to measure how short a longest cycle might be in a graph which has no hamiltonian circuit and which is within a family of graphs. This relatively recent paper lists many references but others can be found using the string: shortness exponent polytopes: $URL$ 

Bin packing (many variants) is a problem whose complexity is known to be NP-hard: $URL$ However, many heuristics when applied to "practical" versions do very well. For 1-dimensional bin packing some of these heuristics, like first-fit; first-fit decreasing; best-fit; best-fit decreasing are very appealing as topics to show students. Students often can discover some of the basic heuristics for themselves. 

The TSP often appears as part of more "complicated" operations research problems. A good example being the vehicle routing problem which has many variants. The following web site tracks some of the developments in this area: $URL$ 

Perhaps this paper which deals with cycle lengths in planar graphs might be of value: Li, Ming-Chu; Corneil, Derek G.; Mendelsohn, Eric (2000), "Pancyclicity and NP-completeness in planar graphs", Discrete Applied Mathematics 98 (3): 219–225. 

Information related to test problems for the Traveling Salesman Problem (TSP) can be found here: $URL$ 

Perhaps this paper will be of interest though I don't know if it addresses complexity issues: $URL$ or $URL$ 

There are many complexity issues that are related to many of the topics that come up in what has come to be called social choice theory. These include the complexity of deciding who is the winner when a particular method is used to amalgamate the ballots of a certain type into a choice for society. There are also complexity issues involved in trying find a way to vote strategically (rather than using one's true preferences) when information may be available about other voter's preferences when a particular method is being used in the hopes of getting a better outcome for a particular person or a group of people. Complexity also comes up in designing "safe" on line voting systems. These is a huge literature about social choice but some good books to get started for those interested would be: Donald Saari, Decisions and Elections, Cambridge U. Press, 2001. Donald Saari, Disposing Dictators, Demystifying Voting Paradoxes, Cambridge U. Press, 2008. Alan Taylor, Social Choice and the Mathematics of Manipulation, Cambridge U. Press, 2005. 

Cayley graphs of codes and derandomized code products can be a good example. See the following thesis (Chapter 6) for details and references: $URL$ 

I'm interested in optimization problems that aim to find a semi-tour that minimizes the total cost or the width cost on a given graph. Of course, if a graph is Eulerian, an Eulerian tour would be optimal, for which the total cost is the number of edges and the width cost is zero. For graphs containing odd degree vertices, the optimal cost in both senses must be larger. I suspect these optimization problems (if not trivial) must have been well studied in the literature. What are the relevant references, if any, and what is known about these problems (both algorithmically and combinatorially)? 

If you have a polynomial time oracle for solving an NP-hard problem, you can use it to solve any other problem in NP as well, including what you want in your "sequential algorithm": Given a graph and a path of length $i$, whether the given path can be completed to a Hamiltonian path. 

The matrix multiplication exponent being $\omega$ does not guarantee that there is an algorithm that runs in time $O(n^\omega)$, but only that for each $\epsilon > 0$, there is an algorithm that runs in $O(n^{\omega+\epsilon})$. Indeed if you can find an algorithm that runs in time $O(n^2 \mathrm{polylog}(n))$, then this shows that $\omega = 2$. You can find the formal definition in the book Algebraic Complexity Theory by Peter Bürgisser, Michael Clausen, Amin Shokrollahi. 

The story behind the author ordering of the first paper is explained here. For the other cases I believe there's not much beyond an agreement between authors. 

Seeing that you are a computer programmer: Without all the theoretical developments, your C++ compiler would take forever to compile your program and even then would most likely crash. That is assuming that your OS could keep running by that time, not using countless clever data structures and algorithms developed in theory. Whatever you look at, be it a compiler, operating system, database, a video game, a web service, or the web browser you are using now to read this text: There is a lot of theory behind its working. 

Your question is equivalent to tail bounds on the weight distribution of Reed-Muller codes. Understanding weight distribution of Reed-Muller codes is an old and challenging question in coding theory, and several interesting results are known about it (the weight distribution is completely understood only for $d=1$ and $d=2$). As a great starting point, see "Weight Distribution and List-Decoding Size of Reed-Muller Codes" by Tali Kaufman, Shachar Lovett, Ely Porat, and the references therein. 

This is a pretty straightforward answer, but I think effective: A suggestive type for your datastructure: where the two lists of Ids are ordered by order1 and order2 respectively. Maintaining and generating this structure has a cost, but queries on a single tag should be $O(log (n))$ in the number of tags. Storing the length of the lists with the lists would help with query optimization, and much better than an actual list would be a lazy bitvector which would make negation extremely cheap as well. If you thrown in another map of type then generating the related frequencies given a list of s should be $O(n * log (m))$ in the size of the list and the size of the sets of tags. 

The concept is simpler than you think. Assuming 8 bit words and a bitvector of only one word length, 

My sense is that SPJ was referring to purely functional languages -- i.e. languages which are referentially transparent. This includes, e.g., Haskell, Miranda, Clean, but not ML. Once you have a purely functional language, in general, you can give it a fairly clean and well defined denotational semantics. This semantics will, in general, look like one for the lambda calculus, with some tweaks here and there. Generally, you will have a type system which desugars to something resembling a variant of System F -- perhaps more powerful in some regards, more restricted in others. This is why code extraction to/compilation to Haskell, O'Caml, etc. is relatively straightforward from sophisticated dependently-typed proof assistants such as Agda. Within that framework, there's lots of room for play. Certainly, there is still a difference between a non-strict and a strict language. However, in the absence of side-effects, the only difference is that a non-strict language contains more expressions which do not denote bottom -- insofar as the two evaluation strategies both do not yield bottom, they agree. Simon's statement also fits in a very important historical context. At the time of the birth of Haskell (1987), there were a panoply of non-strict functional languages -- not only Miranda, but Lazy ML, Orwell, Clean, and many others. Aside from certain syntactic variations, they all were very much the same language. Which was precisely the motivation for the Haskell Committee to form. For more on this, see "A History of Haskell: being lazy with class": $URL$ 

Migrated and expanded from a comment: I think this must vary by subfield. Nearly all the Theory B stuff I'm familiar with (and especially Haskell, Agda, and sometimes Coq -related) includes published code, sometimes even as an appendix or better yet inlined within the paper. A fair number of papers from, e.g., ICFP are written as literate programs to begin with, and their source in its entirety is published by the authors. A fair amount of those in turn have resulted in extracted libraries for distribution. Of the remaining papers, a fair amount never had code to begin with. Of those, there's probably two main reasons. First are the papers whose main content is proof trees, typing rules with associated soundness proofs and the like. Of those, advances in mechanized metatheory have encouraged at least some authors to provide code in their theorem prover of choice (see Weirich's slides on POPLmark: $URL$ Second are those which are descended from the Bird-Merteens stuff (banannas & co.). These are generally translatable into a functional language without too much work. However, I suspect that there's both typically a loss of generality, and that dealng with concrete issues of syntax and typing needlessly complicates things and makes it harder to follow the equational reasoning. I wanted to substantiate my observations a bit, so did a rough count of the first two days of ICFP 2010. Of standard papers (i.e. not experience reports or invited talks), 12 out of 21 provided code of some sort. Three provided Coq (a fourth claimed a partial proof but did not publish it). Three proided Haskell. Three provided Agda. One provided Scheme, one provided Caml, and one provided Twelf. (Note that some provided code for more than one proof assistant, or for both a formalization and an implementation). Of the remaining papers, a few did work at a high enough level of abstraction that implementing it in a proof assistant would be a new paper in itself, and a fair number more did work that I suspect could have been implemented in a proof assistant using standard techniques, but which certainly would have taken a fair amount of work to do so. A few further papers claimed implementations/releases as future work. 

Look at T. Saaty, Thirteen colorful variations on Guthrie's 4-color conjecture, American Math. Monthly, 79 (1972) 2-43 for many examples. Also, in David Barnette's book Map Coloring, Polyhedra, and the Four-Color Problem, MAA, Dolciani Series, Volume 8, 1983 many examples are given. One particularly interesting result in Barnete's book is: If it is always possible to truncate vertices of a convex polyhedron so as to to produce a 3-valent convex polyhedron so that the number of sides of each face is a multiple of three, it implies the truth of the four color conjecture. 

For a very nice account of how error-correcting codes are used in a particular practical situation look at: The Mathematics of the Compact Disc, by Jack H. Van Lint, in Mathematics Everywhere, M. Aigner and E. Behrends (editors), American Mathematical Society, 2010 (This book is a translation from the German original.) 

Interesting results on this issue can be found on pages 24 and 25 of the book: The Stable Marriage Problem by Dan Gusfield and Robert Irving, MIT Press, 1989. 

There are many infinite classes of graphs which are known to have hamiltonian circuits. Two especially interesting classes are the n-cubes and the Halin graphs. One way of thinking of the Halin graphs is to embed a tree with at least 3 vertices and which no vertices of valence two in the plane, and then pass a simple circuit through the 1-valent vertices of the tree. $URL$ These graphs are known to have an HC and in fact they are either pancyclic (circuits of all lengths) or lack exactly one circuit length which must be of even length. 

While not directly related to your question you might want to look at the work on degree sequences of planar graphs. There are no known characterizations of when a degree sequence is the degree sequence of a planar graph. However, there are a variety of interesting papers about such matters including: $URL$ 

Perhaps this paper may be of interest: Kleinschmidt, Peter Regular partitions of regular graphs. Canad. Math. Bull. 21 (1978), no. 2, 177–181. It deals with graphs which can be written as the union of "Z-paths" of length 3. (Specifically, planar, 3-valent, 3-connected graphs-cubic 3-polytopes.) 

This example, while not meeting the letter of your request may be of interest because it bears some spiritual affinity. Specifically, the question of sorting stacks of pancakes and burnt pancakes by reversals. $URL$ One area of application is to computational biology (genetics) where questions about genome rearrangements can be couched in terms of the distance between permutations using reversals of pieces of the permutations subject to various rules. 

You may find this account of interest: $URL$ and in particular the article by G. Ziegler: Lectures on 0-1 polytopes in: Kalai, Gil; Ziegler, Günter M. (2000), Polytopes: Combinatorics and Computation, DMV Seminar, 29, Birkhäuser, ISBN 9783764363512 . 

In coding theory, Belief Propagation is heavily used as a good heuristic for decoding (either explicit or randomly generated) LDPC codes in various settings (e.g., for the erasure channel, you want to satisfy all constraints more quickly than Gaussian elimination. For noisy channels, you want to find the "best fit", etc). I think techniques used there are directly relevant to your question. You may want to have a look at the book "Modern Coding Theory" by Urbanke and Richardson for an extensive discussion. 

In general, since the instance may have no solution at all, the problem is not too meaningful (or if you want to find the minimum weight solution if one exists and fail otherwise, then the problem becomes at least as hard as SAT itself). Therefore, you may want to impose restrictions to ensure that 1) a solution always exists, and 2) smaller weight solutions are harder to find than those with larger weight. One such restriction would be to not allow negations in the instance. In this case, the all-ones assignment is always trivially satisfying and more generally we will have a monotone structure in the solution space (so that flipping variables from 0 to 1 will keep a satisfying assignment feasible). In this case, it is easy to see that the problem of finding minimum-weight satisfying assignment for SAT (with $n$ variables and $m$ clauses) becomes equivalent to the minimum set-cover problem (with universe size $m$ and $n$ sets). Namely, the $i$th element of the universe belongs to the $j$th set in the set-cover instance iff the $i$th clause in the SAT instance contains the $j$th variable. If you restrict yourself to the special case of 2-SAT, the problem becomes equivalent to minimum vertex cover, and in general for $k$-SAT the problem would be equivalent to minimum $k$-uniform hypergraph vertex cover. 

Yes. For example, a Reed-Solomon code contains a BCH code, which is a binary linear code, as a sub-code. These are called subfield-subcodes. 

The Eulerian Tour problem is of course a well-studied classical problem in graph theory (Wikipedia article). This question concerns non-Eulerian graphs; i.e., graphs that contain one or more odd-degree vertices. In a nutshell, I wonder what is the best "Eulerian-like" way of traversing non-Eulerian graphs. Given an undirected connected graph, let an Eulerian "semi-tour" be defined as a "cycle" that passes each edge at least once and can visit any vertex or edge multiple times. Consider the following costs for a given semi-tour: 

I still think Suresh's comment below the question is enough to show that any ratio is possible. If you are not convinced with that, you can look at Boolean Constraint Satisfaction Problems (CSPs), for example. Background: Let $P: \{0,1\}^k \to \{0,1\}$ be a predicate of arity $k$. An instance of Max-CSP(P) is over $n \gg k$ Boolean variables $x_1, \ldots, x_n$. A literal is any variable or its negation. The instance consists of $m$ constraints, each of the form $P(\lambda_1, \ldots, \lambda_k)$ where the $\lambda_i$ are some literals, and the goal is to find an assignment of the variables that maximizes the fraction of satisfies constraints. For example, in $3SAT$ we have $P(x_1, x_2, x_3) = x_1 \lor x_2 \lor x_3$. Define $\rho(P)$ as the fraction of $2^k$ possible inputs that satisfy $P$ (for $3SAT$ it is equal to $7/8$). It is trivial to approximate any Max-CSP(P) by a factor $\rho(P)$ by assigning random values to variables (and then derandimize using the method of conditional expectations). Note that here we have the convention that approximation ratios are positive reals no more than 1. A predicate $P$ is Approximation Resistant (AR) if it is NP-hard to solve Max-CSP(P) better than by a factor $\rho(P)$ (i.e., $\rho(P)+\epsilon$ for any fixed $\epsilon > 0$). Note that any AR predicate demonstrates a tight approximation threshold $\rho(P)$. It is known that there are predicates $P$ with arbitrarily small $\rho(P)$ that are approximation resistant, and remain so even if you add to the accepting inputs of $P$. For example, the following paper shows one such result: Per Austrin and Johan Håstad, Randomly Supported Independence and Resistance, SIAM Journal on Computing, vol. 40, no. 1, pp. 1-27, 2011. So this takes care of all rational thresholds whose denominator is a power of two. For other thresholds, observe that if suffices to show that for every $\alpha$, there is an $\alpha' \leq \alpha$ for which there is an AR predicate with $\rho(P) = \alpha'$ (since it is always possible to add dummy variables and constraints of them that are trivially satisfiable so as to increase the approximation threshold).