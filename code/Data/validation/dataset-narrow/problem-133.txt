To answer your first question, I would say ICMP run up on the third layer TCP/IP but don't use the fourth (TCP/UDP not involved) so I think it's correct to describe it as running on 3.5 Layer. You can analyze yourself the data inside ICMP using a packet analyser (Wireshark recommended). Here a picture to get a better idea. 

After a search on configuration guide and documentation, I didn't any evidence of a limitation on configuration on a certain VLAN on 450x SUP6E. Anyone know where I can found such information or confirm if is there any ? By my perception, there is no limitation on secondary IP possible to configure on a SVI or interface. Thanks. 

Meraki devices should be configured and managed remotely by a controller located in the cloud Meraki Dashboard Login. Locally you have access to limited information like the connectivity info. If your modem provide Internet access on LAN and gives private address through DHCP, you should not configure a public IP. If you don't own a meraki dashboard account and the device is not attached to your account, you will not be able to manage the device. 

Layer 2 redundancy is slow to converge. Spanning-tree is not a protocol meant to assure redundancy. Also be careful that layer2 is not routed: usually one of your red link will be cut by the protocol. That means that for a host plugged on floorSw5 to speak to a host on floorSw1, it will not be routed directly via: distSw2 -> distSw1 -> floorSw1, even if it would take less hops. Last but not least, spanning tree is based on timeouts, which means that you are not allowed to cascade more than 7 switches (you already have 6). If for some reason you artificially lower the timeouts, this setup may become less stable. If you can not come with a full layer3 network, I would suggest looking at the next generation datacenter switches with network fabric and trill (which is basically a routing infrastructure for layer2 devices). 

Look into enabling BFD. BFD is a separate protocol that EIGRP can subscribe to for failure detection. It can provide sub-second failure detection in a LAN environment, but you don't want that over a lossy WAN connection like the internet. You'll tune the timers higher for the WAN, but the advantage with BFD is you can configure BFD dampening. When you are on the border of a cell/sat connection you can get a lot of flapping back and forth when using low timers, if you configure dampening it will detect the flapping and stop the convergence for a time period to stabilize the connection. 

You have to at least create static host routes pointing to the endpoint IPs of your tunnel to make sure they remain established. Front door vrf is handy if your endpoint is using a dynamic IP address, forcing you to have a default route pointing out the internet circuit. Splitting it into different vrfs allows you create a second default route pointing down a tunnel. Edit: When you create a tunnel, you will have a source IP and destination IP for the tunnel. 

Then put your host in a trunk with all the vlans. Having an interface on your PC to each vlan would do the trick: 

You may have one file called x.x.x.10 on your current directory. Then bash will interpret the command as: nmap -sP x.x.x.10 I would suggest you scan this way: nmap -sP x.x.x.0/24 

Might be a case of unicast storm flood If pinging from server2 to server1 is fixing the issue, that might be that. Otherwise, I would check that there is no proxy arp anywhere in your network. 

The proper way: A is sending a FIN, telling that it would not send new TCP segments anymore. It will continue sending ACKs though. The connection is half closed and can stay like this forever. When B has finished sending data, he can close his part of the connection with a FIN too. When he has received the last ACK, the connection will be totally closed. For various reasons A can send a RST packet, telling that this connection is not taken into account. When receiving the RST message, B will stop the connection quite immediately. 

This depends on the model of device. Some of them have dedicated physical management interfaces. Others just use a vlan SVI interface. 

For Net A you are using a /22, but the chart shows each network is a /24. A /22 would actually look like: 

It seems your question is actually about duplicate IP addressing. If your system has the same IP address as a remote system, it will always choose itself because it believes that it owns that IP address and will never consider a remote system as having that IP. 

If you want to see power consumption of PoE, you can use "show power inline" command which will display power usage per port, plus total power and available power. 

Yes you can, you just have to classify the traffic. Here is an example where we classify iSCSI into its own class. The class-default class will match any other traffic. 

eBGP makes your border routers learn about best routes to reach each prefix on Internet. iBGP will let all your internal routers know about those external prefixes. R4 for instance should know that some prefixes are preferred through R1, even if it's directly connected to R2. iBGP does usually not use internal routing protocol directly. If R1 anounces a prefix with its local interface as a source, either: 

The answer is both technical and historical. IP is the network that interconnect several networks. As ethernet nic have a MAC adress at layer 2, something else is needed at layer 3: IP address. With IP, 2^128 devices are theorically adressable, whereas MAC address are limited to 48 bit. Keep in mind that ethernet is a flat, commuted, broadcast network: it would not scale worldwide. But I agree with you on one point: it is totally plausible to have an IP network without ethernet, or at least without a broadcast layer 2 network. 

QoS will apply policies to different traffic classes as it passes through the device, such as giving priority to certain traffic. But it does not signal to the transmitter to pause. Flow control operates at the interface level and will send a Pause to an upstream transmitter telling them to pause transmission (assuming they also are set up to honor flow control messages). This can affect all traffic passing over the interface. There is an enhancement called priority flow control which applies flow control based on CoS class. The devices negotiate this using LLDP DCBX and exchange their QoS configurations, if they match, then they can successfully use priority flow control between them. Overview of PFC 

What if you don't use a patch panel, then decide you have to move your switch further down the rack or you need to move the connection to equipment in a different location in the rack? Now your cable is not long enough to reach the new location. You are screwed because now you have to run an entire new cable through the walls. With a patch panel you would just grab a longer patch cable and be done with it. 

If you can directly attach 2 hosts, do it. If the link goes down, all your routing protocol will react instantly. Otherwise you will rely on routing protocols timeouts. Moreover a more densely meshed network will guarantee a shorter path. 

You may eventually enable proxy arp on each routers (sub)interface. However some things won't work well with proxy arp enabled. It would make a good transition solution, for the time you renumber your hosts. 

The layer 2 broadcast address ff:ff:ff:ff:ff:ff is used on ethernet frames and is supposedly broadcasted on all equipments. 255.255.255.255 is the layer 3 address that is used to adress the exact same hosts. Note that: 

I personally enable cdp/lldp at least internally (some exchange points usually forbid them though). no ip unreachable is not a good idea: it breaks PMTU discovery for instance. $URL$ . ICMP unreachables can be rate limited though. 

BFD Overview BFD for EIGRP BFD Dampening You'll want to get some spare gear and test this out and tune it before configuring on all your production gear. 

You don't need fabricpath to run a vPC+ from a FEX. Fabricpath is really only useful in large data centers where you have a multiple tier topology (Core, Aggregation, Access). In a smaller setup using a collapsed core type design, vPC is simpler and accomplishes the same goal of getting rid of spanning tree. 

This way you can reach the other end of the tunnel. Then you can point the default route down a tunnel and it won't break your tunnel. 

If you want to avoid pointing your default route to your ISP (maybe you want to point down a tunnel instead), you will need at a minimum to create a host route for the tunnel destination IP address pointing to the ISP. 

Everything that has been written is true, but if you're using TCP, there is a strong correlation between latency and bandwidth. TCP does not wait for an acknowledgement to send the next packet. It's using a window. When the window is full, the peer must wait for an ack. Moreover, the faster ACKs are received, the faster the window can grow (it's the slow start algorithm). The stability of the latency can also come into play. 

Note: I used /24 networks but you can totally use /31 (or /127 in IPv6). If your switch can route, then route with it, or you can plug a router too. There may be a way by configuring the interfaces as point to point interfaces, but I have never done this (and it would be even more awkward). 

OSPF will only exchange routes in your Autonomous System. Basically, OSPF knows about all your vlans and all your router interfaces. BGP exchange routes from other AS: how to reach networks from google, twitter, colt, level3... Inside your AS, you may have several BGP routers, that know about those external networks. But somehow they have to exchange the information. For instance your router A may be directly connected to colt network, whereas router B can reach google more easily. The (very) old way was to redistribute all the external routes inside OSPF. It is not a good practice now because OSPF was designed for faster convergence and won't scale with that many routes. OSPF would flood every change, won't be able to apply 'policies' the same way as BGP, and would not support dampening (AFAIK). The current best practice is to have internal BGP sessions between all your routers (iBGP): external routes exchanged between your internal routers. Hope it helped.