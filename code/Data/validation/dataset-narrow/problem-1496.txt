When we were drafting the English national curriculum, we found it easier to think in terms of the foundations, applications and implications of computing, all three of which really should be included in any broad and balanced approach to the subject. You can map these to computer science, IT and digital literacy if you wish, although you would need to accept a rather broader definition of digital literacy than that used by the Royal Society Foundations would be about the underpinning principles of computer science (logic, algorithms, data representation, abstraction), as well as their practical expression through programming and more generally in computational thinking. Applications is about skills in using digital technology to get useful work done, including collecting, managing, analysing and communicating data and information and creative work in a range of digital media. Implications is about a critical understanding of the impact of digital technology on individuals and society as well safe, responsible and ethical use. I'd include intellectual property, privacy and security here too. I've an illustration of all three in response to the question 'How does Google work?' Foundations: big data, Page Rank, Big Table / the Google File System (GFS) etc Applications: type your query, click the button (well, these days it starts searching as you type), but also filtering results, advanced queries etc Implications: profile, filter bubbles, advertising, smart creatives, separating costs and revenues for accounting purposes etc 

Does the concept of pointer arithmetic belong in a FIRST class on Java? That depends considerably on how long the course is. I would say in a full semester course it would deserve perhaps a ten minute discussion at some point fairly well into the course, just for some comparative approach to Java's references and to emphasize that pointers are still used under the hood. And so that your students understand that there are other ways to do things than how Java does them. 

I'm sure there are plenty of others. Not sure I'd use them in every lesson, but they might be a useful incentive towards more purposeful use of the devices they bring. I know one school where in free time pupils are only allowed to play games they've coded themselves... 

We're introducing some aspects of parallel processing quite early on in Scratch. Each sprite has its own script which appears to execute in parallel with those of the others. Scratch has a broadcast and receive message protocol, and support for shared as well as private variables and lists. Children might encounter this in a maze game, perhaps programming a number of similar ghosts to chase the player's avatar. It's also useful for agent-based modelling, e.g. the spread of an epidemic through a population. Of course, it's not true multi-threading, as all of Scratch runs inside Flash, inside the browser, on just the one core, but I doubt those using Scratch will be aware of, or care about, the distinction. This does, though, lead to potential difficulties in 'graduating' from Scratch to a text-based language such as Python - young Scratchers who've been used to programming in parallel in Scratch can find it hard to adjust to doing just one thing at a time in introductory Python programming. 

In application of this, I wouldn't even teach someone the normal forms as the first step. I would mention that there are different ways of structuring your relations, and that there are rules to follow which lead to a logical structure, and that we would get to them shortly. The basic axiom I would teach is that each piece of information should be stored in only one place, so that it can't go out of sync (become inconsistent). Then I would get some student interaction going and get the students to give examples of pieces of information they would keep track of. This PDF was my first introduction to database design. Borrowing and expanding an example from that, I might ask students: what information would they would want to track for a database of books? Title and author are obvious. Some others will be mentioned: Page count. Publication date. Publication format. Publisher. How about "language"? ISBN. Author's date of birth and date of death may be mentioned. Writing these out across a chalkboard and start considering the matter further with a few examples and the huge number of problems with having a single relation will all of these attributes will become apparent: What happens when a book has more than one author collaborating? What happens when multiple books were written by the same author, and then the author dies (and you have to update "date of death" in multiple places)? What happens when the same book was published more than once, in different years, by different publishers, with different formats and page counts? Of course you wouldn't throw all those problems at them at the same time. But gradually, little by little, you could decompose that giant "BOOKS" relation into a database with all originally mentioned attributes, but in proper normal form. And if you're doing it really well, your students would do all the normalizing, even if they don't know the rules they're following. After any particular student suggestion (like making a separate "authors" relation to hold such information as dates of birth and death), I could then acknowledge it, validate it, and then present the general rule. 

Any programming is a two step process: deciding how to solve the problem, then implementing that as code on a particular system: choosing or designing an algorithm is the first step. There are great ways to illustrate how the choice of algorithm matters. An introductory one might be search - comparing random, linear and binary algorithms to, for example, find a missing number, or a word in a (printed) dictionary, or a book in a (physical) library. Another might be exploring different sorting algorithms, for example bubble sort and quicksort using this CS Unplugged activity. Mathematics provides a rich source of contexts, for example asking students to think of an algorithm for finding the greatest common divisor (i.e. highest common factor) for a couple of numbers. Have them try their algorithms out on paper before coding them and then testing with some big test numbers. 

In short, rather than dumping data and explanations on top of them, I would get them to look and derive and establish the data for themselves, so they could really use it. Only then might I add additional explanation. 

I think that the "problem first" approach applies here. Rules like normal form weren't handed down from on high; they were developed as logical solutions to actual existing problems. One key principle from a favorite article on teaching: 

(The balance between theoretical and practical knowledge I am advocating here is well conveyed in Mr. Hubbard's Study Technology, which I highly recommend for all educators and students.) 

A variant on the usual random drill and practice test would be to pre-populate with the questions and answers, then remove question and answer from each as they get answered correctly, allowing players to get more practice on the questions they get wrong. Here's an example for times tables. You could try something for an adventure game, building up an inventory of items collected in a list. Another possibility would be an adaptive 20 questions style game, adding additional questions into a database (of sorts) as the player gets to the end of a branch of the tree. 

Of course, this is a trick answer, because I do think both concepts (pointer arithmetic and reusing pointers as instructions) should be taught to Java students as ideas. As part of the history of the subject, and the layers of abstraction underneath the level they're working at. But you shouldn't make them actually do any pointer arithmetic. It's totally sufficient that they understand that there is such a thing and have some idea about roughly how it was done. It's just a significance that they're not going to actually use in Java, so it's good background, but don't overdo it. It's not closely related to what you're teaching them to DO. Your students need to have: 

Harvard's grading policy for CS50 is worth looking into. There are four components for the grade on problem sets (each of which involves submitting code). The overall grade is calculated as scope * (3 * correctness + 2 * design + 1 * style) Scope: to what extent does the code implement the features required by the specification? Correctness: to what extent is the code consistent with the specifications and free of bugs? This is done by the check50 autograder, and it's essentially unit testing. Style: to what extent is your code readable (i.e., commented and indented with variables aptly named)? there's a component for formatting: I think in Harvard's case these marks are awarded by teaching assistants, but basic static analysis or linting might suffice. Design: essentially, is this good code in terms of clarity, efficiency, logic, elegance - again, Harvard use TAs to award these grades, and it's hard to see a machine (or an inexperienced grader) being able to award these marks accurately any time soon. If you were determined to use automatic grading, I guess you could do something with run times for test data or the more sophisticated forms of static analysis. A compromise might be the use of peer-assessment and a detailed, criteria based rubric: peer assessment might have other benefits. 

An excellent mastery on the practical skills they need to code in Java, and A clear enough understanding of the theory and underlying concepts and background of what they are doing that they won't become obsolete after some new CS breakthrough or new language or new industry direction. 

A related (rhetorical) question: Should C students be taught the [machine language] idea of reusing a pointer as a CPU instruction if the value happens to line up? The speaker in a Functional Programming talk I recently saw discussed part of the history of programming languages, and the opinion seriously held by some at one time that assembly language was too far abstracted because it didn't allow you to reuse CPU instruction codes as pointers and vice-versa if the values lined up. :) Also related (classic): The Story of Mel. Then, the follow-up: How is that fundamentally different from teaching Java students pointer arithmetic? 

There are some history topics that link really well with computing - for example a history of communication, taking in writing, printing, semaphore, the telegraph and Morse code etc, through to the internet and the web. The English history curriculum for 5-7 year olds suggests that pupils compare William Caxton and Tim Berners Lee. Another great topic would be cryptography, perhaps starting with the Caesar cipher, mono-alphabetic and poly-alphabetic substitutions, Enigma, Colossus and other work at Bletchley Park, public / private key encryption (with applications to SSL), some of the contemporary issues around privacy and perhaps a look ahead to quantum cryptography. YMMV, but I think context like these may make these topics more engaging than a straight history of computing unit.