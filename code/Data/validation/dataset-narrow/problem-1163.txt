There are many examples of games studied in combinatorial game theory where the state of a game can be described by a constant number of integer values. For some of these, a winning strategy for the game can be computed in constant time. But they also raise questions about what exactly your model of computation is. One of the simplest and most basic combinatorial games is nim: one has a constant number of piles of beans, and in a single move you can remove any number of beans from one pile, either winning or losing (depending on your choice of rules) if you take the last bean. The optimal strategy can be computed in constant time if you allow bitwise Boolean xor operations (i.e. the ^ operator in programming languages like C/C++/Java/etc.) Is this a constant time algorithm in your model? Here's one where it is known that there exists a constant time exact deterministic algorithm (in a possibly-unrealistic extended model of computation that allows you to test primality of a number in constant time) but it's not known what that algorithm is: given a starting move in the game of Sylver coinage, determine whether it is a winning or losing move. A flowchart for this problem is given in Berlecamp, Conway, and Guy, Winning Ways, but it depends on a finite set of counterexamples to a general characterization of the winning moves, and it's not known what that set is (or even whether it is empty). Another interesting example from combinatorial game theory is Wythoff's game. Each game position can be described by a pair of integers (i.e., constant space, in your model of computation), moves in the game involve reducing one of these two integers to a smaller value, and the winning strategy involves moving to a position where the ratio between these two integers is as close to the golden ratio as possible. But in many game positions there is a choice: you can reduce the larger of the two integers either to the point where it is (nearly) the smaller integer times the golden ratio, or the smaller integer divided by the golden ratio. Only one of these two choices will be a winning move. So the optimal strategy can be defined in terms of a constant number of arithmetic operations, but these operations involve an irrational number, the golden ratio. Is that a constant time algorithm in your model? Maybe it's an example of non-uniform constant time, where the algorithm is constant time if it is given access to a constant amount of extra "hint" information that depends on $n$ (an approximation to the golden ratio accurate to $\log n$ bits) but not constant time using only arithmetic operations (no square roots) and fixed integer constants values? 

This is a combination of comments from me and Chandra Chekuri above, elaborated a bit. As background, if you have a partial matching then its symmetric difference with the optimal matching can be decomposed into disjoint alternating paths (and possibly also some alternating cycles but those can be ignored). Hopcroft–Karp maintains a partial matching as it proceeds in a sequence of stages. Each stage exhausts all the shortest alternating paths left over from the previous stage, and increases the shortest alternating path length by at least two (because the length must be odd). Each stage takes linear time. The exact algorithm can be proven to take $O(\sqrt n)$ stages, leading to a nonlinear total runtime. First, the additive approximation. If the shortest alternating path length is $k$, then the symmetric difference between your matching and optimal can have at most $n/k$ disjoint alternating paths, so the difference in cardinality between your matching and optimal is at most $n/k$. So if you set $k=1/\epsilon$ for your favorite constant $\epsilon$, and stop after $k/2$ stages, you should get an additive approximation of $\epsilon n$ to the optimal matching, in time $O((m+n)/\epsilon=O(m+n)$. Now, for a multiplicative approximation, again suppose that the shortest alternating path length is $k$, and observe that each alternating path includes at least $(k-1)/2$ edges that are already part of your matching. So the ratio between the number of edges you already have in the alternating paths and the number you can get by adding one more edge per alternating path is at most $$\frac{k-1}{2}\left/\left(\frac{k-1}{2}+1\right)\right.\approx 1-\frac{2}{k}.$$ Therefore, if you set $k\approx 2/\epsilon$, and again do only $k/2$ stages before stopping, you will get a $(1-\epsilon)$-approximation, again in time $O(m+n)$. 

The Černý conjecture is still open and important. It is about DFAs that have a synchronizing word (a word with the property that two copies of the automaton started in different states always end up in the same state as each other after both processing the word), and asks whether (for $n$-state automata) the length of the shortest such word is always at most $(n-1)^2$. The best proven bounds are of the form $O(n^3)$. 

My suggestion would be to look carefully at Courcelle's theorem, that problems expressible in (certain extensions of) monadic second order logic have FPT algorithms when parameterized by treewidth. My suspicion is that this covers many or most of the known examples of FPT problems for these graphs. In this view, your local/global distinction seems to be closely related to the distinction between problems expressible in existential MSO vs problems that have higher levels of quantification in their MSO formulations. To return to your actual question, lack of an MSO formulation (which can be proven unconditionally in many cases using ideas related to the Myhill–Nerode theorem) would be evidence towards lack of an FPT algorithm (harder to prove without complexity theoretic assumptions). 

For the consequences of such transmission to theoretical computer science (the only aspect of your question that is on-topic here) see Aaronson and Watrous's "Closed Timelike Curves Make Quantum and Classical Computing Equivalent". 

If the graph doesn't already have any K4, then Q2 is the same as asking for whether the graph has treewidth at most two. This is easy to check in linear time: it's true iff every biconnected component is series-parallel. 

I don't know how to avoid doing $n$ matrix multiplies, but you can analyze it in such a way that the time is effectively that of a smaller number of them. This trick is from Kloks, Ton; Kratsch, Dieter; Müller, Haiko (2000), "Finding and counting small induced subgraphs efficiently", Information Processing Letters 74 (3–4): 115–121, doi:10.1016/S0020-0190(00)00047-8, MR 1761552. The first observation is that, when you're going matrix multiplies, the matrices are not really $n\times n$, but $d\times d$ where $d$ is the degree of each vertex, because what you're looking for is a co-triangle in the neighborhood of each vertex. Second, in a claw-free graph, every vertex must have $O(\sqrt m)$ neighbors. For, otherwise the set of neighbors would have too few edges to avoid having a triangle in the complement. So when you're doing matrix multiplications, you only need to do it on matrices of size $O(\sqrt m)$ rather than $n$. In addition, each edge can contribute to the size of the matrix multiplication problem for only two vertices, its endpoints. The worst case happens when the $2m$ for the total size of these matrix multiplication problems is spread into $O(\sqrt m)$ subproblems of size $O(\sqrt m)$ each, which gives a total time bound of $O(m^{(1+\omega)/2})$, an improvement for sparse graphs over the $O(nm^{\omega/2})$ bound mentioned by R B. 

Most of the patterns in my collection at $URL$ were found with a somewhat different heuristic search process, that was intended to find puffers but also turns out to work for replicators: 

Solving the planted clique problem of distinguishing a uniformly random graph from the union of a random graph and a clique (of size intermediate between $2\log_2 n$ and $\sqrt n$), with success probability bounded away from 1/2. It differs from your ETH-violating example of finding polylog-sized cliques in arbitrary graphs, because this is an average-case problem not a worst-case one. 

The one-dimensional traveling salesperson path problem is, obviously, the same thing as sorting, and so can be solved exactly by comparisons in $O(n\log n)$ time, but it is formulated in such a way that approximation as well as exact solution makes sense. In a model of computation in which the inputs are real numbers, and rounding to integers is possible, it's easy to approximate to within a $1+O(n^{-c})$ factor, for any constant $c$, in time $O(n)$: find the min and max, round everything to a number within distance $(\max-\min)n^{-(c+1)}$ of its original value, and then use radix sort. But models with rounding have problematic complexity theory and this led me to wonder, what about weaker models of computation? So, how accurately can the one-dimensional TSP be approximated, in a linear comparison tree model of computation (each comparison node tests the sign of a linear function of the input values), by an algorithm whose time complexity is $o(n\log n)$? The same rounding method allows any approximation ratio of the form $n^{1-o(1)}$ to be achieved (by using binary searches to do the rounding, and rounding much more coarsely to make it fast enough). But is it possible to achieve even an approximation ratio like $O(n^{1-\epsilon})$ for some $\epsilon>0$? 

As you observe, all perfect graphs can be colored in polynomial time, but I think the proof involves ellipsoid algorithms for linear programming (see the book by Grötschel, Lovász, and Schrijver) rather than anything direct and combinatorial. There are a lot of different classes of graphs that are subclasses of perfect graphs and have easier coloring algorithms; chordal graphs, for instance, can be colored greedily using a perfect elimination ordering. All locally connected graphs (graphs in which every vertex has a connected neighborhood) can be 3-colored in polynomial time, when a coloring exists: just extend the coloring triangle by triangle. Graphs of maximum degree three can be colored in polynomial time: it's easy to test whether they're bipartite, and if not then either they require only three colors or they have K4 as a connected component and require four colors (Brooks' theorem). Triangle-free planar graphs can be colored in polynomial time, for the same reason: they are at most 3-chromatic (Grötzsch's theorem). 

You are being a little sloppy with your definitions: what exactly is the input to U, and what exactly are the conditions to accept? In particular, you probably need to include in the input to U not just a Turing machine but also a specific polynomial that bounds the running time of the machine, because otherwise it's undecidable whether any particular TM really is polynomial time. But that's not the real problem. The actual problem is that L(U) is not in NP. To be in NP, there needs to be a single polynomial p(n) that bounds the time to check a solution. But in your case this time is bounded by a polynomial q(n) that depends on the particular TM given as input. It's easy to choose inputs that cause q(n) to be larger than any fixed choice of p(n). 

According to Vassilis Giakoumakis and Irena Rusu, Disc. Appl. Math. 1997, the (P5,house)-free graphs (aka (P5,coP5)-free graphs) are IS-easy. Another one, credited by ISGCI to V. Lozin, R. Mosca Disc. Appl. Math. 2005, is the family of (K2 u claw)-free graphs. There might also be infinite ascending chains of tractable classes There are definitely infinite ascending chains. If H is a finite set of graphs for which the H-free graphs are IS-easy, let H' be the graphs formed adding an independent vertex to each graph in H. Then the H'-free graphs are also IS-easy: just apply the H-free algorithm to the sets of non-neighbors of each vertex. For instance, as ISGCI describes, the co-gem-free graphs are IS-easy for the reason that a co-gem is a P4 plus an independent vertex and the P4-free graphs are IS-easy. So you probably want to restrict your question to maximal classes in which not all of the forbidden subgraphs have an independent vertex.