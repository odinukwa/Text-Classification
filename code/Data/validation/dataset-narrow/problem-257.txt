Here, Postgres knows that the index is bad and marks it as such in . There are countless other ways to create a corrupt index that Postgres won't immediately notice: Erwin's answer, mismatched glibc or other collation behavior between a primary and a standby, writing arbitrary bytes to the files behind those indexes, performing bogus updates to Postgres' catalog tables, and on and on. 

Canceling queries (or, equivalently, rolling back a transaction) in PostgreSQL doesn't have any database corruption hazards which you might have been spooked by in certain other databases (e.g. the terrifying warning at the bottom of this page). That's why non-superusers are, in recent versions, free to use and to kill their own queries running in other backends -- they are safe to use without fretting about database corruption. After all, PostgreSQL has to be prepared to deal with any process getting killed off e.g. SIGKILL from the OOM killer, server shutdown, etc. That's what the WAL log is for. You may have also seen that in PostgreSQL, it's possible to perform most DDL commands nested inside a (multi-statement) transaction, e.g. 

Having said all that, I still use it and recommend it. I generally try to let developers use just that tool instead of SQL Server Management Studio (SSMS) because I look at SSMS as a DBA tool and SSDT as a developer tool. 

Your server authentication mode may be set to Windows only instead of Mixed Mode, which will allow Windows logins and SQL logins. If true then the SQL login will not work. In SQL Server Management Studio, you can right-click on the server and then go to the properties. Then go to security and see how the server security is configured. You can change it there also. If it is set to Windows only then you will have to use a Windows login. If you want to change to mixed mode to use SQL login, you can make the change but that may require the instance to be reset to take affect. 

I had similar issue before. I had all SQL Services that I needed running on MSA or gMSA except for Reporting Services (SSRS); this stayed this way on Windows 2008 R2 and 2012. This issue went away when moved to Windows 2012 R2. In my experience, SSRS was the only service that would not run on MSA or gMSA on OS prior to Windows 2012 R2 but the database, analysis, agent, and integration services all worked ok with them even when not officially supported (MSA running with SQL Server 2008 R2). At this link, $URL$ it states: 

You can set seq_page_cost and random_page_cost per tablespace via ALTER TABLESPACE without restarting Postgres. 

Assuming you are talking about the column in , or some status field derived therefrom, you can create an invalid (or intentionally corrupt, if you prefer) index like so: 

although semantically the UPDATE above is exactly the same as if it did not have a WHERE clause (ignoring triggers), you will avoid a ton of I/O for tuples which already have some_bool_column = false. Second, if you can, try to take advantage of Heap-Only Tuples aka the HOT optimization for your UPDATEs. So if you can avoid having an index on some_bool_column, these bulk UPDATEs will be faster and may be able to avoid contributing to index bloat. Often, you don't want to have an index on a boolean column anyway, since the selectivity will be low (or if one of the boolean values is rare, just use a partial index on that value.) 

which will more-or-less double the size of the table, since the UPDATE has to keep old row versions around. You may want to read up a bit on how PostgreSQL implements MVCC and how vacuuming works, but to answer this question: 

SQL Server Reporting Services (SSRS) is a server-based platform for hosting reports. Report builder is client tool that may be used to create reports to deploy to SSRS. As such, you would use SSRS to manage security on for the reports that you may have deployed with Report Builder. 

The RSAT tool was updated with the general availability of Windows Server 2016. By going to the same link as the preview RSAT, $URL$ you can update your workstation with the new version. 

I agree with some of the other comments though that perhaps your servers are set to auto update and this is how it could have occurred. Based on how your server is configured and combined with the changes to how CUs are installed, it could have auto updated. 

There is a lot of info out there about this behavior going back years. Here is a thread: SQL Server Management Studio slow opening new windows Common thread there seems to be SSMS is trying to reach a location in the internet. Another thing there states to change the user feedback Opt In setting. Maybe one of these options will speed it up 

In addition to Craig's thorough answer, I wanted to add that the cover of the book you reference says: 

So, 300k rows total doesn't seem like a huge amount, I wouldn't be overly worried unless you have a particular cause for concern (e.g. your UPDATE taking way too long, holding row locks for too long, etc). But two suggestions which may be helpful for your particular use-case: First, make sure that your UPDATE statement does not touch rows it does not need to. If you want to set all values of some_bool_column to false, do it like this: 

You can also see in the pg_constraint table (e.g. via for some referencing foreign key ) that PostgreSQL is internally tracking this dependency by the OID (in particular, the column in ) of the index rather than the name of the index. The OID of the index does not change when you use , which is how Postgres avoids becoming confused by the rename. 

Amazon's RDS only offers PostgreSQL versions 9.3.x, and it seems unlikely that they'll ever offer to host older versions of Postgres. So by jumping from a local 8.4 install directly to RDS, you would in effect be making two significant changes at once (jumping up several Postgres versions, as well as switching to managed hosting). That may be alright or not -- it all depends on what features you're using and depending on. You should do some reading on RDS's limitations (no external hot standby, limited extensions, no shell access to the database instance, etc.) and benefits (hopefully much less maintenance work) and decide whether it's right for you. Also, I suggest you walk through the steps of dumping and restoring your data into RDS and ensure your application works OK, as well as reading through the Postgres major-version release notes for 9.0, 9.1, 9.2, and 9.3, paying particular note to the incompatibilities listed to see if any of them would affect you. 

SSIS can connect to many different servers based on the connection type used in the SSIS package. Since even SQL Server 2016 SSIS still uses Native Client 11 (and has since 2012), you should not have any issues connecting from SSIS 2008 R2 to SQL Server 2016. SQL Server 2008 R2 uses Native Client 10 or 10.1 but has no issues connecting to SQL Server 2016 either. In the connection manager in SSIS, you choose the way to connect to SQL Server. It could be ODBC, OLE DB, or any number of connections types. I say this to say SSIS itself does not care what version of SQL Server you are connecting to but the task you use will determine the connection type. I would assume you are using OLE DB and as such you may choose the provider in the connection string. 

LinkedServerName would be the real linked server name and SourceTableName would be the name of the table in the linked server. 

The USE command changes the database context to the specified database or database snapshot in SQL Server. I believe there should be a query with matching columns after your INSERT statement that would be the source for the INSERT. It may be OPENQUERY and could look like the following: 

Erwin's answer does a good job of answering the question as originally stated, however you added the comment: 

Supposedly it is possible to hook up Bucardo to RDS now that RDS Postgres supports the session replication role, but if you want a nightly snapshot I think you'll be much better off using RDS instance snapshots. 

The space utilized by the table should go down if you run a or ; probably a alone will not be sufficient to immediately reclaim space. 

PostgreSQL reads its (index, heap, etc.) blocks either from shared_buffers, if they are available there, or from disk if not. Postgres does not need to read out of its WAL files other than for crash recovery (similarly, standby) purposes. 

and raise an alarm if that max. age is over 1 Billion or so. And if it gets close to 2 Billion you are in imminent danger! You can use a tool like check_postgres.pl which will handle this check for you with some configurable thresholds. 

You can look under the "base" subdirectory of the data directory, and you should see something like this: 

I would say the stored procedure itself probably does not include the comment with the current script date. SQL Server Management Studio adds that when you generate the view of the procedure. In my experience, the stored procedure itself starts with CREATE or ALTER and anything above that is added by SSMS when you look at the code. 

While it doesn't specifically state anything, something changed in the Windows 2012 R2 OS that allowed using MSA or gMSA for SSRS and I'm assuming it's related to the quote above. I just ran into a similar situation a few months ago during an upgrade/migration to SQL Server 2016. The original OS was running Windows 2012 and I tried everything I could to make it work but it never worked with SSRS until OS changed to Windows 2012 R2. 

One way of finding obsolete columns is to use SQL Server Data Tools and create a database project of the database. Import all objects from the database into this project. Once everything is imported, do a build on the database project and then it will find all the unresolved references in views, stored procedures, and functions, among other things.