Corrected: with MySQL You can store GUID/UUID data as char/varchar in case of UUID() and UNSIGNED INTEGER in case of UUID_SHORT() My personal vision - UUID_SHORT not working case, it is unsigned INT same as Auto-Increment and You can read restrictions in function description. UUID() will return result as utf8 string - "eabd691c-b552-11e6-942e-ee8ae5dda6a8" Business logic for primary key - work well, You always mange this process and do not need spend time for call function. add after question in comment: performance not depend from business logic or auto increment, it depend mostly from 2 parameters: 

it is fasted way for restore, it could take same time as mysqldump for backup, but would be dramatically faster for restore. You can combine methods. Disadvantages of percona backup or file level cold backup - You are restore all databases at once and will need operate all time with 120+Gb of data. If You will make periodical dumps by database - You can restore them separately on any server. 

Add some visual examples: make a proper test it very hard, but this is small example: was created 3 tables, with only 1 column ID - primary key 

Think - the spatial indexes would be right, but just some tricks which we use on one search project with MySQL 5.5 Look for problem from other side - how many cities could be on 1 degree lat/lon? IF exclude some countries like Malta (where my son walking from Home to School cross 3 cities for 15 minutes) - common answer will be not too much. So, with index for lat and longitude queries like: 

Other possible reason - number of rows processed before send to client Query which return just 100 rows, could sort millions before calculate result. If You already identify this 4-5 queries - just go thru each and check: 

Native replication not help, and in this case better to leave both database on same server and just install trigger for INSER/UPDATE/DELETE on Master database, which will check conditions and make direct changes on second database. This is simplest way. There are many other possible. 

which is included in 2 indexes in Your structure plus change order in index - index_collection_notes_on_collection_id_and_note_id for 

check permissions for my.cnf file - ls -l /etc/my.cnf or ls -l /etc/mysql/my.cnf check owner and group, often when after edit my.cnf it have right for read only for root user You can change owner to mysql:root or change cmd mask for something like 755, finally You will have: 

in ideal situation, when Names.id = Users.id return only 1 record, both queries the same. The difference when it not true. 

there are many modifications possible, depending from what finally You need, such as if need select seq_no for single name 

JSON have optimised storage format compare with old method (save as TEXT), but no any other information You will not find, because JSON size always depends from JSON data, some of them could be more optimised, some less, plus different length, plus ... 

It is like - what zip archiver ratio? Is it same for txt and jpg? some more information there - $URL$ 

Single huge file now must not be a problem with most of OS, but could be difficult for edit. In second case, need analyse merging logic simples way - mysqldump with --no-create-info (not CREATE TABLE), but could be problems with duplicate keys. If some duplicate keys (indexes): or edit dump (for big files - use sed) and change insert to INSERT IGNORE or UPDATE ON DUPLICATE or create ETL Job with any of available package and manage merge logic. 

this query return data, but what the reason of this query if same result You can achieve without un-necessary JOINs? 

must work for You When You use subquery, You must control - subquery must return 0 or 1 records, this is mandatory, for example like this: 

I suggest look at Percona Innobackupex - $URL$ It also possible run from slave (best if replication format ROW). One more variant fastest from any other than LVM: 

each records take 300kb? what also stored in this table. Good idea to split table for 2 it of course only top layer, always need more details about application and environment. 

for ERROR 1146 (42S02): Table 'xx-xxx-xxx-xxx' doesn't exist - not all sql dump include create table instructions, best choice check the text of script, again if file huge - use 

This is a trigger, which is same as function could be called multiply time at unpredicted for You time so the code like 

If computer Similar - You can install MySQL from installation package and then copy only data folder (name and location depend from OS) If computer different (like Windows -> Linux) - You can install MySQL on target computer, on Source computer dump all data using mysqldump, on Target computer make restore from dump using command line client 

remove DATE function from timestamp, in more and add index to timestamp column Insert also very slow, but it could be next tasks The main idea of changes: You have only 1 column in this query, which can reduce number of rows, and You not use this column. DATE() == full table scan. So we must use any possible chance for narrow filter conditions, and any chance for use indexes for query 

but You have only one true choice - start change code for proper form because if in case of hidden type conversion, You are just slowdown server (sometime dramatically), in case of suppress 

Size of binary-logs - this is objective, but with good network it not a issue. for make shipping faster You can enable compression - see more Time what Slave need to apply all changes from Master. Even if You enable multithread slave it still slower than on Master, and by default with single thread it could be a problem with very high amount of changes on Master. 

Server tuning it always iteration process, and affects of tune queries more proper and long term I can suggest You start from: 

As mentioned in comments (it is really answer for original question) - using function in WHERE - prevent any indexes using. First of all, You can check - how many of Your queries use form 

I do not know, what part of table You have in MyISAM, but it always better - finish all works with database as fast as possible, than continue other steps GZIP is may be exactly slowest part of Your script and it really not optimised. It use single core not depending how many of them You have. 

indexes - must be, all comparable columns must be same type, such as events. bigint(20) == events_tags. bigint(20) 

DELETE data on linked server if user rights maped properly run the same as normal SQL DELETE command, with full object names: 

so not reduce number of rows in this case best choice let mysql use default key for JOIN and filter of rows will be by 

as showing in question - no chances (You show result exactly for rows not presented in source tables), but: 

as correctly mentioned in comments - LEFT JOIN not proper choice in Your case, even if You fix the syntax (JOIN require ON condition) You need use INNER JOIN: 

Exactly NOT 10k! - it always bad idea, for each open table MySQL will spend memory, cpu time, disk resources and etc Much better give all this resources back to MySQL and it will manage queries for big table. With proper queries and indexes - millions and billions of rows in production databases. One single table or may be few depend from other logic. Possible solutions: 1 Table for active events Second for archived Other split ways like event types - also possible. But generally - for open table MySQL can spend same or more time than for search single event from VERY big table 

I think - there are no one "only right" answer All depend from Your business model. Normal practice for the data which we must persisted store - use isActive and/or DeletedDate columns. In combination with DateCreated and LastModifed - it give You all information and flexibility. (including and Login table - it just not active) But for some situation - real delete of data also accepted solution. 

return as many rows as many rows child table has with same id and not removed by GROUP BY You need run: 

Unfortunately, MATCH() AGAINST() not understand SELECT with WHERE condition inside AGAINST() - without WHERE it work, with WHERE - not. It make impossible using direct JOIN command from 2 tables in single and clean query. For resolve this we can use stored procedure, which open cursor from table Company_name and populate table Company_text with result 

Step#1 - stored procedure, which will run as many queries as You need - $URL$ Step#2 - Native MySQL Events - which will call this stored procedure - $URL$ 

will use indexes add: in case of big column - You of course could try to realise auto-complete by whole words - there You can create full text index, but not sure - how good it will work, because with fulltext always a lot of variants at the begin of search phrase, and variants how suggested text will be relevant to expected result 

unfortunately it is impossible, maximum possible restore structure InnoDB data stored in *.ibd files if set innodb_file_per_table=1 or in idbata1 if it 0. for success repair even when innodb_file_per_table = 1, ibdata1 need as well. if You have only from file = no chances restore data at all 

You can use any other filter for warrant single result from subquery Also - wrong loop, in this case (for each UserID) define cursor as 

SELECT * always not best practice, but SELECT * without any a additional WHERE or LIMIT it bad at all any queries have several parts: 

ApexSQL Search - $URL$ free tools, search database object (for this tasks) - search code for columns name 

tyson and j_text - it name of table and json column in my example first form allow return other columns, not only coordinates in distinct query 

illegal for this type of code. Just think what would be if 2nd session start insert at the same time? (100% realistic situation in any multi user environment) For prevent collisions - this is not allowed. 

It is not a strange - in most of cases it just indicate, some of queries do not proper use indexes. Possible reasons: 

You can search some other. Unfortunately at least my experience show - this products far from "ready to use" state. But this is possible direction for feature research. 

Not depending from database - transaction will wait until trigger(s) finish work. it is fundamentally - at any moment during this process trigger can return error and cancel transaction. 

correctly select other columns corrected for result, if table have more than 1 row with same MAX(column3) - query return 1 row for each MAX(column3) result. Example data: 

In case when You do not want use MySQL bud scheduler, You always can call this stored procedure from PHP, cron, any other SQL script and etc 

born scan of all column and convert it to archer, same if no indexes - it start scan all table and sort at the same time 

than after this look at loading again high cpu loading normally come from a lot of sorting operations