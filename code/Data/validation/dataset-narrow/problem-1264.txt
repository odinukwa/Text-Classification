I've never heard of this problem before, but it's clearly NP-hard. There's a simple polynomial time reduction from subset-sum to this problem. All you have to do is take an instance of subset-sum such as: {x1, x2, x3, ... , x_n} ...and map it to this instance of the problem you describe: S1 = {x1, x2, x3, ..., x_n}; S2 = {0}. (I initially misread your question and thought that you were talking about a decision problem; since that was not what you meant, I cannot say anything beyond whether or not it is NP-hard except that it is EXPTIME. It is likely not NP; see the comments below.) Either way, I don't think you are going to find any good solution or approximation algorithms. 

Another poster alluded to this (by referring to Chaitin), but you can use the Berry paradox to prove that the halting problem is undecidable. Here is a brief sketch of the proof: Let HALT be a machine that decides if any machine M halts on input I. We will demonstrate that HALT itself fails to halt on a particular input, which shows that it is unable to decide the language. Consider the following function f: f(M, n) = a, where a is the smallest positive integer not computable by machine M on any input I with |I| < n Assuming that HALT is a computable function, f is also a computable function; simply simulate HALT(M,I) for every machine M and input string I with the length of I less than n. If the simulation halts, then simulate M(I) and record what the output is, and find the smallest output a that is not outputted by any of the M,n pairs. Now, we show that f is not computable: consider f(f, 10000000*|f|+10000000). Whatever it outputs, it ought to be a (positive) integer that is not computable by the machine computing f on input I with length less than that given...and yet we've just outputted such an integer with f and a much briefer input. Thus, f is not computable, and so our assumption that HALT was computable is false. I do not believe this proof makes any use of diagonalization. 

It's well-known that there are tons of amateurs--myself included--who are interested in the P vs. NP problem. There are also many amatuers--myself still included--who have made attempts to resolve the problem. One problem that I think the TCS community suffers from is a relatively high interested-amateur-to-expert ratio; this leads to experts being inundated with proofs that P != NP, and I've read that they are frustrated and overwhelmed, quite understandably, by this situation. Oded Goldreich has written on this issue, and indicated his own refusal to check proofs. At the same time, speaking from the point of view of an amateur, I can assert that there are few things more frustrating for non-expert-level TCS enthusiasts of any level of ability than generating a proof that just seems right, but lacking both the ability to find the error in the proof yourself and the ability to talk to anyone who can spot errors in your proof. Recently, R. J. Lipton wrote on the problem of amateurs who try to get taken seriously. I have a proposal for resolving this problem, and my question is whether or not others think it reasonable, or if there are problems with it. I think experts should charge a significant but reasonable sum of money (say, 200 - 300 USD) in exchange for agreeing to read proofs in detail and find specific errors in them. This would accomplish three things: 

My only motivation for asking this question is long-standing curiosity, but I am interested in seeing a proof (or disproof) that the Cook-Levin theorem relativizes. If you have a proof that the theorem does indeed relativize, please supply an explanation of how relativization impacts the proposition, i.e., what does the relativized version of the statement look like when an oracle A is added? Also, please explain what happens to the proposition when a TQBF oracle is added to the computation model. 

I'm not a logic expert, but I believe the answer is no. If the Turing machine halts, and the system is strong enough, you ought to be able to write out the full computation history of the Turing machine on its input. When one verifies that the result of the computation is a terminating sequence of transitions, one can see that the machine halts. Regardless of how you formalize Turing machines in your theory, you ought to be able to show in any reasonable theory that a machine that halts does in fact halt. By way of analogy, think of trying to prove that a finite sum is equal to what it is equal to; e.g., prove that 5+2+3+19+7+6=42, or 5+5+5=15. Just as this is always possible as long as the number of steps is finite, so too is proving the result of a finite computation. Just as an additional obvious point--even if your theory is inconsistent, you can still show that the machine halts, actually even if it doesn't, since you can prove any wff in an inconsistent theory, regardless of whether or not it is actually true. 

Assume that we have a fixed UTM and indexing of Turing machines. Then suppose that you have, in fact, found the true "shortest Turing machine" that decides an NP-complete problem. The issue is that there may be a shorter Turing machine that fails to halt on all inputs. This shorter Turing machine cannot be found algorithmically to not decide an NP-complete problem, due to the undecidability of the halting problem. So, as a general statement, we can say the following: Given as input a class of problems X, there is no Turing machine that finds the shortest Turing machine that decides any member of X. This is somewhat related to Kolmogorov complexity; see, in particular, the proof that Kolmogorov complexity is undecidable (it uses the Berry paradox). 

In this paper, it is claimed that computing pure-strategy Nash equilibria of games in standard normal form is NP-complete. This confuses me, because I do not understand why it is hard to guess the right strategy profile (the authors call this the "global strategy") for a game. Surely if the Nash equilibrium of a game is written out in normal form, where every strategy profile is assigned its own cell, then it should take only linear time to run through every possible strategy profile that is a candidate for being a Nash equilibrium. I do not understand what I am missing; please explain why Nash equilibrium computation is hard for standard normal form games. Thank you. 

I strongly suggest Sipser's "Introduction to the Theory of Computation," particularly because it covers at least one of the main barriers to resolving P vs. NP, namely relativization. It contains a very clear proof of the Baker-Gill-Solovay result. I am not sure if it contains anything on the Razborov-Rudich results, but it's a fantastic, very clear, and easy to read introductory resource for learning not only about P vs. NP but for many other related topics in complexity theory as well...which is significant because if your interest is in trying to resolve the problem, you'll want to have some background in the field and ideas for where to start. 

This question is something I've wondered about for a while. When people describe the P vs. NP problem, they often compare the class NP to creativity. They note that composing a Mozart-quality symphony (analogous to an NP task) seems much harder than verifying that an already-composed symphony is Mozart-quality (which is analogous to a P task). But is NP really the "creativity class?" Aren't there plenty of other candidates? There's an old saying: "A poem is never finished, only abandoned." I'm no poet, but to me, this is reminiscent of the idea of something for which there is no definite right answer that can be verified quickly...it reminds me more of coNP and problems such as TAUTOLOGY than NP or SAT. I guess what I'm getting at is that it's easy to verify when a poem is "wrong" and needs to be improved, but difficult to verify when a poem is "correct" or "finished." Indeed, NP reminds me more of logic and left-brained thinking than creativity. Proofs, engineering problems, Sudoku puzzles, and other stereotypically "left-brained problems" are more NP and easy to verify from a quality standpoint than than poetry or music. So, my question is: Which complexity class most precisely captures the totality of what human beings can accomplish with their minds? I've always wondered idly (and without any scientific evidence to support my speculation) if perhaps the left-brain isn't an approximate SAT-solver, and the right-brain isn't an approximate TAUTOLOGY-solver. Perhaps the mind is set up to solve PH problems...or perhaps it can even solve PSPACE problems. I've offered my thoughts above; I'm curious as to whether anyone can offer any better insights into this. To state my question succinctly: I am asking which complexity class should be associated with what the human mind can accomplish, and for evidence or an argument supporting your viewpoint. Or, if my qusetion is ill-posed and it doesn't make sense to compare humans and complexity classes, why is this the case? Thanks. Update: I've left everything but the title intact above, but here's the question that I really meant to ask: Which complexity class is associated with what the human mind can accomplish quickly? What is "polynomial human time," if you will? Obviously, a human can simulate a Turing machine given infinite time and resources. I suspect that the answer is either PH or PSPACE, but I can't really articulate an intelligent, coherent argument for why this is the case. Note also: I am mainly interested in what humans can approximate or "do most of the time." Obviously, no human can solve hard instances of SAT. If the mind is an approximate X-solver, and X is complete for class C, that's important. 

Amateurs would have a clear way to get their proofs evaluated and taken seriously. Experts would be compensated for their time and energy expended. There would be a significantly high cost imposed on proof-checking that the number of proofs that amateurs submit would go down dramatically. 

I think the P vs. NP problem could be explained very gently in terms of Sudoku. I'm assuming the ten-year-old in question is familiar with Sudoku. I will try to favor simplicity over rigor in my explanation. Here is my attempt to explain P = NP to a hypothetical ten-year-old: 

Goldbach's conjecture states that every even number greater than 2 can be expressed as the sum of 2 primes. I'm interested in this function problem: Given an even natural number n greater than 2, find a natural number p such that p and n - p are both prime. (Inputs that are not even natural numbers may be mapped to 0.) If we assume that Goldbach's conjecture is true, this function f(n) = p is a member of TFNP. The reason for this is that we are guaranteed by our assumption that Goldbach's conjecture is true that there exists an answer to the problem, and we can easily guess this answer (it is no greater in size than n), and then verify its accuracy in linear time. Can we do any better than TFNP here? Could there be some algorithm in PPAD that computes this, or could there even be an FP algorithm? 

The Baker-Gill-Solovay result showed that the P = NP question does not relativize, in the sense that no relativizing proof (insensitive to the presence of an oracle) can possibly settle the P = NP question. My question is: Is there a similar result for the question, "Does there exist a PH-complete problem?" An answer in the negative to this question would imply P != NP; an answer in the affirmative would be unlikely but interesting because it would mean that PH collapses to some level. I'm not sure, but I suspect that a TQBF oracle would lead PH to be equal to PSPACE, and thus to have a complete problem. In addition to being uncertain regarding this, I am curious as to whether or not there is an oracle relative to which PH provably does not have a complete problem. -Philip 

(I assume this question will eventually get migrated to CS.SE, but I am posting my answer to it here on cstheory for now.) Technically, one doesn't usually think of relativization as an "operator" or "function"; however, I don't see a reason why you couldn't take a statement and map the statement to a relativized version of it. The trick is that, as others have said, relativization isn't really defined over a complexity class; instead, it is defined on the computation model you are using. Further, what relativizes is the statement, not the classes. (The notation is a little misleading.) An example of this is that I could theoretically say that a statement relativizes (or, less likely, doesn't relativize) even if it doesn't refer to a Turing machine at all. E.g., I could say (truthfully), "1 + 1 = 2" relativizes, because relative to every oracle that could be added to the definition of my universal Turing machine, 1 + 1 = 2 would remain true. So the short answer is: Yes, it's well-defined, but not on classes. 

I will assume that when you say that a natural number n is "very small," that you mean that for some constant j, n < j. Given n and a promise that its prime factors p,q, are both within j of 2^b for some natural number b and some natural constant j, it's not hard to construct a brute-force algorithm that checks every possible value of p and q in time polynomial in the size of n. Here's a quick example: Let j = 20, and let n = 1018081. We are guaranteed that the prime factors of n are within 20 of a power of 2. So we need only consider every m s.t. m +/- 20 <= (2^b) for some b. These values include: 

Because we know that 1024 * 1024 > 1018081, it's clear that we don't need to go beyond 1024 in our list. This is because we are limited to only two prime factors close to a power of 2. It ought to be clear that given an integer n, the number of values that need to be tested as prime factors of n is roughly: (2 j + 1)*(ceil(ln(n)/ln(2))). This means that the "naive" algorithm is good enough in this case. (Incidentally, the prime factors of 1018081 are 1009 and 1009, so you might want to try a harder example if you decide to code it.) 

I have a polynomial time best-response function that has the same properties as a game-theory game (convexity, compactness, set-valued). I don't know that much topology, but my understanding is that the Kakutani fixed-point theorem confirms that this function has a fixed point, i.e., there exists a strategy profile whose best response is itself. The "game" I have is not really a game, in the sense that there are no payoffs; there is only a best-response function. My question is: Given such a function, can one find a fixed-point quickly? Ordinarily, there are polytime approximation algorithms for this task (finding a Nash equilibrium), but here I do not have the game in normal form, or an easy way to put it in normal form given the lack of payoffs specified. Edit: More specific information: Given a fixed input, find a polytime machine bounded by n^8+8 s.t. the machine outputs its description on the input. (There are ways of making this convex by allowing mixing over the edges of the machine.) 

Is it possible to demonstrate that a sentence must be formally independent based on the fact that it is non-relativizing? In other words, are there examples of sentences in computability/complexity theory where it can be demonstrated both a) that all proofs that resolve the question of whether or not two classes are equal must relativize, and b) that there are no relativizing proofs that can be used in such a resolution? I think that results satisfying part b would be easier to come by. Another way to ask this question is: Has there ever been a sentence in computability or complexity theory where it can be demonstrated that the equality or inequality must be established through the use of (and only through the use of) relativizing techniques? An example of this would be interesting to me. Thanks; an answer to either version of this question would be very interesting to me. -Philip 

I can't think of any way there could be an easy, completely elegant/satisfying answer to this question, particularly because the ending payoff is so hard to compute; however, my thoughts are too lengthy to post as a comment. The best idea I have is this: In the case of chess, try approximating the probability that White will win based on White's material advantage (i.e., extra pawns, knights, etc.) for a given position by randomly selecting positions with that exact amount-of-material configuration. Perhaps in the case of "all-rooks chess," we could say, "How likely is White to win with 8 rooks to Black's 17 rooks?" Perhaps this probability is 4%; to calculate it, we would have to examine (say) 1000 different randomly generated chess positions that have 8 white rooks and 17 black rooks, and then look ahead (say) 10 moves deep in every case, and see what the new material configuration is. Then, take the expected odds based on the material configuration at the end, and force each side to select the path that involves each side taking the best possible option at every turn, given the expected odds at the end. Of course, it would be necessary to find the material configuration for every relevant possibility (M,N) of M white rooks to N black rooks...presumably starting at the lowest ordered pair (M=1,N=1) and working up from there. For the original position, don't just go with the statistic you get (i.e., if the original position has (M=6,N=7) rooks, don't just assume that White has a 25% chance of winning because that's the expected odds of victory for (6,7)); instead, because you can be more precise, look 10 moves deep as usual with just this one position and find every possible ending position. Then, find the right path (that involves optimal play by both sides) to a 10-moves-deep-configuration, and select this path's expected odds as the expected odds of the original position. I think this process can be done in polynomial time. Looking k moves deep for fixed k in chess is polynomial in the size of the board, and the total number of white and black rooks is expressed in unary (in a sense) because that number must be smaller than the size of the board. If this sounds complicated and hard to explain, that's because it is. A more succinct summary of what I'm describing is: Use recursion and basic statistics to calculate the odds of victory for white given M white rooks and N black rooks on the board. Then use these values to look k moves deep and ascertain the odds that White will win in the original position. Final comment: I think this problem is also interesting for non-EXPTIME-complete games, such as tic-tac-toe, which according to Wikipedia is PSPACE-complete. Further, I believe a process like the one I described above could also be useful there too, although obviously it would be impossible to have a "material" advantage in tic-tac-toe; there would have to be some other basis for judging the superiority of X's or O's position.