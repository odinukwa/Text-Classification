Generative Adverserial Networks can be tolerant of this type of perturbation. In fact, if you look at figure 8 of the DCGAN paper, the authors demonstrate how the GAN learned to encode perspective directly such that the authors are able to use the GAN to rotate the camera on an image by applying a "turn vector". 

First and foremost: I would strongly advise against modifying your original data file. It introduces risk that your workflow will become unrepeatable. To persist your results, I recommend doing something like this: 

Identify all points that are connected to more than two lines. We know these must be vertex points. For for all the points that are attached to only one line, identify their nearest neighbor and draw a line between them (this would connect that hole on the right wall of your house). All points should now be either labeled as a vertex or touching only two lines. For each point touching two lines, calculate the angle between the two lines. Find the min and max of these angles. Use these as prototypes to classify angles into two proximity clusters using agglomerative clustering. Alternatively, pick a fixed angle value as your threshold (maybe around 120 degrees) and use that as the decision boundary for this classification. The angles that clustered with the min correspond to the missing vertices. Infer the vertex-vertex connectivity graph through the non-vertex points.(i.e. find all paths between pairs of vertices that do not pass through a third vertex). Draw the graph. Find the cycle's in the graph and color each one separately. 

I'm not super familiar with PLS/NIPALS, but I think you need to construct it yourself. Multiply the scores by the loadings and subtract the resulting matrix from the data to get the residual matrix. See here for a more detailed discussion: $URL$ 

I have built a model that accomplishes my goal. I am satisfied that my modeling approach is sound and repeatable. If I'm working on a prediction task, I also want to be sure that my model generalizes well to out-of-sample data. I have evaluated the projected impact of utilizing my model and am reasonably confident that the benefits of applying it justify the time and effort I am putting in building it. I have a clear path forward to implementation. I have a technical plan for making my results actionable, and know whose support I need to make it happen. I am confident that I can communicate my results in a way that will convince non-technical stakeholders that my results are real and assuage their concerns. 

Matrix multiplication is just a simplified notation for a particular set of addition and multiplication operations. You can absolutely represent a neural network without invoking matrix notation, it'll just be really tedious (and it will run slower). Start with a single perceptron and build your way up from there. 

With 5-fold cross validation, in each fold you're reducing your training dataset to 64 observations and evaluating against 16 observations. Assuming your data is balanced and you're stratifying folds, you're only giving your models 32 observations from each class to learn from, and misclassification of a single test set observation results in a 6.25% point change in accuracy for that fold. Even if it's classified correctly in the other folds, that single misclassification will still have a 1.25% point change on the across-folds average. Yikes. It should be no surprise your models aren't performing well: not only do they not have much data to train on, but your evaluation methodology is extremely unforgiving. So yeah, your data is pretty darn small. Off the top of my head, I can think of a handful of general strategies you can potentially use to address this: 

If you use this code for research, I believe this is the citation you should reference (in addition to the UNC webpage that hosted the code): 

How did you split your training and test sets? My guess is that the minority class is under-represented in your test set. Consider the case where the minority class has frequencies of 25% and 5% in the training and test sets, respectively. If my model predicts everything is the majority class, the recall for the minority class is 0% on both sets but we get 75% training accuracy and 95% validation accuracy. Now let's say we train the model a bit more and achieve 50% recall while still correctly labeling all of the majority class samples: now we're up to 87% training and 97% accuracy. Because the training and test sets exhibit significantly different class frequencies, the test set will basically always have a higher accuracy than the training set (up to the point where you overfit, at which point the validation accuracy could be be lower than training accuracy but not necessarily). The solution here is to either use stratified sampling to make sure your classes have equal representation between your different datasets, or to monitor other evaluation metrics like precision and recall. Plotting the taining and validation accuracies could also be helpful, since it should be visually pretty obvious if your model starts to overfit, regardless of class imbalance between your training and test sets. 

The closest I can think of is research into "adversarial" transformations of data (usually images) that cause classifiers (usually CNNs) to fail. Here's a blog post which discusses a bit about how these transformations are constructed, some approaches that have been tried to address them, and what some of the consequences of their existence might be. You might also be interested to read about the No Free Lunch Theorem. 

Rather than 0 use 0.1 or some other small non-zero value. Use the true y as an indicator function, multiply your class probability by your reward component, and sum terms for all possibilities to get a usable loss, like this: $Loss(y, y_p) = (1-y)(1)(y_p) + (y)(-0.7)(y_p) + (0.1)(1-y_p)$ Where $y_p =P(\hat{y}=1|X)$ 

If the input features have the same sign (x>0 & y>0 or x<0 & y<0), it's the positive class, else it's the negative class. But either feature in isolation is completely useless and uncorrelated with the target. Additionally, modern models like deep neural networks are effectively capable of "learning their own features", i.e. constructing extremely complex features by developing abstractions from the raw inputs. The "final" features learned by such a model will likely be correlated with the target, but the input features need not be. For example, if you consider the imagenet task (classifying a photo as a member of one of 10,000 classes), I'd be very surprised to learn that there's any correlation between the values of specific pixels and any target class. That's just not how we interpret photos. The value of the pixel in position [25, 10] should not have any correlation with whether or not the picture is a photo of a dog. But, if we think of the entire network before the output layer as a feature engineering system (such that the "classifier" is just the output layer), then the "features" provided by the penultimate layer (the inputs to "The Classifier") probably have some correlation with the target. TL;DR: If a feature is correlated with the target, it probably contains information about the target that will be useful for modeling. But that does not mean uncorrelated features are useless. Reporting correlation when it's there is a simple way to demonstrate that there's a signal in that variable. But lack of correlation doesn't necessarily mean you should throw those features away. In fact, correlation doesn't even mean you should necessarily use that feature either: you can have multiple features correlated with the target that are highly correlated with each other, in which case you would probably only want one or a handful of that group in your model. 

The appropriate method to evaluate whether you have modeled a real signal or noise is completely dependent on the question you are asking and the modeling approach you've used to address it. Many very thick books have been written on this topic, often constraining their attention to one problem domain and/or type of model. The complexities associated with model evaluation are a big component of why data scientists generally have graduate degrees. Which brings us to the second part of your question: 

If you're seeing columns named "TRUE" and "FALSE" in the matrix returned by , my best guess is that those are the names of columns in your training data. 

Layout algorithms in tools like networkx, igraph, and gephi will associate coordinates with your nodes which you should be able to access fairly easily. Once you have those coordinates, you just need to plot your pie-charts on top of the relevant node location. Alternative, these tools also support using external images as node markers, so instead of building the plots in the same script you could build the pie charts separately, save them to disk, and then associate them with nodes when you draw the graph. I've never seen an "out-of-the-box" solution for this specific kind of graphic, but it shouldn't be too hard to do this yourself. You just need to figure out how to access the layout coordinates. If you clarify what your preferred analytic environment is and/or graph analysis tool, I can give you more specific advice. 

The normal equations are designed such that each coefficient in the model has an input of some kind it's being multiplied against. The column of ones is the "input" to the intercept term. 

Searching "Christina Frost UNC" led me to this page which contains a collection of graph visualization tools for matlab. The one you are looking for is at the bottom: . The site is super slow, but it eventually shared the code with me. Here it is for posterity in case the site crashes: 

You need to convert the categorical features into numeric attributes. A common approach is to use one-hot encoding, but that's definitely not the only option. If you have a variable with a high number of categorical levels, you should consider combining levels or using the hashing trick. Sklearn comes equipped with several approaches (check the "see also" section): $URL$ If you're not committed to sklearn, the h2o random forest implementation handles categorical features directly: $URL$ 

Use more of your data for training. Instead of 5-fold CV, try leave-one-out or bootstrap. Generate fake data. This probably won't get you very far, but it's at least an option. Check out the SMOTE algorithm. Transfer learning. Depending on what you're doing, it might be possible to leverage a pre-trained model and then tweak it slightly to suit your needs. If this is an option it can be extremely powerful, but chances are this won't be something you can reasonably pursue. Anyway, here's an article demonstrating this on another small data medical example: a pre-trained general purpose image classifier was trained to detect cancer from just 600 images! $URL$ Go Bayesian. Bayesian methods allow you to incorporate outside information as a "prior belief". If you have subject matter expertise (or better yet, citations) that you can use to set your expectations for values of the model parameters or hyperparameters, bayesian methods will allow you to incorporate that information explicitly, and this can in turn help your model find good parameters faster since it doesn't need to learn everything about your problem directly from the little data available to it. If you're not careful, this can result in you giving yourself the model you want to have, in which case your evaluation metrics may not be as informative as you think they are. Here there be dragons: this approach is powerful, but it can be hard to do correctly, and playing with the prior is basically an invitation for people to be skeptical of your methods even if what you're doing is sound. Get more data. I'm guessing this isn't an option or you would have done so already. But if there's a chance it's out there: go find it. Try poking around the literature associated with this condition, maybe you'll get lucky and find a public dataset. If you're feeling bold, you could try emailing other researchers and just ask them politely if you can use their data. 

The idea behind cross validation is to understand the performance of some measure of your model's performance on unseen data. This can be applied to loads of different statistics, not just ones relevant to classification like accuracy. Common measures for holdout performance of regression models include MSE and MAPE, either (or both) of which can be cross-validated over. 

I think you're misunderstanding what "weight sharing" means here. A convolutional layer is generally comprised of many "filters", which are usually 2x2 or 3x3. These filters are applied in a "sliding window" across the entire layer's input. The "weight sharing" is using fixed weights for this filter across the entire input. It does not mean that all of the filters are equivalent. To be concrete, let's imagine a 2x2 filter $F$ striding a 3x3 input $X$ with padding, so the filter gets applied 4 times. Let's denote the unrolled filter $\beta$. $$X = \begin{bmatrix} x_{11} & x_{12} & x_{13} \\ x_{21} & x_{22} & x_{23} \\ x_{31} & x_{32} & x_{33} \end{bmatrix}$$ $$F = \begin{bmatrix} w_{11} & w_{12} \\ w_{21} & w_{22} \end{bmatrix}$$ $$\beta= [w_{11}, w_{12}, w_{21}, w_{22} ]$$ $$F*X = \begin{bmatrix} \beta \cdot [x_{11}, x_{12}, x_{21}, x_{22}] & \beta \cdot [x_{12}, x_{13}, x_{22}, x_{23}] \\ \beta \cdot [x_{21}, x_{22}, x_{31}, x_{32}] & \beta \cdot [x_{22}, x_{23}, x_{32}, x_{33}] \end{bmatrix} $$ "Weight sharing" means when we apply this 2x2 filter to our 3x3 input, we reuse the same four weights given by the filter across the entire input. The alternative would be each filter application having its own set of inputs (which would really be a separate filter for each region of the image), giving a total of 16 weights, or a dense layer with 4 nodes giving 36 weights. Sharing weights in this way significantly reduces the number of weights we have to learn, making it easier to learn very deep architectures, and additionally allows us to learn features that are agnostic to what region of the input is being considered. EDIT: To further motivate this, here's an animation of a 3x3 filter applied to a 5x5 input 

Hyperparameter optimization follows the same rules as model selection. Each set of hyperparameters effectively represents a different model you are considering, so the data you use to fit the model with some set of hyperparameters needs to be different from the data you use to evaluate which set of hyperparameters you want to ultimately use. A common approach for evaluating hyperparameters is nested cross-validation. This basically means that you need to treat hyperparameter selection as part of your model training process, and when you evaluate your model you evaluate the entire process front to back, i.e. treating hyperparameter tuning as a component of model training with respect to cross-validating over your training process. There's an excellent discussion of this in section 7.10.2 ("The Wrong and Right Way to Do Cross-validation") of Elements of Statistical Learning, which you can read online and/or download for free. The general idea is that if you're not careful, you can actually overfit to your evaluation data. Play with this demo to see for yourself. But yes, your intuition is correct. Hyperparameter tuning is often very computationally expensive. One way people sometimes try to minimize this cost is by limiting the search space of feasible parameters to a small discrete set, e.g. grid search. Another approach is to use guassian processes or KDEs to approximate the cost surface in parameter space. You can even use a multi-armed bandit approach. Frankly, you can really use pretty much any non-linear optimization technique for hyperparameter tuning, as long as you follow the appropriate cross-validation rules. The trick is figuring out bang-for-your-buck in terms of how much time/effort/compute you are willing to expend exploring parameters vs. the potential improvement to your model. Additionally, there are concerns about the limits of repeated cross-validation/holdout evaluations, but that's a whole other rabbit hole.