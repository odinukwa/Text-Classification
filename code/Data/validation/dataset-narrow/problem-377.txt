RMAN can not connect to the recovery catalog, because the username or password is incorrect. Fix the username and password in the catalog database and the script. 

Use RMAN catalog, because without a catalog, backups taken on one site do not appear in the controlfile of the database on the other site. You need a centralized catalog for backups in a Data Guard configuration. Use role-based services, and TNS entries with connect-time failover, using these services. Use these TNS entries for logging in to the database when performing backups. This way you always log in to the primary/standby database, and you do not need to check for the role of the database. 

A possible bug for not displaying the constraint name with bulk DML and clause: Bug 7210333 : CONSTRAINT NAME IN SQLERRM IS NOT OUTPUT WHEN SAVE EXCEPTIONS IS USED To catch the records causing the error, you could use DML error logging as well: 

You try to connect with , which normally connects locally, and you still get errors related to TNS and listener. Actually, there is a not well-known environment variable, called , and if you set that, SQL*Plus will automatically try to connect using the value of the variable as a TNS entry. So if you use , instead of this: 

Using this coluumn, we can get the desired value with the function if we inspect rows up to the current row. The trick is, that the function supports the option, so when we encounter a value, we can easily ignore all the rows with the same and fall back to the previous value: 

Why do you expect B to be updated, when you inserted a row with a different id? Actually, your does not insert anything into . The above works fine after fixing your trigger and sample data: 

First you need to convert it to TIMESTAMP datatype (because DATE type does not handle fractions of second). Then you can truncate it to date for example: 

Using allows you to open the instance (as long as the affected datafile is not critical) without it trying to access that file. Since you can not restore and recover the datafile without backup, you can drop the tablespace (I am just guessing the name from the datafile): 

Basically, if the result of this query is greater than 0, than the Data Guard is in use as per the procedure: 

You will have your database as it existed when you took the cold backup. If you need a state newer than that, only then you need to recover further the restored database. 

Here you can see the name of these objects in the column. Another way to find them is querying the view: 

Choose , then later you can upgrade your databases one by one with DBUA (Database Upgrade Assistant) from the new Oracle Home, or by performing a manual upgrade. 

Ok, that error ( is related to corruption. Normally a DBA would go to My Oracle Support, follow the notes (for example ORA-600 [6006] ORA-600 [6856] During Startup Instance, Followed by Termination by SMON (Doc ID 549000.1)), identify the affected objects then try to repair them, recover corruption, or restore a valid backup. If you can not do that, and this is a sandbox/development database with no important data, in my opinion the easiest and fastest solution for you would be recreating your database with DBCA (Database Configuration Assistant). 

Standard index build locks the table for the whole index build process, and DML statements on the table will be blocked. You may call that a complication. Online index build is a feature of Enterprise Edition. This method locks the table at the beginning, and at the end of the index build process. In between, DML operations on the table can run as usual. Creating an Index Online 

Anything else in addition to ? When granting , you implicitly "get" on the (I replaced EABINTEG with U2 in your query): 

Physical Database Limits Logical Database Limits The maximum number of users (schemas): 2^31 = 2147483648 The official answer to your other questions: unlimited. The practical answer is: as many as your hardware/operating system/design can handle. 

Users can drop objects owned by them, no extra privilege needed for that. There is such privilege as or . The privileges and allow a privileged user to and objects owned by other users. 

Your subquery is not correlated to the outer query, so it multiplies the rows by the number of rows it returns. Also, in my opinion, using the same table names multiple times without aliases at multiple query levels is bad practice, as it may cause some unexpected "surprises". Your query should be something like this: 

Then this is not the method for doing this. In order to use the ALTER DATABASE RENAME FILE, the file have to already exist at the new location. Instead of this, you can (and should) handle this in RMAN: 

That timeout can be configured with the SQLNET.OUTBOUND_CONNECT_TIMEOUT parameter. For example, place the below in sqlnet.ora, which results in outbound connections timing out after 10 seconds without answer: 

That is not how you connect to another database through database links. Database links require SQL*Net connectivity, not HTTP(S). First you need to enable access, then you can connect using the address of the compute node and the service name for your database. $URL$ 

So far so good, just the sample data. Now do an UNPIVOT, then PARTITION BY val, ORDER BY the C values, then PIVOT back: 

You can not do that in EM Express. Oracle Database 12c: EM Database Express I'm trying to perform a task I used to do in DB Control. Where is it in EM Express? 

Data inserted after will be automatically partitioned and you can leave your old data as it is, or split into partitions if you want. Partitioning data is easy, the cumbersome part may be dealing with the constraints and indexes that need to be global. You may need to change your PK and Unique constraints (and indexes), as you can not have such indexes locally partitioned without the partitioning key included in them. Also because of dropping the old table and having a new one in its place, you need to sort out the FK constraints pointing to this table. 

2 (+1) possible reasons: 0 - does not audit operations. Use 1 - You perform the above as SYS user. Actions performed by SYS are audited into files, under the location of 2 - AUDIT (Traditional Auditing) 

Or , if it is configured. With enabled, the controlfile will be automatically backed up on backup operations, and on changing the structure of the database (e.g creating a new tablespace, adding a new datafile). In 10g it is performed immediately after changing the structure, starting with 11.2, it is performed a bit later (Controlfile Autobackup Deferral), allowing the execution of multiple structure changes without creating an unnecessary controlfile autobackup for each action. , well, that is where all other backups go, by default. Backups of the datafiles, archivelogs and controlfile and spfile backups if is disabled, or created explicitly. 

Unless you have a special case, an with 1.2 million rows should be completed in seconds or minutes at most. Yes, direct-path insert may significantly reduce execution time. The above hint in has no effect without the necessary prerequisites. Enable Parallel DML Mode 

What? Forget the above. You are missing some basic concept if you write a trigger like this. Why would you want to select a value that you already have? This does the job: 

7 is the id of your file on your screenshot. When all the extents are gone (the above query does not return anything), drop the datafile: 

A. Oracle searches for a init.ora to determine the configuration parameters without a parameterfile, you will get an LRM-00109, ORA-01078 error, and nothing gets started E. oracle starts an instance we could argue what exactly starting an instance means, I have just simply put it here because this is the "exact" message that appears first when starting an instance C. memory for the SGA gets allocated and the background processes that are necessairy get started background processes like LGWR, DBWR, CKPT, SMON, PMON start in NOMOUNT, way before doing anything with control files... F. oracle searches for the control files with the parameter CONTROL_FILES thats the next one when going to MOUNT D. Oracle searches the database files and redo log files now tell me, how could the database perform an instance recovery without knowing where the datafiles and redo logs are B. The proces SMON sometimes executes an instance-recovery finally an instance recovery is performed, and database gets opened 

You can specify the block size at the tablespace level at the time of its creation. Different tablespaces can have different block sizes. A conventional table must have the same block size for all of its segments, even if it is partitioned and partitions are in different tablespaces. 

I hope you see why both versions fail. These work properly only as long as you have dense collections (where index values are contiguous). With sparse collections, both method fail, and you should use a loop: 

This way you can log in without a password because of the remote OS authentication. The problem is, anyone can create an OS user named on any machine, and they will be able to log in without providing a password. SYSDBA will not work with this type of authentication, if you try , you will get . 

You should go for the solution that fulfills your requirements. Both and are configurable, so first you should to configure them, and after that, you can set . The default value (on Oracle Linux 7) of is 4 TB, which is irrelevant for most systems. The default size of is 50% of the physical memory. If you use the default settings, following for sizing can be a viable option, but it may be suboptimal ("Unused memory is wasted memory"). To make things more complicated, includes PGA, which is not shared memory. Also, my personal opinion/practice: whenever I can, I use ASMM (sga_target/sga_max_size + pga_aggreate_target) instead of AMM (memory_target/memory_max_size). Especially since hugepages works with ASMM, but not with AMM. 

Unless you have RAC databases with multiple instances per site, and and are actually SCAN names, this is unnecessary. These parameters are used to configure Transparent Application Failover (session failover between RAC instances). By the way, this is far from a really seamless switchover, all that was achieved by this, was that clients can use a single connection string to connect to the database after a role transition. Client sessions will fail and need to reconnect manually, that could take minutes. Or you need to stop your applications and have planned downtime for this. Having a really seamless switchover requires far more work (like configuring ONS, FAN). More details at: Client Failover Best Practices for Highly Available Oracle Databases Fast Application Notification 

Database SQL Language Reference There is no in Oracle. RDBMS products do not necessarily implement all features in the SQL standards. And to add to the above: Check Constraints 

This is for the 1st query, the database calculates the cost of the transformations, then later decides to use it (did not post everything, as that would be much more than the above). Now for the 2nd query, this is what the optimizer did: 

If you have access to the logfile in redo group C, and you can copy it to the new server, then you will be able to use it for recovering the database on the new server, so you will not lose the data in it. If you no longer have access to the redo logfile (because not only the database, but the server or storage also crashed, and you are unable to start/repair them), and all you have is your backup, then yes, you will lose the data created after the last backup. 

Note that this results another execution plan because of the Distinct Placement Cost-Based Transformation (VW_DTP_... internal view). Another solution is to prevent Simple View Merging with the /*+ no_merge */ as shown earlier. However, on 12c (12.1.0.2.3, Oracle Linux x86-64), view merging works quite well, because the database chooses HASH JOIN SEMI instead of HASH JOIN, and execution time is as fast as the other variants: 

We find the first value () for each row, ignore nulls (), for the current "partition/group" (), for the whole "partition/group" () based on the order of record_level (), and get rid of the duplicate rows with . 

The problem with the query is that you have both in the outer query and the subquery. The subquery uses from its inside, not the from the outer query. Because of this, the subquery always returns data, so will always evaluate to , hence no result. The version works, because the inner query is not a correlated subquery, it does not use from the outside, but the inside. The inner query returns customers who made purchases, without s, because the join to eliminates them. With no s to worry anymore, works as naively expected. If you want to make work, remove from the subquery, so it becomes a correlated subquery: 

It is not mandatory, do not use it if you do not want to. Creating such symlinks just makes it easier to distinguish the ASM disks. You can even set to with the above. 

Just use + to find why fast refresh is not possible on your mview. Fast refresh is possible without a join condition. Create the table for storing explain results: 

Means: get the 1st row by unspecified order, then order the result. Order by has no effect at all in this specific example. They are different. Use the first one, or starting with 12c, you can use the below: 

But you want it in a query. This later version also allows you to grant execute just on this single function to users with lower privileges, instead of the whole DBMS_SYSTEM package: 

As you can see, this is bad, because you have number literals but the type of the columns is . The columns are implicitly converted to number with . Becasue of this, the index can not be used, as the index contains columns and , and not and . A function-based index can be used in such a case, but that is not the right solution here. 2) 

Your output shows hugepages were not configured, but the database is configured to use only large pages (), hence it can not start. Configure the required amount of pages by setting the in , the appropriate limit in , then reboot and start the database. Configuring HugePages on Linux If you do not need hugepages, simply edit the parameter file of the database, and unset the parameter. 

No, that does not work fine for selecting highest entries. Your query grabs the first 5 rows in any order, and orders them by commission, it is a very common mistake with . You need to order by first and after that you can use rownum: 

Sorry, but you are doing this wrong from the very beginning. First of all, you should really read about the concept of the multitenant architechture with container and pluggable databases. Multitenant Architecture The below is just a dirty hack to overcome the rules that were made for a reason: 

You move tables, not indexes. You rebuild indexes. However, when you rebuild indexes, you can specify a tablespace, not a datafile, so no difference in that aspect. Since you have to rebuild the indexes anyway, instead of rebuilding them immediately (and not having control in which datafile the new extents will be placed), you can just simply make the indexes first. That drops their segments, and when the second datafile becomes empty (you can check this in ), drop the second datafile, and finally rebuild the indexes. First, make the indexes unusable: 

The platform ID identifies the platform of the datafile, and that is all what matters. You can match the platform ID to platform name by using the information in the V$TRANSPORTABLE_PLATFORM view. For example platform 13 is x86 Linux 64-bit: 

First of all, DBA_SYNONYMS.SYNONYM_NAME is the name of the synonym. DBA_SYNONYMS.TABLE_NAME is the name of the object the synonym points to. In your query, it is not the synonym that is , but the object that it points to. It is not the synonym that needs to be fixed, but the object.