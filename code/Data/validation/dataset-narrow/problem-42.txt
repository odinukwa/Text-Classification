To make an application, OpenGL or not, run for extended time safety-critical applications are mostly built to restart on error, be it a driver bug, your application is broken, or hardware failure. If function during the downtime is required you need redundnacy. In your case maybe start multiple sessions of the same program, or even having dual computers running the same program outputing to the same display. OpenGL is no different from other applications, BUT vendors optimize and test for certain kinds of customers. Your scenario is not common so be suspicious. 

Include a benchmarking phase for of both solutions in your application and then select the winning solution at runtime. This is simple, portable, and future proof. I mean, you do have test for this, right? ;-) I know this is a pretty generic answer to "best practice" for high performance but if you think of it there are thousands of possible target configurations and many vendors to consider. If you need that little extra, go pay your vendor for a driver optimized for your application. 

But I want to avoid the deformation behaviour when stretching, I want to keep margins when stretching my texture or simply cut it in the middle and stretching it. Here is an illustration below. 

I am using this Kawase Blur, to apply blur onto my buffer. But I would like to apply this blur in an uniform way, no matter the distance of the objects from the camera. I guess that I need to use the depth buffer but how should I change the offset according to depth ? 

Huge thanks to @MJP who answered this. The aim is to avoid the simplification made when using tangent space normals. Here is the paper : Blending in detail But only implement equation (4) which gives you this. 

I would like to blur the content of those 4 spheres using the same offset no mater their position. If I apply the same blur on all objects, far object's content will appear more blurry than ones on the foreground and I want to avoid that. I think that the depth make could help but any precision would help me. If I blur the whole image and apply the result on the sphere, the white background will bleed onto the sphere shape and I want to avoid that. I also don't want that the blue (3) and yellow (4) sphere merges with the red (1) and green (2) ones. But I would like that the green and red ones merges. Again This could be done using the depth but if you have more precision about how to do it it would be interesting. 

IÂ´m looking into uses of high amount of samples in multisampling. If I have an RGBA8 framebuffer and render to it using multisample with many samples, say 32, dithering on per-sample level should yield a reasonably accurate 8-bit color from the 32 samples in 4-bit color when resolved. The spec (4.5) seems a bit scary though (glBlitFrameBuffer): 

My interpretation of this is may perform worse than if the index count or implicit range exceed that of the implementation dependent GL_MAX_ELEMENTS_{INDICES|VERTICES}. For platforms where is always superior I would expect to ignore and all together, and the value of GL_MAX_ELEMENTS_{INDICES|VERTICES} to be "infinite". Correct usage according to spec is therefore whenever exceeds GL_MAX_ELEMENTS_{INDICES|VERTICES} and does not. For reference, on my GTX 970 the value is 1048576 for both. 

Depending on your renderer you could make the surfaces more "fuzzy" applying a miniscule offset in multisampling depth using a noise like algorithm. This should result in a sort of blending effect for surfaces in close depth proximity. For what I know setting per-sample depth in fragment shader is quite recent in OpenGL and then only as an extension. OpenGL has the PolygonOffset but requires knowing in advance you are about to render something coplanar. While not feasible for surfaces within a single model it could work nicely for example when rendering a road overlaid on a piece of land. 

I also have another question about the Physically based Refraction part which is not really well explained and I am missing how some values are calculated such as , or . If anyone could provide some explanations about those values. Thanks a lot. 

This gives the standard tiling/stretching and offset behaviour when you tile/stretche a clamped texture as you can see in the image below. First is normal, second is offset and last is stretching. 

This doesn't really answer the question but is a workaround. Beware it's hack time :) Instead of using the and I'll be using the method and a simple to sample the texture. As I am using a sphere mesh, I generate 2D UVs like this. 

I am working on a specific blur effect that implies several behaviours. But before I would like to know what you think about those blurring algorithms according to performances and quality : Kawase blur, Box blur and two pass Gaussian blur. Next are the effects I am currently working on and I would be glad to have your thoughts on those on the proper way to achieve this. Here is a schematic view followed by my questions. 

As a simplification, you can automatically insert header guards for each include in your preprocessing layer, so your processor layer looks like: 

It's harder to control what needs to be included from within the shader (you need a separate system for this.) It means the shader author cannot specify the GLSL , due to the following statement in the GLSL spec: 

There's a bunch of a approaches, but none is perfect. It's possible to share code by using to combine shaders, but this doesn't make it possible to share things like struct declarations or -d constants. It does work for sharing functions. Some people like to use the array of strings passed to as a way to prepend common definitions before your code, but this has some disadvantages: 

A common design is to implement your own mechanism, but this can be tricky since you also need to parse (and evaluate) other preprocessor instructions like in order to properly handle conditional compilation (like header guards.) If you implement your own , you also have some liberties in how you want to implement it: 

When cylinder and frustum intersect the hole will be a view into the internals of the cylinder. The hole is where external of the cylinder occlude frustum and internal of the cylinder does not occlude the frustum. Knowing this you should simply do the following to find the surface (or rather volume) made from the hole. Render frustum without color and set depth accordingly. Set a stencil bit where the cylinder (external) occludes the frustum, without setting new depth. Clear the previously set bit where the internal of the cylinder occludes the frustum, without setting new depth. Alternate between internal and external using CullFace. The stencil mask you now have is the surface where the frustum hole is. Render the frustum (in color) masking it with the stencil. Make sure to use LEQUALS depth test to allow the frustum to render on top of itself. Voila. Note that the depth buffer holds the value of the unmasked frustum so If you need to render something within the hole you would have to render the whole scene again and mask it to only render in the hole unless depth test passes against the frustum. Left as student exercise. 

I am trying to scale and repeat a Cubemap with Latitude-Longitude mapping layout just as you would do with classical UV mapping but without any interesting result. This should be used as a skybox. This comes from the fact that the coordinates are in 3D space and we can't apply this simple formula How would you handle such features : scaling which involves tiling and offsetting. 

As I said it works well but when I scale my shape in the X axis the result is not correct. Here is a illustration of the issue. 

I am trying to achieve a special texture stretching effect in my shader. Here is how I define my uv before sampling my texture, nothing really special. 

It seems that the issue comes from the scaling part of the radius and the smoothness and the length call. But I don't see any way to fix it. If you also have some suggestions about some ways to improve the code I would be glad to hear you. Thanks !