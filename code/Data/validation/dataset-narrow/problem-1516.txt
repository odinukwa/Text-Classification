Continue till you reach the optimum and select the best chromosome. Of course it was the intuitive explanation of this beautiful idea. See more here and if you needed implementation details please drop a comment here. Good Luck. Update: More Details I have an optimization problem which is too complex or large to be solved analytically so I need a heuristic method to approximate the optimal solution. For instance each subset of $28$ ingredients outputs a result $y$ where $y \in [0,100]$ (it's much bigger than your sample problem just to use optimization properly). 

NOTE: If the size of classes are very different you need to come up with some solution for small classes. If it was the case, just drop me a line in comments and we can discuss solutions. Good Luck :) 

I'm not sure if I understood your question! Probably better to plot a scheme at least. But according to what I guess from your question: Q2- You probably need a simple MLP (Multilayer Perceptron)! it's a traditional architecture for Neural Networks where you have $n$ input neurons (here 20-25), one or more hidden layers with several neurons and 3 neurons as output layer. If you use a sigmoid activation function ranged from 0 to 1, the output for each class will be $P(Y=1|X=x)$. Q1- So your question probably is: how many training data you need for learning a model? and to the best of my knowledge the answer is as many as possible! and about the last question, I really could not figure out what you mean. You apparently have a very specific task so I suggest to share more insight for sake of clarification. I hope I could help a little! 

What you are exactly looking for is a modification to the DEDICOM Algorithm (page 4). the DEDICOM itself gives you measure for relation between different components of a directed graph. You just need to be a bit creative to use it for converting a graph into DAG. Read the paper and if further help needed just drop me a comment. 

For different variations of GA (Elitism, Island Model, etc.) see the literature for Evolutionary Algorithms e.g. this one. Hope it helps! 

@Shagun's answer is right actually. I just expand it! There are 2 different approaches to your problem: Graph Approach 

well ... some points first: Logistic Regression is for classification and Linear Regression is for regression tasks. They are conceptually different so be careful what you want to do. Categorical variables can be vectorized using encoding. If there are many categories you may apply a dimensionality reduction before feeding the data to the algorithm. The algorithm is chosen based on a model selection process so, in general, you do not know in advance which one is better. 

According to $P_{mutation}$ choose 1% of survived samples and randomly(again, technically with a predefined probability) change one material in it (technically the samples are usually binary so you flip between $0$ and $1$. Here as you have nominal variables you need to change the variable). Let's say in survived sample $FVBGTZHU$ I randomly choose $V$ and exchange it with $P$ so I get $FPBGTZHU$. Now the new generation is born (keep it as the same size of the original population for better structure). Go back to 2 and continue till you find your answer. 

Please note that the code is abstract so TianX, TrainY,TestX,etc should be properly defined by you. Hints Be careful about what is StopWord. Practically many people (including myself!) made this mistake to remove stop words according to pre-defined lists. That is not right! Stop words are corpus-sensitive so You need to remove stopwords according to information theoretic concepts (to keep it simple you need to know TFIDF kind of ignores your corpus-specific stopwords. If you need more explanation please let me know to update my answer). VotingClassifier is a meta-learning strategy in the family of Ensemble Methods. They take benefit from different classifiers. Try them as they work pretty well in practice. Voting schema simply takes the results of different classifiers and return the output of the one which has higher probability to be right. So kind of democratic approach against dictatorship ;) Hope it helps! 

I mentioned Fake Pattern above so let me say a bit more on it. I think there is a fundamental issue concerning your question: You assume that there is a Right clustering that you got after transformation. Actually there is no right clustering! We do not have fake pattern! If there is a pattern in a feature space, then that is true! i.e. you found an interesting representation of your data. If it does not match with the labels then either the data is very noisy or wrong features have been chosen to represent classes (maybe more reasons. just these two came to my mind now). If there is no label (your case) be sure there is a correlation between features of those cluster members. 

I think this should not be a big issue. Just put zero and try your algorithm with a mock data and see what happens. If didn't work, try to fill missing values by some statistical indicators e.g. mean of the data and see the result. Please write a comment about how it worked. Good luck :) 

But I usually do another oversampling which is on the text (so more intuitive) and is kind of SMOTE. 

Very first input: Please confirm if you are really asking about Graphs as there are tones of misunderstanding between plotting and Graph as a mathematical object. In case you mean visualization and not Graph as a math object please let me know to edit your tags and title. I will continue with my answer anyway as graphs are the right tool for visualization and moreover for analysis of your data even if you touched them accidentally! You can see your problem from Graph Theory point of view in different ways. I assume cases as nodes and changes as edges in simplest way. You may continue developing the idea to more sophisticated structures. Let's assume each case is a node in your graph. Then you can model each bacteria with one graph in which there is no edge between two nodes if the change is zero and an edge if there is, where the weight of edge is either 1 or -1 for different cases. Then you come up with 4 graphs on each of which you may do statistical analysis according to network topology. As a more detailed modeling, you may consider two nodes for each case corresponding to each timestamp and connect them with weights 1 or -1 or not connect them in case the change is zero. To avoid weights you can simply use directed graph in which an edge goes from node of case $X$ in time $t_0$ goes to node of case $X$ in time $t_1$ if the change is one and if it's -1, an edge goes from node of case $X$ in time $t_1$ to node of case $X$ in time $t_0$. The topology of connections also tells a lot in this case. In both cases the visualization of graph might be intuitive might not. If not you can visualize different statistical measures on nodes, edges, degrees, graphs themselves, etc. Hope it helps!