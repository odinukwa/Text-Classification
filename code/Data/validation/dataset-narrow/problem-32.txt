Yes, it's certainly possible to do this kind of thing. First of all, you can typically connect 3 or 4 displays to each GPU, and (with an appropriate motherboard) you can have up to 4 GPUs per machine. That would give you 16 screens. With multiple machines, you could have even more. Rendering to multiple displays from a single GPU is no big deal. You can create a single window that stretches across all the displays, render to it, and it should "just work". Rendering the same scene on multiple GPUs is a bit trickier. One way is to create a separate rendering context on each GPU, and essentially repeat all the rendering—re-upload all buffers and textures to each GPU, and re-submit the draw calls to each one every frame. Another possibility is to use the built-in multi-GPU support in DX12, which allows you to manage several GPUs in one rendering context. 

If you really want to obtain every fragment (shading sample) within the camera frustum, regardless of occlusion, one way to do it would be to disable depth testing, and then use a large SSBO together with an atomic counter to append data into the buffer from the fragment shader. Each fragment could then store a record of its screen position, depth, color, and any other desired information into the buffer to be read out later. The buffer has to be large enough to store every fragment rendered, as it will throw away any extra data once it's full. Using one huge SSBO for the whole screen likely isn't ideal, as there will be huge contention on its counter with every fragment trying to increment it. A better way might be to slice up the screen in tiles with an SSBO+counter per tile. There is a limit of 16 bound SSBOs on all current GPUs, and on AMD GPUs, there is a further limit of 8 atomic counters (source: Sascha Willem's OpenGL Hardware Database). Those limits will determine how many tiles you can create. This will reduce contention and also ensure that even if you run out of memory in one tile, you'll still have valid data in the others. 

Flickering can be a form of temporal aliasing. It's a similar phenomenon to spatial aliasing such as jaggies, but it occurs in time instead of space. For instance, a common cause of image flickering in graphics is when the camera or geometry is in motion, and geometric features fluctuate in pixel size as they move. For example, imagine a railing with thin vertical bars. Depending where a bar appears relative to the pixel grid, it might get rendered as 2 pixels wide, only 1 pixel wide, or it might not appear at all. And in motion, it may rapidly fluctuate between these states, creating a visually objectionable flicker. Another common cause of image flickering is specular surfaces with a bumpy normal map and a high specular power. The specular highlights can flicker in motion, due to their alignment with pixels changing from frame to frame. Antialiasing strategies that address only spatial aliasing will often produce an image that looks good in a static screenshot, but turns into a flickery mess as soon as things start moving. This is one reason why temporal antialiasing has become popular in games in recent years. 

There is a great paper from 2006 on this topic, Filter Importance Sampling. They propose your method 2, study the properties, and come out generally in favor of it. They claim that this method gives smoother rendering results because it weights all samples that contribute to a pixel equally, thereby reducing variance in the final pixel values. This makes some sense, as it's a general maxim in Monte Carlo rendering that importance-sampling will give lower variance than weighted samples. Method 2 also has the advantage of being slightly easier to parallelize because each pixel's computations are independent of all other pixels, while in method 1, sample results are shared across neighboring pixels (and therefore have to be synchronized/communicated somehow when pixels are parallelized across multiple processors). For the same reason, it's easier to do adaptive sampling (more samples in high-variance areas of the image) with method 2 than method 1. In the paper, they also experimented with a Mitchell filter, sampling from abs() of the filter and then weighting each sample with either +1 or −1, like @trichoplax suggested. But this ended up actually increasing the variance and being worse than method 1, so they conclude that method 2 is only usable for positive filters. That being said, the results from this paper may not be universally applicable, and it may be somewhat scene-dependent which sampling method is better. I wrote a blog post investigating this question independently in 2014, using a synthetic "image function" rather than full rendering, and found method 1 to give more visually pleasing results due to smoothing high-contrast edges more nicely. Benedikt Bitterli also commented on that post reporting a similar issue with his renderer (excess high-frequency noise around light sources when using method 2). Beyond that, I found the main difference between the methods was the frequency of the resulting noise: method 2 gives higher-frequency, "pixel-sized" noise, while method 1 gives noise "grains" that are 2-3 pixels across, but the amplitude of noise was similar for both, so which kind of noise looks less bad is probably a matter of personal preference. 

After asking around a bit on Twitter, I found that what you're trying to solve seems to be an instance of the assignment problem. In general, the assignment problem is to take two sets and assign elements of one to the other, in such a way that a cost function is minimized. In your case the two sets would be the stars in each galaxy, and the cost function would be the distances between stars. An optimal solution to the assignment problem would then minimize the total distance between corresponding stars, summed over all the stars in the galaxies. A classic algorithm for solving the assignment problem is called the Hungarian algorithm. If you google around, you'll some explanations of how it works. There's even a Python module implementing this algorithm, called Munkres. A warning: this algorithm runs in $O(n^3)$ time, so it will be rather slow for large point sets. You might want to do some further research on fast algorithms for the assignment problem. It may be that you can find a faster, approximate algorithm if you don't require a perfectly optimal solution. Or maybe there are specialized algorithms for the case of point clouds that can do better. 

The $t$ parameter represents the distance from the ray's origin point to the intersection point, as a multiple of the ray's direction vector. It is positive for intersections "in front" of the origin, negative for those "behind" the origin as judged by the direction vector. In raytracing we always discard any intersections with $t < 0$, because we don't want to render objects behind the camera, and so forth. A ray is only half a line, and only the points $t \geq 0$ are part of the ray. It is not correct to say "if $t<0$ the point will stay in a part of plane that does not contain the polygon". If the polygon is behind the ray origin, then the intersection point may well be inside the polygon. However, we can immediately discard this intersection due to its being located behind the ray origin, so we would not need to perform any further tests of whether the point on the plane is in fact inside the polygon. 

The terms screen, monitor, and display are used more or less interchangeably to refer to the real-world physical device that shows the image you're looking at. (They're not 100% interchangeable: monitor means a discrete display device such as you'd use with a desktop PC. However, a built-in display such as the one in a laptop, tablet, or phone is never called a monitor.) The image plane means the abstract geometric plane that identifies the surface of the display, but extends infinitely in all directions, i.e. isn't bounded by the edges of the display. The viewport or viewing window is the rectangle within this plane that identifies the edges of the display (or more generally, the edges of the image you're rendering, in case your app is running in a window and not fullscreen). The term screen might also be used more colloquially to mean various other things, such as: various coordinate systems on the image plane ("screen space"), the viewport or view frustum ("the object is off-screen"), the back buffer ("render it to the screen"), and probably more. 

An affine transformation doesn't have enough freedom to do what you want. Affine transforms can be constructed to map any triangle to any other triangle, but they can't map any quadrilateral to any other quadrilateral. One way to see this is that the matrix for a 2D affine transform has only 6 free coefficients. That's enough to specify what it does to 3 points, but not 4. For this task you likely want a projective transformation. A 2D projective transformation has enough freedom to map any convex quadrilateral to any other. 

The concept of a point source is an approximation. Physically, light sources are extended objects and emit light from every point on their surface; but when you're far enough away (i.e. the distance to the source is large compared to its size) it's useful to approximate it as a point source. You can get the $1/r^2$ law out of it as follows. Imagine a spherical area light with some radius $r_\text{light}$, and you're looking at it from a distance $r$ away. Then, we can approximate the solid angle that it subtends from your point of view as the area of a circle of radius $r_\text{light}/r$ (just using similar triangles). This area will be $\pi (r_\text{light}/r)^2$, so it's proportional to $1/r^2$. Note that this approximation becomes exact in the limit $r_\text{light}/r \to 0$, i.e. when the light source is very far away or very small. It breaks down if the source is too large or too close. If the source emits a constant radiance from every point on its surface, then when you integrate over solid angle in the rendering equation, you'll get a total irradiance proportional to $1/r^2$. In order to approximate it as a point source in a renderer, we skip the integration and just add an irradiance proportional to $1/r^2$ directly. 

You can detect a matrix that can't be decomposed in TRS form by taking its 3×3 upper-left submatrix, interpreting its columns as vectors, and dotting them together in all combinations (1 with 2, 2 with 3, and 1 with 3). For a TRS matrix, the three dot products should all be zero, i.e. the column vectors should be orthogonal to each other. If any of the dot products are nonzero (beyond some epsilon to account for roundoff error) then the matrix is not decomposable as TRS. This derives from the property that a rotation matrix is an orthogonal matrix, i.e. its column vectors form an orthonormal basis. When you multiply the rotation by a scaling matrix, the column vectors may no longer be unit vectors, but they should still be orthogonal. And the translation part doesn't affect the rotation-scaling part. 

I haven't been able to find such a chart on the web, so I made one here. (Everyone, feel free to add, elaborate, or correct any mistakes. Some of these are just best guesses based on a partial understanding of the API and hardware internals.) API Basics 

The scan-line algorithm (as described on Wikipedia for instance) is concerned with generating the pixels in order, left-to-right and top-to-bottom, with each pixel needing to be touched only once. It was developed in the late 1960s, for devices with no framebuffer memory—so it has to generate each pixel just-in-time as it scans out to the display. The constraint to generate the pixels exactly in order is a strong one. Now, it's possible to imagine a very simple and naive algorithm that doesn't need tables, buckets, or sorting to accomplish this. For example, for each pixel, you might just iterate over all the triangles, test whether the current pixel falls inside it, and keep track of the closest triangle that passes that test. (That's effectively ray tracing without any acceleration structure.) It's pretty clear that this is inefficient as soon as you have more than a handful of triangles. The complicated data structures and stuff that show up in the classic scan-line algorithm are there to optimize this process. For instance, pre-bucketing the edges by Y-coordinate and maintaining an "active edge table" lets you quickly and incrementally identify the edges that affect each scanline as you move down the image. Keeping the edge intersections sorted by X-coordinate allows you to quickly generate the pixels left-to-right within each scanline, and keeping a Z-sorted list of active triangles as you scan left-to-right enables you to do hidden surface removal at the same time.