Let $\mathcal{S}$ be the set of all strongly regular graphs with parameter $(n, k, \lambda, \mu)$. Are there any (interesting) equivalence relations defined on this set? My motivation is to approach isomorphism partition (equivalence relation is being isomorphic) via other coarser partition. 

Input: Any number $n \in \mathbb{Z}^+$ that can be represented in the form of $n = 2^a + b,\ |b|= c $. output: YES if $n$ is prime , else NO . Now, length of binary input is $\log(a) + O(1)$ which otherwise would be order $a$. What would be the complexity of such problem? If we use AKS algorithm in straight forward way we would not get polynomial time algorithm, I think. Can we define new problems by tweaking the standard problems in P to make them harder in this way i.e. by restricting the input in certain way. 

My graphs has following restriction: undirected, connected, simple, unweighted, and maximum degree of any vertex is 8. These are actually molecular graphs. Almost always maximum degree will be 4 but in some cases I have seen it to be 5 or 6, so I am keeping the limit as 8.Usually I will have around 100 vertices. I want to compute for each verticex $v$, the number of vertices at distance $i$ from $v$. I found several reference to algorithm for all-pair-shortest-path problem of Floyd-Warshall type. However, i do not need the exact paths, and not even the distances. These algorithms, can solve my problem, but they do more than what I want. For me time is very critical ( computation over millions molecules), so I want an algorithm which is optimal for above restrictions possibly at the cost of not being generalizable. Solution should be exact and deterministic. I would worry not just about the order of complexity but also of the constants. 

This is called the edge-contraction heuristic, which an upper bound $(k-1)/(k+1)$ on the approximation ratio can be shown. See section 3 of the work by Kahruman et al. for reference. Imagine whenever we combine two nodes u, v into a group, instead of forming a group we contract the edge (u, v) between these two nodes, and updates the weight of all the edges incident to the new formed node. Repeat the procedure until there are only k nodes left. Lemma 3.1 in the above paper shows that the sum of weights of the first i edges being contracted (denoted as $W_i$) satisfies the following inequality: $$W_i \leq \frac{2iW}{(n-1)(n-i-1)} \text{,}$$ where $W$ is the sum of weights of all edges. This can be derived from $$W_{i+1} \leq W_i + \frac{W-W_i}{({n-i\atop 2})} \text{,}$$ since we always contract the lightest pair, the weight of the contracted edge cannot surpass the average weight. One can see that the cut formed by this algorithm has weight $W_C = W-W_{n-k}$, and by observing that $W \geq W^*$ where $W^*$ is the weight of the optimum cut, we have our desired approximation ratio: $$W_C \geq W-\frac{2(n-k)W}{(n-1)(k-1)} \geq \frac{k-1}{k+1}W^* \text{.}$$ For k=2, the algorithm gives a 1/3-approximation to the Max cut problem. 

If we look at a related topic, namely interactive identification protocols, we get a problem that has been well-studied. Unfortunately there are no known protocols that are both secure (against attacks by computers) yet where ordinary humans can reasonably execute the protocol mentally without the aid of the computer. There have been lots of proposals, but all of them have either been broken or have proven too complicated for ordinary people to follow without making mistakes. I think many researchers in the security and cryptography community expect that the problem is very hard and that, in all likelihood, no such secure and usable identification protocol exists. Of course, there is no proof of this -- it is not even clear how you could prove it, since it is not clear how you would formalize "can reasonably be executed by an ordinary human, in everyday life, without aid of a computer". For a good entrance into the broad literature on interactive identification protocols, the following paper is excellent: 

Hence we can assume the theorem holds on the larger n, apply the second observation, and concludes a contradiction by the first case, by setting n' satisfies $({n' \atop k})$ > K > $({n'-1 \atop k})$; such n' must exist by the fact that $({n \atop k})$ > K and K > $({k \atop k})$, n' must lie between n and k+1. 

Distance preserver is also known as an emulator; many related work can be found on internet by searching the term spanner, which requires H to be a subgraph of G. But in my applications we can use other graphs as well, as long as H preserves the distances between T in G. 

Basically what it says is either the odd-hole-free graph belongs to a kind of basic class, or we can decompose the graph into smaller pieces using two kinds of decompositions. The proof of the strong perfect graph theorem follows the same spirit, and similar results are also known for various classes, e.g. even-hole-free graphs. The interesting thing is that although the structural theorem of even-hole-free graphs leads to polynomial-time recognition algorithms (1, 2), the corresponding implication is not known for odd-hole-free graphs. However if we add the restriction that the clique number is bounded then we do have such a result. 

A trivial observation is that if $|S(x)| \le 2$ for all $x$, then this problem is solvable in polynomial time, by reduction to 2SAT. Here's how. Introduce a variable $v_{x,i}$ for each vertex $x$ and each $i$ such that $i \in S(x)$. For each pair $x,y$ of vertices, if there is a path from $x$ to $y$, we get some constraints: if $i\in S(x)$, $j\in S(y)$, and $i>j$, then we get the constraint $\neg v_{x,i} \lor \neg v_{y,j}$. Bijectivity gives us another set of constraints: for each pair $x,y$ of vertices with $x\ne y$, if $i \in S(x)$ and $i \in S(y)$, we add $\neg v_{x,i} \lor \neg v_{y,i}$. Finally, the requirement that each vertex must be assigned a label gives us yet another set of constraints: for each $x$, if $S(x)=\{i,j\}$, we get the constraint $v_{x,i} \lor v_{x,j}$. (Note that only the last set of constraints exploit the promise that $|S(x)|\le 2$ for each $x$.) I realize this observation won't help you in your particular situation. Sorry about that. 

The equivalence of sampling and searching by Scott Aaronson. Here he shows the equivalence of sampling and searching problem in complexity theory in regards to validity of Extended Church-Turing Thesis. Standard information theory, algorithmic information theory and Kolmogorov complexity are used in a fundamental way. He emphasizes: "Let us stress that we are not using Kolmogorov complexity as just a technical convenience, or as shorthand for a counting argument. Rather, Kolmogorov complexity seems essential even to define a search problem .." 

What are the optimal (or best known) bounds (preferably exact or else asymptotic but not expectation on random graphs) on the number of non-isomorphic (unlabelled) simple (no self-loop), undirected graphs with the following parameters/conditions: a) connected, $|V| = m, \ |E| = n$, b) (a) AND spectrum of adjacency matrices is same, c) (a) AND same degree distribution d) (c) and (b) (i.e. all of the above) My interest is in finding lower bounds on the resource (time steps, variables required etc, enegy.) for solving graph isomorphism problem. I am aware of one such work by Cai, Furer and Immerman. Any other references aiming at or helpful in understanding the optimal resources (in any form) required for solving graph isomorphism problem will be greatly appreciated. 

Forward SE is easier to implement, composes better with other testing methods, and thus is more scalable. Usually, on industrial-scale workloads, it's not possible to enumerate all paths in the program under test -- there are simply too many paths. So we need some way to deal with this and sample only a subset of paths, while still having a reasonable chance of finding bugs. With forward SE, there is an easy way to deal with this: if you have a test case, you can run the program concretely for a while, then switch to forward SE partway through execution. Or, you can run the program concretely until it terminates, recording the path it took, and use SE to generate new inputs that will cause it to go down a similar but not identical path. The state-of-the-art tools tend to combine fuzzing (random testing) with SE, and this approach is a nice way to enable that sort of composition. Second, forward SE has implementation benefits. Inevitably there are some kinds of operations that you can't or don't want to model symbolically (e.g., floating point, system calls, etc.). With forward SE, that's no problem: you just execute the program concretely and keep only part of the state symbolic, treating the results of those operations as concrete (you concretize by forcing their inputs to match the observed concrete value, and same for the outputs). It's usually not possible to do that concretization with backwards SE. So forward SE allows "concolic execution", where some of the state of the program is concrete (a single value) and some is treated as symbolic (represented by some variables that we accumulate constraints on). Finally, many modern tools are path-based: they consider a single path through the program, and construct a path expression -- a set of constraints that characterize the set of inputs that will cause the program to follow that path. This is convenient for multiple reasons. When you take this approach, the differences between forward SE and backwards SE largely disappear, as there is no state merging; you only consider a single path at a time. So if you're taking this approach, in some sense you don't need to choose between the two, or the choice is inconsequential (both are basically equivalent) -- you can choose whichever you want based on which one you find a more convenient way to think about it. 

There is a structural result for odd-hole-free graphs by Michele Conforti, Gérard Cornuéjols, and Kristina Vušković: 

If the edges are permitted to be laid both inside and outside the circle, then it is called the 2-page graphs; if edges can only be laid inside the circle, it is the 1-page graphs, which is also know as the outerplanar graphs. See the book embedding entry in Wikipedia for more information. By your comment, I guess the term you're searching for is outerplanar, since the complete graph on 4 vertices is 2-page. Outerplanar graphs can be recognized in linear time; see 

How about complete graphs? Since TSP can always be reduced to an instance on complete graphs (by adding proper distances between non-edges), it is still NP-hard to solve TSP on complete graphs. But any complete graphs is Hamiltonian. 

This is called the Bézout's identity/lemma (not to be confused with Bézout's theorem in algebraic geometry), which states: 

If after many steps, you do not find any solution, then you might guess that the equation is unsatisfiable (but this is a heuristic, so your guess could be wrong). This algorithm might work well for many of the remaining difficult polynomials, but I have no proof that it will always work. In particular, there is a risk that all the possible discriminant polynomials $\Delta(\cdots)$ might have the property that their values are always non-squares (quadratic non-residues), in which case the algorithm above will fail. 

No. It is possible (as far as we know) that $\textbf{P} = \textbf{NP} = \textbf{PSPACE}$. If $\textbf{P} = \textbf{NP}$, the polynomial hierarchy collapses, i.e., $\textbf{P} = \textbf{PH}$. See also Can one amplify P=NP beyond P=PH? for an attempt to understand the limits of the implications of $\textbf{P} = \textbf{NP}$, and see Why doesn't P=NP imply P=AP (i.e. P=PSPACE)? for information about why it seems hard to derive the implication $\textbf{P} = \textbf{PSPACE}$.