I believe the strongest is that $NEXP = MA$. This was proved by Impagliazzo Kabanets and Wigderson. See $URL$ I'd be also interested to know of any stronger collapses than this. Edit (8/24): OK, I thought of some potentially stronger collapse, which essentially follows from the proofs of the above linked paper. Because $NEXP \subset P/poly$ implies $NEXP = EXP$ (see the above link), and $EXP$ is closed under complement, we also have $NEXP$ closed under complement and therefore $NEXP = MA \cap coMA$, which is a little stronger. Indeed, the hypothesis implies that for any $NEXP$ language, a single witness string $w_n$ can be used in the corresponding MA protocol for all YES-instances of any given length $n$, so also $NEXP = OMA \cap coOMA$ (where $OMA$ = "Oblivious MA", see Fortnow-Santhanam-me $URL$ These extra properties, while technical, could prove to be useful in some circuit lower bound argument. Edit 2: Looks like Andrew Morgan highlighted this already. Whoops :) 

The answer to your first question is that in the definition of $PP$, there is no "promise" for a machine to satisfy, like with $RP$. Every randomized polytime machine defines some $PP$ language: on each input, either the machine accepts on $> 1/2$ of the random computation paths on the appropriate inputs, or it doesn't. The first case means $x$ is in the language, the second case means it's not in. Hence, just like in the case of $P$ and $NP$, we can give a recursive list of all $PP$-machines, by simply listing all randomized polytime machines. In contrast, with $RP$, there are randomized machines which accept (for example) exactly one of the random strings on an input, in which case the machine does not satisfy either condition for $RP$: it does not accept on $> 1/2$ of its random strings, and it does not reject all strings. Hence this randomized machine is not an $RP$-machine, in the sense that it has "undefined" behavior on some subset of inputs. This leads to the definition of $Promise-RP$, where you consider "promise problems" which permit arbitrary behavior on some strings. See the Complexity Zoo for more. This leads to question 2. The problem with the notion of a "semantic class" is that it depends on the machines you're using to define the class, and hence it is extremely subtle to properly define. (I am not sure that a completely satisfactory definition has ever been given.) Being a "semantic class" is roughly equivalent to the property stated above: not every machine (in a natural listing of machines) will satisfy the acceptance/rejection conditions needed for your class, and hence it's "hard" to get a list of machines that accept exactly the languages in your class. But if $P=BPP$, then there is some easily computable list of some machines that accept exactly the languages in your class: namely, the list of polynomial time computable machines. Hence it would appear that, if it turns out that $P=BPP$, we have turned a semantic class into a syntactic one. If it were truly the case that there is simply no easily computable list of machines (of any reasonable kind) that accept exactly your class, then yes, I do not think your class can possibly have a complete language. But that seems very hard to formalize properly, let alone prove. 

A SYM of AND (i.e, SYM+) circuit of size $K$ is essentially equivalent to having a multivariate polynomial $h : \{0,1\}^n \rightarrow \{0,\ldots,K\}$ with at most $K$ monomials, a lookup table $g : \{0,\ldots,K\}\rightarrow \{0,1\}$, and computing $g(h(x_1,\ldots,x_n))$. (For instance, a proof can be found in Beigel-Tarui.) The intuition is that each monomial in $f$ is an AND gate, and $g$ is the SYM gate. I say "essentially equivalent" because the multilinear polynomial $h$ could also have negative coefficients for some terms, and negative coefficents are not obviously implementable in SYM of AND. But I claim (and Beigel and Tarui claim) that this is not a problem. Think about it :) 

Literally stated, the problem of exponentially separating neural nets of depth d from depth d-1, for all d, is open, to the best of my knowledge. When your "activation functions" are linear threshold functions for example, it is open whether all nets of all depths d can be simulated, with a polynomial increase in size, in depth 3. 

Wigderson proved that $NL/poly \subseteq \oplus L/poly$. By standard arguments, $L = \oplus L$ would imply $L/poly = NL/poly$. (Take a machine $M$ in $NL/poly$. It has an equivalent machine $M'$ in $\oplus L/poly$. Take the $\oplus L$ language of instance-advice pairs $S = \{(x,a)~|~M'(x,a)~\textrm{accepts}\}$. If this language is in $L$, then by hardcoding the appropriate advice $a$ we get an $L/poly$ machine equivalent to $M$.) I think that would be surprising: nondeterministic branching programs would be equivalent to deterministic branching programs (up to polynomial factors). 

It is a tough problem to give even a coherent definition of "Algorithm A is similar to Algorithm B". For one, I don't think that "they must be solving the same problem" is a necessary condition. Often when one says in a paper that "the algorithm $A$ of Theorem $2$ is similar to the algorithm $B$ in Theorem $1$", the algorithm $A$ is actually solving a different problem than that of $B$, but has some minor modifications to handle the new problem. Even trying to determine what it means for two algorithms to be the same is an interesting and difficult problem. See the paper "When are two algorithms the same?" $URL$ 

Assuming this Linear PCP Conjecture, a $2^{O(\varepsilon^c m)}$-time $7/8+\varepsilon$ approximation, for all $c$ and $\varepsilon$, would entail that 3SAT is in $2^{\varepsilon n}$ time, for all $\varepsilon$. (Here $m$ is the number of clauses.) The proof uses the Sparsification Lemma of Impagliazzo, Paturi, and Zane. 

I think I remember what lecture you're talking about. I remember he was saying that most statistical tests that are done can be carried out in $LOGSPACE$. (Comparing the number of $1$'s versus the number of $0$'s, computing standard deviation, etc.) But the output of Nisan's pseudorandom generator will fool all of these statistical tests, and still is not a truly random sequence. This lecture by Ryan O'Donnell describes the details. An alternative way of defining "statistical test" is given by Blum and Goldreich. 

This is open, as far as I know. It could be provable (because its hypothesis may be false) or it just be difficult to show that any $2^{n^k}$-time algorithm for Succinct3SAT can be converted into a $2^{O(n)}$-time algorithm for Succinct3SAT. In general, theorems of this kind are called "downward collapses" which say if two "large" classes are equal then two "smaller" classes are equal. These theorems are rare. Usually you can either prove an "upward collapse" (small classes equal implies larger classes equal, like $P = NP$ implies $NEXP = EXP$) or its contrapositive, a "downward separation". Something along the lines of what you want is the theorem by Hartmanis, Immerman and Sewelson ($URL$ that $NE = E$ $\iff$ every sparse set in $NP$ is contained in $P$. This gives a "downward collapse" but only for the sparse sets (those sets that contain only $poly(n)$ strings of length $n$). 

That is, $A$ can distinguish between graphs which have $P$ and graphs which have high edit distance from graphs having $P$. Alon, Fischer, Newman, and Shapira proved that a property $P$ is testable in this way if and only if the property can be "reduced" to the property of checking whether a graph has an $\varepsilon$-regular partition (in the sense of Szemeredi). This shows that testing regularity is "complete" for testing, in some sense. (There is also a one-sided error version of testability, see the reference.) 

SETH says that for all $\delta < 1$ there is a $k$ such that $k$-SAT requires $2^{\delta n}$ time to be solved in the worst-case. The computational model is generally taken to be the random-access machine or pointer machine model, which allows for $O(\log N)$ time access to a storage of $N$ items, and is generally assumed to also be probabilistic with bounded error. As far as I know, it is open whether $2^{\delta n}$ time algorithms on such a model can be translated into two-tape Turing machines running in $2^{\delta n} \cdot poly(n)$ time. Nevertheless, proving that such a translation is not possible would separate multitape Turing machines from random access machines, which would have several very intriguing implications. For one, it would prove that SAT is not solvable in quasi-linear time on multitape Turing machines (because, if SAT could be solved with such multitape machines, then random-access machines can be efficiently simulated with multitape Turing machines). Note that many computational primitives (such as sorting, circuit evaluation, simple dynamic programming) can be implemented efficiently on multitape Turing machines. One relevant reference for these issues is Regan, "On the Difference Between Turing Machine Time and Random-Access Machine Time." To address your specific questions: no, a multitape Turing machine is not automatically implied here, but yes, such an "algorithm" for SAT (under the usual random-access model) would break SETH. 

You can find a preprint by following this link $URL$ EDIT (1/24) By request, here is a quick summary, taken from the paper itself, but glossing over many things. Suppose Merlin can prove to Arthur that for a $k$-variable arithmetic circuit $C$, its value on all points in $\{0,1\}^k$ is a certain table of $2^k$ field elements, in time about $(s + 2^k) \cdot d$, where $s$ is the size of $C$ and $d$ is the degree of the polynomial computed by $C$. (We call this a "short non-interactive proof of batch evaluation" --- evaluating $C$ on many assignments.) Then Merlin can solve $\#$SAT for Arthur as follows. Given a CNF $F$ on $n$ variables and $m$ clauses, Merlin and Arthur first construct an arithmetic circuit $C$ on $n/2$ variables of degree at most $mn$, size about $mn \cdot 2^{n/2}$, which takes a sum over all assignments to the first $n/2$ variables of the CNF $F$ (adding a $1$ to the sum when $F$ is true, and $0$ when it's false). Using the batch evaluation protocol, Merlin can then prove that $C$ takes on $2^{n/2}$ particular values on all its $2^{n/2}$ Boolean assignments, in about $2^{n/2} poly(n,m)$ time. Summing up all those values, we get the count of the SAT assignments to $F$. Now we say at a high level how to do the batch evaluation protocol. We want the proof to be a succinct representation of the circuit $C$ that is both easy to evaluate on all of the $2^k$ given inputs, and also easy to verify with randomness. We set the proof to be a univariate polynomial $Q(x)$ defined over a sufficiently large extension field of the base field $K$ (of characteristic at least $2^n$ for our application), where $Q(x)$ has degree about $2^k \cdot d$, and $Q$ ``sketches'' the evaluation of the degree-$d$ arithmetic circuit $C$ over all $2^k$ assignments. The polynomial $Q$ satisfies two conflicting conditions: 

This is a very interesting question. First, a clarifying remark. Note that "upper bound on the number of witnesses" is not a property of a computational problem per se, but of a particular verifier used to decide an $NP$ problem, just as an "upper bound on number of states" would not be a property of a problem but of a Turing machine deciding it. So saying "$NP$ problem with upper bound on number of solutions" isn't quite accurate, and if $P = NP$ then every $NP$ problem has a verifier with any number of desired solutions (including zero, and including all possible strings). So we have to make a definition, to address your question. For $s : {\mathbb N} \rightarrow {\mathbb N}$, let's say an $NP$ problem $L$ "has at most $s(n)$ solutions" if for some constant $c$ there is an $O(n^c)$ time verifier $V$ such that, for every input length $n$ and for every $x \in L$ of length $n$, there are distinct $y_1,\ldots,y_{s(n)}$ of length $n^c$ such that $V(x,y_i)$ accepts for all $i$, and $V(x,y)$ rejects all other $y$ of length $n^c$. All I think I can say at the moment is this: 

I don't understand the statement of this problem. But I think there is a natural way to formalize your more general question which may shed a little light on it. Maybe I am completely missing your point, but hopefully you still find something interesting to read here. 

Maybe... that depends on whether you are happy with the above formalization of your question. For a deterministic 3-SAT algorithm $Y$, I think the above exact simulation of $Y$ problem would be $P^{NP(1)}$-hard, where $P^{NP(1)}$ is polynomial time with one query to $NP$. (The annoying syntax of StackExchange requires that I write (1) instead of the alternative. Earlier I said $C_Y$ should be $P^{NP}$-hard, but I am not sure of the details: you may see how to generalize the below.) We give a polytime many-one reduction from every $L \in P^{NP(1)}$ to $C_Y$. Given such an $L$, let $M$ be a machine recognizing $L$ that does a single $NP$ query. WLOG, that query is a SAT formula. Also WLOG, the SAT query can be "postponed" until the very last step of the computation, in the following sense: there is a polynomial time algorithm $A(x)$ which prints a formula $F$ and bit $b$, such that for all $x$, $M(x)$ accepts iff $A(x)=(F,b)$ such that ($SAT(F)$ XOR $b$) = 1. ($M$ may reject iff $F$ is satisfiable, or it may accept; the bit $b$ captures this.) For simplicity, let's say when $Y$ ends its computation, it moves its tape head to cell 1, writes "accept" or "reject" in that cell, and loops forever. Then, asking if $(F,T,1,accept) \in C_Y$ for sufficiently large $T$ will tell us if $F$ is accepted. (In general, we just need that it's efficient to compute the instance $y$ of $C_Y$ which will tell us.) Note this already shows that $C_Y$ is both $NP$-hard and $coNP$-hard; the latter is true because $(F,T,1,reject) \in C_Y$ iff $F$ is not satisfiable. The final reduction from $L$ to $C_Y$ is: given $x$, run $A(x)$ getting $(F,b)$. If $b=0$ then output $(F,T,1,accept)$, else if $b=1$ output $(F,T,1,reject)$. For every $x$ we are producing (in polynomial time) a $y$ such that $M(x)$ accepts iff $y \in C_Y$. (Thanks to Joe for demanding that I be clearer about this part.) But I don't see (for example) how to get $\Sigma_2 P$-hardness. To reduce $\Sigma_2$-SAT to the problem, it would appear you'd need to write a 3-CNF SAT instance $x$ which simulates your deterministic algorithm $Y$ within it (where $Y$ is solving Tautologies on various subproblems). But as $Y$ has no given time bound, that 3-CNF $x$ could be huge, so you don't necessarily get a polynomial-time reduction. Unless I am missing something. 

Couldn't find this one anywhere... It's an open problem whether $\Sigma_2 EXP$ problems have exponential-size circuit complexity. Is there an oracle relative to which $\Sigma_2 EXP$ has $2^{o(n)}$ size circuits? $2^{n^{o(1)}}$ size? Such an oracle exists for the class$MAEXP$; this was shown by Buhrman, Fortnow, and Thierauf. 

Here's a quick argument for why it works over rings. Given matrices $A$, $B$, $C$, we verify $A B = C$ by picking a random bit vector $v$, then checking if $ABv = Cv$. This clearly passes if $AB = C$. Suppose $AB \neq C$ and $ABv = Cv$. Let $D = AB - C$. $D$ is a nonzero matrix for which $Dv = 0$. What's the probability that this occurs? Let $D[i',j']$ be a nonzero entry. By assumption, $\sum_{j} D[i',j] v[j] = 0$. With probability $1/2$, $v[j'] = 1$, so we have $D[i',j'] + \sum_{j\neq j'} D[i',j] v[j] = 0$. Any ring under its addition operation is an additive group, so there is a unique inverse of $D[i',j']$, i.e., $-D[i',j']$. Now, the probability of the bad event $-D[i',j'] = \sum_{j\neq j'} D[i',j] v[j]$ is at most $1/2$. (One way to see this is the "principle of deferred decisions": in order for the sum to equal $-D[i',j']$, at least one other $D[i',j]$ must be nonzero. So consider the $v[j]$'s corresponding to these other nonzero entries. Even if we set all of these $v[j]$'s except for one of them optimally, there is still equal probability for the last one being $0$ or $1$, but still only one of these values could make the final sum equal to $-D[i',j']$.) So with probability at least $1/4$, we successfully find that $Dv \neq 0$, when $D$ is nonzero. (Note $v[j]$ and $v[j']$ are independently chosen for $j \neq j'$.) As you see, the above argument depends on subtraction. So it won't work (for example) on arbitrary commutative semirings. Perhaps you could relax the multiplicative properties of the algebraic structure, and still get the result? 

However, the $NQL$ vs $DQL$ question is still open. We cannot even rule out a linear time algorithm for SAT. So even if we severely restrict the running time of our algorithms, we are still lost as to how to prove a nontrivial time lower bound for SAT. 

Let me give a toy example of the relativization barrier. The canonical example is the time hierarchy theorem that ${\bf TIME}[t(n)] \subsetneq {\bf TIME}[t(n)^2]$. The proof (by diagonalization) is only a little more involved than the proof that the halting problem is undecidable: we define an algorithm $A(x)$ which simulates the $x$th algorithm $A_x$ on input $x$ directly step-for-step for $t(|x|)$ steps, then outputs the opposite value. Then we argue that $A$ can be implemented to run in $t(|x|)^2$ time. The argument works equally well if we equip all algorithms with access to an arbitrary oracle set $O$, which we assume we can ask membership queries to, in one step of computation. A step-for-step simulation of $A_x^O$ can also be carried out by $A$, as long as $A$ has access to the oracle $O$ too. In notation, we have ${\bf TIME}^O[t(n)] \subsetneq {\bf TIME}^O[t(n)^2]$ for all oracles $O$. In other words, the time hierarchy relativizes. We can define oracles for nondeterministic machines in a natural way, so it makes sense to define classes $P^O$ and $NP^O$ with respect to oracles. But there are oracles $O$ and $O'$ relative to which $P^O = NP^O$ and $P^{O'} \neq NP^{O'}$, so this kind of direct simulation argument in the time hierarchy theorem won't work for resolving $P$ versus $NP$. Relativizing arguments are powerful in that they are widely applicable and have led to many great insights; but this same power makes them "weak" with respect to questions like $P$ versus $NP$. The above is, of course, a toy example -- there are many other more complicated examples of arguments in complexity which still relativize (i.e., hold up when arbitrary oracles are introduced).