In current ER modelling, you would create a conceptual or logical model, capturing the requirements of users, then create a 3NF from it, to avoid inconsistencies and capture the correct multiplicities of the relationships (1:1, 1:N, N:M). Here you would also split entities, if necessary. For example split off addresses from customers, because you might need billing and shipping addresses. The last step would be to create the physical model, which considers indexes, surrogate keys, and careful denormalisation. This also includes considering advanced optimisations later on, like native types (e.g. double, int64, ...), types of indexes (btree vs bitmap, join indexes, etc.), special table layouts (row or column oriented, index organised tables, vertical or horizontal partitioning, inclusion of dependent rows). And this only mentions a few of the options that are available to the database designer in a current commercial database. The access path usually is determined by the sql engine and its optimiser on the basis what is known about the data (statistics, quantity structure) and the indexes available and can be looked up in the execution plan for each SQL statement. 

There's no real pitfalls from doing this. You just need to be aware that there will be a slight increase in CPU on the subscriber, so if you're already CPU bound it might not be the best path forward. You could consider using either pre or post snapshot script in order to help ensure that index compression is maintained in the event you need to run another snapshot (and to add any additional indexes that you require). When performing the compression be aware that you will probably experience transaction log growth, much like you would if you rebuilt any other index. Sorting in tempdb is generally a good practice for something like that. 

You will also need to configure an additional TCP endpoint inside of SQL Server itself. If you run a query of 

Although you're using an ORM, which hides the nitty gritty details of SQL for your developers, you seem to be interested in different approaches to insert a record and a multitude of detail records in a child table. I would opt for the code, which conveys the intend most clearly and which can be maintained more easily. In my opinion, this will likely be the second option. Only if (measured) performance is an issue, then I would start optimizing. 

Create an index on . This way oracle will most likely use the index only to answer your slow running query. You can add more columns to that index, so that you also get the needed content. Hopefully you don't need all columns. Be careful about, what impact this index has on your load performance. 

SQL Server 2012 SP1 CU7 is from November 2013, and so not sufficiently current. There is no SP1 cumulative update that would support pulling that information. You would need to move to Service Pack 2 with CU9 at the least. As a side note, SQL Server 2012 SP1 was no longer supported as of July 2015. Service Pack 2 will stop being supported in January of 2017 (as detailed in the Microsoft Lifecycle Policy). You should look to upgrade to Service Pack 3 as soon as possible to remain in support. 

NTLM is a challenge/response type system, and while it has a resonable level of security it is potentially vunerable to attack and credential theft. In fact NTLM V1 is a known attack vector, considered insecure and should be disabled on your systems. NTLM V2 is more secure, and the current standard. Kerberos handles mutual authentication between parties, ensuring that the tokens are encrypted and secure. I'd recommend reading the Kerberos basic authentication concepts to get a handle on it. The reading is a little dry, but it will give you a greater understanding. Of the two, Kerberos is far more secure, and is the recommended protocol for Windows connections to SQL Server. To resolve your SPN problem look for duplicate tickets using SETSPN-L with both the server, and the service account, and look for duplicates. Remove those, and then adjust the SQL Server service account to allow it to self register its service principal name. This will allow it to handle things itself, which makes life a lot easier. You will not experience any actual problems with SQL (except for times when you might be using linked servers, or some other double hop type scenario where the ticket cannot be passed along to a second machine), and so it may not be a critical fix to you. Only your security team can really define that. 

The usual way to solve such a problem, is to use a bitfield. So you would create a tags table, and link that via an n:m table to the sales figures or products. Then in the tags table, for each tag you would assign a unique bitvalue as a power of 2, e.g. for and so on. Using you can then condense these values into a single numeric value and store this alongside the product or sales figures. For example tags "sports" and "cricket" on one product become 5. If the bitsize of the numeric type you have available is not enough to store all your tags, use multiple of these fields and store the number or column name of the field and the bitvalue with the tag. Then for querying use clauses of the form: or = 10th flag set You can now do any Boolean expression on the flags. If you designate a single field for all colors you can also do other tricks, like querying for products, which have a Color tag: and so on. As you're in a column oriented database (redshift), is only executed once per unique value in the column. Depending on implementation, the database will further reduce this, by analysing the -clause and use constraints on size through sort order of column values (for free). And if you need that last bit of performance, you can do tricks by collecting statistics on the flags and the queries and grouping them together intelligently. I expect in the use case that you're describing (perform sum ... group by after filtering), performance that you could gain through this, would be negligible compared to the cost of calculation. 

SQL Server 2016, I'm attempting to work with some regular data and return a JSON object for processing by another system. The other system does not recognize the array wrapper, and so I am attempting to use WITHOUT_ARRAY_WRAPPER to get rid of this. When used in a subquery odd results get returned... 

Here's an updated version (a little hacky, I know) that writes the header to a file, and then just the XML data that is produced. See if this gets you a little closer. 

You need to add the @job_par parameter to your calling procedure so that you can then pass that down the line. Using your example: 

You would not actually save any space using BIT rahter than TINYINT (and would also give yourself more options to add other entities). This is because 8 or less bits are stored in a byte. If you were using multiple bit entries this could save you space, but it's ultimately very limiting. A TINYINT would use the same 1 byte of storage, but would allow you to store up to 256 different entity values. Saying this though, from a practicality standpoint, unless you are looking at ridiculously high volumes of data the space savings are not going to be that significant regardless of using TINYINT or SMALLINT (~65k values & 2 bytes). Using normalization for this sort of thing is very common, and helps prevent excessive bloat (after all in your second table you would store 1 bytes instead of twelve to represent the person entity). Don't forget to add foreign key references. 

I suggest you have a look at the section on 'rating scales' in the complete guide to writing questionnaires: 

Please research data warehousing concepts like Operational Data Store (ODS), Star Schema, OLAP and ETL, which will definetely give performance and maintenance benefits compared to a physical copy (which I assume to be in some form of 3NF) 

The "object"-reference in object-relational refers to object-oriented programming. This is a programming style where objects like a triangle or a square or some other geographical entity can be told to move itself (coordinates translated) or to rotate a certain degree or to scale some percentage, regardless, which class it is (triangle, square, whatever). As programmer, you don't know which class a particular object is and the programming language takes care of carrying out the correct calculations when you tell this object to move (change 3, 4 or more coordinates). An object-relational database now combines features of object-oriented programming and relational databases and takes care of converting between objects with methods (move, rotate and scale) and tables, which usually lack these methods. 

A further note to filegrowth. If you do not have IFI enabled then you will have IO stalls on writes to the data file that cause it to grow. Those stalls will last as long as it takes to grow out the file and zero it out to ensure the space is clear (2016 does not use zeroes, however the concept is the same). With IFI enabled (requires Perform Volume Maintenance Tasks permission for the service account, this can be done at install time with SQL 2016) then the data file can grow instantly and does not need zeroing out. Log files do not have the option for IFI (for security and data safety reasons), so any growth on that file will cause IO stalls while that completes, impacting performance. The MAXSIZE settings in the create database statement will prevent the files from growing beyond those values. This would mean that you could not store more an a total of 10MB of data (including system data), and that no individual transaction could be larger than 5MB in size. Both of these values can be adjusted using an ALTER DATABASE statement. 

Install the PosgresQL ODBC driver and then use that to register Postgres tables in Access. Issuing the SQL you have given in Access will then transfer data from Access to PosgresQL. 

How are you going to use the rating? Do you need to calculate aggregates like mean, min or max? Then use numbers. If this is not meaningful, don't use numbers. What kind of where clauses do you expect? How will the code look like and how does your design contribute to avoiding errors and do your design choices support that the intention of the code is easily and readily conveyed? Last but not least: Design for clarity and less errors, optimize later. Hope this helps 

Probably it would pay to have a date dimension table listing dates and any date part that might be necessary (like quarter, halves and months). This way you could get rid of doing the date math over and over again and just join. Tricks which this also enables are things like comparing to last years quarter (i.e. add a last_year_quarter column and similar).