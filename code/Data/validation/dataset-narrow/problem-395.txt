For something like this it would be much safer to script an update of just the column(s) that need updating in your user table. Cloning any part of your test environment user data into production could be disastrous. Is it not possible to use the script with which you (presumably) updated your test environment in production as well? 

If your customers use the new login, they will have full control of all pre-existing databases (including being able to drop them!), as well as any new ones created with that login (since creator login gets mapped to dbo). And they will not be able to drop any dbo user. 

But if you can instead wrap steps 1 and 2 in a stored procedure or ad hoc query that handles the retry logic without duplication, then your solution will be much more understandable and maintenance-safe. 

From your comment update, it sounds like Merge replication could work well for you, mostly due to the minimal updates. It's a lot simpler to set up and manage than two-way transactional (which is what the scary link I mentioned deals with--I should have left that for later). In a nutshell: $URL$ 

This will give you the desired answer based upon the example data you provided. You didn't answer my question about the number of levels, so this assumes there are no more than 3 levels. 

What are you trying to achieve with more than two axes? If at the end you just want those fields/values to be dumped into a tabular result, you can rewrite your query to use crossjoins. 

If you are browsing in Excel and want to be able to see dimension attribute members that do not have corresponding rows in a fact table (measure returns nothing), there is a pivot table setting you can use. Access the PivotTable Options dialog box by right clicking on the pivot table and choosing PivotTable Options. On the Display tab, check the box next to Show items with no data on rows. This will show all values of whatever hierarchy is in your rows regardless of if the measure in the Values section of your pivot table returns a non-blank result. You can do the same thing for columns by checking the item underneath the one indicated in the image. This option must be set for each pivot table and will apply to all fields in the rows (or columns) when you check the box. 

I'd suggest the first thing to determine is whether or not the server workload actually struggles during those spikes. Do you see for example IO being maxed out, or other queries being blocked (due to the splits increasing the time the write transactions take to complete) or slowed? If not, the page splits are not an immediate concern, but still may be worth looking into. The question is, are these "good" or "bad" page splits. This link might help you determine what you're seeing. Logging your index fragmentation levels before and after a spike might also be a simpler way. "Good" page splits are simply inserts to the end of an increasing index (clustered or otherwise)that require a new blank page, so it's really not a page split as generally thought of, even though SQL Server counts it as such--presumably because there is some overhead, but probably not more than the inserts cost in general. "Bad" page splits are updates or inserts to the middle of an index that overflow the page, and are the ones that cause both internal and external fragmentation, with external not much of an an issue with SSDs and/or shared storage, and internal being of more potential impact due to the IO and cache memory they waste. It could be that you've got a mix of good and bad, perhaps good into the clustered index and bad in multiple non-clustered indexes. That's pretty much unavoidable, and you'll just need to consider your index maintenance and possibly a specific fill factor on indexes that are frequently affected. But read $URL$ first. However if you find your clustered index is being bad-splitted, then it may be worth considering whether a clustered index that better supports your inserts would be in order. Or if the splits are caused by updates adding more data during the life of a record, a specific fill factor might be in order, but really only if the updates are evenly distributed throughout all your data, since a fill factor to support only your recent data would waste a lot of space/IO/cache if most of your data is static over time. The ideal clustering config really depends how your table is used overall though, not just on how it's written to. 

You can back up and restore SSAS Tabular 2012 databases, but you will have to adjust the security role memberships to include users in the correct domain. You can perform the backup and restore manually or through a script in Management Studio (or executed via PowerShell). You can also re-deploy an SSAS database from the SSDT project or using the SSAS Deployment wizard. The wizard will allow you to deploy roles and ignore members. Next populate the role memberships with appropriate users. From there, you can process the model to populate it and bring the model online. Another option is to script the SSAS database from Management Studio. Once you do this you can remove the collection from each role and execute the XMLA on your target server. 

You cannot use an aggregate like CountRows() in a calculated field in your dataset. But you can use CountRows() as an expression in a textbox (alone or within a table) scoped to your dataset. Now that you have your dataset created, you can put a textbox on the report and populate it with the expression , and it will provide the correct answer. Otherwise, you will need to modify your dataset to include the rowcount. It would seem that using the expression in the report would be the desired option as modiying the dataset would give you a column populated with either a running total or the total rows repeated on each column. 

I believe type = 4 only applies to pre-2008 fulltext files which have been upgraded, because since then there has been no way to create separate fulltext files -- only separate filegroups. (See type = 4 doc for SQL 2012 at $URL$ which confusingly says the same as for 2016 except version number.) Something like $URL$ may technically do what you are looking for, but won't actually be relevant for your report. 

I see you've just done a big update, so I'll perhaps do a separate broader answer. Since I can't yet comment, I'll ask a few things here: - Are you free to add indexing as your query requires? - Is the replacement of CONTAINS([varchar3], 'moreText') with [varchar9] LIKE '%a%' correct (i.e. do you definitely no longer need fuzzy search on varchar3?) - Will OFFSET always be 0? - Can you give some idea of the proportions of data you expect for columns varchar1, date1 and varchar8? 

Date dimensions are pretty standard in a data warehouse, and are highly recommended by Kimball as most facts tie to a date. Typically, the key is an integer. It can be a meaningless surrogate key, or it can be a "smart" key where the integer is in the form yyyymmdd; e.g., the key for August 2, 2014 would be 20140802. Date dimensions provide a set of contiguous dates in multiple formats and allow you to do date calculations once rather than in each query. They make it very easy to do time period comparisons. You can add other fields that could be analytically relevant such as holidays, indicators of work days, fiscal calendars (where different from standard calendar years). There are lots of scripts available online to create and populate a date dimension. Many tools will create the date dimension for you. I'm not sure what you are using for your underlying data source, so here are a couple of examples of date dimensions. Hopefully you can convert these to the appropriate format for your needs. 

The questions of whether/how to directly measure CPU core usage etc. are beyond my understanding, but here's what I'd consider trying: Run a standard profiler trace with database name added, during your normally busiest period. Total up the CPU column for the SQL:BatchCompleted and RPC:Completed events by database, and you'll get a rough idea of how much CPU resources (which may be spread across multiple cores) each database is consuming. (Perhaps also total up the CPU column for the other events to see if anything major was missed. And save the trace "as trace table" for analysis.) Exactly how to translate that to how many cores you'll need, I can't say. But if you also measure the total system CPU usage during the profiler run you might be able to estimate against the specific database's ratio of the total. Note: If your server takes less than a few hundred batch requests a second (see SSMS activity monitor), then a standard profiler trace even across the network will almost certainly not affect performance. And if you instead script a server-side trace then more requests a second can be handled without slowing anything, but I make no promises for your environment. For RAM, I wonder if $URL$ might help you determine if your instances need less/more. I don't think there's any way to do this by database though. 

The validity depends on how users want to look at the data. You are looking at it as just a transaction fact. Other types of fact tables include periodic snapshots and accumulating snapshots. If you want to see all the times that someone corrected a row to help decrease erroneous entries, the effective dates may be appropriate so it's clear that the transaction was updated. This creates a fact table that is somewhat similar to a type 2 SCD. The Kimball Group has an article that directly addresses your question. Here's a Kimball Group design tip that talks about effective dated accumulating snpashot fact tables. You may be correct that you should just add transactions that reverse the original row. That sounds like it could be a valid solution if you just need to see the transactions and sum them up. This is how I've seen most accounting data work. Kimball says the effective dated fact tables may be useful for quickly calculating account balances at a paritcular point in time, especially for tracking slowly changing balances. But this is a fairly rare case. I think your concern that it will be confusing for users is also valid. You have to decide if that can be overcome by education and if its worth it for the added analytical capabilites in the data. I haven't had to do this much in my data warehousing experience because most of my facts were simple transactions or periodic snapshots. But I have created several effective dated bridge tables for many-to-many relationships. 

$URL$ shows SQL 2016 Dev as supported on Win10 Home, and I don't know of any explicit feature limitations--though you are right in general that you're limited to what the OS supports. Have you tried to set up replication using the built-in service logins? I believe the instructions around adding Windows logins are for security best practice, which may not be an issue for a dev environment. 

Switching on will be near-instantaneous (assuming only one DB connection). Yes, switching off is just as painless. See above. Cleanup happens by itself. Section 2 of the post above has some things to consider. 

You can't use a SP in a default, but you can use a function. But presumably your SP code has to increment the value in the NextNumber table, so that won't work for you. Your best bet for doing this within SQL is probably to have an INSERT trigger on MyTable which calls the SP and sets MyColumn. However you'll have to carefully consider the behaviour of your SP code when multiple concurrent inserts are involved. For example, you'll probably need to select the current value from the NextNumber table using an UPDLOCK hint to prevent another user reading and using the same value. (Note that the UPDLOCK hint will only prevent reads in this way if it's within a transaction [which triggers run within in by default], and if other selects on the NextNumber table use UPDLOCK as well.) 

This is commonly represented with an accumulating snapshot fact table or timestamped accumulating snapshot fact table. Accumulating snapshot tables model events in progress for business processes that involve a predefined series of steps. The timestamped. You'll have a date key in the table that represents the date the opportunity hit each stage. If you need to reproduce what pipeline looked like on a given day or track the fact that an opportunity went into and out of a stage multiple times, you might need to consider the timestamped accumulating snapshot. 

You can put your cube formula in a separate cell and reference the cell where users type in the name they want. For example, if you have users put the branch name in cell B2, you can put this formula in another cell. 

Since I can't see your table structure and I don't know the size of your data, here's something that will return the right answer but may or may not need to be tweaked for performance. You can write a query that groups both created and closed tickets together by date. Although you could just write two queries and do a full outer join, I would guess you want SSRS to accurately represent days with no ticket activity in addition to filtering the data for the date range at the source instead of in the presentation layer, so I would suggest creating a date table/CTE/query. If you already have a date table, use it. If not, here is a way to create what you need for the last 30 days. You could also make it a stored procedure and parameterize it to choose the number of days. 

Having a column named "EmiAmount" that also indicates finance availability (a interrelated but separate concept from EMI if I understand your post correctly) is bound to cause confusion. Even your post seems to confuse the terms. And there's probably no compact name that can alleviate this. If you choose to use a single column, I would at least suggest a non-nullable column, and using -1 instead of null to indicate "no finance". The reason is that it's generally best not to assign an actual meaning to null, as the standard interpretation is "data optional and not available/applicable". Also, when you can avoid nulls it's best to do so due to the complications they cause. (Note I'm not suggesting to avoid nulls when "not applicable/available" is fully valid from a data modelling standpoint, which is very frequently the case.) 

This is caused by using Select * in the TVF. It also happens to views that use select * when columns are inserted to the underlying table between existing columns, or when columns are deleted. This to me is the number one reason (among a few more big ones) not to use Select * anywhere except perhaps in an outer query that wraps a fully explicit subquery for example.