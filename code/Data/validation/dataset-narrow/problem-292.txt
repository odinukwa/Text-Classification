I just had a look at the data in the Higher Education Statistics Agency (HESA) zipped file and the files are all different. This means you will either have to create an individual file for each import or create a single file using conditions as described in the following article: Reference: How to use multiple csv files in logstash (Elastic Discuss Forum) Expanding on your first by one level: 

You are granting access to a user from a host/ip_address () to the MySQL instance (). Then you have to ensure that you can TELNET into the MySQL server from the application server on port 3306 (default; or the port you have configured MySQL to listen on). On App Server: 

1. Server IP Configuration Check your Windows Server's IP configuration by setting off an . Your results can contain both IP addresses, but at least your internal IP: - - 2. Check your routing Set off a trace route with the following command: . Verify that you receive the route to your server. 

Because it is Microsoft Access, I would recommend sticking with the supported options rather than trying another solution that might or might NOT work. 

This is where you link the SQL Server Login to a newly created database user or where you link the SQL Server Login to an already existing database user. Solution 

"SQL Server Agent" Service Account Insufficient Privileges If the SQL Server Agent (INSTANCE) Service Account doesn't have enough privileges on the directory it is deleting files from, then it will be unable to delete the TLOG backup files. Solution 

The sys.database_files view The table as pointed out by SQL_Underworld is only for the database you are currently connected to. Attaching database files as database You could attach the database files you have found on your disk with the following command: 

In your screenshot you are only listing the files up until the TLOG backup. However, this file does not contain any modifications past 03:06:01 AM and this is possibly why you are unable to restore to the point-in-time at 03:07:00 AM. The FULL backup is always consistent as is stated by the official Microsoft (Docs) documentation (emphasis mine): 

Reference: Bypass startup options when you open a database Last possibility Buy a repair software - Stellar Phoenix Access Database Repair 

Sometimes it helps to de-install the enterprise manager and then re-install it using the command. We'll leave that for the worst case. Using EMCA After Changing the Database Listener Port 

You can't connect from a 9.2.0 SQL*Plus to a 12.2.0.1 version or Oracle Database. It was never a supported option as displayed in the Interoperability Support Situation matrix chart in an Oracle Support Note.1 (Oracle Support Login required to read the Oracle Document.) A SQL*Plus 9.2.0 client was only able to connect to 11.2.0 server or lower. The lowest supported version of SQL*Plus that could connect to an 12.2.0.1 version of Oracle Database would be an 11.2.0.x client. 1 Oracle: Note 207303.1 Reference: oracle 9i client connecting to oracle 12c server (Stackoverflow.com) 

Reference: Knee-Jerk PerfMon Counters : Page Life Expectancy) Your SQL Server is performing as designed, given the current hardware More Hardware? Now if your users are experiencing performance issues when retrieving data, then you might benefit from more memory, because SQL Server will then be able to store more data in the Buffer Cache without having to purge all of the data when new data is loaded. Your PLE counter will probably still fall when new data is stored in your DW database, but it might not fall so dramatically. Baseline You will have to monitor your system and have a baseline that shows you the normal behaviour. Anything outside the norm is then worth an analysis. 5 Reasons You Must Start Capturing Baseline Data 

It's not an issue with the job scheduler, but an issue with the underlying tables that are being accessed. When Job 2 starts, it manages to acquire a lock on the users table (possibly: IX, IU) before the Job 1 is able to acquire its required locks on the same table. Then Job 1 has to wait. You are encountering blocking as previously observed by @MaxVernon. You could run the following script at the full hour to see if you can catch the statements involved: 

I guess the worst case would be if you were to lose all your data. Consider performing a simple mysqldump to have a backup of your data. Then consider implementing a backup strategy. 

Depending on the wait statistics you find in the results, you can find hints of what may be the issue in the SQL Server Wait Types Library on Paul's site. Then go from there. Depending on your wait stats, you should see what your SQL Server is waiting for. If you don't find any big issues, then your application might be the limiting factor. Disk Alignement Seeing as you had many read aheads without the covering index, you might want to verify that your data disks are formatted with 64k block size. SQL Server reads in Extents which is explained in the Reading Pages information on Microsoft's site. (1 Extent = 8 Pages = 64kB) You can also find information about disk alignment on Microsoft's Disk Partition Alignment Best Practices for SQL Server. This document will also explain how to retrieve the block size with the command: 

Having the MDF/NDF and LDF files on 64k formatted drives can help improve performance regarding read ahead speed. 

This will relink the (Native) SQL Server Login () on your target instance, with the Database User () of the restored database. Alternative Seeing as is deprecated, you could achieve the same with the statement: 

For clarification RPO = Recovery Point Objective; the point in time that you will be restoring to in the case of data loss. RTO = Recovery Time Objective; the duration it will take to restore to the time defined in RPO. You might be able to answer your question yourself if I supply you with ample information and give you some ideas to ponder. Recovery Point Objective (RPO) Is the RPO defined by business really feasible? Will the business survive with an RPO of one day (or 24 hours)? Recovery Time Objective (RTO) Can you bring your database(s) back online to a point-in-time defined by the RPO? Please don't forget: The RPO is a duration. If your database crashed as 2pm in the afternoon then you have to be able to restore the data at least to 2pm in the afternoon the day before. (And not just to 6pm the night you took the backup). Transaction Log Backup: Yes / No? Seeing as you now know that RPO and RTO can be relevant for your backup concept, I can now answer your question about the database's Transaction Log. A differential and/or full backup of the database will not free the transaction log, if the database is in FULL recovery model. Reference: Misconceptions around the log and log backups: how to convince yourself (SQLSkills.com, Paul Randall) You must perform a to CHECKPOINT the Transaction Log and allow it to free up space should it be required. Existing Backup Concept If your RPO is for 24 hours data loss and the RTO is 4 hours (for one database system), how will you fair if you have to rebuild 10 systems or 20 systems? Will you be able to restore all 20 databases to a point-in-time with the given RPO of 24 hours and an RTO of 4 hours? Possible solution