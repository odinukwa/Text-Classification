Extend or adjust for data types as needed. Remove for each data type if you wish to allow null values. The only complexity is the constraint to enforce that the appropriate column is used. If values are mandatory, then the check constraint can be written as 

is purely a shorthand way to create an column with an associated sequence for its default values. The documentation for linked from the question even says as much: 

Then session 1 will be waiting for session 2 to commit or rollback the update on the row where pk = 2 while at the same time session 2 will be waiting for session 1 to commit or rollback the update on the row where pk = 1. Deadlock. 

sqlplusw was a GUI version of the command line tool sqlplus, with its own Edit menu etc. sqlplusw was removed in Oracle 11g. 

Be careful. Your two designs are not the same. Case 1 implements a unique constraint on the name of the book but includes case, so "The Lord of the Flies" and "the lord of the flies" would be different. Case 1 then creates a second index to support efficient searching of queries of the form 

If using Oracle -- and if you're able to run arbitrary statements against it -- then you could first sanitize the input using (at least, it's my understanding that this makes sure that the literal is properly quoted -- even allowing for quotes embedded in strings). See $URL$ for more details. 

Postgresql has many indexing options. It also has some powerful components built on top of these indexing options. One such feature is full text search. See $URL$ 

Which solution you chose is largely a matter of personal preference. I have a strong preference for the foreign key approach, and think I can provide good reasons why. First of all, check that your requirement is strictly to capture a single "primary supplier", and not (potentially multiple) "preferred suppliers". If you can't guarantee that you'll never have more than 1, the foreign key approach will break and you're better off going with the flag approach from day 1. If, however, a part always has a single primary supplier I would use the foreign key approach. If a part must have a primary supplier, then this is the only approach where the database constraints guarantee that the data is always correct -- make the foreign key column mandatory (not null) and you will have to provide the primary supplier when adding the part. There is simply no way that the data cannot be correct. With the flag approach, the primary supplier information can be missing from parts -- unless you want to rely on your application's logic. If that's potentially a problem, the mandatory foreign key approach is the easiest way to ensure this can never happen. Think also about updates. With the flag approach, you need to clear the flag from other "part supplier" records when you change preferred suppliers. 

While profiling a database I came across a view that is referencing some non-deterministic functions that get accessed 1000-2500 times per minute for each connection in this application's pool. A simple from the view yields the following execution plan: 

I have a table that stores version information in multi-dotted strings, ie '4.1.345.08', '5.0.1.100', etc. I am looking for the quickest way to find out which is larger. Is there a faster, or easier way on the eyes than what I have come up with below? One caveat is that I am 90% sure that we should ONLY find dotted numeric values here, but I can't guarantee that in the future. 

Say for example, if a defect is reported in this spaghetti mess, for example a column may not be accurate in certain specific circumstances, what would be the best practices to start troubleshooting? If debugging this was a zen art, how should I prepare my mind? :) Are there any preferred SQL Profilier filter settings that have been found useful in debugging this? Any good tactics, or is it even possible to set breakpoints in nested procs when using debug mode? Tips or suggestions on providing meaningful debug logging when dealing with nested string builders? 

It sounds like you need to add another column in your data table to account for the quantity of ingredient. In the example you linked, it appears that the site calculates how many grams each ingredient adds to the total sum of the final product. But it will be up to you to decide how best to generate these. For example, if your site is all about home made juices, perhaps you can come up with a formula to determine how potent or how much flavor a specific ingredient adds to the final product. But I digress. If you have an additional column for grams you could take the weight of one ingredient, calculate the SUM(Weight) of all ingredients in your recipe and divide the two together, like in this Excel example. 

You'll need to use a instead. If your data is indeed distinct, use a for better performance -- the step to eliminate duplicates is eliminated. 

quoted from $URL$ , Oracle allows you to use versions of Oracle DBMS downloaded from OTN for development "for free". 

Datapump is much more efficient than : Datapump runs within the Oracle server processes, and can read directly from the database files and write directly to a file on the server. As I understand it, data access is direct, not via SQL. uses the ordinary Oracle Call Interface (OCI) -- the library that Oracle provides to connect to the database. It can only run ordinary SQL statements like any other program. 

So doesn't require . Having is applied after the aggregation phase and must be used if you want to filter aggregate results. So the reverse isn't true, and the following won't work: 

But as others have mentioned, optimising expressions isn't going to help you; the Postgresql syntax is merely cleaner. You'll need to show us your execution plan(s) and table definitions for us to be able to help you further. 

One way you could do something like this on Oracle is to use a with . The rows will "disappear" at the end of the transaction. If you're using a host environment such as JDBC with autocommit, this will make it look like what you describe. If performance is important, this approach will be much faster than the "discarding trigger" approach. Syntax: 

You're interpreting the semantics of the delete statement incorrectly. When a clause is used, it doesn't mean that records will also be deleted from those tables. Instead, those tables are purely used to join to in order to determine which rows need to be deleted from . You basically have three choices: 

No. Duplicate entries are basically ignored. The documentation of describes how the rules work pretty well: 

Here is why s are being used as predicates. The column is formed by concatenating: During testing of these, a simple from the view returns ~309 rows, and takes 900-1400ms to execute. If I dump the strings into another table and slap an index on it, the same select returns in 20-75ms. So, long story short (and I hope you appreciated some of this sillyness) I want to be a good Samaritan and re-design and re-write this for the 99% of clients running this product who do not use any localization at all--end users are expected to use the locale even when English is a 2nd/3rd language. Since this is an unofficial hack, I am thinking of the following: 

Is there a best practice method to handle localized strings via a view? Which alternatives exist for using a as a stub? (I can write a specific for each schema owner and hard-code the language instead of relying on a variety of stubs.) Can these views be simply made deterministic by fully qualifying the nested s and then schemabinding the view stacks? 

Also, feel free to vent about your experiences with tasks like these. I'm sure I'm not the only one who has been handed down tasks like these. 

I recently inherited a MESS of a search. It's around 10,000 lines of code in the procs/functions alone. This search is aptly named the "Standard Search." It's a proc that calls about 6 other procs, which are entirely composed of string builders, in which each proc has between 109 and 130 parameters. These procs also call deeply nested functions which generate more strings to be assembled into a final query. Each proc can join up to 10 views, depending on the logged in user, which are abstracted from the table data by between 5 and 12 other views per primary view. So I am left with hundreds of views to comb through. On the plus side, it does have a parameter to PRINT out the final query, (unformatted of course!) It's a hot mess. The string builders also have no formatting, and they don't logically flow. Everything jumps around everywhere. Although I couid do an auto-format, to pretty print the code, that doesn't help with the formatting on the string builders, as that would involve refactoring the content of strings, which is a no-no. 

All I've done is move all the code into a stored procedure, and redefined the trigger to CALL that procedure. 

I don't have a specific term for the actual combination of invoking an external process from a database transaction, but I would classify this problem as tightly coupled. The root problem is that you have tightly coupled the sending of the email with the database transaction. A solution to this problem will be to loosely couple them. Technically, you could solve this in many ways, in rough order from ugly to nice: 

You can do this under Linux/Unix when you embed the SQL commands in a shell script and pipe them into a SQL*Plus instance. 

So the second and any subsequent duplicates will never be considered. So duplicate entires can not be the cause of your error. 

Yes, 'W' will do that -- replace the file. Use 'A' to append. I don't think you can "modify in place"; you might have to copy the file to a new one making the changes you desire, delete the original and then rename the copy. All of this begs the question why you're trying to do all of this in PL/SQL. It might be better -- certainly easier -- to do it in an external program written in eg Python and invoke that. 

If the two tables share ID space, surely then they should have a common parent table? I would create a parent table and make the two other tables children of it with foreign keys. The parent has the autoincrement field, and you have to insert there first before inserting the child. Add a "type" column to the parent table so you know in which table to look for the child with the given ID. The biggest advantage to this approach is that you don't need any triggers. No performance or synchronicity problems to worry about. In PostgreSQL you would implement this using inheritance, but I don't think MySQL has this feature.