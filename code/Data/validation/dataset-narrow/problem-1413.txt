You are thinking of pixels as if they are some atomic structure, indivisible; they are not. Take a look at the following diagram from the Direct3D documentation on multisample rasterization:    Various triangulated objects are drawn here and the diagram shows a pixel structure with 4 samples (at fixed locations) per-pixel. Take a close look at the pixel in the second row of the first column; the triangle only partially covers this pixel. If this diagram were rasterized using a single sample, then the color would be determined at the center of that pixel where the diamond shape is based on the triangle that covers it. However, this is 4x multisampling - the final (resolved) pixel color is the average of all 4 samples (one of which is not even covered by the triangle). The key concept you seem to be missing is the final resolve. You cannot display all 4 samples on screen per-pixel and not every one of the samples is covered by the same geometry. You can get very different results when resolving from n-many samples to 1 or even simply changing the positions of the samples. 

You are aware that when you sample the depth buffer, what is returned is in window-space, right? That is the final coordinate space OpenGL transforms into. After projection, GL applies the viewport transformation from NDC -> window-space, part of which includes depth range. Traditionally, window-space Z is clamped [0,1] and the full range is used; thus 0 is the nearest point and 1 is the farthest. Your discussion of the near plane at 0.1 and far plane at 1.0 leads me to believe you are discussing a different coordinate space (for instance, if you defined those values in your projection matrix then they are measured in eye-space). In window-space, given the default depth range your near plane is actually 0. 

That side force is the horizontal component of the road's normal with respect to the car's forward movement. Roads are banked like you are discussing to facilitate cornering at higher speeds without flying off the track from momentum pushing them to the outside corner. The banking pushes back in a direction the tires do not freely rotate and can hopefully resist; if the banking were steep enough, center of gravity high enough or speed slow enough, the car itself would act as a wheel and simply roll over. Tires have a limited range of steering. Forces acting perpendicular to the longitudinal axis (front to rear) are resisted in-part because tires will not rotate that way. You can compute the force acting sideways against the road from gravity and forward velocity and then calculate the remaining force after the tire scrubs some of this friction off. If the remaining force is great enough to overcome the force pushing the car into the road, the car will slide/skid. The following illustrates this and is discussed here (with and without friction):     The force acting horizontally is of particular interest here. At rest (only force acting upon it is due to gravity), the car is not going to slide down a slope running perpendicular unless the coefficient of friction is exceptionally low (e.g. an icy road). 

This will produce the desired end result, but it eats a lot of memory bandwidth... the very thing you were trying to conserve by using ETC1 texture compression in the first place. Honestly, the only performance-friendly solution is probably going to be to use ES 2.0 and shaders. 

The problem with using texture atlases and adjacent texels leaking has to do with the way linear texture filtering works. For any point in the texture that is not sampled exactly at the center of a texel, linear sampling will sample 4 adjacent texels and compute the value at the location you asked as the weighted (based on distance from the sample point) average of all 4 samples. Here's a nice visualization of the problem:   $URL$ Since you cannot use something like in a texture atlas, you need to create border texels around the edge of each texture. These border texels will prevent neighboring samples from completely different textures in the atlas from altering the image through weighted interpolation explained above. Note that when you use anisotropic filtering, you may need to increase the width of the border. This is because anisotropic filtering will increase the size of the sample neighborhood at extreme angles. 

First of all if you tried to do exactly that it would not work because blitting the depth / stencil buffer using is undefined. It literally lists this very situation as the first possible error on the manual page for : 

If you need separate stencil tests, you have 8-bits worth of stencil to fool around with. Your code is not accumulating stencil values, it only performs binary set/clear, so that means you can do up to 8 boolean tests per-pixel with the proper masking logic. Rather than testing for equality versus 1 using the mask , you can use a more sophisticated mask to isolate the individual bits. The mask and 2 for instance will test the 2nd bit, while and 1 will test the 1st. Right now you are using the entire 8-bit stencil buffer to do a single boolean test, and that is not going to help you at all. 

You are also wasting memory bandwidth by writing your normals using 32-bit floating-point. You can get away with 8-bit fixed-point most of the time, and when you cannot you should try an RGBA_10_2 or half-float (16-bit) format before jumping straight to 32-bit floating-point. Also, make sure to bias and scale your normals before showing them on screen for the purpose of visual inspection. They can have negative values, which will be outside the visible color range. The normal way of tackling this is to multiply by 0.5 and add 0.5. Judging by the name you're using for your texture ID, you probably meant to use an RGBA8 format for normals and pack the material ID into the A channel. In which case you need to write your normals the way I described above and then unpack them using: 

Even with VSYNC off, many games can fail to hit even 98% GPU utilization. The more actual gameplay they implement, the fewer frames they can stage and the more likely the GPU will go underutilized. Good multi-core optimized games can get significantly closer to 100% GPU utilization, but generally gameplay logic keeps the CPU busy enough with other tasks that it is not able to saturate the GPU with a full workload. Pure rendering applications can easily reach 100% GPU load, but games do a lot more than rendering. On a side note, on my home machine my GPU generates significant EMI under high load and it interferes with the cheap integrated audio on my motherboard. I can hear a high-pitch whining over the analog audio whose frequency varies with load. I have come to enjoy that and consider it a feature rather than a design flaw, it makes profiling interesting as I can actually hear the load level without having to sample a GPU performance counter. However, I suppose if you have some device that is highly sensitive to EMI and inadequately shielded this could be a problem... high GPU load could cause failure in another device. 

You absolutely do not want to create an array of floating-point uniforms. Some GPUs are not capable of storing uniforms as scalars, so each one of those may potentially take up the same storage as a . There is a limit to the number of uniform components GLSL programs have for any given stage, and you may be approaching it 4x quicker than you ought to be. You should really pack that into something like 3 . However, the real issue at hand is that what you actually want is either a Shader Storage Buffer Object or Uniform Buffer Object. You can use either in GLSL 4.40, earlier versions of GL only supported UBOs. I would honestly suggest using a simple UBO for this, you would not benefit terribly from an SSBO, but it is an option. 

The client and server have two different notions of visibility, by the way. The sever is generally going to avoid visibility checks using actual geometry and at the most naive level may not implement visibility at all. In past work, I have re-used parts of the graphics engine's dynamic PVS system to implement agent (AI) pathing on the server, however position update filtering is not something that would benefit from that sort of complexity. More than likely, relevant game state updates will be established based on a simpler metric such as distance. EverQuest sends position updates for every PC and NPC in a zone, but it varies the frequency of these updates for mobs that are distant from the player. In EverQuest, the client will resort to delta prediction much of the time to figure out where distant mobs are and that is usually adequate unless the player is moving extremely fast (e.g. on a boat). Even then, there is next to no gameplay that goes on at hyper speed so the lack of state updates will only affect rendering. So distance turns out to be an extraordinarily simple and effective metric. As a point of trivia, EverQuest did not always work that way. For about the first year of its existence it sent out updates in equal frequency regardless where the player was in relation to everything else. This made cheating programs like ShowEQ (which displayed a radar with every mob in the zone) extremely effective, and also did not balance bandwidth effectively. 

I assume you are interested in the alpha channel for the purpose of alpha blending. Off the top of my head, I can think of one solution to this problem, but it is not going to perform well. The fundamental problem here is that you have an RGB texture; it has a constant alpha of 1.0. You can use texture combiners to pre-multiply your RGB texture () by the other texture with alpha encoded into RGB (), but that only solves half of your problem.