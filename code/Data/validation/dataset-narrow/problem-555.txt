Next, the inner loop will find another -1 cell with no higher-numbered neighbors and create a second connected component, although there really is only one. Succinctness: If the code was correct, you might as well eliminate all the statements in the loop except for the one comparing against -1. The others are redundant in their behavior. 

Efficiency Improvement You said that you need a solution "without using extra memory". What you really meant of course is a solution "with only a constant amount of additional memory" (your pointers require memory as well after all). What you did not require, however, is that this operation has to leave the list elements' ordering intact. Therefore, a faster approach in would be to first sort the list (in-place to satisfy the memory requirement), then simply walk through the list comparing only neighbors (another ). While this does not make much difference of course for smaller lists, the memory requirement indicates much larger lists, and then the difference between and may well be the difference between another problem and a solution. 

By going through the first row first, you would create your first connected group with this intermediate result, before the row-loop switches to the second row: 

I would try to split into simple pure functions doing one thing each. Search for Robert Martin aka Uncle Bob advices on clean code. Is this just a helper to display an array: 

This property is either being created when it is defined, or else, the calls like will still fail. You may want to check for existence of that property instead when calling it. 

UPDATE. Having another look I see better what you are trying to achieve. Still I would improve design by making the directive more re-usable. You basically want to throw any HTML inside the directive and pick the associated with event's target. The directive makes this available on the scope of your expression . Just like made available inside the . I would then mark this as something special like: 

Nice work! I presume the complicated way of getting the right element measurements is to cater for many browsers, won't comment on that. 

You are re-defining , which is a bad practice. It will break the chain, forget all your previous properties and can lead to unintended effects if using (which isn't good idea too unless you know what you are doing). It is better to specify individual properties of it. 

This review is two-fold: once, in case you have to uphold your restriction and then what changes if you allow extra space. Review 

Style: Depending on coding guidelines and preferences you may want to consider moving the declaration of and into the for-loops (), as these variables are not supposed to be used outside of the loop, and hence, an outside declaration kind of contradicts this contract. Correctness: I'm afraid your greedy approach does not work out well - in other words: it gives wrong results. Consider the following matrix (or if you loop the other way round its symmetric variant) : 

You may want to consider using at least , as factorials tend to get large quickly. Whenever you write a function that you believe to be tail-recursive, then do add (from scala.annotations) to it. This has two major advantages: First, it corrects your thoughts and tells you immediately, if it isn't tail-recursive although you thought so, and second, it tells everyone else and stops them from making a "fix" that causes the function to no longer be tail-recursive. May not be a big deal for such a small one, but in general tail-recursive functions can be more complex and then it's much harder to see at a glance that the function is tail-recursive if you do not annotate it as such. Naming convention: The semantics of what you labelled is not really an offset as we would usually think about it. If you say most people have a certain meaning in mind that they associate with this term. The traditional meaning that resembles your semantics would be given by the term . Especially, when writing recursive functions we run into this intermediate-result-variable-thing very often and you will almost always see it referred to as an , so for clarity's sake you should just name the variable as such. 

You are rebuilding at every loop exactly the same base string, over and over and over again, what's the point of that? Your string is the same except for the serial number, so build it once outside the for loop and the reuse it. You don't even need to use a StringBuilder here, as in general it begins to be faster when you are concatenating more than 3 strings, otherwise plain normal string concatenation is better. 

It's just 6 milliseconds for each barcode, including save to disk: it's not that slow at the end of day, and you have a custom library doing most of the job, so that means that you really have to squeeze every single possible fraction of millisecond. In every loop you already know the size of the final string (just add the length of each single part of if), so when instancing the StringBuilder use that number to set the initial capacity. Or, just in case, set it to the maximum possible string length and see what's faster. Use a parallel for. If this thing is running on an average working PC, let's say an 8 cores, going from 6ms to 1ms could be a plausible scenario. Even taking into account that you need to instance a new encoder for each barcode, I'd say that it's realistic to at least expect the final time to go down to an half, even a third. Give proper names to loop variables. 

In that case, is a function argument, so using it won't throw any error (in contrast to a variable) and hence there is no need for . Also note that evaluates to , so the two expressions are different, strictly speaking. 

I am puzzled by how you use both inside and outside . Each iteration of has its own scope, so its own . For instance, if , which item do you mean to use inside ? The way it is used now, the seen by that function will be another on the outside scope. If this is the intention, this variable should be named differently: 

This keeps my code DRYer and I don't need to worry misprinting or forgetting to return it (your does need a !) You don't handle errors (but you knew this already ;-) Your has way too many dependencies and responsibilities. The best practice is to keep your controllers "thin" with the sole responsibility to glue your data with its scope. Changing routes (states) looks like one job too much. Say you want to keep another sing-up form elsewhere with its own controller - will you really want to copy over all the logic? This is good for prototyping but is generally to be avoided in production: 

So, one of the thing here is that the CPU is waiting for your hard disk to provide data, and then the hard disk is waiting for the CPU to ask for something. The first and more obvious point is that accessing disk to get a few bytes each time is terribly inefficient: disks (HDD or SDD is the same, in this case) are best for sustained reading. The OS cache will help you so that it will read a bunch of data ahead for every request you make, but you know you are going through the entire file so you shouldn't rely on the cache and try to be as efficient as possible in your code. In fact, as rolfl points out 

The content of the Parallel.For will be executed in multithread, so you need a unique instance of every variable you modify and for every class you call a method of, so keep an eye on the 

First and foremost, Phyton is not my language of choice (or in other words, I don't know it at all), so I'm just trying to add something regardless of the language. Yes, you would benefit a lot from multithreading here, it's quite a basic case. I give for granted that Phyton does nothing to complicate using of multiple thread, obviously, because doing multithreading itself is cheese task even when using the basic OS APIs. As a general rule of thumb, unless you are dealing with a really fast computation where the additional overhead added by threading will worsen the performances, there is no point in not using parallelism everywhere. It's one of the easiest things to do in computer programming, and it allows you to "choke" the hardware to the point that no matter what, you are granted to be going as fast as the PC allows. 

which is, however, blocking, may result in delays, and makes the page unsuitable for offline testing. On another note, library such as async loader, does not seem to work with this particular script. 

Then it is the job of the directive to pick the right model and make it available under this name. This way I can change to inside without breaking things, so the directive is more encapsulated and re-usable. 

Warning. Keep in mind using inside HTML, that your code becomes invalid as soon as those s are not unique. This can easily happen by copy-paste and forgetting to change the . Unless you use reliable validators, and consider this a feature to help your validators, I would avoid using s whatsoever. Styling can be done merely with classes and is a recommended way. 

Note. Your code looks cleaner without after function declarations. You watch and then update it upon change - this looks like a loop and may lead to your browser crash. Read best practices on using Angulars . 

Again, pure functions without side-effects are easier to test in isolation. Use the and methods on arrays for shorter and simpler code and libraries such as Ramda (my favourite) or Lodash. 

Edit Sorry, I was again thinking about point 4 above, got struck by a doubt, checked again, and here you are doing something that make no sense at all: 

190 MB/s is ridiculously low for an hard disk, unless is a 5-8 years old cheap model; right now even an SD card can be faster than that, sometime. From a decent SSD I'd expect at least twice that speed, and today even SSD in the range of 100$ can easily saturate the SATA interface. That is, you want to read big chunks of data every time; no matter what, don't read a line at time. There is no magic number, but unless the computer has serious memory issues 100 megabyte each time should be more than good. Now, problem is, while waiting for data the CPU is sitting idle doing nothing. Then, while the CPU is crunching your data, the disk is sitting idle waiting for something to do: needless to say this is time for some free multithreading. Deciding what to implement is a bit complicate without knowing the exact details and limits of the project, because you can simply have a number of threads equal to the number of cores, each one working onto an equal part of the file (easily doable in an hour total), or going to write a central dispatcher that read chunks of the file, create threads up to a certain limit (maybe doing some throttling), and collecting the results (and this can take up to a day of work). It all depends on money and time you have available to do this but, yeah, go for it.