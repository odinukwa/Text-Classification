You apply for a job, same as any other industry. Prepare a portfolio of your best projects and code and send that with your resume, if asked. There's nothing special about games. 

Your approach is reasonable. A simpler alternative would be to use canned sprite states for each tile, representing various levels of "dug out"-ness of the tile (100% full, 80% full, 50% full, all the way to an empty tile). But it sounds like you want something more dynamic, so you're on the right track. When you implement this kind of technique it's important to note that you do not need to store the entire world at the small (15x15) tile resolution. You can store the world as the larger (45x45) tile size (or, indeed, larger if you wish. This means you have few tiles to track in-memory, particularly for dealing with the AI pathfinding, which can be optimal. When your player hits a tile, you break the tile down into smaller pieces then and there, and being removing them as the player continues to collide with them. This hierarchical approach will help in terms of the number of individual sprites and objects you need to track (as well as with the pathfinding as detailed above). In fact you can leverage this to further optimize pathfinding by only having the AI consider a 45x45 tile passable if some certain percentage of it's smaller 15x15 tiles (say 75%) have been removed. In general the hierarchical approach will probably scale better -- you can use an even smaller "digging" resolution of maybe 5x5, or several layers of hierarchy if you want, and you won't suffer the performance impact of keeping your entire map at 5x5 resolution. Only the parts where the player has dug the map out. 

is not a timer. It's an object representing timing information sampled at one particular point in time (from something that is a timer). If you need a timer, consider using the class. Using a is very straightforward: 

XNA 4 does not support point sprites, which is (I believe) all that PointSize is for; for some rationale, see Shawn Hargreaves' blog post on the subject. The short version is that D3D10+ does not support them, and continuing to expose them in XNA restricts options for eventually moving XNA's underpinnings to a newer D3D version. While it's not a decision that was met with universal praise, it's not the end of the world because you can simulate the effects yourself. It just requires more work. Fortunately, there are third-party solutions to do so that already exist -- something like the RoundLine library might serve your needs, or you can see this blog post about a similar technique (probably actually the same one RoundLine uses). 

Since s are just numbers, it's perfectly possible to encode the image data directly into one's source code as follows: 

This is basically what you want to do. Load the texture data from a worker thread, and once the load is complete construct the OpenGL texture resource (from that data) on the main/render thread. Once you call , you don't need to keep your copy of the texture data in-memory any longer (you can delete it) unless you need it for some other purpose. OpenGL will copy the data it needs elsewhere. 

It says right in the quote: to calculate the effect of lights in the scene (how much light falls on a particular fragment, what color is it, et cetera). Computing the effects of more and more light sources is something that forward rendering handles badly; it scales very inefficiently. Deferred rendering alleviates a lot of that computational pressure, making it significantly easier to throw a ton of lights into the scene ("lots of lights" is probably, at least in part, what is meant by "complex lighting conditions"). One reason to mix deferred and traditional rendering pipelines in this fashion was to allow the use of the deferred pipeline for what it was good for (lights, lights, lights) and the forward pipeline for what it was good for (transparency, which is not readily accomplished with a deferred system, especially early on in the technology's use). 

At the very basic level, you probably want your plugins to be able to provide new tools and new commands (alternative save methods, for example). You can implement this kind of extensibility, along with any other similar extensibility, in the same fashion. You define for your system an interface that a plugin must conform to. This should at a minimum include hooks that the editor will call to load and unload the plugin. Pass, to the load function, references to all the extensibility interfaces you have supplied (or an interface that allows you to locate those interfaces). For each extensibility interface, you want to have a related interface that corresponds to the new thing your plugin may be adding. For example, you might have an interface that defines the properties of a tool the user can select from a palette and use to manipulate the scene. Or, you might have an interface that's used to populate your menu bar with options. The details of those interfaces are up to your particular needs and implementation. The extensibility interface provides methods that allow the plugin to install new instances of , or what have you. For example, you might pass your plugins an instance of the class: 

You can't really do it without some kind of preprocessing step -- either authoring the original sprites so they have the desired effect, or including some extra information along with each sprite about pixel "depths" and normal information, essentially turning your 2D sprite into a 3D-ish data structure. The cel shading effect is designed to make 3D art look like hand-drawn 2D art. It works with the principle of color quantization, most of the time -- mapping a smooth continuous input range into one of N discrete color buckets. Usually the diffuse term of your lighting equation -- the "n-dot-l" term, or the dot product between a vertex normal and the light vector -- is used at the input because this will range smoothly from 0 to 1. You can use this as input to a 1D texture read where the texture contains information about the actual color to apply. It usually is just a band of a small number of discrete colors. Obviously this will be hard to do without that input value, and without encoding additional information -- such as pixel normals or depth (with which you could in theory reproduce the normals at runtime) -- into the sprite you won't have any meaningful data to use as the input. Unless your sprites are being authored in a 3D modelling program where you might be able to write some export script for producing the final renders with encoded normal information (like a normal map, essentially), you are likely better off just drawing the sprites to look the way you want. The closest you could probably get would be to write a shader that maps the [0..1] range of every color component into one of a fixed set of buckets -- say 0, 0.25, 0.5, 0.75 and 1. This could be done independently for each component or on a global basis. It essentially massively reduces the color space available to your sprites, which is more-or-less what cel shading does. But unless your sprites are authored to account for this it's entirely possible that you will spend hours tweaking the mapping and still end up with sprites that look hideous in practice. A render effect that is typically applied with cel shading is feature edge detection and rendering -- "outline" rendering. It would be possible to do this with 2D sprites, but you'd only get the outlines of the sprites and no interior feature edges, which does not look nearly as good. Once again, to achieve a better effect you'd be preprocessing the sprite data or just authoring the edges directly into the sprite images. 

Right now you declare a as a member of this game class, and whenever you get a touch input you call , giving it that bullet. All this does is add the same bullet over and over. I'm not super familiar with the details of the APIs you are using, but I suspect that's basically a no-op. What you want to do instead is create a new bullet every time you get a touch event: 

No, it's fine, you may use whatever coordinate system and handedness you prefer. Just be aware of what you are doing so you can make the appropriate adjustments and assumptions. 

Constructors and destructors in C++ are about the lifetime of the object. Your problem is that you are mixing up the lifetime with the desired behavior of the object. The solution is to decouple them. Give the appropriate entities an "OnDestroy" function, which is called by your game logic when the entity should be (according to the rules of the game) destroyed. Leave your destructor to do what it should do: anything necessary when (according to the rules of the language) the object itself should be excised from your program's memory. Then you create your explosions in OnDestroyed. When you transition between game scenes, that causes the entities to be deleted, cleaning up and invoking the destructor, but it doesn't need to call the OnDestroyed method, so no new entities will be created. 

They're usually stored in GPU memory. In some cases, when the GPU has to evict them, they will need to be restored from CPU-side memory, but this is "uncommon" unless you are really thrashing the card. You should never see them populated from CPU every frame under normal conditions unless you are also updating the texture CPU-side every frame. 

The result of a verification test that concludes an action is potentially malicious should not necessarily be to ban the originator of the action. Instead you should increase a "cheat score" (which may decrease over time), such that users who hit a cheat score over a certain threshold are flagged for a more in-depth review (possibly by a human). It's important to make these verification checks highly tunable, especially to correct situations where any long-running feedback loop has crept into the tests in a way that starts generating false positives for cheat score increases. You may also want to consider not verifying every action every player does all the time, but randomly selecting them or other distributing the processing to avoid wasting too much server CPU validating input instead of actually serving the game. There isn't a hard-science answer to this problem, it's going to involve lots of iteration on your part and tweaking of fairly fuzzy variables. Having good metrics and good tunability is critical. 

Basically, what I said above: don't use a runtime associative array for component attributes by default. If your scripting system needs that functionality, provide it via the components bound to the scripting system. 

One way to handle this problem is to introduce the concept of authority over an object. It's particularly useful in games that aren't backed by a dedicated server process (if there is a dedicated server, just make the server the authority for the ball's simulation). The client that has "authority" over a particular object is the one responsible for the simulation of that object that everybody else eventually sees. Even if each client does some local prediction of the ball movement (which they probably should), they will ultimate defer to the authoritative corrected state of the ball sent to them from the client with authority over that ball. Grant authority to a client when that client first interacts with the ball in some fashion. That client retains authority over the ball until some other client touches or interacts with it. While a client has authority, it's the one responsible for periodically sending updates to everybody else. You'll need some form of consistent timestamp or token mechanism to resolve who wins in the event that two clients interact with the ball at approximately the same time. 

I don't think you need an entity system for this at all; entity systems work well for gameplay-related code, where they are influencing behavior or properties in some fashion and can allow for rapid iteration in a very large way. It sounds like your geometry packaging system (for lack of a better term) doesn't need that kind of wide flexibility, and in fact you wouldn't want to allow it since that could destabilize performance. I would instead implement the system on its own, distinct from any kind of entity system, taking your requirements into account (which appear to be efficient storage of the shape data for both transport and utilizing in OpenCL for deformation computations, et cetera). Once the system is in place, you can expose it to your gameplay objects via a component that acts as a proxy for the geometry package. Such a component would essentially be a dumb wrapper around a reference to one of these geometry packages that is itself managed by the geometry packaging system. This will let your entity system refer to and take advantage of that system without complicating that system's own design with the flexibility of the entity system. (Incidentally, I think this is how one should generally approach interfacing physics and rendering systems with entity systems as well.) 

Further, you can see here that the inclinometer readings are actually derived from other hardware sensors: