I'm running Ubuntu and Postgresql 9. I've enabled postgresql to log to syslog, and added the following directive to the config file: 

I'm running Apache, and trying to monitor it wiht Monit. The monitoring seems to work fine, but when Apache is down, Monit does not succeed to start it. Monit does detect that it's down, but the start command fails. Below is the monit config 

I've tried to configure syslog to log those to a separate file, but that filed. I updtated /etc/syslog.conf so it contains (see last line for the postgresql-directive) 

I've restarted syslog with "/etc/init.d/sysklogd restart" and with "restart rsyslog". When postgresql is reloaded, the messages are appended to /var/log/messages and /var/log/syslog ... and not to the correct file as I configured syslog to use... The same works fine on an other Debian machine with the same version of Postgresql... any ideas? 

I have a SQL server running on Windows 7. Downloaded PHP and uncommented the mssql-extension. Now Apache fails to start up. When I comment the mssql-extension again, it starts up fine. I did some research and tried copying ntwdblib.dll to system32 dir, and to the apache/bin dir, but that doesn't solve the issue. Any ideas? 

Why in the world would you run things off of CF cards? Use solid state media (meant for the purpose) if you need flash storage. CF cards are not made with technical standards to include monitoring. The most you can do is a checkdisk and check it for bad sectors. 

In fuzzyfox config, check you allow "ssl 3.0" and the likes. Otherwise post the "view source" of the page in firefox that doesn't load, and any error logs you have. Good luck. 

Just be sure to uncomment mod_proxy and its reverse from the apache modules section of the config. And change the 127.0.0.1:port to the port and local IP of the "game boxes" And for the port thing, all of these requests are served on the external IP/port of the apache server. If you are trying to serve non-HTTP requests (a real game server) then you are better off using a NAT or DNS load balancer for this, and the client should connect to the default port. (Example: connecting to a minecraft server at mc.domain.com with no port specified, assumes port 25565. Http traffic assumes port 80, ssl is 443, etc.) 

I do not believe this is possible as the name servers stored in the domain name do not have any given priority. The client will use the name server that it is given. 

We did a lot of Moodle performance optimisations where I used to work and used PHP Xdebug profiling to help out with a lot of it. By enabling profiling, it will produce a *.cachegrind file which is then readable by many readers - I use QCacheGrind on Windows, or KCacheGrind on Linux. You should then be able to pinpoint the function which is taking its time. From memory, it is probably the amount of course data that it's trying to load when the user logs in. If there is too much data, and it's grabbing all of the data from the SQL server rather than only whats necessary, this could slow things down quite a bit. This can also be identified using the slow log as mentioned in a comment above, providing that its the SQL query that's causing the performance issue. I would also definitely advise upgrading your Moodle instance to something newer as I know there were a lot of performance fixes pushed across to the upstream repository since Moodle 2.2.3. We used to push these over to Moodle as an official Moodle partner. 

This will surely depend from person/company to person/company. I'd expect that if you explain it well, and if you can limit the requirement to have "exec()" available to the installation process, you'll be able to convince most of them to enable it. Also, make sure to clarify how they can do it in your manual. I'd only mention all this info about the requirement to have the exec() available after your install script has checked and found out it doesn't have the function availabel. 

I'd like to install the new PHP 5.4 on my Windows machine. I'm running Apache 2.2 on it. Reading the info on $URL$ it says "If you are using PHP with Apache 1 or Apache2 from apache.org you need to use the VC6 versions of PHP". I guess I'm in that case, I just donloaded Apache from apache.org. However, there are no VC6 versions available of PHP 5.4, not even of PHP 5.3. How can I install PHP 5.4 on my Windows machine then ? 

This script sets a couple of params which seem to be required for this specific Apache/PHP setup. Works fine when run from the commandline as root, but apparently, for Monit, it's somehow a problem. When running monit in debug mode (started with "monit -Iv"), I get the following 

In a business sense, and if there a lot of users, first initial then last name. Not dots. jkerry or gbush or bobama or such. 

First, you should limit all of that code to your httpd.conf file, not htaccess. More of a security threat, since people can download it freely and use it against you. As for the failed redirects... do the destination pages not exist EXACTLY as quoted in the rules? 

You are using SMTP right? Sending mail through a program using gmail servers should show any specified FROM address (easy to fake) but also the address of the account you used. Agreed, please post the "original" message with headers. 

This means, wherever the virtual host's files are taken from (root directory) you should put that specific favicon.ico file. 

You can't do that without vHosting. If you can wait till 4pm eastern, I'll give you the code to do so. I'm not around my server now. 

You could just install it with Xen without changing the ISO, then edit the files you need after it's installed, then make a clonezilla image for future use. 

You can try this, just add it to the index page if it's PHP, or better yet, pop it in the "Header.html" file as a PHP function. Every time it loads it will give you the user's IP, referrer, user agent, and page viewed. Crudely, save this where it needs to go, for one hour, then cut and paste the file it generates (access-log-php.html) to a place where it won't be edited. You can look inside and count how many hits. Alternatively, set up a PHP SQL function where it increments by one, and stores to a table on a database, for one hour. PHP is the way to go, though. 

I just tested this and it looks like the GitLab API response is using pagination. According to the documentation ($URL$ the default number of results per page is set to 20 and the starting page is 1. To adjust the maximum results per page, you need to use the variable in the HTTP request line. You can change the page number by using as well, if you have more repositories than the maximum value of . You can specify a maximum value of 100. For example, you request may look like: 

If it's not set before it reaches this location block, then it will set it to a blank string. You can just as easily add a string between the quotes. I do not get any errors when doing a configuration test using this. Please let me know if you're seeing otherwise. 

Rather than having a location block for each redirect, you can always just add rewrite rules into an existing location block: 

Your research is right. You cannot redirect with a CNAME because DNS does not work this way. You would need some sort of web server to accept the request and rewrite the URL to HTTPS. Some domain registrars allow you to do a redirect on their web server if you use their name servers. DreamHost may provide this service but I have never used them so I cannot say. 

Using standard https to view files on the server, does that generate any ssl error messsages? If so, it means your certificates are not set up properly. If there are no errors, I have no idea what's goin on unless there is a secondary level if restrictions for creating them. 

Maybe this would help? $URL$ or this: $URL$ EDIT: And .htaccess files are supported by most server types. Well, any server if they are configured to look for them. 

If your PDF files are mainly text based (few to no images) and your server has a lot of traffic accessing the same document and can handle the compressing , then yes, mod_deflate or mod_gzip is a good idea. If you're unsure, try it out first, and if the performance goes down, just undo it. I've been serving all content except images and video with mod_deflate for over a year now, and it's cut my bandwidth to under half (Lots of text documents and scripts). Also consider looking into a cache system if not already, as this will really reduce the workload on the server. 

You don't want to set your INDEX page to a 404 page, otherwise it will not load. Simple. Try this: If you want to have a www-redirect, make a v-host with just your external IP. 

The Amazon free tier is only valid for 12 months since creation of your account and it has quite strict guidelines in terms of what you can do - if you've stuck by these and still being charged, it's best to contact Amazon billing and ask them why. It really depends what instances you're running, what AMI you've used to launch the instances, how much disk space you're using, how much I/O activity you have going on the servers, how much bandwidth you're using and if you're using other Amazon services. Without being able to see the activity statement, it's hard to analyse it. Usually if you give the Amazon billing department an e-mail, they will be able to help you out quite well. If you have anything more specific about the usage though, I'm happy to help you answer it. 

I recently setup a Git server where I work which required SSH and HTTP(S) access for public and private repositories. I first setup the entire thing manually which took some time, but then I came across Gitlab which seemed to solve all my problems - it's pretty much Github, but your own private hosted version. I'd strongly recommend using Gitlab over setting everything up manually, given that it has so many great features and is easily manageable. There's a great tutorial here: $URL$ However, if you do want to run Git through HTTP(S), you will need to setup Apache or Nginx to use the git-http-backend binary which does all the work. There are plenty of tutorials online depending on which configuration you decide to go with. 

In windows, I think if "domain.com" is mapped to localhost, it is independent from sub.domain.com. You need to specify every single subdomain in the hosts file for it to work. 

Try that. You'll need to put it in each article folder in a .htaccess file, since each article folder has a different URL. 

If you want it on a usb, try making a .img of it and using a free program online to burn it to a USB drive. ISO files don't like booting from USB. 

Notice the two different document roots. Also notice that they listen for a specific domain. For the other part of making all subdomains go to x.y.com, use mod_rewrite: Pop this in the .htaccess file: 

You have to delete the entire "xampp/htdocs" folder (well, only what's inside it). Can you post your "error.log" file please? This is my configuration: 

Nameservers are set within the network adapter itself. 2 computers on the network can have 2 different name servers. One could even use the other as a name server. To point a domain to an IP, you need to A) have bought a domain name and be able to set name servers within, if you are hosting the server on the domain name its pointing to; or B) use a custom DNS service like "$URL$ and set up A-records to point the domain to an IP. Email me "Support/@/u4ik.us" for free DNS service. I run a few DNS servers. 

You can't map a port on a public IP address to more than one private IP address, it just won't work because how is the router supposed to know which one to go to? If you're using name-based virtual hosts, you could achieve this by sitting a HAProxy instance in-front of the web instances and direct all traffic from the router to the HAProxy instance. On the HAProxy instance, you create a front-end and specify the domains and the backend to use. Then, depending on which domain is accessed via HTTP, it forwards the request to the appropriate back-end to serve the request. I do it all the time when I want to conserve server or IP resources. 

It's probably not running because it's unable to find the binary. Without any error log though, that's just an assumption I'm making. Try adjusting the part in your script to use its full path. I'm not sure how you installed (either via yum or manually), but you can find out its full path by running . 

Based on your location blocks, files ending with '.php' will only be passed through PHP. So if you have an actual file in the '/images' directory, it should serve it as a static file, or otherwise return a 404 error. If the file ends with '.php' and is in the '/images' directory, it will run through both location blocks in order. Should it do otherwise?