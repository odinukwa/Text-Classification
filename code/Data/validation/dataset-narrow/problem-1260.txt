The de Moivre-Laplace Theorem shows that variables like $|T\cap S_1|$, after being suitably normalised and under certain conditions, will converge in distribution to a normal distribution. That's enough if you want constant lower bounds. For lower bounds like $n^{-c}$, you need a slightly finer tool. Here's one reference I know of (but only by accident - I've never had the opportunity to use such an inequality myself). Some explicit lower bounds on tail probabilities of binomial distributions are given as Theorem 1.5 the book Random graphs by Béla Bollobás, Cambridge, 2nd edition, where further references are given to An introduction to probability and its applications by Feller and Foundations of Probability by Rényi. 

Any #P problem, or even #P/poly, can be written as a polynomial: make a circuit out of NAND gates, write these as $1-xy$ where $x$ and $y$ are 0-1 valued integers, and sum over all inputs. This gives a polynomial in $\mathbb{Z}[x_1,...,x_n]$ for inputs of size $n$. The decision problem is testing whether this is 0. 

This theorem from that paper of Sinclair and Jerrum is slightly stronger than the sentence quoted in the question, and gives the fpras mentioned by Dyer: 

Yes, a polynomial time algorithm was first given in: Neil Robertson, P. D. Seymour, Robin Thomas. "Permanents, Pfaffian orientations, and even directed circuits." Annals of Mathematics 150.3 (1999): 929-975. arXiv edit: Actually, according to the acknowledgements section of the above paper, the result was first obtained by McCuaig who later published it as: William McCuaig. "Pólya's Permanent Problem." Electr. J. Comb. 11(1) (2004) $URL$ 

The correct upper bound is the sum of the $2^k$ most central binomial coefficients: $$|\mathcal{F}_B|\leq\binom \ell {\lfloor (\ell - 2^k)/2+1\rfloor} + \dots + \binom \ell {\lfloor(\ell + 2^k)/2\rfloor},$$ or just $|\mathcal{F}_B|\leq 2^\ell$ if $2^k\geq \ell+1$. The sets $\{B\cap C\mid C\in\mathcal F\text{ and }A\cap C=A'\}$ are antichains. By the LYM inequality the union of $2^k$ antichains cannot be larger than the sum of the largest $2^k$ binomial coefficients. To achieve the bound, let $A=\{a_0,\dots,a_{k-1}\}$, and let $$\mathcal F=\left\{C\subseteq A\cup B \mid \lfloor(\ell + 2^k)/2\rfloor- \sum_{i\colon a_i\in C} 2^i=|C\cap B| \right\}.$$ 

Here is a suggestion for how to show it's NP hard. I don't know if this works or not. First, consider the same problem on multigraphs. NP hardness may be easier to prove there. Try reducing from cubic MAX CUT which is NP hard even to approximate (Berman and Karpinski "On Some Tighter Inapproximability Results"). Divide each edge into two and at each of the new degree-2 vertices attach a vertex with self-loop. Does your maximum partition correspond to a maximum cut? -- Here is a bit more explanation. (1) The problem of maximizing (number of sources+number of sinks) over all orientations of a cubic graph is related to MAXCUT by some linear function. This requires showing some corersspondence between maximal cuts and sources-and-sinks-maximal orientations. In one direction, in a maximum cut (U,V), we can orient all edge from U to V. The internal edges E(U) form a matching, so these can be oriented arbitrarily and similarly for E(V), and the total number of sources and sinks is some linear function of the size of the cut. In the other direction, given a sources-and-sinks-maximal orientation, the partition U=vertices of in-degree 0 or 1, V=vertices of in-degree 2 or 3 gives a cut. (2) In the edge-bisecting transformation I described above, in an optimal configuration each loop is coloured the same as the edge next to it, and w.l.o.g that edge is coloured the same as the some other (non-loop) edge next to that. So each bisected edge has one colour coming from its attached loop and one other colour. This corresponds to an orientation and (1) applies. 

Using a pairing function we can allow $s$ to be defined on $\mathbb{N}^2$. Let $s(i,j)=1$ if and only if the $i$'th program $P_i$ halts in at most $2^j$ steps. Then $V$ can test whether $P_i$ halts by testing $s(i,1),s(i,2),\dots$. If $U(P_i)$ halts in at most $2^j$ steps then $V(P_i)$ halts in about $(i+j)^2$ time, assuming the pairing function is sensible. This means that if $U(P)$ halts in $t$ steps then $V(P)$ halts in about $$(2^{|P|}+\log t)^2\leq \max(4\cdot 2^{2|P|},4(\log t)^2)$$ steps. But $(\log t)^2=o(t)$. 

Tensors (in this sense) are just arrays of numbers, so I don't think you'll find tensor free surveys unless they're completely free of calculations. The "$T^{\otimes k}$" operation formalizes both the operations of changing basis and attaching gadgets to each output node (in fact I like to think of a change of basis as a sort of gadget operation). Let $\Gamma$ be a generator matchgate with standard signature $M_{i_1i_2\cdots i_k}=u(\Gamma)$. This an array of $2^k$ numbers. The signature in a new basis is given by $(T^{\otimes k}M)_{i_1i_2\cdots i_k}:=\sum_{i_1',\cdots,i_k'} T_{i_1i_1'} \cdots T_{i_ki_k'} M_{i_1'i_2'\cdots i_k'}$ where $T$ is some two-by-two matrix descring the new basis. Let $\Gamma'$ be the matchgate formed by adding $k$ new vertices to $\Gamma$, taking these to be the new outputs, and connecting them to the old outputs by an edge of weight $x$. Then the new signature is $C^{\otimes k}M$ where $C_{ij}$ is the matrix $\begin{pmatrix}0&x\\1&0\end{pmatrix}$. If you then perform the change of basis $TC^{-1}$ you get the signature $T^{\otimes k}M$. 

If you restrict to decidable languages, then the decision procedure for the language would solve the Halting problem. If you don't restrict to decidable languages then the language of Turing machines that halt on any input is a counterexample (in classical logic). 

If $\text{#PE}$ is closed under decrement then $\text P=\text{NP}$: consider $\text{#SAT}+1$. Conversely if $\text P=\text{NP}$ then $\text{#PE}=\text{#P}$ is closed under decrement because we can always find an accepting path if one exists then ignore it. I don't know whether $f\in\text{#PE}$ always implies $f_{-1}\in\text{#P}$. Your $M$ might not provide an accepting path for us to ignore. 

Here is an example complementing Harry Yuen's answer. For a counter-example, it suffices to define appropriate $X,E$ and show that any large subset $E'\subseteq E$ must have a low probability co-ordinate of $E$ - a low probability co-ordinate of $E$ is necessarily a low probability co-ordinate of $E'$. Also, I'll ignore the condition about entropy - appending $N$ independent uniformly distributed random variables to $X$ (and taking $E$ to $E\times \Sigma^N$) will increase $H(X)/(n+N)\log|\Sigma|$ to nearly $1$ without affecting whether such an $E'$ exists (I haven't though this through carefully). Here's the example. Let $X$ be a random element of $\{0,1\}^n$ such that every vector with Hamming weight $1$ (i.e. vectors of the form $0\dots 010\dots 0$) have probability $(1-\epsilon)/n$ and the all-ones vector $1\dots 1$ has probability $\epsilon$. Let $E$ be the set of vectors with Hamming weight $1$. Consider a subset $E'\subseteq E$. If $E'$ is not empty, it contains a vector of Hamming weight $1$, say $100\dots 0$ without loss of generality. But $\Pr[X \in E'|X_i=1]=\frac{(1-\epsilon)/n}{(1-\epsilon)/n + \epsilon}$, which is less than $\epsilon$ if $n$ is about $2/\epsilon^2$.