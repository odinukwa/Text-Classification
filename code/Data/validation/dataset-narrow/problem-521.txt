Huh, well that kind of sucked. At 5.2 cycles per candidate it's a bit slower than the algorithm in the OP. For one thing it still has ~9% branch mispredictions. It turns out the main culprit is this line: 

This is just a "saturating" shift, which returns zero if the shift amount is 64 or more3 and which compiles to a branch4. It also turns out that the loop has no less than two very slow division instructions every time around, coming from the two operators in this line: 

Each bitmap is "normalized" such that the LSB is always 1. Then the loop is very simple: it loops over all 30 primes, and shifts the bitmap by the right amount to "align" it, and ORs all the results together. In this way, the loop handles 64 candidates. The shift amount is simply the amount need to correct stitch the bitmap from the last iteration so that it is periodic. Using a 16-bit example for sanity, the bitmap for in binary is . In the next iteration, you can use the same one since the effective 32-bit bitmap would be . Oops, two adjancent 1s! You just need to shift it over by 1: and now it stitches fine. In general the stitch amounts for any prime have period and take all the values between and inclusive. The last line in the loop calculates them. Let's try this guy: 

We are down to 0.16 cycles per candidate! That's fully 25 times faster than the original algorithm, and if you measure it by cycles per prime, we are finding a prime every 1.44 cycles. Unless you are doing almost "zero work" per found prime, it's very likely that the other work will start to dominate here. Further Optimizations If you are so inclined, this can still be made much faster, probably by a factor of 5 at least. Of course, before you pursue that, you would need to benchmark your full application, since it is highly likely that the unspecified work you do per prime is what is slowing this down now. Minor Optimizations The loop above directly admits some minor optimizations. For example, which counts off the 30 primes could be inverted so that it counts down to zero (or from -29 up to 0) allowing use to remove the check at the end (we use the flag from the prior instead). The could be changed to a 3-argument , avoiding the prior , or this whole calculation could be removed by using the induction counter instead by making the row size of the two involved tables and consistent (right now one has an inner dimension of 128 and the other 190). These may shave another small fraction of a cycle off of the existing time, but the ones below are much bigger. Larger Contiguous Reads The above algorithm reads uses on two consecutive registers worth of data (64 bytes) from the calculated index. It is in fact the slightly bigger brother of the not-shown variant, which only reads one 32B value in the inner loop. That guy ran at 0.27 cycles/candidate, so just doubling the read size in the loop nearly doubled the speed. It's easy to see why: it only took one extra instruction to do that, while the other 12 instructions in the loop are pure index calculation overhead which are now doing double work. So by increasing the loop by one instruction it does double the work. You can just carry this idea to its logical conclusion, reading 4, 8 or however many values per loop. There is no particular reason it has to be a power of two, either. These will give very fast and easy speedups: I guess it is easy to get below 0.1 cycles/candidate using this approach. The larger reads come at a size cost for the - larger reads mean a larger table6. This optimization is probably the best and easiest one if you want performance. The code is already kind of half-generic. I call this "unrolling horizontally" based on my mental model of each prime being a long horizontal bitmap, with primes stacked vertically one above another. So the is accumulating in vertical slices (column-wise) and this unrolling moves in the horizontal direction. Unroll the Inner Loop This is the "usual" unrolling and the counterpart of the horizontal unrolling discussed above. Currently the inner loop iterates over all 30 primes. This loop has a fixed trip-count, and it could be completely unrolled. Several instructions in the inner loop would just disappear, such as all of the loop control, the instructions dealing with and the . This should give a reasonable one-time gain and the loop should still easily fit in the uop cache. It's less appealing than the horizontal unrolling since you can only do it once! Unroll the Outer Loop Once you've unrolled the inner loop, you may want to unroll the outer loop as well. Unrolling this by N would result N copies of the unrolled inner loop so, the code would get big, fast, but I think you could probably unroll it by 3 or 4 and still fit it in the uop cache. This allows some very interesting optimization since by unrolling the inner loop you now have unique sections of code handling each prime. When you unroll the outer loop, you may now be handling several reads for the same prime, in explicit unrolled code. The big win here is that you can directly hardcode the "offset sequence" that normally has to be painstakingly calculated by the generic code. For example here's the start of table for consecutive 64-byte reads: 

OK, now you know how it hangs together, let's look at some of my output. After doing a lot of output, and running on my dual-core-with-hyperthreading laptop, I get the best results from 6 strategies as: 

The "middle-ground" approach you have is immediately "identical" to the fluid approach, but in this context is less readable, even if functionally identical. It does have advantages when your function is, for example, a method parameter, or an injected constant (like a custom comparator, or something). Since the code is all localized to a single method, though, the need to separate the declaration and use of the function is simply not there. My only concern with your fluid approach, and it is a small one, is that you reuse the placeholder in two contexts. While it is logical, in this case, to use , I find it is typically better to have unique variable names at each stage in the pipeline, perhaps: 

From the variables section there are a few things of note. At the bottom, the 'value' variable is highlighted, and it's internal value is shown in the area below ( You can see the and variables for too. This means that the variable stores the char[] array for the entire line, but also stores an offset and count in that array which it uses for managing the subset of the chars that this String instance is interested in 

Now, at this point, there is no need to perform an intersection on the maps... the values in we know ae not in , and visa versa. You can just print the contents of the two maps without any further processing... Putting all these suggestions together, your code would look something like: 

Let's review this code purely from a performance angle, without a focus on style or anything else (in addition to optimization suggestions, Peter already mentioned several things in areas other than performance). First, you can play with all the algorithm discussed here in this github repo. I compiled it on Linux but it should approximately work on Windows if you have or - if you add a thunk to adjust the calling convention. If someone really wants it I'll do it. Profiling If you've ever asked for performance help, no doubt someone has told you to profile, profile, then profile some more. So sure, let's start there. I'm going to use Linux's since it is awesome and free and available on Linux, but you can get the same information presented nicely in Windows using VTune or maybe with this stuff. Anyway, let's run on the original algorithm to get a feel for any high-level issues: 

What a difference a removed branch makes! It's about three times faster, at 1.35 cycles per candidate. That's despite the fact that we are executing more instructions: about 280 billion versus 240 million. The upside is all due to removing branch-misses, which are now reported at 0.00%, and the IPC has increased to ~3 instructions per cycle. Loop Splitting Of course in the real world, you don't want to just count the primes, you want to do something with them. That's fine: it's a slight modification to the above to generate a bitmap indicating which values are likely primes, rather than simply counting them, without slowing down much. So to avoid the mis-predictions, you process some fixed number of candidates with the loop above, generating a bitmap, and then you iterate in a branch-prediction aware way over the bitmap (e.g., using and ) to generate the likely prime values for your "secondary processing". I won't actually flesh this out fully for since we are about to move into the fast lane with a different approach entirely (which still ultimately uses the same "bitmap" output format. Bitmaps FTW Let's step back a moment and understand what the core of the algorithm is doing. Basically it implements 30 periodic counters incremented in sync and tries to determine if at least one counter has "wrapped" on every iteration. To do this, it uses 30 byte counters in a . Since AVX2 lets us do 32 byte operations per operation, it means we can do 30 operations on this counter per instruction (and perhaps up to 3*30 = 90 if we use all 3 vector ports fully). We get lucky that the instruction works well to do a 30-way or "wrap" operation! What if instead of using byte counters, we use a series of bitmaps with one bit per candidate, which encode the same periodic behavior as the counters? That is, to replace the counter which goes we use the bitmap with every 3rd bit set? Well now a 256-bit register holds 256 counter values, not 30. Of course, the correspondence isn't exact, since everything is transposed: one register contains a lot of state, but for one prime. You'll need 30 such registers for all the primes. Still, we can ballpark this: first note that combining registers is simply a matter of ing them together - the remaining zeros are the likely primes. So it will take 29 instructions to combine 30 registers, and the result will cover 256 candidate values, so that's ~8.5 candidates per instruction, versus ~1 for the counter approach. Now this is a very rough analysis and leaves out a lot of details like how do you get the bitmaps all aligned properly, but it seems like we may get about an order of magnitude improvement with this approach even over the branchless counter version. A C++ Prototype Let's try to prototype this in C++. Here's the core loop: 

Unlike any code using recursion, this code is efficient. Feel free to benchmark against the recursion versions and note the vast improvement in speed and memory consumption. 

This code is very easy to understand. However, you asked for optimization and my code doesn't look all that effective; it is possibly slower than the original. Particularly, we have multiple checks for '\0' all over the place. Note that any truly meaningful optimization requires that 1) we have actually encountered and measured a real performance problem, 2) we have noticed that the compiler is doing a poor job at optimizing that particular problematic code, and 3) we have fairly good knowledge of the target CPU and hardware. It a bad idea to manually optimize code if those 3 above conditions are not met. Still, I'll attempt a manual optimization of this, since it was requested. It may or may not be more effective. Branch prediction may be a more serious concern than the number of comparisons executed, for example. For what it is worth, here you go: 

Instead of this, I initially had a call to , which will supposedly be nicely optimized for the specific hardware. The problem with that though, is that both parameters of strcpy() are pointers and my requirement is that I should be able to call "in place". In the special case where the user passes a string with no initial characters matching , I would end up with something equal to which invokes undefined behavior, as it violates the contract with . In order to dodge that bug, I rolled out my own . The above issue is my main concern which I would like some feedback on. 

limited stack depth (someone gives you a 1MB palindrome String to check!) iteration is simpler and faster less stack management and garbage. 

The above commands all work on busybox as installed on my Ubuntu 12.04 box. Just thinking through this a bit further, you can use the tee command to save the output at the same time as you count the lines: 

Edge Cases Orders larger than 10 will effectively truncate to , which makes Integer.MAX_VALUE an interesting input..... An order of 10 and an input of Integer.MAX_VALUE, would normally imply an output of , but you respond with Name The term "clamp" is often used when confronted with this.... where a value is clamped to be within a range, or limit. I would use that as the function name. Solution I would recommend transfroming your internal logic to use long, and be done with it... ;-) 

The system you have in place is efficient in the sense that it offloads the network-based work on to an AsyncTask, and the callback updates the wallpaper. You don't give the details of your , but you could neaten a few things up by putting them in to there... and it would look something like: 

Nice little system, I understand though why you think this code could be better. I think there are a few things which would make a difference. The first is a try-finally block, and the second is a simpler loop First, the try-finally: 

For each line of text in the file, you append data to the StringBuilder. StringBuilders have an internal array that they use to store the data you append. In this case, you are adding millions of lines to it. As you add more data to it, the system needs to 'grow' the array to store the new data. To grow the array the StringBuilder allocates a new, and larger array, and then throws away the previous one (See the source code for StringBuilder). As you get larger StringBuilders, you need to do more garbage collection and other memory management. This slows things down a lot. You need to change your algorithm to process one output line at a time, instead of batching them all up in to a single StringBuilder. Do you really need to keep all the output in memory at the same time? Can you not write out each line to output when it is complete, just like you read one line at a time from the source? As an experiment, if you do: inside the loop, it will 'clear' each output line after populating it. How much faster does it run?