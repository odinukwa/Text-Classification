Several authors, starting with Slavik, have noted that the classical analysis of the set cover $H_n$ greedy algorithm does not readily extend to the set partial cover problem, where the goal is to pick a minimum-cost family of sets to cover $p \cdot n$ of the $n$ elements, where $0<p<1$ is a constant. But it sure seems to! Greedy: repeatedly choose the most cost-effective set, i.e., one minimizing $c(S) /\min(|S-C|,pn-|C|)$, where $C$ is the set of elements covered so far. That is, the standard set cover greedy's cost-effectiveness definition is modified so that the benefit of a set is the min of # new elements and # of additional elements you still need to get. Then it would seem that you can just say: number the elements $e_1,...,e_{pn}$ in order covered (ignoring any additional ones covered--we'll allocate all the costs to these first $pn$ elements), and argue that at the moment when greedy covers $e_k$, choosing all of $OPT$ would take care of your outstanding $\ge pn-k+1$ element needs, with cost per "satisfied element need" of at most $\alpha = OPT/(pn-k+1)$, so there's got to be a set that's at least that good, so greedy's going to choose one at least that good, which gives us a total bound of $OPT \sum_{i=1}^{pn} 1/(pn-k+1) = H_{pn} OPT$. But apparently this argument is flawed. How so? (Slivak writes in his thesis, "Even though [the algorithms] are quite similar, it turns out that the approach used by Chvatal, Lovasz, or Johnson cannot be used to establish a reasonable bound on the performance [...]. The reasons are that only a fraction of points of the set $U$ are covered and that the part of $U$ covered by the optimum partial cover can be completely different from the part covered by the greedy cover. This makes the analysis of the performance bound [...] quite complicated." $URL$ And Kearns proved a $2H_n+3$ bound, and presumably not because he simply overlooked the obvious approach.) 

I use the title term in a very loose sense. There is a significant amount of work on evolutionary game theory, including its mathematical foundations. I was recommended "Evolutionary Games and Population Dynamics" but haven't delved into it yet. There is also a significant amount of work on algorithmic game theory, which is a popular topic on this site. What I'd like to see is work that makes computational complexity or convergence statements about certain evolutionary dynamics. Examples (phrased very loosely): 

Google adwords is a well known application of algorithmic game theory. It has a system wherein buyers of ad placements bid to put their ads up on various sellers websites, then pay the second bid below them. If this system were turned on its head, and sellers of ad placements (websites, in other words) bid to be the cheapest space (relative to their quality) for buyers of ads and were paid the second cheapest price instead, would this result in the same prices being paid for ad placements? If these two scenarios are not equivalent, where is the breakdown? In general, is there any duality between these problems? 

Proof: The first assertion is more or less obvious. For the second, let $w\in\cD$, $f,g,a$ be as in condition 4 so that $f\pres w$, and let $(x_i^j),(y_i^j)$ be as in the definition of $g\pres w$. Put $\bar x^j=(x^j,a)$, $\bar y^j=(y^j,a)=f(\bar x^j)$, and $u_i=w(a_i,\dots,a_i)$. Then $f\pres w$ implies $$\sum_{i=1}^nw(x_i)+\sum_{i=1}^mu_i=\sum_{i=1}^{n+m}w(\bar x_i)=\sum_{i=1}^{n+m}w(\bar y_i)=\sum_{i=1}^nw(y_i)+\sum_{i=1}^mu_i.$$ However, $u_i$ are invertible in $M$ as $w$ is a master weight function, hence $$\sum_{i=1}^nw(x_i)=\sum_{i=1}^nw(y_i).\tag*{QED}$$ Before we proceed further, we need to fix one problem: monoids can be huge, hence invariants of this form can be rightly suspected of being useless abstract nonsense. First, given a weight function $w\colon A^k\to M$, we can assume that $M$ is generated by $w(A^k)$ (and by additive inverses of images of diagonal elements in the master case), as other elements of $M$ do not enter the picture. In particular, $M$ is finitely generated. Second, by general results from universal algebra, we can write $M$ as a subdirect product $$M\subseteq\prod_{i\in I}M_i,$$ where each $M_i$ is subdirectly irreducible, and $M_i$ is a quotient of $M$ via the $i$th product projection $\pi_i$; in particular, it is still a finitely generated commutative monoid. By a result of Mal'cev, f.g. subdirectly irreducible commutative monoids (or semigroups) are in fact finite. The mapping $w_i=\pi_i\circ w\colon A^k\to M_i$ is again a weight function, master if $w$ was, and it is easy to see that $$\pol(w)=\bigcap_{i\in I}\pol(w_i).$$ Thus, we can without loss of generality restrict attention to weight functions $w\colon A^k\to M$, where $M$ is finite and subdirectly irreducible. Let $\mathcal{FW}$ be the class of such weight functions, and put \begin{align*} \inv(\cC)&=\mathcal{FW}\cap\inv^*(\cC),\\ \minv(\cC)&=\mathcal{FW}\cap\minv^*(\cC). \end{align*} Examples of finite subdirectly irreducible commutative monoids are the cyclic groups $C(p^d)$, and the truncated addition monoids $(\{0,\dots,d\},0,\min\{d,x+y\})$. The general case is more complicated, nevertheless one can say a lot about their structure: one can write each in a certain way as a disjoint union of a $C(p^d)$, and a finite nilsemigroup with some properties. See Grillet for details. Now we are ready for the main point of this post: 

Two things you might want to look at: Algorithmic Game Theory Ch. 7 : Graphical Games Fluctuations in Evolutionary Games The first goes over how to find equilibria in games or spin systems like you described. Certain meta-strategies for strategy adoption (specifically the one identical to Gibbs Sampling that leads to a correlated equilibria) allow very general, tractable analyses. The second attempts to predict large fluctuations or change in "norms" in an evolutionary game theory model using large deviations theory. The examples tackled are small-scale, but the author attempts to make the mathematical machinery he uses as general and powerful as possible, so it may be applicable to your case. 

Ford-Fulkerson can find sparse s-t flows in time linear in the size of the flow and number of nodes if the edges have unit capacity. How could I use a sparse s-t flow to find an s-t min-cut in time proportional to the size of the flow and the number of my nodes, for the sparse/low-volume max-flow case? 

[1] John Canny, Some algebraic and geometric computations in PSPACE, Proc. 20th Annual ACM Symposium on Theory of Computing, 1988, pp. 460–467. [2] Pascal Koiran, Hilbert’s Nullstellensatz is in the Polynomial Hierarchy, Journal of Complexity 12 (1996), no. 4, pp. 273–286. 

EDIT: A generalization of the clone–coclone duality above is now written up in [1] E. Jeřábek, Galois connection for multiple-output operations, preprint, 2016, arXiv:1612.04353 [math.LO]. 

$\def\mc{M_\mathit{const}}\def\mp{M_\mathit{paradox}}$Let me for the record write up the answer to Q1, so that it doesn’t live only in the comments. The reasoning given in steps 1–5 in the question is correct in the real world. Thus, $\mc(\mp)$ outputs NO, and $\mp(x)$ halts in constant time, but there is no short enough proof of this in ZFC. When trying to formalize this argument in ZFC, the problematic step is 2: here, we need to assert, in ZFC itself, the implication 

The universal theory of the real field is easily seen to be coNP-hard, and Canny proved it to be in PSPACE, but that’s about all that is known about its relationship to common complexity classes. 

Suppose P != NP. We know that we can make easy instances of 3-SAT at any time. We can also generate what we believe to be hard instances (because our algorithms can't solve them quickly). Is there anything preventing the set of hard instances, from being arbitrarily small, so long as for any given instance size (n) there are only Poly(n) (or even constant) instances of size Poly(n) or smaller? For any hard 3-SAT instance, we would have to add the set of all 3-SAT instances it reduces to via looping through the NP-Completeness reduction cycle, but I don't foresee this adding to the number of hard instances very much. In this world, we could construct an algorithm that polynomially solves all NP complete problems, except an exceptional few. Edit: A softer variant of the question: even if we showed P!=NP, how could we know whether a given method to generate size n 3-SAT problems actually generated hard one with some requisite probability? If there's no way to know from P!=NP alone, what is required to show that we can generate a hard NP-complete problem? 

If $\mathrm{P\subseteq CSL}$, then $\mathrm{P\subseteq DSPACE}(n^2)$. By a padding argument, this implies $$\mathrm{DTIME}(t(n))\subseteq\mathrm{DSPACE}\bigl(t(n)^\epsilon\bigr)$$ for every superpolynomial well-behaved function $t(n)$ and every $\epsilon>0$. I believe such a strong advantage of space over time is not expected to be true. The best currently known result in this direction is $$\mathrm{DTIME}(t(n))\subseteq\mathrm{DSPACE}(t(n)/\log t(n)),$$ due to Hopcroft, Paul, and Valiant. 

Let $M(m)$ be the cost of multiplication of two $m$-bit integers. The current best bound is $M(m)=O(m\,\log m\,2^{O(\log^* m)})$, but here it would not make much difference even if we use the trivial $O(m^2)$ multiplication algorithm. We can compute $\log p_i$ to $m$ bits of accuracy in time $O(M(m)\log m)$ using AGM iteration (see e.g. here), and then evaluating $\sum_i\beta_i\pi_i$ takes time $O(M(m))$. Overall, step 4 takes time $O(M(m)\log m)\subseteq O(\log n\,\mathrm{poly}(\log\log n))$. Thus, the running time of the algorithm is dominated by $O(n)$ of the first step. 

Let $q$ be a prime power such that $q\nmid t_i$, and, in the first case, $q\mid s_{ii}$. Then the system $Sx=t$ is not solvable in $\mathbb Z/q\mathbb Z$. 

The Knapsack Problems text by Kellerer et al. cites the two references below as proving that multidimensional knapsack is strongly NP-hard already in the special case of CARDINALITY 2-KP (two-dimensional, with unit values). It also reproduces a proof, reducing from EQUIPARTITION, i.e., the existence of an FPTAS for CARDINALITY 2-KP implies that the EQUIPARTITION decision problem can be solved in polynomial time. I believe the augmented resources version of the (fixed dimension) problem, where you're permitted to exceed the bounds by $\epsilon$ does admit an FPTAS, however. G.Y. Gens and E.V. Levner. Computational complexity of approximation algorithms for combinatorial problems. In Mathematical Foundations o f Computer Science, volume 74 of Lecture Notes in Computer Science, pages 292-300. Springer, 1979. B. Korte and R. Schrader. On the existence of fast approximation schemes. In O.L. Mangasarian, R.R. Meyer, and S.M. Robinson, editors, Nonlinear Programming, volume 4, pages 415-437. Academic Press, 1981. 

(This is shameless self-promotion.) If you don’t mind either assuming the generalized Riemann hypothesis (for $L$-functions of quadratic Dirichlet characters) or using randomized polynomial time, then the following search problems work: 

is not provable in $S^1_2$ unless factoring can be done in probabilistic polynomial time, by the arguments here. Another class of examples is given by irreducibility testing and factorization algorithms for polynomials (primarily over finite fields and over the rationals). These invariably rely on Fermat’s little theorem or its generalizations (among others), and as such are not known to be formalizable in an appropriate theory of bounded arithmetic. Typically, these algorithms are randomized, but for deterministic polynomial-time examples, one can take Rabin’s irreducibility test or the Tonelli–Shanks square-root algorithm (formulated so that a quadratic nonresidue is required as a part of the input). 

The chosen vertex will be $u=2^n-1$. The first clause contributes balance $1$ or $-2\equiv1\pmod3$ to each vertex $\ne u$. Likewise, the second clause contributes balance $-1$ or $2\equiv-1\pmod3$ to vertices that are not fixpoints. Thus, assuming $u$ is not already a fixpoint, it is indeed unbalanced modulo $3$, and any other vertex unbalanced modulo $3$ is a fixpoint of $f$. $1\le_p3$: We may assume that $A=B=[0,2^n)$ with $n$ even, and the given vertex $u\in A$ has degree $\equiv2\pmod3$. We can efficiently label edges incident with a vertex $y\in B$ as $(y,j)$, where $j<\deg(y)$. In this way, $E$ becomes a subset of $[0,2^n)\times[0,2^n)$, which we identify with $[0,2^{2n})$. We define a function $f$ on $[0,2^n)\times[0,2^n)$ as follows. 

Finite Automata, often written about as finite state machines in different contexts, or with their probabilistic variants Hidden Markov Models can be applied to pattern recognition and quantifying structure of a pattern. E.g. what is the smallest stochastic finite automata that will generate strings according, more or less, to a given probability distribution, or matching statistical properties of a sample of strings (or time series) from some distribution. See for example CSSR, an algorithm for blindly reconstruction hidden states; it's more efficient and flexible than Hidden Markov Models. 

Given a population and an evolutionary scheme, can we give a probabilistic regret bound for the long-term population optimality (compared to the best individual produced?). This seems to relate strongly to ensembles of experts and bandit problems. What about in nonstationary settings? Given a set of populations of different species that interact in their environment, playing pretty much any sort of multi-player game, what statements can we make about the eventual stability of their strategies or strategy distributions, given their evolutionary strategies. In any sort of environment with many "niches" (an overbroad way of phrasing it, I understand), either in terms of direct relationship with the environment or in terms of relationships with other species, what statements can we make about how populations will distribute across these niches. Any problem I haven't asked but should - I'm coming at this with little AGT, TCS, Genetic Algorithms, evolutionary game theory or population biology background; I'm asking my questions from an optimization/machine learning/stats point of view, which may be the wrong one or incomplete. 

First, $\mathrm{PPAD\subseteq FP^{NP}}$, hence $\mathrm{\#P^{PPAD}\subseteq\#P^{NP}\subseteq FP^{\#P}}$. Moreover, $\mathrm{PPAD}$ is closed under Turing reductions, i.e., $\mathrm{FP^{PPAD}\subseteq PPAD}$. Thus, if we assume $$\mathrm{\#P\subseteq PPAD},$$ then $$\mathrm{\#P^{PPAD}\subseteq PPAD},$$ which by induction implies $$\mathrm{FCH=PPAD}.$$ Passing to decision problems, since $\mathrm{P^{PPAD}\subseteq P^{TFNP}\subseteq NP\cap coNP}$, this shows $$\mathrm{\#P\subseteq PPAD}\implies\mathrm{CH=P^{PPAD}=NP=coNP}.$$ (Note that using the closure of $\mathrm{PPAD}$ under Turing reductions, $\mathrm{P^{PPAD}}$ consists of predicates whose characteristic functions can be computed as projections of $\mathrm{PPAD}$ problems.) As for $\oplus\mathrm P$, I believe $\mathrm{PPA\supseteq PPAD}$ can be solved by binary search on the predicate “the sum of degrees of vertices whose labels start with a given string is odd”, which means $$\mathrm{PPAD\subseteq FP^{\oplus P}},$$ thus (using $\mathrm{P^{\oplus P}=\oplus P}$) $$\mathrm{\#P\subseteq PPAD}\implies\mathrm{CH=P^{PPAD}=NP=coNP=\oplus P}.$$ A similar argument applies with $\mathrm{Mod}_p\mathrm P$ in place of $\oplus\mathrm P$ for any prime $p$. I don’t know about UP. 

Since no clarification seems to be forthcoming, let me just answer the question as is, for the record. The answer is yes, one can prove there is such a function. (Which doesn’t necessarily mean most people will believe it, but anyway.) Let $g\colon\mathbb N\to\mathbb N$ be an increasing function that eventually majorizes every recursive function, and $h$ be its inverse; more precisely, $$h(n)=\min\{m:g(m)\ge n\}.$$ Then $h$ is an unbounded nondecreasing function with the property that every recursive function eventually majorized by $h$ is bounded. Put $$f(n)=n^{h(n)}.$$ Then $f$ is (barely) superpolynomial, and I claim $$\mathrm{DTIME}(f(n))\subseteq\mathrm{PSPACE}.$$ As a matter of fact, we have $$\mathrm{DTIME}(f(n))=\mathrm P.$$ In order to see this, let $L$ be a language recognized by a TM $M$ working in time $f(n)$. Put $$e_M(n)=\min\{k:\text{$M$ terminates in time $\le n^k$ for every input of length $n$}\}.$$ Then $e_M$ is a recursive function, and $e_M(n)\le h(n)$. Thus, $e_M$ is bounded, which means that $M$ actually runs in polynomial time, and $L\in\mathrm P$.