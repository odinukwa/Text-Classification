On the other hand, I could not find the AccessExclusiveLock mode in the pg_locks at the same time. Does anyone know why it happen ? As Tom Lane's message, It can be a lock on Notify queue. 

Master ----> Slave : relpica from Master to Slave by asynchronous method (M send WAL, S receive WAL) 2/ Question: How can I monitor (catch) speed of WAL (ex: 1MB/s) is sent from Master to Slave ? 

You can use Jailer tool, it will find all rows of child tables that reference to master table. Example: I have 4 tables : employee, employee_detail, phone_address, relationship. I want to delete employee which name="JOHN". With Jailer, it will find rows in employee_detail & relationship which FK to "JOHN" (by id). And because of phone_address reference to employee_detail, so Jailer will find rows in phone_address. 

We want to index and search "text" on log_info column, so we tried Oracle Text 11g . Problems: If we use "data_type" is "clob", we can use "context index" and it have to synchronize after DML . ( we can not use this way because of (**) ) If any, how can we index and search on "log_info" column (max_length < 10000 and data is changed every second) ? 

SQL Server 2014 SP2. As the titles says, we have converted one of our database tables to be in-memory. After we did this, the corresponding memory-optimized filegroup takes up 1GB on the disk, but on a larger server it's up to 4GB). I suspect it has to do with the number of CPUs. The table is EMPTY! The structure of the table is nothing special, something like this: 

I am trying to deploy my 2012 SSIS project to the Integration Services catalog on the SSIS server (also 2012). I am getting the error below. I checked that there is plenty of space in SSISDB and msdb databases as well as on the disk. Any ideas what might be causing this? 

I think I’ve figured it out (at least partially). It seems that in order to avoid creation of duplicate log file for the child package the value of “Log_Path” variable in the “child” package have to be made blank. If there is no value, the validation process will not create an extra file and will properly inherit value specified in the “parent”. This still doesn’t fully resolve the issue with the “parent” package, because I can’t run it from the development environment without any value specified for the “Log_Path” variable. The only way I found around that is to make it blank, save it and then execute it from the command line (DTExec) while passing the desired variable value via SET option. This finally results in just two files instead of four. I still don’t understand why validation process (at least I think it’s validation process) creates those “extra” files using design-time values. This just seems like a wrong behavior. 

I realise this question is already answered, but I feel there are a few points needed for clarification. 

Index tuning is a science, there isn't a "one size fits all" solution, so when you ask "should I aim for a fill factor of 90%", this is a genuine time when "it depends". I already posted this link above, but I really recommend you read it, to understand the impact fill factor has. There are tools out there to help you on your quest (and I don't mean the Database Tuning Advisor!!!). sp_BlitzIndex is the one that I have stuck to over the years... hence my numerous links to their websites!!! :-) 

Yes that looks like the correct syntax (except you have two '--' for your last index name? :-) ) Also, when you are writing your script for these indexes only, you do not need the '-' before the index name. more info here: $URL$ 

I am developing a number of tabular models, which currently have a limited number of users assigned to roles within each. At the moment, with the exception of 1 or 2 cases, the majority of the users are "full read" users in all the reports. I am in the process of implementing row-level security in to a number of these roles and I am getting to the point now where, before these models get deployed in to live I think I should be developing a security model to be able to easily add/remove members to/from these roles. My question is, how can I deploy a model that allows for dynamically adding and removing users, without having to deploy the solution each time. In my mind my options are as follows: 

Question: After researching , I found 4 solutions. Are there any better solution (not using copy data from files) ? 

Here our query and result we want, it means: when deleting id = 1 (parent row), table will automatically set parent_id = null in child rows (first level) . 

Description: We are using PostgreSQL 9.3 - Centos 6 x64 . We have a dtsc_search_data table as below: 

It takes 10 milliseconds to execute the query on the user_info_raw (remote server). But, It takes a lot of time when using theforeign table. When I remove , the query executes very fast. I think that my query on foreign table should send to the remote server for executing, but it's not, I don't know why, may be due to of this excerpt from postgres_fdw document 

Machine 1 (slave) and machine 2 (master) are in a cluster (streaming replication). Sometime, I see "FATAL: terminating walreceiver due to timeout" in slave log. Here is full detailed logs: Slave 

I believe it depends on what you set max memory to, what is available on the host, and what edition you have. Aaron Bertrand wrote a good blog post on exactly this: $URL$ To try and answer your question, "If the server has less than 128GB, you will see these technologies compete with buffer pool memory, and in fact be limited to a % of max server memory." 

ask yourself if fragmentation is really causing you an issue? Fragmentation is only really a problem when reading from disk. Cache all your data in memory, and concentrate on statistics maintenance (link). Depending on the size of your database, if you can't cache it all in memory do you have a resource problem (e.g. RAM), as opposed to a fragmentation problem? Could your server be getting a high volume of hard page faults, and you don't even realise it?! How big are your indexes? If your indexes are less than 8 pages then they will be stored in mixed extents, and no amount of index rebuilding is going to solve fragmentation... move on! What keys are in your index? If any of the keys use something like 'uniqueidentifier', especially as the first key column, then I'm afraid you are unlikely going to solve fragmentation. Due to the nature of this data type, no sooner than you have rebuilt your index, it will be fragmented again after the first few inserts... that's the nature of the beast unfortunately. If you decide that fill factor is a route you need to go down, DO NOT set it globally for all indexes. Doing so could actually make performance worse. Fill factor increases the free space on index pages, ergo making the indexes larger in size. For an index that has an incremental key, reducing fill factor from 100 (or 0) will likely hinder performance, depending on the size of the index, because you will be causing SQL Server to read more pages to acquire the same amount of data. 

About errors above, because of the connection between pgpool and slave server, if I change slave's pg_hba.conf for pgpool host from md5 to trust, it work fine. Two ways to fix: 

Until now, pgBouncer doesn't support rotate log. Hence, you have to do it by yourself. You can refer to sites below: How to rotate PgBouncer logs in Linux/Windows ? How To Manage Log Files With Logrotate On Ubuntu 12.10 

We want to search on "c1, c2, c3" coloumns on dtsc_search_data table with conditions: if seach_value is found on "c1" then return ; if search_value is not found on "c1" then finding on "c2", if search_value is found on "c2" then return ; else return (c3). Example: 

If not, It causes an error . Secondly, please remember that you cannot have when on the trigger. If not, It raises an error . Finally, please take a look at the example as below: 

2/ QUESTION: What is about "FATAL: terminating walreceiver due to timeout" problem ? How can I fix it ? 

I want to maintain a clean environment, so when somebody looks at the model in future they don't see users who left years back. Ultimately, when somebody leaves their AD account will be deactivated so this is less of an issue and more of an OCD. However, new members will need to be added. I'm leaning towards option 3, but are the above options my only choices? Has anybody deployed a different model? Am I missing something obvious!?!?!? I hope not. Thanks in advance for any help/tips/advice. EDIT: I have also just thought of using PowerShell to add/remove users. Based on what David said, I could get one of my guys to maintain this instead. 

Do this manually... not dynamic! Create user groups in AD and add the user groups to the roles. This will be ok for "full read" users (e.g. Directors), or "manager" roles (e.g. sales manager for sales reports), but would not work for users that will be using row-level security (e.g. salesperson). Create a "global security" table to add to the model, which contains the 'variable' for read access permissions for "full read" access (e.g. Directors and Managers). I will also have another security table that will hold the data for row-level security for other users (e.g. salesperson filters). Then each role in the model will essentially use row-level security based on these security tables. Use TMSL to process each deployment in the SSAS instance and "createOrReplace" each role deployment where this user exists/needs to be added... unfortunately I don't know any TMSL so I don't know how hard this is to learn? 

Here, as you can see with value it worked (didn't raise error), however others didn't. I'm curious about that. Could someone please explain to me why ? I attach test script 

Erwin said "You probably don't want to hear this, but the best option to speed up SELECT DISTINCT is to avoid DISTINCT to begin with. In many cases (not all!) it can be avoided with better database-design or better queries" . I think he's right, we should avoid using "distinct, group by, order by" (if any). I met a situation as Sam's case and I think Sam can use partition on event table by month. It'll reduce your data size when you query, but you need a function (pl/pgsql) to execute instead of query above. The function will find appropriate partitions (depend on conditions) to execute query . 

After testing with 1 million row, here's the best way from @Julien Vavasseur combine with Index on "user_id, grade, grade_date" . We should consider using window function when having a lot of rows. 

OK, here is how I ended up addressing this. I basically wrote a script to compare row counts between tables on publisher and the distributor. One per-requisite is to have a linked server between the two servers. In my case it is called "distributor_ls" (since distributor is on the same server as the subscriber). First part of the script gather information about published databases into a temporary table called #tmp_replcationInfo. The credit for this first part of the script belongs here Second part of the script then uses this information to gather the table row counts into a temporary table called #result for those tables where row counts differ. The differences are then displayed. I know it may not be not super-reliable because it relies on row counts stored in sys.partitions table, but it does what I need. I hope somebody finds it useful. 

This seems a browser problem to me. When I render a reporting services report in IE11, the "Export", "Refresh" and "Export to Data Feed" buttons show up under each other taking up a lot of extra screen space. Does anybody know what could be causing this? I am running Reporting Services 2012: 

Here, you can use sequence to increase value. Please noted if the sequence reach to its , you will face this error . For example 

How to cast '2015-11-13 00:00:00' (text or timestamp) to '2015-11-13'(date) in Enterprisedb (EDB) ? With 4 queries below: 

Format CSV, disable quote (replace multiple spaces to one space then space & to ). is result file. However, please check your input data if it is large because I tested on small data . 

I am using nethogs command to tracking WAL send/receive between master and slave. Download: nethogs for centos link Install: Tracking: Ref: 18 commands to monitor network bandwidth on Linux server 

PSQL connection is OK. When running ETL, I am sure that client on machine 1 is connected to PG database on machine 2 , I tracked by query below 

Question: Our solution: write 3 SQL queries search on "c1", "c2", "c3" are followed by conditions above. How can we do that with lowest performance ?