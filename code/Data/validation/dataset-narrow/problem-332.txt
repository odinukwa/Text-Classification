The function creates a date with a time. If I remember well this is 12:00. If the date in your table has a different time then this is also taken into account. When you want to test dates then you better put the function around your date like: 

You can leave out and use and as a primary key. Since a can have multiple you must 'double' the information for every . In your example this is only so is does not seem to be such a problem but imagine that a has 100 and you must change his . You will have to change all 100 rows. Also think about 2 that share the same . 

Is quite simple. Make sure that oracle can write on a folder on the NAS and run the following RMAN job: 

Oracle 'takes care' of the conversion between different NLS settings. As long as both NLS settings can store the same characters the there should be no problem. It is the same as the difference between the database and a client. If for example the database of the customer is in UTF-8 and you create a database in ISO8859-P1 then you will loose (some) characters with accents and from non-western alphabets. The other way around gives no problem. 

Create an SQL that does the same as the would do. Remove the rows from the corresponding ~50 tables in the right order before you remove the users. This is some work but it keeps your data safe from an application error that does an accidental remove of a user with (lots of) messages. 

Starting point at the sandbox sb1, sb2, sb3: All boxes identical. A Fresh debian 9 install mariadb 10.2 Galera3 build a test galera cluster with empty databases works fine. All three nodes come up and work as expected. then after stopping the test cluster copy a LVM snapshot from a production mariadb server over to sb1 start mariadb server with standalone config to see if all databases and tables are correct. (config I took from the server the data come from) shutdown and restart the standalone to see if there is a problem. All is fine, server starts and stops as expected. Then change configs to fit galera try to bootstrap with > galera_new_cluster and that $URL$ is what happens. my.cnf 

I have a server landscape, running mysql 5.7 on debian 8 in some GTID based replication configuration. All is high load servers and 200+GB databases. Now I got some new servers. I like to run that new servers on debian 9 and the recommended mariadb 10.2.X expanding it later to a MariaDB Galera Cluster. I've read some, how to migrate from mysql to mariadb, but anyhow none of them seems to fit for me. Different GTID handling and innodb format between mysql and mariadb does scare me a bit. After all I have to run some tests. So, my idea is: on one of the masters (mysql 5.7), -flush tables with read lock -make a snapshot (LVM) of the database partition -show master status to get the GTID executed -unlock tables -copy that snapshot to a new server to /var/lib/mysql -install mariadb on the new server would mariadb upgrade that data in /var/lib/mysql Any recommendation or another ideas on that? Ju 

The error is happening because the error being thrown part of a recompile error due to deferred name resolution. Looking at SQL BOL those aren't trapped when they happen at the same level as the try...catch. However, if it's happening at a different level, either as dynamic SQL or a SP call, then it will get caught and rolled back. Using Profiler you can see that the "alter table foo add x dog" statement recompiles before executing and then errors and bypasses the catch block. 

This is happening because the server is responding but the mirroring endpoint is down. The partner and witness get a response from the OS that the port that the endpoint is on is closed and immediately starts failover. The timeout applies to when no response is received from the host as opposed to, "the port you're looking for isn't listening anymore," response. To get around this you'll either need to suspend the mirroring session as Max suggested or, if you're also concerned about unplanned cluster failovers, disable automatic failover as swasheck suggested and manually fail the mirror over when needed. 

My reading of Books Online made me think that what you're doing is right since AdminSapr is in the db_owner role but in testing I had to do something like the above to get this to work. 

I'm on 2 debian8 box's with mysql-5.7.19. I have a master slave replication running, replicating one db (db1.1) from master to slave (db2.1). Replicated db's on master and slave have same names. Master has another db (db1.2) that pulls some statistical data out of db1.1. That happens by means of some views (getting data from db1.1) in db1.2 and an event in db1.2 that calls a procedure. Now I like to take that statistical workload of from master and put it on the slave. I created another db, db2.2 on slave with identical table and views as in db1.2. Names of db1.2 and db2.2 are different. Created same procedure on slave as on master and an event with another name on slave.db2.2 that calls that procedure. If I call the procedure by hand on slave it does what it supposed to do. Copies all data as expected. But calling that procedure by an event on slave doesn't run. Names of the events on master and slave are different. What I'm doing wrong? 

did the trick on all machines I tried. And while starting the other nodes, it is also a good idea not use the "systemd way" systemd mechanism has a timeout, and if your sync with the other nodes takes a bit longer, systemd spits out an error, but mysql keeps running and does what it is supposed to do. It's not a bad thing, but it's kind of confusing. So for now I recommend, starting the other nodes also "by hand" 

In your view you do not fetch all the key fields from both tables. Oracle needs that to be able to do the update. In your case you do an insert so I suppose that you do not provide the primary keys for both tables. Since you did not gave the structure of both tables I cannot be more specific. 

But even then it will not give a usable result since there is no relation between the , , and . Who's result it is? Who gave the result? 

Create the new empty table as table1. Make the ID an 'auto-increment'. Insert a union of table1 and table2 with all fields except the ID and insert these in the new table. 

For me the way to go is that you create a pre-insert trigger that will set the for that department when no was given. To get the next you could use a table that holds the last per department. An other way is to get the for that department and add 1 to it. When you work with multiple users then the first solution is better and faster. The primary key for the table would be and perhaps even if you need to start at 1 when the new year starts. 

One of the of goals of a relational database is to eliminate double storage of data. If you can find the right for the through the then you can leave out the reference in one of the tables (they are always the same as you mentioned in a comment). Keeping it in both tables makes your application more complicated. 

SQL Server 2008 always supports Windows authentication, either local or domain. However, individual users need to be granted access. In SSMS connect to the instance and expect Security then Logins. If the user or a group that they're in is listed they they already have access to the server and there's another issue. A more detailed error message with a state code will be found in the SQL log. The following page lists out the different states for login failed errors and what they mean: $URL$ If the user needs to be granted access right click on "Logins" then fill out the dialog that pops up. Make sure to grant them access to the objects they need via a server role, a db role under "User Mapping", or to specific objects after the login has been created. 

This is going to be one of those things you need to try for yourself to find what works best. Replication can be tricky so while there may not be a direct monetary cost there will be administrative overhead maintaining it. To expand on Log Shipping, you don't need to restore the logs every 15-30 minutes. If you choose, you can do it every four hours or once a day. A solution similar to this that I've implemented is doing a weekly full backup and restore to a reporting DB (which can take a while and happens on the weekend). During the week differential backups are taken and those are restored to the reporting database nightly. Users need to get booted before the restore but since the reporting DB is a business hours application there isn't an issue with that. Data is a day old which shouldn't be a problem based on your requirements. To use database mirroring for this you would need to purchase Enterprise to be able to use snapshots if you're not already running Enterprise. It also wouldn't keep the data 100% up to date since the snapshot needs to be dropped (meaning all users need to be out) and then recreated to get the new data. However, this would be less time than either log restores or the method I explained above. If upgrading to SQL 2012 is an option it's possible to set up a read-only secondary that will be kept up to date with the primary database. I only mention this because it's likely to be the smoothest solution. 

Solution 3 is the best solution provided that you move the from the table into the table. The reason why it is better is that when you need to add a variable to one device you can do this without changing your and tables in solution 2. If you want to query the data in the solution 2 way then you can create views that look like the and tables in solution 2. 

On the machine where you installed your database you should have SQL*Plus installed. For your question. Create a database link from to and use your Read Data user. Now you can query the data. 

That might be the reason. The optimizers are cost based and decide what path to choose based on the 'cost' that each execution path has. The 'biggest' cost is getting the data from disk to memory. If the optimizer calculates that it takes more time to read both the index and data then it might decide to skip the index. The bigger the rows are the more disk blocks they take. 

Based on the replies you gave to my comments you must figure out how to resolve the duplicate keys issue. Also I do not think that master-slave replication is appropriate in this case. I would, after making sure that there are no duplicate keys, use to transfer your data into AmazonRDS one by one. Perform checks after every database that you do. If you have time and space then first do a test. During this test the 5 machines can remain active. For master-slave you should check the Replication page of PostgreSQL. The problem remains the duplicate keys as mentioned there as: 

So, for the after world. I did it by following my idea and it worked out well. A few minor things has to be taken into account. After copying data over from one server to the other. The *.pem and the auto.cnf files in /var/lib/mysql/ have to be deleted on the new machine. Deleting auto.cnf is really important because it holds the servers ID and you don't wanna end up running several servers with same server ID, what will cause you problems afterwards. On a fresh installed machine, before installing mariadb, there isn't a user mysql nor group mysql on the system, so change the /var/lib/mysql and it's sub folders to be owned by root:root. I copied the entire /etc/mysql folder from source machine to destination machine, to make the mariadb install script believe there was a running mysql installation before on that machine. In the config file /etc/mysql/mysql.conf.d/mysql.conf I changed the value of the server_id variable to be unique in the entire server landscape. After that you are ready to fire the mariadb installation on the new machine using apt. While installing, mariadb install script detects the mysql stuff and tries to run the mysql-upgrade on it. That will probably fail, as in my case, but that's not a big deal. It is because the install script tries to run mysql-upgrade as root user on your freshly installed mariadb server with old data in place. But there are differences between mysql and mariadb in system tables and in what root on a local machine can do and what not, and system tables needs to be upgraded by the upgrade script. I never run things as root user on the database, so I have my own user with rights do do everything on each of my databases, and that user credentials come with the data from the old machine. So I run the update script just from the command line using my user credentials instead of the root credentials. And it turned out to run well, complaining about some missing stuff, but that is what the update script is written for. To detect missing or changed stuff and correct it. Restart mysql and there we are. All my users and 230+GB of data right into access managed now by mariadb on a debian 9 machine. Hope it helps someone. Ju 

The is the name of the user that runs the application. The is the name of the user defined in the database. Suppose that is logged on his desktop as and runs an application to modify the data. When he is asked to connect he enters the credentials of . In the audit you will find as and as . This explains while you find as since this is the user that is used to run Oracle itself. 

Should work when the directory structure is the same. Check you directory to see if you have other SID related files. If your database does not need recovery then no archivelogs are needed. Do not forget to change at least the DBID if you need to integrate the database in the RMAN backup process. If you use RMAN for backups then you can also clone your database to another server. This way you can place the files in a different location and the DBID (and DBNAME) will be changed during the cloning. 

The databases have both the same name, unique name, and DBID. This can cause problems if you want to back them both up by RMAN. The configuration of sqlnet will have the name server A. Therefore the database might not be accessible from another server. 

There is no downside in creating an index at a later moment. The index will perform the same way like it would when it was created when the table was created. You mentioned that the table has already 10 million rows. Therefore I would suggest that you create it during a 'calm' period. This because Oracle needs to lock the table. This cannot be done if there is a transaction that does an insert, update, or delete in the table. 

For a repeatable process you can use Powershell to script objects. The following link has more detail but essentially you load the SMO objects, loop through the objects in the database, and call the Script function. $URL$ 

The only way to restore a single file group is to run in all the logs so it's up to date with the rest of the files. This would of course run in the bad data modification you're trying to prevent but it's necessary to ensure consistency throughout the database. You'll need to restore it to another location and move the data over. $URL$ 

If the master database is corrupt you have two options. Hopefully you have a backup and can restore it. This link describes the steps: $URL$ Alternatively, if you don't have a backup available you can reinstall SQL and then restore any backups that you do have. You'll have to reconfigure security, sp_configure settings, and possibly other things if you go this route. 

One way is to try executing something with xp_cmdshell such as ping. If you don't have this enabled you can enable it briefly for the test then disable it again. 

If you wrap the statements in dynamic SQL the error isn't returned to Profiler and the transaction is rolled back 

There's no broad standard that I've seen used or heard widely recommended. Some recommend prefixing tables with tbl but some are very against that. Some, like myself, prefer to name the ID column simply so you always know what the ID column of any table will be called ID while others, like my coworker, prefer to put the tablename (ie - serverid) so what it's called in another table matches the column name in the parent table. The most important thing is that the scheme you use remains consistent, especially within the same database. Most DBAs can pick up on the naming format as long as it doesn't change within an environment.