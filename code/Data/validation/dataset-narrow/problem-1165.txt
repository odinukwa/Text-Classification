The authors characterize the leaf language classes as those which are (a) "countable", (b) are "downward" closed wrt polytime many-one reducibility, and (c) "join-closed" (i.e., disjoint union) wrt polytime many-one reducibility. More formally, all the languages $L$ in a leaf language class have a bijection with the natural numbers, and the property that for every $C,D \in L$, if $E \leq^P_m C \sqcup D$ then $E \in L$ as well (the $\sqcup$ denotes disjoint union). Also, every "non-leaf language class" contains a language which fails to have one of these properties. From these three conditions we can get many examples of classes which aren't leaf language classes. For example, the "countable" condition rules out advice classes like $P/poly$, and the "downward closed wrt polytime many-one reducibility" rules out fixed resource-bound classes like $SPACE[n]$. (Recall that the usual proof that $SPACE[n] \neq P$ uses the fact that $SPACE[n]$ is not closed under such reductions.) 

This problem is sometimes called Subset Containment and it is computationally equivalent to: given $n$ sets $S_1,\ldots,S_n \subseteq [d]$, are there $i \neq j$ such that $S_i \cap S_j = \varnothing$? (I believe the reduction is folklore and appears in several places, but one concrete reference is the arxiv paper "Into the Square".) In turn, this disjointness problem is identical to the (gradually becoming infamous) Orthogonal Vectors (OV) problem. Therefore, algorithms for OV apply directly to this problem. Hence the problem can be solved in $O(2^d \cdot n)$ time, $O(n^{2-1/O(\log(d/\log n))})$ time, and so on (see for example, this SODA'15 paper). Also, there is an $\tilde{O}(n + 2^d)$ time algorithm (I can give it, if you're interested). Furthermore, fine-grained hardness results for OV apply to the problem as well. For example, if it could be solved in $n^{1.999} 2^{o(d)}$ time in the worst case, then the Strong Exponential Time Hypothesis is false -- but don't let that stop you ;) The "average case" (depending on how you define it) is generally easier and is (as far as we know) unrelated to Strong ETH; partial search data structures (such as Rivest's work on partial search from the 1970s) can be used to solve the average case in subquadratic time. 

The class ${\cal C}$ you are proposing is probably not $NP$. (If ${\cal C} = NP$, then every $NP$ language would have linear-size witnesses, which would imply that every $NP \subseteq TIME[2^{O(n)}]$ and $NP \neq EXP$, among other things). It is very natural to consider such classes; they arise in several settings. In this paper, Rahul Santhanam (implicitly) proposed the notation $TIGU(t(n),g(n))$ for time-$t(n)$ computation with $g(n)$-guess bits. Hence ${\cal C} = \bigcup_{k} TIGU(n^k,kn)$. In this paper, I defined an analogous class $NTIBI[t(n),b(n)]$. (NTIBI stands for "nondeterministic time and bits".) Also, Cai and Chen would call your class $GC(O(n), P)$ (GC stands for "Guess and Check", cf. L. Cai and J. Chen. On the amount of nondeterminism and the power of verifying. SIAM Journal on Computing, 1996). Finally, if you search for "bounded nondeterminism" you may find three more notations for the same class... 

Here is an interesting problem whose succinct version has interesting properties. Define Circuit-Size-$2^{n/2}$ to be the problem: given a Boolean function as a $2^n$-bit string, does this function have a circuit of size at most $2^{n/2}$? Note this problem is in $NP$. One way to define Succinct-Circuit-Size-$2^{n/2}$ would be: for a constant $k$, given an $n$-input, $n^k$-size circuit $C$, we want to know if its truth table is an instance of Circuit-Size-$2^{n/2}$. But this is a trivial problem: all inputs which are actual circuits are yes-instances. So this problem is in $P$. A more general way to define Succinct-Circuit-Size-$2^{n/2}$ would be: we are given an arbitrary circuit $C$ and want to know if its truth table encodes an instance of Circuit-Size-$2^{n/2}$. But if $n$ is the number of inputs to $C$, $m$ is the size of $C$, and $m \leq 2^{n/2}$, we can automatically accept: the input itself is a witness for the language. Otherwise, we have $m \geq 2^{n/2}$. In that case, the input length $m$ is already huge, so we can try all possible $2^n$ assignments in $m^{O(1)}$ time, obtain the truth table of the function, and now we are back to the original $NP$ problem again. So this is a problem in $NP$ whose succinct version is also in $NP$. This problem is believed to be not $NP$-hard; see the paper by Kabanets and Cai ($URL$ 

This is a favorite question of mine. Fortnow showed, in his paper "Time-Space Tradeoffs for Satisfiability", that $NL$ is properly contained in $\Sigma_{a(n)} P$, where $a(n)$ is any unbounded function. That is, nondeterministic logspace is properly contained in alternating polynomial time with $a(n)$ alternations. Showing that $NL$ is not in $\Sigma_k P$ for a fixed constant $k$ would imply that $NL \neq NP$. (To see this, consider the contrapositive.) It is open whether $NL = P^{\#P}$. The last time I seriously attempted to prove this, it resulted in the paper "Time-Space Tradeoffs for Counting NP Solutions Modulo Integers". I was trying to find some simulation of every language in logspace that would take $n^k$ time for some fixed $k$ when one has access to an oracle for counting satisfying assignments to a given formula. (This would imply $LOGSPACE \neq P^{\#P}$.) My approach didn't work, but I ended up using the same approach to prove time-space lower bounds for solving $Mod_6 SAT$ and other related results. Uniform-$TC^0$ is properly contained in $P^{\#P}$. The proof is in Allender, "The Permanent Requires Large Uniform Threshold Circuits". Any improvement on this separation is open. (For example, proving uniform-$NC^1 \neq P^{\#P}$ is open, and proving uniform-$TC^0 \neq NP$ is also open.) 

In fact, even an infinitely-often derandomization of BPP under PTH would entail $EXP \neq BPP$ unconditionally. So whatever barriers apply to proving $EXP \neq BPP$, they apply to proving statements of the kind "PTH implies derandomization". 

As mentioned above, it is not known in general if there is a faster oblivious simulation. But interesting lower bounds for this problem are known, under more constrained conditions. For instance, what if you want an oblivious simulation that preserves not only the time $t$ but also the space usage $s$? Beame and Machmouchi have recently proved an interesting time-space tradeoff lower bound for this problem: either the space must increase by a factor of $n^{1-o(1)}$, or the time must increase by a factor of $\Omega(\log n \cdot \log \log n)$. The paper is here: $URL$ 

I don't see how that would immediately follow: the isomorphism conjecture is about languages, and doesn't seem to have any implications about the witness structure of NP verifiers. (Every language has infinitely many different verifiers for it, and you could potentially rig those verifiers to do odd things.) But your question reveals another very natural intriguing question, about the following strengthening of the Isomorphism Conjecture: "Are all verifiers for NP-complete sets poly-time isomorphic?" I.e., we want not only a poly-time isomorphism $\phi_{L,L'}$ between any two NP-complete languages $L,L'$ defined by verifiers $V,V'$, but also isomorphisms $\psi_{V,V'}$ between their sets of input-witness pairs which respect the isomorphism $\phi_{L,L'}$. (Note: There are multiple ways one might formally define this.) All $NP$-hardness proofs I can think of give you a one-to-one correspondence of this kind, as well. This stronger "Witness Isomorphism Conjecture" would imply some sort of yes-answer to your question. A quick Google search (typing 'witness isomorphism conjecture') found a survey of some approaches to this kind of question: Eric Allender. Investigations Concerning the Structure of Complete Sets. Perspectives in Computational Complexity: The Somenath Biswas Anniversary Volume, Springer, 2014 

This is a well-known open problem in the area of "faster algorithms for NP". I don't think it has achieved the status of "major open problem" but it has attracted quite a bit of attention. The best known algorithms run in $2^{n-\Omega(n/\log (m/n))}$ time (e.g., here). Related to the Exponential Time Hypothesis (that 3SAT is not in subexponential time), there is also a "Strong Exponential Time Hypothesis" that the optimal running time for $k$-SAT converges to $2^{n}$ as $k \rightarrow \infty$. One consequence of Strong-ETH would be that the answer to the above question is no. Several plausible hypotheses imply that the answer is yes, but who knows. I think it's one of those problems that seem likely to be "resolved" either way: either we'll show a yes-answer, or we'll show that a yes-answer implies something very major. In the first case, we'll have the satisfaction of resolving the problem, in the second case we'll have elevated the question to that of a "major open problem"... a no-answer implies $P \neq NP$, and a yes-answer implies something very major. :) 

This example is along the lines of Dana and Scott's answers. It is "obvious" that the best way of computing the PARITY of $n$ bits with an unbounded fan-in circuit of depth $d$ is the following recursive strategy. When the depth $d$ is 2, there's nothing better you can do than write out the CNF (or DNF) of all $2^{n-1}$ terms. When $d$ is greater than $2$, break the set of input variables into $n^{1/(d-1)}$ parts, guess the parity of each of the parts (take an OR of fan-in $2^{n^{1/(d-1)}}$), and for those guesses which add up to parity $1$, recursively solve the problem on each of the parts (take an AND of fan-in $n^{1/(d-1)}$) with a depth $d-1$ circuit. If you alternate at each level of recursion between doing an OR of $2^{n^{1/(d-1)}}$ ANDs and doing an AND of $2^{n^{1/(d-1)}}$ ORs (taking the complement), you end up with a circuit of depth $d$ and size $2^{O(n^{1/(d-1)})}$ that computes PARITY. In 1985, Hastad proved that the "obviously best" depth-$d$ circuit is optimal up to constants in the exponent. To do this, he proved the Switching Lemma which has been a very valuable tool in proving lower bounds for circuits, parallel algorithms, and proof systems. This is one of the few instances where we know that a particular natural algorithm is optimal, and it led to a very detailed understanding of the power of $AC^0$. 

I think you need to redraw your Venn diagram... any containment of complexity classes which relativizes will also algebrize, at least in the sense of Aaronson and Wigderson. That is, access to the "low-degree extension" of an oracle is only more powerful than access to the oracle. Similarly, any oracles showing that a separation requires "non-algebrizing" techniques implies that "non-relativizing" techniques are also required. 

There are many such examples. When I first learned complexity theory, I found it surprising that basic theorems about roots of polynomials (such as the Schwartz-Zippel-DeMillo-Lipton Lemma) had anything to do with the question of whether interactive proofs can simulate polynomial space ($IP = PSPACE$). Of course, those properties of polynomials had already been used in prior work, and nowadays the use of "polynomializing" computations has become quite standard in complexity theory. 

This would contradict the time hierarchy theorem except that $g$ is not time constructable (indeed this is why we must have constructability assumptions in the statements of most complexity hierarchies). -- There are also interesting strengthenings of the usual time hierarchies, such as: $${\sf TIME}[n^k] \not\subseteq i.o.{\sf -TIME}[n^{k-1}]/(n-\log n)$$ (there are problems in time $n^k$ cannot be successfully solved by any time $n^{k-1}$ time machine using $n-\log n$ bits of advice, even for on just infinitely many input lengths). The proof is easy: let $\{M_i\}$ list the $n^{k-1}$ time machines that take $n-\log n$ bits of advice as a second input. Define $M'(x)$ which splits $x$ into $x=yz$ where $|z|=\log |x|$, runs $M_z(x,y)$, and outputs the opposite answer. Then $L(M') \notin i.o.{\sf -TIME}[n^{k-1}]/(n-\log n)$. -- The lack of known time hierarchies in certain situations should be considered (as open problems). For example, is ${\sf BPTIME}[n] = {\sf BPP}$? 

For the constant-time task of testing graph properties, an interesting characterization is known. A graph property is a function from all graphs to $\{0,1\}$, and a graph property $P$ is testable if there is a randomized algorithm $A$ such that for all $\varepsilon > 0$ and all graphs $G$: 

The "offline" version of this question is addressed in my SODA 2014 paper with Huacheng Yu, Finding orthogonal vectors in discrete structures. For the case of $\mathbb F_2$, we give an $O(nd)$ time algorithm for determining, given two sets of $n$ vectors $A$ and $B$, whether there is a vector in $A$ and vector in $B$ with zero inner product. I'm sure you can modify our algorithm appropriately, and get an interesting preprocessing/query version; we did not consider this question. 

I think the biggest such example at present is $BQP $ (quantum polybomial time) vs $PH $ (the polynomial time hierarchy). Significant effort has been put into separating them relative to an oracle, with no success. (Of course a powerful enough oracle will make them equal.) And the best known containment result is that $BQP $ is in $PP $. Some references for attacks on the oracle problem: $URL$ $URL$ 

This is a question about circuit complexity. (Definitions are at the bottom.) Yao and Beigel-Tarui showed that every $ACC^0$ circuit family of size $s$ has an equivalent circuit family of size $s^{poly(\log s)}$ of depth two, where the output gate is a symmetric function and the second level consists of $AND$ gates of $poly(\log s)$ fan-in. This is a fairly remarkable "depth collapse" of a circuit family: from a depth 100 circuit you can reduce the depth to 2, with only a quasi-polynomial blowup (and one fancy but still restricted gate at the top). My question: is there any known way to express a $TC^0$ circuit family, similarly? More ambitiously, what about an $NC^1$ circuit family? Potential answers would have the form: "Every $TC^0$ circuit of size $s$ can be recognized by a depth-two family of size $f(s)$, where the output gate is a function of type $X$ and the second level of gates have type $Y$". It doesn't have to be depth-two, any sort of fixed-depth result would be interesting. Proving that every $TC^0$ circuit can be represented in depth 3 by a circuit consisting of only symmetric function gates would be very interesting. Some minor observations: 

This is not a complete answer, but some basic observations about applying this to MAX-SAT. At a high level, it looks like this heuristic approach (when applied to MAX-SAT) would be similar to a branching algorithm based on the method of "conditional expectation", a standard method in derandomization. For example, to get a deterministic $7/8$-approximation for MAX 3-SAT (with 3 variables per clause), one sets a variable $x=0$, estimates the expected fraction of clauses that will be satisfied by a random assignment in the remaining formula, then sets $x=1$ and does the same calculation. (This looks extremely similar to "playing a game to completion randomly".) The variable setting with the higher expected fraction of clauses ($x=0$ or $x=1$) will be chosen. This polynomial time algorithm gives a $7/8$-approximation and is known to be tight (you can fool it into satisfying only $7/8$ of the clauses). This connection should make it possible to prove lower bounds on the ability of this heuristic. It is known that approximating MAX 3-SAT better than $7/8$ is $NP$-hard, so we don't expect an efficient heuristic to do better than this. It would be interesting to show (and I conjecture it is true) that a branching algorithm based on the above variable choice heuristic requires exponentially many steps to find a better-than-$7/8$ approximation. There are already lower bounds on backtracking which say that no matter what heuristic you use, even if you guess perfectly, there are still unsatisfiable formulas for which backtracking will only conclude they're unsatisfiable after exponentially many steps. Lower bounds on the lengths of resolution proofs yield these results. One reference is: