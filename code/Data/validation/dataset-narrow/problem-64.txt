When transmitting TCP over IPv4 over Ethernet, there are three levels where checksum (or CRC) is used: 

You should consider that most CSMA/CD networks although in theory using CSMA/CD in practice are really switched full duplex networks where collisions cannot occur. So, scaling up the speed does not change the situation at all. What about real CSMA/CD networks, then? If you scale up the speed and keep the maximum allowed network size constant, you must scale up the minimum allowed packet size. Otherwise collisions cannot be detected. Eventually, the minimum allowed packet size exceeds whatever packet size is most common in your network, and then in that case you must insert dummy data to the end of the packet so that collisions can be detected. So, if increasing the speed is taken to the extreme, utilization will actually become lower. 

While the accepted answer explains well why UDP is used instead of raw IP, it doesn't explain well why TCP is not used. The short reason is that the Ethernet packets traveling inside a VXLAN tunnel have most commonly already one level of TCP. If you encapsulate those packets inside another level of TCP, you have two levels of TCP in the same packet. TCP over TCP is a bad idea: $URL$ While TCP over TCP may appear to work if packet loss is low, having retransmissions at two different levels makes the system increasingly inefficient at high packet loss rates. So: don't do that! UDP or raw IP is a much better encapsulation, as they don't provide another unnecessary level of retransmissions. Tunneling protocols exist both for UDP (GTP, VXLAN, Geneve) and for raw IP (GRE). 

Of these methods, the DNS sniffing is the most reliable, as an IP address can run many websites with different hosts on it. You cannot find the path of the URL, as the purpose of the encryption is to hide that from evil sniffers. The port can be found (it's usually 443), and you can also try to guess the scheme from the port (if 443, it's HTTPS), but that's about all you can do. 

Let us assume that I have a large corporate WLAN network with many access points having the same SSID. The WLAN access points are connected to each other by learning Ethernet switches. Now, each client in the WLAN network has its own MAC address. When a client moves from the area of one access point to another, it has to perform an access point change because its signal level went down. AFAIK, the MAC address of the client does not change during this procedure. My question is, how do the switches in the network learn that the client has moved to another access point if the client is only consuming downlink data? Let us assume that there is a mostly quiescent TCP connection that has data transfer only in the downlink direction at 10-second intervals, with no keepalive. Of course, there will be uplink ACKs but only as a response to the downlink data. When the client has performed an access point change, if there's no uplink data, my understanding is that the switches will direct the downlink TCP data to an incorrect access point until the (MAC address, port) pairs in the switches expire. Is there some kind of dummy uplink packet sent after an access point change to inform the switches in the network that the MAC address has moved to a new switch port? Or is there some kind of mechanism where the old access point will redirect the packet to the new access point? 

I have understood that real networks have routing changes, leading to out-of-order delivered IP packets. Real networks also have frequent packet loss. However, what I have not understood is why could a perfectly operating network in addition to out-of-order and lost packets deliver packets as duplicates. What could cause delivery of such duplicate packets? Could spanning tree protocol (STP) before converging on a spanning tree cause duplicate packets? Note that I don't currently have a network that would frequently duplicate packets, so this is more of a theoretical question than a practical one. 

So, you want to tunnel Ethernet frames over an IP network that has an IPsec link? Works like any IP network, but you have to be careful with MTU (as always). IPsec links usually have lower maximum transmission unit (MTU) than 1500, but with Ethernet over IP you will anyway run into MTU problems. The solution may be VXLAN, specified in RFC7348. However, do note that as VXLAN operates over UDP, there is a large amount of overhead. If the IPsec link has an MTU of 1500, then IPsec, UDP and VXLAN together add overheads meaning the Ethernet link has an MTU smaller than 1500. To have MTU of 1500 for the Ethernet link, you will need an MTU bigger than 1500 for the IPsec link, which isn't usually possible in the Internet. Note that as VXLAN operates on layer 2, it has no way to generate ICMP packet too big messages (which are layer 3 messages). This means that you have to manually configure the MTU to a smaller value for the Ethernet link, or otherwise you will have dropped packets (=no connectivity) or fragmented packets (=performance problems). GRE (generic routing encapsulation) specified in RFC2784 can also be used to transfer Ethernet frames (transparent Ethernet bridging, Ethertype 0x6558), but firewalls may not like GRE running directly on top of IP as much as they like VXLAN running on top of UDP. However, GRE is an industry standard that is almost unanimously used, so most good quality firewalls from reputable vendors should offer the possibility to allow GRE traffic. The MTU/fragmentation issues apply equally to all protocols running on top of IP or UDP without TCP inbetween. Now, what about TCP as the transport for Ethernet packets? The traffic you're transferring through the Ethernet link probably has already one level where TCP is being used, so you would then run TCP over TCP. This is heavily discouraged, as you have then two levels where retransmissions occur, meaning that the performance of the system can catastrophically degrade if there's packet loss. TCP would eliminate MTU problems, but because TCP over TCP can have catastrophical behaviour in the case of packet loss, I don't recommend it. 

I have an issue where a Netscreen 25 is being flooded with IKE packets from an unrelated source, which at times is overloading the firewall's processing capacity. I see thousands of log entries indicating that the IKE messages are being rejected: 

Both traceroutes above should be following the exact same path, and there are no filtering mechanisms in place along it. The same thing happens in the reverse direction as well. What am I missing? What's the difference between BGP routes learned by direct connection versus static configuration with regard to MPLS/label forwarding? 

When configuring ZBFW on IOS, the "class-default" class doesn't allow the inspect action (only pass and drop). What is the recommended way to match all traffic for stateful inspection? Matching TCP, UDP, and ICMP seems to work fine, but this doesn't seem ideal: 

From router B, I can traceroute to both of the directly connected networks on router A with no problem: 

Is there a way to filter the control plane of the firewall so that these packets are dropped at the interface edge rather than processed and rejected? This would be done with a simple interface ACL on a Cisco router or ASA, but I'm not sure how to go about this on ScreenOS. 

Some vendors support IEEE 802.1ae/MACsec for encryption layer two between the access switch and its endpoints. I'm afraid I can't be any more help as I have no experience with it (and frankly it sounds like a nightmare to administer). 

The difference might simply be attributable to differing processing hardware or even temperature sensor locations. 

Traffic always follows the most specific (longest) route entry to its destination. So, any network knowing both the 46.246.32.0/19 route from AS37560 and the 46.246.0.0/17 route from AS42708 will prefer the former. If the route from AS37560 gets withdrawn for some reason, the remaining route (via AS42708) will become the preferred route, assuming no other more-specific routes are available. 

We have all of our network devices attached to console servers for out-of-band access. Occasionally, someone will log in as the local root user and forget to log out. Whoever happens to log into that device next via the console server will be greeted by an open console session. I'm aware that you can configure idle timeouts under login classes, but that doesn't seem to be an option for the built-in super-user class: 

I've been looking at a hardware network tap like this one to replace a pseudo-permanent SPAN that's been running on a Catalyst switch. All the taps I find have four interfaces: A, B, and two output ports (one for each direction). Ideally I would prefer to funnel the traffic from both directions into one cable so I only have to capture from one interface. Why do taps always seem to have two output ports? 

I have two routers, A (Cat6500 w/SUP720-3BXL, IOS 12.2(33)SXH4) and B (Nexus 7K w/SUP1, NX-OS 5.2(4)), separated by several hops across an MPLS core, each with VRF ABC. Router A has two directly connected routes and four static routes within this VRF. 

You could create a second match condition in the class-map matching all source IP networks you want to block (with an ACL). Any requests to youtube.com from a source IP not matched by this ACL will not be dropped. 

The first option ensures traffic is always going through either one firewall or the other so that NAT doesn't break. The second option allows you to load balance across both circuits: One equal-cost default route from each circuit, with your local public prefix(es) advertised out both circuits. (This option ignores how connectivity between the sites is accomplished.) 

However, traceroutes to all of the statically-learned routes timeout across the MPLS path and pick back up only at their last hops: 

This looks like a simple layer two domain. MPLS is typically used only as backbone or distribution transit, that is, among routers. To implement MPLS, you would need to push the VLAN interfaces down to the access switches (and thus split Host5 into a new network or move it to switch 2) and run MPLS among the multilayer switches. Although, this wouldn't bring much benefit to the topology shown. 

Does anyone have any clever tricks around this? We could certainly use a separate user account instead of root, but that still leaves open the possibility that someone will log in as root during a maintenance action and forget to log out. 

I think the link-local scope was set to /10 simply to "fit in" better with the other scopes, e.g. site-local (before it was replaced with unique local). Initially I had thought maybe it was to allow the use of many link-local networks on the same link, but RFC 4291 explicitly states that only fe80::/64 may be used.