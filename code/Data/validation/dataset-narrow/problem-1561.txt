The AUC measure is just a measure of how often true "1" samples have a higher number than true "0" samples. The predictor does not have to even be bounded by [0,1], it can be any range. Your plot would be more informative if it was sorted. Consider the below sorted plot (crude, sorry) for each of the three methods, with the "1" labels highlighted in blue. badaI clearly has the closest grouping of "1"s towards the higher values. 

Im a little late, but better late than never. It looks like your line where you find the coefficients: 

There are two main reasons people typically transform their data. Either to help meet assumptions or to help boost predictive performance. 

You can try using an auto-encoder which also is a non-linear dimension reduction technique. It uses a neural network framework to find the most efficient transformation from $p$ dimensions down to whatever you choose. It then finds how well it can reconstruct the origianl $p$ variables, and keeps tuning until it can optimally recreate the original values as "good" (lowest MSE) as possible. In this case the input has 5 variables, and it reduces it down to 2 variables using non-linear transformations. Once you get it down to the reduced variables (the light blue nodes) it then tries using non-lienar transformations to recreate those original five inputs using a similar nn structure. The right-five red nodes are then compared to the left-five original inputs to see how much information is encoded in those 2 middle light blue reduced variables. I've found the h2o implementation is pretty easy to use, but finding the correct parameters (number of layers, nodes, learning rate, etc) is still tough to select depending on your data. Heres a little blog post that goes through the h2o autoencoder with the MNIST data set. 

When fitting a GLM (at least in R), I know there is a optional weight vector that you can include. This weight is not to give more importance to an observation, but to rather weight observations based on $T$ for example. The R documentation says: 

Yes, sample the same size of your data set (with replacement), then find your AUC, say 10,000 times. You will then have 10,000 different AUCs which will give you some idea of how confident you are in the AUC result. When you sample with replacement, you will only find around 63% of the entries in your new bootstrapped data set will be unique (so many of these will have 2+ identical entries in the bootstrapped data set. Every time you make a new bootstrapped sample, the 63% that are unique will change, as will how many times a record is duplicated in the bootstrapped sample. This helps extract some information on the natural variability of your data. So to recap, if you have 50,000 records, which means 50,000 probabilities/values and 50,000 class labels: 1) samples 1:50,000 with replacement. You should find about 31,500 unique records in this new sample. 2) calculate the AUC 3) repeat steps 1 and 2, 10,000 times(pending computational times, I would shoot for at least 1000), and save those 10,000 AUCS. 4) Now you have 10,000 AUCs. If you take the 5th percentile and 95th percentile, you have the bootstrapped 95% confidence interval. The big benefit here is now you can take the mean of those 10,000 AUCs, and give the standard error $(sd/sqrt(n))$ to give some idea of the variability. Rather than just doing one AUC calculation on your full data and saying the AUC is $.77$, you may end up finding your AUC is $.75 +/- .03$, which is much more reliable to make a claim on. Now in hindsight you can see your single AUC measure was a little optimistically high. 

The results you posted are correct, I did a quick check with and got the same thing. The important feature for AUC/ROC is that the cutpoint of calling a sample "1" or "0" is not set at 0.5. For each of your predictors, the range of the numbers do not matter at all. What does matter is how often higher numbers are associated with true positive labels. Example: 

I recommend using the second option you presented. I would use $T$ with 10-fold CV to select my modeling technique and optimal tuning parameters. Take a look at what performed the best ("best" being the model that gives us the best error, but also doesn't have the error fluctuate too much from fold to fold). After selecting a model, you can use the model on $V$ to get a realistic error rate. The reason I don't recommend the first option is: There are varying degrees of over fitting that can occur when going through model selection and model tuning, then using that same data to get an error rate. CV is a great way to limit this overfitting and it gives us a sense of performance variance which is great, but a classic hold-out validation set is the gold standard for model performance. In your case the first option might not be wrong (depends a lot on data/techniques), but if a hold-out validation set is available I would go for that. 

In short, the correct practice is to use the scaling/preprocessing parameters you found and used on your training set, and apply them to your test set. the motivation for this is that you should be able to deal with someone coming to you with one new observation they want a prediction on. You will be able to preprocess that single observation using your training set preprocessing parameters. Ill go through some of the issues you brought up: Dummy Variables: This might entail you using in R to find what columns are in your training set but not in your test set, then make a column in your test set with a column of zeros. This is the simplest way to do it, but it is very data/purpose dependent. You may also want to change this or look for better options depending on what modeling technique you're using. Min/Max Normalize: When you scale your training set, save the min and max of each column, and scale your test set using those parameters (NOT the min/max of your test set). PCA: When you perform PCA on your training set, you are transform your data based on rotations that were calculated on your training set. You want to perform those pre-calculated rotations on your test set. Check out . This will show you how to apply your rotations from your training set onto your test set.