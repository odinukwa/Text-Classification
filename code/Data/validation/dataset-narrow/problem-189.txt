I came to a dead point in a deadlock analyze. According to msdn: RangeX-X are Exclusive range, exclusive resource lock; used when updating a key in a range. RangeI-N are Insert range, null resource lock; used to test ranges before inserting a new key into an index. So I understand that if I have an Index on 2 key columns - and I insert a new key I would have RangeI-N lock but if I update an existing key from the index I would have RangeX-X. But my question is more or less complicated. Say I have the index IX_keys_included on column A, B and included column C. In Serializable isolation mode I insert a new value for the included column C. Will there be RangeI-N or RangeX-X locks for the index IX_keys_included? Actually , will there be any locks given the fact that I insert a new column for an included column in the index? 

Make sure no one but one sysadmin has access to the restored database. Put the db in single user mode after the restore is completed. Check the code inside all stored procedures and functions and triggers inside this database. Perform a dbcc checkdb to make sure there are no integrity issues. Check the users which used to have access to the database and remove all of them. Start allowing access, very restricted to specific objects checked by you. 

Looks like the disk subsystem on Server B is performing worse than on Server A but, I used to see disk issues not entirely related to disk specs. You could collect some other performance counters such as physical disk --> avg disk sec write (> 25 ms very slow), memory --> page file usage (> 70% bad), cpu --> processor queue length (> 12 very bad), memory --> pages/sec(> 600 slow, > 2500 very slow disk subsystem), sql server: buffer manager --> page life expectancy (< 300 memory issue). You can also limit the growth of 6 tempdb data files to the size they have now. See what happens. You should check the waitstats as well on both servers. If you see a lot of PAGELATCH_XX then you will know where to dig more. Jonathan Kehayias has a good article on this theme. And Paul Randal has also a lot of analysis which could help. 

This question currently reads like a solution looking for a problem. You've decided that a RAM disk is the solution and you want someone to validate that choice. Sorry, not going to happen. If you have measured and observed a spill to tempdb, it will almost certainly be due to a sort or hash operation and an insufficient query memory grant. Depending on the volume of data to be processed this may be inevitable but good odds the query and/or indexing could be improved to avoid it. Take a look at Buffer Management to better understand how SQL Server manages memory and SQL Server Memory Management Explained for some basic tools and DMV queries to understand where your memory is allocated. 

If you cannot create a build in a single step, or deploy in a single step, your development and deployment process is broken. If you donâ€™t think so, ask Joel Spolsky. 

The following will tell us how badly fragmented your existing indexes are. You may want to run this at a quiet time as it can impact on performance. 

Stack Overflow search is now 81% less crappy, thanks to Lucene. Whether this, or any of the other document oriented search servers, would be a good fit is difficult to tell from the detail you've posted. Lucene/Solr and their ilk are built to search textual data. It sounds like you should be considering a graph database. High Scalability has an interesting article on Neo4j which discusses some of the concepts well. 

I have setup a SQL Agent job to delete files older than 7 days. The script does it job when run through windows powershell window. However the same script does not work from SQL Agent job 

Full Backup started on 4/21 12 AM. While backing up one of the database, it is stuck. sp_WhoIsActive shows following information 

Since the command running indexoptimize was a deadlock victim, I am wondering the job succeeded instead of an error!! 

Server Config: RAM: 24GB - 21.5GB for SQL Server Processors: 8 Not much activity in this server - hardly 50 people access this SQL Server via Sharepoint. 

Here is the scenario which I am working with IT team to get it right with no luck. Following are the users who work on a SQL Server. 

Is there a way to resolve this to make work on Domain3 SQL Server? Also, creating a group and adding the individual userID's from other domains worked too. We don't want a new group just for this. 

DatabaseBackup - USER_DATABASES - LOG: This job fails saying "Executed as user: Domain\XXXX-SVC. Unable to open Step output file. The step failed." The error is only with LOG backup job. The other DatabaseBackup jobs (FULL, DIFF) works just fine with same SVC account. So the service account have appropriate permissions. The Output File(Job Step properties-->Advanced) is F:\SQLAgentLog\ which is same for all jobs. Only problem is with LOG backup job. Has anyone else experienced this and is there any solution? Current environment: SQL Server: 2012 SP3 CU8 OS: Windows Server 2012 Note: This was working all good on a Windows Server 2008!! 

Considering the EventSubClass number you can find out what happened with the Query Plan and take specific measures. Additionally you can add other columns to Stored Procedures and TSQL Event Classes if you are interseted in HostName, WindowsUser or other info from Profiler trace. Also the trace can be stored in a SQL table making the analyse more easy and much more customizable. Here is a link describing more the Performance Statistics Event Class. 

But it can be easily hacked if you change the with So instead you should really use the parameters as the sp_executesql is supposed to be used. 

You can prevent creating a procedure with invalid objects before executing the Create Procedure statement like that: You will see a red line under the invalid object which underlines the error when you go with the mouse over the red line. I am not aware of other method, because the SQL will parse the procedure before executing it considering only the validity of the SQL syntax. 

first of all, the replication variables have effect on the slave only when activated on a replicated "slave" server. you need to understand that the filtering rules on master differ from the ones on slave. on master you can choose only to log a whole db or not. on slave you have more options. here is described $URL$ and here: $URL$ I think, you want to skip replicating a set of tables with a given pattern on slave. So, the variables must be configured on the slave. Change the configuration file on the salve and add the db_name instead of % for db part. 

I had a situation where the Native Backups were being made on a Server. I happened to see in that there was a third party backup tool () that was also taking VSS (kind-of) . At some interval, the AppAssure (backup being made to ) was doing a and at some other interval it was doing a breaking the log chain. Is there any way() to know when a backup log chain is broken? Here is a screenshot of the situation from February. 

P.S. The Service Account was granted rights and the old files are being cleaned up. Version Info: SQL Server 2012 SP3 CU8 / Windows Server 2012 

Also, I see another backup on the same database started 30mins later (Backup to VirtualDevice - I know this is AppAssure backup tool). From SQLSkills preemptive_os_waitforsingleobject, I see And this is blocked for 2 days - I don't think one of the session will terminate itself until I kill the other. However, LOG backups were happening without any issues. Only FULL backup and DIFF backup for that particular database was blocked. I killed the AppAssure session and all the long queue went away from AppAssure. I killed the DIFF backup as well. Now the only process left is this FULL backup with 100% complete with same and not willing to complete!! I had no other options but to kill the FULL db backup and restart backup jobs. And, the USER_DATABASES full refuses to start saying but showed nothing. Also, there was nothing in state. CurrentJobActivity in msdb showed that the job was active. Had to right-click-stop under jobs and then start the job again!! Any idea on how do we avoid this situation? (other than telling the IT-team to stop AppAssure?) [EDIT]: Adding the version 

It matters only related to disk space and character length. Of course search on char data types and indexes on these type of data will act slower than integer but this is another discussion. Varchar data type is a "variable" data type so if you set up a limit of varchar (500) than this is the maximum character length for that field. Minimum length can be between 0 and 500. On the other hand the disk space claimed will be different for 10, 30 or 500 character fields. I did sometimes a test for data type varchar (800) and for null values I had 17 bytes used, and for each character inserted it added one more byte. For example a 400 character string had 417 bytes used on the disk. 

the script I use will show you all objects (SP, Tables, Functions) for a specified Database name and all the users that have rights on them, but you can narrow the search and extract exactly what you need. 

Thank you all for answers. Here is what I have done to estimate the restore time. We can't afford currently a real test scenario of restoring the 2 TB backup and I don't know yet the final configuration for the migration. It would be close to what we have now. And it will be in Alibaba Cloud. We will be using Classic Virtual machines (ECS). I tested 2 smaller backup restores on the 4'th node in 4 node SQL cluster which is identical in configuration with the Production SQL Server instance. The restores revealed an increase in restore time of 20% to 50% from the backup time. Taking the 50% and applying to the 720 minutes backup time - I got a restore time of 1080 minutes (18 hours). I also tested the case with the backups for both tested databases split to 5 files and the restore time didn't decrease at all. It was very close to the one backup file restore case. 

The locks are then released in reverse order. At no point has an exclusive lock been acquired on the table. 

A snapshot database will have an entry in , so you can use something like the following to check for a current snapshot. Similarly, you could use the same check to an existing snapshot before creating new. 

Transactions do of course run concurrently and simultaneously. The question is whether or not simultaneous commits can occur, which is what @gbn is addressing in his answer. @gbn is correct that two simultaneous commits cannot occur if you consider the transaction to be committed at the point the WAL entry is hardened to disk. From a crash recovery perspective, the transaction is definitely committed at this point. In the event of a failure occurring, this transaction would be rolled forward (redo), not rolled back (undo). As two writes to disk cannot occur at the same time, we could argue that two simultaneous commits cannot occur. However, you could also argue that a transaction is committed at the point the RDBMS is notified by the IO subsystem that the WAL write has completed successfully AND has released any locks it was holding for the duration of the transaction. Therefore, in a multi-cpu environment simultaneous commits can and will occur. The mechanics of how this can occur for SQL Server is explained in Performance Tuning Waits and Queues, section titled 'Execution Model (simplified)'. Again for the SQL Server folk, some background reading on the WRITELOG wait type is useful to understand the concepts. 

We recently migrated to a new SQL Server and the SQL Services are running under a Service Account. I did observe that the Service Account do not have rights on G drive and the old files are not being cleaned up. Question: Why doesn't the procedure when it cannot delete a file OR simply move on instead of waiting for around 9.5 minutes? Also, doesn't the job error if it cannot delete the old backups? We will not know until we receive alert !! Here is the log details. 

And I verified by only setting up 823 and ignoring 824 and 825, still the sp_Blitz does not report for other 2 missing (824 and 825) !! 

However, if the Domain1/User1 and Domain2\User1 are added as individual accounts then we could Login without issues. 

It turns out that the person migrated the jobs from old server to new server edited the jobs manually. One of the edit was to this was set to default log location on old server. On the migrated server it was edited to where the directory did not exist. (that person missed deleting the text in the path) On the other jobs (FULL and DIFF) the text had been removed so it was set like this and these both were working fine! 

I am working with some old jobs and I found this code snippet. I am just wondering why would someone dump transaction log files from databases in to one file ? 

Is there any way to find who used Dedicated Admin Connection? Not active connection but the previous one which is already closed? 

If that's the question the answer is yes, the performance will be identical. In SQL2000, the optimiser sometimes struggled with deeply nested complex views joining to each other but not with simple scenarios such as this. I've not encountered the problem so much in 2005+. 

As you have an old backup with the correct schema, the problem database is online and you've successfully queried several tables, I'd be inclined to try get a dump of the raw data as fast as possible. 

Depending on the size of the table, an index on IsDefault (DESC) might result in one or both of the queries below avoiding a full scan. 

Might get the odd spurious result but it'll narrow the field. Add SQL:BatchCompleted if you have a mix of procs and statements. 

For 2008R2, I'd take the extra cores every time. If the server might be upgraded to Denali/2012 on release you may need to factor in the switch from per socket to per core licensing. @MrDenny summarised this well in SQL Server 2012 Licensing Changes. 

It is possible for two separate servers to simultaneously access a shared read-only database. The feature is known as Scalable Shared Databases. From your description, it sounds more like you need a failover cluster, not a shared read-only copy. 

Your question may be a candidate for closing as "Primarily opinion-based" but yes, I'd follow your proposed pattern, to a point. What I tend toward is identifying the most frequent parameter combinations and coding discrete procedures for them. It becomes unfeasible with a large number of combinations of parameters to code and maintain for each, although you could code-gen them. Instead, deal with the lesser combinations with dynamic SQL, or static SQL with :