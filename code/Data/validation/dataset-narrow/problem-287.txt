You need to set to a positive number in order to enable database jobs to run. controls the number of jobs that are allowed to run simultaneously so if you set the parameter to 1, only 1 job could run at a time. If you're going to be creating multiple materialized views that you want to refresh at approximately the same time or if you are potentially going to be creating additional jobs, you may want to set this parameter to a value greater than 0. 

You could use an explicit or a rather than date literals (or timestamp literals) if you would prefer. In your original query, your literals do not have a timestamp. If your is potentially in a different time zone than your database server, comparing against dates may not be what you want. You may well need to use an explicit that includes the time zone that you want the date range to be in. If you want to use an explicit so that you are comparing against values including a time zone 

A savepoint is a point within the current transaction. A statement like , , or will issue an implicit commit before and after the statement runs that will end the current transaction. Once the current transaction ends, you can no longer rollback to a savepoint defined within that transaction. Potentially, you want to look at using a restore point instead. A restore point is an attribute of a database, not a transaction, so you can generate many transactions and still restore the entire database back to the restore point. Restoring to a restore point, however, will affect every change that had been made to the database between the time that you defined the restore point and the time that you do restore-- you would lose all changes made not just by your session or your user but all sessions and users in the database. 

Once you upgrade the destination database to 11.2, you won't be able to apply archived logs (or incremental backups) from a 10.2 database. So no, this won't work. It is, on the other hand, often possible to use an actual standby database to perform a rolling upgrade. You can create the 11.2 database as a logical standby of the 10.2 database, let the SQL apply process keep the databases in sync, and then switch over to the 11.2 database when you are ready to migrate. In a similar vein, you could also use Streams between the 10.2 database and the 11.2 database to keep the data synchronized and then switch over when you're ready to migrate. 

in your clause, both references to will resolve to the column in the table, not to the parameter . That means that no matter what value you pass in to the function, the statement will return every row from the table and, thus, throw a error. Normally, you would come up with a convention on how to name variables and parameters that would not conflict with your table naming conventions. Prefixing parameters with and local variables with is one common convention. That turns our function into something like this 

That sounds like the DBMS_ADVANCED_REWRITE package. Tim Hall has an excellent walk-through of using that package to point an application's queries against a different table or view. If you merely want to change the query plan but not point the query at a different table, you can use stored outlines or SQL profiles. For example, I have tables with 1 row and with 2 rows 

First, are you using "database" in the Oracle sense of the term? Or are you using it in the sense that other database vendors (such as SQL Server or MySQL) use the term? If you are using "database" in the Oracle sense, that would be the size of the and tablespaces at a minimum and would possibly include the size of the and tablespaces. On a small laptop system, that's probably on the order of 2 - 3 GB but it could be much bigger. If you are using "database" in the sense that other database vendors use it, that's what Oracle calls a schema. An empty schema (a schema with no objects), by definition, consumes 0 bytes of storage. That's because space is allocated to tablespaces which are independent of schemas-- a single schema can have objects in many different tablespaces, a single tablespace can have objects owned by many different schemas, and a single object (if you're using partitioning) can be spread across multiple tablespaces. 

Walking through an example My database block size is 8k and we'll assume that I'm using the default of 10 (meaning 10% of the block is reserved for future updates that increase the size of the row). I'll create a simple two-column table 

It would be helpful if you could post the actual query you're running. Even better would be if you created a SQL Fiddle so that we can play around with your data and your tables. It appears, however, that you simply want to replace your clause with 

The error you're getting indicates that a query is trying to reference an object that doesn't exist or that the user doesn't have access to. If you started getting it a week ago and weren't getting it before that, something changed a week ago so that an object was dropped, a privilege was revoked, or a SQL statement was added that references an invalid object. It is unlikely that upgrading the database would resolve the error. Can you determine the SQL statement that your application is issuing when it receives this error? If not, can you add additional logging so that you can determine what statement is generating the error? If your application doesn't log the SQL statements it generates, you could create a trigger (based on this trigger code). Obviously, you would probably want to log more than just the SQL statement. 

Note that you need to include the in the query in order for it to make sense. Oracle may use a table scan (depending on whether or not and/or the column(s) in the are indexed) but there will be a stopkey limiter so Oracle knows it can stop scanning the table (or the index) once 60 rows have been read. In 12.1 and later, you can simplify this a bit 

Changing the size of your redo logs won't affect how much space is occupied by archived redo logs. If you reduce the size of your redo logs from 200 MB to 100 MB, you'll end up with twice as many archived redo logs each of which is half the size. The volume of archived logs that you generate is dependent on the amount of your system generates not on the size of your redo logs. In general, you want the redo logs to be large enough that you are not constantly swapping logs (generally not more than every 15 or 20 minutes) and small enough that you aren't risking too much data should there be a catastrophic server failure (assuming that archived redo logs are written to a different server and that you aren't using something like DataGuard to replicate the redo to a backup server in real time). 

Partitioned views are a (very) old technique for partitioning data that are very rarely used today. Oracle added the ability to partition tables back in Oracle 8, which provides much more functionality than partitioned views, at which point partitioned views became obsolete. The only reason to consider using partitioned views would be if you can't afford a license for the partitioning option and you're willing to accept the reduced functionality and extra maintenance required to use partitioned views. The documentation on partitioned views will pretty much all date back to the Oracle 7 days. In order to use partitioned views, you would define individual tables for each logical partition, create a constraint on whatever column you want to partition on so that the optimizer knows which table a particular row must be in, and then create a view that does a on each table to create the partitioned view. Queries against the partitioned view using the partition key can then be resolved by the optimizer to access only one of the physical tables comprising a particular logical partition. Of course, if you use this technique, you have to create a new physical table and rebuild the partitioned view every time you add a new partition which is quite a bit of maintenance. Getting the partitioned view to work correctly and making sure that partition pruning is happening correctly is much, much more challenging than when you're using a partitioned table. Plus you end up with a ton of separate tables with very similar names polluting your namespace. 

This leverages the fact that Oracle doesn't store completely NULL rows in the index so only the values that are part of transactions will be stored in the index. 

You are trying to write a query. If you're trying to use a to do so (which implies that you are not on 11g where you can simply use the operator), you need to do an aggregate 

As with all things, it depends. If you're using OCI, I assume you're developing a client/ server application. I would generally want the database in that situation to support shared server connections and I would want to request a shared server connection when the application connected to the database. That improves the performance of the application by limiting the amount of work the database has to do in order to create a new session. That has the side effect of leaving more resources available on the server for others. Using shared server connections does involve a longer code path when the database executing subsequent queries since the query has to be sent to a shared server process but that generally isn't a big deal if the number of shared server processes is reasonable. Opening a number of sessions could also be a problem if the DBA is not using automatic PGA management. If you're using manual PGA management, PGA is configured on a per-session basis so each session can allocate a separate for sorts among other PGA components. If you created a large number of sessions in a database using manual PGA management and each session tried to maximize its PGA usage, you could easily starve the server of RAM and cause performance problems for everyone. Assuming you are using Oracle 10.1 or later, however, automatic PGA management is available. In that case, the DBA configures a (or includes the PGA in the in 11g) and the database takes care of ensuring that the aggregate PGA across all sessions is limited so that the database runs out of resources. If the database supports shared server connections, your application gets a shared server connection, and the database uses automatic PGA management, most DBAs won't care too much how many sessions you create. Now, if you are creating many sessions so that you can do more work in parallel, that would create performance problems over and above the number of sessions. It's easy enough to configure the database to support 1000 sessions, for example, it's much harder to configure the database to not die in a pile if all 1000 session simultaneously issue a meaty query against a data warehouse. If your application is using up all the resources available to the database for your queries, the DBA would probably want to consider using Oracle Resource Manager to prioritize different applications and/or different users. For example, the DBA could configure Resource Manager so that if the CPU utilization goes to 100%, your application in the aggregate gets 50% of the CPU, some other application in the aggregate gets 25%, and all others get the remaining 25%. If no other requests were pending, your application would be free to use all 100% of the CPU. If you are running things in parallel, it might also be useful to investigate Oracle's ability to run statements in parallel because that might involve less overhead than writing your own parallelization code. I would expect, for example, that it would be far easier and quite possibly faster to write a client application that serially submitted statements that used Oracle parallel query to execute rather than opening multiple sessions and executing each statement from a separate thread of your application while Oracle was not using parallel query to execute any of the statements. If you use parallel query, the database can also adjust the number of parallel slaves that are spawned so that when the database is particularly busy fewer parallel slaves are started and when the database is relatively idle more parallel slaves are started. That would potentially satisfy both the desire to make the application as efficient as possible as well as to share the resources of the server more equitably with other applications.