Yes, there has been a lot of work since Cheeseman, Kanefsky and Taylor's 1991 paper. Doing a search for reviews of phase transitions of NP-Complete problems will give you plenty of results. One such review is Hartmann and Weigt [1]. For a higher level introduciton, see Brian Hayes American Scientist articles [2] [3]. Cheesemen, Kanefsky and Taylor's 1991 paper is an unfortunate case of computer scientists not paying attention to the mathematics literature. In Cheeseman, Kanefsky and Taylor's paper, they identified the Hamiltonian Cycle as having a phase transition with a pickup in search cost near the critical threshold. The random graph model they used was Erdos-Renyi random graph (fixed edge probability or equivalently Gaussian degree distribution). This case was well studied before Cheeseman et all's 1991 paper with known almost sure polynomial time algorithms for this class of graph, even at or near the critical threshold. Bollobas's "Random Graphs" [4] is a good reference. The original proof I believe was presented by Angliun and Valiant [5] with further improvements by Bollobas, Fenner and Frieze [6]. After Cheeseman, Kanefsky and Taylor, Culberson and Vandegriend [7] published a paper giving an algorithm that in practice performed extremely well for all Erdos-Renyi random graphs, even near the critical threshold. The phase transition for Hamiltonian Cycles in random Erdos-Renyi random graphs exists in the sense that there is a rapid transition of the probability of finding a solution but this does not translate to an increase in "intrinsic" complexity of finding Hamiltonian Cycles. There are almost sure polynomial time algorithms for finding Hamiltonian Cycles in Erdos-Renyi random graphs, even at the critical transition, both in theory and in practice. Survey propagation [8] has had good success in finding satisfiable instances for random 3-SAT very near the critical threshold. My current knowledge is a little rusty so I'm not sure if there's been any large progress of finding "efficient" algorithms for unsatisfiable cases near the critical threshold. 3-SAT, as far as I know, is one of the cases where it's "easy" to solve if it's satisfiable and near the critical threshold but unknown (or hard?) in the unsatisfiable case near the critical threshold. My knowledge is a bit dated now but the last time I looked at this subject in depth, there were a few things that stood out to me: 

While I see no direct connection between the two, the tricks and final form of this problem have a lot of commonalities with the Birthday Problem as initially guessed at in the comments. 

I read this to mean that, by Ladner's, there are problems that are neither ${\bf P}$ nor ${\bf NP}$-complete, but by Schaefer's, problems are either ${\bf P}$ and ${\bf NP}$-complete only. What am I missing? Why don't these two results contradict each other? I took the condensed version of the above theorem statements from here. In his "Final Comments" section, he says "Thus, if a problem is in ${\bf NP} \setminus {\bf P}$ but it is not ${\bf NP}$-complete then it can not be formulated as CSP". Does this mean ${\bf SAT}$ problems miss some instances that are in ${\bf NP}$? How is that possible? 

While the constants are off, the form of the function appears to be correct. Below is a log-log plot for varying $n$ with each point being the average of 100,000 instances as a function of $m$: 

Not sure if this is what you were looking for but phase transitions of NP-Complete problems rely heavily on probabilistic arguments, which are just another form of counting. LLL has been used to solve some 'low-density' Subset Sum problems, the success of which relies on high probability short lattice vectors existing that meet the criteria of being a Subset Sum solution. Survey Propagation relies on the structure of solution space (and the number of solutions as it fixes variables) to find solutions near the critical threshold. Borgs, Chayes and Pittel have pretty much completely characterized the phase transition of the (Uniform) Random Number Partition Problem and thus have characterized how many solutions one can expect for a given (random) instance of the Number Partition Problem. 

Unfortunately it's behind a paywall so I'm unable to view that paper but from reading the abstract it bears at least a superficial similarity to some "cartoon pictures" that I've seen on survey propagation and it's use to solve 3-SAT. Here is a "cartoon picture" from Maneva, Mossel and Wainwright's "A New Look at Survey Propagation and Its Generalizations" 

Lattice reduction techniques, such as LLL or PSLQ, are highly geometric and solve problems of pure number theory, such as linear Diophantine approximation and integer relation detection. LLL was used to provide a polynomial time algorithm to factor (univariate) polynomials over $\mathbb{Z}$ whose co-efficients are drawn from $\mathbb{Z}$. 

While reading Dick Lipton's blog, I stumbled across the following fact near the end of his Bourne Factor post: 

A professor I knew in grad school told me about asking his students to reduce an NP-Complete problem to another, then back to the original, then back again and then watching with amusement as the resulting reductions grew out of control. Eventually this led me to think that there should be some theory that could get a handle on the size introduced by reductions. For example, what prevents us from taking a source NP-Complete problem (3-SAT, say) and adding a particular type of structure in the resulting NP-Complete problem (HAMILTONIAN CYCLE, say) such that reducing back to the original (3-SAT) would notice the structure and get back the original instance? Maybe the original problem would need to be 'pre-structured' so that this type of 'ping-ponging' between NP-Complete problems could occur. In other words, what prevents us from adding some (polynomial amount of) structure to our NP-Complete problems so that reductions from one NP-Complete problem to another converge in size? The algorithms for reductions would need to notice the structure embedded in the instance and add their own if need. I could imagine the features to be indicators for the reduction algorithm so that subsequent reductions would be able to avoid instance inflation. What do these reductions look like? What's our current best 'inflation factor'? Surely I'm not the first to think of this. Does anyone have references that discuss this type of reduction? Barring that, does anyone have any ideas on what type of structure needs to be added to achieve this result? I would be surprised if such a method wasn't possible but I would happily hear about any references about impossibility of such a method. 

Usually one constructs a graph and then asks questions about the adjacency matrix's (or some close relative like the Laplacian) eigenvalue decomposition (also called the spectra of a graph). But what about the reverse problem? Given $n$ eigenvalues, can one (efficiently) find a graph that has this spectra? I suspect that in general this is hard to do (and might be equivalent to GI) but what if you relax some of the conditions a bit? What if you make conditions that there is no multiplicity of eigenvalues? What about allowing graphs that have "close" spectra by some distance metric? Any references or ideas would be welcome. EDIT: As Suresh points out, if you allow undirected weighted graphs with self loops, this problem becomes pretty trivial. I was hoping to get answers on the set of undirected, unweighted simple graphs but I would be happy with simple unweighted directed graphs as well. 

I hesitate to include it here as I have not published any peer reviewed papers from it but I did write my thesis on the subject. The main idea is that a possible class of random ensembles (Hamiltonian Cycles, Number Partition Problem, etc.) that are "intrinsically hard" are ones that have a "scale invariance" property. Levy-stable distributions are one of the more natural distributions with this quality, having power law tails, and one can choose random instances from NP-Complete ensembles that somehow incorporate the Levy-stable distribution. I gave some weak evidence that intrinsically difficult Hamiltonian Cycle instances can be found if random graphs are chosen with a Levy-stable degree distribution instead of a Normal distribution (i.e. Erdos-Renyi). If nothing else it will at least give you a starting point for some literature review. [1] A. K. Hartmann and M. Weigt. Phase Transitions in Combinatorial Optimization Problems: Basics, Algorithms and Statistical Mechanics. Wiley-VCH, 2005. [2] B. Hayes. The easiest hard problem. American Scientist, 90(2), 2002. [3] B. Hayes. On the threshold. American Scientist, 91(1), 2003. [4] B. Bollobás. Random Graphs, Second Edition. Cambridge University Press, New York, 2001. [5] D. Angluin and L. G. Valiant. Fast probabilistic algorithms for Hamilton circuits and matchings. J. Computer, Syst. Sci., 18:155–193, 1979. [6] B. Bollobás, T. I. Fenner, and A. M. Frieze. An algorithm for finding Hamilton paths and cycles in random graphs. Combinatorica, 7:327–341, 1987. [7] B. Vandegriend and J. Culberson. The G n,m phase transition is not hard for the Hamiltonian cycle problem. J. of AI Research, 9:219–245, 1998. [8] A. Braunstein, M. Mézard, and R. Zecchina. Survey propagation: an algorithm for satisfiability. Random Structures and Algorithms, 27:201–226, 2005. [9] I. Gent and T. Walsh. Analysis of heuristics for number partitioning. Computational Intelligence, 14:430–451, 1998. [10] C. P. Schnorr and M. Euchner. Lattice basis reduction: Improved practical algorithms and solving subset sum problems. In Proceedings of Fundamentals of Computation Theory ’91, L. Budach, ed., Lecture Notes in Computer Science, volume 529, pages 68–85, 1991.