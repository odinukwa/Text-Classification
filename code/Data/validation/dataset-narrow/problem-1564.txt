I'm wondering if it's possible to view sequential data as an embedding in Tensorboard? I've been creating a RNN model that takes in a sequential data and outputs a probability value (many to one). However there isn't that much data to train, so even with the loss going down, the accuracy does no increase in the test set. Then I was wondering if it's possible to view sequence data in tensorboard while using PCA and SNE-T to see if there's any high level correlation (clusters, etc). If there's no correlations, then it might explain why the model is having a hard time to train. 

Which shows that it has a high importance on these 5 features, and they are all continous features. Then I applied LIME: 

I have an xgboost classifier trained on 82 continous variables, and 488 label encoded variables. The 82 continious variables have any values of float, and the 488 label encoded variables only have values of 0 and 1. When I checked for feature importances under the xgboost classifier: 

However, merging A with C gives a significant interaction resulting with the feature apperance in more instances than it follows from simply summing instances: $\{2nd, 3rd, 4th, 6th\}$ vs. $\{2nd, 3rd, 4th\}$. The occurrence in the 6th instance is this coalitional effect. 

Even now, we can clearly see a sort of cooperative, since those three singletons (A), (C) and (E) all together cover only three subsets {2nd, 3rd, 4th} = {2nd, 4th} $\cup$ {2nd, 3rd} $\cup$ {2nd}, whereas subset (A, C, E) triggers off the feature in most subsets except the 1st one. Moreover, merging A with E or C with E gives none additional coalitional effect, so E itself contributes poorly to overall effect. 

I have a problem that could be described as logistic regression with all dichotomous variables: 1 response variable (DV) Y (I would call it later as a feature/violet star) and 5 explanatory variables (IVs) A, B, C, D and E (also binary). What's most important, I'm trying to infer which IVs contribute the most to Y and preferably somehow assess quantitatively their severity (something like ranking predictors, variable selection, etc.) Of course, before I sent this post, I've made some reconnaissance, but I'm not sure if articles I've found reflect my problem properly, because of it's strong cooperative nature. It's very similar to coalitional games and Shapley value, but I'm not sure whether I can use it in this particular case and also I would like not to "preach to the converted" and would rather use something proven and well-tried. Could you have a look on the situation below and recommend some methods, articles, books, etc. how to infer which explanatory and to what extent contribute the most to response Y (feature/violet star). Let's say, we have five different objects denoted as labelled balls: A, B, C, D, E creating some subsets and the single special feature of interest marked as violet star. If the star is filled, the feature is present in a specific set, whereas hollow star denotes its absence. 

I'm wondering how Word2Vec is being constructed. I've read tutorials simply stating that we can train a skip grams neural network model and use the weights that are trained as word vectors. However, I've also seen this picture: 

If reading this diagram correctly: 1) The CBOW and Skip Grams model are both trained with some inputs 2) The output of CBOW is used as an input to the middle neural network 3) The output of skip grams is used as an output to the middle neural network. The output of CBOW is the a prediction of the center word given a context, and the output of skip grams is the prediction of the surrounding center word. These outputs are then used to train another set of neural network. Hence we first train the CBOW, and Skip-gram neural network, then train the middle neural network afterwards? And the input to the middle neural network is one hot encoded. Is the above interpretation correct? 

Any subset of these five objects (there're $2^5 - 1$ possible types of such nonempty subsets) can appear many times, sometimes with the feature being present and sometimes without it, as depicted below, where there're six instances of the full set and $\frac{2}{3}$ of them have the feature. We can assume (if it's needed) there's always at least one instance of the full set with the feature present (for example the one marked with thicker red envelope). For every of the $2^5 - 1$ possible nonempty subsets we have three values, e.g. for (A, C, E): 

The cardinalities of instances $T_X$ of every subset type $X$ can be different, but there's a very obvious and rather trivial rule that if $X \subset Y$ then $T_X \geq T_Y$. For example, $(A, C, E) \subset (A, B, C, D, E)$ thus $T_{ACE} \geq T_{ABCDE}$. That's because any instance of subset (A, B, C, D, E) is at the same time an instance of (A, C, E), but for narrower (A, C, E) we can have more instances. In this description I use six instances for each subset type just bacause of limited space for pictures and those six instances are intended to reflect the overall proportion between instances having and lacking the feature. Among all these $2^5 - 1$ types, some of them are more likeli to have the feature (for this particular subset/type, there're more instances with the feature than without it), whereas other types are more prone to lack it. Please look at two figures below and the following numbers: 

I have a dozen of databases that stores different data, and each of them are 100TBs in size. All of the data is stored in AWS services such as RDS, Aurora and Dynamo. Many times I find myself need to perform "joins" across databases, for example a student ID that appears in multiple databases with data that I want to gather. The joins are usually done after data is streamed out of the database, since the data is not located in the same database, and this sometimes requires hours just for thousands of records. Can services such as AWS redshift or Google BigQuery allow you to somehow "import" data from many data sources and then you can perform SQL queries to join them? How about Hadoop and Hive? Where we steam data out from the database and place it as files in Hadoop, and let Hive Query the data? 

Which is the lowest feature importances according to xgboost. Is there a way give the feature importances from xgboost to Lime? I've tried changing the kernel options but it seems like it does not work? 

$$ \frac{F_{ACE}}{T_{ACE}} = \frac{5}{6}, ~~ \frac{F_{BD}}{T_{BD}} = \frac{1}{6}, ~~ \frac{F_{ABCDE}}{T_{ABCDE}} = \frac{4}{6} ~\text{(the situation of the very first picture)} $$ A blatantly obvious property is that in my model the ratio $\frac{F}{T}$ is not monotonic (in neither direction) with respect to inclusion relationship. And finally the most crucial property I would like to make my model had, is some kind of cooperative interaction between objects in subset, which is the last thing I'm going to describe beneath. A moment before, we've spotted that the subset (A, C, D) is exceptionally favorable for the feature appearance, so we would pay a special attention to its constituent singletons and their corresponding ratios: $$ \frac{F_A}{T_A} = \frac{2}{6} ~~ \text{(2nd & 4th)}, ~~ \frac{F_C}{T_C} = \frac{2}{6} ~~ \text{(2nd & 3rd)}, ~~ \frac{F_E}{T_E} = \frac{1}{6} ~~ \text{(2nd only)} $$ 

Nonetheless, the E is necessary to trigger off the full potential and reveal the feature also in the 5th instance, as depicted in the second figure. To sum up, I would like to infer from the data which single object are favourable for the feature appearance. In our example we can clearly see that A and C are very conducive (in cooperation with other), E is not so useful, but required to reach the full potential, whereas B and D seem to have no cooperative effect on the feature.