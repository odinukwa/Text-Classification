Here $\alpha_d$ is the value for the density transition and $\alpha_c$ is the critical threshold value ($\approx 4.2$ for 3-SAT). It'd be interesting to see if the locations of the different fractal regions reported by Ercsey-Ravasz and Toroczkai correspond to the different critical thresholds noticed in survey propagation (or if I'm completely wrong and the similarity really is superficial). 

Given a list of (small) primes $ (p_0, p_1, \dots, p_{n-1})$, is there an (efficient) algorithm to enumerate, in order, all numbers that can be expressed as $ \prod_{k=0}^{n-1} p_k^{e_k} $, where $e_k \in \mathbb{Z}, e_k \ge 0 $? What about in a certain interval, potentially at an exponential starting point? For example, if we had the set $(2,3,5)$, the first few numbers would be $(2, 3, 2^2, 5, 2 \cdot 3, 2^3, 3^2, 2 \cdot 5, \dots )$. Is there an algorithm to efficiently enumerate all the numbers not expressible as a product of powers of primes from the set? How about in an interval? Note: I just saw the Polymath paper on deterministic prime finding in an interval ( Deterministic methods to find primes ) and thats what inspired this question. I don't know if it's important that the set be a list of primes, but I'll keep it in there just in case. EDIT: I was unclear by what I meant by 'efficient' . Let me try making it more precise: Given a list of $n$ primes $(p_0, p_1, \dots p_{n-1})$ and a bound, $B$, is it possible to find in polynomial time with respect to $lg(B)$ and $n$, the next integer, $x$, such that $x > B$ and is expressible as a product of powers of primes from the list? 

Asking whether a graph is non-Hamiltonian is CoNP-Complete so it is doubtful that such an algorithm exists. If you are satisfied with a heuristic that provides a certificates of non-Hamiltonicity, see Bondy and Murty's book (page 53) for a sufficient, but not necessary, condition for non-Hamiltonicity: If $G$ is Hamiltonian then, for every nonempty subset, $S$, of $V$: $$ \omega(G - S) \le |S| $$ Where $\omega(G - S)$ is the number of connected components left in $G$ after removing the vertices in $S$. A simple counterexample for the converse is the peterson graph: It passes the above test but is not Hamiltonian. 

I would appreciate any reference or thoughts on this matter. Please feel free to consider other minor closed properties, the list given above is only illustrative. Obs: By explicit I do not mean necessarily small. It is enough to give an explicit argument or algorithm showing how to construct the formula characterizing the given property. Similarly, in the context of this question I consider a family of forbidden minors to be known if one has given an explicit algorithm constructing that family. EDIT: I found a paper by Adler, Kreutzer, Grohe which constructs a formula characterizing graphs of genus $k$ with basis on the formula characterizing graphs of genus k-1. So this paper answers the first two items of Question 2. On the other hand this does not answer Question 1 because there is indeed an algorithm that constructs for each k, the family of forbidden minors characterizing graphs of genus k (See section 4.2). Therefore this family is "known" in the sense of the question. 

The notions of tree-decomposition and treewidth can be generalized to arbitrary relational structures. See for instance sections 2 and 3 of this paper by Dalmau, Kolaitis and Vardi. Courcelle's theorem states that MSO logic can be decided in linear time on relational structures of constant treewidth. A theorem of Bodlaender implies that if a relational structure has treewidth $t$, then a tree-decomposition of such structure can be found in time $f(t)\cdot n$. In other words, such a decomposition can be found in linear time on graphs of constant treewidth. One can define suitable relational structure $\tau$ which can be used to encode a formula $F$ together with its incidence graph $I$. The treewidth of $\tau$ is at most a constant plus the treewidth of $I$. The set of relational structures $\tau$ which encode satisfiable formulas + their incidence graphs is MSO definable. Therefore, by Bodlander+Courcelle's theorem, one can decide whether a formula of constant treewidth is satisfiable in linear time. Elberfeld-Jakoby-Tantau-2010 show that "linear time" can be replaced by "logarithmic space" on both Bodlaender and Courcelle's theorem. Therefore, for each MSO formula $\varphi$, and each relational structure $\tau$, one can determine in logarithmic space whether $\tau$ satisfied $\varphi$. In particular, SAT can be determined in logspace on graphs of constant treewidth. 

I'm not sure if this is what you're looking for but there's a sizable literature on the 3-SAT phase transition. Monasson, Zecchina, Kirkpatrcik, Selman and Troyansky had a paper in nature that talks about the phase transition of random k-SAT. They used a parameterization of the ratio of clauses to variables. For random 3-SAT, they found numerically that the transition point is around 4.3. Above this point random 3-SAT instances are over constrained and almost surely unsatsifiable and below this point problems are under constrained and satisfiable (with high probability). Mertens, Mezard and Zecchina use cavity method procedures to estimate the phase transition point to a higher degree of accuracy. Far away from the critical point, "dumb" algorithms work well for satisfiable instances (walk sat, etc.). From what I understand, deterministic solver run times grow exponentially at or near the phase transition (see here for more of a discussion?). A close cousin of belief propagation, Braunstein, Mezard and Zecchina have introduced survey propagation that is reported to solve satisfiable 3-SAT instances in millions of variables, even extremely close to the phase transition. Mezard has a lecture here on spin glasses (the theory of which he has used in analysis of random NP-Complete phase transitions) and Maneva has a lecture here on survey propagation. From the other direction, it still looks like our best solvers take exponential amount of time to prove unsatisfiability. See here, here and here for proofs/discussion of the exponential nature of some common methods in proving unsatisfiability (Davis-Putnam procedures and resolution methods). One has to be very careful about claims of 'easiness' or 'hardness' for random NP-Complete problems. Having an NP-Complete problem display a phase transition gives no guarantee as to where the hard problems are or whether there even are any. For example, the Hamiltoniain Cycle problem on Erdos-Renyi random graphs is provably easy even at or near the critical transition point. The Number Partition Problem doesn't seem to have any algorithms that solve it well into the probability 1 or 0 range, let alone near the critical threshold. From what I understand, random 3-SAT problems have algorithms that work well for satisfiable instances nearly at or below the critical threshold (survey propagation, walk sat, etc.) but no efficient algorithms above the critical threshold to prove unsatisfiability. This is just state of the art right now and could of course change in the future. 

It seems that this idea is attributed to Levin (It is called optimal search). I believe this fact is well known. A similar algorithm is described in wikipedia for instance, although using the subset sum problem. In this article from scholarpedia you can find several references on the subject, including a pointer to the original algorithm and to some other optimal search algorithms. Comment 1: Levin's optimal search guarantees that if $\varphi$ is a satisfiable instance then a solution will be found in polynomial time assuming $P=NP$. If $\varphi$ is not satisfiable the algorithm may not terminate. Comment 2: As Jaroslaw Blasiok pointed out in another answer, this algorithm does not decide Sat only assuming P=NP. 

Does the problem given above has always a solution? In other words, from $G$ we want to construct a context free grammar $G'$ accepting the same language as $G$ but such that every string in this language has a parse tree of logarithmic height. The size of the obtained grammar $G'$ is allowed to blow up polynomially in $n$ and in the size of the original CFG $G$. I'm mostly interested in references dealing with the problem above or similar problems. Obs 1: Without the requirement that $|G'|=|G|^{O(1)}\cdot n^{O(1)}$, we can construct a grammar $G'$ with size $2^{O(n)}$ by considering a distinct set of production rules for each string in $L(G)$. Obs 2: I don't care about the time necessary to construct $G'$. The only important thing is its size $|G'|$. Obs 3: Both grammars are required to be in Chomsky normal form. Also both are allowed to be ambiguous. 

Let $G$ be a context free grammar in Chomsky normal form (CNF) with language $L(G)\subseteq \Sigma^n$. In other words, all strings generate by $G$ have size $n$. Say that a string $w\in L(G)$ has height $h$ if $w$ has a parse-tree of height at most $h$. Say that $G$ has height $h$ if each string $w\in L(G)$ has height $h$. Let $|G|$ be the number of production rules in $G$. I have the following problem, which I believe it is well studied in the field of parallel parsing, but with a somewhat distinct terminology. Problem: Given a context free grammar $G$ in CNF accepting a language $L(G)\subseteq \Sigma^n$, construct a context free grammar $G'$ in CNF such that 

Strictly speaking, the paper pointed out by singsumit handles the P2P shortest path problem (not the all pairs shortest paths problem). If you really want to compute the all pairs shortest paths problem, I will recommend you to read these two papers. 

There is a sort algorithm with $O(1)$ auxiliary words and achieving $O(n\log n)$ worst-case run time, where $n$ is the length of the input array. $URL$ I don't think that the question seeks an algorithm with $O(1)$ auxiliary bits, because such algorithm must be tricky. I mean, if we cannot use even $O(\log n)$ bits, then we cannot use a simple for-loop (for(i=0, i < n, i++) {...} ) in the algorithm without introducing variable i (which require additional $O(\log n)$ bits). 

It is well known that an instance of $n$ men/women can have an exponential number ($O(2^n)$) of stable matchings, but giving a tight upper bound is still open. See Encyclopedia of algorithms $URL$ 

Your problem is the multiple knapsack problem. Although I am not familiar with this problem, I believe you'll find some papers on your problem, since there are many papers on this problem (see for example a SODA 2009 paper) 

My recommendation is the survey paper written by the inventors of the R-MAT random graph generator. $URL$ The R-MAT random graph generator is very simple and widely used. For example, this generator is adopted in the Graph500 benchmark ( $URL$ ). 

Removing the min/max operations in your problem, you can write your problem in a standard mixed integer programming. If your problem contains $N$ min/max operations, then your problem can be solved in $O(2^N)$ time, ignoring polynomial factors. The removal of the min/max operation is easy. For example, let us remove Max[b,c]. First, add a variable x such that x = Max[b,c], and replace Max[b,c] with x. The relation x = Max[b,c] can be written as the following inequalities adding another binary variable y (i.e, y=0 or y=1): x >= b x >= c x <= by + c(1-y) You can remove the min operations in a similar way, and you'll obtain a standard mixed integer programming formulation. 

The problem with using Erdos-Renyi random graphs ($G(n,p)$ or $G(n,m)$) is that they follow a Poisson degree distribution, which gives them finite second moment. Many real world graphs, including the "Web graph" or the "Internet Graph", tend to not follow this degree distribution in favor of a degree distribution that has much more variability in it's second moment. In my opinion, one of the biggest differences is the power law degree distribution that many of them have. See Emergence of Scaling in Random Networks for example. As you probably know, there appears to be a difference between the connectivity graph for the World Wide Web and opposed the connectivity graph for the Internet infrastructure. I certainly don't claim to be an expert, but I've seen Li, Alderson, Tanaka, Doyle and Willinger's paper "Towards a Theory of Scale-Free Graphs: Definition, Properties, and Implications" who introduce an 's-metric' to measure the 'scale-freeness' of a graph (with the definition of scale-free graphs still under debate as far as I know) that claim to have a graph model that creates graphs that are similar to the internet connectivity at a router level. Here are a few more generative models that might be of interest: Berger, Borgs, Chayes, D'Souza and Kleinberg's paper "Competition-Induced Preferential Attachement" Carlson and Doyle's Highly Optimized Tolerance: A Mechanism for Power Laws in Designed Systems Molloy and Reed's A Critical Point for Random Graphs with a Given Degree Sequence which introduces the "Erased Configuration Model" Newman's Clustering and preferential attachment in growing networks (which has been mentioned already) One could also explicitly generate a degree distribution and create a graph this way, but it's unclear to me how close this models the internet graph at a router level. There is, of course, much more literature on the subject and I've only given a few of (what I consider to be) the highlights. As far as I understand, many results that worked for the Erdos-Renyi models of random graphs ($G(n,p)$ or $G(n,m)$) do not work precisely because the scale-free or power law degree distributed random graphs diverging second moment in the degree distribution. I don't claim to know enough about to the subject to categorically make claims about "most" proofs, but from what I've seen, one of the first few lines of proofs for properties on Erdos-Renyi random graphs explicitly assumes a finite second moment in the degree distribution. From my point of view, this makes sense as a finite second moment makes Erdos-Renyi graphs much more locally tree-like (see Mertens and Montanari's Information, physics, and computation) which effectively gives properties/paths/structures independence. Since power-law degree distributed random graphs have a diverging second moment, this local tree-like structure is destroyed (and thus requires different proof techniques?). I would be happy to have this intuition invalidated if someone with more knowledge or insight were to show why this is not so. Hope that helps. 

Upon request of Marzio De Biasi I'm converting my comment into an answer. A graph is asymmetric (some authors refer to it as rigid) if it has a unique automorphism, i.e., the identity. As pointed out by Chad Brewbacker, most graphs are asymmetric. However the following two questions are open: 1) Is isomorphism of asymmetric graphs in P? 2) Can isomorphism of general graphs be reduced to isomorphism of asymmetric graphs? Question 1) has received a lot of attention in quantum computing due to the fact that the isomorphism of asymetric graphs can be reduced to the nonabelian hidden subgroup problem and to the non abelian hidden shift problem. However the results are negative, showing that one needs to prepare at least $\Omega(n\log n)$ hidden subgroup states or hidden shift states to have enough information to solve the isomorphism problem. 

Genus 1 (the graph is embeddable in a torus) (see EDIT below) Genus k for some fixed $k>1$ (see EDIT below) k-outerplanarity for some fixed $k> 1$ 

I would also be interested in classes of graphs with treewidth $O(\log^k n)$ or $O(\log\log...n)$ where the logarithm is repeated a constant number of times. Obs: Of course it is easy to cook up artificial families of graphs with a given treewidth, like the family of $\;O(\log n)\times n\;$ grids. So I'm primarily looking for family of graphs which have been studied in other branches of graph theory and which happen to have treewidth $O(\log n)$ or $O(\log\log n)$, but non-constant treewidth. 

Any references for this problem, or suggestions of algorithms are welcome. Note that the notion of permutation avoiding subsequence defined above is not the same as the notion of permutation avoiding pattern where only the relative order of elements is important, and which seems to be well studied in combinatorics. 

As suggested by Peter, I converted my comment to an answer: M. Ajtai's paper "The Shortest Vector Problem in $L_2$ is $NP$-hard for Randomized Reductions." discusses complexity results of finding shortest vectors for lattice reduction using randomized reduction steps. 

This gives us the (polynomial) run time. Proof of correctness that for a properly setup lattice will give you the minimal factor of a reducible polynomial is a bit more involved, but please refer to theorem 14 of the same chapter to see the relation between the reduced basis and the minimal polynomial. By setting up the basis with dimension $n=k$ you can easily see the bound as roughly $O(k^5( \lg(||f(x)||_{\infty}^3) + \log n))$. Since, by assumption, you know the degree of $f_1(x)$ and $f_2(x)$, the lattice reduction algorithm only needs to be run once to find one of the two factors of $f(x)$. You can then use the discovered $f_j(x)$ to find the other by standard polynomial division. The original paper by Lenstra, Lenstra and Lovasz, "Factoring polynomials with rational coefficients", is also quite readable and I found it to be a good compliment to Yap's introduction. 

Fundamental Algorithms in Algorithmic Algebra by Chee Yap (available online here). This text covers (fast) integer multiplication, polynomial root finding, integer polynomial factorization, lattice reduction techniques (specifically LLL), elimination theory, Grobner bases and continued fractions, all from an algorithmic perspective. I found this text indispensable when learning about lattice reduction. 

In other words, the $(2^n)!$, which has an exponential number of bits, can potentially be represented efficiently. I have a few questions: 

Yes, assuming you want both $f_1(x)$ and $f_2(x)$ with integer coefficients. One of the reasons why LLL is so popular is precisely because it gives a polynomial time algorithm to factor polynomials with integer coefficients. For an excellent introduction, I recommend C. Yap's "Fundamental Problems in Algorithmic Algebra" (available online, for free), specifically chapter 9 "Lattice Reduction and Applications" (section 9.6). Following Yap, choose an approximation, $\alpha$, of a (complex) root for $f(x)$. Setup the lattice reduction with the following basis: $$ B_k = \begin{bmatrix} \text{Re}(\alpha^0) & \text{Re}(\alpha^1) & \text{Re}(\alpha^2) & \cdots & \text{Re}(\alpha^k) \\ \text{Im}(\alpha^0) & \text{Im}(\alpha^1) & \text{Im}(\alpha^2) & \cdots & \text{Im}(\alpha^k) \\ c & 0 & 0 & \cdots & 0 \\ 0 & c & 0 & \cdots & 0 \\ 0 & 0 & c & \cdots & 0 \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & 0 & \cdots & c \end{bmatrix} $$ Choosing $c = 2^{-4t^3}$, with $\alpha$ to have $O(t^3)$ bits for each of the real and complex portions. Here, $t = \log ||f(x)||_{\infty}$ (that is, the cube of the number of bits of the maximum coefficient of $f(x)$). Quoted from FPiAA: