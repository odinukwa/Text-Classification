You are right that improved reductions from CNF-SAT to any one of various planar graph problems would give improved algorithms for CNF-SAT (via graph algorithms with runtimes exponential in treewidth; such algorithms exist for many graph problems). If you could get $|V(G)| = o(m^2)$ in the reduction you mention, this would imply that the Exponential Time Hypothesis (ETH) is false. This is well-appreciated, yet it has not led to any improvements on running times for CNF-SAT (as far as I know). For more on how ETH has been used to classify the difficulty of planar (and non-planar) graph problems, see e.g. this survey of Lokshtanov, Marx, and Saurabh. As for reducing the constant in such reductions, achieving $|V(G)| = c m^2$ for $c$ as small as possible, I don't know how much effort has been devoted to this. 

Two comments, neither of which amount to an answer, but which may provide some useful further reading. 1) Schöning defined two classes of NP problems called the "Low Hierarchy" and the "High Hierarchy", that are related to your notions. In particular, problems in LowH are "on the P-side", and problems in HighH are on the NP-side. A number of well-known results in complexity can be stated in this framework. For example, the Karp-Lipton theorem says that NP is not in P/poly unless PH collapses; this is a consequence of the fact that NP $\cap$ P/poly is contained in a fixed level of LowH (as the Karp-Lipton proof technique shows). Note that we don't expect that NP $\cap$ P/poly, or LowH, is contained in P. For a survey on LowH in particular, see $URL$ 2) Consider the problem where we are given a full truth table of a Boolean function, and asked if it has a Boolean circuit of some size $t$. This problem is in NP, and unlikely to be in P (it would imply several surprising consequences). On the other hand, a proof of NP-completeness for this problem, if it obeys certain fairly natural restrictions, would give us powerful new results in complexity theory. This was shown by Kabanets and Cai in $URL$ To be clear, there's no real evidence that this problem is not NP-hard, or that it is easy in any sense. But it seems quite different from other hard problems in NP. I think this is among the most interesting candidates for NP-intermediate problems, and not one that is well-known. 

The problem is coNP-complete and so unlikely to have a poly-time algorithm. (I'm sure the observation was made before; I don't know where, but look in Garey-Johnson.) Here is a simple reduction from 3-UNSAT $= \{\psi: \psi$ is an unsatisfiable 3-CNF$\}$. Say $\psi(x_1, \ldots, x_t) = \bigwedge_{j = 1}^m (y_{j, 1}\vee y_{j, 2} \vee y_{j, 3})$, where each $y_{j, a}$ is either a variable or a negated variable. In your notation, let $A = B = \ldots = K = \{0, 1\}$, with $t$ such sets (one for each variable). Think of each element of $S = A^t$ as an assignment to $x_1, \ldots, x_t$. For each $j \in [m]$, create a Cartesian product set $S_j$ which describes the set of assignments that will falsify the $j^{th}$ clause. Note that this can easily be done: we restrict precisely the 3 components corresponding to the variables appearing in this clause. The union of the $S_j$'s equals all of $S = \{0, 1\}^t$ iff every assignment to $\psi$ falsifies some clause, i.e., iff $\psi$ is unsatisfiable. 

If the function f is in #P, then given an input string x of some length N, the value f(x) is a nonnegative number bounded by $2^{poly(N)}$. (This follows from the definition, in terms of number of accepting paths of an NP verifier.) This means that many functions f lie outside of #P for uninteresting reasons---either because f is negative, or, in the case you mention, because the function grows faster than $2^{poly(N)}$. But for the $n$-queens problem as modeled in the paper, this is just an artifact of the authors' decision to let the input value $n$ be encoded in binary. If the expected input was the unary string $1^n$, then $f(1^n) :=$ (number of valid $n$-queen configurations) would certainly be in #P, by a simple NP verifier that checks validity of a given configuration. If you want to explore some functions that (conjecturally) lie outside of #P for more interesting reasons, consider e.g. these: 

Lattice problems are a good source of candidates. Given a basis for a lattice $L$ in $R^n$, one can look for a nonzero lattice vector whose ($\ell_2$) norm is smallest possible; this is the 'Shortest Vector Problem' (SVP). Also, given a basis for $L$ and a point $t \in R^n$, one can ask for a lattice vector as close as possible to $t$; this is the 'Closest Vector Problem' (CVP). Both problems are NP-hard to solve exactly. Aharonov and Regev showed that in (NP $\cap$ coNP), one can solve them to within an $O(\sqrt{n})$ factor: $URL$ I've read the paper, and I don't think there's any hint from their work that one can do this in UP $\cup$ coUP, let alone UP $\cap$ coUP. A technicality: as stated, these are search problems, so strictly speaking we have to be careful about what we mean when we say they're in a complexity class. Using a decisional variant of the approximation problem, the candidate decision problem we get is a promise problem: given a lattice $L$, distinguish between the following two cases: Case I: $L$ has a nonzero vector of norm $\leq 1$; Case II: $L$ has no nonzero vector of norm $\leq C\sqrt{n}$. (for some constant $C > 0$) This problem is in Promise-NP $\cap$ Promise-coNP, and might not be in either Promise-UP or Promise-coUP. But assume for the moment that it's not in Promise-UP; this doesn't seem to yield an example of a problem in (NP $\cap$ coNP)$\setminus$UP. The difficulty stems from the fact that NP $\cap$ coNP is a semantic class. (By contrast, if we identified a problem in Promise-NP$\setminus$Promise-P, then we could conclude P$\neq$NP. This is because any NP machine solving a promise problem $\Pi$ also defines an NP language $L$ which is no easier than $\Pi$.) 

Let's say an algorithm A solves a "special case" of the decision problem L if on input x, $A(x)$ always either outputs the correct answer $L(x)$, or outputs "?". These algorithms (which may be randomized) are sometimes called "errorless heuristics" and have been studied in complexity theory, esp. average-case complexity; see e.g. this paper of Watson and references therein. A basic question you can ask about a very difficult decision problem is whether it contains any nontrivial easy cases. E.g. if L is decidable, is there any (deterministic) errorless heuristic A for L that solves $L(x)$ on infinitely many inputs? This has been explored in the notion of immunity (along with co-immunity, bi-immunity, simplicity and others) in computability theory. See e.g. this wiki article. Complexity-theoretic analogues have been studied as well. Since you mention multiple instances, I might also interpret the question as looking for research into algorithms which, given instances $x^1, \ldots, x^m$ for a decision problem (i.e. language) L, compute some partial information about the membership/nonmembership of the given strings in $L$. One of the earliest and most influential works of this type was by Jockusch, who in his dissertation and this 1968 paper defined "semi-recursive" languages. L is semi-recursive if there is a computable "selector" function that, given instances $x^1, x^2$, selects one of these two strings as being "logically more likely" to lie in L. That is, we require that whenever at least one of $x^1, x^2$ lie in L, the string selected by the selector lies in L. The study of polynomial-time analogues of this concept was initiated by Selman in this 1979 paper. Since then there has been tons of work on "p-selective" languages (the polytime version of semi-recursive languages) along with "membership comparable" languages and many other related concepts. I can't summarize all this work, but consult e.g. this article of Beigel, Fortnow, and Pavan, and this book of Hemaspaandra and Torenvliet. The concept of bounded queries in computability and complexity is also related, see this survey by Gasarch. 

3-SAT may be one such problem. Currently the best upper bound for Unique 3-SAT is exponentially faster than for general 3-SAT. (The speedup is exponential, although the reduction in the exponent is tiny.) The record-holder for the unique case is this paper by Timon Hertli. Hertli's algorithm builds upon the important PPSZ algorithm of Paturi, Pudlák, Saks, and Zane for $k$-SAT, which I believe is still the fastest for $k \geq 5$ (see also this encyclopedia article). The original analysis showed better bounds for Unique $k$-SAT than for general $k$-SAT when $k = 3, 4$; subsequently, however, Hertli showed in a different paper that you could get the same bounds for (a slightly tweaked) PPSZ algorithm, without assuming uniqueness. So, maybe uniqueness helps, and it can definitely simplify the analysis of some algorithms, but our understanding of the role of uniqueness for $k$-SAT is still growing. There is evidence that Unique $k$-SAT is not too much easier than general $k$-SAT. The Strong Exponential Time Hypothesis (SETH) asserts there is no $\delta < 1$ such that $n$-variable $k$-SAT is solvable in $O^*(2^{\delta n})$ time for each constant $k \geq 3$. It was shown in a paper of Calabro, Impagliazzo, Kabanets, and Paturi that, if SETH holds, then the same statement is true for Unique $k$-SAT. Also, if general $k$-SAT requires exponential time, i.e. there is some $k \geq 3, \epsilon > 0$ such that general $k$-SAT cannot be solved in time $O^*(2^{\epsilon n})$, then the same must be true for Unique 3-SAT. See the paper for the most general statement. (Note: the $O^*$ notation suppresses polynomial factors in the input length.) 

I will point out a simple connection to nondeterministic circuits, and comment briefly on cryptographic hardness. For $S \subseteq \{0, 1\}^n$, define the image complexity, denoted $imc(S)$, as the minimal number of gates in any (fanin-two, AND/OR/NOT) Boolean circuit $C: \{0, 1\}^m \rightarrow \{0, 1\}^n$ whose image is $S$. The question asks about the complexity of computing $imc(S)$, given a truth-table representation of $S$ (a string of length $2^n$). Also define the nondeterministic circuit complexity of $S$, which we'll denote $ncc(S)$, as the smallest nondeterministic circuit $C(x, y): \{0, 1\}^{n + m'} \rightarrow \{0, 1\}$ accepting exactly $S$. That is, we require of $C$ that $x \in S$ iff $\exists y: C(x, y) = 1$. This is a standard notion, used to define the non-uniform class $NP/poly$: it is the class of all sets $S = \{S_n\}_{n > 0}$, with $S_n \subseteq \{0, 1\}^n$, such that $ncc(S_n) \leq poly(n)$. What I wanted to point out is that $imc(S) = ncc(S) \pm O(n)$. Both directions of this inequality are simple to verify. Let $dcc(S)$ denote the deterministic circuit complexity. Using Razborov-Rudich, the paper that Dai Le mentions shows (roughly speaking here) that under certain cryptographic assumptions, it is computationally hard to distinguish truth-tables of $S$ with $dcc(S)$ small, from truth-tables of truly random $S$ (with $dcc(S)$ near-maximal). Random $S$ also have $ncc(S)$ nearly-maximal, and we of course have $ncc(f) \leq dcc(f)$. So your problem is hard under the same assumptions. Which is harder to compute given a truth-table for $S$, $dcc(S)$ or $ncc(S)$? Is there a reduction either way? I don't know. 

for Question 1, the answer is Yes, and can be shown as follows. (I will also be implicitly sketching an affirmative answer to Q4, since the argument is uniform and will treat all input lengths at once.) Fix any NP-complete language $L$, and a family of good binary error-correcting codes (with rate 1/4 and correcting from a .1 fraction of errors, say). Let $Enc_n: \{0, 1\}^n \rightarrow \{0, 1\}^{4n}$ be the encoding function for length $n$; we use some such code where $Enc = \{Enc_n\}$ is computable by a uniform polynomial-time algorithm. Define $L'$ as the set of strings $z$ that are within distance at most $.05 |z|$ from a codeword $y \in Enc(L)$ encoding some element of $L$. Note that $L'$ is in NP, as you can guess and check the nearby codeword, the decoded word, and the NP certificate for the decoded word's membership in $L$. Then the claim is that any "approximation" of $L'$ in your sense is NP-hard for $\varepsilon = .01$. For if we consider a valid codeword $y = Enc(x)$ of some length $4n$, then with probability $1 - o(1)$ over a random $\varepsilon$-perturbed version $y'$ of $y$, it will disagree with $y$ in at most a .05 fraction of coordinates, and will therefore disagree with any other codeword of $Enc_n$ in a more than $.05$ fraction of coords. For such $y'$ we have $y' \in L'$ iff $x \in L$. So if $h$ is any approximation to the $\varepsilon$-smoothed version of $L'$ in your sense, then we must have $h(y) = L(x)$. As $Enc$ is efficiently computable, this gives us a way to efficiently reduce membership questions for $L$ to ones for $h$. So $h$ is NP-hard. Two notes: (1) error-correcting encodings of NP instances have been used before in several papers, notably D. Sivakumar: On Membership Comparable Sets. J. Comput. Syst. Sci. 59(2): 270-280 (1999). (2) the argument above of course says nothing about the average-case complexity of any NP problem, since error-correction is being applied on an instance-by-instance basis.