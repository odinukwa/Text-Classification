The normal way do do audit logging of this sort is to have a shadow table and log changes with triggers on the base table you are auditing. The other tables can be placed on a different physical disk if you need to to that for performance, and you can put indexes on them if you need to support quick retrieval of the data. The tables will have roughly the same structure as your original tables, but will have a datetime column for when the change took place and a marker for whether the row was inserted, changed or deleted. Sequencing the versions can be done by the time stamp. The change date can be done by making the datetime column not null with a default of getdate(); an audit user column will capture the user with a not null column defaulted to Suser_Sname(). Assuming the actual user is being impersonated in the session this will capture the identity of the user making the change. The database has no way to be aware of the IP address connecting to a web server. The application will have to explicitly capture and log the IP address with the transaction. If you have a large number of tables you wish to audit, you can use the metadata from the system data dictionary to generate the triggers programmatically. This solution is by far the best for several reasons: 

Oracle comes with less B.I. tooling out of the box, although Oracle do sell OLAP servers, reporting tools and ETL tooling. 

See the comment on the question about #1 and #2. I don't believe MySQL supports recursive CTEs, so a union is likely to be the best way to do this. The self joins are only going to work where the parent key is not null, because NULL will never evaluate as equal to anything (unless MySQL has weird NULL semantics). In order to capture the parent the is a reasonable approach. Left joins for this are barking up the wrong tree because you can only specify them as a join predicate for the child and you're trying to include data from a parent row that actually has no join against itself. Another approach would be to make equal to on the pop () row, so the self join would also pick up the parent. You can tell the parent because the and keys are the same value. 

One point that hasn't been mentioned here is completely irrelevant to the feature set. If you're doing a new build you can put off a database upgrade for quite a bit longer, so this will save in migration costs. For a greenfield project you have some breathing space to work around bugs and raise them with the vendor if they do turn up, so it's not a completely uncontrolled process. I was involved in one of the first data warehouse projects on SQL Server 2005 just as it went to RTM and we got away with it. If the feature set of 2008R2 will do what you want then the decision is down to some risk of bugs/workarounds vs. the value of postponing the necessity to upgrade and saving an upgrade cycle. 

Managing environments I think you definitely don't want to be forced into a single database version. You've got enough developers that you will inevitably have multiple development work streams, and requirements to apply patches to the current production environment independent of development workstreams. You can use Liquibase or a manual process to produce patch scripts to upgrade versions. I suggest starting out with a manual process and using the schema comparison tool for QA on the patches. Clean, automated, transparent synchronisation of a nontrivially complex database is a bit utopian. Your central data model can be kept in whatever system takes your fancy. I've used everything from tedious enterprise repository tools to create table scripts. Create table scripts play nicely with ordinary source control tools such as subversion and not all repository tools do a good job of versioning. Whatever you use as your master data model repository you need a fairly clean mechanism for deploying an environment from that model. It should be structured so that rollouts to an environment are easy. You also need a mechanism to patch from one released version to the next. What I've done I've done the following in the past when I was managing development environments. It's not particularly high tech, but it's amenable to version control and automated builds, so it makes it easy to roll out an environment to a specific version, and maitaining a large number of environments is quite practical. Maintain a central repository: This could be a set of database creation scripts held in a version control systems, or a repository model in a data modelling tool. Take your pick. This model should have a build mechanism that allows an environment to be rolled out from the scripts without a lot of manual intervention. If you have a lot of reference data you will need a load mechanism for it. Depending on how you want to do it, you could keep this in a database or in a set of files. The advantage of files is that they can also be versioned and labelled from the same version control system as your code base. A bunch of CSV files and bulk loader scripts in a source control repository can do this easily enough. One option for deploying development environments is to take backups of the production database patched to the appropriate version and make them available for devs to restore into a development environment. Make it easy to roll out: Like any CI build process, the database should be deployable from a single script. Set it up so that database connections can be paramaterised, or the script is location independent and can just be run through the connection. Patch scripts: You will need roll forward and probably roll back scripts from each released version. Build test environments from the repository model: This ensures that development on environments that are out of sync with the repository gets caught in testing. Test the deployment process: Automated patching scripts, however they are created should be testable. Schema comparison tools are quite good for this, even if you dont't use them to generate the patch scripts. 

There is pl/proxy, which provides a basic federation capability, but it's not an engine optimised for this sort of work. Greenplum is a commercial product that does have a shared nothing engine that might do what you want. However, it's not cheap - less expensive than Oracle but more expensive than SQL Server. There are no pure open-source engines of that type for PostgreSQL. However, you might get what you want from PL/Proxy. 

You won't be able to automate this process. Any release of the base system will need to include an impact analysis and change to the analytic system, and this will be a manual process. You will have to update the ETL process to source data from the new structure and possibly modify the data mart as well. This will add latency to the release process, and I've seen plenty of occasions where this was neglected and the analytic system broke. The business or owners of the transactional system will have to do one of three things: 

This is much more efficient than solutions involving multiple self-joins on the table, and will be somewhat efficient with a clustered index on the base entity and attribute type keys. You will probably also want to put type conversions into the view so you can sort the results correctly. 

This would give you 'Data', 'Functional Logic' and 'Presentation' layers as per your example. Some data warehouse systems are implemented a bit like this, although it (IMHO at least) would be something of an anti-pattern. Some issues with pushing business logic downstream of your ETL include: 

Calculate a running sum from the beginning of time, or some periodicaly checkpointed position. This requires you to record changes rather than totals. Calculate periodic snapshots and do your historical statiscs based on the snapshots. Hybrid running-sum with periodic snapshots to baseline positions and reduce the data set over which the running sum needs to be calculated. 

3 letter ISO currency codes are relatively small, so there is little overhead in using them as the key for the reference table. Having certain items such as dates, GL codes and other analysis codes and currency codes directly on the tables is quite convenient for people reporting off the transactions and balances. Many accounting packages - even big players like Oracle Financials - do it for this reason. 

A CTE may be called repeatedly within a query and is evaluated every time it is referenced - this process can be recursive. If it is just referred once then it behaves much like a sub-query, although CTEs can be parameterised. A temporary table is physically persisted, and may be indexed. In practice the query optimiser may also persist intermediate join or sub-query results behind the scenes, such as in spool operations, so it is not strictly true that the results of CTEs are never persisted to disk. IIRC table variables (on the other hand) are always in-memory structures. 

Absolute numbers for months and quarters allow them to sort correctly when used in a query tool. A 'month of year' value is only useful for sorting within a year, but an absolute month value that is in order will sort a list of months correctly across years. This is very helpful to support (for example) a report over a rolling 12 month period. Keys for week, month or quarter (the year itself is inherently ordinal and suffices for this on years) also allow attributes such as day of month to be keyed to a logical key. Although the dimension table itself may be flat, many tools (OLAP servers, for example) will still impose a logical hierarchy within the data and require a key that is unique at its level to do this.