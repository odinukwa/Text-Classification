Probably the easiest paper to read for the applications of binary parametricity is Wadler's Theorems for Free!. Actually, I am a bit surprised by the question because binary parametricity is what is most often mentioned in parametricity papers. Even the original Reynolds paper "Types, abstraction and parametric polymorphism" mentions it everywhere. It is rather the unary parametricity that is not widely known. 

It is obvious that these four rules are complete for deducing clauses, i.e., Proposition 1 For any clause $C$ and set of clauses ${\cal S}$, we have ${\cal S} \models C$ if and only if ${\cal S} \vdash C$. Refutation proof converts the problem of ${\cal S} \vdash C$ to ${\cal S} \cup N(C) \vdash \bot$, where $N(C) = \{\{\bar{A}\} \mid A \in C\}$ is the collection of clauses representing the negation of $C$. It is clear that ${\cal S} \vdash C$ if and only if ${\cal S} \cup N(C) \vdash \bot$. Our four-rule system is still adequate for proving the converted problem, but we notice that we don't need identity and weakening any more. The remaining two rules are called the "resolution proof procedure". Proposition 2 For any clause $C$ and set of clauses ${\cal S}$, we have ${\cal S} \models C$ if and only if ${\cal S} \cup N(C) \vdash \bot$ using only cut and contraction. The point of converting the problem to refutation proofs is two-fold: 

I will go out on a limb here and say that, if you are willing to squint a bit, proofs and terminating programs can be identified. Any terminating program is a proof that you can take its input and produce its output. This is a very basic kind of proof of implication. Of course, to make this implication carry information more meaningful than stating the obvious, you need to be able to show that the program works for any and all instances of input drawn form some class with logical meaning. (And so too for the output.) From the other direction, any proof with finite inference steps is a symbolic program manipulating objects in some logical system. (If we don't worry too much about what the logical symbols and rules mean computationally.) Types and propositions can be made to work similarly. Any type T can be assigned the proposition $\exists x : x \vdash T$ with obvious truth conditions. Any proposition can be turned into the type of it's proofs. This is pretty simplistic, but I think it does suggest the robustness of the idea. (Even if some people are bound not to like it. ;-) ) 

A lot of the senior Computer Scientists in Britain have had industrial experience before they came to work in academics. Christopher Strachey, the founder of denotational semantics, was a consultant programmer before entering academics. Tony Hoare, the founder of axiomatic semantics, worked in industry (Eliott Computers) for several years. Samson Abramsky, who holds the Christopher Strachey Chair at Oxford, in fact developed his interest in Computer Science during his work in industry (GEC). Cliff Jones, a Fellow of RAEng, worked in IBM, Vienna for several years before coming to do his PhD, and did another stint at a start-up company called Harlequin even afterwards. I have to say that all of them probably did innovative R&D type of work while in industry, which might be necessary to keep your mind active in thinking about research directions. 

The fact that each step is expressed compositionally doesn't mean that the entire behaviour is expressed compositionally. There is a nice article by Carl Gunter called Forms of Semantic Specification, where the different methods of specifying semantics are compared and contrasted. Much of this material is also reproduced in the first chapter of his "Semantics of Programming Languages" text. This should hopefully clarify the picture. Another word about "operational semantics". In the early days, the term "operational" was used to refer to any semantic definition that referred to detailed evaluation steps. Both denotational semanticists and axiomatic proponents looked down upon "operational" semantics, regarding it as being low-level and machine-oriented. I think this was really based on their belief that higher level descriptions were possible. These beliefs were shattered as soon as they considered concurrency. de Bakker and Zucker's Processes and denotational semantics of concurrency has these interesting passages: 

A fairly comprehensive discussion of this stuff can be found in this book: Lectures on the Curry-Howard Isomorphism. This is based on the freely available older version: Lectures on the Curry-Howard Isomorphism. 

I don't know of any work that pursues this line, but a few moments thought about it led me to this hypothesis: wouldn't the "root" of the exponential type just be the codomain, and the "logarithm" of the exponential just the domain? 

CoC is most likely the way to go. Just dive into Coq and work through a nice tutorial like Software Foundations (which Pierce of TaPL and ATTaPL is involved in). Once you get a feel for the practical aspects of the dependent typing, go back to the theoretical sources: they'll make a lot more sense then. Your list of features sounds basically correct, but seeing how they play out in practice is worth a thousand feature points. (Another, slightly more advanced tutorial is Adam Chlipala's Certified Programming with Dependent Types) 

[Note added:] A nice analysis of the mathematical underpinnings of objects can be found in William Cook's article "On Understanding Data Abstraction, Revisited". 

I think Simon PJ's quote is a bit of an off-hand remark actually. Similarity between languages is determined by what consensus has been generated in the researcher and language designer community. There is no question that there is a higher degree of consensus in the functional programming community than in the imperative programming community. But it is also the case that the functional programming languages are mostly designed by researchers rather than practitioners. So, it is natural for such consensus to emerge. Almost all functional languages use garbage collected memory management and recursive data structures (originated by Lisp), most of them use "algebraic" data types and pattern matching (originated by Hope), a lot of them use higher-order functions and polymorphic functions (originated by ML). Beyond that, the consensus disappears. They differ in module systems used, how state change operations and other computational effects should be handled, and the evaluation order (call-by-name vs call-by-value) etc. Imperative programming languages generally use nested control structures (originated by Algol 60), and type systems (originated by Algol 60, but consolidated by Algol 68). They generally have cumbersome surface syntax (again going back to Algol 60), make half-hearted attempts to handle higher-order functions and polymorphic types, and differ in their support for block structure and module systems. There is probably more uniformity in the evaluation order because, after the 60's, call-by-name has essentially disappeared from imperative languages. So, it is not clear to me that the difference between the two classes of languages in their uniformity is all that great. It would be really worthwhile to bring the cleaner and uniform notations of functional programming into imperative programming languages. I see that Scala has made a beginning in that direction. It remains to be seen whether the trend will continue. 

It is is worth thinking about WHY intuistionistic logic is the natural logic for computation, since all too often people get lost in the technical details and fail to grasp the essence of the issue. Very simply, classical logic is a logic of perfect information: all statements within the system are assumed to be known or knowable as unambiguously true or false. Intuistionistic logic, on the other hand, has room for statements with unknown and unknowable truth values. This is essential for computation, since, thanks to the undecidability of termination in the general case, it will not always be certain what the truth value of some statements will be, or even whether or not a truth value can ever be assigned to certain statements. Beyond this, it turns out that even in strongly normalizing environments, where termination is always guaranteed, classical logic is still problematic, since double negation elimination $\neg\neg P \implies P$ ultimately boils down to being able to pull a value "out of thin air" rather than directly computing it. In my opinion, these "semantic" reasons are a much more important motivation for the use of intuistionistic logic for computation than any other technical reasons one could marshal. 

Something I missed out in my original answer: Proof nets are a way of writing proofs, and we know that proofs are programs. So, proof nets are also a way of writing programs. The traditional functional notation for writing programs is asymmetric, just like natural deduction is. So, proof nets point to a way of writing programs in a symmetric form. That is how process calculi enter the picture. Another way of representing the symmetry is through logic programming, which I have explored in two papers: A typed foundation for directional logic programs and Higher-order aspects of logic programming 

I am looking for tutorial material that covers compiler correctness proofs, preferably using denotational methods, at the level of a beginning grad student. Alternatively, do you know of some simple compiler examples that I could use to illustrate the issues? (The first example that occurred to me was a translator from infix to postfix expressions. But it failed to show anything interesting other than how to do induction on syntax.) 

Let me offer the simple, intuitive way that I think about this. If you restrict yourself to closed lambda expressions, you have an equivalent of the combinatory logic. In fact with just a few simple closed lambda expressions you can generate all the others. Closed lambda expressions give you the equivalent of implications where any conclusion/output you reach is either something you put in as an input, or something that you built by combining your inputs (in the general case, possibly recursively). This means that you can't pull a result "out of thin air" the way you can with non-constructive logics / mathematics. The only tricky bit left is how you handle negation / non-termination, which is a whole area by itself, but hopefully I've already given you the simple, but deep, correspondence between the three that you are asking for. 

Though it many not be obviously directly related, one thing that comes to mind is the concept of "blame" by Wadler et al.. This give you a theoretical basis to think about mixing together different typing regimes into a coherent whole. In essence, blame allows you to mix together languages with weaker type guarantees with languages that have stronger type guarantees without losing all the benefits of the strong guarantees. The idea is that the parts of the system with weaker guarantees will get the "blame" if certain things go wrong, localizing runtime type errors. Hopefully you can see how that might be useful for FFI and bindings that apply to languages with varying type systems. Edit: See Sam TH's answer for a fuller intellectual history of the concept of "blame". 

I would divide the books on programming language semantics into two classes: those that focus on modelling programming language concepts and those that focus on the foundational aspects of semantics. There is no reason a book can't do both. But, usually, there is only so much you can put into a book, and the authors also have their own predispositions about what is important. Winskel's book, already mentioned, does a bit of both the aspects. And, it is a good beginner's book. An equally good, perhaps even better, book is the one I started with: Gordon's Denotational description of programming languages. This was my first book on semantics, which I read soon after I finished my undergraduate work. I have to say it gave me a firm grounding in semantics and I never had to wonder how denotational semantics differs from operational semantics or axiomatic semantics etc. This book will remain my all-time favourite on denotational semantics. Other books that focus on modeling aspects rather than foundational aspects are the following: 

If you are having trouble with the concept of least fixed point, I would recommend spending some time getting a background in more general order theory. Davey and Priestley, Introduction to Lattices and Order is a good intro. To see why the transitive closure is the least fixed point, imagine building up the closure from an empty set, applying the logical formula one step at a time. The least fixed point arrives when you can't add any new edges using the formula. The requirement that the formula be positive ensures that the process is monotonic, i.e. that it grows at each step. If you had a negative subformula, you could have the case where on some steps the set of edges would decrease, and this could lead to a non-terminating oscillation up and down, rather than a convergence to the LFP. 

I would recommend investigating the field of Finite Model Theory and more particularly its sub-field Descriptive Complexity. It can be used to model such sorts of problems. 

I don't have a specific recommendation for your reading list, but I want to alert you to the excellent survey on "Rewrite Systems" by Dershowitz and Jouannaud in Handbook of Theoretical Computer Science, volume B. 

There are unfortunate confusions in the way you have posed the question. Programs have semantics. Programming Languages are given semantic definitions. In more detail: every program has a meaning, either as a computation or as a mathematical function (relation, trace set, strategy,...). A semantic definition is given for an entire programming language, so that the meaning of every program in the language is defined. Programs can also be given specifications. While the semantics of the program says what the program means, its specification states what we care about its behaviour. So, the specification can be partial. It need not state what the program does under all possible situations. Neither does it need to state everything about the outputs of the program, only the properties we care about. The sample specification you have shown for the minimum function is incomplete. It says that the output has to be smaller than (or equal to) both its inputs, but it does not say that the output must be one of the two inputs. To make it complete (which is apparently the "intent"), you need to add this condition