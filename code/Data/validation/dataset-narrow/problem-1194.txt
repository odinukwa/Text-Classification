Given two strings x and y, I want to build a minimum size DFA that accepts x and rejects y. One way to do this is brute force search. You enumerate DFA's starting with the smallest. You try each DFA until you find one that accepts x and rejects y. I want to know if there is any other known way of finding or building a minimum size DFA that accepts x and rejects y. In other words, can we beat brute force search? More Detail: (1) I really do want an algorithm to find a minimum size DFA, not a near minimum size DFA. (2) I don't just want to know how large or small the minimum DFA is. (3) Right here, I'm only focused on the case were you have two strings x and y. 

Now that we have an auxiliary pushdown automaton with only $k\log(n)$ bits on the work tape that runs for $2^{n^k}$ steps before accepting, we can construct $P$ along with $D_1, D_2, ..., D_k$. This construction is inspired by Kozen (1977) and related works. The idea is to build the PDA and DFA's to read in a string that encodes a computation of the auxiliary pushdown automaton. The computation is accepting if and only if the PDA and DFA's all accept its encoding. The computation is a sequence of configurations. Each configuration can be encoded by a string so that all of the strings concatenated together make up the computations encoding. Then, the DFA's each are assigned to a $\log(n)$ length chunk of the work tape and verify that the computation for their respective chunk of the work tape carries on correctly. Finally, the PDA pushes, pops, and peeks, as is instructed by the configurations and simultaneously verifies that what is on the stack actually matches what the configuration says is on top of the stack. They accept if both their respective verification is successful and the accepting configuration is reached. Because the auxiliary pushdown automaton requires a computation of length $2^{n^k}$, the smallest string in the intersection has length at least $2^{n^k}$. And, although it wasn't discussed in detail, the automata can be built to have polynomial in $n$ states. The smaller the degree of the polynomial, the closer that $c$ can be made to 1. Other Works 

There is very easy "deterministic" proof which gives a better $\epsilon$-net: that is, of size $O(\frac{1}{\epsilon})$. You can find it here in Section 4. The idea is the following: we build convex hull of $X$, and then find minimal $\epsilon$-net that consists only of points from the convex hull. 

If $P$ is a tree, then there is a polynomial-time algorithm that constructs an optimal decision tree. Generalization of Binary Search: Searching in Trees and Forest-Like Partial Orders 

The influence of variables on boolean functions, J. Kahn, G. Kalai and N. Linial This paper introduced Fourier techniques for TCS community and solved very neat open problem. I find this paper very readable. 

Let $\varphi$ be a CNF formula. Suppose that each of $\varphi$'s clauses consist of exactly $t$ literals (and, moreover, all literals within one particular clause correspond to different variables). It is well known that if every clause has less than $2^t / e$ clauses that share variables with it, then $\varphi$ is satisfiable (let us call such formulae easy). Satisfiability can be proved easily using LovÃ¡sz local lemma. Moreover, using a recent result by Moser and Tardos one can show that one of the satisfying assignments can be found in polynomial expected time using the following very simple procedure: 

Simple Answer: For each $EXPTIME$-$hard$ problem there is some constant $c$ such that if we could solve the problem in $NTIME(2^{o(n^{\frac{1}{c}})})$, then $P \neq NP$. Note: The constant $c$ comes from the instance size blow-ups that result from the reductions. Justification: Let $X$ denote an $EXPTIME$-$hard$ problem. That means that every problem in $EXPTIME$ is polynomial time reducible to $X$. In fact, we can show more. The acceptance problem for $2^n$ time bounded deterministic Turing machines is in $DTIME(n \cdot 2^n) \subseteq EXPTIME$ and therefore is polynomial time reducible to $X$. Therefore, there must be some fixed constant $c$ such that every problem in $DTIME(2^n)$ is polynomial time reducible to $X$ where the instance size blow-up is $O(n^c)$. That is, instances of size n are reduced to instances of size $O(n^c)$ for $X$. Now, if we had $X \in NTIME(2^{o(n^{\frac{1}{c}})})$, then $DTIME(2^n) \subseteq NTIME(2^{o(n)})$. However, this implies $P \neq NP$ (see below for details). Additional Details: One can show that $P=NP$ $\Leftrightarrow$ $\exists c^{\prime}$ $\forall k$ $NTIME(n^k) \subseteq DTIME(n^{c^{\prime}k})$. In other words, if you can solve an $NP$-$complete$ problem in polynomial time, then there is a uniform way of speeding up any problem in $NP$. Now, let's suppose that $P=NP$. By the preceding (with $k$=1) we get a constant $c^{\prime}$ such that $$NTIME(n) \subseteq DTIME(n^{c^{\prime}}).$$ Next, we can use padding to scale up this inclusion and get $$NTIME(2^{n}) \subseteq DTIME(2^{c^{\prime}n}).$$ Then, by the deterministic time hierarchy theorem, we have $$NTIME(2^{n}) \subseteq DTIME(2^{c^{\prime}n}) \subsetneq DTIME(2^{(c^{\prime}+\epsilon)n})$$ for any $\epsilon > 0$. Therefore, we couldn't have $DTIME(2^{(c^{\prime}+\epsilon)n}) \subseteq NTIME(2^{n}).$ Further, we couldn't have $DTIME(2^{n}) \subseteq NTIME(2^{o(n)})$ because by padding we would get $DTIME(2^{(c^{\prime}+\epsilon)n}) \subseteq NTIME(2^{o(n)})$. Further Question: Does anyone have any simple examples of $EXPTIME$-$complete$ problems where we can easily determine the instance size blow-up constant $c$? 

The following result is supposedly known. However, the proofs I am able to find all prove a weaker result with an extra log factor. Where can I find the proof of the tight bound? 

Symbolic multiplication of degree $<n$ polynomials $A(x)$ and $B(x)$ would take $O(n^2)$ time. That may be fine, but they are giving an algorithm that takes $O(n \log n)$ time, which is better. Instead of symbolic multiplication, they are suggesting that you evaluate the polynomials at $2n$ points, multiply those values pointwise, and then interpolate the product polynomial (i.e. compute the coefficients of $A(x)B(x)$ from the values). The reason you need $2n$ points is that $A(x)B(x)$ could have degree $2n-2$ and you need more points than the degree to interpolate. 

If you want $Y$ to have entropy less than $0.99 n$ bits, the answer is no, by the uncertainty principle: Either $Y$ has high entropy or its Fourier transform has large support. Theorem. Let $H(Y)$ be the Shannon entropy of $Y$ and let $F \subset \{0,1\}^n$ be the support of $\hat{Y}$. Then $H(Y) \geq n - \log |F|$. Proof. Consider the collision probability of $Y$ (the probability that two independent samples of $Y$ are the same). By Parseval's identity, $$CP(Y) = \sum_y \text{Pr}[Y=y]^2 = 2^{-n} \sum_s \hat{Y}(s)^2 \leq 2^{-n} |F|,$$ as $|\hat{Y}(s)| \leq 1$. On the other hand, $CP(Y) = 2^{-H_2(Y)}$, where $H_2(Y)$ is the Renyi entropy of $Y$. Noting that $H_2(Y) \leq H(Y)$ gives the result. Q.E.D. If $|F| = \mathrm{poly}(n)$, then $H(Y) = n-O(\log n)$. So, if $Y$ is the output of a pseudorandom generator, the seed length is at least $n-O(\log n)$. 

Edit: Based on @MRC's comments, it seems that this problem is equivalent to calculating the weight of $s$ relative to the weighted CFG obtained by attaching the semiring $(\mathbb{N}, +, \times)$ to $G$ and assigning the weight of 1 to each of $G$'s rules. 

A language is in $L/poly$ if there exists a logspace Turing machine that decides the language with polynomial amount of advice. See here for more info: $URL$ 

Now, you may ask, in the worst case, how large can this shortest string be? It turns out that there is a constant $c \leq 1$ such that it is always possible to construct a PDA $P$ and DFA's $D_1, D_2, ..., D_k$ so that the shortest string in the intersection has length $2^{n^{c \cdot k}}$. Note: You will see where $c$ comes from below, but you can probably make it close to $1$. Maybe even $1 - \varepsilon$? Construction 

As pointed out by qsp in the comments below, Tseytin transformation is a reduction where $v(n,m) = O(m)$ and $l(n,m) = O(m)$. However, does there exist a reduction where $v(n,m) = O(n \sqrt{m})$ and $l(n,m) = O(m^2)$? What about a reduction with $v(n,m) = O(n^2)$ and $l(n,m) = O(m^2)$? Revised Question 

The question: is it true that a version of DPLL that splits on random variables finds a satisfying assignment of any easy formula in polynomial expected time? 

This question is motivated by the Leighton-Rao relaxation for SPARSEST-CUT. Suppose one wants to find a non-trivial semimetric over an $n$-point space that minimizes a certain linear functional. More formally: 

Of course it is a linear program that can be solved in polynomial time using ellipsoid method. But what I would like to know is if there are any combinatorial algorithms for this optimization problem. I suspect that the answer is 'no' since the problem looks pretty general to be solved combinatorially, but who knows... 

On the other hand, most of modern SAT solvers are DPLL-based. It means that they try to find a satisfying assignment using brute force with two simple prunings: 

Consider $\mathbb{R}^n$ equipped with the standard dot product $\langle \cdot, \cdot \rangle$ and $m$ vectors there: $v_1, v_2, \ldots, v_m$. We want to build a data structure that allows queries of the following format: given $x \in \mathbb{R}^n$ output $\min_i \langle x, v_i \rangle$. Is it possible to go beyond the trivial $O(nm)$ query time? For example if $n = 2$, then it is immediate to get $O(\log^2 m)$. The only thing I can come up with is the following. It is an immediate consequence of Johnson-Lindenstrauss lemma that for every $\varepsilon > 0$ and a distribution $\mathcal{D}$ on $\mathbb{R}^n$ there is a linear mapping $f \colon \mathbb{R}^n \to \mathbb{R}^{O(\log m)}$ (which can be evaluated in $O(n \log m)$ time) such that $\mathrm{Pr}_{x \sim \mathcal{D}}\left[\forall i \quad \langle x, v_i \rangle - \varepsilon (\|x\| + \|v_i\|)^2 \leq \langle f(x), f(v_i)\rangle \leq \langle x, v_i \rangle + \varepsilon (\|x\| + \|v_i\|)^2 \right] \geq 1 - \varepsilon$. So, in time $O((n + m) \log m)$ we can compute something that is in some sense close to $\min_i \langle x, v_i \rangle$ for most $x$'s (at least if the norms $\|x\|$ and $\|v_i\|$ are small). UPD The abovementioned bound can be somewhat sharpened to the query time $O(n + m)$ if we use locality-sensitive hashing. More precisely, we choose $k := O(\frac{1}{\varepsilon^2})$ independent Gaussian vectors $r_1, r_2, \ldots, r_k$. Then we map $\mathbb{R}^n$ to $\{0,1\}^k$ as follows: $v \mapsto (\langle r_1, v \rangle \geq 0, \langle r_2, v \rangle \geq 0, \ldots, \langle r_k, v \rangle \geq 0)$. Then we can estimate the angle between two vectors within an additive error $\varepsilon$ by computing $\ell_1$-distance in the image of this mapping. Thus, we can estimate dot products within an additive error $\varepsilon \|x\| \|v_i\|$ in $O(\frac{1}{\varepsilon^2})$ time. 

A Counting Problem Given a CFG $G$ and a string $s$, how many distinct parse trees are there for the string $s$? An Example Instance Let's consider an example instance consisting of a CFG $G$ with grammar rules: $S \rightarrow aS$ $S \rightarrow aSb$ $S \rightarrow \epsilon$ And, a string $s = aaab$. Then, there are three parse trees for $s$: (1) $S \rightarrow aS \rightarrow aaS \rightarrow aaaSb \rightarrow aaab$ (2) $S \rightarrow aS \rightarrow aaSb \rightarrow aaaSb \rightarrow aaab$ (3) $S \rightarrow aSb \rightarrow aaSb \rightarrow aaaSb \rightarrow aaab$ 

It seems that we can reduce Subset Sum to your problem (2). Hence, your problem (2) is NP-complete. Consider the following formulation of Subset Sum. Instance: A multi-set consisting of $n$ integers. Question: Does there exist a subset of size at least $1$ that sums to zero? Now, we reduce Subset Sum to your problem (2). Let a multi-set $X$ consisting of $n$ integers be given. We construct a new multi-set $X'$ with $n^2$ integers by taking $X$ and adding integers to it. We throw $(n-1)$ zeros into $X$ and we throw $(n^2 - 2n + 1)$ very large numbers into $X$. The very large numbers are large enough so that we can't choose any of them and still sum to zero. However, we can either choose the zeros or not choose the zeros for a subset. Now, we have that $X$ has a subset of size at least $1$ that sums to zero if and only if $X'$ has a subset of size exactly $n$ that sums to zero. This $X'$ can be viewed as an instance for your problem 2. Hence, we are done with the reduction. 

Here is an algorithm that completes the task by flipping $O(\log n \cdot \log \log n)$ coins: Perform a binary search over the coins - this takes $O(\log n)$ steps. For each step, flip the coin $O(\log \log n)$ times so that the bias of the coin can be approximated to within $0.1$ with probability at least $1-1/O(\log n)$. A union bound shows that this will be correct with high probabiliy. Can we do this using $O(\log n)$ coin flips? 

Here $f(x) = \sum_{S \subseteq [n]} \widehat{f}(S) \prod_{i \in S} x_i$ for all $x \in \{ \pm 1\}^n$ defines the usual Fourier transform. It's clear that $$\mathrm{Var}\left[ f \right] = \sum_{S \ne \emptyset} \widehat{f}(S)^2 \leq \sum_S p^{2|S|+2} = p^2 (1+p^2)^n,$$ but, unless $p \leq \tilde{O}(1/\sqrt{n})$, this is worse than the trivial $\mathrm{Var}\left[ f \right] \leq 1$. I'm interested in $p = (1/\log n)^{O(1)}$. I don't know exactly what the right bound on the variance of $f$ should be. My motivation comes from the area of pseudorandomness, but this seems to be an interesting question about discrete Fourier analysis in its own right. In particular, this seems related to the majority is stablest theorem: Suppose $f = T_p(g)$ for some balanced $g : \{\pm 1\}^n \to \{\pm 1\}$ with $\mathrm{Inf}_i(g) \leq p^2$ for all $i$, where $T_p$ is the noise operator. Then $f$ satisfies the hypotheses of the conjecture and the variance of $f$ is the noise stability of $g$, which is bounded by $O(p)$ as a consequence of majority is stablest.