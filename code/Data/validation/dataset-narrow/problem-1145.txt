Some tiny improvements of this result are also known, for instance by Liśkiewicz and Schuster (2014) to $O(1.255^n)$. 

(1) Now finding a solution is very easy: Simply output the word $ww$ in linear time. (2) On the other hand, deciding whether the word $w$ is a feasible solution for a given instance is an undecidable problem. 

A linear grammar is a context-free grammar that has at most one nonterminal in the right hand side of each of its productions. A linear language is a language generated by some linear grammar. See also: $URL$ 

Consider a satisfying truth setting for the THREE-SATISFIABILITY instance. If $u_i$ is true, then set $x_i=1$, and if $u_i$ is false, then set $x_i=-1$. This makes all polynomials $x_i^2-0.9^2$ and $1.1^2-x_i^2$ positive. Since every clause contains at least one true literal, the sum of the corresponding three real variables is at least $-1-1+1=-1$, so that the corresponding polynomial yields a value of at least $0.1$. 

Preparatory Rant I've gotta tell you, I don't see how talking about "proofs" of the CT or ECT adds any light to this discussion. Such "proofs" tend to be exactly as good as the assumptions they rest on---in other words, as what they take words like "computation" or "efficient computation" to mean. So then why not proceed right away to a discussion of the assumptions, and dispense with the word "proof"? That much was clear already with the original CT, but it's even clearer with ECT---since not only is the ECT "philosophically unprovable," but today it's widely believed to be false! To me, quantum computing is the huge, glaring counterexample that ought to be the starting point for any modern discussion about the ECT, not something shunted off to the side. Yet the paper by Dershowitz and Falkovich doesn't even touch on QC until the last paragraph: 

Well, one crucial observation is that the new algorithm apparently only works for groups of the form $Z_{p^k}$ where $p$ is small --- it doesn't give a speedup for groups of the form $Z_p$. The latter is the much more common setting that people talk about, both for cryptography and for Shor's algorithm, and the new algorithm doesn't threaten the quantum speedup there. On the other hand, yes, unless I'm mistaken it does make the speedup much smaller in the $Z_{p^k}$ case. 

This problem is known to be NP-complete (since the 1970s). Dyer and Frieze established its NP-hardness even for the highly restricted special case where the graph $G$ is planar and bipartite: 

Your problem is NP-complete. Take an instance $G'=(V',E')$ of the (NP-complete) max-cut problem. Let $m=|E'|$. 

The following paper establishes NP-hardness of essentially all non-trivial questions in this direction: 

Consider all Boolean formulas $\Phi$ in 3-CNF. The objective value of an $n$-variable formula $\Phi$ is $1/n$ if $\Phi$ is satisfiable, and is $1$ if $\Phi$ is not satisfiable. The corresponding minimization problem has an asymptotic PTAS (it is easy to come within an additive $+1$), but is not in APX (as this would allow you to solve 3-SAT). Consider all undirected graphs. The objective value of a graph $G$ is its edge-chromatic number $\chi'(G)$. Holyer has proved that deciding $\chi'(G)=3$ is NP-hard. The corresponding minimization problem is APX-hard (there is no approximation algorithm with worst case ratio better than $4/3$), but the problem has an asymptotic PTAS (Vizing's theorem tells us that $\chi'(G)$ is between the max-degree and the max-degree plus 1). 

This question is a lot like asking for Newton's contributions to physics, or Darwin's to biology! However, there's an interesting aspect to the question that many commenters have already seized on: namely that, besides the enormous contributions that everyone knows, there are plenty of smaller contributions that most people don't know about --- as well as many insights that we think of as more "modern," but that Turing demonstrated in various remarks that he understood perfectly well. (Incidentally, the same is true of Newton and Darwin.) A few examples I like (besides the ones mentioned earlier): In "Computing Machinery and Intelligence," Turing includes a quite-modern discussion of the benefits of randomized algorithms: 

The main thing missing from your list is the beautiful 2006 paper of Klivans and Sherstov. They show there that PAC-learning even depth-2 threshold circuits is as hard as solving the approximate shortest vector problem. 

Well, the title pretty much says it all. The interesting question above was asked by commenter Jay on my blog (see here and here). I'm guessing both that the answer is yes and that there's a relatively simple proof, but I couldn't see it offhand. (Very roughly, though, one could try to show that, if a language in $P^R$ were not in $BPP$, then it must have infinite algorithmic mutual information with $R$, in which case it wouldn't be computable. Also, note that one direction is trivial: the computable languages in $P^R$ certainly contain $BPP$.) Note that I'm not asking about the class AlmostP, which consists of those languages that are in $P^R$ for almost every $R$ (and is well-known to equal $BPP$). In this question, we first fix $R$, then look at the set of computable languages in $P^R$. On the other hand, one could try to show that, if a language in $P^R$ is computable, even for a fixed random oracle $R$, then in fact that language must be in $AlmostP$. A closely-related question is whether, with probability 1 over a random oracle $R$, we have $ AM = NP^R \cap Computable.$ If so, then we get the following interesting consequence: if $P=NP$, then with probability 1 over a random oracle $R$, the only languages that witness the oracle separation $P^R \ne NP^R$ are uncomputable languages. 

The problem is NP-complete. I sketch a reduction from 3-SAT. Consider Boolean variables $x_1,\ldots,x_n$ and clauses $c_1,\ldots,c_m$ over these variables, so that each clause consists of three literals. The question is to decide whether there exists a truth setting of the variables so that every single clause is satisfied (and hence contains at least one true literal). We construct a matrix with $n+m+1$ rows and $2n+1$ columns. 

For a word $w=w_1\ldots w_{\ell}$ and for two integers $i,j$ with $1\le i\le j\le \ell$ we denote by $w(i,j)$ the subword $w_iw_{i+1}\ldots w_j$ of $w$. Furthermore we let $w(0,0)$ denote the empty word. 

This problem is a special case of the b-matching problem, and hence can be solved in polynomial time. Extensive information on b-matchings can be found for instance in the book: László Lovász and Michael D. Plummer: Matching Theory ISBN-10: 0-8218-4759-7 ISBN-13: 978-0-8218-4759-6 

The paper shows that the following problem is NP-hard: Given a graph $G=(V,E)$ and two vertices $s,t\in V$, is there a "good" partition of $V$ into two sets $V_1$ and $V_2$, so that $G[V_1]$ and $G[V_2]$ are connected and so that $s,t\in V_1$. Take such an instance, and make $w(s)=w(t)=1$ and $w(x)=0$ for $x\in V-\{s,t\}$. If there exists a good partition, you can reach $w(V_1)=2$. If there is no good parition, the best you can get is $w(V_1)=1$. (This argument shows NP-hardness, and also in-approximability within a factor of $2-\epsilon$.) 

While I don't know the answer to this question, it seems crucial that the phenomenon isn't limited at all to theoretical computer science. I believe SIGGRAPH plays the same sort of role for graphics, NIPS for machine learning, ISCA for architecture, etc. that STOC and FOCS play for theory. Yet it's true that the emphasis on publication in conference proceedings is a striking feature of computer science as a whole, one that isn't shared by any other academic field that I know about. (But maybe there are other such fields?) 

Showing that your problem is in coAM (or SZK) is indeed one of the main ways to adduce evidence for "hardness limbo." But besides that, there are several others: 

If anything in TCS is outdated, it's this inclusion hierarchy of the tiny subset of complexity classes that happened to be known / considered interesting in 1956. Rest in peace, Chomsky Hierarchy, and may you haunt the undergrad theory curriculum no more. 

The problem of counting such "imperfect" matchings in bipartite graphs is #P-complete. This has been proved by Les Valiant himself, on page 415 of the paper 

For every concrete Turing machine $M$, the halting problem (Problem $P_M$ without input: "Does the Turing machine $M$ halt on the empty input $\varepsilon$?") is decidable. The corresponding decision algorithm is either the algorithm that outputs "Yes" and halts, or the algorithm that outputs "No" and halts. 

The NP-hardness proof for CLIQUE in the book by Garey and Johnson shows that the following problem is NP-complete: 

A famous result by Motzkin and Straus expresses the $k$-clique problem as the maximization of a quadratic function subject to a system of linear constraints. In particular, they prove: 

It can be shown that the 3-SAT instance is satisfiable, if and only if the column selection problem has a solution $S$ for which the sum of products is at most $v=n+m+1$. Some comments: 

Yes, Max-Cut is still NP-complete in unweighted graphs. This is explained in pretty much every survey article on tthe Max-Cut problem, and in many texbooks (as for instance "Computational Complexity" by C.H. Papadimitriou). The first proof goes back to the year 1976: 

Prompted by a question Greg Kuperberg asked me, I'm wondering if there are any papers that define and study complexity classes of languages admitting various kinds of proofs of knowledge. Classes like SZK and NISZK are extremely natural from a complexity standpoint, even if we forgot entirely about zero knowledge and just defined them in terms of their complete promise problems. By contrast, on googling 'proofs of knowledge,' I was surprised not to find any papers or lecture notes discussing this lovely concept in terms of complexity classes. To give some examples: what can one say about the subclass of SZK∩MA∩coMA consisting of all languages L that admit statistical zero-knowledge proofs for x∈L or x∉L, that are also proofs of knowledge of a witness proving x∈L or x∉L? Certainly this class contains things like discrete log, but we couldn't prove that it contains graph isomorphism without putting GI in coMA. Does the class encompass all of SZK∩MA∩coMA? One could also ask: if one-way functions exist, then does every language L∈MA∩coMA admit a computational zero-knowledge proof, that's also a proof of knowledge of a witness proving x∈L or x∉L? (My apologies if one or both of these have trivial answers---I'm just trying to illustrate the sort of thing one could ask, once one decided to look at PoK in complexity-theoretic terms.) 

Furthermore use the old equation system $Ax=b$. There exists a 0-1 solution to the original system $Ax=b$, if and only if the new system has a solution in which at least $100(n+m)n$ variables are zero. 

We claim that the THREE-SATISFIABILITY instance has answer YES, if and only if there exists a positive point for the constructed set of polynomials. 

The problem is NP-complete, by reduction from the following problem: Given an $m\times n$ matrix $A$ with integer entries and an integer vector $b$ with $n$ entries, does there exist a 0-1 vector $x$ with $Ax=b$? For every coordinate $x_i$ of vector $x$, 

The book "Computers and Intractability" by Michael Garey and David S. Johnson contains a textbook NP-hardness proof for the $k$-clique problem. If you look into this proof, you will see that the constructed graph is in fact $k$-partite. So you get NP-hardness of deciding the existence of $k$-clique in $k$-partite graphs for free. 

Reference: T.S. Motzkin and E.G. Straus (1965), "Maxima for graphs and a new proof of a theorem of Turán." Canadian Journal of Mathematics 17, pp 533–540. 

To complement what Joe wrote, and maybe explain this question a bit more (without answering it!): The computational complexity of simulating "realistic" quantum field theories has been considered an open problem for a long time. One of the main problems, as I understand it, is that (3+1)-dimensional QFTs aren't sufficiently well-defined mathematically for it to be clear what model of computation should correspond to them. But the situation is different for the (2+1)-dimensional QFTs called topological quantum field theories (TQFTs). For those, there is a rigorous mathematical description based on the Jones polynomial, due to Witten from the 1980s. It's that description that led to the deep and celebrated result of Freedman, Kitaev, Larsen, and Wang, which showed that simulating TQFTs is indeed BQP-complete, as one would hope and expect (see Aharonov, Jones, and Landau for a more computer-scientist-friendly version). This remains essentially the only rigorous result we have about the computational complexity of quantum field theory. Now, the questioner is asking whether some new work by Witten could give a handle on the computational complexity of (3+1)-dimensional QFTs. I don't know the answer to that, but it seems obvious that whatever it is, it would involve a significant research effort, and probably not fit within the margins of CS Theory StackExchange. :-) Addendum (Oct. 12, 2013): I just saw this answer again, and I thought I should add a note that, shortly after I posted it, Jordan, Lee, and Preskill released an important paper showing how to simulate "φ4 theory" (a simple interacting quantum field theory) in quantum polynomial time, in any number of spacetime dimensions. This doesn't directly address the OP's question, but it does render obsolete my comment about Freedman-Kitaev-Larsen-Wang remaining "essentially the only rigorous result we have about the computational complexity of quantum field theory."