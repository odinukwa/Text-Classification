I would use: 1. Code management GIT (and the awesome reference), a distributed source code manager, for managing my code, and host it on GitHub as a private project if I want to keep it off limits. (There are A LOT of options here, just google for source code management, you don't even NEED to use GitHub or any other website, Git will work just fine on your local computer, but using GitHub will make the pain of managing backups a lot easier. If you have two computers you can create a repository on one that you'll call your backup machine, then you clone that repository over the local network and use it for development, when you're done with a feature you can push it to the backup machine and you'll have a 1:1 backup!) 2. Issue & Feature management I would use Trello or GitHub's built-in issue management to keep track of bugs and things to do. 3. Have a Design Process I would design my game first; 

Extra using statements will very rarely cause problems, and when they do they will be because you have introduced an ambiguous name into the scope (for example, two namespaces with both define a type, and then trying to write will be ambiguous). The compiler will tell you of these name clashes, and fail to compile your code. If you run afoul of this, you can either fully-qualify the name reference ( verus ) or remove the unused directives. Note that if you are using Visual Studio, there is an option in the right-click menu while in the text editor for a source file to "remove unused usings." You can also have them sorted via a similar menu command. 

You write the player position to the file. For example, if you identify every player with a unique number (or a GUID), you could use that as the file name. In the file, simply write the position data out in a format you can parse later. For example, might contain if player #467239 is at that (x, y, z) location. This is not that different from what you would do with a database, however -- a database should not be "slow" at this operation, it should be very fast (probably faster than files, because you have more disk IO overhead or IO locking contention -- if you stored multiple positions per file -- in a filesystem-based approach). Perhaps you were trying to use the DB or filesystem to store the player position at runtime? You should not do this at all. At runtime, in your server, player positions should be kept in-memory and updated there, as you would do with any other kind of game. Periodically they can be saved to disk or other persistent storage -- for example, when the player rests, saves, or logs out. But writing every players position to storage every update is unnecessary and extremely inefficient; it will never be fast enough to handle anything resembling "massive" player scales. 

I wanted to know what game it was so that I could go look it up. And now that you have revealed that it uses T2D instead of T3D, what you want is simply impossible. T2D renders 3D objects the way it renders sprites: each one has its own space. There isn't a global camera for T2D unless the game creates that concept itself. And if it does, that global camera can only be changed via a mod if the developer specifically exposes it. 

Has it ever occurred to you to ask... why that is? Sure, SVG is a complicated specification to implement on a basic "getting the XML into memory and validating it" level. But I don't think you fully appreciate the simple fact that SVG rendering is not fast. Web browsers are fast if they render a webpage in less than 500ms or so. SVG images may take 20ms+ to render, and that's for small, simple ones. Something you might find in a game, a non-graphically-trivial game, are going to take much, much longer. SVG image rendering is not intended for fast animation. Even something lower-level like libCairo isn't exactly a high-performance renderer. Sure, it's fast enough for a browser, but it isn't exactly blazing in its software-rendering performance. In short, I would ditch SVG entirely and see if libCairo can serve your needs. That's just for evaluation purposes, to see if it performs fast enough in circumstances that approximate the purpose you intend for them. Once that's determined, you can decide what to do next in terms of technology (Qt, SDL, whatever). 

How to manage a project remotely Establish communication channels and routines for how to communicate so that information is never lost, and so that it is shared between everyone who wishes to partake in that flow of information. Communication is the pillar your project will stand on, if it isn't rigid and sturdy, your project's downfall is inevitable. Suggestions: 

The calculation above is the distance between the player object and the enemy object (it is actually an application of the Pythagorean Theorem!). The green circle is the , it can be represented as a single number (the length from the origin to the border of the circle). After we have calculated the distance we can check if it is less than the radius. If it is, we know there is a collision between the two objects! For an easy to understand explanation you can read this! If you want to optimize your code, you can do the calculation like this instead: 

So rather than mucking about with built-in vertex attributes like and such, just do that. Indeed, you can build up an entire naming convention scheme using . The best part is that you can bind any name to any location, even if that name never appears in your shader. It will simply be ignored by OpenGL. So you can have a standard set of naming-to-attribute location mappings somewhere that you use for every program. Just apply this mapping as part of your program setup before linking, and you get the behavior you want. And even better, you can keep doing this even when you work in GLSL 3.30 and syntax. See, has precedence, so it overrides any calls. So they'll just be ignored. Most importantly, DO NOT USE . Always directly specify your attribute indices, either with or with . I would have thought the reason for this was obvious (since it's the same reason to use to begin with). But since some asked... By specifying attribute locations, you create a convention for your attributes. Positions are always attribute 0. Colors are always attribute 1. Or whatever. You establish a convention and you maintain it. This now means that when it comes time to load, setup, and render your models, your calls can use the convention without having to know which program they use. is known for many things; speed is not one of them. So you want to make as few calls to it as possible. This means packing multiple objects in the same buffer(s) and using and/or to draw different segments of the buffer(s). Each draw call represents an object. But they don't have to use the same program. If you switch programs without an established attribute convention, now you need to use (a function who's performance is unknown) and for each attribute for each object in the buffer. Rather than just doing it once for all of the objects and rendering each one in turn. Plus, it makes everything cleaner. Your mesh objects can make the necessary calls they need without having to know what program they use. There's no reason why a mesh should need to know what program it will be rendered with. The NVIDIA Problem This somewhat overblown. Indeed, I advise you to ignore pretty much all of the documentation on the "OpenGL SDK" page on OpenGL.org, except for the actual man pages (and even some of those are wrong). You cannot rely on to have a particular attribute index. Indeed, the OpenGL specification is quite clear that does not have an attribute index. NVIDIA's implementation is in violation of the specification by giving it an attribute index that can conflict with user-defined indices. Non-NVIDIA implementations (aka: those that actually follow the specification) don't do this. But this is an easily avoidable problem. Never use any of the built-in attributes. Then you won't have a problem. 

I don't think you'll find any testing frameworks specifically geared towards testing bounding volumes and other geometric primitives or operations, but you can accomplish what you want with unit testing frameworks. 

I would suggest that you create editing software to construct your maps; this hypothetical editor would let you paint individual tiles onto a grid -- probably an isometric grid, from the sounds of it -- and eventually export that map into a format consumable by your game. In the long run, this will be a more efficient means of constructing your maps that trying to paint one sprite for the map. It may take longer to build up your initial tile set, but once you have a tile set creating new maps and adjusting existing maps will be much faster. The only real feasible approaches to starting with a single sprite and then extracting information from that about where each tile position would be are almost equivalent to constructing a tile-based editor anyhow, especially if you're going for an isometric approach because your 2D source image lacks any encoded depth information. Plus, with this approach you'll already have a basis for a tool that allows you to set up additional level data, such as placing invisible triggers for events or other scripted behaviors, placing the initial enemy or player unit formations (which obviously couldn't be painted on to your "single sprite"). Such a tool can also be scalable into creating 3D tile-based geometry, which is what Final Fantasy Tactics actually uses (allowing the screen to be rotated). 

So, why is the gold looted from a monster in DotA random? Because it's "supposed" to be random. Not because of any clever design, meticulous planning, whatever. You could make it not random and change virtually nothing about how the game plays out. Yes, you would be able to know for a fact how many enemies of that type it takes to get the money to buy X. But since the range distribution is so small, you already know the maximum number you have to kill. So you must have a plan to deal with the eventuality that you will need to kill that many. Therefore, all that changing it to a fixed number does is turn it from a probability into a certainty. Let's say something costs 250 gold. So, worst-case, that's 5 monsters that drop from 52-60 gold. But best-case, that's... 5 monsters. So if they dropped 56 gold, it would change nothing. But let's say you're talking about a 1000 gold item. Worst-case, that's 20 monsters. Best case, that's 17. On average, it's 18. But, since it takes so many monsters, the highs and lows will average out. So you're much more likely to need 18 than you are to need 20. Again, changing it to the average changes nothing. The ultimate evidence of this can be seen in the various WarCrafts. WarCraft 2 used random damage, with a large base and a smaller range. So did WarCraft 3. And while it owes a very great deal to WarCraft 2 in terms of design, StarCraft specifically has no random damage at all. The only random numbers used in StarCraft (multiplayer) matches are when attacking high ground. You might have heard of StarCraft: the single most widely played eSports game in the world. More than WC2 and WC3 put together. To summarize, I would say this. If you want random elements to matter: 

Use a timer with a decent resolution (such as on Windows). At the end of your update routine, query the timer and store the result somewhere. At the start of your draw routine, query the timer again and compare the result to the value from the end of the update routine. That will tell you how much time elapsed between the two. This seems like an unusual interval to measure, though; normally I'd think you'd want to measure the time taken from the start of update to the end of update, or the start of a draw to the end of a draw, and not the time between update and draw, during which (generally) not very much happens. 

At the bare minimum, that's all a locator server needs to do. When a player elects to join a listed game, the client can attempt a direct connection to the server using the information reported from the locator server -- you don't need to shuffle state between the two by way of the locator at all. However, a locator server can be useful to act as a third-party in NAT punch-through (alternate explanation) scenarios as well, which does involve transferring more information between the locator as a third-party, though never full game state. You could use it to pass through complete game state updates and act as a proxy, although in that case while the locator itself is still very straightforward to implement, it's potentially moving a lot more data around and that won't scale as well to a larger number of clients and servers. If that's what you're after (effectively avoiding the need for the game client and game server to ever talk directly), then you'll probably want to additionally have the locator server responsible for talking to a bunch of proxy servers you run. Each of these proxy servers exist solely to act as the intermediary for communication between a game client and server, which never talk directly. The proxies also must periodically communicate with the locator some information about how many games they are managing, for load-balancing purposes. The game client and server still use the locator server to establish an initial connection, but the locator immediately negotiates with which proxy is least-loaded at the time and messages are sent to the game client and server to "talk to proxy 32" (or whichever). The game client and server will then use proxy 32 (or whichever) to rely communication about game state updates and so on, until the game ends. It's quite likely your game won't be high-traffic enough to actually warrant needing a whole fleet of proxies, but if you do intend to avoid having a direct communication between the client and server for some odd reason (which seems unnecessarily complex), you should probably at least design the locator and proxying aspects of the intermediate server as separate. Just in case. You don't want to have your game become suddenly popular, and then have your network infrastructure fall over when the surge of new players happens. 

Deferred rendering. 90% of what you need to implement SSAO is found in basic deferred rendering algorithms. That's one reason why SSAO is often used by deferred renderers; it's a natural fit. 

XNA does. And generally most people who make games with 2D graphics do. It's the fastest way. But ultimately, this is an implementation detail that you should leave up to XNA. 

This blend mode is used to keep overlapping shadows from growing ever darker or lighter. It simply takes the darkest value written to that pixel. It should look good enough. You should also use the color write mask to turn off color writes. Render the table. When doing so The blending should be set up as follows: 

If you are doing debug drawing, then performance really shouldn't be your highest concern. Getting it working is far more important. After all, you won't be using this stuff in release builds, so what does it matter? However, if you are interested in one way to do it, this page on buffer object streaming suggests ways to improve performance for manually-generated vertex data. Pay particular attention to the "Buffer Update" method. 

Then you'd use that vector in a fragment shader to perform an cube texture lookup (into an environment map). Typically this is done alongside a reflection effect as well, and combined used a computed Fresnel term. To simulate chromatic aberration, then, you can perform three different refraction vector computations, each one slightly offset via differing indices of refraction, in the vertex shader: 

V8 itself is C++ and can be embedded in any C++ application. Thus, it is theoretically possible to use Node on consoles, although I'm not aware offhand of anybody who has tried. You may or may not run into porting issues as you would with any codebase. That said, I don't see why you'd choose to use Node for client-side work on consoles, given the hoop-jumping you'll need to do.