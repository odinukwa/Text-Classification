If $d$ is of the order of $n$ then you can write a constant-width branching program as a finite-state automaton, and logarithmic seed length is not known. But if $d$ is very small, say a constant, then you can do better and achieve logarithmic seed length -- I think, this is something I thought about years ago but never wrote down. The trick is to use Nisan's result RL $\subseteq$ SC. Basically, he shows that if you are given a branching program then you can find a logarithmic-seed distribution that fools it. His result extends to a small number of branching programs. So if $d$ is a constant then you can enumerate over all possible finite-state automata, and find a distribution that fools all of them. This should still work as long as the number of programs is polynomial in $n$. 

Think of the cell-probe model. Is there a data structure that can allocate contiguous chunks of memory of any length (like e.g. malloc in C), and free them, while avoiding memory segmentation, and executes every operation in worst-case deterministic O(log n) time where n is the total size of the memory? By avoiding memory segmentation I mean that if the total number of free cells is F, then I should be able to allocate a contiguous segment of F cells or about F cells. 

Do not restrict the length of submissions. Instead, ask authors to make their points within X pages, and inform them of the length of camera-ready versions. While some recent PC have courageously done so, this does not yet seem standard. 

What about { (M,$1^t$) : M is a turing machine that, run on a blank tape, accepts within t steps} ? The proof of NP-completeness is a simple exercise from the definition. 

First let us handle $k^2$ people and $k$ meals. Pick a finite field $F$ of size $k$ and identify people with $F \times F$. To each meal there corresponds a slope, to a table a line parallel to that slope. Specifically, meal $a$ has $k$ tables $\{(x,ax+b) | x \in F\}$ for every $b \in F$. The intersection property you want is the fact that lines with distinct slopes intersect in exactly one point. 

If I understand correctly you are asking about the relationship between the degree necessary for exact representation and the degree necessary for approximate representation. The seminal paper by Nisan and Szegedy shows that for $\epsilon$ constant these degrees are polynomially related. 

1) What is meant by necessary is that one way to generate a $k$-wise independent distribution is to break the input in blocks of $k+1$ bits, and let the $(k+1)$th bit of each block be the parity of the other $k$ bits in the block. Obviously this distribution can be broken just by computing parity on $k$ bits. The result you claim follows from the fact that poly($n$) circuits of depth $d$ can compute parity on $\log^{d-1} n$ bits. 2) No. 1) is only talking about a specific construction of $k$-wise independent distributions. Conceivably there are $O(\log n)$-seed generators that fool poly-size bounded-depth circuits (this also follows from sufficiently strong lower bounds against bounded-depth circuits, though the standard hardness vs. randomness tradeoffs do not suffice, see e.g. the discussion of a paper by Agrawal in Section 3.2 of $URL$ 

Here's a variant of the original answer (below) that gives the desired setting: tables of size 5, 45 people, and 10 meals, except that one meal has a few tables of size 4. Let $F$ be the field of size 9. Pick 4 vertical, degenerate lines $\{(b,x) | x \in F\}$ for every $b = 0,1,2,3$ and declare their people "empty." We are left with 81 - 9x4 = 45 people. 9 meals are given by slopes $a = 0,1,\ldots,8$. The intersections with the 4 empty degenerate lines reduce the table size to 9-4=5. An additional meal is given by the remaining degenerate lines $\{(b,x) | x \in F\}$ for every $b = 4,5,6,7,8$. Here the table size is 9. However (in any solution) we can break a table of size 9 into a table of size 5 and one of size 4. If there are a few more people one can use the field of size 11. 

Can one maximize $\sum_i c_i x_i^2$ where the $c_i$ are constants (possibly negative), subject to linear constraints over the $x_i$? This paper seems to come close to answering "no." They show it is NP-hard for target functions $x_1 - x^2_2$. However they have $x_1$ which is not squared, and from the two pages I can access online I can't understand if that's critical or not. Bonus question: Is there a free software that can be tried to solve these problems (possibly heuristically)? 

[CKKL99] Artur Czumaj, Przemyslawa Kanarek, Miroslaw Kutylowski, and Krzysztof Lo- rys. Delayed path coupling and generating random permutations via distributed stochastic processes. In Symposium on Discrete Algorithms (SODA), pages 271{ 280, 1999. [CKKL01] Artur Czumaj, Przemyslawa Kanarek, Miroslaw Kutylowski, and Krzysztof Lo- rys. Switching networks for generating random permutations, 2001. 

Some open problems in complexity theory lower bounds, together with their relationships, are mapped here. 

The following seems to be new and relevant information: The paper [CKKL99] shows how to get 1/n close to a uniform permutation of n elements using a switching network of depth O(log n), and hence a total of O(n log n) comparators. This construction is not explicit, but it can be made explicit if you increase the depth to polylog(n). See the pointers in the paper [CKKL01], which also contains more information. A previous comment already pointed out a result saying that O(n log n) switches suffice, but the difference is that in switching networks the elements being compared are fixed. 

There is a significant gap even when it comes to basic primitives such as pseudorandom generators. Consider for example pseudorandom functions. In practice people use things like AES, which differ from theoretical candidates (Goldreich, Goldwasser, Micali; Naor, Reingold; etc.) along several dimensions: First, the parameters are completely different, e.g. AES can have key length which equals the input length, which is unheard of in theoretical constructions. Perhaps more importantly, AES (and many other block ciphers) follow the so-called substitution-permutation network paradigm which is quite different from the way theoretical constructions go (e.g. those mentioned above). 

Consider the following card game (known in Italy as "Cavacamicia," which may be translated as "stripshirt"): Two players randomly split in two decks a standard deck of cards. Each player gets one deck. The players alternate placing down in a stack the next card from their deck. If a player (A) places down a special card, i.e. a I, II, or III, the other player (B) has to place down consecutively the corresponding number of cards. 

The question you are asking is equivalent to unary NP = unary P, which in turn is equivalent to NE = E, by padding. From the title, perhaps you meant to ask if it is possible to generate input/output pairs such that the distribution on the inputs is "hard." The possibility of doing this lies somewhere between P $\ne$ NP and one-way functions exist. In restricted computational models, it is known that this is possible. E.g. one can generate input/output pairs for the parity or majority functions in AC$^0$ or below. See The complexity of distributions. 

Regarding your last question: The paper Size-Depth Trade-offs for Threshold Circuits shows that the parity function requires depth-$d$ threshold circuits with $\ge n^{1+\epsilon(d)}$ wires, which is tight up to the function $\epsilon$. But for gates not even $\Omega(n)$ lower bounds are known. 

Can you provide a summary of the approach that is tailored to an audience of complexity theorists? What would it take for this approach to prove anything, including re-proving known lower bounds? 

Added later: As noted in the comments, the NP upper bound is trivial if a, b, and c are positive, as was asked. Theorem 1.2 in this paper shows that deciding if a given diophantine equation in two variables has a solution is in NP. 

Create a new node $s$ and connect it to every node in the first list. Create a new node $t$ and connect it to every node in the second list. Find a shortest path between $s$ and $t$. 

See The communication complexity of addition. As Grigory mentioned, there is a protocol with communication $O(\log n)$. This is due to Nisan and Safra. Their protocol either uses public randomness or is not explicit. The above paper gives one that uses private randomness and is explicit (via a relatively standard use of pseudorandom generators); it also discusses matching lower bounds in the public-randomness model. 

The first player to run out of cards loses the game. Note: The outcome of the game depends exclusively on the initial partition of the deck. (Which may make this game look a bit pointless ;-) Question: Does this game always terminate? What if we generalize this game and give any two sequences of cards to each player? 

I think the above is such an example, see for example this paper with Eric Miles (from which essentially this answer is taken). 

If a submission is (essentially) a resubmission of a previosuly-rejected paper, the authors should copy and paste previous reviews (in a text box on the submission server), together with a very brief statement on the main changes (if any). This would make the whole process more transparent, and spare the pc the need to fish out the old reviews by themselves. Of course the pc should be wise enough to give the paper a second chance without being biased by a previous strong negative review. (Personally, this is something I almost did a few times, but in the end didn't out of fear it would bias the PC negatively since nobody else seems to do it.) 

To handle $2k^2$ people, divide them up in two groups of $k^2$ each, and apply the construction above to each group. To handle $2k^2 - k = 45$, label (in the first group) a fixed line such as $\{(x,x)|x\in F\}$ as "empty." You may have a few tables with $k-1$ people. For more meals one could e.g. pick a different partition in two groups at the beginning of the 6th meal. (Say you interleave the original partition, to ensure that the two groups "mix.") Though of course this may result in some intersections. 

Gowers has recently outlined a problem, which he calls "discretized Borel determinacy," whose solution is related to proving circuit lower bounds. 

Regarding log-space: Several candidate one-way functions are computable in log-space or below (and are supposedly secure even against poly-time adversaries). You can find several pointers for example in the Cryptography in NC$^0$ paper. Two specific examples include: Integer multiplication (there are some subtleties for standard OWF, but if you only care about worst-case this is enough) The Impagliazzo-Naor candidate based on subset-sum: $f(a_1,...,a_n,S) := (a_1,..., a_n, \sum_{i \in S} a_i \mod 2^n)$. 

Include among important dates the date for special issue invitations. Otherwise, how long are authors supposed to wait for? 

It is possible ;-) It would give new circuit lower bounds. Since you are making a pretty strong assumption this could follow from the seminal work by Impagliazzo, Kabanets, and Wigderson, I haven't checked. If you use Williams' approach, tightened here, you get a lower bound of $n^{1+\Omega(1)}$ for a function on $n$ bits in the class E$^{NP}$. (For this approach, a fast co-nondeterministic algorithm for satisfibility is sufficient, as you are postulating.) Specifically, the latter paper shows that a lower bound for size $s$ follows from a satisfiability algorithm for circuits of size $O(s)$, which we can translate to a 3SAT instance with $O(s)$ variables by Cook-Levin. It would not directly contradict ETH because that's for deterministic algorithms.