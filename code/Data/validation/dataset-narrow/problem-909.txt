I am converting a legacy Apache server to Nginx and do not have the luxury of changing URL's or rearranging the filesystem. Is it possible to use nested location{} blocks in the Nginx configuration to tell it to feed the .php files in an aliased directory to fastcgi while serving static content normally? Similar configuration to what fails me: 

I have a branch office behind a Watchguard XTM that needs VPN into an EC2 VPC. I am unfamiliar with Watchguard and am unable to find all of the knobs and dials in the flash admin interface to bring it in line with Amazon's expectations. After much ui frustration, I managed to create configurations for both expected tunnels that meet most of the specified criteria - as I was unable to locate some settings like MTU and ESP options but I have the correct PSK, PFS settings, and am telling it to use SHA1 and AES128 as requested. Attempts to access EC2 private addresses from the office generate a bunch of debug spam as is expected. However, IKE fails with: 

Why does SQL Server need to lookup all of the LOB data pages in order to X lock them? Is that just a detail of how locks are represented in memory (stored with the page somehow)? This makes the I/O impact depend strongly on data size if not completely cached. Why the X locks at all, just to deallocate them? Isn't it sufficient to lock just the index leaf with the in-row portion, since the deallocation does not need to modify the pages themselves? Is there some other way to get at the LOB data that the lock protects against? Why deallocate the pages up front at all, given that there is already a background task dedicated to this type of work? 

Update 1 Tested a theory that the data is being written to the transaction log as part of the delete, and this does not seem to be the case. Am I testing for this incorrectly? See below. 

If these both came from the same csr, then the md5 will match. Check the certs against the private key as follows to ensure the cert and private key match up: 

It appears that you may be missing some settings and may have some incorrect. You might try these settings 

Office 365 will not be allowing you to send on behalf of a 3rd party address. To do what they want, you will need an email server. The problem is that many of the emails will not get delivered since they will not originate from the proper email server(s) for the client and will be flagged as spam. To make this work, you will need that email server, and the client IT department will have to make DNS changes to make your server an authorized source of email from their domain. Or just set up a quick ubuntu server with postfix configured as the MTA for the client and send the blast. Some (or most) won't get delivered though. 

Our data size distribution doesn't warrant it. In practice, we add data in many chunks, and filestream doesn't support partial updates. We would need to design around this. 

For a file over 5 MB in size, this returned . Further, I would expect the pages themselves to be dirty if data were written to the log. Only the deallocations seem to be logged, which matches what is dirty after the delete. Update 2 I did get a response back from Paul Randal. He affirmed the fact that it has to read all of the pages in order to traverse the tree and find which pages to deallocate, and stated that there is no other way to look up which pages. This is a half answer to 1 & 2 (though doesn't explain the need for locks on out-of-row data, but that is small potatoes). Question 3 is still open: Why deallocate the pages up front if there is already a background task to do cleanup for deletes? And of course, the all important question: Is there a way to directly mitigate (i.e. not work around) this size-dependent delete behavior? I would think this would be a more common issue, unless we're really the only ones storing and deleting 50 MB rows in SQL Server? Does everyone else out there work around this with some form of a garbage collection job? 

To terminate an IPsec VPN you probably want to look at Openswan or strongSwan. The Openswan wiki has an entry on interoperability with Cisco: 

There is no good reason to suffer with half duplex. :) It sounds like the switch is inadequate for your needs. In my (admittedly limited) experience, cheap consumer-grade switches (even from "name" brands) cannot run at the full capacity implied by their number of ports and advertised speeds. Do you have another switch you could try in this one's place? Is it always the same ports that are failing? Always the same clients? It isn't difficult to get a decent 24-port managed switch for $150-500. 

Percona's xtrabackup utility has a --throttle option to reduce the IO load of the backup job. The docs say that the value passed is the number of read/write pairs per second. Is 1000/sec an appropriate value on modern hardware? How about 5? I cannot find any meaningful frame of reference for these values. For reference - I am reading from and writing to the same drive array (10k SAS). A 55gb backup job with --throttle=20 ran in roughly an hour with no apparent strain on the system during off-peak hours. But I honestly don't know if this is a high or low value for the throttle. 

This question is related to this forum thread. Running SQL Server 2008 Developer Edition on my workstation and an Enterprise Edition two-node virtual machine cluster where I refer to "alpha cluster". The time it takes to delete rows with a varbinary(max) column is directly related to the length of the data in that column. That may sound intuitive at first, but after investigation, it clashes with my understanding of how SQL Server actually deletes rows in general and deals with this kind of data. The problem stems from a delete timeout (> 30 seconds) issue we are seeing in our .NET web application, but I have simplified it for the sake of this discussion. When a record is deleted, SQL Server marks it as a ghost to be cleaned up by a Ghost Cleanup Task at a later time after the transaction commits (see Paul Randal's blog). In a test deleting three rows with 16 KB, 4 MB, and 50 MB data in a varbinary(max) column, respectively, I see this happening on the page with the in-row portion of the data, as well as in the transaction log. What seems odd to me is that X locks are placed on all of the LOB data pages during the delete, and the pages are deallocated in the PFS. I see this in the transaction log, as well as with and the results of the DMV (). This creates an I/O bottleneck on my workstation and our alpha cluster if those pages are not already in the buffer cache. In fact, the from the same DMV is practically the entire duration of the delete, and the corresponds with the number of locked pages. For the 50 MB file on my workstation, this translates to over 3 seconds when starting with an empty buffer cache ( / ), and I have no doubt it would be longer for heavy fragmentation and under load. I tried to make sure that it wasn't just allocating space in the cache taking up that time. I read in 2 GB of data from other rows before executing the delete instead of the method, which is more than is allotted to the SQL Server process. Not sure if that is a valid test or not, since I don't know how SQL Server shuffles the data around. I assumed it would always push out the old in favor of the new. Further, it doesn't even modify the pages. This I can see with . The pages are clean after the delete, while the number of modified pages is less than 20 for all three small, medium, and large deletes. I also compared output of for a sampling of the looked up pages, and there were no changes (only the bit was removed from PFS). It just deallocates them. To further prove that the page lookups/deallocations are causing the issue, I tried the same test using a filestream column instead of vanilla varbinary(max). The deletes were constant time, regardless of LOB size. So, first my academic questions: 

That line is asserting that the specified port is an integer. My guess is that the assert fails since wgSphinxSearch_port is now undefined. I don't know why you'd need multiple searchd instances. What prevents you from using a single instance and different indices for each wiki db? 

For obvious reasons, I haven't even started trying to configure BGP throught the tunnel. At this point, I am wondering if it is even proven possible to configure l2l into VPC with a Watchguard firewall? If so, where might I be going wrong? 

If you have to go through scp/ssh, my experiments show that the fastest cipher enabled by default these days is RC4. You specify the cipher via '-c arcfour' in your ssh/scp command: for initial copy: 

Requests for /foosite/static.jpg are served fine, but nginx appears to garble the path to any .php files when attempting to dispatch them to fastcgi. 

GFS is some seriously black voodoo. The amount of work required to get a simple two client cluster working is staggering compared to the alternatives. OCFS2 is a lot simpler to deploy but is very picky when it comes to the kernel module versions involved on all attached servers - and that's just the beginning. Unless you really need the kind of low-level access a cluster filesystem offers, NFS or CIFS is probably all you need. 

The public key to use is set in the authorized_keys file for the user account on the server to which you connect. It is not set in the client. $URL$ is a good read for how this works. 

The output md5 hash values should match. You can check your csr also to ensure that it matches your private key and cert. 

I am a system administrator and not a programmer. I frequently su to different system users for task so a .vimrc file for each system user is cumbersome. I need to be able to read the comments in config files and VIM syntax highlighting gets in the way for me. How does one globally turn off syntax highlighting in VIM on Ubuntu systems? 

Determine the IP address that is assigned to your server and then go onto the DHCP and set a DHCP reservation for that server. 

This is known to work, so no worries there. The ssh tunnel needs to be automatically created and there must be monitoring ensure that it is functional end to end, and if not to be restarted. Additionally, this functionality may not be dependent on or interrupted by a VPN tunnel that may or may not be present from the user remote workstation into the corporate network. I would appreciate any suggestions. Please, no comments on the why of using ssh tunneling if a VPN is available. I have listed the requirements levied upon me.