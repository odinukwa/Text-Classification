The difficult part with SQL Agent jobs on Always On Availability Groups (AG) instances is that you usually want the job to run only if the node it is on is the PRIMARY. You want the jobs to exist on each node so that they are there in the event of a fail over. I've found making the Agent Jobs "AG aware" is the easiest approach. I talk about a similar situation here. The general idea is to add a piece of code to each Agent job that checks to see if it's node is currently the PRIMARY. If it is then run the job, if not stop executing. The code below (modified from my blog post) should help: 

This is a common issue, you want to use groups to keep things simple but you don't want to lose visibility of who has access to your databases. The way I see it you have two options: 

Create a database master key Create a certificate Backup your certificate and keys Import the certificate to each node of the AG and any other instance you plan on restoring to Use the option in your backup statements 

The cluster will pickup all the different subnets you have configured on your NICs. Once a server with NICs that have IP addresses of the 2nd subnet is included in the cluster you will be asked to add an IP for each subnet for each role as you add them to the cluster. 

This is a becoming a big requirement in todays environments and I don't see any easy solution other than using a 3rd party tool. I'm no expert but I was looking at Delphix site the other day and it sounds very interesting. $URL$ It solves the problem with the use of virtualisation and versioning of the VMs that hold the data. Just a shame it has the overhead of virtualisation. 

I'd say you should have fields on the and so that it's possible to see when the status of the ticket changed. This would allow you to track changes from start to end. I would also recommend that use the same name for columns that are used to join tables. So the table should have a column that matches the column of the same name in the table. This is standard practice as it avoids a lot of confusion that can happen when writing and bug tracing queries. Those two points are my only comment as the general design seems to meet your requirements. 

Come clean ASAP. The sooner you do this the better. If I'm reading the timeline correctly, when you restored the backup over your production database you broke the backup chain. This means you wont be able to get the data for 06:30PM to 10:00PM into your current database that holds data for 01AM to 06:30PM + new data since the . I would suggest you the database with 06:30PM to 10:00PM data and figure out a T-SQL code based way to move that data into your live database. You may have problems with columns as new rows in both databases have probably taken the same IDs. IMO the quicker you get other people involved to build a plan to fix this the better. Good Luck 

I was getting this error when attempting to backup a database with multiple file groups without specifically specifying each file group in the backup statement on SQL Server 2008. As soon as I added a FILEGROUP = N'FileGroupName' for each file group it worked. 

You could use an unstructured column type like XML or JSON. But MySQL doesn't support these types AFAIK. You could store these types as a string in MySQL but I wouldn't recommend that. The reason for that is if you need to change one value in the XML/JSON you would need to change the whole string or somehow parse it with with RegEx. Nasty. So to use an unstructered type you would need to move to a different RDBMS. MariaDB has some JSON support. Switching from MySQL to MariaDB should be an easy migration, see this guide from DigitalOcean. If you don't want to change RDBMS then I think the EAV model mentioned by Phil is the best choice although these tend to degrade in performance as the data grows. Filtered indexes can help but they too AFAIK are not available in MySQL. 

Something must be trying to use xp_cmdshell to execute a batch file outside of sql server. It looks like xp_cmdshell has been switched on but you need to run for it to be active. Then the messages will disappear as what ever is trying to use it will be able to. 

It's a good idea to create SQL alerts in the SQL agent for all errors of severity 16 through to 25. Also create 1 SQL alert for each of these error IDs 823 824 825. I would create an operator that points to an AD group (probably DBA or ITsupport) so that a team can be alerted or you can drop users in and out of that group when you are away so that errors are not missed. This will keep you in the loop when things are happening on your server that you should know about but something like Redgate SQL Monitor is a much better solution. 

The default roles all have db_ and then something descriptive of the roles permissions. I would stick with that. 

I have used option 2 from this article many times to fix this issue. Basically insert the windows install disk, enable the .net 3 feature but select the option to 'specify an alternate path' at the end of the wizard. The different path should be D:\sources\sxs (if the disc is in your D drive) 

Will having enabled decrease the amount of memory used by an in memory OLTP table? If yes I assume the CPU workload will increase. 

I don't think is what you want here. I think you are looking for or . This will rank the rows your query produces starting at 1 up to the total amount of rows returned. The example below illustrates the different way and handle the ranking of rows with the same value. 

A restore failing on the secondary will have no impact on the primary. You want to monitor your secondary DBs to make sure this hasn't happened. 

Your script is checking for indexes with 8 or more pages (64kb). This is far too low. Small tables can easily have a high percentage of fragmentation. Also fragmentation wont have much impact on scanning a table that small. It's is most likely these small tables that you are seeing with 40% fragmentation after the script has ran. The script is doing a on indexes with fragmentation between 15% and 30%. Anything over 30% receives the command. This is the Microsoft stated best practice but IMHO it is too aggressive. I tend to have something like 30% and 50% but it's really down to your system and requirements. I would highly recommend Ola Hallengren's Maintenance Solution as a replacement for this script as it covers much more. 

The image in this article is used quite a bit around the internet in various different forms but it highlights which systems fall in each category of CAP So your are basically asking which horizontally scalable systems don't rely on eventual consistency. To which MongoDB, HBase and many more would be a good answer. 

The "Backup set will Expire" option is used to indicate how long a backup should be prohibited from being over written. It will not clean up old backup files. You can use the "Maintenance Clean up Task" in your maintenance plan to clean up backup files older than a certain number of days. 

Remove this line or set this value to and the database will restore as you expect. This setting is useful when restoring log backups. This includes when you restore the tail of the log. Books Online has more info here As for the duration of a it depends on your storage and the size of your database. With that said, 80MB shouldn't take very long at all. 

From my limited knowledge of how query plans are compiled, stored and retrieved by queries I understand that a multi statement query or stored procedure will generate it's query plan which will be stored in the query plan cache to be used by the query in future executions. I think this plan is retrieved from the query plan cache with the query hash, which means if the query is edited and executed the hash is different and a new plan is generated as no matching hash can be found in the query plan cache. My question is: If a user executes a statement that is one of the statements in the multi-statement query can it use that relevant part of the query plan already in the cache for the multi-statement query? I expect the answer is no because the hash values will obviously not match, but would it be better to hash each statement in a multi-statement query so they could be used by users running individual statements from the query? I expect there are complications that I'm not taking into account (And it's these that I really want to know about) but it seems like we could be storing the same 'statement plan' in many query plans taking up more space and taking more CPU and time to generate. Could just be showing my ignorance though. 

I am looking to deploy log shipping in my production environment. This will mean I will have to manage backup chains which could be disrupted by people taking backups to refresh development environments. If I have a failure and want to use the log backups to restore to a point in time I will also need the backup taken by the developer. This wouldn't happen if the developers only used COPY ONLY backups. So my question is: Is there a way to restrict users to only be able to perform COPY ONLY backups? 

I would rephrase your question to what is likely to make my company shell out all the extra cash for Enterprise edition? Well it depends on the version of SQL server but below is a list of features and reasons that businesses upgrade for.