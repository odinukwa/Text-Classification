If you take care to replicate the records exactly, then it will not affect your users at all. Many providers will let you export your "zone file" so it can be reimported elsewhere. If this is an option, it would save you some typing and help avoid possible typos. 

I've set up this stack many times on AWS. Option #1 has always worked well for me and is the one I normally choose because it's the simplest. I'm curious to know how much traffic you're dealing with where the less-than-ideal initial cache hits are an issue? I'm serving a few million pageviews a month on a pair of m1.small instances and they're barely scratched. Other thoughts: 

I am looking at rolling out a new configuration management tool to replace our home-grown solution. The defacto standards are Chef and Puppet, both of which are Ruby-centric (though can be used to deploy non-Ruby environment, obviously). The vast majority of our development is done in Python and our in-house deployment tools make heavy use of Fabric. Therefore I am learning towards Salt since it too is Python, even though it is not as mature as Chef or Puppet. But since I'm not familiar enough with the options, I'm finding it difficult to compare apples-to-apples. Other than the smaller community, would I be giving up anything signifcant by using Salt rather than Puppet/Chef? Update It's been six months since I posted this question. And despite it being closed, it's been viewed over 1,000 times so I thought I'd comment on my experiences. I eventually decided on Puppet since it had a bigger community. However, it was an immensely frustrating experience, mainly due to the convoluted Puppet configuration syntax. Since I now had a frame of reference to compare the two, I recently took another look at Salt--I'm not going back. It is very, very cool. The things I like best: 

Sqlite isn't a traditional client/server DB application. It's essentially a library that's embedded within another application. It's designed for single-user desktop applications. You absolutely do not want to try to use it as some sort of standalone MySQL/PostgreSQL/MS-SQL replacement in a multiuser enviroment because the entire DB is locked on write. You'll be dealing with contention issues on even a light load which will destroy performance. 

I'm very much as "do it yourself" guy when it comes to almost all tech, including hosting. But I don't host my own DNS because it's so critically important and commercial providers are extremely cheap. All my zones are hosted at ZoneEdit. Each of my zones has at least two US-based DNS servers (the min. required), but a couple of my more important zones also have a third server located in a separate network in Germany. I could add additional servers if I felt it was necessary. Total cost for this? About $20/year/zone. Edit: The concern about a registrar's servers going down is understandable but unwarranted. The hierarchical nature of DNS means that your site will continue to work even if they go offline. The root servers are at the top of the hierarchy and are the only part of DNS that must remain operational for everything to work. 

This will vary considerably: ball bearings vs sleeve bearings, made in Japan or China, ambient temperature, number of on/off cycles, etc. But this is kinda like asking if you should be changing out hard drives periodically as they age. You don't do that because there's no reliable way to predict failure. So the proper way to do this is to gracefully handle failures when they do occur. This would be a RAID array for hard drives, or using redundant fans for cooling. 

The Environment I have a small, dual-core Intel Atom-based server running CentOS 5.5 x64 with a slightly customized Xen kernel. It also has one on-board 10/100 NIC with an additional 3-port 10/100 NIC. Within this server, I also run a single Xen domU which functions as a firewall, DHCP server, and caching DNS forwarder. The domU is running CentOS 5.5 x64 as well, but with a stock Xen kernel. I'm using the pciback kernel module to hide the 3-port NIC from the dom0 and assign it to my virtualized firewall. Eth1 is my public interface and the on-board NIC (eth0) is my private interface, which is on a XEN bridge and shared between both the Dom0 and DomU. The Problem The problem is that eth1 (the public interface on my virtualized firewall) decides to stop working several times a day. It seems to be related to usage: if I barely run much traffic across that interface, it might last a couple days. Heavy web browsing though will take it down in a couple hours. When it dies, this is the error in /var/log/messages on my firewall: 

Yes. VPC paired with a IPSec VPN connection to your corporate network. This will allow your AWS resources to function as an extension of your existing network. Be warned though: it's not for the faint of heart to configure. 

I've been trying to get LDAP authentication and NFS exported home directories on CentOS 6 working for a few days now. I've gotten to the point that I can now login to the client machine using the username and password in LDAP. On the client, /home and /opt are mounted in the fstab over NFS. However, every file in both /opt and /home is owned by (uid: 99, gid: 99) on the client. However my uid and gid appear to be properly set: 

Yes they do. Amazon will reject your submission to the marketplace if you embed any SSH keys into the AMI so you cannot use your own. The users' own keys will get added to the instance when it's launched. 

This tells you who ARIN assigned my netblock to, which is Online Technologies, my datacenter provider. But there's no mention of me or my company. It's because I don't have my own assignment from ARIN, so I'm subletting address space. But all the equipment is owned an managed by me. Does this mean I'm a reseller of the datacenter? As part of our hosting, we also provide CPanel accounts. A feature of WHM/CPanel is the ability to create sub-accounts. I have one business partner who resells accounts that he sets up and provides first-level support. But I've available for second-level support, which I provide on a routine basis. Is he considered a reseller? I'm a bit confused on how you can be writing a book on the web hosting business without understanding how nebulous your question is? 

I have a dual-port Intel Pro/1000 (82571EB) PCIe card. I've installed in into a Dell PowerEdge 1950 running CentOS 6.2 x64. The card is detected and the kernel module is loaded, but a start-up script is not created in . Manually creating a startup script doesn't help; I get an error about the device not being detected. Thinking it might be a bad card, I tried another card (same model) and even put it in a different slot. No help. However, the strange thing is that this card works without issue in a PowerEdge 2950. Does anyone have any suggestions? Thanks! output: 

Given the distance and proximity to power cables, yes, fiber makes sense. The maximum length of a Cat6 run is 55 meters for it to be within spec. I would simply buy two switches, each with a GBIC uplink port. Install a fiber module in each and you should be good to go. It's not all that expensive. 

Login using PuTTY and run This will change the directory owner to . If this doesn't work, make sure this is the correct account. It doesn't sound correct. 

Login to WHM and go to "Configure Backup." All the way at the bottom are the options for the backup destination. You can choose either FTP or a destination on the local filesystem (for example "/root/backups"). Chose the local filesystem and then just configure s3cmd to run via a cron job to replicate that directory to S3. 

Looks fine, but remove the directive unless your cert will work on both "www.example.com" and "example.com".