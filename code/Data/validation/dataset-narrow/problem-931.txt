If you want no single points of failure at all, you need to do global server load balancing -- you obviously can't rely on a single datacentre, and even with a redundant BGP configuration, your BGP tables constitute a single point of failure that can be messed up if someone pushes a bad config. What you do is configure DNS to advertise multiple IP addresses for the A record for your domain name, pointing at copies of your site that are in different datacentres (preferably in different cities), and the browser will pick one (usually at random, but watch out for Windows Vista which implements the stupid bits of RFC3484 and is thus not random), and will store the others. Depending on the browser, it will generally use one of the other addresses if the one it's using becomes unavailable. Your DNS servers have to continually monitor all of the sites and stop advertising any that go down. They also need very short TTLs. There are hardware solutions for this -- e.g. F5's BigIP devices. You'll also need ways to replicate your database, your files, and your users' session states between the datacentres in realtime. You'll obviously also need to get network diagrams and supplier lists from all of your ISPs to make sure that all your network routes are fully geographically diverse and the ISPs don't rely on the same upstream supplier. It's probably worth making sure they're not on the same power grid, as well. See here for more information on global server load balancing (although it's a bit old and out-of-date): $URL$ Your failover won't be quite as fast as BGP failover, but you won't be able to bring your site entirely down with a single bad BGP config. You may mess up the configuration of a single DNS server or datacentre, but that won't bring you down completely (unless you push your DNS updates automatically to all of your DNS servers). 

Before you even start to look at optimisation, you need to work out where your bottleneck is. Check your CPU load, memory usage, network load and disk load on all of your machines. Then try isolating the time taken for different parts of the process -- time taken to execute queries against the database, time taken by your app in generating pages, time taken by the browser downloading and rendering the content, and so on. That will give you some clue about what kind of areas you should be looking to optimise. 

No, I fear the previous answer is wrong. If you stick to giving users subdomains then your wildcard certificate will work, but if you allow users to map custom domains to their subdomains with CNAMEs, then they will need separate certificates for each custom domain, and separate public IP addresses as well. 

The browser is warning about mixed domains because you're using mixed domains. The fact that they happen to use the same wildcard certificate is not relevant here. Everything is working the way it's supposed to. The fix is to not use mixed domains on your SSL websites. 

That's not a bug, it's how PHP is supposed to work. Restart your PHP server or call apc_clear_cache() from your code to clear the APC cache. 

For redundancy within a single site, on a single Internet feed, you want to put clustered hardware on your front end, with a standby box ready to take over the IP address of a failed box. But you'll be out of action if your ISP has a failure, or your site loses power or suffers some other problem. If you want protection against loss of a whole site, or loss of your ISP, then there are really only two options. One is to get your own BGP autonomous system number, and run your own BGP routes, with peering (well, paid transit) with several different ISPs. The smallest netblock you can do that with is a /24, so you'll need to have a netblock at least that big. You can then advertise different routes to a different site if your main site goes down. Your other option, as you suggest, is round robin DNS. Some people advise against this on theoretical grounds, and there are problems with Windows Vista clients selecting addresses non-randomly, but it should work fine for redundancy, with the backup box just reverse-proxying traffic back to the main box unless the main box/site/Internet feed goes down. 

Yes, UDP routes over IP in just the same way as TCP. The default gateway is actually an IP concept, and has nothing to do with TCP -- it will work with any protocol that is built on top of IP, whether that's TCP, UDP or anything else. 

A need for PCI compliance would be a contraindication. PCI DSS version 3.2 requires that you "Implement only one primary function per server to prevent functions that require different security levels from co-existing on the same server". Config and sharding would be two primary functions. 

With a TLS/SSL connection to a website, your network provider can't see your traffic but they can see what IP address you’re connecting too, and can make a pretty good guess at the kind of content you’re viewing from the data volumes and the timing of the packets. With a VPN, they get less of that information, because all of your traffic just goes to the VPN (though of course the VPN provider gets the information instead). 

The only reason it's looking for id_rsa and id_dsa is because you've not told it where to find a private key that works -- it falls back on the defaults instead. So don't worry about having an id_rsa file, and instead worry about how to get it to use your PK-xxx.pem file. The following syntax should work: 

The DNS traversal checker at squish.net is extremely useful for diagnosing DNS issues. In this case, it says: 

The best tool is the scientific method. Form a hypothesis about the cause of the problem. Write it down. Formulate an experiment that will test the hypothesis. Write it down. Conduct the experiment. Write the result down. If the experiment confirms the hypothesis, then you're done. If it fails to confirm the hypothesis then you need a new hypothesis. If it's inconclusive then you need a new experiment. 

The first MX means that the IP addresses in the MX record(s) for the domain you're actually attaching the SPF record to should be accepted as valid. The second one means that IP addresses in the MX record(s) for the domain mail.mydomain.com should be accepted as valid. If this SPF record is for the domain mail.mydomain.com, then the second one is redundant. However, if the SPF record is for mydomain.com, then the second MX is not redundant. 

For InnoDB tables, --single-transaction will ensure that the dump of each table is consistent, but it will not necessarily be consistent with the other tables, since it locks and dumps them one at a time rather than locking them all for the duration of the dump. And you will, of course, have delayed writes waiting for the lock to release if you try to write to a table while it's being dumped. A better bet is to use an LVM or filesystem that allows you to take snapshots, then lock the entire database for just long enough to take a snapshot, which you can then back up at your leisure. 

While Openssl itself does not include any root CA certificates, most operating system distributions that include openssl do include such certificates, so you probably already have the Verisign root certificate installed on your system. For example in Debian Linux, they're in the ca-certificates package. 

Yes, you can, because there's no such thing as a "domain name for a server". Domain names are not attached to servers. You can buy any domain name you want, that's not already owned by someone else, and then map it to any IP address you want. It doesn't matter what, if anything, is currently attached to the Internet on that IP address. Having said that, if you buy the domain triplehotxxxbabes.com and point it at your school's server then they may not be very happy with you. 

Most VPS companies prohibit and block outbound traffic on port 25 to stop them being used for spamming. You'll need to use a third-party mail relay that listens on a different port (which may well be a service that your VPS company can provide). 

It's not a routing problem, it's a DNS problem. Everyone can route to your site, they just can't look up the IP address from the domain name. Your DNS appears to be set up correctly, so the problem is probably with 4.2.2.4. 4.2.2.4 appears to be owned by Level 3, so I wouldn't worry about search engines -- they'll be using their own DNS setups, and as I say, your DNS appears to be set up correctly. Your best bet is to get a Level 3 customer to raise a query with them. 

As well as disabling export DHE cipher suites, you need to use a 2048-bit Diffie-Helman group rather than the 1024-bit that Tomcat is probably using. It's believed that someone with the resources of the NSA could break 1024-bit. To do so, add to your Java or Catalina options. Note that this only works in Java 8 or later -- if you're on 7 (or earlier) then it's time to upgrade. And while you're at it, disable the RC4 cipher suites -- RC4 is no longer secure. 

You've only got one VirtualHost defined on port 443, so any access to port 443 will have to use that config, no matter what domain name they're using. If that's not what you want, set up VirtualHost configs for the other domains and point them somewhere different. The same in reverse for your issue with $URL$ -- you've not defined a VirtualHost for that domain name, so it's using your default VirtualHost, which is the first one, and so it's using /var/www as the root.