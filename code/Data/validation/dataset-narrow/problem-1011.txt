You don't need to create the overlay mov as a separate step. First create the overlay like a movie in this way: 

See this guide for more details. Using icecast, with password for source as "hackme" (the default), and port 8000 (default) as one of the ports, fire up using: 

Here the background image ( eg ) is used as one input for the overlay filter, and the main movie (eg ) is the other. To match the start time stamps I used the filter for both, setting the start time stamp to 0. The output of the second input is scaled to 800x452 not 800x453 because the scale filter has problems when the number is not divisible by 2. The output of this is labeled [V2] and passed on to the overlay filter, while setting , implying encode until the shortest input. Because the background image is generated in infinite loop, the shortest length here is the length of the input main movie. So it encodes till the end of the movie. The positioning is at half the value of 1280-800=480, divide by 2 to get 240. Similarly for y pixel value. But your actual corner of the box may be different. And you may need to use more encoding parameters, depending on the input movie file and output required. 

This is across : One internal drive One external USB Drive One Network drive on another windows machine on the same domain The command use is : 

If you need audio only (I hope I understood right), then you don't need the first step at all. If you use: 

All the above options will return only the matched portion. For the full line to be returned, do not use the -o option. So it would be: 

The title in this case is nested under . The label is required. But to do this ensure that you have imported the namespace in the begining of the stylesheet: 

The filter needs to extract each channel (y+u+v[y][u][v]), which can later be combined with . Note that I stated format of the output explicitly to be yuv420p, because my input material is of the same format. Note that I used your strengths and values for the filter, which made my "normal" footage completely unusable. That is not to say that it will not work on badly shot footage. But even a value of was right at the edge of usability. This is because histeq is a global filter and applying to one channel is not optimal process. In any case you can use the filter to individually target channels as well. But the principle of splitting and merging is as above. 

In a production environment when there are many videos to join, to automate the process you could break it into pretty much four components. Lets say you have 100 main training videos to begin with. These are named main001.mov, main002.mov and so on till main100.mov. 

You need to specify filter or filter-complex and you dont need input pad because it is default and understood. Also, note that here you are not really "turning it to mp3 and then converting to two different qualities". You are encoding into mp3 into 2 different qualities. 

This does not use any other filter, it uses the pad to decrease the size, and then the overlay filter to lay it on at four pixel positions. But in the filter chain it has to do this several times, combining the effects of the first 2 inputs, then overlaying on the third and then on the fourth. I have used container but it can work for other containers. You will not need under most circumstances- here it is used because my ffmpeg defaults to audio encoder for mp4. If there is no audio which is very likely in your case, again you will not need the option. If you do have audio as an input but do not want to use it in the output, add the parameter in the command. Alternatively, and this is a good option, use the option: 

Note that I have not included any encoding parameters (quality) at all. When you concatenate ts files you should be able to simply say: 

for your example. See here. In the trim parameters every pair of values are in to out. 0 is in point, 3:00 outpoint then 6:00 inpoint and 24:00 outpoint. The = sign means relative to beginning. 

Let me take a completely different approach, to add to Johannes' answer. or "frames per second" originates in the film world. film used to run run at 24 frames per second. In other words, the motion you saw on screen as continuous was actually due to "persistence of vision" creating a fluid motion out of 24 discrete images or Frames. With video came other frame rates. Countries with 110volts electricity at 60 Hz chose 30 fps as it was easy to keep perfect time this way. Other countries with 50Hz chose 25 frames per second (see the relation?). When and encoding came into the picture, we started talking in terms of bitrates or the amount of information contained (in a frame or per second). With the same kind of encoding algorithm, higher bitrates usually offer better quality. But the same amount of data in a smaller frame (=fewer pixels) also means better quality. And obviously if measured in seconds, you add more data if there are more frames per second. Conversely, if you are using say 1024 KBps (kilobytes per second), then for 60 fps it gives lower quality than at 24 fps, because there's fewer frames in 24fps over which this 1024 is distributed, that is 1024/24 = 42.6 KB per frame. With 60 fps, it is only 17.06 kilobytes per frame. Although this is simplistic in many ways, it does give a general idea. 

Here we are outputting to mp3 from 3 different audio codec inputs. You will have to input the mp3 encoding parameters. I'm assuming your input files are audio only. Good parameters for mp3 might 256 kbps. Then use this: 

Open VLC and connect to stream- $URL$ (That's my designated ip, you can use localhost:8000/radio). You can hear the 2 streams. With amerge (rather than amix) the 2 streams will be panned left-right and its easy to distinguish. For mixing 3 streams use the filter: 

The histeq comes close to what you want. But you can also look at the various flavors of LUT filters and also lut3d. The lut filters are more customizable in that you can actually look at a frame and determine what values to use. Moreover, if there is wide variance in the input material viz-a-viz luminance levels, the output might judder when applying a "per frame" correction. Applying a uniform strength of correction to all frames would produce a smoother result. 

You are essentially talking about separate encodes for separate chunks of video, which can be achieved as separate runs- as in the case of your command . To use the filter to slice a chunk from say 5 seconds to 10 seconds, use this (this does use and ): 

One way to join 2 films with a "chopping off" from one film, is to use the and filters in a chain. First set up to trim the film to be chopped and then make the trimmed version available to the concatenate filter to join. Essentially . With audio and video the 2 streams require their own chains. So for trimming and joining to in a 2 pass format, this would be the scheme: 

You only need the once at the beginning to overwrite existing file with same output name. The parameter must be in format or format. 

I'm trying to create an associative array in bash containing the name of a file (a video) and its duration. The code is : 

The and filters reset the beginning timestamp of the trimmed videos to zero, even though in this particular instance only the end is being trimmed off. Note 1: There will be re-encoding. I have not specified the encoding parameters. You will have to add your own as per your requirements. Note 2: The and filters' durations have to be specified in seconds. 

You haven't said what player you are using. In all probability it is your player that is unable to display the correct aspect ratio. is not required. An flag is required. If however that flag is missing, (I'm guessing it is not missing) in it can be set using . In Handbrake use a at the end of the command: 

I want to pass the filename and store it in the key variable, and the duration in the value variable. The duration will actually be queried through ffprobe, but that is immaterial at this point because the values are not being passed into the variables correctly. The results are : 

In the case of mp3 files video streams do not exist. So you should drop the video stream references in the command. In case of your last command that works- yes that is a valid solution for mp3 files. 

The help is from itself and not under . In linux distributions the being in the path allows you to type . In windows you might want to to the installed folder and type , but I don't work on windows for ffmpeg so I can't say surely. As noted in the reference you provide, most distributions are 8 bit. Why don't you use a value of and see if it throws an error- then you most probably have the 8 bit distributuion. 

Note that you use 2 inputs one after another, and then use to say that there should be no re-encoding- only joining. Also, this is the concat demuxer. This is flexible, but does require the 2 input files to have same codecs. The content of the text file is as you wrote: 

Do not use the concat demuxer or concat protocol when the source files may have different codecs or other differences. Use the concat filter. That will re-encode all, but will produce stable results. 

The size I put in is as an example only- yours will be different. Note that the size is the same for the picture as well as the movie frame, and no scaling is actually being done here. The is for alignment of the picture to the top left of the movie frame- this is logical since they are both the same size. 

Use this option: within your parameters, but use it before the output file's name. The limit_size is normally expressed in bytes. This option sets a hard limit on the size of the output file. Do not set bitrates when using this option, because then the encoding takes the supplied bitrate and simply truncates if it overshoots the limit size. Also, quality could be affected badly whenever you use hard file size limits. 

That would apply to multiple drives and paths. Here is a real example of a concatenation file that works: 

Note that I'm not familiar with the Windows globbing in batch files- I'm assuming %%~i is correct form. 

Mixing the two channels is trivial. Here is a test that mixes the two channels and records a file for 500 seconds, using the filter: 

You need the filter in a chain. See this documentation. To delay the output of one pad, if it is use that as input to the adelay filter at the end of the chain: 

Possibly you should convert the 32 bit images to lower bits if you wish to use to save a mov file. Else you are better off using to render to mov. Nuke is perfectly capable of rendering to mov format, and applying the gamma correction in a grade node will convert from linear space to sRGB. The bits will be also "scaled" down automatically when you save to a 10 bit or 8 bit .