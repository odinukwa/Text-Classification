Maybe this paper can be of interest for you : Tiling a polygon with rectangles by Kenyon & Kenyon in FOCS 92. 

There is a very large gap between understanding deeply a result (history, motivation, what it implies, etc.) and just applying it (implementation is one of the way of applying a research results) ! This is why it can be hard to understand a research paper, and why an intuitive explanation can give enough to implement... My only tip is the following. When I was a master student, I start reading research papers, it took me weeks to understand "easy" (in fact old papers, so papers with well known results) papers in details. I spent basically my first year of PhD reading hundreds of papers. And reading papers is still by far the task where I spend most of my time. Now I can more easily understand what a paper is about, and if the paper is about incremental results in a familiar area I have quickly a good comprehension, but It is still a hard task to understand new results. So, my tip is then: read a LOT of papers, and spend a LOT of time on a paper if necessary. 

First of all, sorry for this long and maybe not very informative title... Context: Let $G=(V,E)$ be a directed graph, let $v_0 \in V$ be the initial node of paths that I will consider in the graph. Let $\Sigma$ an alphabet and let $T$ (for target) a set of words built from $\Sigma$ (e.g. $T \subset \Sigma^*$). Nodes of $G$ are tagged with letters from $\Sigma$. Each node is tagged with only one letter, and most nodes are tagged with the empty letter. The general question is: how to find (algorithm, complexity) the set of paths starting at $v_0$ that contains all words from $T$ and whose cumulated length is minimum. This problem is not that hard to solve (I am not speaking from the complexity point of view): this can be solved using classical optimization methods. For instance we take all paths of a sufficient length that cover elements of $T$, then we choose a subset of these paths that cover all elements of $T$ and whose length is minimum. Before that we off course checked (reachability analysis) if all elements of $T$ can be found in the graph. My problem: Imagine now that we don't have access to the graph explicitly (not enough memory for instance). Instead we are given a function $f : V \rightarrow subgraph_k(G) $ that gives us, for a node $v$, the subgraph of diameter $2k$ centered around $v$ ($k \in \mathbb{N}$ fixed a priori). Moreover we have only a very limited memory (let's say we cannot store more than a few paths of reasonable length). With this very restricted access to the graph, how to solve the problem? All solutions that I can imagine are based on sampling and are clearly not effective. Does any of you have an idea? 

I am not really sure that I understand the question well. If you are asking about a pre-doc student position I guess you can do some kind of research internship before the PhD (maybe as a supplementary year of your master program, or with a hiatus in your curriculum). But if you ask for a research position without PhD, yes it exists in many countries. I guess this is call research engineer or research officer. You can start a PhD afterwards, or even in the context of this position (it can be an easiest way of finding funding). Most researchers have a PhD because this is the best way to become a researcher, and probably the only (almost surely) way to enter academia. But the ability to conduct (good) research is not correlated to the fact of having a PhD (this is my opinion ;)). 

If you crawl Wikipedia from this page $URL$ you will learn a lot on similarity measures. Afterwards, you should consider reading the book of Manning, Raghavan and Schutze: Introduction to information retrieval. 

Knowledge of basic mathematical logic is, in my opinion, a plus. You can have a look at the two books by Cori and Lascar. Mathematical Logic: A Course with Exercises Part I Mathematical Logic: A Course with Exercises Part II 

Never underestimate hiring committee members. It is likely that any manipulation you can think of, they will think of as well. They are more experienced than you and they already saw various types (all types ?) of applications. Sure, if you have papers in various fields, some persons can say that you have a lack of focus, while others will take that as an opportunity to broaden the scope of their team. At the end of the day, it is more a matter of how balanced your application is, on how known your results/you are, etc. Don't confuse good (resp. poor) papers and good (resp. weak) conferences/journals. IMHO, there is no such thing as a weak conference or journal (BTW it is not really true, there are terrible conf. and journals, but that can be easily detected since they are basically frauds maintained by crooks). There is only good, weak, false etc. papers. Of course some conferences/journals have mainly "good" papers. But what is a good paper ? If you are young, it is likely that nobody can really assess the global merit of your papers, unless you solve some long-standing problem. But it is surely possible to assess your technical merit, your work integrity (in applied fields), your seriousness, your ability to communicate,etc. About google scholar. Not all the hiring committee will use it, but you will always have someone that will do. Most probably your strongest ally and your opponents : the first to find some evidence that you are good, the others for the contrary. So, finally my advice is to always show ALL your publications, but in a list with clear category : journal/conf/other stuff, and field by field if necessary. And keep in mind that in a hiring committee, there is always someone in charge of verifying that list. 

Let $S=${$0,...,n-1$} and $\circ : S \times S \rightarrow S$. I want to compute the communication complexity of deciding whether $\circ$ is associative. The model is the following. $\circ$ is given as a matrix $M$. Alice (resp. Bob) is given half the entries of the matrix at random (same for Bob). I want to compute the worst case number of entries that Alice must send to Bob so that Bob can decide on the associativity of $\circ$. In fact, it is simple to reduce the problem of deciding the equality of two bit strings of size $\Omega(n)$ to the problem of deciding the associativity of $\circ$ over $S$. This means that the communication complexity of the associativity is lower bounded by $\Omega(n)$. However, I suspect that this LB is not tight. Being defined on an input of size $n^{2}$, I would have prefer to find a communication complexity of $\Omega(n^{2})$. Is there a known result on this problem ? Is the answer is $n^{2}$ for an obvious reason I am not seeing ? 

It is definitely a judgement call. In software engineering, people have designed very strict methodology to find/write/confirm the specifications. This is done by real humans and using a non formal (in the sense non mathematical process), so it is still subject to inconsistency, but at the end of the day, it corresponds to what people want to verify, no more no less. 

I am not sure of what you hide under the word "verification" so I give a try. Maybe you can add something about quantitative verification of Markov Decision Processes and the use of probabilistic temporal logic (pLTL and PCTL). In this framework you have a pretty good way of modelling adversaries, of expressing properties and there exists easy to use verification tools (PRISM for instance). 

Social choice seems to be a nice area at the crossroads of many fields: control theory, complexity, etc. Moreover, it is always a surprise (I mean to me) to see that the problems of the guys from the dept of economics are almost the same as the ones we are trying to solve... Believe me, it is worth having a coffee with them (and let them pay, they won't mind ;)). 

In my opinion, to be sexy to high school students you need to be some kind of magician. That's why I think that randomized algorithms are very good as a student attractor. For instance property testing is really something intriguing, and also something that can be explained (not the technicalities, but the idea) to anyone. PCP is also magic, but I guess that this is out of reach... 

My very own personal opinion (wow, what a disclaimer) : finding a thesis advisor, being in a PhD program, all this kind of stuff is not science, this is administrative stuff. So, my advice is to read (a lot of) papers, preprints, blogs and to listen to (a lot of) talks. Then to find some that present work interesting to you, Then to discuss (real life, mail, phone, skype) with the authors of those works and then finally try, if possible, to extend what has been done/find new solutions to the problems you saw. Afterwards, you may need to (or want to) work with other people, for guidance or for collaboration (or because this is more fun than working alone). At that point you can start looking for the advisor and the rest. 

The gossiping problem in distributed systems is the following. We have a graph $G$ with $n$ vertices. Each vertex $v$ has a message $m_v$ that must be send to all nodes. Now, my question is in the context of the ad-hoc network model (we assume that a node does not have any prior knowledge about the topology of the network, its in and out degrees, and the set of its neighbours. In fact the only knowledge of each node is its own identifier and the total number of nodes). I also assume that all nodes have access to a global clock and work synchronously in discrete time steps called rounds. The complexity of an algorithm in this context is the number of rounds needed for completion. I remember that there exists an algorithm that solves the gossiping problem in $O(n \log ^2 n)$ rounds with high probability. But I cannot find the reference anymore, and I am wondering if there are more recent results on that matter. edit following the judicious comment: at each round a node can transmit the message to all its neighbours and can receive the messages from them. A node will receive a message at a given round if and only if exactly one of its neighbours transmits at that round. Otherwise a collision occurs and none of the messages is received by the node. 

First of all, I am still not sure whether cstheory is well adapted for this question, so I won't be offended if the crowd thinks it is not the case... In search engine marketing, several problems are interesting. Design of fair (and profitable) auction mechanisms and computation of optimal bidding strategies under bounded monetary resources are two examples of interesting (and well documented) problems. Another problem of interest is the one of keyword selection: how to select the most profitable keyword (without any link to the amount of money available or to the "topic" of the keyword). "Profitable" can be either giving the best revenue, or the best profit. These problem deals with uncertainty: the clickthrough rate of a keyword is not known, the conversion rate is not known either. Are you aware of some theoretical work related to this problem? 

Also, keep in mind the 3 third rule : 1/3 of the talk is for everybody, 1/3 is for people from your (broad) area, and the last third is for the other experts (you can put the technicalities here). edit following Dave's comment: Presenting ideas that are not clear is indeed difficult, even for faculty I think. In the past we used to proceed in 2 steps with our students. As a second step, we had formal talks according structured with the WWH and 1/3 rule, it was called the doctoral seminar. In this seminar, students presented almost finished work in progress. For the first step, we had informal lunch talk by students on the early stages of their ideas. Typically, it consists in a bunch of people eating pizzas, sitting in front of a whiteboard + projector screen, listening to a student that wanted to formalized his/her ideas. I asked my PhD for this talk to prepare slides where the "formal things" where written next to a natural language sentence explaining thoroughly the "formal thing". The whiteboard was used for rewriting things all together (students + faculty). Each speaker had a limited time, so the idea is that on your first talk you try to make a few things more precise, then you come back for a next round the week after, etc. We stopped that because sometimes, with some people, this approach can lead to troubles (think about co-authorship issues for instance).