Make sure you still take the paths that hit nothing into account when averaging the color of the pixel, otherwise this will also introduce a bias. As joojaa said in the comment, you should try using a simpler scene: a few spheres or cubes, no texture. You can also reduce the number of bounces to one or two to see only direct lighting. If necessary, maybe even use a constant sky dome first instead of a light area. 

Back to the problem So we want to know the color of the points of an object, we know that to compute it we need to integrate light in all directions, and we have a cube map that represents light coming from all directions. So far so good. A problem though is integrating basically means we need to go over every single texel of the hemisphere (so half the environment map: $(64 \times 64 \times 6) / 2$ texels), do some math, and add that to the result. That's a lot of computation for each point, which we'd like to avoid if possible. We know that the contribution of one light depends on the light direction. If we consider the environment to be static (the lighting doesn't change), then we can isolate the part of the computation that only depends on the light and surface normal (and not on the material or the observer), pre-compute it and store it to use later. For the diffuse term, that's the diffuse irradiance mentioned earlier, and it typically looks like the center figure of the illustration. Each pixel represents the irradiance term for a given surface normal. The right figure is the specular environment map, computed in a similar way but with a different integral. 

The problem of reducing noise is still under active research though, and there is no silver bullet. At this point you will need to read what are the latest developments. 

Addendum To use depth or normals, you would need to save them in a texture if that's not done already. When you create the frame buffer for your regular rendering pass, you can attach various textures to it (see ) and write other information to them than just the color of the scene. If you attach a depth texture (with ), it will be used automatically for depth. If you attach one or several color textures (with ), you can write to them by declaring several outputs to your fragment shaders (this used to be done with ; either way, see ). For more information on the topic, look up "Multi Render Target" (MRT). 

At CEDEC and GDC in the early 2000s, Masaki Kawase has presented a series of fast post-processing based lens effects, including large bloom. This presentation from GDC 2003, Frame Buffer Postprocessing Effects in DOUBLE-S.T.E.A.L (Wreckless) (see slides 15 and upwards: "Bloom"), gives a first version of the bloom. It consists in doing a Gaussian blur by sampling at exponentially increasing distances. In this presentation from GDC 2004, Practical Implementation of High Dynamic Range Rendering (see slides 44 and upward: "Glare generation"), he updates the technique. Instead of varying the sampling distance, he uses downsized versions of the original image, using a carefully crafted equation to achieve a large yet spiky Gaussian blur. 

While browsing to properly write my question, I actually found the answer, which happens to be very simple. Another Fresnel term is also going to weight in as the photons make their way out of the material (so being refracted into the air) and become the diffuse term. Thus the correct factor for the diffuse term would be: $$(1 - F_{in}) * (1 - F_{out})$$ 

So in the case #2, the diffuse is left out, which is equivalent to a 0%, pure black, diffuse color. In case you haven't checked it already, this presentation is an excellent code explanation of SmallPT, dense but thorough: smallpt: Global Illumination in 99 lines of C++. 

So you simply need to make sure texture coordinates generation is disabled for the color texture, but enabled for the shadow texture. As you guessed, this is done with , by writing for example: 

I've read numerous papers and articles but haven't gotten very far because I can't seem to understand how the image on the right is generated from the volume data set. Also what would you call this image? From my understanding the two axis are data values (intensities) on the x and gradient magnitude on the y. But where is the gradient magnitude coming from? I have a basic understanding of gradient magnitude and don't see how it can be used to make that picture. How is it calculated? How can I generate a picture like the one on the right? 

When my camera is behind my model I see a grayscale specular highlight. However, in front of it is perfectly fine. Here's my shading code. (Software rendering) 

You can see from the "tilt" in the texture that it's not quite right. If I add more triangles to the mesh it becomes more correct. From my understanding these are not perspective correct textures. But this perspective divide thing is confusing me. I read that I needed to divide the u, v coordinates by the w component (perspective divide). I'm guessing that this is the w component from the vertex after multiplying with the projection matrix. But I couldn't get that too work. So I looked a little bit more into the problem. First, here's what is happening to one vertex of my triangles. 

I multiply it with the wVP and then do my perspective divide with x and y. The + 0.5f is to center it in homogeneous space and then multiply with the screen dimensions to get it in screen space. I then stuff the w component of the vertex as the z value so it will be sent to the triangle drawing function and can be used for things like the depth buffer. To my understanding the z value from the vertex gets put into w after multiplying with the projection matrix. This makes sense since m32 = 1 in my projection matrix. So after multiplication with the projection matrix w = z. I first tried dividing the texture coordinates per vertex by w. Like so: 

Revisiting this question from awhile ago. It turns out this sort of histogram is created by iterating through every pixel value in both the normal image and gradient magnitude image plotting it in the 2d image. (x intensity, y gradient magnitude). The count/occurrences of the 2d histogram are represented by the intensity of the grayscale in figure b. So the brighter a pixel in figure b is, the more pixels with that (intensity, gradient magnitude). For volume rendering this is called a 2d transfer function which lets users filter out or assign lower opacities to pixels with large/small gradient magnitudes. This is especially helpful in real time volume rendering with lighting. Real time volume rendering typically uses a BSDF at every point along a ray march as an approximation (Assuming that light reaches every point in the volume). IE: Dot product of the incoming light direction with the gradient as the normal. Using the gradient as the normal means that homogeneous areas are not well represented as they have low gradient magnitudes. Small noise in these areas can easily cause the gradient and normal to flip. 

So I'm trying to correctly map my textures in my software renderer using the u, v coordinates but I can't seem to get it working. I got affine texture mapping working. This is what I can produce using it: 

Then in the triangle drawing function I used the bary centric coordinates to interpolate across these the uv coordinates. This did not work. Since w = z. When the texture is far from the camera (ie z is a large value. With my world translations applied z = ~700) then the texture coordinates shrink a lot and in every render I just get the (0, 0) value of my texture. But I also tried this instead in my triangle drawing function: 

I have no knowledge of the literature on the topic, but I did something very similar to what you're asking some time ago: I wanted to generate lathe meshes and bend them according to a spline. I think the same technique could be adapted to your case quite easily. First you would need to define what your default axis is: if the input mesh corresponds to the case when the spline curve is a straight line, where is that line in 3D? It could be defined arbitrarily as the Y axis, the PCA of the mesh, or some manually defined axis. The algorithm is then: 

A diffuse shading with a sharp transition between lit a unlit, that happens somewhere below 0 so the light leaks behind a little. The rim lighting that, as you mention, has an almost constant screenspace width. From the look of it and some of the artifacts, my best guess is that it's actually some edge detection filter based on depth, which is then used in combination with the diffuse lighting so the rim lighting doesn't affect the unlit parts. 

This approach should give the expected result, but the problem with it is the first part, which is not trivial at all. Idea B: Approximate the mesh with fat particles, that are big enough to hide points in the background. 

To be able to give a good answer, we need to know what is the compiler error you are referring to. At first sight your shader looks ok, although: 

A note first From the look of your screen capture, I suspect there might still be a bug in your code. Noise is to be expected with only 16 spp, but your picture still looks surprisingly dark to me. For comparison, here is what my implementation of SmallPT looks like with 16 spp, 15 bounces, and no next event prediction: 

All these values are very low; even crystals with high refractive indices like diamond ($F_0=0.17$) and moissanite ($F_0=0.21$) hardly exceed 20%. Yet most metals have $F_0$ values above 50%. Moreover, I have read multiple times that the formula mentioned above doesn't apply for metals (which can be easily confirmed by trying to use it and see completely wrong results), but I haven't found any further explanation. What phenomenon explains this difference? How can I calculate $F_0$ for a metal (in particular if the medium it is in contact with has an IoR different than 1, like water)? 

This being said, you're adding to yourself a lot of trouble by sticking with fixed pipeline instead of using GLSL. 

I assume a similar rule will apply to the OpenGL equivalent, Uniform Buffer Objects, since they map to the same hardware feature. What about vanilla uniforms though? What are the rules that apply when declaring uniforms? 

The well known Schlick approximation of the Fresnel coefficient gives the equation: $F=F_0+(1 - F_0)(1 - cos(\theta))^5$ And $cos(\theta)$ is equal to the dot product of the surface normal vector and the view vector. It is still unclear to me though if we should use the actual surface normal $N$ or the half vector $H$. Which should be used in a physically based BRDF and why? Moreover, as far as I understand the Fresnel coefficient gives the probability of a given ray to either get reflected or refracted. So I have trouble seeing why we can still use that formula in a BRDF, which is supposed to approximate the integral over all the hemisphere. This observation would tend to make me think this is where $H$ would come, but it is not obvious to me that the Fresnel of a representative normal is equivalent to integrating the Fresnel of all the actual normals. 

Reduce shading when possible Lens distortion Part of the NVidia VRWorks SDK is a feature, Multi-Res Shading, that allow to reduce the amount of shading, to take into account the fact that some pixels contribute less to the final image due to lens distortion. In Alex Vlachos' GDC 2015 presentation, he also mentions the idea of rendering the image in two parts of different resolution, to achieve the same goal. Foveated rendering Finally, like your question mentions, foveated rendering aims at scaling the resolution to take into account the fact that eyes have more precision in the fovea and less in the periphery. There is some research on the topic, but this requires eye tracking like in the Fove HMD.