Monte Carlo simulations are very easy in R. The simplest approach is to write your own scripts that carry out the steps you need for your simulations. To construct these scripts you will need to understand what you are simulating, that is what is the distribution of outcomes, and what are you measuring about those outcomes. If you understand what you are simulating than writing these scripts is easy, but will require programming in R. If you are new to R and/or to programming then it will take some effort to get up to speed. In the end, you will probably understand your data, simulations, and results better. There are packages for Monte Carlo in R, but these are not packages that will do the simulations for you and would require as significant learning of R to be able to use (as much as writing your own scripts). I suggest reading more about conducting Monte Carlo simulations in R. There are many resources online and offline, such as this book or this tutorial. 

Similar discrepancies can be found in terms such as or for instance. Additionally, you need to consider what is a token for your task at hand. In my previous example make sense either with or without . However without the hash is just a poorly written word. Maybe you want to keep the hashtags in your corpus or maybe not, but this depends on what you want to do with it. So first you need to know how your tokenizer handles these cases and decide if you want to remove them beforehand or later to keep hashtags in your corpus or only (sometimes weird) words. In the case of the is exactly the same, you can keep it, remove it or maybe delete the whole instance as you don't want to keep user names in your corpus. As I said, it all depends on your task. PS: In case you want to play around with different tokenizers, try this. 

Apart the mentioned resources, this also might be of help: MLP Java example. It's from the University of Sydney and includes theory and a Java implementation. 

For more details sample csv file here: $URL$ I am trying to create a model to predict the column “actionspost” (the extreme right column of the above file). The column currently holds comma separated values like “2, 4, 6”. Each numerical value is a coded action that a patient would take after knowing the results of the genetic test as per the survey filled by the patient. For example, 2 indicates: Change eating habits, 4 indicates: Getting members of his/her family tested and so on. Based on the features in the csv file above, I want to create a model that would help me predict the “actionspost” values. One way I thought, I could do this was break down comma separated values in “actionspost” to create duplicate rows for same customer Ids holding only one “actionspost” value at a time (For example: There would be 4 rows for customer ID: C00003 with first row having “actionspost” value of 2, second row with “actionspost” value 8, third with 9 and fourth with 10. Then create subset of the data set for each unique value of “actionspost”. This time, the dependent variable will not be actionspost but something like “isactionpost2?” (Yes =1 and No = 0). And train models for each of this subset for different values within the “actionspost” column. I personally don’t like the above method as it too cumbersome and in my opinion, not the optimal one too. I was wondering if there could be a better way to address such scenario? My end goal is to try to train different models like Decision Trees, Naïve Bayes, and Neural Nets and check which one leads to better predictability. 

Basically in new column, you check if and assign -1, if not you check if and assign 1; otherwise you assign 0. 

The answer depends on what you want to do with the hashtags/words and also on what tokenizer you are using. Consider this example tweet: 

As you mentioned, the paper doesn't clarify. However, my guess is that this is not due to concatenating 2 previous layers (I don't really see an specific reason to do this here) but because of concatenating the ResNet shorcut. Generally, all conv layers have a number of filters, thus determining the output size (num_filters, size) regardless the inputs. On the other hand, MaxPooling does keep the input num_filters (though in this case reducing the size). In the paper, note that the num_filters is doubled at the output of all convlayer except for the one that does not keep the ResNet shorcut (last 512 conv layer). So my guess is that they are concatenating the output of the conv layer and the shorcut which would explain the output size. Hope this helps! 

There are a few possibilities. First, there is some variability in performance. It could have been only by chance that countvectorizer performed better than tf-idf. Did you use cross validation (with how many folds)? Is the superior performance of the countvectorizer reliable? I would compare performance across folds to make sure countvectorizer consistently performs better. Second, if you find that countvectorizer reliably outperforms tf-idf on your dataset, then I would dig deeper into the words that are driving this effect. It may be that common words (words which will appear in multiple documents) are helpful in distinguishing between classes. There is substantial research that shows that use of some function words (e.g. first person singular pronouns, “I”) change depending on someone’s psychological state. Function words like pronouns are very common and would be down weighted in tf-idf, but given equal weight to rare words in countvectorizer. I’m not suggesting that first person singular pronouns in particular are driving your results, but it’s worth looking at what words are driving the effect. I would examine which words are important in both types of models, countvectorizer and tf-idf, and then think about whether the words that are most important for the countvectorizer make sense in the context of your text documents and labels. Also, are you removing stop words? You could also see how the models perform with and without stop words, which would be another way to test whether frequent words are actually helping you to distinguish between classes. 

I am trying to do some analysis about user behaviour when typing (keystroke biometrics). Ideally, it will include traits extracted when people are writing code. Although not technically Natural Language, code also has some structured characteristics as language and I wanted to leverage that. I was wondering if there has been some research about performing language analysis focusing on programming languages instead of traditional spoken languages. Mainly, I am interested in having a comprehensive list of stopwords for as many languages as possible. For example, stopwords will include: for, while, return, break, string, if, else, and so on. Although it would be nice to have them separated by languages, I wouldn't mind a list comprising several languages. I know this could be done for example by getting some sample code and retrieving the most frequent terms, but I also wanted to know if there has been some research towards this direction. Any ideas, papers, methods would be welcome. Thanks! 

Have a look at this question. There is a nice discussion about how to implement this. The idea in to create a separated Input to your model and concatenate it AFTER the recurrent layer(s). Also in the Keras documentation, there is an example on how to build such models with a few lines of code.