Production, yes or no: is there an real-world example of a large-scale Cassandra in a Swarm/Kubernetes environment? I thought, initially, yes? Guidelines like "Cassandra and Docker - Lessons learnt" (Slide 20) Docker Official Image packaging for Cassandra on GitHub 

If I startup MySQL server during (derived from official MySQL Dokcer Image) in two different statements, would be database files be pertained? I would say yes, but it seems to be wrong. Why? Logfile: 

To use a dockerized SLES, you need to use an official base image and run it on a SLES host. But this does not seem to work out of the box - what needs to be done to build a custom image? Dockerfile: 

In Jenkins, you could configure the system to retain a certain number of either successful or failed builds. With Bamboo, it seems like you can just retain a certain number of builds which rotates the last successful build after 10 failed builds out of the system. Is there a way in Bamboo to retain the last successful build for an unlimited time? UPD wait, but is that true with the 10 builds limitation, really? I have just realized that I could click the UI through the whole build history, both successful and unsuccessful builds. $URL$ 

Normally, the example you start with Docker introduce environment to right more or less one app - frontend, backend, something else.. Would you put all containers of a larger app (20-30 containers and above without replicas) still to the same stack or introduce a set of stacks representing system layers? 

Reading the K8S images article, I see there support for images from Azure, Google Cloud Registry and Amazon EC. 

Imagine two product teams. Team 1 has delivered a service stack on Rancher/Docker Swarm. Team 2 has delivered a service stack on Apache Mesos, using custom scheduling frameworks. Now which pitfalls would expect Team 3 aiming to deploy these architectures together? To be more specific, could they migrate them to Rancher 2.0 with Kubernetes and still somehow use Mesos logic as well? Or drop Rancher and say use Marathon to integrate 1 into 2? (or are these architectures just incompatible and need independent infrastructure systems) 

If you run everything in the same container... PRO: you have solved the CasC challenge for configuring the testing environment inline as well CON: you miss the blackbox tesing part, imagine your container wouldn't accepts connections from outside. Ouch! Possible solution: I'd go therefore for a multi-stage Docker-based declarative pipeline: one Docker environment for each stage where artefacts cascade along it. 

Is the warp speed delivery speed real now? In 2014, management was very sceptical about even 30x speedup. Now I'm sceptical. Question: this must be absolute minimum values for very lightweight microservices. That is, as the bugfix commit arrives.. 

While the dev team works on a REST service to manage binary data in Cassandra, ops would like to store local blobs directly from shell. Is it actually possible via CQL/bash without a Cassandra driver? I could find so far only an example for Ruby. 

Using CasC with F5 would allow for versioned and optionally dynamic network endpoint configuration, saving time and reducing risks. Does this tool support this? Are Single Configuration Files (SCF, F5 term) the vehicle to do that? 

Imagine you want to reset your Docker swarm from time to time. How do you distribute join tokens to those zillions of nodes? How much automation is possible here? 

Currently I find myself managing a set of embarassingly similar YAMLs for environment profiles. I see there a scaling trouble if there will be lots of dynamic environments (i.e. fluid acceptance test configurations, with disposable environments you can do that!) Has anybody done a working implementation to build YAMLs dynamically on scale based on some generic represenation ( kind of CasC level 2 beyond some naive scripting), or if this makes no sense, why and which way is to prefer then. 

Now my question - how do you name the lead time from a code commit hitting the repository to a first running deployment where you could do a first acceptance test to the feature? To a production system? 

Yes because you can use advanced automation to create value and establish reusable foundations of your future bigger company. Depends of course how often you are going to update your digital products. But you will value the possibility to to as much manual work as necessary on repetitive excercises. 

Finally, it is possible in Bamboo to let a build run in a Docker container without dealing with agent configuration. The system just asks for a image. Now, what about the underlying infrastructure, what happens there? Will then Bamboo server host just spawn containers locally (which limits horizontal scalability)? 

Due to my lacking vocabulary in this context I am not sure how to designate the two types of organizations I would like to talk about before I can put my question. Anybody capable of better wording feel free to improve this. Probably there are mixed types but I suppose I have to polarize a little bit to make the point. OrgType1: "digitalized product organizatons" either B2B or B2C, they have their products, as they become more and more digital, they have some own IT product teams and IT operations. They have found out that they need (more than before) product diversification quite recently. OrgType2: "digital consulting service organizations" (not necesarily just IT consulting but they are typical example here) these guys are B2B only, if they have sustainable products and related operations then it is rather a side effect. Rather, their software product lifecycle is limited to just one or several projects, and that diversification has always been their core business. Actually, the real product is here much more every single member of this organization who depending on the role might, but by far not always does, develop software. OK then, so OrgType1 is clearly what we already know that DevOps is of a great help, because there are established delivery processes, they get disrupted, need to be optimized, and so on and so forth. Well, bigger organizations consist of many parts, regional business units, business lines, departments, and every one has its combination between operations and their-own-breed-of Devops. Okay - anyway there can be some evolution. OrgType2 can care about DevOps only so much as it is important for their customer (or they even sell DevOps consulting ;) but then if it comes to onsite software development, you somehow start from zero in almost every project, and this is magnified as with OrgType1 through all departments, business units and so on but it is worth because every "DevOps entity" starts just with 1-2 people who need to discover what they are up to. Seems like OrgType2 is "siloed by design" in a much more dense way which makes it also somehow scalable and persistent. This all considered and taken into account, the question from my current perspective is: how do DevOps practices apply for the OrgType2, maybe there are resources where this has been explored? 

If you compare the "mainstream" DevOps if there is such, is a DevOps practice in a spacecraft related context much different, or not? If this is too broad, take avionics software context as example. 

Now, you could have a microservices with more than just one codebase and need therefore more than just repository. What is the best practice to reflect this if you have many different projects? My idea now is to encode microservice prefixes to repository names, but is this really the way to go? Like: 

For example, to play with Docker Swarm in a way which makes sense you need 2 or better 3 hosts, right? What is that number for K8S? 

Many available Docker installations use the package either . They both seem to link to recently obsolete Oracle URLs: 

Karma is a nodejs tool to automate browser testing. It seems that it depends on nodejs plugin fsevents. But, fsevents is for processing Mac OS X events, therefore I get the error "not supported on your system" for the phantom karma launcher. Questions I have found on GitHub seem to come from other Mac OS users dealing with upgrading npm. My question is: how to install karma launcher in headless ubuntu/docker context? 

Given the simplest example, the test swarm consists just of one node so all containers run in one place. The stack is setup to run in a custom Docker network; the containers are not directly mapped to the host (if I do not want this explicitely) but the mesh network should take care. Now I have observed the following: 

Normally, one important topic in DevOps is how we take care of automated creation and delivery of software artefacts. With the rise of data science there is a new type of artefact - monolitic binary blobs representing a trained neural net for example or other machine learning models. Such a blob can have a size many GB and its creation is not yet standardized AFAIK which brings organizations back to the pre-CI age. Nevertheless, they have their version and associated collections of training data (corpora) which tend to grow rapidly as well. What are best practices to address this new challenge using DevOps methods - if even possible? 

Imagine in your stack you have RESTful services which also provide some rudimentary frontends, mostly for admin/other tech user use. Do you include the UI inside the container or are these two containers? Why? 

Aside other questions I believe the following answer is yet missing - I want to summarize what I have learned so far in a mostly humble tone: Introducing DevOps skills to your team is probably not enough (that is to consider that you cannot hire a DevOps)! (Like asking on a lower level what are best tools to make things work, here you seem to want your organization work). The real question is: what is your oganizations digital transformation strategy and how much good DevOps will help you to create more customer value and reduce your costs, still having a good climate? DevOps reveals the need for change, and change requires taking responsibility. Imagine a situation were working DevOps might introduce pressure on other teams. DevOps is sometimes a result of evolution from Agile and Continous Integration - what are your assets here? 

Assuming there was e.g. not enough memory, how to determine that? Maybe a symptom, the workers get here and now "dissociated" but then reconnect. 

Would you say from experience, either task or relationship leadership style works better for DevOps? Or would you go for something else in this context than Forsyth's theory? 

By current state of the art, we cannot enforce an ordered start sequence of service containers directly, neither in Docker Swarm nor in Kubernetes. (Maybe there is a standard solution for that via a specialized orchestraton service) Now, imagine we want to have a Solr instance with considerable amount of data, and want to get it ready before other services can start their work. What is the best way to verify this status? It seems for example that calling and validating the expected list of loaded collections is not enough: 

After a chat with a colleague who collaborates with quite high-end projects on Microsoft stack (for example, ASP.Net + MS SQL Server). An interesting outcome was that although I believe that DevOps must be possible in the Microsoft world as well, my knowledge is too little so that I cannot give good examples how he could organize a CI/CD approach. Indeed in our community there are some hints about Team Foundation Server and PSBuildStack (?), but an official source does not reveal all that at a glance. The described hosted services make it even less transparent for a newcomer - is that a cloud build environment like CloudBees/BlueMix? So, my question is now: how could I describe an example of mature toolchain using established DevOps terms which are valid more or less independent from specific environment/tooling - or do we have to admit that this engineering terminology is not there yet? Something like this: 

Articles like "Thou shall not run a database in container" or "Why database are not for containers" (related yc debate) A colleague told me, Cassandra would store data associated with IP addresses and this makes usage of Docker/Cassandra not possible if you need data persistence - can't yet google up this but if there is no workaround this will make persistence impossible in my opinion. 

From certain size of the codebase, would you still have Git or are there more specialized solutions? (Also to checkout just a part of the codebase) 

Google for "left shift" in DevOps context. Consider DevOps team patterns $URL$ "While DevOps raise problems and dispatch them to Dev to solve, the SRE approach is to find problems and solve some of them themselves." $URL$ Depends on your requirements to their profile IMO but in general I think that the question is like "do smb has to learn advanced things which might help a lot..?" no, but.. ;) Try out Docker as integration testing tool combined with your CI/CD solution. 

This question is not about specific orchestration of dependent services but the problem that if you run 10 services each with one JVM this creates a load peak. The system slows down and you need complex loops to wait for other services. Are there any mechanisms known in other orchestration concepts to manage the startup somehow? For example, in completely other domain, in JMeter you can tell a timeframe in which a thread group will start. So could I say "try to start this service with some offset"? 

Here are for example some arguments from a blog post published back 2014 and titled in way quite matching your answer: 

Given you can run K8S containers from your Docker images - are there any backdrawals? and if yes, what is the "Dockerfile" in the K8S world? Side note. My overall impression is that Docker is very nice to adapt in development teams because of its easiness, but in terms of high-end scalability K8S is more powerful. So is this again a new gap between Dev and Ops?.. 

Puppet Labs' annual DevOps reports are a very nice and representative source of information, AFAIK. Indeed, with Kim Gene on board they are THE industrial gurus I'd say. Now I have a question to the following data in this year's report (page 21). Can we multiply the data for 2016 and 2017? For example, take the lead time for changes. 

But what is then the difference to the original (su-exec), which works fine? And how to get it right with ? ==== UPD: su-exec is here a third-parthy Alpine Linux package $URL$ 

For some unknown reason, the manager host which gets the command to launch a stack, tries to pile up all the containers on itself; other worker/manager hosts stay unemploed. Unfortunately there are no log entries for that. Has anybody observed this behaviour already with Docker CE as well? 

If the new paradigm is "You build it, you run it" (Werner Voegels, Amazon CTO) which obviously puts much more responsibility - and pressure - on software engineers, what does this change introduce to the task of the test team? 

I find a lot information about the blue green deployment pattern but maybe miss a point. Please help me to understand. Could it be scenarios where you can't successfully duplicate the information flow? Examples: 

CI/CD environment is the production carrier for DevOps teams. An error here is a show stopper for possibly many teams. Additionally, it is a product offered to teams which needs maintenance (quartely new features sometimes and security patches anyway) and room for experiment (new metrics plugins for example). How to allow this? The only argument against a sandbox I can imagine is its cost. But then, isn't a transition to IaC, aka Pipelines as Code providing a "serverless" CI/CD like in Gitlab or Travis, not the next step? Otherwise, CI/CD is the new "pet" system?