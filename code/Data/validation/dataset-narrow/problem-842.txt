There's nothing -inherently- wrong with a large broadcast domain if it's appropriately (and safely) configured. Employing PVLAN's, for example, can allow for very large networks without too much drama as isolated hosts don't see traffic from one another. Similarly if the network is relatively static, the links very stable and controls are in place to block broadcast/multicast/unicast flooding then it can be made to work. That said, more often than not the sort of networks you're describing (2000+ hosts) are basically a crisis waiting to happen. Some of the issues/warning signs might include- Excessive broadcast traffic - Either app traffic being blasted everywhere (i.e. like old school Windows), excessive ARP traffic, etc. Think of this in terms of packets per second moreso than absolute bandwidth - hundreds of packets per second of background traffic is getting up there. Bear in mind that certain network events (switches coming up or down) may exacerbate this terribly. Network diameter / topological stability - Do transitory spanning tree loops occur under certain conditions (i.e. device reboot)? What volume of TCN's and so forth are you seeing? Is the root bridge moving around at all? Physically how many switches are cascaded together? How do link failures work? If a link drops, what happens? I've seen situations where things were badly broken to the point where the network topology would literally never stabilize when a redundant link came down. It required mass reboots - well, more properly, it required a complete redesign, but that's a separate issue. Interface drops on routers and switches? Buffer issues? These can also be hints. In general bridges that cross physical sites cause a disproportionate amount of trouble. Is there a compelling reason why your sites (or floors) couldn't be broken up into routed subnets? Best practice is certainly to route were possible and bridge where not... 

The Cisco side isn't configured for MST. Take a look here for specifics, but you need to set the 3750 to use the appropriate mode and assign the various ports in the switch to an instance corresponding to the HP's (likely instance 0, but I don't know their implementation). That should - in broad strokes - get the switches at least speaking the same protocol. Beyond this, I would highly recommend that you explicitly configure one of the three as the root bridge. This is configured on a per-instance basis - lower switch priority wins. A quick Google search turned up some suggested example configs for interoperability. The document in this link has some appropriate warnings about older IOS revisions, but if you're running something reasonably current it's safe to disregard. 

Is there some reason why you have spanning tree enabled? Unless you're providing some kind of redundant connectivity between external segments it's not necessary and could potentially either block traffic or cause an upstream switch to temporarily shut the port down. 

There are two connections made for an FTP session - control (port 21) and data (port 20). The normal behavior for FTP is for the client to connect to the server (again, port 21) and then the server opens the data connection back to the client. This breaks in a number of ways in environments using NAT, firewalls, etc. This traditional mode is known as active mode. Bear in mind that many commands that would seem like they'd be control traffic (i.e. listing a directory) actually require a working data connection. Passive mode (PASV) is when the client specifies which port to use. This, in turn, allows NAT (in various forms) to open up a session and allow for data to pass. You need a client (and server) capable of supporting passive FTP. It's all well explained here - $URL$ 

You can also pursue VGT mode - set the VLAN to 4095 on the port group and then set the VLAN tags on the vhost itself. You'd still need to configure the switchport to accept tagged packets and to assign untagged packets to the appropriate VLAN. 

Higher quality recording doesn't necessarily map to better sound quality in telephony - particularly given the nature and history of voice codecs. You need to make sure you've either recorded- or down-sampled- your file to an 8khz sampling rate and 16-bit samples (i.e. PCM). 

The rule of thumb is that the speed of light in SMF is ~.6c (same neighborhood as Chris S' description). The port-to-port latency introduced by routers and switches is going to average out in the low double-digit range once any kind of buffering or fancy features are thrown into the mix. There's also a nominal amount of serialization delay that's introduced every time a packet is mapped onto the media itself. This value decreases as a function of link bandwidth - so there will be measurable differences between, say, an OC3 link and an OC768. This also tends to run in microseconds. The rule of thumb I've generally used is about .25ms per 100km of actual link (one-way) with minimal hardware involved (i.e. DWDM equipment). This estimate has served very well in the construction of private metro networks and even a few long-haul dedicated runs. That said, the relationship between this number and reality in the circumstance of carrier MPLS clouds, the public Internet, etc is pretty much a toss-up. I'd suggest something like this for establishing the minimum practical latency. That said... First - the actual physical path taken by a given circuit is sometimes only tangentially related to actual geographic distance. Backhauling a circuit by hundreds of kilometers to the west and south only to send it hundreds of kiometers to the north is not unheard of. Second - Nor, indeed, is the process of sending IP traffic through one or more exchange points that may, or may not, really be on the way. Economics of both circuit construction and carrier hand-off locations drive practical engineering more so than absolute minimization of latency. Finally - when running a traceroute take the naming of the intermediate nodes with a grain of salt unless you personally know the network you're crossing. Naming conventions with airport codes (for example) can give a hint, but there's no way to conclusively know what you're looking at without some measure of additional knowledge. 

You're on the wrong track altogether, and it doesn't have anything to do with Shorewall (or really any other firewall package) but rather IP networking in general. If you imagine that a given IP host has three interfaces in the same subnet and a packet arrives for that subnet from the fourth, how does that host know which of the three interfaces should receive the packet? Does it ARP in all three? Does it just send all the traffic to the first one that came up, thus preventing traffic bound for the other two? This is why if you tried to configure this from a shell you'd be given an error message about overlapping subnets. So - you have three reasonable options, with a couple of sub-options. 

Twinax is rated at 10^-17 at <= 10M Fiber is generally 10^-18 or better (lot of variables here, though - especially on long haul) Ethernet's minimum specification is 10^-12. This is where the contention comes in as far as FC goes - it may be that a TP infrastructure can support better than 10^-12 (and hopefully most do) but at 10G speeds even that much error translates to a bad packet every few minutes. 

So - when an arbitrary client sends a packet toward the anycast IP, said packet will tend to find itself to the "closest" point of advertisement. I've put scare-quotes around closest because it's only close in the sense of how the routing topology has been laid out and what policies are in place for the routers along the way. It's entirely possible that the closest instance of the anycast address might actually be the furthest physically. If, in turn, the point at which this route is advertised fails (...which could be result of the service failing on the host and the route retracting or a more traditional network reachability issue) then packets bound to the anycast address will be routed to the next-closest (again - in routing protocol terms) instance of the route. During network reconvergence the client's resolution might fail and be re-attempted, with the re-attempt now following a longer path to reach what is - apparently - the same address. This is all transparent to both the client process and the user and is best thought of in network terms as following an alternate path to a given network. It's sometimes helpful to think of an anycast network as a logical construct. It's a virtual subnet that contains the service you're interested in. That virtual subnet is reachable via many paths through the network. That said, here are the major caveats to anycast designs: 

1.) 512 isn't really a high window scale - it's just saying to shift the offered window size to the left by 9 bits. Setting the window size to 130 (otherwise a very, very low value) and then applying a scale factor of 512 gets you to 66560 (130<<9). 2.) 100M is likely too small a file. The fact that the scale has been negotiated at all suggests that things are working OK. Try a larger file to better observe the behavior. If nothing else you'll get a better sense of the real throughput. 3.) Also bear in mind that the behavior of particular clients can actually specifically override the behavior of the OS - the built in FTP client in Solaris, for example, used to limit window size to 64K regardless of how the OS was configured. 

Flow control in the generic sense refers to mechanisms that allow for the increase or decrease of traffic across a given data link by a transmitter based on some kind of signal (implicit or explicit) captured from the receiver. Back in the days when serial communication was more common we used hardware flow control (RTS / CTS) to allow the endpoints on the serial link to signal when they were- or were not- capable of receiving data. A DCE (a modem, for example) might have buffers that could be overrun by the sending station. When this device passed a certain threshold of buffering, it would lower the appropriate signal line and the sending station would respond by pausing its data transfer until the DCE indicated that the immediate congestion issue had cleared. A similar mechanism was also implemented in-band (i.e. as part of the data being transmitted) known as XON/XOFF - same ideas as RTS/CTS but implemented as special control characters rather than dedicated hardware lines. More recently (~15 years ago, or so) similar mechanisms were introduced in Ethernet in the IEEE 802.3x standard. This introduced a so-called "pause" frame. As in the serial case, a given receiver can emit such a frame when it is unable to accept more traffic. This is a MAC-layer mechanism (i.e. layer 2) which is has been implemented in a fair number of devices but whose actual usage and deployment has been quite limited. The issue with 802.3x is that when a PAUSE frame is issued then all traffic is held, regardless of the importance of said traffic. More recently there have been newer standards (collectively known as DCB) that allow for more fine-grained controls (i.e. pausing traffic on a per CoS basis) as well as complimentary facilities to define different classes of traffic and how they map to these CoS values. Other examples of extensions to L2 networking for active flow control include buffer credits in Fibre Channel and the feedback mechanisms found in ATM ABR. True flow control isn't really applicable at layer 3, which is largely concerned with reachability and addressing. There are mechanisms at layer 4, however - notably TCP windowing - which allow senders to throttle back transmission based on network conditions. The operation and caveats of TCP windowing deserve their own question/post as there's a huge amount of literature on the subject. Another mechanism that has been specified but not extensively implemented/used for TCP is ECN (explicit congestion notification) which potentially allows a more proactive approach to throttling transmitter bandwidth (vs relying on packet drops for TCP windowing). In addition to strict flow control there are also mechanisms to shape, selectively drop and police traffic on a per-sender basis (i.e. L2 / L3 and some L4 QoS mechanisms) but these aren't precisely flow control, at least in the usual definition of the term. 

It shouldn't be different. As an example from one of my routers (redacted, obviously): Jun 19 16:39:56.440: %IPV6_ACL-6-ACCESSLOGP: list tu0-internet-in/190 denied udp 2001:x:x:x::2(123) (Tunnel0) -> 2001:x:x:x:x:x:x:x(123), 2 packets Jun 19 16:41:04.636: %SEC-6-IPACCESSLOGP: list internet-in denied tcp x.y.z.q(443) (GigabitEthernet0/3 beef.1aa1.beef) -> a.b.c.d(xxxxx), 1 packet Do you have a terminating line on your ipv6 ACL with an explicit deny w/log-input, like: ipv6 access-list tu0-internet-in ... sequence xxx deny ipv6 any any log-input Adding a sample of the ACL's in question for purposes of comparison would help, but I suspect it may just be the explicit deny that should fix things. 

You can separate clients within a VLANM if your switch supports PVLAN (private VLAN) which can be configured to allow any host to talk to the firewall while being unable to communicate with any other device. You can additionally configure your PVLAN to also allow communication amongst limited groups of servers. What sort of switch are you using? 

The priority setting only comes into play when a resource is limited - so if both 1G servers were transmitting at full speed to another 1G host there would be twice as much traffic as the destination link could handle. The switch would then, in theory, reference the relative priority of the frames to determine which would be transmitted and which would be dropped. Note that there may be multiple mechanisms that you can select to perform evaluation and drops and that each may yield different results (not very familiar with HP's network gear) - for example a 2:1 ratio of high to low priority packets (vs all high and no low)