You're fine - if there is a possible data loss AWS wouldn't provide a single click update. As this is only a minor version update it is pretty easy to upgrade without any problems. 

You should be able to see that information in the Billing Management Console with the Usage Reports for the Elastic Compute Cloud service. I can select "SnapshotUsage" for the different regions in there. 

So my call would be the last option - but that means you need to point the clients/browsers to a separate subdomain for all static assets (or for all POST requests). It sounds like you want to have a look at technologies like AngularJS or React to build a truly API-driven application in the browser. With this approach you're running a real API which is handling all the "dynamic" requests with an API Gateway and delivering the application itself from S3 as an static asset. Maybe looking at those might help you to find your way - even if you don't use them, the architectural pattern on how to build things like this is what you're asking for imho. 

mod_auth_mysql's crypt algorithm uses a different format of hash to Bugzilla's e.g. a crypt SHA-256 password would look like this: 

It will be the first VirtualHost that matches which will be loaded, and since the files are loaded in alphabetical order, you can change the ordering by changing the filenames. What I do is put a number prefix for each filename so I can set the priority in order to do something like what you mention. e.g.: 

Yum does a check on /etc/yum.repos.d/webtatic.repo to see if it has changed, but this appears to not be happening when this file is installed by the second rpm -Uvh command, and yum clean all does not stop this Webtatic EL6: 

You should then only see the packages you've set to be removed and installed in the list, and you can confirm the installation to switch over. Any services currently running with php loaded will need to be restarted, for instance httpd or php-fpm. As for this being 'seamless', any software changes happening while users are able to access the website should have the consequences fully understood. Shared libraries being removed and added when a process hasn't already loaded them could potentially load while the shared library isn't there. It's better to do software upgrades like this offline, and preferably tested on a non-production machine first to verify the process works as expected. 3 . To switch to a hypothetical CentOS base php54 package (CentOS 5 used php53 prefix), you just run the above steps replacing php removal with php54w removal, and php54w installation with php54 installation e.g. 

Did you create a Cache Subnet Group in you custom VPC? You need to create a cache subnet in your VPC (inside the ElastiCache Management) first - after that your VPC/Subnet will appear for nodes. 

Debian Lenny is very old and I assume that's the reason why Oregon doesn't provide the kernels you need to run your image. The only thing I can think of is to update your PV-Grub and create new AMIs. Maybe this helps: $URL$ If you don't find matching AKI in the other region there is no way afaik. 

There is a read preference value "nearest" which actually uses the latency between the MongoS and MongoD to determine which is the best/fastest set member for the query. You can find the documentation about it here: $URL$ The selection is based on the "member selection". For a sharded cluster like yours you can find the way how the MongoS selects the node here: $URL$ The most important thing to notice here is, that "nearest" doesn't care about the type of a node. So even if you do a read query it is possible that the MongoS selects a primary (which sometimes isn't the way you would expect it). To fix this the only way is to use the tags you already mentioned. Hope that helps! 

Whichever you do, it will reset to the original permissions if you upgrade httpd. It shouldn't change otherwise. Also, the reason why apache can write to the logs with the default permissions is because the log files are opened by apache whilst it runs as root. As soon as apache is fully loaded, it will change its user to apache, but will continue to have open log files. 

I would change the permissions on the /var/log/httpd directory to solve the problem of accessing the files from the console. add a new group and add the user to it: 

As Electrawn mentions, the libmcrypt package is now in the Webtatic repo, it was depending on the EPEL repository, but not everyone has this installed. As for the php54w-mysql installing MySQL 5.5 issue, this was resolved 6th May: $URL$ 

Have you set up yum priorities? RPMForge has subversion and mod_dav_svn 1.6.13, so they should be listed in installations and updates. Yum priorities will hide them if another higher priority repo has the same package names. If so, you will need to add exclude patterns to your /etc/yum.repos.d/CentOS-Base.repo file, by adding to the [base] section: 

The short answer is: You can create an A or CNAME record to the external IP address or the DNS name of your instance, but you don't want to do this. The long answer: You actually can create a CNAME (or even A) record without an Elastic IP. But every time your EC2 instance is restarted and so moved to another host system the IP address (and your external hostname) of your instance will change. If you can live with this and accept the fact that during the TTL of your DNS record your instance is not reachable you can use the external IP and create an A record with your subdomain. But as DNS is not very fast in distributing changes (even with a low TTL you can't make sure every resolver handles the TTL correctly) you don't want to do such things most of the times. This is why AWS provides the Elastic IP - so your IP address which is in the DNS record never changes but the routing behind this IP address is changed by AWS if you reassign it to another instance (or you reboot your host). This routing change is only inside the AWS data centers and so it is quite fast (within a few seconds) and your instance is reachable again for all users. Hope this helps! 

There is no APC extension for PHP 5.5, you should switch to Zend Opcache instead. I'm submitting a Webtatic answer as I'm the maintainer of it, and I dislike the lack of package pinning of Remi repository older RPMs, and the SCL new rpms. The former's way would have made this issue clearer as well. As you have the Webtatic repo already installed, you can run: 

The most secure way of doing it I would say is to have a group called git-readers add git and www-data to it, then have the following folder structure: 

Webtatic has added yum-plugin-replace to it's repository, and the guide is updated to allow upgrading via this method: $URL$ Assuming CentOS 5.x: 

Note the 's' in the group permissions. This makes the writer users use git-writers group as their default group. This will only work properly if the writers are all umask 0002. 

Perhaps this is because the Webtatic 5 package was built earlier than 6. I could add to the build process something that would update the timestamp, however there was never any plan to have Webtatic 5 support CentOS 6, so you may get other unexpected results if you try to use it on that version. A workaround for now would be to force Yum to spot the file has changed: 

In EC2 you should take a look at cloudinit ($URL$ and the user-data ($URL$ With this you're able to provide scripts which will run during the instance launch based on data you can send to the AWS API. But besides that: Why are you doing a RAID and copy over data from the boot volume to the RAID on launch? Without knowing your exact use case but that sounds wrong ;-) Maybe you can elaborate more on this so we're able to actually provide a better solution without the need of doing launch scripts and so. 

Is there a reason why you don't use EBS snapshots? You can use those to save the whole EBS device (incremental) with a simple API call and the snapshots are saved within S3. If you need an old version back just create a volume from this snapshot and connect it to your instance instead of the broken EBS. 

From the connection point of view "something" needs to answer your requests (GET, POST, PUT, everything). First of all you have a TCP connection and "something" needs to make sure it is understanding layer 7 and making sense out of the bytes the client is sending. Only at this point it is possible to handle GET requests differently than POST requests or one URL than another URL. So in the end you need a service which is capable of understanding and routing HTTP. The following services are capable of doing this: CloudFront ELB/ALB API Gateway (limitation comes later) API Gateway uses CloudFront internally (without giving you the chance to actually configure anything on the CloudFront level) - that means there is no way to run CloudFront and API Gateway side-by-side as in the end this would mean you run CloudFront with CloudFront side-by-side. CloudFront gives you the chance to select different origins based on patterns - but you can only select S3 or ELB/ALBs as an origin - not Lambda functions (besides the Lambda@Edge functionality). ALB/ELB can only use EC2 instances as a backend - no Lambda or S3 here. The only ways I can think of which might do what you want to do are these: 

proxy_redirect will only rewrite redirect headers, not the HTML content needed for this to work. There is no easy way to rewrite the HTML content in Nginx. There is the HttpSubModule, which you can manually add text substitutions, but is lengthly to set up each substitution: $URL$ It is easliest if possible to just update the content at $URL$ to have the redirected path in instead. 

2 . You need to first see what existing installed packages need replacing, you can do this by using yum shell to combine removing php-common and installing php54w-common in one transaction (so shared dependencies aren't removed) 

CentOS 6.4 broke backwards compatibility in some library versions, and so latest Webtatic builds only support CentOS 6.4 for some packages (most of the PHP packages are unaffected). If you can update your CentOS 6.x version to 6.4, that would solve the issue. CentOS has in recent years started making it hard to fix to earlier releases to try to push users to the latest release. It's not an ideal situation for servers that can't be upgraded, but repositories would have to support each minor release to avoid this situation. 

There is no "Attachment History" - so just start a new instance (default AMI with the Distribution of your choice) and attach all volumes to different /dev/* then mount them readonly on the machine and check for yourself what is on the volumes and if you still need the data. So you can make sure you won't change anything on these devices/volumes (even booting from these volumes will change logs etc.) 

Yeah that should work (and is the way you should do it in the cloud). Just copy the whole infrastructure and then shift over the traffic to the new one and control the metrics of both infrastructures. 

DynamoDB is built for high throughput low latency. So the core feature of this Database is providing answers within single digit milliseconds. As you already figured out you pay per second which means if you don't need nearly the same amount (within the ability of scaling up and down) of reads/writes over a period of multiple hours DynamoDB is expensive as you pay for queries you don't need. 

As you mentioned you're running Ansible on a EC2 instance you should actually don't use credentials but roles attached to the EC2 instance: $URL$ The idea is that your instance itself is able to get temporary credentials and can execute the necessary commands which are defined in this role. As you never store any credentials anywhere this is the most secure way to work with the AWS API from an EC2 instance. As Ansible relies on boto this will work out of the box - you just need to create a role which has all the necessary IAM permissions and attach it to your instance you're running Ansible on. After that your dynamic inventory will work without needing any additional credentials. 

Actually, I probably wouldn't use my other answer in some situations, but I'll keep it there for reference. If your colleague has been given their own VirtualHost, I would set that virtualhost to output their logs to a different directory, so that they would only see errors for their system: 

Since you've already upgraded PHP however, it may be easiest to either compile eAccelerator yourself (its not available via pecl): $URL$ or switch to a repository that supplies eAccelerator rpm for the PHP version installed. I provide eaccelerator for PHP 5.3, along with PHP 5.3 itself if you need it. I don't supply eaccelerator for PHP 5.2 though. $URL$ then 

Without adding an additional external repository, you wont get a later version of git. See this article for getting Git 1.7 via yum: $URL$ 

You could configure Squid on the server to allow the proxying of Yum from the private subnet to the Redhat update servers. It can also cache packages, meaning it will only download a package once for all updates. $URL$ 

If there are other packages than php* in this list then you can't seamlessly switch to Webtatic PHP 5.4, but will have to investigate alternatives. Webtatic has replacement packages for all base php packages (see the packages listed on the page linked to in 1. for confirmation), so there should be no issues, unless you use other 3rd party repositories that have packages installed dependent on the specific php version installed. For the rest of the installation (still in yum shell), you just remove these packages and install their php54w-* counterparts: 

I assume we're talking about dynamic IP addresses so you can't limit their IPs inside of the security group as they're always changing. The only way I can think of right now is blocking every external IP address in the security group of the Redshift cluster and setting up a "SSH jump host" inside of the same VPC. If you now enable this host to connect to the Redshift cluster users can access the Redshift via a SSH tunnel. As you can roll out different SSH private keys to each machine you can limit and allow the people independent from each to other to access the Redshift cluster. Several SQL tools are available for DB Connection through SSH tunnels so maybe that is something your users already use and you're able to limit the access to certain users. 

The check is a "<=" (I assume you overlooked the -infinity instead of +infinity) - so the value you put in is the upper bound in that case. ("Scale in as soon as CPUUtilization is between your value and -infinity") Just to mention that here: you could also use the "target tracking policy" - with this you can say that auto-scaling should scale in a way that your target value is matched - that includes scale-out AND scale-in out-of-the-box. Hope this helps!