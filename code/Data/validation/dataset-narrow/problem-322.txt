You can use tools which are very efficient in migrating the data from one type of the database to other. There are many types of tools some of them are freely available.I have used Talend which is very efficient in doing this. It is also freely available. 

Instead of creating trigger you can use below code, It will also return you success or failure message. 

UPDATE What will be the estimated upper bound (Max size of table) with 1 million records and how can we estimate it. 

I don't have changed anything in my.cnf.I don't know how it get crashed. How can i recover it ... Content of error log are as 

If you face the above error set to some higher value. Otherwise in my opinion increasing it to some higher value will not allow you to quickly load a 700MB file. 

Restore the data in table and find out index_length for MyISAM table and set the Key_Buffer_size accordingly. See How to set Key Buffer Size. 

Key Buffer Used Find value of key_buffer_size as convert it into MB. Find the All MyISAM index Size : 

The is used with the purpose as .. If the value of is 0, the log buffer is written out to the log file once per second and the flush to disk operation is performed on the log file, but nothing is done at a transaction commit. When the value is 1 (the default), the log buffer is written out to the log file at each transaction commit and the flush to disk operation is performed on the log file. When the value is 2, the log buffer is written out to the file at each commit, but the flush to disk operation is not performed on it. However, the flushing on the log file takes place once per second also when the value is 2. Note that the once-per-second flushing is not 100% guaranteed to happen every second, due to process scheduling issues. The default value of 1 is required for full ACID compliance. You can achieve better performance by setting the value different from 1, but then you can lose up to one second worth of transactions in a crash. With a value of 0, any mysqld process crash can erase the last second of transactions. With a value of 2, only an operating system crash or a power outage can erase the last second of transactions. InnoDB's crash recovery works regardless of the value. In My opinion using to 2 should not be an issue.But to use 1 is the safest. 

For me too many indexes means that it is more expensive to insert, update, or delete rows. Because, if the indexes are of any use they need to be maintained. Find all of the indexes with columns that you either don't query, or don't query very often and drop them. Find all of the indexes where the columns are the same, but in a different order drop all but one version of those. For example a good index would start with low cardinality and get more specific. For example country, state, city, zip code. Another good method would be that if you search by date and other fields lead with the date column to eliminate the largest number of rows first, then choose the values where the other fields match. Try to get the number of indexes down to less than 10 per table. Although the number of indexes that are useful depends on whether this is OLTP, DSS or data warehouse. 

The following transformations may help. I have not used the DBMS_XSLPROCESSOR.CLOB2FILE method, but I did use these to migrate an Oracle database from Solaris to Linux. I could not use data pump because of the version of Oracle that they were using and the fact that they used XML data types for column data types. 

What I understand from your question I am answering on that basis. If you want database dump use client utility.You can Execute it as 

You are missing column name of parent table to which your child refer, while creating child table, do like below 

The effective maximum table size for MySQL databases is usually determined by operating system constraints on file sizes, not by MySQL internal limits. Have a look at the link for your reference Limits on Table Size. 

There is no constraint like You can load a GBs file. Depending on the size of the rows, you'll probably exceed this.One way would be to split it up into several chunks. This is probably a good idea anyway. LOAD DATA isn't subject to the limit. If you are adjusting your (default is 1M). Set it as 

If you will always have two records corresponding to each id then you can use query below. If there is some other constraint please update that in your question 

this will create the TableName.txt file and TableName.sql files at the location as you specified with --tab option where TableName.txt is your CSV you can rename it as TableName.csv. Note : use the (--tab="PATH") path where the mysql has write permissions. for various other options of mysqldump. please see.. 

I have two sql boxes, one primary and one mirror. The mirror stopped synchronising last night for about 12 hours and so now the primary is transferring about 25GB of logs that built up. However it's taking a long time. What happens if the primary falls over (for whatever reason) during this time? Presumably the mirror will take over as primary, but I don't know how much of the transaction logs have been transferred. 

I have two sql boxes, one primary and one mirror. The mirror stopped synchronising last night for about 12 hours and so now the primary is transferring about 25GB of logs that built up. It's now synchronising again but I can't connect to the primary db server to check the status and to see how much there is left to transfer. Is it possible to do this through the mirror/witness? 

I have some statements that I am running. Not all of them work (they are the result of running SQL Data Compare) and I want to group them in some transactions and roll back the statements if something goes wrong. Is this possible, or is it only data that can be rolled back? 

I have a an SQL server that normally listens on a private IP address (10.x.x.x). However, I would like to be able to connect to the database over the public (31.x.x.x). I tried to enable this tonight, but when I restarted the SQL Server service, the server wasn't listening on the internal IP address any more. I was using the configuration manager to let the SQL Server know the IP addresses it should be listening on. I am running SQL Server 2008R Standard. 

You would be better off using the nid command to change the dbid to a new value. then register your database with the rman catalog, if you use one. 

Also under sqlplus you can set the number of days for the control files to track the backup information for RMAN. 

You can clone your production database to a point in time before the table was dropped, then export the table you need and import into your prod database. You can also try a table space point in time recovery. I tested it out a few years back, but I haven't needed to do it since. Performing RMAN Tablespace Point-in-Time Recovery (TSPITR) 

If it was me I would use an ERD tool like Erwin to generate an ERD diagram of your database. You can use Erwin to take the SQL Server ERD and generate an Oracle equivalent. I would then generate all of the table creates from the ERD tool and either with the tool or manually generate the DDL for the constraints, indexes and triggers. Once you do that you can create a schema with all of the tables that you need and create a database connection to SQL Server using heterogeneous services. At that point you have all of the tables that you need in Oracle without any constraints, indexes or triggers. You can go through each SQL Server table and do an insert into as select from. Finally you would run the script to create constraints, indexes and triggers. Since T-SQL and PL/SQL are not compatible, you will need to rewrite stored procedures by hand, including triggers. Erwin has/had a macro language that allows someone to write code that can translate to either SQL Server or PL/SQL and I managed an application that worked with both almost 15 years ago. But if you are migrating once, I would not bother with it. Based on personal experience, this process works best if you can script the process and run many iterations of your migration process. Then test after each iteration. 

So I need to delete 611992998 records from biggets table. We have One MySQL Master and 4 MySQL Slaves, We need to delete data from all the servers, What I am thinking is i will delete data in chunks from master so that slaves also didn't lag too much.For that i have created a procedure here is the procedure, I have not yet tested it 

This is nothing but a folder/file in database directory. The default location for data directory is on ubuntu.Go to that location() and find what is there in that folder if it not your database please move that to some another place. If you need to find the location of data directory issue command on MySQL shell 

I am very new to mysql-cluster.I downloaded the binaries and was trying to configure it on my local machine. Can anybody suggest how to install it on single machine with the 1 management node,2 data nodes,1 sql node. Also what are the importance of each type of node? I need the basics steps to install the mysql-cluster.As I have installed it by seeing some website and stuck in between i am getting an error as 

Use Backtick Sign around columns names. As you have a column which is a keyword in MySQL, So you need to put Backtick around column the name 

You should be able to backup an 11.2.0.2 database using RMAN with an RMAN repository that is set to 11.2.0.4. There seems to be some invalid objects in your RMAN repository schema. You may want to try compiling all of the invalid objects in your RMAN repository to see if they become valid. This probably has nothing to do with you backing up an 11.2.0.2 database with an 11.2.0.4 RMAN repository. 

Erwin has a macro language for writing stored procedures that can be compiled against Oracle or SQL Server. You write, IMHO ugly code, with variables that will get translated to something understandable by a particular database engine, then Erwin will create a translated version for that database engine. I have not worked with Erwin in years, so I am not sure that it still has the feature. IMHO, you develop databases and applications for a specific database engine, so that you can use the best features of that database to get the best database application. 

My feeling doing a Merge is the same as doing a select statement, then inserting data that does not yet exist in the target table and updating data where the primary key is already there. It sounds like you are saying that all of the data should be there in advance and you are just updating some columns based on new data. You could do a minus query first and if that returns rows then raise your exception. Otherwise you would do either a merge command or simply an update command. Row level triggers tend not to scale to large volumes of data. if you are doing batch processing, then you probably only have one process changing the data at a time, so worrying about concurrency might not be relevant. Why is it an error to not have data? 

After all that one of the servers didn't have the SQL Server service running under the same user. I'm an idiot. 

Back in the days of yesteryear, it was considered a big no-no to do or because of the performance hit. Is this still the case in later versions of SQL Server (I'm using 2012, but I guess the question would apply to 2008 - 2014)? Edit: Since people seem to be slating me slightly here, I'm looking at this from a benchmark/academical point of view, not whether it's the "right" thing to do (which of course it's not) 

I'm using SQL Server 2008R2. Last night, the host machine on which my mirror database was running basically went belly up. Thankfully the principle database was on a different host server and so was fine. However... The principle is working but says suspended. When i try to resume the mirroring, I get an error in the SQL error log giving an error number of 9004. A quick Google of this error number come back with this article. (tl;dr: transaction log is damaged) So, does this mean that the transaction log shipping between the principle and the mirror has somehow got screwed up? How do I fix this? Is it as simple as doing a full backup on the principle and a full transaction log, then restoring them both on the mirror database with norecovery switch on and then set up the mirroring again? Or will I need to do something more drastic?