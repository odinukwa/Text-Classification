The boat changes velocity and orientation and heads off. My character however has a velocity of 0,0,0 but I would like him to stay onboard. When I move my character around, I would like to move as if the boat was the ground I was standing on. 

In the following example there is a guy and a boat. They have both a position, orientation and velocity. 

If you're building in HTML5 I suggest you look into Node.js and Socket.IO, as they fit well for your type of gameplay and are easy to implement due to Node.js' event loop. On top of that, you only need to use javascript for the client and server. 

I'm building a game using WebGL and Three.js, and so far I have a terrain with a guy walking on it. I simply cast a ray downwards to know the terrain height. How can I do this for other 3D objects, like the inside of a house? Is this possible by casting many rays in every direction of the player? If not, I would like to know how I can achieve the simplest collision detection possible for other meshes. Do you have to cast a ray to every triangle in every mesh nearby? 

The problem I'm facing, is the fact that I don't know how to make some monsters appear harder than others. Since nothing has a level, and characters can only get stronger by acquiring better items, it's difficult for me to "warn" players that a monster is difficult for them. Fortunately, the whole world is designed by hand and we know which areas should be more difficult than others. So this means, I as developer knows which monsters are stronger than others. But new players won't! I don't want them to get frustrated by walking into the wrong monsters reserved for higher players. So then, how can I make monsters appear more dangerous than others? Some things I thought of: 

You can use openGL atomic counter to increment a value inside your fragment shader. Here is a tutorial on opengl atomics: OpenGL Atomic Counters 

It is very simple to do it in one simple draw call, by iterating every texel in the depth buffer inside the vertex shader. You will choose to draw points, as many as the depth buffer resolution or less, and in the vertex shader you will fetch the depth buffer value and project it into the cube, which the 3D texture covers. You can choose from the vertex shader to which slice you want to render the point ( e.x. in openGL you pass the slice to the variable gl_Layer ). Then in the fragment shader, you do whatever calculations you want. 

You create an array of your map with size = width * height of the matrix and pass it into the shader as an array of booleans using glUniform1iv ( or safer ints because the driver will first convert your boolean into int and then into boolean in the shader ). But with this approach you have to set the maximum size of the array inside the shader ( uniform int map[320*320] ), because GLSL does not allow the creation of an array of unknown size, and pass the current size of the array as a uniform. With your second approach : 

OpenGL Transformation UPDATE Also as Lasse said in his comment you can do the lighting calculations in view space (ECS). You can achieve that by multiplying the position and light direction of each light with your view matrix. Keep in mind that if you use a scaling factor inside your view matrix you should multiply the light direction with the inverse(transpose(viewMatrix)) as you do with normals. 

I am fortunate being able to use the same programming language on both the server and client (javascript). I would therefore like to share my code and classes between server and client, like Half-Life (and possibly others) did. To make things simple, suppose I have an MMO with a Player and an Enemy class, with a position, velocity, and health. 

I'm building an MMO using Node.js, and there are monsters roaming around. I can make them move around on the server using vector variables acceleration, velocity and position. acceleration = steeringForce / mass; velocity += acceleration * dTime; position += velocity * dTime; Right now I just send the positions over, and tell the players these are the "target positions" of the monsters, and let the monsters move towards the target positions on the client with a speed dependant on the distance of the target position. It works but looks rather strange. How do I synchronise these properly with the players without looking funny to them, taking into account the server lag? The problem is that I don't know how to make use of the correct acceleration/velocity values here; right now they just move directly in a straight line to the target position instead of accelerating/braking there properly. How can I implement such behaviour? 

Much like a heart is used a lot for health and lives in old retro games... minecraft heart $URL$ ...what would be a good icon to use for Energy, Mana and/or Magic points? 

I know ideally these should inherit from each other (or use a CES system). But my question is as follows: Since these classes will be used on the client and server (since both client and server needs to know about these entities) with some but not all shared logic, would it not be crazy to define two separate classes on both the server and client? What are the advantages and disadvantages of sharing classes between server and client? Are there any helpful websites with tips/tricks for this approach? 

...which all works fine. Now I need to build a view matrix to pass to DirectX SetTransform function. So I build a rotation matrix from the camera orientation quat as follows: 

...(The world matrix is always an identity, and the perspective projection works fine). ...So, without the commented code being compiled, the rotation works fine. But to be proper, for obvious reasons, the camera position should be rotating around the sphere, which it currently is not. That's what the commented code is supposed to do. And when I add in that chunk of code to do that, and look at all the data as I hold the keys down (using UP, DOWN, LEFT, RIGHT to rotate different directions) all the values look correct! The camera position is rotating around the sphere just fine, and I can watch that happen visually too. The problem is that the camera orientation does not lookat the center of the sphere. It always looks straight forward down the z axis (toward positive z) as it revolves around the sphere. Yet the values of both the rotation matrix and the view matrix seem to be behaving correctly. (The view matrix orientation is the same as the rotation matrix, just transposed). For instance if I just hold down the key to spin around the x axis, I can watch the values of the three axes represented in the view matrix (x, y, and z axes)... view x-axis stays at (1.0, 0.0, 0.0), and view y-axis and z-axis both spin around the x axis just fine. All the numbers are changing as they should be... well, almost. As far as I can tell, the position of the view matrix is spinning around the sphere one direction (like clockwise), and the orientation (the axes in the view matrix) are spinning the opposite direction (like counter-clockwise). Which I guess explains why the orientation appears to stay straight ahead. I know the position is correct. It revolves properly. It's the orientation that's wrong. Can anyone see what am I doing wrong? Am I using these functions incorrectly? Or is my algorithm flawed? As usual I've been combing my code for simple mistakes for many hours. I'm willing to post the actual code, and a video of the behavior, but that will take much more effort. Thought I'd ask this way first. 

The guy already has a reference to the boat he's standing on, and thus knows the boat's position, velocity, orientation (even matrices or quaternions can be used). 

You can use Blender for that. But it has nothing to do with matrixes, the only thing you need to set up is the ortographic camera. See this blog post for a generic approach in making graphics from 3d models. You will need the same approach except for a custom camera with orthographic view. 

We're creating an action MMO using Three.js (WebGL) with an arcadish feel, and implementing physics for it has been a pain in the butt. Our game has a terrain where the character will walk on, and in the future 3D objects (a house, a tree, etc) that will have collisions. In terms of complexity, the physics engine should be like World of Warcraft. We don't need friction, bouncing behaviour or anything more complex like joints, etc. Just gravity. I have managed to implement terrain physics so far by casting a ray downwards, but it does not take into account possible 3D objects. Note that these 3D objects need to have convex collisions, so our artists create a 3D house and the player can walk inside but can't walk through the walls. How do I implement proper collision detection with 3D objects like in World of Warcraft? Do I need an advanced physics engine? I read about Physijs which looks cool, but I fear that it may be overkill to implement that for our game. Also, how does WoW do it? Do they have a separate raycasting system for the terrain? Or do they treat the terrain like any other convex mesh? A screenshot of our game so far: 

In my personal opinion, i will go with the second approach since it does not bound me to a maximum size of the map. 

If you want to "scale" the function you should change your input range ( or in mathematical terminology the domain of the function ). I advise you to make your domain irrelevant to the screen size. Also in order to make your display irrelevant of the the screen size you should use normalized coordinates ( e.x [-1...1] or [0...1] ). Screen coordinates normalization is very easy, you just divide your coordinates with the size of the screen. 

This way you can find the which is the surface area of your pixel when its normal is perpendicular to the viewing direction. To find the correct area surface you just have to multiply with where is the angle of normal and view direction (actually ). 

The solution is simple. You just translate the camera to the position in the center of the player. If the stored player position is in the center of the player sprite then cameraPosition = playerPosition. If it is in the left bottom of the player sprite then cameraPosition = playerPosition + playerSize/2.0. One important thing is to first update the player position and then the camera position, otherwise your camera will lag one update behind. 

The easiest think to do is to pass as a uniform the position of the camera in World-Space and calculate the direction from the vertex to the camera and then calculate the dot product between that direction and the vertex normal. The code would be something similar to 

...Now (as seen below) if I just transpose that rotationMatrix and plug it into the 3x3 section of the view matrix, then negate the camera's position and plug it into the translation section of the view matrix, the rotation magically works. Perfectly. (even when I add in rotations for all three axes). There's no gimbal lock, just a smooth rotation all around in any direction. BUT- this works even though I never change the camera's position. At all. Which sorta blows my mind. I even display the camera position and can watch it stay constant at it's starting point (0.0, 0.0, -4000.0). It never moves, but the rotation around the sphere is perfect. I don't understand that. For proper view rotation, the camera position should be revolving around the sphere. Here's the rest of building the view matrix (I'll talk about the commented code below). Note that the camera starts out at (0.0, 0.0, -4000.0) and m_camDistToTarget is 4000.0: 

Background: I'm trying to create a game where the camera is always rotating around a single sphere. I'm using the DirectX D3DX math functions in C++ on Windows. The Problem: I cannot get both the camera position and orientation both working properly at the same time. Either one works but not both together. Here's the code for my quaternion camera that revolves around a sphere, always looking at the centerpoint of the sphere, ... as far as I understand it (but which isn't working properly): (I'm only going to present rotation around the X axis here, to simplify this post) Whenever the UP key is pressed or held down, the camera should rotate around the X axis, while looking at the centerpoint of the sphere (which is at 0,0,0 in the world). So, I build a quaternion that represents a small angle of rotation around the x axis like this (where 'deltaAngle' is a small enough number for a slow rotation): 

You create the texture buffer with the size of width * height and set each pixel with the desired value. Then you create the texture with a format which has one channel ( GL_R8 or GL_R8I for example ) and every time you update it with glSubTexImage2D. Keep in mind that you should set the filtering to NEAREST or use textureFetch, otherwise the texture fetch will give you incorrect results. An important thing to note, for both approached, is that you should update the buffers only when there is a change in the allow/Disallow map. 

Your positions are converted from WCS ( World Coordinate System) to NDC (Normalized Device Coordinates) in order to by saved inside the depth buffer. This is achieved by multiplying your coordinates with the view matrix to convert them to ECS (Eye Coordinate System) and then with the projection matrix. And at last they are divided by W component in order to be converted to NDC. So in order to get them back to WCS you need to multiply the coordinates by the inverse(projectionMatrix * viewMatrix). Your fragment code should be: 

( There is an extension that supports the gl_Layer in the vertex shader (GL_AMD_vertex_shader_layer). You can use it if you can or you can have a simple pass through geometry shader that just sets the gl_Layer value ) 

You should multiply the normal with the inverse - transpose Model matrix since if your Model Matrix is not an orthonormal matrix you would get incorrect results if you multiply him with the normal. Also the code will work only if your bufferMatrix contains only translation information and not rotation or scaling. Keep in mind that a direction in a 3D space is defined by X,Y,Z components, so it is better to have the W component as zero when you normalize or calculating the length of the normal ( or not use it at all as in the code above ), otherwise you will get incorrect results.