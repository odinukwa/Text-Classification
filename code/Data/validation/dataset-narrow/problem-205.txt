Well the problem is that your CASE statement is ambigious. If the value is @p or @p1 both case when expressions will be true. In that case just the first one hit (returns 1) is going to be evaluated. There are two alternative ways I could think of: 

Eventually I have found a solution to my problem. As on a StackOverflow question described your can hack you way around the problem using . So I added these two lines to my code (within the IF clause and before doing other things) and got things to work: 

By the way: Having a time as datatype varchar(10) sounds a little bit odd. Datetime would seem to be more fitting. Have a look here for advice. 

I have still got all the rows but assigned a rownumber based on my priority. Then the last thing I have to do is to filter only the most relevant rows which are the one having row number 1 . Presto...here's the result of the whole query: 

will replicate statements on ads database (your context should be ads, ) and ignore the statements on other databases: The context is : REPLICATED 

I usually use mysqlcheck with --optimize and --databases combination. mysqlcheck is a command-line interface for administrators to check, optimize and repair tables. 

3. ALTER TABLE (ONLINE) The problem of and that it locks table during operation. You can use the Percona tool : pt-online-schema-change (from Percona Toolkit : link ). pt-online-schema... will construct a mecanism with triggers and temp table that you permit original table to be available for read and writes during the operation. I use this tool in production for the big it's pretty cool. Note that you should have referenced to your table, FK and triggers risk to produce a mess. To check this prereqs, query: 

I dont know about feed_id but your timestamp column by definition have probably many differents values. I will advise you to try to create a simple index on timestamp column. Do you have "disparate" values too on feed_id column? For me you should take your attention on the timestamp column (WHERE + ORDER BY). Maybe it'll not a index problem, maybe your MySQL instance have some difficulties to sort the result (bad temp table management) so you can try without ORDER BY clause to see its impact. Let me know. Max. 

You'll need to copy over ALL the contents of your old data dir to the new data dir. Specifically it sounds like you did not copy the mysql folder (read mysql database) over to the new data dir. 

Completely shutdown the server you are going to clone. Copy it over, make relevant .cnf changes (especially server-id), start up the new instance. If you're using LVMs you can take an LVM snapshot while the server is running and then copy that snapshot over. 

Before any percona script it will give you TONS of debug info that can help point you in the direction of what's going wrong. Also remember these are all perl script so you can inspect the source yourself to (maybe) see what's going wrong. 

I'm in the process of decommissioning an old mysql server. Of course there's a few things left connecting to it periodically people had since forgotten about. I turned on general query logging to get a better view of what was still going on there to track down the culprits and noticed something odd. There was an entry like: 

The "the closest matching" constraint make me think to the MySQL function SOUNDEX. I never used this feature but maybe it's well designed for you :) Max. 

Your index is obsolete. It's a duplicate index as the PK starts by sorting so the queries on this field will use the PK. Note that I always create a ID field of type INT (or BIGINT for big tables, storing logs for instance) for the PK, then you can add a UNIQUE KEY of the field(s) you want but the PK should be simple as possible. It facilitates consistency checks and deployment on cluster solution as Galera. Second note, you should always set the option on your PK field, you will never need negative values in them (even more because you use AUTO_INCREMENT) and you double the capacity of your datatype. For me the table creation statement will be: 

MySQL calls these section "Groups". The [mysqld] group contains the variables apply to the MySQL Server (the mysqld process). The [mysql] group contains the variables for the client program (mysql). The [client] option group is read by all client programs (but not by mysqld), so "mysql", "mysqldump" etc... None of these groups are mandatory but usually we set at least the [mysqld] because it's where we configure the server. About how MySQL interprets these variables, MySQL Documentation says: 

I'm wondering if anyone knows of a good way to track down queries causing max_allowed_packet errors. Even if you turn on general query logging the query isn't logged as the connections aborted before it gets that far. I'm trying to give something to the development team to help track this down. All I know is a general application account that causes it periodically. It's not like clockwork so it seems to be because of some kind of user generated interaction (versus a cronjob for example). I had thought about bumping the limit on the master with out adjusting the value on the slave to let replication break but this is coming from a read only account, meaning it has to be a select of some kind. The current value is set to 8M and there's not an expected use case dev would expect to generate much so just upping the limit to sweep it under the rug isn't really addressing the problem. I'd like involved parties to know what exactly is causing it so an informed decision can be made on whether bumping the limit is the right course of action (versus fixing some bug that shouldn't be generating queries that large). I'm trying to get buy in to get app changes to either inspect query size before lobbing it over the fence, or at least catch those specific exceptions in a manner that provides sufficient information to find out what's generating it. They have other priorities so it would be nice if I could just say "this is exactly it" Edit: Turning on general query logging with an upped max allowed size to let the selects goes through is prohibitive since the pattern is not predictable and just 'letting it run for a couple days' isn't possible with the space available. 

The procedure "AUDIT_TO_OTHER_FG" is a database level trigger. Its purpose is to put audit tables (with history data) into another filegroup. Our Java Application running on top of the database is using Hibernate and doesn't bother specifying filegroups. However all of these audit tables follow a certain naming convention. Thus the trigger fires at a CREATE_TABLE event, rolls back the table creation and creates the table again on a different filegroup. Maybe this is not the most elegant version to put the tables onto a different than the default filegroup...however it has worked fine in the past and has never been a problem until now. I had Management Data Warehouse data collectors set up before for that environment as it was running on SQL Server 2008. There haven't been any problem regarding these triggers in that version. Recently we moved to SQL Server 2017 and now I am experiencing these issues. I dropped the trigger temporarily and the data collector worked fine. So somehow it appears to be interfering with the actions of the data collector and the problem is the dynamic SQL used. However I do not get why this causes a problem as the data collector seems not to create any table in my user databases and the trigger doesn't fire while the data collector is run. Workarounds tried I have read a bit into "WITH RESULT SETS" and tried to change my trigger as follows: 

To switch of MySQL engine you could : 1 Make a ALTER TABLE myTable ENGINE=InnoDB 2 Make a mysqldump of your table, then edit the CREATE TABLE statement to replace MyISAM by InnoDB and restore the dump in the new table (i called it myTable_InnoDB): 

Unfortunately MySQL doesn't support "Functioned Based" index (but you will have noticed that). It is therefore not recommended to use this trick, Peter Zeitsev wrote in a blog post on the ORDER BY thematic : 

This is a good question. You have several solutions but your table is quite big so none will be without pain :) You have three solutions to "shrink" InnoDB tables: 1. OPTIMIZE TABLE You can use as you mentionned it but you should care about the variable : 

--replicate-ignore-db applies only to the default database (determined by the USE statement). Because the database was specified explicitly in the statement, the statement has not been replicated. My advice is to change the context with a instead of explicitly specify the database in the statement. Max EDIT (for questions) 1. 

As you described we'd like to return X first, then possibly Y (if no X present) and at last Z and only one row per student if the applicable subject code. XYZ is easy because it is alphabetical but let's introduce some additional complexity guessing that there could be also an A which we would like to return if there is no X. Here is a SQL Fiddle with my example I would use the following query to achieve this: 

So nothing of this really worked. Could you think of an alternative how to keep the trigger doing its job but also have the data collector work properly? Resources Here's the original sourcecode of the database level trigger: 

as fas as I understand you would like to control the order of execution if x is not present search for y and so on. Therefore a simple IN clause or the case expressions probably won't work if you've got entries for x as well as for y and z....by result returning multiple rows. The best way I can think of would be to introduce a priority factor within a cte and use this to return only the relevant row. Here is an example. For simplicity I use a table with the following structure 

Since the semicolons need to be part of the trigger definition you need to change your delimiter first. From the prompt first run 

You'll have to specify a character set of UTF8 on the table schema. See $URL$ Depending on your needs you can specify table defaults which then apply to all unspecified text columns (char/varchar/text) or you can specify on a per column level. You'll also need to have your applications to specify a UTF-8 character encoding. The specifics on how to do this will depend on the language you are using. 

So in summary, the general approach of partitioning tables can offer many benefits. However it's not a magic bullet to be applied blindly without consideration to access patterns and how exactly you are partitioning. I could imagine situations where the desired partitioning is very application specific and would be better suited to have that logic sitting in the application layer. However given your straight modulus 10 description this does not seem like such a case. EDIT In writing up my description I forgot that you stated your table is 100K rows. With out the full schema of your table and it's average row length it's hard to say for certain, but in general that sounds medium sized even for modest hardware. At the same time, if it's not causing problems the way it is now or in the foreseeable future then don't spend time and introduce risk by changing it.