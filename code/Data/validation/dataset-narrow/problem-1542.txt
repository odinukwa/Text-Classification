A batch is a grouping of instances from your dataset. For example a batch of 100 text samples that will be fed to train your model together. 

Now if we get a new data point $x_{new} = [2, 8]$ we will label this as being a woman. So the entire decision is based on the numbers $1, -2, 1$ from our linear equation. We need to tune these values using the training data. We usually call these trainable parameters the weights $w$ associated with the features $x$ and we also add a bias $b$. In general the linear separator in 2D is $0 = w_1x_1 + w_2x_2 + b$. Our predicted label is $\hat{y} = w_1x_1 + w_2x_2 + b$. If $\hat{y} > 0$ then woman, else man. In $n$ dimensions this can be written as a matrix multiplication as $\hat{y} = w^Tx + b$ Obviously, a linear separator is not sufficient for most classification tasks. Things are not always linearly separable. So we use more complex models. Neural networks In neural networks each node is associated with a function much like the linear separator. However, we will use the sigmoid function $\sigma(w^Tx) = \frac{1}{1 + exp^{-(w^Tx + b)}}$. The weights here have the same effect. They will modulate the input values $x$ such that we are able to learn some classification or regression. 

The model I would use is the one that minimizes the accumulated quadratic error. Both models you are using, linear and quadratic, looks good. You can compute which one has the lowest error. If you want to use an advanced method you can use RANSAC. It is an iterative method for regression that assumes that there are outliers and remove them from the optimization. So your model should be more accurate that just using the first approach I told you. 

I would try to set a multilabel classification algorithm and make the output standard by adding zeros. So if your data is like this: <1, 1>, <2, [1, 1]>, <3, [2, 1]>, <4, [1, 2, 1, 1]>, <5, [1, 1, 1, 2, 2, 1]>. The maximum number of output is 6. So you could transform your data into something like: <1, [1,0,0,0,0,0]>, <2, [1, 1,0,0,0,0]>, <3, [2, 1,0,0,0,0]>, <4, [1, 2, 1, 1,0,0]>, <5, [1, 1, 1, 2, 2, 1]> Another option that occurs to me is to add the limit dynamically. Let say you have your training and test set. You can search for the biggest length and create an algorithm that adds the zeros to both datasets. Then let's say a new data you want to predict has a bigger length, then you'll need to recompute all training and test with for this new prediction. You can even check how extending the limit affects your model. 

In your specific case you only have 2 clusters, however this is not necessarily always going to be the case. I would allow for more flexibility. I assume from your sample code that you are following what is shown in the docs. Following from what they are doing you should have the following 

Loading your best set of weights Then when the training is done you can retrieve the best set of weights using 

Preparing the data We will generate some random lists containing integers between [0,49], we will take a random permutation of the list and then take the first 4 values. We will then set our targets $y$ as the sorted rows of $x$. 

You are getting blocked because people do not want to waste server bandwidth on someone who is trying to exploit it without bringing significant profits. Try to make your crawling less predictable. Slow down the frequency with which you ping the server and vary the actions of your crawler. This will make it harder to detect as it will act less predictably and may be wrongfully identified as being a very quick human. 

This is the typical value function of Reinforcement Learning. The discount factor evaluates the importance of the accumulated future events in your current value. The smaller the number, the less important are the future events in the current action. Usually this number is selected heuristically. I usually select 0.9. If I don't want any discount then I would select 1. 

Yes, they can connect natively. You can manage data and then put it all in different services like it is showed next. As you can see you can use SQL database, blob storate and also PowerBI. 

I think there might be a problem in the way you are stating the problem. You say that you test data doesn't have two fields, but that can not be correct. You have to take all your data and split it into 2 groups, the training set and the test set. In a proportion of 80%-20% or 70%-30%. Then you train your algorithm with the data in the training set, and test the accuracy of the model with the data in the test set. The accuracy you get is the probability that your model is correct. Or said in another way, the next time you use your model to predict a sale, the accuracy is the probability that your prediction is real 

You want to see which variables best describe your output $Y$. First step plot your features against your output to see how they are distributed. 

Convolutional layers are useful for images because they take into consideration the neighborhood of pixels. However, for labels like gender and handedness a convolutional layer may not be particularly useful. However, after the convolutional layers you usually tend to place some densely connected layers. It is there that you may want to add these additional features. When you reshape the 2D matrices which results from the convolutions, you can concatenate the additional features, then feed this new vector to your Dense layer. 

For each cluster as determined by the unique labels we will plot all the values associated with it as 

You will need to do some web scraping. This resource may be useful to you, it explains how to do web scraping with Beautiful Soup, a very common python package. However, web scraping is never an easy task because webpages are rarely formatted in a friendly way. 

I think mxnet is one of the best options if you code in R. They have an R wrapper but the core is in C++. They have several examples in the web. One of them is the character recognition with MNIST database. They have support for multi-gpus and also for Spark. 

SVM can be used for classification (distinguishing between several groups or classes) and regression (obtaining a mathematical model to predict something). They can be applied to both linear and non linear problems. Until 2006 they were the best general purpose algorithm for machine learning. I was trying to find a paper that compared many implementations of the most known algorithms: svm, neural nets, trees, etc. I couldn't find it sorry (you will have to believe me, bad thing). In the paper the algorithm that got the best performance was svm, with the library libsvm. In 2006 Hinton came up with deep learning and neural nets. He improved the current state of the art by at least 30%, which is a huge advancement. However deep learning only get good performance for huge training sets. If you have a small training set I would suggest to use svm. Furthermore you can find here a useful infographic about when to use different machine learning algorithms by scikit-learn. However, to the best of my knowledge there is no agreement among the scientific community about if a problem has X,Y and Z features then it's better to use svm. I would suggest to try different methods. Also, please don't forget that svm or neural nets is just a method to compute a model. It is very important as well the features you use. 

An amazing reference for the inner-workings of the gradient descent variants can be found here. Gradient descent Vanilla gradient descent is defined as $\theta_{t+1} = \theta_t - \eta \nabla_\theta J(\theta)$ where $\theta$ is the set of parameters which define our objective function $J(\theta)$. The learning rate is $\eta$ and $\nabla$ defines the gradient operator. We will extend this function. Adding Momentum Firstly, if we throw a ball down our cliff it will get some momentum as it falls. This will cause it to tend towards the steepest part of the gradient and left to right oscillations would be minimized. We can do the same to the gradient descent algorithm by introducing momentum $\gamma$ as $\nu_{t} = \gamma \nu_{t-1} + \eta \nabla_\theta J(\theta)$ $\theta_{t+1} = \theta_{t} - \nu_t$ This is a good result but we can still do better. Adagrad It is preferable to have a distinct learning rate for each parameter such that we can adjust their impact on the gradient based on their past values. This is achieved by weighing the gradients by a sum of squares term of their past gradients up until the current time $t$. The matrix $G_t$ is a diagonal matrix where its entries are precisely this sum of squares of past gradients for each parameter. The Adagrad update is thus, $\theta_{t+1,i} = \theta_{t,i} - \frac{\eta}{\sqrt{G_{t,ii} + \epsilon}}\nabla_{\theta_t}J(\theta_{t,i})$. However, this method suffers from diminishing gradients when the sum of squares matrix makes it such that the past values are all too small and cause no more updates to be possible. Or for parameters which may be impactfull in the future to lose all relevance due to their low gradients at the start. RMSprop This is a solution to the diminishing gradients problem. We will use a running average of the past gradients until a time $t$ as defined by $E[g^2]_t$. The update rule is thus $E[g^2]_t = \gamma E[g^2]_{t-1} + (1-\gamma)g^2_t$ $\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{E[g^2]_t + \epsilon}}g_t$ where $g$ is the gradient for a single parameter. Adam In addition to keeping track of the running average of the past gradients we will also keep track of their second order moment. $m$ is the mean and $\nu$ is the moment. Thus we keep track of them as follows $m_t = \beta_1 m_{t-1} + (1-\beta_1)g_t$ $\nu_t = \beta_2 \nu_{t-1} + (1-\beta_2)g^2_t$ We will correct these terms to remove their bias that tends towards a zero vector as $\hat{m}_t = \frac{m_t}{1-\beta^t_1}$ $\hat{\nu}_t = \frac{\nu_t}{1-\beta^t_2}$ Finally, the update rule is $\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{\nu}} + \epsilon}\hat{m}_t$ 

I don't think theano have spark support yet, however there are at least 2 deep learning libraries that are good with that. The first one is mxnet, they have support for spark, R, python and C++. Another option is deeplearning4j, made in Java with direct access to Spark. 

I would suggest Recurrent Neural Nets. They are good for time series, however they need a huge dataset to get good performance. Here you can find an implementation in torch. 

Microsoft announced a couple of weeks ago virtual machines in Azure with GPUs. They use K-80 NVIDIA cards. The biggest machine has 4 GPUs and 224 GB of ram. Good to play with deep learning :-) 

The main difference between supervised and unsupervised learning is the following: In supervised learning you have a set of labelled data, meaning that you have the values of the inputs and the outputs. What you try to achieve with machine learning is to find the true relationship between them, what we usually call the model in math. There are many different algorithms in machine learning that allow you to obtain a model of the data. The objective that you seek, and how you can use machine learning, is to predict the output given a new input, once you know the model. In unsupervised learning you don't have the data labelled. You can say that you have the inputs but not the outputs. And the objective is to find some kind of pattern in your data. You can find groups or clusters that you think that belong to the same group or output. Here you also have to obtain a model. And again, the objective you seek is to be able to predict the output given a new input. Finally, going back to your question, if you don't have labels you can not use supervised learning, you have to use unsupervised learning. 

We will repeat this vectorization process for each comment. We will then be left with a matrix where the rows represents each comment and the columns represent the frequency at which these words appear in the comment. We will call this matrix $X$ it is our dataset. Each of these instances will also have a label we will call that vector $Y$. We want to map each row in $X$ to it's label $Y$. Assume this is the data you have for 40 instances in a training dataset for the cars/hotel comments. With it's corresponding label on the right. Label 0 is for cars and label 1 is for hotels. 

Firstly, in all the linear separator algorithms such as linear regression, logistic regression and the perceptron, adding the bias is as simple as adding a feature column consisting of all 1's. Then the third weight that will be trained will act as the bias $b$. 

A good way to measure the difference between two probabilistic distributions is Kullbak-Liebler. You have to take into account that the distribution has integrate to one. Also you have to take into account that it's not a distance because it's not symmetric. KL(A,B) not equal to KL(B,A) 

First of all I think that what you're trying to do is very difficult. A research paper success depends not only of the words but the math, the time when it was published, the journal, etc. You have many features you should take into account. I would try deep learning. For input, I would add all the features above and perhaps more. As the output I would use the number of citations or maybe something more sophisticated like a custom function of popularity. It's not the same to be published in Nature than in other journals. 

Just to add some more resources. Recently there was a paper studying the differences between several packages of neural networks and deep neural networks. Here you can find the information. It looks that Torch and TensorFlow are the winners. Note: not all of them are in python. However, I posted it to open the discussion.