Practical Issues in Database Management - Fabian Pascal. Chapter 7 provides the best and most understandable explanation I have found. Joe Celko's Trees and Hierarchies in SQL for Smarties, Second Edition. This is an entire book on the topic specific to the SQL standard. Enterprise Model Patterns - David Hay. A book about patterns common to all organizations (unfortunately the ER Diagrams are presented in UML but that can be overcome) there are several examples of network structures. 

Other Points The diagrams I show are created using Oracle's Data Modeler tool which is a free download and very powerful. You can create DDL directly from it. One thing you want to consider that I didn't discuss is that, if you use surrogate keys as I did here, you want to also define an alternate unique key for each table using more natural columns. In the case of Person that could be Name. This way, you won't accidentally add the same person twice if your list gets long. A second thing is you have to think about if you have one value per statistic or more than one. For example, Weight and Steps are good examples of a single statistic for the Measurement. Today you record that I weight 200 pounds and walked 5000 steps. But for something like Bad Foods Eaten there could be more than one. If that is the case you have what we call a repeating group. To resolve this, you either have to create a fixed number of columns - one for each bad food - or create a new table that will be a child of the Measurement that will have one row for each bad food. Examples Here is an example of the way your data would look using the column approach (skipping the surrogate keys for simplicity of display): 

This means that the value of A and B determines the value of C. Perhaps A is employee number and B is Dependent Number, and C is the dependent's name. So in this sense they apply to each and every table in each and every schema in each and every database. Some good references on relational theory are Fabian Pascal's Practical Database Foundation Series and Chris Date's book Relational Theory for Computer Professionals. 

One approach you could take is to design a table that implements a stack into which can be placed the components of the logical expression in reverse-polish (postfix) notation. In such a design, the components of the predicate are each inserted into the table with an order number to specify the reverse evaluation order. To evaluate them a query is written to read them out by order number, apply the operators when encounter, and store the intermediate result to be applied the next. The use of the reverse-polish notation eliminates the need for parenthesis and can support nesting of expressions to any level. Here is an article specific to SQL Server that shows such a solution. Now in this article the author is developing a more complex example of implementing business rules to determine whether an employee is authorized to execute a particular operation. It can easily be adapted to your use case as in both cases what is being implemented is a logical expression storage and evaluation engine. I hope this can point you in a direction that will enable you to more easily handle the complexity that parsing nested logical expressions bring! 

The perceived simplicity of a single table with an abstract reference is seductive but ultimately proves to be the opposite of simple due to the above issues. I think the appeal results from a misapplication of the principle of cohesion. The cohesion here is really between the documents and the entity which they are about, not their structural similarity in that each table stores a "document." Applying the principle of cohesion with respect to structure instead of functional meaning can get you into some other ill-advised practices such as OTLT and EAV. My recommendation is to stick with the design you already have. It is correct and doesn't need "improved." 

Employee manages Department Department controls Project Employee assigned to Department Employee works on Project Employee has Dependent 

This can easily be done by using cascading keys that overlap. Here is an example using the Oracle Data Modeler (note there is a bug in this tool or a configuration issue as the Provider_Feature table should show each column as PF meaning both PK and FK): 

This is a great question. Normalization beyond BCNF is extremely hard to understand. Hopefully I can provide an answer that makes sense. I struggled with these concepts for over 20 years before finally making sense of them thanks to Fabian Pascal's Practical Database Foundation Series. The example provided is an R-table that looks like so: 

The relational design is clearly superior to the hierarchical design used in the NoSQL example. A database schema designed using relational principles does not favor one access path over another. Each table represents a real world entity type and through the use of relational algebra queries of arbitrary complexity can be constructed to answer not only current questions of the data but any future questions you think of that haven't yet been conceived. A hierarchical schema imposes a single structure upon the data - in this case work packages contain other work packages, the worker, and the parts, and the files. If you want to ask questions of the data not aligned with this single view it will be much more difficult if not impossible. Furthermore, the relational model includes a data integrity component to ensure data is consistent with respect to rules declared to it approximating the real world, whereas systems, like a NoSQL system, using a hierarchical or network model do not provide these. 

Physical Level The physical level is the sole place where performance, disk and memory storage structures, and scalability live. I do not specialize in this area but can say that mastering this level is primarily an endeavor to master the given DBMS you are working with. The DBMS is such a sophisticated piece of software that you are fooling yourself if you think you can master the whole thing, much less master more than one. For this reason I would recommend sticking with the conceptual and logical levels and creating a sound logical database design, and then working with a really good DBA who specializes in the target DBMS to develop the physical design. One really good source however for physical design that lays out the fundamental topics and options common to most DBMS' is Sam Lightstone, Toby Teorey, and Tom Nadeau's Physical Database Design. 

In this same way, a data practitioner who doesn't understand the fundamentals of relational theory cannot be in as complete command of the technology as they can be with that understanding. Some great references on relational algebra are SIRA_PRISE's Introduction to the Relational Algebra page, and CJ Date's SQL and Relational Theory. Date's book shows the practicality in understanding relational algebra so that you can write much more accurate SQL queries. SQL has many quirks and pitfalls and having a sound grasp of how it works vs. the original relational algebra operators really helps realizing where the pitfalls are and avoiding them. 

Creating an ERD also helps you start to clarify the requirements. For example, I made the key to the section table the resume and a sequence number. This means you could have 2 or more sections on a resume with the same type. Does this make sense? Or does it make sense to say there can only be one section of each type per resume? Second, you want to think about what uniquely identifies a row in each table. For example, I have used a Resume Number for the key to the resume table. Wouldn't it make sense to say that each row would be uniquely identified by the User Name and the resume name? This would prevent the storage of duplicate resumes. DDL Generation Once you nail down all the business rules, you can use Oracle Data Modeler to generate your DDL. While it doesn't support mySQL, you can still generate it and get the base structure and then correct for mySQL specific syntax. Here is a sample of what I quickly generated from this model: 

Row Wise A second option would be to make the statistic more generalized by having a Statistic Type table. There would be a row in Statistic Type for each kind of statistic you want to record. Then, you would associate Measurement to the Statistic Type and record the Value. The advantage of this approach is that you can easily add more statistics, and you only have to record just the values for the statistic types you measured. The disadvantage is that this is more abstract and complex, and you have to use a generalized data type that can support all of the various units of measure, or you have to create a mutually exclusive set of columns each of a data type matching a statistic type. If you go with the first approach, you can really turn it into a science project in order to support all the various data types and still ensure data integrity.