You can use SSH's feature for that. If you already have SSH set up (very likely) then this will be much easier than the alternatives for just connecting two hosts. 

An antivirus program should handle this case, and there are of course plenty of those for Windows. Since PDF files are nowadays a pretty common attack vector, PDF file scanning is a standard feature of these products. It won't give you a contains code/does not contain code answer, but it will block or remove files that contain known or potential exploits. 

contains a lot of information. Many source files under that directory also contain big comments. That's about as up-to-date and relevant as you can hope for. 

The error message refers to a Unix-domain socket, so you need to tweak your invocation to not exclude them. So try it without the option : 

The background of this is: The old Postgres system used the PostQUEL language and used a data type named (because someone thought that was a good name for a type that stores text). Then, Postgres was converted to use SQL as its language. To achieve SQL compatibility, instead of renaming the type, a new type was added. But both type use the same C routines internally. Now, to some degree and in some places, is hardcoded as a default type, in case nothing else can be derived. Also, most functions are only available as taking a argument or returning . The two types are binary compatible, so casting is a trivial parse-time operation. But using is still overall more natural to the system. But aside from these fine points, there is no noticeable difference. Use whichever one looks prettier to you. ;-) 

and then look for the processes. Since listening on is the default for PostgreSQL, chances are your package decided to use a nonstandard port. 

This is probably not going to work without some installation routine that patches the pg_hba.conf file for the local situation. You could look for existing lines that initdb created in the initial file and just modify those, if necessary. initdb won't put in an IPv6 line if the system doesn't support IPv6. In PostgreSQL 9.1 (not yet released), you will be able to put host names into pg_hba.conf. So just writing might work on most of your target systems. 

The intrepid release is simply not on the archive servers. I don't know the reason for that. But if you want to proceed, you will probably need to upgrade to something newer. 

When the crash is related to a storage failure, even momentarily, the kernel sometimes chooses to remount the file system read-only to prevent further damage. You can undo that using 

You can also use to discover what hosts are up, do a reverse DNS lookup on the hosts that it finds, and then filter on that. has a bunch of options to fine tune the output, but perhaps start with 

No, this is not possible. Generally, the cache algorithms are smart enough to keep the most used tables and indexes cached. 

It looks like you are mixing Debian and OpenSCG packages. That won't work. Install PostgreSQL 9.1 from Debian. 

Autovacuum is on by default. Reindexing and clustering aren't really baseline maintenance tasks. You can do them when needed to improve performance or (reindex) to fix index corruption, but they are not something you would blindly do regularly. Log rotation is taken care of automatically by the Debian packaging framework. So as a baseline, you don't need to do any of these things yourself. What you should actually be doing is setting up backups (for a start, run daily from a cron job), and monitoring (something like Nagios and Munin, perhaps, for a start). And then based on such monitoring you will regularly tune server settings and try to improve query performance. 

I probably wouldn't go for this without further evidence of MySQL-specific knowledge or qualifications. You wouldn't hire an experienced Windows admin to take care of your Linux boxes either, would you? With experience in a different environment, you would know most of what to do, but probably not much about how to do it. That said, MySQL offers a number of trainings and certifications, so maybe the candidate should take a round of those. That should get him up to speed with the product specifics. 

Copy all your configuration files to . Make any adjustments that you want to make as part of the upgrade. Then you should be able to start the new server using 

As you have figured out, the Debian packaging of PostgreSQL requires to be located in that place, because it starts there to figure out where everything else is located, including the data directory and the other configuration files. Make sure your points to the right places for those. Next time, use , and it will figure all of this out for you automatically. 

I suggest you take the specific issues to the respective authors/communities of the software packages you are dealing with, try to resolve them there, and learn from those experiences. While there is various literature around to learn about "Linux", issues like in your examples are quite specific and can't really be learned a priori. 

You can tweak this a little to start automatically or in the background. See the documentation. Then you can connect: 

In order to add SSL support to your installation, you need to rerun the , , sequence with the appropriate options. But you don't have to delete anything. Just install the new binaries over the old ones and restart the server. 

Aside from the valuable specific advice given in other answers, the canonical answer to this sort of question is to read the release notes. For example, in an upgrade from 8.2 to 8.4 you should read the release notes 

to download the source package. Inside the source package, the configure options are in the file . Edit that, and then rebuild (with for example). 

The tutorial only works when you install PostgreSQL from source. On your Ubuntu installation, access control was already set up, so not everything can log in (hence "authentication failed"). To follow the tutorial, try logging into the user () and then follow the steps in the tutorial. 

This is looking for the client certificate files. The server certificate files belong in , as you correctly pointed out. The client certificate files are looked for in . You are probably logged in as the user, who happens to have a home directory of , so that's how you got that path. So either put the client certificate files where they are looked for, or log in as the correct user, or reconfigure the server so that it doesn't require client certificates. 

Yes, it's a straight file-level copy of the template database, so whatever was in there will be in the new database. 

You need root access for the second step. Not as convenient as the option, obviously, but works in a bind. Could be scripted, if necessary. 

Unless you want to become a hardware expert, I'd first of all suggest software RAID. It's much easier to set up and manage than hardware RAID, and for a medium-traffic site like you describe it certainly performs just as well. The appropriate RAID level depends on your data volume. If you can afford it and the data fits, you can't do much wrong with RAID 10. If you need to go higher volume, then RAID 5 is the next best option, although you will sacrifice some write speed, especially for your database. If you're concerned about that, it'd be best to try it out with your hardware and software. 

Have you considered Bucardo? It's asynchronous multimaster. It hasn't completely caught on and is not a general solution, but it might be worth a try. 

The OOM killer on Linux wreaks havoc with various applications every so often, and it appears that not much is really done on the kernel development side to improve this. Would it not be better, as a best practice when setting up a new server, to reverse the default on the memory overcommitting, that is, turn it off () unless you know you want it on for your particular use? And what would those use cases be where you know you want the overcommitting on? As a bonus, since the behavior in case of depends on and swap space, what would be a good rule of thumb for sizing the latter two so that this whole setup keeps working reasonably? 

This works on Linux/GNU. On other systems you might need to go a more complicated route, or install the GNU coreutils. 

You could use PL/Proxy to set up a frontend, and then write some functions that run the queries that you need. It could be quite tricky, though. You should plan this architecture carefully. 

If you don't want to actually check if the hosts exist, is to just loop through the IP addresses and do a reverse lookup. The following recipe works on Linux and FreeBSD; I don't know how the program on Mac OS X behaves: 

I don't know how it works on Mac, but on Linux the EnterpriseDB installer leaves an uninstall script, possibly linked to some menu entry. You should look for that. The two installations should be able to coexist peacefully. But if you don't need the installation, you should consider removing it, as a matter of course. In general, I do recommend using Macports for PostgreSQL and other software. 

If the two servers have the same architecture, then you can just copy the data directory. Normally, you'd have to shut down the server to do that or set up WAL archiving. But for creating a testing database, it might be sufficient to do it anyway and clean up your copy with afterwards. You can use rsync to reduce the amount of data to be copied in subsequent runs. If the two servers have a different architecture, then is the only route. 

Vacuum and autovacuum replicate like any other write operation. (Well, they are obviously somewhat special internally, but as far as your question is concerned, they are normal write operations.) Running vacuum or autovacuum on the slave doesn't do anything and is not necessary. 

The canonical answer to this is: Use what you are familiar with. Since you are not familiar with either, it's a really close call. So here is a semi-philosophical argument: Debian is an original community distribution. Do you value a direct feedback channel and the possibility to shape the future of what you use? CentOS is a community distribution based on a commercial distribution based on a community distribution. Do you value something that has been polished three times but where the channel to the original author is perhaps more murky? But really, it's very hard to distinguish these options. 

Follow the instructions on $URL$ to set up your APT sources (you may have already done this), then do 

I think this is best answered by trying it out in your particular application. It's not much per individual savepoints, but when you are talking 100k+, the effects will accumulate and other factors can come into play as well. 

Nothing production grade, but for the curious, there is/was a research project to implement something like that. Search for "PostgreSQL index advisor". 

I suggest rsync. It's the standard tool for this sort of thing, and I gather it works on Windows as well. 

I would guess that the server is actually listening on the socket rather than the that your client is attempting to connect to. This is a typical problem when using hand-compiled or third-party PostgreSQL packages on Debian or Ubuntu, because the source default for the Unix-domain socket directory is but the Debian packaging changes it to . Possible workarounds: 

If you have a lot of contention, there is some anecdotal evidence that performance doesn't get better or degrades after about 32 CPU cores. It is difficult to get definitive results in that area, though. If someone knew the answer, they would probably also be able to fix the problem. If you think your application will be pushing the limits on this, I suggest running tests yourself. pgbench might help you get started. It doesn't look as though it will happen anytime soon. Don't plan on it. This is highly dependent on what you are doing. If you have an OLTP application with lots of concurrent clients, it won't matter much. If you have more of an OLAP style usage, then it will suck because you might only be using one or two out of many CPUs, and especially your sorts could conceivably be faster. 

The instructions you are looking at are for from-source installations. When using APT, just run and everything will be taken care of for you. Note that this will restart the PostgreSQL server, so don't do it when you can't have downtime. 

PostgreSQL data files are operating system and CPU architecture specific. So first of all your Linux and Windows systems would need to be of the same architecture. And then the data layout that the compilers use would need to be the same as well. I have tried this in the past between Linux and Cygwin, and it did not work. So I would be very surprised if it worked between Linux and native Windows. In general, don't count on it. 

In case of a failure, you have to restore your database from a backup. It will not (in general) recover by itself. 

Use the built-in binary replication. Unless you have a reason not to use it, that should be the default choice. Reasons not to use it include the requirement to do partial replication and cross-version or cross-platform replication. I don't think that applies to you. 

I would try the equivalent of to see whether and where it looks for and what it does with it. The option is unnecessary and unrelated to your problem. 

You can't use pg_upgrade to upgrade from 32-bit to 64-bit, or in general from any OS/CPU/platform to any other OS/CPU/platform. The data files are platform dependent, and pg_upgrade works by just copying (or linking) over the data files unchanged. So this is never going to work. Your options at this point are dump/restore, or using a logical replication system to move your data (Slony, Londiste, Bucardo). 

Why not just set up a cron job that runs some variant of ? Perhaps you mean that you only want the behavior for one of your packages, but I think it's not worthwhile to address that specially. In a stable release of Ubuntu, should very rarely pull in new packages, and if it does, there is probably a very good reason for it, such as a security update. 

This sends a mail to root whenever there is a package to be upgraded, which in a stable release means mostly security updates. As a general matter, you should of course redirect the root mail to somewhere you can read it. I have been using this for some years in a personal environment, and it works great. 

That basically means the security of the box itself becomes your last line of defense. So make sure your logins are secured and the applications that are exposed to the outside of the box don't have any exploits. It's possible to do this; after all you have to put the fences somewhere, but usually having more than once fence is a good idea. It depends on the nature of the applications and the data, who the attackers might be, and what value all of this has to them. 

1) If you insist on storing backups in subversion, then there is nothing wrong with this approach. It is strange, though. 2) You should keep a checkout around, place the dump into the working directory, and run and as appropriate before committing. 3) If you run the commands as shown from a shell script, there should be no overlap. 

Since most file systems don't keep checksums (notable exception is ZFS), a checksum in the database would still be useful. In PostgreSQL, the WAL records are checksummed, but not the data pages, so you won't be able to detect if the OS or hardware has shredded your data. Implementing a checksum for data pages is a planned feature in PostgreSQL, but there are some very difficult problems with the implementation if concurrency is to be maintained. Search for "postgresql block level crc" or something like that to learn more (or become totally confused). 

You will probably want to apply one of the suggested workarounds, but the answer to your question is no(t easily). If you want to change how sorts, and none of the special sort orders offered by the command-line options suit you, you will need to define your own locale. See . 

The first three steps are definitely within Puppet's realm. Installation is a resource, initdb can be done with if the package doesn't do it itself, and starting the service is a resource. But populating the database is probably not a good use of Puppet. You could probably do it with a lot of custom code (e.g, a separate resource type for a table, a function, etc.), but it would be a lot of work and somewhat unchartered territory. I suggest you look for a different way to do that. 

It doesn't make sense to start a service in run level 0 or 6, which are the levels for shutdown and reboot, respectively. There is apparently some magic in update-rc.d that attempts to prevent this case and misparses the arguments in that case. If you use more sensible run levels for start (probably 2 3 4 5), it will work better. 

The package description is "PostgreSQL libraries and clients". So the idea is apparently that you use this if you don't want to install the full server, whose package name is .