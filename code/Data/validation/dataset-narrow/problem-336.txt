Here's the SQLFiddle with the clause commented out to show this would only return records 5 and 2 in the event this logic is indeed what you're after. 

As much as I'd love to fall back to consistent SSIS development practices, these packages are mainly inherited, so I'm stuck with a mixed bag of approaches to work with. This screenshot was taken from within Visual Studio 2012, but I believe the issue exists on a number of other versions as well. 

A few notes here: Table Partitioning on the New Table This new table is also going to be partitioned based on your requirements. It's a good idea to think about future maintenance needs, so create some new file groups, define a better partition alignment strategy, etc. My example is keeping it simple, so I'm throwing all of my partitions into one Filegroup. Don't do that in production, instead follow Table Partitioning Best Practices, courtesy of Brent Ozar et. al. Check Constraint Because I want to take advantage of Partitioned Views, I need to add a to this new table. I know that my insert statement generated ~440k records, so to be safe, I'm going to start my seed at 500k and create a defining this as well. The constraint will be used by the optimizer when evaluating which tables can be eliminated when the eventual Partitioned View is called. Now to Mash the Two Tables Together Partitioned Views don't particularly do well when you throw mixed datatypes at them when it comes to the partition column in the underlying tables. To get around this, we have to persist the current column in your current table as a value. We're going to do that by adding a Persisted Computed Column, as follows: 

Now that you know where this query is coming from, the next step to take will be based on if you think this process is critical or not. If I were in your shoes, I would setup a Login Trigger that won't allow this connection in and see who complains or what tool starts erring out. Just a word of warning, a poorly defined Logon trigger can make your life maddening, but hopefully this example will do the trick. I modified an example of a different restrictive trigger based from the one found in this post: 

I think you're running into a key limitation of distributed queries (e.g. four part notation). As per this MS blog post: 

This has a scent of a parameter sniffing problem. See what I did there? Punny, right? Anyway, Paul White, wrote up a great article on the various approaches available to resolve this issue, so I'm not going to go into detail when you should just read his article instead. What you can try quickly though, because this is an ad-hoc statement, is to force a recompile using the hint. If this fixes your issue, you definitely are running into what I suspect. However the best way to properly resolve this is to convert your statement into a Stored Procedure and then execute the Stored Procedure passing in the date parameters. A parameterized Stored Procedure won't fall prey to the current issue and should be reusable as well. 

If you're only using TCP/IP to connect to your instances, you only need the ports specified. The Instance Names are used when connecting to the SQL Instances via the Named Pipes protocols. Sadly the MS article doesn't come right out say which format is required for which protocol, but it is derived from (many tests in my environment) and the following MS article sentance: 

Hopefully that points you in the proper direction. If not, please update your question with some more information such as the error you receive, the installer you're using, the installer options, etc. 

Let's say for argument's sake that I'm getting close to the limit (even though I'm obviously not). Because I don't want run out of valid values in the column, I'm going to create a similar table, but use for the column instead. This table will be defined as follows: 

If you want to improve the speed of your tlog restores, you will want to look into the bulk-logged recovery model. I would never suggest running a database under this recovery model exclusively, though switching to it during your maintenance window will help significantly in your case. The key here is to be very explicit on the approach you take when switching recovery models. From the source, anytime you wish to utilize the bulk-logged recovery model, you should follow these steps: 

The query was gratuitously lifted from this MSDN blog: $URL$ In addition, make sure your JDBC driver supports the ApplicationIntent=ReadOnly parameter. 

Even though it's a bit late, I'm going to field a response with hope that it helps or at least spurns some additional ideas/commentary on this issue because I think it's a good question. First, and I don't know if you're doing this or not, but please don't assume that high fragmentation levels on the index are always going to cause poor performance. Stale statistics (e.g. sys.dm_db_stats_properties) and high amounts of white space per page (i.e. avg_page_space_used_in_percent column in sys.dm_db_index_physical_stats dmv) hold more relevance regarding performance issues than fragmentation alone. Yes, highly fragmented indexes will generate more read-aheads and you typically do see stale statistics and higher levels of white space per page coupled with fragmentation, but fragmentation isn't directly tied to query plan optimizations nor how much memory loading the index from disk will actually consume. Query plans are affected by statistics and your memory footprint bloats with more white space. For instance, an index that is 99% fragmented but has less than 5% avg. white space and up-to-date statistics is likely not causing you drastic performance issues as compared to either a bad execution plan as a result of stale statistics or constant paging of an index that's too big to fully fit in memory because there's a significant amount of white space present per page. If fragmentation is truly an issue, you can reduce it, ONLINE, by issuing an statement as identified by Dan Guzman in the comments. This won't create as streamlined an index as a operation will, but it will reduce your fragmentation. The key here is to identify windows of lower usage on your database and run it then. This could be 15 minutes or multiple hours, obviously the longer the better, but the key here is this operation doesn't rollback and retains any progress made even if you kill it mid-execution. If, in a perfect world where your fragmentation was eliminated, would it make more sense to utilize partitioning on this table? Azure SQL Database does allow for table partitioning and Microsoft has a great article outlining some Partitioning strategies for Azure SQL Database. If your data is non-volitile, partitioning it may help reduce maintenance needs, and if coupled with Table Compression, you may even be able to reduce your overall storage footprint as well. Alberto Murillo's earlier answer alludes to utilizing Horizontal Partitioning based on a data region, and this approach may help create some maintenance windows for you as your data would be more regionally specific instead of global. Transitioning to a partitioned table won't be easy with your current absence of maintenance windows, but you may be able to utilize an approach outlined by Maria Zakourdaev which uses Partitioned Views over the top of your current table and a new partitioned table to start partitioning future data. As time goes on (and hopefully your old data is purged), you can eventually transition fully over to the partitioned table. Again, I don't know your data or application, but maybe this approach is something you can employ.