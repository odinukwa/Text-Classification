You should delete and make the primary key clustered instead. The index that is created to support the primary key will do the job for you when filtering on . 

The execution plan for two other obvious cases to test, more rows in B than A and equal number of rows in the tables, also shows the exact same execution plan for the queries. Update Before query optimization takes place the query goes through a simplification phase. You can see what the logical tree looks like using the trace flag 8606. The Input Tree for the queries are clearly different but after simplification they are the same. Ref: More Undocumented Query Optimizer Trace Flags and Query Optimizer Deep Dive â€“ Part 2 Input tree and simplified tree for query using distinct: 

To change the value in a XML column with decimals included you need to first extract the value as a and write it back as an with the appropriate precision and scale. Something like this. 

SQL Fiddle calculates the for each team and match. finds the previous ordered by . generates a running sum ordered by keeping a a long as the is the same as the last value. Main query sums up the streaks where is . 

According to Craig Freedman the order of execution for the concatenation operator is guaranteed. From his blog post Viewing Query Plans on MSDN Blogs: 

If you would like to create the view dynamically from the XML Schema you can extract the colors with this query. 

You are shredding on the node so specifies the current node value and and if you use to specify that you want an attribute you don't have to care about the parameter to . 

It executes in 406 milliseconds regardless of where it is executed from. Looks like there is no communication with the client in the loop. 

By default the SQL Server does not create a node for NULL values so if you change your query to use two CASE statements and return NULL when there is no match you will get the XML you are looking for. 

Dropping a constraint requires a Sch-M (Schema Modification) lock that will block others to query the table during the modification. You are probably waiting to get that lock and has to wait until all currently running queries against that table are finished. A running query has a Sch-S (Schema Stability) lock on the table and that lock is incompatible with a Sch-M lock. From Lock Modes, Schema Locks 

What you should have instead is a scan or seek operator on the internal table used for the XML index. 

You can filter out the rows you need to consider from the target table in a CTE and use the CTE as the target in the merge. 

If you have data in your column and it makes sense to convert the values to a float value you can rename the column, add a new column, move the data using and then drop the old column. 

Testing this on my computer Client Statistics reports a Total execution time of about 750 millisecond. For comparisons the sub-query version takes 3.5 seconds. Extra ramblings: This algorithm could also be used by regular T-SQL. Calculate the running total, using not rows, and store the result in a temp table. Then you can query that table with a self join to to the running total as it was 45 days ago and calculate the rolling sum. However, the implementation of compared to is quite slow due to the fact that is needs to treat duplicates of the order by clause differently so I did not get all that good performance with this approach. A workaround to that could be to use another window function like over a calculated running total using to simulate a running total. Another way is to use . Both had some issues. Finding the appropriate index to use to avoid sorts and avoiding spools with the version. I gave up optimising those things but if you are interested in the code I have so far please let me know. 

I have not found a simple way to just modify the statement to work with anonymous simple type definitions. Simple repro of what you have: 

You can take the max value of to get the distinct count of A partitioned by B. To take care of the case where A can have null values you can use to figure out if a null is present in the partition or not and then subtract 1 if it is as suggested by Martin Smith in the comment. 

require that the database compatibility level is SQL Server 2008(100). If they are executed in compatibility level (110) or (120) you will get 

I have no idea why you see that. The query you have returns a count of 1 for the departments without employees. SQL Fiddle 

Not sure I actually provided a definitive answer for you here but I thought this was too much for a comment. 

Split the string to get one row for each character, order it so numbers go last and all other characters go first and then rebuild the string. Code below uses a numbers table to split the string and the for xml path trick to rebuild it. 

This will scan the index from the beginning as long as is less than and it will make wonders for performance if you are looking at early dates and not so much for later dates. An even better option could be an index on with as include columns to remove the Sort operator from the plan. 

Same solution as the one provided by Aaron Bertrand when it comes to building the comma separated values but a bit different in connecting with the values in . SQL Fiddle MS SQL Server 2014 Schema Setup: 

The query cost calculated for a query is an estimate. Even in the actual execution plan it is the estimated cost that is shown. For some reason SQL Server Management Studio uses those estimated costs and comes up with a cost percentage for each query in a batch. Generally speaking that percentage has nothing to do with the actual performance of the query and should (in my opinion) not be used. Estimates are based on statics of the involved tables and columns but XML variables (and columns) does not have any statistics that describe what shape for form they have so for XML queries the estimates are even less useful then for other types of queries. Measure the actual performance instead like duration and IO. 

It is not possible to validate XML against a DTD file using SQL Server. You can validate the XML with a XSD stored in a XML Schema Collection using a strong typed XML variable 

Count the number of rows for every distribution and compare that to the number of rows returned for a distribution where you apply your filter. 

I have guessed the data type of to be you need to modify that in two places in the script if it is something else. The XML parameter has to be a instead of as since the datatype is not supported with remote queries. Table Valued Parameters is not supported either so that is not an option here. The query plan of this query is not very interesting. It will just be a Remote Scan. If you want to know what the plan looks like on the remote side you have to capture the plan there. 

CURRENT_TIMESTAMP returns the current datetime something like . returns a string with only time . The second parameter of DATEDIFF takes a datetime so your string is implicitly converted to a datetime. In lack of something better to use for a date SQL Server uses so your second parameter ends up to be and the function returns the number of minutes since the beginning of the previous century. Remove the convert in the second parameter and it should work just fine for you. 

The query costs are based on estimates even in the actual execution plans. They do not tell you how efficient the query actually was. The estimates are in turn based on statistics and those can be outdated giving you estimates and costs that are wrong. The estimates for XML queries is always wrong. There is no statistics generated for XML columns and there is certainly no statistics generated for XML parameters or variables. Have a look at this rather simple XML query. 

The result from the Table Valued function is returned in the or column. The expression checks by using to figure out where it should be fetched from and then converts it to your desired datatype. 

Note: Your trigger will fail if you insert more than one row at a time. The trigger is executed once per insert statement, not once per row.