Teach it as a pure concept and not as a mathematical formula. Big O simply describes how your algorithms runtime behaves as your data increases. Let's say you are looking for a certain item out of a pile of 10 items. Let's look at an algorithm which is essentially "pick up each item and check it, and set it aside if it isn't the one and keep picking". That algorithm may be fine and perform within a runtime that meets expectations, but as the number of items in the pile increases, the runtime does too. If the number of items increases to billions, the runtime may become unacceptable. In this example, the Big O is basically linear. As the number of items increases the runtime increases in a linear fashion. It is important to understand the Big O of your algorithm from the start because that can help you avoid starting out with a bad algorithm. To illustrate that, now consider that we started with an algorithm that just inspects one item, checks to see if it is the one and if not throw it back into the pile, then repeat. In other words, we don't set the ones we know aren't the item aside. Now our algorithm is very inefficient and has an exponential Big O because there is potential to inspect items more than once. It could also happen that the algorithm never finishes at all, and the likelihood of that happening increases as the number of items gets humongous. If we happened to start with that idea, and pause to understand the Big O, we'd have started with the better algorithm. Searches of unmanaged data should be linear, meaning the worst scenario in searching n items is that you have to pick and examine all n items. Now you move into ways we get better than linear performance. If you have control over how the pool of items you will search is organized, and you can insert them in a sorted fashion, then your search can be better than linear. Or, if you take the time to index the items, your algorithm can be much better than linear. A good example is a library -- books are arranged by section, then by author. You don't have to search every book in the library to find the one you seek. The library also has a card catalog, so you don't have to search at all. But there is work/time involved in organizing and maintaining the library and its catalog. But it makes sense to accept this overhead because the volume of books that come in is much, much lower than the volume of searches. It also takes space. An organized library has a larger physical footprint than it would have if the books were not organized for better searching. The card catalog takes space too. A computer scientist weighs all the constraints and device the best strategy. Any algorithm for searching that is better than linear will slow down the additions and changes of items into the pool. If the items are added/removed/changed a lot more frequently than they are searched, then an index search may not be a good choice. I would expect high school students that understand these concepts to be able to answer questions on a test that propose a search algorithm, and provide the key constraints, and ask "is this a good strategy?" or "how might this strategy be improved and what down-sides should be considered?" When you can ignore the overhead that inserting/updating the data in an organized (or even indexed) fashion then it makes sense to target a better than linear approach. But when you can't ignore that because the volume of add/delete/updates is huge, and cannot be slowed down, then it makes sense to target a linear approach. I'd expect junior computer science students to be able to understand the considerations and make choices and explain their choice. If they can do that but cannot necessarily express it in formal mathematical language, I'd be OK with that. But as an educator you would need to align to whatever your district/state expects, because if ultimately then end up taking a standardized test that does not focus on concepts and does focus on mathematical formulas, that would not work. If your school aligns to the below, then what I describe above would work: $URL$ Because as 12th grade student: "Analysis using sophisticated mathematical notation to classify algorithm performance, such as Big-O notation, is not expected." 

Some personal experience. I have a PhD in computer science and have had a very successful 40-year career in software development. But often, when presented with a new concept, I have had great difficulty "getting it". I think the problem happens when you build an incorrect mental model of what is going on. I think the first time this happened was on first introduction to algebra: with x=3 and y=4, I was convinced xy had to be 34. There's nothing stupid or ignorant about this, and it doesn't mean the pupil will never overcome it: there's just a misunderstanding that has crept in somewhere, and I think it can be very hard for a teacher to discover where that misunderstanding lies, and correct it, because the teacher simply doesn't realise where the faulty step in the reasoning occurs. Later, when introduced to programming, I remember I found it really hard to understand function calls. I don't know now what the problem was, just that I had great trouble with it. (I also found it hard to grasp that xy was no longer 34, or even 12, it was now completely unrelated to the values of x and y.) In recent years I've taught a good number of XSLT courses. One of the areas where I find students really struggling is with recursion. The main frustration in teaching this is that you don't know why students are finding it difficult. Sure, it's a very abstract concept but it becomes so second nature that you no longer see the difficulties, and that makes it hard to help the students overcome them. 

Firstly, I think CS degrees vary widely in how "theoretical" they are. Some try to teach you about programming languages and even vendor-specific products that you are likely to encounter in the real world; others focus much more on teaching the fundamental principles. Personally (as someone who has always worked in industry) I would much rather have people who know the principles and the theory, because that knowledge will remain useful for much longer. But whether your background is theoretical or practical, on your first real project there are going to be a dozen technologies or tools you have to master that you have never seen before. Secondly, I think CS (and even Software Engineering) courses find it very difficult to teach the things you will only encounter on large projects: how do you grapple with half-a-million lines of code, how do you work on a team with 40 developers? Then there are the non-computing aspects of the job. How do you handle a boss who has made an incorrect decision? How do you handle customers who try to make design decisions for you, and do it badly? How do you handle an experienced and respected senior developer on the team whose knowledge has actually become rather rusty? How do you manage conflicting demands on your time? One of the things that I think you can only learn with experience is to acquire a mature attitude to risk. Many of the decisions in programming are concerned with risk. Should we release now, or write more tests? Is it wise to use a new open-source library that has just become available? Should we move forward to the latest version of X? We know the customer's stated requirements, but how likely is it that they will change? How much should we invest in extra resilience or security? In the end, these are the judgements that make a project succeed or fail. 

Despite all this, I still feel there is no alternative to a Paper-book. I used to hear it since long time, but there are some reasons behind it: 

A really good question as arguably MOOCs these days are preferred/easily accessible over conventional textbooks in CS. I had the experience of supplementing my curriculum with MOOCs in 2015 and since then, in every course, there are some supplements from them. Here are my observations about this experience: Clear Advantages 

It's not too much difficult to make non-C/C++ programmers understand about pointers if we take the above points and start from some basic examples like URLs being used to point to web pages etc. It has worked for my case (a great deal), hopefully it will help you and others too. 

C/C++ in PF, followed by either C# or Java (more frequent one) Universities which start right from Java 

E-books can breach privacy laws and myself being from sub-continent can better understand this situation as this practice is so common in universities and not discouraged even by majority of teachers. A paper-book is a physical entity and free from all the virus/hard-disk format/failure etc. our students are so common practitioners of. I had purchased Elmasri's DB book in my BS days but to date still using it. Reading through an E-book (either online of PDF, etc.) is prone to the reader losing his/her focus through a social media's notifications, some live match, even a little search on internet for finding the meanings of a word has often ended me up having 50+ tabs open on my browser. 

C/C++ Pointers are a nightmare for programming students - whether they have any Java/C#, etc. experience or not. It's the syntax of the pointers (the terrifying*) which is more difficult for students than its semantics. Every student of Java/C#, etc. knows that variables are stored in the RAM (and they have studied in Intro to Computing about RAM addresses etc. basics) 

Since the OP asked about functional programming, so I would highly recommend you to read "Learning Concurrent Programming in Scala" by Aleksandar Prokopec, (2014). All the examples for this book are available on GitHub to give you some idea of the book before you purchase it. C# I would recommend you to read "Professional Parallel Programming with C#" - its based on .Net 4.0 and I have no update about following editions (if any) C++ I think Pacheco's book would be my recommendation for sure Best of luck! 

Personal Conclusion One must ensure to have a fine balance between benefits/hypes of these MOOCs etc. and the related knowledge for the course. After few semesters of continuous use of MOOC etc. supplements, I have concluded: 

I have experienced the same problem and I found a pretty easy solution by dividing the lecture into two parts: Theory (usually taught on the board) and Practical Demonstration (on IDE). Now I come to the important point in your question (I had similar views to yours) on: 

I have been teaching Operating Systems basics in a class and after Processes, Child Processes and Threads, we have managed to move to "Process Scheduling" Now, ironically, as we know that it is not the Process which is scheduled, but the threads precisely (we are following Linux Ubuntu there). Here is what Silberchatz book (by the way, the title of the chapter is also "Process Scheduling") says about it: