To me, one of the most basic and surprising consequences of $\mathsf{NP}=\mathsf{coNP}$ is the existence of short proofs for a whole host of problems where it is very difficult to see why they should have short proofs. (This is sort of taking a step back from "What other complexity implications does this collapse have?" to "What are the very basic, down-to-earth reasons this collapse would be surprising?") For example, if $\mathsf{NP}=\mathsf{coNP}$, then for every graph that is not Hamiltonian, there is a short proof of that fact. Similarly for graphs that are not 3-colorable. Similarly for pairs of graphs that are not isomorphic. Similarly for any propositional tautology. In a world where $\mathsf{P} \neq \mathsf{NP} = \mathsf{coNP}$, the difficulty in proving propositional tautologies isn't that some short tautologies have long proofs - because in such a world every tautology has a polynomially short proof - but rather that there is some other reason that we are unable to find those proofs efficiently. 

For some problems there seems to be no difference. In particular, Vassilevska Williams & Williams show: 

[I was reading something I thought was totally unrelated and then had an "aha moment" so I think I've figured out at least part of the answer. I'm not sure if this is what Gurvits had in mind, but this makes sense to me.] A distribution on n binary variables $x_1,...,x_n$ can be viewed as an element of the tensor product $\mathbb{R}^2 \otimes \dotsb \otimes \mathbb{R}^{2}$ (n factors) (actually the associated projective space, but we'll get to that). If we label the basis elements of each copy of $\mathbb{R}^2$ by $|0\rangle$ and $|1\rangle$, then a basis of this tensor product space is given by the set of all n-bit strings. If we have an element of this tensor product whose coefficients sum to 1, then we can interpret the coefficient of any given n-bit string as the probability of that string occurring -- whence, a probability distribution! Now, since we only want probability distributions (coefficients summing to 1) we can normalize any vector in the tensor product to have that property. By only considering normalized tensors, we are really only considering elements of the projective space of this tensor product. Now we have to connect tensor-rank to Deolalikar's notion of polylog-parametrizability. According to this page by Terry Tao, it seems that Deolalikar's notion of polylog-parametrizability is that the distribution $\mu$ can be "factored into potentials" as $\mu(x_1,...,x_n) = \prod_{i=1}^{n} p_i(x_i ; x_{pa(i)})$ where pa(i) is a set of polylog(n) variables, defined to be the "parents of i" and $p_i( - ; x_{pa(i)})$ is a distribution on $x_i$ that depends only on these parent variables. Moreover, the directed graph of parents should be acyclic. Let's start with a very simple kind of distribution. Suppose $\mu$ satisfies $\mu(x_1, ..., x_n) = \prod_{i=1}^{n} p_i(x_i)$ for some distributions $p_i$ (where $p_i$ depends only on $x_i$). Then it is hopefully clear that the corresponding tensor is the rank 1 tensor: $(p_1(0)|0\rangle + p_1(1)|1\rangle) \otimes \dotsb \otimes (p_n(0)|0\rangle + p_n(1)|1\rangle)$. For a slightly more complicated distribution, suppose we want to consider the uniform distribution over the strings where $x_{2i} = 1 - x_{2i+1}$ (they are each other's negation) for all $i$. In Tao's interpretation of Deolalikar's language, this would be a $O(1)$-parametrizable distribution. Then this corresponds to the tensor $(|0\rangle \otimes |1\rangle + |1\rangle \otimes |0\rangle) \otimes \dotsb \otimes (|0\rangle \otimes |1\rangle + |1\rangle \otimes |0\rangle)$ (needs to be normalized). If we write this out in full, it contains $2^{n/2}$ terms, and so has tensor-rank at most $2^{n/2}$ over $\mathbb{R}^2$. However, over $\mathbb{R}^2 \otimes \mathbb{R}^2$, it has tensor-rank 1! I believe the latter fact corresponds to the fact that the factorization can be described by $O(n)$ numbers -- $O(1)$ for each pair of adjacent bits, for each of $O(n)$ adjacent pairs. Significantly smaller than the $2^n$ real numbers required in theory for an arbitrary distribution mu on the Boolean cube. I'm still having trouble formulating two issues, and would appreciate further answers on them: 

Because of these last two examples - especially sorting, where the standard proof is nonconstructive - it seems to me that the question may not just be about natural interpretations in terms of efficient algorithms, but also somehow about the constructiveness / effectiveness of the proofs of various complexity results (depending on what the OP had in mind). That is, the standard sorting lower bound is not constructive or algorithmic, but there is a constructive, algorithmic proof of the same result. [1] Atallah, M. J. and Kosaraju, S. R. An adversary-based lower bound for sorting. Inform. Proc. Lett. 13(2):55-57, 1981. 

In particular, [AKKV] show that one can approximate the maximum (over bijections $V(G) \to V(H)$) of the number of matches to any constant factor in $n^{O(\log n)}$ time, but that any constant-factor approximation to the minimum number of mismatches is $\mathsf{NP}$-hard. 

You might want to check out g-tries. This is essentially the data structure you're looking for, but designed for use with general graphs instead of just trees. As such, I'm not sure that g-tries have good theoretical guarantees -- I think they use a graph canonization algorithm as a subroutine -- but in practice they seem to work well. (Don't be scared that the linked paper is about "network motifs in biological networks": the g-trie is a perfectly good abstract data structure for graphs.) 

Hensel lifting is very closely related to the $p$-adics: it's basically getting a better and better approximation to a $p$-adic number, "better" in the sense of "closer in the $p$-adic valuation. Hensel lifting is used in many algorithms such as factoring polynomials or doing linear algebra over $\mathbb{Z}$ (if I recall correctly Dixon has a paper on the latter). 

The answer to (1) is yes (regardless of the properties D.W. asked for in the comments), depending on how $R$ is given: First, note that since $R$ is finite, the abelian group $(R,+)$ is of the form $\mathbb{Z}_{p_1^{k_1}} \oplus \dotsb \oplus \mathbb{Z}_{p_\ell^{k_\ell}}$, where $\ell \leq \log_2|R|$. Now, if $R$ is only given to you by generators and relations, even finding this decomposition is as hard as factoring (but given a factoring oracle, it can then be done easily). But if $R$ is fixed for all time (not part of the input), then this can be done as a once-and-for-all off-line precomputation, or if $R$ is part of the input, this can be done in $O(|R|)$ steps (even when given generators and relations). So depending on your assumptions, the preceding step is either hard or easy. But let's suppose it's done. Now we'll use two viewpoints on the elements of $R$ to reduce to linear algebra over abelian groups. Viewpoint 1: We may consider elements of $R$ as "vectors" with $\ell$ integer coordinates, where we consider the $i$-th coordinate modulo $p_i^{k_i}$. Viewpoint 2: This requires a little setup. Let $Hom(\mathbb{Z}_{p_j^{k_j}}, \mathbb{Z}_{p_i^{k_i}})$ denote the set of ring homomorphisms between these two rings. In particular, when $p_i \neq p_j$, this is zero; when $p_i = p_j$ if $k_i < k_j$ then this involves taking element of $\mathbb{Z}_{p_j^{k_j}}$ modulo $p_i^{k_i}$, and possibly multiplying it by an element of $\mathbb{Z}_{p_i^{k_i}}$ (which we may represent by an integer, treated modulo $p_i^{k_i}$); if $k_i > k_j$ it involves possibly multiplying by an element of $\mathbb{Z}_{p_j^{k_j}}$ (represented by an integer), and then applying the unique embedding of $\mathbb{Z}_{p_j^{k_j}}$ into $\mathbb{Z}_{p_i^{k_i}}$ (as the multiples of $p_i^{k_i-k_j}$, modulo $p_i^{k_i}$) Now, consider the left multiplicative action of any $r \in R$ on $R$. Then we may associate to $r$ an $\ell \times \ell$ matrix, where the $(i,j)$ entry is an element of $Hom(\mathbb{Z}_{p_j^{k_j}}, \mathbb{Z}_{p_i^{k_i}})$, which can be represented by an integer, following the conventions above. Now, replace each entry of $A$ by the associated $\ell \times \ell$ matrix as in viewpoint 2, replace each entry of $b$ by the associated vector as in viewpoint $1$, and replace each entry of $x$ with a vector of length $\ell$ (as in viewpoint 1). Now the original problem has been reduced to linear algebra over abelian groups, which can be solved in polynomial time (see, e.g., Mikael Goldmann and Alexander Russell. The complexity of solving equations over finite groups. Inf. Comput., 178(1):253â€“262, 2002). 

I think, at this level of generality, you've already highlighted some of the main values of syntactic classes in the question: they have an enumeration of machines, and as a consequence they have natural complete problems and one can more easily do diagonalization. Of course, specific semantic classes (such as UP) may have other benefits, but for just "syntactic vs semantic" in general, I think the machine enumeration and its conseequences are the main benefit. 

The permanent would seem to qualify, at least conditionally (that is, assuming $\mathsf{VP}^0 \neq \mathsf{VNP}^0$). Note that the Boolean version of the permanent is just to decide whether a given bipartite graph has a perfect matching, which has poly-size circuits. [Summarizing the comments below:] Despite this example being conditional, nothing more than a logarithmic gap can be expected unconditionally at the moment, since $\Omega(n \log n)$ is still the best known lower bound on general algebraic circuits. As pointed out by Stasys, this logarithmic gap is achieved by the function $\sum_{i=1}^n x_i^n$ (requires algebraic circuits of size $\Omega(n \log n)$ by Baur-Strassen), whose Boolean-ized version is just $x_1 \vee x_2 \vee \dotsb \vee x_n$. 

Yes. Time-bounded Kolmogorov complexity is at least one such "generalization" (though strictly speaking it's not a generalization, but a related concept). Fix a universal Turing machine $U$. The $t(n)$-time-bounded Kolmogorov complexity of a string $x$ given a string $y$ (relative to $U$), denoted $K^t_U(x | y)$ (the subscript $U$ is often supressed) is defined as the shortest string $p$ (a "program" for $U$) such that $U(p,y)=x$ and such that the computation of $U(p,y)$ takes at most $t(|x|)$ time. If you take this as your definition of "conditional information", then you can likewise define all the usual concepts from information theory. However, in this time-bounded setting, not all of the usual theorems of information theory are known to hold. For example, symmetry of information is known to hold for usual Kolmogorov complexity (no time bound), but not known to hold for time-bounded. See, for example, Chapter 6 of Troy Lee's thesis. If you are concerned that this applies to strings rather than distributions, I suggest reading the following papers, which say that in fact Kolmogorov complexity of strings and Shannon entropy of distributions are very closely related: 

Hari Narayanan recentely posted a paper on the arXiv in which he uses estimating the volume of a convex polytope to prove certain results about the Littlewood-Richardson (LR) coefficients. The LR coefficients are certain integers in representation theory that have applications in geometric complexity theory, particle physics, and many other fields (see the introduction of the above paper for more references). Again, probably not exactly what you wanted, but an interesting connection nonetheless. 

Moser's proof of the constructive Lovasz Local Lemma. He basically shows that, under the conditions of the local lemma, the second-simplest algorithm for SAT you can think of works. (The first simplest might be to just try a random assignment until one works. The second simplest is took pick a random assignment, find an unsatisfied clause, satisfy it, then see what other clauses you broke, recurse, and repeat until done.) The proof that this runs in polynomial time is perhaps the most elegant use of information theory (or Kolmogorov complexity, whatever you want to call it in this case) I've ever seen. 

Many of the best results around graph isomorphism and permutation group algorithms are parametrized by some structure of a relevant group. For example, let $\mathcal{S}_d$ denote the class of groups all of whose composition factors are isomorphic to subgroups of $S_d$. Then coset intersection for groups in $\mathcal{S}_d$ can be solved in polynomial time for constant d. This is closely related to the polynomial-time isomorphism test for bounded-degree graphs. I think there are also several other problems that are solvable in polynomial time when certain associated groups are in $\mathcal{S}_d$ for $d=O(1)$, but I don't have references off the top of my head. 

... Caveat If your answer is essentially of the form "Theory is about asymptotics, but practice is not," then either the theory should be really central, or the answer should include specific examples where the practical experience on real-world instances differs from the expectations based on the theory. 

This is one of my favorite examples not just of the success of algorithms, but of the importance of studying algorithms for NP-complete problems. They can literally save lives, and have already done so! 

then $p(X)$ is a constant multiple of $perm(X)$ for all matrices $X$. Hence we say the permanent is characterized by its symmetries. More generally, if we have a (homogeneous) polynomial $f(x_1, ..., x_m)$ in $m$ variables, then $GL_m$ (the group of all invertible $m \times m$ matrices) acts on $f$ by $(Af)(x_1,...,x_m) = f(A^{-1}(x_1),...,A^{-1}(x_{m}))$ for $A \in GL_m$ (where we are taking the variables $x_1,...,x_m$ as a basis for the $m$-dimensional vector space on which $GL_m$ naturally acts). The stabilizer of $f$ in $GL_m$ is the subgroup $\text{Stab}(f) = \{ A \in GL_m : Af = f\}$. We say $f$ is characterized by its symmetries if the following holds: for any homogeneous polynomial $f'$ in $m$ variables of the same degree as $f$, if $Af' = f'$ for all $A \in \text{Stab}(f)$, then $f'$ is a constant multiple of $f$. 

Group Isomorphism! Although Ricky Demer gave lots (though certainly not all) details on this, there is an important point I want to highlight, esp. given the stated motivation for the question, namely: 

L. Fortnow. Counting complexity. In L. Hemaspaandra and A. Selman, editors, Complexity Theory Retrospective II, pages 81-107. Springer, 1997 This gives more of the structural complexity point of view (complexity classes, oracles, etc.), and discusses other classes related to #P. Although it's from almost 15 years ago, it's really not that out of date in terms of results.