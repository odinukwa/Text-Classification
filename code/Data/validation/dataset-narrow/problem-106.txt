So what can you do to mitigate these issues or respond? It generally comes down to observing an attack and then profiling it. There are a few basic things you can do such as implementing uRPF at your edge to filter spoofed IPs that do not actually exist on the public internet and using rate-limiting heuristics that will block an IP if it repeatedly sends what you consider "bad" traffic (e.g. rapidly transmitting small UDP datagrams). When it comes to an active attack, analysis is the key: Do you observe a common source port, IP (address/range) or payload data within the attack traffic, if so, you have criteria to define a block rule with. On the other hand, if you're falling victim to a DNS amplification attack or anything that relies on exhausting your available inbound bandwidth - the only real counter-measure if it is killing your throughput is to blackhole the target addresses at your upstream provided. Basically, you sacrifice the reachability of the target for the good of the rest of your network. If it doesn't saturate your links, any decent stateful firewall should block unsolicited DNS response packets. 

Team Cymru provides a bogon reference for both IPv4 and IPv6 that you can use to filter out unassigned/reserved/private IP addresses - it's offered both as a simple list for well-known prefixes and also in a much larger list that includes space that is as-yet unassigned by RIRs. They also run a BGP bogon server that you can request a free peering to - invaluable if you're unable to run a default-free zone to the internet. 

Pragmatically speaking, nothing needs to be done at the RIR, you just announce the /24 from whereeveand you're done. Good practice, on the other hand dictates that you should create a new object for this /24 at the RIR that specifies the origin AS etc. Since the ISP is the maintainer of the object, it's their responsibility to do that. 

The way to determine this is to examine interface statistics for FEC failures. Generally speaking, 1000GBASE-T is relatively tolerant of wiring that's a little noisy, but you will see CRC32 errors - if you're getting these, especially when transferring at however close to full-whack you can get, then yes, it's likely the wiring. Otherwise, nope. 

Now for the low-level: Get a prefix (ideally from an RIR) and get it announced, I'd recommend OSPFv3 for your IGP unless you happen to be running a large flat network that you feel IS-IS is better suited to. If you have legacy equipment that is IPv4 only, consider getting around that by doing 6in4 or GRE tunnels across the equipment. Have a plan for address management, IPv6 space is not precious, if you have a /32 and you think customers will be wanting something larger than a /64, route sufficient space up to your equipment; if a single router or router pair serves 100 customers and each one gets a /56, your IGP should not contain each /56 - allocate a /48 for that router (pair) and you're done. The IPv6 space is so huge that subnet fragmentation should never be necessary if you're doing it right. Make sure that all of the services you run are dual-stacked, be that DNS, e-mail, whatever - in the majority of cases it's as simple as ensuring the service is configured to listen on v6 and your DNS has an AAAA record. If you provide hosting services, servers etc, be proactive about informing people they can dual-stack their offerings. Start getting familiar with technologies that will prevent disaster when you have no IPv4 space and cannot get any more of it either - keep a finger on the pulse of things like NAT64 and 464XLAT so that when the time comes, you can determine the viability of having IPv6 only hosts vs. using RFC6598 space and NAT44(4). Setup a lab, experiment. Then? Wait for Encourage the IPv4 wind-down. 

Is anyone aware of an IOS version for the C3560G that supports configuring a neighbor to exchange IPv6 routes? Apparently 12.2(37)SE Advanced IP Services can't do it - as soon as you try to activate the address-family it throws (great Engrish there Cisco). For the record, I have enabled dual ipv4 & ipv6, it does OSPFv3 just fine. 

Yes, configure it, just because your port channel is good doesn't mean your entire topology necessarily is - just connect another cable between the two switches on an active VLAN and watch the badness happen. I recommend MST (Multiple Spanning Tree) providing the switch supports it - interoperable, backwards compatible and of course, has the obvious benefit of being able to partition vlans into separate domains. 

A lot of the content in this question is very Cisco-ish, let's dial that back and dispel some mythical assumptions. 1. I need to do Layer 2 at the core Not unless that's all your hardware is capable of; you could quite freely have point-to-point (links/portchannels) between everything in your core and bring up OSPF/ISIS adjacencies, this is arguably superior to a large L2 domain at the core since you're now precluded from inflicting L2 loop prevention upon yourself; topology changes are now going to be handled by your IGP. If you need to multipath, ECMP takes the place of things like GLBP. 2. Traditional DC architecture centralizes routing Depends what you mean by "Tradition" - IPv4 is "traditionally" classful, modern IPv4 is not. In the same vein, a modern datacenter architecture will make the vast majority of non-access links between equipment pass traffic over L3, because from a security and performance standpoint, you want to mimimize the size of your broadcast domains and avoid reliance on L2 loop prevention which tends to impact traffic flow for significantly longer periods than an L3 protocol when a topology change happens (recent development such as SPB and TRILL not withstanding) 3. Traffic between broadcast/multipoint domains must happen at the Core Again, this is down to the hardware you have; if your distribution layer is capable of L3 forwarding on par with your core and also happens to be the termination point for the particular domains you wish to route traffic between, there's no logical reason to send that traffic up to the core only to have it sent back down. So, with all this in mind, why do large, flat L2 networks remain so pervasive? Because it's "simple" and doesn't require too much thought. Setting up multi-area OSPF and ensuring optimum paths throughout the network by configuring link costs appropriately makes for an excellent architecture, but it requires you to think. spinning up a few VLANs, using something like GLBP and letting spanning tree just "do its thing" does not. The real WTF however is that a decent L2 setup should have a well-thought out STP configuration just as OSPF should. 

In any modern stateful NAT implementation, connection tracking is generally a fundamental part of facilitating it. There's not really any limitations on what IP protocols can be handled by NAT - Linux Netfilter can happily NAT any ip protocol, but obviously, with the limitation that if there is no special handling for that protocol (i.e. additional discriminators) only one inside host is going to be able to communicate with a particular outside host at a time. In the case of ICMP , the identifier and timestamp field are highly unlikely to match that of another host pinging the same remote endpoint - thus, providing the NAT/connection tracking implementation is capable of utilising this data, it can differentiate between the two. For the NAT device would have to track payload data (namely the first 8 bytes) in order to assure the validity of the ICMP error message - but even so, the host endpoint should most definitely be validating such a message itself. Generally speaking, assuming an RFC compliant network stack, forged ICMP messages should not be an issue due to the fact that several fields are unique... Unless the attacker is a direct man in the middle, at which point they can interfere pretty freely - of course, this is why things like IPsec, TCP-MD5 and TCP-AO exist. nb: whilst a number of RFCs do exist regarding NAT, it should not be considered an agreed standard in the manner that things like routing protocols are. 

Depending on how you're using the peering exchange, you've got a few different options: Firstly I'll cover RPKI and say that whilst it's something you should definitely go ahead and deploy, both for your own routes and validating others, it's unfortunately in such low use that you cannot at this point expect it to do all that much. The real solution here is WHOIS - Merit's RaDB is arguably the best since it'll allow you to return results for all RIRs at once. But, if you prefer to query each RIR directly, go for it. Now, if you're on the exchange and you're just getting a pile of prefixes from the IXP's route server, depending on the tools you've got available to you and the capabilities of your router, you have two possibilities:    1. Filter by origin AS Essentially, this consists of validating the origin AS of a prefix against the one in WHOIS - if the origin AS isn't matching the one in WHOIS, you drop the prefix and any more-specifics that might also be announced. This is generally a good protection against unintentional hijacks. The vast majority of prefixes should have this data.    2. Filter by transit AS This takes it a step further and filters routes with any AS in the path that isn't authorised within WHOIS - you can't do this for every prefix however, since not everybody will have created objects specifying who their authorised transit AS providers are. 

On the other hand, if you're using the peering exchange to directly peer with others, then your life gets a whole lot simpler; you can lookup what prefixes they have in WHOIS and permit those. Good practice in my opinion is to permit peers to announce more-specifics up to a max length of /24 whilst also setting a sensible maximum-prefix value (i.e. proportional to the number of subnets they have) on your peering so that they can't flood you with routes but can respond to a prefix hijack. If you're looking for tools, check out IRRToolSet and IRR PowerTools 

Firstly, you need to get the MAC address, so get into a machine on the same VLAN and look at its neighbour table - Windows is , Linux: Cisco: . Once you have that... If this is a discovery job on a layer 2 switch, do - replacing the mac address bytes as appropriate. If on the other hand it's a router, use - again replacing the MAC as appropriate. Obviously when you were getting the MAC, if it turned out to be directly connected to that router, you're already done. Long-term however, I heartily recommend that you setup LLDP (failing that, CDP) to your hosts so that you can identify them from either side. lldpd is an absolutely excellent LLDP daemon for Linux that also supports CDP, EDP, SONMP and FDP. If you're currently able to reach the host and it does happen to run linux/BSD, I'd recommend skipping the above and just turn on LLDP. 

I had this problem using multicast with iperf myself - in my case it was due to the fact that iperf does not let you bind to a multicast source on a specific interface and instead just hardcodes itself to the first interface of the system, meaning that, if the response isn't being received on what is typically eth0, it's useless. From what you say, it sounds like you have this problem too. So, I wrote a patch that allows you to specify the interface to bind to - you can grab the source from my github repo. To use it, enter either or where is your actual interface name. I did submit my patch to the project, but, given the inactivity, I suspect the author has either abandoned iperf or is just too busy. 

My suggestion would be to adapt your AS number by treating it as a 32 bit AS (if it isn't already) - zero the upper 8 bits and with (makes it look like multicast space). For simplicity of explanation, I'll use hexadecimal (no really, that makes it easier to see the boundaries) e.g. if your AS number is 717232 (0x000AF1B0) you get initially 0xE00AF1B0 and you can increment the upper-most octet for each BGP speaker in your AS 0xE00AF1B0, 0xE10AF1B0 etc. Converting to decimal? easy, split the hex up and break out your favourite prograamer's calculator: E0.A.F1.B0 -> 224.10.241.176, E1.A.F1.B0 -> 225.10.241.176, etcetera. Of course, there's an infinite number of methods you could devise to handle this situation, the key point is just to be proactive about avoiding duplicates. Within BGP, you must ensure that neighbours do not have the same router ID, however, you can peer two separate routers with the same ID to a third one, just bear in mind that routerid is used as a tie breaker for bestpath selection. 

When you assign the IP to the ethernet interface as a , The router sends out an ARP query for the MAC address of the neighbouring IP since it's on-link and thus requires a known MAC address in order to actually send packets. If you move the IP to loopback, obviously that is an internal, virtual interface that is not neighbouring anything and thus, nothing happens. You could get around this by creating a manual ARP entry and route for the neighbour IP, but there really isn't a sensible reason to do this. Also, consider using a /31 rather than a /30 - make sure that both devices are happy with it (most are) and you'll have saved yourself a couple of addresses. 

The optimum solution here is BGP ECMP via - however, I will say that "accepting routes > /24" requiring a community tag sounds like epic stupidity - assuming you're a customer of theirs, they should accept whatever you give them up to a maximum number of prefixes and simply filter outbound according to whatever agreements they have with their other peers. one of the upstreams I have puts me in a similar boat with regard to not doing ECMP, I always expect to be allowed to announce any prefix size I desire to a transit provider and have them use it. So, given that they don't sound terribly competent, to balance traffic, do not withdraw your /24 prefix - instead, leave it be and ensure that your /25 routes are tagged with NO_EXPORT in addition to whatever it is that your provider allegedly requires so that your /25s won't accidentally leak out of their AS (not that it's likely to get very far if it does). One last note - make sure that this "undocumented community" isn't in fact a blackhole community, because that'd be... y'know, bad.