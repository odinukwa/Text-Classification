Approach #3: Use filesystem snapshots (if available & appropriate) A less impactful backup strategy than your current would be using filesystem snapshots, assuming you have a filesystem that supports snapshots (and all of your data and journal files are on a single volume so you can get a consistent snapshot of a running ). The upside of filesystem snapshots is that all data does not have to be read into memory by , however snapshots can still have impact (particularly when creating the initial snapshot on a busy system). Successive snapshots are more efficient and less impactful, but are still not a complete backup solution as the snapshots are local to your server (and you only have a standalone at the moment). Caveats 

For data migration (or backup) between MongoDB servers, you should be using and (binary backups) rather than / (text backups). Backups (and restores) of sharded collections need to be done through a . There are several reasons to use /: 

Assuming you are describing a sharded cluster with two shards, that is correct. Sharded collections will be distributed across shards while unsharded collections will live on a primary shard for each database. 

MongoDB does not require you to predeclare schema, but there is still some onus for an administrator to remove unneeded indexes or data constraints. The Mongoose Node.js framework you are using includes a declarative client-side schema, but the MongoDB server is unaware of any changes to field or index usage. By default Mongoose creates new indexes on startup (depending on the option of your schema), but does not have any tracking to remove or alter index definitions once created. Since you had previously created a unique index on , the index will need to be manually dropped if you no longer require that constraint. The duplicate key error message indicates the index name and offending key value: 

As per the info you've found, this is a normal (and benign) message: no administrative action is required. This message may be logged when a client/driver issues the command (for example, when an operation reaches a cursor timeout or an active cursor is explicitly closed). Normally inactive cursors are automatically cleaned up by the server without a client killcursor command. If you wanted to investigate further you could ask your application developers if they are using timeouts, closing active cursors before the results are exhausted, or possibly calling the command explicitly. You could also check on the specific version of the driver they are using if the frequency of this message has changed significantly from past occurrences. However, given only 4 occurrences in more than a month of log activity (according to your output), I would not be concerned by these messages. 

Your question is very broad as there are multiple storage engines in MongoDB 3.0+ (and a storage engine API). Physical layout for data and indexes will depend on the storage engine implementation, but it sounds like your general question is how documents with varying schemas are stored and indexed. Conceptually each document is stored in a contiguous binary record allocation which may include padding depending on the storage engine. The server-side document representation is in BSON ($URL$ which is a binary JSON-like format with a richer type system than JSON. BSON records can vary in size; indexes (typically B-tree) point to the current location of an indexed record. MongoDB (as at 3.2) does not maintain any schema information outside of the actual BSON data. Each BSON document includes all of the field names as well as the values. This dynamic schema (or "schema on read") approach allows documents to be interpreted as they are read from storage without consulting or updating a central schema catalog. For more information, see: 

As at MongoDB 3.4 there is no equivalent of "initializing" a MongoDB data directory beyond copying data files from another deployment before starting with the appropriate configuration file. There are also no standard set up scripts that would be analogous to , which does preparatory tasks such as creating schema and user accounts. MongoDB production deployments are generally configured as replica sets or sharded clusters, so set up requires context of the overall deployment. Typical solutions for deployment automation include MongoDB Cloud Manager (SaaS) and MongoDB Ops Manager (on-premise). New AWS environments provisioned using MongoDB Cloud Manager will include O/S settings as per the Production Notes in the MongoDB manual. 

Hidden replica set members are invisible to client applications via replica set or connections so all three of your examples are working as designed. As per the documentation on Hidden Replica Set Members: 

The answer from Kalhara is correct if you know the position of the embedded document in your array, however there is an alternative approach that can be used to update the first matching array element without knowing the position. Assuming you update with a single array in your query criteria, the positional operator () can be used to refer to the matching array element: 

WiredTiger's general strategy is to only compress pages on-disk where there is a storage benefit. The counter is incremented when compression was attempted but didn't result in an on-disk storage saving (so a page was stored uncompressed). Another metric for pages that are stored uncompressed is . In this case, the size of a page was considered too small to attempt compression. The commonly expected case is that pages can be stored compressed, which increments the metric. 

In general, most replica set deployments have sufficient members to allow for automatic failover (i.e. minimum of three members). If high availability is not a concern (or you prefer to have manual intervention), a two member replica set is not disallowed. However, a more typical option to allow failover would be to add a third voting-only member (aka an arbiter) to your replica set. An arbiter is a lightweight voting-only member that will provide the additional vote needed for a majority in the event one of your two data bearing members is unavailable in a three-member configuration. The arbiter helps with availability but is a compromise over a third data-bearing member. When a three member replica set with an arbiter is in a degraded state (i.e. one of your data-bearing members is unavailable) you will still have a primary but no longer have data redundancy or replication. An arbiter will also prevent your application from being able to rely on the write concern to ensure that data is committed to a majority of replica set members. My personal recommendation would be to deploy three data-bearing members as a minimum for a production replica set, however fewer data-bearing members could be considered for a development or non-critical deployment. 

One consideration with this use case is that your documents are consistently growing. MongoDB used a record allocation or padding strategy to allow documents to grow in-place. For example, if your document starts off as 1000 bytes MongoDB 2.6 or newer will round this up to a 1024 byte record allocation for MMAP (as per the Power of 2 Size default strategy). Updates that don't grow the size of the document beyond the current record allocation are more efficient for the server to execute. However, if you added 100 bytes to a document which was initially 1000 bytes, the document would have to be moved to a new record allocation in storage (and associated index entries would also have to be updated). So in this example, the next allocation for a 1100 byte document would be 2048 bytes (allowing for ~9 more 100 byte fields to be added before a new record allocation was needed for this document). Indexes in MongoDB include the storage location of the document, so a document move will result in an update for every index entry referencing that document. You can check the frequency of document moves by looking at the value for slow updates (or by enabling increased levels of logging / system profiling). Frequent document moves can definitely have a performance impact. Common strategies include either reconsidering the data model (eg. moving the growing portion of the document to a separate collection if appropriate) or adding manual padding to the initial document allocation. The default power of 2 allocation strategy is designed to avoid the need for manual padding in most cases, but if your documents start small and grow quickly you might be able to avoid some initial document moves. 

If you lose one of the members in a two node replica set, you can force reconfigure the surviving member to be a single node replica set and then add new member(s). You only need to run once in the lifetime of a replica set. 

There is no prescribed need to run on a regular basis for a healthy MongoDB deployment. Validation with the or parameter can be resource intensive, as this iterates through the collection's data & index structures. The command is typically only used as a diagnostic aid in the event of suspected local data corruption or (for WiredTiger in particular) to true up collection counts after an unclean shutdown. As at MongoDB 3.4, is a read-only command with the exception of which will check & adjust collection counts if you are using the WiredTiger storage engine. Validation generally only surfaces obvious problems in data structures, and cannot detect all possible forms of data corruption. Successful validation can be used as a sanity check if one of your replica set members is encountering data problems that result in an obvious assertion and you want to verify if other secondaries appear to be healthy. If full validation is unsuccessful on a replica set member, the general remedy is to resync the member rather than attempting to repair (which may result in a divergence of data from the other replica set members). 

The production notes in the MongoDB manual are updated regularly based on issues and experiences reported by MongoDB users, so should be part of your preflight checklist for production O/S deployments or upgrades. 

If the outcome is similar (all inserts going to a single shard and then being rebalanced), this also suggests a poor shard key choice. If you are bulk inserting into an empty sharded collection there is an approach you can use to minimize rebalancing: pre-splitting chunk ranges based on the known distribution of shard key values in your existing data. 

In general you should consider the command a last resort to be used when you actually need to salvage or repair a database and don't have a better data source to copy from (eg. a copy of the data from another replica set member of backup). Historically the repair command was often (ab)used to forcibly rebuild databases using the older MMAP storage engine, but this is no longer recommended practice. The repair operation obtains a global write lock and will block all other operations on your MongoDB server until complete. With MongoDB 3.2.3+ using WiredTiger (the default storage engine), you can use the command to release unused disk space to the operating system for a specific collection. Typically this isn't required unless you find space is not being reused effectively over time. NOTE: while compacting a collection other operations on the same database will be blocked so this is an activity you would only want to perform during scheduled maintainance periods. Depending on the layout of data on disk, compaction may also not result in significant storage space savings. A better operational approach to reclaim all unused space while avoiding downtime would be to resync a as a member of a replica set. A few points that should help clarify your observations on size numbers with WiredTiger: 

Install MongoDB 3.0 or 3.2 for compatibility with your existing MongoDB 3.0 replica set members. If you add a new member with MongoDB 3.2, I would recommend upgrading your other replica set members to the same release version. For more information see: 

Using in the shell to determine document size, your sample document in the question is about 43 bytes in BSON. With only 10,000 documents you would not be close to reaching the 64 megabytes of data required to trigger any chunk splits. Something closer to 1.5 million documents of that size would be required. You will also need to ensure the values for your chosen shard key have reasonable uniqueness. A poor shard key (i.e. with many common values) can lead to indivisible jumbo chunks that cannot be further split. If you just want to test how sharding works you could: 

The approach to reclaiming preallocated space in MMAPv1 is rebuilding the database either through a resync (recommended for replica sets) or a repair (the only option for standalone deployments). 

This isn't answerable without the context of a specific query, example document, and the full explain output. However, you could run some benchmarks in your own environment for the same query with and without projection to compare the outcome. If your projection is adding significant overhead to the overall query execution time (processing and transferring results), this may be a strong hint that your data model could be improved. If it's not clear why a query is slow, it would be best to post a new question with specific details to investigate. 

If you want to migrate with minimal downtime, a straightforward approach would be to use MongoDB replication: 

The large number of key comparisons is explainable by the skip value: the query is skipping 903,462 documents () in order to return 21 (). The value in your output is the sum of and . The number of (identical to ) is because the skip stage in MongoDB 2.6.x query processing happens after the document fetch and projection stages. There are a few suggested optimizations for skip performance you can watch and upvote in the MongoDB issue tracker: 

MongoDB doesn't have an automatic backup feature built-in -- you have to choose an appropriate backup strategy using command-line or third party tools. The backup procedure will vary depending on your deployment type (standalone, replica set, sharded cluster), disk/dbpath configuration, and backup requirements. Unless you are using a third party tool, it's best to follow a supported procedure from the MongoDB manual. 

The answer will depend on the size of your document and the nature of updates since the last background flush. I'll assume you are using a default configuration with MMAP storage engine and journal enabled. By default data changes are written twice: once to fast append-only journal files (committed to disk every 100ms) and again to a private view in memory (flushed to data files every 60s). The background flush process is a periodic asynchronous write of all pages that have been "dirtied" in memory since the last flush. Journal commit and background flush intervals can be influenced by both server configuration and write concerns. For a good overview of the process see How MongoDBâ€™s Journaling Works. The MMAP storage engine will fetch the full document into memory before applying updates. The standard x86 page size is 4KiB so a single document may be represented by one or more pages -- or multiple documents may be part of a single page in memory. So, if you are updating a single document the writes will include: 

The outcome will depend on the storage engine you are using and whether concurrency control is the limiting factor for your insertion rate. The MMAPv1 storage engine has collection-level locking, while the WiredTiger storage engine (default in MongoDB 3.2+) has document-level concurrency control. If you are pushing the limits of MMAPv1 locking for your deployment, splitting the reports into different collections may improve your insertion rate unless the underlying issue is a resource bottleneck (slow disk, insufficient RAM, ...). The WiredTiger storage engine has more granular concurrency control as well as data compression (which reduces I/O but adds CPU overhead). With WiredTiger you should be able to increase insertion throughput to a single collection by adding more insertion threads in your application (at least until your server resources are saturated). Rather than trying to design around possible MMAPv1 limitations with multiple collections, I would encourage you to test a single collection with the WiredTiger storage engine and multiple insertion threads. Outside of storage engines, a more general question is whether the different report types should logically be in the same collection. If they have different fields or indexes and you don't query across report types, you may find it more efficient to use separate collections. 

Hashed indexes were added in MongoDB 2.4 (March 2013) specifically to support hashed shard keys. I'm not aware of any use case outside of sharding. When choosing a shard key you can generally get the best outcome (appropriate balance of read/write locality) by defining your own compound index. However, an effective shard key requires an understanding of the attributes of your chosen field(s) (eg. cardinality, divisibility, randomness) as well as your application use case (common update/read queries). The field(s) included in your shard key index also need to be present in every document and cannot be changed after insertion. Where there is no natural choice of shard key based on your data, a hashed shard key can be used to achieve a more uniform distribution of values which will help distribute writes across multiple shards. The field being hashed still needs to have good cardinality (i.e. large number of different values), so values or timestamps work well. The downside of a hashed shard key is that it supports equality queries, but cannot be used for range queries since the values in the index are effectively distributed. For example, the default (ObjectId) includes a leading timestamp component. While the field is unique, immutable, and present in every document .. an is not a suitable choice for a shard key because the values are monotonically increasing. If you shard on an ever-increasing value like an , all of the new inserts will target a single "hot" shard that currently has the highest shard key value. Documents will then have to be re-balanced to your other shards, which renders your sharding ineffective for scaling -- a poorly sharded collection will be limited by the write throughput of a single shard plus the overhead of frequent re-balancing of newly inserted data between shards. For more information, see: Shard a Collection Using a Hashed Shard Key.