Again, this is only hop-by-hop encryption and the mail is available in clear for reading and modifying on each of the hops. Thus an attacker on any of the hops involved in the transfer (usually at least two, one at the sender and one at the recipients site) can intercept and also modify the mails and can of course restrict itself to deal only with selected mails. The only protection is end-to-end encryption using PGP or S/MIME. 

In this case neither one should be listen at the port 443 at all, i.e. the browser should not be even able to connect to this port. Since you get responses or error messages the browser is obviously able to connect to this port so something is obviously setup to deal with this port. But it might not be setup properly on the second server. 

I recommend against explicitly dealing with captive portal problems on the server side. Mobile devices include captive portal detection already as does Chrome and I think I've seen it in Firefox too. These existing mechanisms will recognize problems due to captive portals and give the user the ability to accept the necessary conditions etc inside a separate window or even app. 

DDOS is a very broad term and includes a variety of attacks. TCP keep alive is only relevant for already established TCP connections, which usually excludes attacks using IP spoofing in the first place. This means that it is not relevant for the majority of DDOS attacks which are attacks using a high bandwidth (like amplification attacks using spoofed IP addresses) or SYN flooding. This leaves attacks like Slowloris which try to tie resources on the servers by keeping many connections open or attacks which do a proper TCP handshake from a user space application and then abandon the connection without closing. TCP keep alive would not work against the first since there is a proper client which replies as expected to TCP keep alive. It might help with a naive implementation in the second case but this could be modified to handle TCP keep alive too without using more memory. In short: it might help for very specific and rare kinds of DDOS. But even for this DDOS it might be more effective to use instead an idle timeout on the connections and adapt the timeout dynamically depending on the number of open connections and the specific state of the connection. This would probably cover more kind of attacks. 

The option is for specifying your own certificate (client certificate). But it fails to verify the servers certificate. To specify this certificate use either or , depending on how you have the servers certificate/CA (see documentation of curl). Note that you usually don't have a private key for the servers certificate, so only the certificate w/o the key should be given. 

Loss of network connection between client and server will only be detected if packets are transmitted and the response (ACK) is missing. Packets will only be transmitted if data are sent or if TCP keep alive is used (i.e "empty" data). Thus if neither TCP keep alive is used nor data are transmitted the socket will be considered open forever. If TCP keep alive is used the detection time depends on the settings of the keep alive timer. 

A web server does not have any FTP server passwords, but a FTP server has. If no FTPS (i.e. FTP over SSL) is used you can do a packet capture (wireshark, tcpdump...) on port 21 (ftp) and analyze the packet capture to extract the usernames and passwords. Look out for the following sequence from the client: 

A SSL session spans several SSL connections (e.g. TCP connections which got upgraded to SSL) and their lifetime is determined by the servers SSL stack. It is usually not directly related to the user session which is determined by the web application and spawns several HTTP requests. These multiple HTTP requests can be sent inside SSL connection (which might have the same SSL session but does not need to) or TCP session without SSL or both. User sessions are usually maintained with HTTP level cookies, while SSL sessions have a similar mechanism, but bound to the SSL protocol. 

There is no way to redirect invalid SSL. Redirecting from HTTPS to HTTP first needs a successful SSL connection so that the redirect can be done at the HTTP level inside the SSL connection. Failing to establish the SSL connection in the first place (because invalid) thus makes it impossible to get to the redirect at the HTTP level. 

The source IP is you, the source port is 53 (DNS). Looks like you have a DNS server, which is open for queries from outside. This can be intended use like to provide DNS for your own subdomains, but it also can be a misconfigured DNS server which can be used for DNS amplification attacks, which then can be used in (D)DOS attacks against other hosts. So better check if you need a local DNS server at all and if it should be open to queries from outside. If you need the openly accessible server to resolve your own (sub)domains, make at least sure it cannot be used for recursive queries. For more information have a look at $URL$ or $URL$ 

From the perspective of the client (i.e. browser) there is no difference between an optional client certificate or a required one: in both cases the server will request the certificate and only at the server side it will be determined if a client which did not send a certificate will be accepted or not. My guess is that your client has no certificate from any of the CA you have specified inside . This file is used to build the list of acceptable CA's which is sent to the client. If none of the installed client certificates in the browser matches any of these CA no prompt will be given (since there is nothing to choose from) and no client certificate will be send. 

You don't need to restrict yourself to a specific cipher, but instead simply enable all ciphers which are acceptable to you and in the order you prefer them. The resulting cipher then will be negotiated between client and server depending on the supported ciphers on both sites. Don't restrict yourself unnecessary. As for the ciphers typically used at the server side you might have a look at Quantifying the quality of TLS support where I've analyzed the TLS support for SMTP from the top 1M sites according to Alexa, which are about 600000 mail server with TLS enabled. According to my tests about 33% of the servers use ECDHE ciphers and 52% DHE ciphers, so that 85% use forward secrecy. And for some more information about the ciphers used you will not find in the study here is a detailed list of ciphers negotiated when used with the DEFAULT cipher set of OpenSSL 1.0.1: 

Your scan only shows what the ciphers a specific TLS server role (i.e. web server, mail server...) supports, i.e. provides information about a specific part of an specific application. Any other server application on the system can use different ciphers and cipher order. Even a single application might have different server roles with different ciphers (like one web server listening on multiple ports with different configuration). Also, the ciphers shown for a specific TLS server role does not mean that the same application uses the same ciphers and order in a TLS client role or even that some other application at the same system will use the same cipher set and order. Thus, the information you have can not be used to deduce which cipher will be used for communication between these servers. It can at most be used to find out which ciphers will not be used. 

It is not really possible to do this if both sides should share the same hostname (e.g. and ), because this name will resolve to the same IP address independent of http or https. In theory you could forward all traffic on port 80 on your VPS to the shared hosting, but this would introduce lots of overhead (for forwarding data for port 443 from the shared host to the VPS you probably don't have the rights). But, if you could use different hostnames (like $URL$ vs. $URL$ this would be possible if you have control over the DNS settings. 

Then simply concatenate the translated ciphers with ':' for your ciphers string. Please note that the order is relevant. But, as you might see the rating from SSLLabs for google's server is 'B' because they still support SSL 3.0 and RC4. So instead of simply copying the ciphers (and forgetting to update them once google does) you might better follow the advice from Mozilla where several cipher combinations are shown and where they also explain in which cases which combination is useful. 

Which means to make at least hop-by-hop encryption safe enough you have to add non-trivial fixes to multiple places and most of them are not in control of a sending MTA. To get in detail to questions from the comments: 

If the connection succeeds you have probably a ftp server listening, which will give you a welcome message which usually contains the version. Also, helps to see, what listing sockets you have, something listening on port 21 is usually a FTP server. 

Implicit TLS is only used from client to MTA, i.e. to the initial hop. This is the most secure step because it is actually in control of the client and the client uses usually a fixed SMTP server with fixed TLS settings, so no problem with MX spoofing and connection downgrades. If the client is instead configured to use TLS only if available the attack might work against implicit TLS too. 

Benchmarks are lies, don't reflect the reality but might be a useful tool to detect bottlenecks. But you have to understand the benchmarks. Given that you omit essential details needed to understand the benchmark results it might be that you don't really understand what might affect the results of the benchmark. Especially information about the size of the test payload and detailed CPU load information for server and client are missing. Thus it might be that you are reaching CPU limits on the client or on the server already. It might also mainly be a problem of the more round trips you need for the requests. Let's explain the aspects of HTTP vs. HTTPS in more detail: 

If there is one given with pathLenConstraint then there is such a limit. If no such constraint is given there is no limit in theory but in practice TLS stacks might disallow insanely large chains. 

For a list of ciphers supported by your version of OpenSSL (and thus by nginx) call on the machine where nginx is running. To translate between the syntax used by OpenSSL and the syntax you see in the SSLLabs results have a look at the man page for the ciphers command where you will find the translations like: 

From the documentation at Let's Encrypt which is one of the first hits when searching for "CAA CNAME": 

Since the certificate is for mail.example.com it thus matches the name used for connection and thus the certificate can be successfully validated. The PTR record should point back to to play well with spam filters. 

If you try such requests with other servers then they would either close the connection without any error at all or just hang because they are still hoping to get a TLS handshake from the client. 

stunnel is a program to create a gateway between non-SSL and SSL. From the description on the home page: 

Restricting SSL/TLS to only selected ports like done with stunnel or SSH forwarding is not possible because the design of FTP uses dynamic ports for the data transfer. This leaves some local proxy on each site which could translate between FTP and FTPS and back (don't know if there is a product for this) or a VPN/IPSec between the two systems. 

Your site does not provide the necessary chain certificates. This is visible from the SSLLabs report: 

I think you should probably understand the reason for these warnings instead of just trying to ignore them as best as possible. If content on a secure site is included from a insecure site it might affect the security of the original site. 

Thus these restrictions are an explicit choice of the site operator to add better security even for the non-technical users. 

The server should send its leaf certificate and also the intermediate certificates which are needed to build the trust path to the trusted root CA. If the server only sends the chain but not the leaf certificate something is wrong. But maybe you did not notice that the server sends also the leaf certificate (should be the first) and not only the chain certificates. 

If you are redirecting from HTTP to HTTPS then there is no original certificate, since HTTP has no certificate, only HTTPS. If you redirect from HTTPS on host A to HTTPS on host B then the first access (to host A) must result in the certificate for host A and the second request (to host B) must result in the certificate for host B. 

In theory you could open a process which opens the socket, forks and each of the clients can then read from the socket. But the end result is probably not what you want. Since every read will remove the data from the socket buffer only one of the processes will get the data, i.e. depending on the scheduling sometimes the first child and sometimes the second child. But I guess that you want to have all the readers get all data. In this case you need to have some kind of replicator, i.e. a process which reads the data from the socket and replicates these data for all readers attached (with socket or whatever) to this replicator process. syslog/rsyslog can be such a replicator, so you need to attach your custom reader there instead. 

The client key is not used as the base for encryption, but only for identification of the client. With RSA key exchange the servers private key is used, which you have hopefully given in ssl.keys_list. With DH key exchange you will not be able to decrypt the pcap file because the key is based on random data only known to client and server. 

The server is closing the connection directly after receiving the ClientHello. There are various possibilities why this happens: 

Since python3 and the browser use SNI they will succeed. Python 2.7.6 will instead not use SNI and get the right certificate but with the missing chain. Because of that it is not able to verify the certificate against the local root CA's. 

TL;TR: you cannot serve both HTTP and HTTPS on the same port (443). While it would be in theory possible to figure out based on the first data from the client if the client is sending a HTTP request (i.e. or similar) or is starting a TLS handshake () most web servers don't do this. Instead they expect the client to behave properly, that is use plain HTTP on one port (usually 80) and HTTPS on another port (usually 443). Your URL of is causing the browser to make a plain HTTP request to port 443. But the server is expecting TLS there which means that your plain HTTP request is unexpected. Apache is at least nice enough to check if the incoming data for a plain HTTP request in this case so that it can offer you a more useful description: 

Port 22 is ssh. Which means you are trying to use sftp (file transfer over ssh) and not ftps (ftp with ssl). But with proftpd you have to use either ftp or ftps. 

Since the contents of these macros differs between OpenSSL versions you should run this command with the OpenSSL on the servers system. For more details about the ciphers and macros see the man page of the ciphers command. 

The request you want to have is not a bad one, but just a proxy request. To create it just specify your target host as a proxy. Apart from that curl might be not the right tool to generate real bad requests. 

Since it looks like the client is a normal web browser I would suggest that the problem is no the browser itself but there is some SSL interception going on. This is very typical for current client side virus scanners (which usually put the necessary proxy CA into the systems CA store) or for some middleboxes like firewalls in corporate environments. Usually in this case other SSL sites will fail too but it might also be that there is some special handling going on because of the non-standard port. A look at the certificate and chain as delivered to the clients browser will probably help to debug the issue.