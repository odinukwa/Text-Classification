No, there is no auto-magical defragging of indexes. If you have fragmentation, you need to or . Reorganizing an index defragments the leaf level of an index by physically re-ordering pages to match the logical order. Lock durations are short and will cause minimal blocking of queries. Rebuild drops an index and builds a new one. With Enterprise edition this can be done as an online operation if the index doesn't include any LOB types. With Standard edition or where LOB types exist, it will cause blocking. See How Online Index Operations Work for an explanation of how online rebuilds can occur concurrently with user operations. The Reorganize vs Rebuild recommendation is roughly the threshold at which the amount of work required to defragment by reorganizing is comparable with a rebuild i.e. at >30% fragmentation it will require more resources and greater elapsed time to complete a reorganize than a rebuild. Multiple reorganize runs would not further defragment the index. 

You appear to have setup a trace with no understanding of why you've done so. There is little point in running a trace for 10 minutes, let alone months, without a reason. 

Two excellent suggestions from @MikeWalsh. I would add Grant Fritchey's SQL Server 2008 Query Performance Tuning Distilled along with the excellent free ebooks from Redgate. SQL Server Execution Plans is an excellent reference to start with. From a different angle, I strongly believe knowledge of internals are important for all aspects of working with SQL Server. To this end, both Microsoft® SQL Server® 2008 Internals and Professional SQL Server 2008 Internals and Troubleshooting are essential reading. 

Rather annoyingly, the SSMS default is for whereas the majority of client libraries (ADO .Net, ODBC, OLE DB) specify . Likely you had a plan "go bad" but when you attempted to replicate via SSMS, the difference in ARITHABORT resulted in a different plan being used, which was "good". Slow in the Application, Fast in SSMS? is a great reference for this. 

While by no means a dead cert, and in equal weight can be indicative of parallel table scans. Without a baseline to compare with I'd suggest looking at the top CPU and IO consumers and looking for those which appear in both. 

Of course, most workloads are a mix of read and write so its necessary to consider the r/w ratio to determine the capacity needed. 

Requires programatic access. Cost of storage. More of an issue if you have to deal with platform restrictions on size e.g. 50GB on Azure. 

The one liner explanation would be that the log entry isn't required once modified database records are flushed to disk. When this occurs is dependent on a variety of factors which vary according to the database. SQL Server for instance has a configurable recovery interval which controls the checkpoint frequency (the action of flushing changed records to disk). 

Any reasoning for not using Change Tracking-Based Population and letting SQL Server take care of the updates automagically? Unless you are depending on or to change LOB columns (which are not tracked), I'd suggest accepting the slight overhead associated with change tracking, enable automatic population (which is the default) and rest easy. 

Yes there are other tools which follow a similar pattern and for projects outside the constraints I placed on my answer, they are worth equal consideration. But, if you are working with the Visual Studio and Team Foundation Server ALM, I don't think they can compete. What's wrong with VS2010 database projects? 

I can't recall if there is a DMV or combination of that will give you the total memory allocation but the following will show the bulk of it. 

I guess you could include the repair mechanisms of DBCC CHECKDB under the self-healing banner and Page Checksums under self-protection. Clustering, mirroring and the new HADRON features in SQL Server 2012 are arguably relevant topics also. 

I'd be inclined to point a finger at the storage of the poorly performing server. Sounds like you're in the unfortunate position of having a storage admin telling you everything is ok when it quite possibly isn't. Only way to be sure is to prove it. If you have the luxury of a maintenance window, run a batch of SQLIO tests against each server and compare the results. @BrentOzar has a good intro article on SQLIO which also covers some alternative tools such as Crystal Diskmark, which might be enough to prove the point in your case. Run the tests in collaboration with the storage admin, so they can witness the issues first hand. Perhaps arm yourself with How to Prove It’s a SAN Problem before hand. If IO testing proves fruitless or impossible, throw sp_whoisactive or a similar monitoring solution at the problem. Kendra Little's Collecting Data from sp_WhoIsActive in a Table would be the best approach for a long running ETL process. Before doing any of the above, take 1 minute to check something very very basic that crops up time and time again... the NTFS allocation unit size on each of the arrays. 

Or, if you aren't clustered go the tempdb route and drop your state database, by re-running with the option . 

To the additional questions... I would add , to the . Less certain, without visibility of all the data elements, I would probably put and in a separate junk dimension together as one row along with any other degenerate data items (identifiers for the truck, train etc). I would add 3 new dimensions for , and and 6 Voyage keys (inbound/outbound) to this one fact (it's 6 new keys, not 6 additional rows). You then have the option of placing and in the appropriate Voyage dimension. If you keep the generic data in these dimensions (, ) and the specific in a junk dimension (, ) they won't explode in size. 

Was the database in FULL recovery but no backups being taken previously? Paul Randall's article Running Out of Transaction Log Space covers the other likely scenarios. The DMV query below will tell you why the log cannot be truncated. If you're unsure how to interpret the output, add the response to your question. 

Perfectly reasonable proposal. No technical issues from your description of what you're aiming for. I'm wary of commenting on licensing issues and would encourage you to verify you are correctly covered with either Microsoft or your reseller. In principle, the same licensing would apply for this arrangement as would apply if you were also deploying the database engine to that server. 

For SQL Server, you could argue that a commit operation is nothing more than writing LOP_COMMIT_XACT to the log file and releasing locks, which is of course going to be faster than the ROLLBACK of every action your transaction performed since BEGIN TRAN. If you are considering every action of a transaction, not just the commit, I'd still argue your statement is not true. Excluding external factors, speed of log disk compared to data disk speed for example, it's likely the rollback of any work done by a transaction will be faster than doing the work in the first place. A rollback is reading a sequential file of changes and applying them to in-memory data pages. The original "work" had to generate an execution plan, acquire pages, join rows etc. Edit: The it depends bit... @JackDouglas pointed to this article which describes one of the situations where rollback can take significantly longer than the original operation. The example being a 14 hour transaction, inevitably using parallelism, that takes 48+ hours to rollback because rollback is mostly single threaded. You would most likely also be churning the buffer pool repeatedly, so no longer are you reversing changes to in-memory pages. So, a revised version of my earlier answer. How much slower is rollback? All other things considered, for a typical OLTP transaction it isn't. Outside the bounds of typical, it can take longer to "undo" than "do" but (is this a potential tongue twister?) why will depend on how the "do" was done. Edit2: Following on from discussion in the comments, here is a very contrived example to demonstrate that the work being done is the major factor in determining the relative expense of commit vs rollback as operations. Create two tables and pack them inefficiently (wasted space per page): 

For the entities and attributes you've described so far and assuming I've understood the domain, I can't see a need for anything more than the tables below. If you there are attributes specific to a Fitter or Admin, add distinct tables for those attributes. 

Unfortunately this is a big ol' "it depends". Complicated further by your having so many applications, probably with wildly different IO profiles, sharing the same instance/server and arrays. Usually you would want to isolate data and log files as the IO profiles are polar opposite. Typically weighted toward random read for data files and sequential write for logs. The log element is somewhat different in your case as you have multiple log files. I'd start by getting an understanding of the IO the databases are consuming relative to each other. 

Does the time taken to create the index reduce the execution time of the query. Does the maintenance overhead of leaving the index in place outweigh the time taken to create/drop when it's needed. 

I've been toying with how to structure an answer to this question since it was originally posted. This is difficult as the case for VS2010 isn't about describing the features and benefits of the tool. This is about convincing the reader to make a fundamental shift in their approach to database development. Not easy. There are answers to this question from seasoned database professionals, with a background mix spanning DBA, developer/dba and both OLTP and data warehousing. It's not viable for me to approach this for every facet of database development in one sitting, so I'm going to try and make a case for a particular scenario. If your project fits these criteria, I think there is a compelling case for VS2010: 

Create a compressed folder Create a symbolic link to the compressed folder Restore your database with the ldf file pointing at Shrink the log file to an appropriate size Detach the database, move the log file to an uncompressed folder, attach 

Brent's recent article offers an updated perspective on one of his previous series, Index Fragmentation Findings. The older article highlights statistics from much older studies on the damaging effects of fragmentation, the more recent makes a case for mitigating a large percentage of the performance downsides by ensuring your database is fully cached. Ram is now so comically cheap that it is probably the cheapest, easiest, lowest risk solution to a heavily fragmented database. Especially if the nature of the database design is such that it will naturally become fragmented despite maintenance efforts. 

For this particular situation however, one does wonder why the administrators can't manage storage by way of the (storage) virtualisation layer. 

No. Absolutely not. On a pocket change server of 8 cores you would be spending ~$50k and be unlikely to see any benefit. I would suggest trying, in this order: 

From there, I'd follow the advice in the comments and identify what the actual problem is, rather than focusing on what the problem is not. 

A connection from the pool will have the isolation level set by the last client to use that connection. Yes, it really is that scary. The long and the short of it is that if you change the isolation level of a connection you must explicitly set it back to before closing. Better is to explicitly declare your required isolation level at the start of any batch, to ensure your code isn't impacted by somebody else being sloppy, and return it to the default at the end. This behaviour, while baffling, is apparently by-design. 

2 * 10k RAID1 for OS, 6 * 15k RAID10 for everything else. Honestly, with this many disks 1 array is the safest and usually fastest bet. If you've got time to test and have a real world, repeatable, measurable workload then by all means do a test run with your tempdb on the OS drive (caveat: limit tempdb file growth to ensure you don't splat the OS). Flip side, you may see moderate improvements to your data load and maintenance with the log there instead so worth a test run or two if time permits. 

The linked answer tackles the problem bottom-up, database first. As your responsibilities encompass the applications and database, I'd be inclined to attack this top-down starting at the application(s). Focus your attention on understanding the most frequently used features of the application through consultation with the user base. Trace the database interactions of those features through profiling/logging tools so you can identify the key tables and procedures. This way your early efforts are constrained to the "stuff that matters", rather than wasting time documenting tables and queries that may be rarely or never used. The focus should also bring the Pareto Principle to bare on your bug fixing efforts (so says Microsoft anyway). 

A trace that captures RPC:Completed and SQL:BatchCompleted events (Stored Procedure and TSQL classes) with Duration, Reads and CPU data columns will provide you with a diagnostic log of how your queries are performing. You can import this trace file to a table and run queries against it to identify the slowest, highest IO or highest CPU consumers for example. If you have direct access to the server and want to analyse performance, I'm inclined to suggest that you ignore profiler traces and start with the SQL 2005 Performance Dashboard Reports. When you've grasped the basic concepts of Troubleshooting Performance Problems, invest some time understanding Dynamic Management Views (DMVs) and how they can help with performance analysis. 

You are backing up the log to the same file repeatedly. Subsequent backups are being appended to that file, which is what you can see in your screen capture. Typically, we tend to backup to dated files: 

If you dig into the internals of how snapshot isolation is implemented in SQL Server, the reason why remote access is not supported becomes clear. When snapshot isolation is enabled, up to 14 bytes of versioning information are appended to each row that is modified. The additional bytes contain the transaction sequence number of the transaction that modified the row, along with a pointer to the versioned row in tempdb. A transaction running at snapshot isolation will see the version of a row that existed at the point it started (that exists at the time the first statement is executed, rather than ). It is the transaction sequence number that is used to identify the correct version in the version store. As there is no correlation between the sequence numbers on your local and remote server, it's impossible for the remote server to retrieve the correct version. There are undoubtedly ways a SQL Server to SQL Server connection could manage this first issue, should the SQL development team decide it was worth doing. But there is a second problem, how would you deal with removing versions from the version store? The background process responsible for cleanup again uses the transaction sequence number to determine if a row version is still required. This requires nothing more than identifying the oldest running transaction sequence number and removing any versions that are older than this. With a remote connection... sounds ugly, which is why it isn't supported. Could a different approach be taken, perhaps enforcing or on the remote connection? No, not without compromising consistency as the row at the remote server could not be locked at the time your local transaction started. You could not rely on the behaviour, compromising ACID compliance. Row Versioning Resource Usage and SQL Server 2005 Row Versioning-Based Transaction Isolation contain most of the salient detail. How best to workaround this will depend on what you're trying to achieve. If you can expand on your specific scenario we can explore the alternative approaches. 

In this situation I'd be very tempted to design a new schema that fits the model you now require and create the necessary scripts to migrate data across (your option 4). 

If you're a Microsoft shop and your developers are MSDN licensed, you might want to expand the scope of this idea and take a look at Visual Studio 2010 Lab Management. 

If the live version of this database is version 1 and our next release is version 3, the script below would be all that were needed but instead both ALTER statements will be executed. 

Restore the PRIMARY filegroup (instantly, as its tiny). Retore the filegroup containing the smaller supporting tables. Restore the filegroup allocated to the current quarter order data (again relatively quickly compared to restoring all history). Set the remaining partitions as offline. Bring the database online. You can now accept new orders and process existing orders. Gradually restore the remaining partitions and bring online in turn. 

Only recently waved goodbye to the last SQL2K server I looked after, so have a few scripts in the toolbox still. You could also root around in the SSC scripts archive. Here's an old index maintenance script by Kimberly Tripp: