I am trying to get Exchange 2010 to change the MessageClass (PR_MESSAGE_CLASS) of an incoming message to that of my custom form (IPM.Note.MyCustom) when the incoming message has a certain header set. () I have seen some information about setting another MIME header () that Exchange will use to set the MessageClass, but it doesn't seem to work for me. (I've seen examples that use and ) I've even looked into writing a transport agent with C#, and I did find a property on the class, but it's read-only. I also looked at added a MAPI property in a TNEF section, but the Exchange API does not offer a way to create a TNEF section if one does not already exist. (And most mail from the Internet doesn't.) There's got to be a way to do this. What am I missing? 

Changing the BOOTUP line to something like eliminates the formatting for all init scripts. If you just want to disable formatting on one of your scripts, add: 

I have a PXE menu configuration that I'm working on. It incorporates the RHEL6 Rescue option in order to perform repairs after booting from the network. The Rescue boot environment asks three basic questions to set itself up: Language, Keymap, and source for the rescue image. I have the first two sorted by adding the "lang=" and "keymap=" options to the "append" line in the PXE menu: 

I work on a platform which serves several different projects. For the most part, all of the projects install their web server using Red Hat's RPM. We never created a policy for this which would require the projects' application developers to install from either source or in-house binary/tarball. This is causing issues now because as we patch servers with security updates, the Red Hat httpd package gets updated with configuration files that are either not originally part of the project's configuration or overwrite the configuration. We need evidence or supporting documentation which points to a safer, more stable option which can be written into policy. Is there any documentation which can provide this support? I'm looking through Google results, but my Google-fu is weak so it requires sifting out the cruft. If someone here has a source they can point me to directly I would appreciate it. 

But I have one router where the FastEthernet0 interface is not initializing IPv6 properly. The IPv6 addresses stay at tentative status: 

Windows 2003 domain controllers do not seem to utilize IPv6 fully. Exchange 2010 needs to see all subnets containing domain controllers in Active Directory. But then Exchange was trying to contact the 2003 domain controllers via IPv6 and failing. When I removed IPv6 from the 2003 domain controllers, things started working. A 2008 R2 domain controller worked fine with IPv6. 

According to the manual page for , the CPUs are specified as a bitmask. So CPU0 is 0x01, CPU1 is 0x02, CPU2 is 0x04, CPU3 is 0x08. You can add them together if you want a process to run on more than one cpu. (e.g. 0x03 is CPU0 and CPU1) 

In our network, we have a Cisco router acting as the core router, connecting all our sites to the main site. We also have a Linux firewall connecting us to the Internet. 

You can run it before or after calling maildrop, important note: the script will receive the email via stdin, so you will have to buffer it if you intend to pass it to maildrop later (which the example above does not do). I would probably recommend using perl for this as there are more mail handling libraries there than in bare shell. Edit: If you only want to do this for a single user then this is overkill, have a look at maildrop - in particular the section about "external commands" via backticks. Define a maildroprc for this user with the rules required. 

The Xvnc session and application startup: I would place all this in a script and start it from xinet.d The tricky part is to prevent users from re-connecting to an existing session. This is a unusual requirement since that is one fundamental feature of VNC. You may be able to get away with parsing the output of the Xvnc process and killing it (with the app) when you see a disconnection event. For killing the Xvnc when the app terminates, just wait for the appication to terminate in your script and kill Xvnc if it is still running at that point. 

I'm trying to figure out how to get some of our servers to send email through our relay when I use the mail command. If I simply type 

Each of the IPs in the setting is one of the slaves. Example slave. Again, there are several more zones but they are identical save for the domain. 

I've verified via that I am able to connect to the slaves from the master via port 53. EDIT 2 One thing I've done is to ensure that all of the slaves have the same configuration as well as fixing the logging which was misconfigured. About three hours after I modified the zone file the transfer happened. I still can't sort out why it isn't happening immediately. Below is the master . There are several more zone entries, but they are all identical with the exception of the domain. 

We statically assign our routes using the /etc/sysconfig/network-scripts/route-ethx files. This makes managing them fairly easy since we add the routes during the kickstart process (by way of post-script). They rarely change and if they need to be updated we simply push out a change and update the build scripts to include the new route. Recently, we had a customer ask for a virtual interface (eth0:0) with a unique IP. It needs to connect exclusively to one network. The first assigned IP on eth0 will handle all other traffic. Everything I've seen states how to add a route using , however I can't sort out how to specify that anything going to a particular IP or network can have the source IP set. If I use the ip command will it update the route files or is it stored elsewhere for persistence across reboots? If I have to add the entry to the route file for eth0:0 what should the line look like? So basically: If it goes to 10.0.0.2 use IP 10.0.1.3 as the source. If it goes anywhere else use IP 10.0.1.2 as the source. Is this possible? How do I accomplish it? 

It might be important to note that while Zanchey's answer is correct with regards to x::0 hosts, the example in the question does not describe a ::0 host. The example was A:B:C:D:E:F:0:0/64. A:B:C:D is the network portion, and the host address is actually E:F:0:0, not 0. 

The LAN between the Cisco and the Firewall has (for example, not actual) , and the rest of is distributed among the other sites. Currently, the Cisco router is advertising on the LAN, and the Linux Firewall is not advertising anything. As a result, the Cisco advertises itself as the default gateway, and sends any Internet-bound traffic to the Cisco router first, then gets redirected to the Firewall by the Cisco router. Is there a better way to do this? I'd like to be able to advertise to the LAN that should be directed towards the Cisco router, but the default gateway should be the Linux Firewall. Is this possible with stateless autoconfig? The Cisco router's current interface config: 

After a few minutes I wised up and thought to look at the isolinux.cfg file from the DBAN ISO. The only option after append that is common on all of them is 'nuke="dwipe"'. I put that option in place and the application loaded properly. 

I've also explicitly set despite it being enabled by default. Additionally, I've removed the option on the master to ensure that all hosts are able to dynamically update. Replication still isn't occurring. I am, however, seeing that the master is sending the NOTIFY announcements: 

I just installed RHEL 7.1 on a server. I'm using it to study for the RHCSA/RHCE exams. One of the steps in the study guide is to install a VM through using installation media on an FTP server. The OS variant list only goes up to RHEL 7.0. I attempted to install 7.1 but it threw an error every time: 

I have a functioning PXE configuration that allows to boot to an installation by Kickstart or rescue, both via RHEL6 media. I'm trying to add DBAN to the mix to make for a more complete solution for our needs. I have it set up to boot and it will load, but it keeps failing with 

I have a brand new Dell VRTX box set up as a Failover Cluster running HA Hyper-V virtual machines. This is my first time setting up clustering, and my first time with one of these boxes, so I'm sure I've missed something. The virtual machines are experiencing high disk latency and bad performance when accessing their VHD(x) files located on a Cluster Shared Volume. The VRTX has 10 x 900 GB 10K SAS drives in RAID 6 configuration, and the VRTX has the redundant Shared PERC 8 controllers. Both blades have full access to the virtual disks. There are two M520 blades installed, each with 128 GB RAM. MPIO is configured for the PERC 8 controllers. Operating system on the blades is Server 2012 (NOT R2). The RAID 6 array is split into a small (8 GB) volume for cluster quorum witness and a large (6.5 TB) volume for a Cluster Shared Volume (mounted on the nodes as C:\ClusterStorage\Volume1) An example of slow disk access: logging into a Server 2012 VM and having Server Manager come up automatically. Disk access goes to 100%, with write speeds at 20 MB or so, read speeds of 500 KB or so, and Average Response Time of over 1000 ms, sometimes spiking at 4000-5000 ms or so. It's the latency that really worries me. Is there something specific I should look at in my configuration? It doesn't seem to matter whether I use VHD or VHDX, dynamic or static. 

We've recently gone through a security audit and among several valid and other, quite pointless findings is one that states that tcpwrappers is not disabled. I've never used tcpwrappers so I don't have a great deal of experience in how it is configured. Having said that I've always been under the impression that it isn't simply disabled or enabled like a daemon. Instead, rules are created which define services that use it and which servers are and are not allowed to access the service. By default, if nothing is defined then tcpwrappers is, effectively, disabled. Am I wrong? 

I've also tried without the root= or init= options with the same result. I'm not even entirely sure about those options. Not much that I've found online gives much detail or explanation. One thing I know for sure, is that right now I don't want to enable the autonuke option. I simply want to load up the interactive method. Has anyone been able to make this work? EDIT: Potentially pertinent information The server that I'm testing this on (and, in fact, 99% of our servers) are VMs running on VMware ESXi. The CD works when I attach it as an ISO to the VMware console and boot to it. 

I have to say that this list here is helpful, if a bit confusing: it includes low level protocols (like NX, VNC, and now xpra) as well as high level wrappers (like neatx, freenx, and now winswitch). Also it points to some VNC implementations, but not the more recent TigerVNC fork... 

Are you aware of this root filesystem site? All the filesystems on there were originally developed for use with UML, but they should work with any virtualization solution. Note however that there is no bootloader installed as the images are made of a single loop mounted disk (without a partition table). You can still boot them with kvm using its command line option, for the others you will need to boot another image (recovery cd/image perhaps?) and install the bootloader yourself. Obviously you may have to convert this raw format into whatever format you need (vdi/vmware/..) using the relevant tools. The scripts are included should you want to create the filesystems yourself. 

In my experience, Test-Cluster never triggers a failover event. It is designed only to check hardware and software configurations to see if everything is compatible with failover clustering. As I understand it, Test-Cluster is also run when using the GUI "Validate Cluster" function from within Failover Cluster Manager. It doesn't actually "Test" the "Failover" function of the cluster. 

What could cause this to happen? Where should I start troubleshooting next? I did try on that interface with no success. 

This should loop every 5 seconds until the postfix queues are empty. Adjust your path to postfix files accordingly. You might want to leave the part out of the command, otherwise any temp send errors that cause an email to be deferred will keep the modem connection open until it retries. 

It turns out that Windows 2003 Domain Controllers will not accept IPv6 subnets in Sites and Services. After adding a 2008 R2 domain controller, I was able to add IPv6 subnets. But I also found out that running IPv6 on Windows 2003 does not work out very well, especially with Exchange in the mix.