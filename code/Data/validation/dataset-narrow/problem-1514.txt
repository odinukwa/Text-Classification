Since you aren't sure which software you would like to use, I can explain this concept only in general, by example. The clearest example is a content management system like Drupal or Wordpress (not necessarily a data science tool but the idea is the same). In a content management system, there are different types of content which can be accessed. In your case, there would be two types - document type A and document type B. There can be an unlimited number of documents, each assigned one or more types. The software has a permission system that gives access to each document type. This is built with roles. In your example, there would be a system administrator role, and a general user role. Each role has permissions associated with it. You would configure system administrator to be allowed to view document types A and B, but you would configure general users to only be able to see document type A. Each user registers an account in the software system. A site administrator assigns each account one or more roles. So a user with the system administrator role would be able to see all documents of type A and B. However, if a user only has the general user role, they would only be able to see documents of type A. Also noteworthy is the fact that each user can have multiple roles and will get all permissions associated with every role they have. USER -has-> ROLE(s) ROLE -has-> PERMISSION(s) CONTENT TYPE -requires-> PERMISSION(s) Every time the content is requested, the software system ensures that the user making the request has the permission to access the content. When a search is run, the software system has to validate every potential result individually before displaying the search results to the user. The user must have the permission to see each piece of content that is be delivered back from the search. 

The Enron Corpus: A New Dataset for Email Classification Research paper describes the kind of data set you want. The paper mentions the following link to download the data set: $URL$ Additionally, the paper also mentions various other papers which have used smaller data sets related to email classification which may not be of much use given this larger dataset. 

In Image Processing, this task is known as localization. You basically want to localize each digit in the image and then use your digit recognizer over the digits. A cursory google search for digit localization in images gives me following papers which seem to be very helpful. 

This does not mean there is nothing to learn from the data. In the worst case, there is nothing to learn from data, with the given architecture, hyperparameters and the time you are willing to let the network learn. 

This Networks add-on for Orange data mining suite should help you. If you are open to using other solutions, I would recommend networkx Python library. 

This means that is in fact an RDD and not a dataframe (which you are assuming it to be). Either you convert it to a dataframe and then apply or do a operation over the RDD. Please let me know if you need any help around this. 

You can code certain simple rules like the ones you have mentioned in the question. Additionally, you can use knowledge bases like Freebase and WordNet to enrich your language model. Note that this will not necessarily "noisify" your data but would have effect similar to the effect on data augmentation on say images for downstream tasks. 

There are several techniques that you could apply in order to cluster data if your input is a matrix of pairwise distances between elements. As usual, the best option depends on your specific data, so it is hard to answer to the question of what is the best one, but you could try any of the following ones: 

In this example, we compute the training score and the test score (cross validation score) of a Naive Bayes model as we increase the number of examples in the training dataset. The higher the score is, the better the model is performing. A larger training set decreases the score because it is more difficult for the learning algorithm to learn a model that correctly represents all the training data. However, as we increase the size of the training set, the test score also increases, due to an increase in the model's ability to generalise. As you can see, both lines in the plot reach a limit on the right size. This means that in order to learn properly, an algorithm requires enough data, just enough for us to get to the right side of this plot. Once we reach that asymptote, we can not improve the test score by using more training data. Now, notice that this plot only refers to the size of the training set, and not to the size of the test set, but inferring from it, I may assume that when you only use 2-3 examples in your test set you don't have enough test data to determine your test score. You may even get completely different results for other 2-3 different test examples. Therefore, I would assume that the 30% test error of your larger test set is a most feasible value. It seems that, in order to increase the performance of your model, you will have to look for other reasons: are your features well defined? Do you have enough training data? 

I am not very sure how effective NN would be for this problem. The way I see it is that you have 48 entries in a time series and now you are trying to predict the next 4. Keeping aside the correlations for a second, it seems like there is very little data to "learn" from. I mean the power consumption is a seasonal thing (much like temperature) but you have only 1 years data so it should be difficult for NN to capture this seasonality. I would like to raise another point about the way you are using correlations. If you are taking correlations using the actual temperature value then I do not think that to be a good idea. Think of it in this way, I would be using a lot of power at both very high and very low temperature. So instead of using the raw temperature values, I should normalize them with respect to some "comfortable" temperature value by taking the absolute difference between the actual temperature and the "comfortable" temperature. For instance, let us say that "comfortable" temperature is 25C, then both 5C and 45C would be normalized to 20 and this looks more plausible as my power consumption should be high in both the cases. This might also explain the poor correlation that you observe for 3 months of data. What should be the "comfortable" temperature is an entirely different story altogether. Do let me know if this line of thought makes sense. 

You should refer this survey paper on Anomaly Detection (from University of Minnesota). Please let me know if this helps you. 

All algorithms always do better with unreduced data sets (more data is better). More complex algorithms aren't necessarily better. 

There is a basic introduction to the Bayesian method for spam detection in the book "Doing Data Science - Straight Talk from the Frontline" by Cathy O'Neil, Rachel Schutt. The chapter is good, because it explains why other common data science models don't work for spam classifiers. The whole book uses R throughout, so only pick it up if you are interested in working with R. It uses the Enron email set as training data, since it has emails divided into spam/not spam already. 

Show the user one high-tier product and one low-tier product (A). Show the user two high-tier products and no low-tier products(B). Which page generates more revenue? Send out marketing emails for a seasonal sale 5 days in advance to one group of users (A). Send the same email to a different set of users 1 day in advance (B). 

There are many ways to calculate similarity between articles. I have not seen anybody doing a vector conversion and running comparisons. However, there is a text mining strategy called "Term-Frequency / Inverse-Document-Frequency" which is a clever way to find unique words and phrases in documents. You can run this on multiple documents, and compare the extracted keywords to match them for recommendations. Check out my ebook for more details: $URL$ If you want to leverage this technique on web documents (like a blog), there is a free service to do so: $URL$ 

Let's assume I'm building a content recommendation engine for online content. I have web log data which I can import into a graph, containing a user ID, the page they viewed, and a timestamp for when they viewed it. Essentially, it's a history of which pages each user has viewed. I've used Neo4J and Cypher to write a simple traversal algorithm. For each page (node) I want to build recommendations for, I find which pages are most popular amongst other users who have also visited this page. That seems to give decent results. But, I'd like to explore alternatives to see which method gives the most relevant recommendations. In addition to my simple traversal, I'm curious if there are graph-level properties I can utilize to build another set of recommendations with this data set. I've looked at SNAP, it has a good library for algorithms like Page Rank, Clauset-Newman-Moore community detection, Girvan-Newman community detection, betweenness centrality, K core, and so on. There are many algorithms to choose from. Which algorithms have you had success with? Which would you consider trying? 

The question asks about the and I would answer in terms of space and time complexity. Let us say that number of input transactions are N(=20) and the number of unique elements be R(approx 900). Assuming your threshold count is quite small (which means very few candidates are pruned), the time and space complexity for size i candidates would be . So you see, if very few candidates are pruned, the space (and time) requirements become exponential. It might seem unintutive at first, given that you have only 20 transactions. But the bottleneck is the number of candidates which increases exponentially with number of items. 

I would answer the question at two levels. The first level is "can it be done using machine learning?" I would say that machine learning is essentially about learning. So given that you prepare sufficient examples of sample documents and the output to expect from those documents, you can train a network to learn the structure of documents and extract the relevant information. The more general form of extracting information from documents is a well-researched problem and is more commonly known as Information Retrieval. And it is not limited to just machine learning techniques, you can use Natural Language Processing tools as well. So, in its general form, it is actually being done in practice. Coming to the second level, "should you be doing it using machine learning?". I would agree to what @NeilSlater said. The better and more feasible approach would be to use good programming practices so that you can reuse parts of your parser as your dataset evolves. 

The fact that a feature is redundant in the presence of another one, or is not informative enough to describe the target variable, is not necessarily a sign of that feature not being useful. Indeed, it may be the case that such feature may be extremely informative when combined with another one, in spite of not being very useful when considered in isolation. Therefore, when applying feature selection methods, you should also consider combinations of features. However, and as pointed out by another answer to this question, finding the best combination of features is a NP-complete problem. Therefore, applying feature selection to individual features may be a good approximation. However, I'd rather apply a greedy approach (see for instance $URL$ for more information about the topic.) EDIT to answer OP's comment: a) The table below displays an extreme example of a feature that by itself is very informative, but in combination with others is totally redundant (feature_2). This is a regression problem in which we are trying to build a model to predict the "output" variable from "feature_1" and "feature_2". 

I'd suggest to use ARIMA (autoregressive iterate moving average) as a way to detect the regularities in your time series. Take a look to the following link, in which you will be introduced to ARIMA by means of series of blog posts: $URL$ 

As you can see on the rightmost part of the plot, the two lines in the plot tend to reach and asymptote. Therefore, you eventually will reach a point in which increasing the size of your dataset will not have an impact on your trained model. The distance between the test error and training error asymptotes is a representation of your model's overfitting. But more importantly, this plot is saying whether you need more data. Basically, if you represent test and training error for increasing larger subsets of your training data, and the lines do not seem to be reaching an asymptote, you should keep collecting more data. 

I don't think there is a way to build your graph from raw data without using at least basic programming skills. I'm not aware of a drag-and-drop interface for importing and displaying data. Graphs are just a bit too complex. Imagine trying to find the profit of selling a product if all you had was CSVs of receipts dropped into Excel. You'd need labels of the columns, some basic calculations, and so on before you had anything intelligible. Graphs are similar in this regard. Thankfully, there are open source solutions, with some elbow grease and a few days of work, you can probably get a nice visualization. Cypher queries are relatively simple to write. Using Neo4j and Cypher, you can create a basic visualization of your graph, which is displayed using D3.js GraphAlchemist recently open-sourced their project Alchemy.js which specializes in graph visualization. $URL$ 

One way to handle this is to use 'supervised classification'. In this model, you manually classify a subset of the data and use it to train your algorithm. Then, you feed the remaining data into your software to classify it. This is accomplished with NLTK for Python (nltk.org). If you are simply looking for strings like "hardware" and "software", this is a simple use case, and you will likely get decent results using a 'feature extractor', which informs your classifier which phrases in the document are relevant. While it's possible to implement an automated method for finding the keywords, it sounds like you have a list in mind already, so you can skip that step and just use the tags you are aware of. (If your results aren't satisfactory the first time, this is something you might try later on). That's an overview for getting started. If you are unhappy with the initial results, you can refine your classifier by introducing more complex methods, such as sentence segmentation, identification of dialogue act types, and decision trees. The sky is the limit (or more likely, your time is the limit)! More info at: $URL$