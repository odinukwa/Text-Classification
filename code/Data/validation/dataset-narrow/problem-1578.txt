the weighted average distance of each object to the other cluster, using the other clusters Mahalanobis distance. You could approximate this by using the distance of the centroid only. Maybe use the maximum of the two clusters to resolve asymmetry. This will likely not be a metric. divergence measures, that measure the overlap of the two Gaussians, and not of the individual data points. I believe some of the divergence measures should be metric. 

First, understand and solve the problem. On manageable data. Gather experience on how to organize the data, and where the difficulties are. Try to identify points where parallelism is possible. Second, parrallelize and scale up as necessary. Don't do it backwards, popular mistake. Solving the wrong problem with the wrong tools will fail big, with big data. 

You can try running SVM just on this similarity matrix. But you'll then need to provide the sikikaritirs also for new data points. Furthermore, SVMs rely on the similarities bring dot products in some vector space. If they aren't you may get inconsistencies. They may rely on the triangle inequality being satisfied for the distance function d(x,y)=sqrt(2k(x,y)-k(x,x)-k(y,y)). Although I cannot find a clear reference on whether or not this is needed. If k is a scalar product in some vector space, this obviously is satisfied. Last but not least, SVMs are good for larger amounts of data, where you cannot afford to keep the entire similarity matrix in memory! By reducing the data set to the support vectors only, the resulting classifier will need much less memory and much less time. Much of the challenge of learning a SVM is to manage memory during training. 

Clustering will be much too expensive for your purpose (most are O(n^2), and the good ones like HAC may even be O(n^3) - you won't be able to run them on 300k instances). Also beware of the prerequisites of the various algorithms - they may not be applicable to your data, or the results may be as bad as random! Also, clustering algorithms may behave rather unpredictable, and will profuce unbalanced data sets. It would not be unusual if 99% of your data ends up in the same cluster, while other clusters are almost empty (or even empty!) I would consider first to remove all duplicates and maybe near-duplicate records (ignoring identifiers, of course). These algorithms are much more scalable and tolerate more data types. That may already be enough to achieve your goal of data reduction. 

In the last picture we are lucky to get a sample that has almost the same mean and standard deviation as the population. Therefore, the separating line is very close to the line in the first picture where the data were balanced. Notice that the standard deviation for SMOTE is always smaller because new data are added between the existing data, not outside of them. You might consider undersampling instead of oversampling. Check this link. The code for the last 3 pictures: 

Some approaches when there is a small amount of labeled data and a large amount of unlabeled data: Semi-supervised learning $URL$ - mixtures of supervised algorithms on labeled data and unsupervised algorithms on unlabeled data. One of them (label propagation) is even implemented in scikit-learn $URL$ Active learning $URL$ - algorithms that actively choose the data from which they learn, so they can achieve better performance using less labeled data. These two approaches are complementary. Therefore, there are combinations of active + semi-supervised learning algorithms. 

The network is trained simultaneously for both classification and regression. It splits only in the last layer. The input is one-dimensional. The hidden layer has 10 neurons. The output layer for regression is one neuron with the linear activation. The output layer for classification has several softmax neurons. Their amount depends on how many bins are filled with target variables. In this toy example, I have 6 training data points: 

I divide the whole range were the target variable changes (0 to 1) into 10 bins. Each bin is 0.1 wide. The amount of bins can be thought of as a hyper-parameter. The more bins the closer the classification problem to the corresponding regression problem. But too many bins is probably not good. 

This time x-component of the sample mean is OK, its y-component is a little overestimated, and the standard deviation is underestimated. The separating line is again incorrect. 

k-means tries to minimize a sum-of-squares function, looking roughly like this: $$\sum_x \sum_i (x_i - \mu_{xi})^2$$ where $\mu_x$ is the centroid of the closest cluster. This whole expression is only sensible to use on continuous variables. It makes no sense to use it on IP adresses, or port numbers, or binary numbers. For example on port, port 80 (http) and 443 (https) are closely related, but their "distance" is $363^2 = 131769$. The port 88, (kerberos), which is used for a single-sign-on type of authentication system and not related to http has a distance of only $8^2=64$. As you can see, while the attribute is "numbers", it nevertheless must not be used with the k-means objective. Even with "real" numbers, this objective may be a bad idea. For example, the number of connections made, or the number of bytes transferred are not linear. The difference between 0 connection attempts (or 0 bytes transferred) and 1 connection attempt (or 1 megabyte transferred) is much more important than the difference between 1000 vs. 1001 connection attempts (or 1000 vs. 1001 megabytes transferred) - but with the k-means objective, these difference are equal. Therefore, use a different clustering algorithm, not $k$-means. k-means isn't suitable for this kind of data. K-means assumes continuous variables (if you observe a difference of x, you may also observe 2*x or 0.5*x), translation invariance (a difference of 1 is the same at 0 and at any other location x), and that larger errors are much more severe (a difference of 2 is 4 times as important as a difference of 1). 

Cosine is only good for long documents. and have 0 cosine similarity. Similarly, and are completely dissimilar for cosine. Because it is based on counting the number of identical words. If you want letter-based similarity, consider levenshtein. But you will likely need to go to something complex n-gram based to also detect and . 

This depends on the data in your minority classes. The data in each class can be considered as a sample of observations from some population. This sample may or may not represent well the whole population of instances from that class. If the sample represents the population well, then oversampling will introduce only a small error. However, if the sample does not represent the population well, then oversampling will produce data that have statistical properties different from those of the population. All estimates (like confidence intervals, prediction intervals) are calculated from the statistical properties of the sample (mean, variance, etc), the exact calculations being different for different distributions and learning algorithms. If statistical properties of your oversampled data are different from the statistical properties of their populations, you will get wrong estimates of the confidence and prediction intervals for your model. I will illustrate this with an example. Let's assume that you have 2-dimensional data (two features in each observaton) that belong to 2 classes. Data in each class are normally distributed with the standard deviation for each feature = 1. The population mean of the class 1 is (-2, 0). The population mean of the class 2 is (1, 0). I illustrate a large population, taking 500 points for each class. The classes can be separated by logistic regression line as follows: 

Different loss functions are used for classification and regression. I also assign different loss weights which can be thought of as another hyper-parameter. 

Intuitively, the probability is high where there are training data, and it decreases in the regions between the training data. The model becomes less sure about its predictions far from the training data. The maxima of the prediction probability are not exactly at the training points. This might be because there is no exact correspondence between the underlying classification and regression problems. They are related but they are not the same, and the relationship between them depends on the values of the hyper-parameters, and the learning algorithm. For example, if I change the loss weights, 

If you use $dist$ instead of $\text{core-dist}$, then you get single-linkage clustering (hierarchical agglomerative clustering), probably the oldest clustering method out there. Same happens if you set $minPts=1$. In essence, you pretty much lose any notion of "density" in the algorithm. 

Do not use tSNE visualizations for clustering. The results are misleading. See this great answer: $URL$ Apart from that, you just need to fix the initial positions. For examples by fixing the random generator seed. But the fact thar it does not work every time should already warn you that it is not too reliable... 

Where i iterates over your attributes, a is a data vector and b is a centroid vector. Don't assume there is one, or two attributes. It's simply a parameter of the data set, how many variables it has. Could be 42 variables. 

Clustering algorithms usually assume that you have positions of objects, and want to find dense groups of observations. If I understand you correctly, you have a 2d grid of sensor readings, and you want to segment them into regions. That is a slightly different problem. If you'd just put your sensor readings into a clustering, then the clusters will not be spatially coherent: clustering assumes there is no particular order to the points. So you'll need to look into segmentation. A naive way would be to use (sensor.x, sensor.y, sensor.value) tuples. Including the sensor positions will cause the results to be somewhat spatially coherent. But that makes it very sensitive to scaling, and there is no "correct" way of scaling this. There is a trade-off between spatial coherence and measurement coherence. 

No algorithm directly works on the CSV data. Even the people that use a single CSV fike will have to parse it and load the data into a suitable in memory representation. 

Accuracy is probably not a good metric for your problem. For the original dataset, if the model just makes a dummy prediction that all samples belong to the bigger class, the accuracy will be 83% (100/120). But that's usually not what we want to predict in an imbalanced dataset. Let's take a fraud detection problem. The probability that a transaction is a fraud is very small (let's say 0.01%) but the loss of an undetected fraud transaction is enormous (e.x. 1 millions dollars). On the other hand, the cost of manually verifying if a transaction is relatively small. In that case, we would like to detect all possible frauds, even if we have to make a lot of false positive predictions. To tackle an imbalanced dataset, first you have to choose which question you want to answer. Then, what's the good metric for this question. Answer these 2 question first before deciding which technique you should use. Come back to the original question. Why does accuracy reduce when we oversample the smaller class? That's because this technique puts more weight to the small class, makes the model bias to it. The model will now predict the small class with higher accuracy but the overall accuracy will decrease. 

Competitive self-play without human database is even possible for complicated, partially observed environments. OpenAI is focusing on this direction. According to this article: 

That's an important reason for the success of self-play. OpenAI achieved superhuman results for Dota 2 1v1, in August 11th 2017, beat Dendi 2-0 under standard tournament rules. 

I highly suggest using the same API as OpenAI Gym because of its popularity and high-quality. Then you can try to apply direct these algorithms before trying on your own. They are state-of-the-art algorithms and the quality is guaranteed. About the neural network architecture, you can exploit the nature of the game for better result. For example, in Go, the position is symmetric and the actions are simple and position-independent, which is very suitable to use convolutional neural network (AlphaGo Zero). On the other hand, in Chess, because the actions are asymmetric and position-dependent, Google had to redesign the architecture to train an agent to play chess. 

take the last column of that matrix sort descending plot index, distance hope to see a knee (if the distance does not work well. there might be none) 

In constrained clustering you can provide examples of objects that should, or that must not, be in the same cluster. This can be used, e.g., for model selection: run several times and return the result with the fewest violations. Or to guide cluster extraction from a hierarchical result. 

What is wrong with simply enumerating them? i.e. 0= apple, 1 = banana, 2 = orange? By definition of one hot encoding, only one of them may be set at a time. No need to jugle with bit encoding (i.e. power set) or prime encoding (bag-of-x). But thrmain question is how do you (plan to) use the data, not so much how you encode it. Sparse data can be stored compactly by compression and sparse formats (which pretty much reduces to above approach). But the storage must fit you access pattern or performance will be bad. 

I guess that book answers your question better than I can... Choose the distance function most appropriate for your data. For example, on latitude and longitude, use a distance like Haversine. If you have enough CPU, you can use better approximations such as Vincenty's. On histograms, use a distribution-baes distance. Earth-movers (EMD), divergences, histogram intersection, quadratic form distances, etc. On binary data, for example Jaccard, Dice, or Hamming make a lot of sense. On non-binary sparse data, such as text, various variants of tf-idf weights and cosine are popular. Probably the best tool to experiment with different distance functions and clustering is ELKI. It has many many distances, and many clustering algorithms that can be used with all of these distances (e.g. OPTICS). For example Canberra distance worked very well for me. That is probably what I would choose as "default". 

Each row of the array contains probabilities of putting a test point to one of three classes. I estimate a regression's analogue of by taking the maximum of these three probabilities. 

The blue line is for bootstrap, and the black line is for SMOTE. The sample means and standard deviations for the minority class are as follows: 

The x-component of the sample mean is overestimated (it should be = 1) and standard deviations are underestimated (they should be = 1). As a result, the line separating the classes is different from the true line on the previous picture, so a lot of new data will be classified incorrectly. Let's take a couple of more samples randomly from the same population. Again, the size of the minority class is 10, and I resample them using the same two methods. 

Now, let's assume that the red class is under-represented (a minority class). I take 500 points for the green class, and 10 points for the red class. Then I oversample the red data by two methods. One is duplicating them 50 times, like bootstrap resampling (I color them red), and another is something like SMOTE (they are magenta). This is not exactly SMOTE. I simply added data that are in the middle between red data points. I was too lazy to calculate nearest neighbors for each observation in this simple example but it illustrates SMOTE nevertheless because in SMOTE, synthetic examples are generated inside the convex hull of existing minority examples which reduces the variance of the data. This is what I get: 

The training data fall into three bins. You can see in the picture why. The four points far on each side (two on the left side, and two on the right side) are all in one bin, and each of the remaining points in the middle is in a separate bin. The remaining seven bins are empty. So, the output layer for the classification task has 3 softmax neurons. I use 1-hot encoding for the labels. This is the network: