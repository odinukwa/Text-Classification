Being new to machine-learning in general, I'd like to start playing around and see what the possibilities are. I'm curious as to what applications you might recommend that would offer the fastest time from installation to producing a meaningful result. Also, any recommendations for good getting-started materials on the subject of machine-learning in general would be appreciated. 

Check your version of JPS and make sure it's the same as the version of java that you are running. Sometimes you start out with an out of the box jdk install, upgrade, set alternatives for the java bin, but still have the original jps binary being referenced. Run and look for the resource manager threads. Maybe it's actually running. If it is, try to see what binary jps is pointing at and compare it with the java binary that you are using. If the resource manager is not running, it's time to do some basic linux troubleshooting. Check log files and barring that check actual command output. On the system I'm looking at now, the log files for resource manager are placed in the directory in and . Your configuration may place them in /var/logs or what have you. Also, have a look at the syslog. If the logs don't yield any good information, which can happen, my process is to generally try to figure out the command line from the startup script (usually by prefixing the command line with ), and then trying to run the command directly to see the output as it comes out. 

It seems to me that they do this based on the history of the general end user population, they rotate recommendations when there are many popular ones, and they do some additional processing to determine that the result set is of a reasonable size. I would postulate that it works as follows: 

I have actually run into this problem before, but I can't remember the specific issue. I'm sure the same result can manifest itself from a variety of problems. Considering that you are as far as you are in the process of getting set up, I believe it's likely to be a minor configuration issue. 

Thrift is generally faster because the data being exchanged is smaller. Stargate offers a web service which is an integration method that is widely supported, which is a concern when you are working with commercial products with limited integration possibilities. In a closed environment where everything is controlled, I would prefer Thrift. If I'm exposing data to external teams or systems I would prefer Stargate. 

Most companies won't sell the data, not on any small scale anyways. Most will use it internally. User tracking data is important for understanding a lot of things. There's basic A/B testing where you provide different experiences to see which is more effective. There is understanding how your UI is utilized. Categorizing your end users in different ways for a variety of reasons. Figuring out where your end user base is, and within that group where the end users that matter are. Correlating user experiences with social network updates. Figuring out what will draw people to your product and what drives them away. The list of potential for data mining and analysis projects could go on for days. Data storage is cheap. If you track everything out of the gate, you can figure out what you want to do with that data later. Scanning text messages is sketchy territory when there isn't a good reason for it. Even when there is a good reason it's sketchy territory. I'd love to say that nobody does it, but there have been instances where big companies have done it and there are a lot of cases where no-name apps at least require access to that kind of data for installation. I generally frown on that kind of thing myself as a consumer, but the data analyst in me would love to see if I could build anything useful from a set of information like that. 

Each table contains fields for each attribute of the class. A join exists which contains all of the fields for each of the sub-classes which contains all of the fields in the class table and all of the fields in each parent class' table, joined by a unique identifier. There are hundreds of classes. which means thousands of views and tens of thousands of columns. Beyond that, there are multiple datasets, indicated by a field value in the Main-class table. There is the production dataset, visible to all end users, and there are several other datasets comprised of the most current version of the same data from various integration sources. Daily, we run jobs that compare the production dataset to the live datasets and based on a set of rules we merge the data, purge the live datasets, then start all over again. The rules are in place because we might trust one source of data more than another for a particular value of a particular class. The jobs are essentially a series of SQL statements that go row-by-row through each dataset, and field by field within each row. The common changes are limited to a handful of fields in each row, but since anything can change we compare each value. There are 10s of millions of rows of data and in some environments the merge jobs can take longer than 24 hours. We resolve that problem generally, by throwing more hardware at it, but this isn't a hadoop environment currently so there's a pretty finite limit to what can be done in that regard. How would you go about scaling a solution to this problem such that there were no limitations? And how would you go about accomplishing the most efficient data-merge? (currently it is field by field comparisons... painfully slow). 

I have a variety of NFL datasets that I think might make a good side-project, but I haven't done anything with them just yet. Coming to this site made me think of machine learning algorithms and I wondering how good they might be at either predicting the outcome of football games or even the next play. It seems to me that there would be some trends that could be identified - on 3rd down and 1, a team with a strong running back theoretically should have a tendency to run the ball in that situation. Scoring might be more difficult to predict, but the winning team might be. My question is whether these are good questions to throw at a machine learning algorithm. It could be that a thousand people have tried it before, but the nature of sports makes it an unreliable topic. 

Personally, I don't think it's all that difficult to set up a hadoop cluster, but I know that it is sometimes painful when you are getting started. HDFS size limitations well exceed a TB (or did you mean exabyte?). If I'm not mistaken it scales to yottabytes or some other measurement that I don't even know the word for. Whatever it is, it's really big. Tools like Redshift have their place, but I always worry about vendor specific solutions. My main concern is always "what do I do when I am dissatisfied with their service?" - I can go to google and shift my analysis work into their paradigm or I can go to hadoop and shift that same work into that system. Either way, I'm going to have to learn something new and do a lot of work translating things. That being said, it's nice to be able to upload a dataset and get to work quickly - especially if what I'm doing has a short lifecycle. Amazon has done a good job of answering the data security problem. If you want to avoid hadoop, there will always be an alternative. But it's not all that difficult to work with once you get going with it. 

1. Do not modify the original data Having the original data source intact is important. You may find that updates you make to the data are not valid. You may also find a more efficient way to make updates and you will want to regression test those updates. Always work with a copy of the data, and add columns/properties/metadata that includes any processed corrections. Example, if your data is a .csv file that includes a city name that contains several misspellings: 

The Cloudera program looks thin on the surface, but looks to answer the two important questions - "Do you know the tools", "Can you apply the tools in the real world". Their program consists of: 

Understanding data to identify errors is a whole different ball game, and it is very important. For instance, you can have a rule that says a serial number must be present in a given data set and that serial number must be alphanumeric with a maximum string length of 255 and a minimum string length of 5. Looking at the data, you may find one particular serial number value reads It's perfectly valid, but wrong. That's kind of an obvious one, but say you're processing stock data and you had a price range for 1000 stocks that was under a dollar. A lot of people would not know that a stock price so low is invalid on certain exchanges and perfectly valid on others. You need knowledge about your data to understand if what you are seeing is problematic or not. In the real world, you don't always have the luxury of understanding your data intimately. The way I avoid problems is by leveraging the people around me. For small data sets, I can ask someone to review the data in it's entirety. For large ones, pulling a set of random samples and asking someone to do a sanity check on the data is more appropriate. Further, questioning the source of the data and how well that data source can be trusted is imperative. I often have multiple conflicting sources of data and we create rules to determine the "source of truth". Sometimes one data set has great data in a given aspect, but other data sets are stronger in other areas. Manually entered data is usually what I'm most skeptical about, but in some cases it is stronger than anything that can be acquired through automation. 

I'm curious about natural language querying. Stanford has what looks to be a strong set of software for processing natural language. I've also seen the Apache OpenNLP library, and the General Architecture for Text Engineering. There are an incredible amount of uses for natural language processing and that makes the documentation of these projects difficult to quickly absorb. Can you simplify things for me a bit and at a high level outline the tasks necessary for performing a basic translation of simple questions into SQL? The first rectangle on my flow chart is a bit of a mystery. For example, I might want to know: 

I didn't see any other finance related studies at Cloudera, but I didn't search very hard. You can have a look at their library here. Also, Hortonworks has a case study on Trading Strategies where they saw a 20% decrease in the time it took to develop a strategy by leveraging K-means, Hadoop, and R. 

Personally, I think Netflix got it right. Break it down into a confidence rating from 1-5 and show your recommendations based on the number of yellow stars. It doesn't have to be stars, but those icon based graphs are very easy to interpret and get the point across clearly.