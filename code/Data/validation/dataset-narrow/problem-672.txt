This is exactly what padding was made for. I don't really know how to pad with cout, but printf makes it very easy. It looks like: 

There is no need to overwrite choice with the default again and again if it fails. Also a more explicit error makes debugging and reading easier. Finally I only catch on ValueError as something unexpected would be better off thrown back up rather than silently dying here. 

This is not really written pythonically. Opening and closing a file (or anything with buildup/teardown) can be done with with. One small point is that loop=0 is essentialy trying to break out of the while loop. There is a nice keyword break that will do that just fine. The code would look more like: 

I may have missed some bits and pieces, but the idea here is to give you a starting point so you can look at your own code and decide what is good, and what needs to be changed menu.py 

I attempted this problem from the ieeextreme, and I got timeout for just over 40% of the cases. Now that the competition is over, I was wondering what could be improved. The problem is as follows: 

Now getting the median of this is a piece of cake, it will be in the middle slot of the middle row! Since N * M is odd, both N and M must be odd. Therefore the median is at matrix[N // 2][M // 2]. 

Lets have a look at the algorithm you are using, and see if we can tweak it. First you get each vowel. Then you reverse the list Finally you get the index of each vowel and replace it with the next vowel in the list. There is a bug here, list.index will return the index of the first occurrence of a vowel, so if you replace the word with "aapiple" you will see the bug in action. The main area of interest is getting the index of each vowel, so lets do that. 

Since this function is called n times where n is the length of the string, and there are t strings, it is called roughly n*t times, so it is where a bottleneck has a good chance of forming There is no real need for a comment to say when a function ends, as good indentation will tell you that anyway. So, we are checking if the two characters from either end are equal, and if they aren't it goes like this 

I created a vector of TRUE/FALSE rather than a vector of indices like you did with . Both work but it is less typing without . See that I used so I did not have to type over and over. This also makes your code shorter and easier to read. I used instead of two statements separated by . That's another good function to know (imagine having many more than two allowed values...) Be careful that has higher priority than so what you had written was equivalent to which is not the same as what I think you had in mind: . Priority rules are documented under . As it stands, none of the rows in your example data match all the conditions you have specified so please let me know if I misunderstood something, I am sure it will be a simple fix. 

Here is how you would do it with no for loops. The function computes the distance between every pair of rows of a matrix. So you have to apply it to two matrices: 

Here is an implementation of the ideas I had suggested in the comments: to store the output of so it is only called once, and to limit the expensive name comparisons to individuals that share the same initials. I hope it helps. 

is a data.frame (i.e. a list), so is also a data.frame (a sub-list). When you then do , the operator has to convert your one-column data.frame into a matrix before it can compare it to , which is very expensive. This is where the item at the top of your profile comes from. Instead, you meant to do: 

Note however that a loop is quite taxing on your CPU. Instead, you should use the friendlier function. It takes a number of seconds as input: 

If you only want to work with the 3 closest stations, you can write a function that will only keep the three highest weights on each row and turn all other weights to zero: 

This can be handled using matrix multiplication. Under the hood, matrix multiplication contains a for loop just like your code does, but it is a lot faster since it is all implemented in pre-compiled code. So first compute a matrix of and where each row corresponds to a combination and each column corresponds to one of your variables: 

Edit You can create a derived class which constructor accepts only one object instance which is holding all the information what are needed to establish the connection. (The implementation can be read the configuration values from XML or any other source.) 

You can derive from the model class without any problem your attributes will get applied in you view model also (not every but for exmple the validation attributes will). I also recmommend to have copy constructors becouse they can help you a lot. Beside this my opinion is that no need to worry about display/display format attributes in you model classes it doesn't feel an incorrect way and it helps to keep things clear if you have a lot of derived classes. 

Error class If something goes wrong simply throw an exception and handle it. No need for some custom magic error handling stuff. PDO usage Yes you can use PDO like this but if i were you i would create a wrapper for it to minimalize code duplicates and provide more readability. I recommend you to fatch objects from database rows not associative arrays becouse it is a bad habbit in the PHP community (arrays are for storing data which are same types). Hashing First see @Peter Taylor's comment. In my example i'm injecting an IHasher implementation through the constructor but this is not the best i think. You should have some membership provider stuff and that can have a configurable hashing method what can be used at authentication/user registration and password changing scenarios. 

First i have provided az overload to avoid unnecessary passinf if the IEqualityComparer and in the method i'm checking against null values. Then checking their references because if they are the same there is nothing to do. After this i'm using the as operator to check if the input collections are real ICollections and if they are are their Count equal. The casting is fast no need to iterate throught the IEnumerables to get a fixed size collection to get the item counts. Next step is to get the item counts it is using a Dictionary (as Hashlacher did it) with another casting to try get the initial size of the dictionary (not good idea when we have a list with 5 million items but every item is the same). If we have the counts we can make another checks against them: if the item counts or the null counts aren't equal then the collections aren't equals. If the counts are OK then we try to check every item from the source collection whether it is in the other or not. If not the collections aren't equal. 

The profiler showed that rbind-ing your 16.5k data.frames was the main culprit. Instead, I create a data.frame after dumping the list of sentences into a single vector. I am also able to compute the vector of corresponding filenames using the function . Untested: assuming was also computationally expensive, I used again to append the and . You could check by yourself if this is instead faster this way. I used the supposedly faster instead of Your is presumably a slow re-implementation of the function I replaced with the faster construct. Also with . and are slower because they try to simplify your output data. Where possible, I have avoided large duplications of your data, e.g. where you were using . Instead, I just appended column(s) to your existing data 

is applied directly to rather than . The output is a matrix where each column is a -long sub-sequence of . takes advantage of R's recycling rules to compute, for each item in each sub-sequence, the signed distance to the corresponding element of . converts to absolute (unsigned) distances. summarizes each column into a single value: the total distance from for that sub-sequence. picks the minimum total distance across all candidates. 

You can improve your code by using vectorization to speed up the computation of Euclidean distances in the inner loop. The code would be: 

2) Your choice of will not properly handle the . Also, it does handle the case properly but it is a bit of luck considering you are looping over ... The robust alternative to the operator is to use or . Or you could just have a near the top of your functions. 3) While on the topic of corner cases, your code assumes that is a numerical vector. You could be checking for that by adding: 

With thousands of files though, it makes more sense to process the files one by one and only keep the useful information before moving from one file to the next. It also makes sense to write a function to process each file. It could be: 

You can make a couple easy changes to improve readability. First, use or so you don't have to carry a lot of . Second, reorganize your logical tests so each nested appear in the "if-false" part of the previous , not in the "if-true" part. Hopefully, this will make sense: 

I am implementing a game in python, which will give the user a sample encryption, and explain each step from plaintext to cipher. The task for the player is to decrypt a word by undoing each step shown above on the new word. For this, I have created an abstract algorithm which requires an encrypt function, and a current development status. In the future I think it will require other properties such as explanation text or a difficulty level. The abstract algorithm looks like this: 

I think keeping it simple is the way to go. When would you really need a function to square a number though? That's all i've got, its not much, so hopefully someone can comment on the c++ itself 

This could be move into the function, though I'm fuzzy on if it makes a difference or not, I'd need clarification that it is a good idea to stop namespace pollution 

This is the perfect time for if/else statements. One good piece of logic here is if they aren't equal, and a isn't bigger than b, we know the answer to a < b without needing to check. There are more things to mention, like moving the check if two numbers are equal to a method, and making use of the library of methods available, but get it working first, then post a follow up. Best of luck 

Profile the code, and with each change test it again, the only way to really improve performance is to know where the bottleneck is. Sometimes it is in the last place you expect. 

I only have a few quick points, Names and consistency: In general, keep variable and function names to starting with lowercase. When I see an uppercase word, I think it will be a class. A second point here is if you are getting input, make sure the function name states that. There is no reason NameAndEmail couldn't be getNameAndEmail, and I know what the latter function does immediately. NameAndEmail looks like a class. 

This extends the first row of the matrix to contain every row in a flat list. Before the matrix was of size N * M, whereas now it is N * M (the first row + (N - 1) * M (all the other rows). Subtracting the original size from this tells us how much extra memory we are using. We use (N - 1) * M additional memory or in other words O(NM) extra memory. This is not what we want. The reason to put all the elements in one list is to make sorting easy. Lets see if we can sort without needing a flatten (1d) list. There are many sorts that don't require extra memory, they are called "inplace" sorting algorithms. For simplicity we will modify selection sort to work for our case. How selection sort works is it picks the smallest element in the list, and puts it at the front. Then it finds the next smallest element, and puts it second, and so forth. To implement this, we can find the smallest in the whole list, and swap it with the first element. Then we can find the smallest of the list skipping the first slot. 

Last, stay away from for writing your output to a file. It is a really old function; the fact you have to provide as an input tells me it was designed with on-screen printing in mind. Instead, you could use to be consistent with your using of at the beginning of your script: 

The best tool to diagnose slow code is the profiler. Here is how you could run it on a few function calls to see what is slowing down the execution of your code: 

Finally, switching to a (from the R package of the same name) is always recommended when dealing with large size data.frames and when speed is a concern. 

To fix that, you need to allow for some very small tolerance. You could mimic by setting that tolerance to : 

First, let's point out that and do not require that the input be sorted so they should asymptotically (i.e as the input becomes large) perform way worse than your code if implemented properly. My main concern with your implementation would be that you are disregarding potential floating point errors. See for example that 

But before doing that: Why do you need to convert the list items to data.frames, then bind? Could you instead rewrite so that it returns a data.frame? And better, could take a vector of cutoffs instead of a single value and return a data.frame with rows? 

Note how I am using vectors everywhere and only in the end putting the results in a data.frame. Indeed, there is arguably no need for a data.frame until you want to see the results in a nice format, at the very end. Keeping things in vectors avoid the repetitive and the annoying conversion from character vectors to factors (what forced you to use ). I have slightly modified your algorithm, where instead of replacing valid postcodes by so they won't get affected by fixes, I am propagating a vector of valid or "best guess" codes () via the construct: 

It returns a vector containing for each distinct value of . In the event that the data has no value for a given index, you could overwrite the output-ed with a so as to not affect the upcoming roll sums: 

Instead, I would suggest you write a function to process a single sheet and only loop once using that function (disclaimer: code could not be tested!):