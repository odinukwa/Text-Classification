If your are related to the and are related to the so you have to three tables, then resulting table by and all entries grouped together: 

When you run some INSERT or UPDATE query on the trigger will replace submitted value of the by string stored in the variable. Sure you can do as complex and randomized transformation as you want. 

The problem is that values of the UDVs is restricted to the statement an do not passed across different s and s. If you want to refer to the values calculated in the one from another you have to use values stored in the resulting table from the previous statement. Try the next form of the second operand of the : 

Here table should have index (foo) while table should have index (bar) When you filter out rows by clause, columns mentioned in the conditions should be indexed: 

Here we can see that there is no penalties for using s with proper index. Here is the suggestion: lat-lon coordinates have the anomaly near to the prime meridian. Every rectangle dissected by it should be splitted into two different rectangles to the left and to the right of PM and checked separately. 

The main problem I've see is that clause performs non-constant comparison. For each of the 11574092 rows the same value calculated again and again that produce the overhead. Moreover, index can't help with such kind of comparison as far as is non-deterministic function that can return different results in two consequent invocation. Therefore engine is forced to check all and every row for desired condition, calculating each time from the scratch. It's easy to avoid this very common trap. Calculate the value once and store it in the user-defined variable: 

You already have the best matching index chosen by the optimizer now see how it works with constant comparison. 

Each row in the 's output having in the 'key' column should be investigated. Those row marked as aren't the culprits but rather the victims. In your case table need the indexes: , , and . I can't predict which one of the last two will be choosed by optimizer. Then derived tables produced by s will get the suitable inherited indexes for further proceeding. 

There is no "default" order in the RDBMS. Just because tables are not lists but sets. Imagine records as a cards in a big bag, not in a stack. There is no guarantee that cards will be fetched from the bag in some specific order. Yes, cards are enumerated by primary key, but you have to request the fetching in that specific order. It is not the default order when order is not specified. 

For each table involved you need an index that mentioned all the used columns in the clause and in the as well: 

You have to test which index get better result. Keep in mind that indices are expensive so create only those indices you really need. The other suggestion is to precalculate all values that are constant within the query. In fact will be recalculated as many times as many records have . If you precalculate the value into the variable, then calculation will be performed only once: 

User-defined variables can't be used as identifiers in the function or procedure definitions. Use plain variables and initialize UDV like that: 

Here table should have index (foo). Complex conditions that refers to the multiple columns in the same table require multicolumn indexes: 

You have to choose one of the terminal stations as and store the distances from to the each station on the route: 

You have to use UUID() function that guarantee that returned string is unique not only within your table or database but all over the world. $URL$ 

I prefer the next strategy: the stored routine that fill up the table on each inserted record also delete few expired ones. This look like that: 

The most powerful approach is to wrap all the basic operations like , , and into the set of the stored routines. That acts like an API to the database and can be extended by any desired checkouts/conversions/calculations you ever want. If parameters passed to the stored routine meet some conditions you can them. Otherwise you reject those data without inserting. Same effect can be achieved by trigger that look like this: 

Such complex conditions used by , especially those having greater/lesser comparisons, can not be efficiently indexed and will be VERY slow. Even worst case is when some kind of functional condition is used for or I suppose that this is the case of bad database design that can't be speeded up at all. Even if some progress will be made with this particular query, every next query's performance become worst. Structural and functional decomposition should be revised 

View is just an alias to the query that allow you to use its result in other queries in terms of tables not subqueries. But each time you refer to the view, the query from its definition is invoked. No temporary tables created when view is defined. Therefore views do not improve the overall performance at all and are intended to make DB structure more clear and logical. Prior to the MySQL 5.7 query results can be cached that really speed up some queries. But as far as caching conflicts with InnoDB internals, it is disabled now and can cause significant slowdown and overall performance losses when enabled. So views should be considered as threat to the performance. On the other hand, temporary tables, especially those with , can significantly improve the performance. But you have to ensure that temptable is updated each time the referred tables are updated. Triggers are the good tool to keep reference consistency of the derived tables. Sure even memory-based temptables should be properly indexed for maximum performance. 

where id=1 is already existent record and column have constraint. But triggers in general are unclear and non-transparent tool that can be very difficult to debug. Also autoincrements and triggers can interfere with unforeseen consequencies so it's better to avoid triggers as possible. 

There is no performance difference between single base and number of bases, because "schema" is just the domain of authority. Grouped together or aparted, tables stored and processed in the same way. From the administrative point of view multiple bases are bit preferrable as far as you can dump the whole data for single client by DB name, not the pattern for the mangled tablenames. Sure your experience may vary. 

may be the culprit. Too many concurrent connections that consume resources. Try the default value of 151. Try to install utility that analyse lot of mysql settings and post its report here.