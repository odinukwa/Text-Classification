Generally recovering from this sort of mistake involves editing the boot loader to force a root shell. Since you don't have physical access to the (non-physical) machine, things are a bit different. If this is an EBS boot drive, you can try detaching it, fixing it on another machine, then reattaching. This is a good opportunity to learn why it's useful to use configuration management tools for everything: if you were doing this, you could replace the instance in just a few minutes and one command. 

Now, sometimes the situation is that you do indeed need to have these shared servers. If you've gone through and determined that you really do, then there are a number of techniques you can take to simplify the process. 

As mentioned in the question and the comments on it, this isn't an area in which Ansible excels. The "best" solution depends a bit on the type of file you're dealing with, and the structure of it. The cleanest way to deal with these sorts of changes in Ansible is to templatize the entire thing; this allows you to see the entire file at once, rather than piecing it together through many places, and is much less fragile than pattern- or line-based approaches. When you need to add configuration in multiple places (for instance, different roles), that's when dynamic-loading directives become very useful, like Apache httpd's : 

So what is happening is that the changed code just sat until internal testing was done (never a good thing as this is how code get stale quickly). Once internal testing was ready, the code was then merged into the trunk causing it to be published into nugget under the trunk's feed. Some business politics ensued and we finally got to a real single code base mindset from the product owner's perspective (which was the issue that got us into this situation). Moving forward I don't anticipate this to arise. Code can be branched out. However, only code in the trunk will go to our internal nugget feed or our build/deployment processes. 

So I have a local jenkins server building artifacts. I trigger this build through VSTS. When browsing through the logs when uploading the artifacts to VSTS from the local jenkins server I found this: This could pose an issue where the triggered build may get beat out by another build (theoretically). While this is not a high likelihood, I would like to mitigate all possible artifact confusions as that is one of the main points my company is moving to build automation and continuous integration. I did see something related to the VSTS Task in issue 4110 but not sure what is coming of this. What I'm trying to accomplish is that when Jenkins finishes the build it uploads it to VSTS. Because of the size of the build, when we go to releases, I would like to not download the artifacts back to our local premise, (using deployment groups) but instead copy it internally directly from the Jenkins build server. Does anyone have any suggestions? 

Snapshot Permissions Boto3 has a function that allows you to create volume permissions, which is what AMI Sharing with AWS Marketplace requires you to do. will allow you to share your AMI with the marketplace account like so (you can also use a JSON representation if you prefer, it's in the docs): 

So if you want to keep your Docker image as small as possible, consider using OpenJDK instead. There's an official OpenJDK Dockerfile repository or you can just use . The basic "easy to run" Dockerfile for OpenJDK 7 is as follows (taken from the website listed in the previous sentence): 

When it comes to affecting business culture, the best way is probably via the well-known "boil the frog" method. You have to introduce these tasks to developers slowly, because I know I myself (as a dev) would balk at having all this new responsibility at once. First, start out by introducing one or two new tasks only to be performed during normal business hours. They need to learn how to do devops which could be quite the learning process for a (to this point) code-only dev and might require some supervision. They will also likely be hostile to the idea of changing their work-life balance since you mention they're used to 9-5. At this point, record data on the new processes for use later (have them write this code, data is always useful). Later as you're running out of new devops-y tasks to introduce (so the first, second, and fourth bullet points are almost complete), bring the first tasks you've introduced up as candidates to be performed outside of standard work hours. You may see some backlash at this and you might even see some attrition depending on how strongly you push this and how heavily the work-ends-at-five culture is ingrained. To defend against this, hopefully your data supports the idea that work beyond standard hours will be rare, only occur in extreme situations, and will greatly benefit both the business and the customer. If your data doesn't support this, then you'd better be ready to deal with the consequences of this choice. Even with data, it may still be easier to have the developers write the monitoring/alerting code (so they become devops but still mainly dev) and keep the alternate ops team as front-line support (as a few others have suggested). Like I said, small changes are important to avoid backlash. Integrating work for devs beyond standard hours will be challenging, as they may know they can look elsewhere for employment if they don't like it since the market for devs is strong right now, especially if they already had devops skills. Caveat emptor! 

passing in the a personal API key (created by Jenkins user manager to use by visual studio team services), this script first checks if it is can access the server using local connections, if so set the variables appropriately. From there we create local handlers for environmental variables. Our release artifact name defined in VSTS is the same as that in our jenkins server. The build number used is created by VSTS and passed to jenkins and stored by the VSTS plugin(Jenkins TFS Plugin download). We get this version number from the environment as the artifact build number. We also ensure that our team project path url is windows URL encoded brief explanation. Next we hold our authentication which is stored as a variable in our VSTS variables. The is marked as secret. As such, we must pass it in as it isn't visible as an environment variable. We then encode the authentication and add it to our request headers. We then add a debug line to print what target we are looking for and what project we are looking in, just to make sure we are looking for the right thing. We create a variable to hold the build information that we find and do one more debug write stating what are query url is so that we can manually run it if we run into any issues. Next we then run this url query against the build server and iterate over all the builds returned. The query filters out all the JSON to just the build ID, it's absolute url, and the VSTS build number. We check to see if the build we are examining has the same build number as the artifact we are deploying, if so store it in the variable, print the was found for debugging, and exit the loop. Coming out of the loop we check to see if we found a result. If we did not find a build (due to it being cleaned up most likely) we inform the user that the artifact needs to be rebuild from source. Otherwise we download the artifact zip to a temporary location from the jenkins server and extract it to the target directory, defined as a variable. Lastly we delete the downloaded zip. This is ran as a powershell script task and due to it's length, can not be an inline script. So to deploy this, i have a devops git repo for my scripts and I just include this as an artifact that is downloaded and ran. This task is preceded by other tasks which stop the current process from running, and delete the current files in place before running this task. After this task is ran, more scripts are ran to run the downloaded artifacts. I hope this helps someone else trying to coordinate and link VSTS builds with Jenkins artifacts. 

One of the easiest ways is to use part of the url to switch which code branch gets executed (this change is part of why this is not an appropriate staging environment). I've done this in the past with a wildcard DNS (so any would redirect to the same server) and routing code that loaded up in the include path. You can also have a command (using something like Ansible) that adds the necessary configuration for a branch as necessary. This would probably be part of the deploy process, which I'll talk about in a minute. 

That's only the case if the server you're downloading Python from relies on SNI. I imagine most of their mirrors do not. But really, you shouldn't be compiling Python on every individual server. You should do it once, either on a machine that you freeze into an image that you launch further machines off of, or you package it as an rpm and the machines just download that and install it. This allows you to control the download process. Even simpler: download the package on your desktop (via verified https). Copy it to your file storage. Download from there onto your servers. There's no need to download directly from python.org every time, and it's more reliable to have a local copy anyways. 

I have a few CI environment in VSTS. One for dev, QA, staging, and production. my builds are created in the cloud by VSTS build agents. these builds are not obfuscated by design and need to be deployed to my dev and test environments as such. When they are approved by QA in the test env, they go to staging. What I need to have happen is for the build to be obfuscated before being put in this environment. Is it possible for me to run a docker container in VSTS release pipeline to obfuscate the build in the container and then download the result in a deployment group? My main questions are boiled down to this: I would like to have a container image in the cloud running a tool installed on the image. During my VSTS release pipeline, I would like for my release agent to pull down this image, pass in the build artifact, and then do something with the results of running my artifact through this tool. I have an Azure subscription. I've tried reading the documentation but I'm confused as to what I need to set up. This container is not hosting any web app, but is being used as a way to run my tool locally on the release agent. I see that I can run a docker run command as a VSTS pipeline action. What do I need to set up on my azure subscription to host this image for the VSTS agent to pull it down? 

The way we handle this is to run our alerts through Sensu first. You can configure a Sensu check to require multiple failures to alert, and then also configure the check's notify to get sent to PagerDuty. This way, by the time PagerDuty hears about it (and pages you), it has already passed the "N failures in a row" criteria. If you need to check not for number of failures in a row, but rather N failures across a time range, then that's a good use case for adding elasticsearch or graphite or similar into the mix. Whenever there's a failure, log an error or increment a metric, and then your check can simply look over the time range and see if the aggregate is above the limit. 

When you run this file with , it will look up your host list (as simple as an ini file, but can also be a dynamic list written in any programming language) to determine what servers exist and what groups they're in, and then run the defined tasks on the servers that you tell it to. Now, you specified that you wanted to stay in the Javascript ecosystem. However, while Ansible is a new thing for you, it doesn't have all the complexities of another programming language, since configurations are written in YAML. It will also allow you to do much much more later, and unlike several of its competitors, uses ssh to communicate, so there's very little setup involved (nothing to install on the remote machines). 

This should add the permission that the marketplace needs to access your AMI. As for replacing with the snapshot's ID, you can do that with boto3 by using your snapshot object's snapshot_id property before making the API call (that being said, there's little reason for you to use this API since you have to use boto3 anyways). Product Metadata You should be able to use the relevant ServiceCatalog functions in boto3 such as to upload product metadata. There are also equivalent official API functions (this one is just CreateProduct, they're not listed in a way that allows me to link to them nicely). It's worth noting that the official API only accepts JSON whereas boto3 will generate the JSON itself. Which one is more useful to you depends on how your data is structured in your product metadata file(s). Since I don't know that I can't directly write the code to do it. 

There is no official Oracle JDK implementation provided by Docker. Docker used to support their own Java library on hub.docker.com, but they deprecated it in favor of the actual OpenJDK implementation since it was "OpenJDK-specific since it was first introduced". Their reasoning: 

There is also an official API function that will help you achieve the permissions requirement listed in AMI Sharing with AWS Marketplace if you decide to migrate from boto3 (or if a reader isn't using boto3). If your AMI is private, you need only make this API call: 

Relatively speaking, the concept of devops is new and still defining itself in my opinion. I currently fulfill a devops engineer role. For me, this means I facilitate and develop the tools and processes used by both our dev and ops teams freeing them to focus on the product that generates revenue for the company. The ops and dev teams spin up their own servers and such as needed. I just hook up the CI for our products, ensure our processes makes sense and seek out what process can be improved/automated. I meet with all of our departments, from sales, to warehouse, to developers and operations (QA and release managers) to see what they are doing and how I can improve their process. 

I have hosted VSO build agents building our exe and that pipeline setup. I am trying to deploy the produced exe to a dev testing environment. I have a deployment group set up and the files are being successfully copied to the correct location. What I'm trying to do now is launch the exe after it was copied. I have an inline powershell script task running consisting of 

So I wrote this powershell script to be ran by the deployment group agents who have access to the server locally. 

First of all, Rancher actually contains implementations of both Kubernetes and Mesos within itself. However, they did make their own system called Cattle which is heavily based on Docker's Swarm. I'll touch upon this in the section for Rancher. Secondly, since each one offers similar base features (load balancing, resource isolation, etc) I'll try to go more into what makes them different rather than focusing on differences among those common features unless they're significant. Kubernetes Highly performance focused, also featuring cloud storage orchestration (a feature missing from Mesos, although there's probably a plugin for it). Has API options to allow for automated scaling of resource usage as needed by individual containers (and for the reverse if a container is not being hit hard while others are). Something important about Kubernetes is that unlike other container orchestration software it doesn't provide a comprehensive configuration or any kind of comprehensive self-healing. Instead, it focuses on continuous deployment of multiple apps with an easy rollback system at the app level (as a result you might want to look into micro-services when using it). Each app is a small piece and can be deployed/configured individually. They make a point in their docs to say that Kubernetes is not a traditional PaaS (platform as a service) system since it lacks middleware for virtual hardware or databases and it doesn't build your app itself. It's designed to (as they say themselves) remove the need for manual container orchestration, instead automating the process by continuously pressing towards a target app state. Mesos Monolithic in comparison to Kubernetes. Focuses on the big picture moreso than the individual services, although it still allows for management of individual services. Provides built-in middleware for things like Spark, Hadoop, etc. Best usage of Mesos will involve many plugins as it's designed to be easily extended. If you need fine-grained control over managing your application (insofar as there's a plugin available for what you want to do or you have a team member willing to build one if there isn't) you'll want to use Mesos. Rancher (and Cattle) Potentially the best option in that it is itself a superset of the previous two, having an implementation of both. This might also be seen as a downside as more complication in your management software is rarely a good thing and might lead to unforeseen issues. Rancher features an application catalog that allows for one-click deployment, something Kubernetes doesn't have due to its design philosophy. However, seeing as Rancher has an implementation of Kubernetes, you can use Rancher if you feel these features are missing from Kubernetes. Cattle is based off of a stack system, where you group related services together. It also houses Rancher Compose, a feature similar to the Docker service of the same name. This is probably the most interesting part of Cattle, the rest of it being fairly standard (although the Secret management is still in beta). If you have experience with Docker Compose you should read up on it here (I don't, so I'm probably not the best person to write about it). Resources: "What is Kubernetes?", "Overview of Rancher", "mesos.apache.org: 'What is Mesos?'" 

The biggest challenge I've encountered with people resisting the change is the reason of not seeing the value of it, or build automation in general. His idea was, we were our getting product(s) out the door, and on time. So why do we need to "create work" for ourselves, and take a developer off of working on a product. After 4 years since then I've finally got the owners on board with CI by capturing metrics on how much time is wasted copying files manually from bin folders to SVN for qa to copy from SVN to environments. This along with the overhead of managing what build is in what environment and other pitfalls of you are testing the wrong build etc. He has finally came around, but I think he feels that the time could be put to other tasks. 

From the perspective of build automation, I find that it is better to have multiple repositories. This allows for smaller configurations and more granular control of your build/release process. You can allows reference and pull in source code to build or release (of course you can also ignore source code as well in VSTS build pipeline). This leads to being able to release just the project you want or need to your desired environment. You can of course pull in artifacts from other builds and include them in your release if you need to. To summarize, I tend to go as small as possible and and only group things into the same repository if they are indeed part of the same project and no part of the project can be used elsewhere.