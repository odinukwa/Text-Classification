This question arose from a related question for hypergraphs, but it seems interesting in its own right. 

The lower bound is $n$ as you point out, but somewhat surprising there seems to be an upper bound of $(1 + \pi^2/6)n$ for the expected number of steps. To derive this, note that a sequence of balls will clear all the bins precisely if it contains a subsequence $b_1b_2\cdots b_n$ such that $b_1 = n$, $b_2 \ge n-1$, $\dots$, $b_i \ge n-i+1$. Additional conditions are necessary on the sequence to avoid balls being chosen that are no longer in the system, but for the purposes of an upper bound, suppose that there is an infinite decreasing sequence of bins (so the balls don't disappear when leaving bin 1, but are moved to bin 0, then bin -1, and so on). Then the expected number of steps for such a subsequence to be seen is the expected number of steps before $b_1$ is seen, plus the expected number of steps before $b_2$ is seen, and so on (down to 1, since $b_n$ can be any of the numbers $1,2,\ldots,n$). These can be seen as separate events, one after the other. The expected number of steps is then $\begin{eqnarray*}n + \sum_{p=1}^n \sum_{k=0}^{\infty} \frac{k+1}{n} \left(\frac{n-p}{n}\right)^k & = & n + \sum_{p=1}^{n-1} \frac{1}{n-p} \sum_{k=1}^{\infty} k\left(\frac{n-p}{n}\right)^k \\& = & n + \sum_{p=1}^{n-1} \frac{1}{n-p} n(n-p)/p^2 \\& = & n + n\sum_{p=1}^{n-1} 1/p^2 \\& \le & (1 + \pi^2/6)n. \end{eqnarray*}$ 

By a classic result of Kuroda, the complexity class NSPACE[$n$] (also known as NLIN-SPACE) is precisely the class CSL of context-sensitive languages. The satisfiability problem SAT is in NSPACE[$n$], since a linear-sized guess for a solution can be checked with at most a linear amount of overhead for book-keeping. This means that SAT must have a context-sensitive grammar (CSG). 

Collaborative bibliography managers such as Mendeley and CiteULike have been around for several years, but have not yet caught on in the theory community (at least, they include a vanishingly low proportion of the papers I would like to cite). Note: I am listing these services in a spirit of completeness. They are relevant to the question, but I do not endorse any collaborative bibliography manager. 

Note that for large enough $n$, $$\sum_{i=0}^k\binom{n}{i} \le \left(\binom{n}{k}\right) = \binom{n+k-1}{k},$$ and since $$\log\binom{n+k-1}{k} \le \log[(e(n+k-1)/k)^k],$$ when $k = cn$ then information-theoretically it follows that $d \le c\log(e(1+c)/c)$ would be achieved with a perfect code. (This is less than $1$ if $0 < c \le 0.2728$.) I am therefore looking for a reasonably clean code that can be manipulated without using lots of space. To obtain a perfect code, one could pick some enumeration of the subsets, run an index through the enumeration in increasing order, and then obtain each combination by decoding the index. However, decoding such a code when $k \ge \Omega(n/\log n)$ seems to require using at least $n$ bits of space for the enumerations I have looked at, such as via characteristic vectors ordered by increasing Hamming weight and then lexicographically, or via Gray codes. There might be a way to do this with $o(n)$ space, but I think $(1-\varepsilon)n$ is more likely to be feasible. Note that since $\log \binom{n}{cn} \ge cn\log(1/c)$, the information-theoretic lower bound is already $\Omega(n)$ bits, so this really is about whether $(1-\varepsilon)n$ can be achieved for some $\varepsilon > 0$. A code that is nice enough (but not necessarily perfect) would seem to be enough to answer my question in the affirmative. It might also be the case that Quarter-Subset Membership can be decided efficiently without explicitly constructing a code. On the other hand, such an enumeration may not exist: for instance, every sequence of enumerations for values of $n$ might be inherently non-uniform, or it might be the case that any $(1-\varepsilon)n$-bit bound must be breached infinitely often. 

Discussion Let $\log x = \log_2 x$. It is easy to enumerate all subsets of at most $k$ elements chosen out of $n$ by keeping track of $k$ indices of size $\lceil \log n \rceil$ bits each. (See also the discussion in Knuth's TAOCP section 7.2.1.3.) When $k$ is constant this is just $O(\log n)$ bits. However, if we let $k = cn$ for some constant $c \le 1/4$, then such enumeration schemes use $\Omega(n\log n)$ space. One can also use an $n$-bit characteristic vector together with a check for the number of bits set. I'm interested in schemes that beat $n$ bits. A closely related question is then: 

although this paper does leave several major questions unanswered. The approach via propagators in this paper is closely related to existing constraint solver implementations. I think work on SMT (satisfiability modulo theories) is also closely related to your question. SMT theories are often motivated by problems from software and hardware verification, but there do exist theories with an AI flavour. I look forward to more applications built with SMT as the core technology, and to more work in constraints applying ideas from SMT. 

So no polynomial-time algorithms seem likely for your problem, unless you can exploit special features of the functions you are interested in. 

It sounds like you are looking for a characterisation of the features required to capture diagonalization arguments. Lawvere's Diagonal Arguments and Cartesian Closed Categories unified each of the main diagonal arguments back in 1969, including Cantor's and Rice's Theorems, the halting problem, and GÃ¶del's first incompleteness theorem. Yanofsky's 2003 tutorial discussion A Universal Approach to Self-referential Paradoxes, Incompleteness and Fixed Points provides a nice overview of Lawvere's ideas (arXiv:0305282). More recently, Samson Abramsky and Jonathan Zvesper have been taking this further (arXiv:1006.0992). 

This is an expansion of my comment on Tsuyoshi's answer. I think the negative answer to the question can be made unconditional. This problem seems to require $\omega(n)$ addition operations in the worst case, even for graphs with $O(n)$ edges. Hence it does not seem possible to attain the required bound. Consider a graph $G_{r,c}$ consisting of $r \times c$ vertices, arranged in a grid. The vertices in each of the $r$ rows depend on precisely two vertices in the row above. The family consists of graphs like this, for suitable combinations of values of $r$ and $c$, and suitable arrangements of edges. In particular, let $r = (\log\ n)/2$ and $c = 2n/\log\ n$. Also, let the weights of the top row vertices be distinct powers of 2. Each of the vertices in the bottom row will then depend on $\sqrt{n}$ vertices in the top row. As far as I can tell, there then exists a specific DAG with different values for each of the bottom row weights, such that $\omega(\log\ n)$ non-reusable additions are required on average for each of these sums. Overall this yields an $\omega(n)$ lower bound for the number of additions, while the number of edges is $2c(r-1) = O(n)$. The point seems to be that the underlying partial order is dense, but the DAG represents its transitive reduction, which can be sparse. 

Friedgut provided a rigorous proof of these heuristic arguments. For every fixed value of k, there are two thresholds $\alpha_1 < \alpha_2$. For $\alpha$ below $\alpha_1$, there is a satisfying assignment with high probability. For a value of $\alpha$ above $\alpha_2$, formula $\phi$ is unsatisfiable with high probability. 

Finally, any class of CSP instances can be transformed into a representation with worst-case fractional hypertree width. In many cases this transformation is polynomially bounded in size and can be done in polynomial time. This means that it is easy to generate CSPs with unbounded fractional hypertree width, even modulo homomorphic equivalence. These CSPs are not going to be of the form CSP(A,_) since the target structures are special, but they do provide a neat reason why the CSPs defined by restricting just the source structures are not all that interesting: it is often just too easy to hide the tree-like structure of a CSP instance by changing the representation so that the source structure has large width. (This is discussed in chapter 7 of my thesis.) 

In particular, is it possible for fixed $k$ to reduce $k$-Clique to SAT while keeping the increase in instance size linear? Or can one use an existing result to argue that this is unlikely to be possible? I have tried using Fortnow/Santhanam and Dell/van Melkebeek but the overheads seem too large for these results to imply anything specific. (I have been working with a reduction that seems to avoid the log factor, but before wasting more time on the gory details to verify its correctness, I'd like to know if such a reduction is already known, or if it is unlikely to exist.) 

Some comments: not an answer. If $c$ is small enough with respect to the number of vertices in the graph, then the improper colourings will add up to less than 1. Hence there is a trivial reduction from the weight-0 case to this case: simply choose $c$ to be small enough. This means that the problem is #P-hard for any collection of instances with $c \in [0,\epsilon)$, for any $\epsilon > 0$. (Here I allow $c$ to be different in different instances, so the classes are unions of classes with fixed $c$.) Now suppose that $c$ is truly fixed, as in your problem setup. Then for large enough graphs it is always possible to exceed a weighted sum of 1 for improper colourings, so this direct reduction does not work. You are asking for structural properties of the class of graphs which would allow the problem to remain hard. As far as I can tell, it will be hard nearly always. But this is very sketchy and needs more work. 

for a comprehensive interdisciplinary overview of related terminology for many such problems. These kinds of problems are studied in operational research, management science, computer science, and statistics. There is an extensive literature about nearly every kind of such problem, but it can be hard to find the right terms and to translate between the different styles of exposition. Good luck! 

The second paragraph of RJK's response deserves more detail. Let $\phi$ be a formula in conjunctive normal form, with m clauses, n variables, and at most k variables per clause. Suppose we want to determine if $\phi$ has a satisfying assignment. Formula $\phi$ is an instance of the k-SAT decision problem. When there are few clauses (so m is quite small compared to n), then it is almost always possible to find a solution. A simple algorithm will find a solution in roughly linear time in the size of the formula. When there are many clauses (so m is quite large compared to n), then it is almost always the case that there is no solution. This can be shown by a counting argument. However, during search it is almost always possible to prune large parts of the search space by means of consistency techniques, because the many clauses interact so extensively. Establishing unsatisfiability can then usually be done efficiently. 

A colouring in which every hyperedge is polychromatic (or rainbow) is also known as a strong colouring. Note that a strong colouring of a hypergraph is precisely a proper colouring of the Gaifman graph of the hypergraph. (The Gaifman graph (or primal graph or 2-section) of a hypergraph is formed by adding edges between any two vertices that appear together in some hyperedge.) So if you are looking for a $k$-colouring of an $r$-uniform hypergraph $H$, then you can equivalently look for a $k$-colouring of the Gaifman graph of $H$. The case $r=2$ corresponds to graph colouring, which is polynomial-time for $k=2$ and NP-complete for $k\ge 3$. Obviously $r <2$ is trivial, $k\lt r$ leads to no solutions, and the other cases are all NP-complete. A useful reference which has most of the above definitions is Vitaly I. Voloshin, Coloring Mixed Hypergraphs: Theory, Algorithms and Applications, Fields Institute Monographs 17, AMS, 2002, ISBN 0-8218-2812-6. This book covers the more general case of weak colourings, with particular focus on combining two types of coloured edges: $C$-edges, which have at least two vertices with a common colour, and $D$-edges, which have at least two vertices of different colours. 

The Complexity Zoo points out in the entry on EXP that if L = P then PSPACE = EXP. Since NPSPACE = PSPACE by Savitch, as far as I can tell the underlying padding argument extends to show that $$(\text{NL} = \text{P}) \Rightarrow (\text{PSPACE} = \text{EXP}).$$ We also know that L $\subseteq$ NL $\subseteq$ NC $\subseteq$ P via Ruzzo's resource-bounded alternating hierarchy. 

Problems that are P-complete are not known to be parallelizable. P-complete problems include Horn-SAT and Linear Programming. But proving that this is the case would require separating some notion of parallelizable problems (such as NC or LOGCFL) from P. Computer processor designs are increasing the number of processing units, in the hope that this will yield improved performance. If fundamental algorithms such as Linear Programming are inherently not parallelizable, then there are significant consequences. 

As is hinted at by Razborov's survey, there are open questions about the quantum communication setting; this area is not covered by the 1997 textbook. Edit 2015-03-14: After some thought, I don't understand problem 4, since the telescope protocol of Grolmusz uses $2k-1$ bits of communication to compute $IP^k_n$ per block of at most $2^{k-1}-1$ rows in the communication matrix. For $k \ge 2+\log n$, there is precisely one such block and the cost of the protocol is then just $2k-1$ bits. In particular, for $k=(\log n)^2$ this is $2(\log n)^2 - 1 = o(n^\epsilon)$ bits for every $\epsilon > 0$. In summary, IP seems to have structure that can be exploited in the number-on-forehead setting when there are a large number of parties. To reach the $\Omega(n)$ lower bound in the region $2+\log n \le k \le n/2$ therefore seems to require a different function. 

Yes, my opinion is that classical MapReduce is a BSP model (and therefore has its inherent limitations on the maximum possible parallel performance that can be achieved). However, newer work on MapReduce seems to be focused on looser notions of synchronization, which would take this "generalized MapReduce" out of the strict BSP framework. In particular, if one replicates some of the data then the synchronization structure can be relaxed, yielding performance gains. See for instance work by Foto Afrati and Jeff Ullman: Optimizing joins in a map-reduce environment, EDBT 2010. (preprint)