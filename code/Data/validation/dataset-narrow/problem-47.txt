Now you have both the old UV coordinates and the new UV coordinates. You can use the new UV coordinates for you diffuse map and the old UV coordinates for the occlusion map. 

Weight for final pixel: $$weight(0, 0) = (1.0 - 0.1)(1.0 - 0.1) = 0.81$$ $$weight(0, 1) = (1.0 - 0.1)(1.0 - 0.9) = 0.09$$ $$weight(1, 0) = (1.0 - 0.9)(1.0 - 0.1) = 0.09$$ $$weight(1, 1) = (1.0 - 0.9)(1.0 - 0.9) = 0.01$$ Subtract from 1 both the horizontal and vertical distance (because closer pixels should carry more weight). The scale factor in this example is 0.75 in each direction. Scaling Up vs. Scaling Down Scaling up in the same process as scaling down. By normalizing both the original image and the new image to between (0, 0) and (1, 1), you can do either with the same process. As an exercise, in the first figure, if the red points were from the original image and the black points were from the enlarged image, you should be able to see that the process is the same. 

Stereo instanced rendering is specifically a technique for VR. It relies on the assumption that the scene is pretty much identical in each eye, and so it uses instancing to render to two sides of the screen at once. However, if your bounces render similar geometry, then you may be able to render multiple bounces onto a larger framebuffer provided that the bounces are independent. There is potential for speedup from avoiding framebuffer switching (which is a rather expensive process). In newer OpenGL versions (4.0 and above), there is added support for indirect rendering, which allows you to determine parameters through GPU computing, but it's unclear if that will help you in this case. 

Yes, but with tradeoffs. The problem is that there are two optimizations going on here at different potential bottlenecks, the CPU and the GPU. Reducing Overdraw The goal of reducing overdraw primarily is to reduce the number of times the fragment shader runs via early-z discard. This is usually desirable when you have expensive fragment shaders. However, maintaining a strict depth ordering here may not be necessary. A few common techniques are to either use deferred rendering or to use a depth pre-pass (particularly if you have a forward renderer variant) to minimize the time it takes to run the fragment shaders. The depth pre-pass technique will require double the draw calls and double the execution of the rest of the shader types (vertex, tessellation control, tessellation evaluation, and geometry) twice, but it may be worth it depending on the scene. The fragment shader is executed twice as well, but usually you would use a very simple one for the pre-pass. Preserving Transparency Transparency of course is a different story. Many applications avoided transparency because of two problems: 

When depth testing is enabled, the testing occurs after the fragment shader executes. The procedure is relatively simple (and spelled out pretty well here): 

Context As I was trying to learn more about Vulkan, I learned that certain fixed-function steps could be implemented with shader code from this video (at around the spot where he talks about the following problem): $URL$ In this video, the presenter says "hitching" can occur in OpenGL because for some GPUs the colorblend stage apparently needs to be recompiled at certain times. He gives an example of when a player takes out a torch and causes hitching due to bloom. Questions Why would this shader need to be recompiled in this scenario? In what scenarios would this shader need to be recompiled? Is this common practice/an issue across vendors? 

You should always profile to determine where your slowdown occurs. From your question, it's unclear whether your work is CPU bound or GPU bound. 

GLSL has built-in fragment shader inputs, and one of them is gl_FragCoord. You can get the $x$ and $y$ value from this and use that as the lookup coordinate of the texture map in a sequence of two passes, but that could be a good or bad thing depending on the application (such as caching the result). This would both be rotation invariant while allowing you to scale and move the mesh. First Pass Transform the mesh without rotation. Bind a texture to the fragment shader and write the interpolated UV coordinates to it. Second Pass Transform the mesh correctly this time. Bind the texture from the last pass and read from it this time. You preserved the old UV coordinates, and now you can use these to look up whatever texture you have before. Obviously, this works best with a mesh that's a sphere (such as an eye). 

Explanation With a non-linear scale, you apply different weights for each pixel (or whatever unit you are using). You can use Euclidean distance towards the closest pixels in both directions to determine the weights. Example Normalization For example, in the image below, the red image is the downsampled image (3x3) and the black image is the original image (4x4). The dots represent the sample points (center of the pixels). You can align the two images by treating each pixel coordinate as a set of two values (x and y) between 0 and 1 (inclusive). Pixels should not be treated as square, so you should treat each pixel as a point rather than a square (when aligning the two images). 

With flat shading, the normal is the same across the whole triangle (or quad or whatever). There are some assumptions that were made here and two approaches. The latter approach will probably be best for most basic problems (although more work). We don't have the winding order, so we need to calculate the normal based on the vertices. Average normal approach The assumption is that each vertex contributes the same amount of weight towards the triangle normal. We first calculate the average of each: $$normal = normalize(0,0,1) + normalize(1,1,0) + normalize(-1,-1,1) \approx \frac{(0,0,1)+(0.7,0.7,0)+(-0.58,-0.58,0.58)}{3} \approx (0.04,0.04,0.53)$$ $$normalize(0.04,0.04,0.53) \approx (0.076,0.076,0.994)$$ For Lambert shading: $$cos(\theta) = normalize(0,3,4) \cdot (0.76,0.76, 0.994) \approx 0.84 $$ Geometric approach (recommended for this problem) Another assumption Nathan Reed suggested was that each vertex normal holds equal weight, which would make sense in more common use cases. For this, we will want to find the face normals in both winding directions to compare this value to: Winding order 1 $$normal1 = normalize((B-A) \times (O-A)) \approx (0.29,0.43,-0.85)$$ Winding order 2 $$normal2 = normalize((O-A) \times (B-A)) \approx (-0.29,-0.43,0.85)$$ Now, we can take the dot product between these two vectors and the one we computed above. Whichever one is greater or equal to 0: $$geometric\ normal1 = normal \cdot normal1 \approx -0.79$$ $$geometric\ normal2 = normal \cdot normal2 \approx 0.79$$ So then, $$normal = geometric\ normal2$$ For Lambert shading: $$cos(\theta) = normalize(0,3,4) \cdot (-0.29,-0.43,0.85) \approx 0.42 $$ 

Ambient occlusion isn't a technique, but rather a concept. It can be implemented (and approximated) as a post-processing effect such as SSAO, SSDO, HBAO+, etc. @trichoplax gave a good answer, I just wanted to elaborate on the comments. 

Summary TL;DR: there are too many inefficiencies that software rendering can't past, and these things add up. There are also many larger limitations, especially when you are dealing with VRAM bandwidth, synchronization problems, and extra computations. 

I personally would consider deferred rendering to be a post-processing effect, but it depends on your definition of a post-processing effect. For this answer, I will assume it's anything that doesn't require geometry other than a screen-space quad or (triangle) to be rendered. Post-Process Techniques Screen space AO techniques involve reading the depth and/or normal buffers to determine if light should be occluded. There are expansions on these techniques that deal with temporal aliasing as well as occlusions by objects in the scene. Some people also apply AO to separate parts of the scene (dynamic and static objects for instance). This is where the definition of post-processing might become ambiguous. Some renderers do post-process AO before the lighting pass however. Usually these renderers either do a Z-prepass or are light-prepass renderers. This can be used on devices with lower VRAM or eDRAM to preserve memory since you can also do tonemapping during your lighting pass. World-Space Static Techniques Lightmaps can be used particularly for static environments AO can be easily computed offline. Similarly, art assets can have AO textures. World-Space Partially Dynamic Techniques Some objects just need rough AO and are largely rigid. For instance, AO fields and decals can be used with or without SSAO to improve AO for some objects (by Nathan Reed). Another example includes placing a circle around a character's feet. World-Space Dynamic Techniques NVidia came up with VXAO, which is a voxel-based ambient occlusion effect that helps simulate AO in places that aren't visible of screen. This has been one of the downsides to SSAO. Another issue with SSAO is that it must be computed each frame and is subject to temporal aliasing (bad for VR, although some people have developed improvements for this). VXAO can also reduce the memory/production costs of having many AO textures. Also, VXAO can have considerable latency with updates (if the application wants this). However, VXAO has a high performance cost. Other applications, such as voxel-based games, can approximate AO very easily by using algorithms based on neighborhood information. This is kind of similar to VXAO, but without the rasterization (voxelization) steps, and they may have much rougher approximations. As you can see, there are many implementations of AO, many of which aren't post-processing effects. 

The draw commands are for allowing you to render multiple objects in one draw call. In terms of OpenGL 3.0 and later, this means that you could specify a VBO holding a bunch of objects and use to render them. You would use this with the same shader. If you want to render the same object multiple times, then using something such as instancing would be a good solution. This all concerns draw calls though. If you have one million triangles for only a few meshes, you won't see a massive speedup with rendering. You didn't specify your framebuffer size, but if it's small enough, you can cause severe overdraw from many small triangles. If the images are really that small, maybe you should think about adding an LOD system or use lower polygon meshes and tessellate. 

Typically, you calculate lighting in either world-space or view-space (or camera-space). The benefit to calculating it in camera-space is the increased precision, although this may involve introduce additional math operations in shaders depending on your setup. So it wouldn't be in screenspace, or you would need to reconstruct the positions (which could happen in deferred shading depending on your G-buffer layout). I assume you are talking about a punctual light type (such as a point light). If so, if your light position equals your camera position, then the view and light angles are the same, which can be factored out in the BRDF that you use, so you save a dot product. 

In the above image, the four corners would all carry the weight of the original image's four corners, for example. The points in the middle would carry different weights depending on the difference between the four neighboring points. With numbers: This is a hypothetical area to sample in larger images (not related to the image above). The distances are between original pixels are normalized from 0 to 1 to simplify the math. The black dots represent the sample points (pixels) of the original image. The larger red dots represent the sample points of the downsampled image. The red dot in the top left is closer to the black dot in the top left than the other three surrounding dots, so that pixel will have a heavier weight for determining the color of the downsampled pixel. 

In short, performance reasons are why they aren't programmable. History and Market In the past, there used to be separate cores for vertex and fragment processors to avoid bloated FPU designs. There were some mathematical operations you could only do in fragment shader code for instance (because they were mostly only relevant for fragment shaders). This would produce severe hardware bottlenecks for applications that didn't max out the potential of each type of core. As programmable shaders became more popular, universal units were introduced. More and more stages of the graphics pipeline were implemented in hardware to help with scaling. During this time, GPGPU also became more popular, so vendors had to incorporate some of this functionality. It's still important to note though that the majority of the income from GPUs were still video games, so this couldn't interfere with performance. Eventually a big player, Intel, decided to invest in programmable rasterizers with their Larrabee architecture. This project was supposed to be groundbreaking, but the performance was apparently less than desired. It was shut down, and parts of it were salvaged for Xeon Phi processors. It's worth noting though that the other vendors haven't implemented this. Attempts at Software Rasterizers There have been some attempts at rasterization through software, but they all seem to have issues with performance. One notable effort was an attempt by Nvidia in 2011 in this paper. This was released close to when Larrabee was terminated, so it's very possible that this was a response to that. Regardless, there are some performance figures in this, and most of them show performance multiple times slower than hardware rasterizers. Technical Issues with Software Rasterization There are many issues that were faced in the Nvidia paper. Here are some of the most important issues with software rasterizers though: Major Issues 

Old Hardware Some older cards didn't used to jump instructions in warps, so this was indeed an issue. If you have a conditional with these cards, the inside logic of the conditional would still be evaluated even if the block wasn't entered by any of the pixels, vertices, etc. There's little you can do in this case because every instruction will be executed. The worst case is also the best case here. Modern Hardware However, if you are working with relatively modern hardware though, this isn't an issue and "if/else" blocks will be skipped when necessary. There really shouldn't be a downside to using "if/else" blocks compared to not using them. You are still branching with an "else"-less statement. In fact, some post-processing effects/deferred rendering specifically benefit from this. The worst case scenario is that some threads go through the "if" block and the rest go through the "else" block. The best case scenario is that all go through the "if" block or the "else" block. There isn't a downside to using "if/else" conditionals as they are needed (either on old hardware or newer hardware), but there's a possible upside with newer hardware. 

Mapping the Left and Right Sides This step should be trivial. If I remember correctly, the CMU database animation skeletons have easily identifiable bone names. If not, then it's slightly less trivial. What you would have to do then would be to start at the root and go down each side. The bone structure should be the same for each animation though. Create the Rotation Matrix You should have three Euler angles: $X$, $Y$, and $Z$. You first need to create a rotation matrix for each bone. I use $\phi$ for $X$, $\theta$ for $Y$, and $\psi$ for $Z$ since this is a more common notation for matrix $R$. This is a right-handed matrix. $$ \begin{bmatrix} \cos(\theta)\cos(\psi) & \cos(\phi)\sin(\psi) + \sin(\phi)\sin(\theta)\cos(\psi) & \sin(\phi)\sin(\psi) - \cos(\phi)\sin(\theta)\cos(\psi) \\ - \cos(\theta)\sin(\psi) & \cos(\phi)\cos(\psi) - \sin(\phi)\sin(\theta)\sin(\psi) & \sin(\phi)\cos(\psi) + \cos(\phi)\sin(\theta)\sin(\psi) \\ \sin(\theta) & - \sin(\phi)\cos(\theta) & \cos(\phi)\cos(\theta) \end{bmatrix} $$ Flip the Sides To flip the rotation matrix, you will need to multiply the axis you want to flip by $-1$. This is equivalent to scaling in across that axis. I will assume you want to scale across the x-axis for matrix $S$. $$ \begin{bmatrix} -1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} $$ Combine the Two Transforms Now just multiply these two matrices, and you get the opposite rotation: $$T = R \times S$$ Chain Together the Local Transforms Now you just need to apply forward kinematics, so you need to multiply each parent matrix by the local matrix to get an absolute transformation matrix. 

Early-z discard is when a rasterizer discards a fragment before a fragment shader runs because it's clear early on that it won't pass the depth test. However, the rasterizer can't safely do this if certain things happen (such as calling or modifying the fragment depth in a fragment shader). This is a hardware optimization and isn't required to occur unless you force it with OpenGL 4.2 or with an extension. There are things that can prevent early-z discard from occurring. To achieve it: 

Both suggested methods are fine, but they should be preferred in certain situations. Dynamic state should be preferred in situations where you have many, perhaps thousands of, pipeline objects or you need to change the resolution often (such as in some sort of windowed tool. The downside to this is that it may reduce runtime performance since the driver may not be able apply optimizations for this. Having the viewport only defined when building the pipeline objects might be more appropriate for games or simulators where you don't change the resolution often. Of course, when the viewport does change, it hits performance hard during resizing. This also has the drawback of needing to generate new pipeline objects, which could be especially problematic if you cache pipelines in a hash table of something similar. 

Even if you are able to sort each object by depth at the VBO or VAO level, it's still not guaranteed to be accurate. For example, two objects could overlap each other. Even within the same object, two triangles can overlap each other, so unless you perform clipping, the transparency order may not be correct. You can sort by the fragment level. There are various order-independent transparency techniques, ranging from screen-door transparency to more accurate variants such as depth-peeling. However, if you are concerned about draw calls and want accurate transparency, then there are techniques that are single-pass order-independent transparency that are becoming more popular. These rely on saving fragment shader output values in additional buffers (generally called A-buffers) and sorting through them later, performing blending manually. These single-pass techniques requires shader atomics though, so OpenGL 4.2 is required.