I have a query that has been several minutes in waiting and is not running at all. In the SQL Developer monitor session view it shows that query has been waiting for 667 seconds? 

The error seems to be misleading and I suspect something else is the problem here, because the users table has only some 1000 users or so and the tablespace in question has 2 GB allocated to it, all of it free. Furthermore trying to copy just a single row also gives the same error: 

The requirement to specify a directory object is only there with the new utility. The older tool allows specification of an absolute path. I was able to take a backup using without having any special privileges and without creating any directory objects, just using my username and password. Though succeeded by , is still included in recent versions of Oracle. For example an dump would look like: 

Oracle TDE features comes with various limitations such as not being able to encrypt columns which are used in foreign key constraints. And probably is available only with enterprise edition which costs upwards of 100 thousand dollars. Why would one want to pay and use TDE instead of simply using file system encryption with the OS? This is free and byepasses various limitations regarding FKs etc. Am I missing some advantages that it provides? 

Yes, there is. transfer the files to a new server, if you have one, using the same file structures. next create a init.ora file with the dbname and control_file parameters. Start the instance using that init.ora in nomount. If you happen to have snapshot controlfiles in $ORACLE_HOME/dbs/ of you database, start rman: 

It depends on the exact version. From 11gR2 we can use job_queue_processes = 0 to prevent any job from running. Before 11gR2 you could use services for that. To use services make the job classes that you want to use depend from a service that is controlled using the instance parameters and leave it out when starting the service. 

Using a job status from a scheduler is not reliable at all, often the backup scheduling has problems so it is important to also notice that backups are not started at all. For this you can find a zabbix Oracle module at github that does the backup monitoring based on v$rman_status. Zabbix does a real good job for this kind of monitoring. 

Which says is the costgroup repeated and is it standard material, if so mark substitution variance as 0. I thought you couldn't use OVER within a CASE statement? Or is it only certain window functions you can't use within CASE? 

I'm trying to figure out how to create a From and To column based on ID.LocationID Column. I'm hoping with a suggestion this can be done (i can figure it out) without having to create sample tables and data as that would take me all day. (fairly complicated query) I'll paste basic structure to give you an idea of what i'm doing... 

but this gave me 6 * 6 36 rows... I want to just bolt the right query on to the end of the left query. exactly like a union 

we have a DWH load job that sometimes hangs. We have not been able to determine why this is happening it seems to be random and network/hardware related (which we have no control over). We are ordering new hardware which will isolate our processes but this is weeks away. Therefore when our job hangs, we would like to specify a timeout option if any of the steps reaches a certain amount of time and have it automatically restarted. How can this be done? Thanks! 

I'm using following PLSQL loop to delete a large number of records from my table. However it is giving me an error that loop exit condition is not correct. 

Given that natural join here is implemented using inner join why the multiple rows? Update: Running same join with table aliases has different output: 

So a natural join can be a shorthand way of implementing inner join if both tables have a common column. Consider following table: 

I've seen this referred to using different terms: inline views, WITH clause, CTE and derived tables. To me it seems they are different vendor specific syntax for the same thing. Is this a wrong assumption? Are there any technical/performance differences between these? 

Why does oracle give invalid number error when truncating the timestamp? I need to do a group by with date on this view. 

I have a query that is selecting some data out of a table. The execution plan shows that an INDEX (FAST FULL SCAN) is used and the cardinality is 22. On the running the query it gives 227,652 rows. I understand that query optimiser relies on row count estimates instead of actually counting rows. So I run a And it gives 910,087. So why is the cardinality estimate in query plan off by such a large margin? Query as request in comments: 

For sequences the best way is to copy the current value from dba|all|user_sequences.last_number. Fetching the current value from the sequence is not safe; it can have higher values in other instances of the database. Also make sure that your app is not using sequences for ordering .... 

This would have taken care for cleanup of the old tablespaces. I also expect that you would have gotten errors during the import. When messing with databases and tablespaces, a little dba training would be very good. For 10g start here Oracle® Database 2 Day DBA 10g Release 2 (10.2) 

instance_name has a default value null implicating it is not a required parameter. In a RAC database this would just enable tracing for your service in all open instances. 

Looking at the paths of the datafile, I assume/hope this is no production database. If this database is setup as a regular production database, it is running archivelog mode. In that case it is quite simple to recover, if you have a valid backup for the damaged datafile[s], including all archived logfiles created since the start of that backup. restore the datafile[s] effected recover the database open the database. Oracle still has the docs for this ancient release online. Check Oracle8i Backup and Recovery Guide for the details. After that, upgrade to a current release. My guess is that many dbas have never had training on this release .... I think having a good dba at hand could be helpful... 

Gives absolutely nothing. What can I do to get sql history over a longer period of time, say 2 months? 

I had to modify René Nyffenegger's answer to make it more generic and easier to see space usage for all indexes in a schema. Thought I'd share the modified code here, in case anyone else finds it useful: 

I have two schemas and . I'm trying to copy over table to . I've followed these steps: Run in schema : 

I'm trying to understand the performance characteristics of SQL statement that does DELETE cascade on 4 linked tables. When I do the auto tracing of the statement it only shows me the DELETE statement executed on the table. It does not show details of deletes done on other tables. Why doesn't SQL Developer show cascades in tracing? 

This may sound like an overly simple question but I'm not finding it easy to locate a proper answer. To the question "What are sql clauses?" most of the resources on internet simply provide a list of clauses and explain what they do. But I'm trying to understand in abstract terms how it is defined. Like a generic notation that captures all kinds of queries that can be written and how clauses fit in there. Are there properties common to all clauses? Why is UNION called an operator whereas HAVING is a clause? 

Basically i'm selecting from huge table (in cte) then selecting from this and joining on to other tables below. Sample output 

Hi all i'm building a data warehouse and noticed that my tran log is massive (in simple recovery). MDF file is 2.6 GB. LDF file is 7.8 GB!!! Why is this??? i don't want a stupid tran log that's why i've put in simple recovery! (i know the system needs tran log). so i tried to shrink the DB and get this error message. 

I need to multiply these together for result to replace values in PackWeight/Vol i.e. Units/Pk * PackWeight/Vol AS PackWeight/Vol. How do i do this? i.e. where units is 4 result would be 400g, 500g, 500g respectfully. Other results would stay the same as units are 1. Units/Pk is numeric and packweight is nvarchar. Thanks! 

Hi all please see attached data below. I'm trying to get a row based on various conditions. scenario 1 - get highest row if no hours exist against it that has (setup + processtime > 0). scenario 2 - if there's hours (like in this example) show next operation. (which would be 60). 

One of the nice features of Oracle are those like Oracle Resource Manager that make consolidation of applications in one database a lot easier than having a separate database for every application. It looks like this is not used in your organization. I have designed consolidated databases in the past for projects just like you mention. 

Yes, you can and it is quite easy too. In Oracle, the ORACLE_SID is just the name for the Oracle Instance and has not very much to do with the DBNAME. A database with the name PROD, can be served using Instances with any valid name. There is no direct connection between the SID and the DBNAME. This connection is made using the parameters. The parameter file is identified as init${ORACLE_SID}.ora or spfile${ORACLE_SID}.ora In the parameter file is the parameter db_name. This is where the connection between the Oracle Instance and the database is made. So, you don't need to re-create a controlfile, you don't need to use nid, just make sure that your parameterfile has the right name, bring down the old Oracle Instance and start the new Oracle Instance after having set ORACLE_SID to the new Oracle Instance name. The parameterfile and the password file are both found using the ${ORACLE_SID} as part of their name. Re-creating the controlfile is only needed when the DBNAME has to change. nid is needed after a clone operation where you need to change the DBID to prevent accidents that could hurt the backups of the source database. 

There's a simple answer to this. If the relationship between the tables does not exist to give you the results you want, you need to CREATE your own relationship. I used ROW_NUMBER OVER (ORDER BY PALLETID) to give me unique ID's for both datasets. THen simply join (make sure you check different joins to make sure you don't exclude rows from one side or the other) ACODEID = BCODEID. Simple! 

seeing as no one has helped me with this i'll answer myself to help anyone who is looking at the same thing. It looks as if to enable CDC on an existing warehouse you need to do this on a table by table basis, recommending capturing all the ETL in package per table. Also, it is recommended to enable CDC state per table to avoid conflicts. Recommended approach would be --> enable CDC on source DB. Transfer incremental loads to staging DB or schema inside DWH. Incrementally load records to Dimensions and Facts. Every example i can find on net is for one table, which is fairly useless... be much appreciated if someone can find a CDC example for multiple tables... 

If you are using the 11g duplicate from active database option, the only port that needs to be open is the listener port (usually 1521). If you are using any other options to copy your files to the standby host, the required ports for the protocol that you use (scp/ftp/nfs ...) Apart from the 11g 'from active database' option, RMAN does only read backupfiles that are present on the node on which the restore/clone is to be taking place. Present in this setting means that they thould be readable. The can be on [n]fs or tape, as long as your host can read them it is ok. I hope this helps 

Easiest is to use Oracle Managed Files for this. Oracle will just place -and name- the files in the locations configured by the 

your database is running in archivelog mode. An initial backup + archives allow you to recover your database to any point in time between the backup and the last available archivelog file. If this is a big import you are most likely going to make a backup afterwards. If you have no need to recover the database to a state between now and the previous backup, you could use rman to delete the archived log files, freeing space so the archiver can continue. An other option is to increase the db_recovery_file_dest_size parameter that could be in place since you claim to have plenty of space available.