(In practice, I could definitely restrucure my algorithm to use depth testing instead, but it is a problem that makes me curious about the theoretic implications and underpinnings nevertheless.) 

When using Transform Feedback to capture the vertices generated by the primtive generation stage, we can use to directly draw the generated data without the need for querying the number of generated primitives and thus without CPU-GPU synchronization, which is especially useful with geometry shaders generating a possibly unknown number of primitives. However, in my case I'm not using the generated primitive data per-vertex, but per-instance (it's basically a classic GPU-based instance culling scenario). So I need to supply the number of generated primitives into the instance count parameter of . But the various Transform Feedback rendering functions only ever use the primitive count as vertex count and never as instance count. Now this does require me to wait for the transform feedback to finish and get the result of the corresponding query object, thus instroducing a synchronization point. I considered that I could also use the quite powerful Indirect Rendering functionality and together with a Query Buffer Object route the result of the transform feedback query directly into the instance count field of the draw indirect buffer. But this has the problem that I'm not only issuing a single call, but multiple different ones (each with their own state, per-vertex attributes and draw parameters, but all with the same instance count and per-instance attributes). So I would actually have to scatter the query result into multiple drawing parameter sets (though, possibly in the same buffer), probably by use of a compute shader. But this just seems like overkill to me. So my question is, is there any other less involved way to just use the number of primitives/vertices generated from transform feedback directly as the instance count in (preferably multiple different) instanced rendering commands without the need to explicitly query that count onto the CPU? In light of GPU-based scene culling techniques I can't imagine this to be a rare use-case (but maybe in that case you just have to go the way over indirect drawing commands anyway). I'm not afraid of using extensions if available, so maybe there's some obscure little extension that does exactly that? Or I'm just missing something else obvious here. 

After five bounces, RR is used to determine if the ray path continues or not. If it continues, the BRDF is scaled to compensate for it. This is Ok for me. RR is also used in a second code segment to select between refraction or transmission during the rendering of dielectrics (expanded code version): 

We can evaluate the total amount of radiance $L_r$ reflected by a multilayer BSDF considering each layer as a individual object and applying the same approach used in ordinary path tracing (i.e. the radiance leaving a layer will be the incident radiance for the next layer). The final estimator can thus be represented by the product of each individual Monte Carlo estimator: $$ L_r = \left( \frac{fr_1 \cos \theta_1}{pdf_1} \left( \frac{fr_2 \cos \theta_2}{pdf_2} \left( \frac{fr_3 \cos \theta_3}{pdf_3} \left( \frac{fr_2 \cos \theta_4}{pdf_2} \left( \frac{L_i fr_1 \cos \theta_5}{pdf_1} \right)\right)\right)\right)\right)$$ The paper by Andrea Weidlich and Alexander Wilkie also takes absorption into consideration, i.e. each light ray might be attenuated according to the absorption factor of each transmissive layer and to the distance traveled by the ray within the layer. I've not included absorption into my renderer yet, but it is just a real coefficient computed according to the Beer's Law. Alternate approaches The Mitsuba renderer uses an alternate representation for multilayered material based on the "tabulation of reflectance functions in a Fourier basis". I have not yet dig into it, but might be of interest: "A Comprehensive Framework for Rendering Layered Materials" by Wenzel Jacob et al. There is also an expanded version of this paper. 

This clearly contradicts my practical experience and made me a little worried/confused. I took a look at the specification (both the newest 4.6 core as well as good old 2.0, which we're actually targetting in our project for compatibility) and it also says that occlusion queries count the number of samples passing the depth test. It does however not make such a definite statement as the wiki pertaining to the stencil test. And since the depth test technically comes after the stencil test, I'm not entirely sure if that maybe already includes the stencil test anyway and the wording of the specification is simply using the last stage for convenience. So, my question is, do occlusion queries respect the stencil test (or any other earlier per-fragment tests, I'm actually using scissor testing with occlusion queries in another unrelated algorithm)? I see a few possiblities here: 

While the other answers are giving possible techniques to achieve what you're looking for, rendering your stuff into a texture is not always the best way if you need it as VBOs (i.e. vertex attributes) in the next render step (requiring either a read into a PBO and some index/coordinate/sizing magic or a vertex texture fetch). An approach that would more accurately represent your 2nd point and be better when you need a transformation from a buffer into a buffer would be what's called Transform Feedback (or Stream Output in the DirectX world). However, this requires OpenGL 3/DirectX 10 hardware (however, not necessarily the GL4/DX11 needed for compute shaders). It's also a little less of a synchronization mess than compute shaders with arbitrary shader storage buffers when your use case is really just to transform a bunch of vertices/primitives into a nother bunch of vertices/primitives. It basically lets you hook into the render pipeline after the vertex (or rather primitive) processing step and before the rasterization and write out whatever per-vertex values you computed directly into a buffer object (that you can then use as vertex attributes in the next step). It therefore doesn't draw anything into the framebuffer either. You just specify a bunch of variables and how to format them into the buffer and bind a buffer to the corresponding binding point. You can also use a query to gather information of how many primtives were written (since a geometry shader could change that dynamically) or, in later versions, just render the results directly without any need for synchronization. For more details, see the above Wiki article. Together with the geometry shader (which TF hardware should have, too) and its abilities for changing the primtive type or removing/adding vertices, this makes a useful tool for transforming your geometry however you deem fit in preparation of further rendering stages. Of course, it has to be evaluated if that really buys you anything in the particular use-case, especially if you're really just doing a simple vertex transformation. 

You've said that "... bilinear filtering on in the texture ...". It seems that you are interpolating the depth values of the shadow map. The correct way of using interpolation with the shadow map is to apply it over the outcomes of the shadow tests (as far as I remember, OpenGL supports that). You might even combine the interpolation of the outcomes of the shadow tests with PCF, which will deliver much better results. However, as you might have noticed, aliasing is a plague that always pursue the shadow mapping :) Although I understand that you are looking for solutions regarding shadow mapping (even because it is quite simple to implement), have you ever considered the use of shadow volumes? It is much more intricate to implement, but does not suffer from aliasing at all, and I think would fit nicely your purposes. 

According to the code above, the path branches recursively if the ray has bounced up to two times. After two bounces, however, RR is used to select the path to be followed. This is also Ok for me. What is a bit confusing is the fact that the radiance returned by both possible non-branching paths (refraction and transmission) is scaled. I understand that there are different probabilities regarding reflection and transmission. However, if for instance Re = 0.3 and Tr = 0.7, and 100 rays strike the surface, about 30% of the rays will be reflected and 70% of will be transmitted due RR. In this case, I understand that there is no path termination neither energy loss, so there wouldn't be anything to compensate for. Thus, my first two questions are: why are these radiances scaled? Should they be scaled, or would it work without scaling at all? My third question is related to the scaling factors: Why the author has used P, RP and TP instead of Re and Tr? Any indication of a good reading about this topic is also very welcome!! Thank you! 

Well, yes. But that's not actually what you want to do in skeletal animation. You have it reverse. It's the other way around. You assume you have a point in C space and want to compute its position in root/global space, in order to pump it further through the transformation pipeline, which gets increasingly more global with each transformation until your point ends up in eye-space (and further). That's why A doesn't transform from root space into A space, it transforms from A space into root space. So $ABC$ transforms from C space into root space. Now your mesh vertices already are in root space, that's where the inverse bind pose comes into play, which transforms a point from root space into C space. So you basically transform your point from root space to C (or whatever joint) space with the inverse bind matrix that's only dependent on the joint and constant over the whole animation and then from that joint space back into root space with the animation-dependent joint transformation. So C's transformation matrix for frame $t$, that transforms a vertex from unanimated root space, which is how your mesh is defined, into animated (with respect to C's joint angle) root space would be $(A_RB_RC_R)^{-1}A(t)B(t)C(t)$, where $M(t)$ is the local joint matrix at animation time $t$ and $M_R$ is the local joint matrix corresponding to the rest pose, the pose in which the mesh is defined, and thus $(A_RB_RC_R)^{-1}$ is C's inverse bind pose. 

This question is somewhat related to this one. As Alan has already said, following the actual path of the light ray through each layer leads to more physically accurate results. I will base my answer on a paper by Andrea Weidlich and Alexander Wilkie ("Arbitrarily Layered Micro-Facet Surfaces") that I have read and implemented. In their paper they assume that the distance between two layers is smaller than the radius of a differential area element. This simplifies the implementation because we do not have to calculate intersection points separately for each layer, actually we assume that the intersection points are the same for all layers. According to the paper, two problems must be solved in order to render multilayered material. The first one is to properly sample the layers and the second is to find the resulting BSDF generated by the combination of the multiple BSDFs that are found along the sampling path. Sampling In this first stage we will determine the actual light path through the layers. When a light ray is moving from a less dense medium, e.g. air, to a more dense medium, e.g. glass, part of its energy is reflected and the remaining part is transmitted. You can find the amount of energy that is reflected through the Fresnel reflectance equations. So, for instance, if the Fresnel reflectance of a given dielectric is 0.3, we know that 30% of the energy is reflected and 70% will be transmitted: 

It just depends on which space your light directions are actually specified in. You compute both your normal and view vector correctly in view space in order to perform your lighting computations in view space. So you need your normalized light direction in view space, too. So it depends on how you actually transfer your light direction into the shader. Usually it's best to transform the light direction into view space before uploading it to the shader (from whatever space it's originally in in the scene, likely some kind of object space local to a certain light source, but in case of a directional light maybe already in world/view space). This has a few advantages: 

As you analyzed correctly, GPUs aren't particularly good at dynamically sized data structures. A common approach to situations like this is speparating the problem into multiple passes, first computing the data in a rigid fashion and then compacting it for further processing. Off the top of my head, one way to approach this would be: 

If your viewport is not the same as your window dimensions however, you need to fiddle with the mouse coordinates a little more and transform them from the entire window into your viewport rect before unprojecting them. But the point is that expects window space (i.e. pixel) coordinates. 

As can be seen, TIR or Fresnel reflectance might keep some rays bouncing indefinitely among layers. As far as I know, Mitsuba implements plastic as a two layer material, and it uses a closed form solution for this specific case that accounts for an infinity number of light bounces among layers. However, Mitsuba also allows for the creation of multilayer materials with an arbitrary number of layers, in which case it imposes a maximum number of internal bounces since no closed form solution seems to exist for the general case. As a side effect, some energy can be lost in the rendering process, making the material look darker than it should be. In my current multilayer material implementation I allow for an arbitrary number of internal bounces at the cost of longer rendering times (well... actually, I've implemented only two layers.. one dielectric and one diffuse :). An additional option is to mix branching and RR. For instance, the initial rays (lower deep levels) might present substantial contribution to the final image. Thus, one might choose to branch only at the first one or two intersections, using only RR afterwards. This is the case with smallpt. An interesting point regarding multilayered materials is that individual reflected/transmitted rays can be importance sampled according to the corresponding BRDFs/BTDFs of the current layer. Evaluating the Final BSDF Considering the following light path computed using RR: 

You might, however, still want to compare this approach's performance against an approach using a small uniform array (i.e. , or maybe less in practice, keep an eye on your implementation's limit on uniform array size) which you just load with the respective values using and access by adjustig your angle to $[0,360)$ using the function and rounding it to the nearest value: 

This is not true, $J^+J$ is not necessarily $I$. $JJ^+=I$ but the multiplication by the pseudoinverse is not commutative. Let's look at this in more detail: $JJ^+ = JJ^T(JJ^T)^{-1}= (JJ^T)(JJ^T)^{-1} = I$ $J^+J = J^T(JJ^T)^{-1}J = ?$ The underlying misunderstanding comes from the assumption that the matrices $J^T(JJ^T)^{-1}$ and $(J^TJ)^{-1}J^T$ are equal (since the latter would produce $I$ if right-multiplied by $J$), which is not true. They are both pseudoinverses of $J$ but they are not necessarily the same matrices, in fact they don't necessarily both exist. As the book says, the $(JJ^T)^{-1}$ only exists if the rows of $J$ are linearly independent, and in turn Wikipedia says that $(J^TJ)^{-1}$ exists if the columns of $J$ are linearly independent. However, they can't both exist if the matrix is not square, since one of the dimensions would necessarily have more vectors than the dimension, which can't all be linearly independent. Specifically, for a normal redundant joint system you probably have more columns than rows, making only the pseudoinverse $J(JJ^T)^{-1}$ exist. We can, however, see that for a square matrix they are indeed the same and actually the real inverse: $J^T(JJ^T)^{-1} = J^TJ^{-T}J^{-1} = (J^{-1}J)^TJ^{-1} = I^TJ^{-1} = J^{-1}$ $(J^TJ)^{-1}J^T = J^{-1}J^{-T}J^T = J^{-1}(JJ^{-1})^T = J^{-1}I^T = J^{-1}$ (But in that case the system would have a unique solution and there's no point to angle control anyway.) But you cannot resolve the brackets with a non-square matrix, since the individual matrices aren't square and don't have an actual inverse. The only situation when both matrices exist are square $J$s and then they are indeed equal, but then there's also no point to angle control. But I understand why you are confused and I am a little, too, since Parent actually seems to use the alleged identity $J^T(JJ^T)^{-1}=(J^TJ)^{-1}J^T$ in equation 5.20, where he derives the pseudoinverse's relevance for solving the inverse kinematics problem. He computes everything with the left inverse but than concludes with using the right inverse. I guess that works conceptually because they're both pseudoinverses, even if it doesn't make complete mathematical sense (to me and you at least). He seemed to have used the assumption of a square matrix as a trick for transforming the problem and then used the conclusion for a non-square matrix, which might be conceptually correct. I guess it simply doesn't matter which of the two pseudoinverses we use, since only one of them ever exists (for a non-square Jacobian) anyway and if the left inverse would exist (making your above assumption true), that means you have more constraints than DoF and thus your system is overconstrained and there's no point in angle control. 

When light hits a conductor or a diffuse surface, it will always be reflected (being the direction of reflection related to the type of the BRDF). In a multilayer material, the resulting light path will be the agregate result of all those possibilities. Thus, in the case of a 3-layer material, assuming that the first and secong layers are dielectrics and the third layer is diffuse, we might end up with the following light path (a tree actually): 

When the light ray is moving from a more dense to a less dense medium, the same principle described by the Fresnel reflectance applies. However, in this specific case, total internal reflection (a.k.a TIR) might also happen if the angle of the incident ray is above the critical angle. In the case of TIR, 100% of the energy is reflected back into the material: 

I was taking a look at Smallpt ($URL$ more specifically at the Russian Roulette part. Actually, RR is used in two places along the code: first, to determine ray termination; and second, for the rendering of dielectrics. That is the code for the ray termination case (expanded code version): 

We can simulate this type of interaction using recursion and weighting each light path according to the actual reflectance/transmitance at the corresponding incident point. A problem regarding the use of recursion is that the number of rays increases with the deepness of the recursion, concentrating computational effort on rays that individually might contribute almost nothing to the final result. On the other hand, the aggregate result of those individual rays at deep recursion levels can be significant and should not be discarded. In this case, we can use Russian Roulette (RR) in order to avoid branching and to probabilistic end light paths without losing energy, but at the cost of a higher variance (noisier result). In this case, the result of the Fresnel reflectance, or the TIR, will be used to randomly select which path to follow. For instance: 

Or you compute just the same and don't unproject the origin at all, rather than just transforming the eye-space origin (which is conveniently $\mathbf{0}$) back into model space (assuming your modelview matrix isn't projective, which it really shouldn't be, otherwise don't forget the W-division): 

First, adressing the concerns of speed and performance, you will always have to make some trade-offs between quality and performance, as you already do. If the first version looks fine enough for your use-case and all other versions are much slower, you have to make the decision yourself. You will, however, not get around actually measuring any time difference. But since various ways to do this are not too hard to integrate into an existing wide lines renderer, just testing the various approaches in action and getting some time measurements shouldn't be too difficult, especially with OpenGL timer queries (or whatever the Direct3D equivalent). So let's get to some possible improvements to your situation. It seems from your link and your explanations that you're using a classic geometry shader based approach for wide line rendereing, converting each line into a screen-aligned quad (or two triangles), computing the vertex positions based on the line width you want to achieve. I recently built something similar, too and there's a few ways you can improve the quality.