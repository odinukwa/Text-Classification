I would like to post this as answer. You can use various methods to make sure you are able to knock out any connecton to database when you are trying to restore but there are repercussions when you use command 

Adding to what is already mentioned by @Gameiswar, the other difference which I know with , when you run it for all tables of database it ONLY updates statistics of tables which has at least one row changed. BOL says 

Yes you can do it. But is it not possible for you to take transaction log backup on some other drive. Transaction log backups will truncate logs(if no long running transaction still requires it) and then you can shrink the log file. You must also note that shrinking logs also cause performance issue if you are doing it for just one time its ok. Why did log file grew out of proportion have you thought about it ? I guess because you did not took transaction log backups. Please schedule proper transaction log backup for the database if you don't require point in time recovery why not let database in simple recovery mode and you can still take full and differential backups. 

I will take you question point wise 1.In my opinion its higly unlikey for an unused large table to cause issue with query running for diffrent database. SQL Server memory is dynamic in nature if suppose large portion of memory is occupied by datapages of DB1 Lazy writer and checkpoint pages will work together to age out pages which are not used recently or have committed records. So if DB2 data pages require memory they would be granted and I dont think memory crunch would be there 2.No, I dont think so like I said a query for particular database(DB2) will have no affect with records present in other database(DB1) IF that database tables of DB1 are not used in this query. Can you define your performance issue is it query slowness, disk slowness memory crunch what?. Please use this link for analysing slow running queries 3.Yes there are lot of monitoring tools available in market I have Spotlight in my enviroenment you can use SCOM as well. You would liek to refer this whitepaer Troubleshooting performance problems in SQL Server 

You said you have only 4 G for SQL Server have you set max server memory for SQL Server ? Is windows server 2003 patched to latest Service pack. Make sure it is there was bug in windows server which trimmed SQL Server memory excessively. Edit: 

No its not necessary that LSN of differential backup would match the LSN of full backup. LSN is basically a stamp of last records that accessed the log. You should have also included DatabaseBackupLSN/checkpointLSN and then you will note that DatabasebackupLSN of differential backup will match the checkpointLSN of full backup and this will show to which full backup this differential backup relates to. Differential backup is also kind of full backup just taking backup of extents which has changed so LSN number might not be same as that of Full backup. Neither Transaction Log backup nor LSN chain would is affected by Differential backup or Full database backup. Only log backup effects the chain.Proof you can see last LSN of 12 AM trn backup is same as first LSN of 1 AM transaction log backup. 

You can fix max server memory using the query but remember, SQL Server can still use memory outside buffer pool for third party DLL's and extended stored procs. This can lead SQL Server to use memory more that what is restricted in buffer pool Please read this article. It would help you in setting starting value for max server memory. You should use below counters to set correct value for max server memory and monitor SQL Server memory usage. SQLServer:Buffer Manager--Page Life Expectancy(PLE): PLE shows for how long page remain in buffer pool. The longer it stays the better it is. Its common misconception to take 300 as a baseline for PLE. But it is not,I read it from Jonathan Kehayias book( troubleshooting SQL Server) that this value was baseline when SQL Server was of 2000 version and max RAM one could see was from 4-6 G. Now with 200G or RAM coming into picture this value is not correct. He also gave the formula( tentative) how to calculate it. Take the base counter value of 300 presented by most resources, and then determine a multiple of this value based on the configured buffer cache size, which is the 'max server memory' sp_ configure option in SQL Server, divided by 4 GB. So, for a server with 32 GB allocated to the buffer pool, the PLE value should be at least (32/4)*300 = 2400. So far this has done good to me so I would recommend you to use it. SQLServer:Buffer Manager--CheckpointPages/sec: Checkpoint pages /sec counter is important to know about memory pressure because if buffer cache is low then lots of new pages needs to be brought into and flushed out from buffer pool, due to load checkpoint's work will increase and will start flushing out dirty pages very frequently. If this counter is high then your SQL Server buffer pool is not able to cope up with requests coming and we need to increase it by increasing buffer pool memory or by increasing physical RAM and then making adequate changes in Buffer pool size. Technically this value should be low if you are looking at line graph in perfmon this value should always touch base for stable system. SQLServer:Buffer Manager--Freepages: This value should not be less you always want to see high value for it. SQLServer:Memory Manager--Memory Grants Pending: If you see memory grants pending in buffer pool your server is facing SQL Server memory crunch and increasing memory would be a good idea. For memory grants please read this article SQLServer:memory Manager--Target Server Memory: This is amount of memory SQL Server is trying to acquire. SQLServer:memory Manager--Total Server memory This is current memory SQL Server has acquired. 

You can perform an in place upgrade,you don't need to uninstall the express edition. Please see Supported Version and Editon Upgrade. You need license key mostly its embedded in new installation setup you get. 

To allow the backup file to grow only as needed to reach its final size, use trace flag 3042. Trace flag 3042 causes the backup operation to bypass the default backup compression pre-allocation algorithm. This trace flag is useful if you need to save on space by allocating only the actual size required for the compressed backup. However, using this trace flag might cause a slight performance penalty (a possible increase in the duration of the backup operation). When you start compressed backup you would see some size of backup file created on the drive but this would not be the correct size during backup operation it can grow and final size would increase. You can initiate a backup with compression and see what is the size then tentatively multiply it by number of days you want to keep it plus few more space. This would again give you tentative size of backup drive. I would also say it would be safer to more space to backup drive