I'm using scikit learn to try to find patterns within thousands of biological sequence records as a way of finding clues pertaining to protein mechanisms of action. To explain the problem briefly; I work on a protein that transports a substrate across a membrane, but we don't know how this transportation mechanism/event works. This protein has somewhere between 2 and 5 'sub-types'. Each sub-type transports a specific substrate and CANNOT transport on of the other types. I took 2000 primary sequence records of this protein that could be any mixed distribution of types 1-5. From the primary sequence, I extracted values that numerically represent biochemical features of each protein. Using machine learning, I want to find clusters, or structure in the data, that may aid in determining the mechanism, or give clues as to how these different protein subtypes function/differentiate between substrates. From each sequence, I extract ~30 different features and plot them against each other to look for clues. I've run the K-means clustering algorithm on this dataset to see what kinds of clusters it comes up with and then colored the scatter matrix to visualize it. As you can see below, there is a very obvious clustering in the top right box (this is just an excerpt from my scatter matrix). However, when this is put into K-means prediction, the clusters it comes up with are very much not what the human eye would pick out. Apologies if my questions are vague but I have two: 1) Is this a viable approach for what I'm trying to do? I realize the K-means might be finding nice clusters in the left hand side data, and then just applying those 'prediction values' to the rest of the plots, even if they don't really cluster. Ideally, I want to provide the computer with as many features as I can possible extract, and have the computer show/tell me which features are correlating with each other in some informative way, and the features that don't correlate with each other, disregard. 2) I'm trying to imagine multi-dimensional clustering and it's difficult (mentally). I'm pretty sure this is possible though, right? Is my major problem just in the limitation of 2D-representations of multi-dimensional data? How should I go about best representing/visualizing my clusters to get the most information out of them? 

Clearly now, our vector of diagonal averages is messed up, and we cannot exploit it anymore... My assumption is that this could be solved by a customized distance function (instead of Levenshtein), where the insertion of a whole block may not be so much penalized. That is what I am not sure of. Conclusion None of the explored convolution-based solutions seem to fit our problem. The levenshtein-distance-based solution seems promising, especially because it is compatible with probability-based-type tokens. But I am not sure yet about how to exploit the results of it. I would be very grateful if you have experience in a related field, and a couple of good hints to give us, or other techniques to explore. Thank you very much in advance. 

The goal is to figure out that syntax without prior knowledge of it. From now, new line is considered as a token as well. A document can then be represented as a 1-dimension sequence of tokens: 

Here the repeated sequence would be because it is the token that creates the least conflicts. Let's complexify it a bit. From now each token has no determined type. In the real world, we are not always 100% sure of some token's type. Instead, we give it a probability of having a certain type. 

Now, its obvious that any suffix compared to itself will have a null distance. But we are not interested by suffix (exactly or partially) matching itself, so we crop that part. 

because it is the one that matches the best the sequence. The syntax (token types and orders) can vary a lot from one document to another. e.g. another document may have that list 

PajekXXL is designed to handle enormous networks. But Pajek is also kind of a bizarre program with an unintuitive interface. 

Shiny is a framework for generating HTML-based apps that execute R code dynamically. Shiny apps can stand alone or be built into Markdown documents with , and Shiny development is fully integrated into RStudio. There's even a free service called shinyapps.io for hosting Shiny apps, the package has functions for deploying Shiny apps directly from R, and RStudio has a GUI interface for calling those functions. There's plenty more info in the Tutorial section of the site. Since it essentially "compiles" the whole thing to JavaScript and HTML, you can use CSS to freely change the formatting and layout, although Shiny has decent wrapper functionality for this. But it just so happens that their default color scheme is similar to the one in the screenshot you posted. edit: I just realized you don't need them to be dynamic. Shiny still makes very nice-looking webpages out of the box, with lots of options for rearranging elements. There's also functionality for downloading plots, so you can generate your dashboard every month by just updating your data files in the app, and then saving the resulting image to PDF. 

Since it sounds like you're fitting a model, there's the additional consideration that certain important combinations of features could be relatively rare in the population. This is not an issue for generalizability, but it bears heavily on your considerations for sample size. For instance, I'm working on a project now with (non-big) data that was originally collected to understand the experiences of minorities in college. As such, it was critically important to ensure that statistical power was high specifically in the minority subpopulation. For this reason, blacks and Latinos were deliberately oversampled. However, the proportion by which they were oversampled was also recorded. These are used to compute survey weights. These can be used to re-weight the sample so as to reflect the estimated population proportions, in the event that a representative sample is required. An additional consideration arises if your model is hierarchical. A canonical use for a hierarchical model is one of children's behavior in schools. Children are "grouped" by school and share school-level traits. Therefore a representative sample of schools is required, and within each school a representative sample of children is required. This leads to stratified sampling. This and some other sampling designs are reviewed in surprising depth on Wikipedia. 

Try every combinaison of every size. Could actually work, but would take some (long (long)) time. Solution considered B: Build a table of levenshtein distances of suffixes The intuition is that there may exist some local minima of distance when computing the distance from every suffix to every suffix. The distance function is the Levenshtein distance, but we will be able to customize it in the future in order to take in account the probability of being of a certain types, instead of having a fixed type for each token. In order to stay simple in that demonstration, we will use fixed-type tokens, and use the classic levenshtein to compute the distance between tokens. e.g. Lets have the input sequence . We compute the distance of every suffix with every suffix (cropped to be of equal size): 

Solution considered A: Convolution of a patch of tokens This solution consists in applying a convolution with several patches of tokens, and take the one that creates the least conflicts. The hard part here is to find potential patches to roll along the observe sequence. Few ideas for this one, but nothing very satisfying: 

There are clearely 3 local mimima, that match the beginning of each Item. Looks fantastic! Now let's add some more imperfections in the sequence: 

This seems to be used extensively in biology in order to analyse nucleobases (GTAC) in DNA/RNA. Drawback: Suffix trees are good for exact matching of exact tokens (e.g. characters). We have neither exact sequences, nor exact tokens. 

Now we can clearly see 2 clear diagonal lines emerge. There are 3 Items (Item1, Item2, Item3) in that sequence. The longest line represents the matching between Item1 vs Item2 and Item2 vs Item3. The second longest represents the matching between Item1 vs Item3. Now I am not sure on the best way to exploit that data. Is it as simple as taking the highest diagonal lines? Let's assume it is. Lets compute the average value of the diagonal line that start from each token. We can see the result on the following picture (the vector below the matrix) : 

The sample must be representative. In expectation, at least, the distribution of features in your sample must match the distribution of features in the population. When you are fitting a model with a response variable, this includes features that you do not observe, but that affect any response variables in your model. Since it is, in many cases, impossible to know what you do not observe, random sampling is used. The idea with randomization is that a random sample, up to sampling error, must accurately reflect the distribution of all features in the population, observed and otherwise. This is why randomization is the "gold standard," but if sample control is available by some other technique, or it is defensible to argue that there are no omitted features, then it isn't always necessary. Your sample must be large enough that the effect of sampling error on the feature distribution is relatively small. This is, again, to ensure representativeness. But deciding who to sample is different from deciding how many people to sample. 

There is an excellent comparison of the common inner-product-based similarity metrics here. In particular, Cosine Similarity is normalized to lie within [0,1], unlike the dot product which can be any real number, but, as everyone else is saying, that will require ignoring the magnitude of the vectors. Personally, I think that's a good thing. I think of magnitude as an internal (within-vector) structure, and angle between vectors as external (between vector) structure. They are different things and (in my opinion) are often best analyzed separately. I can't imagine a situation where I would rather compute inner products than compute cosine similarities and just compare the magnitudes afterward.