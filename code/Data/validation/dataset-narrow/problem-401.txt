and all comments meeting you condition will be returned. That is, every comment from the table (if there are any entries, since this might just be an empty parent table), plus all comments from the child tables , , etc. will be returned by the . You can also have propagate changes to all child tables through this structure. Does this meet your needs? 

Uh oh! Well, that's OK that it won't let me move it. I can just get it back into the schema by dropping it and re-creating, right?... 

Your issue appears to be that you are applying the same (named ) for both your and your . When you use a which contains an clause, and you then apply certain aggregations such as or , it applies the aggregation continuously across the ordering, which is why your and are identical. If you modify your query have multiple windows as 

I know that I should test my SQL script as well, but I don't have time to make a test before leaving for work! I can make a SQL Fiddle later, but for now it seems that no one has quite answered the question correctly. 

Your server authentication mode may be set to Windows only instead of Mixed Mode, which will allow Windows logins and SQL logins. If true then the SQL login will not work. In SQL Server Management Studio, you can right-click on the server and then go to the properties. Then go to security and see how the server security is configured. You can change it there also. If it is set to Windows only then you will have to use a Windows login. If you want to change to mixed mode to use SQL login, you can make the change but that may require the instance to be reset to take affect. 

Having said all that, I still use it and recommend it. I generally try to let developers use just that tool instead of SQL Server Management Studio (SSMS) because I look at SSMS as a DBA tool and SSDT as a developer tool. 

LinkedServerName would be the real linked server name and SourceTableName would be the name of the table in the linked server. 

Back in March 2016, Microsoft had a blog announcing updates to the SQL Server Incremental Servicing Model (ISM) and one of the things they have there says: 

We were unable to start Service Broker on as SQL Server said it was already running. The databases were pointing to different & files. Why does the Service Broker use the logical names and not the database names? I always thought that the scope of logical name was restricted to a single database? EDIT This is the SQL Statement that was used to enable Service Broker 

I am trying to use Powershell within a SQL Server Agent job to download a zip file. The script uses PuTTY (PSCP.exe) to download a zip file from a SFTP site. The issue I am having is that when the job runs it connects to the SFTP site and PuTTY sends a prompt back about storing the server's host key in the registry. I don't want to do this so I am trying to pipe the command to PuTTY. This doesn't seem to be working though. 

I have multiple databases that I want to store in one data warehouse database. I am wondering how I design the import process to handle multiple lookup tables. For example, say I have 5 databases all with the lookup table CustomerState. In one datatabse it could look like this: 

The RSAT tool was updated with the general availability of Windows Server 2016. By going to the same link as the preview RSAT, $URL$ you can update your workstation with the new version. 

I would suggest using SQL Server Data Tools (SSDT), which is essentially the latest iteration of the Visual Studio Database Projects. I use this at places that do not already have a way to source control their databases since it easily integrates with TFS, which most Visual Studio shops use for source control. A couple pros I have for using it are: 

I would say the stored procedure itself probably does not include the comment with the current script date. SQL Server Management Studio adds that when you generate the view of the procedure. In my experience, the stored procedure itself starts with CREATE or ALTER and anything above that is added by SSMS when you look at the code. 

SSIS can connect to many different servers based on the connection type used in the SSIS package. Since even SQL Server 2016 SSIS still uses Native Client 11 (and has since 2012), you should not have any issues connecting from SSIS 2008 R2 to SQL Server 2016. SQL Server 2008 R2 uses Native Client 10 or 10.1 but has no issues connecting to SQL Server 2016 either. In the connection manager in SSIS, you choose the way to connect to SQL Server. It could be ODBC, OLE DB, or any number of connections types. I say this to say SSIS itself does not care what version of SQL Server you are connecting to but the task you use will determine the connection type. I would assume you are using OLE DB and as such you may choose the provider in the connection string. 

I can't build a full fledged test just right now, because I've gotta hit the road. Just note that this code is as of right now untested, but you can easily give it a shot since you already have the data tables. ;) Using JSON + + CTEs It looks like to me, from your description, that a entry is an array of JSON. I'm just making this clear because you earlier stated that is a JSON, but your data sample seems to conflict. In this case, why not use a simple Common Table Expression (the keyword) along with to extract the relevant JSONs as rows, and then perform your query? 

If you want to test the performance, use the link to the SQL Fiddle below! SQL Fiddle EDIT: So, a commentor below asked why I used . It's because of the . In this case, you are asking the query to return rows , but the ensures that entries are returned whenever there is no match. In these cases, you can't know if a particular will be in table , table , or in both, so by using you ensure that so long as a one of the tables has the field (which is guaranteed, since you did a ), this will be filled in. WARNING: One of the alternative posted responses is an incorrect solution. Applying that erroneous query 

Per Books Online, compatibility level sets certain database behaviors to be compatible with the specified version of SQL Server. As such, some queries may no longer work or may need to be slightly changed. These may be performance related especially when going to level 120 or higher as those change how the query optimizer works. If you do not change compatibility level before upgrade, the upgrade process will upgrade to lowest level compatibility on the new install, in this case to 100. The benefit of changing compatibility first before migration is to see if any queries need modification (assuming an application is attached to this database). Since this is a migration, one way to test performance is to migrate the databases to the 2016 server and then run same queries on both and gather metrics from them to compare. However, on same server performance should be similar, assuming the query is still able to run. An example of a query that will no longer run after changing compatibility mode would be as follows: 

One way of finding obsolete columns is to use SQL Server Data Tools and create a database project of the database. Import all objects from the database into this project. Once everything is imported, do a build on the database project and then it will find all the unresolved references in views, stored procedures, and functions, among other things. 

Kyle, I'm afraid that, due to the storage structure of PostgreSQL, a simple copying of indexes is not possible. PostgreSQL uses a storage method called heap-structured tables, where the insertion of data into the table has no guaranteed ordering or position. If PostgreSQL were to copy a complete table from one table record to another, including all versions of every row, information about deleted rows, etc., then I imagine that one would be able to copy indexes as well. In the case of how PostgreSQL currently operates, copying over data from one table to another replica table only guarantees logical consistency: that is, every row present in the original table will be present in the new table, and the data will be a row by match at the row level only. In terms of physical consistency, there is no guarantee that the way in which the data is physical stored on disk in the original table will match the physical storage of the data in the replica table. The reason this is important is because an index is essentially a method for mapping some information about the data in the table to the CTID of the row which contains the data. Long story short: other than some creative hacking, I believe the conventional wisdom is that you must rebuild these indexes on the new table. Sorry I don't have better news. 

I have 5 tables that contain data split by quarter each table has a similar structure to the following: 

If I open up a command window and do this manually it works fine. e.g. Typing this in the command window 

I have a database that is set to Simple recovery mode. There is 99% free space in the log file and I can manually truncate the log file right down to 500 MB. The next day, after a daily import process runs, the log file will be back up to ~70GB. If I check in it returns 0 which would indicate nothing is blocking a shrink operation. Why isn't the log file naturally shrinking? Also, how can I find what SQL queries are causing the log file to grow to such a size? 

I have a single table that contains 4 billions rows that I plan on splitting up in to separate tables for each Quarter of the year. Then I want to create a partitioned view across all tables. At the moment I have one filegroup for data (where most of the data resides) and one filegroup for non-clustered Indexes (I have been slowly migrating indexes across to this filegroup) The data filegroup and non-clustered indexes filegroup are located on different disks. The individual tables will have primary keys and non-clustered Indexes added to them. Where should I put these? (In terms of filegroups) Should I create filegroups for each table, for each type of index etc? Note: I can't use Data Partitioning as we don't have Enterprise edition. 

SQL Server Reporting Services (SSRS) is a server-based platform for hosting reports. Report builder is client tool that may be used to create reports to deploy to SSRS. As such, you would use SSRS to manage security on for the reports that you may have deployed with Report Builder. 

I agree with some of the other comments though that perhaps your servers are set to auto update and this is how it could have occurred. Based on how your server is configured and combined with the changes to how CUs are installed, it could have auto updated. 

There is a lot of info out there about this behavior going back years. Here is a thread: SQL Server Management Studio slow opening new windows Common thread there seems to be SSMS is trying to reach a location in the internet. Another thing there states to change the user feedback Opt In setting. Maybe one of these options will speed it up 

You may have to shrink the data file to reclaim the space. Deleting data will not free up the space to the operating system, it will just make the currently used space less full. Truncate is similar to delete but doesn't log everything like a normal delete does. Here is a dumb analogy I use sometimes: putting a cup on a napkin can take up most of the surface space on the napkin but filling and emptying the cup has no affect on the napkin. In this analogy, the napkin is available space and the cup is your table. In your case you just emptied the cup. Now you have to remove it from the napkin, and shrink will do that for you. But what you should look into is why this table got so big in the first place. You may very well run into the same problem soon so you may have to revisit the space used and add more space to the database. 

Is SSIS subject to the maximum memory limitations for SQL Server 2012 Standard edition? If I have a server with 128GB RAM and I allocate 64GB of RAM to SQL Server does SSIS utilise this memory or does it utilise the memory that is available to the OS? I am assuming it utilises the remaining memory left to the OS since there is no settings for memory consumption and it runs as a separate service. If anyone has any information on the available memory that should be left for SSIS to consume I would appreciate the help. 

There is also a CHECK constraint on each table to ensure that the is within the specified quarter. I have then created a View across these tables which I am querying against. I need to execute several queries which aggregate the date by week, month, 3 months and 12 months. When I look at the Query Plan for my query it is querying all tables regardless of the filter on the DateStamp column. I have a controller Stored Proc which in turns calls a Processing stored procedure. The processing stored procedure accepts the start and date as parameters, assigns them to local variables within the procedure and runs the query. The query is similar to the following: 

Based on the comments, it seems the answer is to create a clustered index on the column you want to use to partition the data and then make sure your non-clustered Primary Key index is not partition aligned so you will not need to add the partitioned column to the primary key. By doing this, you can keep the primary key "simple" and use it like you normally would in a non-partitioned table. 

One reason for separate drives even with SAN is that a given disk or set of disks can only do so much I/O at once. Any of the things on the drive could saturate the I/O on the SAN by keeping OS, TempDB, logs, page file, and the user database on the same drive. Depending on the query and server configuration, TempDB, the page file, and the LDF could all be in contention for the I/O. By separating those, the I/O for those set of disks is exclusive to where they are allocated. Another thing to consider is any one of those things could also use all the available space if sharing disk space and instead of one component shutting down, the whole server could be shut down.