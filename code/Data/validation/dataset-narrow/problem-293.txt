If you only need to create the structure of the database the easiest method is to script out the current database. This will allow you to create a new one, providing a new database name and MDF/LDF file names. If you need particular static data to deploy as well, depending on how much, you could script the statements or restore from backup. In order to restore a copy you will need to restore the database with a new database name and physical filenames (MDF/LDF). You do this using the clause within the restore command. You can find that reference here on TechNet. 

Build T-SQL query with desired dataset I wanted for a single server. Create PowerShell script that has function to execute that query (use or ) Export that data to CSV (e.g. ) Import that CSV file into Excel 

I would start with this list of errors, most of the login errors I believe are around 18400+. The only other two I think I would add would be: 

If you want to store the configuration file and/or the file you need to "pick up" via the SSIS package on a UNC path, SQL Server is going to need permissions to that direct UNC path. This permission is based on your service account being used for SQL Server Agent. If you are in a domain environment, then it is best to have your service run as a domain account and it will allow you to easily provide access to network resources. If this is a configuration that is meant to change often and I would not be the one to do it necessarily every time, then I might chose to store the configuration in SQL Server. There are a few examples out there online that show how to do this, just one example here. 

I use SSMS more than I do VS2010 because it is there when I install SQL Server. That is probably the most important thing why SSMS is used more than VS2010, IMHO. I did also find that vendors like Red-Gate are putting out tools that integrate with SSMS and not necessarily VS2010. This can be positive in that it allows you to improve SSMS where Microsoft has not. One example of this is Red-Gate's SQL Source Control, which is an add-on to SSMS that allows you to connect SSMS up to your company's Source Control system whether it be Visual Team Foundation or what have you. VS2010 has this built-in but compared to the price of Reg-Gate's tool I just saved a bunch of money without having to buy VS2010. I think overall it comes down to preference, you work with what you are comfortable in. If you are training a new person and you train them on VS2010 then that is going to become their preference because they know how to get around with it. If you started playing with SQL Server 2012 you may have noticed that SSMS is getting the makeup of VS2010 slowly but surely. So eventually you may not be able to tell the difference between them. 

As labeled the SMO object outputs in "KB", you can modify this to be the measurement of your choice by just putting the abbreviated reference for it. So if I wanted "MB": . In the function , the brackets "[]" mean the parameter accepts an array of objects. So if you have your servers listed in a file you can call the function like so: 

Now why this has changed I have not found yet. If I ever do I will try to remember to come update this post. 

During the install process for SQL 2008 R2 it gives you the option to change the start-up type. I can see a slight desire to not have it start automatically depending on what you might be using it for. I have some setups that the user has some Agent jobs that clean up data or immediately start processing data. If I am troubleshooting, or in general the server goes down, I may not want all that processing to start back up immediately after bring the server back online. In most instances though you will see the Agent set to automatic start up. 

ApexSQL Restore will mount the backup as virtual Database, and you can query it using SSMS. I also published a PowerShell script that will let you analyze your backups in a given folder. I wrote it for analyzing size but you could easily adjust this to include properties like the date taken and such to find which backup you need to mount. You can find the script and article around it here. I have not played with the Apex tool extensively but do not believe it will search all backups in a folder for you to find the one with the specific data you need. You would have to mount at most the full and then all the logs to get it to the date you need. I have never seen one that does exactly what you are looking for, but the above should help I think. 

The components for SQL Server 2012 are divided by server and then management tools. You can get the server level components by using a PowerShell command like: 

I would look at using extended events. Truthfully I have not worekd with these and it's one of those things I have not sat down and tried to learn...yet. So I cannot give you an exact example of how you would impliment with your example. The advantage to these over trace files is no performance hit. They were specifically designed with performance in mind. So letting it collect information over a long period should not be a problem. You can check out using it for monitoring system activity here. Jonathan Keyhayias did a good month-long series on using extended events. The first day gives a good overview of them here. He also created a SSMS add-in that makes it a little easier to work with the Extended Events sessions. 

Permissions to assign her are going to be based on what deployment model she is going to use within your environment. SSIS 2012/2014 added an additional model: Project and Package For Package Deployment model she has the option of deploying the packages to . Granting her permissions for this type of deployment you will grant her permissions to an SSIS role. If she is the only person that is going to be working with the packages you can simply grant her . This will provide her permissions to do anything she needs with the packages she creates and deploys (making her the owner of those packages). You can read through the article linked to get an idea of more. For Project Deployment model you will need to work with the catalog. With this it is just granting permissions at a database-level role. If this is not already created on the instance you will need to do this for her. Once it is created you should see a database that will have a specific role called , this role is special to the catalog database and is not a normal role found in every database. The database will show up under the Databases node in Object Explorer, just go to and find the role. Within the properties of that role you can add her login. 

Have you considered looking at the minimal logging features available in SQL Server 2008? New update on minimal logging for SQL Server 2008. The author also has a few blog post on examples using this feature here, here, and here. it mentions use of a trace flag that is available to help limit the log use. 

Each option has their own level of complexity and knowledge to setup and run. If this is something you have to do frequently for moving code through environments you might find the time to learn and set one of the above options up worth your time. 

Apparentely, after some testing...there is a way to speed up SQLPS load time (tremendously) and prevent the WMI warnings (if you don't need the module loaded. How I tested this on my laptop, was simply by shutting down the WMI service: . I then proceeded to edit the module files for for my SQL Server 2012 local instance. As soon as I load the module initially, I get the same warning messages you show in your question. The one that causes the specific problem is the file. This file is a nested module based on the module manifest, so everytime you load this script is going to be executed. This file is also why when the module is loaded it changes your location in your prompt. The first line of the file is . [If you comment out that line it can also speed up the load time for the module a good bit.] The lines that are causing the WMI warnings are from the two lines of code that will load the module. Which if you look at the documentation for PowerShell in BOL it states it is loaded with here. It also goes to mention when configuring remote administration for that in step 3 WMI Service has to be running. Which if you don't have access to WMI equates to the service not appear as running. So in the end, as long as you don't need Analysis Services PowerShell module you can edit the file, found under: 

If you are using a named instance change "DEFAULT" to the name of the instance. So after some testing I found out the replica name is a special case if you are dealing with named instances. If you are dealing with a named instance you cannot just use: "SQLSERVER:\Sql\MyServer\MyInstance\AvailabilityGroups\MyAG\AvailabilityReplicas\MyServer\MyInstance" You will get path not found errors...here is a kick on how to fix it. I happened to issue this command against a named instance with an AG: 

Your syntax is wrong for adding a backup device. When you create a backup device in SQL Server you are actually creating the "bak" file as a media set. Along with the suggestion from Bob that you should be using the full UNC path, your syntax should be changed to something like this: 

If you want a method to run 200 statements, along with the checks to see if they already exist your options (which some have already been mentioned): 

Large deletions are fine but understand that the space that data used was not released back to the operating system, unless you did a shrink of the data file. 

Unless you can point to the direct requirement you are referring to, I do not see any requirement in Skype for Business (SFB?) that states it has to be on a dedicated instance. I think what you may be referring to is the database needs to be in a dedicated Availability Group. If that is the case you can create another AG on the same instance and cluster to support your database for SFB. A given SQL Server instance can support multiple AGs and each one can have their own listener. 

What do I have wrong in the expression for the ? I can obviously evaluate the expression successfully in the properties window, and understand that is because the variables are not populated. So how do you configure that were it will not truncate no matter what error is passed in, just set it to some large number length? I am not looking for some custom method or code to perform this as the package is fairly simple and is only run every blue moon. However, would it be best to simply create an event for each task? As I understand it any child task that fails has the error information passed up to the parent package, so it should work right? 

Config: Window Server 2008 R2 Enterprise SP1 / SQL Server 2008 R2 Enterprise (RTM) I have searched through Google using many different words and phrases. I know this was a common issue to occur with SQL 2005 and most mention doing a command line to re-register the mof file and a few other things. I'm getting the same issue similar to what everyone else receives: I can only see SSIS service within SSCM. 

The function below is for the purpose of pulling information about one or multiple databases of a SQL Server instance. So far I know this works on SQL 2005 and up. I have used but that ends up cutting off columns. I have used which can work but defaults to a list view which could be a pain to get into Excel. It also cuts off column data that exceeds a certain length. EDIT: Purpose of this function is to run it against 100+ servers for gathering inventory of each instance. PowerShell Function: 

The alternative would be to use WMI, or you can use SQL Server WMI (microsoft.sqlserver.management.smo.wmi namespace). In T-SQL there is no mechanism native that lets you interact with the services. Outside of just using and calling PowerShell or cmd commands. 

Since your thumbprint is being returned as a you actually cannot easily convert it to a . I have not gotten a full example that you are doing to work completely but will show you how I convert the thumbprint, and it seems to work: 

This lock type is commonly seen with deadlock queries that SQL Server has executed as parallel, sometimes referred to as "intra-query parallel deadlocks". I have seen a few statements that this also points out system resources are low, which I guess could be involved to a small degree. A general guideline that I have noticed to determine if it is parallel deadlock is when you pull the XML deadlock graph (which can be done with the system_health session in 2008 and higher) you will notice different process IDs showing the same bit of code within the execution stack. As well, looking at the resource list of the deadlock graph and noting the type of waiter event. They will most commonly show "e_xxxxxx", or something like this maybe: 

Configure SQL Agent through the Advanced properties window for Idle CPU Condition. You would set the "Average CPU usage falls below" value to 20%, and then how long it has to keep at that level before the job will run. After that configure the job step to check the time ensuring it is within your window, if within the window issue the update command. Now the requirement for every 5 minutes may not be meet exactly with this setup. I could not find anything on this schedule type that indicates how often the job would be executed. [e.g. if you set the "and remains below this level" to 30 seconds for Idle CPU condition, does it run the job that frequently?] That would require a bit of testing as I have never used this configuration before. If I get a chance I can try to see what it does and will update this answer. 

Just as caveat it is best not to disturb the physical files involved with FILESTREAM for a database. Below are a few good links on the architecture: Paul Randal White Paper - FILESTREAM Storage Paul Randal Blog Post - FILESTREAM directory structure As quoted from Paul's blog post above: 

Another way would be to copy the execution plan XML out and paste it into Plan Explorer and view the tab: 

To get the management tools would require either registry search for the uninstall list or you can query the WMI class : 

I have not actually tried this (but it peaks my interest to later). You will not be able to use because it only reads native backup formats from SQL Server, not Azure SQL. The file created from your backup of an Azure SQL database is in a data-tier format. Which in this format contains the schema and data for the database. In order to restore this to a local instance in PowerShell you will have to utilize the objects to do the . Which if you are loading the module the assembly is loaded for you. Basic examples that I find online seem to follow this format: