Data Science is the new "sexy" job. I got dozens of colleagues who ask me how to become one. Some, come to me and tell me that Andrew Ng's coursera course is hard, and what can they learn instead so they can put it on their CV. What all this means is that there are a lot of not well prepared people that think they can become a data scientist, and advertise themselves as such, but are really not. Many companies are starting to learn to vet better their candidates because of that. That said, I am going to answer your question: you have some real skills, you actually understand statistics, which most data scientist I work with don't. Many financial institutions will want people like you, they can find plenty of people who can write Java and set up a cluster and set up a machine learning algorithm without understanding what it does. They are technicians, but do not understand what the data implies. If you can show that you actually can "think" and provide insights, you can be appreciated much more. In fact, you can command a higher salary. What you need is to learn most used machine learning algorithms, maybe learn some python (easy if you already know R) and tailor your resume for those jobs who require actual thinking. You have real skills. 

I can easily use a neural net to calculate an average. I can train on a few examples and the neural net will quickly learn to have weights equal to 1/(length of the list). However, can a neural net learn to calculate the median of a series? is there any way to train a neural net to approximate a median? 

Keras has two border_mode for convolution2D, same and valid. Could anyone explain what "same" does or point out some documentation? I could not find any document on the net (except people asking that it be implemented in theano as well). 

It is a well known fact that a 1-layer network cannot predict the xor function, since it is not linearly separable. I attempted to create a 2-layer network, using the logistic sigmoid function and backprop, to predict xor. My network has 2 neurons (and one bias) on the input layer, 2 neurons and 1 bias in the hidden layer, and 1 output neuron. To my surprise, this will not converge. if I add a new layer, so I have a 3-layer network with input (2+1), hidden1 (2+1), hidden2 (2+1), and output, it works. Also, if I keep a 2-layer network, but I increase the hidden layer size to 4 neurons + 1 bias, it also converges. Is there a reason why a 2-layer network with 3 or less hidden neurons won't be able to model the xor function? 

Simply assume that since I used points from the already standardised data that the new synthetic samples will also be standardised? Therefore just add the synthetic samples to the train data and be done with it? (Without touching the test data) Create the synthetic samples from the pre-standardised data, standardise the synthetic samples and add them to the train data? (Without touching the test data) Create the synthetic samples from the pre-standardised data, add them to the train data, and standardise the whole set? (Without touching the test data) Or ... ? 

I read the likelihood is defined in logistic regression as the probability $$ L(w) = P(y|x, w) = \prod P(y^i| x^i,w) = \prod (\sigma(z^i))^{y^i}(1-\sigma(z^i))^{(1-y^i)} $$ and the log of the last equation is: $$ log(L(w)) = \sum y^i log((\sigma(z^i)) + (1-y^i)log(1-\sigma(z^i)) $$ I understand (independent probabilities) $$ L(w) = P(y|x, w) = \prod P(y^i| x^i,w) $$ and I understand that from $$ L(w) = \prod (\sigma(z^i))^{y^i}(1-\sigma(z^i))^{(1-y^i)} $$ the log is (basic log properties) $$ log(L(w)) = \sum y^i log((\sigma(z^i)) + (1-y^i)log(1-\sigma(z^i)) $$ However, how do I get $$ \prod P(y^i| x^i,w) = \prod (\sigma(z^i))^{y^i}(1-\sigma(z^i))^{(1-y^i)} $$ This basically means that $$ P(y| x,w) = (\sigma(z))^{y}(1-\sigma(z))^{(1-y)} $$ and I just don't see that. 

and I want to classify my cars as "Sport", "Luxury", "Family", "Compact" using these indicators. However, say that I realise the manufacturer's name is the strongest indicator, so I want that to weigh more. For example, if I buy a Ferrari, that is a sport car, even though the price tag may make it look like a luxury car. So, while I want to use all features, I want one of them to have a stronger weight (this is just a silly example, my actual problem is different, but this gives an idea). If a manufacturer makes all 4 kinds, speed and price are important, otherwise the name should have a stronger weight than the price and top speed. Basically, my tree should really look at the manufacturer's name and then split depending on the other features. Can I set up sklearn in python to do this? How? 

Now, say I have millions of examples and thousands of classes, my network learns relatively well. However, sometimes a new "phrase" that belongs to a never seen class appears. Of course, the model cannot map it to something it has never seen, however, when I look at the output layer values, I was hoping to see all neurons to have small values. My idea was that, I would use the softmax and then if the max value is less than 0.95 or some high value, I would assume the network is unsure, therefore remove that result for manual observation. However, in many cases, I have seen my network to be almost sure (a neuron with value close to 1) even when it has never seen that class. For example, say I have the two classes What and How, I would expect: 

LSTM can be used to generate text, can they be used to fix corrupted text files? Say that my original was: 

Let's say I am training a neural net (e.g. convolutional network or LSTM). Generally, the longer the training (more epochs) leads to better accuracy, albeit at times at the expense of overfitting. Another approach is to duplicate the data. Again, if I have 1000 examples, I could duplicate them 10 times and randomise them, and the neural net will learn better (but again, it may not be able to generalise well). Is there any difference between duplicating data and adding epochs when training a neural net in terms of its learning? 

Basically, a lot of crap was added (some meaningless words but also meaningful words, and some letters may have been erased). Is there any way LSTM would be able to fix this? (Maybe by cleaning some beforehand and then let LSTM do the rest) 

Since test and train are the same, I was expecting the same predictions, but I have different results. What am I doing wrong? Predictions on test: 

I created a convolutional network to recognise certain substrings. For example, the following phrases would be mapped to the "What" class: 

In the last case, the network is not too sure on either, and I would check the input. Unfortunately, this is not working, and I am getting high values (close to 1) neurons for some new input. How can I fix this? Please, bear in mind, I am happy to sacrifice some accuracy in order to ensure the network never tells me it is sure about new inputs, in other words, I am happy to go and manually check more inputs (even if seen before) rather that let even one unseen input go unnoticed. 

I have the following simple weka code to use a simple decision tree, train it, and then make predictions. 

I am reading the original paper by Chawla and others for SMOTE. I am trying to understand how to generate these synthetic examples for over-sampling the minority class. The paper says: "Synthetic samples are generated in the following way: Take the difference between the feature vector (sample) under consideration and its nearest neighbor. Multiply this difference by a random number between 0 and 1, and add it to the feature vector under consideration. This causes the selection of a random point along the line segment between two specific features". I understand the idea, take your sample, the nearest neighbor, pick a random point in between, what I don't understand is how these nearest neighbors are defined. 

I have a set of 10,000 integers, and another set of 100. The integers in the first set are mapped to integers in the second set according to some rules (not mathematical rules, think of these values as codes for naming certain items, it is some categorical mapping). The mapping is not necessarily 100 to 1, in some case I may have just 30 or so integers from the first set mapped to an integer in the second set, in other cases 300, but on average of course it is 100 to 1. Using sklearn, I created a decision tree that was able to get over 99% accuracy, as I would expect. When I tried logistic regression, though, accuracy was just 45%. The training sample is about 100,000 example, so, it should be enough to learn. What is going on? Is there something inherently different in the logistic regression method that I am missing? 

I am trying to figure out how many weights and biases are needed for CNN. Say I have a (3, 32, 32)-image and want to apply a (32, 5, 5)-filter. For each feature map I have 5x5 weights, so I should have 3 x (5x5) x 32 parameters. Now I need to add the bias. I believe I only have (3 x (5x5) + 1) x 32 parameters, so is the bias the same across all colors (RGB)? Is this correct? Do I keep the same bias for each image across its depth (in this case 3) while I use different weights? Why is that?