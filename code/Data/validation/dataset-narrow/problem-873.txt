I wouldn't be scared of building from source: it's fun and rewarding. The only big problem you'll have is the same problem you're already having: dependencies. To get around the dependency problem, you'll need a package manager. Hmmmm. You can install Yum, which is what'd you'd get on Fedora instead of up2date...It's pretty good at handling dependencies, and googling Yum and RHEL and Repository gives a good number of hits, so there are repositories out there which will have RPMs built for your system. If it were me, I'd probably go ahead and upgrade KDE and Gnome, if it was that important to install the newer version of samba (that's a lie actually. I love the command line, so I'd just go ahead and break kde and gnome and not look back). Resolving this sort of dependency problem is what it's all about. 

I like 10. It's nice and short, and offers a massive amount of room for expansion. After 10 I usually work in /16's, but I plan them by /8's (which are usually a nice size for a business unit). Working in 8's is nice because (unless your company is massive) you can just assign a business unit 10.1.0.0 and you won't have to worry about them running out of space any time soon. Obviously, if you have more than 255 business units, ymmv. I usually use 1 for the gateway, just because it makes it easy to remember. Either way, as long as you use the same number on every subnet, it doesn't matter. Other than the gateway, I don't reserve specific ips for specific types of servers. Usually I dump all the servers on their own ghetto subnets, so I can keep an eye on them, and make sure they don't mix with crappy desktops. If I have to have them mixing, then, yea, I reserve the first 50 or so addresses for servers/anything that needs a static ip. Again it's just a matter of less typing. Desktop users seldom care what their IP is, and you don't often need to type it in. I like DHCP (we have tons of laptops), but you need to couple that with registered MAC addresses, or any shmuck off the street can come in and plug in and that's a no no. MAC's aren't secure, but they're at least as good as statics, as far as security is concerned. I don't use "registered" DHCP; I'm not a windows DHCP person. If I'm going to have statics and dynamics on the same subnet, I just set the DHCP range to be 51-255 or similar, and put the statics in 1-50. 

Here's a couple of links explaining what you'll want to be doing. Take a look through the rest of the documentation on the official site if you have any more issues. 10 minute tutorial. Part 5 shows what you need to be doing. Parsing old logfiles with correct timestamps. Might be useful as you get into the swing of things. 

We've been using an EC2 micro instance for a little remote monitoring for the past few weeks, and it's bothered me badly how the system time is never accurate; no matter how much fighting I did with Windows time syncing, it was never working properly. Didn't find an answer on serverfault that explained how to fix it, but found a good one finally on Amazon's forums, and wanted to share it here for anyone else looking. Scenario: Windows 2003 Server on EC2 Micro Situation: Time always wants to sync to UTC, no matter what is configured in Windows. 

You will need to recompile screen to be able to edit this option. You need to manipulate the "MAXWIN" flag. 

All the 16 drives work together as a RAID10 array with one big volume. I'm leaning towards leaving the 3Gbps in and taking the hit, but I was wondering what other people think would be best here. 

Time+ represents CPU time, or more specifically, "Cumulative CPU time which the process and children of the process have used". 

Go into Task Scheduler (taskschd.msc). While there, there's a button on the right menu for Display All Running Tasks. I think this is what you want. While you're there, you should turn on Enable All Tasks History, so you could track something like this better in the future. 

You're doing two different methods of resizing the drive. If your drive was just a filesystem sitting on a partition, you could extend the drive, delete the partition, recreate the larger partition, and then just extend the filesystem with xfs_growfs. That's how it was done in the old days, before LVM. You're using logical volumes though, which means you have defined physical devices, volume groups, logical volumes, etc. The easiest way to extend that would have been to create a new partition, create a new physical device from that, add it to the volume group, then do the lvextend/xfs_growfs. Here is a more thorough how-to for that. You're kind of split down the middle here, but it's fixable. You need to pvresize to fix the size of the physical volume: it's not showing the full size of the 750GB you've allocated to it. That should increase the free space in your volume group, and then you can extend the volume. 

Well, Powershell has $?, so I'm assuming the problem is that it only populates on exit? When you're dealing with errors in running code, the best practice is to use a Try/Catch block. Code inside a "Try" block will automatically fail over to "Catch" in the event of problems, and, even better, you can have different catch blocks for different exceptions, so you can handle different problems on the fly. Very cool. 

Have you tried topas? It's pretty good for that sort of thing. You can also try nmon but it's third party, so you'll have to download and compile it. 

I'm trying to find out how to measure the total bytes written (or a percentage of maximum expected, either is fine) for a few RAID arrays behind LSI controllers. The controllers are all LSI MegaRAID SAS 9271-8i controllers. I've tried using MegaRAID Storage Manager and MegaCLI, but neither seems to show the information that I need. I've found a couple solutions online, but they only seem to be for Linux, where you can patch the kernel or use smartctl in unconventional ways. That won't work for me on Windows. I'd really like to avoid pulling the drives out, putting them in another machine, testing with SMART, and then putting them back. Would be a real pain in the neck. If it's important, each controller has two virtual drive groups of 4 disks each, in RAID10, with SAS SSDs forming the groups. 

Leave the drive in, ignore the media errors, and just swap it out in a week when the new drive arrives. This isn't really an option, but it's worth listing to cover all possibilities. Boss and I agree we're not going to do this. Take drive offline in management software, and let array function as a degraded set. I've never actually done this, so I'm not sure of the performance hit we might take. Understandably, this way is sort of risky. We have a very similar drive from another (unused) server, that is similar to the other 15 drives, but is 3Gbps instead of 6Gbps. This is where my question comes in; will this performance hit be MORE or LESS than taking a drive offline? 

The answer is yes, but since we're talking about IIS, the answer isn't that easy. Depending on your application, the limit can be overridden in the code (goooo ASP.NET with maxConcurrentRequestsPerCPU/maxConcurrentThreadsPerCPU) or otherwise, you'll need to change the appConcurrentRequestLimit property in the IIS Settings Schema. The default is 5000, but some people recommend bumping that up substantially (like by a factor of 20. I only use IIS when I have to, so YMMV). 

The easiest way is to put in an .htaccess file in that directory. The "engine" directive is only supported in php 4.0.5 or later. You can also try and remove the handlers in apache using .htaccess (e.g ), but I'd try the other one first. Removing the handler is one of those "force it to break" methods that sometimes backfire. 

Well, I'm lucky in that I'm in house, but also corporate. So I have my site, and my network, but I'm also a corporate "expert" who gets called by everyone and their mother to remote in and fix this or that. I even get to travel a bit. So it's nice. On the other hand, I've worked in house in places where there was no money, and no good toys, and you had to put everything together as best you could, and weather the inevitable storms when some piece of substandard gear kicked the bucket. So I'd say, on balance, it all depends on the location. Being a contractor can be fun because you're seeing new things, and you get to do huge satisfying jobs, but you've also got to deal with angry local employees, and you don't ever get to rest, or deal with the big system (aside from epic contracts). And being an in-house guy can be fun...in the right house...but in the wrong house it can be ugly. 

Depends on what you're comfortable doing; when starting to "professionalize" our office network for a similarly sized business, I set up a pfSense Firewall behind our modem, and assigned it routing tasks for the office. All you need is a machine to dedicate with a few NICs and you're on your way. It's pretty well documented online, and I haven't had an issue finding support when I've needed it. Once you have that installed, you can use the OpenVPN package to easily route the traffic you want to your network from anywhere in the world. The only steps besides setting this up on pfSense would be allowing traffic through your ISP's modem, which shouldn't be too difficult either. I'd say give it a shot! It's been rock-solid for us, and we run it in production now for our datacenter rack as well. It also never hurts to have a little more experience under your belt, and you could look to it as a cheap learning experience. The added benefit is when you want to do this, you can set up a VPN tunnel to your hosting provider, and then you'll have full access (on the terms you decide through firewall rules, etc) to your infrastructure more easily in the office. 

The flag will overwrite files. It is 'include same files', and should accomplish your task. I would also use to set the retry wait time to 1 second, rather than the default 30. Your delay might be from locking on your target side; have you checked that out? I don't know of a way to copy without checking, but that should get you where you need to be. Of course, you could also use another line of robocopy to just delete all the files from the target directory BEFORE running the mirror. That would certainly work as well. 

Public/Private Key Cryptography is based on a very simple system: You have a public key that is capable of doing one-way encryption, and a private key that is capable of decryption. The public key can then be given to everyone in the world, and no one will be able to decrypt your encrypted data, though they WILL be able to encrypt data that you can decrypt with your private key. So the answer to your question is, "Your public key." 

Try tracert. Or "traceroute" for non-windows users. It will tell you every server the ping has to pass through to get to your destination. A count of the number of hops is a good metric for how much latency you're going to have, and it will also tell you how much latency is being added by specific hops. 

I've done it on "zero-downtime" systems. Really though, you're just as likely to lose a different drive when the RAID rebuilds...I swapped one out once, then ended up swapping it back in when another drive started throwing errors during the rebuild. It's a philosophy question really: if you believe in pro-active stress testing (both of the array and of your cardiovascular system) then you should swap your drives. But really, you're never going to know which drive is going to go bad next. It's not at all unlikely that you could lose the newly replaced drive before you lose any of the older, proven drives. That being said, I'd waste my time on stress-testing my backup solution, and leave the drives in peace until they start actually throwing errors. 

And so on. This is annoying, as our code needs to be maintained across multiple folders, and people often forget to do so; that's an issue that can be fixed, but I'd rather solve it in the solution rather than the process. The code in all the folders is identical, only the web.config files are different. The reason we have our sites split this way despite the identical .NET code is that we need different connection strings to different servers. This way, whichever site hosts the binding gets the traffic. This is how we handle load balancing, which is not the prettiest way, but it gets the job done. I'd rather MULTIPLE sites (with one or multiple app pools, doesn't matter) which work the same way, but function with ONE shared code folder. What I need is a solution to handle the connection strings based on the site in IIS. I didn't have much luck looking into virtual directories, which seemed straightforward but I don't think is possible, as it needs a web.config to even know WHERE to look for virtual directories. I'm sure I'm missing something simple here, but I need a little help to get to this: 

Rather than beat yourself up trying to troubleshoot the sometimes tempermental web platform installer, you can always just install directly from the Microsoft provided ISO instead: here. Also, you might check that you're on Windows 7 SP1, as that's listed as the minimum requirement for this version, NOT standard Windows 7. 

I don't know of a way to restart individual processes without having multiple different apache installs (which is a perfectly sensible way to do it, especially if people are breaking things). However, you might want to look at the graceful restart directive...Basically, it's a restart that waits for running processes to complete, before it cycles the server. I work on a lot of really large websites, and this is commonly used in production with very little user impact. 

One thing I find useful is dumping the entire database to a text file, and running keyword searches on it to look for shenanigans. If you use a decent text editor (e.g. Notepad++) you can get a lot done that way. In the end though, it's almost always a better idea to purge and start over. If you're not doing it professionally, on a day to day basis, it's extremely difficult to keep up with crackers who do it for a living. 

I've got a problem with OWA, and I'm pretty much at the end of my rope with this. OWA authenticates perfectly on the internal network. No problems at all. On the outside, you have to log in two or three times before it "sticks" at which point it won't ask you to authenticate again for hours. The problem cropped up after an admin left and we were forced to change a bunch of passwords, so it's almost certainly a password/directory ownership issue somewhere. However, I've gone over the configuration and I can't find anything that's not running with a password unique to the local machine. The setup is Exchange 2003, running behind an Apache proxy. Since the problem is external only, I've gone over the proxy extensively, and I can't find any problems. The whole setup has been running fine for ~4 years, so again, it's probably tied to the password change (which shouldn't have effected the proxy in any way). I'm sure it's some stupid configuration setting that I'm missing, but I can't find it for the life of me. Anyone have any ideas? @PQD Here's mine: