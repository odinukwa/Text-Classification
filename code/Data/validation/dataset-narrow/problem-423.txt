The last two functions just get rid of the column to get the output in the format you want. If you don't mind having the column there, you don't need them. 

The constraints of your particular problem, in particular , mean that timing differences of this magnitude will be of no practical concern. You didn't do the requested formatting, as SuperBiasedMan has noted. 

Speed comparisons are always a good thing, but it can be tricky to determine what is actually being compared. I think it's premature to decide that the comparison is "Python" vs. "R" without a lot of work to verify that all the libraries you use and functions you write are reasonably optimized for each language. One of the strengths of python is a pretty good set of line profiling tools. I like , because it works in IPython notebooks, a format I find convenient for "real time" coding and tooling around. I used line profiling to run a (Python 2 version of) your code, and I found that one single line in your script was responsible for 94.5% of the execution time. Can you guess what it is? It was this one: 

You should use a line profiler tool to examine what the slowest parts of the code are. It sounds like you did that for your own code, but you could keep going and profile the source code that NumPy and SciPy use when calculating your quantity of interest. The module is my favorite. 

In IPython the result of the line profiler is displayed in a pseudo-popup "help" window. Here it is: 

It looks like a not-insignificant amount of time is being spent checking and removing invalid arguments from the function input. If you can be sure you will never need to use that feature, just write your own function to calculate the . Plus, if you are going to be multiplying probabilities (i.e. adding log probabilities), you could use algebra to simplify and factor out common terms from the summand for the normal distribution's pdf. That will lower the number of function calls to etc. I did this in a hurry, so I probably made a math mistake, but: 

DISCLAIMER: I don't know cython and have never used it, so if any of my advice doesn't apply because of cython limitations, feel free to disregard it. 

Your algorithm is needless constructing the entire filtered list and then finding the length of it. Instead of and similar commands, you could try . On my chat machine, this gives a speedup of about 2×. Here's some code to back that up, along with testing of a NumPy version. 

This makes the values in the column into numbers so we can use arithmetic to define the column groups. 

The way you have broken down the code into three functions is reasonable and logical. Nice! The functions you wrote don't have docstrings, so it is hard to know how to use them. I couldn't get your function to run initially, for example, because I didn't know what was supposed to be. Adding docstrings would make this clear. For the specific case of the parameter in , why do you need it? When I used the function I did it like this: 

Since is a hard-coded variable, Python convention is that it should be in all-uppercase, i.e. is a better variable name. What are and in your code? It's hard to figure out what those variables mean. Could you be more descriptive with your naming? Your call to on is hidden after the very long sentence. For readability I wouldn't hide any function calls at the end of very long strings. Python has multi-line string support using the delimiters. Using it makes the sentence and the code more readable, although at the expense of introducing newline characters that would show up on the histogram if they are not removed. In my code below I use the delimiter and remove the characters I introduced to break the string into screen-width-sized chunks. PEP8 convention is that code lines shouldn't be more than about 80 characters long. You should consider breaking this code up into two functions, one to make generate the data, and one to make the graph, but we can leave that for another time. 

As you can see, building the pandas data frame takes almost all the time. That suggests a trivial optimization: 

I wanted to post a -based solution to this and was prepared to insert the standard rant about how great dplyr and tidyr are for this kind of thing. But given the unique format of your particular data frame, I don't think the tidyverse approach (at least not the one I came up with) is all that great. Nonetheless, here it is: 

A different (faster) approach Conceptually, the connections between neurons can be thought of as a square matrix of boolean values. If there is a connection from neuron i to neuron j, then this matrix will have a at position . This is a much different representation than the array (i.e. list) of sources and targets you are trying to generate. I find it much more intuitive. The downside of the matrix representation is that many of the entries will be 0, and it requires creating a very large matrix, which is memory (and sometimes speed) inefficient. Thus, the lists of sources and targets you are trying to create are much more efficient. However sparse matrices offer a way to get the intuitive advantages of the matrix representation with the speed and memory properties of the "linked list" representation you are shooting for. In particular SciPy's COO sparse matrix format has an underlying representation that is nearly exactly the same as your list of target nodes and source nodes (i.e. a list of the column indices and a list of the row indices with nonzero values). Here's a version of that approach: 

Why instead of just arbitrarily specificying that the first neurons are part of the first population of encoders, the next, from to , the second population of encoders, etc. Since the number and ordering neurons is (presumably) arbitrary, it shouldn't matter if you number them in a way that's convenient for you. It's tough to understand your variable names. What is ? What is ? It would be nice to have a docstring that explained at least this much. 

Each word is converted to a list of integers by . The words list is converted to a numpy matrix. Instead of nested for loops, there is just one for loop, over the permutations of the row order of the matrix. The key expression is . This tests if every element () in the transpose of the row-permuted matrix is equal to the corresponding element original matrix. Since only one function from and one from were used, I just imported them by themselves. I added more comments. Rather than just from the middle of the loop, I added a variable and set it to only on success. This allows putting the print messages for success/failure in the same place of the code, which should facilitate future changes or porting this script into a function. If you want/need it, you could add at the end of my code block. 

This first function is a generalized function that creates random boolean matrices with non-zero elements in every column at random row locations. It can be used to create all the "sub-matrices" for each population of selective/encoding neurons and also for the other neurons. I tried to make a function that has the population you want and I think I did so. But you should check it carefully to make sure the population structure is really want you want. I may have gotten the rows and columns (sources and sinks) backwards, for example. Even if there are errors, I think this code serves to illustrate the approach: 

So with this alternate , the sigmoid method is now only 2× slower, not 20× slower. I should emphasize that (a) I don't know enough about neural networks to know if is an appropriate expression or approximation to the actual derivative of , but if it is in fact OK, then (b) the reason that is so much faster is because of this approximation/error. The remaining 2× difference is because in your factored expression of , you are needlessly evaluating twice. I'd instead do this: 

BTW, the more common choice for the error model would be Gaussian noise instead of uniformly distributed noise. You might consider adding a comment to the code to explain your choice of the uniform model.