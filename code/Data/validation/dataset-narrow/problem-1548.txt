1 and 2. You are in the right direction, you need to extract the features using a CNN, then instead of predicting the class you want to reshape the last layer of features and feed it directly into the RNN. A couple of things to pay attention to: 

The most common way of processing images in python through numpy arrays. Since you have already loaded your image through nibabel, you need to get the data from the image object and then cast it as a numpy array. 

The TF-IDF is a measure of the discriminating ability of a term in a document. The TF-IDF of a specific term increases if the term is more frequent in a specific document but decreases if it is frequent in the whole corpus. The problem with TF-IDF is that while it is good in distinguishing the document from the corpus, it isn't good in distinguishing one label from another! What you could try to compute is a variation of the TF-IDF, where in the nominator you would count the term frequency, not a specific document, but in the set of documents under the same label. The denominator would stay the same. This way, instead of getting the top terms for each document you could get the top terms for each label. This metric could give you a rough estimation of what you want. Note that while the top term for each label would be the most important in classifying examples to the specific label, it isn't necessarily the best for classification in general. The best term for classification, theoretically, would be one that could divide your data in two equal parts. 

A good library for machine learning with GPUs is mxnet. The package is mostly deep learning though, so if you are looking for specific machine learning algorithms you might not find them there. However they have a good set of deep learning algorithms. 

A good way to measure the difference between two probabilistic distributions is Kullbak-Liebler. You have to take into account that the distribution has integrate to one. Also you have to take into account that it's not a distance because it's not symmetric. KL(A,B) not equal to KL(B,A) 

I think there might be a problem in the way you are stating the problem. You say that you test data doesn't have two fields, but that can not be correct. You have to take all your data and split it into 2 groups, the training set and the test set. In a proportion of 80%-20% or 70%-30%. Then you train your algorithm with the data in the training set, and test the accuracy of the model with the data in the test set. The accuracy you get is the probability that your model is correct. Or said in another way, the next time you use your model to predict a sale, the accuracy is the probability that your prediction is real 

You might want to take a look at mxnet. It is a distributed library for deep learning. It supports C++, python, scala and R. There are many examples with R. Here you have an example of LSTM in R with this library. 

Just to add some more resources. Recently there was a paper studying the differences between several packages of neural networks and deep neural networks. Here you can find the information. It looks that Torch and TensorFlow are the winners. Note: not all of them are in python. However, I posted it to open the discussion. 

There are a lot of ways bias and variance can be minimized and despite the popular saying it isn't always a tradeoff. The two main reasons for high bias are insufficient model capacity and underfitting because the training phase wasn't complete. For example, if you have a very complex problem to solve (e.g. image recognition) and you use a model of low capacity (e.g. linear regression) this model would have high bias as a result of the model not being able of grasp the complexity of the problem. The main reason for high variance is overfitting on the training set. That being said there are ways of reducing both bias and variance on a ML model. For example the easiest way of achieving this is getting more data (in some cases even synthetic data help). What we tend to do in practice is: 

3.This is a good example of what you are trying to do. They basically try to recognize text from street photographs among other things with the same methodology you describe. Similar methodologies can be found in other research domains such as multi-label image classification, sequence labelling, facial expression recognition, etc 

First of all, you are correct that your code is old as some functions being used are deprecated (e.g. Convolution2D is now Conv2D see here). However, the error clearly states that you are trying to concatenate two tensors that their dimensions do not match. When concatenating two tensors along a specific axis, all other dimensions except the one being concatenated must be the same. In your case, you are trying to concatenate along axis=1, but the last dimension is different (256 for the first tensor and 80 for the second). I would recommend basing your U-Net code on a newer implementation (link1, link2, link3). These implementations replace your 

The main difference between supervised and unsupervised learning is the following: In supervised learning you have a set of labelled data, meaning that you have the values of the inputs and the outputs. What you try to achieve with machine learning is to find the true relationship between them, what we usually call the model in math. There are many different algorithms in machine learning that allow you to obtain a model of the data. The objective that you seek, and how you can use machine learning, is to predict the output given a new input, once you know the model. In unsupervised learning you don't have the data labelled. You can say that you have the inputs but not the outputs. And the objective is to find some kind of pattern in your data. You can find groups or clusters that you think that belong to the same group or output. Here you also have to obtain a model. And again, the objective you seek is to be able to predict the output given a new input. Finally, going back to your question, if you don't have labels you can not use supervised learning, you have to use unsupervised learning. 

Can somebody answer that? It would be good if the answer comes with evidences or some research paper. I'm not asking for opinions 

I would suggest Recurrent Neural Nets. They are good for time series, however they need a huge dataset to get good performance. Here you can find an implementation in torch. 

I think mxnet is one of the best options if you code in R. They have an R wrapper but the core is in C++. They have several examples in the web. One of them is the character recognition with MNIST database. They have support for multi-gpus and also for Spark. 

You could fit your model/pipeline (with default parameters) to your data once and see how long it takes to train. Then you would multiply that by how many times you want to train the model through grid search. E.g. suppose you want to use a grid search to select the hyperparameters a, b and c of your pipeline. 

By default this should be a 3D numpy array with a shape of (height,width,image). If you want to access one image you can always use PIL. For example to save the first image of the .nii file: 

The callback you are using isn't for displaying the desired metrics, just recording them. For example if you want to access the F1-score you need to type: . This is useful, let's say, if you want to make a graph on how the F1-score reduced during training. To use the EarlyStopping callback, however, f1-score needs to be a metric not a callback like you have it! You need to write (or find) a function that calculates the F1-score through keras' backend functions. You might want to check if this works for you. 

Suppose you have a categorical feature in your dataset (e.g. color). And your samples can be either red, yellow or blue. In order to pass this argument to a ML algorithm, you first need to encode it so that instead of strings you have numbers. The easiest way to do such a thing is to create a mapping where: red --> 1 yellow --> 2 blue --> 3 and replace each string with its mapped value. However this might create unwanted side effects in our ML model as when dealing with numbers it might think that blue > yellow (because 3 > 2) or that red + yellow = blue (because 1 + 2 = 3). The model has no way of knowing that these data were categorical and then were mapped as integers. The solution to this problem is one-hot encoding where we create N new features, where N is the number of unique values in the original feature. In our exampel N would be equal to 3, because we have 3 unique colors (red, yellow and blue). Each of these features be binary and would correspond to one of these unique values. In our example the first feature would be a binary feature telling us if that sample is red or not, the second would be the same thing for yellow and the third for blue. An example of such a transformation is illustrated below: Note, that because this approach increases the dimensionality of the dataset, if we have a feature that takes many unique values, we may want to use a more sparse encoding (like the one I presented above). 

I would try to set a multilabel classification algorithm and make the output standard by adding zeros. So if your data is like this: <1, 1>, <2, [1, 1]>, <3, [2, 1]>, <4, [1, 2, 1, 1]>, <5, [1, 1, 1, 2, 2, 1]>. The maximum number of output is 6. So you could transform your data into something like: <1, [1,0,0,0,0,0]>, <2, [1, 1,0,0,0,0]>, <3, [2, 1,0,0,0,0]>, <4, [1, 2, 1, 1,0,0]>, <5, [1, 1, 1, 2, 2, 1]> Another option that occurs to me is to add the limit dynamically. Let say you have your training and test set. You can search for the biggest length and create an algorithm that adds the zeros to both datasets. Then let's say a new data you want to predict has a bigger length, then you'll need to recompute all training and test with for this new prediction. You can even check how extending the limit affects your model. 

The model I would use is the one that minimizes the accumulated quadratic error. Both models you are using, linear and quadratic, looks good. You can compute which one has the lowest error. If you want to use an advanced method you can use RANSAC. It is an iterative method for regression that assumes that there are outliers and remove them from the optimization. So your model should be more accurate that just using the first approach I told you. 

SVM can be used for classification (distinguishing between several groups or classes) and regression (obtaining a mathematical model to predict something). They can be applied to both linear and non linear problems. Until 2006 they were the best general purpose algorithm for machine learning. I was trying to find a paper that compared many implementations of the most known algorithms: svm, neural nets, trees, etc. I couldn't find it sorry (you will have to believe me, bad thing). In the paper the algorithm that got the best performance was svm, with the library libsvm. In 2006 Hinton came up with deep learning and neural nets. He improved the current state of the art by at least 30%, which is a huge advancement. However deep learning only get good performance for huge training sets. If you have a small training set I would suggest to use svm. Furthermore you can find here a useful infographic about when to use different machine learning algorithms by scikit-learn. However, to the best of my knowledge there is no agreement among the scientific community about if a problem has X,Y and Z features then it's better to use svm. I would suggest to try different methods. Also, please don't forget that svm or neural nets is just a method to compute a model. It is very important as well the features you use.