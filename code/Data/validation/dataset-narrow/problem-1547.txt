It's hard to answer your question because it is too general. It's like asking how to build a website without even knowing how to do programming. But I will be constructive Given your background and assuming you are familiar with Python, I recommend you using auto-sklearn. is a library based on the popular sklearn which automates a lot of steps involved in general machine learning, like feature selection and hyperparameter-tuning. It also tries a bunch of algorithms and ensemble them to produce the best result. It is hard to beat its performs as a machine learning beginner. While is very good for a beginner to get started, I must warn you that, without fully understand the how and why it is extremely risky for you rely on it blindly. One thing of particular importance is you need to understand how to evaluate the performance of a machine learning algorithm. Please make sure you understand the difference between train, validation and test dataset, before you make any claim on how good/accurate you prediction is. 

If your old data is representatively of the underlying population and there is no radical shift of the underlying true model over time. Than you don't need to update you model as new data comes in. You might want to retrain you model with the new dataset periodically, but you don't need to do it in real time on your production machine. If the underlying true does change radically overtime and you don't have any existing data to captures those changes then doesn't matter what algorithm you use, it's not going to work. The information is simply not there. 

(I don't have a machine with install at the moment, so don't expect the code to run with some debugging) 

Randomness does not implies a 50/50 probability. You can have randomness as long as the probability is not 100/0 or 0/100. However, I believe it is not the randomness you are really asking here. In stead what you really asking is how to test whether the random process is bias. Formally, let's use $Z \in (0, 1)$ to denote the outcome of an experiment of assigning a user. Then $X$ follows a Berniulli distribution with some probability $p$. The sum of the a series of Berniulli variables $X = Z_1 + Z_2 + ...+Z_n$ is a Binomal variable. We have also obvsered samples ($n = 304,641$) drawn from the experiment. Your question is essentially a hypothesis test with $H_0: p=.5, H_a:p\neq.5$. Given the null hypothesis, the mean and variance of X can be calculated. $\mu_x = np = 304,641 \times 0.5 = 152,320.5$ $\sigma_x^2 = np(1-p) = 76,160.25$ ($\sigma_x = 275.97$) Because our $n$ is large, we can approximate the binomal distirbution by a normal distribution $N(\mu_x, \sigma_x )$. It follows, assuming the null hypothesis, then $P(x >= 154,490) \approx 0.0$ . So we can reject the null hypothesis and conclude that the random process is not a 50/50 process. 

I have a multiclass(7 labels) classification problem implemented in MLP. I am trying to classify 7 cancers based on some data. The overall accuracy is quite low, around 58%. However, some of the cancers is classified with accuracy around 90% for different parameters. Below cancer 1,2,3 etc means different types of cancer, For example 1= breast cancer, 2 = lung cancer etc. Now, for different parameter settings I get different classification accuracy. For example, 1. hyper parameters 

Group similar/clear images of cat as value 1, a less similar/blur images as 1.1 and so on. Similarly, group similar images of dog as value 2 and less similar images as 2.1 and so on ... Do the same for all types of images. 

There are lots of examples of classifier using deep learning techniques with CIFAR-10 datasets. The way it works is that, train thousands of images of cat, dog, plane etc and then classify an image as dog, plane or cat. But I want to do the reverse thing. I want to train dog, cat, planes and it should output images. Here is my idea 

Now train values of input and label of images as output. Input will be just 1 dimensional, may be I will think of some other data. Output layer will have 24*24 i.e 576 units or neurons. At the end of training, I want something like this, if I give a input for example 1.15 it should output a new image since we trained with values 1.1 and 1.2 but we didn't train with 1.11 or 1.115. 

In short, it could be described like this. Suppose, there is a 1000m long straight line. I randomly select a number n between 20-100. Then randomly select n segments from the reference line and create a sample. Similarly, for second sample, select a random number between 20-100 and take that many random segments from the reference line. Then randomly select 50% of the samples as class 1 and rest of the samples as class 2. So, there are two types of samples, class 1 and class 2. Each sample consists of n number of segments. In above example, sample 1 consists of 3 intervals and second sample consists of 4 intervals. Number of intervals vary from sample to sample. Now I want to use a deep learning algorithm to classify sample type as either class 1 or 2. Which deep learning algorithm should I use? Or what would be the solution for the problem using deep learning technique. 

In other word, how to we do interpolation with a fixed value, or a operation but ignore the at the beginning and end of the array. 

You method could work in theory. But typically, for vanilla LSTM, the setup looks like $x = [x_0, x_1, ..., x_{n-1}]$ and $y = [x_1, x_2 ..., x_n]$. I suspect you will get much better result if you do it this way. Highly recommend Colah's excellent explaination if you haven't read it already. 

Summary Say you have a single sequence [a,b,c,d,e,f,g,h,i]. Using a sliding window of size 6, your single squence will be transformed into 4 sequences 

Yes, it is very common and sometimes necessary to use the target variable for stratified sampling. Consider the case of fraud detection, given a bunch of features about a person (e.g. income, gender, position etc) we want to predict the likelihood of that person has committed the crime (a boolean value indicating whether the person is a suspect). This dataset is likely to be very asymmetric with very few positive examples. Now if we want to use k-fold cross-validation, we must stratify the samples using the target variable. If we don't, we might end up with a fold without any positive example at all and no metrics can be calculated from that fold. 

Correction. Had a bit better thinking about it. it is not a gradient explosion problem here. More likely it is due to saturated neuron. Because LSTM uses sigmoid function, if the input is not properly scaled or weight is initialised inproperly, it could cause neuron saturation very quickly. 

You don't need to do this kind of feature engineering like you would in traditional machine learning. If you have a network that is sufficiently wide and deep, your network should be able to "engineer" those features itself. Just think about how the first hidden layer is a linear combination of input features. And the second hidden layers is a linear combination of the first hidden layer. It should be obvious to see how a neural network is capable of structuring any polynomial features itself. Suggestion Try using the original 60 features directly and start with 3 layers. Given the size of the network, the training should take less than 10 minutes on a GPU-enabled machine. Compare your result with your benchmark model (you should always have a benchmark model). Play around with the hidden size and layers, rinse and repeat.