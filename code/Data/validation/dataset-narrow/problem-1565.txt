The target data is the desired output that happens to be in the real word. This information is known to you already(given the problem is supervised learning problem). So, for an 'AND' gate the target value is the vector stored in y on the right hand side as shown below: 

It depends. If you are doing a task for which the weights of a very large network, trained on the same type of problem, are present, then it's better to use the weights of that pre-trained network. You can also fine tune the layers later on. One such example is the VGG16 Net. This NN was used in the ImageNet challenge in 2014, hence if you are trying to do image classification tasks where your images are the subset of ImageNet, then you should use the pre-trained VGG16 weights. This is a great tutorial if you want to go in details. But the above situation might not be true in every case. For example, the above case is not true if you reverse the situation. 

I was trying to build a model in scikit-learn. I used RandomForestClassifier as my method for classification. In order to improve the score and efficiency of my model, I thought to use GridSearchCV. Here is the code: 

I have a corpus on which I want to perform sentiment analysis using LSTM and word embeddings. I have converted the words in the documents to word vectors using word2vec. My question is how to input these word vectors as input to keras? I don't want to use the Embeddings provided by keras. Thanks in advance. 

Yes, your approach seems to be right. The only thing that I would like to point out is that while it is desirable to convert categorical features to dummies but if you don't have enough memory then you can even consider to factorize your categorical variables. You can read about it more here. Also, just make sure that after converting to dummies, the number of features are reasonable compared to the number of training samples. 

Will encoding the activity_type in the above code using OneHotEncoder in sklearn will improve the model in anyway? Is it necessary to encode that feature. And if yes, which should I choose : LabelEncoder or OneHotEnocder? Thanks in advance. 

Suppose I write a program in Keras for MNIST data set. I used Tensorflow as my backend in keras. Is it possible in any way that I can extract the back end code used for tensorflow that has been used by keras. if yes, please provide an example. Thanks in advance. 

I am working on a dataset. The dataset consists of 16 different features each feature having values belonging to the set (0,1,2). In order to check the distribution of values in each column, I used method which gave me a plot as shown below: I want to represent the distribution for each value in a column with different color. For example, in column1 all the values corresponding to '0' should be in red color while the values corresponding to '1' in green color and so on. How can I do this? Please help !! 

For confusion matrix you have to use sklearn package. I don't think Keras can provide a confusion matrix. For predicting values on the test set, simply call the model.predict() method to generate predictions for the test set. The type of output values depends on your model type i.e. either discrete or probabilities. 

I was trying to implement a regression model in Keras. But I am unable to figure out how to calculate the score of my model i.e. how well it performed on my dataset. 

This is because you have very little amount of data. If not enough data is provided to CNNs, they are very likely to overfit. You can do the following things in order to overcome this problem: Data Augmentation : It is a technique to create new examples from the training examples by doing some preprocessing on them e.g. Rotation, Scaling, etc. Rigorous Dropout : Dropout is a very powerful technique to control overfittting. You can try using more rigorous dropout in your architecture 

I have a dataframe with around 37,000 rows and 54 columns. Out of these 54 columns two columns namely 'user_id' and 'mail_id' are provided in avery creepy format as shown below: 

Before briefing about the other things, I would like to suggest you something good. It is always better to set a seed number in machine learning and deep learning problems to get a reproducible result and by that I mean that every time when you try to run your model anywhere, it can produce the same best result which you got on your machine while experimenting. So, you should always set . Now how to compare your result to your expected result? There are a number of parameters that you can look upon during training your model. Some of these are: 

Suppose I have a dataset of size(10000, 45). One of the features in the dataset is activity_type in which the values vary from 1 to 15 as shown below: 

And above all, I suggest one thing. Training such a large dataset on a local machine can be too much of pain. So, it's better if you deploy your model on AWS or Google's Machine Learning platform provided by them on cloud. I hope it helps!! 

Then for weight_regularizer, . But for activity_regularizer, . Activity regularizers are generally used when you are quite aware of the distribution of the test dataset. For your second argument, I would say you are pretty wrong. ANN as well as CNN both can suffer from overfitting. In order to prevent the model to overfit, we generally use a lot of regularization techniques among which Dropout is quite popular. 

It's always a good practice to perform data cleansing before actually building a model and applying some algorithm on it. In order to data cleansing like handling missing values, "pandas" library is highly preferred. Here is the link to the "pandas" latest version : $URL$ And here is the link for "Working with missing values" reference in pandas : $URL$ 

I know how to split the dataset into train and test sets using train_test_split but is there any way that I can split the dataset into three different sets i.e. "Train set", "Test set", and "Validation Set". An example will be enough. Thanks in advance. 

Having highly correlated features is a type of redundancy in features. And yes, it effects a regression model if you are having highly correlated features. A very nice explanation is given here. PCA is a nice choice when it comes to dimensionality reduction. 

In order to perform such tasks with high accuracy, I suggest you to build a LSTM model with word embeddings with the help of word2vec. LSTMs can help to retrieve information from sentence as well as predict the next character or word given a set of words is already present in the sentence. 

This is probably the best answer to your question from the guys who uses too much of xgboost and stacking: $URL$ 

This is most likely a database file. You cannot directly open it in pandas. You can use a software in order to convert it first to a csv or something that can be opened in pandas. One such application is here $URL$ 

Since I am noob to word vectors I have two doubts. 1- Setting the number of features to 300 defines the features of a word vector. But what these features signify? If each word in this model is represented by a 1x300 numpy array, then what do these 300 features signify for that word? 2- What does down sampling as represented by 'sample' parameter in the above model do in actual? Thanks in advance. 

Suppose I build a NN for classification. The last layer is a Dense layer with softmax activation. I have five different classes to classify. Suppose for a single training example, the is while the predictions be . How would I calculate the cross entropy loss for this example? 

I don't know how to put this but your understanding of distribution is wrong and confusing you. Data Augmentation is a technique which helps the model generalize well. It is also useful when you have a limited amount of data and you still want to train a deep neural net. Transfer learning is the best example of that. And yes train/test should come from the same distribution. For example, and it makes sense if you put this thing in the real world too. If you are trained to speak English, would it make sense to evaluate you on how good you speak German? Does augmentation change the distribution? I will ask you a question on this and you should reason that for yourself. Here is my question: If you rotate the image of a dog by 30 degrees, does it change the fact that it's a dog? 

It is a problem specific and data specific problem. For one problem, 95% accuracy may be good, for some other it might not be very good and there might still be room for improvements. Likewise, it might happen that the problem is well defined, your architecture is good but due to a lot of noise in the data, you can't do better than 95%. In that case, we will say the results are good. Ensembles: Ensembling is a general term for combining many classifiers by averaging or voting. It is a form of meta-learning in that it focuses on how to merge results of arbitrary underlying classifiers. Generally, ensembling improves the final results but again, not necessarily. If you are ensembling two highly correlated models, your ensembled model will give almost the same result as your stand-alone models. 

It depends. A high correlation between two features suggests that they represents almost the same information. For some problems like clustering, it is always useful to remove redundant features while some algorithm like Gradient Boosting in xgboost is not affected at all by such features. So, it all depends on what you want to do with your data set. As per my opinion, if your dataset has too many features, then I would suggest to check the correlation between those features and apply PCA to reduce the dimensionality of your dataset especially if you are doing tasks like clustering or regression. 

After doing a lot of analysis on my data I have found that I cannot drop this feature set from my model so I have to convert it to something meaningful. Can anyone please explain me how to do this efficiently? Thanks in advance. 

Before trying to extract features, you need to define your network. Suppose your network has an architecture like this: 

If you have more than two targets, then you should use activation instead of sigmoid. Softmax activation gives you the probability associated with each class in the output. The only thing you need to do before applying softmax is to convert your targets into one-hot encoded vectors. If you want to know more about softmax in detail, you can read about it here 

Assuming that the three columns in your dataframe are , and . Then you can do the required operation like this: 

You may or may not apply neural network to your problem. Neural network will give predictions for sure but it can't be said how much efficient will it be for this problem. You have to code and check it for yourself. Also, if you have all the above data in tabular form and you know the labels(which I guess is ), I will say try it with xgboost. Neural networks are exceptional for unsupervised learning but in the case of supervised learning, there may be a model which may outperform a neural network. 

Now I want to visualize the vote_count for the timestamps and do some analysis on that further. How can I plot the two columns against each other using matplotlib or seaborn? Note : The timestamp is in 24hr format. Also, at any timestamp, there can be multiplt vote counts. 

The last line in your code is the result of above error. This is because the 'fit' method takes only two arguments i.e the data and the labels. In order to get rid of the above error, modify your code as following: 

How do I interpret this visualization in order to check for things like skew in the data points,etc? Thanks in advance. 

I am trying to do sentiment analysis. In order to convert the words to word vectors I am using word2vec model. Suppose I have all the sentences in a list named 'sentences' and I am passing these sentences to word2vec as follows : 

In the above code an instance of svm is your estimator for your model for which the hyperparameters, in this case, are and . But your model has another parameter which is not a hyperparameter and that is . I hope it helps!! 

I was building a model for a classification problem in Keras for which I used the KerasClassifier, the wrapper scikit-learn. Below is the code for the same. 

Both Python and R are extensively used in the field of data science.Though both have their advantages and disadvantages, python has many libraries that are awesome to use for data science purpose. For example, scikit-learn, Keras, Theano, XGBoost, Matplotlib,etc. These make lives so easy for data science. Yes, analyzing and understanding the code of others can give you a lit of information but you cannot learn until you don't get your hands dirty with some python code. In order to learn how to use python in data science, the following link provides an excellent tutorial. $URL$ 

As per my knowledge, it doesn't matter for a decision tree model whether the features are ordinal or categorical. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. Decision trees describe patterns by using a list of attributes. For a more detailed explanation, I am providing here some links which you will find helpful for such queries. $URL$ $URL$ $URL$ 

Though there can be a very detailed explanation for this question but I will try to make you understand much minimal words. 1) Cropping the images to a particular size isn't a necessary condition and neither is scaling. But put this way, it doesn't matter whether a dog is represented in a B&W image or RGB image because a convolution network learns features in the images which are independent of colors. Scaling and resizing help to limit the value of pixels between 0 and 1. 2) Once you have trained your CNN model, it has learned all the features like edges,etc. to recognize a dog in the image. Because the model has learned the features, it acquires certain properties like translation invariance which means that no matter where you position a dog in the image, it's still a dog and have the same features. How the model recognize it? It checks for the features of a dog, learned during training, no matter what the size of the new image is or where the dog is in the image or what the dog is doing. For getting a in-depth understanding you can refer to the following resources: $URL$ $URL$