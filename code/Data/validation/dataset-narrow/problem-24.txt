Therefore you don't do any matrix multiplications to get to a projection matrix. Those multiplications happen in the shader, where you do $Projection\cdot View \cdot Model$$^1$ Matrix (for example in the vertex shader to specify your position output). Also, remember that a perspective projection changes angles (i.e. parallel lines won't be parallel anymore). As far as I can see that is missing in your derivation. Edit As to how you actually derive it, I'll largely use the explanation byEtay Meiri. It has some additional information and illustration, so you may want to check it, if something seems unclear. First, your camera is (as mentioned earlier) positioned in the origin. Now assume, you will only project onto a plane of distance $d$ and the plane you project onto is bounded by the aspect ration $ar$ (that is $ar = \frac{window\text{ }width}{window\text{ }height}$ in positive and negative $x$ direction and by $1$ in positive and negative $y$ direction. You can now determine $d$ by your vertical field of view $\alpha$ (since looking from the side, your camera, the center at the top of your projection plane and the center of the bottom of your projection plane build a triangle and your vertical field of view is the angle at your camera): $\frac{1}{d} = \tan(\frac{\alpha}{2})\\ \implies d = \frac{1}{\tan(\frac{\alpha}{2})}$ Now you go about calculating projected points. Assume you have an arbitrary point $v = (v_x, v_y, v_z)$, and you want to calculate the point on your plane $p = (p_x, p_y, d)$. Looking at it from the side again, the triangle with your camera, your projection plane top center point and your projected point is similar to the triangle with your camera, your projection plane top center prolonged to have the same z coordinate, as your point $v$ and the point $v$, i.e. they have the same angles$^2$. Therefore, you can now calculate the projected $p_x$ and $p_y$ coordinate: $ \frac{p_y}{d} = \frac{v_y}{v_z} \implies p_y = \frac{v_y\cdot d}{v_z} = \frac{v_y}{v_z \cdot \tan(\frac{\alpha}{2})}\\ \frac{p_x}{d} = \frac{v_x}{v_x} \implies p_x = \frac{v_x\cdot d}{v_z} = \frac{v_x}{v_x \cdot \tan(\frac{\alpha}{2})} $ To additionally take into account, that the projection plane ranges from $-ar$ to $ar$ in $x$ direction, you would add $ar$ to the denominator in order to make the projected $x$ range be $\left[-1, 1\right]$: $ p_x = \frac{v_x}{ar \cdot v_x \cdot \tan(\frac{\alpha}{2})} $ If you think about applying this to a matrix now, you need to take into account the division by $z$, which is different for any point with differing $z$ value. Therefore, the division by z is deferred until after the projection matrix is applied (and is done without any code of you, i.e. for OpenGL you'd specify the in Clip Space ($\implies$ after projection) and the division is done for you). The depth test (Z-Test) still needs the $z$ value though, so it needs to be "safed" from the $z$ divide. Therefore you copy your $z$ value to the $w$ component (which is what you got right in your assumption) and end up with the following matrix: $\left( \begin{array}{cccc} \frac{1}{ar\cdot\tan(\frac{\alpha}{2})}&0&0\\ 0&\frac{1}{\tan(\frac{\alpha}{2})}&0&0\\ 0&0&0&0\\ 0&0&1&0 \end{array} \right)$ The next step is to not project onto a plane, but into a $z$ range of $\left[-1, 1\right]$ (this range is for correct clipping, as is the same range when handling the $x$ and the $y$ coordinate). To be more specific, you don't want all points to end up in that range, but all points between your near and your far plane, so you map $[n, f]$ to $[-1, 1]$. The resulting range is $2$, so you first scale your near-to-far range to $2$. Then you move it to be $[-1, 1]$: $ f(z) = A\cdot z + B $ And now taking into account that we want to safe the $z$ value from $z$ divide, we get to $f(z) = A + \frac{B}{z}$ Mapping this scaling to $[-1, 1]$ is a little bit of leg work: you know that any point with $z = n$ (on your near plane) will be projected to $p_z = -1$ and any point with $z = f$ (on your far plane) will be projected to $p_z = 1$. This leads to the following equation system: $ A + \frac{B}{n} = -1\\ A + \frac{B}{f} = 1 $ Solving this for $A$ and $B$ leads to$^3$: $ A = \frac{-n-f}{n-f}\\ B = \frac{2fn}{n-f} $ Your third row of the projection matrix must produced the (undivided) $p_z$ (projected) z value. Therefore, you can now choose the individual elements of said row $(\begin{array}{cccc}a&b&c&d\end{array})$ such that the correct $z$ value is produced when multiplying with your point. The correct $p_z$ value is (as established earlier): $p_z = A\cdot z + B$ Therefore this must be the right hand side of your multiplication. Assume your point to project is $(x, y, z, w)$, then you must achieve the following: $ a \cdot x + b \cdot y + c \cdot z + d \cdot w = A \cdot z + B $ Obviously, your point's $x$ and $y$ coordinate should not influence the projected $z$ coordiante, so you can set $a$ and $b$ to $0$. $ c \cdot z + d \cdot w = A \cdot z + B $ Since you know that $w$ for any point is $1$, you're left with $ c \cdot z + d = A \cdot z + B $ And thus, you have $c = A$ and $d = B$. Now your matrix is complete for projection $\left( \begin{array}{cccc} \frac{1}{ar\cdot\tan(\frac{\alpha}{2})}&0&0\\ 0&\frac{1}{\tan(\frac{\alpha}{2})}&0&0\\ 0&0&\frac{-n-f}{n-f}&\frac{2fn}{n-f}\\ 0&0&1&0 \end{array} \right)$ Obviously, the tutorial works a little differently in projecting with the vertical field of view, so we can take a look at how you can additionally get to that (with the help of Eric Lengyel: Instead of projecting onto a plane at $z = d$, we will project onto the near plane, and thus get for a point $v = (v_x, v_y, v_z)$ the projected point $p = (p_x, p_y, n)$: $ p_x = \frac{v_x \cdot n}{v_z} $ Additionally, you want to map any point with $l\leq x \leq r$ to $\left[-1, 1\right]$, as before. Thus you get $f(x) = (x-l) \frac{2}{r-l}-1$ Combine those two and you achieve $p_x = \frac{2n}{r-l}\left(-\frac{v_x}{v_y}\right)- \frac{r+l}{r-l}$ Now put this (and the term for $y$) into the matrix and you get to the first row, first column being $\frac{2n}{r-l}$, and the first row, third column being $\frac{r+l}{r-l}$. Since you may assume $r$ and $l$ to be different, here you can see why those two matrices differ. $r$ and $l$ are guaranteed to be the same in the tutorial's matrix and therefore you get $r+l = ar-ar = 0$ in the denominator, making the third row first column (and second column accordingly) $0$. 

I'm sorry I'm bringing this topic up again, but I need to expound some of the topics. With reference to this question, I was wondering if someone can help me out in expanding some of the details. In the answer to that question we have: 

A good reference for game physics is this chapter 4 describes the basic free deformable surfaces (nurbs and bspline are of course cited and treated enough well) fluid and gases are instead treated in chapter five (basically the author derivate simplified model of navier stokes equations, suitable for real time applications). So actually i guess what i've written commenting your post was correct, i.e. combine the physical deformation of the point that controls the shape of the surface with a ffd computations. The book i cited should provide somewhere/somehow on the web source code, and itself it cites some example involving the technique i cited. If that is what you need let me know for a more specific explanation. 

In my opinion and experience i don't think there exists an univocal answer... since basically in literature you can easily find example of adaptive filters too (i.e. of variable size). I think the actual answer should be related to the both context of applications (i.e. hardware or sofware, real time or not) and kind of scene you're going to synthesize (some scenes usually involves different kind of aliasing when are synthetized (i use this general term on purpose)). Basically computer graphics is the study of algorithms and data structure for image synthesis, and such definition isn't stricly related to any kind of application. Of course an important factor is even the goal to be achieved by a filtering process (i.e. not necessary an excessive blurring could be bad...). If you're speaking of "nice to see" i think you could agree with me when i say that there's no specific measure of "pleasant image". 

In homogeneous coordinates i would do something like this: $$\vec{p} = \left(\frac{2}{x+y}, \frac{5y + z}{2x + 2y}, 3\right)^T = \left(\frac{2}{x+y}, \frac{\frac{5y + z}{2}}{x + y}, \frac{3x + 3y}{x + y}\right)^T$$ adding the fourth coordinate $$\vec{p}^H = \left(2, \frac{5y + z}{2}, 3x + 3y, x + y\right)^T = \begin{pmatrix} 0 & 0 & 0 & 2 \\ 0 & \frac{5}{2} & \frac{1}{2} & 0 \\ 3 & 3 & 0 & 0 \\ 1 & 1 & 0 & 0 \end{pmatrix} \cdot \begin{pmatrix}x \\ y \\ z \\ 1\end{pmatrix}$$, If you want a systematic way, since a linear transformation is completely characterized from the transformation of a basis, take the canonical bases i.e. ($\vec{e}^H_0 = (1,0,0,0), \vec{e}^H_1 = (0,1,0,0),...$) apply the transformation in $\vec{p}^H$ it will give you for $i = 0,...,3$ the columns of the matrix. 

You have missunderstood this. $D$ is a Normal Distribution Function (or short NDF), so it doesn't really give you a single normal, but a distribution. In a (specular) BRDF you are always using the normal that is the half vector between the incoming and outgoing light, since via your theory, every microfacet is a perfect mirror and thus every microfacet reflects light exactly along the (micro) surface and that one only. You also must not confuse roughness with microfacet normal. The roughness is a more or less arbitrary value (and a scalar, so a single one at that). The roughness is being used differently, depending on the overall BRDF (e.g. roughness in GGX(/Trowbridge-Reitz) is different from the Roughness in Oren-Nayar) and therefore can have different ranges. Still, if you look at one specific NDF, you will see, how the roughness is used. $D(\omega_m) = \frac{\alpha^2}{\pi((\omega_n \cdot \omega_m)^2 (\alpha^2-1)+1)^2}$ with $\alpha = roughness^2$ (a common remapping), $\omega_m$ is the microfacet normal, $\omega_n$ is the geometry normal. You can see here, that the roughness is being treated like a single value. The higher the roughness is, the more random your microfacets (or normals thereof) are distributed. The more that happens, the less your surface will reflect light concentrated into the same direction (i.e. the less highlights you will have). Thus, your material is more diffuse. Since now your roughness is a single value, the map makes sense to have only values in $\left[0, 1\right]$ and therefore is a gray scale image. 

Your way of calculating XYZ functions is probably the most efficient way to go about calculating accurate colors from a spectrum. It is standard practice afaik, for examples the books Physically Based Rendering (3rd) and Real-Time Rendering (3rd) both use this method. You can add the colors in RGB space, but only if you convert from sRGB to linear RGB first. Otherwise you need to take into account, that sRGB sums will not lead to correct colors. The blogpost Adventures with Gamma-Correct Rendering by Naty Hoffman is a good read regarding this, topc: 

If that is what you wanted, then you need to remove the fragment shader position calculation. On a sidenote: I had problems with using in a shader the other day, so it might just be that this doesn't compile. Try using instead.