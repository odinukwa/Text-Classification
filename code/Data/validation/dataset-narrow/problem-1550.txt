Think of it this way: a PCA "transform" with $k$ components essentially approximates your $n$-dimensional data points by projecting them onto a $k$-dimensional linear subspace, trying not to loose too much data variance along the way. More precisely, what you are doing is representing your original points $y \in R^n$ as: $$y \approx \mu + Vx$$ where $x \in R^k$ are the lower-dimensional "new coordinates", $\mu \in R^n$ is the mean of your data and $V\in R^{n \times k}$ is the matrix of principal component vectors. The "new coordinates" $x$ tell you how many steps you need to make along the $k$ principal components in order to reach the best possible linear approximation to $y$ by starting your trip from $\mu$. Now, if $k=0$ the model becomes: $$ x \approx \mu. $$ In other words, you are modeling all of your data as a single, fixed center point. Of course, you do not need to store any "new coordinates" here (because you do not need to move away from the mean), hence it does not make much sense as a "transform", but it is, none the less, a proper probabilistic model (a maximum-likelihood fit for a Gaussian distribution of errors, to be precise). In particular, you can speak about the log-likelihood of data under this model (which is, up to an affine transform, equal to the sum of squared errors here, but is not as trivial as you might think in the general case) and we can compare various models, choosing the one with the best likelihood. This is exactly what is done in the example from the docs you mention in the question. 

Your intuition of "trading off" by "subtracting" a value is correct when you speak in terms of $\Delta R$ and $\Delta P$ (as you yourself noticed in the edit), but is not valid if you speak in terms of derivatives. Think of it this way: a partial derivative with respect to a parameter tells you by how much your metric changes if you improve that variable. By the very design, $F$ will always increase if you improve either $R$ or $P$ - its partial derivatives are positive everywhere. They are not equal everywhere, however. At some configurations $(R,P)$ you would gain more by improving one parameter rather than the other. Say, if your $R$ is much lower than $P$, your $F$-score would increase more if you improved the $R$ a bit rather than $P$. In the case of an $F_1$ metric, $R$ and $P$ enter the formula symmetrically and it is kind-of easy to see that whenever $R=P$, either of the components contributes equally to the increase of $F$. In fact, the set of points where $R$ and $P$ are "equal in importance" is exactly the line $R=P$ (which can also be written as $R/P=1$, if you prefer). Now consider the equation for $F_\beta$: $$\frac{\text{constant}}{\beta^2\frac{1}{R} + \frac{1}{P}}.$$ Let us see how much it changes in response to increasing $R$ or $P$. As both parameters are in the denominator, the change in value of $F_\beta$ is proportional to the change in the value of its denominator, hence we may simplify a bit and only consider the change in $$\beta^2\frac{1}{R} + \frac{1}{P}$$ You can now compute that a $dx$ increase in $R$ would decrease the value of the expression above by $\beta^2\frac{dx}{R^2}$. A similar increase in $P$ would decrease the value of the above expression by $\frac{dx}{P^2}$. There is a set of points $(R,P)$ where these effects are exactly equal. These are the points where $$\beta^2\frac{dx}{R^2} = \frac{dx}{P^2} \quad\Rightarrow\quad \frac{R}{P}=\beta$$ When $\beta > 1$ you are effectively putting more marginal importance on recall, in the sense that until your recall is at least $\beta$ times the precision, it is always beneficial to work on it, rather than improving precision. 

If you somewhy tend to "think" in a right-to-left manner. Suppose you want to list courses along with the number of students enrolled. The "left to right" thinking goes as follows: first select all Courses, then left-join all Enrollments, then aggregate. You may also think differently. The "important" data source you want to start with is the Enrollments table, after all, so why not start by reading data from it, and perhaps even aggregating it first. Then you could join the Courses table to get the course names in place of IDs. In this situation you would be right-joining the Courses. If you suspect a right join would be more efficient. Suppose that, in the example above, there are thousands of courses, only a couple of them are non-empty, with hundreds of students. In this case, a "straightforward" manner to execute a "Courses left join Enrollments" query would be to scan the Courses table, trying to match up each course by scanning the Enrollments table (assume the query optimizer is dumb and there are no indexes on the tables, for the sake of example -- is there a query optimzer in dplyr, after all?). Only two courses would have matches but the scan would have to proceed for a thousand times. The resulting table would have the two nonempty course names repeated hundreds of times for each enrolled student alongside thousands of course names with NULL students, after which you'd proceed to the aggregation. On the contrary, if we first aggregated Enrollments and then right-joined the Course names in the "right-to-left" manner, the aggregated Enrollments table would only have two rows, and thus perform just two ID matches over the Courses table (writing out the remaining course names as-is). Fortunately, most SQL database engines are smart enough nowadays to deal with such optimizations internally without the need for the user to hand-tune the direction and order of joins in the query. Finally, sometimes you generate SQL programmatically. In this case again, the need to "join in" the Course or Student names to some previously computed main result table could be quite common, and a right join perfectly appropriate. 

That was a long digression on right situations for right joins, let's get back to the outer join. It is also rarely used, in my experience. They say outer joins are harder for SQL engines to optimize (although I can't at the moment exactly see why so). Anyway, in our example one could use an outer join to create a table of the form (Course, List of Students), with two extra conditions: 

You are missing the fact that the condition $\frac{\partial F_\alpha}{\partial P} = \frac{\partial F_\alpha}{\partial R}$ does not hold everywhere, but just on the set of points $R=\beta P = \sqrt{\frac{1-\alpha}{\alpha}}P$. This line intersects the level curve at just a single point. The line with slope -1 would be the tangent to the level line at that point - the green dashed line on the illustration below. 

There are two possibilities: either you need to have all your data in memory for processing (e.g. your machine learning algorithm would want to consume all of it at once), or you can do without it (e.g. your algorithm only needs samples of rows or columns at once). In the first case, you'll need to solve a memory problem. Increase your memory size, rent a high-memory cloud machine, use inplace operations, provide information about the type of data you are reading in, make sure to delete all unused variables and collect garbage, etc. It is very probable that 32GB of RAM would not be enough for Pandas to handle your data. Note that the integer "1" is just one byte when stored as text but 8 bytes when represented as (which is the default when Pandas reads it in from text). You can make the same example with a floating point number "1.0" which expands from a 3-byte string to an 8-byte by default. You may win some space by letting Pandas know precisely which types to use for each column and forcing the smallest possible representations, but we did not even start speaking of Python's data structure overhead here, which may add an extra pointer or two here or there easily, and pointers are 8 bytes each on a 64-bit machine. To summarize: no, 32GB RAM is probably not enough for Pandas to handle a 20GB file. In the second case (which is more realistic and probably applies to you), you need to solve a data management problem. Indeed, having to load all of the data when you really only need parts of it for processing, may be a sign of bad data management. There are multiple options here: 

Use an SQL database. If you can, it is nearly always the first choice and a decently comfortable solution. 20GB sounds like the size most SQL databases would handle well without the need to go distributed even on a (higher-end) laptop. You'll be able to index columns, do basic aggregations via SQL, and get the needed subsamples into Pandas for more complex processing using a simple . Moving the data to a database will also provide you with an opportunity to think about the actual data types and sizes of your columns. If your data is mostly numeric (i.e. arrays or tensors), you may consider holding it in a HDF5 format (see PyTables), which lets you conveniently read only the necessary slices of huge arrays from disk. Basic numpy.save and numpy.load achieve the same effect via memory-mapping the arrays on disk as well. For GIS and related raster data there are dedicated databases, which might not connect to pandas as directly as SQL, but should also let you do slices and queries reasonably conveniently. Pandas does not support such "partial" memory-mapping of HDF5 or numpy arrays, as far as I know. If you still want a kind of a "pure-pandas" solution, you can try to work around by "sharding": either storing the columns of your huge table separately (e.g. in separate files or in separate "tables" of a single HDF5 file) and only loading the necessary ones on-demand, or storing the chunks of rows separately. However, you'd then need to implement the logic for loading the necessary chunks, thus reinventing the bicycles already imlpemented in most SQL databases, so perhaps option 1 would still be easier here. 

Questions about definitions are always fun, so let me try to offer another answer here. Firstly, let us model mathematically what you are doing. At the highest level, you are estimating some measure of "reward" $R(s)$ for each board state $s$. You are doing it by interacting with the environment and updating your internal parameters (i.e. the table of $R$ values) towards reinforcing the favorable behaviours. Consequently, by most standard definitions, your algorithm should indeed be categorized as reinforcement learning. To understand what kind of reinforcement learning you are doing and whether it is "good", we should go a bit deeper. One of the key notions in reinforcement learning is the value function $V$ (or its alter-ego, the $Q$ function), which reports the best possible total reward you may expect to gain if you play optimally from a given board state. If you can show that your algorithm is, at least in some sense, estimating $V$, you can claim a certain "goodness" guarantee, and proceed to classify it into any of the known "good" algorithm types (which all essentially either aim to estimate $V$ directly, or to act as if they estimated it implicitly). Note that when we speak about two-player games, there does not necessarily exist a unique $V$ to aim for. For example, assuming the reward 1 for winning, 0 for losing, 0.5 for a draw, and no discounting, $V(\text{empty board})$ is $0.5$ if you are playing against an optimal opponent, but it is probably close to $1$ if your opponent is random. If you play against a human, your $V$ may differ depending on the human. Everyone knows that the first move into the center is the safest one, however I myself have never won with such a move - the game always ends in a draw. I did win a couple of times by making the first move to the corner, because it confuses some opponents and they make a mistake on their first turn. This aside, assuming $V$ to denote the game against an optimal opponent is a reasonable choice. Getting back to your algorithm, the crucial step is the update of $R$, which you did not really specify. Let me assume you are simply accumulating the scores there. If it is the case, we may say that, strictly speaking, you are not doing Q-learning, simply because that's not how the $Q$ function is updated in the classical definition. It still does not mean you are not implicitly estimating the correct $V$, though, and it is rather hard to prove or disprove whether you do it or not eventually. Let me tune your algorithm a bit for clarity. Instead of adding up the final reward to $R(s)$ for each state $s$, which occurred in the game, let us track the average reward ever reached from each state. Obviously, the position where you always win, although rarely reach, should be more valued than a position where you rarely win, even if you often reach it, so we probably won't break the algorithm by this change, and the overall spirit of learning stays the same anyway. After this change, $R(s)$ becomes easy to interpret - it is the average expected reward reachable from position $s$. This average expected reward is not really the value function $V$ we are interested in estimating. Our target $V$ should tell us the best expected reward for each position, after all. Interestingly, though, when your policy is already optimal then your average reward is equal to your optimal reward (because you always do optimal moves anyway), hence it may be the case that even though you are kind-of learning the wrong metric in your algorithm, if the learning process pushes your algorithm ever so slightly towards optimal play, as you gradually improve your policy, the "average expected reward" metric itself slowly becomes "more correct" and eventually you start converging to the correct value function. This is pure handwaving, but it should illustrate the claim about it being hard to prove or disprove whether your algorithm formally learns what it should learn. Maybe it does. In any case, let us, instead of tracking the average reward for each state, change your algorithm to track the best possible reward so far. This means, you'll check all the alternative moves from each position and only update $R(s)$ if your current move resulted in an improved score down the road (in comparison to alternative options you could have taken from this state). Congratulations, now your algorithm is equivalent to the usual Q-learning method (it is the "value iteration" method, to be more precise). Finally, "is it learning or brute force" is a valid question. The word "learning" can be interpreted in at least two different ways. Firstly, learning may denote simplistic memorization. For example, if I discover that the first move to the center is good, I may write this fact down in a table and use this fact later directly. People call such memorization "learning", but this learning is really quite dumb. A second, different meaning often ascribed to "learning" is generalization. It would be the case when, besides simply writing down which moves are good, your algorithm could generalize this information to previously unseen moves. This is the "intelligent" kind of learning. Q-learning, as well as many other RL algorithms are typically formulated in terms of updates to the tables $V$ or $Q$. As such, they are inherently "dumb learning" algorithms, which do not even aim to generalize the state information. True generalization (aka "smart learning") emerges only when you start modeling the state or the policy using something with a built-in generalization ability, such as a neural network. So, to summarize. Yes, your algorithm is reinforcement learning. No, it is not Q-learning, but it becomes that with a minor change. Yes, it is more "brute force" rather than "intelligent learning", but so is the default Q-learning as well. Yes, adding generalization by modeling states with a neural network makes the algorithm "more intelligent".