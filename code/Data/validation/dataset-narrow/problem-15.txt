I normally use stackshare. It doesn't show you usage as reported by the people running the tools but it has a decent community size and seems to be gaining more use rather than less. It lets you compare tool features, community popularity and shows you who else is using those tools in their stack if that matters to you. 

I have seen quite a few of the blogs I follow recommending more and more books over time. I enjoy reading fiction and have no aversion to books but where a blogpost can be updated/rewritten when tech moves on these books which are normally ~£20-30 cannot. Is there a particular quality to DevOps related titles which is missing from the online world or is everyone but me nuts? 

For an this is the most concise syntax available. For most other resources you use the syntax which looks like: 

Vault will distribute multiple unseal keys but there's nothing to stop you from keeping all of the keys in one place for a personal project. The one time I did it I set the keys to 3 and kept them in different physical locations along with online locations. 

Yeah go nuts, you can run whatever you want in the container so long as the host supports virtualisation and can run the docker binary! 

This way, if you call somewhere else you'll have available to you. Always be careful of trying to use an output of a module to create the module, Terraform doesn't handle loops like that well and the errors can be confusing to diagnose. 

Proxying is gonna cause you some issues when someone takes a trip to China and tries to access data in the USA, depending on what your app does they could cause you some $$$ issues when your Chinese POP is sending/receiving all of their traffic each way. Instead I suggest: If your API is authenticated and you're not going to replicate any of the data and you're using AWS then I'd use lambda@edge to do what you need. You can use GEO-DNS to route to a lambda function which handles authentication. This lambda function can also hold a record of where users need to go and issue a redirect to the correct endpoint. This does mean that you'll still need domains like but it will allow to perform the task of being universal. It's down to the client application then to hold on to that URL? At least using redirects instead of proxies should help with network traffic. 

The good news for your question is that all of these tools can be played with on one free tier instance in AWS. Using Docker/K8's, which it seems you want to learn as well, you'll be able to run all of the other tools that you want to learn as containers on that one host which should have enough grunt to allow you to play around. 

If your secret backend is a separate service, which it should be, then with regular backups you should be able to stop vault, replace the backend with a backup and be good to go. I've done this for a file backend but don't know what would happen for other backends so ymmv. 

You're going to have to write a script to do this unless there's a tool to do it that I can't find. If you're working with multiple aws accounts you'll need to be switching between accounts to get the bucket sizes. To get all of the buckets and their total size you can use 'aws s3api'. Using you can get all of the buckets in one account and then you'll be able to list their sizes and try the next account. 

If it doesn't outright fail you can expect to see a cycle warning where you've created a loop in Terraform and it's trying to fulfil your request for before the asg has been created. To get what you need you'll need to have something like the following: module_apps/module_apps.tf 

If you want something quick to get this sorted without much more knowledge I'd recommend elastic beanstalk. It's another AWS application which will handle the load balancer configuration and instance scaling for you. There's no extra cost on top of the load balancer and instances so you can keep using t2 type instances but let elastic beanstalk scale as much as you want to help. Auto scaling isn't instant and in times of spiking traffic will take a short amount of time, minutes normally, to be able to handle a spike but it will be much better than manually scaling the instance size and is really easy to get to grips with. 

Microsoft in collaboration with EdX is running a comprehensive DevOps course. It covers Chef/Puppet for configuration management, Selenium for testing, Docker, Nagios, Loggly etc. Obviously being Microsoft they use Azure, but all this tooling is Open Source and so the skills are very transferrable. All the courses are free, you only pay if you want the Certificate. 

One way to do this would be to write a recipe that stored the stage it was on on each node and just keep running it on both nodes repeatedly until eventually it had a chance to do all operations on all nodes. But this is clumsy and won't scale if there's node C, D, etc. I obv know about notify to sequence dependencies within a single node, but that won't work across multiple nodes. I can't be the only person who needs this, so is there a mechanism or a design pattern/best practice/TTP for this style of activity? 

Let us say I do and within the section I edit the JSON to add . I save this and verify that it has taken effect by issuing , and I see it there. How would I then use this value from within a recipe? I would have expected to just be able to say when running the recipe on but that just returns nothing. The usual Ohai attributes such as obviously work as expected. 

Do a thing on node A capturing the output Do another thing on node B with that output Back to node A now for some operation Now on node B again ... 

The answer is: a bit of both. To satisfy the constraints of "use git" and "manage a vast codebase" Microsoft developed a new filesystem (previously they were using a variant of Perforce called SourceDepot). It's open source but I have no personal experience of using it. Why would you want a monorepo? The most obvious reason is that you can modify an API and all the callers of that API in an atomic commit. Also there are advantages to being able to do a search across the entire codebase... 

DevOps fundamentally is an arbitrage play: it exploits the idea that if the things you are developing and operating are known to be stateless and immutable and ephemeral you can move very quickly, because there are a bunch of things you just don't need to do. And this is true, you can, and you don't. But "traditional" IT is all about managing statefulness and controlling the transitions between states (or "mutation" if you prefer). A database server, a file server. A database server for example: once you have made changes to a schema and data has gone in, you can no longer "roll back" because you probably have nowhere you can put that data that satisfies the constraints of all the stakeholders. If the database server goes down you can't just spin up a new one, it has to be exactly the same to the greatest extent possible, and you need to have a plan for resolving the deltas. DevOps doesn't really address this and even worse, the typical DevOps pitch includes some jabs at "traditional" IT for being slow and obsolete, without any acknowledgement that maybe there were reasons for things to be done the "old fashioned" way other than the practitioners being dinosaurs. What that sounds like to a stakeholder is that DevOps people are willing to take risks with the company's most precious assets - customer records, accounts, inventory control, all the other things that typically reside in the stateful parts of the technology infrastructure. Therefore to work with and bring on board skeptics the very first thing you must do is convince them that you have mitigated all the risks of operating the stateful parts of the infrastructure in a DevOps fashion. I will wager that some variation on this theme is the root cause of TSB's woes at the time of writing. They failed to exercise sufficient care with the stateful parts of their system. 

Where does not exist yet because it is installed by . This will fail at recipe compile time for that reason, but will obviously work at convergence time providing the package installs successfully (and if it doesn't then obviously the recipe has failed anyway). This also fails if the package is installed be a previous recipe in the runlist since they are all compiled together upfront. How do I include things in a Chef recipe that that recipe or runlist installs itself? 

Not impossible, very easy in fact with read-only routing. Clients intending to be read-only specify it in their connect string, so this is per-session rather than per-statement. You can have up to 9 replicas in an availability group. With modern hardware and storage, that goes a long way to being “scalable” - 10’s of thousands of concurrent connections easily. 

First, DevOps is a journey, not a destination. Not every organization approaches similar problems with the same solution; while automation is a component of a DevOps platform, it's not the single defining characteristic. The role and responsibilities of QA professionals haven't really changed; they're still responsible for validating that code is ready for release. What HAS changed is the focus on both automation and smaller releases. Much like operational roles, QA staff should now have the opportunity to focus on adding additional automatic quality checks into the development life cycle. In a shop with a mature CI/CD pipeline, QA becomes more about designing great tests, and letting the software do the actual heavy lifting. As Tensibai commented above, "some QA tests can't be automated" (yet), so there will still need to be some engineering responsibilities, but the goal should be to utilize automation in every aspect of delivery in order to facilitate adding additional quality. 

People resist change, particularly when it's perceived to be outside of their control. Organizations that have been around a while naturally devolve into silos, and organizational structures often reflect that (e.g., database administration teams often manage all of the databases for the org, even though they are often distanced from the business purpose of those databases). DevOps has the potential to disrupt silos, and may cause realignment of org structures. For example, you could have product teams composed of developers and operations people (like DBA's) that focus solely on a single product. For some mid-level managers, that could threaten their sense of job security. The benefit of a DevOps mindset is that it encourages everyone to understand the value stream to the customer, and the relationship of all the roles in delivering that value. IT could mean that the manager of DBA's suddenly has to understand development processes because they're now managing a product team. 

In short, I don't think Adam Smith was wrong, but I do think there are some serious differences between his model of labor division in manufacturing and silos in software development. First, the pin factory example (as far as I know) was merely hypothetical; although most modern manufacturing factories can trace their roots to this division of labor, I am unaware of any studies that have actually tested scientifically this hypothesis. Second, Smith was primarily concerned with manufacturing material goods; there are certain tangible outputs associated with material production that do not have similar analogs in software development. For example, in pin making, physical dimensions are important as a functional requirement; there's no obvious comparison to that in software. This is important because tangible objects can be replicated through exact repetition; software development is never the same problem twice. Developers develop common methods to produce predictable results, but you never code the same problem twice. Any component developed in the stack has intricacies unlike physical components, and those intricacies have interactions beyond tangible measurement (height, weight, length, etc). A pin pointer doesn't care how the wire cutter works, as long as the wire gets cut according to spec. In software development, the boundaries are never quite that clear. Full stack developers are not expected to do all of the work themselves (they're not intended to be a single pin maker), but they are expected to be capable of understanding all of the elements of the stack and how those elements interact. A full stack team should be comprised of T-shaped individuals who specialize in one or more areas, but understand the spectrum (and may be able to step in at some level). Where I think Smith's work holds true in software development is in the area of context switching (or multitasking); if a single developer is responsible for all areas of development, it takes time to shift from responsibility to responsibility. At scale, collaboration between team members with different experiences in the same product team can balance context switching and complex interactions. 

My first thought is "why is the he the only person doing ops, on a dev team, especially when he gets to work with loads of automation?". I think there's an opportunity there to address the lone wolf syndrome; particularly in a dev environment, infrastructure-as-code provides a great framework for sharing the load. Ops people should be experts in determining and defining infrastructure needs for application, but they should also be able to build a platform to let dev roles do as much as they can independently. It sounds like a silo within a team; old habits die hard. A coder may not feel comfortable spinning up and hardening a server, but in a pure devops model, they should have the tools to do so. An ops person in a devops team shouldn't responsible for delivering infrastructure for the app itself, but they should provide some insight into what is needed and some guidance on how the app developers can do it themselves. It's almost a meta-infrastructure model; ops engineers build infrastructure that can build infrastructure on demand when requested by the development team. Consultation, communication, and blending of responsibilities are all crucial to the success of a devops team.