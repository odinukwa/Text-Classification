This will give you a sorted list of all files, starting with the largest at the top, in KB and without crossing onto other mounted drives. If you only have a single, huge / partition, then replace "/foo" with "/". More often than not, you have a small number of large files that are eating up space, such as log files, core files, or crash dumps. It will really pound the server, so either nice it and/or run it when the machine can handle the extra load. 

For a specific inbox, this is easy enough to do with procmail. Here's a link to get you started. The problem is if you want to do this to all incoming emails. That, too, could be done, but it's a little more involved due to trying to determine the correct public key for each recipient. If you want to encrypt with the same key for all messages, then it gets easier. However, as someone else said, if you want to do wholesale encryption, I'd resort to partition-level encryption, which is an entirely different (and much more involved) discussion . More specific requirements and your desired goal would be helpful to provide you with an answer. 

If done right, all you need to do is update the master DNS server, and have the rest as slaves, which will either poll the master or be informed by the master of a pending update. Once you have automatic updates set up, you can have as many slaves as meet your availability needs and then they'll keep up to date, provided the master itself is reachable. 

I've done both. I once worked for a large shop (a university computing center), and transitioned to one day in-office and four days at home. It worked out very well for two years before I moved on. I currently only do remote work as a consultant/contractor and I routinely do work for people I've never met in person. It all depends on what is expected of you. 

The fastest method would be to offline the VM and do a loopback mount of the disk image under linux and copy the files. That way you're avoiding the overhead of both the network and the VM execution. 

For really critical and sensitive changes, I will typically have a text file with the actual commands I'll use with #comments explaining what's going on. That way, I can cut-n-paste them into a terminal quickly. 

Examining DNS records for all related site hostnames will give you a hint a the topology of the site. You may see multiple IP addresses (which don't necessarily mean multiple physical machine, but often will) and the same or different network addresses, which may hint at how they distribute the load for redundancy or speed reasons. Examining the HTTP headers of a site's various services will give you a possible idea of the front-end. Are they using a reverse proxy, such as nginx or Varnish, or are you hitting the web servers directly? Are requests for PHP pages coming from a different server (apache) than those for static HTML and image files (nginx,lighttpd , etc.)? Examining SMTP headers from mails sent from a site will give you more hints. Traceroutes and pings will yield a little more info. Of course, much info gathered will be speculation and guessing on your part, because a well configured site will not give out too much info about its internal architecture. What you'd be doing is, in essence, much what a penetration tester would do for certain info. Just make sure to not cross the line and disrupt the site. 

Try restarting the netlogon service on your DC? (Or probably in your case, just giving it a good reboot wouldn't hurt) 

3.) If you use HP in a large corporate setting and have a vendor agreement or something set up with them, check into HP StorageWorks Storage Mirroring, however it's a bit heavy-handed for two windows folders. 4.) Finally, there's also, (beta warning! Beta warning!) Windows Live Mesh, but it's.. You know, beta. :) Personally, I'd go with the robocopy option. 

Since this is subjective, I thought that I'd just toss out the ones that help me the most. All of `em can be found elsewhere, cribbed from "How to do better at time management!" articles, etc. They're just what help me. They may not necessarily help YOU. i.e. Your Mileage May Vary. Also, while they're not necessarily a direct and succinct answer to the question at hand, I believe that when used, they contribute overall to improving time-management, which has the direct overall effect of helping me free up some time, to complete those important-yet-not-urgent-tasks. 

After reading all the descriptions of what's going on, (greyed out controls, unable to RDP, etc.) I'd bet a soda pop that your RPC service is having great difficulty, (and/or is possibly affecting your server service.) If this is the case, none of the pstools will work, (psexec, pslist, etc.) And if time is of the essence, suggest wrangling a user on-site to perform a hard power down/up. I'd probably also have the user watch for RAID errors, during boot process. Good luck, mate.. I've been where you are, before :) 

I've got a pretty good one. Admittedly, it was prior to my time as a sysadmin, but still tech-related so I figured I'd add it. Back in the day, I was working as a satcom/wideband tech for the USAF. Having recently graduated technical school, I found myself stationed in South Korea. Shortly after arriving on-station an opportunity arose to travel down south with the "big guys" who'd been there for a while and actually work on some real-world,(i.e. `production') equipment. I went down with the crew and as an eager, young tech, was chomping at the bit, quite excited at the prospect of getting my hands on an actual piece of equipment that was passing LIVE military voice and data traffic. To start me off slowly, they handed me a manual, turned to the preventative maintenance section and pointed me in the direction of four racks filled with several large digital multiplexers. The equipment was easy enough, we'd covered the same equipment in tech school. First page of the manual read; "Apply power to the ditigal multiplexer. Turn both rear switches to the ON position and wait for the equipment to power-up, then begin tests." I looked up, and there was already power APPLIED! I was in a quandary for sure. Not knowing how to proceed, I shot my best, `Ummmm.. Kinda lost here' look at the senior. He looked over at me and laughed, "No, no, it's ok. You can ignore that part of the checklist." Then, as he noticed the look on my face, (since we were taught in school to NEVER, EVER ignore any part of a checklist, and it was certain death and destruction if one was to do so) he put a serious look on his face and said, "Ignore ONLY that part! Follow the rest of it, to the letter!" Dutifully, I ran through the multi-step PM instructions, happy as a clam and proud that they were letting such a low-ranking, (albeit smart) tech do this important work. Somewhere between the fifth and sixth preventative maintenance checklist on these huge multiplexers I started noticing an increased level of activity around me. Phones were ringing, people were moving quickly. Quizzical looks were being exchanged. Finally, a group of folks ran up to me, headed by one of the senior techs who had brought me down. "Hey! We're seeing HUGE outages in data traffic, and we've isolated/traced the path back to the racks that you're working on! Are you seeing any weird.." (At that point he was cut off by another one of the troubleshooters who'd made her way around to the first group of multiplexers that I had been performing the PMs on.) "HOLY NUTS! THEY'RE TURNED OFF! HE'S BEEN TURNING THEM OFF!!!!" In short order, I watched as they hurriedly ran through the first step in the manual, "Turn both rear switches to the ON position..." When the senior tech was done, he came over to me and incredulously asked what I was thinking of, by turning the critical pieces of equipment off. Scared out of my wits, I handed him the checklist that I'd been following, swearing that I hadn't deviated at ALL. That I had followed it, `to the letter' as he'd instructed. After a while he laughed and pointed out where the problem lay. In the manual, the FINAL step in the preventative maintenance checklist was: "Record final probe reading, wipe down front panel, removing all dust and particulate, then turn both rear power switches to the OFF position." :) 

Enter your IP address in the "Multi-RBL Check" box here: $URL$ If you get any hits, that may well be your problem. 

The "wget" utility has a --mirror option you can use. I've used it a lot for archiving sites, and it does the job well enough. 

Sounds like billable busywork to me. Aside from the fact that many consumer appliances use the 192.168.x.x address space (which can be exploited, like anything else), I don't feel that really changes the security landscape of a corporate network. Things inside are locked down, or they aren't. Keep your machines/devices on current software/firmware, follow best practices for network security, and you'll be in good shape. 

I don't believe that there is an official bacula mechanism for doing what you want. This is what I did this for a customer with a small office (5 PCs, a server, and 2 servers on the internet). First, I ran the nightly backup jobs, which backed up to the server's disk. Next, I ran a script that would restore to another location on disk (we have plenty of space) and then tar and feather to tape. The reason I did this, as opposed to sweimann's good suggestion of simply backing up the backup files to tape, was that I wanted maximum portability of the final tapes. You can pop a tar'ed tape into any machine (even windows, w/ the right software) and restore the files. The online backups are the true primary, and they go back for six months. The tapes, which were always limited to the most recent full restore, are mainly for CYA disaster recovery in case the office burned down (they were meant to be taken off site). No, it doesn't scale terribly well, and it takes much longer than a direct backup, but it works for some scenarios. 

Beware of the potential for DoS from the respect of the script being able to fill the file system it has write access to. 

If hiding the script's source code is the goal, you may want to look at shc. Note, I've never actually used this until today. It works, at least for my simple one line script /bin/sh test script. Otherwise, you can try using gpg to encrypt the script and give each user the password. Basically, you take your finished shell script, then encrypt it to armored ASCII format: 

I prefer either little shell scripts or shell aliases. In the finest UNIX tradition, I name them as short as possible to minimize typing. For example, I have an alias "ns1" which is the 1-line SSH command needed to login into the (obviously) first name server I maintain. Usually the command name is a short mnemonic for the machine name, and the un-prefixed version of that name defaults to ssh. So "ns1" will ssh me in, and (were it a Windows box) the alias "rns1" might fire off a remote desktop client command to that machine. If I had so many that I couldn't keep track of them all, I'd use the shell script method, and keep a description line on a comment in each one, with a common tag for all scripts. Then I'd write a small script that would print out the name of each file in my custom script directory followed by whatever I grepped from that comment line. Running that one script would then document to the screen each command and what it did. 

I, personally am a big fan of restricting accounts that are logged into the machine as non-admins, but providing them the ability to invoke installations, etc. with the "run as" command. You could create a local admin user on the box, granting them privs and allowing them to use it to install programs? 

First of all, you've got good backups, right? :) Always a good idea to make sure that you're all backed up, just in case a disk decides to not spin up, etc. when you bring everything back online. Nothing like breathing easy after a power outage then realizing that one critical SQL server never came back. :) Second, you're right, gracefully shut down and turn off all servers, etc. If it's not a modern building and/or you don't have a UPS in-line with surge protection, (which it sounds like) or anything like that, it's always better being safe rather than sorry, and I would suggest unplugging equipment. Bringing power back online could result in a power spike. Also, a little FYI; bring your core servers down last, (such as DNS, etc.) You don't want to bring down your DNS servers and find yourself unable to resolve the rest of the servers if you're shutting them off remotely :) When bringing everything back online, bring the network equipment up first, then your core critical servers, then finally your app/file servers. Good luck! 

Yours is a good question.. Here's a link that might help you, and another one that could be of some use. To be completely honest, this is something that I've seen a.) tackled by server admins and IT managers with the best of intentions yet yielding the worst results :) and b.) honestly better left up to HVAC/building engineering/facilities professionals who know the right questions to ask and what to think of. I do, of course realize that you may not have that option though. 

I would agree with RainyRat, (that's an awesome username, BTW dude) and express my own incredulity with your anti-wireshark sentiments. (I also don't know of any security vulnerabilities in it either, and I use it quite a bit.) That being said, I'd love to know if there are any. In the interim, there's always LanHound 

DISCLAIMER: I'm assuming that you've already checked your replication, made sure everything else is working properly, etc. This is intended to be a quick-check option. 

We run firewalls on all local workstations where I work and we view it as a "good thing", insofar as has been mentioned previously, some lunkhead could bring a personal laptop in that's infected with a virus or worm and release it into the `trusted' corporate network. Running a solution like Symantec or McAfee, you can centrally manage the firewall rulesets on all clients to quickly respond to a fast-spreading worm, (then again, if you're a quick thinker on your feet, you can also create pre-set ACL's on interconnecting switches and routers within your LAN to potentially block a malicious code from spreading, once you know how it traverses a network.) That being said, be advised that you MAY POSSIBLY run into application issues where your clients need to communicate with servers on specific ports, etc. To summarize; Firewalls on XP, Vista, etc. workstations = good thing. Do not deploy onto servers, unless you're ready to document and tweak for clients connecting to applications. 

There is a maintained Windows port of Squid. You can get pretty fancy with Squid, as far as ACLs go. I'd give that a try first. 

(That, or simply boot the LiveCD directly.) Just remember that a simple "free space" wipe (several suggested programs here, including two of my own) may leave some recoverable data lying around. Google for a PDF titled "One Big File Is Not Enough" by Garfinkel and Malan for a really interesting 2006 paper on the topic. 

Set up a basic wiki and pgp/gpg-encrypt the document/documents with such info. Set the client(s) up with the FireGPG firefox plugin. You can even mix inline encrypted sections into a plain-text wiki page and it'll take care of decrypting it for you. Just make sure you encrypt documents to yourself (in case the client loses their key), in addition to whoever needs access. 

As said by someone else, never run a web server under the root account. Fortunately, most web servers will drop privs to a non-root account immediately after binding to port 80. Another way to handle this is to run the web server program on a port over 1024 and use a front-end (such as varnish or nginx) to listen to port 80 and forward to the back-end web server. 

Someone could be throttling your HTTP traffic (at the application layer) between your home and server. I'd be curious if any other protocols (besides SSH) have fast transfers while HTTP does not. You should try https, ftp, and maybe even something like rsync, git, or svn. Between A and B and the VPS, you should also try other protocols? From what you've said so far, it doesn't seem like a problem with the http server itself. Is your home connection also in Sweden, or does it cross any national boundaries? 

You could install git and parse the output of "git status" (or maybe the exit codes?) for the directories in question. Git is pretty fast at what it does. Just make sure to commit the changes, so successive calls to "git status" will show changes. Another idea would be to use tripwire or some similar tool. A more brute force approach would be to periodically tar the directories anyway and compare an MD5 of the previous tar. If the directories are large, though, this would not scale too well. 

A couple of other things to try would be to compact the database itself (I assume Access still has this function) and defrag the actual database file on the machine doing the sharing. For defragging single files, I recommend the sysinternals contig command line utility. You could also test for a bad network, by running ping for an extended time (I believe "ping -t" is the correct Windows incantation of the command), and seeing if you're dropping packets or seeing high network latency.