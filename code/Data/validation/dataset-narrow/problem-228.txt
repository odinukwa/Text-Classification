The result of a shortcut must still be valid SQL. As the documentation makes clear, the parameter values must be surrounded by quotes. You could use trace or Profiler to capture the actual code submitted to see exactly what's going wrong. As the additional text seems to be appended to the shortcut rather than embedded in it there's no way to pre-wrap the additional text in quotes. Perhaps a custom snippet would suit your needs better? Though it does take a little work to establish and you're obviously sensitive to gratuitous keystrokes. If you're feeling very brave you could change the user's default schema inside the dynamic SQL and omit the schema from the appended text, removing the need for quotes. 

Size is one consideration. An can hold up to -2,147,483,648 in four bytes. A will need 11 bytes to hold the same value. There are built-in functions to manipulate the various data types. and are two examples. This will not be possible with date-stored-as-text. Constantly ing back and forth will not make for efficient processing, or legible code. Automatic validation is another foregone benefit with the all-text approach. You may think that a column contains dates but there will be nothing to stop someone entering the value '2014-13-97'. Sorting is unlikely to give the intended result with columns which are "really" numbers. For example, if a column contained integers '1' through '100', and the query sorted by this column, one would expect the result to be 

Ask yourself if the thing under consideration continues to have an existence if all the other things in your model go away. If so it's an entity, if not it's an attribute. Does the thing you are considering have a way to distinguish it from all other items in its class i.e. a primary key? Then it's an entity. The answer you come to may depend on the context in which you are working. For one computer application a particular value may be treated as an attribute. In another it may be an entity. This is not a contradiction; it is a natural outcome of simplifying the real world into a model and in doing so selecting the important parts and excluding the others. If you intend to implement your model you should recall that first normal form requires that each attribute hold only one value. 

Design a shared-everything system. This can be deployed in a shared-nothing way i.e. put each tenant in their own database, should that prove desirable. The extra development effort to type the additional predicate is very small and easy to do up front. The reverse - where you re-factor a shared-nothing system to become multi-tenant - will be a significant project and difficult to get right retrospectively. You will need some scripts or ETL packages to load / unload one tenant from a database. This will help with size & load balancing, privacy concerns and such like. The schema-based approach has all the disadvantages of single-database and none of the advantages of multi-tenant. You will end up with a lot of dynamic code, stored procedures will be difficult to get right and statement recompiles will be ubiquitous, increasing response times. Invariably some of your tenants will be much larger than others. My experience is that you will end up with a handful of tenants representing 30%-70% of the total data volume. These will each have their own DB and, perhaps, infrastructure. They will consume 60% of your support and maintenance time. The others will all fit into one or two databases. I've run development instances with roughly 1,000 databases online sharing about 8GB of memory. This worked well enough for dev. I'd never try this in production as the risk of problems snowballing would be too high. If you're feeling brave you may like to experiment with auto-close on one or more database. This will save some resources on the instance. This is definitely not recommended. I mention it merely as an observation. 

There are a couple of patterns for splitting a single logical entity across several physical tables. Vertical partitioning puts some of the columns in one table and some in another. There can be several such additional tables, if required. All these tables share the same primary key. Columns that are used together are stored together so one page fetch read all the required values. The advantage is there are more rows per page so scans and aggregates require fewer IOs. Sometimes it can be tricky deciding which columns should go together. Every INSERT becomes proportionately more expensive as the number of partitions increases. Horizontal partitioning splits the data by key range. For example, all users with a surname beginning A through M go into table User_1 and N through Z go into User_2. The application figures out which sub-table to use at run-time, usually algorithmically or through a look-up, though most DBMS these days will offer this as a build-in feature, implemented through DDL, transparent to the application. If you have hot spots in data writes this can spread the pain out. The optimiser can eliminate whole partitions from range scans, improving response time. Loading and removing whole partitions can be very fast metadata operations. Sharding is where key ranges are moved, not just to a different table, but to a whole different instance of the DBMS. The application decides which instance to connect to, depending on key range. This is a scale-out technique. Some DBMS support this as a feature, such as Galera for MariaDB. The obvious cost is in additional hardware to run the other instances, duplication of reference data across all nodes to maintain RI and application complexity. The techniques may be combined, of course, so a table could be both horizontally and vertically partitioned. General advantages of splitting: 

You should have a table for each class of thing you want to offer a discount on. From your question that would be a table each for products, customers and stores. Most likely you will have these tables in your design already. Each will have a column of an appropriate type to hold the discount. You don't say what you will offer but a percentage discount could be a since it can only be in the range 0 to 100 or a if you prefer to handle your math that way, and so forth if you'll offer fixed-amount discounts ("$10 off your purchase over $120. This month only!"). Name your column to match your rule e.g. or . If it is important to track changes to discounts over time separate tables keyed by date will be required e.g. . Be sure to work out with your business how these rules interact. If I'm a 5% customer buying a 6% product do I get an 11% discount or a (0.95 * 0.94) price? For a $100 item will I be charged $89.00 or $89.30? Can I even get more than one discount on a single purchase? Which? If only a few combinations receive the discount you could have an intersection table keyed by, say, product and customer as you mention in your question. This works up to a couple of levels then gets very unwieldy. Then a general decision table will be your best bet. This will have a column for each of your decision variables (product, customer and store in your examples) and another for precedence/ sort order. You manually assign the precedence to match your rules and at run time pick the highest precedence which matches this sale's parameters. An example: 

A table with several hundred columns is within the capabilities of any DBMS. It may be strange but it is not nesessarily wrong. 

The partioning column must be in the clustering index. Therefore, if the value of IsDeleted changes the row has to move from one bit of disk to another. If your intention in using the flag was to defer the IO cost of the delete by performing an UPDATE instead of a DELETE this solution is counterproductive. With it you will be perfoming both a DELETE (from the row's current location) and and INSERT to its new location. If the intention was to isolate archive data then you are onto a winner. I would suggest you use filtered indexes on the active partion of your table, too. 

Create a filtered index on with the predicate . The optimiser is likely to favour this as long as the query conditions exactly match the index definition. It may help to use a small tinyint for the status instead of a longer character string. There will be more rows per data page and less IO. 

Will this index end up lopsided? No. The way the BTree algorithm works keeps it balanced. (One interpretation of the "B" in BTree is for "balanced".) As leaf pages fill they cause parent pages to split, with half the rows going to each new parent. Further rows cause further splits, which cascade up the tree until the root has to split, at which point a new root page is created. At every stage all paths from root to leaf have the same length and hence "balanced". Should it be reindexed regularly? Yes. Each DBMS has its own idiosynchracies but likely regular re-build of indexes will be recommended. If for no other reason, the statistics will get out of date eventually which will result in less efficient query plans. Should I use a low, normal, or high fill factor? As all writes will be on the right-most pages leaving free space anywhere else will be a waste. There will never be any writes to use up that free space. Use a fill factor of 100%. Should we try to find something else to put into the index Indexes aren't (generally speaking) created for entertainment purposes. They're there to improve query performance or support constraint enforcement. If your workload would benefit from a index then create that. If not, don't. Indexes require extra work during writes and consume space and maintenance time. Create the ones you need and no more. And no less. Keep in mind filtered indexes and included columns. Bear in mind that optimiser math may mean your indexes aren't used anyway. Will there be issues with hotspots or index page contention .. What can I do to mitigate that? Well, yes, in theory. First test and prove you'll have a problem meeting your expectied transaction-per-second count. If you can't, are you sure it's this index that's the limiting factor? If it is, in-memory tables and snapshot isolation may implement concurrency mechanisms different to the "base" system's, depending on DBMS. There may be alternative storage engines whose characteristics differ. Try those to see if the pain receeds. You can always post a follow-up question here with quantatative observations! I understand that for an ascending surrogate key.. Surrogate keys are made-up values that replace human-intelligible natural keys. They're used for reasons releated to DBMS implementation peculiarities and are not inherent to the relational model. Your date is not a surrogate key - it is business data. a reverse key index might be better. Hmmm .. OK, so instead of writing all today's rows to key value 20161021 you'd write them to .. 12016102? How does that help? Hashing won't help either: to use the hashed values for lookup each date would have to produce the same hash so there's still a hotspot. Or is all of this a total non-issue, and indices of this kind are fine and I should stop worrying about it? Probably. The vast majority of indexes on most systems are like this. By-and-large they work just fine and the DBMS is able to process significant workloads on modest hardware. Indexing is one of those things you can (and should) tweak ad nauseam after go-live. Concentrate on delivering a normalised, feature-rich, debugged system. Tune it afterwards.