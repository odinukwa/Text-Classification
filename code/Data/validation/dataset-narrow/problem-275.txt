Next step is to create a job that will check (before tran log backup job): Step 1. Whether there is a database with value of 0 (meaning not backed up) Step 2. Back it up taking values from the table (DB name,BackupName) Step 3. Update column to 1. Optional: Send an email informing DBAs about newly backed up database. 

Depending on transaction level, those queries will block writes. Serializable & Repetable Read transaction isolation level will hold S locks (for the whole duration of the transaction), which are incompatible with X locks that are required for writes(inserts/updates) . And yes it makes no difference, if you check execution plan you will see that they are exactly the same. So in order to prevent locking and blocking implementing different kind of objects wont give you no results, but performance boost(if used stored proc). Instead you should change isolation level 

When you use the nodes() method with the XML column from the table, you must use the APPLY operator. Example: 

See if you can find SOS_SCHEDULER_YIELD & CXPACKET waits. If SOS_SCHEDULER_YIELD waits are high you might have some very CPU extensive queries, which you should pay attention to. This: 

Havent really tried to debug the script, but i can see just from the parameters what it essentially does. There are many scripts online that you can use to do just that The one that i use pretty often is this: 

As specified ,returns table therefore SQL server checks the underlying structure of the table because it needs to know what kind of table is begin returned. How many columns it has,if any of the columns are without names(in aggregate cases) and inform you about it. You can test this by running the query above without referencing the column name( 'as user name' ) in case username table does not have a specified column name. Note: I had to tweak this two functions a little bit, to make them work. Hope this clarifies what is going under the hood for you 

So it looks like the time increased because there has to be a comparison to confirm that the value itself has not been modified, which in the case of a 1G longtext takes time (because it is split across many pages). But the modification itself does not seem to churn through the redo log. I suspect that if values are regular columns that are in-page the comparison adds only a little overhead. And assuming the same optimization applies, these are no-ops when it comes to the update. Longer Answer I actually think the ORM should not eliminate columns which have been modified (but not changed), as this optimization has strange side-effects. Consider the following in pseudo code: 

The simple answer is: it is not possible to guess what is the source of contention. The more detailed: MySQL 5.6+ can instrument a break-down of query execution time, so that you can see if performance is stalled waiting on IO, locks, etc. The diagnostic feature is called . The easiest way to start using it, is to download MySQL Workbench 6.1 and chose "Performance Reports" (under Performance). 

The file is usually used for client options (for example changing the default port or host). will likely start as a different user and will not read your file. It is recommended to set these options in instead. 

The minimal package was designed for use by the official docker images for MySQL. It cuts out some of the non-essential pieces of MySQL, but is otherwise the same product. 

When you are all set and done, you can transfer those views from ViewSchema to dbo or some other more meaningful schema with command: 

There are couple ways to achieve what you are after. I will show you two, but there are other ways as well. The first one is signing a procedure, and allowing other users to use it with specifying only. First of all you need a master key, which i assume you already have, and you can check it using DMV, and looking for record that has '##' as a prefix. If not simply create a DB master key 

After delete trigger is executed after the record has been removed from the table. Joining a table Emp will yield no result because record with that ID does not exist in that table. Also note that inserted table will always be empty in after delete trigger. 

Like i said there are other ways, including roles but these two could get you a job done. Just remember you cannot track a user that executed procedure by specifying 'execute as'(login is possible tho). Also creating a certificate/database keys can be a headache when you`re migrating DB, or simply restoring it somewhere else. 

There is a workaround for that issue. It will require an additional table,a trigger, and an agent job to be ran before transaction log backup job. So if you think its worth going through that than bare with me: You should create a table where you should add Database name,whether database was updated or not (you will need this for an agent), and some other info needed for backups such as : 

It used to upset me how features were decided at MySQL... How was it decided that partitioning was a critical feature for 5.1, but backup totally missed the radar? There seemed to be a bunch of low hanging fruit (years old bugs) that were not being addressed, and I was always cynical that unless it could check off a box on a features grid, it would never be handled. There was a bit of talk, but no indication it was any better under Sun's management. However, now that Oracle is in control, several years old bugs are being addressed, performance has become a feature, and I actually find really compelling reasons to upgrade to 5.5 and 5.6. I feel awkward having to defend one of the world's biggest software companies, but they're really not getting enough praise. Instead everyone is making claims they are somehow screwing the project. Most of the projects they 'screwed' made no commercial sense to them... however they make a non trivial amount of money on OEM licenses and subscriptions/professional services for MySQL. 

Since the granularity of caching is done at a page level (both in InnoDB and de-facto in MyISAM due to filesystem block), having large 42-column rows means that you will fit fewer rows per page on average and there is no split where the hot sub-set of columns are kept in memory while the inactive ones can not. This results in a sort-of cache dilution, where you may require more memory than if you were to normalize the schema and split into a few different tables. (Note: InnoDB does overflow large text/varchar/blob columns to separate pages. I agree with Rick's comment that InnoDB is the way to go.) 

Once this is done, the next step is to actually sign this procedure, so whoever has rights to 'execute' the procedure will inherit rights from the certificate user. 

Just to expand on previous answer that was posted. Just like George said, shrinking in general is not something that should be part of maintenance job. Rather log size miscalculations, some unexpected scenarios (such as uncommitted transaction, large and intensive DMLs etc etc) or insufficient amount of log backups can cause excessive log growths. If your log size does not seem large enough, you should monitor it during busy hours, or during night time ETLs(If you have some) to see the average log size and change it if needed. Also make sure to set log size auto growth in specific MB size, which will mostly depend on your initial log size. More info could be found here Database log VLFs Now to answer your questions: 1) No in general. But in scenarios i mentioned above, it could be helpful. Which is the only time when it should be used - out of ordinary situations. 2) If you determined you want to shrink your log file, you should be aware that the log file is made out of VLFs(Virtual log files), which are gradually filled one at the time. Once all of them are filled,if you reached your log maximum size, log auto growth will happen and depending on size will grow in 4/8/16 VLFs. Once the log is backed up, these VLFs will become empty again (you will always have some in use, so it can track current LSN). To keep it short, once you backup you log, you can check used and unused VLFs using command 

I find talking about storage engines using cores can be misleading for beginners. Provided that a program is sufficiently multi-threaded, the operating system will schedule it across as many cores as possible. The specific problem that limits cpu-scaling is when internal locking code (mutexes) have contention and block threads from running concurrently. All storage engines will require mutexes, but certainly there are some hot ones in MyISAM. If we ignore mutex contention for a second and get back to your main question: how important is it to have many cores? - I like having lots of cores for workloads that serve user facing requests. Having many can reduce variance between query times. Think of this as like lining up at the super market with 12 aisles open versus just 2. Update: I wrote a blog post on why vertical scalability (multi-cores) is important. 

You most likely have a file in your home directory, which is specifying a password. The clue here is that when you did not specify a password (), the error message is still saying that you did. 

You've got to put it in context - InnoDB only verifies the checksums when it reads a page from the block storage device, and updates it before flushing it back to the storage device. While in memory, the checksum is not maintained. For many years, an IO to a disk has taken something on the order of 5-10ms (1ms = 1/1000th of a second). Computing a checksum probably takes somewhere around 50us (1us = 1/1000000th of a second). I don't have the actual data what it is in InnoDB's case, but if you Google "Numbers everyone should know", you'll hopefully agree I'm correct within an order of magnitude. Enter an era now where we have things like Fusion-io flash devices, which on paper have a ~20-30us access time, and you can see that reevaluating the checksum makes sense. My general advice: Don't ruin your backwards compatibility with MySQL releases unless you really need it. Most people do not need it yet. 

As Kris mentioned, you could create a stored procedure and run it as a job Here is a sample script that will accept Table name and Threshold(in KB) and send an email if table size exceeds its threshold 

You can also use extended events and capture high resource consuming queries, with included user names 

Green rectangles are tables, red eclipses are attributes, blue triangles are tables that connect tables and a relationship descriptors so you could understand their relationship. Do not name triangle like tables as referred on the diagram, such as 'contains' , give them meaningful names. Purple eclipses are additional attributes of tables that connect other tables Numbers and letters next to triangles are telling you what kind of relationship it is Whether is : One to many - example Table Account has 1 and table Character has N which means one account can have many characters , but character can have only 1 account Many to many - example Character can "Ran into" many creeps and many creeps can ran into many characters What does it means? It means that depending on a relationship type, you will create foreign keys and constrains accordingly. If you are still in doubt what the foreign keys are, i suggest you to read some books/articles before you actually start designing database. Databases are base for any project, and it is not something to be taken lightly 

I used examples on value and node, because you provided the code with those two functions only, if you want to read more about it please visit this Hope this simple examples give you an idea of how to query xml types 

requires setting triggers on a table. MySQL (prior to version 5.7) only allows one trigger per event (before insert, after insert etc.) This error is informing you know that a trigger is already defined, and is prevented from running. tl;dr: You need to remove your triggers so the tool can run. With MySQL 5.7+ this will not be required. 

What you are doing is called denormalization. The rule of thumb is "normalize till it hurts, denormalize till it works." Denormalization should follow the access pattern of your queries. If you have a common query that goes from joining on 10 tables to just a few with just a small bit of duplication, go for it! 

Fan in (multi-source replication) will be supported from MySQL 5.7. A labs release is available here: $URL$ 

There's really risks associated with both approaches: Option a) Index from the start, but not realize you have created a number of indexes which are never used. These add some overhead (most noticeably to queries that modify data, but also with optimization of SELECT statements trying to identify the best index). You will need to discipline yourself to identify indexes no longer being used and try and remove them (PostgreSQL can do this; unfortunately MySQL by comparison is very weak at this out of the box.) Option b) Don't add indexes until people start complaining, or your diagnostic tools trigger that certain queries are slow and could be improved. The risk that you introduce is that you don't have a big enough time window between when you notice you need the index, and when you have to add it. PostgreSQL does support building indexes , which does reduce some of the stress from this sudden-index-add-requirement, but there are some caveats noted in the manual.