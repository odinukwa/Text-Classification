If the reference to 7 chunks of information for humans is a reference to the 'Magical Number Seven' Paper, which is the human working memory, then: Consider the high level operation of DNC at a timestep. An input comes in, and is processed by an LSTM to generate a 'Interface vector' which interacts with memory with Write, Erase and Read operations. The operations are then executed, and a final layer produces an output based on the retrieved information and the input. Where is the analogy to human 'working memory'? 1). If you consider the working memory is what is retrieved on read, then there is a hyperparameter for number of memory reads. This applies generally to Write and Erase as well as those are all hyperparameters which determine the size of the Interface Vector. In the paper they used a write size of 1 in all cases for purposes of controlling the experiments. Note that the 'size' of what is read/written in a single operation is a separate parameter, which just depends on how the problem is encoded (see the methods section of the paper). 2). If you consider working memory as external memory, then this is another hyperparameter. I think it is a conceptual mistake to view the external memory as short term memory, as the addition of this and all the gating was specifically developed to store information long term and then recall when needed. 3). There is a part of the DNC's operation which queries a link matrix (storing what was written to long term memory and when) during the read phase. This can be seen as being related to human working memory, as numerous behavioral economics studies showed that what we recall and how we process it is effected by what we have just seen. This is the only part in the paper where the computation is O(N^2) w.r.t to the size of the memory, (though they approximate with an O(NlogN) method). This portion of retrieval/bias of what is retrieved to most recently accessed information can then be said to have a limit, since for large memories it will not scale with rest of network, but this is still not constant. 4). Probably the best candidate for the working memory analogy IMO is the hidden state of the controller LSTM, since this part of the algorithm is responsible for storing what is needed over time for retrieval, and on its own is used for storing a state over time. The reason that this is short term memory in this analogy is just that the external memory is an explicit external memory, and this is the only other place where a state is maintained. So in short no, there is no Constant, as just about every conceivable analogy to 'working memory' and the entire architecture is controlled by parameters. It is worth checking out the code and paper for more details on exactly how the parameters are set for different experiments. Let me know if this makes sense. 

3) So instead of this table with the colored class labeled, you just draw a circle ("ball") for each class label (=element) 

Now you throw all the balls in a bag. There you go! Your elements of a set are represented as balls in a bag. If you open the bag and look into it, you might see 

You have a sum over all class labels so n = 4. So you assign a class label to each of the i's. E.g. For i = 1 being cinema: - (5/5 * log(3/3) = - (1 * log(1)) = - (1*0) = 0 For i = 2 being tennis: - (0/5 * log(0/3) = - (0 * log(0)) = 0 For i = 3 being stay in: - (0/5 * log(0/3) = - (0 * log(0)) = 0 For i = 4 being shopping: - (0/5 * log(0/3) = - (0 * log(0)) = 0 So the sum of all four is 0. 0 times the weight 0.5 equals 0. Set 2 also refers to five rows, having the decision categories = 1*cinema, 2*tennis, 1*stay in, 1*shopping So the weight is also 5 rows out of 10 rows = 5/10 = 0.5. Applying the formula: For i = 1 being cinema: - (1/5 * log(1/5) = - (0.2 * -2,32) = 0.46 For i = 2 being tennis: - (2/5 * log(2/5) = - (0.4 * -1,32) = 0.53 For i = 3 being stay in: - (1/5 * log(1/5) = - (0.2 * -2,32) = 0.46 For i = 4 being shopping: - (1/5 * log(1/5) = - (0.2 * -2,32) = 0.46 So the sum of all four is 1.92. 1.92 times the weight 0.5 = 0.96. Thus, the Gain for this attribute is: Initial entropy - weighted entropies for all attribute categories: 1.57 - 0 - 0.96 = 0.61. For the other attributes you do the same calculation. 

Most of us want to build a recommendation engine as accurate as possible, however, an experienced chief data scientist believes a practical machine learning algorithm should randomly shuffle the results, therefore non-optimal results. This is known as "Results Dithering". Slide 15 at: $URL$ While I understand how to do it (adding a small fixed Gaussian noise), but why would we do it to make the performance worse. 

Boosting trees is about building multiple decision trees. Decision tree doesn't require feature normalization, that's because the model only needs the absolute values for branching. Wikipedia for decision tree: 

I think your question is off-topic, but I'll still try to answer because it might help you. Getting a job in data science isn't something everybody can do, it's a competitive field. Well done, for landing yourself in data science. However, data science is a big area, so you'll need to think what you want to do and plan ahead. Do you want to get yourself into machine learning? Do you want to get yourself into statistics? Right now, you're more like a software engineer in the field, still not quite being a data scientist. Do you want to become a real data scientist? Or are you happy being a software engineer in the field? If you're planning to become a data scientist, you should train yourself hard in statistics, machine learning etc. Being a Hadoop user is simply not enough to step yourself fully into the field. I give you my own experience. I was a software engineer myself. It wasn't easy to get a job in data science. I had to do a postgraduate degree in mathematics. Trained myself as much as possible. In the end, I landed myself a job as a bioinformatician. Recommendation Don't leave just because nobody mentors you. Learn as much as possible and prepare yourself. Once you're comfortable, move on. 

B. On doing so, is it correct that it is the right thing to do to cite the paper (lets say I am required to hand over competition code). Now lets say the author published some code to go along with the paper under an MIT/other open source license on github, and I mess around with it to fit my needs. What if the paper was by a company (like facebook or google, but does not cite any patent or limitation on use of the code/method) Do the same rules apply or are there other considerations? I guess I have not found a good blog covering this and any surrounding issues I may not be aware of as self-taught professional human running into this for the first time. 

Never used this, but $URL$ has something close... pretrained models on jewelry. You can probably extend those models. The new google dataset may also have labeled versions of the stuff you need. I cannot find the link I saw origionally, but it has bounding boxes for 20K classes down to 'teacup' 'teacup handle', 'tea', so I would not write google off before testing either. Since this is DataScience SE, I will give the more DS answer as well. The thing you are looking for sounds like a multi-category classifier. You can probably find good resources and pretrained models here, and on the end, sklearn has apis for common classifiers. The general idea is run your image through a Convnet, minus the classification part. You then have 1K-8K 'features' which a second algorithm set of classifiers, [decision forest]($URL$ ), logical compisitional model, embedding, etc... gives a score for each binary attribute. 

You'd only use GPU for training because deep learning requires massive calculation to arrive at an optimal solution. However, you don't need GPU machines for deployment. Let's take Apple's new iPhone X as an example. The new iPhone X has an advanced machine learning algorithm for facical detection. Apple employees must have a cluster of machines for training and validation. But your iPhone X doesn't need a GPU for just running the model. 

The statistics here are obviously very good, in fact too good for any practical data set. Your model is almost perfect... Unfortunately, it's practically useless and I'll explain. In machine learning, if you see something like this you know you are in trouble. That can happen if there are problems with your data workflow. For example, you might have removed all outliers that you shouldn't, or you actually used a subset of your training data for the test set. It's fine if you're just toying SVM, but you'll never encounter something like this in real life. 

just balls of the same color --> one pure color. The entropy says, it's not impure at all, I give it a zero! entropy = 0 the same amount of balls in different colors --> that doesn't look pure at all, it's impure! The entropy assigns the highest possible value to this set (bag). The highest possible entropy value depends on the number of class labels. If you have just two, the maximum entropy is 1. In your case, you have 4 different class labels, so the maximum entropy would be e.g. if you had 12 balls and 3 balls of each color. The maximum entropy of a 4 class set is 2. none of the above. Then your entropy is between the two values. If one color is dominant then the entropy will be close to 0, if the colors are very mixed up, then it is close to the maximum (2 in your case). 

EDIT: How many respondent you need is hard to tell. Apple probably needs better data quality than you. If you'd like to do it statistically, you may calculate the minimum sample size and it's related to statistical power. 

You have to try to understand those outliers to come with up a decision. 2:) You can just explain like "the probability of the difference in means is (or isn't) significant". 3:) You should draw a box-plot for each group. 

Note that is the number of moves searched for the move and it's in the denominator. The more likely our first move is searched, the smaller the function is. Thus, the probability for selecting our second move improves because AlphaGo actually picks a move by this equation: 

No, you don't need to understand measure theory and real analysis to do machine learning in data science. However, it'd be hard to for you to read academic papers (eg: kernel methods) if you don't have the knowledge. Unless you want to be a mathematician or wish to pursue a Phd, you really don't need to know too much about the theories. In fact, most of the engineers working in the field I know don't understand statistics at all. They simply use some framework, get some good results, then get paid. 

An alternative approach which is probably not as advanced yet for this is some combination of a deep learning system with and modification of output to provide a 'similarity likelyhood.' This paper Code2vec, and this one have a neural network training on code to encode it as a vector in a scalable way in order to predict its properties, and then perform some task depending on the paper. It could be conceivable that if you have 'labeled' code that fits the criteria that you want to train on, that a NN could be something to look into (atleast from a distance). The Code2vec intro seems to fit the problem for more complex examples, in terms of being able to predict outputs and compare code in a way that is similar to language (as there are plenty of NN architectures which do that in terms of summarization and sentiment analysis). 

Going off the discussion in the @Andres Espinosa's answer, here is an option that might work for this without any extra gating voodoo. The Nested LSTMs paper, Implementation here. The nested LSTM idea is to build in the idea of a temporal abstraction hierarchy - which in your case could possible capture the current pattern at the first level, and the need to switch between patterns at a deeper level. In addition there is the untouched idea about why your LSTM is not working in the first place. Before doing extra work, maybe it is worth trying visualizing the LSTM activations. In theory, if the LSTM has learned different phases in your input, there would be a different distribution of activations depending on the 'phase' or more significant changes in the hidden state between phases vs within the phase, since it would pay attention to different things depending on the phase, it could be a matter of just more training time. If it is consistently not picking up something that can be visually diagnosed, then it would suggest trying one of the other ideas. 

Let's be precise. "Distance" has lots of meanings in data science, I think you're talking about Euclidean distance. The Gaussian kernel is a non-linear function of Euclidean distance. 

has the details. There're also lots of papers on Google. Alternatively, you can use the cross-validated likelihood as a performance measure, although this can be slow, since it requires fitting each model N times, where N is the number of CV holds. Slide 17 in 

2 hours for a batch is too long even with a CPU. How many layers you have? How many neutrons you have? Have you tried a single layer? Try to reduce the complexity of your model. You might want to train a softmax regression, just to get an lower bound for your computation power. If you find your machine struggle even with softmax regression, you'll need to upgrade your hardware. 

To understand why, we should try to understand what we're doing here. Let's start with the first p-value: , what does that mean? This is the p-value under the null hypothesis that the linear model and the second polynomial model are statistically identical. We say that they are the same if the extra coefficient in the polynomial is statistically zero. This is exactly what the p-value is telling you. It reads like this: . This is a very small probability, so you can conclude that they are not identical. This means, the second-order polynomial is a better model than the linear model with a significance level of 5%. (PS: a more correct statistical interpretation of the p-value relates to false-positive rejection, but let's not go into that deep) Now, using the same logic, you can conclude the third-order polynomial fits better than the second-order polynomial. Next, the p-value comparing the third and fourth order polynomial is about 0.05. You can reject it or you don't want to reject it, this is up to you. But If was you, I'd simply fail to reject it because it's larger than 0.05. Finally, the final p-value is about which is too high. This means although the fifth order polynomial fits better than the fourth order, the loss in your RSS is insufficient to justify the loss of the degree of freedom. Therefore, we say . Conclusion: It's "bad" to have a large p-value because you really want to reject a null hypothesis. Statistically, we do it to control the false positive rate. PS: In your example, R uses the F-test to compare the two models.