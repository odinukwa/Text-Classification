I think the OP was confusing about AlphaGo with alpha-beta. In alpha-beta, you'd indeed use the policy network for helping with pruning, but not here. Again, there is no pruning as the algorithm relies on Monte-Carlo tree search (MCTS). Anyone who thinks my answer is too long might skip to the summary section, where I state why the two networks are not redundant. In the following example, I'll do some simplification to make my ideas easier to understand. Example: Imagine you have a position where there are two legal moves. The first move is a dead-lost for you, however, the second move gives you a winning advantage. 

Now, I have another multi-layer convolutional network to check each square in the board. The stride length is the dimension of the image divided by 8 (since there're eight squares in each dimension). The patch size is the same as the stride length. My pipework worked as I was able to combine two different classifiers. I personally prefer to train two classifiers, as it'd be easier to train and verify than trying to put everything into a single model. 

Policy network Let's assume Google gives you two policy networks. The probabilities generated for our position is: 

Locally weighted linear regression is a non-parametric method for fitting data points. What does that mean? 

Likelihood function is the product of probability distribution function, assuming each observation is independent. However, we usually work on the logarithm scale, because the PDF terms are now addictive. If you don't understand what I've said, just remember the higher the value it is, the more likely your model fits the model. Google if you're interested. Obviously, your input data is bad. You should give your model a proper data set. While I don't have your data set, we can take a look at the likelihood function for linear regression: You will get infinity if the likelihood function is zero or undefined (that's because log(0) is invalid). Look at the equation, most likely your sample standard deviation is zero. If it's zero, the last term will be undefined. Have you given a data set that you copied and pasted the same data over rows? Boosted trees should also be undefined if your sample deviation is zero. However, decision tree is estimated based on impurity and won't crash here. Summary: please check and double check your data. EDIT I think you just had a bug. Linear regression will always give you something here. Have you fitted the models in R with the same dataset? â€“ 

What's your expected allele frequency range? If you have something like close to 0.5 (ie: maximum heterozygosity), you'll probably be safe with a smaller sample size. However, if you're trying to detect a rare disease, something like <0.1 as in the diagram, you'll need to have more samples and most likely more features. Empirically, we generally use spike-in standards to measure the sensitivity. Once you draw the sensitivity against training size, you'll know at what training size to use achieve whatever sensitivity you're looking for. A typical tool would be ROC. 

Evaluation network Let's assume the evaluation network Google gives you is perfect. It can evaluate any leaf position in our example perfectly. We won't change our value network in the example. To simplify our example, let's assume our value network gives: 

Do you see the point? You'll need to consider the correlation with all other variables in your data set, not just 1-to-1 pairwise comparison. VIF addresses the issue. 

Please double-check if there's the only data you have got, because all you have is a single predictor . If this is indeed your only data source, then you only have a single predictor, and your independent variable is continuous. Now, you should plot vs and fit a single linear regression. Does the fitting look good? Only you can tell because we don't have the full data-set. If it's not a good fit, look at the plot and ask yourself does this look like a curve? If so, you might want to fit a spline curve or something like that. You should also check the autocorrelation. This makes sense because your data look like a time series (you'll need to check it yourself). If this is the case, you might want to consider MA and ARCH model. It's not possible for us to give you accurate advice because we don't know your data. 

Since chess board is always a square. I use square-detection available in OpenCV to give me a list of candidates. I would throw away any square that is too small, too large or not divisible by 64 (since there are 64 squares). Next, I'd crop the image to something like this: 

Short answer is: as much as possible. The more data you have, the more likely you can make correct inference. In the worse case, you could simply discard anything that you don't like. It's like money, the more the better - you always have an option to spend or use less. Statistics could be doggy without sufficient sample size, your statistical power would be affected and any model you have could be biased. Having said that, you don't want everything from your user. Use your common sense, and ask yourself what exactly you want to do. Based only on the data in your question, I'd say: (I could be wrong because I don't understand where you got the data) 

Now, if your data points have the x-values from 1 to 100: [1,2,3 ... 98, 99, 100]. The algorithm would fit a linear model for 1,2,3...,98,99,100. That means, you'll have 100 regression models. Again, when we fit each of the model, we can't just use all the data points in the sample. For each of the model, we find the closest points and use that for fitting. For example, if the algorithm wants to fit for , it will put higher weight on [48,49,50,51,52] and less weight on [45,46,47,53,54,55]. When it tries to fit for , the points [92,93,95,96,97] will have higher weight than any other data points. Do you see the pattern? The points closer the where you want to fit have higher weight and the points further have lower weight (zero if too far). That's what the weight function is for. To answer your question directly: 

Your question could be closed for too broad, but let's give a try. You want the enrolment probability, this sounds like a logistic regression for me. Neither categorical nor continuous data type should present a problem. You may model the additional information, applications who don't have one simply be assigned a category. You can include the category in your model. I recommend you read the book , I think it has a section on credit-card applications, which is close to what you are doing. You should try to learn from it. 

When the number of queries is small, it looks like your classifier did better simply because it was a simpler data set. Again, overfitting can also be an issue. 

1. The simplest and most common way is to use AIC or BIC. You would pick a model with the minimum AIC/BIC value. AIC/BIC work well here because you have a likelihood function. Bayesian model selection is another possibility. It's more advances than AIC or BIC, but you get chance to add your own prior distribution. Section 5.3 in 

So AlphaGo is much more likely to pick the losing move to simulate (in our very first simulation). In our first simulation, we will also use the value network to get a score for the simulation. In the paper, it's: 

I agree @Emre. To me, they are all important to machine learning. Of course, it depends what exactly you want to do in machine learning. For instance, if you're just doing image recognition you don't need to understand time series. Another example, "Decision and Game Theory" is absolutely critical for machine learning in board games like chess (e.g reinforcement self-learning). Everything you list here applies to machine learning - machine learning is really just statistical algorithms for learning an unknown function. Why don't you tell us what you want to do, and we tell you what you should learn? I'd give you the top five I think most important in your list (subjective): 

Q1: Logistic regression is able to handle categorical and continuous variables. In your example, number of hours for each student in your training set is your inputs. Of course, you'll also need a binary response variable (pass or failed). Q2: Logistic regression is not a classifier, the model gives you fitted probabilities conditional to the number of hours. You can set a threshold to your model (many posts exist, please search). Or you can apply a classifier such as and many others.