I'm working with computable probability distributions over all finite strings. These are usually formalized as the space of all semicomputable semimeasures. Informally: a probability distribution is semicomputable if the probability it assigns to a given string can be computed to arbitratry precision from above or below by a Turing machine. If it can be computed from above and below, it's computable. (formal description here) Now, I prefer to think of the space of distributions that can be sampled by a probabilistic Turing machine (PTM). That is the distributions $p$ for which a PTM with no input outputs the string $x$ with probability $p(x)$. I have a (rough) proof that the two classes of distributions the class of lower semicomputable semimeasures and the class of semimeasures samplable by PTM are equivalent: every lower semicomputable semimeasure can be sampled by a PTM and vice versa. However, this seems like the sort of thing that has been worked out ages ago, and is well known to everybody. Can anybody help me with a reference that asserts and proves this equivalence? edited after @domothorp's comments 

We have given a weighted graph $G=\{V,E\}$, where $V=\{v_1, v_2,...,v_n\}$, and for all $i,j$, the weight of edges $w(v_i, v_j)\in (0,W)$. And we have also given a weight threshold s $w$ (where $0<w<W$) on the edge as the input. Now, our goal is to find (probabilistically) cliques (of any size >=3) in the graph such that the edges which are in cliques having weight at least $w$, i.e., we need to find cliques having edge weight at least $w$. Pls let me know if I am able to put the question clearly. 

Theorem 14 of this paper by Tam´as Sarl´os gives a relative error rank-$k$ approximation of a given matrix $A$ under the frobenius norm. I am looking for reference of a similar result (relative error low rank approximation) under the $l_2$ norm. Pls let me know I am able to put the question clearly. 

Carathéodory's theorem says that if a point $x$ of $R^d$ lies in the convex hull of a point set $P$, then there is a subset $P′ \subseteq P$ consisting of $d + 1$ or fewer points such that $x$ can be expressed as a convex combination of $P′$. A recent result by Barman (see paper) shows an approximate version of the above theorem. More precisely, given a set of points $P$ in the $p$-unit ball with norm $p \in [2,\infty)$, then for every point $x$ in the convex hull of $P$ there exists an $\epsilon$-close point $x'$ (under the $p$-norm distance) that can be expressed as a convex combination of $O\left(\frac{p}{\epsilon^2} \right)$ many points of P. Now, my question is that does the above result implies (or have some connection with) some kind of dimensionality reduction for the points in the convex hull of $P$. It seems intuitive to me (however I don't have a formal proof of it) - as for any point $x$ inside the $P$ there is a point (say) $x'$ in a close neighborhood of $x$ which can be written as convex combination of constant many points of $P$, which in some sense dimensionality reduction of $x'$. Pls let me know if I am able put my question clearly. Thanks. 

I can answer my own question here. The original code over $S = \{A, B, \ldots\}$ corresponds (by the Kraft inequality) to a probability distribution on $S$. I can partition S by the subsets I'm ambivalent about, so in $S'$, there's an element $a = \{A, B\}$ to denote that I don't care about the difference between $A$ and $B$. This gives me a simple probability distribution on $S'$: $p(a) = p(A) + p(B)$. By the Kraft inequality, again, there is a coding that corresponds to this distribution, so that I can use an algorithm like arithmetic coding to achieve acode of length $- \log p(a) = - \log p(A) + p(B)$. 

You don't need symmetry of information. The invariance theorem does the trick. Let $p$ the smallest program such that $U(p) = \langle x, y\rangle$. One way of producing $(y, x)$ is to take make a program $q$ that runs whatever program it is given as input, interprets the output as a pair, and flips the two parts. This gives you a program $\overline{q}p$ to produce $\langle y, x\rangle$. Since $\overline{q}$ is only a constant number of bits long, the two $K$'s are equal up to a constant. Now for higher numbers of arguments, the same idea works in principle. However, the constant does depend on the number of arguments: if I want to write a $q$ program to compute some $n$-tuple of numbers and re-arrange it into an arbitrary order, I need to encode the order in $\log(n!)$ bits (if the ordering is random). So if the number of arguments is not fixed to a constant, you need to take that into account. Isn't there some other way, besides the $q$ program? No, if there were we could encode free information in the ordering of the tuple. Assume for a contradiction that $K(x_1, \ldots, x_n)$ is invariant up to a constant to permutation of the arguments, with the constant independent of $n$. Let $X = \langle x_1, \ldots, x_n\rangle$ be the enumeration of the first $n$ binary strings, so that $K(X) \leq_+ K(n)$. Let $z$ be a random string with $|z| = \log(n!)$ Let $\langle x_1, \ldots, x_n \rangle$ be a random string. Index all permutations of $n$ elements by binary strings and pick the one corresponding to $z$. Call this permutation of our tuple $X_z$. Let $p$ and $p_z$ be the shortest programs for $X$ and $X_z$ respectively. Build a program $q$ that reads input $\overline{p}p_z$ and returns $z$. Putting all this together gives us $$\log(n!) \leq_+ K(z) \leq_+ |\overline{q}\overline{p}p_z| \leq_+ 2K(X) \leq_+ 2K(n) \leq_+ 4\log(n).$$ 

I am trying to understand the notion of $\epsilon$-coreset and its relation with sampling bounds of a range space having a finite VC-dimension. Although both of them give an $\epsilon$-approximation sketch of the input data. However, for the later case we know a characterization i.e. if a range space has finite VC dimension, then it can be approximated by a small (constant) size sample. On the other hand, for the case of $\epsilon$-coreset (similar to the concept of VC dimension) is there any characterization known over the problems which can or cannot be sketched by a small size sample? Thanks, 

For $n>1$ and $~0<p \leq 1$, can we upper and lower bound the following binomial series in terms of $n$ and $p$ $$\Sigma_{i=\lceil p n \rceil}^n {n \choose i} (p )^i(1-p)^{(n-i)}$$ 

Let $G=(V,E)$ be a graph. For a given $\rho \leq |V|$ and $\epsilon$ with $(0<\epsilon<1)$, is there any sublinear query algorithm known/possible to decide if the graph has a clique of size $\rho$, or all the cliques are of size at most $\rho(1-\epsilon)$. More precisely, for a given probability tolerance $\delta$ and error tolerance $\epsilon$, algorithm queries $f(\frac{1}{\epsilon}, \frac{1}{\delta}).o(n^2)$ many position of the corresponding adjacency matrix of the graph, and accept with probability at least $1-\delta$ if the graph has a clique of size $\rho$, or reject probability at least $1-\delta$ if all the cliques are of size at most $\rho(1-\epsilon).$ Thanks in advance. 

Basically, almost any machine learning or compression method is an approximation to Kolmogorov complexity: 

Thus, you can just look for patterns with any compressor or probability distribution and the better they compress your data, the better your upper bound for K(x). Just make sure to add the size of the compressor itself to the size of the compressed data to get the estimate. Better yet, write a self-contained program that outputs your data. The smaller you can make that program, the smaller $K(x)$ is. Of course, bounding from above may not be enough. What if your estimate is quite big, what's the probability that $K(x)$ is actually very small? For this you need to make some assumptions, on your data: fix a class of probability distributions (like probabilistic automata), and use a Baseyian mixture over those distributions to compress your data. If your data was actually produced by a probabilistic automaton, the probability that $K$ is substantially smaller than your estimate vanishes exponentially. You can also use a time bound to define your model class, which leads you to Suresh's answer. Basically, if you assume that your datasource has polynomial time complexity, and you try all polynomial Turing machines to compress it, you can be pretty sure that you've accurately estimated the Kolmogorov complexity. This may still not be that practical, but for lower time bounds, you may be able to compute the full Bayesian mixture, of a good approximation to it. For technical details see this paper. Disclaimer: I'm one of the authors. Finally, you should note that $K(x)$ is only meaningful in an asymptotic sense. For any one dataset the value of $K(x)$ can change by a constant from one computer to the next, and that constant can be as large as you like. So take approximations of this kind with a grain of salt. 

Beck's theorem is a classical result in discrete geometry which describes about the geometry of points in the plane. The result states that a finite collections of points in the plane fall into one of two extremes; one where a large fraction of points lie on a single line, and one where a large number of lines are needed to connect all the points. Formally, the statement of the theorem is as follows: Theorem: Let $S$ be a set of $n$ points in the plane. If at most $(n-k)$ points lie on any line for some $0 \leq k < n-2$, then there exist $\Omega(nk)$ lines determined by the points of $S$. Now, my question is that if the generalization/variation of the above result is known in the case of higher dimension $(dim\geq 3)$? 

Let $M$ be a binary ($0-1$) matrix of size $n \times m$. We define binary rank of $M$ as the smallest positive integer $r$ for which there exists a product decomposition $M = UV$, where $U$ is $n \times r$ and $V$ is $r \times m$, and all entries of $U$ and $V$ come from $\{0, 1\}$. My question is that is there known algorithmic way to determine the binary rank of $M$; and Singular value decomposition that support the binary rank. Any reference in this regard would highly help.