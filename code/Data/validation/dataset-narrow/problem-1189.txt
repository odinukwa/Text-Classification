Check out Luis Von Ahn at CMU. He is the original Captcha guy. You will find enough videos like this google techtalk on the subject of Human Computation. 

Some additional references I remember of that era: 1) Diaconis and Stroock, Geometric bounds for eigenvalues of Markov chains, The Annals of Applied Probability, 1991; but I remember getting my hands on a preprint sometime in 1990. This paper in turn refers to 2) Dodziuk, Difference equations, isoperimetric inequality and transience of certain random walks,Transactions of the American Mathematical Society, 1984. Also, an important "algorithmic companion" paper to Sinclair and Jerrum at that time was 3) Dyer Frieze Kannan, A Random Polynomial Time Algorithm for Approximating the Volume of Convex Bodies, STOC 89. Of course, the results here were built on top of SJ. 

Parity and $AC^0$ are like inseparable twins. Or so it has seemed for the last 30 years. In the light of Ryan's result, there will be renewed interest in the small classes. Furst Saxe Sipser to Yao to Hastad are all parity and random restrictions. Razborov/Smolensky is approximate polynomial with parity (ok, mod gates). Aspnes et al use weak degree on parity. Further, Allender Hertrampf and Beigel Tarui are about using Toda for small classes. And Razborov/Beame with decision trees. All of these fall into the parity basket. 1) What are other natural problems (apart from parity) that can be shown directly not to be in $AC^0$? 2) Anyone know of a drastically different approach to lower bound on AC^0 that has been tried? 

The $N$x$N$ bipartite matching problem can be written as finding a configuration of variables ${\mathbf y}^* = \{y^*_1, \ldots, y^*_N\}$, $y_i \in \{1, \ldots, N\}$ such that $${\mathbf y}^* = \arg\min_{{\mathbf y}} \;\; E({\mathbf y}; {\mathbf w}),$$ where $y^*_i$ is the index of the point in partite set $B$ that the $i$th point in partite set $A$ is matched to in the optimal matching$^1$, and ${\mathbf w}$ is a $N$x$N$ matrix of matching costs, giving the cost of matching $A_i$ to $B_j$. The 1-to-1 constraints are implicit in $E$. For purposes here, further restrict ${\mathbf w}$ to lie inside a unit hypercube ${\mathbf w} \in [0,1]^{N\cdot N}$. I'm interested in what I'm calling the inverse bipartite matching polytope, $R({\mathbf y})$, which contains all ${\mathbf w}$ that lead to ${\mathbf y}$ as a minimum cost matching, ie., $$R({\mathbf y}) = \{{\mathbf w} \mid \arg\min_{{\mathbf y'}} \;\; E({\mathbf y'}; {\mathbf w}) = {\mathbf y} \}.$$ It is straightforward to show that this is a convex polytope, but I don't know of any way to define it without using an exponential number of linear inequalities. So I'm interested in a separation oracle for $R({\mathbf y})$: given a $\hat {\mathbf w}$ that lies outside $R({\mathbf y})$, I'd like a linear constraint that separates $\hat {\mathbf w}$ from $R({\mathbf y})$, ideally cutting away as much space outside of $R({\mathbf y})$ as possible. (So while one simple solution is to find the optimal matching under $\hat {\mathbf w}$, $\hat {\mathbf y} = \arg\min_{{\mathbf y}} \;\; E({\mathbf y}; \hat {\mathbf w})$, then to enforce that the cost of ${\mathbf y}$ is less than the cost of $\hat {\mathbf y}$, this is unappealing because it seems to be a pretty shallow cut). Is this a problem that has been studied, or is it closely related to a problem that has been studied? I'd appreciate any references or terms to narrow down my search. [1] I'm assuming throughout that the minimum cost matching is unique. 

Check out this paper of McKenzie, Reinhardt, Vinay. We use multiplex-select gates to characterize classes between $NC^1$ and $LOGCFL$, including $L$, $LOGDCFL$ etc. For example, $L = MWidth, Size(log,poly).$ $NL$ has a natural circuit based characterization using Skew Circuits. This is just a circuit representation of the branching program that represents $NL$. Skew circuits are due to Venkateswaran. 

@JeffE, Here is a paper that counts min weight cycles in a graph. As far as I remember, it was definitely inspired by Karger's technique/result and it was a fun proof. Hope this helps with the teaching. 

Eva Tardos proved that the gap is truly exponential by showing that there is a monotone boolean function that has poly size circuits but requires exponential size monotone circuits. Nothing better than super-polynomial is known for matching. Raz has a result that monotone circuits for matching have linear depth. (Thanks Klauck, for pointing the typo.) AFAIK, we know nothing better. Ref: (1) $URL$ (2) $URL$ 

Shortest paths in DAGs are typically problems in NL and many times complete as well. A slightly "larger" class is LOGCFL (of course, we don't know if NL=?LOGCFL) where typical problems solvable with dynamic programming live. (Typical here means polynomial time. Unlike for example, DPs for knapsack type problems that take exponential time.) As an example, consider the word problem on groupoids. You have a groupoid table and a word $a_1 ... a_n$ and the question is whether we can bracket the word in some way so that it evaluates to identity. (Note that the groupoid table need not be associative.) This problem is complete for LOGCFL. Here is a reference; if I remember correctly, this goes back to Valiant. So it would seem that typical DPs are more powerful than typical shortest path problems under typical complexity theoretic assumptions! Given the vagueness of what constitutes DP, this is as far as we can say. update: Suresh, here is one way to see. LOGCFL is a nondeterministic machine (like NP) but running in log space, polynomial time, with a stack. Think of the final solution as a witness. It will be a polynomial sized proof tree (because I assumed polynomial running time) with the children being the "smaller" subproblems etc. But to traverse this tree, you will have to explore all children at a node and therefore you need a stack. (Because of the logspace restriction, you don't have space to record the witness and then check). 

(Apologies if this is misplaced or too broad. I'm open to suggestions on how to reformulate it.) I'm interested in tracing back the "ancient" history of max-flow algorithms, and discrete optimization algorithms in general. Ford-Fulkerson is my straw-man of a starting point. What were the significant advances prior to that? How far back can we go while still being able to make a reasonable argument that somebody was working on max-flow? How about graph algorithms? How about discrete optimization in general? I'd also be happy to get references to places where this is discussed. 

I'd appreciate any pointers or terms that could get me started in the right direction. We have a directed graph $G=(V,E)$ and lengths $l_{ij}$ for each edge $ij$ that can be assumed positive. There is a special start node $s$ and end node $t$. For each edge $ij$, we'd like to compute the length of the shortest path from $s$ to $t$ that does not use edge $ij$. A simple brute force algorithm is to run a shortest path algorithm for each edge, each time removing a different edge from the original graph. Is there a more efficient algorithm that takes advantage of the fact that there is a lot of repeated computation happening in this brute force algorithm? Thanks in advance. 

If you're just looking for good empirical performance, the affinity propagation algorithm usually works better than k-medians. There is code available in several languages and publications describing the algorithm in more detail are here: $URL$ The objective that it tries to maximize is: $$\sum_{i} s(i, c_i)$$ where $s$ is a similarity measure defined between pairs of points (e.g., negative distance), and $c_i \in \mathbf{c}$ gives the cluster that $i$ belongs to. There is one additional parameter given in $s(i, i)$ that controls whether you prefer large or small clusters. 

Let me start with some examples. Why is it so trivial to show CVP is in P but so hard to show LP is in P; while both are P-complete problems. Or take primality. It is easier to show composites in NP than primes in NP (which required Pratt) and eventually in P. Why did it have to display this asymmetry at all? I know Hilbert, need for creativity, proofs are in NP etc. But that has not stopped me from having a queasy feeling that there is more to this than meets the eye. Is there a quantifiable notion of "work" and is there a "conservation law" in complexity theory? That shows, for example, that even though CVP and LP are both P-complete, they hide their complexities at "different places" -- one in the reduction (Is CVP simple because all the work is done in the reduction?) and the other in expressibility of the language. Anyone else queasy as well and with some insights? Or do we shrug and say/accept that this is the nature of computation? This is my first question to the forum: fingers crossed. Edit: CVP is Circuit Value Problem and LP is Linear Programming. Thanks Sadeq, for pointing out a confusion. 

1) An average human consumes 100 Watts of power, which is roughly the same order of magnitude as a desktop (mostly display). The brain consumes about 20% of this, which is about 20 Watts. Now, your ipad consumes about 2 to 3 Watts on average, which is an order of magnitude better than the human brain. Therefore, I am not quite sure what you mean by "... extremely low energy compared to a common computer." 2) It is definitely true that our thoughts/engineering are inspired by nature, but we don't have to mirror them in-toto. I doubt if you will call our aeroplanes flawed/lacking because they don't flap their wings to fly! We abstract important properties from nature (e.g., as JeffE says above, about neurons and neural nets) but we do not have to mimic every aspect of nature. 

Given a graph $G = (V,E)$ and a vertex weight $z_v$ for each $v \in V$, find an (EDIT) induced subgraph $G' = (V', E')$ with minimum weight $z_{G'}=\sum_{v' \in V'} z_{v'}$ where $G'$ is disconnected (i.e., has 2 or more maximal connected components). The maximum weight connected subgraph problem is known to be NP-hard by reduction from MINIMUM-COVER [1]. Is this "opposite" disconnected problem easier? Also, maybe this deserves its own question, but are there interesting (non-contrived) cases where the "opposite" of a well-known hard problem is easy? Here's an attempt at defining opposite for vertex-weighted graph optimization problems: The problem P is defined as follows. Given a vertex-weighted graph $G$, and a set $\mathcal{S}$ of induced subgraphs of $G$, find an induced subgraph in $\mathcal{S}$ with maximum weight. Then, the opposite of $P$ is defined as follows. Given a vertex-weighted graph $G$, and the set $\mathcal{\bar S}$ of induced subgraphs of $G$ that are not in $\mathcal{S}$, find an induced subgraph in $\mathcal{\bar S}$ with minimum weight. (Note: since $z_v$ is not constrained to be positive, minimizing or maximizing is arbitrary. I switch only for aesthetics related to the term "opposite".) [1] $URL$ 

We know from e.g. Koutis-Miller-Peng (based on work of Spielman & Teng), that we can very quickly solve linear systems $A x = b$ for matrices $A$ that are the graph Laplacian matrix for some sparse graph with non-negative edge weights. Now (first question) consider using one of these graph Laplacian matrices $A$ as the covariance or (second question) inverse covariance matrix of a zero-mean multivariate normal distribution $\mathcal{N}(\boldsymbol{0}, A)$, or $\mathcal{N}(\boldsymbol{0}, A^{-1})$. For each of these cases, I have two questions: A. How efficiently can we draw a sample from this distribution? (Typically to draw a sample, we compute the Cholesky decomposition $A = LL^T$, draw a standard normal $y \sim \mathcal{N}(\boldsymbol{0}, I)$, then compute a sample as $x = L^{-1} y$). B. How efficiently can we compute the determinant of $A$? Note that both of these could be solved easily given a Cholesky decomposition, but I don't immediately see how to extract $L$ more efficiently than just by using a standard sparse Cholesky algorithm, which wouldn't use the techniques presented in the above-referenced works, and which would have cubic complexity for sparse-but-high-treewidth graphs.