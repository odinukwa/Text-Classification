I recommend Jenga! Assuming you have two perfectly logical, sober, and dextrous players, Jenga is a perfect-information two-player game, just like Checkers or Go. Suppose the game starts with a stack of $3N$ bricks, with 3 bricks in each level. For most of the game, each player has $\Theta(N)$ choices at each turn for the next move, and in the absence of stupid mistakes, the number of turns is always between $N$ and $6N$. So crudely, the game tree has $N^{\Theta(N)}$ states. If you explored the game tree by brute force, you might spend exponential time to find a winning move or convince yourself that you can't win. But in fact, Uri Zwick proved in 2005 that you can play Jenga perfectly by keeping track of just three integers, using a simple set of rules that you can easily fit on a business card. The three numbers you need are 

For any integer $n>0$, writing $3^n$ in binary requires exactly $L = \lceil \log_2(3^n)+1\rceil$ bits. Some elementary algebra implies that $$ \frac{L-2}{\log_2 3} \le n \le \frac{L-1}{\log_2 3}. $$ For any bit length $L\ge 1$, there is at most one integer in this range. Thus, given an integral power of $3$ that is $L$ bits long, the exponent must be the integer $$ n = \left\lfloor\frac{L-1}{\log_2 3}\right\rfloor. $$ 

Arbitrarily large plane graphs with these properties can be constructed by taking a sufficiently large finite portion of a regular tiling of the hyperbolic plane by $g$-gons. Finally, to obtain a surface graph $G'$ where every face has length $g$, identify pairs of boundary edges in $G$ according to the pairing described above. The bounded faces of $G$ become the faces of a cellular embedding of $G'$ on some closed surface without boundary. The distance condition on the pairing guarantees that the girth of $G'$ is $g$. By choosing both $G$ and the pairing more carefully, once can construct arbitrarily large $d$-regular graphs satisfying your girth condition, for any integers $d$ and $g$ such that $1/d + 1/g < 1/2$. Even within these constraints, the construction has lots of degrees of freedom. 

In fact, most of the time, you only have to remember $n\bmod 3$ and $m\bmod 3$ instead of $n$ and $m$. Here is the complete winning strategy: 

This is a followup to Suresh's answer. As he says, there are lots of construction problems in computational geometry where the complexity of the output is a trivial lower bound on the running time of any algorithm. For example: planar line arrangements, 3-dimensional Voronoi diagrams, and planar visibility graphs all have combinatorial complexity $\Theta(n^2)$ in the worst case, so any algorithm that constructs those objects trivially requires $\Omega(n^2)$ time in the worst case. (There are $O(n^2)$-time algorithms for all three of those problems.) But similar constraints are conjectured to apply to decision problems as well. For example, given a set of n lines in the plane, how easily can you check whether any three lines pass through a common point? Well, you could build the arrangement of the lines (the planar graph defined by their intersection points and the segments between them), but that takes $\Theta(n^2)$ time. One of the main results of my PhD thesis was that within a restricted but natural decision tree model of computation, $\Omega(n^2)$ time is required to detect triple intersections. Intuitively, we must enumerate all $\binom{n}{2}$ intersection points and look for duplicates. Similarly, there is a set of numbers where $\Theta(n^2)$ triples of elements sum to zero. Therefore, any algorithm (modeled by a certain class of decision trees) to test whether a given set contains three elements that sum to zero requires $\Omega(n^2)$ time. (It's possible to shave off some logs via bit-level parallelism, but whatever.) Another example, also from my thesis, is Hopcroft's problem: Given $n$ points and $n$ lines in the plane, does any point contain any line. The worst-case number of point-line incidences is known to be $\Theta(n^{4/3})$. I proved that in a restricted (but still natural) model of computation, $\Omega(n^{4/3})$ time is required to determine whether there is even one point-line incidence. Intuitively, we must enumerate all $\Theta(n^{4/3})$ near-incidences and check each one to see whether it's really an incidence. Formally, these lower bounds are still just conjectures, because they require restricted models of computation, which are specialized to the problem at hand, especially for Hopcroft's problem). However, proving lower bounds for these problems in the RAM model is likely just as hard as any other lower-bound problem (ie, we have no clue) — see the SODA 2010 paper by Patrascu and Williams relating generalizations of 3SUM to the exponential time hypothesis. 

Yes and no. Set interaction is one of the problems specifically studied in Ben-Or's seminal paper on lower bounds for algebraic decision and computation trees. The problem is formally defined as follows: Given two sets of n numbers, is their intersection empty? Equivelently, does their union have exactly 2n elements? Ben-or proves a lower bound of Ω(n log n) for this problem. If the sets have diffent sizes n>m, the lower bound becomes Ω(n log m), but this only beats the naive O(n log n) bound if m is subpolynomial in n. On the other hand, if your list elements are integers, you can solve the problem in o(n log n) time using fast integer-RAM sorting algorithms. For reasonable word sizes, I believe the fastest integer sorting algorithm runs in $O(n \sqrt{\log \log n})$ expected time [Han and Thorup, FOCS 2002]. 

Ben-Or directly proved $\Omega(n\log n)$ lower bounds for several fundamental problems in the algebraic computation tree model: 

If you choose $\mathbf{a}$ uniformly at random from the hypercube $[0,1]^n$, the matrix $[\mathbf{A}~ \mathbf{a}]$ will have the desired property with probability $1$. 

This representation is not unique. For example, reversing the cyclic order of incoming darts at any vertex $v$ and changing the signs of all edges incident to $v$ leaves the embedding intact. The theorem you are asking about is the following: 

Yes, at least if you define your terms correctly. Following standard usage in Riemannian geometry, a subset $M \subseteq S^n$ is convex if and only if, for every pair of points $x$ and $y$ in $M$, some shortest path from $x$ to $y$ lies entirely within $M$. With this standard definition, any convex subset $M \subseteq S^n$ is both the convex closure of the points in $M$ and the intersection of all closed hemispheres that contain $M$. In particular, the only convex subset of $S^n$ that does not lie entirely in a closed hemisphere is the entire sphere $S^n$. (And yes, $S^n$ is the intersection of the empty set of closed hemispheres containing $S^n$!) Your definition of convexity allows some non-standard convex sets, like the complement of a single point. Let's assume that the sphere $S^n$ is centered at the origin of $\mathbb{R}^{n+1}$. Following the standard definitions, a subset $M\subseteq S^n$ is convex if and only if $M$ is the intersection of $S^n$ with a convex cone in $\mathbb{R}^{n+1}$. So let $C$ be a convex cone in $\mathbb{R}^{n+1}$, and let $M = C \cap S^n$. Let $C^*$ denote the dual cone of $C$; this is also a convex cone, so the intersection $S^n\cap C^*$ is convex. Your set $X$ is $S^n \cap (C^* \cup -C^*)$ = $(S^n\cap C^*) \cup -(S^n\cap C^*)$. Thus, $X$ is the union of two antipodal convex subsets of $S^n$. 

Given an input vector $\vec{x}\in\mathbb{R}^n$, we compute by traversing a path downward from the root, branching according to the sign of the query polynomials in the visited nodes. The traversal eventually reaches a leaf; the label of that leaf is the output. The "running time" of the algorithm is defined to be the length of the traversed path; thus, the worst-case running time is the depth of the decision tree. Note in particular that each query polynomial may have $\Omega(n^d)$ distinct terms; nevertheless, the model assumes that we can evaluate the sign of any query polynomial in constant time. For each leaf $\ell$, let $R(\ell) \subseteq \mathbb{R}^n$ denote the set of input vectors for which execution reaches $\ell$. By construction, $R(\ell)$ is a semi-algebraic subset of $\mathbb{R}^n$ defined by at most $t = \text{depth}(\ell)$ polynomial inequalities of degree at most $d$, for some constant $d$. A classical theorem independently proved by Petrovskiĭ and Oleĭnik, Thom, and Milnor implies that such a semi-algebraic set has at most $(dt)^{O(n)}$ components. Suppose we want to decide if the input vector lies in a subset $W\subseteq\mathbb{R}^n$. We can make this decision using a $d$th order decision tree with depth $t$ only if $W$ has at most $3^t (dt)^{O(n)}$ components. Equivalently, we have the lower bound $t = \Omega(\log \#W - n\log d)$. For example, suppose we want to determine whether all $n$ coordinates of the input vector are distinct. The set $W$ of "yes" instances has exactly $n!$ components, one for each permutation of size $n$, so we immediately have a lower bound of $\Omega(n\log n)$. Note that this lower bound strengthens the classical $\Omega(n\log n)$ comparison lower bound for sorting in two ways. First, the model of computation allows much more complex queries than comparisons at unit cost. Second, and more importantly, the lower bound applies to a decision problem—there are only two distinct outputs, so the naïve information-theoretic bound is trivial. Extensions of this argument use more interesting complexity measures than the number of components, such as higher Betti numbers, the Euler characteristic, volume, or number of lower-dimensional faces. In all cases, generalizations of the Petrovskiĭ-Oleĭnik-Thom-Milnor theorem imply that each set $R(\ell)$ has "complexity" at most $(dt)^{O(n)}$. This lower-bound technique has two significant downsides. First, consider any problem that can be solved with a family of algebraic decision trees with polynomial depth. The Petrovskiĭ-Oleĭnik-Thom-Milnor theorem and its generalizations imply that the semi-algebraic sets that define such a problem have complexity at most $n^{O(n)}$. Thus, this technique cannot be used to prove lower bounds bigger than $n\log n$ in this model, for any problem that can be solved in polynomial time. It is possible to prove $\Omega(n^2)$ lower bounds for certain NP-hard problems, but for similar reasons, there is no hope of anything better. In fact, Meyer auf der Heide proved that some NP-hard problems can actually be solved using linear decision trees with only polynomial depth; specifically, Knapsack can be solved in $O(n^4\log n)$ "time". Meyer auf der Heide's algorithm was later adapted by Meiser to any problem whose solution space is the union of cells in an arrangement of $2^{O(n)}$ hyperplanes in $\mathbb{R}^n$. Thus, it is impossible to prove lower bounds bigger than $n^4\log n$ in this model, for any such problem. An example of such a problem is $k$-SUM (Do any $k$ elements of this set sum to zero?) for any constant $k$; the fastest uniform algorithm for $k$-SUM runs in roughly $O(n^{k/2})$ time. Actually executing Meier auf der Heide's algorithm would require repeatedly solving several NP-hard problems to find the appropriate $O(n^4\log n)$ query polynomials; this construction time is free in the lower-bound model. Hooray for double-negative results! 

More often than not, it means "I don't want to write out Algorithm B in detail, because all the interesting details are nearly identical to those in Algorithm A, and I don't want to go over the 10-page limit, and anyway the submission deadline is in three hours."