If we're estimating a continious distribution's density, perhaps we should introduce an integral in here right? A kernel estimate should be such that $\int_{-\infty}^{\infty}K(x)dx = 1$. Therefore, it should be relatively easy to see that an estimate for $f(x)$ called $\hat{f}(x)$ should have the following: $\int_{-\infty}^{\infty}\hat{f}(x)dx = \frac{1}{n}\sum_{j=1}^{n}\frac{1}{h}K(\frac{x-a}{h}) $ $= \frac{1}{n}\sum_{j=1}^{n}1 = 1$. Naturally since, the kernal and the estimate for the pdf are greater than 1, then our hat function is also a probability density function. Now for a bit more detail: $\hat{f}(x)$ is usually derived from a definition of the derivative of the emperical CDF. So instead of justifying it via the way you would a parzen window, you instead just justify it from what it means to be a pdf and what you want a good estimate for that pdf to be. edit: With regards to knn and your estimator. I think it's also important to realize that the for any fixed point the nearest neighhor estiamte is the kernel estimate. However, it is different estimate for each point. The kernel still remains an estimate because each individual estimate is a density so overall the kernel is a linear combination of densities. Furthermore the coefficients for the k estimates will sum up to 1. 

Resamble your data. You can oversample the minority class or undersample the majority class. The end goal is to balance out the data more or less. Give your model a prior to help inform frequency. Similar you can pass a weight argument. The weight will penalize the classification function for misclassification of a rare positive cases. Lastly you can modify the accuracy measurement. A common measurement would be to use F1 statistic instead of just accuracy. Maximize the F1 statistic by cross validation and see if it's stable during testing. 

Combine all trees as an average or weighted via some scheme. Gradient Boosting One key note is that random forest trees essentially indepedent of each other. Boosting algorithms add a certain depedency to the model. 

I don't necessarily think it's that batch normalization is necessarily stochastic, but rather just how batch normalization works when compared to the inference stage. As you may know, during the training phase batch normalization depends on the mini-batch. However, that dependency is undesirable for inference so instead a moving average over all mini-batches is taken. Doing this obviously can cause problems since what your model is inference on is different than what it was trained on. Furthermore, the smaller your mini-batch the worse the performance gets with more depth since the the inaccuracies computed for the smaller batch just get compounded more and more. If you insist on having a batch normalization phase, a way to help mitigate this might include implementing a batch re-normalization setup instead. 

Just to add on a bit. I'm not sure where you got "(3*3*9+3*3)/5*5". A 3x3 filter has 9 parameters and a 5x5 has 25 paraemters. Therefore, even if you stack two 3x3 filters, you still only have 18 compared to the 25. That is in fact a 28% reduction in parameters that need to be calculated. This simple reduction spread out across an entire network can dramatically improve speeds. In fact, you can even take this principle and apply a nx1 filter that moves only moves across the horizon followed by a 1xn flter that moves vertically to see similar gains. (Although this may introduce bottlenecks.) I think a good way to understand the computational cost is to actually calculate a simple filter by hand. Keep track of all the operations you're doing and find out what is reusable. Lastly, i'll also mention that stacking two 3x3 kernels gives you a receptive field of a 5x5 kernel. Stacking three of 3x3 kernels gives you a receptive field of a 7x7 and so on and so forth. Understanding the receptive field is critical for any computer vision task. Here's some more info DCNN Receptive Field Info 

There's more than two or three variants with regards to LSTM. A paper that explores these variants can be read here: LSTM:A Search Space Odyssey 

I think you're quite confused. Hadoop is a collection of software that contains a a distributed file system called HDFS. Essentially HDFS is a way to store data cross a cluster. You can access file stores as you would in a local file store (with some modification) and modify things via Java API. Furthermore, ON TOP OF the file system there exist a MapReduce engine that allows for distributive workflow. Python on the other hand is a generic programming language that can be made to do a myriad of task such as build a web applciation, to generating reports and even peforming analytics. SciPy is a package that can be used in conjunction with Python (and often numpy) as a way to perform common scientific task. Truthfully, they focus on different paradigms. If you have LARGE DATA (ie terabytes worth of it), it might be worth wild to setup a hadoop cluster (ie multiple servers and racks) and use Java MapReduce, Hive, Pig or Spark (of which there is a python version) to do analytics. If your data is small or you only have one computer, then it probably makes sense to just use python instead of adding the overhead of setting up hadoop. Edit: Made correction via comment. 

The hyperbolic trig functions follow the equation for a Rectangular hyperbola, which is something you should be familiar from analytical geometry. The recentgular hyperbola is defined by $x^2 - y^2 = 1$ and if you let $x = cosh(t)$ and $y = sinh(t)$ and plug it into the rectangular hyperbola equation you can verify this fact quite easily. From this idea you can derive the rest of the hyperbolic trig functions. (Also they reason they are given trig names is because they resemble the properties trig functions have). 

You probably want to use the functional API. Functional API has a lot of examples you can follow and used whenever you need to build more complicated models. 

The simple answer is that it's conviences rather than necessities. You're more than welcome to take the absolute value and in many cases it may be better to do so. Squaring a function makes the math a bit happier and easier and for proofs has desirable properties. While you can basically do the same proofs with an absolute value function, you may have to handle certain edge cases and just amounts to more writing. 

This is rather common. The algorithm for KernelRidge requires a SVD to be performed. Sadly, the SVD cannot handle a sparse matrix so the _pre_compute_svd function in sklearn just converts the matrix into a dense matrix and moves on. This tends to blow up memory rather quickly. You have a couple of choices. Rewrite the method to handle sparse matrices or just use a different method. SVR would be the most similiar alternative. 

It's not uncommon for someone to label it as an unsupervised technique. You can do some analysis on the eigenvectors and that help explain behavior of the data. Naturally if your transformation still has a lot of features, then this process can be pretty hard. Nevertheless it's possible thus I consider it machine learning. Edit: Since my answer was selected (no idea why) I figured i'll add more detals. PCA does two things which are equivalent. First, and what is commonly referred, it maximizes the variances. Secondly, it minimizes the reconstruction error by looking at pair-wised distances. By looking at the eigenvectors and eigenvalues, it becomes rather simple to deduce which variables and features are contributing to the variance and also how different variables move in conjunction with others. In the end, it really depends on how you define "learning". PCA learns a new feature space that captures the characteristics of the original space. I tend to think that can be meaningful. Is it complex? No, not really, but does that diminish it as an algorithm? No I don't think so.