I've used forward/backward momentum here since it's easier to describe, but the same principle applies to lateral movement and turning. In order to turn, they need to shift their weight counter to the direction of the turn while moving forward or backward. 

Since it sounds like you're using a human sized perspective, you can either increase your texture or your polygon budget. Usually, you would bake fine geometry into something like a normal map, but if it's that small then it probably isn't worth it unless that's a vital part of gameplay. If it is, then go ahead and use a model for it. Just be careful that you aren't blowing all your resources on things the player isn't going to look twice at. A game with multiple size perspectives (if the player can grow or shrink to that size) could use variable level of detail - to increase the detail of now large objects - and depth of field - to hide distant objects consuming unnecessary draw cycles - for these things, but that implies a lot more engineering than it's worth if they aren't going to be spending much time there. 

Implement your own version of the rigidbody script. Make your world a whole lot bigger and rotate it under the player as they move rather than rotating the player round the world. 

I have a radius and the 2 angles for northSouth and eastWest rotations in degrees. I need a Spherical position or a cartesian position (I have code translate one to the other) Where a position is defined as one of the following ... 

experimenting with chunk sizing may help you determine some interesting answers to how your code performs. 

I wont be rewriting large portions later. What i'm writing is a small enough chunk that i can handle it as a "task". 

Define a rule / set of rules that state what is considered "reasonable input". Forget validation just accept what you're given and roll with it. 

Then you do something like ... Apply Shader state to context foreach mesh { Set vert and index buffer call draw indexed } My c# code for this works something like this ... I have a material class which basically wraps up the creation and management of a shader stack in which i do something like this once before i begin dealing with any meshes ... 

Yes - you can use the Marching Squares algorithm to detect the bounds of the image within the frame, and then I would suggest storing the actual bounds (height/width) somewhere. Particularly if your sprites contain islands, as this can take a long time to locate all the little pieces in the frame. Ideally, you would repack your spritesheets into an atlas so that you aren't wasting so much space on unused pixels and don't need to switch texture sources as much, but that is more relevant to hardware acceleration and probably premature optimization for small projects. 

Of course you can check the syntax automatically using an IDE like Visual Studio. However, without some more work the only measure of correctness is "does it do what you expect?" If it doesn't, then you would debug to find out why. Of course, you can take it a step further and apply Unit Testing. This is where you write code that sets up scenarios where you know there will be an expected result. For example, if you had a method like 

Typically most of the more feature complete game engines actually have 2 game loops. The first is a fixed step loop aimed at iteration around every 16ms for that ideal 60fps zone. The second is the "as fast as possible" loop. The idea is that some actions like physics processing need to have some idea of process in order to be calculated correctly and other actions like drawing things on the screen are best served up at the highest possible rate (unless vertical sync is enabled forcing a limit). Essentially the fixed step is handled by a timer that simply calls a function every x amount of time and the fast as possible loop is just an endless "while(true)" type loop. For the most part you want to aim to keep your main thread in your game / engine as clean and empty as possible. With multicore cpu's being the common situation these days even if you maxxed out that thread you should only see around 10% to 30% cpu load so my guess is you are doing a lot of multithreaded cpu intensive work in order to maxx out your cpu. 

I have some idea of how to do this for a single gravity source (conic section, parametric curves), but how do you blend multiple forces, and thus multiple curves, together? The idea is to give the player some idea of the trajectory their shot will take, so it doesn't need to reflect the actual path exactly, but the closer the better. 

Since rays extend to infinity in one direction, you'll have to define the segment you want to render: 

I think you want Dijkstra's algorithm, which is also used for internet routing. In short, you must create a graph where each edge is a path from one node to another. These could be pre-calculated if some of your geometry is fixed. For each starting point, you will order the nodes according to their distance from the destination. Then apply Dijkstra's algorithm through the graph to build routing tables for each one. The first route in the table will be the shortest path. Each route after that will be a variation depending on which nodes are blocked. These extra routes could be calculated as necessary, provided their order is maintained in the table (they should be ordered by the sum of their edge lengths). You'll refer to these routes again when a node is changed (blocked/unblocked). You can then add and remove nodes by splitting edges and moving the new midpoint, then recalculating from the origin node. 

So logically thinking about that, to move the entire world 1 chunk in any direction what do I actually change on my normal render cycle ... ... Nothing ... I do the same in my draw calls, but in my update loop I update my root object transform and all children inherit that change and the render loop continues as normal. So, With moving a chunk effectively being "free" I can't see an issue moving the whole world when a player steps over a chunk boundary. However ... I took a more liberal approach by setting "areas" in which the player could be to prevent a constant recalculation of the transform information. For example: lets say im moving --->>>> c1, c2, c3, c4 when i move from c1 to c2 I do nothing, when I move from c2 to c3 I move the world back by 2 chunks. How does this help ... Lets say a player is having a rather large fight and every frame counts but they are doing this right on a chunk intersection or worse on the corner of a chunk so this applies in 3 dimensions. Having that 1 chunk buffer means I make the change once until the player moves a reasonable distance so the "expensive but still cheap" math only happens once for that area. I apply the same logic to loading and unloading chunks on my world edges too to sort of apply a "temporary cache" around the currently being rendered terrain. 

The default Unity project settings include some joystick axes already. So looking further down, we find the same axis defined again! Removing the extra axis definitions fixes the problem. 

Essentially, the normal of the vertex would be the average of the adjacent faces' (triangles) normals. In pseudocode: 

Vertices are stored in memory using local coordinates. So no vertex has a "currentPosition," they're always positioned according to their offset from their original origin. In the "world," they are displayed according to some transformation: scale and rotation about their local origin, then translation with respect to the world origin. With the exception of static level geometry, vertices are almost never stored in a current position. So, to get the "current" position of any vertex, you must apply a transformation to it that takes it from local to world space. Applied to all vertices, you need only find the vertices with the greatest Y coordinate. It's been a while since I used XNA, but going off of the MSDN docs it should look something like this: 

I think you're making the mistake of thinking that you can produce this with a single perlin function. Depending on the result you are trying to achieve you have 2 options: 

Calling this up in the same way as my original question results in me getting what appears to be a not populated buffer of voxels however, before I started this I created all this code as cpu based versions. If I take my "not populated voxel buffer" and run that through my meshing code on the cpu I get as expected a single cube. I'm now not sure how it's possible to debug the results of a compute shader since it seems that you cannot view the data using the debugger in visual studio. I'm going to take a look in unity's built in MonoDevelop IDE and see if that shows anything but it may be that my original question was actually not even broken code in the first place! EDIT2: Ok MonoDevelop doesn't work either, nor does logging out the data using UnityEngine.Debug.Log(); As per the code, all values are -1 for the weights of my voxels except for the center voxel in my 3,3,3 grid and yet debugging / logging out any of the values always reports 0. The cpu code appears to get what Iwould expect though and outputs the right result. Now I have a problem when my gpu based meshing code doesn't work and I have no way to determine why ... does anyone have any ideas how I might be able to debug this code without apparently logging or using an IDE? ... jeez unity!!! EDIT3: I have spoken to unity about this by way of a bug, they initially stated that the problem was in my code but that was before I extracted this simple demo of the problem, I raised the problem in the unity forums too in the hope that someone with internal unity knowledge might be able to see something I can't, that discussion is here: $URL$ It looks like my next step is to build the same scenario in raw DX perhaps using SharpDX to talk to DX's API directly that would at least tell me if a "RWTexture3D" buffer can be mapped to an array in this manner. Personal note: I can't believe that noone has faced this problem before, surely someone out there has seen this? I mean ... even the 2D scenario isn't doing what I think makes sense and if your problem is 1D then why use the GPU at all right? Someone therefore must know the answer to this, my gut feeling is that there is some underlying architectural reason why a 3D buffer can't just be created like this, maybe it's worth asking the guys over at nVidia this question! 

What you want to do is test how far the rectangle can move in a direction, then change it. For example: 

It looks like half of the tris are flipped the wrong direction. Triangles have a winding order and a normal, and if they face the wrong way they will be treated as backfaces and culled from the render. The other half could be one of a number of things: 

Sadly, the only way to do it the way you want to is to get or write your own custom input manager. That would also give you the opportunity to assign representative images to each input (ie picture of button for A-button) and get better control over thumbstick deadzones. Unity's input manager is really more suited to defining names for input axes on different devices, rather than translating those inputs into gameplay terms. However, if the platform you're targeting has a fixed set of input buttons that shouldn't be too much of an issue. Although it's not the best practice, you can hard code the axes to the corresponding buttons. 

every "command" a user intends to issue is stacked up in a local list then sent to the "server" at the end of each turn who then forwards on the lists from other players to ensure the data is replicated to all players. Every time a player issues a command to be added to the list of things for this turn its immediately passed to the server then at the end of the turn a final "im done" is received from each player and the server triggers a new turn. 

... You get the idea ... Most people use float values for their voxel data as this allows more precision in the mesh generation as you are looking for a point at which the data evaluates to 0 for a given "cell" where a cell is a chunk of 27 voxels and you are evaluting the center voxel. for example ... if the center voxel = 0.5 and the voxel above = -0.4 the sum of the two values is -0.1 which means that the surface you want to render is -0.1 from the top of the voxel space for your center voxel. Meshing algorithms like Marching cubes, or Dual Contouring work this way.