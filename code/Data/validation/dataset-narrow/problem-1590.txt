I'm training a Neural Network in Keras (with Tensorflow backend) on the IRIS data set. After fiddling around with the number of layers and neurons a bit, I got to a point where the accuracy on the test set is 0.99 - OK, Great! Success!!! Then I retrained the model and re-evaluated - and ended up with accuracies between 0.7 and 0.97 - I never got back to that original 0.99, and this despite using the exact same configuration and training parameters every time. How do we avoid this problem, especially if we want to run out NNet in production ? 

Consider the following example: You have a rare disease whose occurrence seems to depend on a certain number of variables. You build a model which tries to predict patients most likely to be effected by the disease which is partially successful, that is, it predicts likely onset of the disease with some accuracy, but less than desired. Ideally, you update your model as more historical data on patients with this disease comes in, and the accuracy starts to improve. Eventually you start notifying high risk patients and provide them with steps to counter this disease. Because of this, more and more patients who would have been classified as high risk are actually not catching the disease and therefore decreasing the accuracy of your model. In a sense the model was a 'victim' of its own success. Are there any strategies for dealing with such prediction scenarios: Where a model designed to predict an undesirable outcome looses accuracy due to it successfully averting the outcome in real world cases? 

Why not compute the inter-cluster distance as the average Mahalanobis distance from each point in a cluster A to each point in cluster B? 

To be short: yes, trash it. Any ML model you use has one job - to predict. If it can't do that job then why on earth would you ever use it? The whole impetus for using ML algorithms is to predict better than guessing randomly. Ideally, much, much better than guessing randomly. Understanding why a model makes a decision has some value, sure, but only insofar as that decision has value to the end user/solves a business problem. 

If you're interested more generally in speech understanding or speech-to-text, some approaches to natural language parsing and speech-to-text use recurrent neural networks or Hidden Markov processes for learning, as well as a number of signal-processing algorithms to extract more data from the input stream that just raw audio. Keep in mind people have spent their whole careers on this work, so it's not a good problem to just pick up and run with unless you're a MS/PhD candidate looking for a capstone/dissertation project. Here's the iconic paper from Bell Labs that inspired a lot of the DFA/HMM solutions. I've yet to find a paper that does a good job of explaining how to actually implement the RNN style solutions, but here's one in case you're interested. It's likely that Alexa uses some combination of these methods, but I doubt you're going to get any good answer out of anyone here. After all, it's an important Amazon project and it's not like their engineers are going to come on Stack Overflow and start giving away trade secrets. 

You might want to try a Bayesian classifier instead of SVM or Decision Trees. Your problem is a generalization of the spam filter problem, i.e. classifying emails into 'Spam' (class 1) or 'Not Spam' (class 2). Most Spam filters use Bayesian classification instead of the more common supervised learning method. 

When you build a classification or regression model, you typically split the data into a train data set and a test data set. The test data is a randomly selected subset of the overall data. Once you are done with the training, you discard the test data, and apply the model you built to new unknown data. But in the case of time series, this wouldn't work: You can't just randomly choose any subset of the data for your test data set, since the data is ordered. The test data set has to be composed of later observations of the data, while the train data set is made up of earlier observations of the data. Say for example that you have data for 11 months of the year, and you want to forecast the values for December. You would train your model with the data from January through September, and then test it on the data from October and November. But after you've successfully trained your model, what do you do with the test data? On one hand, it doesn't make sense to forecast the values for December using a model built with Jan-Sep data. Such a model would miss any important trends that occurred in Oct and Nov. On the other if you bring the data from Oct and Nov back into the model, the parameters of the model will change, and so we're no longer sure that we are going to get the same accuracy that we got when we trained the model with just the Jan-Sep data. There seems to be a dilemma here. So how is this problem approached when using machine learning models, especially non parametric models like Neural Networks for time series forecasting? Do they incorporate the test data into the model or do they discard it? 

GraphLab is a good choice. Check out NetworkX. You could make this really easy on yourself by eliminating the father of/friend of/etc. distinctions and just treat every relationship as fundamentally the same. Then its as simple as building a graph that tracks the connections between your users. After that you could limit collaborative filtering to people in this person's close circle of friends. You could also make distinct Friend/Family/Coworker implementations, but unless you have a LOT of data (and a lot of Family members) collaborative filtering could be a little disappointing. 

Then you don't need to update the hash every time, all you have to do is transform into the vectorized format and add it to your collection. As long as you don't need to fit new features this can be done relatively fast. However, if you're doing something less structured like NLP this won't give you hardly any useful information. 

If you're doing anything at the millions of records per second level, you're going to need something more high-performance than Python. Spark/PySpark is ideal for this since it interfaces with your existing Python knowledge and supports HDFS/distributed technologies that are made to take on the millions per second sized tasks. As for sklearn: you might be out of luck here. The fit_transform() or fit() methods necessarily have to consider every record that you pass to them to keep the hash up to date, so if you have millions of data points coming in you're going to have to do millions of computations to keep moving, whether you do them one at a time or in a batch. The source code for the transform() method should convince you that doing either is equivalent. One caveat is that if you have predictable features, e.g. maybe you're receiving counts of foo's bar's and baz's, like in the first link: 

I've invested lot of time trying to understand the theoretical aspects of Deep Learning and Neural Networks - but I'm now questioning whether it is worth it or not, given that I am someone who works mainly on the applied/business side of things. By advanced theoretical knowledge, I mean things: Like understanding the details of how the Kernel trick works in SVM, the "no free lunch" theorem and its relationship to machine learning, the details of how neural networks are universal approximators, the VC dimension of a classifier, etc... By applied Data Scientist, I mean someone who solves business problems using existing algorithms as opposed to someone who develops new ones. So my question: Are there situations in an applied Data Scientist's life where such theoretical knowledge is useful? Or is this type of knowledge useful only for people who work on developing new algorithms? 

Step (2) can be automated to some extent using a grid search, but are there ways of automating the whole process? I'm thinking in particular of large scale data applications (for example when dealing with retail data, or customer analytics on a site like Netflix, ) where there are millions of instances of similar but distinct machine learning problems that each need to be trained and validated separately. In such a situation it is impossible for a team of analysts or data scientists to perform the above steps and some sort of automated model development framework must be used. What are the frameworks that allow for this? 

Can you clarify what you're trying to predict with these responses? My initial reaction is that with open-ended surveys you'll have a tough time implementing classification algorithms. Open-ended-ness means you don't have a finite feature space and thus you can't do the usual transformation of responses into a feature matrix. However, there may be other ways to make this work. If you have 5 questions, for instance, you may be able to use sentiment analysis or other methods to come up with metadata about the open ended response that can help you design a classification scheme. 

Small sample size is probably the concept you're looking for. A common failure in statistics is trying to draw conclusions from data that isn't "big" enough to accurately represent the underlying distribution. It's worth noting that "small data" increases your chances of overfitting - which is another way of saying that your model is weak to outliers and noise. It's also worth noting that bootstrapping, simulation, and duplication might help (but not always). There may be some other niche term for this, but if you relate it to basic stats there's a high probability everyone in the room will understand what you're talking about. 

In line with the above comment, I suggest you try a rule-based approach. For each server you have, query its services. If all of the services are down on a server, then you have a server problem. For each service, if no server reports that the service is running, then you have a problem with the service. In the case that it's both, you'll get notices for each and then be able to go in an inspect what's going on with any of the constituents. The cost, maintenance, and risk of bad results using an ANN model all exceed the simple, rule-based solution and your boss will probably pat you on the back for just doing what makes sense here. If you're really serious about keeping your servers and processes functional, I suggest you invest in a APM service that gives you reliable, real-time notifications on what's going on in your production environment. In the case that you're just trying to learn how ANN work - try a different problem. Any well-known dataset for classification or anomaly detection will provide you with a lot more insight into how ANN work than a custom dataset, which can be horribly hard to coerce into an effective learning scheme.