So, you were partially right: they just average the independent results over video frames. Probably for performance reasons, they chose to average the first 100 frames. They do describe that increasing this to the first 1000 frames increases performance from 95.12% to 95.18%, which is not significantly more. 

As you correctly say, we calculate the probability of a hidden unit $h_j$ being one and then make it binary. That probability is given by $$p(h_j=1) = \sigma\left(b_j + \sum_{i=1}^V w_{ij}v_i \right)$$ where $\sigma$ is the sigmoid function, $b_j$ is the bias of hidden unit $h_j$, $V$ is the number of visible units, $v_i$ is the (binary!) state of visible unit $i$, and $w_{ij}$ are the weights. So, your MATLAB code for obtaining the probabilities is something like this (we write the sum implicitly by making a matrix multiplication): 

The objective function they use to train the CNN minimizes the squared L2 distance (i.e. the squared Euclidean distance) between two similar (positive) images and simultaneously maximizes the distance between two different (negative) images. That means, the (squared) Euclidean distance between two representations is a measure of their similarity. Then, recognizing a face in a new image is as simple as 1) running it through the CNN and 2) finding its nearest neighbors with a KNN algorithm. The last paragraph was only about images - in the Youtube Faces DB, we are handling videos of different persons. In section 5.7 of the paper, they describe how they evaluate performance: 

There are many different publicly available datasets out there, and most come with a paper describing how the dataset was acquired. Almost nobody takes a camera and starts taking thousands of pictures themselves. You may find some inspiration by looking at those papers and adapting their methods for finding images. A very popular way is to download the images from Flickr: This is a photo platform where users share their photos and add comments or tags, describing the contents of the images. Flickr also has an API to find and download images. A couple of test queries show that there are thousands of photos available: 

which would create vector which contains either or , drawn randomly for each probability in . As you probably have noticed, nobody does that! E.g. describes it in his Practical Guide to Training RBMs, as 

However, this is not a clean high-quality dataset: it includes old models, wrong tags, photos from the interiors, and so on. Still, it might be a good starting point to acquire huge numbers of images. Dataset Cleanup Maybe you can live with a couple of low-quality images, but I guess the better way is to clean the dataset up. There are a lot of different possible steps - some may not be needed in your case, and you might need other or additional steps: 

When you are unsure about how something (e.g. RBMs) should really be implemented, it is often useful to look at the code of others. G. Hinton himself has published a MATLAB-script (here) which demonstrates the training of an RBM. There, you can see that for each mini-batch, he does the positive phase, then the negative phase, and finally updates the weights - and that's it. So he doesn't iterate between the visible and hidden states. However, this is not the full truth: for the weight updates we need to know the probability $p(v,h)$. This is very complicated to calculate, as it would contain a sum over all possible states of the RBM. There is a "mathematical trick" called Gibbs sampling: it allows us to iterate back and forth between visible and hidden units to calculate this probability $p(v,h)$. But: for the result to be correct, we have to iterate forever, which is not really practical. So what Hinton proposed is to iterate for only 1 step instead (this is $CD_1$), so he only goes back-and-forth once. But, you can also iterate any number of times $k$, which is denoted by $CD_k$. While for Hinton's $CD_1$, you would do 

A scheme like this should work. Namely, you pass training data directly into the the learner widget and pass its trained Model into Predictions widget along with labeled test data. 

You can use with . It will return a a matrix some models can work with. Your matrix will indeed be 100e6 columns wide, but not densely populated won't take a lot of RAM. 

In Orange documentation - Loading your Data, it says to prefix your column with to be marked as instance weights column. 

Perhaps have a look at MDS (multi-dimensional scaling) widget, Line Chart widget (from Timeseries add-on), or Network Explorer widget (from Networks add-on). 

As far as I know, Orange supports advanced annotation with a three-line header in both CSV and Excel files. So if you have a discrete attribute that gets marked as string, you should annote it as on the second header line, or prefix the attribute name on the first line with . When no advanced annotation is provided, it seems Orange's heuristic treats attributes containing general strings (as opposed to numbers) as discrete only if they contain fewer than approximately $\sqrt{|data|}$ unique values. 

I don't think Orange was ever intended to be used that way, but if you can convert your data into , you might be able to instantiate the widgets you need with it. See the block at the bottom of most widgets for some example. 

You can use an online epoch converter to confirm. Apparently, Predictions widget treats datetime variables as continuous variables (which is ok) but fails to present them nicely. You should probably submit a bug report about it. 

If you can find a way to convert python code into JavaScript, Highcharts JS might support a large-enough heatmap: $URL$ 

Ad 1. Assuming the measurements at any given time are normally distributed (they shape approximately a bell curve), you could use simple standard deviation to detect outliers. Specifically, for any given time, you can calculate the mean and standard error. Then you calculate the mean sans outliers by taking into account only the measurements that fall at most some pre-set distance from the mean (e.g. given normal distribution, 68% of measurements fall within one standard deviation from the mean). Pseudo-code example: 

It appears those numbers are Unix timestamps. The numbers you quote correspond to the following human-readable dates in ISO format: 

Not only the naming has changed, also the individual widget settings and layouts are completely different. Some Orange 2 widgets are not even available in Orange 3 yet, or have been consolidated into other ones. I'm afraid Orange 2 workflows are just not compatible with Orange 3. 

If you are mostly stitching up together calls into other software, like unix utilities (awk, grep, sed ...), python, and matlab scripts, bash is just fine or possibly even best for the job to construct simple pipelines and workflows. It's easy in bash to read user input, store it in variables, then launch other software depending on the set variables. It's perfectly fast enough for that, and nothing else gets any easier. If you, however, were to use bash for preprocessing itself, like looping through files line by line, packing and unpacking tab-separated values into arrays etc., that would be excruciatingly slow and not recommended. 

Say $x$ is sex and $y$ is employed. You can restate your $p(y|x)$ as: $$ p(employed=1~|~sex=male) $$ Now, it's a simple matter of counting the relevant examples or looking up in the appropriate contingency table. 

Don't compress files that are already compressed (like how JPEG/PNG images or video files are). Their inherent compression is usually good enough, and you only trade some 5% in lower compressed size for a much lengthier and often non-seekable decompression that results in using twice the disk space. If you need to batch the files together, just use tar. 

You are making a mistake regarding what is given: during training, you don't have a radius $R$. You have the coordinates $\vec{x}$ and the label $y$ for each point: $$ \vec{x}_1 = [-2;1] \quad \vec{x}_2 = [1;2] \quad \vec{x}_3 = [0;-1] \quad \vec{x}_4 = [1;0] \quad \vec{x}_5 = [1;1] $$ and $$ y_1 = T \quad y_2=T \quad y_3=T \quad y_4=S \quad y_5=S $$ with that, your "trained" centroids are $$ \mu_T = \frac 13 [-2 + 1 + 0; 1 + 1 - 1] = \left[-\frac 13, \frac 13\right] $$ $$ \mu_S = \frac 12 [1 + 1; 0 + 1] =\left[1, \frac 12\right]$$ You calculate these centroids before you get any test values. Then, for testing, for your observed point $\vec{x} = [1,1]$, you calculate the Euclidean distance between the point $\vec{x}$ and the centroids $\mu_T$ and $\mu_S$: $$ \| \vec{x} - \mu_T \| = \left\| [1;1] - \left[-\frac 13; \frac 13\right] \right\| = \left\| \left[\frac 23; \frac 43\right] \right\| = 1.49 $$ $$ \| \vec{x} - \mu_S \| = \left\| [1;1] - \left[-1; \frac 12\right] \right\| = \left\| \left[0; \frac 12 \right] \right\| = 0.5$$ Finally, the term $\hat{y} = \arg\min_{l \in \mathbf{Y}} \|\vec{x}-\mu_l\|$ is used to find the estimated class $\hat{y}$ (the hat symbol is to denote that this is an estimated $y$, not one we knew before.). $\arg\min$, means that you find the minimum value - which is 0.5 in our case, and chose which "argument", i.e. which class, leads to that minimum value. In our case, the class which leads to the minimal distance is $S$, so the result is $\hat{y} = S$, and our test point is a square. 

As you know, a deep belief network (DBN) is a stack of restricted Boltzmann machines (RBM), so let's look at the RBM: a restricted Boltzmann machines is a generative model, which means it is able to generate samples from the learned probability distribution at the visible units (the input). While training the RBM, you teach it how your input samples are distributed, and the RBM learns how it could generate such samples. It can do so by adjusting the visible and hidden biases, and the weights in between. The choice of the number of hidden units is completely up to you: if you choose to give it less hidden than visible units, the RBM will try to recreate the probability distribution at the input with only the number of hidden units it has. An that is already the objective: $p(\mathbf{v})$, the probability distribution at the visible units, should be as close as possible to the probability distribution of your data $p(\text{data})$. To do that, we assign an energy function (both equations taken from A Practical Guide to Training RBMs by G. Hinton) $$E(\mathbf{v},\mathbf{h}) = -\sum_{i \in \text{visible}} a_i v_i - \sum_{j \in \text{hidden}} b_j h_j - \sum_{i,j} v_i h_j w_{ij}$$ to each configuration of visible units $\mathbf{v}$ and hidden units $\mathbf{h}$. Here, $a_i$ and $b_j$ are the biases, and $w_{ij}$ are the weights. Given this energy function, the probability of a visible vector $\mathbf{v}$ is $$p(\mathbf{v}) = \frac 1Z \sum_{\mathbf{h}} e^{-E(\mathbf{v},\mathbf{h})}$$ With that, we know that to increase the probability of the RBM generating a training sample $\mathbf{v}^{(k)}$ (denotes the $k$-th training sample), we need to change $a_i$, $b_j$ and $w_{ij}$ so that the energy $E$ for our given $\mathbf{v}^{(k)}$ and the corresponding $\mathbf{h}$ gets lower. 

Important note: all photos are property of their respective owners. All images on Flickr have specific license conditions, which you can also query through the API. A list of the available licenses on Flickr is available on their website. You have to make sure you don't infringe the copyrights of the respective owners. Especially if your work is commercial, this complicates things. References [1]: Gordo, A., Almazan, J., Revaud, J., & Larlus, D. (2016). Deep Image Retrieval: Learning global representations for image search. arXiv: 1604.01325.