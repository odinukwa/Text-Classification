I'm trying to setup an AlwaysOn AG with T-SQL commands but my secondary is always in a disconnected state (when I failover, my old primary becomes disconnected). This is not an firewall/network issue, I'm able to send messages from SQL1 to SQL2 with a TCP-sender/receiver. The option New Availability Group... in SMSS also generates the same problem. However when I'm creating an AG with New Availability Group Wizard... it works perfect. This is the T-SQL statement I'm using: 

The second method is twice as fast as the first one. But the disadvantage is that after a few years the archive disk will contain a lof of folders/partitions containing the filestream data + database needs to be offline. As with the first method there is only 1 folder/partitions (or I can split that big partition by year). I hope my explanation is clear enough because it was hard to put all this in writing. What is the best method to accomplish the archiving or am I missing something? 

Assuming you mean that the database is "missing" logs from P->R, and that your primary database no longer has those logs either, then the answer is that you'll have a functional database (upon standby activation as a primary) but you will be missing data from P-> the Activation point. The thing to remember here is that your standby is only as good as your last archive log that was fully applied. Without intervention on your part, your standby database is essentially frozen in time, and will never advance. If you're missing archive logs, and they are truly unrecoverable (did you check storage backups? Backups to tape?), then your standby, IMO, is worthless as a production level standby. You, and your business, may have a different opinion based on your usage of the database in question. But if this was truly a production level standby database that is meant to take over for production in a disaster scenario, personally, I would rebuild the standby immediately. 

We have a SQL Server 2016 SP1 CU7 Enterprise server where we enabled the query store on a database. This is a highly used database.The has been set to 200MB and is 1 day. I've used to check the properties of the query store and everything works perfect as long as stays under the 200MB. I've set to Auto but no cleanup happens, normally a cleanup should be triggered when the query store is 90% full. What happens next is that the query store goes into readonly mode because there is no available space left. I've tried changing to Auto but nothings changes.If I then change to 30 days the changes to READ_WRITE and the query store starts capturing again. The just becomes higher than the and after a while the goes back to READ_ONLY. Can anybody explain this behavior? 

I'm a newbie regarding the query store and have some problems understanding what I'm seeing. We have a third-party application (running on SQL Server 2016 Enterprise SP1 CU7) that uses the query-hints and . When I do some monitoring in the querystore I see that some queries has multiple plan ids in the plan summary window. Why can a query that uses and have multiple plans (sometimes completely different, sometimes the same)? And the second question, if there are plans that look exactly the same (same physical operators, same set options, same queryhash), how can one plan have a missing index and the second plan not? This are two anonymised plans: Plan 1 Plan 2 

This gives me a block-for-block recreation of the production level database, and depending on how many threads you give RMAN, can be completed pretty quickly. From there, I have a set of environment specific scripts I run that change passwords, null out email addresses, etc in the DEV database to configure it the way it needs to be. 

As someone who spent 4 years of his life dealing with converting CLOB type data from legacy systems into Oracle, I feel your pain. I highly recommend using an Oracle built function, such as to go through and scrub all of the garbage characters from CLOB type fields. Additionally, remember that most systems consider a "return" to be a new line and carriage return in unison, so you might have to scrub ascii characters below 33. IIRC the new line/carriage return combo was char(10)char(13). You can easily google a list of of the "invisible" ascii characters, and hunt down the ones you want removed. HTH. 

I need to rebuild some big indexes and I'm doing some tests with the various options (sort_in_tempdb, maxdop, online) of the statement on an test index with 4 levels and 800000 pages on leaf level. I noticed when I'm running the statement with the intermediate pages (level 1) of my index are higher fragmented as before (89% in stead of 3%). The intermediate pages only get defragmented when I'm setting . With the options the level 2 fragmentation jumps from 0 to 100. This are the statements that caused an increase of fragmentation on level 1: 

Somebody told me there's a difference in the internal working (I not mean a difference in features or supported resources) of SQL Server Enterprise and SQL Server Standard. One difference I remember was that the Enterprise version uses multiple threads for certain operations and the Standard version only one (I forgot the details). I've searched the web for an overview of the differences (+ some explanation) but I was unable to find such a list. I now wonder is there a difference in the internal working between the two editions? 

I know you already accepted the answer, but it seems to me, since you're constantly aging out the active rows, you could simply create a function based index on the column. It'd be something like this: 

It's not necessarily the number of transactions, but the timeout. The parameter is set to 60 seconds by default. The purpose of this parameter is to avoid having distributed transactions in a long running wait status while something else is performing work on that row; the transaction will wait 60 seconds, then Oracle kills it. You can modify this parameter (requires an instance restart) to whatever you want (in seconds). 

It looks to me like the string you're passing in is too long for the UTL_RAW procedure you're trying to call. From the Oracle documentation: 

found here: $URL$ I think you may need to utilize the DBMS_LOB package. Documentation found here: $URL$ HTH. 

After some more research I discovered that when I create an AG with autoseeding SQL Server creates a new worker (that executes the VDI_CLIENT_WORKER command) for each scheduler. To remove those workers I need remove every 'autoseeded' AG and restart the SQL Server service. If I then create the AG with full backup/restore none of these workers are created. 

I've setup an AlwaysOn AG on SQL Server 2016 SP1 Standard. Then I created an AG and added a database with autoseeding (synchronous mode). I used SSMS 2017 to create my AG and to add the database. Everything works fine. But when I check the wait stats I get waits of type VDI_CLIENT_OTHER (80%) on the primary with an average resource time of 42 seconds. After some research I found out that the waits are generated by 4 sessions that execute the command VDI_CLIENT_WORKER. As I understand the wait means that a thread is waiting for work when seeding a new AG. But what I do not understand is why I have those waits because my AG is ready and why do I have 4 sessions that execute the VDI_CLIENT_WORKER command? I found out that each scheduler has one VDI_CLIENT_WORKER Can somebody try to explain what VDI_CLIENT_WORKER-command does and how can I solve the problem of the many VDI_CLIENT_OTHER waits? 

Personally, I would start with DBA_HIST_ACTIVE_SESS_HISTORY, and look at all statments that contain that type of hint. From there, you can pull the index name coming from that hint, and then do a lookup on dba_indexes to see if the index exists, is valid, etc. You should be able to do this via PL/SQL if you want to make it really fancy and do it all in one step, otherwise a few pieces of SQL and a spreadsheet application can be your friend. 

In your example, you have Autoextend on with 4M as the next extension, but you also have MAXSIZE set to 8G, which means that Oracle is being told that once a tablespace reaches 8G, it will no longer automatically add space. This is for safety purposes, as sometimes administrators want to know when a datafile is growing very large very quickly. In your case, you could issue , which would allow this tablespace to automatically extend to that maximum size, depending on how much space you have on your server! If you run out of actual physical disk, nothing in the Oracle database will protect you from this. Additionally, you'll still want to periodically monitor the size of this tablespace, as you will (theoretically) still eventually reach the maximum size of the autoextend. Even if you set the maxsize to unlimited, there are still factors that determine the maximum size of a tablespace that are out-of-scope for your particular question. Just know that there is always a maximum upper limit to a tablespace, and someone will have to monitor it to take appropriate action. 

A while ago a consultant said that I should set processor affinity when I'm running SQL Server on VMWare. The advice was to disable CPU0 so it was free for the OS. When I read the "Architecting Microsoft SQL Server on VMware vSphere"-PDF on the site of VMWare I find following: 

I've a database on SQL Server 2016 SP1, in the db there is only one table with a Filestream column and a few columns of type . The table is partitioned and every month a new partition is added by scripting. Each filestream partition has its own filegroup, while the other partitions are in the primary filegroup. New data is inserted in the new partition + old data is not be updated. The partition function is created with . I want to hold one year of data (12 partitions) on SSD and archive the rest to slower storage. This will be an automatic process (SQL Job/Scheduled task). 

I completely agree with dartonw's comments about reading through the guide. That's the only real way to start understanding the concept of indexing as it relates to Oracle and the Optimizer. One thing I will add to it though is this - Philosophically, as a DBA, I rarely decide what columns to index on my own. What I mean by this is that I go back to the developers and ask them for the use cases of searching that table. If the table has five columns, and the dev tells me "we'll be joining this table to that table by columns a and b to retrieve columns c,d,e", that tells me that I need the index on columns a and/or b. The Developers/business users and their SQL should dictate what columns are indexed, not what you have read about in a book. Each app is different, and your indexing should be built specifically around data storage and retrieval of that app. I hope that makes sense.