It sounds like this class would be exactly MA. The witness could be the results of pre-processing (which is of polynomial size). The probabilistic verification procedure would then be to simulate the protocol, including the multiple provers (who are polynomial-time given the results of pre-processing). Russell Impagliazzo 

This result can be proved using a simple counting argument. Consider a random function applied to the first $k$ bits of the input. This function almost certainly has circuit complexity $(1+o(1))(2^k/k)$ by Riordan and Shannon's counting argument, and matching upper bounds. Thus, picking $k$ so that $2g(n) < 2^k/k < f(n)/2$ we can distinguish size $g(n)$ from size $f(n)$. Note that the functions in question won't necessarily even be computable, but we can put them in the exponential time hierarchy by standard techniques (as long as we can compute the right value of $k$). We of course cannot prove any bound greater than $2^n/n$, because that is the worst-case circuit complexity of any function. Natural proofs do not apply for this type of argument, because the property in question is ``not having a small circuit'', which is not easily computable from the truth table of the function (presumably). It is not clear how low in complexity classes this type of counting can go. Is there any reason why we can't use a counting argument to prove lower bounds for $NE$? Not that I know of. 

As far as I have understood, "uncomputation" in quantum computing is a way to restore the working memory to its initial state, while keeping the result of the computation in another register. This trick is usually explained as follows: 

I don't understand the convexity argument here. Assume that the acceptance probability is given by $||\Pi |\psi\rangle ||^2$ for some projector $\Pi$. I know that $||\cdot||^2$ is convex but I don't see how to use it here. For instance, if $|\psi\rangle = \sum_i \alpha_i |\psi^i_1\rangle \otimes \cdots \otimes |\psi^i_r\rangle$ for orthogonal unit vectors $|\psi^i_1\rangle \otimes \cdots \otimes |\psi^i_r\rangle$ (thus $\sum_i |\alpha_i|^2 = 1$) we would like to have $||\Pi |\psi\rangle ||^2 \leq \sum_{i,j} |\alpha_i|^2 \cdot ||\Pi |\psi^i_j\rangle ||^2$ but this is not what convexity means... 

Step 1: Just plugging in Stirling and doing some cancellation/rearranging, \begin{align} p_{n,k} &:= \frac{{n \choose k}}{2^n} \\ &\sim \frac{n^n}{k^k (n-k)^{n-k}}\frac{1}{2^n}\sqrt{\frac{n}{2\pi k (n-k)}} . \end{align} This won't be very tight for every term in the sum, but I think it will be asmyptotically tight towards the middle, which is all that matters since all the probability is in the center. We can rewrite the entropy function as follows. It's just some arithmetic to combine the logarithms. \begin{align} H(k/n) &= \frac{k}{n}\log\left(\frac{1}{k/n}\right) + \frac{n-k}{n}\log\left(\frac{1}{(n-k)/n}\right) \\ &= \log\left(\frac{n}{k^{k/n}(n-k)^{(n-k)/n}}\right) . \end{align} So, using Sterling's approximation above, the logarithm of a probability term is \begin{align} \log(p_{n,k}) &= \log\left(\frac{{n \choose k}}{2^n}\right) \\ &\approx n H(k/n) - n + \log\left(\sqrt{\frac{n}{2\pi k(n-k)}}\right) . \end{align} Step 2: \begin{align} \mathbb{E}[H(k/n)] &= \sum_{k=0}^n p_{n,k} H(k/n) \\ &\approx \sum_{k=0}^n p_{n,k} \left(\frac{\log(p_{n,k})}{n} + 1 - \frac{\log\left(\sqrt{\frac{n}{2\pi k (n-k)}}\right)}{n}\right) \\ &= 1 - \frac{H(Binom(n,0.5))}{n} - \frac{1}{n}\sum_{k=0}^n p_{n,k}\log\left(\sqrt{\frac{n}{2\pi k(n-k)}}\right) . \end{align} Here, $H(Binom(n,0.5))$ is the entropy of the Binomial distribution for $n$ coin flips and $p=0.5$, which by wikipedia is $\log\left(\sqrt{\pi e n / 2}\right) + O(1/n)$. Step 3: Now, we just need to approximate the third sum. I will take a very rough approximation (feel free to do better, but it probably doesn't gain much). All the probability mass is concentrated on $k = \frac{n}{2} \pm o(n)$. So approximate this sum (which is an expectation) by its value on the term $k=\frac{n}{2}$, when it is $\log\left(\sqrt{\frac{2}{\pi n}}\right)$. So now, we get \begin{align} \delta_n &\approx \frac{\log\left(\sqrt{\pi e n/2}\right)}{n} + \frac{\log\left(\sqrt{2/\pi n}\right)}{n} \pm O\left(\frac{1}{n^2}\right) \\ &= \frac{\log\left(\sqrt{e}\right)}{n} \pm O\left(\frac{1}{n^2}\right) . \end{align} 

$\mathcal{Q}_*$ is a quantum circuit that takes some witness $|z\rangle$ as input, and outputs $0$ or $1$. It can be thought as a circuit for QMA, for which the real input $x$ has been fixed and we want to know if there exists a witness that is accepted with high probability (the idea of this paper is to only try with pure states witnesses $|z\rangle$). In this context, I don't understand why the "uncompute garbage" step restores the memory. I think that the register containing the output $b$ is CNOT in an extra register (which will be measured later), but since it's usually a superposition of $|0\rangle$ and $|1\rangle$ I don't understand why the uncomputation would work. 

Nash Equilibria are uncomputable in general. An $\epsilon$-Nash equilibrium is a set of strategies where, given the opponents' strategies, each player obtains within $\epsilon$ of the maximum possible expected payoff. Finding an $\epsilon$-Nash equilibrium, given $\epsilon$ and a game, is $\mathsf{PPAD}$-complete. Going strictly by the definitions, there seems to be no particular reason to believe that the strategies of a given $\epsilon$-Nash equilibrium are anywhere close to the strategies of any Nash equilibrium. However, we often see the literature somewhat sloppily use a phrase like "approximately compute a Nash equilibrium" when it means to say "compute an approximate-Nash equilibrium". So, I'm wondering when the second implies the first; that is, for what games might we expect $\epsilon$-Nash equilibria to be "close" to Nash equilibria? 

Arxiv is not very useful for computational complexity, although certain subfields such as quantum computing do use it. On the whole, there is no quality control at all, and many of the papers listed as belonging to computational complexity are either incorrect, or only marginally related to the field. ECCC (Electronic Colloquium on Computational Complexity) reports are usually much more germane, and mostly by experts in the area. There are only a few each week, and cover a wide variety of topics. So I recommend looking at new ECCC reports, at least glancing at abstracts, and perhaps reading more if they look interesting. Another resource you can use is Oded Goldreich's list of ``Papers I find interesting'' or something like that, off his homepage. He gives a summary and discussion of stuff he likes. The additions are irregular, and seem to average one or two a month. You are also welcome to come to talks at IAS any time. I could add you to the mailing list if you are not already on it.... Russell Impagliazzo 

Postscript. An expert I asked about this mentioned "On Medium-Uniformity and Circuit Lower Bounds" (pdf), Santhanam and Williams 2013, which maybe is the most closely related work, but it proves lower bounds (that poly-time-generatable circuits are not too powerful). I'd be interested in any other related work! 

I haven't seen this written down, but I'd define an algorithm as "a series of well-defined steps for solving a problem". By well-defined, I mean that each step is either an algorithm itself (like a subroutine), or else a simple and completely mathematically unambiguous operation. A more precise mathematical definition can follow by defining your model of computation, which mathematically specifies exactly which steps are allowed and in what sequence. For example, a Turing Machine formally specifies what steps it may take and the order it takes them in. Similarly for, say, a pushdown automata, and close to similarly for many programming languages. One cannot have a universal definition that encompasses all possible models of computation, unless one somehow could formalize the notion of "all possible models of computation", which seems impossible. For example, tomorrow I may invent a model of computation involving dropping beads into buckets, and as long as I formally specify the model of what steps can be taken and in what order, one could write algorithms for my new type of computer. The Church-Turing thesis is related: It is a proposed natural law stating that any such model of computation could be "simulated" by a Turing Machine. So in that sense, my algorithm for my new type of computer would be no more "powerful" than algorithms for Turing Machines. Because of this, if one accepts the Church-Turing thesis, then it is without loss of generality to define an algorithm as a type of Turing Machine. (But on the other hand, in theoretical CS we often think of models beyond Turing Machines, such as TMs with access to randomness, access to oracles for the halting problem, etc.) 

I have seen many papers in which quantum measurements are assumed to be replaced by unitaries. See this quotation from [KW00] for instance: 

After the first Hadamard has been applied, $x_1$ contains the result of the computation (it is the same as $|f(x)\rangle$ previously, but now it is a superposition). It is CNOT with the extra register $x_2$ and then another Hadamard is applied. The final state is $(|00\rangle+|01\rangle+|10\rangle-|11\rangle)/2$. Obviously, the first qubit has not been restored to its initial state. My first question is: am I right when I say that the uncomputation trick only works when the register that is copied is not in superposition? My second question is about the following algorithm taken from this paper from Scott Aaronson: 

Well, at least $\#\mathsf{P}$-hard. Given a SAT formula, construct a graph with two vertices, $v_x$ and $v_x'$, for every possible assignment of variables $\vec{x}$. If $x$ is a satisfying assignment for the formula, draw an edge between $v_x$ and $v_x'$; these are the only edges. It is easy to construct the circuit for this graph from the SAT formula, and the number of odd vertices is exactly twice the number of satisfying assignments. 

I will attempt to atone for my previous error by showing something opposite -- that $\tilde{\Theta}\left(\frac{1}{\epsilon^2}\right)$ samples are sufficient (the lower bound of $1/\epsilon^2$ is almost tight)! See what you think.... The key intuition starts from two observations. First, in order for distributions to have an $L_2$ distance of $\epsilon$, there must be points with high probability ($\Omega(\epsilon^2)$). For example, if we had $1/\epsilon^3$ points of probability $\epsilon^3$, we'd have $\|D_1 - D_2\|_2 \leq \sqrt{\frac{1}{\epsilon^3} (\epsilon^3)^2} = \epsilon^{3/2} < \epsilon$. Second, consider uniform distributions with an $L_2$ distance of $\epsilon$. If we had $O(1)$ points of probability $O(1)$, then they would each differ by $O(\epsilon)$ and $1/\epsilon^2$ samples would suffice. On the other hand, if we had $O(1/\epsilon^2)$ points, they would each need to differ by $O(\epsilon^2)$ and again $O(1/\epsilon^2)$ samples (a constant number per point) suffices. So we might hope that, among the high-probability points mentioned earlier, there is always some point differing "enough" that $O(1/\epsilon^2)$ draws distinguishes it. 

I think I finally figured out what the answer was :) It seems that the "uncomputation trick" was first used for quantum computing in this paper: Strengths and Weaknesses of Quantum Computing (proof of Theorem 4.14). So basically I was right when I said that it only works for "exact" computation (i.e. the output register that contains $f(x)$ is not in superposition) but it can also be used when the error probability of the algorithm is low (in this case the uncomputation is not perfect, but the amplitude of the error term is low). Let me explain how it works: we have a quantum algorithm $\mathcal{Q}$ that takes an input $|0\rangle |0\rangle$ and produces a state $|\phi\rangle = \sum_y \alpha_y |y\rangle |b_y\rangle$ where $y$ is the working space and $b_y \in \{0,1\}$ is the output register (the result of the computation). If the computation is "exact" then all the $b_y$ are equal to a same value $b$, i.e. the state is $\sum_y \alpha_y |y\rangle |b\rangle$. We now add an extra register in which we CNOT the result: $\sum_y \alpha_y |y\rangle |b\rangle |b\rangle$. Finally, we can apply $\mathcal{Q}^{-1}$ on the first two registers so as to obtain $|0\rangle |0\rangle |b\rangle$. The uncomputation is exact. Assume now that there is some error probability $\varepsilon$, i.e. the probability of correctly measuring $b$ is $\sum_{y : b_y = b} |\alpha_y|^2 = 1 - \varepsilon$. Let's add the extra register to CNOT the result and denote $|\phi\rangle = \sum_y \alpha_y |y\rangle |b_y\rangle |b_y\rangle $ (in which some $b_y$ are different from $b$) and $|\phi'\rangle = \sum_y \alpha_y |y\rangle |b_y\rangle |b\rangle$. If we apply $\mathcal{Q}^{-1}$ on the first two registers of $|\phi'\rangle$ we obtain $|0\rangle |0\rangle |b\rangle$ which is what we want. Unfortunately, we only know how to compute $|\phi\rangle$ (in which the last register is a superposition of $b$ and $1-b$). However, note that $\langle \phi | \phi' \rangle = \sum_{y : b_y = b} |\alpha_y|^2 = 1-\varepsilon$. Consequently, the inner product between $\mathcal{Q}^{-1} |\phi\rangle$ and $\mathcal{Q}^{-1} |\phi'\rangle = |0\rangle |0\rangle |b\rangle$ is also $1-\varepsilon$ (inner product is preserved by unitaries), which means that the amplitude of $|0\rangle |0\rangle |b\rangle$ in $\mathcal{Q}^{-1} |\phi\rangle$ is $1 - \varepsilon$. Thus, if $\varepsilon$ is small, then $\mathcal{Q}^{-1} |\phi\rangle$ is close to the correct uncomputed state $|0\rangle |0\rangle |b\rangle$. The uncomputation is almost exact. I think this is the underlying reasoning in Aaronson's paper when he talks about "uncompute garbage". His circuit $\mathcal{Q}^{\star}$ has a very low error probability (due to an amplification step he performed before) and thus he does not care about the error part in the uncomputation. There is also this paper: Quantum Entanglement and the Communication Complexity of the Inner Product Function (section 3) for another detailed use of uncomputation. 

The correctness and confidence bounds ($1-e^{-\Omega(M)}$) depend on the following lemma which says that all of the deviation in $L_2$ distance comes from points whose probabilities differ by $\Omega(\epsilon^2)$. 

EDIT: this is incorrect! See the discussion in the comments -- I will point out the flaw below. I think we can say that $\frac{1}{\epsilon^4}$ are required. Set $n = \Theta\left(\frac{1}{\epsilon^2}\right)$. Let $D_1$ be the uniform distribution (probability of each point $= \Theta(\epsilon^2)$) and let $D_2$ differ from uniform by an additive amount $\pm \Theta\left(\epsilon^2\right)$ at each point. Check that the $L_2$ distance is $\epsilon$. So we have to distinguish an $n$-sided fair coin from an $n$-sided $\Theta(\epsilon^2)$-biased coin. I think this should be at least as hard as telling a $2$-sided fair coin from a $2$-sided $\Theta(\epsilon^2)$-biased coin, which would require $\Theta\left(\frac{1}{(\epsilon^2)^2}\right) = \Theta\left(\frac{1}{\epsilon^4}\right)$ samples. Edit: this is incorrect! The coin is additively $\epsilon^2$-biased, but it is biased multiplicatively by a constant factor. As D.W. points out, that means that a constant number of samples per point distinguishes $D_1$ from $D_2$.