Query will only continue for duration until it finishes its task, unless you run a never ending loop. The NOLOCK only ensures that query is minimally blocked and also note that when using NOLOCK if required other locks like schema stability are taken 

And below is what output looks like. Attaching only relevant portion of it. The query produced 50 rows since we inserted 50 rows in our dbo.HeapTest table. 

Weekly full backup with no recovery 10:05PM Differential backup with no recovery All log backups after this differential backup with no recovery and finally tail log backup with stopat command with recovery 

To start with you have SQL Server 2012 patched to SP1. I would urge you, ASAP, patch SQL Server with SQL Server 2012 SP2 There has been some prominent memory related fix in SQl Server 2012 Sp2. Please read one of the KB Article documenting OOM error fix. If you look closely the fix is also for 

If you see memory grants pending in buffer pool your server is facing SQL Server memory crunch and increasing memory would be a good idea. For memory grants please read this article: $URL$ 

I am quoting a paragraph Appendix A: Using Management Studio to Change Data Types from SQL Server 2005 document on Impact Of Changing Collation and Data types 

You can use DMV sys.dm_os_schedulers to get this information. The column you have to refer is . As per BOL it means 

Here is one more reason why you cannot blindly rely on DMV sys.dm_index_usage_stats. sys.dm_db_index_usage_stats does not get updated after an index is used for just the statistics associated with the indexed columns. Paul showed this in his link. What sys.dm_index_usage_stats DMV does not tells you You would also like reading This article about index usage stats DMV Moreover IMO I dont think by using DMV there is PERFECT way to find out when was table last used. I Belive using Profiler trace or Extended event trace would be more better option. Although profiler can cause load but server side trace is a good option. 

If windows is not sending the correct notifications to all listening processes at the right moment and thresholds. If SQL Server is not responding fast enough to the low memory resource notification from Windows. When low physical memory notification is received by SQL Server it will scale down its memory usage by trimming internal caches. This effect is applied for 5 seconds and then paused for 1 minute. This is to avoid any hungry/faulty application to consume all the memory making SQL Server to scale its usage continuously. If low memory conditions still exist after 1 minute pause, the effect is applied again. So if there is physical memory pressure even after SQL Server scaling its usage for 5 seconds windows will still page out SQL Serverâ€™s working set. Conditions in Windows where working sets of all processes are trimmed. Windows might decide to trim a certain percentage of working set of various or specific processes 

You should have waited to restart the machine after it prompted for reboot. I am sure it would have prompted but you would have selected for reboot then only it went ahead and rebooted. Avoid rebooting SQL servers when restore process is going on this would disrupt restore process and you would have to start all over again, which is waste of time. 

I don't see any logic in putting alert but If you want to just get information about SQL Server memory utilization you have DMV sys.dm_os_process_memory. This DMV was introduced in SQL Server 2008 so you cannot use this if you have SQL Server 2005. You can put this in SQL Server agent with condition like it If > 450 G you call stored proc and send mail to who ever you want. I am sure you can create that simple query. If you cannot let me know. 

Please note that restore 'verifyonly' does not checks consistency of backup completely only a successful restore will guarantee that you backup is in consistent state. There is a option to use Continue_after_error can you please use that in your restore script. 

On the other hand if you just want to configure SQL Server failover cluster it does not necessarily require a shared disk. Please read Configure Failover cluster with SMB You can also use UNC path for database in failover cluster. Please also read Brent's Article When you use SAN for shared storage they are mostly robust and have mechanism for fault tolerance so you should not worry about SAN storage as being single point of failure. The RAIDS configured provide you good level of tolerance 

where session_id is SPID of request running on sql server and whose memory you want to track. Point to note here is the value may change while query is executing and is only tracked for period of query execution. If query has finished executing the value will not be tracked and you might not see any value in the column. Other way is via DMV sys.dm_exec_query_memory_grants. The columns to look for is and . Again this DMV will track information only if query is running or for the duration of the query. 

As you already know there is No general formula to calculate max server memory you can do some quick maths and reach to a value but still you would need help of Perfmon counters at last to monitor memory usage and change accordingly. I know below general formula and I use it as well. I learned this formula from This Link For SQL Server 2005 to 2008 R2 Please note from SQL Server 2005 to 2008 R2 max server memory only controls buffer pool. So max server memory configuration is bit tedious here and involves few calculations 

Yes number of connections are limited by the OS you are using. Please refer to this thread which says only 20 connections are allowed on windows 7. I am not sure about other client OS but if you use server OS there is no such restriction. Unlimited for practical purpose. There is no restriction as such if your OS is Windows server 2012. 32767 databases is what maximum capacity specification states. But you wont be able to go far than 10 due to express limitation on memory and CPU 

RAMMAP is not the tool of your interest again. If you want to check memory utilized by SQL Server 2008 and above you can use below query. 

After launching the setup and providing necessary details you would reach to Server Configuration page as below. When you reach this page which is also called as Local System account. Please see screenshot 

When you begin installation 2nd time dont install SQL Server client tools they are already installed. NOTE: This is low level troubleshooting a more detailed and accurate information can be provided by MS engineer. 

No, it's not written it is a it says it's and I am sure MS books online cannot write it as good, this is because of fact that environment varies and what is good for one environment might not suit other. Its mostly good to have restriction on something which is heavily consumed although managed efficiently. I consider a good practice to define max server memory on system because it will restrict buffer pool and will tell SQL Server how much max a buffer pool can grow although SQL Server can consume memory outside buffer pool/max server memory setting if it heavily uses Third party DLL's extended stored procs and Linked servers. Page file is used by Windows to hold temporary data which is swapped in and out of physical memory in order to provide a larger virtual memory set. Page file largely depends on how much memory OS is committing and changes accordingly as per min and max value set. if you want to monitor page file you must rely on perfmon counters. Please read this MSDN Blog Memory: Committed Bytes --Number of bytes of virtual memory that has been committed. This does not necessarily represent page file usage - it represents the amount of page file space that would be used if the process was completely made nonresident Memory: Commit Limit-- Number of bytes of virtual memory that can be committed without having to extend the paging files. Paging File: % Usage-- Percentage of the paging file committed Paging File: % Usage Peak-- Highest percentage of the paging file committed 

There is no Slipsteam file available for download its a way to install updates with SQL Server. Yes you can install SQL Server 2014, SP1 and Sp1 CU6 all together Microsoft blogs already has information about Create a merged (slipstream) drop containing SQL Server 2008, Server Pack 1 and a Cumulative Update (CU) based on Server Pack 1. I also suggest you read SQL Server 2008 Slipstreaming FAQ Boris Hristov has also written good article see This link. This uses command line to install SQL Server and the updates. Here is what we should do to slipstream SQL Server 2012 and 2014: 

I believe you meant does shrinking of log file brings index fragmentation, not it does not causes index fragmentation. It definitely does not removes indexes I am not sure what made you asked this but rest assured no indexes would be removed. 

Simple answer: You are following correct path. You can create your own maintenance plan from MP wizard its easy and works fine. You don't need to use complex Ola Script. 

Just go ahead click and apply the service pack, you can safely ignore this process. The check is to handle scenario where end user does not want to restart after applying service pack in that case you need to make sure all such processes are stopped, but in any case I strongly recommend to start windows machine/node on which you are applying service pack. The only thing which can happen is after successful upgrade you need to start the windows machine. This as such is NOT going to cause SP to fail 

I just took backup of Adventureworks database on my local machine and provided media name below screenshot will show you the result. 

What does Fragmentation Means in a Heap The fragmentation value in Heap which you get from column by querying DMV states that 

What you can do is you can ask TSM guys to also backup the folder holding full and transaction log backups. 

The hardware( the underlying disk) on which master,model and msdb file resides is incorrectly formatted or corrupt. You need to get the Storage verified. This might be bug ( Which I dont believe, actually some external factor is preventing SQL Server from coming online which can be known by analyzing dump ) 

What TSM guy is doing is taking of mdf and ldf file which does not guarantee point in time recovery. You are correct to say that he should also be backing up folder(s) which holds transaction log backup or other similar backups i.e . In case of disaster you would always want zero data loss or as much less data loss as possible. If you follow what TSM guy is doing you would be only able to to point when it was . But with full backup and log backup you could do point in time restore(depending on level of disaster/corruption) and that would give you no or minimum data loss. 

Edit: Regarding low virtual memory condition its coming because page file is not set correctly. You must use This Link and This Link to configure appropriate value. This link will help to guide how to change There can be many reasons for low virtual memory condition. The current workload which is running on OS is memory hungry and OS due to limited RAM has to use virtual memory for temp storage and ultimately its getting so much utilized that its giving warning. So you must check what all processes are running. Page file is adjusted as per memory committed if more memory is committed page file value changes use below counters to determine appropriate value of page file Memory: Committed Bytes Number of bytes of virtual memory that has been committed. This does not necessarily represent page file usage - it represents the amount of page file space that would be used if the process was completely made nonresident Memory: Commit Limit Number of bytes of virtual memory that can be committed without having to extend the paging files. Paging File: % Usage Percentage of the paging file committed Paging File: % Usage Peak Highest percentage of the paging file committed 

If you use analysis services backup Analysis Service configuration files, databases and repositories. Backup files present at location C:\Program Files\Microsoft SQL Server\MSAS11.\OLAP\Data\ C:\Program Files\Microsoft SQL Server\MSAS11.\OLAP\Config\ directory. Make sure enough space is available on drive where resource/system databases are present. Resource database [This link is external to TechNet Wiki. It will open in a new window.] is utilized during service pack upgrade. The Resource database makes upgrading to a new version of SQL Server an easier and faster procedure. In earlier versions of SQL Server, upgrading required dropping and creating system objects. Because the Resource database file contains all system objects, an upgrade is now accomplished simply by copying the single Resource database file to the local server. Service pack upgrade would require a downtime of application. Make sure application does not access database during service pack upgrade. Make sure you stop all jobs and activities on database before proceeding. you don't need to shutdown database services to apply service pack. Make sure you are administrator on the system and always run the service pack executable with administrator privileges. 

Download SQL Server 2012 express setup again Make sure you meed all Hardware and Software requirements specially related to .net framework. I strongly suggest you to read the article before you again proceed with installation Make sure you always right click on file and select Make sure account you are using to run installation has full permission on system drive C:\Windows\ 

Edit: As per question regarding believe me its not showing correct value for buffer pool. For my system with 4 G of RAM target pages are 7979008 with almost nothing running on the system. This is totally absurd and I would request you not to look at this counter. I had a chat with one of the experts on this field to confirm the behavior and he told me you should avoid looking at buffer pool output and target pages specially on NUMA system its calculation( way of calculation) is not correct. I can raise a connect Item but it would take few months to get response from MS team. Not every thing in DBCC MEMORYSTATUS output matters to normal users many things are internal to MS and they would just not give any information regarding the same. If you want to monitor SQL Server memory usage please use below query. 

I would start by saying Task Manager is not a correct place to gauge SQL Server memory consumption, it will not tell you correct value when SQL Server service account has Locked Pages in Memory(LPIM) privilege. This is because normally task manager tracks Process Private bytes memory which is pageable and allocated via VirtualAlloc() function but with LPIM chunk of memory allocation is done by AWE API which is NON pageable so task manager does not tracks it and this can lead to incorrect value. For percentage thing which you are looking is actually not the complete memory and it does not provide any relevant information so just stop looking at it. It is quite normal for SQL Server to utilize memory allocated to it and to know how much physical memory SQL Server is using please use query 

The message is straight forward there is corruption in transaction log and since full backup also includes some amount of transaction log the backup failed. You might get lucky with backup by using continue_after_error clause. 

We only have one jump rest all pages are allocated page ID serially. Since just one jump fragmentation decreased considerably. I rebuild the Heap again and now when I checked fragmentation it was completely gone. And page ID allocation is like 

When you detach database you dont delete it you just remove it from SQL Server instance. The mdf and ldf files would be present at location where it was before, if you have not deleted the files. Detaching a database removes it from the instance of SQL Server but leaves the database intact within its data files and transaction log files. This link has code which can be used to find which database is not attached to SQL Server. Or you can simply go to windows serach functionality and search *.mdf and if you can see your database name listed it was detached.