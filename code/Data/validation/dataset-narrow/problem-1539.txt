It allows specifying multiple metrics for evaluation. It returns a dict containing training scores, fit-times and score-times in addition to the test score. 

Now if the above assumptions are met, then you can apply Machine Learning algorithms which are used for doing multi-class classification. Applicable Algorithms: 

As far as I remember Kmodes is used for Categorical data, even in the documentation I couldn't find anything related to Mixed Data Type, if you have some reference do share. I've used Kproto for Mixed Data types as you have mentioned and the package is ClustMixType, it combination of Kmeans and Kmodes. Do have a look and let me know if have any questions. 

Reason for the error is, it was taking x as 1 element. So, I called this function using its name. Do let me know if you any issues. 

This gives me the values the node "a" and random numbers for all the node "b". I don't know where I am going wrong. I thought it has something to do with the way the data has been set up in my file. So I created a file for each column of my main file and wrote the create statements to get the nodes. I still don't get the actual values of the related nodes. 

Here I need to group by countries and then for each country, I need to calculate loan percentage by gender in new columns, so that new columns will have male percentage of total loan amount for that country and female percentage of total loan amount for that country. I need to do two group_by function, first to group all countries together and after that group genders to calculate loan percent. 

Comparing two libraries or tools in terms of these things is somewhat that is opinion dependent. Some people prefer for doing almost all the tasks. has also gained quite reputation. But what is better for you depends on what you want do. In my personal experience, I have found that along with libraries is all that I need to do all the Natural Language Processing tasks. 

There can be many reasons for this thing but in most of the cases I have observed one common reason. When you split your data using train_test_split or any other method, it is important to note that the column on which you are splitting the data is significant for splitting considering both the training and testing set. For example if I have a 'time' field in my data and I have split the data into training and testing sets on this column such that there is no value in this column of the test set which matches with any of the values in the same columns in the train set. 

I have a SQLContext data frame derived from pandas data frame consisting of several numerical columns. I want to perform multivariate statistical analysis using the pyspark.mllib.stats package. The statistics function expects a RDD of vectors. I could not convert this data frame into RDD of vectors. Is there a way to convert the data frame? Code: 

Could someone help me in achieving this output? I think this can be achieved using dplyr function, but I am struck inbetween. 

This is the code I have written in normal python to convert the categorical data into numerical data. It works fine. I want to do the conversion in spark context. And, there are 9 categorical columns in the data source. Is there a way to automate the dictionary update process to have a KV pair for all 9 columns? 

As told by @smci, this technique is called Data Imputation. There are several techniques which can be used to deal with the missing data. Some of these are: 

Hyperparameters and parameters are often used interchangeably but there is a difference between them. You call something a 'hyperparameter' if it cannot be learned within the estimator directly. However, 'parameters' is more general term. When you say 'passing the parameters to the model', it generally means a combination of hyperparameters along with some other paramaters that are not directly related to your estimator but are required for your model. For example, suppose your are building a SVM classifier in sklearn as shown below: 

I have a dataframe consisting of some continuous data features. I did a kde plot of the features using seaborn kdeplot functionality which gave me a plot as shown below : 

I have an excel file containing lot of columns. I want to create a graph database in Neo4J with all these columns as a separate node so that I can establish relationship between them and play around with the cypher query to get to know my data quite well. I did write a cypher query to make each column as a node, but when I run the relationship query it shows me the name of one node and gives a random number for all the related nodes despite changing the captions in the properties section. Sample Create statement: 

I am trying to do this in R. I tried the below function, but my R session is not producing any result and it is terminating. 

Please refer to the links below for deep diving into the topic, appended couple of links with respect to health care industry: 

One-Hot encoding for categorical data Arbitrary numbers for ordinal data Use something like group means for categorical data (e. g. mean prices for city districts). 

If these conditions are met then you can use Poisson Model, go through this link Implementation of this in R, Python. Finally, to address your 2 questions: 

This is helpful when the data is not understandable and you don't have any data dictionary or you have too many columns which doesn't make any sense even after through investigation, then it is wise to go for Dimensionality Reduction. There is very good chance that you loose information at granular level, for example you gave 100 columns and you got 10 PC's(Applied PCA) which can explains most of the Data. As you can only get that much out of that technique. Feature Selection: If you miss even a single feature which is significant WRT target variable but you overlooked that feature then your model might not be able explain the most, you are even satisfied with the result but still there is window of improvement. You should be very careful and you need check as many times(many iterations) as possible to build a good/decent model and get the best results out of it. I hope this may help you. 

I created a sample dictionary with key value pairs for work class. But, I don't know how to use this in a map function and replace the categorical data in the CSV file with the corresponding value. 

I have a dataframe with columns as defined below. I have provided one set of example, similar to this I have many countries with loan amount and gender variables 

I have provided a sample data, but mine has thousands of records distributed in a similar way. Here, Col1, Col2, Col3, Col4 are my features and Col5 is target variable. Hence prediction should be 1,2,3 or 4 as these are my values for target variable. I have tried using algorithms such as random forest, decision tree etc. for predictions. Here if you see, values 1 and 3 are occurring more times as compared to 2 and 4. Hence while predicting, my model is more biased towards 1 and 3 whereas I am getting only less number of predictions for 2 and 4 (Got only 1 predicted for policy4 out of thousands of records when I saw the confusion matrix). I am planning to do feature extractions and try to see how my model behaves, but is there any way to make my model predict in a generalized way where it can generalize some values 2 and 4? I just read of undersampling and oversampling concepts, but I am beginner here, is there any good approach I could try? Should I split the training dataset based on Col5? I am implementing this in python using pandas. 

are used to control the output of a neural network. They tend to make the output smaller. Suppose the loss function is give as : 

So this vector y = [0,0,0,1] is your target to these input types that you feed into the network as labels corresponding to your above input data. 

If you don't define the validation set/ validation split for your model, then it has no way to check for it's performance because you have not provided anything to the model on which it can validate its performance. In this case, the model will run through training examples, will learn a way to minimize the cost function as per your code and that's it. You get a model which has learned a hypothesis from your data but how good the model is, it can only be checked by making predictions for the test set. 

Procedure-1: Convert them into percentage and then take average of outcome of both. so for example for person A, judge-1 rated 5 out of 7 and judge-2 rated 7 out of 10, converting judge-1 score to percentage i.e., $5/7 *100$ = 71%(rounded 71.45) converting judge-2 score to percentage i.e., $7/10 *100$ = 70% Now take average of both i.e., $71+70 / 2$ = 70.5%, now you can convert it back on to a scale from $1-10$ : This person A's overall rating is 7(rounded 7.05 ) out of 10. Procedure-2: You can normalize the rating of both the judges by using this formula, $$z_i=\frac{x_i-\min(x)}{\max(x)-\min(x)}$$ where $x=(x_1,...,x_n)$ and $z_i$ is now your $i^{th}$ normalized data. As a proof of concept (although you did not ask for it) Procedure-3: You can convert the 0-7 rating to 0-10 and then take average of both. Once you get the outcome then you can rank them accordingly. Do let me know if you have any doubts. 

I have a python script written with Spark Context and I want to run it. I tried to integrate IPython with Spark, but I could not do that. So, I tried to set the spark path [ Installation folder/bin ] as an environment variable and called spark-submit command in the cmd prompt. I believe that it is finding the spark context, but it produces a really big error. Can someone please help me with this issue? Environment variable path: C:/Users/Name/Spark-1.4;C:/Users/Name/Spark-1.4/bin After that, in cmd prompt: spark-submit script.py 

I have a data set, in excel format, with account names, reported symptoms, a determined root cause and a date in month year format for each row. I am trying to implement a mahout like system with a purpose of determining the likelihood symptoms an account can report by doing a user based similarity kind of a thing. Technically, I am just hoping to tweak the recommendation system into a deterministic system to spot out the probable symptoms an account can report on. Instead of ratings, I can get the frequency of symptoms by accounts. Is it possible to use a programming language or any other software to build such system? Here is an example: Account : X Symptoms : AB, AD, AB, AB Account : Y Symptoms : AE, AE, AB, AB, EA For the sake of this example, let's assume that all the dates are this month. O/P: Account : X Symptom: AE Here both of them have reported AB 2 or more times. I could fix such number as a threshold to look for probable symptoms.