I'm running small Ubuntu devel virtual on VMware Fusion. 4GB disk and 512MB RAM is enough for development. Network of VM is in NAT mode, so I can access it even when not on Internet. I also configured AFPd so I can edit files directly mounting share. As far as I'm doing Django only that way setup is as following... Django app running under some user account I created, that user homedir is also a root (loaded on login), I use that user to login to AFP. When new project I just clone template machine and create new user account + . Installing to VMware Ubuntu chooses kernel that holds main virtualisation abilities... thus XEN, KVM, VMware. Deploymnet should then be DevOps way... just copying VM files to cloud and starting it online (maybe conversion of disk file or its growing to production size). 

The most likely you forgot to enable forwarding. Add to , then or restart. Also try to add following to OpenVPN config: 

Configure status plugin and install collectd to collect system performance data. It's a very lightweight daemon in means of system resources it needs. There's plugin for nginx monitoring: Plugin:nginx and of course can monitor whole other system performance data. As far as is just collector of performance data (stores it in RRD DBs), a tool for displaying data is required. I'm pretty comfortable with CGP... git version is OK. is a PHP app thus it will eat you CPU just only when you will look at graphs. Example graph: Nginx_connections_and_requests.png Btw Amazon EC was always significantly slower than others and most notably for storage. That could be root of higher load. 

So the only thing left over is the directory with the same 5x layers listed. This seems to be the reason why it's still listed on When I delete the folder (from ) then the repository is no longer shown in the This seems to be a method to purge it from the listing. However - what if an image has 2x tags, yet only 1 is deleted - is there a reason to delete anything from in that case? How would that be handled with multiple versions of an image? Obviously I can't just clobber the directory as the final method since, in the real world, there will be many tagged versions of an image. So this needs to be done intelligently. 

Next, run the deletion via API calls (You can easily test via Postman or just using curl/etc) NOTE: In the below calls, add to the HTTP Header 

Firstly, I want to make it clear I don't care about data loss, and I understand the risks involved. What I'm looking for is guidance and if what I am hoping for is even possible. My Scenario: I have 3 1TB SAS drives in the server. Wanting to combine to a main 3TB volume. OS is Ubuntu Server 14.04 I want to avoid using the RAID controller (RAID 0) since I know that if a single disk fails, then the entire array is compromised. I can comfortable use LVM but I'm not sure if it can do what I'm trying to do. My goal is if ONE disk fails, then I lose the data on that bad disk but the other disks continue to operate in the array and the files on the good disks are still available. I know this isn't technically "striping" because no data would span across disks (all blocks in a file on one physical disk) -- Is this possible? One more time to reiterate - lost DATA is OK, but a lost VOLUME is not. If it's possible, great, if not, that's fine too as I am just looking for guidance. 

I accidentally started backing up much more than was expected so I filled all volumes. I now fixed the issue of what is being backed up and what isn't; but how do I get backing up again? I cannot create more volumes and I'd rather not destroy volumes. What should I do? 

Rails and Django are development frameworks, to be able to serve requests from a web browser, they need a web server that will execute the code. I'm not sure about Node.js, but I would expect they are similar. Rails and Django are not web servers by themselves, but they use a small web server during development to makes things easier. That's how you can run, for example: 

For off-site backups we are running a Bacula SD on another location and the internet connection to it, although quite good for what you can normally get at an office, is not as good as a single ethernet cable. It's slow (100Mb/s) and not very stable. One of the backups seem to take around 16hs or more to finish. The chances of the connection being lost over that long period of time is quite big. So far in a week I never got a single backup to finish and this is the problem: Bacula seems to start from scratch every time. Can anyone confirm this? It also seems not to be re-using the volumes, so I already run out of space having 95% of my volumes used by useless pieces of a backup. Does it mean Bacula cannot work on this conditions or is there something I'm missing about Bacula's configuration? Anything else I should try before giving up Bacula? 

option - when you want to stay with FreeBSD, check FreeNAS to automate complexity you are afraid of. option - NexentaStor, it is Solaris based storage appliance SW with great management web gui. Up to 18TB setup is for free. Again there you can easily manage complex vs. a lot of datasets configuration. 

If you have speedy lines between sites you want to mirror, I can imagine something like you export iSCSI volumes from sites storages and put them mirror and add some local disks for ARC, ZIL, cache to lower read/write peaks running over iSCSI. If your storage is mainly for backups, then it would be OK. Nevertheless SUN once had such product behaving like that on ZFS. 

First one is pure software and second one is kernel accelerated provider accesible as PKCS11 token. Exactly those two on my old T1 Niagara are doing 8.4 sign/s versus 19740.0 sign/s. That's for sure huge difference. Modern x86 CPUs can accelerate AES for example and as far as I know it is used in software kernel provider. Check yourself what's the difference. More important is to have speedy asymmetric ciphers, because they are used during establishing a connection and are more CPU hungry... web applications close connection often. Btw KSSL is in fact just in kernel SSL encrypting proxy... a fact it happens in kernel contribute to speed too. Just to compare... on another machine, ~ same age as T1 noted above, but x86 in VMware is doing for me 42.1 signs/s versus 98.6 signs/s for rsa2048. So more than doubled speed. 

As part of my deployment process I would run a chown -R www-data:www-data in all of /var/www/projectx. That is taking way too long, so instead of periodically fixing permissions, I want to avoid breaking them. I think one of the reasons why they are broken is because we run Rails-related commands on the server, like: 

I have a web app running with nginx and I'd like to analyze the logs to get information about how many hits each file is getting, and which 404s and other errors we are generating (something Google Analytics can't provide). Normally I would just drop awstats in a server, but with nginx, due to the lack of cgi-bin, it's not trivial. Is there another trivial solution to get this information? I don't mind paying a third party to do this analysis for me. 

If I manually run service mount_public_uploads start, it mounts just fine. Maybe it's trying to mount before glusterfs is ready? 

I enabled SSL by following this guide: $URL$ except that I have the keys and cert in both gluster and glusterfs files as the guide seemed to use one at the beginning and the other one afterwards: 

I am running Ubuntu 12.04 and I'm aware of the bug related to the init script, but I'm running GlusterFS version 3.2.5-1ubuntu1 which has the fix. I am also aware of some IPv6 so I made sure both IPv4 and IPv6 work fine: 

I have an OpenVPN server that authenticates against a single AD domain. Here is my auth-ldap.conf that is used with OpenVPN 

Other than BOTO, many other AWS interaction utilities will often have a similar filter or query system based on instance tagging. Like Ansible, etc. 

$USER1$ points to /usr/local/nagios/libexec and it is the same for all my command declarations. If I run it as ROOT: PASS 

ANSWER TO THE QUESTION: Logical 'OR' -- the GPO will apply if any of the Security Filters match. SOLUTION TO SITUATION: Configure user Group Policy Loopback Processing Mode 

If you post your server.conf it will greatly assist others in helping. Firstly, the errors you are getting are specifically about the ROUTES being pushed, not necessarily the IP assignment. The logs show that it's trying to get an IP, so I assume that's configured on the server, but it doesn't look like routes are. I'll go over both items... IP Assignment: To define what range of IP addresses you get from the OpenVPN server, you set it at the "server" item in your server.conf and, in my working config, defined the topology as "subnet": Example: 

This solved the problem for both the logon script and the GPO drive mount. I ended up ditching the script and just using the GPO. Unfortunately this doesn't identify the root cause of why it failed with the explicit account in the first place. If someone had an environment were they "had to" set it up that way, I foresee them running into much frustration. 

I'm not sure what went wrong because as I was fixing things, Chrome kept on showing me a download of the PHP file. It still does, while other browsers shows the PHP being executed. Is Chrome using some cache? Anyway, since I asked the question, the only missing part was installing php5-mysql. 

But I'm not so sure how it should work. It doesn't seem to be working out of the box. Even though mounting of glusterfs volumes happens after the network starts, it happens before GlusterFS starts: 

I'm using Puppet to administrate my servers and one of the things I'm doing is installing Mongrel from the Gem: 

UPDATE: I've tried running the command as bacula to make sure there were no permission errors and it worked. I did like this: 

How can it be 11G? What's going on? Just rounding up? but then why does it fail to fit on the other machine? 

Are there any guides, howtos, books, etc about installing and maintaining a publicly-accessible Windows Server 2008 (with IIS and SQL server) for programmers (that want to deploy their own apps)? 

I couldn't get the verification to work (anywhere else than in my workstation computer), but in the end I moved on and everything worked just fine, so maybe it wasn't that important. 

I was just checking the status of my glusterfs volumes and I have one with split-brain entries that have no path: 

I got many DMA errors on drives, when air condition issue in datacenter and ZFS was able to fix that mess. And it was just simple mirror. I do remember promo video issued by SUN when they introduced ZFS... they made RAIDZ on USB flash drives deployed to 8 port USB hub and then randomly changed position in hub for few of them while doing IO on that pool observing no outage. 

May be your old mirror set was a hardware one, not the ZFS. Depends on your HW. Check partitions table with if something is there. ZFS can reconstruct its pools no matter what order are disks placed in. 

It will inherit netmask from global zone interface. Of course you can setup more interfaces, or put zone only on 'internal' interface (no public IP) and the let provide . 

Zone virtual interface has some features limited... some states can't be setup, packet filter doesn't work in zone too. If I remember right, zone interface can't send ethernet broadcasts, so then no DHCP. Btw why you doing that bloat about setting up zone interface? What about this? 

I think it is more probably a DNS issue than GSSAPI. likes prompt DNS responses to work promptly during connection phase... cause of logging and access checks. 

Every chunk of data has fair checksum on ZFS. So ZFS know which drive holds correct data in redundant setup when failure. Running will repair data or spread data to all running drives for RADZ. ZFS employs Reed-Solomon's error correction which is best for bursts of errors. Missing drive is such burst of errors, which R-S can correct. 

There are very good answers in this question, be sure to check them out, but what I've done is this: 

Also, there's no reason to have separate files. I have one file pupeno.com.conf which includes the definition for non-ssl and ssl pupeno.com virtual hosts. Think about what you want to enable and disable in one go with a2ensite and a2dissite. I consider $URL$ and $URL$ the same thing thus it's on pupeno.com.conf. 

How do I set all volumes into Append mode in a Bacula server so it starts backing up from scratch? I have 70 volumes which are file volumes and due to a mistake it backed up too much and run out of space. I want to have everything reset to initial, like if the volumes were just created, so I could run a full backup and take it from there. I'd like to start backing them up in alphabetical order so I know that Volume-02 comes after Volume-01 chronologically. Any ideas how to do that? 

Any ideas what's going on and/or how to fix it? As an alternative that I'm not too thrill about, I tried having an upstart job mounting those volumes. I added noauto to my fstab glusterfs entries so that they wouldn't be automatically mounted at boot item and created an upstart job with these content: 

I created a bunch of upstart jobs for my services that I'm running on an Ubuntu 12.04. I can successfully start them and stop with with: