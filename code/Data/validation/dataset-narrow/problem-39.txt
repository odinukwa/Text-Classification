Nope. That would be true if you wanted the caller to only pass the lower-left and upper-right points, but they can be any pair of opposite points. Let's truncate to two dimensions to make it simpler. If $p_1 = (1,2)$ and $p_2 = (2,1)$, they're not the bottom-left and top-right of a rectangle, they're the other two corners. The bottom-left corner is $(1,1)$ and the top-right corner is $(2,2)$. It's no different in three dimensions. To put it another way, an axis-aligned bounding box is the Cartesian product of three intervals or ranges, one along each axis. It's the set of all points $(x,y,z)$ where $x_{min} < x < x_{max}\ \wedge y_{min} < y < y_{max}\ \wedge z_{min} < z < z_{max}$. You're trying to find the smallest box that contains all of the points you're passed. (In fact, you're only passed two, but it would be the same if you had a whole set of points.) To do that, you find the interval on each axis separately. Just because one of the points you're passed in has the smallest $x$ co-ordinate, that doesn't mean it'll have the smallest $y$ or $z$ co-ordinate, so you take the min and max separately for each axis. 

You can't have a camera with collinear look/front and up vectors. It doesn't make geometric sense. The whole point of the up vector is to disambiguate roll. As russ says, for a "first-person" camera it's reasonable to always use world-up for the up vector and simply lock the pitch angle to within (say) 85 degrees of horizontal, to avoid degenerate cases. For a CAD viewer where the user would expect to pitch the camera arbitrarily, you should rotate both the up and front vectors about the right vector when the user pitches the camera, to ensure they are always perpendicular. Beware that when you do this kind of arbitrary rotation, it's easy for the user to get stuck in unfortunate roll angles. You need to do at least one of the following to let them get themselves out: 

If you're using a GI system, the high-frequency lighting detail that you get from a projector light will create a more challenging GI environment. You can control the badness by making the albedo of the surface you're projecting onto as small as you can get away with. If you don't need accurate GI results, you can use a proxy in the GI system instead of the real projector map. The proxy could have an aggressively blurred image, or even a normal spot-light whose colour is the average of the whole image. The blur technique is very applicable to a path tracer, while a single colour would be simplest for a radiosity or virtual lights implementation. 

In a polygon mesh, all the faces that meet at a point don't have to share the same vertex. They can each have their own vertex, and those vertices can all have the same position but different normals. In meshes with smooth edges (such as on a sphere, where you want to hide the edges), you want all the vertices at the same point to have the same normal. Generally this will be close to the average of the normals of the faces that share the vertex. That way, Lambertian shading computes the same colour for each, and you don't get a discontinuity on the edge. In meshes where you want a deliberate sharp edge (such as your cube example), you want the vertices to have their own normals. In the cube case, each face has its own four vertices, and those vertices all have the same normal (the face normal). Each corner of the cube has three (or six, depending on the triangulation) vertices, with three normals perpendicular to each other. This way, Lambertian shading gives each face the same colour (because all the normals for one face point in the same direction), and adjacent faces don't affect each other. A mesh with some rounded parts and some sharp edges can have both. Some vertices can be merged (i.e. each triangle meeting at the same point has the same vertex normal) and others can be split (i.e. each triangle meeting at the same point has its own normal for that vertex). One triangle can have some smooth vertices (shared with its neighbours) and some sharp vertices (with the face normal, not an averaged normal). It's best if the vertices on one edge are all the same, otherwise the results look unphysical. Generally, the 3D modelling tool you use to create the mesh will give you the choice: you can make the whole mesh smooth or sharp, or you can select some edges and make them individually smooth or sharp. 

In a fragment shader, I need to draw a line at a given value of u. The shader is applied to a planar polygon mesh that's being drawn in the scene with a perspective camera. (It isn't a full-screen textured quad; that would be easy!) The problem is, the thickness of the line is given in screen pixels, not in world units. How can I draw this line in my shader? The shader is Cg, but a pseudocode answer outlining the technique is fine. 

Yes, and there are several ways this can apply. First off though, we need to fix a misconception in your diagram. Even if pixels really were square, the volume contributing to the pixel gets wider as it gets further away from the image plane, because of perspective. For a square pixel, the volume would be a square pyramid, whose apex is at the camera. It's not quite a regular square pyramid, because the apex is not in the centre of the square: the outer pixels are very skewed. But pixels aren't square anyway, they're cone-shaped. First off, some people have experimented with cone tracing, where instead of a single ray, you trace a cone with a half-angle determined from the area of the image covered. It's not very popular, because when the cone intersects the edge of a polygon, you need special handling for the remainder of the cone - the part that wasn't blocked by the polygon. For a square pyramid, it's a little easier, because you can change the shape of the base as parts of the pyramid are blocked, but it's still tricky and slow. You also don't gain much from this technique compared to just supersampling. Even if you trace rays, you don't just trace a single ray through the centre of each pixel. You trace many rays across the area of the pixel, and use a reconstruction filter to take the shape of the pixel into account. This technique has many possibilities that cone tracing doesn't offer: it works better with volumes, you can jitter your samples to avoid structured artefacts, you can use full path-tracing techniques, etc. Another area where the pixel size matters is in texture sampling. If you want to get nice texturing, you don't just take a point sample of the texture, you set some filter width according to the area of the texture the sample relates to. This doesn't just make your sampling more efficient by using the right mipmap level: it is required for correct sampling to avoid sampling artefacts. In a GPU fragment shader, you might use the and functions to do this. In a ray-tracer, you might have use a cone-tracing style of carrying the filter width with the ray, initially based on the level of supersampling at the camera, but updated whenever a child ray is spawned. 

which reads from the file into the instance. It won't even do that in this case, because the file handle has to be open for reading only. I'm no expert on Skia, but you probably want to use to write the file. Or just call and write the bytes to the file using the standard library. 

Diffuse colours on materials typically come from within the material, while the specular colour is from the very surface. Coloured plastic materials are made by embedding particles of dye inside a colourless medium, so the diffuse colour is the colour of the dye, while the specular colour is white from the colourless surface. With metals, all of the reflection is at the surface, so the diffuse and specular colours are the same. There are many different kinds of paint, but most commonly they're made like plastics: putting dye particles inside a white or colourless medium. Emulsion, acrylic, and enamel paints are all made this way. For this reason, painted or varnished metal surfaces have white specular as if they were plastic. There are some kinds of paint (metallic paints being the most obvious example) that aren't made this way, so it's not always true. Ambient doesn't correspond to a physical property of the material: it's a cheap substitute for global illumination. It represents all of the indirect light that falls on the shading point and is reflected in the viewing direction. There's no physically correct way to set it: you should just do what looks best in your scene. That said, I'd offer a rule of thumb that shinier objects probably want less ambient, because less of the indirect light is going to be reflected diffusely to where you can see it. However, you might consider not using ambient at all, and pointing dim fill lights at places that will come out too dark without indirect light. 

This is kind-of the same as ratchet freak's answer, but fleshed out a little more. The usual way to handle this is with a special projector light type. It's like a spotlight, but (usually) with a square instead of circular fall-off. That is, when you cast a shadow ray to the light, you don't just measure a single angle (or radius) from the light's axis, you measure $(x, y)$ co-ordinates in the light's space. You divide these co-ordinates by the size of the projection (a parameter of the light just like the cone spotlight's angle parameter), and use the result for a texture look-up in the image you want to project. 

Conceptually simplest would be to treat it as a ray-casting problem, representing each point as a small sphere. It should work like the shadow rays in a conventional raytracer: iterate over all of your points, and for each one, trace a ray to the camera. If the ray intersects the sphere representing another point, then remove it. If the number of points is small enough, you can do this directly, but if that's not fast enough, you should put the points into an acceleration structure such as a kd-tree. (If you need this kind of optimization, it might be best to start from an existing ray-tracer such as PBRT instead of writing it from scratch.) If the points represent surface detections (from something like a laser scanner), you might also have the surface normal at each point, depending on the scanner technology. In this case, you might get better results by using small discs instead of spheres. That'll be more robust on faces seen at a glancing angle. Finally, if you also need to remove points that are outside the camera's view (e.g. behind the camera), you can do this at the same time by comparing the direction of each ray with the camera's view bounds. 

So far, I've been able to attach to with RenderDoc and intercept the D3D calls that do the composition. According to Baldur Karlsson (the author of RenderDoc), it's not that hard to hack RenderDoc to use its hooks to run arbitrary code on each call of the chain I'm interested in. I've not had a chance to try that out yet, so this isn't a final answer! I'll update this post with any further progress. 

The compositor in the OS wants to run at a consistent 60 FPS whatever rate your application runs at. If there were a single framebuffer for your application, then you might end up writing to it at the same time the compositor or overlay hardware wants to read from it to put it into the final frame. This would cause tearing. (Tearing could be avoided with a lock, but then the compositor might get stuck waiting for your frame to finish, which would also stop the user using the system GUI to get out of your slow/hanging app.) This is pretty much the same reason double buffering is used when you're drawing to a monitor directly, just earlier in the pipeline. This doesn't usually apply for processing within your application, because generally everything has to run at the same rate, and you can't run multiple stages in parallel. Say you draw some 3D stuff and then use a full-screen fragment shader to add some film grain and vignetting. It doesn't help if your full-screen shader can run faster than the 3D work, because you'll just be processing the same intermediate frames again. I can think of two situations really where it might help. The first is if it's not a post-process but something different, such as if you have a GUI or HUD that you want to run at full frame-rate even if you get frame drops in the main content. (This is the case the OS's window compositor falls into.) The second is if you can run the multiple stages of your processing pipeline in parallel, e.g. on multiple GPUs. That way, you can start rendering the next 3D frame while the full-screen shader is vignetting your previous frame. You'd have to know your hardware pretty well to split the work appropriately. On something like a gaming PC or graphics workstation that's using SLI to use multiple GPUs, you might find your parallel processing is slower because the workload is unbalanced and one GPU is idling a lot.