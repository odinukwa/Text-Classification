Vertex cover (rather, various computations/approximations of it) was the main algorithmic engine in our paper on nearest-neighbor classification: $URL$ 

I see that Expectation Maximization (EM) has not been mentioned, and it's certainly "up there" in the top 10: $URL$ 

Here's a question to ponder: why turn to a public forum composed of people who know nothing about the specifics of your situation when you're supposed to have an academic adviser precisely to handle such questions! Don't get me wrong, you got some great tips here (and meta-tips), but ultimately it's precisely during such crises that one turns to one's mentor for guidance. The real question at this point, I think, is not whether or not to leave grad school, but rather: Is this still a good problem for a PhD student? Should I seek a less challenging result at this point? Or perhaps change directions? These are the substantive questions I'd be posing -- and not to unfamiliar (even if good-intentioned) strangers, but rather to people better acquainted with my specific case (such as, ahem, the adviser!). PS I understand that not everyone is blessed with an approachable, supportive adviser. I would invest serious effort in trying to find one. This PhD journey is hard enough with good guidance. 

Random forests have a reputation among practitioners of being among the most effective classification techniques. Yet we don't encounter them much in the learning-theoretic literature, from which I surmise an absence of deep theoretical results. If one wanted to delve into this theory, where would one start? 

See the power method for computing eigenvectors: $URL$ Convergence is exponential (geometric in the ratio of the top two eigenvalues). 

One approach would be to stratify your concept class by VC-complexity. For example, suppose that $C$ is the set of all functions $f:\{0,1\}^n\to\{0,1\}$; its VC-dim is $2^n$. However, you can decompose $C$ in any number of ways. Say, $C_k$ is the collection of all functions whose (minimal) binary tree has depth $k$ or less. Or, $C_k$ is the set of all functions whose Fourier expansion puts no weight on XORs of more than $k$ bits. Then $C_1\subset C_2 \subset \ldots \subset C_n$, and selecting the appropriate $C_k$ is the problem of model selection. Machine learning theory offers a principled way of doings this via Structural Risk Minimization; here is a good start: $URL$ 

There is currently zero evidence contrary to the Church-Turing thesis -- namely that the Turing machine is the strongest physically realizable computational paradigm. In the case of AI systems, the claim is obviously wrong: these are implemented on regular computers, which are equivalent to Turing machines in computational power (or would be, if they had unbounded memory). In the case of human reasoning, one has some very dubious wiggle room by suggesting that neural computation might be exploiting some as of yet not understood physical process which enables it to decide non-Turing decidable problems. I don't know of any serious scientist (computer or otherwise) who takes that claim seriously. 

Answering your edited question: any learning algorithm (more specifically: supervised classification) algorithm always (either implicitly or explicitly) works with a well-defined class of hypotheses or mappings from examples to labels. In the case of the perceptron in my previous answer, it works with linear separators (and hence will not consider hypotheses involving square roots). A standard no-free-lunch theorem shows that an algorithm whose hypothesis space is too rich will no be able to learn (it will overfit). 

Consider Context-Free Grammars with rules of the form $S\to\epsilon$ or $A\to aB | Ba|a$, meaning that any nonterminal other than $S$ can be replaced by a terminal, or by a pair of letters exactly one of which is nonterminal (and the other one terminal). What class of languages do such grammars generate? Certainly the regular ones, but also $a^n b^n$, via the rules $S\to\epsilon|aB$, $B\to b|Ab$, $A\to a|aB$. Short of a full characterization, I conjecture that these grammars do not generate the full CFL language family, and would be curious to see a proof. 

Regarding the difficulty of learning grammars, let's stick to regular ones for concreteness. These are precisely the grammars/languages recognized by Deterministic Finite-state Automata (DFAs). The source of difficulty is purely computational; the statistical aspects are quite straightforward. If you were able to find the smallest DFA consistent with your finite sample, simple Occam/cardinality arguments guarantee that this will generalize very well (it's the "optimal" learner in some sense). See Theorems 2 and 3 of Graepel et al. $URL$ for actual state-of-the-art bounds. However, finding such a small DFA is VERY hard (see Angluin, Gold, Pitt-Warmuth, etc): $URL$ $URL$ $URL$ [the latter even gives a hardness-of-approximation result]. But wait, it gets worse! Suppose you didn't care about a DFA and just wanted to learn the grammar in some representation (i.e., a mechanism for predicting the labels of test strings drawn from the same distribution as the training set). If such an algorithm were to exist, and succeeded against all distributions, then it would also break RSA and related cryptographic primitives: $URL$ 

VC-dimension has multiple multiclass extensions: pseudo dimension, Natarajan dimension, graph dimension. See here for example: $URL$ $URL$ There are also extensions to continuous-valued functions (fat-shattering dimension): $URL$ 

Let's denote $N:=|\Omega|$. Then $\Theta(|N|/\epsilon^2)$ examples are both necessary and sufficient to learn the distribution to additive precision $\epsilon$ in total variation. This follows from classic VC analysis, since for two distributions $p,q$ on $\Omega$, we have $$ \sum_{x\in\Omega}|p_x-q_x| = 2\max_{A\subseteq\Omega}|p(A)-q(A)|.$$ Now the concept class $2^\Omega$ has VC-dimension $N$ and estimating the weight of each set uniformly is exactly equivalent to estimating $p$ in total variation. The lower bound is actually stronger: not only does the empirical average estimator, $\hat p_x:=m_x/m$, require at least $\Omega(|N|/\epsilon^2)$ examples, but so does any other estimator as well. The upper bound follows from the Uniform Glivenko-Cantelli property of VC classes (and is achieved, in particular, by the empirical average estimator $\hat p_x$). Both bounds are proved, for example, in these notes: $URL$ 

I think you'll find the answer in the paper "Approximate Inclusion-Exclusion" by Linial and Nisan: $URL$ 

The question is a bit ill-posed, since you did not specify that equivalent automata should be counted as a single object. Without that restriction (or the reachability one), the set of all $n$-state DFAs over a binary alphabet and with starting state $s=1$ has exactly $n^{2n}2^n$ elements, and it is trivial to sample from this set uniformly, as indicated in the link in my comment, $URL$ Now the linked paper gives sharp bounds on the expected size of the minimal equivalent DFA, when the original one is drawn from the naive uniform distribution. For binary alphabets, this quantity is concentrated about $0.7968n$. That suggests that if you naively sample DFAs on $n/0.7968$ states and then minimize them, you'll get automata with roughly $n$ reachable states. Of course, no claim is being made about this sampling procedure being uniform. See, however, references in the linked paper -- especially $URL$ and $URL$ -- the latter seems most relevant to your question. 

If your function is $f:\mathbb{R}\to\mathbb{R}$, you can "learn" $f'$ as a standard regression problem (linear, polynomial, etc.) and then recover $f$ up to an additive constant by integrating $f'$. Obviously, you will only able to recover $f$ up to an additive constant from the derivative. Update: As for error estimates, here's a simple one. Suppose you've managed to recover $f'$ up to $\epsilon$-accuracy in $L_\infty$. Suppose further that you know that $f(0)=0$ and you only care about $f$ on $[0,T]$. Then you estimate of $f$ (obtained by integrating $f'$) is accurate up to $T\epsilon$ in $L_\infty$. 

Suppose I have a regular language $L$, and I would like to lower-bound the complexity of deciding membership in $L$. Suppose I know that the minimal DFA for $L$ has $N$ states. I would like to claim that determining the membership of a string of length $n$ in $L$ requires time $\Omega(n\log N)$. The dependence on $n$ is obvious -- I have to read the whole string, in general, to know if it belongs to a language. The $\log N$ factor is because to know what state I am in I need to write it down, and this takes time $\log N$. This requires arguing that there is no faster way to determine membership in $L$ than to run its minimal DFA (implicitly or explicitly). I think such an argument can be formalized (via equivalence classes, for example), but perhaps I am being naive. Question: Is the $\Omega(n\log N)$ lower bound correct? Edit: let me make this more formal. Is there an infinite family of distinct minimal DFAs $A_1, A_2,..$ such that some algorithm simulating the $\{A_i\}$ on a deterministic RAM machine can determine membership of $x$ in $L(A_i)$ in time $o(|x|\log|A_i|)$, where $|x|$ is string length and $|A_i|$ is the number of states? Edit2: Kaveh's and Aaron's answers seem to indicate that my lower bound is false. But I would love to see a non-trivial counter example. Suppose: (a) I have a family of regular languages $\{L_n\}$ (b) each $L_n$ has a compact description (say, as an NFA) of size $poly(n)$, but the minimal DFA for $L_n$ has size $2^n$ (c) for each $n$, there is no fixed-length prefix that determines membership in $L_n$ (this rules out Kaveh's examples -- you really do need to read the whole string) Can somebody give an example of a family $L_n$ satisfying (a,b,c) for which there is a TM taking $n$ and $x\in\Sigma^*$ as inputs and deciding $x\in?L_n$ in time $o(n|x|)$? [Note that we've switched from RAM to TM as the computational model; $n$ is given in binary.] 

"Easy to check" is the understatement of the century: can you actually carry out your proposed plan of "just" writing down all the registers/RAM cells, etc? You're right that it takes finite time, but a large amount of finite time. Our best algorithms for performing this check are exponential in the size of the input on the tape, and a strong version of the $P\neq NP$ conjecture states that there is no algorithm that achieves a faster (subexponential) runtime uniformly over all inputs. So the P=NP problem is a finitary version of the halting problem. The latter has no algorithmic solution when input length is unbounded. The former admits a brute-force solution, but the amount of time required to find it is so large that for all practical purposes it's as good as unsolvable. Edit: My statement that the halting problem can't be decided by any algorithm because the "input length is unbounded" is inaccurate, since for DFAs the latter also holds but the languages they accept are of course decidable. But certainly when the number of configurations is finite -- as is the case for a finite-tape TM -- a brute-force search solves the halting problem. Somewhat confusingly, the finiteness of the number of configurations is actually not necessary for decidability, as the DFA example shows. 

I can see why the question is being downvoted, but this is too good not to post. Suppose you have a coin with bias $p\in[0,1]$, which might be rational or irrational. Question: using only finite memory, can you devise a test for determining whether $p$ is rational or not from a sequence of independent $p$-coin flips? Incredibly, as Hirschler and Cover showed in 1975, "an 8-state memory with a time-varying algorithm makes only a finite number of mistakes with probability one on determining the rationality of the parameter of a coin. Thus, determining the rationality of the Bernoulli parameter $p$ does not depend on infinite memory of the data." $URL$ 

For any language $L$ over $\Sigma^*$, define $$L_{1/2} = \{x \in \Sigma^* : xy\in L, y\in\Sigma^{|x|} \}.$$ In words, $L_{1/2}$ consists of all $x$ for which there is a $y$ of equal length such that $xy\in L$. An exercise in Sipser's book asks to show that $L_{1/2}$ is regular whenever $L$ is. I have seen two distinct solutions, and both involve an exponential blow-up of states. Question: can anyone construct a family of languages $\{L_n\}$ such that the canonical automaton for $(L_n)_{1/2}$ is significantly (say, exponentially) larger than that for $L$? My best efforts so far only increase the state size by $+1$! 

I now realize that a lower bound has indeed been established by Anthony and Bartlett (see the presentation here). I've also come up with an alternate proof, based on some recent work of mine in L1 deviation of the empirical distribution. 

No, you don't need a bias. You can have a "dummy" input (input(n+1) in your formualtion) which is always set to 1. Then the bias term is absorbed into the weights. 

Whenever I teach automata, I always ask my students if they find it surprising that nondeterminism doesn't add any power to finite-state automata (i.e., that for every NFA is there is an equivalent -- possibly much larger -- DFA). About half the class reports being surprised, so there you go. [I myself have lost the "feel" for what is surprising at the intro level.] Students definitely find it surprising at first that $R\neq RE$. I challenge them to produce an algorithm that determines whether a given java program will halt, and they typically try to search for endless while loops. As soon as I show them ways of constructing loops whose termination is far from obvious, the surprise factor goes away. 

I'll give an answer for generic learning algorithms -- nothing specific to neural nets. For answering basic conceptual questions of what can or cannot be learned by an algorithm, try to put yourself in the algorithm's shoes. If I told you that a certain treatment worked for patients A and B but failed for patient C (and, crucially, that is all the data I gave you), could you possibly predict whether the treatment will work for patient D? Of course not, and neither is it reasonable to expect such magic from a machine. If I gave you many more details about these patients (say, their ages, maybe some health history, etc) -- what we call features in the ML lingo -- your chances of learning to predict whether a treatment will succeed will be much better. Needless to say, the usual caveat applies: The features must be relevant to your prediction problem. For example, if the features I gave you were actually the movie ratings these patients gave on Netflix, that would probably be (a lot of) useless information. So that's pretty much your answer. Present the data to the learning algorithm with sufficiently many useful features to make prediction possible (but not too many, because then you'll need huge sample sizes to avoid overfitting). I strongly urge you to take an intro stats/ML class before drawing medical conclusions from running learning algorithms on data. It's rather common for folks lacking a basic statistical foundation to make hair-raising mistakes.