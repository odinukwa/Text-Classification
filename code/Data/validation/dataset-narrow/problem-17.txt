Note it is all zeros, from CapInh. Then if I had to enable features for the application I would use after the Example, will break ping: 

Assuming that "publicly available" is on the public internet it will be difficult. DNS isn't really that reactive to changes and is challenging to run in a secure fashion on the public internet. It also depends on what protocol you are running, because of an oddity around SRV records where pre-existing protocols were excluded from supporting it in the RFC. But in general the public and the private solution I would implement are similar with an additional layer for a public internet solution. (assuming no use of a 3rd party or geo specific answers) On the internet edge use Unbound DNS server on the edge, and have it relay queries to consul which would handle service registration. If this is for private usage this would allow for some data-center awareness and fail-over. If it is just for internal use you can just use consul and either configure your internal DNS servers to delegate subdomains or forward to the default domain. 

This means that the container can access all resources including the hosts disks and hardware. It is security through obscurity, which is not security. Anyone who knows how to use or walk the /sys and /proc trees could easily compromise the host and all containers with zero logging. The reality is that docker shifts both complexity and the security boundaries. All users who can launch container or hit the API need to be restricted to trusted users in that security context. disables all apparmor and selinux policies which is actually far less secure than a native package. Namespaces are not a security function and depend on apparmor and selinux to enforce reasonable constraints. Docker notes this on this page. $URL$ 'First of all, only trusted users should be allowed to control your Docker daemon.' The security functions of docker/kube are not administrative boundaries like classical unix permissions, they are tools to prevent non-privileged containers from breaking out. In a docker world the administrative boundary becomes the host, and the selection and segmentation required to isolate applications or users within that context needs to be applied at that boundary. The benefits of this shift in complexity and responsibility generally outweigh the risks if implemented with those changes in mind. TLDR Docker API user == Sudo ALL user Running a container with the --privlaged flag == running a web service as suid or as the root user. Edited per the OP's request for additional information The referenced issue with breakout int he OP's edit was an non uid0 privilege escalation. Unfortunately, due to the need to perform root only actions Docker needs to enable some capabilities so that apt/dnf can install packages etc... This need does pose a risk if production workloads are run in this default configuration and one should adopt the security principle of least privilege for production workloads. disables apparmor/selinux and opens up capabilities I am using ubuntu but it may be useful to work through the following steps. First start a default container with docker run -i --rm -t debian bash From the parent host find the PID for bash using ps and note that the process is owned root. If you look in you will see the contexts it is running under. 

I would keep the ECS container instances (I'm talking about the Docker hosts - I don't like AWS terminology here) and the deployment as two separate things. Get your ECS stack up and running. You can manage it through CloudFormation and Auto-scaling groups, that's fine. Just think of your cluster as a platform where you will deploy to, not something you need to redeploy. Then, for CD, the easiest method by far is to update the service definition to use a new task definition and let ECS rolling update the containers for you. Every time it start a task, ECS will run docker pull image:tag even if it has the image locally to make sure it has the latest version of that image:tag. So the image tag you use really don't matter (there is no need to change the tag on every build). That means that you can build myimage:latest over and over in order to deploy it easily. What you need is a task definition where the image = myimage:latest. Create a service with that task definition and every time that ECS start a task (an instance of your service) it will be the most recent "myimage:latest" you have built. From there, you are missing only one piece in the puzzle, from CodeDeploy, you can call something, perhaps a lambda function, to create a new revision of your task definition and update your service and ECS will automatically create new tasks for that revision and remove the old tasks. An example: Let's assume you have created a service called MyService. That you have configured that service to run 2 tasks for the task definition MyTaskDefinition:1 (revision 1). In that task definition, you have one container definition which image is set to "myimage:latest". 

Please don't consider to above diagram to represent all that is DevOps, or all this is automation. It is to help the reader picture how the two concepts are related. 

A big part of DevOps is making it possible to release very often. That comes with automated build, automated testing, etc. You can say that to achieve its goals, DevOps need to use automation to be efficient. Here's how DevOps and automation are related. DevOps is not just automation, there's more to it. Conversely, automation is not exclusively used by "DevOps people". A lot of automation was taking place in IT before DevOps came around. 

Proper management of an application's secrets has always been a challenge. New challenges came with the adoption of the cloud. There's a great OWASP presentation about the reality and challenges of storing secrets in the cloud. You might be surprised to hear that storing secrets into the source code is one of the solution (or "architecture") presented. That's because, right now, there is no perfect architecture or way of doing this. In the end, your secrets might be encrypted... but what is guarding the encryption key? "Turtles all the way down", they said. Every type of secret management has its strengths and weaknesses and the presentation already covers that. Instead, I'll try to go over some of the features you might be looking for in an secret (credential) management solution: 

In theory you don't care about the performance of an actual container, just that your overall performance is good. This method makes it easy to have the system self repair and to scale with a minimal amount of complexity. Basically you only have to check if the number of systems you expect are alive, and if not you spin up some more. If you need to add capacity you simply change the number of expected nodes. This also simplifies refactoring as you only need to replicate or modify this test with no external dependencies or state machine. It should also reduce down time and middle of the night Pagerduty alerts as the system self repairs. As for the overall systems metrics, which are needed to trace down issues like latency I would want them in a central location using a tool like elasticsearch. If you use syslog, logstash or log4??? to collect metrics that will be far more useful in the long run. When systems are small and simple traditional polling based monitoring may provide enough metrics but it is preferable to have them in a format that is searchable and more importantly relational to other systems. Solutions like monit still have their place, but it is to monitor the long lived components like the VMs or bare metal hosting your swarm, but the containers themselves should be decoupled from that system to get the most benefits from a micro-services model. 

But be very careful adding them as an example. cap_sys_module will allow a container to add or remove kernel modules from the parent host. cap_sys_rawio will open memory and all block devices to attack cap_sys_admin is super dangerous. So in this case I would see if you can make things run in this context. $ docker run -i --rm -t --cap-drop=all -u nobody:nogroup -t debian bash Hopefully the robustness of apparmor and selinux profiles improves over time, but if that is not enough for your security needs you can look in there too. Really, avoiding the --privileged flag and using principle of least privilege will make the most impact. Especially if you take advantage of the ephemeral nature of containers to keep packages up to date. If you need more Red Hat covers some basic seccomp options here. $URL$ 

Now do the same with and you will find that the effective capabilities are Also just do a in both VM's and you will see just how much access a privileged container has. By looking at for apparmor systems or the lables in SElinux you will see the implications. Going back to the principle of least privileges, I would ensure that my docker container is running process as a unprivileged user and with as few enabled capabilities as possible. As an example, you can test this by running it first by dropping all caps.