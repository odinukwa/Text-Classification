I would agree with the suggestion with holding out the 2016 data to check the external agency's work. Without inspecting the code, you just can't be sure that the k-fold cross-validation process has been performed properly. Another benefit of using the 2016 held out set is that you can see if the model trained on past years' data work well in future years data. There might be a concept drift in the new year as the true relationship between Y and Xs changes. In cross validation, each fold belongs to the same time period as the training data set. 

A correlation matrix would help discover any pair-wise correlation (between any 2 variables). I think that would be a great place to start, as it is just one line of code for initial analysis. After that, if you want to investigate "an increase in parameter 1 and parameter 2 is correlated with a fall in parameter 10", you could write code to form interaction variables like sum / product / divide and build a correlation matrix between the interaction variables vs. the original variables. Personally, I found the corrplot R package useful in visualizing the correlation matrix. Hope this helps. 

You might find it useful to treat n-grams of characters as your feature space. Then you could represent a string as a bag of substrings. With or greater, you would capture things like ".com" in emails, "##.#" in dates, etc. It might also help to encode all single digits as one reserved number-only-character. An easy way to to this might be to create all the n-gram substrings for each string in your dataset, then simply treat that list of words as a document. Then you could use term frequency or tf-idf vectors in your supervised step. For example to create the substring uni-, bi-, and tri-grams for "whatever@gmail.com": 

tf-idf vectors are an easy start, but it can be hard to cluster very high dimensional data. You might try topic modeling (LDA, LSI for example) to reduce the dimensionality of your features. A newer approach is paragraph vectors, which learn a distributed representation of arbitrary-length text. Here is an implementation in python. Learning a reasonable, lower dimensional representation of the text can help with the issues that arise trying to cluster high dimensional data. 

The confusion matrix suggests that you are performing classification rather than regression. RMSE and R-square are measures associated with continuous variables; For categorical variables, I'd suggest using Accuracy / Recall / Precision / F1 score to measure the performance of the model. $URL$ 

A partial dependence plot might be what you are after: these plots show the relationship between the probability and the input variable. The mlr package in R takes care of this. Discussion of Partial Dependence on XGBoost Git: $URL$ General Tutorial using mlr: $URL$ 

Couple of suggestions: Your training set is smaller than your test set? It should be the other way around. You should also tune the RF hypermeters using the held out set or Cross Validation. The two parameter people tune on RF are ntrees and mtry. 

You might consider using word2vec to identify phrases in the corpus. The presence of a phrase (instead of single tokens) is likely to indicate a 'template.' From here, the tokens most similar to your template phrase are likely to be the values for your parameters. 

Bagging and dropout do not achieve quite the same thing, though both are types of model averaging. Bagging is an operation across your entire dataset which trains models on a subset of the training data. Thus some training examples are not shown to a given model. Dropout, by contrast, is applied to features within each training example. It is true that the result is functionally equivalent to training exponentially many networks (with shared weights!) and then equally weighting their outputs. But dropout works on the feature space, causing certain features to be unavailable to the network, not full examples. Because each neuron cannot completely rely on one input, representations in these networks tend to be more distributed and the network is less likely to overfit. 

This sounds like a challenge for trees with sufficient interaction depth, as you've found that the year interacts with other factors that results in improvements. Ordinary least squares regression here do not capture that type of interaction well. I would suggest setting up the following regression model: 

Decide on the KPI to be measured as the objective, is that revenue per customer? Profit per customer? Or total profit per store? Is the algorithm designed to increase per customer KPI or more customers? Check that the stores selected have some history to give assurance that they were similar in terms of the selected KPI before the implementation of the tests. Remember to remove outlier customers / transactions. Check that enough stores are selected in control vs. test buckets to detect a desired threshold increase that is statistically significant. 

A linear regression would work, but the real issue here is feature extraction. You have to encode your categorical features somehow, likely by vectorizing them. You can one-hot encode your features, treat them as text and countVectorize them, etc. 

You might try looking into sentiment analysis. There was a kaggle competition on it, and you might find insight there. Treating this as either a regression or a classification problem is fair. Also, it's important to judge your performance against the proper baselines. Your feature space might not be rich enough for the classes to be linearly separable. You might do better using an SVM with a non-linear kernel. It also appears you haven't scaled the counts of the bigrams, which is generally helpful for SVMs. Another thought for an approach would be to apply LDA to the set of documents (reviews) and use the topics as your feature space (you'll have a topic vector per document). Some places to get python LDA implementations: gensim Blei 

The drawback of picking XGBoost in your application however, is that the interpretation of the impact of a particular variable on the target variable is not obvious. You'd need partial dependence plot to observe how the target variable vary with the bespoke input variable. If interpretability is very important, one could pick a single tree regression model like rpart. 

I would recommend against switch back as the purchase data is likely to exhibit seasonality (e.g. 2pm - 6pm / tuesdays exhibit lower volume, December exhibit higher volumes, company running advertisements during certain months etc.), which would become an unnecessary exogenous factor that complicates the post implementation analysis. I am inclined towards the causal impact approach. However, there are certain things that should be checked before selecting the stores as control vs. test (it is a kick-start list, please feel free to adapt to your business domain). 

1) Activation is an architecture choice, which boils down to a hyperparameter choice. You can make a theoretical argument for using any function, but the best way to determine this is to try several and evaluate on a validation set. It's also important to remember you can mix and match activations of various layers. 2) In theory yes, many random initializations would be the same if your data was extremely well behaved and your network ideal. But in practice initializations seek to ensure the gradient starts off reasonable and the signal can be backpropagated correctly. Likely in this case any of those initializations would perform similarly, but the best approach is to try them out, switching if you get undesirable results. 

There are multiple factors to consider, but the first thing to realize is that in regression, you don't want to think about whether an example is "correct" or "incorrect" but rather how close it was to the true target value. Therefore you can ignore your original intuition about "80% of predictions are 'correct'." Second remember that RMSE is in the same space as your target values. So it is relative to the variance in your target values. The benchmark of random guessing should get you an . So lower than this, your model is demonstrating some ability to learn; above that number, you haven't even learned to guess the mean correctly. There isn't a cutoff for "my model is doing well" in RMSE space, just like with other metrics. Everything is relative to a naive solution/benchmark or the state-of-the-art. 

What is the x and y axis of this scatter plot? If precision and recall are on these axes, then the range of both axes would be 0 - 1. I'm assuming one point would then represent one model, in this case you'd want to select the model on the top right, I.e. where precision and recall are both high. As for the concepts behind precision and recall, wiki has a good write up $URL$ 

Instead of thinking it in terms of GBs, unless you're asking about the physical limits of JVM, my suggested approach towards this question would be to plot the cross-validated accuracy vs. the number of rows used in the modelling dataset through stratified sampling. You might observe a point of diminishing return where additional rows do not contribute towards higher accuracy. In addition, breadth-wise in terms of the number of variables used, you could plot a variable importance plot and plot the cross-validated accuracy vs. the top X important variables. In this case, you might even observe that accuracy increases when you use fewer variables as the noisy variables are discarded. You could then select the top X variables that give you the highest cross-validated accuracy. 

Both the answers from @Emre and @Madison May make good points about the issue at hand. The problem is one of representing your string as a feature vector for input to the NN. First, the problem depends on the size of the string you want to process. Long strings containing may tokens (usually words) are often called documents in this setting. There are separate methods for dealing with individual tokens/words. There are a number of ways to represent documents. Many of them make the bag-of-words assumption. The simplest types represent the document as a vector of the counts of words, or term frequency (tf). In order to eliminate the effects of document length, usually people prefer to normalize by the number of documents a term shows up in, document frequency (tf-idf). Another approach is topic modeling, which learns a latent lower-dimensional representation of the data. LDA and LSI/LSA are typical choices, but it's important to remember this is unsupervised. The representation learned will not necessarily be ideal for whatever supervised learning you're doing with your NN. If you want to do topic modeling, you might also try supervised topic models. For individual words, you can use word2vec, which leverages NNs to embed words into an arbitrary-sized space. Similarity between two word vectors in this learned space tends to correspond to semantic similarity. A more recently pioneered approach is that of paragraph vectors, which first learns a word2vec-like word model, then builds on that representation to learn a distributed representation of sets of words (documents of any size). This has shown state-of-the-art results in many applications. When using NNs in NLP, people often use different architectures, like Recurrent Neural Nets (like Long Short Term Memory networks). In some cases people have even used Convolutional Neural Networks on text.