I'm not that familiar with SonicWALL wireless, but as I'm going through the docs of SonicOS 6.1, there are 3 modes of managing the AP's, 2 of which can apply to your situation. I'm quoting from the documentation. 

Create a new VPN connection in Windows with type L2TP and as PSK test1234. When connecting, supply as username "youruser" and as password "1234". 

The router you dial into forwards the authentication request to a RADIUS server, the client itself does not contact the RADIUS server directly. The router's configuration will have a list of RADIUS servers to use. There are numerous of configuration samples to be found on the internet, it's pretty standard and widely deployed. A quick google gave me this: $URL$ The same goes for PPPoE, the encapsulation is just different. Here we can see the following statements: 

(You might want to add some other stuff like bpduguard but that's beyond the scope of this question). How to configure the NSA, that starts on page 389 of the Administrators guide ($URL$ As the AP's will be doing DHCP, via CAPWAP they will connect to the NSA and set up a tunnel, through which all traffic will pass. When the AP's have successfully connected and the SSID's are visible, yet you cannot connect (DHCP doesn't work or DHCP does work but you can't reach outside the local network), firewall rules may be preventing communication, in which case we'd have to look further. 

I'm working on testing several FESX448-PREM switches. One of the switches in my test group is known to be bad. It was previously installed as a top of cabinet switch, 42 servers were connected to it, all port lights came on, full duplex, no errors, low CPU, etc but ports 13-24 would not forward traffic. As I understand it, this is due to a bad ASIC that covers port region 13-24. However, I now have this bad switch at my work bench and I cannot replicate the same forwarding issue with port region 13-24. At my work bench, I have port 1 as the uplink and I've been connecting my laptop to ports 2-48 sequentially using a CAT6 cable while running a continuous ping to a public IP. Interestingly, all the ports now work fine -- port region 13-24 no longer has forwarding issues. Does anyone know how this is possible? If there's a bad ASIC for port region 13-24, then I'd expect this problem to occur 100% of the time. I tried a couple other things afterwards. I had the theory that I needed more ports active at once in order to trigger the forwarding issue. So I first took a layer 2 switch and connected it on a bunch of ports with the FESX448. CPU usage immediately went to 100% on the FESX448. I figured something recursive routing was happening with the layer 2 switch. Next, I put the layer 2 switch into boot monitor mode so it wouldn't do any routing. That resolved the 100% CPU issue, but again I'm still unable to replicate the traffic forwarding issues with ports 13-24. Any suggestions on how I can replicate the forwarding issue and effectively test the remaining switches would be much appreciated! 

Multiple RADIUS servers can be defined to allow for redundancy. It would help if you provide us with the brand and type of router you have to provide a configuration sample. 

I'm assuming you are using the "local layer 3 management" mode, as that would be the best fit for your network as it is most scalable. In that case, CAPWAP (an industry standard) is used, which makes every AP create a tunnel to the controller, and all traffic is processed at the controller. So as far as the switchports to the AP's are concerned, they would have to be in access mode, with a VLAN that can connect to the NSA's wireless management port. If the above diagram is correct, you do not appear to have a wireless management VLAN or you've combined it VLAN 2 (the corporate wired LAN). So what I would do: create a new VLAN (let's pick ID 88, name: ap-mgmt) to be used to connect the access points. Set up a DHCP scope on that VLAN that sets option 138 to the IP address of NSA on which wireless management is active. Set all switchports connecting to AP's to: 

No other servers in the cabinet are experiencing packet loss. The gateway switch and bad server can ping each other without issue. If I log into another server in the cabinet and attempt to ping the bad server, then I do get the packet loss. The routing table on the bad server is fine -- the default route points to the proper gateway, no other entries exist (except for local IPv4 assignments). Firewalls have been disabled. No VPN setup is in effect (i.e., routing table on the bad server just has the default route). CPU load and network traffic are both very low. Server has been power cycled. Speed and duplex settings are set to auto-neg and are the same on both the switch and server. Forced 100mbit full on both ends, still had the packet loss. There are no port errors (no drops, collisions, FCS etc) recorded on the switch. CPU utilization on the switch is low ($URL$ 

Normally you'd have two SSID's, one "company" SSID and one "guest" SSID that maps to their respective VLAN's. However, you could use something like NAC (Network Admission Control) and "trick" the RADIUS server to send an admission deny for clients authenticating with the guest credentials. Check out this doc from Cisco: NAC Support for MBSSID 

The documentation says: "NOTE: If a port is configured as a mirror port, all traffic sent from that port will retain the encapsulation of the port being monitored and not add the encapsulation of the Egress port.". So it's a 1:1 packet copy, any tagging is preserved. 

This could easily be achieved with VRFs (Cisco, in combination with route leaking) or policy based routing. Using VRF will give you the performance advantage over policy based routing, but is more complex to configure. My blog[1] has an article on policy based routing and there are plenty of VRF examples on the internet. [1] $URL$ 

This ended up being a failing switch. A couple days later we started having issues on ports 37-48. The FESX648-PREM is powered by port ASICs which control port regions. Those regions are: 1-12, 13-24, 25-36 and 37-48. One of the failure modes on this box is that a port ASIC can die and cause forwarding problems. The "bad server" above, was the only server we had in use on the 37-48 region. So when we switched the port and re-tested, we had the same result because the failing ASIC affected multiple ports. We replaced the entire switch and that resolved the issue. 

I have a Brocade FastIron FESX648 and I'm attempting to increase the ARP cache timeout on a port that is connected to an IXP. According to the Brocade docs, I can increase the timeout up to 4 hours with the setting . I've tried applying this setting globally, specifically to the IXP-facing port and variety of other combinations (lower and higher timeout values), but nothing seems to work. Whenever I run I can see that the ARP entries for IXP hosts are continually refreshed and never age higher than 4 minutes. Has anyone faced a similar problem with Brocade FastIron gear and have any config suggestions or possible workarounds?