The other two "classical" methods are Haken's bottleneck method and Karchmer's fusion method (so named by Avi Wigderson), both much easier to apply in the monotone setting. 

The issue in play here is whether you use a self-terminating encoding (like your C example) or not. If you use a self-terminating encoding, then the subadditivity property does hold. If you don't (as in the common definition), then you need to expend bits on delimiting encodings. Self-terminating encodings have other advantages, and even though real programming languages are always self-terminating, the pioneers of Kolmogorov complexity (Solomonoff, Kolmogorov and Chaitin) defined their notion of complexity with respect to non-self-terminating encodings. The classic monograph of Li and Vitanyi treats both variants. 

Your problem is equivalent to maximum matching. In an optimal coloring, each color class is connected. Choosing one edge from each color class, we get a matching. This shows that the maximum matching is at least the antichromatic number. In the other direction, take any maximal matching, and color each of the pairs using a different color. Every other vertex is adjacent to some colored vertex (otherwise the matching isn't maximal), so you can extend this to an antichromatic coloring, showing that the antichromatic number is at least the size of a maximum matching. 

Here is an even simpler problematic situation. Let $A(k)$ be the first string (in lexicographic order) such that $K(A(k)) \geq k$; such a string provably exists for all $k$. Then $K(A(k)) \geq k$. The culprit might be that the formal system cannot compute $BB(T)$. Edit: Here is a "more explicit" problematic situation. Let $\alpha(k)$ be the maximal length of a string whose Kolmogorov complexity is at most $k$; $\alpha(k)$ provably exists. Then $K(0^{\alpha(k)+1}) > k$. 

A kind reviewer sent me a paper which proves exactly the same lower bound I do, in exactly the same way. The paper is 

The number field sieve has never been analyzed rigorously. The complexity that you quote is merely heuristic. The only subexponential algorithm which has been analyzed rigorously is Dixon's factorization algorithm, which is very similar to the quadratic sieve. According to Wikipedia, Dixon's algorithm runs in time $e^{O(2\sqrt{2}\sqrt{\log n\log\log n})}$. Dixon's algorithm is randomized. All (heuristically) known subexponential algorithms require randomization. Dixon's algorithm needs to find integers $x$ such that $x^2 \pmod{n}$ is smooth (can be factored into a product of small primes) and "random", and the number-field sieve has similar but more complicated requirements. The elliptic curve method needs to find an elliptic curve modulo $n$ whose order modulo some factor of $n$ is smooth. In both cases it seems hard to derandomize the algorithms. The nominal worst-case complexity of all these algorithms is infinity: in the case of the quadratic sieve and the number-field sieve you might always be generating the same $x$, while in the elliptic curve method you may always be generating the same elliptic curve. There are many ways around this, for example running an exponential time algorithm in parallel. 

I would like to construct a set $S\subseteq\{0,1\}^{2n}$ that satisfy the property: $$\forall x\neq y\in S\ \ \exists k\in [n]:\forall i,j\in[n], \sum_{t=k+i}^{2n}x_t\neq\sum_{t=k+j}^{2n}y_t$$ In other words, for every two vectors in $S$, there exists $k\in\{0,1,\ldots,n\}$ such that if we delete the first $k$ bits of $x,y$, we can't get the same number of $1$'s in both, after deleting some additional $i,j\in\{0,\ldots,n\}$ bits accordingly. The motivation for the problem comes from a showing lower bound for a streaming problem in which the answer may rely on approximate sliding window whose size can vary between $n$ and $2n$. A simple construction for such $S$ is $$\{0^{n+b}1^{n-b}\mid b\in[n]\},$$ As every two words $0^{n+q}1^{n-q}$,$0^{n+a}1^{n-a}$ satisfy the condition for $k=0$. This gives a lower bound on the size of such $S$ of $n+1$. If I am not mistaken (didn't get to fully prove it yet), my algorithm implies an upper bound of $O(n^2)$. 

I have actually thought about the same question (although in a completely different formulation) a few months ago, as well as some close variants. I don't have a closed form (/ asymptotic) solution for it, but you might find this view useful (are you only looking for upper bound perhaps?). The process you describe here is a generalization of the Chinese Restaurant Process, where each "table" is a subtree whose root's parent is $v_0$. This also gives us a recursion formula for your question. Denote by $h(n)$ the expected heights of a such tree process with $n$ nodes. Denote by $P_n(B)=\frac{\Pi_{b\in B} (b-1)!}{n!}$ (the probability of distribution $B$ of the nodes into subtrees). Then the quantity you're looking for, $h(n)$, is given by: $$h(n)=\sum_{B\in \mathcal B_n}P_n(B)\cdot \max_{b\in B} h(b)$$ If you wish to code this recursion, make sure you use the following so it won't go into infinite loop: $$h(n)=\frac{\sum_{B\in \mathcal B_n\setminus \{\{n\}\}}P_n(B)\cdot \max_{b\in B} h(b)}{1-\frac{1}{n!}}$$ Where $\mathcal B_n$ is the set of all partitions of $n$ identical balls into any number of non-empty bins, and $h(1)=1$. 

This problem is known as the (Undirected) Orienteering problem (I'm unaware of any work that examined distances that come from 2D Euclidean embedding). It is NP-hard (and moreover, $APX$-hard) and there exists a $2+\epsilon$ approximation for it. 

This is an extension of the classical secretary problem. In the hiring game you have a set of candidates $\mathcal C=\{c_1,\ldots,c_N\}$, and order on how skilled each worker is. W.l.o.g, we assume that $c_1$ is the most skilled, followed by $c_2$, etc. The order in which the candidates interview is picked uniformly at random and it is (obviously) unknown to the employers. Now suppose you have a market with 2 potential employers. In every round, a new candidate is interviewing for both companies (call them $A,B$). During the interview, both $A$ and $B$ observe the partial ordering of all of the past candidates, including the current interviewee. The firms then (independently) decide whether to hire today's applicant. Unfortunately for $B$, it can not compete financially with $A$'s offer, so if both extends an offer for a worker, $A$ gets preference. Also, once a secretary signs, the company may not interview any further candidates and the competitor becomes aware of the signing. The goal of each company is to hire the better skilled candidate (as opposed to the classic problem, where a single company wishes to find the best secretary), as it is known that the company with the better secretary should be able to take over the market. 

There is a totally different algorithm due to Subramanian which uses a fixed point approach. The first idea is to represent the knowledge that we have at any given point in time using intervals, which represent which women each man is considering, and vice versa. Using ideas similar to Gale-Shapley, we keep shrinking the intervals. In contrast to Gale-Shapley, this algorithm is symmetric with respect to gender. Eventually a fixed point is reached, and we can read the man-optimal and woman-optimal matchings. Subramanian found a neat way to implement this using three-valued logic and comparator networks. Check out for example this paper. Subramanian's approach was later elaborated by Feder. 

Every function $\mathbb{F}_p^n \longrightarrow \mathbb{F}_p$ (where $p$ is prime) can be written as a polynomial. For the proof, consider all $p^n$ monomials, and show that they are linearly independent. 

The definition is composed of two parts. First, we want a reduction from $f$ to $g$. That's a way of computing $f$ given an oracle to $g$. This reduction is formed by two polytime functions $\alpha,\gamma$ which are used in the following way: $f(x) = \gamma(g(\alpha(x))$. Second, we want the reduction to be one-to-one. That explains the if-and-only-if in (2). Moreover, for some reason we want an efficient one-to-one right-inverse $\beta$ of $\gamma$, satisfying $y = \gamma(\beta(y))$. The function $\beta$ can be used to answer the following question: what value of $g$ corresponds to a given value of $f$? (In contrast, $\gamma$ tells us what value of $f$ corresponds to a given value of $g$.) Why this is need will be apparent from the way the paper uses $\beta$. The actual properties stated in the definition are slightly weaker (you want the function to be one-to-one only for values which actually occur in the formulas), but this is the gist of it. 

As far as I understand this list includes all algorithms actually used to factor integers in real life. Finding just one bit of a factor sounds very unnatural to me, and I can't imagine an algorithm which can find one bit without finding all bits. (Even the SAT algorithm for factorization will find all bits at once.) 

Judging from their archive, it seems that since 1994 the Journal of the ACM is issued 6 times a year, so roughly every other month. Until 2006 it was published on odd months. From 2007 on, the schedule seems to be somewhat random, but there are still 6 issues a year (though sometimes one of them is published on the wrong year, as in 2009 and 2010). 

This class is actually a uniform version of AC. There follows an alternative characterization by Ruzzo and Tompa, appearing in a technical report by Stockmeyer and Vishkin, and later on in "Constant depth reducibility" by Chandra, Stockmeyer and Vishkin from 1984. They use the notation SIZE-DEPTH(poly, constant) (see page 3). Cook goes on to mention another unpublished characterization by himself and Ruzzo. Further results due to Ruzzo are mentioned, including $AC^k \subseteq NC^{k+1}$ ("On uniform circuit complexity", Ruzzo, 1981). The latter paper (as well as Ruzzo's earlier paper also mentioned) doesn't contain the notation AC, but rather a myriad of other notations, emphasizing the notion of uniformity used. All these papers mention alternating Turing machines a lot, giving credence to the hypothesis that A stands for alternating. 

Related result: Recent result by Bläser and Curticapean shows that weighted counting of $k$-matching in bipartite graphs is $\#W[1]$-complete. 

These data structures use $n(1+o(1))$ bits and answer such queries in $O(1)$ time (we can get improved memory bounds if some bound $m<n$ is known on the set size). 

The motivation for this question comes from a streaming algorithm for heavy hitters, where such sequence could be the worst-case scenario. $b$ here represents all "tail" items (assumed to appear no more than once) which interfere with the heavy hitter $a$ we are trying to count. 

Suppose that we have a two commodities flow network $N=<G=(V,E), s_1,s_2,t_1,t_2\in V>$. The problem is to find a minimum cost two-commodity flow in which there a flow $f_1$ from $s_1$ to $t_1$ of 2 units and a flow $f_2$ from $s_2$ to $t_2$. All edges has capacities 1 and cost 1. (In two-commodity flow the two flows share the edges capacities, that is $\forall e\in E, f_1(e)+f_2(e)\leq 1$, and the cost is defined as $\Sigma_{_{e\in E}}f_1(e) + f_2(e)$ ). In the general case, multi-commodity flow can have fractional optimum flow even if the capacities/costs are integers, but I was wondering if there's an example for it which doesn't use negative weights and only 2 commodities, so my question is: Is there be a network with my specifications that doesn't have an optimal integer solution? The following example (taken from Idan Maor's presentation) shows that that 3-MCF can have only a fractional optimal flow: 

Turns out that not only that this is doable in $O(n)$ memory bits as proposed in Benjamin Sach's answer, this can actually be done in $n\cdot(1+o(1))$ bits. The idea is to think of the $n$-bits input as the characteristic vector of a subset of $\{1,2,\ldots,n\}$ (i.e., the $i'th$ bit is set iff $i$ is in the set). Then, we can simply use a succinct dictionary to represent the set with the required number of bits. 

A more interesting example (decision is even in $P$ while counting is parameterized-hard) would be counting $k$-matchings in bipartite graph. Not only that the problem is $\#P$-complete, it was recently shown to be $\#W[1]$-hard as well ! 

Note that this is not the same as $L\cap I$ being regular (e.g. $I\subseteq L$ for some non-regular language $I$). 

First, notice that the first objective is a minimization problem, who's solution is a vector $x$, while the second is merely a number. The objective $\min_{x} E\left( \parallel Ax-b \parallel_2^2 \right)$ asks for the vector $x$ which best explains the data. If $A$ is stochastic, it still looks for the best $x$ which, on average, is the best one. The lower objective asks for "What's the expected error of the best $x$, in hindsight ", i.e. after we fix $A$, take the minimum over all $x$'s, and ask what's the error. (more formally, for different $A$ values there are different "best" $x$s, so the each time take the best one for computing the error). Since we are usually interested in finding the feature vector $x$ which explains the data, and not the expected error of the best $x$ after $A$ is fixed, the first stochastic objective makes more sense. The only reason I can think of as why would someone be interested in the second is for competitive-analysis like proofs (i.e. what's the gap between the best $x$ prior to knowing $A$ and the best one afterwards). 

It depends on whether the uniformity of your hypergraphs is bounded. Bounded uniformity In this case, you can represent each hypergraph with a colored bipartite graph which has vertices on one side (colored blue) and hyperedges on the other side (colored red). A vertex and a hyperedge are connected if the hyperedge connects the vertex. We can get rid of the colors in any number of ways, for example attach to each red a vertex a very long path which will force any isomorphism between two such graphs to match vertex colors. These graphs have bounded degree, and so GI for this class of graphs is in P. Unbounded uniformity In this case the problem is GI-complete. Given a graph $G$, create a bounded degree hypergraph whose vertices are the edges of $G$ and whose hyperedges correspond to vertices of $G$; a hyperedge contains a vertex if the corresponding vertex of $G$ belongs to the corresponding edge of $G$. This hypergraph has maximum degree 2. Two graphs are isomorphic if and only if the corresponding hypergraphs are isomorphic (this requires some argument but seems correct). 

The problem is equivalent to LP. To solve LP using an infeasibility oracle, determine a polynomial-length bound on the optimum (this reduces to raising the modulus of the largest coefficient to some power which depends on the number of variables and inequalities) and use binary search. 

De, Kurur, Saha and Saptharishi gave a modular version of Fürer's integer multiplication algorithm in their paper Fast integer multiplication using modular arithmetic, in which the p-adic numbers replace the complex numbers used by Fürer. Both algorithms give the best bit-complexity for integer multiplication. 

The algorithm you describe that runs in space $O(n_1 + n_2)$ actually recovers the final edit, and the state just before the final edit. So if you run this algorithm $O(n_1 + n_2)$ times, you can recover the entire edit sequence, at the expense of increasing the runtime. In general, there is a time-space trade-off which is controlled by the number of rows you retain at the time. The two extreme points of this trade-off are space $O(n_1n_2)$ and space $O(n_1+n_2)$, and between these, the product of time and space is constant (up to big O). 

It will be somewhat easier to replace $\{0,1\}$ with $\{\pm 1\}$, so that the parity function is just $x_1\cdots x_n$. Since this is an affine transformation, it doesn't affect degrees. I'm also assuming that the denominator of a rational function must be non-zero for all $\pm 1$ inputs. Since we only care about $\pm 1$ inputs, we work below over the ideal generated by $\{x_i^2 - 1\}$. We prove by induction on $n$ that if $P/Q$ represents parity of length $n$ then $PQ = \alpha \prod_{i=1}^n x_i + \cdots$ for some positive $\alpha$, where the dots represent lower degree terms. This implies that $\deg P + \deg Q \geq n$. The claim is clearly true for $n = 0$. Now consider $$\frac{P}{Q} = \frac{x_nP_1+P_2}{x_nQ_1+Q_2}, $$ where $P_1,P_2,Q_1,Q_2$ are over $x_1,\ldots,x_{n-1}$. Substituting $x_n=\pm1$ and applying the induction hypothesis, we deduce that for some $\alpha,\beta > 0$, $$ \begin{align*} (P_1+P_2)(Q_1+Q_2) &= \alpha \prod_{i=1}^{n-1} x_i + \cdots, \\ (P_1-P_2)(Q_1-Q_2) &= -\beta \prod_{i=1}^{n-1} x_i + \cdots. \\ \end{align*} $$ Subtracting both equations, we get $$ P_1 Q_2 + P_2 Q_1 = \frac{\alpha+\beta}{2} \prod_{i=1}^{n-1} x_i + \cdots. $$ Therefore $$ PQ = (x_nP_1+P_2)(x_nQ_1+Q_2) = x_n(P_1Q_2+P_2Q_1) + \cdots = \frac{\alpha+\beta}{2} \prod_{i=1}^n x_i + \cdots. $$