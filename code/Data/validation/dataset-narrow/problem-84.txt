Sequence numbers are negotiated and incremented on a per-socket basis. If there are two TCP sessions running between two hosts the TCP sequence numbers are completely independent. 

Nope. Something needs to provide firmware for download, initial configuration and some kind of control plane to allow the phones to find one another and negotiate codecs and such. A basic installation of Asterix or Call Manager Express on a router would be a good place to start. 

The cores go away. Generally speaking there will be some kind of L3 connectivity between the spines and leaves. It could be static routes, but almost never will. It realistically needs to be a protocol that can support equal cost multipathing (ECMP) for greater than 2 nodes - which, in any kind of sane practice, means either an IGP or BGP. Both can be valid choices and, indeed, for several popular options both are used simultaneously (iBGP + an IGP). Leaf and spine communication needs to be L3. You may use an L2 overlay (i.e. VXLAN-EVPN) to provide L2 as a service, but the fundamental premise of spine-leaf is the use of something that is capable of ECMP - which, again, is not native L2. Even L2-only protocols like Cisco's FabricPath are actually encapsulations (in the FP case it's a mac-in-mac encapsulation that's using IS-IS to advertise nodes and associated addresses). In the EVPN case, VXLAN is used as a MAC-in-UDP encapsulation while BGP provides mapping between a given tenant's MAC and IP addresses and an associated endpoint. 

Be very careful when looking at this kind of data. There's considerable variation by hardware platform and even software revision that influences whether TCP information is collected and whether that data is actually collected consistently. When it does work, the TCP information collected by Netflow is a logical AND of the TCP flags seen during that flow interval. Keep in mind that a TCP flow lasting, say, 30 minutes might actually generate between 5-6 and potentially dozens of records as aging timers hit, caches overrun or, indeed, FIN packets are seen (in the case of TCP). As such, unless something is assembling these flow records into some kind of a cohesive whole what you're actually seeing is a bunch of (hopefully) sequential chunks. If TCP flags are being correctly flagged then you might see an interval from, say, the first 30 seconds of the flow where both SYN and ACK were seen. This would create a value of 010010 (decimal 18). If retransmissions were happening later in the session - say 10 minutes in - then a later record might include 000100 (decimal 4) reflecting that the RST flag was observed. Toward the end of the overall session you might see a final record that included 000001, indicating a FIN flag. Similarly, if an entire flow occurs within the bounds of a single record then the entries for SYN, ACK, RST and FIN might all be seen (010111). During an uneventful section of the flow there also might be no flags set at all. FYI - the TCP flags (from most to least significant bits) are URG,ACK,PSH,RST,SYN,FIN. 

40G bidi (initially sold by Cisco but now generally available from other vendors) uses standard duplex LC connectors on multimode fiber. 

So you need to understand how a given switching device defines a flow. There's the obvious identifying stuff - the tuple of source and destination port/address, IP protocol number, etc. There's also clearly a beginning time and, of course, an end time. The beginning is pretty straightforward. It's the first point at which the identifying tuple is observed. The ending, however, is a lot trickier. In the case of a TCP session seeing a FIN sequence might trigger a stop time but generally the flow end-time is going to be caused by either a timeout (i.e. a UDP session where a packet hasn't been seen in n seconds) or - much more likely in practice - the flow cache filled up and space is made for new flows by triggering the purge and export of older ones. The thing is that the flow (as we've defined it) might not actually be over. Indeed, a long-lived flow might actually be represented by many consecutive flow records. It's the job of the analysis tool to tie all of these individual records together into some kind of sane whole. It's for this reason that your question doesn't really make sense. The actual flow collector/exporter likely has no idea whether the flow cache entry it's working from represents the beginning or end of the actual flow on the wire. One of the nasty truths about flow-based collection mechanisms is that the relatively limited size of the cache means that a suitably large variation in identifying tuples on the wire can lead to a degenerate case where a flow record only sees a few packets before being pushed out. This is why the use of these kinds of analytics on large-scale Internet core boxes tends to be pretty limited and in the places where flow-based measurement is occurring (aggregation/edge platforms) that it's almost exclusively sampled. So - if you want to only look at the first few seconds of a flow then set the aging time on the collection device to be very short. This will result in lots of flow records being exported (and lots more resources consumed) but will give you data on a very short interval. In turn, at the point of analysis you can choose to ignore all but the first record generated. I don't see the win to such an approach, but it at least nominally fits what you're asking for. 

There is a traditional standard for flow control (802.3x) that is a mac-layer frame that causes all traffic on the wire to pause while the signaling switch transmits its buffers. This is exactly wrong for FCoE (which responds badly to dropped frames) and is distinct from priority flow control (PFC) which is a component of Data Center Bridging (DCB). In contrast to 802.3x, PFC allows the traffic to be paused on a per-CoS basis. This is a key element in providing lossless forwarding, as non-protected classes of traffic can be slowed down such that there is always bandwidth available for critical traffic. The 3750X doesn't support DCB (..or PFC) and isn't intended as a platform for lossless Ethernet. It does support the older style of flow control. I'm going to assume that the storage in question is IP-based (i.e. Equallogic), in which case you should follow the storage vendor's recommendations and enable flow control end-to-end. Some have found anomalous issues with this setup and have gotten better results with flow control disabled, but I wouldn't try this unless dictated during troubleshooting. 

You have the basic point: your /24 can't realistically be split and advertised across multiple providers. If both sites connected to the same carrier and they opt to accept your pair of /25's then you could potentially have the aggregate the route into a /24 for advertisement to upstreams and peers while still allowing traffic to flow to the appropriate facility. Failing this you'll need to advertise the /24 from both sites and establish some kind of logical connectivity that isn't tied to that /24. As others have mentioned, provisioning a private link between the sites would accomplish this. Another option would be to build some kind of tunnel (IPSEC, GRE, etc) tied to your carrier-assigned outside address (I'm assuming both to be static). In this case you'd potentially be receiving traffic for the other site that would then have to be encapsulated and sent across the tunnel (or private link) which, depending on your setup, might represent an unacceptable degree of inefficiency. 

So you don't specify the connection type (fibre channel? Ethernet? SONET?) but it's safe to say that at 700M you're outside the realm of what should be run with multimode on just about anything. Since the link needs to be single mode you'd likely have an easier time just using OS1, which has somewhat higher attenuation but is less fragile and thus much easier to run within a building. The lesser attenuation of OS2 isn't going to be a factor until you're into many tens of kilometers. 

Simply clearing the process may not be enough to re-start the RID selection process. That said, as Ron points out in the comments, it is by far best practice to manually specify the router ID in OSPF (and, indeed, all dynamic protocols) to avoid this kind of thing. 

It's hard to address the degree of diligence of providers in blocking spoofed packets as there's a huge degree of variability out there. It is safe to say that it's still very possible to successfully send such packets from lots of places as the continued prevalence of certain types of DoS attacks attests. In practice the most common mechanism of blocking spoofed traffic is the use of unicast RPF checking. This basically causes a given interface to drop packets sourced from an address that isn't reachable by that interface (validating the packet based on the return route, in other words). uRPF is a good mechanism in general but tends to lose effectiveness the further from the edge the point of enforcement moves. Applied against an individual customer advertising a couple of /24's? Great. Applied against a peering connection with a few thousand routes? Could be ok, but can start to falter in bigger/more complex environments. Against a transit link with 600K routes? Almost useless. The other problem - paradoxically - is that it's a somewhat expensive feature to implement in hardware and tends not to be as commonly found on cheaper edge ports. 

"VXLAN" by itself is kind of meaningless. It's an encapsulation mechanism. Depending on the control plane and implementation in use it can be used to bridge L2 over L3 networks, provide multi-tenant L3 routing (again, over an L3 underlay) or even provide some measure of traffic engineering. There are a bunch of really key differences between various control planes for VXLAN - ranging from flood-and-learn (VTEP's flooding via an underlay to build a forwarding table) to proprietary/open SDN approaches to EVPN (BGP-based control plane). The last option (EVPN) is potentially useful, as it removes the need for excessive underlay flooding and is generally a lot more deterministic / controllable at scale. I've traditionally been hesitant about deploying EVPN outside of DC fabrics, as there tends to be a common failure domain in a single BGP / VTEP cluster but there has been some recent work done for multi-site EVPN. The basic idea is that the fabrics at two sites operate their own respective underlays (usually a local RR cluster and underlay reachability) with a site-to-site provisioned connection between spines (w/EBGP) to provide clear separation. You can think of it as automated VXLAN stitching between the two local domains and an inter-site. This provides some much-needed abstraction and hierarchy. Now...that said, L2 extension is often the least difficult technology in the stack when compared to keeping storage appropriately synchronized, network services (FW, IDS, load balancer) properly set up, managing asymmetric routing, etc. Some of the gyrations I've seen folks get through trying to get active/active firewall gateways going between geographically dispersed data centers are the stuff of nightmares and the potential troubleshooting woes in a properly configured multi-site setup can be exponentially worse than just running a secondary site with appropriate scripting and a nice, healthy L3 boundary. So... VXLAN-EVPN will get frames back-and-forth between sites reasonably efficiently, localize first-hop routing (i.e. anycast gateways) and (with some implementations) both locally proxy ARP and provide fabric-wide IGMP/MLD snooping but it inherently can't speak to all the nasty problems around that extended LAN... If you're curious, here is a link to a whitepaper written by the folks at Cisco about the multi-site extensions I allude to above, most of which are currently in the standards bodies for review. 

All links introduce latency. It's a trivial amount when passing traffic over a 2M Ethernet patch cable but it's substantial on a trans-Pacific circuit. Some links also have packet loss. It might be a function of the link being congested or there could be a physical issue like a flaky cable, RF interference on a wireless link or other anomalies. This is particularly relevant when simulating the behavior of a network (or application) mapped over an Internet link, where occasional packet loss is pretty much expected. So - if you're trying to simulate real-world network conditions then being able to model the what-ifs of different latencies and the possibility of packet loss can be a huge help. It allows something closer to a real-world simulation as well as testing how various network protocols, designs and actual implementations respond under different kinds of stresses. 

Traceroute (the technique) technically doesn't rely on ICMP echo requests but rather a type of ICMP unreachable. The idea is that the host sends a packet with a low TTL value and then more with successively higher values. As these packets are dropped by the various routers in the path a TTL exceeded / unreachable message is sent back. The source of this message is then added to the list of hosts in the path. As for the packet the host sends in the first place? That can vary, but to give you an idea the standard Linux command uses UDP. I believe the Microsoft command uses an ICMP echo, though. 

This question is kind of opinion-based, but I'll take a crack- 1.) The design you have is fine. Unless there's some kind of compelling functional problem I wouldn't change it just for the sake of changing it. If anything I tend to see folks moving from the design you're asking about to what you have now. 2.) In general I tend to think it makes a lot of sense to the let the switches handle any kind of local routing. It has a bunch of advantages - better performance for local traffic, separation of configuration/fault domains (FW issue doesn't cause everything to die, local issues within a subnet don't hit the firewall) and the ability to add additional firewalls/gateways to provide redundancy/traffic controls without changing end-host configurations or local site policies. 3.) It's possible to trunk the VLAN's directly to the firewall but I don't think it's a better design. You're trading the determinism, scalability and clean fault tolerance of an L3 design for one where you'll need to engineer all of this at L2. Along similar lines you can run a aggregated link (i.e. etherchannel) into the ASA but with those 2960's you'll end up with all of the connectivity run through a single switch. 

OK - so there's an old saying I learned when I started doing networking: LAN's are bridged and WAN's are routed. What this means is that networks that host lots of end stations are going to be well-served by physically proximate switches with effectively infinite, cheap, and readily available bandwidth. Lots of broadcasts and multicasts to find services? No biggie. Random bursty transfers? It's all localized, so who cares. Someone blew something up and the LAN is fried? Painful but it's only effecting people within that site and within that network. In larger enterprises this tends to mean a few dozen to a few hundred people in part of the floor of an office building. If something breaks it's a critical problem but in such situations the workers can often just take the elevator to another floor while someone fixes the problem. The problem with what you're describing (old-style bridging) is that it exactly fails at all of the above. A problem that saturates a given LAN now also saturates your most limited resource: inter-site bandwidth. That saturation means that an entire site is now potentially offline. The cause of such an issue could be a bad NIC, misbehaving app, a user who decided to run a huge backup job in the middle of the day, a misconfiguration of that STP you've been running for years. Heck, something as innocuous as an end user in a conference room accidentally plugging in two ports from the wall into a single dumb switch can cause a loop that ends up taking down both of your wireless links (and thus all communications) while someone tries to find a laptop and console cable to attempt to isolate the problem - assuming, of course, they're in the right site to help. Seriously - everything is going to run better if you run L3 over those links. You have the potential to implement traffic shaping / QoS in an intelligent way to prioritize your voice traffic before it hits the radio, you can actually engineer some traffic to normally use the backup link while sending the remainder over the primary. As Ron correctly points out, failover is measured in milliseconds rather than minutes. Want to add a third site? Easy. Try that with STP and it starts to get ugly. Add a third link? Maybe come up with a solution where you put an Internet connection in the second site and use a site-to-site VPN to carry certain traffic? No problem. The thing is that while - ideally - each site ends up with its own subnets (the model shown to scale and function reliably and the recommendation of literally every significant network vendor on the market) you could add in some kind of overlay to map L2 over the safe/sane L3 backbone. Set up L2TP or OTV via some routers and have the subnets show up wherever you want. Heck, consider some of the possibilities with VXLAN-EVPN where a modern switch can make sure any subnet shows up anywhere while still locally routing traffic over that L3 backbone. Relying on spanning tree across remote sites for redundancy is a bad idea even with two sites. It was bad in 1995 and it's no better in 2017. It gets progressively more horrible as any amount of size or complexity is added. Seriously - to this day I still periodically see horrific STP problems in major data centers operated by highly qualified dedicated crews under fairly controlled conditions. Moving this into the realm of potentially flaky long-haul wireless and workgroup-scale hardware isn't going to improve things.