I am sure that Strassen had had conversations with pure mathematicians who said something along the lines of 

It's not unitary, so it's impossible because all quantum transformations have to be unitary. Consider the states $$ \frac{3}{5} |0\rangle + \frac{4}{5} |1\rangle \quad \mathrm{and} \quad \frac{3}{5} |0\rangle - \frac{4}{5} |1\rangle. $$ These get taken to $$ \frac{1}{\sqrt{2}}\left(e^{3/5 i} |0\rangle + e^{4/5 i} |1\rangle\right) \quad \mbox{and}\quad \frac{1}{\sqrt{2}}\left( e^{3/5 i} |0\rangle + e^{-4/5 i} |1\rangle \right). $$ The inner product between the first pair of states is $-\frac{7}{25} = -0.28$, while the inner product between the second pair of states is $\frac{1}{2}(1+e^{8/5 i} )\approx 0.4585 +0.4998 i.$ Since unitary operations preserve inner products, this is not unitary. 

What you need for a Markov chain to converge to an equal distribution over all possible sequences of balls is that it is reversible: the probability of moving from sequence $i$ to sequence $j$ is the same as moving in the opposite direction. I thus propose that you use the following moves (with some fixed probability distribution to choose which type of move to make) to perform a Markov chain on all possible sequences. In the following, a "run" is a maximum-length consecutive subsequence of balls of the same colour. This Markov chain relies on there being at least three colours. 

Why shouldn't you be able to send the empty string? Shouldn't it be considered a valid message? But if you don't allow the empty string as a message, then your modified protocol will work fine, and will save you on average a small fraction of a bit per message. Is this tweak worth putting into your implementation so as to improve the performance? Let's work out how much it saves you. Suppose you're using a binary alphabet, and your average message is around 100 bits in length. You then want the termination symbol to have probability 1/100. By not allowing the possibility of the termination symbol being the first bit, you save on the order of 1/100 bits* per message. If you assume a compression ratio of 2, the total improvement in performance is 0.02%. Since 100 bits is a smaller average message length than I expect you would see anywhere, the actual performance improvement would probably be even smaller. I thus suspect that this variant would not actually be used in practice. * 1/100 nats, or 0.014 bits, to be more precise. 

Never mind. I misremembered the definition of 3-partition. I'll leave what I wrote here here, but since division into 3 subsets with equal sums is not strongly NP-complete, this doesn't work. By 3-subset sum, I mean a: given a set of integers, find a partition into 3 subsets whose sums are all equal. The problem is roughly equivalent to: let $\{p_i\}$ be the multiset of prime factors of $C$; find the best way of partitioning the multiset $\{r_i\}$ into three parts with equal sums, where $r_i = \log p_i$, for $p_i$. Here's the wrong argument I had before. ========================= Find a bunch of primes $p_1$, $p_2$, $\ldots$, $p_{\alpha(n)}$ so that $\log p_i \approx M\ i$ for some real $M$. Now, take a 3-subset sum problem on integers $1 \ldots \alpha(n)$, and replace the integer $i$ with prime $p_i$. Let $C$ be the product of all the primes $p_i$ corresponding to the integers $i$ in the 3-subset sum problem. Then, any $x$, $y$, $z$ with $x \approx y \approx z$ and $xyz = C$ gives a good partition of the integers in the 3-subset sum problem. Let $p_x$, $p_y$, and $p_z$ be the multisets of primes making up the prime factorization of $x$, $y$, and $z$, respectively. If $x$, $y$, and $z$ are close, $\log x \approx \log y \approx \log z$ means $ \sum_{p_i \in p_x} \log p_i \approx \sum_{p_i \in p_y} \log p_i \approx \sum_{p_i \in p_z} \log p_i$, and so you have a good solution to the 3-subset sum problem (and if you've chosen the primes right, you can tell from the optimal $x$, $y$ and $z$ whether or not the original 3-subset-sum problem is solvable). Since you know what the primes you used were, factoring $x$, $y$ and $z$ can be done by trial division to recover the partition. 

Look at John Preskill's Lecture Notes; particularly Section 3.2. As you noted, you can do a NAND gate by using a Toffoli gate and throwing away some of the output qubits. This results in decoherence, so you no longer necessarily have a pure state of your system. For non-unitary quantum operations, if you assume that the environment doesn't remember anything, then the most general thing you can do is a unitary operation followed by throwing away some part of the system. If you assume that the environment has memory, things get much more complicated. But note that here, your operations aren't consistent because the outcome of an operation will depend on what the input was the last time you performed the operation. I don't know if you want to include this class of operations. While they are important in physics, they aren't very useful for quantum computation. 

There is only one instance for each size n. Thus, the problem is clearly in P/poly, and cannot possibly be #P-complete unless the polynomial hierarchy collapses. In fact, it is polynomial-time computable. You can compute the first few terms of the sequence, and find that it goes 2, 6, 24, 160 ... . You can then look at the online encyclopedia of integer sequences and find it. When you look at the page for this sequence, you find a polynomial-time formula for computing it. 

If you're not looking for asymptotic results, there are extensive tables that are maintained by researchers. You can find them at www.codetables.de. Go to that webpage, and click "linear codes". They have complete tables of known upper and lower bounds for linear GF(3) (ternary) codes with $n \leq 243$. 

Why should this be true? Let the dimensions of $A, B, C, D$ all be the same. Let $\omega_{AB}$ and $\omega_{CD}$ both be pure states, with $$S(\omega_A) = S(\omega_B) = S(\omega_C) = S(\omega_D).$$ Now, let $U$ perform a cyclic permutation on $A, B, C, D$. We have $S(\tau_{AB}) = 2 S(\tau_B)$, and $S(\omega_{AB}) = 0$, while $S(\tau_B) = S(\omega_B)$. Now all you have to do is show that you can perturb this construction slightly to get the desired inequalities in your hypothesis. 

You may also want to look at Delsarte's linear programming bound, and the Gilbert-Varshamov bound. The linear programming bound gives a lower bound on the number of redundant bits necessary. The Gilbert-Varshamov bound gives a non-constructive (randomized) upper bound on the number of redundant bits required. 

The problem as presented is NP-hard. Suppose we have $f(y)=0, y\leq 1$ and $f(y) = y, y > 1$. Now if we have the constraints $y_1 + y_2 \geq 2.01$ and $y_i \geq 0$, and we want to minimize $\sum_i f(y_i)$, clearly we should choose one of these $y_i$ to be $1$ and one to be $1.01$. Now, suppose we have a graph $G$, and we put a constraint like this for every edge. Replace $2.01$ by $2 + \epsilon$ for a sufficiently small $\epsilon$. (You don't even need to do this, because vertex cover has an inapproximability result.) To minimize the objective function, we want to find the minimum number of vertices that hit every edge. This is the minimum vertex cover problem, and it is NP-complete. The only reason that this isn't exactly your problem is that the matrix $A$ is not square, because the number of variables is the number of vertices in $G$, and the number of equations is the number of edges in $G$. But you can always add dummy vertices that don't participate in the equations to make $A$ square. Possibly your real problem has more structure so that this NP-completeness proof doesn't apply. Maybe if you told us more about the real problem, it might be solvable in polynomial time. For convex $f$, this should be solvable in polynomial time ... see Chandra's comment.