-- NOTE -- The below answer worked perfectly for me. I did have to have a volume added. I created a second Filegroup and a datafile on the new drive. Additionally, another log file as well. 

What you will see is that less data can be cached and, thus, some queries will have desegregated performance. The default value for RAM just, kind of, means that SQL Server will use 'whatever it can.' Once it goes up, it won't come back down until the service is restarted as it then reserves all that RAM (because it caches data on it.) This is my favorite resource for determining how much RAM a server really needs. My only issue with it is that it doesn't really take in to account smaller VM based machines and just assumed that you CAN throw 128gb+ at a server. $URL$ A general rule of thumb is 10%, or 8GB, dedicated to OS/Auxiliary with the rest being dedicated to SQL Server (through the MAX Ram setting.) (10% or 8GB, whichever is more.) 

We have a production box that has about 60 publications on it. Some of these are used, and some seem to not be... What I need to do is create, or find, a script that will get the count of subscribers/subscriptions per publication so I can easily identify unused publications. We are using SQL Server 2005. :( Thank you, Wes 

We have a 2-node clustered SQL Server 2014, Enterprise, environment for our data warehouse. Storage, network, and servers are all high performing (1tb mem, 32 cores CPU per node, all SSD) We are trying to come up with where to put SSIS itself. Based on Microsoft documentation, it isn't recommended that it be clustered and, therefore, shouldn't be on the 2 nodes (though I did install it with the instances, we don't have to use if from there can could disable the services.) On the flip side, we don't really want to spend money on licensing of 8 vCPU's to put it on an available server in the same ecosystem (used for a proprietary ETL from vendor.) What are the best practices and recommendations on how/where to install SSIS for a clustered SQL Server database environment? Thank you, Wes EDIT: Microsoft discusses this here: $URL$ but does not go over the recommended approach as an alternative. 

I get incorrect syntax at the CASE statement but I don't quite see why. Perhaps the issue is actually how I'm accessing the inserted table. Where am I going wrong here? Thank you. EDITED CODE: 

If the driver is a solo-driver, then they have an associated Vehicle_ID If the driver is a company-driver, then they have an associated Company_ID - The company_ID would have associated (one-to-many) vehicle_ID's 

Assuming there is a 'company' table as well that drivers can be associated to? If so, one option is as follows: Driver: 

Here is my new brick wall... On this DB server, I have a 'shell' db that they will be running the SP from, that has a history table so I can keep track of who created/deleted databases using my SP's... The only parameter for sp_startpublication_snapshot is @publication... I can give it the publication name, but since I am not running it from the publishing database, how do I specify the publishing database? i.e.: the publication shows up as: 

We are researching the direction of using Snapshot backups for SQL Server, rather than more traditional backup methods. We are using CommVault (and would be for snapshot too..) I am trying to understand, better, how they work compared to traditional backups, or agent backups using CommVault. As I am not really a storage guy, I don't quite understand how the restore processes work from a snapshot backup for a SQL Database. I do understand the absolute speed benefit, but would like to hear more about the pro's and con's of utilizing snapshot backups for SQL Server. 

We have a production server that dumps it's Page Life Expectancy many times a day (climbs to 2500s and falls back to near 0s)... After watching some training videos about virtualization for SQL Server, I was pointed towards several Perfmon counters to look at: 

We have an OLTP database (running SQL Server 2008 R2) that has 6 tempdb files. All of them have an Autogrowth by 10%, restricted growth to 10000mb. At one point, someone changed one of the growths to restrict by 120000mb which caused that tempdb file to grow exponentially larger than the others. We are trying to track down when this change could have occurred. Is it possible to see, either, when a setting was changed or a history of tempdb growth? One issue is that the issue has been fixed and the tempdb file is back down to normal size. 

MySQL provides an Employee Database for instruction and learning purposes: Database on GitHub: $URL$ Instructions and Information: $URL$ 

It is still just a one to many relationship. You should be able to reference it in the exact way you referenced the other entities. 

I have a MySQL database that is storing business data... In the database, when running a select against a business name that contains an accented e (Ã©), it comes up correctly (showing me that it is stored correctly.) On our website, though, it is showing up as the diamond with the question mark. I checked out database and found the following: 

For SQL Server, Brian Kelly did a great write up at MSSQLTips.com. You can find it here: $URL$ You can enable login auditing (and specify the level of logging) on the Server Properties screen under 'Security.' To view the logs, execute 'EXEC SP_ReadErrorLog.' There are some parameters that can be added to filter, and look at older than current logs as well. You can also view events in Windows Event Log Viewer. The 'Source' will be the SQL Server instance name on the machine. 

I need to create a trigger on a table (after insert) that simply checks for a string in a cell. If the string exists, I need to log an event in the windows event logger. I am not really sure if I am able to use the inserted tables this way, but I thought I would try. This is what I have so far: 

From researching further, and a bit from the comment on my question, I found that I needed to generate the DDL statements from the previous database. Once doing so, and setting the correct directory, everything worked perfectly. 

but I am running the script from the database [WC_QACatalog] Any ideas about how to accomplish this? Thank you, Wes 

I am trying to implement some better security practices for our database systems. Right now, dbo is owned by SA. I plan to create new accounts on each server, with differing passwords, and assign SysAdmin to that local user. For many databases, dbo user is linked to the SA login. I can change these using ALTER AUTHORIZATION... no biggie. My question is, I see MASTER and some other DB's have this mapping of DBO to SA. I don't believe that the DB Owner can be set to anything but SA on these db's. Will disabling the SA account have impact on the opperation of the system databases when DBO is mapped to SA? Thank you! Wes 

The above worked perfectly. Now the error puts an entry in to my event log and I can trigger scheduled task events from it. 

This is a pretty good question. This topic, many times, gets over-thought. It really all comes down to business needs. If the need to compartmentalize data in to different subject areas (Finance, Sales, Manufacturing, Inventory, etc...) If this is far out of scope for the business cases, then you should probably stick with your standard database schema. From my view point, a single data mart is not terrible useful except for in fringe cases where a single department has needs for a data warehouse like reporting infrastructure... Then a data mart might be useful. 

We have just completed an 11g to 12c migration to a new server. There is a schema that didn't seem to come over that consists fully of external .dat files. I can't seem to find a way to simple 'attach' these pre-existing (and moved from the old server) .dat files. Is the method to get this data back in to re-create the schema, table by table, pointing to the existing .dat files? If so, is there a simple way to generate all the DDL statements for an entire schema of tables? Thanks, Wes