These latter characteristics have more to do with the limitations of sending low voltage electrical signals on copper wires. As such, they won't ever change much even with other improvements to the cable and these are also the factors that primarily go into determining the "distance" a signal can be sent on a link. So even with a better grade of cable, that doesn't really have much impact on the distance a signal can be sent reliably on a cable used for networking. 

2-ISP with Netflix The ISP on the other hand, while getting paid by the consumer notices that the vast majority of the traffic is coming from outside it's network and going to the consumer. The increased bandwidth usage means they need to expand the capacity of their network, but they don't want to increase prices to consumers, and believe Netflix should compensate them. This results in the same situation as above, with the ISP thinking along the same lines as Carrier A. 3-ISP with Consumer The ISP decides to offer a "premium" service to consumers, if they want to pay more. This would give the premium consumers' traffic priority during peak times when the ISP's network might be short on capacity. Non-premium customers may notice increased latency and lower speeds during these times. In my mind, this is an artificially created revenue stream, much like some ISPs that charged a premium for "always on" DSL/cable service. They are creating a perceived need that isn't really necessary so their customers feel better about paying more for the service. Basically, it allows them to raise their rates in a way where the consumer feels better about what they are paying for rather than getting upset. 

Is there something wrong with my config that's causing this? EDIT: On both interfaces from the core and the access stack, there is a stationary amber light which usually indicates STP has suspended the port to avoid a loop. Port Channel 1 (Access Stack) 

I've tried to find an explanation online that just simply tells me how inverse ARP is configured, and I can't seem to find a straight answer for it. Let's use the below diagram as an example. 

I've recently been studying the fundamentals on IPv6, but there are some things that I don't quite understand about how the addressing is structured. I know that link local addresses start with FE80, unique local addresses start with FC00 and global unicast addresses start with 2001 for example. Also from what I've seen, most addresses use the /64, sometimes accompanied by EUI-64. What I don't understand is the slash notation ranges with some of the address types. For example: Global Unicast 2000::/3 Link Local FE80::/10 Unique Local FC00::/7 With the above three address types for example, could someone please explain to me what the slash notations indicate? I can't seem to find a proper explanation for this anywhere. For the Global Unicast address for example, I've mostly seen 2001:0000 etc etc addresses. Does the notation mean the most I can go up to is 2003 as it's a /3?The same with the Link Local and Unique Local. The slash notations just don't make sense to me, and the books just don't seem to bring any logic to the table. I'm more than likely way off with my assumptions here, so some clarity would be good. I'd really like to understand this, rather than just assigning 2001:0000 etc etc Global Unicast addresses, without truly understanding why I'm doing it. Thanks in advance. 

This last has been alluded to by others in their answers, but this will likely permanently damage your cable. I believe the basis for this solution is in mandrel wrapping, which is a technique used in multi-mode fiber to limit or eliminate higher modes allowing for better signal loss measurements. This also results in the attenuation of the signal, and has often been used in a pinch for that purpose. However mandrel wrapping done correctly doesn't damage the cable as it follows specific standards as well. Your typical mandrel will be about 2cm in diameter, with some variance depending on the core size and jacket thickness of the MM fiber in question. Wrapping around a smaller diameter object can introduce cracks in the cable, allowing light to escape; the smaller the diameter, the more damage and the more signal loss. Additionally, by it's nature, SM fiber doesn't have multiple modes, so any real loss is a result of damaging the cable. 

Generally speaking, everything you capture at L2 is a frame, no matter if it is Ethernet, FDDI, ATM, etc. In your example of the 158223 bytes captured, 100% were Ethernet frames. 100% bytes of those frames contained L3 information. In your example, 100% of them are IPv4 packets. They could also include IPv6 or some other L3 protocol (or contain no L3 information at all). 100% bytes of those packets contain L4 information. In your examples, 100% of them are TCP segments. They could also include UDP for example. Of those segments, 32.4% bytes are HTTP and bytes 8.03% are SSL. So, to answer your question, 158223 is the size of both the TCP traffic and the frame traffic (as well as the Ethernet and the IPv4 traffic). 

I have two Cisco Catalyst 2960-X Series switches which I've stacked. I want the master to be switch 1 and the member to be switch 2. After doing research on stacking the switches, it's my understanding that the election takes place with the highest priority switch becoming the master during the election process. Once I set this, the stack was restarted and the member switch that I want keeps getting elected as the master. 

As you can see, the switch with the lowest priorty is still being elected as the master, and I don't quite understand why, unless I'm misunderstanding this election concept? If I turn on the switch I want as master first, and then turn on the member switch I can make it a master this way. However, if I ever need to restart the stack then always elects itself as the master. So to clarify what I'm after, I would like the below: 

Marking as answered as this was discovered a long time ago through numerous lab setups and research. 

As these switches are not related to eachother from a previous stack, how I do remove the switch provisioning for each switch? If I were to attempt to remove the setting, it comes to back to say that it can't be done whilst the switch is present. The first switch appears to be a master, with the other switch being the slave. Is there a config file that stores the settings for this? I've cleared the startup-configuration but this still remains. Would I have to connect them both with stacking cables to create a stack first before I can deactivate the setting? These switches will be used individually in the future, so this is why I want the stack removed altogether. Please advise. Thanks. 

Small problem in your question and two possible scenarios since you didn't provide enough details. The problem with your question is that as you laid out what can see what, C will never see a CTS from A nor will B see a CTS from D. This is the core of the exposed node problem as C will not be able to immediately send an RTS immediately after A sends it's CTS. Instead, C should wait the period of time requested in the RTS plus some before it sends its own RTS. This is the operation of some older 802.11 implementations of RTS/CTS. Newer 802.11 RTS/CTS implementations will allow C to transmit simultaneous to B if it doesn't hear A (or the CTS from A) as long as B and C are using the same timing and data rates. This helps to alleviate the exposed node problem. 

Since you are using a resource outside the network you control as your ICMP echo destination, the best you can say is that you may have an unstable connection. You would need to do additional testing, and possibly with tools other than ping, to make a real determination. ICMP ping is only a reliable tool for determining unstable connections within a network that you control and know how it handles ICMP. When you get outside your network, the only way it can be used reliably is to show that a connection is stable (i.e. good results would indicate a stable connection). While output like the above may indicate an unstable connection, it may also be the result of normal operations. The reason that ICMP is not reliable to identify an unstable connection when used like above is that often ICMP echo/reply messages are given lower priority or rate limited, creating additional latency or dropped messages. Historically, ICMP has been used in a number of different attacks which has caused many providers to implement such measures. This typically happens most frequently at the destination of your pings, but can also happen anywhere along the path to the destination depending on how the devices are configured. These limits or rules could be based on a total aggregate amount of all ICMP passing through that device (from any source) or it could be based on the traffic from a single host (or both). They can also use a number of other factors, such as the total amount of ICMP traffic your host has sent in some previous time period, the number of different destinations your host has sent ICMP traffic, and so on. In this example, the volume of ICMP traffic appears to be fairly low and would ideally not experience such problems when the ICMP traffic is not considered in aggregate with other traffic. However there are a number of ways that the same host may appear to be sending more ICMP traffic than the single ping output provided, which may cause it to exceed limits: 

I feel that I've done everything I can here with the USB stick. Is there specific software that I need to use to prepare a USB stick for the boot loader? I've tried diskpart and also Rufus, but it just seems to be the boot loader that doesn't like anything I've tried. Please adivse. Thanks. 

I know that 90 is the administrative distance, but the metric of 30720 seems high going over fast ethernet to only one hop. Could any light be shed on why this is so high in comparison to OSPF for example? I understand that this is probably a simple case of apples and oranges being two separate routing protocols, but I'm interested to know how this is calculated. 

I've been given two switches to work on, which separately have been used in a stack previously. One switch for one stack, and one switch for another st.ack in a different building. So they're not related. First switch 

Some help required please. I have a stack which contains two access switches, and an interface from switch 1 has a successful link to the core. However, when I try adding a secondary link on switch 2 back to the core, the interface is in a suspended state. Access Stack 

You don't say which switch you are replacing nor specify the type of transceiver it has (you only specify the connector). However, the only 24 port Cisco switches I know of personally that have 100Mbps MT-RJ ports utilize 100base-FX, but make sure you double check. 

Check the output of show vlan. When a VLAN doesn't show up as active in the management domain, it will almost always be because the VLAN doesn't exist on the switch. The switch may have a configuration problem with VTP or it may be configured to operate in transparent mode. 

You can do all these from your desk, and while a tap is disruptive while you are connecting/disconnecting it, it should only affect you and your devices. I would start at the top of this list, because it should allow you to capture the DHCP discover packet at least once and then any subsequent captures of the DHCP discover packet can be compared against this one to see if there is any modification. You may also want to capture DHCP discover packets from devices that are working as well to see if there are any differences from the discover packet the SRX100 sends out. Once you know where the packet goes wrong, you can look into specifically troubleshooting why it goes wrong at that point. 

I'm looking for an understanding of how the metrics are structured within EIGRP, this includes the routing table, and also the topology table. I'm sure I'm not the only one that finds the concepts of EIGRP a tad tricky to grasp. My small network has been created and I've enabled an AS of 1 on both connections for an EIGRP neighbouring relationship. This is the output from both the routing table and the topology table: 

Recently, I've been working on two spare switches which I need to stack at some point, but before I did that I needed the clear the config on both switches. Unfortunately, I made the mistake of typing erase flash: instead of nvram: and it's obviously caused me a problem. Not a massive issue though as I could just load the IOS image of the other switch as they're both 2960x switches. I've taken a USB drive and partitioned it to 2GB and formatted it as FAT16. When loading the USB onto the working switch, it accepts it without issues and allows me to copy the IOS image to usbflash0: and when doing a dir usbflash0:, it shows me the IOS image so I know it's all ready to go. When I console into the switch that does not have an IOS image, it takes me to the boot loader which is expected. However, it's at this point that usbflash0: is not recognised, and displays the following: 

The reason for this is that while the standard allows the use of a 40Mhz channel on 2.4GHz, this makes for only a single non-overlapping channel. As the instances where you can actually do this without negatively impacting surrounding 802.11 networks are incredibly rare, the industry best practices state that you should only use 20Mhz wide channels in 2.4GHz. As such, many wireless client devices will be configured by default to only utilize a 20MHz wide channel on 2.4GHz. While some drivers will allow you to adjust this setting, others will not. On MacBooks, you generally can't tweak the driver settings for most hardware options. 

You updated to say that no UDP appears to be working. I am not familiar with your product, but I have seen this type of behavior when the device uses a stateful firewall and it is configured to only allow inbound "established" traffic. TCP will establish a connection, but UDP does not. To allow UDP return traffic, you typically have to allow "related" traffic in addition to established traffic. 

If your phone system supports extenders over T1 or the two phone systems can be linked by T1 in some fashion, you could look into using something like the Cisco NM-CEM-4TE1 module. I was in an environment where they had an older Avaya system and wanted to install an extender at a remote location. We used Cisco routers with this module at the time which allowed the Avaya PBX to "hand off" a T1 to the routers which then encapsulated it as IP traffic to provide a T1 to the extender at the remote location over the WAN link. You may have to do some research as it has been awhile and some of the hardware may be end of life at the point, but there should be something similar. 

As you can see, I've configured this as an MP frame relay all in the subnet. Now I could quite easily link all of these together by creating static maps. That's not the issue. What I'd really like to know is what configuration needs to take place in order for inverse ARP to work between the routers? My guess would be to place routes on the frame relay switch from one DLCI to an outgoing interface, but how does the frame relay switch know where the DLCI is coming from? Does it need to still be mapped somewhere else, so there's communication between the routers and frame relay switch? Otherwise it just seems like a free for all. Any help would be appreciated, as I'd really like clarity on this. Thanks, 

As you can see from the above, the calculation is 30720/28160. The second part of this calculation also seems to be the feasible distance of 28160, with the first part being the metric of 30720 from the routing table. Could someone please explain to me the purpose of this calculation? And will this figure change over time? Or is this now set in stone? Thanks in advance.