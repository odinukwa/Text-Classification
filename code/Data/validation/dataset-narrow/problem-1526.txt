To clarify: you build one random forest on training data and get some results which seems to have no overfit, since CV and test results are similar. The second RF is built on the predictions of the first RF and some other additional features. The error on CV training data is very low and on test data are very high. Now we will analyze the performance of the second RF without considering how it was built. Usually if you fit data very well on training set and very poor on test set it means you simply overfit data. This happens when your model learns too much from the data. The point is that in your training data you have two kinds of information. The first one is the real structure of the data, and the second one is the noise around that data, assuming that you have enough data and your model is able to handle the useful structure. The ideal case happens when you learn only the useful structure and when leave out the noise.When that happens you will have close errors in training and test data, which means that what your model learnt is at least almost only useful data. When you learn too much, which means you learn useful and noise together, your training error decreases because the results have the noise structure which is in training data, and on test data the errors grows since the noise from test data does not look like the noise from training data. Now the first learner looks like it does not over fit. This of course does not always mean that you can't do better. It only means that what your learn is a structure which is also found in test data and thus has predictive power. So is understandable that you aim to improve the performance of the first predictor. One way would be to try to improve that predictor, and you can search for that by studying how that model learns. For example you can study if that model improves performance with more data by training repeatedly on samples of increased size and see if stabilize itself or not. There are many approaches. Another approach would be ensembles and a prominent one is stacking. But you did not understood the idea of stacking. The whole point of stacking can be expressed in the following statement: explore the model space by building various different model families and in the end get the best from all of them. So stacking starts with the assumption that since we do not know which would be an ideal model for our data, we should try many imperfect ones. Some of them will behave well in some regions of space, some others in other regions of space, but none of them will be able to handle all the space at high performance. The key point is to have models from different families. Sometimes the same algorithm can produce very different models, and sometimes not. For example two SVMs with very different kernels can produce totally different surfaces. Sometimes even for the same kernel an SVM can produce very different results for very different complexity parameter. Often for RF this is not the case, but it is not impossible to get different surfaces with different parameters on RF. So the conclusion is that stacking aims to explore different ways of learning. What you have done is to apply stacking on a single base learner by adding new features. First of all is that in general it does not work. The predictions from the first learner has a string structure, it comes from averaging on regions. So, this predictions compared with additional variables from the second model are more powerful. This means that every wrong thing learned in the first RF becomes very powerful on the second RF, because it uses the same averaging over regions procedure for learning. So the errors from the first learner amplifies in the second learner. I do not know about the additional features, but in this conditions it looks like it is very easy to overfit. Turning back to what's in a data set: this data set has a structured content which you have to learn and an unstructured content (noise) around that. Your purpose of learning is to learn only the structure. The first RF is able to distinguish the structure from noise simply because the noise is not altered. The second RF is not able anymore to make that distinction, since the predictions of the first RF are already smooth in regions, which means it has a powerful structure, at least from the point of view of the second RF, which learns in the same way. A last idea is that it looks like what you have done is somehow similar with a boosting procedure. The first learner gives you a structure which is used as a baseline by the second learner. But there is a big difference than in boosting. While in boosting you have a base line which you aim to improve, you improve that baseline by looking again at the original features, which have the noise unstructured. In your case at the second iteration you do not have again that chance and everything is structured. Thus errors amplifies. My 2 cents: try multiple learners, eventually using additional features in those learners, try different models for base learners, start with very simple (low complexity or small degrees of freedom for) the stacking learner, like a penalized logistic regression and more further to more complex ones in a controlled fashion. 

The question is very interesting and I do not remember to read about interesting answers. Because of that I dare to give you one possible solution even if it looks crazy enough. Usually one avoids having the same information in multiple features, since many algorithms can't handle that. But this is not the case of random forest. Contrasting linear regression (and all models based on similar ideas), random forests test all the features, by taking into consideration each feature one at a time. This way it is possible to code the same information in multiple ways without affecting learning performance, onyly space and running time. So my suggestion would be to create 24 features, each of the form $(h+offset)%24$. It's like when you encode the time in local time zones. Thus you give the occasion to rf to detect using the same units some interesting agglomerations around some hours, because each possible hour has the chance to be encoded properly in at least 1 of 24 features. It waste some space and time, but I would giv it a try to see how that works. 

As far as I understand you simply don't have to worry about that. So, you have a sample split randomly in control and treatment. You measure something before treatment, and after, for the same individuals. Because you measure on the same individuals than you have paired measures: delta = before - after. You are interested to measure if the mean of delta for control sample is significantly different than the mean of delta for control sample. This is done with paired sample test known also as dependent test. If you assume a normal distribution you can use paired t test. If you can't reasonably assume the normal distribution than you can use Wilcoxon signed rank test. The idea of pairings in test is to eliminate the effect of confounders. What you computed can be the effect of a confounder, but if the sample was split randomly in control and treatment you should not worry about that, unless you have strong reasons to doubt the randomization procedure. In the later case you perhaps should include confounders in the equation and take the route of random effects modeling. 

I don't understand the purpose when the quality of the tree is not important but you should be able to make a tree to have 100% accuracy on training data set easily. Just avoid any pruning and let the tree grow as maximum as possible. As far as I remember Weka set a pruning policy for J48 by default, you should disable that. Also check to see that the split nodes with minimum number of instances possible. Now that I an considering better, there are cases when full accuracy is impossible. Think for example a simple toy data set with 10 instances. Suppose that all instances are the same for input variables, but for target variable 5 are positive and 5 are negative. Either way you take a decision for prediction, the accuracy on training data would be 0.5. In this case you simply do not have enough data to discriminate on. 

A possible answer. Check to see if you have nominal variables with more than a reasonable number of levels. If that is the case then a heuristic is applied to select the best split. One such heuristic is to group levels on left to right, and that might affect the order. Anyway, it is reasonable to expect that the order of levels should not affect the output. See this link for detalied explanation on how this is implemented on matlab matlab documentation. 

This procedure exists and is called stacked generalization or simply stacking. See stacking section from wikipedia page. Strating from there you can red more by following references from the page. The first paper on subject was published by Wolpert in 1992. [Later edit] Do not combine the results with original features, but keep only the predictions combined only with the target. 

The zero time delay seems wrong there. Suppose you have an autoregressive model. A general formula could be: $$y_t = \alpha + \sum_{i}\beta_i y_{t-i} + w_t$$ Now, if $i$ is zero then you will have $$y_t = \alpha +\beta_0 y_t + w_t$$ which is really weird. It looks like you predict values at time $t$ from the values at time $t$. In plain English is like you predict the future values from the real values. In that case the error is zero, since it is not very hard to predict something from the real values. So, I think the offsets should start at $1$. 

You can do that, and you actually do that correctly. You simply misinterpret the results. For the target variable, you have instances encoded with and encoded with . The classifier does not find a proper separator and simply classifies everything to class . The obtained score is not misleading. The score is obtained as a fraction of the correct prediction count divided by the total number of predictions. Since the only correctly predicted instance count is the count of the instances labeled with $0$, the formula is simply: $$\frac{\#zeroes}{\#zeroes + \#ones} = \frac{54289}{54289+2430} = 0.9571572136321163$$ This is clearly shown also in the confusion matrix which you plotted. On the other hand what you might probably do is to put some more weight on penaly errors. Thus you probably have to play with 'C' parameter from the svm learner, probably by increasing the value of 'C' to much bigger values. It is pretty hard to find a proper value for this parameter, a rule of thumb would be to use first values of the form $10^k$, where $k$ is an integer in $[-7,7]$ and refine around good values. 

I think I can answer that, since I implement such a thing in my own library, even if I really don't know how it's implemented in other libraries. Although I am confident that if there are other ways, they don't differ too much. It took my a few weeks to understand how such a graph can be drawn. Let's start with a general function $f:\mathbb{R} \times \mathbb{R} \to \mathbb{R}$. What you want is to draw points with a colour which signifies the value of the function. One way would be to simplify the whole problem and draw one point for each pixel. This would work, but will draw only shaded surfaces and it's impossible to draw lines with various formats (dotted lines with some colours and line widths. The real solution which I found makes two simplifications. The first one would the that instead of colouring with a gradient you can colour bands. Suppose that your function $f$ takes values in $[-1, 1]$. You can split your co-domain into many subintervals like: $[-1, -0.9]$, $[-0.9, -0.8]$ and so on. Now what you have to do would be to paint a polygon filled with appropriate color, for each interval. So your original problem is simplified to draw multiple instances of a simpler problem. Note that when your intervals are small enough it will look like a gradient and even a trained eye would not notice. The second simplification would be to split the space which needs to be drawn into a grid of small rectangles. So instead of drawing a polygon on the whole surface, you need to fill each small rectangle with appropriate much simpler polygon. If it's not obvious, the problem is much much simplified. Take for example a one of the rectangle. This rectangle has four corners and you can take an additional point in the center of that rectangle (you might need that point in some certain situations). The question is how to fill with appropriate colour the proper region? You need to evaluate function in all four corners and in the center. There are some specific cases: 

Check this project on github: $URL$ It contains a comprehensive list of open source projects grouped by language, with some short descriptions. I think you can find there some of them which meet you needs. 

There are plenty of them available. I do not know if I am allowed to do this (please let me know if it is wrong), but I develop one and it has already over 2 years on git hub (it actually started one years before github). The project is called rapaio, is on git hub here and recently I started to write a manual for it (some of my friends asked me about that). The manual can be found here. It fits your needs if you are willing to develop in Java 8, if you like to do yourself any tool and if you like to experiment. There are only two principles which I enforce. The first one is write something only when you need it. That is because I strongly believe that only when you need a tool you also know what you really want from it in terms of output, performance, information. The second principle is you depend only on jdk, if you need something you will write it. I can agree that I am old fashioned, but you can tailor any feature for your purpose in this way. If I am not allowed to do that as an aswer, again, please let me know. Although, since it's an open source initiative, a give something back to the people with no profit type of project I see not reason why I could not do it. 

Among Naive Bayes assumptions the main one is that features are conditionally independent. For our problem we would have: $$P(Play|Outlook,Person) \propto P(Play)P(Outlook|Play)P(Name|Play)$$ To address question is Harry going to play on a sunny day?, you have to compute the following: $$P(Yes|Sunny,Harry) = P(Yes)P(Sunny|Yes)P(Harry|Yes)$$ $$P(No|Sunny,Harry) = P(No)P(Sunny|No)P(Harry|No)$$ and choose the probability with bigger value. That is what theory says. To address your question I will rephrase the main assumption of Naive Bayes. The assumptions that features are independent given the output means basically that the information given by joint distribution can be obtained by product of marginals. In plain English: assume you can find if Harry plays on sunny days if you only know how much Harry plays in general and how much anybody plays on sunny days. As you can see, you simply you would not use the fact that Harry plays on sunny days even if you would have had that record in your data. Simply because Naive Bayes assumes there is no useful information in the interaction between the features, and this is the precise meaning of conditional independence, which Naive Bayes relies upon. That said if you would want to use the interaction of features than you would have either to use a different model, or simply add a new combined feature like a concatenation of factors of names and outlook. As a conclusion when you do not include names in your input features you will have a general wisdom classifier like everybody plays no matter outlook, since most of the instances have play=yes. If you include the name in your input variables you allow to alter that general wisdom with something specific to player. So your classifier wisdom would look like players prefer in general to play, no matter outlook, but Marry like less to play less on Rainy. There is however a potential problem with Naive Bayes on your data set. This problem is related with the potential big number of levels for variable Name. In order to approximate the probability there is a general thing that happens: more data, better estimates. This probably would happen with variable Outlook, since there are two levels and adding more data would probably not increase number of levels. So the estimates for Outlook would be probably better with more data. However for name you will not have the same situation. Adding more instances would be possible perhaps only by adding more names. Which means that on average the number of instances for each name would be relatively stable. And if you would have a single instance, like it is the case for Harry, you do not have enough data to estimate $P(Harry|No)$. As it happens this problem can be alleviated using smoothing. Perhaps Laplace smoothing (or a more general for like Lindstone) is very helpful. The reason is that estimates based on maximum likelihood have big problems with cases like that. I hope it answers at least partially your question.