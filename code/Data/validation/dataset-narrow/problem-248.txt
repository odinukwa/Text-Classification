If we together build a comprehensive list of Oracle DBA daily/weekly/monthly routine tasks, then many Oracle new DBAs like me will benefit from it. Thanks 

Analyzed the objects routinely. Check the Index need to Rebuild Check the tablespace for respective Tables & Indexes 

Recently I'v achieved Oracle Database 11g Administrator Certified Associate (OCA) after passing two exams (1Z0-051 & 1Z0-052), then I got a Job as an Oracle DBA in startup company. So, I would like to know comprehensive list of Oracle DBA Routine Tasks, Checklists, Operations etc based on Oracle Database 11g and later version(s), I googled it and got different lists which all are seemed to be different in some areas. The following list is part of what i got from googling Daily Activity 

Oracle Database instance is running or not Database Listener is running or not. Check any session blocking the other session Check the alert log for an error Check is there any dbms jobs running & check the status of the same Check the Top session using more Physical I/O Check the number of log switch per hour How_much_redo_generated_per_hour.sql Run the statpack report Detect lock objects Check the SQL query consuming lot of resources. Check the usage of SGA Display database sessions using rollback segments State of all the DB Block Buffer 

The title was best I could find to explain my question, I don't think it helps though. Anyways, I have two tables: 

I've been asked this question but neither I seem to be able to answer it on my own, nor can I find anything related on the web. So what are the cases that might cause a runtime exception when committing a transaction in Oracle?The only thing that I can think of is the low disk space. Are there any other? 

First I rename the original tables in schema B Then, using SQL Developer I copy the table to copy from schema A to B. (I right click the table to copy and select Copy and choose the destination schema) Then I create all the indices that exist in the original tables.(Copying tables does not copy indexes, so you have to create them manually) 

Im using mysql 5.5 All of my Tables are in innoDB. Today my app went down and the mysql was unable to start. Here are my findings. 

I'm trying to import a CSV file using import export wizard, Im facing an issue. The first row is also added as a column name. Import wizard: 

I have a PostgreSQL master/slave setup. Today I created a new slave from the existing slave (cascading replication). 

I also had the same problem while implementing mirroring to one of my customers, the problem was SQL server Service account for both instances must be a domain name. Once again confirm that. And try this too. 

Then I tried to query the actual database, and I can see the primary key value twice. And tried to reindex and got this error. 

I have two tables, namely Employees and Payments. Below is the contents of the two: The Employees table 

I know that there's a one to one relationship between the document types and Instructions, so according to database normalization I should not have a separate table for it. But on the other hand I'll have to include the same columns in two different tables. What do you think? P.S. I won't be able to create a foreign key constraint between Instructions table and the two other. 

Sorry for the title but I could not find anything better that could suit my needs. I'm designing the database of a Document Management (Circulation) application. There are two types of documents - A and B. I've created a table one for each. Any document independent from its type has Instructions information that has to be attached to it. Here are the details of the Instrcutions: 

Here somehow my sec_file get deleted so I want to restore latest NDF backup, how can I restore it? Output of restore headeronly 

There are few simple solutions. Snapshot and restore Take a Snapshot of the old RDS and create your new RDS from this snapshot. $URL$ $URL$ Mydumper Use mydumper to backup and myloader for restore the db. This will be faster than native mysqldump. $URL$ Tune the parameter For Faster restore use this parameter. MySQL any way to import a huge (32 GB) sql dump faster? 

So you can create a view to converting your datetime column to the specific date format. Create table with Datetime 

Basically we don't need any Postgresql server to take the backup. Postgresql client is enough for this. But you must have sudo privilege to install this client. For Ubuntu: 

Now if I want to show only the first ten rows between 50 and 60, the only way I can think of is to first run the above query with ROWNUM pseudocolumn and then select from the result of this query. Something like this: 

I need to select PERSON_ACCOUNT.PAID_YEAR, PERSON_ACCOUNT.PAID_AMOUNT,PERSON_ACCOUNT.PAID_AMOUNT * multiplication of the YEARLY_RATES.RATE WHERE YEARLY_RATES.RATE_YEAR >= PERSON_ACCOUNT.PAID_YEAR. So for instance if I have the following data in PERSON_ACCOUNT 

Now, there might be multiple entries for one customer_id value in the table B. I need the latest date to be updated into the table A. I wonder, if I order the result by last_login_date column in Ascending order, will the merge statement eventually update the record in A with the latest last_login_date? 

I have two schemas namely A and B. I regularly copy contents of 10 tables from A to B. Here's is how I do it 

I need to setup a Highly available Postgresql Cluster. So I found Pacemaker and DRBD with pgpool load balancer are the best options. So the architecture will look like this. 

Any suggestions to achieve this? I need to map multiple Linux users. The above one is for who all are using root user. 

Recomendation: Keep you database backup in multiple locations. This will help you in case of any disaster. 

Restore collections with different pattern There is no straight forward method for this, but we can create a loop to restore this. create a file with list of collections that we need to restore 

Im using Postgresql 9.2. I have disabled auto vacuum in Conf, but still, I can see this auto vacuum is running on few tables every day. Can anyone help me find out why its running? 

But this way, I'll end up first fetching all the employees whose positions are 1 or 3 and then extract the 10 rows among them. I find this a bit inefficient. Is there any way to achieve this without making full scan first? 

After the process is over, I run queries against all the tables. Everything goes fine with 9 of them, but with one of the tables the executed query hangs. The next day, when we run the same query against the same table, the execution takes only a moment. I have no idea what the problem is. I tried not adding indexes but it didn't help. I know he way we copy table content might seem weird. 

EXP(SUM(LN(YR.RATE))) is just a trick to get the multiplication of the column values as there's no ready function available in Oracle. 

I want to setup 2 node master-master replication. Im aware of the point of failiures in this replication. Its a huge database around 1.5TB. Heavy OLTP is going on. But its mandatory to implement right now. I have 2 servers. 

Keep your databases in SSD will handle more IO in very less time. Keep temp DB in SSD, obviously, it will improve the performance of the queries. We can use SSD as buffer pool (extension). 

I have some huge tables which have around 10years data, and each table has approx 10000000+ rows. Basically, I want to remove older than 2 years data and make a copy of deleted data in a separate table(I'll export the separate table to CSV and drop it later) I found this great article: Archive huge record I want the same thing, but I don't have an ID column in my tables. So the process should like this,