For general $m$, exactly-m-sat is strictly harder than u-sat (thus does not reduce to it) unless the PH collapses. The reason is that PP can be obtained using an existential quantifier over exactly-m-SAT queries (exists m>(half of the assignments) such that exactly-m-SAT), thus if exactly-m-sat is in the k'th level of PH, then PP is in the (k+1)'st level, and then the hierarchy collapses (since P^PP contains PH). But u-sat is clearly in the second level of PH (actually in a subclass called DP). On the other hand, as @Tsuyoshi mentioned above, if $m$ is polynomial, then exactly-m-sat is many-one reudcible to u-sat. 

The answer is $\Theta(m\log m + \log\log |U|)$. In the public coins model, we have (as described above) $\Theta(m\log m)$. As Yuval suggested above, for the upper bound in the private coins model we need only an additive $O(\log n)=O(\log m + \log\log |U|)$ bits (see theorem 3.14 in the K&N book), where $n$ is the length of the encoding of the input ($n=m\log|U|$). For the additional lower bound of $\Omega(\log\log |U|)$ in the private coin model, it is enough to concentrate on the case $m=1$ (as the other items can be fixed to be all different), which is just the equality function on $\log |U|$-bit strings, whose private coin complexity is logarithmic in that (example 3.9 in K&N). 

for which every correlated equilibrium would give each player more than M/2 utility. On the other hand, consider the joint probability distribution giving probability 1/2 to each of the 1's, and thus utility 1 to each player. The claim is that this is a coarse equilibrium. In a coarse equilibrium the possible deviations of the row player are to one of the pure strategies independently of the outcome of the joint distribution. Now if it is only known that the column player is mixing evenly between the 2nd and 4th column, then the maximum utility the row player can get is 0.5+e < 1, so deviation is not profitable. 

Using limits is a bit confusing since (1) its a more complicated notion (2) it doesn't capture f=O(g) nicely (as we can see in the discussion above). I usually talk about functions from the Natural (strictly positive) numbers to the Natural numbers (which suffices for running times), skip the little-o stuff, and then the definition is concise and appropriate for 1st year undergrads: Dfn: f=O(g) if for some C for all n we have that f(n)<=C*g(n) 

In this blog post an example where there is an unbounded gap between the price of stability of CE and MN is given; I believe that something similar would show an unbounded gap for the PoA too. 

There are known functions on $n$ variables where depth $n$ branching programs (as non-oblivious BDDs are usually called) require exponential size. The book to read is by Wegner. Stronger lower bounds are given by Ajtai, and there is some newer work, but I'm not sure about which is best to read. 

NO. The following two-player game, with e very close to 0, is a potential game: (e,0) (e,1) (0,0) (0,e) Taking the 8 equivalent games (switching the roles of the players / rows / columns), then a convex combination of them gets you as close as you want to any game with positive utilities, like to matching pennies with a positive constant added to each utility. But that is not a potential game. 

You may be looking for something like Sink Equilibria (start from e.g. $URL$ ) -- but the cycle length is not considered. 

There are two aspect that need to be mentioned. The first is the general idea of defining a PRG by having its output look different than uniform to small circuits. This idea goes back to Yao and is really the strongest possible definition you can ask for when aiming explicitly at pseudo-randomness for computationally-bounded observers. The second aspect is the choice of parameters where we limit the circuit size to be $n$ and the acceptance probability difference to be $1/n$, where $n$ is also the PRG output size. This choice is somewhat different than the usual crypto one where the circuit size is $poly(n)$ and the probability difference is required to be smaller than any $poly(n)$. In our case specific parameters (rather than $poly(n)$) were needed as to get the tightest results including, in particular polynomial simulations. While in principle one could have 3 different parameters, it turned out that our results had these working essentially in the same way so we folded them to a single one (in addition to the input size $l(n)$ which was viewed as a function of $n$). 

Even for giving an IP for coNP, using current techniques, one needs to arithmetize, i.e. use counting, which means essentially the full power of #P. Any weaker prover even for coNP would be very interesting, I think (in particular would imply a new non relativing technique.) 

Today's cognitive psychology is really based on the "brain as computer" viewpoint. (Although, this may be considered as part of "neuroscience" mentioned in the question.) 

Regarding Algebraic complexity, I don't know an example where exponential complexity goes down to sub-exponential amortized complexity, but at least there's a simple example that the complexity of M disjoint copies can be significantly less than M times the complexity of a single copy: For a "random" n*n matrix A, the complexity of the bilinear form defined by A, (the function f_A(x,y)=xAy, where x and y are 2 vectors of length n) is Omega(n^2) -- this can be shown by a "counting-like" dimension argument since you need n^2 "places" in the circuit to put constants. However, given n different pairs of vectors (x^1,y^1)...(x^n,y^n), you can put the x's into the rows of an n*n matrix X, and similarly the y's into the columns of a matrix Y, and then read all the answers x^iAy^i from the diagonal of XAY, where the this is computed in n^2.3 (or so) operations using fast matrix multiplication, significantly less than n*n^2. 

Generally speaking, you can sort n numbers where each is given by O(log n) bits using time that is linear in the input size (i.e. O(nlogn)) using something like radix sort (but you may need to be careful about the exact model of computation -- and I don't know the details of which models admit such linear time algorithms). 

Rabin shows the one-wayness of (x^2 mod N=pq) without the factorization of N by a reduction showing that if you can take square roots module N=pq then you can factor N. 

One of the most appealing aspects TCS is how it uses abstract mathematical ideas for day-to-day practical applications. A presentation can focus on the abstract ideas that lie one step behind what they see daily on the Internet: Shortest paths becomes exciting once they are put in the context of friends-of-friends on Facebook. More graph algorithms can ride on Pagerank; Amazon recommendations raise the challenge of machine learning; and buying-stuff on the Internet is certainly a good lead for the public-key crypto. 

You can get an $\Omega(n^2/\log n)$ lower bound using a counting argument: there are $exp(n^2)$ $n$-term CNFs on $n$ variables (this is easy but requires just a little care as to make sure that there is no over-counting), but there are $exp(s \log s)$ formulae (or even circuits) of size at most $s$. It seems that winning the last $\log n$ factor will be hard since it would require proving a quadratic lower bound on formula size, and my guess is that the few existing techniques for this don't suffice. $O(n^2/\log n)$ may even be the right answer -- at least for circuit size -- using some four-Russian-like technique. 

Once you are not just talking about poly-time but rather look at the many models of computation we study, there are examples everywhere: In Logspace: Un-directed ST connectivity (in RL since 1979, and in L only since 2005) In NC: Finding a perfect matching in a bipartite graph in parallel (in RNC and still not known to be in NC) In interactive proofs: deterministic ones give NP, while randomized ones can do PSPACE. Related: checking a proof deterministically requires looking at all the proof, while PCP proofs allow you to check only a constant number of bits. In Algorithmic Mechanism Design: many randomized truthful approximation mechanisms with no deterministic counterpart. In Communication complexity: the equality function requires linear communication deterministically but logarithmic (or, depending on exact model, constant) communication randomly. In decision trees: evaluating an and-or tree requires linear queries deterministically but much less with randomization. This is essentially equivalent to alpha-beta pruning which gives a randomized sub-linear algorithm for game-tree evaluation. In streaming models, distributed computing models: see previous answers. 

Here is a point for AM: For a complexity class C, almost-C is define to be the set of languages that are in C relative to almost every oracle (almost = Probability 1). Then almost-P=BPP and almost-NP=AM. 

No such implication is known. In particular it may be that $L \ne P = NP$ in which case all problems (including trivial ones) are NP-hard under poly-time reductions (as the reduction can just solve the problem), but trivial ones (in particular ones that lie in L) are surely not P-hard under logspace reductions (as otherwise L=P). The same goes for NC instead of L. 

The second and fourth row and column are strictly dominated so any correlated equilibrium cannot have them in its support, thus it would be on the sub-game: 

Another natural one: finding a Nash equilibrium is (likely) not NPC, but finding one with some natural property (e.g. that maximizes the sum of player utilities) is NPC. The original NPC proof was by Gilboa and Zemel in the late 80s, and for a recent reference see, e.g., $URL$ 

In the most basic form this can be viewed as a learning problem: each keyword gives some profit (taking into account CTR, conversion rate, prices, etc.) which is not known and needs to be learned. More or less a Multi-armed bandit problem. 

Well, that is false: it is possible to evaluate M copies of ANY f using only O(N(M+2^N)) gates which can be much less than M*exp(N) (in fact, you get linear amortized complexity for exponential M). I don't remember a reference, but I think that it can go something like the following: First add 2^N fictitious inputs which are just constants 0...2^N-1 and now denote the i'th N-bit input by xi (so for i<=2^N we have xi=i, and for 2^N < i <=2^N+M we have the original inputs). Now we create a triplet for each of the M+2^N inputs: (i,xi,fi) where fi is f(i) for the first 2^N inputs (a constant that is hardwired into the circuit) and fi="*" otherwise. Now we sort the triplets (i,xi,fi) according to the key xi, and let the j'th triplet be (i_j,x_j,f_j) from this we compute a triplet (i_j,x_j,g_j) by letting g_j be f_j if f_j is not a "*" and let g_j be g_(j-1) otherwise. Now sort the new triplets back according to the key i_j, and you got the correct answers at the correct places. 

This was discussed a bit in the following survey (starting slide 21): $URL$ which mentions Euclidean TSP, approximation of actual Nash equilibrium, and talks about the classes PosSLP and FIXP. 

In some technical sense you are asking whether $P = NP \cap coNP$. Suppose that $L \in NP \cap coNP$, thus there exists poly-time $F$ and $G$ so that $x \in L$ iff $\exists y: F(x,y)$ and $x \not\in L$ iff $\exists y: G(x,y)$. This can be recast as a minmax characterization by $f_x(y) = 1$ if $F(x,y)$ and $f_x(y) = 0$ otherwise; $g_x(y) = 0$ if $G(x,y)$ and $g_x(y) = 1$ otherwise. Now indeed we have $max_y f_x(y) = min_y g_x(y)$. So in this sense, any problem known to be in $NP \cap coNP$ but not known to be in $P$ can be turned into an answer to your question. E.g. Factoring (say, the decision version of whether the $i$'th bit of the largest factor is 1). 

You can't expect this to be true since you can reduce the error of a constant-error algorithm to 1/n using an additional O(log n) bits using expanders (and btw you don't really need even these extra bits to do that). So if an error of 1/n can go down to exp(-n) error using only O(n/logn) additional bits, then you could always go from constant error to exp(-n) error using O(log n)+O(n/log n)=O(n/log n). 

This one seems related, although it focuses more on parallel time (NC) rather than the closely related space (L): $URL$ 

Fix M>>1>>e and look at the following two player coordination game (both players get the same utility): 

Let's start with the decision version of the problem: given a "satisfaction level" k, does there exist a matching where everyone gets matched to a person with at least k desired attributes. This is just solving bipartite matching on a graph where we connect a male and a female if each gets satisfaction k from the other. (Running time seems like O(mn^2).) To get the original problem, just do a binary search over k (for an extra log(m) factor in running time.) 

A good source for education purposes in general is CS unplugged, which has lots of neat CS ideas translated into high-school and middle-school activities. 

Formally, the class of functions that can be computed in polynomial time is called FP. People often say "P" instead of "FP" since the distinction is just syntactic and no real confusion will happen. 

The prototypical f and g here would probably be poly-time and polylog space. The interesting problem here is connectivity (in directed graphs) which can be solved in polynomial time (using linear space) or in polylog space (using super-polynomial time). It is a famous open problem whether it can be solved in TIME-SPACE(poly,polylog), a class known as SC. I.e. your question is a well-known open problem. I don't think that anything non-trivial is known here. 

For every question that you can't solve there's an easier variant that you can solve; for every question that you've just solved, there's a harder variant that you still can't solve. Going back and forth across the "boundary of solvability" is extremely useful as it (1) allows you to progress in baby steps (2) gives you a clearer picture of the landscape. 

A linear lower bound on deterministic CC follows by fixing one of the sets to be empty. For a randomized logarithmic upper bound, first note that this problem can be reduced to the problem asking whether the sum of three 3n-bit numbers is exactly 2^{3n}-1. This one can be solved in O(log n) randomized communication by the players operating mod a random O(log n)-bit prime. 

Generally speaking, auctions can go both ways in a dual manner: you can sell something via an auction among buyers or you can buy something via a procurement auction among possible providers (sellers). The asymmetry usually comes from where the competition lies: the auctioneer holds a "monopoly" while the bidders compete with each other. In the example of ad auctions we usually view the advertisers as having unlimited demand for "impressions" and thus there is no effective competition between websites which each can be viewed as having a monopoly over the impressions it delivers. More realistic models may take into account some limits on the demand of advertisers for impressions (e.g max impressions or budget limits) in which case taking competing websites into account too would take us out of the usual auction model and into a two-sided market scenario. Generally speaking there is no reason for the prices of a buyer-led auction to be equal to those from a seller-led one. Consider even the case of one buyer and one seller: certainly the side running the "auction" (which in this case is just setting a transaction price for the other to take or leave) can set it at the others' value in order to optimize for himself. A related example would be the stable-marriage problem where a "men-propose algorithm" produces a man-optimal matching, while a "women-propose algorithm" produce a women-optimal one. 

It depends on whether in your definition of the Oracle TM, the oracle query tape is also bounded to be of logarithmic size: if it is bounded then the PRG fools also L^A for any A too, if it is not bounded then A can contain the list of "pseudorandom strings" and thus L^A will not be fooled. 

All this hierarchy is intentionally robust under polynomial changes of the input size. Any class in it can thus contains functions whose complexity is say n^{1000000000} which would be theoretically "feasible" but certainly not practically so. These, however will most likely be very artificial problems. In particular by a counting argument there exists a family of DNF formula (=AC^0 depth 2 circuits) of size n^1000000 which no algorithm whose running time is less than n^999999 can compute. (In a uniform setting we expect something similar but can't prove it.) 

It is a long standing open problem to find an algorithm for st-connectivity that runs in simultaneously sub-linear space and polynomial time, an easier task that what you are aiming at. Such algorithms are known for the un-directed version, but even these require a large polynomial time (rather than O(km) time which would be implied by a k-pass algorithm). See especially the reference to Tompa's paper on why the directed case seems hard.