} When I render, whatever the hell I do, that Matrix.translateM resets the mv matrix and draws to world coordinates, not a local system, regardless of what it pops back to. How is this supposed to work? I've everything I can find on the web, every tutorial and even trawled open game engine source code, to no avail. Could anyone help? Just some pseudocode on how that MatrixStack is supposed to work to replicate the one in openGL3+ES1. Thanks. 

This is sort of driving me nuts, I've googled and googled and tried everything I can think of, but my sprites still look super blurry and super jaggy. Example: Here: $URL$ If you click through to the actual full size image you should see what I mean, it's like it's taking and average of every 5*5 pixels or something, the background looks really blurry and blocky, but the ball is the worst. The clouds look all right for some reason, probably because they're mostly transparent. I know the pngs aren't top notch themselves but hey, I'm no artist! I would imagine it's a problem with either: a. How the pngs are made example sprite (512x512): $URL$ b. How my Matrices work This is the relevant parts of the renderer: 

This way we're rotation the vector from the target to the origin( camera position ) about ( 0, 0, 0 ). This will give us our rotated vector relative to the target, then we simply add the target vector to get back to where we were. 

Will clear the compressed asset bundle data. This, unfortunately, destroys the bundle object as well, which means nothing is keeping track of the loaded resources. Using: 

I'm developing a multiplayer game that does not have prediction. It implements client-side interpolation. The problem with this is that input delay can be perceived. I've measured the various times involved in calculating the delay(packet arrival time, server respond time, time spent interpolating), and it seems about 50ms is spent on interpolating(correct), and 60ms is spent on ping and what I can only assume to be unfortunate message arrival time. The ping is 18ms, which leaves 42ms. If the message arrives at the server immediately after the server has finished reading it's incoming messages this frame, we suffer at least a 16ms delay(at 60fps) that wouldn't be there if the message arrived right before the reading. If this happens with both the server and the client, the delay really adds up, especially at lower frame rates. How can posts like this $URL$ claim to have exactly 50ms delay with 0 ping if this is an issue? If it's relevant, this is being developed with Unity3d and the Lidgren networking library in C#. I've moved the server snapshot writing to a different thread with a lock around the game world update method. This helped a lot, but from what I understand moving the reading to a different thread won't help in the same way, because the client won't be able to use the new information until the next frame anyway. 

I'm sure I'm doing about 20,000 things wrong, so I'm really sorry if the problem is blindingly obvious... The test device is a Galaxy Note, running a JellyBean custom ROM, if that matters at all. So the screen resolution is 1280x800, which means... The background is 1024x1024, so yeah it might be a little blurry, but shouldn't be made of lego. Thank you so much, any answer at all would be appreciated. 

I've been trying to get openGLES2 to work on my Android device and this is starting to drive me insane. I've spent well over ten hours now trying to get this to work. I just don't understand anymore. What I want is the ordinary GL3.0/ES1.0 behaviour, there's a stack of modelview matrices and when I call a 'push' function, I get a duplicate to work with and any subsequent matrix operations happen in the local coordinate space. Here's my MatrixStack class: 

You are describing a template for every meteor you are going to create. This will describe what a SINGLE meteor should contain. So, following that, what attributes should EACH meteor have? 

You need to translate them before scaling them. Otherwise, the entire coordinate space is scaled. For example: 

If draw is making the whole application slow, logic update will still be on time. If the user doesn't see a few frames, that's okay. If updates don't happen, that's not okay. That actually changes the game. If you're using a well competent 3rd part library for this, then it should have this covered for you. So, basically, everything is still based off time. View can be updated as much as possible, logic can only be updated if a certain amount of time has passed. 

Here I have targetTimeStep at 1000 / 60. This is 60fps. The HUGE advantage of this game loop is that updates will be consistent. So, all your objects can move based on time, rather than frames. No matter how slow your game draws, your updates will still be consistent. Here is where I learnt this from: $URL$ It's a great article that's certainly worth your time to read. 

With the red being the original line, and the purple being the new smoothed line. If the user suddenly stops and turns direction, we think we want it to not exactly do that but instead have a rapid turn or a loop. My current solution is using Cubic Bezier, and only using points that are X distance away from each other (with Y points being placed between the two points using Cubic Bezier). However there are two problems with this, amongst others: 1) It often doesn't preserve the curves to the distance outwards the user drew them, for example if the user suddenly stop a line and reverse the direction there is a pretty good chance the line won't extend to point where the user reversed the direction. 2) There is also a chance that the selected "good" point is actually a "bad" random jump point. So I've thought about other solutions. One including limiting the max angle between points (with 0 degrees being a straight line). However if the point has an angle beyond the limit the math behind lowering the angle while still following the drawn line as best possible seems complicated. But maybe it's not. Either way I'm not sure what to do and looking for help. Keep in mind this needs to be done in real time as the user is drawing the line. 

Unity will invoke the RPC method on ALL methods with that name that appear in any of the game object's components. Yes, you could do that, I personally wouldn't recommend it. It will add unneeded dependencies in your project, and pretty much destroys the point OOP. Right now the RPC scope thing is really quite terrible. Imagine if you have an RPC method in a base class called A. Suppose classes B and C inherit from A. Now, if you attach scripts of both B and C to a single game object( not unrealistic ), RPC calls to methods in the base class simply won't work properly. Fortunately, this seems to be getting fixed: $URL$ They seem to be sorting out the networking component of Unity. Hopefully we're not waiting too long. 

I want to instantiate an object on only the server and one client. This means Network.Instantiate is out of the question and I have to instantiate it manually. Since Network.Instantiate takes a prefab, there must be some way of identifying a prefab over a network( you can't send a reference in an rpc. ) How can I do this? Drag and dropping the prefab from the inspector is also not an option, since the component with this reference is on the server only( the client doesn't have a local reference. ) 

I have recently been trying to develop a more basic one of those 'falling sand'/'powder game' style games (See here) the past few months, and after prototyping on a plain canvas, then trying PIXI.js, I decided to try and roll my own WebGL based solution so I could control the whole pipeline. Currently I am just iterating over a 2D array representing the pixels of the canvas (500x500) then calling glDrawElements to draw a pixel sized quad at a specified location. Obviously things get pretty heavy pretty quickly in a game like this, and the frame rate drops to unusable very quickly. I can't find examples online of drawing batches of quads in different locations with one DrawElements call in OpenGLES/WebGl, as there is no instancing support, but people seem to mention having done it. Can anyone give any pointers? Here is my extremely simple vertex shader: 

From what I can gather from your code spawnedSphere is NOT the game object that has been spawned, but the prefab. Do this: 

You haven't shown any code, so I don't know for sure but I think your sprites have an origin that lies in the middle of it. Here is how the actual sprite is created in model space: 

Using these rules, a component can be enabled and disabled on state transitions and work correctly. A serialized list or dictionary can allow you to drag and drop each component into a state to allow this control from the editor. 

This is a bad way to go about it. Every time you want to move your model AT ALL you have to change every triangle. You should have one transform per mesh. I assume you have a model class, so what you'll want to do is give each model class either 3 vectors for scale, rotation, and translation or a single matrix. When you want to move the model you simply have to change the vectors or matrix accordingly. Now, right before you draw you should upload your transform to your vertex shader. Now you can use it as needed uniformly across all your vertices. This is a pretty standard way of dealing with this. A vertex should only describe it's local coordinates. That is, its position relative to other vertices in the model. You then construct your model matrix per model object, and your view matrix per viewport. Your scene should follow this structure: 

So I am using Kinect with Unity. With the Kinect, we detect a hand gesture and when it is active we draw a line on the screen that follows where ever the hand is going. Every update the location is stored as the newest (and last) point in a line. However the lines can often look very choppy. Here is a general picture that shows what I want to achieve: 

It tells me Animator.speed can only be negative when Animator recorder is enabled. Animator.recorderMode != AnimatorRecorderMode.Offline So I try putting on an Animation component on the game object, and making the clip the animation. Every time I do anything with the Animation component in script, it tells me the component must be marked as Legacy. That is no good, because when I tried making it Legacy it caused multiple problems including not being recognized by the Animator. Also GetRef errors. The entire Animation component seems like it's treated as legacy now or is just broken. So I decided well maybe I could just try to get the animation to play backwards in the animator and having the animator record it, and then using that backwards recorded animation in the actual game. I cannot figure out how to this, even if I click record in the Animation view anim.recorderMode != AnimatorRecorderMode.Record. I don't know what to do. All I want to do is play the same animation backwards without having to copy and paste an animation in the Project and moving all of the keys around so that they are reversed. Please help. Unity's animation system seems so convoluted in script. 

I'm not talking about something such as an object position, which changes very often. I'm talking about some object state that may be changed infrequently, such as a door opening and closing. Would it be better to send an RPC to saying "The door is opening/closing" or would it be better to replicated a property that tells us the objects state? Obviously this can be done both ways, but what are the advantages/disadvantages of each? As far as I know any competent implementation will only consider replication for properties that have changed. This means when the property does actually change, it should have the same cost( assuming it must be reliably replicated ) as sending an RPC. The other advantage to doing this would be removing the need for more complex RPC buffering. If a client connects, they get sent the state, no need to buffer/unbuffer things. All I can see is advantages for replicating over RPCs in this case. However, I always see suggestions to have as few replicating properties as possible.