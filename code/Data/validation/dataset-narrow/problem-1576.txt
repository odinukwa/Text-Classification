I'm using a rich dataset of Movies and I currently need to group if a Movie is the same across different Retailers. Example: Movie: Beauty and the Beast Platforms: Google, Netflix, iTunes, Amazon. I have access to signals like: Studio, Movie Name, Runtime, Language, Release Year, etc. But in the case some Movies which are not the same and signals mentioned before are not capable to find right match I need to do what a human would do: Check Movie cover. Example: 

I'm building a function to pass training information for my Tensorflow model. It is similar to Keras: 

I'm new to RL, does this scenario makes sense for live systems? Can I use offline data to train an RL model? Offline data includes Fraud cases detected historically. Reference. 

I need to do pre-processing, My prediction data contains as hence it ends up with 0, instead of 10. How can I pass data to my predictions that needs to be transformed to categorical values. If I tried to convert it to categorical I end up with a different class value. User-agent is just an example, but could be IP address or called number which corresponds to very large finite set hence a dictionary does not scale. I think this is a common problem, but not sure how can I solve it. I tried using dummies Approach#3 and I end up generating additional columns which doesn't match my prediction dataset. Issue reported here. Complete code here. 

Depends on how much data, how much features, how much computing power and which algorithm you use. In many cases you can train a model within seconds on a normal computer. The real question The problem: You have some characteristics (features) like CPU usage, memory usage, ... for every automated test. For some instances (meaning a test and the associated features) you have the binary information "normal", "abnormal". Now, when you get the characteristics of a new test, you want to automatically decide if it was normal or abnormal. This is a binary classification problem. If you have the label ("normal", "abnormal") for all instances, you can apply supervised learning methods. If you have the label only for some instances, it is semi-supervised learning. If you don't have any labels it is unsupervised. The Answer I assume you are in a supervised learning case. Then, I would suggest to use a decision tree (see - sklearn is pretty easy to use and has many standard algorithms implemented) for this decision making. It basically automatically generates many nested if/else statements to get to the decision. Hence it is interpretable and you could manually adjust or improve it. 

For my answers, I assume you are talking about batch (not mini-batch or stochastic) gradient descent. 

I tried this example and works fine for a specific number of clusters (K) where K < N | K <= N. But since searches are unpredictable need to find a way to automate the number of clusters: my goal is to cluster 2 or more similar items and let alone single searches in independent clusters, example: Cluster 1: 

I do have access to the art image. I'm using Python to do this comparison. Is there a library that can help me compare 2 images and determine if they are similar? 

I'm using Google Cloud NLP Entity Recognition to recognize Persons/People, when I paste a chunk of text, it correctly provides PERSON information: In the example: 

The dataset in use is PIMA indians which contains exactly 768 records. In Keras when I pass it runs fine. In my model, since at epoch 78 there is no more data (78 * 10), we start feeding empty information. What would be the correct behavior when training a model and number of epochs already consumed all data for model. The obvious answer would be stop training or continue feeding data to model and restart counter and start passing information from beginning. Any feedback is appreciated. 

I read about Word Embeddings and before I start coding want to check if converting my text information to embeddings may improve my results. 

The title says it all: I have seen three terms for functions so far, that seem to be the same / similar: 

This is only true if you talk about max-pooling with $s > 1$. Which is usually the case. Although I would not say "correlation" but "activation". 

I've tried to find it in the only resource they referenced in this context (D.L. Reilly, L.N. Cooper, C. Elbaum: "A Neural Model for Category Learning"), but sadly I don't have access to that one. I found an explanation on $URL$ 

Let's say we have a neural network with one input neuron and one output neuron. The training data $(x, f(x))$ is generated by a process $$f(x) = ax + \mathcal{N}(b, c)$$ with $a, b, c \in \mathbb{R}^+$, e.g. something like 

For this kind of stuff, you might want to have a look at . Image/ML specific stuff Things you can do with images: 

Hence the cross entropy loss dropped (as expected), the probability for the correct class increased (as expecte) but it makes a mistake (if you simply take the argmax). 

I've recently read about maxout in slides of a lecture and in the paper. Is maxout the same as max pooling? 

Is there literature about it? Blog posts? I know at least one example where RL went wrong / monitoring didn't quite work: 

I have 3 different datasets with company information, in all of them I have company name, but is not perfect: For example: 

Currently I have a list of Books which I need to compare to a different list. For example in my local list I have: 

Im new to ML. I'm trying to predict if a new Music Album will exceed X amount of dollars in Sales. I'm looking to build a model to go only after potential best sellers. I do have historic data for Music Sales from 2010 till 2016. I have many signals: 

I'm able to train my model and test data. When I generate my training dataset where categorical features were encoded. (Using ), this numerical data is different than my predictions. Example: 

Not all the features present in training data/test data will be present when I will be making predictions. Is this normal in ML ? What is the rule of thumb when doing feature engineering for this type of cases. 

I am completely new to analyze cluster texts, I'm using Goodreads API to get Books synopsis. My goal is to group similar books, for example: 

Since questions and answers contain HTML, URLs, special characters, quotes, single quotes, commas, etc in body property. I'm looking to convert this text into structured data which can be represented as a single text chunk and be analyzed as tokens and then I use TF-IDF. Details: 2 Objects: Question and Answers Question object has an array of answers objects as properties. Each Question and Answer objects contain a body property which is a single string containing each text. What's the recommended way to store this information? 

are in one equivalence class. According to that source, the models represents exactly the same joint probability distribution. 

First, neural networks are good in dealing with"label noise". I'm currently on mobile/vacation, so remind me to search the paper on Friday. Second, the more important question is how to get a good ground truth. Without a good ground truth you can't evaluate your models, no matter how good they might be. I see the ways: (1) have multiple experts label the stuff. Then you can make the ground truth a probability, not a simple label. If 9 experts say it is cancer and 1 says it is not, you would label it with 90% (2) wait. If you can access the patients data, it will likely be more obvious in a year (especially if it was not treated) (3) other diagnostic methods: I'm not a medical doctor, but I'm pretty sure there are invasive methods to diagnose cancer which are reliable 

Of course, one can (should?) let the agent learn all the time as products change and probably search terms / language changes. But when do I know that the agent learned something weird / that I should stop it? I can imagine the following: 

See Analysis and Optimization of Convolutional Neural Network Architectures for the pooling types in general. 

I have a dataset which contains various columns: numerical and categorical. Dataset here: I was able to process the categorical data using and features in Pandas dataframe as explained here in Approach #2. 

I provide an offline library of Music to my users. My goal is to understand what my users are looking for, which means translate raw user searches to: Music Artists, Songs, Albums and then add music to the company library. What are the suggested clustering algorithms to group common short sentences into a single entity. Example: 

While Goodreads provide genre, I would like to use synopsis and use the text for this. Lets say I will get N books synopsis like this: 

Will be potential high revenues. I found house prices example: $URL$ is it the same type of problem? I'm looking which input signals may be the highest revenue. Any insights or pointers will be helpful. 

My goal is to create a Model which I can help me understand which Music may generate a lot of revenue (Boolean). I created a field AverageRevenueGenerated which is the average of all Revenue generated for all artists. Im looking for a tool that can help me associate or generate insights based on input signals above. This cold be automatically or a specific guide that allows me to say for example if: 

I want to visualize my word2vec vector space (with zoom in and zoom out). I found a really interesting GitHub project named word2vec explorer. However, it does not seem to work in windows and mac. Hence, I am interested in knowing if there are any other off-the-shelf project I can use for this? 

I have set of newspaper articles. I want to identify important less frequent words in the set of newspaper articles. Currently I am using TF-IDF scores. However, it does not seem to be a good metric in my problem. Is there any better way of doing this? Thank you in advance :) 

I am new to Doc2Vec. As I understand Doc2Vec group similar documents based on the context of their words. I have a set of newspaper documents and I want to identify what are the main topics of the newspapers (group 'Politics' news documents to one group, 'Sports' news documents to another group etc.) based on their content. Thus, I am interested in knowing; 

I am hoping to use affinity propagation to cluster my data using . But I came across a question whether to use a distance matrix or similarity matrix in the method. Please let me know what is suitable to use? 

I'm currently using Levenshtein distance, based on that I can find out if Book is in remote system or not. In this case if Book1 vs Book2 result score exceeds X, I assume is the same Book. 

In case of Toll fraud and client being insecure, the attacker can send calls via original agent hence the Service Provider won't reject calls immediately at least based from IP information. (Toll fraud). I'm exploring which approach is the best to implement an ML model, instead of static rules to be able to detect Toll fraud in a live system: 

return ~0.5 similarity. hardwired metallica and metallica hardwire return ~0.433 Other docs with more words return higher values. (Im using cosine_similarity from sklearn.metrics.pairwise) I iterate over each document and get the similarity among all docs, after that I extract the highest values. (cosine similarity > 0.55) So far is working fine but there are cases in which I can't find similar sentences unless I reduce my coefficient, doing so it may associate other values to non-related items. I want to know what is the best technique to group common sentences from a list of sentences. Not sure if that would be semantic similarity. 

I'm downloading Stackoverflow questions & answers for a specific tag using Stackoverflow API and Python. The goal is to perform document clustering to find relevant terms across the documents and find similarity among them. Example: 

Start with a minimal network with input and output units only Learn those weights with standard algorithms (e.g. gradient descent - they seem to use another training objective which I don't quite understand, so it is gradient ascent in the paper) When the network doesn't improve, add a single new hidden unit. This unit gets input from all input nodes and all hidden nodes which were added before. Its output goes to all output nodes only. Repeat step 3 

Although I like the images, I don't see how these methods are better than simply pushing all images through the network and showing the top-$n$ images which activate the neuron of interest most. Was this evaluated? Did the authors have any insights into the features which other authors didn't have before / without those techniques? (The Zeiler&Fergus paper at least added the occlusion sensitivity analysis which does help. However, a big part of the paper is this filter visualization by deconv-nets. And I don't see how this helps to address any of the points mentioned above) 

A good answer to this question has to rely on the specific dataset / domain. The questions I would ask myself are (in this order): 

Dropping complete feature maps (hence a kernel) Dropping one element of a kernel (replacing an element of a kernel by 0) Dropping one element of a feature map 

Not sure if this is the right forum, but currently i have a dataset which contains a list of TV shows. Each record contains pricing between competitors (price in provider 1. Example: Itunes) TV show cover image, synopsis, country of origin, language, etc. Looking for ideas what project is suggested that i can prototype it, i want to learn ML and this may be a useful dataset. 

Most of the examples I have found online for LSTMs refer to "random" text generation. One of the problems that I'm trying to solve is to generate a "summary" of many docs into 1 doc. For example: 

Some other signals like company url exists, but in terms of name matching wondering if text similarity is a good approach for this grouping problem. 

Hi I'm currently trying to predict if an item will be successful in my store, this means (How much is going to sale in USD) My training dataset contains many features: 

I'm using Cosine similarity to find common documents and group them. I have realized that similar docs: 

When I want to do a prediction and I pass the original record which looks like this: (Header just for reference) 

Sundar Pichai is correctly identified as a person, but also the word users. How can I differentiate real names vs words which refer to persons? I have seen that for popular people there is metadata like Knowledge Graph mid or Wikipedia articles, but for others there is no reference, (Example: from "Susan Fowler" recent Uber scandal) Any ideas/pointers will be greatly appreciated.