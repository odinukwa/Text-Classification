Sorry if this is redundant, but due to the crazy naming of the tools, it's hard to find the answer to the question. Question 1 Will SSIS packages, reports and so forth built with Microsoft SQL Server Data Tools - Business Intelligence (SSDT-BI) for Visual Studio 2013 work on SQL Server 2008 R2? Question 2 I'm currently using SQL Server Business Intelligence Development Studio (BIDS) for Microsoft Visual Studio 2008. I want to potentially upgrade to Data Tools - Business Intelligence for Visual Studio 2013. I assume I would need to A) purchase a new copy of Visual Studio 2013 and then B) download the free SSDT-BI software? That's assuming SSDT-BI for VS2013 works for 2008 R2. 

I know there are a number of topics on this question, but I'm always seeking more insights. I have a large table with a billion+ records. The amount of records could be reduced and archived, but the size will still be large. My task is to change a existing data type of a single column where the old value of data is safe to convert into the type. Here are some of my approaches: 1 - Drop the constraints on the table that impact the targeted column, drop the indexes that also impact the targeted column, add a new column with NULL's at the end of the table, update the new column with the old column values in chunks of 10K, 50K or 100K increments, drop the old column when data has been copied and reapply indexes for that column only. 2 - Copy all data into a new table with the data type change in chunks as before, verify data is completed, drop the old table, rename the new table to the old and apply indexes. 3 - Import all data from another data source like a flat file to a new table with the data type change using BULK INSERT and MERGE SP's, basically like option 2 with having 2 duplicate tables, verify data, drop old to replace with new table and apply indexes. What would be the fastest and safest option? Are there other options I'm not considering? I've updated 100 million records for other tables really well with option 1. The bigger the table, the harder option 1 becomes due to the time duration of updating. 

The size of the blob can be queried without having to read the file to memory and calculate the size of the blob, by simply getting the file size from the filesystem: 

Another advisable strategy would be separating this query in parts, using temporary tables, step by step. This is not necessarily faster, but convenient for finding problems with the selection logic, or using a strategy that best suits our own knowledge of the data: 

Note that casting to is faster than the slution by @Sole021, but it is not UTF8 compatible (or any other encoding for that matter), returning simply the first byte, so should only be used in cases where the comparison is against plain old 7-bit ASCII characters. 

For this you could keep a table for general use, it can be quickly created with 10,000 rows like this: 

DROP SCHEMA would attempt obtaining exclusive use of the schema first, so it would only actually manage to drop the schema when PostgreSQL is done retrieving data from ongoing queries. Further queries would likely be locked until the end of the transaction in your question. Queries accessing tables in the schema may fail after this transaction is finished if they (queries or functions procedures) were compiled prior to the dropping and renaming of the schemas, but should be safe for queries from newly established connections. 

So I'd open the altering tables script, add an alteration somewhere or at the end of the script, run it, and then if there's a cascade dropping or affecting other objects, open the script for those objects and re-run it. Note that running scripts in this manner on a 24/7 system can lead to a few seconds of downtime/failures, so it would be best preparing a separate script with all modifications to be run at once. 

A possibly simple way is replacing all MS-Access tables with links to views in your SQL Server with the exact same structure as the old Access tables. If the views are simple enough (e.g. a select statement from a single table with a primary key and unmodified columns -other than renaming them-) they'll be directly updatable, otherwise you can use updatable views. 

USQL Just the base USQL job that defines the schema then selects all fields to a new directory. No transformation happening outside of leaving out the headers. The file is CSV, comma delimited with double quotes on strings. Schema is all strings regardless of data type. Extractors tried is TEXT and CSV with both set to be encoded:UTF8 even though both are default to UTF8 according to Azure documentation on the system. Other Notes This same document was uploaded in the past to BLOB storage and imported in the same fashion into Azure Data Warehouse without errors via Polybase. 

I have a document that was compressed in gunzip from a unknown source system. It was downloaded and decompressed using a 7zip console application. The document is a CSV file that appears to be encoded in UTF-8. It's then uploaded to Azure Data Lake Store right after compression. Then there is a U-SQL job setup to simply copy it from one folder to another folder. This process fails and raises a UTF-8 encoding error for a value: Ã©e Testing I downloaded the document from the store and removed all records but that one with the value flagged by Azure. In Notepad++, it shows the document as UTF-8. I save the document as UTF-8 again and upload it back to the store. I run the process again and the process succeeds with that value as UTF-8 What am I missing here? Is it possible the original document is not truly UTF-8? Is there something else causing a false positive? I'm a bit baffled. Possibilities 

I have a bulk loading process that loads millions of records into a few fact tables within a warehouse. Those tables are primarily clustered on time. The non-clustered indexes are in place for how the data is used to speed up performance. I typically drop some of the non-clustered indexes to both speed up inserts and to reduce index fragmentation after large data loads. However, this process of dropping and rebuilding is causing a huge amount of time as the data grows. Example: One table took 2 hours to apply a new non-clustered index on 100m+ rows. Likewise, if I leave the non-clustered indexes in place, they will increase the inserts 3x to 10x in some cases that force your hand to drop and rebuild. While it's awesome to drop and rebuild indexes, they don't really pan out that well as data grows in those tables. What options are available to me? Should I bulk up the server with more memory (currently 32GB) and cpu (4 vCPU)? Should I rethink my indexes? Should I find a balance of keeping some indexes on for reorganization versus dropping and rebuilding? (Note: I don't have enterprise edition.) I'm thinking my only option is enterprise edition with table partitioning where I can rebuild indexes per partition as opposed to the whole table. 

Why are you using ? Why not just ? See perl's documentation on Finally, you can simply (and elegantly): 

NOTE: MySQL Workbench may not allow running the whole script at once. Run until first, then run the rest. 

Without knowledge of your schema, query attempted and statistics from explain analyze, any response can only deal in generics. In this sense and in terms of SQL, there are generally two commonly used strategies for dealing with finding missed relations: and . NOT EXISTS: 

You could create a third table C, with a primary (or unique) key defined in it, and create before each row INSERT, UPDATE and DELETE triggers in both of your tables A and B that insert, update or delete their key into table C. You could use table C for foreign keys also, if you need to. 

You can also set it as a default for any user or role (takes effect after a new connection is established): 

With your datasets, MySQL has to obtain those 450,000 records from posts (in 1000 little chunks from each matching source_id), sort it, and then return the top 10. It is a costly exercise. You could resort to using a stored procedure, and accumulate results going back in time, say daily or weekly, looping until obtaining at least 10 records, and then returning the 10 most recent ones. You'll need an index on by . It would return quickly for the most recently active users, but take much longer for users without recent posts. Something like the following: 

Ensure has an index (or key much better if applicable) on ! You run the function several times for each row, this is likely taking a significant toll. I've replaced it with string_to_array, and then fetch the individual pieces as needed. I also didn't understand what you intended to obtain for the field using reverse? The query below returns the last element for it. Your query returns many million rows. Even if PostgreSQL processes the query reasonably quickly, your client application (especially if you use PgAdminIII) will struggle allocating enough memory and receive and format the results, and probably be what takes the most time. So you may want to create a temporary table with the results, and then query against the temporary table: