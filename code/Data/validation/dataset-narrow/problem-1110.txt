To optimize for sequences of $k$ consecutive numbers, you can use a greedy algorithm. You generate each element successively. You do this by working with a (modifiable) copy of the $p_i$'s, which I'll refer to as $q_i$'s. For the first element, pick the $i$ with the greatest $q_i$, and I'll call the result $r$. Then set $q_r = q_r - 1$. Next, for all $q_i$, including the one you just picked, set $q_i = q_i + p_i$. Now you can repeat this procedure over and over, each time picking the greatest $q_i$. This algorithm distributes the $i$'s evenly throughout the series. Any subset of consecutive numbers will be about as close as is possible to representing the distribution of the entire series. It also turns your series into repeating cycles of digits. 

I'd like to know some good starting points (such as books, papers, lecture notes, etc.) on EXPTIME and EXPSPACE. I'd like to learn more about these two topics, but I'm not sure what the best approach is.... 

Given in this problem is a set of values $0 \le c_{a,b} < n$, where $0 \le a < n$ and $0 \le b < n$. The problem is to find the following sum as quickly as possible: $$\sum_{a,b}{c_{a,b}x^a y^b} \bmod n$$ Here $x$ and $y$ are also assumed to be integers modulo $n$. I'd like to know what the best method/approach is to solving this problem, and what kinds of worst-case behavior we can expect to get the sum. Are there any similar problems whose solutions may help with this? SOME NAIVE IDEAS There are $n^2$ different constants and only $n$ different possible assignments for each $c_{a,b}$, so it seems that grouping some of the coefficients together is the best way to go. However, beyond simple ideas, I'm not sure what will work best. I'm hoping I can get some good suggestions. MOTIVATION My motivation for tackling this particular problem is that its solution may help improve the bounds of #3-SAT by some small degree. However, the relation between the two is fairly complicated. 

I'd like to know where I can turn for a good, gentle introduction to k-SAT (this may be for mathematicians that may not have a good computer science background). I'd also like to know papers that maybe survey or explain current methods used to solve k-SAT. Finally, I'm interested in the best known methods for solving k-SAT. I'd like to get an idea of the best average case and best worst case behavior. In short, I'm looking for papers that will help someone in mathematics (not computer science) become much more of an expert in k-SAT. 

I'm wondering if someone can provide a good algorithm for the following problem. If we take 3-SAT in conjunctive normal form, we can partition some or all of the variables (not the literals) into sets. I'm interested in finding a collection of sets such that at least two of the variables in every 3-SAT clause are in the same set. We can call this a 2-partitioning. In order to really solve the problem I'm working on, I need to assign weights to the sets. If there are $\alpha$ variables in a set, we assign the weight $2^\alpha$ to the set. We can then total all of the weights of the sets. For example, if we can find three sets that have sizes $\alpha_1, \alpha_2, \alpha_3$, then the total weight is $2^{\alpha_1} + 2^{\alpha_2} + 2^{\alpha_3}$. I'm looking for an algorithm that finds a 2-partitioning in time/space less than the total weight. So, how fast can we make this algorithm? EXAMPLE 2-PARTITIONING If we have the formula: $$(\underbrace{x_1 \lor \neg x_2}_\text{set 1} \lor x_3) \land (\underbrace{\neg x_4 \lor x_5}_\text{set 2} \lor x_6) \land (\underbrace{\neg x_1 \lor \neg x_6}_\text{set 1} \lor \neg x_7)$$ We can create sets $\{x_1, x_2, x_6\}$ and $\{x_4, x_5\}$ so that at least two of the variables in each clause are part of the same set, as shown above. The total weight is therefor $2^3 + 2^2 = 12$. 

Has anyone explored running times of 3-SAT or #2-SAT given by the occurrences of the highest occurring variable? In other words, if the variable that appears most often appears $x$ times, has anyone been able to give a runtime that is a function of $x$? 

In essence, I'm trying to get a better feel for when there is a use for FFT with small coefficients, compared to the length, assuming that we get a better runtime. I've been toying with an idea for a Fourier transform with elements of small size compared to the length of the transform. It seems that the running time is $O(n^m \log_2{(n^m)})$ total bit operations of size $n$, but it doesn't require any multiplications. I'm wondering if this could be useful, or if this can be totally circumvented with smaller transforms. In the case that it does seem useful, or comparable to other situations, what are the uses for it? Also, if we suppose that it requires $O(f(m) \cdot n^m)$ multiplications, could it still be useful? As an example, it seems to me that this would decrease the depth of a multiplication algorithm, since we can decrease the size of the coefficients that we use recursion on. 

So I'm wondering, first off, where I can read up to get a feel for state-of-the-art matrix multiplication concepts. I'll try to be more specific: I'm wondering if there has been research on circuits that compute a pre-defined matrix function. For instance, if we have a function such as $(AB)+C+(DEF)(AB)$, where all the variables are matrices with given sizes, how efficient can we make a circuit that computes this function? I've been toying with an idea in which the entries are all computed modulo $p$. So I'm wondering, broadly, what I can read up on to be up-to-date. 

Basically, if we are given a natural $x$, in binary, with $x < m_0 \cdot m_1 \cdot m_2 \cdot \dots \cdot m_n$, how quickly can we find $x \bmod m_0$, $x \bmod m_1$,..., and $x \bmod m_n$? In other words, how quickly can we find $n$ residues in modular arithmetic? Note that the residues needn't be primes. I suppose that this is a fairly broad question, and I'm curious if there are special cases where we can find these residues much faster (in other words, if there are special values for the modulus that allow faster calculations). My motivation is that I'm entertaining some ideas concerning integer multiplication. Along these lines, I'd like to break apart numbers into smaller residues. I'm interested in any results besides the trivial situation when some of the moduli are powers of 2. 

Thinking about residue number systems, one major operation is to extend the set of primes that a given value is modulated by, also known as base extension. For instance, a given number $N$ can be represented by a sequence of remainders or modulated values by another set of numbers, which are primes. We then have: $$N = (n_0, n_1, \dots, n_m)$$ where $$n_0 \equiv N \bmod p_0$$ $$n_1 \equiv N \bmod p_1$$ $$\dots$$ $$n_m \equiv N \bmod p_m$$ Suppose we have a new sequence of additional primes, $(p_{m+1}, p_{m+2}, \dots p_{m+q})$. How quickly can we get the new sequence of remainders (or modulated values), $(n_{m+1}, n_{m+2}, \dots n_{m+q})$ for the same number, $N$, in the worst case? 

Please accept my apologies ahead of time since I fear that this isn't an adequate question for cstheory. I plan on releasing my ideas to get feedback, but I don't know if my target audience will include state-of-the-art reaseachers. I'd like to know more about the research that is already out there concerning this question. Essentially, what I think I've discovered is an oracle. The oracle must be precomputed, which will probably take an exponential amount of time. However, once computed, it can essentially answer any question about 3-SAT fast. In other words, it can then solve any 3-SAT problem in polynomial time/space. I'm wondering what has been researched in this scenario. MY ORIGINAL QUESTION As an armchair cstheory enthusiast, I'm now working on a way to preprocess 3-SAT. I'm wondering how this relates to research on oracles and related ideas. Please let me briefly describe my method, or conjectured approach. We take a given problem size. Essentially we have exactly $2^n$ clauses and $v$ variables. (Note that problems smaller than this can automatically be solved, too.) After we know the maximum amount of clauses, we can preprocess information for every possible 3-SAT clause, given the information just stated. This may take an exponential amount of information, even for each individual clause. However, once the information is preprocessed, we can solve 3-SAT fast. In other words, using this pre-computed database, which is polynomial in size, we can determine satisfiability for a particular instance as well as generate a certificate in polynomial time and space. So if I'm correct, we use an oracle and we can ensure that 3-SAT can be solved in polynomial time and space. However, the oracle itself takes a possibly exponential amount of time and space to compute. My question is where in the literature can I find information concerning pre-computing information (like I have described) to solve NP-hard and NP-complete problems in polynomial time and space? What is known about this approach? I don't mean to be vague, but I'm simply trying to find out what has been considered about an approach like this in the past. I plan on eventually releasing my ideas if they are interesting, but I would like to know if they have already been researched. 

My understanding is that if we have a totally random k-SAT formula, for a ratio $m < \alpha n$ far enough below the satisfiability threshold, we can solve for satisfiability in polynomial time (with a high probability of success as $n$ tends to infinity). What I'm curious about, and which I guess is the beginner's question, is what happens if we increase $k$, $m$, and $n$. To elaborate, we can suppose that the original formula is significantly above the threshold, and therefor no polynomial method is guaranteed. However, the new increase in $k$ puts the formula significantly below the threshold, and so if the new formula was random, it could be solved in time polynomial in the new $n$. However, the new formula is not random. It's based on the old one. So it seems that the polynomial methods don't apply. My curiosity is, if we could generate several formulas with increased $k$, $m$, and $n$, below the threshold, they may in some way be considered random. So perhaps the randomness criterion can be satisfied. I'm wondering if there is any way to prove this... ANOTHER APPROACH Perhaps what I'm really after is finding out what constitutes randomness in satisfiability instances. Of course it's a random formula with the proper parameters, but doesn't this mean that if we can repeatedly generate formulas with the same parameters, they can be considered random? I'm really hoping that someone can give me a better perspective on things. I keep wondering if there's some way to modify SAT formulas to take advantage of polynomial methods.