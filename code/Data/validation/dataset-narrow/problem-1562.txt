Perhaps a good way to paraphrase the question is, what are the advantages compared to alternative formats? The main alternatives are, I think: a database, text files, or another packed/binary format. The database options to consider are probably a columnar store or NoSQL, or for small self-contained datasets SQLite. The main advantage of the database is the ability to work with data much larger than memory, to have random or indexed access, and to add/append/modify data quickly. The main *dis*advantage is that it is much slower than HDF, for problems in which the entire dataset needs to be read in and processed. Another disadvantage is that, with the exception of embedded-style databases like SQLite, a database is a system (requiring admnistration, setup, maintenance, etc) rather than a simple self-contained data store. The text file format options are XML/JSON/CSV. They are cross-platform/language/toolkit, and are a good archival format due to the ability to be self-describing (or obvious :). If uncompressed, they are huge (10x-100x HDF), but if compressed, they can be fairly space-efficient (compressed XML is about the same as HDF). The main disadvantage here is again speed: parsing text is much, much slower than HDF. The other binary formats (npy/npz numpy files, blz blaze files, protocol buffers, Avro, ...) have very similar properties to HDF, except they are less widely supported (may be limited to just one platform: numpy) and may have specific other limitations. They typically do not offer a compelling advantage. HDF is a good complement to databases, it may make sense to run a query to produce a roughly memory-sized dataset and then cache it in HDF if the same data would be used more than once. If you have a dataset which is fixed, and usually processed as a whole, storing it as a collection of appropriately sized HDF files is not a bad option. If you have a dataset which is updated often, staging some of it as HDF files periodically might still be helpful. To summarize, HDF is a good format for data which is read (or written) typically as a whole; it is the lingua franca or common/preferred interchange format for many applications due to wide support and compatibility, decent as an archival format, and very fast. P.S. To give this some practical context, my most recent experience comparing HDF to alternatives, a certain small (much less than memory-sized) dataset took 2 seconds to read as HDF (and most of this is probably overhead from Pandas); ~1 minute to read from JSON; and 1 hour to write to database. Certainly the database write could be sped up, but you'd better have a good DBA! This is how it works out of the box. 

Probably the most useful (and fun) answer is provided by the excellent Swami Chandrasekaran ... in the form of a subway map: 

Do you know if the scheduler has a memory? Let us assume for a moment that the scheduler has no memory. This is a straightforward classification (supervised learning) problem: the inputs are X, the outputs are the schedules (N->M maps). Actually, if every N gets scheduled and the only question is which M it gets, the outputs are lists which channel (or none) is scheduled to each block, and there is only a certain possible number of those, so you can model them as discrete outputs (classes) with their own probabilities. Use whatever you like (AdaBoost, Naive Bayes, RBF SVM, Random Forest...) as a classifier. I think you will quickly learn about the general behavior of the scheduler. If the scheduler has a memory, then things get complicated. I think you might approach that as a hidden Markov model: but the number of individual states may be quite large, and so it may be essentially impossible to build a complete map of transition probabilities. 

This is a huge topic, so I will just give you a high-level overview and some pointers to more info. Yes, there are definitely ways to work with video that are different from working with individual still images. At the simplest level, it could be running an object detector such as HoG (or a sliding-window convnet) on each frame, and then some means of assigning nearby detections in adjacent frames to be the same object, and discarding detections which do not seem to have continuity in time. Many of the algorithms in this field seem to look at a single frame as a building block to looking at the whole sequence, where perhaps the data from adjacent frames is combined, aggregated, and/or used to disambiguate the current frame. Another approach is to first estimate object motion between frames (using optical flow, phase correlation, pyramid block matching or another method) and then treat areas of multiple frames that are collocated after accounting for motion as being the same object. This is very powerful but limited by the accuracy of the motion estimation. In newer research, there is a back-and-forth between finding where things are (detection) and how things move (tracking), where each task can help the other one, for example (Kalal 2010) or (Andriluka 2008), to the point that the two parts of the algorithm are no longer separable. Kalal's TLD algorithm is a recently famous version of this. There are also algorithms which work directly in the spatiotemporal (or sometimes just temporal) domain. An example of purely temporal would be detecting vehicles by the periodicity of variation of their wheel spokes. Some of the frequently studied model problems are: 

I am interested in any data, publications, etc about what is the smallest neural network that can achieve a certain level of classification performance. By small I mean few parameters, not few arithmetic operations (=fast). I am interested primarily in convolutional neural networks for vision applications, using something simple like CIFAR-10 without augmentation as the benchmark. Top-performing networks on CIFAR in recent years have had anywhere between 100 million and 0.7 million parameters (!!), so clearly small size is not (always) a bad thing. Small networks are also in general faster to train and overfit less. Moreover, recent work on Knowledge Distillation, FitNets, etc show ways of making smaller networks from large networks while preserving most of the performance. Another question is, what is the best performance achievable with a network no larger than a fixed size? Examples of especially small networks that get good performance (100k parameters with 10% on CIFAR, anyone?) or systematic studies of the size vs performance tradeoff would be appreciated. 

The last two statements imply that the right way to combine variables into a cost function is something similar to min(v1, v2, ...). But that has a problem, in that the direction of optimization would always be determined by the lowest variable alone. Intuitively, it is desirable to trade off a slight worsening in one variable for a large improvement in another, even if the variable that is getting worse is already pretty bad, for example: 30 and 50 is less preferred than 28 and 75 (or something like that). This implies some relative elasticity of variables which in this case is close to 100:1 for variables near the opposite ends of the range, and 1:1 for variables which are equal valued. How can a good cost function be constructed for this kind of problem in a general-purpose way? (with some adjustable parameters for elasticity, etc) 

Yes, this is a straightforward application for neural networks. In this case yk are the outputs of the last layer ("classifier"); xk is a feature vector and yk is what it gets classified into. For simplicity prepare your data so that N is the same for all. The problem you have is perhaps that in the case of time series you won't have enough data: you need (ideally) many 1000's of examples to train a network, which in this case means time series, not points. Look at the specialized literature on neural networks for time series prediction for ideas on network architecture. Library: try Pylearn2 at $URL$ It's not the only good option but it should serve you well. 

I think a number of clustering algorithms that normally use a metric, do not actually rely on the metric properties (other than commutativity, but I think you'd have that here). For example, DBSCAN uses epsilon-neighborhoods around a point; there is nothing in there that specifically says the triangle inequality matters. So you can probably use DBSCAN, although you may have to do some kind of nonstandard spatial index to do efficient lookups in your case. Your version of epsilon-neighborhood will likely be sim > 1/epsilon rather than the other way around. Same story with k-means and related algorithms. Can you construct a metric from your similarity? One possibility: dist(ei, ej) = min( sim(ei, ek) + sim(ek, ej) ) for all k ... Alternately, can you provide an upper bound such that sim(ei, ej) < sim(ei, ek) + sim(ek, ej) + d, for all k and some positive constant d? Intuitively, large sim values means closer together: is 1/sim metric-like? What about 1/(sim + constant)? What about min( 1/sim(ei, ek) + 1/sim(ek, ej) ) for all k? (that last is guaranteed to be a metric, btw) An alternate construction of a metric is to do an embedding. As a first step, you can try to map your points ei -> xi, such that xi minimize sum( abs( sim(ei, ej) - f( dist(xi, xj) ) ), for some suitable function f and metric dist. The function f converts distance in the embedding to a similarity-like value; you'd have to experiment a bit, but 1/dist or exp^-dist are good starting points. You'd also have to experiment on the best dimension for xi. From there, you can use conventional clustering on xi. The idea here is that you can almost (in a best fit sense) convert your distances in the embedding to similarity values, so they would cluster correctly. On the use of predefined parameters, all algorithms have some tuning. DBSCAN can find the number of clusters, but you still need to give it some parameters. In general, tuning requires multiple runs of the algorithm with different values for the tunable parameters, together with some function that evaluates goodness-of-clustering (either calculated separately, provided by the clustering algorithm itself, or just eyeballed :) If the character of your data doesn't change, you can tune once and then use those fixed parameters; if it changes then you have to tune for each run. You can find that out by tuning for each run and then comparing how well the parameters from one run work on another, compared to the parameters specifically tuned for that. 

Here are some more references: Shah, Mubarak, and Ramesh Jain, eds. Motion-based recognition. Vol. 9. Springer Science & Business Media, 2013. Turk, Matthew. "Gesture recognition." Computer Vision: A Reference Guide (2014): 346-349. Rosenfeld, Azriel, David Doermann, and Daniel DeMenthon, eds. Video mining. Vol. 6. Springer Science & Business Media, 2013. Kalal, Zdenek, Krystian Mikolajczyk, and Jiri Matas. "Tracking-learning-detection." Pattern Analysis and Machine Intelligence, IEEE Transactions on 34.7 (2012): 1409-1422. M. Andriluka, S. Roth, B. Schiele. People-Tracking-by-Detection and People-Detection-by-Tracking. Computer Vision and Pattern Recognition (CVPR) 2008 Sadanand, Sreemanananth, and Jason J. Corso. "Action bank: A high-level representation of activity in video." Computer Vision and Pattern Recognition (CVPR), 2012 

The standard setup when training a neural network seems to be to split the data into train and test sets, and keep running until the scores stop improving on the test set. Now, the problem: there is a certain amount of noise in the test scores, so the single best score may not correspond to the state of the network which is most likely to be best on new data. I've seen a few papers point to a specific epoch or iteration in the training as being "best by cross-validation" but I have no idea how that is determined (and the papers do not provide any details). The "best by cross-validation" point is not the one with the best test score. How would one go about doing this type of cross validation? Would it be by doing k-fold on the test set? Okay, that gives k different test scores instead of one, but then what? 

If you get a single exact title match then you have probably found the right article, and can fill in the rest of the info from there. Both give you download links and bibtex-style output. What you would likely want to do though to get perfect metadata is download and parse the pdf (if any) and look for DOI-style identifier. Please be nice and rate-limit your requests if you do this. 

Some factors you might consider: Developer familiarity: go with whatever you or your developers are familiar with. Mongo, Couch, Riak, DynamoDB etc all have their strengths but all should do ok here, so rather than going for an unfamiliar solution that might be slightly better go for familiar and save a bunch of development time. Ease of cloud deployment: for example, if you are using Amazon AWS, then DynamoDB is likely an excellent choice. Sure, you could use Mongo on AWS, but why bother? Other cloud providers have their own preferred db, for example if you are using Google AppEngine, it makes sense to use BigTable or Cloud Datastore. Your use case seems both well suited to NoSQL and not very challenging since your data has a natural partition by user. I think you'd be technically ok with anything, which is why I'm mainly covering other factors. 

When using IPython, you very nearly don't have to worry about it (at the expense of some loss of efficiency/greater communication overhead). The parallel IPython plugin in StarCluster will by default start one engine per physical core on each node (I believe this is configurable but not sure where). You just run whatever you want across all engines by using the DirectView api (map_sync, apply_sync, ...) or the %px magic commands. If you are already using IPython in parallel on one machine, using it on a cluster is no different. Addressing some of your specific questions: "how to reconcile distributing work across cores on an instance vs. instances on a cluster" - You get one engine per core (at least); work is automatically distributed across all cores and across all instances. "Is it even practical to parallelize across instances as well as across cores on each instance?" - Yes :) If the code you are running is embarrassingly parallel (exact same algo on multiple data sets) then you can mostly ignore where a particular engine is running. If the core requires a lot of communication between engines, then of course you need to structure it so that engines primarily communicate with other engines on the same physical machine; but that kind of problem is not ideally suited for IPython, I think. "If so, can anyone give a quick rundown of the pros + cons of running many instances with few cores each vs. a few instances with many cores? Is there a rule of thumb for choosing the right ratio of instances to cores per instance?" - Use the largest c3 instances for compute-bound, and the smallest for memory-bandwidth-bound problems; for message-passing-bound problems, also use the largest instances but try to partition the problem so that each partition runs on one physical machine and most message passing is within the same partition. Problems which would run significantly slower on N quadruple c3 instances than on 2N double c3 are rare (an artificial example may be running multiple simple filters on a large number of images, where you go through all images for each filter rather than all filters for the same image). Using largest instances is a good rule of thumb. 

I would add just one comment to that: school (or learning for the sake of learning) doesn't provide the same kind of experience as solving a real problem does. Therefore, to learn, find real problems you can tackle. To stay motivated, make them problems you really care about. Kaggle competitions are a great place to start. Even reproducing a winning solution (perhaps with some variations, or on a new dataset) will be a huge learning experience. 

I am looking to do k-means clustering on a set of 10-dimensional points. The catch: there are 10^10 points. I am looking for just the center and size of the largest clusters (let's say 10 to 100 clusters); I don't care about what cluster each point ends up in. Using k-means specifically is not important; I am just looking for a similar effect, any approximate k-means or related algorithm would be great (minibatch-SGD means, ...). Since GMM is in a sense the same problem as k-means, doing GMM on the same size data is also interesting. At this scale, subsampling the data probably doesn't change the result significantly: the odds of finding the same top 10 clusters using a 1/10000th sample of the data are very good. But even then, that is a 10^6 point problem which is on/beyond the edge of tractable. 

You are asking about Data Dredging, which is what happens when testing a very large number of hypotheses against a data set, or testing hypotheses against a data set that were suggested by the same data. In particular, check out Multiple hypothesis hazard, and Testing hypotheses suggested by the data. The solution is to use some kind of correction for False discovery rate or Familywise error rate, such as Scheffé's method or the (very old-school) Bonferroni correction. In a somewhat less rigorous way, it may help to filter your discoveries by the confidence interval for the odds ratio (OR) for each statistical result. If the 99% confidence interval for the odds ratio is 10-12, then the OR is <= 1 with some extremely small probability, especially if the sample size is also large. If you find something like this, it is probably a strong effect even if it came out of a test of millions of hypotheses.