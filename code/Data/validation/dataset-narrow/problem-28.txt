This question is somewhat related to this one. As Alan has already said, following the actual path of the light ray through each layer leads to more physically accurate results. I will base my answer on a paper by Andrea Weidlich and Alexander Wilkie ("Arbitrarily Layered Micro-Facet Surfaces") that I have read and implemented. In their paper they assume that the distance between two layers is smaller than the radius of a differential area element. This simplifies the implementation because we do not have to calculate intersection points separately for each layer, actually we assume that the intersection points are the same for all layers. According to the paper, two problems must be solved in order to render multilayered material. The first one is to properly sample the layers and the second is to find the resulting BSDF generated by the combination of the multiple BSDFs that are found along the sampling path. Sampling In this first stage we will determine the actual light path through the layers. When a light ray is moving from a less dense medium, e.g. air, to a more dense medium, e.g. glass, part of its energy is reflected and the remaining part is transmitted. You can find the amount of energy that is reflected through the Fresnel reflectance equations. So, for instance, if the Fresnel reflectance of a given dielectric is 0.3, we know that 30% of the energy is reflected and 70% will be transmitted: 

When the light ray is moving from a more dense to a less dense medium, the same principle described by the Fresnel reflectance applies. However, in this specific case, total internal reflection (a.k.a TIR) might also happen if the angle of the incident ray is above the critical angle. In the case of TIR, 100% of the energy is reflected back into the material: 

What it does is draw capsules on the screen. Basically these capsules connect where the particle was to where it is. On the cpu I compute the vertices of this with some linear algebra, but in screen space terms the rectangle we will be shading has a maximum width of and a minimum height of . If the capsule is moving faster then then the capsules geometry will start to get longer to accommodate. Here are some diagrams of what is going on: This is an image of a single capsule. Because the distance between the current and last position is not large enough so the screen-space geometry is a square. You can see that to compensate that lighter area is shaded in as well to connect the two points. 

I am starting to think about how I can make my app (currently written in Metal) available on older device that dont have Metal as well as Android. This is a predicament because while I could write two separate rendering classes that use the different frameworks it doesn’t teach me anything about how to code for multiple graphics libraries. I can only get away with this because my rendering code is fairly short and has few states and draw calls. I want to learn for the future. Writing a renderer that supports multiple frameworks especially becomes difficult when thinking of the two libraries I hope to support — OpenGL and Metal — which are quite different in how they expose the GPU. I am curious how exactly one usually does design their code for apps that can target multiple graphics languages. Here are my theories of what could be done: 

As can be seen, TIR or Fresnel reflectance might keep some rays bouncing indefinitely among layers. As far as I know, Mitsuba implements plastic as a two layer material, and it uses a closed form solution for this specific case that accounts for an infinity number of light bounces among layers. However, Mitsuba also allows for the creation of multilayer materials with an arbitrary number of layers, in which case it imposes a maximum number of internal bounces since no closed form solution seems to exist for the general case. As a side effect, some energy can be lost in the rendering process, making the material look darker than it should be. In my current multilayer material implementation I allow for an arbitrary number of internal bounces at the cost of longer rendering times (well... actually, I've implemented only two layers.. one dielectric and one diffuse :). An additional option is to mix branching and RR. For instance, the initial rays (lower deep levels) might present substantial contribution to the final image. Thus, one might choose to branch only at the first one or two intersections, using only RR afterwards. This is the case with smallpt. An interesting point regarding multilayered materials is that individual reflected/transmitted rays can be importance sampled according to the corresponding BRDFs/BTDFs of the current layer. Evaluating the Final BSDF Considering the following light path computed using RR: 

You've said that "... bilinear filtering on in the texture ...". It seems that you are interpolating the depth values of the shadow map. The correct way of using interpolation with the shadow map is to apply it over the outcomes of the shadow tests (as far as I remember, OpenGL supports that). You might even combine the interpolation of the outcomes of the shadow tests with PCF, which will deliver much better results. However, as you might have noticed, aliasing is a plague that always pursue the shadow mapping :) Although I understand that you are looking for solutions regarding shadow mapping (even because it is quite simple to implement), have you ever considered the use of shadow volumes? It is much more intricate to implement, but does not suffer from aliasing at all, and I think would fit nicely your purposes. 

At least on iOS every frame you are likely to get a completely new texture that you need to draw to due to the OS switching between a couple textures each frame. My question is essentially if the OS feels the need to use multiple textures should we also be using multiple textures for some reason when doing full screen post processing? Is there ever a time where switching between off screen textures can improve performance? 

Case #3 proving that texture drawing works by drawing to front buffer (I put a texture into a fbo and never drew to the fbo) 

I am making a 2d game in opengl es 2.0 Inside are tons of rectangles defined by 4 points and one 4 component color. I am using vertex buffer objects, and I have heard that it is efficent to interlace the data. So like traditionally you would do 

So I have a batch draw call I am doing with various squares. I have a 4x4 tile map with the numbers 1-16 going in right/down order. When it gets to the fragment shader their is a varying float "id" that holds the number. In a perfect world if the id was 0, it would sample the top left and display "0", if the id was 4 it would sample the top right and display "4". Thankfully this is mostly working! However the numbers 5, 9, and 13 (which happen to be on the left of the tile map flicker! The values on these squares just change frequently. I have traced it down to being the fault of the sample location. And probably this function here: The goal is to take the id and return the proper row and column to texture map. 

After five bounces, RR is used to determine if the ray path continues or not. If it continues, the BRDF is scaled to compensate for it. This is Ok for me. RR is also used in a second code segment to select between refraction or transmission during the rendering of dielectrics (expanded code version): 

When light hits a conductor or a diffuse surface, it will always be reflected (being the direction of reflection related to the type of the BRDF). In a multilayer material, the resulting light path will be the agregate result of all those possibilities. Thus, in the case of a 3-layer material, assuming that the first and secong layers are dielectrics and the third layer is diffuse, we might end up with the following light path (a tree actually): 

According to the code above, the path branches recursively if the ray has bounced up to two times. After two bounces, however, RR is used to select the path to be followed. This is also Ok for me. What is a bit confusing is the fact that the radiance returned by both possible non-branching paths (refraction and transmission) is scaled. I understand that there are different probabilities regarding reflection and transmission. However, if for instance Re = 0.3 and Tr = 0.7, and 100 rays strike the surface, about 30% of the rays will be reflected and 70% of will be transmitted due RR. In this case, I understand that there is no path termination neither energy loss, so there wouldn't be anything to compensate for. Thus, my first two questions are: why are these radiances scaled? Should they be scaled, or would it work without scaling at all? My third question is related to the scaling factors: Why the author has used P, RP and TP instead of Re and Tr? Any indication of a good reading about this topic is also very welcome!! Thank you! 

This is for a game, the input is derived from some time value that represents a proportion of primary paints. Sort of. I realize that in cmyk k stands for key and is a black dye that is purer than the black that can be obtained from mixing the primaries. In my game cmyk is unimportant, only that I can mix three primary colours cyan, magenta and yellow to mix the colour wheel. It is unlikely that black will even be required, but the chroma effect of mixing some proportion of all three primaries together is required as I would want more than just the hues. I appreciate that lightness of the hues is not possible without white but that is also fine. I want it so that given some floating point value for representing some proportion of each primary, I can calculate that as percentages and then resolve that to an rgb colour code. It need not be a perfect conversion as only the rgb colour will be displayed, the cmy will just be for the calculation. A close approximation will suffice. I don't need an especially wide gamut. I just need to be able to mix some proportion of the paints to mix a colour palette. If the player mixes equal parts of each primary then it would be black, if they keep mixing in magenta then it will become more and more magenta again as the volume of magenta dwarfs the volume of black. If you mix red and blue you're mixing magenta and yellow with magenta and cyan. I hope that clarifies some of the questions. The goal is to take the intuitive representation expressed to the player, three primary colours, and resolve that to a colour code that can be used to actually render to the screen. The player will be able to clear the colour so they can then return to a pure primary colour. But to mix any particular colour they will do so by mixing the primary colours. Hue, chroma and brightness should be possible though lightness isn't possible without white as this is subtractive. 

I was taking a look at Smallpt ($URL$ more specifically at the Russian Roulette part. Actually, RR is used in two places along the code: first, to determine ray termination; and second, for the rendering of dielectrics. That is the code for the ray termination case (expanded code version): 

We can simulate this type of interaction using recursion and weighting each light path according to the actual reflectance/transmitance at the corresponding incident point. A problem regarding the use of recursion is that the number of rays increases with the deepness of the recursion, concentrating computational effort on rays that individually might contribute almost nothing to the final result. On the other hand, the aggregate result of those individual rays at deep recursion levels can be significant and should not be discarded. In this case, we can use Russian Roulette (RR) in order to avoid branching and to probabilistic end light paths without losing energy, but at the cost of a higher variance (noisier result). In this case, the result of the Fresnel reflectance, or the TIR, will be used to randomly select which path to follow. For instance: 

We can evaluate the total amount of radiance $L_r$ reflected by a multilayer BSDF considering each layer as a individual object and applying the same approach used in ordinary path tracing (i.e. the radiance leaving a layer will be the incident radiance for the next layer). The final estimator can thus be represented by the product of each individual Monte Carlo estimator: $$ L_r = \left( \frac{fr_1 \cos \theta_1}{pdf_1} \left( \frac{fr_2 \cos \theta_2}{pdf_2} \left( \frac{fr_3 \cos \theta_3}{pdf_3} \left( \frac{fr_2 \cos \theta_4}{pdf_2} \left( \frac{L_i fr_1 \cos \theta_5}{pdf_1} \right)\right)\right)\right)\right)$$ The paper by Andrea Weidlich and Alexander Wilkie also takes absorption into consideration, i.e. each light ray might be attenuated according to the absorption factor of each transmissive layer and to the distance traveled by the ray within the layer. I've not included absorption into my renderer yet, but it is just a real coefficient computed according to the Beer's Law. Alternate approaches The Mitsuba renderer uses an alternate representation for multilayered material based on the "tabulation of reflectance functions in a Fourier basis". I have not yet dig into it, but might be of interest: "A Comprehensive Framework for Rendering Layered Materials" by Wenzel Jacob et al. There is also an expanded version of this paper.