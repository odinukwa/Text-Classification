create a init.ora file containing the dbname, db_unique_name, and control_files parameter. The latter one points to at least one of your ctl files. With a little luck you can guess the dbname from the file names. install the correct oracle rdbms version in a location called ORACLE_HOME export ORACLE_HOME=/where/your/Oracle/product/version/home_1 is export PATH=$ORACLE_HOME/bin:$PATH sqlplus / as sysdba startup nomount pfile=your_full_name_of_init.ora from step 1 alter database mount - this accessed the ctl files and checks dbname. Fix dbname in init.ora if reported wrong and shutdown abort the instance to start over at step 5. now you are able to look into the original files locations. If your files are in exactly the same locations as the original database thi sis easy: alter database open read only; 

For this you need a logical standby database, instead of a physical one. This is a little more complex. Not all data types are supported and when data is updated on both sides on the same time, there could be some problems ahead. 

If you get problems before or during mount, it is probably because of wrong dbname, db_unique_name. Set them as they were/are in prod. 

instead of looking further. He missed the point that the database was started during the cold backup. This is one of my reasons not to be a big fan of COLD backups. They make me shiver. Be smart and carefully read OracleÂ® Database Backup and Recovery User's Guide 11g Release 2 (11.2) It is worth every minute you spend studying it. Oracle backup and recovery is not hard, there are just a lot of options. 

As always with Oracle, there are many ways to reach your goal. You might want to check out flashback database. This enables you to restore your database in a previously saved state and might be the easiest in your case. Using Flashback Database and Restore Points Drop user cascade could also do the trick, followed by impdb as you described. What was the problem with the drop user action? Try flashback database. 

Most of the time the session tries to tell the client something nasty happened. If the client is gone, this will never be acknowledged so the dbms will wait forever. When you kill a session, the connection between v$session and v$process is gone. The process of the killed session remains in v$process. You can find the OS pid for these processes by checking the processes that are not PQ slaves without a session. Kill those processes from the OS. Oracle is very robust and if anything should be recovered, smon takes care of that, if needed. The state of the database does not depend on a client that gets killed. Would be a bit strange since this would mean that if a client crashes it would corrupt the rdbms... If a database gets - logically - corrupted it would mean that the client did not make use of the regular transaction mechanism. The following SQL can be used to identify the processes that have no session tied to it: 

Your initial idea was not bad at all. What you can do is store the unwanted partitions with their local indexes in separate tablespaces. Use rman for the cloning but use the SKIP TABLESPACE option to not clone the tablespaces with the unwanted partitions. (assuming online backup) After the clone, the skipped tablespaces have datafiles with status RECOVER. see RMAN DUPLICATE DATABASE: Options In the end you just drop the unwanted partitions. To be able to do that you first have to get rid of things like constraints and indexes that need to be re-created later on. This worked in 10gR2. Make sure that you don't drop the last partition of a table, in that case drop the table. It is a bit of work but certainly possible. If the difference in Volume is huge or there are lots of copies, it might be worth spending some time for it. 

You could try with replicating you changes to change tables. Dbvisit replicate could be of good value here. See Real-time Oracle data and database replication It is not a free tool but it saves a lot compared to running EE. 

Your database instance is open. Your database open_status is a different thing, it could be open READ ONLY. The database is expected to be open for READ WRITE when you switch a logfile. So you might conclude that the error message is not complete. 

this might not be the answer you expected/hoped for but take a look at DbVisualizer. This is a nice tool that organizes sql scripts as bookmarks and we can choose in what encoding they are written to disk. The tool is multiplatform and multidatabase. Using this I - as an oracle dba - can use a SQL server database without having any dedicated SQL server tool installed, from my macbook. Choosing a smart encoding if you work multiplatform can help. I hope this helps. 

yes, backup of your parameter files is a smart thing to do, like backing up your database is a smart thing to do. If you still have an alert log file of your database, you can extract the parameter settings from there and use them to create a pfile using good old vi or whatever simple text editor you like. Most important parameters are 

No. The words Secure Files should be interpreted that files can safely be stored in the database. This is often safer than storing them on the FileSystem with a reference from the database. If the file is in the database it is also protected by many of the database features. SecureFiles is the modern implementation of LOB's. The new implementation has a great performance boost compared to the old inplementation. dbfs is a fileystem that can be created inside the database and that can be mounted on Linux using dbfs_client. Regular operating system processes see it as a regular POSIX filesystem. The LOB's in a dbfs should be implemented as SecureFiles to have the best performance. The performance of dbfs is better than that of NFS, if using filesystem like logging. With normal logging dbfs has about the same performance as NFS, with the added possibility of replicating the data to standby databases. 

If you really want fast transactions in your database, make sure that you handle the database correctly. For example, when talking about Oracle it is easy to have it handle 30.000 tps. With a few subtle tweaks it can be brought down to only a few thousand transactions per second. For a very nice demo look at OLTP Performance - The Trouble with Parsing It all comes down to prevent extra work, re-use connections as often as possible, prepare statements and bind variables. Do this and your database can perform and scale in an optimal way, assuming that your storage can handle the load. 

Your problem might very well have to do with incorrect versions or wrong default connection. Make sure that you use the exp utility from the 10g installation and the imp utility from the 11g installation. Also make sure that the environment variables like ORACLE_HOME are correct and that PATH is adjusted for the correct ORACLE_HOME/bin How does your connect string look like? Is your TNS_ADMIN variable defined? (if not, you might need to have multiple tnsnames.ora files in place, or tnsnames.ora is in a platform specific location (/etc, /var/opt/oracle etc.)) tnsping shows where it gets the sqlnet.ora (next to tnsnames.ora) from. Even better is to start using expdp and impdp. You can use impdp to directly import your data in the 11g database, using a datasbase link. 

Use the full normalized database, scan which queries need more performance than this model can give them and create materialized views on them. If the data is very volatile, having them fast refreshing introduces some overhead but you know the value and validity of your data, that is protected by the database. Oracle handles this very nice. You queries remain using the normalized tables and Query Rewrite will take care of the usage of the Materialized Views, when appropriate. When the Materialized Views are not fresh, the queries fall back to the original tables. For docu see Materialized View Concepts and Architecture 

The correct answer for licensing questions is: contact your oracle sales representative. With Oracle you license cpu's. If your test and prod run on the same cpu's, they don't need a new license. As long as it all runs on the same cpu's, it does not matter if you have multiple databases (and instances) or run it all in one single database. Normally we don't mix test and prod in the same database so at least make sure to create separate databases for test and prod. If you run this on the same machine, it could be wise to run test using a test account and prod database using a prod account. This also requires 2 separate ORACLE_HOME's on that single machine but this does give you some control about the separation of your environments.