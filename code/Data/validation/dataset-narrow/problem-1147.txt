Edit: This answer is unfortunately incorrect. The error is highlighted below. The argument does work if we are allowed to transpose the matrices. We start by proving a lemma. Lemma. Let $A$ be an $n\times n$ matrix and let $N$ be the $n\times n$ matrix with ones on the secondary diagonal. If $AN^t$ and $N^tA$ are nilpotent for all $t \geq 0$ then $A = 0$. Correct conclusion: $A$ is upper triangular with zeroes on the diagonal. (The original conclusion is recovered if we are also allowed to multiply by powers of the transpose of $N$.) Proof. Suppose for example that $n = 3$, and write $$ A = \begin{pmatrix} a & b & c \\ d & e & f \\ g & h & i \end{pmatrix}, \quad N = \begin{pmatrix} 0 & 1 & 0 \\ 0 & 0 & 1 \\ 0 & 0 & 0 \end{pmatrix}. $$ We start by calculating $AN^2$: $$ AN^2 = \begin{pmatrix} 0 & 0 & a \\ 0 & 0 & d \\ 0 & 0 & g \end{pmatrix}. $$ This matrix is in triangular form, and so if $AN^2$ is nilpotent then $g = 0$. Continue with $AN^1$: $$ AN^1 = \begin{pmatrix} 0 & a & b \\ 0 & d & e \\ 0 & g & h \end{pmatrix} = \begin{pmatrix} 0 & a & b \\ 0 & d & e \\ 0 & 0 & h \end{pmatrix}. $$ Again the matrix is in triangular form, and so if $AN^1$ is nilpotent then $d = h = 0$. Continuing, $$ AN^0 = \begin{pmatrix} a & b & c \\ 0 & e & f \\ 0 & 0 & i \end{pmatrix}. $$ As before, we conclude that $a = e = i = 0$, and so $A$ is upper triangular with zeroes on the diagonal. If we now consider $N^2A,N^1A,N^0A$ instead, then we conclude that $A$ is lower triangular with zeroes on the diagonal. In fact, we don't get anything new from considering $N^tA$. Therefore $A = 0$. $\square$ This is how the proof would go if the original version of the lemma were correct. Now back to the problem at hand. Say that the matrices $A_1,\ldots,A_k$ satisfy property P if for every infinite sequence $i_1,\ldots \in [k]$, we have $A_{i_1} \cdots A_{i_m} = 0$ for some $m$. If one of the matrices $A_i$ is not nilpotent then property P clearly fails, so suppose that all the matrices are nilpotent. If all matrices commute then property P clearly holds, so suppose that $A_1 A_2 \neq A_2 A_1$. Change basis so that $A_1$ is in Jordan normal form, and let the corresponding decomposition of the vector space be $V_1 \oplus \cdots \oplus V_t$. Let $V_i$ be a vector space on which $A_1 A_2 \neq A_2 A_1$; note that $\dim V_i > 1$ since $0$ commutes with everything. Restricted to $V_i$, $A_1 = N$ and $A_2 \neq 0$. Therefore the lemma implies that for some $t \geq 0$, either $A_2 A_1^t$ or $A_1^t A_2$ is not nilpotent, and therefore property P clearly fails. Summarizing, property P holds iff all matrices are nilpotent and all of them commute. 

Bob's best bet is to guess the $t$ values with largest probability. If you're willing to use Rényi entropy instead, Proposition 17 in Boztaş' Entropies, Guessing and Cryptography states that the error probability after $t$ guesses is at most $$ 1 - 2^{-H_2(\mu)\left(1-\frac{\log t}{\log n}\right)} \approx \ln 2 \left(1-\frac{\log t}{\log n}\right) H_2(\mu), $$ where $n$ is the size of the domain. Granted, the dependency on $t$ is pretty bad, and perhaps Boztaş was focused on a different regime of the entropy. For the Shannon entropy, you can try to solve the dual optimization problem: given a fixed failure probability $\delta$, find the maximal entropy of such a distribution. Using the convexity of $-x\log x$, we know that the distribution $\mu$ has the form $a,b,\ldots,b;b,\ldots,b,c$, where $a\geq b\geq c$, $a+(t-1)b = 1-\delta$, and $c = \delta-\lfloor\frac{\delta}{b}\rfloor b$. We have $t-1+\lfloor\frac{\delta}{b}\rfloor$ values that get probability $b$. Conditioning on $s = \lfloor\frac{\delta}{b}\rfloor$, we can try to find $b$ which minimizes the entropy. For the correct value of $s$, this will be an internal point (at which the derivative vanishes). I'm not sure how to get asymptotic estimates using this approach. 

The question depends on the exact encoding. However, it seems that in many reasonable encodings, as the length tends to infinity, the number of production rules $S\to a$ (for an appropriate interpretation of the starting symbol $S$ and the terminal $a$) will be more than one with high probability; here I literally mean the same terminal $a$. If we consider this as ambiguity, then I expect "most" grammars to be ambiguous. We can also concoct similar situations such as the rules $S\to S$ and $S\to a$ each appearing at least once. Assuming this general hypothesis, that every (fixed) conceivable rule should appear with high probability as the length tends to infinity, we find that "most" grammars generate $\Sigma^*$ in an ambiguous manner. As an example, consider the following encoding for grammars over $\Sigma = \{0,1\}$. The grammar alphabet consists of the symbols $\{0,1,;,.\}$. Non-terminals are indexed by binary strings of length at least 2. Rules are separated by full stops. Each rule is a sequence of binary strings separated by semicolons. The first binary string is the non-terminal on the left-hand side, and the rest (if any) constitute the right-hand side; if the first binary string is not a non-terminal (i.e., it is $\epsilon$,0,1), then the starting non-terminal is assumed. The starting non-terminal is always 00. Under this encoding, every string in $\{0,1,;,.\}^*$ describes some grammar. A random grammar will with high probability contain many copies of $.00;00.$ and $.00;0.$, and in particular will be ambiguous. 

I can give several answers to your question. Algorithmic randomness. When should we call a sequence $x_1,\ldots,x_n$ of bits random? A priori, all sequences have the same probability, so it's not clear on what grounds we should single out one sequence or another as not being random. This is partly a philosophical question, but "applied philosophy" has given rise to several definitions of algorithmic randomness, detailed in the Wikipedia entry. These notions are actually for infinite sequences, but some of them can also be used for finite sequences, for example the notion of Kolmogorov-random sequence that you mention in your question. The trouble with these notions is that one cannot compute random sequences – that's a feature of the definitions. Pseudorandomness. Complexity theorists and (theoretical) cryptographers prefer a more permissive notion of randomness. Suppose you need a random sequence to run algorithm A which belongs to complexity class C. A pseudorandom sequence is a sequence that "fools" algorithms of complexity class C – that is, it looks random to such algorithms. More formally, algorithms of class C cannot distinguish a small set of pseudorandom sequences from a truly random sequence. As joro mentions in their comment, for this definition to make sense, you need to look at a collection of pseudorandom sequences, since one sequence could always be distinguished from a random sequence (at least in non-uniform computation models). There are no known pseudorandom sequences against complexity classes beyond some very weak ones, though there are constructions which work assuming some complexity assumptions. Such constructions are not necessarily useful for practical applications, for three reasons: (1) you are given a large set of sequences rather than one sequence, and you have to run your (decision) algorithm on all of them; (2) constructions in complexity theory tend to be impractical; the pseudorandom sequences are probably really hard to generate in practice, especially for realistic values of $n$ (the input size); (3) asymptotic analysis doesn't usually yield concrete bounds, so it would be hard for you to figure out what parameters to use; worse, if the security of the construction relies on some non-explicit complexity assumption (e.g. P$\neq$NP), the guarantee could be purely asymptotic, and not give any explicit bounds. Practical randomness. Random numbers are used in practice in two kinds of situations: numerical simulations and cryptography. When using random numbers for numerical simulations, the "quality" of the numbers is not as important as the time it takes to generate them; in practice, even rather simple pseudorandom number generators seem to "work". A search of the relevant literature will reveal some modern examples. Some of the old pseudorandom number generators were indeed bad (in some situations), but the this threshold has been crossed, and from an engineering perspective the problem can be considered solved. Random numbers used for cryptographic purposes need to be more secure, since now we're against somebody who's construction the algorithm for the express purpose of reverse engineering the random stream. In this case one needs to be a bit more careful and use a secure, industry standard stream cipher. Such ciphers are available, and I strongly urge against using anything else, especially anything which you designed yourself. For extra security, XOR the output of several stream ciphers using independent keys. 

If $\xi$ is allowed to be ternary, I can give a counterexample; presumably a suitable binary encoding of $\xi$ would give a binary counterexample. Let $(k_n)$ be some non-computable sequence, and let $(w_n)$ be a computable enumeration of all binary strings. Consider the word $$ \xi = 1^{k_0} 0 w_0 1^{k_1} 0 w_1 1^{k_2} 0 w_2 \ldots $$ Clearly $T(\xi) = (0+1)^*$ is regular. On the other hand, $T(2\xi) = (0+1)^* + 2P(\xi)$, where $P(\xi)$ is the set of all prefixes of $\xi$. Since $(k_n)$ can be extracted from $P(\xi)$, we conclude that $T(2\xi)$ cannot be regular (or even computable). 

Your problem is solved in the paper Spheres of Permutations under the Infinity Norm — Permutations with limited displacement by Torleiv Kløve. See also A002524 and other sequences linked there. I found Kløve's paper by calculating the first few values of A002524 and finding it on the OEIS.