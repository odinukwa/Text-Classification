You can store pre-calculated gaps, and use constraints to make sure that your pre-calcualted data is always up-to-date: Here is the table and the first interval 

Anyway, it looks kind of hacky to me. Is there a better way to retrieve, for example, numeric[] as a type if my parameter is an array of numeric? 

If you use a multi-statement UDF, then your inner select is executed exactly once for each outer row. The multi-statement UDF is treated as a black box: the execution plan will now show access to the objects used in your complex view. On the other hand, a subquery and/or an inline UDF is flattened out by the optimizer. When this is the case, the execution plan will include access to the objects used in your complex view. 

If possible, I would redesign the table. If we can have VersionNumber as an incremental integer with no gaps, that the task of retrieving the next chunk is a totally trivial range scan. All we need is the following index: 

FG_data1.ndf which is restricted to 200 GB and current free space is 0 GB FG_data2.ndf which is restricted to 200 GB and current free space is 10 GB FG_data3.ndf which is restricted to 200 GB and current free space is 0 GB FG_data4.ndf which is restricted to 200 GB and current free space is 60 GB 

As a best practice, i am trying to schedule an activity where all SQL agent jobs should log output to text file. Manually i can do this for single job. But i am looking for a better way either by script or any good method i can make this change for multiple jobs about 80-90 jobs per server. Moreover i have to do this in QA, UAT and prod, covering over 100 servers. Please share you're experience or any helpful links how can this be achieved in best way as path of log files where output will be generated should be common across servers/jobs. 

This might be a simple one, but i searched a lot and unable to find one. Is there a sql query or a way i can run on registered server to find the list of all sql related services running like, ssis , ssas, ssrs, sql, agent, broswer, etc... Tried googling out and can find sys.dm_server_services dmv but it does not show all the services running 

I would use a CHECK constraint to make sure that previousInt < int, and a FK constraint (name, previousInt) refer to (name, int), and a couple more constraints to ensure watertight data integrity. That done, selecting gaps is trivial: 

The fastest solution is as follows: you create an additional column, IsLastID, and build a filtered index or an indexed view using it. You can use constraints to ensure the integrity of IsLastID, as described here Grant Fritchey wrote up a detailed comparison of various solutions here 

This is why whenever we add a new index, we need to have some kind of baseline testing to verify that none of these issues happen. 

The following behavior may be caused by missing indexes on referring side of your FKs: "the price changes take approx 1 hour to process (vs. 1-2 minutes) and sys.dm_tran_locks shows the transaction taking almost 90,000 different locks, compared to around 100-150 when foreign keys were being dropped/recreated" When a row is deleted or its PK/Unique is mutating, the database engine need to make sure there are no orphans. When there is no proper index to support it, it scans the whole thing. 

We are doing migration from SQL2008R2 to SQL 2014 for some of the databases hosted on 2008R2 to 2014 and remaining DB's to other servers. I am aware of migrating the login using dbatoo or using sp_helprevlogin.. But using that it would move all the logins from 2008R2 instance to 2014. Since we are only moving some databases we only need to move their respective logins or i should say only those logins that are present under users in the database geting migrated to 2014 instance. How can we achieve this intelligently via script, because doing manually is taking lot of time as we have hundred of logins and about 40 databases 

Even though, I've searched couple of them online but still looking if there is a better way to manage the SQL server backups, almost across 250 servers. Issue is we are not allowed sysadmin login to be created on one server to pull this report from. Please suggest a link or way to view the health report for sql server backups. Also SSRS works as we are not supposed to use database mail to sen emails 

and you are all set, as long as all your constraints are trusted. I am not sure why would you need the third table CourseSize at all. 

Note that your design does not prevent cycles. We can easily enforce that via constraints as well. I can elaborate if you are interested. Regarding the efficiency of finding qualifying descendants, we can add some redundant data and get much better speed. For example, E is a descendant of A, but not a direct one - B is between them. We can store the following rows: 

In T-SQL, you cannot modify any data in a function. There is no straightforward way around it. There are some obscure hacks, but I would not use them. Use a stored procedure. The hack, quoting from Erland Sommarskog: 

As usually, it depends ;) For instance, suppose that we have a small-size working prototype developed in Python and using flat files, and the users are happy with the features of the prototype, so all we need to do is to productionize it, using RDBMS as its back end. When this is the case, it is reasonable to expect to do it right the first time - the problem is small and well-defined. In such cases designing up front is feasible. On the other hand, when we are discovering the requirements in an Agile environment, we need a few iterations to understand them better. In such situations the database evolves with the rest of the application. This is what we usually do. Because we can refactor live OLTP tables without any downtime and with low risk, we are comfortable with the possibility of database refactorings. 

I have a database of size 840 GB which has 948 VLF's. On other hand there is one with size of 1.6 TB having 320 VLF's. Does that indicate the smaller database with that high count of VLF's is actually a problem? Also how can we decide that number to be too big or within OK range. Please suggest 

Actually i have to add 2 TB file to this 3 TB disk, but trying to add in chunks of 100 GB But it seems to be running for past 30 mins HOw can i improve this? Update: The Query above completed in 40 mins. The wait was ASYNC_IO_COMPLETION Also, this database is 24*7 and has DB mirroring configured. Now, i am trying to increase in 50 GB chunks using below 

Before proceeding, I know the cons of doing this and have read many questions on same. But since we are here on urgent need and adding disk would take over a couple of days and the DB in question being not that critical, so is there a way I can release the unused space from database. Below screenshot says unused space as 36 GB, but when I execute sp_spaceused it says unused as some 400 MB but unallocated much more. 

I need to let my users specify the list of columns they want to select. So far I know two ways of accomplishing that. 1. Using refcursors 

It saves me a few keystrokes. Yet I think naming conventions are very subjective and as such I don't have a strong opinion one way or another. 

If you switch to snapshot isolation, this effect should be gone. The following repro script shows how COUNT(*) running under REPEATABLE READ returns wrong results with high concurrency. Prerequisites We need a table with data and a function that provides random integers: 

As usual with such problems, it is very easy to accomplish in Java or C++ or C#. If you really need to do it in the database, you can use an RDBMS with fast cursors, such as Oracle, write a simple cursor, and enjoy fast performance without having to write anything complex. If you need to do it in T-SQL, and you cannot change database design, Itzik Ben-Gan has written up several solutions in "MVP Deep Dives vol 1", and some new solutions using OLAP functions in his new book about window functions in SQL 2012. Alternatively, you can add another column consecutiveMarker to your table, and store precalculated values in it. We can use constraints to ensure that pre-calculated data is always valid. If anyone is interested, I can explain how. 

Is there a way i can really find queries which caused almost 300 GB bloat last week. Yes, am aware of 3rd party tools which would have save the info. But we do not have one on this. How can i find one if possible? 

Apart from above we have PRIMARY FG with mdf data file set to unrestricted growth. All are set to auto grow by 10 GB Now, i see alert for Filegroup FG being FULL and rebuild index job failing with same saying "could not allocate space for object within the table tb1 for the database above because the Filegroup is FULL" But if i see from above data files tagged to FG, we still have 70 GB left, then how come its failing ? Please suggest how SQL works here in allocating new data for case above? Edit 

Suddenly we are facing some errors in starting up the integration services. From event viewer and ssms when we tried connecting we get below error message The SQL Server Integration Services 11.0 service failed to start due to the following error: This version of SQL Server Integration Services 11.0 is not compatible with the version of Windows you're running. Check your computer's system information and then contact the software publisher. Some info:- This is the mirror server where databases are in mirror state. We had some windows patching last night and after reboot started seeing these errors. Interestingly we had other servers also patched along with its principal, but did not witness the similar IS error out there and other servers. Tried researching on this but unable to much details on the error posted. Thanks, please suggest