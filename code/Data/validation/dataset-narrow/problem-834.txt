Start capturing traffic on box 2. Observed that box 2 was seeing and replying to pings from the Barracuda Load Balancer. Logged into box 1 and pinged box 2. Observed that box 2 saw but DID NOT reply to pings from box 1. Observed that box 2 saw but DID NOT reply to pings from the LB for a period of 100 seconds after the first ping from box 1. 

So somehow traffic between the two boxes is causing box 2 to crap out on ICMP for a period of time. I should note that box 1 was working fine throughout this test, but did not see any requests from box 2. While pinging box 1 from box 2, Wireshark on box 2 showed a message "Destination unreachable (Communication administratively filtered)" from a source IP I did not recognize. 

Question: What possible reasons are there for this? What should I try to diagnose this? I haven't ruled anything out. Switch configuration, domain/dns server, all ideas are welcome. Sadly, I have very little knowledge of good network administration, so obvious answers are welcome too. EDIT: In answer to some of the questions posed. I have contacted Barracuda and they seem to be of the opinion that the problem is related to the network. I think I agree at this point. The IP is assigned to a physical interface, not shared between servers. Pinging is done from within the same subnet. The third box handles all the site load when the other two go down and hasn't had much problem with it, but occassionally it too has trouble. I haven't found a pattern with that one yet. This evening I sat down with another (more experienced) network guy to look through some of the domain and server configurations. One of the things he found was a bad dns setup on the domain controllers. They were configured with external dns servers as their alternates rather than the other DC. We switched them to reference each other for dns, and added forwarding to the dns service. We also removed external dns references from all the web servers. EDIT 2: With Wireshark I was able to examine the ICMP traffic during one period of down time. I began this test because I could not reach a shared folder on box 2 from box 1. Test: 

You seem to be set on using vmware. I find vmware's hardware requirments for their SAN's a bit prohibative (they want fibre LAN)I recommend openstack, especailly for an acedemic setting - Devstack has a really nice getting started giude and config script which can easily be deployed on any modern (2.6+) linux kernel. Just run the script and your ready to start deploying VMs - it also has nice provisioning tools to help manage students machine images in the class 

Will the following will be true: Files (including data drives) will be synced between the two servers eliminating the need for third party backup/mirroring software to sync/backup files. Also supposing i use roaming profiles w/ folder redirection; How will client computer in the WAN access their data through the cluster (i.e. will they automatically choose the best route) 

Most likely these are ad bots reading your customers cookies and therefore trying to scrape data for tailored ads. michael hampton had a good approach, look for a pattern in the ip addresses and also do in depth investigation on a selection of address as well - look up the whois information you might be even able to call and ask the webmaster why get requests are coming from his ip. 

You have to create a dns record for www.yourwebsite.whatevs Additionally you need to change the IIS dns binding form just yourwebsite.whatevs to either add www.yourwebiste.whatevs or to *.yourwebsite.whatevs I haven't done this since IIS 6 so correct me if im wrong. 

I'm researching a sever clustering architecture as a redundancy and backup solution for a client, and something that isn't made clear is whether or not i can use server clustering to replace a file server with backup solution. Forgive my Elementary understanding of server clustering but supposing: 

2 Sites (NJ, CA) Identical Servers at each site setup as a Remote Site Cluster nodes with Windows Enterprise server 2008 r2 Services: File, Terminal, AD, and maybe DNS 

Now, the filesystem reports a usage of approx 110GB, but the zramfs device reports 165GB. At the same time, the zramfs memory is exhausted, and the filesystem becomes read-only. The zram figures confirm that we are getting a 2.2 : 1 compression ratio between orig_data_size and compr_data_size; however, why does the filesystem show much more free space than the zram device? Even if this is space already allocated for re-use by the filesystem, shouldn't it be reused rather than allocating new space? The data consists of a large number of small files which are added and removed at irregular intervals. 

This assumes your DS to be names of course, and is not as efficient as doing a when you already have the required RRA. 

If you place another gateway device between your Ironport and the Internet, then your only option is to disable SenderBase and any other IP-based authorisation in your Incoming Policy definitions in the HAT. You cannot tell the Ironport to obtain the previous-hop IP address from any other method than the incoing TCP connection itself (for obvious reasons - otherwise it would be far too easy to forge). One option would be to reverse the order of the devices -- IE, put the Ironport between the Internet and the Proofpoint box, and set the Ironport to have a fixed SMTP route to send all incoming email via the ProofPoint. Otherwise, you lose out on the Ironport's Senderbase rules, which are (IMHO) one of the primary benefits of the Ironport. 

dsync is 'idempotent' and will synchronise from whatever state you are currently in; there is no queue of pending changes as there is with something like MySQL replication. This means that, when server B comes back up, the next time a dsync is triggered, you will end up with both servers back in sync. No manual intervention is required, and they will get back into sync even if all the mails filesystem on B was wiped (though it might take a bit of time to achieve this). You would probably want to have users normally access only via server A, and in the event of A failing and your proxy redirecting them to B, remove A from the proxy pool until your dsync has completed and A is once again in sync. 

Typically the 'owner' is the creator, but since the user is part of the administrators group all admins are given owner rights to it (this way when an admin leaves an organization there aren't files orphaned on a file-system that no one but root can delete etc). This isn't so much a problem with windows itself, so much as NTFS - and additional creator field would be nice. If you need to do a more forensic analysis, proprietary data formats (office: .doc(x), .xls(x) etc) typically have some metadata in them about who created them. To this end its worth noting that the office 2007+ formats are simply zipped xml files and directories (change the file extension to .zip or ...openwith your gunzip program of choice) 

The best possible answer to your situation is cluster storage whereby data is redundantly stored at the block level. There are several different way to implement this but the best that i can imagine (at least to your up-time specifications) would be an open stack cluster. Openstack will distribute both storage and computation so that in the event of a hardware failure both execution and storage are redundant and unceasing. In other words the best way to maintain data integrity and uptime is to make sure the application doesn't crash in the first place. As yoonix pointed out this will not protect you from user/logic errors but open stack includes tools for disk imaging/backup as well - loading an image and booting takes minutes if not seconds. Amazon Web Services and Rackspace are examples of openstack deployments. $URL$ A good place to start with openstack is devstack (pretty much a deploy script with various different deployment modes to test with) $URL$ The weakness of this implementation is lack of hardware, this system doesn't exactly shine in a small office with just two physical servers or the like (works great with blade systems though) 

In ecelerity, the sieve++ function can optionally be passed the IP of a custom DNS server as an argument. This is used, for example, when querying an RBL server. We have multiple RBL hosts for resilience. How can I ensure that, should one server be down, sieve will fail over to use one of the others in a timely manner? When querying normal DNS, the is used and we get failover to our backup DNS host. However, in RBL lookup, it seems that only one DNS host can be defined and so there is no failover. Example: 

The list is most likely configured to not allow self-unsubscription. This is why you cannot unsubscribe by email. The list of users is held in the database, and so - if you are unable to use the web interface to manage the subscribers - you would need to delete the appropriate record from the subscriber_table. If this list uses an external datasource for the users, then you may need to instead add the user to the exclusion_table as otherwise they will be re-added on the next synchronisation. Having said all of that, you are using a very old (4.x) version of Sympa, and it may predate some of this advice which is based from knowledge of 5.x and 6.x. 

Since you are using MRTG with Routers2, there is a generic cfgmaker host template available at $URL$ which will automatically generate MRTG configurations for many things, including the storage OIDs. These take advantage of the Routers2 additional features to give you combination graphs. It should work with any SNMP-capable host. You can use it with standard MRTG cfgmaker like this: 

However, when the application is running with live data over a longer period, we find that this no longer matches. 

Recipient domain checked for local: If the recipient mail domain is handled by this MTA, then any aliases are expanded. If it is still local, it is delivered, and the process stops. Recipient domain checked for explicit route: If an explcit SMTP route is defined for this recipient domain, then the mail is passed to the defined server using the defined method and the process stops. Smart host: If a 'smart host' SMTP route is defined, then all mails are passed to this server and the process stops. MX Resolution: The recipient domain is checked for MX records. If any are found, then they are tried in order until one accepts the email. Then the process stops. A record resolution: The recipient domain is checked for an A or possibly AAAA record. If one is found then the mail is passed to the MTA at this address and the process stops. Bounce: If it gets this far, the message is undeliverable and is bounced.