For an introduction to these topics I recommend you read: Introduction Ray-Tracing Introduction to Shading The Phong Model In order for you to progress, I personally recommend you proceed by step and don't try to understand everything at once. The problem of rendering and shading is very complex. You need to first understand the principle of light-matter interactions, what is diffuse what is specular and how are objects illuminated (direct and indirect lighting). Once you have these notions you can jump onto shading and understand or learn about the concept of shading model, and what the flaws and strengths of existing shading models (many of them exist). 

For example. Now check what the matrix this call will produce with the matrix of the default camera in Maya. You will notice they are different. The second is an identity matrix, the first one is not. Now for example if you create a default camera in Maya it will be position at (0,0,0) and looking down the negative z-axis. If you look at the 4x4 matrix of this camera you will get identity. And maybe what you are trying to do is to create a 4x4matrix using the function and multiply this matrix by the Maya's default camera matrix in which case, your camera will point in the wrong direction (it will be flipped along the camera's local coordinate z-axis). That's because the matrix returns by properly orient the camera (as suggested in the first paragraph) but since the camera in Maya is already oriented along the negative-z axis, your Maya camera will look the wrong way. So in summary: 

If you wonder what is . In fact it can be any point that lies on the plane. Imagine this as a sheet of paper; draw two lines on this sheet to define the center of the piece of paper, imagine that this point is aligned with the world coordinate system origin and that sheet is in xz plane of that coordinate system (in which y defines the up vector). So the paper is horizontal in a way. Now move the sheet around. The can be the coordinates of the point you drew in the center of the sheet of paper with respect to the world origin. All you have to do is rotate that sheet of paper. That rotation is given to you by the plane's normal. The rest is basic trig. If the point P is one the plane, the dot product between the vector defined by (P - PointOnPlane) and the plane's normal will be equal to 0. The dot product tells you whether two vectors are orthogonal, on the same side of the plane or point in opposite directions. FASTER SOLUTION - EDIT As mentioned by SimonF in a comment, you don't really need the parameter and save the subtraction between this point and the tested point: 

First, it is quite unusual these days to see people representing colors using a computer byte that is integer values within the range [0:255]. Most people these days use floats, and you will understand why in a moment. When you look at a material in general you can see a shiny part and a diffuse part. Shininess and diffuse appearance are computed using two different types of equation. Let's call the functions that compute the diffuse part and the function that compute the specular component . So now you can write: ObjectColor = Ks * specular() + Kd * diffuse(); 

So if you look at the reference they use by default a right-hand coordinate system, which means x-axis points to the right, y is up and z goes out of the screen (outward, going your direction when x-axis points to the right). So they build a matrix that "follows this convention" which makes sense. Then they say in the note "yes but wait, normally the camera points down the negative z-axis, so you should be doing and not , and they explain that in the case of ray-tracing the fact that the camera looks down the z-axis is performed when the rays are cast (by inverting their z-coordinate). 

You have a similar situation. If you look at the PDF for the cosine sampling, then you will realise that terms can be cancelled out. Which doesn't mean they are 'not' strictly necessary. They are, they just cancel each other out which allows to optimize the code slightly (and avoid a few division, multiplication, etc.). You are more in the micro-optimisation here... which can be confusing if you try to learn the theory by just looking at optimised code (which is often not properly commented). $ \dfrac{(cos(\theta) ... )}{PDF} = \dfrac{(cos(\theta) ... )}{\dfrac{cos(\theta)}{\pi}} = ... $ 

This is a very common question that most people in your situation (interested or fascinated shall I say in Computer Graphics) have and a problem they want to figure out. The solution is pretty simple, though not necessarily that straightforward to implement. I can't make a specific answer because it depends on the OS you are using, and as I said this is not something you can do in 2 lines of code. The answer is to use some sort of library that: 

Your question is very broad because it is close again to "how to I render 3D objects". So some people might give you a more precise answer than mine but at least I will try to give you some pointers. 

That's the a hierarchy where is the parent of . What does that mean is that if you move then will move with it. If you move instead, then of course will move but will stay where it is. That's what parenting is used for. So I as mentioned you can apply a transform to as well as a transform to and transforms as you know can be expressed as 4x4 matrices. So has a 4x4 matrix to represent its transformation (scale, rotation, translation) and has one too. 2) so the sphere is affected by the matrix applied to (since it is parented to ) and the matrix applied to the sphere itself do you agree? If you want to transform the sphere then you need to apply both transformation: 

allows you to create a window on your system allows you to display some content (an image) in the window you created and displayed to the screen 

You don't seem to get the concept right: A look-at camera matrix is created by defining two points, the "from" point (the dog eyes for example assuming you take the position of the eyes of the dog and average them to get the point in between) and a "to" point which is where the dog looks at. So if your dog is at (3,3,3) and that you want it to look at the some food which is at (-10, 100, 20), then you need to call the lookAt() method with these 2 points and it will create a matrix that will actually orient and position the camera properly. Now you are getting confused (probably) because most cameras in most 3D packages are positioned at the origin (by default) and looking down the z-axis (by default). You can create such camera with a lookAt() function btw that would look like that: 

If you use the method in a renderer, it will work as you expect. There's no z problem. if you try to use it on a camera that is already oriented along the negative z-axis and yet has identity matrix yes indeed you will be looking the wrong way. This can be fixed easily by flipping the camera back one more time along the z-axis. Simply create another matrix that scales the object/camera by (1,1,-1). 

A lot of applications like Maya for instance use libraries for creating their UIs (Maya uses Qt so as most products developed by the Foundry: Nuke, Marri, etc.) which is why a lot of these applications look the same ... (sadly). Qt and most UI library/framework use a graphics API to accelerate the rendering of widgets. Some applications like Unity do not use "commercial" libraries like Qt. They develop their own, but in the end they also use the GPU to render their GUIs. 

So no it's not a typo. Now the reference you are pointing out maybe assume this matrix needs to be used in some OpenGL render in which there is no possibility to invert they ray direction when they cast. So they indeed use instead which is the same as multiplying the cam-to-world matrix you would get from the reference by a matrix that flips the coordinates z-axis (multiply by (1,1,1-)). So no typo, things are quite logical in both cases. The result you get is always constrained to a lot of small details, such as the handiness of the coordinate system and how is the matrix later used down your graphics pipeline (OpenGL pipeline, ray-tracer, renderer, animation package, etc. which may follow its own conventions, etc.). EDIT 2 Note that in the reference your provide (screenshot of book) they do but then they use the negative of that in the matrix construction. Not sure why. Then also no idea why the multiply the eye by the vectors for the translation part of the matrix. It seems to be using row-order matrices for the axis of the coordinate system but then the translation part should be at , and and not , and . Not sure what book this is but seems to be terribly wrong and confusing. EDIT 3 Code (that seems to be the only solution): 

First look on the web. There are plenty of tutorials that explain to you exactly what you to do (which is to raytrace texture mapped objects). While it's great to ask questions, there is really no excuse for posting such a generic question here, considering the amount of information that is available on the topic on the Web. Though I will try to give you some pointers nonetheless (because we are kind and not here to judge each other;0) 

Please read the references that are given to you: $$S = \dfrac{1}{\tan(\dfrac{fov}{2}*\dfrac{\pi}{180})}$$ $$ \left[\begin{array}{cccc} S && 0 && 0 && 0 \\ 0 && S && 0 && 0 \\ 0 && 0 && -\dfrac{f}{(f-n)} && -1\\ 0 && 0 && -\dfrac{f*n}{(f-n)} && 0\\ \end{array}\right] $$ In this reference they use row-major order matrices so your assumption that your matrix is column-major is wrong (to start with). Then you will need to check the code you use to multiply this matrix with points. All source code is provided on the reference, so you just need to read and reproduce. 

You always need to multiply by the cosine term indeed (that's part of the rendering equation). Though when you do indirect diffuse using ray-tracing and thus monte-carol integration (which is the most common technique in this case), you have to divide the contribution of each sample by your PDF. This is well exampled here. Note also that in the mentioned reference, if the PDF has terms that you also find in the rendering equations then you can optimise the code if you wish by cancelling out these terms. 

The first task is generally quite complex regardless of the OS you use (MacOS, Windows, Linux). For the second step, you need to generally go through some sort of graphics API (like OpenGL). So there is hope for you. One thing you can do is use some other library that works as a wrapper around 1) and 2) In other words they help you especially with the creation of the window, and then if you know OpenGL you can do the rest of the work. You will need to create some texture that you will fill with the content of your image as it gets rendered and display this texture to the screen (that's pretty basic in OpenGL though you need to know this API in order to do so). GLFW is a great library that will allow you to do that. It's simple to use and compile and install, etc. You can alternatively use Qt but I don't recommend it unless you want to become a professional programmer. I had some documentation about the whole process but it's offline right now. I might put it back in a few weeks but you will need to wait I am afraid. EDIT I am editing the answer to respond to your comment. The algorithm would look like this: 

if you don't do path tracing (aka don't simulate global illumination) the object won't reflect light from other objects in the scene, just light from direct light sources. So in the old time, in order to compensate for that, they would add an ambient term. In these days in age, this is considered as 'heresy'. Though if you account for the ambient term still then Ka + Kd + Ks should be again equal to 1 (same principle). Not sure to understand what you say here but all these components are additive. You compute the diffuse multiplied by Kd and then you multiply the specular multiplied by Ks and then you add the two resulting numbers together. And that's your object color at the intersection point. 

Comment: if you make references to book, then it would be good if you could provide references. To answer your questions: 

and control how shiny or/and diffuse your object is. About conservation of energy. In short an object can not reflect more light that it receives. So that means that technically if the specular() and diffuse() function do the right thing ought to be . If that wasn't true you would violate the principle of conservation of energy. In practice, it is generally good to follow this rule though most of the basic shading models are based on simple diffuse and specular models (for example the Phong model) and are not themselves energy conserving, so trying to be purely energy conserving at this level, is like trying to say you can paint a miniature with a paint roller. Forget about it, just try to be more or less in the right range. and generally stand for the specular and diffuse coefficients. 

Use acceleration structures. The idea is that you store your geometry (triangles) into generally some kind of volumes. The general idea behind acceleration structure is to discard quickly large parts of the scene, which we know (using these structure) don't have any object that your ray will intersect. An introduction to the topic can be found here. You can also take advantage of multi-threading and parallelism (threads and SSE instructions or similar - this is harder than 1. and the benefits are not always as high but you optimise for your hardware). You can ray-trace in the GPU but ray-tracing is a memory-bound problem so you will have memory problems quicker with the GPU than with CPU. 

This is called "deep shadow mapping" and deep compositing. Sadly invented long before you had this ideas yourself); Now you are talking about implementing this idea specifically on a given architecture (GPU). It's up to you to make it work for this given architecture, and if you have technical difficulties with that maybe you can ask a question on this forum again. 

What you want to do is called texture mapping. The way it's done is simple. Each triangle of your geometry holds what we call vertex coordinates, basically it explains how a triangle is actually mapped onto an image (or texture). Imagine your triangle laying flat on the image. The part that it lays on is that part of the texture that should appear on the triangle once rendered. Note that texture coordinates are 2D coordinates and that the shape of the triangle in the texture space (when it lays flat on the texture) doesn't have to match the shape of the triangle in 3D space (in which case the texture will be stretched). So your 3D object when you render it should have a set of 3D points (the vertex position) 3D normals (the normals of the object at the vertex position) and 2D coordinates (read from the OBJ file). When the ray hits a triangle your compute the texture coordinates at the intersection point using the barycentric coordinates of the intersection point and the texture coordinates set at the 3 vertices of the hit triangle. This is very common technique you will find many tutorials about that. Then once you have your interpolated texture coordinates you use it to do a lookup into the texture. The texture coordinates are in the range [0:1] (in general), where (0,0) might be the top-left corner if your texture and (1,1) the bottom right. You need to remap that texture coordinate to pixel space (multiply it by image width and height and cast to integers eventually). Now just read the pixel value at that position (remapped to pixel coordinates), and that's the color of the mapped intersection point. Easy, breezy! 

First start to be sure you understand what matrices are (4x4 which are the most common in CG). 1) about your first question. You are taking about a hierarchy of transforms. I don't know if you use Maya at all, but if you do, you know you can for example create a sphere and group this sphere. So you have: