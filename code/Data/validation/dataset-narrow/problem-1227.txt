The center for computational intractability keeps a list with theory-only positions. There's also a page on the FPT wiki for positions related specifically to parameterized complexity theory. (Update: Sorry Aaron, I only noticed later that you posted the same link, and earlier; I'll leave it here for completeness.) 

I am looking for an implementation of an algorithm to compute the pathwidth of a graph. It is well known that computing the pathwidth is equivalent to computing the node searching number, vertex separation number, or interval thickness of the graph. The algorithm does not have to be very fast; I want to run it on graphs of at most 20 vertices. I do require the algorithm to compute the pathwidth exactly, rather than giving an approximation. I am aware that there are some implementations to compute the treewidth of a graph (a related concept) but have not been able to find any to compute the pathwidth. Any pointers are appreciated! 

Consider the Dominating Set problem in general graphs, and let $n$ be the number of vertices in a graph. A greedy approximation algorithm gives an approximation guarantee of factor $1 + \log n$, i.e. it's possible to find in polynomial-time a solution $S$ such that $|S| \leq (1 + \log n) opt$, where $opt$ is the size of a minimum dominating set. There are bounds showing that we cannot improve the dependency on $\log n$ much $URL$ My question: is there an approximation algorithm which has a guarantee in terms of $opt$ instead of $n$? In graphs where $n$ is very large with respect to the optimum, a factor-$\log n$ approximation would be much worse than a factor $\log opt$ approximation. Is something like that known, or are there reasons why this cannot exist? I am happy with any polynomial-time algorithm which produces a solution $S$ such that $|S| \in O(opt^c)$ for some constant $c$. 

I am asking this question again. I am aware of, and have read the other similar "alternative proof TM" questions, but unfortunately, they do help me. I am looking for a TM Halting Problem proof that does not have the following properties: 

Specifically, note the / gates which are / consisting of arity sized first function feeding the second function, where the number of first function gates is equal to the number of times arity appears. For example, represents "Two 2 input AND gates feeding a NOR gate". My point is: Taken separately, a function can be built using three 2 input OR gates and a 3 input NAND gate, for a total area of ~4.56, not including any area used for interconnect. Yet this primitive can be realized in an area of just 1.72, which means a discrete manifestation of the same boolean function consumes 2.65 times more area. Also note that the area for an $n$ input {AND, NAND, OR, NOR, XOR, XNOR} gate, where $n\ge2$, is much less than the area that it would take to build the same function using discrete 2 input gates. Also note that while the area given for {XOR, XNOR} for this process is "large" relative to the other gates, there are other ways to build the same $n$ input gates using less area. The propagation properties for the more complex primitives is also significantly better than what would be achieved using discrete gates. Why is this important? Because for me, at least, I've spent a simply enormous amount of time sifting through results from complexity theory that are built on a set of assumptions that has the effect of either rendering the result useless or wrong once the assumption is violated. The following is from Steven Cooks $\mathcal{P}$ vs $\mathcal{NP}$: 

Is this true? Does it look familiar to anyone? I'm not even sure what keywords to use when searching for literature on this topic, so any input is appreciated. Observe that the converse certainly holds: if $t = \sum_{i=1}^n \alpha_i s_i$ for integers $a_i$, then evaluating the same sum mod $q$ for any modulus $q$ still gives equality; hence a linear combination with integer coefficients implies the existence of a linear combination for all moduli. Edit 14-12-2017: The conjecture was initially stronger, asserting the existence of a linear combination over $\mathbb{Z}$ whenever $t$ is a linear combination mod $q$ for all primes $q$. This would have been easier to exploit in my algorithmic application, but turns out to be false. Here is a counter-example. $s_1, \ldots, s_n$ are given by the rows of this matrix: $\left( \begin{array}{cccccc} 1 & 0 & 0 & 1 & 1 & 1 \\ 0 & 1 & 0 & 1 & 1 & 1 \\ 0 & 0 & 1 & 1 & 1 & 1 \\ 0 & 0 & 0 & 0 & 1 & 1 \\ 0 & 0 & 0 & 1 & 0 & 1 \\ 1 & 1 & 1 & 0 & 0 & 1 \\ \end{array} \right)$ Mathematica verified that the vector $t = (1,1,1,1,1,1)$ is in the span of these vectors mod $q$ for the first 1000 primes, which I take as sufficient evidence that this is the case for all primes. However, there is no integer linear combination over $\mathbb{Z}$: the matrix above has full rank over $\mathbb{R}$ and the unique way to write $(1,1,1,1,1,1)$ as a linear combination of $(s_1, \ldots, s_6)$ over $\mathbb{R}$ is using coefficients $(1/2, 1/2, 1/2, -1/2, -1/2, 1/2)$. (You cannot write $t$ as a linear combination of these vectors mod $4$, though, so it does not contradict the updated form of the conjecture.) 

The follow link gives an overview of most CMOS gates. Note that "AND OR Inverted" (AOI) and "OR AND Inverted" (OAI) in the link. These circuits are typically a fraction of the size it would take to create the same circuit using their discrete components. For example, a OAI33 circuit (taken from a commercial foundries standard cell library) takes ~$1.62^2$ area, but building the same circuit using the equivalent discrete cells takes ~$3.82^2$ area. 

To which the answer is: no, no one builds PARITY circuits in the real world this way. The last time anyone wanted to do this was when the only thing they had to work with was mechanical relays and this is why Shannons ~$2^n/n$ lower bound for most circuits result is for {AND, OR, NOT}. Even Shannon knew XOR could not be represented efficiently using just {AND, OR, NOT}: 

Interestingly, VLSI circuit complexity has a tendency to treat depth as "irrelevant" as there is one and only one "depth" that matters: the critical path. For most practical purposes, an arbitrarily complex circuit can be treated as $O(1)$ with a latency of $n$. In fact, I'm not even sure that the concept of $DLogTime$ / $NLogTime$ directly translates in to VLSI circuit complexity. Even Shannons $2^n/n$ result does not readily translate: Shannons results is valid only for a boolean basis consisting of arity $\le2$ {AND, OR, NOT}. This is not the only basis, and the number of "gates" needed drops dramatically as you allow more and more gate types. The following are $area^2$ from a commercial VLSI standard cell library normalized to the size of a 2 input NAND gate: 

I think it is far more interesting that the circuit complexity classes used by CS complexity theory make different predictions and use different metrics than those in the VLSI community. From The VLSI complexity of Boolean functions: 

Directed feedback vertex set (make a given digraph acyclic by deleting at most k vertices) parameterized by the size of the solution Planar Vertex Deletion (make a graph planar by deleting at most k vertices) Edge Multiway cut (given an undirected graph and a list of terminals, delete at most k edges to ensure all the terminals end up in a different connected component) 

An instance of CNF-SAT with $k$ variables can easily be written as a 0/1 integer linear program over the same variable set, since a clause such as $x_1 \vee x_3 \vee \neg x_4 \vee \neg x_6$ naturally corresponds to a constraint $x_1 + x_3 + (1-x_4) + (1-x_6) \geq 1$, when all variables are forced to take values $0$ and $1$. Hence if integer programming in $k$ variables can be solved in time $k^{O(k^\alpha)}$ for some $\alpha < 1$, then CNF-SAT with arbitrarily long clauses can be solved in $k^{O(k^\alpha)} = 2^{O(k^\alpha \cdot \log k)}$, which would contradict the Strong Exponential Time Hypothesis since $c \cdot k^\alpha \cdot \log k < k$ for any constant $c$, $\alpha < 1$, and sufficiently large $k$. 

After computing the list of minimal satisfying partial assignments for the output gate, the answer to the weighted circuit sat problem can easily be read off. If there is a partial assignment that causes the output gate to be satisfied and that assigns values to at most $k$ variables, then there is a satisfying assignment of Hamming weight $k$; just set additional variables to true until reaching an assignment of weight exactly $k$. By the definition of partial satisfying assignment, this does not change the output gate's answer. The main point is then to analyze the runtime of the procedure when applied to a circuit of depth $d$ with $n$ gates. The runtime depends on the sizes of the sets of partial assignments, i.e., the number of different minimal partial satisfying assignments for each gate. The minimal satisfying assignments for the input gates have size 1. The set for an OR-gate has size at most $n$ times the size of sets one level lower. The sets for an AND-gate have size at most quadratic in the size of the sets one level lower. So we get a recurrence. Let $S(d)$ denote the maximum number of minimal partial satisfying assignments for a gate at depth $d$. Then $S(0) = 1$ and $S(d) \leq \max(n \cdot S(d-1), S(d-1)^2)$. It follows that $S(d) \leq n^{O(2^d)}$, so that for constant depth the sizes of the sets remains polynomial. Hence the algorithm runs in polynomial time. 

I have spent quite a bit of time looking for alternate formulations of the Halting Problem that are not just simple permutations of the original proof given by Turing. Question: Can you point me to a vetted proof of the Halting Problem that shares as absolutely as little in common with the one given by Turing? Please, instead of arguing with me as to whether or not my reasons are valid, or that I "don't get it and should just accept the proof given by Turing", it would be a great help to me and possibly someone else if you could simply help me locate an alternate proof. Yes, I am looking for proofs with certain properties, properties that inconveniently cull a number of candidates. Despite this, they are properties that I unfortunately need. 

The following describes an eight transistor full adder circuit, which is typically defined in boolean algebra as $s = a \oplus b \oplus c_{in}$. For comparison, a typical 2 input {NAND, OR, XOR, etc} gate is typically composed of four to eight transistors. 

I'm looking to use something close to the following as a definition for a circuit class. This is obviously semi-informal. I am curious if any one sees any potential problems with it, or where something in it's informal nature can potentially be a problem when it is tightened up to something more formal. Also, I'm not aware of any vetted and accepted circuit class "like this", so pointers to something I missed are welcome as well. In particular, the use of time is critically important to what I'm working on. TIA. The circuit class TLU, or Table Look Up, is defined as taking $n \ge 1$ $\{0,1\}$ inputs that produces a single $\{0,1\}$ output. An individual $n$ input TLU gate is defined as an indexed array of $\{0,1\}^{2^n}$, where $n$ represents the index that selects a particular $\{0,1\}$ from the array. For example, $\{1,1,1,0\}$ would represent the common $\text{NAND}$ gate. An individual $n$ input TLU gate has a depth of $1$, a gate count of $1$, and takes atomically $O\left(1\right)$ time to produce an output from a given input. For each $n$ input TLU gate set, there are $2^{2^n}$ possible indexed arrays. All TLU gates advance in time atomically with respect to all other TLU gates. For some large value of $n$, one may wish to map a large $n$ input TLU gate to a set of $\lt n$ input TLU gate primitives. A common example would be the set of $\le 2$ input $\{\text{AND}, \text{OR}, \text{NOT}\}$ gates. In The synthesis of two-terminal switching circuits, Shannon proved maximum worst case bounds for gates and depth for the $\{\text{AND}, \text{OR}, \text{NOT}\}$ set. This result can be extended to make the following claim: As long as the set of primitives contains a complete boolean basis, there is guaranteed to be a maximum worst case bounds for gates and depth for that set of primitives. 

I have a set of $n$ binary vectors $S = \{s_1, \ldots, s_n \} \subseteq \{0,1\}^k \setminus \{1^k\}$ and a target vector $t = 1^k$ which is the all-ones vector. 

Here's another example from graph theory. The graph minor theorem tells us that, for every class $\mathcal{G}$ of undirected graphs that is closed under minors, there is a finite obstruction set $\mathcal{Obs(G)}$ such that a graph is in $\mathcal{G}$ if and only if it does not contain a graph in $\mathcal{Obs(G)}$ as a minor. However, the graph minor theorem is inherently nonconstructive and does not tell us anything about how big these obstruction sets are, i.e., how many graphs it contains for a particular choice of $\mathcal{G}$. In Too Many Minor Order Obstructions, Michael J. Dinneen showed that under a plausible complexity-theoretic conjecture, the sizes of several of such obstruction sets can be shown to be large. For example, consider the parameterized class $\mathcal{G}_k$ of graphs of genus at most $k$. As $k$ increases, we can expect the obstruction sets $\mathcal{Obs}(\mathcal{G}_k)$ to become more and more complicated, but how much so? Dinneen showed that if the polynomial hierarchy does not collapse to its third level then there is no polynomial $p$ such that the number of obstructions in $\mathcal{Obs}(\mathcal{G}_k)$ is bounded by $p(k)$. Since the number of minor obstructions for having genus zero (i.e. being planar) is just two ($\mathcal{Obs}(\mathcal{G}_0) = \{K_5, K_{3,3}\}$), this superpolynomial growth is not immediately obvious (although I believe it can be proven unconditionally). The nice thing about Dinneen's result is that it applies to the sizes of obstruction sets corresponding to any parameterized set of minor ideals $\mathcal{G}_k$ for which deciding the smallest $k$ for which $G \in \mathcal{G}_k$ is NP-hard; in all of such parameterized minor ideals the obstruction set sizes must grow superpolynomially.