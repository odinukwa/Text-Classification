What about negative exponents? Your function is underspecified; you may want to try to coerce a more precise specification out of Jessie. One possibility is that whenever , which would make extensionally equal to . But it's also possible that whenever . You can also prove properties of the function independently of the function's definition. For example, you can use induction on the arguments to prove interesting properties like 

I doubt that this author is misinformed… but in any case, you did miss part of the problem. In all fairness, the statement you quote doesn't describe the whole problem. I don't have the book, perhaps a later section expands on the topic after covering more background material. You're only describing how to implement a test for alpha equivalence. This is not enough: you need to maintain alpha equivalence in any transformation that you apply to terms. Formally speaking, whenever a function takes a term as argument, it must return the same result (or an alpha-equivalent result) when you apply it to an alpha-equivalent argument. Sounds easy? Let's take substitution, which is the first major function you encounter when studying lambda calculus. $$ (\lambda x. x y) [y/z] = (\lambda x. x z) $$ Looks easy so far. Let's rename a bound variable — we're just changing a term for an alpha-equivalent one. Can we do the same renaming in the result? $$ \color{red}{(\lambda z. z y) [y/z] = (\lambda z. z z)} \quad \text{?!}$$ Oh dear. That's wrong. What did we miss? Ah, yes: the rules of capture-avoiding substitution say that we aren't allowed to substitute that $z$ under the $\lambda z$. So what's the result? Undefined? Of course not — substitution happens when you do a beta reduction, and lambda terms don't get stuck, so there must be a result. The normal way to see this result is to apply an alpha conversion first, and then perform the substitution. $$ (\lambda z. z y) [y/z] \equiv_\alpha (\lambda x. x y) [y/z] = (\lambda x. x z) $$ Renaming bound variables so that they don't clash with free variables is the gist of the variable convention — the convention that people invariably follow when they aren't reasoning about variable names, which explains how not to worry and learn to love variable names. The variable convention is not too difficult to apply on paper, though you need to be careful when you take terms from different sources and put them together. But when you're processing terms automatically, it gets to be a pain, and error-prone. You need to be especially careful when manipulating terms under binders, which compilers and optimizers do. Automatic reasoning up to alpha equivalence is painful in many theorem provers, which has motivated a lot of research on metatheory of binders and their representations, especially with the increased interest in mechanizing programming language theory in the 2000s. The POPLmark challenge was a focal point of this research (it wasn't solely about variable binding, but that was a major part of the difficulty). One approach worth mentioning is locally nameless representations, where free variables get names but bound variables are represented canonically (i.e. there is a single way to write a term) using De Bruijn indices — but there are many other approaches. 

The worst case of a comparison sort is always $\Omega(n\,\log n)$. This is not something that can be improved by further research: it is a mathematical theorem. The sketch of the proof is as follows: given $n$ input elements, there are $n!$ (factorial $n$) permutations of these elements, only one of which is sorted (when the elements are all distinct, which can happen by hypothesis 2). The sorting algorithm must work for all possible permutations of the input, so it must have $n!$ different possible executions. If the algorithm makes at most $k$ comparisons (which by hypothesis 1 is the only way to distinguish between inputs that must be sorted differently), then there are at most $2^k$ possible different executions. Therefore $n! \le 2^k$, i.e. $k \ge \log_2 (n!)$. It is known that $\log_2 (n!) = \Omega(n \log n)$ (it's a consequence of Stirling's approximation). Hence $k = \Omega(n \log n)$, i.e. the sorting algorithm must make at least $\Omega(n \log n)$ comparisons in the worst case. There are well-known $O(n \log n)$ sorting algorithms on arrays and lists (heap sort, merge sort). Hence $\Theta(n \log n)$ is the best worst-case bound for a comparison sort on an array or a list. If you look at average-case time complexity, you still can't do better than $\Omega(n \log n)$ if all permutations of the input are equiprobable. On the other hand, if you allow different permutations to have different probabilities, you can get a linear sort on average — with assumptions like “the input is already sorted with probability $1 - 1/n^2$”. Other models of sorting It is possible to have a $O(n\,\log \log n)$ or linear or even better sorting algorithm if you relax the assumptions. If you allow very exotic models, such as “wave a magic wand” or “the only valid input is already sorted”, then $O(1)$ is possible. Any model that allows $O(1)$ sorting is not likely to be useful. If you allow “return the smallest input element” as a primitive (which invalidates hypothesis 1), bounded-time operation, then selection sort has a linear running time. If there is a finite bound on the number of distinct input elements, then a linear-time sort is possible. For example, if the input consists only of 's and 's, then you can put all the 's before all the 's in linear time. Radix sort generalizes this to input data that are $m$-bit strings: it has a run time of $\Theta(n \, m)$. With a fixed $m$ (i.e. a fixed finite input domain), this is linear in the number of inputs. 

To my knowledge, such a reduction is in fact known: Hrubes and Wigderson ITCS 2014 show how division gates can be eliminated from non-commutative circuits and formulas which compute polynomials. They also provide exponential-size lower bounds for non-commutative formulas with division (not circuits) that compute any entry of the matrix inverse function $X^{-1}$. Moreover, your main question about lower bounds for non-commutative circuits, is not known (while for formulas it is known as mentioned above), because non-commutative circuits in which each gate computes a polynomial (not a rational function) with division constitutes a class which is at least as strong as non-commutative circuits. But there is no known super-polynomial non-commutative circuit lower bound (see [Hrubes, Yehudayoff and Wigderson STOC 2010] on this). 

If I understood correctly the question, the so-called Buss-Pudlak game provides a simple transformation from a proof system to such a decision tree (see Buss-Pudlak '94 $URL$ The queries are formulas (not just variables). The tree is also completely deterministic. Other decision trees that correspond to different propositional proof systems exist: e.g., Linear Decision Trees correspond to Res(lin) refutations (cf., $URL$ and $URL$ But there are many other examples (cf., Tonian Pitassi's work on CP-like proof systems). 

For strong enough proof systems the graph representation of a proof in the system seems less consequential, since (as Joshua Grochow already commented), DAG-like and tree-like Frege proofs are polynomially equivalent (see Krajicek's 1995 monograph for a proof of this fact). For weaker proof systems such as resolution, tree-like is exponentially weaker than DAG-like proofs (as Yuval Filmus described above). Beckmann and Buss [1] (following Beckmann [2]) considered restricting the height (equivalently, depth) of the proof-graph of constant-depth Frege proofs and investigated the relationship between DAG-like, tree-size and height of constant depth Frege proofs. (Note the distinction between restricting the depth of the proof-graph and restricting the depth of a circuit appearing in a proof-line). There might also be separations between tree-like and DAG-like Nullstellensatz (and polynomial calculus) proofs, which I currently don't remember. 

If $f$ is an elementary symmetric polynomial over a finite field then it can be computed by polynomial-size uniform $TC^0$ circuits. If $f$ is an elementary symmetric polynomial over a characteristic $0$ field, then it can be computed by polynomial-size depth three uniform algebraic circuits (as you already mentioned the Newton polynomial; or by the Lagrange interpolation formula); and so I believe this then translates to polynomial-size uniform Boolean circuits (though perhaps not of constant depth) (but this may depend on the specific field you're working in; for simplicity you might consider the ring of integers; though for the integers I presume $TC^0$ is enough to compute symmetric polynomials in any case.) If $f$ is a symmetric polynomial over a finite field then there is an exponential lower bound on depth three algebraic circuits for $f$ (by Grigoriev and Razborov (2000) [following Grigoriev and Karpinsky 1998]). But, as mentioned in 1 above, this corresponds only to constant-depth Boolean circuit lower bounds (while there are small uniform Boolean circuits in $TC^0$; meaning also that the polynomials are computable in polynomial-time). 

The question seems quite open ended. Or perhaps you wish to have a precise characterization of the time-complexity of any possible symmetric polynomial over finite fields? In any case, at least to my knowledge, there are several well-known results about the time-complexity of computing symmetric polynomials: 

The following result by Raz (Elusive Functions and Lower Bounds for Arithmetic Circuits, STOC'08) is aimed at $VP\neq VNP$ (and not directly $P\neq NP$), but it might be close enough for the OP: A polynomial-mapping $f:\mathbb F^n \to \mathbb F^m$ is $(s, r)$-elusive, if for every polynomial-mapping $Γ : \mathbb F^s → \mathbb F^m $ of degree $r$, Image($f$)$\not⊂$ Image($Γ$). For many settings of the parameters $n, m, s, r$, explicit constructions of elusive polynomial-mappings imply strong (up to exponential) lower bounds for general arithmetic circuits. 

It is well known that random $ k $-CNF formulas over $ n $ variables with $ cn $ clauses are unsatisfiable (i.e. they are contradictions) with high probability, for sufficiently large constant $ c $. Thus, random $ k $-CNF formulas (for $ c $ large enough) constitute a natural distribution over unsatisfiable Boolean formulas (or dually, over tautologies, i.e. negations of contradictions). This distribution has been studied extensively. My question is the following: are there any other established distributions over propositional tautologies or contradictions, that can be considered as capturing the "average-case" of tautologies or unsatisfiable formulas? Have these distributions been intensively studied? 

Yes. If $ 0< \epsilon <1$ is a constant (or $1/\textit{polylog}(n)$), and you are promised that at least $ \epsilon 2^n $ of all possible assignments are satisfying the input 3CNFs, then you can find such an assignment in deterministic polynomial-time. The algorithms is not difficult: Claim: Under the promise stated, there must exist a constant size set $ S $ of variables that hits all clauses in the 3CNF, in the sense that every 3-clause must contain a variable from $ S $. Proof of claim (sketch): Otherwise, there must exist a large enough family of 3-clauses from the 3CNF, in which each variable occurs only once. But this family, when sufficiently large, has already less than $ \epsilon $ fraction of satisfying assignments. QED Thus, you can run over all possible (constant number) of assignments to $ S $. Under every fixed assignment to $ S $, the 3CNF becomes a 2CNF, by the assumption that $ S $ hits the original 3CNF. Now, you can use the known polytime deterministic algorithm for finding a satisfying assignment for 2CNF formulas. Overall, you get a polynomial time upper bound. The algorithm for 2SAT is I think already in S. Cook famous 1971 paper. The algorithm for 3CNFs is from: L. Trevisan A Note on Deterministic Approximate Counting for k-DNF In Proc. of APPROX-RANDOM, Springer-Verlag, page 417-426, 2004 The original paper showing the result for 3CNF is: E. Hirsch, A fast deterministic algorithm for formulas that have many satisfying assignments, Journal of the IGPL, 6(1):59-71, 1998