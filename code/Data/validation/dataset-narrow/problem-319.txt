I used to move databases almost constantly, due to SAN reconfiguration and migrations. Assuming that you are moving a whole server at a time, I would go with something like your path #2. (If you are moving one database at a time, and eventually doing every database on a server, that would be more problematic since you would have to be changing paths to the files.) Note that "single_user" doesn't necessarily mean YOU. You could go to DBCC CHECKDB a database and not be able to get in because someone is already in there. Prepare a script that you can run to boot "everyone but you" out of a database and keep it in a handy place. Note that SQL 2000 doesn't have the same "keep everyone out" features as the more modern versions. One old trick is to pause the SQL Server service. This will prevent new logins, but anyone who is already connected can continue as usual. So: connect via an SSMS window so you can do work, then pause the service, then kick out the undesirable connections, do your thing via SSMS command window (not the GUI, it makes and breaks lots of connections) and then un-pause the service. Warning: I'm not sure how that would play out on a cluster. It might want to failover. It is handy to have a way to keep all app users out of a server until you are done your work. Otherwise, connections can start popping up while you are trying to do things, that can lead to resource contention and/or slowness. I have used the following ways in the past, depending on the exact situation: Turning off the app server(s) Use of ALTER DATABASE .. SET RESTRICTED_USER (If app accounts are members of db_owner, sysadmin or dbcreator roles, that's a problem.) Telling the users that the system will be offline at some specific time, like a Sunday morning. (This won't work in a "for real" 24x7 environment.) Unplugging the NIC that faces the app servers or users. (In this case, I could get in via another NIC connected to an admin-only network or through ILO.) Detaching a large number of databases and reattaching them can be a lot of work. If you do that, make sure you have your "attach" script written ahead of time. I've had plenty of success stopping the SQL Server, copying everything, changing the drive letters and starting SQL Server. No detach/attach. As long as SQL Server is off and you are copying (not MOVING) files, you can't get into too much trouble, even if you are moving the system databases. Since the paths are the same, SQL Server won't realize anything has changed while the service was off. Just make sure that you get the drive letters pointed back to the correct volumes or things will go badly for you. My most frequent problem was not getting the ACLs on the file directories correct. More modern versions of SQL Server are better at setting just the permissions that the service account needs while older versions seem less fussy. If you forget to set the ACLs, and the service account isn't a local administrator (not that I'd recommend that), one or more databases may not open when the instance starts. Don't panic, just change the ACLs and attach the database. I generally use ROBOCOPY to do this sort of work. There is a command line switch to preserve ACLs. Using a CRC calculation/verification isn't a bad idea, but I've never done that. When the databases come back up, I do run CHECKDB() on all of them. I will usually prepare a script for this ahead of time, rather than relying on manually kicking off a maintenance job. That way, I can check a couple of smaller databases first before checking a large database which could take many minutes or hours to run. I doubt that a CRC check (or a Redgate Data Compare tool) would find something that CHECKDB() would miss, and if it did SQL Server wouldn't be able to fix it. After I copy the files, but before I restart the instance, I will go and slightly change the filepath of the OLD folders by renaming one of the folders. This is an extra check against the "oops, the server is still pointing to the old files" problem. Don't be in a hurry to drop the old files and recover space on the old storage and make double sure that your full backups have run successfully. Test restore a couple of those backups to somewhere else. Once you have good checkdb() runs and good full backups, then you can think about dropping that old storage and shutting down the Lefthand. The worst problems I've had with these migrations have happened after I thought I was done. That would be the SAN admin telling me that something had happened and my file systems were scrambled. (Repartioned, reformatted, copied again.) Another fun problem is the SAN being slow for no apparent reason. If you think that it will take 10 hours to copy your data and you are 30% copied at hour number 9, you have a problem. Watch the transfer times (robocopy shows % copied and gives time estimates, or you can use Perfmon) and have a fallback plan if something goes wierd. Also, I am not sure if your volumes will be partitioned for you, but you might want to be sure that they are using a 1 MB offset. On Windows Server 2008 and better, this should not be a problem. On older OS, it is. There is a ton of googlable stuff on this, and your storage guys should know about it, but I'd ask. 

Note that running drivers in-process could affect the stability of the instance, if those drivers are buggy. After the drivers are installed, you will need to configure a valid linked server. You also need to be sure that SQL Server has permission to read (and maybe write) the files. 

I think that the deadlock is due to the engine acquiring row locks on each of those itemid values, one at at time. Each query is working on an unsorted list of itemid values and acquires those locks in a different order, allowing them to get intertwined and, eventually, each query winds up with a lock the other query already has and has a lock the other query wants. The other thing is that SQL might be trying to promote the locks to a table lock. SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED on the connection that SELECTS might help, or you might get rid of the deadlock by dropping the locking granularity to page or even table. 

The versions of Visual Studio that support "Database Projects" should provide a schema comparison tool that can be used against two different "live" copies of the database (as opposed to a "live" copy and the copy that you are working on in Visual Studio.) You don't need to create a full-blown project (although Microsoft pushes you to do so since that is really the point of supporting database projects and it does have it's advantages) just to compare a couple of databases. The comparison tool will show you what is different and can generate a script to "merge" the changes. You don't have to manually keep track of your changes and you shouldn't have to write a significant amount of code to "upgrade" your production database to match your development database. The trick is that Database Projects support for older Visual Studios was provided as a download, while newer versions included it. Some editions of Visual Studio don't support it. They've changed the name of the feature every time they have released it, it seems. "Frankly, I have found the licensing surrounding this feature one of the most confusing things about it. They key thing is that you want to look for support of "Database Projects". TL;DR: If you are using Visual Studio for other things, you might already have a comparison tool via the "Database Projects" feature. If you've already paid for Visual Studio, that makes the comparison tool effectively free. 

400 million names is a lot. Am I in there? ;-) My gut level feeling says that using substring isn't going to be terribly much slower than coding up something via the CLR. I'm a SQL guy, I've done a fair amount of simple parsing in the past (2000 or 2005), and I was involved in what was going to be a very complicated parsing scheme (addresses, world-wide) written in c and called via an xproc until we found that out prototype "native" code wasn't any faster than the same thing written with tsql functions. If you want to use a language other than tsql, I'd suggest writting a CLR in c# or vb.net. For simple things, it's not hard to code in CLR. I went from newb to having a couple of working directory and file utilities in less than one morning. There are plenty of examples of simple clr procedures on the net. and you wont have to learn anything (or install visual studio) to write it in tsql No matter what, you will have to go through the table at least once. If you export, then parse and then put back what is not a small amount of data, that's a lot of time. Can you guarantee that your source isn't going to change in the mean time? Here's the thing that always seems to sneak up on everyone: What happens with the parsed data? Where does it wind up? Do you intend to update the row, perhaps there are lastname and firstname columns that you don't show in your example? If you do, and those columns are currently null or have zero length stings in them, you may find that the update statement performance is very bad because sql may have to split pages to store the lastname. In other words, your performance problem isn't parsing, it is storing the parsed data. Often, this is worse than inserting the data into another table. Also, all of those page splits will fragment your table and cause query performance to drop, which may enrage your dba because s/he will have to run a defrag procedure on the (large) table. Here's one last thought: Do you really need to store the parsed data? Can you get away with a computed column that calculates the last name on the fly? Those are indexable, with certain conditions, if you need that. Another approach would be a view that exposes the columns of the table as well as your "parsed lastname column". 

I would also expect things to be faster on the new system, especially if you have also changed the hardware. When migrating a database from older versions of SQL Server to newer versions of SQL Server, using either the backup-and-restore, detach-and-attach or upgrade-in-place methods, Microsoft recommends reindexing all of the tables in the database. Did you do that? This minimizes fragmentation in the indexes, but it also seems to update/improve the statistics for the tables in some way. Better statistics means better query optimization and (hopefully) shortened query durations. Depending on how much data we are talking about, the "DISTINCT" and "ORDER BY" clauses may cause significant tempdb use. If your tempdb is somehow slower on the new system than it was on the old system, this could be your problem. It would be very noticeable if the other queries in the system don't use tempdb as much or at all. "About 1 minute" also sounds suspiciously similar to the default timeout for queries from ASP pages. In other words, if the query used to run in 59 seconds and now it runs in an average of 1 minute and 1 second it may "suddenly" seem like it fails all of the time even though the running time has only gotten a little bit larger. The query time out is adjustable from your ASP code (the "query timeout" setting on the server pertains to linked servers, not client calls from IIS servers), but I would only suggest lengthening the query timeout as a last resort. 

There are third party products, such as SQL Lightspeed, that provide backup compression for Standard editions of SQL Server. There is a good chance that the purchase and installation of such a product will allow you to back up locally. The size of the nacho file depend on how much data is actually stored in the database and it's compress ability. 

I have used DNS C names extensively for similar purposes in the past. For the uninitiated, a C name works like an alias for a server and is configured via the DNS system for your network. One catch is that you need to shut the old server off when putting the C name into production, so all applications need to be migrated at the same time. BUT: This doesn't work for named instances. I would suggest that you consider using a named instance in order to make the migration easier. 

I'm assuming that you are using a "sql server login" for the vendor's server, and you are using that when creating the linked server. When you use a linked server, a query running on your server connects to the vendor's server. If a firewall blocks your local server from connecting to the vendor's server, your connection attempt will fail. If connecting directly from your workstation to the vendor's server you go through a different firewall or no firewall, then the connection may succeed. This scenario matches the behavior you describe. My usual tests would be: 1. Can I ping from my server to the vendor's server? Both by IP and by hostname? ping usually gets through firewalls. 2. Can I connect using SSMS from my server (using an RDP session) to the vendor's server? Both by IP and by hostname? If you can ping OK but not connect with SSMS, this usually indicates that the ports for SQL Server probably need to be opened on the firewall. In short, check all of the firewalls involved. That would include any software firewall on your server or on the vendor's server, or any hardware firewall between them. In these situations, ping and traceroute are your friends. Ping and traceroute may help you locate the IP of the router that doesn't send your packets on to the vendor's server.