$URL$ says that the node_name option also sets the client_name, but the formatting of the page makes me think that client_name might be a settable option somewhere. Can it be set differently or separately from node_name? 

There are no google hits for this error message. I am attempting to run chef-client on a server that has run it successfully in the past. I do not know what changed to produce this error. Here is the most verbose output I can think to include: 

Is there a way to make this configuration backwards compatible, so that the new option is used for newer clients and ignored for older clients? 

I don't know of any package to do this, but here's a hacky idea. You could write a small wrapper script for [HKEY_CLASSES_ROOT\exefile\shell\open\command] that logs the name of the binary being run, then fetch those logs later from each machine? It would only tell you what was launched directly by the user, not from other processes, and wouldn't tell you anything other than when it was run. 

My current project involves sending computers to clients that they will plug into their network. These computers fill the role of a web server, primarily. During a peak hour one of these servers will serve 10-20GB of mostly static content, and that content is updated locally or remotely with some frequency. Using consumer grade small form factor devices for this project is proving to be a bad idea due to failure rates and inconsistent hardware availability, and occasionally due to I/O and memory bottlenecks. What are some widespread standard options, in terms of form factor and manufacturers, for this sort of project? The OS for this device is CentOS 6, possibly 7 at some point in the future. Our current solution is approx 2"x8"x8". Slightly larger would be acceptable, but not as large as an ITX PC. We are handling backup and software security ourselves, and do not need hardware security features. 

I've encountered VM templates distributed as an OVF with a feature so that when you boot the VM for the first time you are prompted for configuration options such as hostname, IP address, root password, etc. What is used to do that? Is it specific to a certain virtualization environment, or to a specific client OS, or something else? 

I have a number of VMs that were based on the same template, each with about 2GB of storage. They all share read-only access to a 50GB volume (currently via NFS, but that is up for negotiation). I want to package up that template and volume so that a similar arrangement can be reproduced in another virtualized environment. Ideally that package would be about 52GB in size, and would allow another admin to create an environment with an arbitrary number of VMs. How should I tackle this? Can I accomplish this setup, or something similar, using OVF, or do I need to use something more proprietary like a vApp, or even a combination of multiple archives for different parts of the problem? 

I copy this configuration across many systems. Recently I copied it to a system running OpenSSH v5.3 and that entry is now an error: 

I have enabled access logging for an AWS S3 bucket. Now I get log files delivered to my chosen destination. Each file has a name of the form YYYY-MM-DD-hh-mm-ss-guidguidguid with the time representing the last entry in the log file. Each file contains one or more lines. Each line represents one access. I am getting a lot of tiny log files. Most files contain just one line, some as many as five. I am accessing the bucket up to a few dozen times a minute, so I'm getting 3-20 new log files per minute. I have found mention online that these logs are normally delivered hourly. Have I overlooked an option somewhere that controls the delivery frequency or possibly thresholds that trigger delivery? 

The traditional solution to a problem like this is for a permanent user account to exist with precisely the access levels and permissions required by the program in question. Network services (like apache) often run as their own user, with privileges distinct from those of a normal interactive user and significantly less than root. If root is running the script in the first place, they won't need to provide a password to su to another user, even if that user has a password. 

There are IPs on my network that are not configured as fixed or floating in Neutron, they are simply claimed by an instance (e.g. via ). How do I get openstack (possibly via neutron-openvswitch-agent?) to tell me which instance, if any, has claimed a certain IP? Or produce a list of such IPs? 

Running a bash script involves running the bash interpreter (which will be a process), and that interpreter reading the file, then following the script inside the file. If a process owned by a user can read the file, then the user themselves can read the file. This leaves the only option being allowing the user to spawn a process owned by root, and having that root-owned process read your script. This can be accomplished in some distros using the setuid functionality, but it is generally considered a bad idea and can lead to security holes if the script has any bugs. 

allows one user to run commands with the permissions of another user. By default, on most systems, some users are allowed to use sudo to run commands as root. requires a user's password to log in as that user, unless it is run by root. Thus, allows you to pretend to be root in order to log in as root without root's password. PS: is recommended over which is recommended over for most everyday purposes. 

I'm trying to set-up a private git repo for code sharing, but found out that most of the implementations out there require use of SSH public keys, for example: $URL$ The only approach looking reasonable is the git-daemon, but it does not contain any authentication, and while it might be a good option for LAN, it is no go for remote working. Coming from SVN daemon, where all the access was conveniently controlled via single file, the SSH keys scheme quite a hurdle for me. Is there any way to securely share multiple Git repositories, without using SSH authentication? Thanks in advance! 

Sorted it out with PowerBroker Identity Services Open Edition: $URL$ Worked out of box, without the complications of samba. 

I have a quite big problem with customer's MS Exchange. The server got it's disk filled about 2 weeks ago, so it's currently offline. They plan to upgrade it, but not in hurry, as they use it mainly for OWA and back-up - the mails exchange is done via SMTP and POP3. Trying to diagnose some problem today, one of the users has (following the ISP instructions), removed the Exchange account from Outlook, which essentially left the OST orphaned. The user naturally didn't move the emails or any other data to the Archive / PST before, so these emails located on the OST only. So currently I'm trying to figure out how to restore them. There are 2 options: 1) Make the user buy some tool to convert them to PST, and import as archive / main Outlok file? 2) Reconnect the Outlook to Exchange (once it up), let it sync the old server content, then shutdown Outlook and replace the new OST with the old one, start Outlook again in offline mode and move these files to archive. 3) Any other method? Can someone advice what would be the best approach here? The used versions are Outlook 2007 and Exchange 2003. Thanks! 

I noticed that using a dot (.) in Apache Location requires adding trailing slash in URL to work properly. Anyone knows why this happens, and how to enable it to work without the trailing slash? 

We have a quite urgent issue, where company old SBS server BSOD's right before presenting the logon screen. I ran the mini-dumps via the WinDbg, and got following output: 

We have an Exchange 2003 SP2 server on 2003, whose store got corrupted after IBM RAID got bad-stripes and bad-blocks. Unfortunately the backups were discovered out to be out of date, so now we facing almost a month worth of data loss (and very certain ire of management), unless we manage to merge the data from that old back-up, and users Outlooks. So my question, if the following possible? 

Have anyone ever managed several geographically distributed systems with Puppet? I have several almost exactly similar deployments (except the server IP's), which management I'm looking to convert to Puppet. I have 2 options: 

False alarm - apparently a new router we added wreaked havoc in network, possibly by intercepting the DNS queries. Removing the router sorted out this issue. 

I moved my CentOS soft-RAID1 drives to another box, and now it fails to boot. When I launched the boot process from the grub menu, I get the following error: "Unable to access resume device /dev/md1". (I should notice that before that I got another error related to super-block, which was fixed by re-running the mkinitrd). Any idea? 

I'm trying to install CentOS 6 on Dell R410 server. As I need a GPT large disk support, I set the BIOS to UEFI configuration. The problem is that I'm getting crashes once the server tries to boot from the media. Any idea what I can do to continue with the installation? Thanks! 

Our old 2003 mail server has died, and we are hastily setting up a new one based on Exchange 2010. We looking for a way to recover the 2003 data into 2010, any idea if possible? Thanks. 

Installed a fresh CentOS 5.10 installation on RAID5 provided by Perc H700 Integrated RAID, however the system fails to boot. Checked the system via recovery CD, and noticed that CentOS 5.10 now uses GPT instead of MBR. Updated H700 to latest firmware version, same result. Previous version of CentOS (5.x circa 2011) worked just fine on this server. Any idea how to make the RAID recognize the partition / boot record and start the system? 

Just moved the machines to second NIC - and magically they started seeing the network. Quite strange but it worked. 

One of disks in our RAID 1 configuration gone bad, and caused the whole logical disk to become in bad stripes mode. We replaced the bad disk with a new one and complete the rebuild, but the logical drive still in bad stripes mode. Now, IBM says that bad stripes is actually a counter that going up, and the only way to reset it is to erase and re-create the RAID. As this going to cause quite some downtime here due to huge amount of data involved, I'd like to know if it possible to keep running the RAID with bad stripes, or it's a really bad idea and RAID re-creation should be done ASAP. Thanks for any advice! 

We have an IBM server x3650 M3, and I'm trying to install Linux without using HW RAID (ServeRAID), as per cluster storage software vendor requirements. The problem is that the server exposes the physical drives only to WebBios interface, and Linux insallation (RedHat Anakonda) sees only the virtual drives, not the physical ones. If we clear the RAID configuration, the installation doesn't see any drives at all. Is there any way to disable the built-in ServeRaid and expose the drives to operation system? Or the only way is to bypass RAID physically and connect the disks to motherboard directly? Thanks in advance!