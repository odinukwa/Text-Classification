I have a model pipeline for finding similar text documents given an input query text. The model is very simple; I have a corpus of documents on which I train a TfIDF model. When a query is input, we can infer its TfIDF vector. Finally, we compare the query's TfIDF vector to all the vectors of the documents in the corpus using cosine similarity, which finds us the "most similar" texts. My question is, is there a way to incorporate more micro structure features into the pipeline such that similar document retrieval will perform better, and if so how? By this I mean, can we use Part of Speech tagging, intent recognition or other methods, as extra features in the pipeline. I am looking for more unsupervised methods here as I have a lot of data (1TB) and it is all unstructured and un-tagged / un-labelled, but supervised suggestions will also be welcome. I appreciate that by incorporating new features, we will most likely have to move to a completely different model paradigm as TfIDF and cosine similarities will not work depending on the structure of the new features. NOTE: I have already performed a significant amount of text pre-processing on the corpus e.g. stemming, tokenizing, word replacement, stop word removal etc. 

Like suggested in one answer on this SO question, you could use elastic matching with Levenshtein distance to your task. Levenshtein distance obeys triangle inequality and is therefore a metric distance. Use of elastic matching was suggested for time series data comparison. Levenshtein distance works with characters data. There is an implementation of elastic matching and Levenshtein distance calculation in Python. To put them together you most probably need to build your own implementation. 

Boosting can't help if decision tree in my example exactly knows before decision for example the side of turning possibility and for stump always one step after. So, stump can help in finding a statistical pattern in that example but not the underlying external facts affecting the system in certain move, if the conditions vary randomly in time. 

When considering how to clean the text, we should think about the data problem we are trying to solve. Here are few more step for preprocessing which can improve your features. 1.) Use Good tokenizer(textblob,stanford tokenizer) 2.) Try Lemmatization , stemming always not perform well in case news article. 3.) word segmentation 4.) Normalization (equivalence classing of terms) For selecting model 1.) In your example above, we classified the document by comparing the number of matching terms in the document vectors. In the real world numerous more complex algorithms exist for classification such as Support Vector Machines (SVMs), Naive Bayes and Decision Trees , Maximum Entropy. 2.) You can think your problem as making clusters of news and getting semantic relationship of source news from these cluster. You can try topic modelling(LDA and LSA) and Doc2vec/word2vec technique for getting vector for document/word and then use these vectors for classification task. Further if you are confuse to select a appropriate model for problem , you can read from this link Choosing Machine Learning Algorithms: Lessons from Microsoft Azure 

So, similarly as in convolution every nth pixel is selected for the operation. Edit Selecting points like this: 

There are alternative solutions for self organizing maps. Best of them I found pymvpa where the example is easy to read and understand. It is also maintained quite activately as you can see from their Github. I tried to run the kohonen 1.1.2 test file, but it did not run after two days of trying. So, let's have a try for the another solution. To run pyMVPA example som.py, you have to do the following (at least): 

Decision stump are decision trees with one step from root to leaves whereas Decision trees can have several steps between root and leaves. Easy example of these two is that a decision stump could be which side of coin faces up when thrown and a decision tree would be that if the coin could is touching the ground already (states are interconnected): 

Basically Original accuracy_score() function takes two arguments. first is your testing data labels(actual labels) and second your model's predictions. 

here similar document is list of tuples. where first element is label. let me know if this help you. 

Doc2vec (aka paragraph2vec, aka sentence embeddings) modifies the word2vec algorithm to unsupervised learning of continuous representations for larger blocks of text, such as sentences, paragraphs or entire documents. It will going to cluster each documents topics in vector space , learn it's semantic meaning. It will perform good with your given size dataset size.check this too Doc2Vec - How to label the paragraphs (gensim) 

I made little search some days ago to get familiar with some Backpropagation related thing and came across to find this pdf. In the beginning the author says that the approach is there taken so that it is most understandable. Directly from Author: 

I did not find scientific data, but interesting ratio is mentioned on several sports sites like this: Women have ratio 0.413 and men 0.415 and it is used by multiplying height with ratio and we get the stride length. 

In this picture from the question you can observe how it is inaccurate, even compared to SVC with kernel=linear. Further differences may be explained with differences how SMO and SVC with linear kernel are implemented. This post compare the two, and the answer is SMO should be the fastest one, otherwise your dataset maybe is not suitable for it. I would test SVC with linear kernel and see if things change. 

I always avoid for loops when operating on pandas rows - it’s slow and inefficient. If possible, try to creat some functions () and then apply these functions to the rows of the dataframe: which will vectorise the operation of applying to each row, making it much faster and more efficient. 

Assuming your data is normally distributed, you could fit a Gaussian to your data and calculate the Probability Density Function (PDF). Once you have the PDF, you can set a threshold probability, below which a data point could be classified as an anomaly If you have enough data, use a Variational Autoencoder neural network. Very roughly speaking, you train this on all the data you deem to be “normal” (the neural network learns how to reconstruct the input data in the output), and then when anomalous cases are passed to the network, it can’t reconstruct it. If the network can’t reconstruct it accurately, the data is an anomaly. 

As Kyle said on his answer word2vec can be run with the data dump data and you would get a mapping that shows the closest words, that are possible synonym candidates. Same approach is on this Quora post. Here is explained how word2vec makes a vector of probabilities of different words and with cosine similiary (highest cosine distances) you can find the nearest ones = the synonym candidates. A code example is on this Github. There is a KDT tree used and its cosine distance. (KDT = k dimensional tree) Basically a synonym is a word with enough little distance, and you can set some threshold to find all enough near ones or only the nearest. All that in code of course. In the mentioned Quora WordNet was mentioned as a source of synonyms too, but then I came up also with idea of using SE Tag Synonym dump (see here), where we have a superwised source of common synonyms. Those can be used as alternative source of synonyms, or as a database to verify the ones found by the distance method. 

Named Entity Recognition is technique which can be used here. Location is one of the 3 most studied classes (with Person and Organization). Stanford NLP has an open source Java implementation that is extremely powerful. For Example let say sentence is "i will be going to Sweden from Boston." 

As far as I understood you are using type of TV as tag of particular sentence , and you are using doc2vec model for future classification . So As above answer is suggesting that model will learn semantic meaning of type of TV(tag). let's suppose s is your future sentence for prediction. then you use infer vector. 

As you mentioned, each decision tree is trained on p (sometimes sqrt(p) random features). This ensures that each tree is “grown” (trained) differently so that the model 1. Does not overfit the training data (reduces variance) and 2. Generalizes better to new data (reduces bias). Therefore we don’t weigh the trees differently, as this would be similar to having all tress trained on the same features. You can change the voting threshold however from the standard 50% to anything you want (e.g 40%, 70%, 90%) which will change the precision and recall accuracies of the model. EDIT: changing the voting threshold means changing the number of trees needed to make a classification. For example, in binary classification most standard random forests require 50% or more of the trees to vote for a class for that class to “win” (be predicted to be that class). But if you change this threshold, to say 70%, then 70% or more of the trees need to vote for the same class for that class to win.