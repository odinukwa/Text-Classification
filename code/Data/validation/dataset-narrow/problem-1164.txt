Consider the NP-complete problem, for $k>2$, of $k$-coloring a graph: Either the graph is not $k$ colorable, or it has at least $k!$ distinct $k$-colorings, simply due to permutation symmetry of the $k$ colors. For $k=2$, the problem matches your specifications: 0 or 1 accepting paths (the latter of which will never occur) will not convince the verifier, while 2 solutions (or multiples thereof) will. 

We all know the PCP Theorem. Is there any software package availalbe taking a CNF in e.g. DIMACS format as input, and producing a PCP encoding in the same format as output? It might be interesting to run existing SAT solvers on such encoded instances and compare the performance relative to plain instances. 

Physics models reality with theories that define a concept of time-dependent state associated to a system and a time-evolution operator that describes how this state evolves. As soon as you find a physical system that (after some state-space discretization) implements the state space of your Turing machine, and that features interaction terms that implement (maybe after some time discretization) the time evolution according to the state transition table of your Turing machine on its state space, you have found a Turing-complete physical model of your system. Thus you can arguable say, your system "is" Turing-complete. When looking at quantum computing, you will find discussions of implications physical theories have on the Turing model of computation. For example, physical theories have to be reversible. A property, which is not shared by ordinary Turing machines. Yet there is no loss in generality, as any Turing machine can be simulated by a reversible one, with some overhead that can trade-off time vs. space, etc. 

Precomputation is often related to the class P/poly which captures problems solvable in polynomial time by Turing machines with access to some poly-sized advice string, that only depends on the size of the instance, but not the instance itself. The Karp-Lipton theorem states, that NP is not contained in P/poly, unless the PH collapses to the second level, which is considered unlikely by many complexity theorists. 

The paper "BQP and the Polynomial Hierarchy" by Scott Aaronson directly addresses your question. If P=NP, then PH would collapse. If furthermore BQP were in PH, then no quantum speed-up would be possible in that case. On the other hand, Aaronson gives evidence for a problem with quantum speedup outside PH, thus such a speed-up would survive a collapse of PH. 

Basically, $M$ will for all $i \in 1, 2, ...$ simulate $R$ on input $x$ and on every string from ${{0,1}}^i$ as a prefix of the string on $R$'s random tape. Now: 

We have to convince ourselves now, that if $R$ accepts (rejects) $x$ with probability $p >\frac{1}{2}$, then for some $i$ it will accept (reject) for $>\frac{1}{2}$ prefixes of length $i$ of the random string without trying to read more than $i$ bits from the random tape. It is technical, but quite easy - if we assume otherwise then the the probability of accepting (rejecting) approaches $p >\frac{1}{2}$ as $i$ goes to infinity, hence for some $i$ it will have to be $p >\frac{1}{2}$. Now we just define our deterministic machine $H$ solving the halting problem (i.e. deciding whether a given deterministic machine $N$ accepts a given word $x$) a as $H(N,x) = M(P(N,x))$. Note that $M(P(N, x))$ always halts, because deciding a language by our probabilistic machines was defined in such a way that one of those two always occurs: 

General TSP is not approximable within any factor, so your algorithm doesn't really have a chance of working well. A worst-case example: We select one Hamiltonian cycle in the clique and give all its edges weight 1 (assume this cycle is $(v_1, v_2, ..., v_n, v_1)$. All the other edges have some big weight $\alpha(n)$. Now let's have your algorithm start with odd vertices. Between any odd vertices there are only edges with weight $\alpha(n)$ and we don't have any tie breaking rule, so the cycle on odd vertices your algorithm constructs can be totally arbitrary (we can for egample forbid it to contain any edges of the form $(v_i, v_{i+2})$. When the algorithm starts working on the even vertices, it will have just two possible places to put $v_i$: after $v_{i-1}$ or before $v_{i+1}$. Again we have no tie-breaking rule, so we can assume that it always choses to put $v_i$ after $v_{i-1}$ (of course we do all this modulo $n$). When we look at the constructed cycle, we see that it is built of alternating weight 1 and weight $\alpha(n)$ edges. So we have $OPT = n$, and our solution has weight ${n \over 2} + {\alpha(n) \over 2}$. We can chose $\alpha$ arbitrarily to disprove any claimed approximation factor. There are some technicalities involved with encoding weight $\alpha$; when we increase $\alpha$ the input size also grows (but more slowly), but still I think we can get any factor. 

This problem is NP-hard. As proof, the maximal clique problem (or rather the decision variant find-a-K-clique) can be reduced to this problem as follows. Start with a problem on a graph with N vertices where we wish to find a clique of size K. The set containing these original vertices we'll call S. Add a clique (we'll call it C) of size (N - K)*K which is joined to every vertex in S. Additionally, adjoin one more vertex V which is joined only to the vertices in S (not the ones in C). Now we have an instance of your problem (never mind the edge weights) where we want to divide the resulting graph up into N - K + 1 cliques of size K+1. I claim that there is a solution to this problem if and only if there is a clique of size K in the original graph. only-if follows from the fact that V must belong to some clique of size K+1 which is only the case if there are K vertices which form a clique in S. Furthermore, there will be enough leftover nodes in C that every S-vertex not in the solution clique can be assigned to a separate set of K vertices from C. So once we've managed to find a clique for V, finding the other N-K (K+1)-cliques is always possible (and indeed trivial). 

Intuitively it seems like an optimal (either minimum depth or minimum gates) sorting network should never have to compare-swap two numbers the "wrong" way (such that the larger one goes into the smaller-indexed position and vice versa). Is this true? If so, how do you prove it? 

In this paper $URL$ they claim to have a practically fast persistent union-find data structure for most use-cases, but it's still not polylogarithmic in the worst case (the worst case being, I have two variants which I alternately update between); only in a backtracking search. Are there any implementations of a persistent union find datastructure which guarantee amortized polylogarithmic time complexity per operation regardless of how it's used? 

$v_i$'s deepest descendant $v_i$'s child on whose branch (1) is contained $v_i$'s nearest ancestor $w_i$ for whom $w_i(2)$ is not an ancestor of $v_i$ Which child of $w_i$ is an ancestor of $v_i$ 

If a CDCL SAT solver only selects negative literals as decision literals (but can set positive literals through unit propogation) but has a perfect heuristic for determining which literal to select next, what kind of claims can be made about its running time? Is it necessarily exponential on some infinite class of satisfiable instances? 

There is A Simple Proof that Toffoli and Hadamard are Quantum Universal by Dorit Aharonov which first shows how complex amplitudes can be simulated by real amplitudes over a larger Hilbert space with one more qubit. "This is done by adding one extra qubit to the circuit, the state of which indicates whether the system’s state is in the real or imaginary part of the Hilbert space, and replacing each complex gate $U$ operating on $k$ qubits by its real version, denoted $\tilde{U}$, which operates on the same $k$ qubits plus the extra qubit. $\tilde{U}$ is defined by: $\tilde{U}|i\rangle |0\rangle = [Re(U)|i\rangle]|0\rangle + [Im(U)|i\rangle]|1\rangle$ $\tilde{U}|i\rangle |1\rangle = -[Im(U)|i\rangle]|0\rangle + [Re(U)|i\rangle]|1\rangle$" Secondly, she proofs the universality of the {Hadamard,Toffoli} gate set, which has only real amplitudes $\{0,1,\pm\frac{1}{\sqrt{2}}\}$. 

In arXiv:quant-ph/0409035v2 Buhrman and Spalek present a quantum algorithm beating the Coppersmith-Winograd algorithm in cases where the output matrix has few nonzero entries. Update: There is also a slightly improved quantum algorithm by Dörn and Thierauf. Update: There is an improved quantum algorithm by Le Gall beating Burhman and Spalek in general. 

The question appears to be related to the question of whether there exists an oracle relative to which the counting hierarchy CH is infinite or not. This is an open problem according to the Zoo. 

You might be interested in Scott Aaronson's recent paper Why Philosophers Should Care About Computational Complexity, to appear in Computability: Gödel, Turing, Church, and Beyond, edited by B. J. Copeland, C. Posy, and O. Shagrir, MIT Press, 2012. ECCC TR11-108, arXiv:1108.1791. 

Counting constraint-satisfaction problems (#CSP), evaluating partition functions of many physical models, as well as many topics in classically simulating quantum states/circuits, are all fundamentally contracting tensor networks, which is a #P-complete problem. For a good overview see the following papers: Itai Arad, Zeph Landau, Quantum computation and the evaluation of tensor networks Cai, Lu, Xia Holographic Algorithms with Matchgates Capture Precisely Tractable Planar #CSP See especially the introduction of the latter for the connection to physical models. 

Early distributed systems theory, especially papers by Leslie Lamport et al., has had some impact from Special Relativity to get the correct picture w.r.t. to (fault-tolerant) agreement on a global system state. See entry 27. (Time, Clocks and the Ordering of Events in a Distributed System, Communications of the ACM 21, 7 (July 1978), 558-565) in the Writings of Leslie Lamport, where Lamport gives the following background information on his paper: 

The maximum independent set problem gives a lower bound for the minimum clique cover problem. This is easy to see because given any clique cover together with an independent set, any two vertices which are part of the independent set must belong to separate cliques in the clique cover. Is there any way to put a worst case (hopefully constant) upper bound on the ratio between these two values? What about when the number of vertices grows large? I know it must be at least $\frac{3}{2}$, since we can have a graph composed of an arbitrary number of pentagons (each pentagon has a maximum independent set of 2 vertices, but a minimum clique cover of 3 cliques). Can anyone find a graph where the ratio is larger than $\frac{3}{2}$? larger than 2? 

The longest path is just the path from to Now to show that the number of deepest-branch changes is amortized constant with each add, just imagine that each time we compare two depths and get d(a) == d(b) (when we stop), we reserve one time unit for later. When a node changes deepest branches, the paths must have first been equal so we use up that time unit. So now all that remains to prove is that the path-compression step is amortized fast and I unfortunately don't know how to do that. Please note I haven't tested this code and it is almost certainly buggy. Also, I may have left out some edge cases such as how to insert the first node or what to do in certain situations if a node already is on the primary path. It shouldn't be too hard to account for these. 

Here's the (updated) program for generating G.Bach's graph. I added indicators at the end to check that the graph is a well-formed tournament graph: 

Ok, I have an algorithm I believe is O(nlog*(n)), that's n log-star(n) (log-star n is constant for all practical purposes), but it's hard to prove and I suppose I could be wrong. The running time guess comes from assuming the path compression step takes amortized O(log*(n)) as it does in a union-find data structure, but I don't remember the proof of that so I don't really know for certain that it applies here. Nonetheless, I certainly can't generate any examples where the path-compression is generally slow and I can prove the other operations are amortized constant time. Following Chao Xu's suggestion, I'll describe the "insert a new leaf" operation on the data structure. Each vertex has to contain pointers to 4 other vertices in addition to remembering its depth. Each vertex $v_i$ must remember: 

No, there isn't. If there was such a particular instance of subset sum problem that would be "the hardest" we could solve it by brute-force, and then hard-code it (and its answer) into our program solving subset sum problem. Then our algorithm, given an instance to solve, would first check whether the input matches that hard instance, and if so, it would simply output its answer in O(1) time - and thus, the instance wouldn't be hard anymore. In general, hardness lies in the problems (which we can see as infinite sets of instances), and not in particular instances (or even some finite sets of instances), and searching for explicit instances of the problem that are "the hardest" doesn't make much sense. You could restate the question to ask about some "hard" family of instances - i.e., such families that our problem remains NP-hard when limited only to these instances. You could probably get one example of such families by looking at the details in the reduction showing that SUBSET-SUM in NP-hard. Answer to the comment: Generally, this kind of questions is too elemental for this site - I would really advise you to read some good intro to complexity theory, or at least to do a DFS on wikipedia starting from this article: $URL$ I think you will find all your answers there. Then, if you're interested specifically in the SUBSET-SUM problem, you could look into the reduction showing its NP-hardness - you will see there arbitrary instances of SAT (or some other NP-hard problem) being mapped into some particular instances of SUBSET-SUM (the set of these "hard" instances will probably be smaller than the set of all instances, but still infinite and "dense"). And yes, if you can solve the SUBSET-SUM problem restricted to this set of instances, you will prove that P=NP. 

OK, so your goal is to show that $CLASS_1[g(f(n))] = CLASS_2[h(f(n))]$ basing on $CLASS_1[g(n)] = CLASS_2[h(n)]$ (we don't specify what exactly are this classes, we just know that they're somehow parametrized with the input size). We have a language $L \in CLASS_1[g(f(n))]$, decided by some algorithm $A$. Now we make a language $L'$ by padding each word in $x \in L$, so that it's length is now $f(n)$, and we see that it is contained in $CLASS_1[g(n)]$ (our new algorithm $A'$ basically just ignores the added zeroes and runs $A$ on the real, short input). What we do is: we take a language from the bigger class and we pad it, so that it can be solved by a weaker algorithm giving us containment in the smaller class - the weaker algorithm can do it, because it has the same amount of 'real work' to do as before, but it has its restrictions (being a function of the input length) lifted by extending the input. Now we know that $L' \in CLASS_1[g(n)]$ and hence $L' \in CLASS_2[h(n)]$ (decided by some algorithm $B'$). We would like to get from here to $L \in CLASS_2[h(f(n))]$. But that is straightforward - algorithm $B$ deciding $L$ just pads the input accordingly and runs $B'$ on the padded input. This step may be summarized as follows: we want to decide $L$ in the bigger, more resourceful class. Using our extra resources we pad the input and run the algorithm deciding the padded language. Of course there are some technical detailes involved here (f.e. we have to make sure that the padding can be implemented in the classes we consider) but I just ignore them to give the general intuition.