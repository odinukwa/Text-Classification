I have an Nginx/Gunicorn/Django web server and PostgreSQL database server that I only want to access using SSL. I've purchased, installed, and configured a certificate on my web server from a certificate authority and so now my users can only access my website via HTTPS and it's working fine. Now I'd like to implement secure two-way communications between my web server and database server over SSL. Since the only machine talking to my database server will be my web server, will it be OK from a security standpoint to generate my own private key and certificate using the openssl command ("self-signing") or should I get a free ones from somewhere like letsencrypt.org? 

I have a Django 1.6.2 application that uses Celery 3.1.7 for asynchronous tasks. I'm starting my Celery workers using Supervisor. So far everything is working well except when I reboot my Debian 7.8 server. When that happens, my Celery workers won't restart because when the server reboots, it changes ownership of the celery log files from my "celery" user to "root". Also, the system deletes my /run/celery directory where I write my pid files. If I make these changes by hand and restart Celery, all my workers start normally. Since these changes need to occur before starting the workers, I thought the solution would be to write a shell script which, due to its higher priority, gets executed from my supervisor.conf script before the celery worker commands (See below). However, this setup script won't run. My supervisor log just says, 

I have a Django/Nginx/Gunicorn web server ("web02") and an Nginx file server ("fs02") that is used to store user images. When a user uploads images through the web site, they are saved to the file server via a directory that is cross-mounted from the file server via NFS. I build my servers using an Ansible playbook that provisions each server and then configures the file server first and the web server second. When I initially build my servers, NFS works perfectly. However, if I rebuild and reconfigure just the file server (for example, if it crashes and I need to rebuild and restore it), NFS doesn't work. In that situation my web server is unable to see the exported directory on the file server. I have confirmed this two ways: 

I'm in the middle of doing some testing on a mobile device, and want to test it from the network. The mobile device will only connect to networks via WiFi, and doesn't support ad-hoc mode, so I've set up an old WiFi router as an AP, which is connected via ethernet to a port on my laptop. The router operates under 192.168.0.0/24. My laptop is also connected to our normal network via another ethernet port. This network operates under 10.0.2.0/24, and provides internet connectivity. My laptop is running a VM which has two NICs, each bridged onto the two respective physical NICs. The 10.0.2.0 network is reachable via eth0, and the 192.168.0.0 network is reachable via eth1. The problem I'm having is that all connectivity drops as soon as I run . I've tried changing routing tables and adapter metrics, but have had no luck. Here's the output from : 

I recently noticed that my ISP doesn't like routing native SCTP traffic over the Internet, unless it's tunnelled through UDP. A bit of a pain, but I solved the issue by using tunelling. This issue got me thinking - other than TCP and UDP, which transport layer protocols are (generally) allowed to be routed properly across the Internet, over IPv4? Is the "normal" policy to allow all forms of IPv4 traffic, regardless of the protocol? 

I'm getting really slow queries in MS SQL Server 2008 R2 on my dev machine. This problem has been plaguing me for about a month. Other developers don't have the same problem, but we all run the same code. It seems to be that any query that includes a takes >20s, some taking up to a minute. Inserts and updates are fast. The total database size is about 30MB, so it's hardly huge. During the laggy queries, the CPU usage stays flat, the IO rates stay low, and the pagefault delta stays low too. I've not tweaked any performance settings in the db config - it's all stock from the setup. The software that connects to the SQL server is running on the same machine as it. I've tried multiple dev database copies, and customer databases that are known to be fine, all to no avail. Any ideas what might be causing this? 

UPDATE 2: Also worth noting that while my localhost has trouble with these outbound connections, other clients (including on one, and one off, my LAN) have no issues whatsoever to any of the same remote hosts, or any other hosts. Again, leading me to believe that this is solely some config issue within localhost (but maybe I'm wrong?). 

This is a recent problem, but unfortunately I don't recall making changes to anything relevant. I'm not sure what to check. Maybe it's worth noting that this affects all SSH connections (such as git over ssh and ) and not just the command line SSH tool. I don't have any trouble accessing over any other protocol (e.g. , , BitTorrent, etc). The only active/uncommented line in is: 

Which exits cleanly, and brings the device back into the pool. Perhaps this is due to the load order of different modules during boot? Maybe the activation of dm-crypt devices is done such that ZFS begins importing pools before the LUKS container is properly open? 

Keep SPF, DKIM, DMARC, and ADSP and give up using mailing lists, or Drop this DNS security/reporting layer and have my normal outgoing mail rejected by Google, Yahoo!, Live, etc. 

I've got an IP that isn't on any blacklists, a PTR correctly configured, DKIM signatures validate perfectly, I thought everything was set up correctly. But now I can't contribute to mailing lists. When I send to the list address, sometimes the message goes into a black hole, sometimes I get an email to my address, and in other cases I see entries I believe are related in reports sent to . My theory is that the SPF policy is too restrictive. The mailman (or other) list server is acting as an SMTP relay for my messages, right? So I changed 

Is there some other step I'm not aware of? UPDATE Here are the firewall rules if I run the command "sudo iptables -L -n -v": 

I've solved the problem. Here's what's going on. /etc/apache2/apache2.conf includes a call to any config files that have symlinks in /etc/apache2/sites-enabled. Since there was a symlink in that directory pointing to /etc/apache2/sites-available/000-default.conf, that latter config file was being loaded and it was over-riding the blocks and directives in my vhosts.conf file. Once I deleted that symlink, my vhosts.conf settings were able to take effect. The lesson for me was that any file that has a symlink in sites-enabled will be enabled. 

Should one always use "www" as the prefix to one's official domain name? I will be launching an e-commerce website shortly and my attorney needs to know the official domain name of my site for copyright purposes. Are there any reasons why I should designate it as "www.example.com" as opposed to "example.com"? I've always used "example.com" during development because it was easier to type and I'd have Nginx rewrite all requests for "www.example.com" to "example.com". Should I perhaps say "www.example.com" is the official domain name and reconfigure Nginx to rewrite all requests to "example.com" to "www.example.com"? Thanks! 

Is this the right approach to take to change my worker log permissions and recreate the pid directories before the celery workers are restarted? If this is the right approach, why isn't it working? If it's not, what's the right approach? If I were using an init.d celeryd daemon script instead of Supervisor, there's a CELERY_CREATE_DIRS setting that will automatically create pid and log directories that are owned by the user/group. Is there a way to replicate this setting when using supervisord? 

This is a time/space tradeoff that allows you to reduce the number of computations from 2168 down to 2112 with a space cost of 256 64-bit blocks (512 petabytes). Now, for some bizarre reason, all security tools seem to report 3DES-EDE as 112-bit without actually qualifying why. 3DES-EDE does not have a 112-bit key length, nor does it really even have a 112-bit effective key length unless you specify that your attacker has 512PiB of lightning-fast storage available alongside their massive array of DES-cracking ASICs. The practice of reporting it as 112-bit appears to have started with the "sslscan" tool, and has been copied by various other tools since then, leading to all sorts of confusion and misconceptions (I even saw this incorrectly marked in a security exam!) This isn't to say you shouldn't disable 3DES - it's an old algorithm now and there are problems with it, so it's probably worth moving away from it. It's just worth knowing why. If you want to do so, add and to your list of disabled algorithms. These names are defined in the cryptographic providers documentation, in case you want to disable any others. 

Find a known plaintext and ciphertext block pair for a given key you want to crack. Compute the first encryption step (i.e. one DES encrypt operation) on the plaintext block for all possible 56-bit keys. Store all of the resulting 64-bit blocks in a big lookup table. For each possible remaining 112-bit part of the key, perform the other two operations (decrypt, encrypt) on the ciphertext. If the result of the two operations matches any block in your lookup table, you've found the key. Otherwise try the next 112-bit key. 

Where else can I look? What other debug tools can I use (all I can think to do is run )? I tried running on remotehost while attempting a connection, but couldn't figure out how to filter the packets from the shell running thus infinite-loop spamming myself out of any useful diagnostics. 

Turns out there doesn't seem to be anything wrong with my configuration. What's happening is that my messages are being processed by mailman correctly, and being relayed out to the list. There are a couple of receivers however which (for whatever reason is unique to them) reject the message. Because I have actually correctly configured SPF, I'm seeing the rejection message from those destination SMTP servers, not from the Mailing List relay itself. Some awesome folks in the Arch community helped me chase this one down, as they had access to said ML server. 

I’ve heard that if you have a guest with N virtual cores under ESXi, the hypervisor waits for N total logical processors on the host to become available simultaneously before delegating work from the guest to the hardware. Therefore, the advice is that you should very carefully consider increasing the guest by X cores unless you truly need the processing cycles because you’ll incur an increase in that wait proportionate to X, and you want to ensure your gain from adding vcpus outweighs the cost of this increased delay. In the extreme example, suppose ESXi hosts hA and hB have identical hardware and configurations, and each has a single guest (gA and gB, respectively) and the guests are identical other than the fact that gA has 1 virtual cpu and gB has 2. If you put the same (non-parallelizable) workload on both hosts, gA “should” compete the task “faster.” 

What you're looking for is a combination of penetration testing and code security review. There are a lot of companies and freelance individuals who will do a pentest for you, for a price. Depending on what you're trying to secure (webapp, payment gateway, physical box, hosted VM, entire network, etc) there may be certain regulations which need to be followed. You'll also have to liase with your hosting provider, since they don't like random people sniffing around in their network. A code security review, on the other hand, requires in-depth knowledge of your codebase and security concepts, so most companies hire a security developer (or contract one in) for this kind of role. They'll need to get comfortable with your entire application and network, then do a detailed analysis of any potential security issues within the code or infrastructure. 

Before you go changing your SSL config, it might be worth understanding exactly what the vulnerability is here. When 3DES was introduced, there was a requirement that it was interoperable with legacy single-DES systems. The idea behind 3DES is that you can multiply the security by performing multiple DES operations with different keys. In order to provide compatibility, they used an EDE construction: 3 DES operations in sequence - Encrypt, Decrypt, Encrypt - or EDE for short. It turns out that a DES decrypt operation is basically interchangeable with an encrypt operation in terms of security, so this works quite well. When you use three independent keys for each operation (known as keying option 1) you essentially have a 168-bit key. If you want to go back to old single-DES mode, you use a different keying option (3) which has all three subkeys set to the same value, i.e. k1 = k2 = k3, so that two of the operations cancel out and only a single DES operation actually matters. There's also another keying option which has two of the keys with the same value but one different, producing a 112-bit key, but this isn't really used in reality and (somewhat confusingly) is completely unrelated to why you're seeing 3DES reported as 112-bit. To make things even more confusing, you'll sometimes hear people talk about 64-bit DES or 192-bit 3DES. These are, from a cryptographic perspective, identical to 56-bit DES or 168-bit 3DES. DES specifies a key padding system whereby 8 padding bits can be added to a 56-bit key to produce a 64-bit padded key. This was for use in some old systems and it's not really important, but the 8 bits can be ignored and only 56 bits are actually key material. In 192-bit 3DES the same thing happens, where each 56-bit subkey is padded with 8 padding bits, but again the real cryptographic key is only 168 bits long. Now, what's the 112-bit thing all about? 3DES suffers from a problem called a meet-in-the-middle attack. The approach is as follows: 

EDIT 1: During cryptsetup creation, I used the SCSI identifiers (e.g. ) to initialize the device with LUKS. However, in I'm specifying the devices through the UUID of the underlying physical disk. Is the utility sensitive to how you identify targets? In other words, do I need to re-do my and pass it the disk UUID instead of the SCSI name? 

Looks to me like this is a DKIM signature failure, but I have no idea why. Is the receiving server trying to verify my DKIM signature against the mailing-list-server's key, or vice versa? For some reason, I wouldn't expect this to happen - I remember reading somewhere that in cases like this Relays and such will sometimes remove/munge headers like this to ensure these types of failures don't occur? 

I have recently configured my own mail server (Linux-based postfix + dovecot scenario). This is just for personal use - I have no bulk mail going out, no automatically generated mail outbound from the host, nothing like that. I've gone through the trouble to configure all the additional fun-to-debug email DNS records: 

Is this proc wait delay real? Confirmable by VMWare documentation? If it is real (and not obvious from provided documentation), are there equations to measure projected impact of guest vcpu increases ahead of time? Tooling to measure what kind of real world impact the delay is already having? 

UPDATE 1: Worth noting that I've been able to duplicate this behavior on a second SSH host, as well as an IRC server, thus proving (in my mind) that this is an issue wholly within some config on my local host. 

From what I know, if it's in it is - by definition? - not subject to change (even across reboots). I will replace the definition of the dm-crypt-name in my zpool with the id-name and report back. Same drive, same LUKS container, just a different way of addressing it.