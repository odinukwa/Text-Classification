My advice is to only configure Auto Approval rules for a limited set of Windows Updates, generally anything that is classified a Security Update, and only to a limited set of your servers and workstations. You can then manually approve those updates for a wider set of your servers and workstations after a period of testing. As an aside, be aware that there is a distinction between Classifications vs. Severity. Microsoft has helpfully reused the term 'critical' to refer to both a classification of Windows Update and a Severity Rating so you have Critical Updates that fix a specific problem that addresses a critical, non-security-related bug (Classification) and you have Security Updates that have Severity Rating of Critical. You will notice the same applied to Updates with a Severity of 'Important'. My focus with Windows Update is primarily to ensure that security vulnerabilities are fixed, hence I only really have Auto Approval rules for Updates that are classified as Security Updates irregardless of their severity. If I find a Critical Update that needs to be deployed that is generally a one-off for our organization. I don't bother with any others. Also be aware that Service Packs and Feature Rollups contain Security Updates along with a host of other things. You need to think very, very carefully about how you want to handle these Classifications of updates because of how much other stuff they include. Again, my organization's focus is on security vulnerabilities so we do not approve Service Packs or Update Rollups on any automatic or wide-spread basis unless we have a specific need to do so. I would advise that you only auto approve Security Updates and you are more selective with Critical Updates but it is really what works best in your organization. WSUS Auto Approval Rules 

I seem to recall running across a "formal" criteria or a standardized method for helping to define and talk about a network's size or complexity - A set of generalized standards that people use when talking about networks (e.g., "I'm currently working for a company with a 'Stage II NETWORK'"). In fact, I'm pretty sure it was in a SF user's profile, but my google-fu has failed me here. I'm aware of the different network topologies and the LAN, MAN, and WAN scale distinctions but I'm looking for something more specific. 

If you don't mind using a web broswer darkstat might meet your needs. It's ridiculously easy to setup and use. I just added the following one-liner to my . 

What operating system, web browser, JRE and firmware version are you using? We have dozens of 2600 series switches and we haven't seen any issues with the Java applets embedded in their web interface. For the records we're using R.11_72 firmware on our switches and the web interface works fine in Firefox 10.0.2 with Java 1.6.0_27. I would try upgrading to the most recent firmware available for your switches (probably R.11.72) and make sure your web browser and Java is updated and configured appropriately. Have you tried using different browsers? If all this fails, you should take advantage of ProCurve's free lifetime support and call HP. Lastly, unless you have specifically disabled it you should still be able to manage your switches via the console. 

The Automatic Deployment Rules list their Last Error Code as and the Last Error Description as "Auto Deployment Rule Download failed". Re-running the rules manually reproduces this error. Deleting and recreating the Automatic Deployment Rules also reproduces the same error. Looking at the SMS_RULE_ENGINE log shows the following errors: 

You might be able to condense some of your rules by not specifying which interface you want the re-direction to take place on and allow pf to evaluate it for you. For example: 

It looks like in the past there were some two improperly decommissioned Domain Controllers (DC1 and DC2) that belonged to this domain. I am concluding this based on some remaining records in the domain's DNS zone, mostly NS records and an A records under the subdomain. There are no Active Directory objects for the old domain controllers that I can see, either in in Active Directory Users and Computers or NTDS settings in Active Directory Sites and Services that I can delete as suggested in Clean Up Server Metadata. If I try to use ntdsutil to remove the orphaned domain controller's metadata I get the following error: 

Run a supported operating system. If you have a requirement for Windows 7 instead of something more suitable like Windows Server I would recommend you install VMWare ESXi and then run Windows 7 in a virtual machine. If you want to continue with the bare-metal install of Windows 7, investigating basic Windows Debugging may yield something useful (presuming this is a kernel-mode crash). 

OK... Where did that Maintenance Window come from? A little bit of SQL should show me all the Maintenance Windows applied to a particular SCCM client: 

What? There's a package in the main repository: nginx (1.2.1-2.2) . If you're having issues installing stuff nginx's repositories it's because they are not maintaining them properly or haven't updated them yet. If you go to their dists folder you'll notice they have not yet provided a package tree for wheezy. Use the version of nginx included in the main repository or build your own package from the latest stable release's source until nginx updates their repository. 

The Problem: What we are trying to do When trying to log into vCenter using the VMware vSphere Client we are greeted with the following error when using both Windows Sessions credentials or manually supplied credentials (): 

I'm not familiar with Pingdom, but I think Smokeping will do everything you want. If you specifically want to monitor an HTTP/HTTPS server and not just general connectivity (by way of ICMP), check out the EchoPingHttp and EchoPingHttps probes. There are also probes that will monitor DNS and SSH services. We use smokeping for general latency and "network health" monitoring all over our network with good results. I just used it last week to help diagnose a duplex mismatch at a remote site. Setup is pretty minimal, especially if you are familiar with Linux. 

Remember, it's not your network. It's their network. You are here because they are here. I think if you take the long view, you can educate the segment of your users that fall into the "don't know" category and not the "don't care" category. Their knowledge will gradually increase (and your headaches will decrease). They can even become pretty valuable assets. 

Nope! SCCM is complex and somewhat fidgety application. In order to install it you will need a whole bunch of prerequisites, not limited too IIS, Reporting Services, MS SQL, and WSUS. For such as small Site you would co-mingled these services and Site Rolls on a single server, your Domain Controller, which also happens to run a complex and somewhat fidgety application. I highly recommend you do not do this. Take a look at can domain controllers also serve other functions?. It used to be fairly common to deploy a single physical server that had ADDS, DNS, DHCP, File and Print Roles all co-mingled. However, with the prevalence and low cost of virtualization in the Microsoft ecosystem it is becoming more common to deploy your domain controllers in single-purpose virtual machines to avoid problems and isolate them if they occur. As an aside, note I said "domain controllers". You will want at least two Domain Controllers, one of which is a physical standalone machine if you plan on clustering your Hyper-V hosts. You should always have two domain controllers (see: Risks of having only one domain controller?). Furthermore you should pay particular attention to the caveats of running virtualized domain controllers, especially things like cloning and time synchronization. 

I hate suggesting this because I'm generally on board with Default Deny policies but I eventually gave up trying to make the Exception List work for all of different web applications and browser combinations and just set Java's Security Level to Medium. 

Those are standard non-privileged users/groups created by packages so their programs can run without root privileges. If this is the only reason you suspect that your system has a security problem, then it is likely that you don't. Additionally, it is generally a Bad Idea (TM) to just start disabling things unless you really know what you are doing. The defaults are default for a reason. 

I'm trying to submit my certificate using the following certreq.exe command: I receive the following error upon doing so: 

SuperMicro's Super Doctor III is the software I was looking for. It is SuperMicro's equivalent of IBM's ServerGuide or Dell's OpenManage. Unfortunately, while it supports hardware monitoring and SNMP or Email alerts it does not support running on Server Core. 

This is goofy. Both of these buildings now house employees in the same company (they merged right?). They will both be better served by pooling their resources, buying a single WAN connection and combining their networks. Having separate infrastructures makes absolutely no sense to me and is terribly inefficient from a number of standpoints. 

You didn't specify whether or not you are working in Windows networking environment or not so I will focus on general networking security suggestions. 

You do not want to use the normal System State Restore procedures when dealing with Active Directory Domain Controllers nor can I find any reference to copying over all those System Files in the System Restore documentation; you might want to rethink that step for your other servers. For Active Directory Domain Controllers you need to decide what kind of restore you are going to do (authoritative or non-authoritative), take special care to avoid any race conditions that could cause ugliness like a USN rollback and figure out how you are going to handle the FSMO roles if required. If your infrastructure has at least one healthy Domain Controller the recommended restore procedure is to do a fresh install on a new machine, promote it and wait for Active Directory replication from your healthy Domain Controller to do the rest. No restore from System State backup or worse a VM snapshot/checkpoint is necessary. If you no longer have any working Domain Controllers or you accidentally nuked a significant portion of your Active Directory things get substantially more complicated. Please read the relevant documentation and I recommend you engage with a Microsoft support engineer to help unless you are very familiar with the process. Also once you have finished your restore, you should investigate why your Active Directory design is not robust enough to sustain a failure of a Domain Controller or two or why you haven't started using Active Directory Recycle Bin respectively. Please see: Restore Process with a Working Domain Controller Restore Process Without a Working Domain Controller 

If you read Mark Russinovichâ€™s Blog post The Machine SID Duplication Myth (and Why Sysprep Matters) you can get an idea of Microsoft's justification for depreciating NewSID. It basically boils down to a combination of a lack of confidence in the idea that duplicate SIDs actually cause the problems that people think they do and lack of confidence that a tool like NewSID actually changes all of the locations where the SID of the computer is hidden. That being said, Michael Murgolo's Sysprep, Machine SIDs and Other Myths tentatively agrees with Mark's post but adds a dash of caution, saying there are a number of problems that turned out to be caused by duplicate SIDs (or more appropriately by not SysPrep-ing your servers as part of your cloning or deployment process). TL;DR: Always SysPrep your servers (or workstations) as part of your deployment process. 

This doesn't address your specific question but it does address the larger underlying problem. Debian Etch has been at End-of-Life status since February 2010. That means no security updates. If this is a publically facing web server, you will be quite lucky if it hasn't already been successfully hacked. You should upgrade to the current version of Debian Stable (version 6.0). If you do not have the skills to do this yourself nor the time to acquire them, you should hire someone who can. Nothing personal, this is just how it is. My recommendation is to perform your "upgrade" from Apache to Apache2 by building a new server using Debian 6.0, installing Apache2 and then migrating your website from your old server to the new one. What you suggest will be painful and ultimately unproductive. 

Your firewall is misconfigured. Don't worry, it happens to the best of us. Good on you for testing! One do your network adapters is using a different network profile than your desired firewall profile. Network Location Awareness can sometimes play into this as well. NMap is erroneously detecting open ports. I have seen this happen rarely when testing from behind certain draconian firewall/proxy/IDS implementations. 

If you can successfully connect then you can confirm that you can reach the FTP server (i.e., there are no firewalls blocking your connection) and the FTP server is running. If this is the case then your problem lies somewhere in the Application Layer. As others have pointed the problem is more likely that there is either no FTP server listening and/or a firewall is blocking the connection. 

If what you want to test is your detection logic, I'm not sure exactly how we could accomplish this. The Model property is Read-Only so I cannot think of a way to modify it using PowerShell script within your Task Sequence prior to the conditional install of device drivers. 

Your service is bound only on and not to the loopback addresses (, ). You need to reconfigure IIS to listen on the loopback address of your choice, either IPv4 or IPv6, or listen on all available addresses () as appropriate. 

I like Bruce Schneier's take on it: "Cryptography is all about safety margins". The larger the key size, the larger the safety margin (and the larger the performance hit). A 512-bit key is probably OK, but probably not for very long. As Shane Madden mentioned, there's ample evidence that they can be broken by a concerted attack in under a few months. If that is enough of a safety margin for you, then you don't immediately need to replace them. 1024-bit keys are probably next to go. So if you already have Digital Certificates you have purchased that use these key sizes and you are NOT protecting vital company/nuclear secrets you are probably OK for the short term, but you will want to purchase SSL certificates with a larger key size in the future. Is there a big enough safety margin for you? Is it worth the performance hit? Who would be attacking your cryptosystem? A little threat analysis is good for A) figuring out whether you need to do something RIGHT AWAY and B) covering your ass in case something bad happens. How large a key should be used in the RSA cryptosystem?