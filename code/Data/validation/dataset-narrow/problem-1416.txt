You could just as easily be telling your graphics API to enqueue sprites or colored triangles/quads at that point, though, instead of printing characters. The reason I suggest this approach is that, as you can see, the board itself ends up only knowing about the board's logical state... the size of the board and the value of every slot in the board. It also provides a relatively minimal (but functional) interface to manipulating the board. It has a single responsibility -- to represent the board. There's nothing in there about rules for what colors must go where (although a real-world class should perform some bounds checking that I have eschewed here for brevity). And there's nothing in there for rendering the board -- that is accomplished entirely by an external, non-member function and can be done entirely via the public interface of the board object. This keeps the class lightweight and reusable and helps to increase it's maintainability as well -- it's a good habit to get in to trying to approach designs like this (and also to, in general, keep logical interfaces and rendering interfaces distinct). 

(You'll have to make sure to follow the steps for authentication detailed in the documentation to get the access token you should pass instead of ). You should be able to use approximately as follows (untested, sortof-pseudocode): 

I am not a lawyer and this is not legal advice; you should consult a lawyer for proper legal advice. That said, the idea itself for the game is unlikely to be protected in any capacity as it is very difficult to do so. What you want to avoid is copying the name, artistic assets, and probably the puzzles, of any other instance of this game that is out there. 

Accurately measuring the performance of graphics API (D3D or OpenGL) calls isn't always a matter of timing the CPU-side code, because some of those CPU-side calls you're making could dispatch work to the GPU and block (or not) and the timing you get doesn't really account for that in a useful fashion. This guide discusses some techniques for profiling D3D which are applicable (since you're just using a wrapper). You'll need to adapt them if you aren't using D3D9 but it's not that difficult as most of the discussion is conceptual: why profiling is hard, how correctly profiler the CPU aspect, and how to control driver optimizations and use the query APIs to get information about GPU-side performance. 

Putting game content data in code means that to see any potential change or iteration of that game content data, you have to recompile the game itself. This is bad for two reasons: 

You still need to know the appropriate format at compile time when you call this method, but that's usually not a problem because you are going to be directly manipulating the individual properties of the vertices when you call this (usually) so you'll know anyhow. But if that's still a problem you can even manipulate the raw bytes directly according to the vertex format at runtime. Assuming you have a series of memory IO routines that allow you to write arbitrary primitive types (ints, floats, et cetera) into arbitrary byte buffers, and you have something that can query a vertex format object for information about whether or not that format contains normals or colors or whatever, you could write to an arbitrary format buffer like this: 

A given shader model exposes a particular set of registers to HLSL; these registers are underlying hardware registers on the GPU, like CPU registers, but have more refined scopes (for example, there are registers dedicated to holding samplers). Registers are where all your data is stored during the execution of your shader (with the exception of data, like texture or vertex buffer data, that is held in the GPU's RAM). On a GPU, you don't have the same flexibility in reading and manipulating memory as you do on a CPU. There's no "store this value to RAM for a moment," so everything has to be in registers. This is particular obvious in old shader assembly (from the shader model 1 era). Data transfer can occur via these registers as well, which is why the keywords still exist in more modern shader models. If you're not using the effects framework, for example, you can manage the allocation of your sampler or constant registers yourself (for example via methods like SetVertexShaderConstant). Similarly, data from buffers can be mapped to registers; the vertex input registers typically get your vertex buffer data mapped to them prior to execution of your shader. You can find a list of registers and their purpose on the MSDN. Here, for example, is the set for VS 5.0. Often, you don't need to use them directly, much like CPU registers. They will be used automatically by the higher-level language constructs that you use in your shader code or in the API (such as the aforementioned effects framework). 

PIX is a fairly low-level data collector/analyzer for D3D. PIX can examine the state of device objects -- including vertex and index buffers -- from within an experiment. The PIX tutorials page has a document on examining mesh data. The tutorials tend to build on each other so it's probably a good idea to at least skim them from the first one. It's not a bad idea to write code to compute this information for you though, and allow you to enable/disable it at will. This isn't really "polluting" you code in any way -- you can always configure this code to be compiled-out of release builds, and by having it in your program you can set it up so you can turn it on and off at run-time as well -- this way you may be able to enable it when you encounter a difficult-to-reproduce bug that warrants looking at this diagnostics data. If you rely only on external tools like PIX, you'd have to be able to ensure you can reproduce whatever bug or issue you're looking at before the tool will be useful. Since you're using XNA, and not D3D directly, you may also find it a bit harder to correlate the D3D commands XNA sends with the XNA operations you're doing, since XNA is a higher-level API. To compute your primitive count in code, you'll want to iterate all the ModelMesh objects inside your Model. Each ModelMesh has a collection of ModelMeshParts, which each have a PrimitiveCount. The sum of all PrimitiveCounts is the primitive count of the model. 

Fundamentally, the input causes the action (the action in this case is regenerating the buffer). You'll never be able to decouple those unless you remove the functionality. You can place layers of abstraction in between them, sure, and in some cases that's desirable. Maybe not so much in this simple program; this key handling code is basically part of the "top level" game layer of abstraction, which is going to know about every other system in the game somehow already, or that system would never be instantiated/ticked/whatever. You've already (apparently) got a start on the idea of keeping logical data (grid) and render data (the grid's buffer) distinct. This is a good direction, but I'd suggest that you focus on is continuing that separation rather than worrying so much that your input handling code is triggering a graphics rebuild. For example, operates on the currently bound buffer, and I don't see where you set that here. That means either you're setting it in or you're setting it at the start of the program and expecting it to remain set. That is bad, as it will cause you no end of pain when you start adding more than one buffer to your program. Spending time getting OpenGL buffer management correct early can save you a lot of trouble in the long run. Consider building an abstraction around the renderable grid that makes it just as easy to update the render buffer data, but does it without making assumptions about which buffer is currently bound, et cetera. 

Most of the concept art you're likely familiar with from big names in the industry was produced for a game, while the artist was in the employ of some company. That often means the ownership of the copyright falls to the company itself. It's similar to how all the code I've written while working for my current employer is not mine to keep if I were to leave the studio. As a result, it may be company policy not to sign art produced for a game. Possibly for reasons Steve H pointed out in his comment (they want to continue to make use of the art, even after you leave or even after your name becomes sullied in some fashion that they'd want to distance themselves from... although the former is more likely than the latter I'd hope). Possibly simply because they want to keep it "unadulterated." I just checked, and the large of some of Daniel Dociu's work I have in my room isn't signed, for example. That said, I'd sign everything you create on your own time for your own projects, if I were you. It's only after you get a job in the industry (and if that job requires you to not sign anything you do as part of that job) that this will affect you. 

However, the line is problematic in the context of the rest of your program. produces a value between 0 and 2, which you then add one to. That means can be from 1 to 3. It will never be zero and it will never be four. But arrays in C++ use zero-based indexing. That means the first element in the array is at index 0 -- in order words, is the "Imp" string. Similarly you don't want to try to access because you only have four elements in that array, so the only valid indices are 0 through 3. Thus the ideal solution to your problem is something like this: 

If you are using D3D9, you can query for device capabilities reported by your card using the IDirect3D9::GetDeviceCaps method. This will give you a structure containing a lot of interesting information about what the hardware supports. Of interest to you concerning this problem will the fields in the resulting structure called and as well as possibly and . All four are described on the linked documentation page. For D3D10 you should be guaranteed SM4. For D3D11 (which I'd recommend over 10, since it should be the case that you can use 11 if you can use 10) device capabilities are categorized into feature levels. If you're using the 10_0 feature level or greater, you should be guaranteed SM4. Below the 10_0 level you have some odd 10Level9 differences to take into account -- the upshot for you is that you have to use odd shader model designations like in some (perhaps all, we're getting into territory I haven't explored much in practice) scenarios. You'll note that I said "should be guaranteed" in a few places. This is because, as you alluded to, it's possible for cards to lie or for there to be driver bugs that effectively render particular hardware/driver combinations "non functional" (or at least broken in a fashion you'd want to work around). This is much rarer these days than it used to be, but in these cases you can't really trust the hardware or driver anyhow and will have to "do it yourself." One way to do this is to simply try to create something using SM3 and see if it fails... although this will not catch all bugs/failures. What I have done to account for that kind of issue in the past is build up a locally-maintained "feature database" API that allows me to store information about particular card/driver failures and how to fall back to safe alternative code paths when that hardware/driver combination is present on the end-user's machine. Populating this database generally requires trial-and-error and a lot of different hardware configurations, so can be difficult for the lone developer to do, unfortunately.