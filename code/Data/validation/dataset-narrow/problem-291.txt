If I were you, I would use a view to transform the date with the DATE_FORMAT function: $URL$ $URL$ To avoid refactoring your SELECTs, rename the table out of the way, and use the original name for the view. As long as you keep the view simple like that, it will perform about as well as the physical table (negligible overhead) on anything except for anything that might enjoy an index on that date field. Because the DATE_FORMAT function make the view non-writeable, you still have to refactor your write statements to use DATE_FORMAT to go the other way. 

Without knowing the query set that's going to hit this structure, I'll give a generalized answer: I try to use the TIMESTAMP type for this, which works well with time/date functions, and is timezone agnostic (under the hood it is storing a unix timestamp). By default, this will only be a 4 byte wide key (instead of an 8 byte bigint), so if you index other columns (which always tacks on the primary key at the end...albeit hidden), those structures can stay skinny and quick as well...and also not eat your RAM ;-). TIMESTAMP by default has a resolution of 1 second, which is overkill for your purposes, but the easy use of date functions on it makes that worth it. Main downside: TIMESTAMP is based on a 32-bit unix timestamp integer (it stores that way, but displays as a date/time), so it does eventually run out. At that time (decades from now, when RAM is cheap) you can adjust it, and the date/time display/handling nature of the type will allow you to make the change transparently to your software. If you want to use it as an integer, you can use FROM_UNIXTIME(your_integer_here) to insert, and UNIX_TIMESTAMP() in queries that need that. I'm going to guess though that the date/time format is easier to work with for you though. Also, you do not require a second UNIQUE key, as any PRIMARY key implies this already, and is included for free in the storage/RAM price of your data table (which is sorted by PK as a B-Tree...your table is an index, and can/will be used as such). 

I think any of the major databases can handle the load if designed well. Sadly I would estimate that less than 1% of all databases are designed well. (I have personally dealt with data from literally thousands of different databases performing a wide variety of functions, so I think I have a good idea of the lack of quality that is out there inteh real world.) I would strongly suggest that you get some books on performance tuning for the database you choose and read them thorughly before begining to design. There are many things that will help your database perform better that should be designed in from the start. Just knowing how to write performant queries and design indexes is critical to getting a good design. This type of study and designed-in performance is not premature optimization. There is no reason at all to use known performance killing techniques in the design. Databases need to be designed for performance from the start. 

Likely you have some data that doesn't fit the datetime column somehwere in the file. Insert to a staging table with a varchar or navachar filed for the column and look at the data. You may need to adjust some of it (or null out bad records) before loading to the real table. 

THe following book talks about using ID as a SQL antipattern and I gree with the author that it is. $URL$ This is a particular problem when you are doing complex reporting and need more than one id as you then have to alias. Using tablename ID also makes it easier to identify the correct FK to join to (as they have the same name) and makes errors from joining to the wrong thing less likely. That said, many databases don't support the USING syntax which makes the problem you brought up not an issue for these databases.Nor do many databases support using a natural join which I would not recommend using in any even whatsoever as they join could change if the table structures change. So suppose you add a field called modifieddate to both tables, you would not want to join on that but the natural join would do so. 

If you want to store it as a hierarchical structure, the pattern that would best serve you is the "Closure Table" pattern, which allows for subtree operations (select/update/delete all descendants of x). A generic example (closure table and data table ) of what flagging an entry and all children at all levels below it would look like this: 

I can think of a perfect case for it, and we have tested thoroughly and run it in production...I call it the "fast lane" clustering strategy: If you do read-write splitting with a proxy like MaxScale, or your application is capable, you can send some of the reads for those seldom invalidated tables only to slaves that have the query cache turned on, and the rest to other slaves with it turned off. We do this and handle 4M calls per minute to the cluster during our load tests (not benchmark...the real deal) as a result. The app does wait on master_pos_wait() for some things, so it is throttled by the replication thread, and although we have seen it with a status of waiting on Qcache invalidation at very high throughput, those throughput levels are higher than the cluster is even capable of without Qcache. This works because there's rarely anything relevant in the tiny query cache on those machines to invalidate (those queries are only relevant to infrequently updated tables). These boxes are our "fast lane". For the rest of the queries that the application does, they don't have to contend with Qcache since they go to boxes without it turned on. 

Is it possible to get the file as a .csv file which can be imported like a text file and will not have this problem? I generally kick back all Excel files and ask for another format because SSIS and Excel do not play well together on many levels. 

You don't want simpler, you probably need more complex. Invocing is a complex field that requires a good bit of accounting knowledge (knowledge of internal controls to prevent fraud in particular and how those would be implemented in your database). But lets start with a simple example, customers do not have only 1 address (or ship to only one address), so you should have a separate address table. You should have a separate phone table. You should have lookup tables for phone types and address types. You probably need product lookup tables. I can't say for sure in mysql, but float is generally an inexact datatype and should not be used for any number you intend to do calclutions with. Not unless you like dealing with rounding errors. Or losing money. Accountants tend to hate that kind of thing. Shipping address should be linked to the invoice not the customer. I may ship one order to a customer in Maine one day and another to a differnt person in Washington the next. I may ship to any one of my three offices or to my home or to my parent's home. How are you planning to maintain those updated by and updated date fields? They shoudl be in a trigger or they are useless. Normally products have some sort of product details that can vary such as color or size or no of items in the package. You have a description field for this but 80 chazractesr seems way too small to me. Truly you need to learn normalization before you even attempt to design a database, let alone one with serious legal implications. Taxes can be quite difficult depending on how many states or countries you are selling to and the typesof products you sell. Without knowing more detatils it is imposooible to design a good set of tables for taxes. There is no way under the sun that I would consider deigning such a thing without consultations with a good experienced accountant. Preferably one with auditing experience. There are many many professionally designed accounting packages that do invoicing, it would be cheaper and far better from a legal standpoint to buy one. 

In some versions of Oracle, you cannot put space between the beginning-of-line and the WALLET_LOCATION keyword, and you must put space between the beginning-of-line and the definition of a wallet. Your snippet indicates that you failed at one of these things. I think they removed this silly requirement starting from some Oracle version, but better safe than sorry. Good: 

The "device" in RMAN is a misnomer, it should be really called "storage". The "sbt" (synonym of "sbt_tape") is a misnomer again, as it has NOTHING to do with any tapes, it should be simply called "non-rman". This is just an empty placeholder, to be filled with any "plugin"; the plugin is called by Oracle either the "Media Manager library" or SBT_LIBRARY. This plugin allows rman to store and retrieve files through it, so rman only tells that it needs a file handle "xyz" (file is identified by a string handle) and doesn't need to know how the file is delivered, from tape or anything. The plugin is normally a part of an independent backup software, such as IBM TSM or Symantec NetBackup or many others. Oracle provides a simple emulator SBT_LIBRARY=oracle.disksbt for testing. Since you didn't fill that placeholder with any "Media Manager library", you receive an error message as expected. 

The next thing in terms of cost is an OS (machine) and its IP address. You cannot afford a separate system for every TNS name. So crmdb.mydomain.local is not the only name for the IP address; the same IP address would have more names, like financedb.mydomain.local. Your OS admin would decide how to do this best, and how to determine the main hostname of the OS. They have the same problem with many other systems - multiple names referring to one OS - so they should have a solution at hand. The only people who are confused now are DBAs and OS admins, they see multiple hostnames leading to the same IP address. But users don't care about that and are not confused by that. (By the way this approach is coherent with SCAN. ) The next thing in terms of cost is either one of the two: an Oracle instance or "administrative cost of separating schemas out of instance". The tradeoff is for you to decide. 

First - you must be able to backup and restore a database. You must know how to set up a recurring schedule to backup both the database and the transaction logs. You should know what other maintenance is required periodically such as updating statistics. You should understanding indexing - how to create them and when to create them and when not to create them. You should understand how to read query plans - execution plans or explain plans depending on the db backend. You should understand datatypes and why using the correct one is important. You should also understand why every table needs a PK and how to set up PK/FK relationships and you should never allow application developers to think this stuff should be handled by the application and not the db. You should be familiar with database normalization. You shoud read about performance tuning and database internals for your particular database and be proficient in advanced SQL. You should know how to monitor performance of your database. You should know how to set up new users and use roles based security. You should be able to install the database on a new server. I'm sure there's more, but this is a starting point. 

I like the idea of using a generic linked server name. However, in many environments this may not be possible, In this case, you can use dynamic SQl in your sp. 

The data will likely long outlive the application code. If the rule is critical to the data being useful over time (like foreign key constraints that help keep the integrity of the data), it must be in the database. Otherwise you risk losing the constraint in a new application that hits the database. Not only do multiple applications hit databases (Including some that might not realize there is an important data rule) but some of them such as data imports or reporting applications may not be able to use the data layer set up in the main data entry application. An frankly, the chances of there being a bug in the constraint are much higher in application code in my experience. In my personal opinion (based on over 30 years of dealing with data and experience with hundreds of different databases used for many different purposes) anyone who doesn't put the contraints in the database where they belong will eventually have poor data. Sometimes bad data to the point of being unusable. This is especially true where you have financial/regulatory data that needs to meet certain criteria for auditing. 

It is part of the CREATE TABLE syntax. The manual explicitly recommends this as a supported alternative to symbolic links. Also, there may be a list of bad reasons to do this, and even a list of reasons that look good but bear gotchas, but there's also a perfectly legitimate list of good reasons for suitable cases. There are cases where this can and should be leveraged for performance when your storage hardware is tiered, and faster devices are limited in capacity. 

Otherwise, you will always only need 3 statements to select/update/delete all rows relevant to an entry in : 

...exactly as we expect. There is, however, now a view by the same name in its place (which refers to the new table in the other schema), and it has a corresponding .frm file, and of course, no .ibd file, as we would expect. Further, the table in the new schema works just as expected, as does the view in the old schema that aliases it. In fact, as far as we can tell, this is causing no problems...we just want to stop worrying whoever sees the log. We can even create and drop a table in its place (after moving the view and before putting it back), and still get that error. So why does MySQL expect an ibd file by that name in the old schema? How do we convince it to forget? 

Here are some well written resources on how to implement that: $URL$ $URL$ $URL$ The best place to read about it is Bill Karwin's book "SQL Anti-patterns", which details it as an alternative to less savory patterns. 

When you are building complex queries, you should build them in stages checking the results as you go, not build one whole huge query and then try to figure out what is wrong. Here is what I do. First I list all the columns I want on a spearate line and comment out all the columns except those in the first table. Then I add the from clause and any where conditions on the first table. I note how many records are returned. Then I add each table one at a time and check the results again. I am especially concerned when the number jumps up really high unexpectedly or goes down unexpectedly. In the case of the number jumping up high, you may have a one to many relationship that needs further definition. This is especially true if the field or fields you are getting from the table are almost always the same. You may need a derived table or a specific where condition to resolve. You might even want to do some aggregation. Now I'm not saying it always bad if the record counts go up, only if they go up when you didn't expect them to or when the result set looks suspicious. In the second case, you generally have an inner join where you need a left join. The number of records went down because you did not have a matching record in the joined table. I often check each inner join with a left join to ensure that I return the same number of records. If it does then the inner join is appropriate, if it doesn't then I need to decide if it is filtering records I should be filtering or if I need a left join. When I am not sure why the record counts are off from what I expect, I use a select * (temporarily) just to see all the data, so I can determine why the data is off. I can almost always tell what the problem is when I see all the data. Do not ever use select * in a multiple join query like this or you will be returning much more data than you need and thus slowing the query as at a minumum the join fields are repeated (plus it is truly unlikely you need every field from 20 joins!). To find you issue, you are going to have to repeat this process. Start with just the from clause and the where condtions on it and add tables until you find the one (s) which are causing the query to eliminate all records. Don't stop with the first find, it is possible in a long query like this that you have multiple problems.