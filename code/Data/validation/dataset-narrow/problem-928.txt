I have a similar working set up - I'm not using DKIM or DMARC, just SPF for spam fighting. Here's mine - I set mine up using this as a guide and while I follow it 90% of the way there are a few things I do differently - $URL$ Here's what is in my various files for hostnames. Like you, I'm hosting multiple domains and they all work fine. Reverse DNS is set to I get a letsencrypt cert via the standalone method, and specify example.com, mail.example.com, and www.example.com as hosts on it. - ( w/ appropriate mapping to 127.0.0.1 in ) - In I have the following references to hostnames or FQDNs - 

What is more important than a FQDN is the server has a sense of self-identity and it can confirm it. So should have a simple hostname in there (I have multiple linode machines, so I use node1, node2, etc). That same name should have a pointer to 127.0.0.1 or 127.0.1.1 in You'll need a real DNS-level FQDN if you want to use DNS to access your box, get SSL certs, etc. Good thing is linode GIVES you a FQDN - in the form of liNN-NNN.members.linode.com where the Ns are numbers (and may be more of them). You can see it in your control panel under "remote access" or contact the support folk. If you start configuring services - mail, http/https, etc - then each service will have a config file (or files) and you can specify in that what hostname/FQDN to respond to. As long as DNS is pointing that hostname/FQDN to your IP, stuff will work. Depending on what you do with your machine - for example, if you set up a mail server it MIGHT pull your hostname, etc. in to create its own config on initial install. 

We're currently using Mailman as a mailing list manager. Mailman modifies the content of mail messages. The problem is that some of our users are sending digitally signed messages and the modification makes the signature break. I've seen this behavior with Apple Mail, Outlook, and Thunderbird. The problem seems to be this: S/MIME signed messages are implemented with a MIME Content-Type. Mailman wraps this inside a MIME Content-Type. None of the mail readers look inside the outer for the inner . We won't be able to get the clients fixed. Is there anyway to modify Mailman so that it doesn't have this behavior? 

Amazon's marketing materials claim that the m4.16xlarge node has 64 vCPU. When I look at on the system, however, I get the following information: 

I need to use a port that is already defined becasue I am upstream from a middlebox that only allows connections on ports allocated to specified services, and port 22 is blocked. 

I am trying to get a modern GCC to compile on Centos 6.4. The problem is that Centos does not have a modern glibc and GCC 4.8.x and 4.7.x keep giving me the following compile error: 

If it reports loading OK (with maybe a "out of zone info ignored" for your glue record servers) then edit again, increase your serial, and restart bind. 

Running the should return a error. However, some ISPs will return the IP of an ad server, a search server, etc. instead of a proper NXDOMAIN. What service provider are you using? What DNS servers are you using? What happens if you use a known-NX-issuing server for DNS requests? I'll bet that if you try (ie, ask Google's public server about $URL$ you'll get a proper NXDOMAIN return. 

the option sets a umask for the internal-sftp program/subsystem and any files uploaded through it IF the user is a member of the group. Personally I also the users so that they can only access their directories - check the option as it applies to a directive in the file. 

is set to only accept incoming SMTP connections from the 3 other servers (think firewall and connecting to port 25). The server is set to only accept mail from the 2 virusco.com servers. the two virusco.com servers are configured to accept mail from the world as MX servers for example.com My server - foo.org - can't get to mail.example or backupmx.example (by configuration design) so it delivers to mail.virusco, which is configured to forward the mail to mail.example via normal SMTP MX decision. This means that if mail.example is down, AND backupmx.example is down, mail.virusco simply holds the mail and tries again "later" If mail.virusco is down for a bit, mail goes to backupmx.virusco, gets scanned, and then backupmx.virusco follows the MX decision tree and tries delivery to mail.example, backupmx.example, mail.virusco in that order, and if none are successful it too will sit on the mail for "a while" and then retry delivery. You can even take backupmx.example out of the system entirely and it will still work in that fashion. A lower priority (higher value) MX server will try to deliver to the highest priority (lowest number) server, until it reaches itself, at which point it will sit on it and try again later. 

I have a Centos VM running with SELinux enabled. I wish to have sshd listen to another port --- says, 993. I've modified the sshd_config file to listen to another port, but SELinux is getting in the way. I don't want to disable SELinux. How do I tell SELinux that it's okay for sshd to be reading TCP connections on port 993? The correct command to use is but I cannot use that command because port 993 is already in use in another policy: 

RHEL 7 includes in the official repository. However, this module does not install mod_php. I have tried all of the approaches for installing rh-php70-* and none of them installs and registers the appropriate php module. How does one do it? 

We are using Lustre in a cluster with approximately 200TB of storage, 12 Object Storage Targets (that connect to a DDN storage system using QDR Infiniband), and roughly 160 quad and 8-core compute notes. Most of the users of this system have no problems at all, but my tasks are I/O intensive. When I run an array job that has 250-500 processes that are simultaneously pounding the file system typically between 10 and 20 of my processes will fail. The log files indicate that the load on the OSTs are going over 2 and that the Lustre client is returning either bad data or failed function calls. Currently the only way we have of resolving my problem is to run fewer simultaneous jobs. This is unsatisfactory, because there is no way to know in advance if my workload will be CPU-heavy or I/O heavy. Besides, just turning down the load isn't the way to run a supercomptuer: we would like it to run slower when running under load, not produce incorrect answers. I'd like to know how to configure Lustre so that clients block when the load on the OSTs goes too high, rather than having the clients get bad data. How do I configure Lustre to make the clients block? 

And in or or where ever you are storing your "domains to service" info, specify that for that domain your nameservers will be slaves, and his server will be the master, replacing the ip wtih the ip address of subdomain.example.com (your friends address). 

The "milter service" is a pass-off from Postfix to run messages through an anti-spam (spamassassin) or anti-virus (amavis, etc) scanner before sending out and/or delivering to Dovecot/etc. to be sent ot a users in-box. That "can't connect" message means that your milter service is either not running or it isn't running on the port that you've specified. Check in your to see exactly what the milter service is supposed to be providing, and then look at the configuration for that thing. Or just post your file (properly formatted please) 

RAID - no matter what level - is not a backup. It is hardware redundancy and is meant to mitigate any loss of either data OR run time/uptime due to sudden catastrophic hardware failure. RAID will not protect against accidentally deleting the wrong file, redirection of the wrong output in the wrong way to the wrong file, uninstalling the wrong package, overwriting the contents of one file with another, the effect of a virus or trojan (file removal, corruption, or ransomware encryption), etc. That is what backups are for. Now, that said, I'll admit to not using backups, or if I do it is a one-off backup of "gee, its been a year since I've saved my mozilla bookmarks, better export that out and upload it to my VPS", or "I just finished this final project for a programming class, I'll tar it up and put a few copies on other machines". BUT I also run RAID-1 for my /home partition on both my work and home desktop machines. Why? Simple - I've lost more data over the years to "just lost power and my hard drive is now dead" than I've lost data to "where did I leave that floppy" or "I got a virus" or "I deleted the wrong file" 

The problem here is the is part of the modern and Centos 6.4 doesn't seem to have it. I've tried building my own but as soon as it gets installed and in my local I can't run any other programs, because all of the existing executables on the system try to link against it and they fail. I want to use the new compiler because it has dramatically better handling of C++ STL code, and because the optimizer in GCC 4.8 makes my code run in 1/2 the time as the GCC 4.4.7 compiler that comes with Centos. Any suggestions on how to do this? 

It appears that Hive, Impala, Pig, and others all provide SQL or SQL-like access to data stored on Hadoop clusters. They all seem to have support for HDFS, S3, and other forms. So why are there so many different ways for accessing Hadoop information by SQL, how are they different, and how does their performance compare? Do we have so many different versions because all of the projects were started at the same time for more or less the same reason? If so, is there an advantage to knowing more than one of them? I have found several articles that attempt to explain the differences (e.g. 10 ways to query hadoop with SQL and Selecting the right SQL on Hadoop, but mostly they just list features. 

I don't collect form info, etc. on my sites, but I do want everything to run HTTPS so I set up vhost configs to redirect non-HTTPS requests to the HTTPS side, with a non-named catch-all that redirects to example1.com - 

If your mail servers are actually provided by a 3rd party, the correct thing to do is for them to provide SSL certs of their own on their own machines with their host names, and for you to set your MX records in your DNS zone to point to their server(s). IE, 

And one using serverfault.com - you can see the owner of the netblock is "Fastly Network Operations" 

First, run against the domain to find the IP address it points to. Then run against the IP address - this will tell you who IANA assigned the netblock the IP is in to. From there it is determining if it is a network provider, ISP, colocation company, etc. Here's an example using google.com - 

What are the network protocols that can be used to measure the system clock (time) of a remote server? So far I have: 

An Amazon Machine Image contains an EBS volume. Is there a way to get the EBS volume out of the AMI without booting the AMI? 

We would like to have a network backup system with a user that can read any file on our servers but not write any file. Is there any way to do this under Linux (and specifically Fedora)? We would rather not have a remote that can erase any file... 

So this makes no sense to me. This means that I have 64 CPUs with 16 cores each, or a total of 1024 cores. However, online documentation for the Intel E5-2686 v6 claims that it has 36 cores and hyperthreading, for 72 virtual cores. What's going on? How many cores are there? 

RAID5 and RAID6 can detect and usually correct bit corruption if you verify parity of the entire drive. This is called "scrubbing" or "parity checking" and typically takes 24-48 hours on most production RAID system. During that time performance may be significantly degraded. (Some systems allow the operator to prioritize scrubbing over read/write access or below it.) RAID6 has a higher chance of correcting it, because it can correct it if you have two drive failures, whereas RAID5 can only handle 1 drive failure, and drive failures are more likely when you are scrubbing because of the increased activity. 

Create a new group, put the www-data user and your trusted dev(s) (ie, you or whovever else), set perms and ownership appropriately. Freelancer could still write code that could fopen() the files (and then display in his browser, whatever) though since the www-data user needs to be able to read the files... If www-data doesn't need to read the files, they don't belong in the webspace. 

Works fine on my machine (Debian Jessie). I've had an issue with an older version of the client, on an older version of Debian, using Courier - the fullchain.pem certificate wasn't created automatically, I had to several certs together to generate it each time I updated. Output of your same sslclient command followed by output of from one working machine: 

Yes, you are basically wanting other hostnames to be valid in your DNS zone (domain) and then once you have DNS working for the other names you'll want to set up a new name based host (Apache term) or a new server definition (nginx)