The tables are stored on slow storage. The database is under recovery model and transaction log backup is made each 15 minutes. The issue is I am not able to perform partiatl backup (restoring only data from the fast storage) and apply transaction log chain as well. Right? To create mirror tables on the slow storage and continue to store the history table on the slow storage. Each day a job is migrating data from the primary history tables to the mirror tables. So can I now restore a partial backup and apply the transaction log chain as well, as the tables on the slow drive are not same? 

I know that when columns are used the data is stored - the data row will have a pointer to another location where the 'large value' is stored. I have the following questions: 

I have recently learned about sp_tableoption which gives you the ability to store "large value types out of row" for particular table. A colleague of me told me if I enabled it, each field is stored on 8 KB page at least, no matter how large it is. I was not able to find a confirmation of this, so I run the following test: 

Is it possible to set the name of a unique constraint when it is part of table variable function definition? 

I have a function which is generating dynamic statement. The generated code is executed then in a stored procedure. I want to log some statistics about how long the execution of the generated code is and some IO statistics (logical/physical reads and stuff). In order to do that, in the function which is building the dynamic statement I append the following line to the final statement: 

I am going to use a cloud storage where the fast storage is too expensive to hold all the data. So, I want to store only the the new data in on fast storage, and the older data on slow/cheap storage. Is there a way to achieve this? Also, since the major part of the data is not () I want to perform only partial backups on the data stored into the fast storage only.Let's say a full backup each week and a partial backup each 12 hours. I am looking both for storage and recovery strategies (any links are welcome, too). 

I am trying to compress some tables that have fields. Unfortunately, the and the compression do not have the desire impact (only ~100/200 MB saved for 20 GB table). Also, I am not able to apply column store and column store archival compressions because they do not support compression of fields. Can anyone tell if I have any alternatives here? I also guess the and compression do not have effect because the content of the columns is unique. 

execution plan - slow query: $URL$ execution plan - fast query: $URL$ The queries are executed on the same hardware. 

recovery mode a backup of the transaction log file is made each 5 minutes is enabled the transaction log file size is 128 MB (only 1-5 percent of it are in used) 

where the is the current user location. For two different users, I get two different execution times - 0 seconds vs 5 seconds and I want to understand what is causing this. I get two identical execution plans: 

The issue is I need to calculate this distance for each . I have try several things (using cross apply, creating separate function, etc) but nothing worked. So, I decided to create a loop which will calculate the distance for each . 

The two tables are pretty large (more then 50GBs). The first one contains some data, and the second one records' changes.The database is in recovery model (). 

I want to create a new column master key using certificate created in advanced. I use the following command to create a certificate in the . 

So, using password is not best way to protect it. I was not able to find any pros and cons for using the other techniques. Can anyone tell is there are difference from security or performance matter or it is just a matter of choice? 

I have table with 42 columns and a trigger which should do some stuff when 38 of these columns are updated. So, I need to skip the logic if the rest 4 columns are changed. I can use UPDATE() function and create one big condition, but prefer to do something shorter. Using COLUMNS_UPDATED I can check if all of certain columns are updated? For example, checking if column 3, 5 and 9 are updated: 

I am going to a large amount of data from one table to other table. The target data source table has foreign key constrains, so I am not able to use , but need to have a table instead. The table is not going to be read by other users, but the (in which I am inserting records will be frequently used during the operation) - so, I need to do the migration without blocking the other users activity. The database is under recovery model, and transaction log backup is made on every minutes (the transaction log growth during the operation is not an issue anyway). I have the following statement: 

This will probably work, but the will be force to learn how to work with the interface, and the application functionalities will consume a lot of time to develop - it will be easier to use tool that is already created and the user is familiar with (for example excel). I know I can export/import data between T-SQL table and excel file, but is there a way to build some kind of collaboration between them - for example the excel to call a stored procedure which is returning the data (it will be nice if I can send a hash of the data or the rows in order to return only the rows that have been changed). And to call a stored procedure which can upload/delete/insert data in the table. If not, I guess I am going to work with files - download the file, edit the data in excel, then upload it, if there aren't other ways? 

I am wondering what could cause such LOB reads when the plan is the same and the data size and rows of each operator is the same (except the one)? There is another difference in the operator - the of the faster query is 1142 and for the slower is 1180. 

Is there a way to get compressions saving for column store and column store column store archival compression? In order to check these compressions saving I have to drop all clustered and non clustered indexes and then to create a clustered column stored index - most of the time this a time consuming operation and I am wondering is there an other way to do this. 

I want to install SQL Server 2014 and editions on two separated servers with same hardware in order to test the performances differences running internal set of procedures. The DBA gave me two s with the following name: 

I have several concept questions about encrypting data. The task My goal is to encrypt column but I want to encrypt only small percent of the data (less than 1% of it). As the table is referred in so many SQL objects, ASP files, .rdl files and SSIS projects I do not want to create separate table to hold these values there (at least for now I am looking for alternatives). The plan I am thinking for one of the following solution: