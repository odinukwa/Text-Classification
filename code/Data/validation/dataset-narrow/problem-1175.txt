I am tempted to read into this that even when an abstract operation is implemented by a sequence of multiple rep operations, the rep invariant has to hold after each of those rep operations, and when it holds, then the rep value must have at least one abstract counterpart. Many people have addressed some of the challenges posed in this paper. One of the more interesting and recent developments is R Jagadeesan and J Riely's Quantitative Quiescent Consistency [$URL$ 

Short answer: yes. Long answer: Using process algebra as a witness to the claimed existential is certainly admissible, but the way the question is phrased might warrant are more direct answer. If TMs are used as mathematical model for sequential computation, we can surely come up with a concurrent version, and show that it is no more powerful than the good old sequential ones. There are many ways to do so. We could aim for modelling shared variables as a tape shared by a family of TMs, we could add message passing steps to TMs, we could allow TMs to fork off child TMs with their own little tape, or we could invent TM equivalents of starting and ending a supposedly atomic transaction. Every single one of these extensions is easily shown wo be simulatable by an ordinary good old TM. If you're prepared to elaborate where you see a difficulty, the community may be able to help, even if I'm concerned that none of this qualifies as a research-level question. 

Constant functions have unique fixed points. Another criterion that may be applicable is to compare approximations, $\mu_i = \bigcup_{k<i}f^k(\bot)$ and $\nu_i = \bigcap_{k<i}f^k(\top)$, of the least and the greatest fixed points. Trivially, as soon as $\mu_i = \nu_i$ for some $i$, it has been settled that $f$ has a unique fixed point. The problem with this characterization is that, depending on the lattice and $f$, it is incomplete unless you are prepared to explore transfinite approximations. 

I may very well be missing the point but you stated that $n$ is fixed, so all DFAs of that size could be considered precomputed and stored in an easily simulatable format. Compute $K$ as follows: On input $x$, $y$ where $x,y\in\Sigma^*$ 

$[\mathit{true},\mathit{length}(a)=\mathit{length}(a')\land\mathit{elements}(a)=\mathit{elements}(a')\land\forall i(1\leq i <\mathit{length}(a')\Rightarrow a'[i]\leq a'[i+1])]$. With those ingredients, we have quite some power at our disposal, too much as it turns out. Even the domain of $S$ is undecidable because we would need to say whether a spec $[\mathit{true},\phi]$ is implementable or not, for arbitrary formulas $\phi$ of Peano arithmetic, before worrying about how to synthesize implementations. But that is already undecidable. 

I know that Exceptions as a means of flow-control is generally frowned upon. But in my opinion, Exceptions have little value short of the flow-control aspect - after all, if you didn't want the program to continue, you could just output an error-message an terminate the program. Exceptions, on the other hand, provide a means of reporting errors "locally", allowing a service/component to fail, and a consumer to handle the failure - and regardless of how you look at that, it is a means of controlling the flow of the program. So here's my question - over the years, I have frequently wondered, why isn't it possible to resume execution after an exception is thrown? Now, you wouldn't want to allow any consumer to resume after an Exception thrown by any other component, as that component was probably not designed to resume after a throw-statement, which would lead to unpredictable results. So let's say there's a supertype of Exception called Interrupt, that allows this behavior. An Interrupt would behave just like an Exception in every respect, except that by throwing an Interrupt, you indicate that the component is ready and able to resume execution after the throw-statement, and that the stack needs to be preserved either until (A) the Interrupt has been handled, or (B) the program exits with an error-message. Let's say we add a new "resume" statement to the language, to be used inside a traditional "catch" block - if you catch an Interrupt, and issue a "resume" statement, control would return to the point from where the Interrupt was originally thrown, and execution would continue from the next statement. I've presented this idea in other circles, and I'm met with a lot of resistance, but no clear argument as to why this is not a good idea. To me, it seems like a natural extension of the idea of exceptions - there are plenty of cases where this could be useful, for example while enumerating a sequence, e.g. in a function that "yields" one result at a time; an unexpected condition could occur while producing one of these results, and you may want the calling code to decide whether or not it makes sense to continue producing more results. An exception does not allow for that. Let's say this function throws an interrupt instead - if so, the calling code now has a chance to look at that and decide whether to resume execution (as if the exception never occurred) and produce more results, perhaps log the condition and then resume, or perhaps throw an exception, or perhaps re-throw the interrupt in case it can be handled up-stream. I'm sure I'm not the first person to have this idea, but I would like to understand why this isn't feasible or why it's not a good idea. (PS: I'm a programmer, not a scientist, so go easy on me.) 

A few more examples: Sleator, Thurston and Tarjan used a geometric representation of trees as partitions of polygons, and hyperbolic geometry, to prove lower bounds for binary tree rotation. (Also, I believe the history of a dynamic binary search tree can be represented as a tetrahedralization.) The reduction of least common ancestor to range minimum queries, due to Berkman and Vishkin, relates a data structures problem on trees to an arguably-geometric problem. (and thanks for the article David) The reduction of a scheduling problem to max weight independent set of axis-parallel rectangles [1] or the reduction of a different scheduling problem to geometric set cover [2] might qualify. The reduction of the largest common subsequence problem to finding layers of maxima is well-known (meaning, I'm too lazy to look up who actually thought of it). [1] (Liane Lewin-Eytan, Joseph Seffi Naor and Ariel Orda) [2] Nikhil Bansal, Kirk Pruhs. The Geometry of Scheduling, FOCS 2010. [later edit] A couple more cases where a "geometric" view seemed surprising (though the "submission to SoCG" or "makes something to visualize" standards are probably not met): algebraic topology applied to lower bounds for distributed computing incorporating computability into Hausdorff dimension defining a notion of distance for groups, then volume, then growth of volume as a function of distance, then using "polynomial growth" 

For the particular case of points and half-planes, there is a simple (but a little tricky) argument that there is $R\subset X$ of size $r$ such that the supporting open half-planes of the convex hull of $R$ each contain at most $O(\log r)/r$ points of $X$. (Such half-planes are exactly the set of half-planes that are bounded by a line through two points of $R$ and contain no points of $R$.) Also, any open half-plane that contains no points of $R$ is contained in two such supporting half-planes. (I won't give a proof of this.) Therefore, $R$ is an $\epsilon$-net for a size $r$ which is $O(\log(1/\epsilon)/\epsilon$. For the existence of such an $R$, suppose $R$ is a random sample of $X$ of size $r$, each member of $R$ chosen independently. Let $\cal H$ be the set of at most $r(r-1)$ open half-planes $H$ bounded by lines through two (or more) points of $R$, and let $\cal H'\subset \cal H$ be the set of $H\in\cal H$ that contain at least $\alpha |X|$ points of $X$, where $\alpha > 0$ is to be determined. For a given $H\in {\cal H}$, the probability that it is in $\cal H'$, and that none of the other $r-2$ points of $R$ falls in $H$, is at most $(1-\alpha)^{r-2}\le \exp(-\alpha(r-2))$. There is $\alpha = O(\log r)/r$ such that this probability is at most $1/2r(r-1)$. Since $\cal H'$ has at most $r(r-1)$ members, by a union bound, the probability that every member of $\cal H'$ contains a point of $R$ is at least $1/2$. So there is some $R$ such that every supporting half-plane of its convex hull contains at most an $\alpha = O(\log r)/r$ fraction of the points of $X$. This sort of argument goes back to at least 1985, where $\epsilon$-net results were given for balls and halfspaces, and given in more generality in 1986, where results for $\epsilon$-nets and a kind of partial $\epsilon$-approximation were given. The set $\cal H$ above, a collection of half-planes defined by a set of points, is generalized to a collection of "regions" defined by a set of "objects", where the number of defining objects is typically a function of the dimension, but the simple union bound argument is the same. However, not all range spaces of bounded VC dimension fit in this framework.