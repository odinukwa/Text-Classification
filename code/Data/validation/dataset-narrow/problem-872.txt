You need to have a better understanding of the risks associated to running a (any) web server. There are OS related risks and Web Server risks. As for the applications risks, well that's for a different discussion. While I know it's popular to say that Windows is insecure the my opinion is that every operating system can be made secure/insecure depending on your skills and diligence in applying patches and updates. For securing a Windows Web Server you could rely on the built-in Windows firewall software to lock down the open ports. Another option is to leverage IPSec to prevent any communication from the internet to the server except over port 80/443. For web server risks you will find that both Apache and IIS have their share of vulnerabilities. The key here is to keep the patches and updates current. 

I contacted RARLabs support and recieved a response. It turns out that RAR.EXE can handle streaming input similar to how gzip works. You just need to specify the -si option: 

The short answer is that IIS allows you to configure exclusions to what gets logged in IIS. The documentation is pretty easy to follow and should meet your requirements. Check out my article titled Limit what gets logged in IIS 7... 

Wordpress on Windows 2008 IIS requires that you grant read permissons to the IUSR account and the IIS_IUSR account on the root folder. If you want to add attachments with your posts then you need to add write access to IUSR on the wp-content/upload folder. If you want to be able to dynamic updates to plugins and such then you need to grant write access to the wp-content folder. I would recommend only granting write permissions to the wp-content folder when you are performing upgrades, and then remove the permission when you have finished. I wrote an article about this on my blog after working through the same questions. 

Questions like this always end up being a religious decision. For the most part all three of these platforms have their strengths and weaknesses. Looking over everyone else's answers it's pretty clear that each person has their choice of platform regardless of the facts... To correct a previous incorrect statement... 

Being in a similar situation, I created a free account at editdns.net and configured a secondary zone for my domain. Once the zone transfers were working I updated my domain registrar's name servers to point to the EditDNS.NET name servers (free01.editdns.net and free02.editdns.net) instead of my DNS servers. Once I was sure the domain was resolving correctly from editdns.net I had the ISP switch over my IP addresses. Next I updated all of my DNS server records with new addresses. Finally I reconfigure the editdns secondary zone to pull from my new dns server IP addresse. Now I could have switched my domain registrar name servers back to my DNS servers but I figured it was nice having editdns.net name servers handling the dns lookup requests. So I left my domain registrar name servers pointing to the editdns.net name servers and I keep them updated through secondary zone transfers from my dns servers. If done correctly there would be minimal impact to the internet world... 

Have you configured the VMWare to not push the host time to your VM's? The default behavior of Virtual Services is to have the VM's get their time/date from the VM Host... Here is an article I found using a search that talks about enabling/disabling time sync with the host... 

Are you running a high-end video adapter on your Hyper-V server? There is an issue with this configuration and a recommendation to use the basic svga drivers on Hyper-V servers. Understanding High-End Video Performance Issues with Hyper-V A possible option to solving your RDP problem is to add the Terminal Services Gateway to your Hyper-V server and then you can connect to your VM's through your Hyper-V Server. I wrote up an article that outlines this feature in Windows 2008 Server and use it every day to get to my VM's on my lab server. Once you have TSG enabled you can extend it by adding RRAS with SSTP to add the option of a VPN connection into your home network, all over your Hyper-V server. 

If your Windows server is behind a NAT device then I would recommend creating a port forwarding rule on your NAT that can accept an inbound connection on TCP/5555 and then forward to TCP/3389. This way you aren't modifying the server. Also, if you have more than one server you would like to connect via RDP then I would recommend you check out Windows 2008 Terminal Services Gateway. 

Exporting private keys on certificates that have been marked non-exportable? Uh, how about a tool called Jailbreak... 

My Wordpress tables appear to be in need of optimization so I looked into the commmand OPTIMIZE TABLE . When I run the command I get the following results: 

Check out the System Firewall policy, where the default setting is to not allow ICMP into the ISA server except from specific management computers... 

With limited memory capabilities (2 GB PC2-6400 unbuffered DDR2 800 MHz Maximum) and no video output included you might be a little hard pressed to install and configure Windows 2008. This device is specifically built for the Windows Home Server. You would be better off building a custom server that can handle what you want to achieve. 

IPSec should fill your needs and it's free/included in Windows. Nothing fancy here, just a simple deny rule and a filter matching the offending IP Address spaces... 

Microsoft used to say that Domains were a security boundary but now they agree that the forest is really the security boundary. Having multiple domains is mainly used now for isolating traffic to specific regions (ex. East vs West, or Northamerica vs. Europe). That being said, in many cases the choice to use multiple domains ends up being the wrong choice since it introduces the additional cost of servers and management. OU's can be configured with delegation to isolate management and security tasks sufficient to meet almost any inter-forest security requirements. My recommendation is that unless you have a siginificant network connectivitity difference between two separate locations stay with a single domain. UPDATE: I also forgot to mention that Windows 2008 allows for OU level Password policies, making one of the primary reasons used for separate domains obsolete. 

You could use Windows 2008 Server and the built-in Secure Socket Tunneling Protocol? You could use the built-in Windows Vista/7 networking client and the configuration is very simple. 

You will notice that I did not configure a username and password for connecting into Active Directory. That's because I am running BlogEngine on a domain member server and the IIS services are running under an application pool using Network Services account. If you must use explicit credentials then you can add connectionUsername and connectionPassword to the MyADMembershipProvider entry with the appropriate information. 

I'm using RARLABS RAR.exe to archive/backup my server data. I am familiar with using RAR for creating an archive and adding files from a folder, but what about streaming data directly into an archive? For example, when backing up my MySQL databases I use the mysqldump command that includes a pipe command into a text file. It would be nice to skip the file step and go directly into an archive file using something like the following syntax: 

I ran into a similar requirement when trying to figure out how to leverage Active Directory for BlogEngine.NET. After spending some time researching I was able to use Active Directory user accounts with the Basic Authentication .NET Membership framework. This worked on my domain member web server but could easily work for non-domain member servers assuming you add the username and password to the configuration section of the web.config. From my blog post about how to configure: Add an entry into the section pointing to your domain controller. 

I have been using FreeSSHD for some time with much success. It can run as a service, supports certificate login, and full tunneling... 

The goal of a DMZ is usually to separate the untrusted network from the trusted network. Depending on the level of firewalls you can control many aspects of the connection besides just opening up ports. My experience has been that companies put in place a policy that prevents any untrusted connections from communicating directly with trusted resources, with exceptions being carefully reviewed. For example, reverse proxy servers in the DMZ can provide pre-authentication for untrusted connections before they are served up data from trusted resources. Another point worth considering is that even with opening up ports, it is possible to control which devices can communicate over those ports. For example, it might be necessary to open up a port to allow SQL communication between a web server in the DMZ and a SQL server in the trusted newtwork, but you could restrict the traffic flowing through that port to only the Web server and the SQL server. Utimately it's up to you to craft a boundary policy that will provide the minimal level of access to satisfy your application requirements. 

There shouldn't be a problem mixing the wildcard and specific certificates. Not sure why you would need to do this as the wildcard certificate should be able to handle all domain.com requests. You should also be able to bind the same wildcard certificate to multiple websites, meaning you don't need a unique certificate for each website. 

Our environment prevents network capture solutions on production servers for the primary reason that you don't want it to be overly easy for tier'd admins to perform network captures. The actual files necessary to run WireShark and/or NetMon don't themselves present much of a risk, rather the ability of admins to perform captures can be considered a risk. 

You mention an internal network of 192.168.0.1 for the Hyper-V host but nothing about whether the Hyper-V host has another interface that allows communication to the internet. It is usual practice to configure the Hyper-V host with an "external" interface that can communicate out to the internet, and then the internal interface that can communicate to the VM servers. Your VM servers on the 192.168.0.x network will need a router to route the traffic through which can be another VM configured as a router or you can configure the Hyper-V host to be the router. With the router configured you then need to set the VM machines default gateway to be the router IP address... that should get you going... 

If you're using IIS then your log files can record the field cs(Referer) which will show when someone has come to your site via an external link... 

The VPS is probably configured to set the time of the VM's it's hosting, and your Windows 2008 VM is probably getting time from the default time source (time.windows.com)... Try disabling your VPS from setting the VM time and see if that fixes your problem. 

Double check your web browser isn't configured to use a proxy server... Worst case you could run WireShark on the web server and see if the web requests are actually making it to the web server on that port (8081)... Also, run the command line tool "netstat -na" and see if there is an entry for the 8081 binding. Something like: 

You should also be able to create a url rewriting rule on the main site to send clients to the correct domain: 

Notice the first part of the LDAP:// syntax specifies the name of the domain controller (server.domain.com). You have a couple of options here. You can specify the Fully Qualified Domain Name as shown in the example; you can specify the relativeDistinguishedNamek (ex. server); you can specify the IP Address of the domain controller (ex. 192.168.1.10); or for more redundancy you can specify just the domain name (ex. domain.com). Make your section look like the following: 

Configuration and data files that are for internal use only should be placed in the App_Data folder: The App_Data Folder To improve the security of the data used by your ASP.NET application, a new subfolder named App_Data has been added for ASP.NET applications. Files stored in the App_Data folder are not returned in response to direct HTTP requests, which makes the App_Data folder the recommended location for data stored with your application, including .mdf (SQL Server Express Edition), .mdb (Microsoft Access), or XML files. Note that when using the App_Data folder to store your application data, the identity of your application has read and write permissions to the App_Data folder. What's new in ASP.NET Data Access