Follow this pattern, and don't put an [OR] on the very last one. EDIT: New solution: If you want to block all (friendly) bots, make a file called "robots.txt" and put it in where your index.html is. Inside it, put this: 

You'd still need to maintain a list like my original answer (above) to disallow the bots that ignore robots.txt. 

Incremental backups simply add files or remove files or apply changes to pre-existing files from an original backup. If this drive gets full (assuming it's only backup) then just take it out, pop in a new one, full backup, then do incremental. 

and MAKE SURE you allow indexes and the goody-two-shoes options directives. That should fix you. EDIT: You can pretty much remove the "ifmodule" tags. Try it that way too. At least explain what your problem is, it's a little bland. 

I think you're better off running it under $URL$ emulation. If you've already tried compiling it for native running, then at least try this. It's less prone to cause errors with Squid. 

Here's the deal: Fix the C:/'s in my example and update the paths for yours. Make sure you have the "NameVirtualHost" at the top of the vHosts section. 

It can never be confident of others. For example: website $URL$ (test.com is lookup one, then www.test.com is #2) has 20 images, all hosted on different sites. DNS can not predict this without loading the HTML first, but thats what the client is for. The client's browser will decode the HTML and request all the sources to the images, usually all at once. What if the page changes and they update all the links of images? BL: It's never going to be able to PREDICT dns requests, but it CAN cache prevoius requests. The TTL of the record (expiration time) will let the DNS server when to purge that record from the cache. That's just about it. 

I believe THIS: $URL$ can help you with the FTP server, but I'm not sure about group permissions. Can you run a virtual server with simple windows XP on it and use FileZilla server? If not, try this other option: $URL$ I think that has groups too. EDIT: FileZilla for windows (server version) DOES allow groups and inheritence. Also custom read/write and download/up speeds for groups or users individually. Here's a screenshot of the group config page: $URL$ (it's an RTF document with screenshots). 

Try FileZilla Server for Windows: $URL$ That will let you create password protected users with some of these settings: $URL$ (PDF file with screenshots). It does work on allowing you to attach network drives if you first map them to a letter under the Host. (In the server-computer, map folder XYZ to letter A-Z (one that's not in use) so that accessing drive X (or whatever letter you choose) will indeed open the folder of choice. Then under filezilla, grant a user access to drive X (or which you choose). If you need more help, ask. 

If you are using Apache for the webserver you can do a mod rewrite rule to forward requests on ts.v...com to the ts4.game....com server and include port numbers. Is the service http based or is it a program directly cOnnecting? 

I've got a file called "stream.ogg" which is, well, an internet radio station. If a user tries to download the file, it always appears as "210mb" no matter what OS they try to save it from. I've got several other files for download, some are .zip, and some are .exe. When someone downloads a file, it says "unknown time remaining" or "unknown file size total" and I was wondering if I could get Apache to relay the total size to the client? Other servers on the net do have this set up somehow with Apache, for example when downloading a linux distro from any website, it always knows how big the file is. (ex. 19 minutes remaining). Can anyone help me with this please? Thanks. PS: Apache is the latest public release, PHP and scripts are enabled. 

For virtualizing an OS in windows, VMWare's Workstation is a great tool. You can run as many machines as your computer can handle (depending on if you set each OS to use a certain amount of RAM or processor usage). It supports linux, mac, windows, and solaris all the like (Though it can be tricky installing mac, if you so choose). Storage wise, you will need: - A RAID controller (can be an internal card like so: $URL$ 

Well even if the site is in the trusted sites list, you need to either adjust the security setting: Tools>options>security settings> low/medium/high to allow scripts without warning. Click "custom" then look under the category of ActiveX controls. There's an option not to show warning for both previusly used and new scripts. That will resolve the "do you trust my script" error in IE. 

You should put the directory tag inside the vHost tag. Make sure your server will support execution of these scripts too. ALSO: You should just do "directory as /" as noted above. Otherwise you'd have to go to joebloggstest1.couk/joebloggstest.co.uk. 

Are too many "accounts" trying to log in for every single time a database is accessed? Do they not carry persistent sessions? 

Look under "Installed Programs" then "Show updates" and go side by side and compare, or print screen them and print it out to reference if they aren;t near eachother geographically. Here's a link to a program with really detailed information about the computers. This will help with everything you could want to know about a computer... LINK: $URL$ NOTE: The program takes a long time to scan everything because it is thorough (maybe 30 seconds). 

Cloudflare uses a proxy server system to "optimize" your site: If your site is set up to be "optimized" then whenever you access that site from an internet browser, it will come up as cloudflare's IP. (Try pinging www.website.com that's used by cloudflare and maybe make a direct.website.com that's a direct to your IP. You'll notice that first, the IPs are different. Pinging my website (www is direct, www1 is through Cloudflare, my ping times to the direct server are 12 ms avg. The ones through cloudflare (thus only pinging cloudflare's server) is 232 ms avg.) So the ping times with using cloudflare are often higher. Page load times are hopefully shortened, since much of the content on your site (if it's set in cloudflare's settings) is cached, and will load faster, if cloudflare's servers are faster and/or have better net connection than yours. I personally don't think cloudflare's proxy option is good, though it is much more secure, it makes server logs illegitimate (only showing up under cloudflare's list of IPs). Some requests don't even appear since they are cached. You are correct that cloudflare does block some of the multiple requests, if they are from the same IP and in a short amount of time, though I have no solid proof of this, I've noticed this trend too. And maybe if you have a lot of options on the site enabled, it has to check for certain aspects, as noted by the options in the cloudflare config system. (Has to check against blacklist, has to proxy, has to look for google analytics, make sure the javascript and images is compressed, etc. Overall opinion: use it if you need it otherwise just go direct. 

Gmail is tricky. Make sure you're using the right port. 990 I think. And for the username, it's your ENTIRE email address. 

So what I'd do, is upload a copy of all the static files, to EC2. Set up the server-side settings in rails, mongo, sql, etc. If you've got databases that change really often, edit the web pages and scripts to READ from the old servers and WRITE to the new ones. Copy all remaining records with the export function. When the files and things are just how you like it, you can change over the DNS settings. 

Is it not like apache and just pick the default (first) configuration? If you don't want people to forward their domain to your server, make two or more virtual hosts, the first one pointing to an error page, and then any request for a domain not hosted by you will return an error. 

Well port 21 is supposed to be FTP unsecure. Not sure if you're running that on a "game box." If you're using Apache as the external server and trying to map it to other servers and ports, your best bet is to use apache mod_proxy and mod reverse proxy. Here's a sample config: 

If it's a windows machine, you can run "WinAudit" (free, just search for it) on a scheduled job. It generates HTML or text reports and such, of which you can save to a network drive and view from a remote computer. OR: Remote desktop. You connect, and use it as if it were a local machine. Works in *nix too. 

You let them get sent (just a few) then look at the email's "original text" if your server supports archiving mail. Look in the queue files if possible to see if you can actually read the mails before they get sent. I mean, depending on how important the mail server is, shut it down for a few minutes and note who connects to it when it comes online again. Even better, disallow use of it outside the domain. That will put an end to the issue for good. 

If centos 6 does not support legacy PHP, you are better off running v5 because if you have to mod the OS to trick it into running it, it could open up a security hole. Recommend you use the latest PHP and OS. If that's not possible, then make sure you have very tight security when using old PHP. That's why they update their software. I'm looking for a way to get this to work right for you. EDIT: I was beaten to it. Refer to the other poster's wget method. EDIT2: removed useless text. 

For this you need to make another virtual host that has the different settings, and for the redirects you want under "this situation" use that vHost instead. This is not PHP code, it's Apache language. Sorry. There are no if-thens. 

Mod rewrite. There are so many different configurations that I can't post any specific ones here. But you could use dreamweaver to update all links. Are you trying to get /test/app/images/stuff? What directory are the images in? And it is a good thing you are in relative path mode. That saves much time. Try putting the images folder inside test folder. 

If you are positively sure that you've indeed got 2 php's, try this: $URL$ which lets you compare 2 text files (php.ini1 to php.ini2). If you've only got one php.ini file, make sure that if you ever upgraded the PHP version that it was done so correctly, as any old remnants that contain the version number may be throwing the result off. If possible, please post the results of your PHPInfo tests. Thanks, and good luck. If both of the above statements came back normal, maybe it's a bug. But I highly doubt it's mac-only. I'll look into it further if you can post your results and still not solve it. 

This is not a good idea. Your users wouldn't be able to trust you, in the sense that you can ADD CODE TO THEIR SITES. You could theoretically add keylogger scripts or something to the site, and so if you're going to be doing this, make it like a codeen server, where all new users are greeted with a welcome message. 

It depends on the specs of the server and what type of content it serves. It may not be possible at all, (simultaneous right?) if it's serving active content and media. Database and small websites should do fine. In apache config, try making it start many worker/child threads. In the box itself, just test it to see if Apache will handle it. If apache can but not the box, maybe someone else can help with that. Sorry. Good luck. Yes, sorry for not reading it thoroughly. 

Why not store one large file and have the server convert them to requested sizes on demand then store them in the cache? Consider also running several frontend servers (Through load balancer) to serve the requests, then maybe using NAS or several other servers to serve the static content. The number of frontends you need depends on how much traffic you'll get (youtube capacity or just storing the content for occasional accesses).