The Bayes error rate is a theoretical bound that determines the lowest possible error rate for a classification problem, given some data. I was wondering whether an equivalent concept exists for the case of regression algorithms. My aim is to determine how far my regression algorithm's error is from that theoretical bound, as a way to assess how far am I from the best possible solution. Is there any way to obtain a bound of the lowest regression error for a given dataset? 

There are ranking algorithms based on machine learning that are aimed to build ranking models. Training data for these models is given in the form of partial ordering between each pair of elements in a sample. A brief description, together with a list of useful references, is given in the corresponding Wikipedia page. 

Feature selection is a very well established field in Machine Learning. The objective of feature selection algorithms is to select a subset of your feature set in order to maximize your system's prediction performance. There are two kind of feature selection approaches: Filter methods: filter methods select features without taking into consideration any specific prediction algorithm. They are based in applying different kinds of measures (like mutual information or correlation) in order to evaluate the relative information that a feature or a set of features can provide about the output variable. Filter methods are faster than wrapping methods, but do not perform as well as wrapping methods. Wrapper methods: these methods evaluate features or sets of features by using a specific classification/regression algorithm. I'd suggest to read one of the seminal papers in the feature selection field as a starting point in order to get to know the basics: Guyon, Isabelle; Elisseeff, Andr√© (2003). "An Introduction to Variable and Feature Selection". JMLR 

You are facing a regression problem: your aim is to predict the value of a continuous variable given the value of a set of input variables (these input variables can be of any type: numbers, categories, etc.) A decision tree is usually applied to classification problems, in which you are aiming at predicting a discrete value. There are several regression methods in sklearn that you could use. The simplest ones, and maybe the ones you should start from, are linear models: $URL$ Notice that you may need to transform your input data. For instance, if one of the features is categorical, you may need to transform it into a set of binary variables. Other processes, like data normalisation, may also be advisable in order to get better results. Edit: as stated in the comments, Decision Trees can also be applied to regression problems. However, and in my own experience, the output curve that you obtain by means of this algorithm usually has a step-wise shape that may affect the final bias (see, for instance the example in the scikit-learn docs.). I would suggest not to constraint yourself and try different types of algorithms. 

Your use case boils down to categorizing news feed on an online forum and then finding out top-n categories. I would suggest you look at this Hacker News Categorizer developed by MonekyLearn. This way you can understand how to get started with such projects. PS : I am not affiliated with MonkeyLearn. 

This Networks add-on for Orange data mining suite should help you. If you are open to using other solutions, I would recommend networkx Python library. 

I am not very sure how effective NN would be for this problem. The way I see it is that you have 48 entries in a time series and now you are trying to predict the next 4. Keeping aside the correlations for a second, it seems like there is very little data to "learn" from. I mean the power consumption is a seasonal thing (much like temperature) but you have only 1 years data so it should be difficult for NN to capture this seasonality. I would like to raise another point about the way you are using correlations. If you are taking correlations using the actual temperature value then I do not think that to be a good idea. Think of it in this way, I would be using a lot of power at both very high and very low temperature. So instead of using the raw temperature values, I should normalize them with respect to some "comfortable" temperature value by taking the absolute difference between the actual temperature and the "comfortable" temperature. For instance, let us say that "comfortable" temperature is 25C, then both 5C and 45C would be normalized to 20 and this looks more plausible as my power consumption should be high in both the cases. This might also explain the poor correlation that you observe for 3 months of data. What should be the "comfortable" temperature is an entirely different story altogether. Do let me know if this line of thought makes sense. 

You can code certain simple rules like the ones you have mentioned in the question. Additionally, you can use knowledge bases like Freebase and WordNet to enrich your language model. Note that this will not necessarily "noisify" your data but would have effect similar to the effect on data augmentation on say images for downstream tasks. 

I'd suggest to use ARIMA (autoregressive iterate moving average) as a way to detect the regularities in your time series. Take a look to the following link, in which you will be introduced to ARIMA by means of series of blog posts: $URL$ 

Dynamic Time Warping (DTW) has quadratic complexity. There are several other versions of the algorithm, like FastDTW (linear complexity) that decrease the complexity by computing approximations. FastDTW is implemented, for instance, in this Python module. 

If I correctly understood your problem: you have a set of routes between the same two start and destination locations, and each route is described as a set of (lat, lon, time) tuples. In order to compute the distance between two routes, one possibility would be to apply a variant of the edit distance between strings. This algorithm computes the distance between two strings as the amount of string operations (edit, remove, add letters) required to transform the first string into the second one. You just have to come up with a set of "route operations", and the cost of each of these operations (add point routes, remove point routes, replace one point route by another one, and so on). 

In this example, we compute the training score and the test score (cross validation score) of a Naive Bayes model as we increase the number of examples in the training dataset. The higher the score is, the better the model is performing. A larger training set decreases the score because it is more difficult for the learning algorithm to learn a model that correctly represents all the training data. However, as we increase the size of the training set, the test score also increases, due to an increase in the model's ability to generalise. As you can see, both lines in the plot reach a limit on the right size. This means that in order to learn properly, an algorithm requires enough data, just enough for us to get to the right side of this plot. Once we reach that asymptote, we can not improve the test score by using more training data. Now, notice that this plot only refers to the size of the training set, and not to the size of the test set, but inferring from it, I may assume that when you only use 2-3 examples in your test set you don't have enough test data to determine your test score. You may even get completely different results for other 2-3 different test examples. Therefore, I would assume that the 30% test error of your larger test set is a most feasible value. It seems that, in order to increase the performance of your model, you will have to look for other reasons: are your features well defined? Do you have enough training data? 

You should refer this survey paper on Anomaly Detection (from University of Minnesota). Please let me know if this helps you. 

Another way would be to create your own dataset by downloading say Github user's graph, defining a network by adding edges between user A and user B if A follows B and so on. Then you can take snapshots of this graph at different time instances and see if you can use last snapshots to predict the edges in the th snapshot. Of course that would be a lot more extra effort and the links I posted may suffice. Please let me know if it helps. 

The Enron Corpus: A New Dataset for Email Classification Research paper describes the kind of data set you want. The paper mentions the following link to download the data set: $URL$ Additionally, the paper also mentions various other papers which have used smaller data sets related to email classification which may not be of much use given this larger dataset. 

This means that is in fact an RDD and not a dataframe (which you are assuming it to be). Either you convert it to a dataframe and then apply or do a operation over the RDD. Please let me know if you need any help around this. 

In Image Processing, this task is known as localization. You basically want to localize each digit in the image and then use your digit recognizer over the digits. A cursory google search for digit localization in images gives me following papers which seem to be very helpful. 

I understand that your time series are unevenly spaced. In this case, why not simply use a library like traces and transform them to evenly spaced time series. 

The dataset has around 50 unique persons. Each person will have multiple entries corresponding to multiple visits. The data spans over a year so I have quite a lot of entries (around 1 million). These people can be classified on the basis of the department they work under (2 departments - mutually exclusive) or on basis of their role (4 possible roles - all mutually exclusive) I was wondering what kind of data analysis can be done with this kind of dataset. I am not looking for straight-forward things like "who spent the most time in building". However things like finding correlation between visits of 2 people would be interesting. So if person A visits the premise, what is the probability that person B would also visit. Since I have only around 50 unique visitors, I think such an analysis is feasible. Another line of thought was to apply some interval-pattern mining techniques but I am not much familiar with them. Can someone give me some pointers/ideas about what kind of data products can be build using this or what kind of techniques can be used with such data. Edit : As discussed in comments, I call it a product in the sense that I do not want some simple or trivial analysis. And I am not looking for any commercially viable idea - just some cool fun idea :) 

There are several techniques that you could apply in order to cluster data if your input is a matrix of pairwise distances between elements. As usual, the best option depends on your specific data, so it is hard to answer to the question of what is the best one, but you could try any of the following ones: 

As you can see on the rightmost part of the plot, the two lines in the plot tend to reach and asymptote. Therefore, you eventually will reach a point in which increasing the size of your dataset will not have an impact on your trained model. The distance between the test error and training error asymptotes is a representation of your model's overfitting. But more importantly, this plot is saying whether you need more data. Basically, if you represent test and training error for increasing larger subsets of your training data, and the lines do not seem to be reaching an asymptote, you should keep collecting more data. 

As you can see, creating the best set of features is not an automatic process, and domain expertise is very important. This is a very common thing in many data science related tasks, as you will see in the next point. 

Object recognition is a subfield of Computer Vision which consists of methods/algorithms aimed at determining whether a given object is present on a 2D image. Many object recognition systems just return a 'yes/no' answer, but there are also many methods that return the location of the object on the image by means of a bounding box. This subfield comprises a vast range of methods, from low-complexity ones (like methods based solely on applying simple filters on the image, like colour/edges/etc.) to more complex ones in which machine learning methods (like deep learning) play a very important role. Depending on the complexity of the objects you are planning to detect, and on the overall complexity of the images, machine learning may be completely unnecessary. This Wikipedia page summarises many of these methods. You can start exploring from there, or you can start by using any introductory computer vision text. 

There is not such thing as "the best way to impute data". The best method will always depend on your specific application and model. Just remember the No Free Lunch Theorem. There are many ways in which you can impute values. You can ignore the rows containing missing values, you can impute values based on the other rows (mean, using classification, etc.), or you can even replace the missing values by a constant. Which method to use will depend on how much effort you want to put into the imputation algorithm and the final results. As usual, I'd start by using a simple method (the average, as you suggested), and increase the complexity of the imputation only if the results are not satisfactory enough. 

Spark support for Mahout came from Mahout 0.10 release while you are using 0.9 release. So this should explain why you get the error. I would suggest using a higher version of Mahout. 

This does not mean there is nothing to learn from the data. In the worst case, there is nothing to learn from data, with the given architecture, hyperparameters and the time you are willing to let the network learn. 

I would answer the question at two levels. The first level is "can it be done using machine learning?" I would say that machine learning is essentially about learning. So given that you prepare sufficient examples of sample documents and the output to expect from those documents, you can train a network to learn the structure of documents and extract the relevant information. The more general form of extracting information from documents is a well-researched problem and is more commonly known as Information Retrieval. And it is not limited to just machine learning techniques, you can use Natural Language Processing tools as well. So, in its general form, it is actually being done in practice. Coming to the second level, "should you be doing it using machine learning?". I would agree to what @NeilSlater said. The better and more feasible approach would be to use good programming practices so that you can reuse parts of your parser as your dataset evolves. 

I have a dataset which contains information about when do people enter and leave a premise. I have the following information in the dataset : 

Predicting "future" especially in the context of economics is a well-researched domain. For example, you can check out the research involving "predicting stock market using social networking". I am sure you would find such results for other domains as well. 

The question asks about the and I would answer in terms of space and time complexity. Let us say that number of input transactions are N(=20) and the number of unique elements be R(approx 900). Assuming your threshold count is quite small (which means very few candidates are pruned), the time and space complexity for size i candidates would be . So you see, if very few candidates are pruned, the space (and time) requirements become exponential. It might seem unintutive at first, given that you have only 20 transactions. But the bottleneck is the number of candidates which increases exponentially with number of items.