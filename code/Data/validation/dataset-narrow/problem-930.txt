What's next? Updates: Further reading shows that this problem would be simple if the drives were not attached to a raid controller (all be it in passthrough mode), as then smartctl would provide the information required. Using mfiutil: 

So after a lot of testing and diagnosing I've found the solution, but I still don't understand why. If anyone can explain this to me I i'll award the answer. The problem is was with the /boot directory. As it's installing under UEFI, the ESP gets setup under /boot/efi, in Grub this worked fine, i could read this ok. However, some of the /boot directory, which contains the required kernel was not readable. When i attempted to manually load the kernel and boot from grub, the kernel loaded fine, but when attempting to load the initrd, i received error: error: attempt to read or write outside of disk hd0 From what I understand this is because /boot is on the main 4.5TB partition, the files can end up anywhere on the drive, and in this case, and my many test cases before this, the files in /boot are too far up the drive for Grub to read. Creating a dedicated /boot partition before the ESP partition has resolved this. This is the same issue as documented here: (according to $URL$ What I don't understand however, is that from my understanding, under UEFI the full 4.5TB should be readable. Ubuntu should boot fine under it's default partition layout with only and ESP partition? This is confirmed as I've managed to install Ubuntu without a /boot partition on 3 other identical hardware and bios settings servers. It's just this one server which couldn't read inside some of /boot.a I've ensured the disk was booting under UEFI in the BIOS. 

I've followed the Mitaka setup guide for my first OpenStack cloud. This all went ok (2nd time around!), however i'm now having issues with networking. The instance launches ok, and it's assigned an IP via DHCP, but it won't ping. I don't know if my network is setup right, so i've provided the appropriate outputs below. I setup Mitaka with option 2 - 'self service networks', but for simplicity i've only created a Flat DHCP 'provider' network just to get a feel for things. On my test setup I only have 1 working public IP address, x.x.x.111 

You do not need to recompile anything. In fedora you can get the sqlite php module by installing the php-pdo module. The following should do the trick. 

Nothing requires that ping be possible between two hosts. It might be that somebody between you and google is dropping ICMP packets. If everything else is working I'd not worry much about this. If you are particularly worried check with whoever runs your networking equipment or firewall and see if they are letting ICMP traffic through. Also check to see if you can ping anybody else in the outside world other than google? 

You should try the Django work without any restarting of apache. Most sever side environments work fine while doing development work without any need for stopping and starting the web server over and over again. But, yes you can run any number of apace instances as long as you make sure your second server's config file is pointing at different resources. Like: 

Port 25 is the standard port SMTP traffic runs on. If you intend for you system to be an email server than those might be legit servers trying to send you or your users email. If you do not intend your system to be an email server, figure out how to get port 25 turned off. Historically email servers would be configured to politely send on email for other servers. Today this is bad, bad, bad. It's called being an open email relay. It would be wise for you to verify that you are not doing this. But, don't go to far and try to block port 25 traffic if you do mean to accept email from the outside world. 

I'd suggest using the expensive raid controller to do the bulk of the raid work. LSI cards and the software they come with works quite nicely. When properly configured, they will send you email when intereting things happen to the array. Like when disks fail. There is nothing wrong with either of the two linux software raid options, but you've gone out and purchased a somewhat fancy raid card. Let it do the work. Configure the disk array to expose one big device to Linux. If you would like to break up the final device into small volumes use lvm for that. One big physical volume, one big volume group and cut the volume group into whatever number of logical volumes you need. 

The above triggered the mentioned symptoms almost immediately. RAID shows healthy, however I'm unsure if I have the optimum caching configuration enabled: 

Installation Steps: Selection of UEFI Virtual CD which is Ubuntu 16.04 Server. NOTE - I did chose the UEFI Virtual CD rather than the highlighted non UEFI as shown in the screenshot (i realised after i took the screenshot) 

Attempting to install Ubuntu 16.04 Server onto a newly initialised disk. Using UEFI, and a 4.5TB / partition. After installing without issue, on rebooting, the server will not get past Grub. Hardware: 

To streamline the process we would like to install the driver while the machine is online. Unfortunately dism is requiring the disk to be offline, returning the error "This command can only be used with an offline image". We have tried pnputil to install the driver but this does not work, we believe this is because pnputil is for plugged in hardware with no driver currently. Is there a way to install a driver into an online image? 

In the end i couldn't find a way to map the mfiutil to a device. I'm sure there is a way but it escapes me. In the end i rebooted into the raid controller bios and luckily as the drive was completely dead it was showing in the controller. I think if i had studied megacli i could have realised this without rebooting, which would have been better. But in the end mfisyspd5 actually mapped to E1:S9, serial 1EJ49HWH But overall, if you want to run ZFS, don't use a RAID controller even in passthrough, just get a HBA. Will save you hassle in the long run. 

Also, should i look to make the conntrack settings on the DNS server more aggressive to close connections faster, these are the current settings on the DNS server: 

I'm looking at moving our XenServer cluster (150 VMs) to OpenStack on KVM. After extensive reading it looks like virt-v2v will do this. However I'm confused about it's usage. I was going to copy the VHD file and then run virt-v2v on this, then import into Glance, and start an instance. However it appears this isn't the process virt-v2v uses. Could someone explain the overall process, how to use virt-v2v or any other tool(s) that I will need to convert VMs from XenServer to KVM and import into OpenStack. The two 'clouds' are separate hardware, over the internet - so i would like to avoid shared storage between them if that's possible, however if it makes it too complex we can sling up a VPN between. 

Your sendmail.mc looks ok. Apart from stopping and starting sendmail again (which I would guess you have already done). I would look at the actual configuration file which would be called sendmail.cf. Make sure you see the lines: 

You can also add just '%' and it would work for any host, its like a wildcard. If you can't even get into the database to make the above changes, then you should change your hostname to localhost from mydomain.com. Your allowed connections to the database should at the very least be the localhost ip 127.0.0.1 UPDATE: 

Get rid of it. Thats is not an openvpn parameter. UPDATE Yes, I see you needed to add the client parameter as well. I am in the habit of setting up OpenVPN between networks with static keys and IP addresses. I never need the client parameter in those cases. 

here you create another scheduler for the virtual interface and a class with a rule defining a rate of parameter $2, which is ingress, in kilobits: 

You could either have a firewall rule blocking access or Your /copos directory does not have full permissions. You should be able to figure it out by doing a: 

But to answer your question, I have seen this behavior many times (at both the ISP level or the individual router level). If I read you correctly, your existing sessions still work during the "incident" but new ones do not. This is caused by either your ISP placing a limit on the amount of sessions an end user can establish, or because the router simply choked on the high amount of sessions established. It could also be the ISP's modem/router having a limit on the amount of sessions per IP. That would explain why your 'computer 2' with another IP continues to work fine. ISPs place these session limits to prevent malware like bots/trojans/viruses spreading like wildfire and bringing a network to its knees. Just imagine a home user with a 50Mbps connection opening hundreds of thousands of sessions all over the place. Then multiply that by X users and you see how this cascades down into total chaos. I suggest you retest under a more controlled environment. Maybe just one PC and in safe mode to prevent any strange software from loading that is opening the sessions (maybe a bot/virus/trojan). Then put a permanent ping (no -c or -w). I would be pretty confident that you would not see the loss of connectivity every 15 minutes. UPDATE Here is a new test for you to narrow down the issue. Since you say the failure lasts 40 seconds, it should give you plenty of time to do the following. As soon as the failure is detected, change the wan IP of your router (from 192.168.0.10 to maybe 192.168.0.20). If the connectivity is restored, it will prove to you the ISPs modem is limiting sessions based on IP. 

I believe this is simply a case of a bad idea using RAID 5 in this situation, but I would like to see if I have missed something else. Hardware: 

Ubuntu default guided partition layout, non LVM, completely unmodified. Confirmation of partitions, including ESP is present. 

Symptoms When high disk write access occurs (when a vm is being spawned for instance), the OS and subsequently virtual machines experience high i/o wait, to the point of becoming unresponsive, the OS becomes very laggy. Normal i/o is about 10MB/sec read or write (according to iotop). When simulating using dd: 

We're in the process of migrating Windows Server virtual machines between cloud infrastructure. In certain circumstances (HyperV Gen2 UEFI) servers, the system will fail to boot unless we install the storage driver ready for post migration boot up. While this works when doing it offline using (eg the disk attached to another server and injecting the driver) : 

I have a failed drive in a FreeNas server hosted at OVH. I need to get the drive swapped, but i'm extremely conscious of them pulling the wrong drive. FreeNas isn't reporting any serial numbers in it's GUI. I have done the below so far, I don't know how to either get the drive serial, or better yet blink the LED? 

I have some public facing DNS servers which came under attack from what appears to be a UDP based DDOS to port 53. The servers are CentOS 7 virtualised running on OpenStack Hypervisors (Ubuntu 16.04). The attack filled the conntrack table of the hypervisor causing connectivity disruption to all other neighbours also. I'm looking to tweak the conntrack settings, if possible, that an attack on one instance has less chance of affecting others, perhaps increasing net.netfilter.nf_conntrack_max to an extreme value? This was the specific error: These are the current conntrack settings on the hypervisor: 

I have a Windows Server 2012 running on Openstack which is hanging at the logo screen on boot. I'm trying to get the boot menu up (F8) however due to the speed the instance boots I can't get the F8 in quick enough. As the Spice client disconnects every time the instance is hard rebooted, by the time i refresh the console, it's already back at the hanging screen. Is there a way to boot it directly into the boot menu / safe mode, or delay the post so i can get the console up and start tapping F8? 

This might already be what your doing but, here is a thought. If the contents of sample are only to be accessed via FTP then you can move that directory outside the web server's document root and use your 0775 with owner=user and group=nobody plan. The php script will be able to write, user will be able to use FTP and the outside world will not be able to get at sample through the web server. 

I'm assuming, possibly incorrectly that your users are sitting in from of the windows boxes and need access to some linux gui applications. VNC works very nicely. Most Linux distributions have a the server sides vnc x-server included these days. Get a compatible windows vnc client and you should be good. If you need some additional security look for the ssl-enabled vnc stacks that are starting to crop up. Getting a windows side xserver like the one in cygwin will also works but is more complicated and will be much harder to explain to windows users. If your users are already linux savy this won't be to bad, otherwise go with vnc. 

About serving static files. Yes, you can use a lighter web server to do this. But, before you go to the effort be sure that it will do you any good. Is apache really using resources you need elsewhere? Maybe just configure apache to not start quite so many child processes. Be sure the added complication will pay for itself, because down the road it will almost assuredly confuse somebody when they try to figure out how everything is working. 

Be sure to try generating lots of different types of failed login attempts. Try from X (gdm or kdm or xdm), try from the console, try from ssh, try from sudo and try from su. Different subsystems can (and will) be configured different ways. It's not uncommon for ssh to be configured to use an internal login command that cuts around the /var/log/btmp business. Try the command as well. You might look in /var/log/secure to see if your failed login attempts are being stored there. But, I'm afraid I don't know the structure of the Debian log directory. Try on anything in the /var/log directory. It's quite likely that ssh is logging something someplace.