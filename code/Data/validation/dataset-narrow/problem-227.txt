You just need to optimize for InnoDB environment. is still the key for performance. And beacuse your slave server is weak than master, you might need to deal with replication lag. (i.e. Write to master, but cannot read data from slave immediately) 

So in SQL92's definition of , it must preclude P1, P2, and support for atomicity, no update lost. A5B (Write Skew) On the other side, A5B (Write Skew) is defined in A Critique of ANSI SQL Isolation Levels: 

This will reduce production downtime to minimal. I have done this on a 100GB table before (but without Foreign Keys) and it worked very well. 

In short Use to get more information. Long version This means your taking lots of time. There are serveral issues might cause it: 

Is preclude A5B (Write Skew) ? In the later paper, it claims will preclude A5B (Write Skew) in Table 4. Isolation Types Characterized by Possible Anomalies Allowed., and I am not conviced. Any thought? 

Yes, you should shutdown down the slave parts (including web & database servers). However, you can use GeoDNS + health check to redirect all users to the master site to avoid this issue you mentioned (I guess you have done this already). This should be a reasonable solution for business. 

This design looks like that you stored log data into MySQL, and you want to do something analytic on it. I would suggest you to look at Cassandra (or ScyllaDB, a C++ re-written compatible solution to Cassandra) + Presto, they are all open source softwares, and they can process your SQL query parallelly and effectively. Especially because Presto's SQL language is very similiar to MySQL (because it's developed by Facebook in the beginning), it should be very easy to learn. Anyway, if you must use InnoDB, there are serveral optimizations you should do for this SQL query: 

I'm using UTL_MATCH's Jaro Winkler similarity function and it seems to be performing well. However, I would like to adjust the prefix scale according to the situation. Is this possible? Is it possible to see what the default prefix scale is? I could not find any documentation on this, but it seems that in order to be a J-W distance, it must use a prefix scale. 

I partially solved it: PHONE is actually a VARCHAR2 column. However, this doesn't at all explain why comparing in one direction versus another works or doesn't work, when I make that mistake. So I'm still very curious. 

I have this sub-procedure in a fairly large program, and it is taking hundreds of times longer than comparable sub-procedures. Is there any way I could improve efficiency here? None of this procedure's sub-procedures are absurdly slow, so I am sure that it has to do with the structure of this one. This program looks through a table containing hierarchies of doctors. If it finds a matching root doctor, it adds to that root. Otherwise, it tries to find a matching not-root doctor. I think this might be the source of the inefficiency; the fact that it's opening two very similar cursors and looping through both quite often. Additional info: UNIQUE_GOOD_MATCH is, on average, taking about four seconds. UNIQUE_PHYSICIAN has roughly 200k records, and is indexed by first and last names. It doesn't seem to me that it should be taking this long, especially when other procedures are completing at a fraction of this time. 

I've encountered this issue before and in previous cases for us the solution was to develop a system of ETL processes which bring the information over into the warehouse or reporting database. What we did was design the process to run near continuously and at the head of each iteration we would grab the current maximum datetime stamp of each table. We would then process each record that was generated or updated between the last timestamp and the current one we just retrieved then repeat this process constantly. There is a chance with this for partial data, but with the process running continuously any partial data would be resolved within a few minutes. 

SQL Server will always use all the available memory is given. As soon as something is read from disk into memory, it will stay there and never be released to optimize performance in case it needs to be read again. It is not uncommon in production environments to see SQL Server instances pegged at 95% memory utilization permanently. This is at odds with normal application use of memory which can cause some confusion. This behavior is regardless of any transaction use in your TSQL scripts and objects. EDIT: Here's an article from Brent Ozar on the topic which contains an absolutely fabulous quote: "SQL Server is using all of the memory. Period." - $URL$ More EDIT: If you want to limit the memory that it uses, the following article has instructions for updating the maximum memory setting which will place an upper limit on memory used. $URL$ 

You'll need to put a trigger on the siteArea table so that whenever it's updated or inserted, it'll call a stored procedure with an argument representing the new site area and that procedure will calculate the needed values and insert them in a table. Your question is too ambiguous for me to describe further. 

Sorry if the way I phrased that is confusing. I'm writing a program that acts kind of like a filter - it retrieves an answer to a query, counts the rows, then I want to do a slightly more narrow query. I want to do those queries until I get a count of 0, which means the query just before it is the narrowest I can get. I then use a similar version of that query, except instead of counting I actually retrieve the records. The best implementation I know of is to just write all of the different queries, put them into different statements and feed those statements into different result statements, and retrieve the counts from there. Then, I use the last one that counted > 0 and use that main query without a count, and use those results. This seems like a horrible implementation. It seems like I'm just going to be bombing the database over and over. I'd prefer to hit it once, and then narrow the resultset on the client. Any ideas? 

Will this trigger propagate? Like, if it updates a ROOT_ID for another record, will that trigger its own trigger? Further, if it does trigger that, will it use the new ROOT_ID? I want the ROOT_ID to propagate down the tree I've built. Edit: How this works is that each record has a unique ID, a parent ID, and a root ID. I basically have a tree, each member of that tree has a root_ID pointing at the unique ID of the root and a parent ID pointing at the one above it. The root's root and parent IDs are its own unique ID. in the case that a user manually changes a record to point at a new root and parent, I want all the children of that node to have the new root ID. Is there a better way to do this?