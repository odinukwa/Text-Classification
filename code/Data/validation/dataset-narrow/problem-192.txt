These block each other but not always! I can not understand this. These refer to different rows why do they block each other? 

If you have a big table that you want to partition and you have 2 candidates as the partitioning key (in the sense that 50% of the queries use one column and 50% of the queries use the other column and almost no query uses both in the same SQL query unless all the code is rewritten to do that) how can one determine which key is the best candidate to be used? Does it matter for example if the "entity" that one key represents is "fixed" and the other is constantly increasing? E.g. as an example of what I mean "growth" (taken just as an analogy of what I am asking) if you have decide between using the doctor id or the patient id as the partitioning key where we can have only so much doctors but infinite number of patients 

Also there are a bit different instructions in $URL$ but I believe things a bit changed now when flash MOS page is retired. 

INSERT ... SELECT syntax reference. Of course id field has to have unique key and your csv files has to fit in memory. MEMORY engine tables are very fast so you should have just tiny overhead to load data in two steps. As one of drawbacks they have just table level locks so their use is limited. But that does not impact this data load scenario. 

First insert/select goes from dba_objects then it is faster to insert/select from filler table itself. Now you just have to check datafile using RMAN: 

Can someone please explain why the delete shows a different number of rows than the count? I am using the same join. Are they not equivalent? 

I have noticed that even on huge tables with millions of records when I do a the index is created immediately (I get the console back in nsecs). Why is this? Doesn't it create the B-Tree with the table data at that point? I can only assume not due to the immediate return of the . But then how is the index created? On each access? 

I have a rather basic question on transactions. Assume a table with a column amount. Now we have the following code: 

Certifications are just cherry on the top. But you have to bake the cake first. They mean nothing if you do not have any real life experience. But when you have - they give you a bit of an advantage in your job interview. Also do not concentrate just on Oracle. Most DBAs are qualified in several databases. Start with MySQL, PostgreSQL. They are simplier to start but still the core principles are the same. Also it should be much easier to find some real life practice with those databases. Then continue climbing the ladder. Good luck! 

30min for 1TB is quite normal. That all writing stops is also normal if your tablespace completely run out of space. When writes has nowhere to go they have to wait for the RESIZE operation to complete. If one extends datafile while there is still space in it database I/O does not stop. Just why you extended for such huge amount? Now those +1TB will add to your RMAN backup. Of course they will compress well but still not 100% and RMAN will need time to read all those blocks. I would setup autoextend on those tablespaces: 

id is the PK of the table. These statements execute without blocking each other. Makes sense since they refer to different rows. Next: 

Let's say I have an entity which has a location as part of its attributes Now a can have N of other that are close by. So seems to be a self referencing relationship 1-N (optional) If we know before-hand which exact are close by each other what would be the best way to represent that? Since we have a self-referencing 1-N relationship I think another table would be needed In this case we would store e.g. etc to show that pizza store with id 1 has 22, 23 and 78 near by etc Now in order to get these rows back in-order I would need to create a PK and query based on that. I was wondering would an auto-increment guarantee the insertion order? Or would I need to use a float representing the distance e.g. (where 2.04 is the distance in miles) I was also thinking is there a better way than this? We know that if is close to then is also close to right? Is there a more efficient way to represent this information? I think it would suffice to store just the row to capture that is close to and that is close to . But this way we are losing the order Update: All the answers are very useful. One thing though that perhaps is not clear. I already have the order of the entities based on the distance on insert time. I.e. I have a set of ids of objects already sorted based on distance and I want to insert this set in the database in a way that I can retrieve the rows back in the order I have inserted them. I want to avoid sorting on distance on retrieve since I already had the order at insert time. These ids would be used to join to another table (they are ids) and I want to retrieve the rows in the insert order 

To import data into a different schema you need to grant role to the user. Oracle MOS Doc ID 351598.1. Or you can use SYS user: 

Maybe db_cache_size, shared_pool_size, sga_target or other memory related parameters are set to non zero? Remember that when using AMM those parameters specify minimum memory allocated for particular pool. So if sga_target is 6GB you will not be allowed to set memory_target to 4GB. Also sum of internal variables __sga_target, __db_cache_size, etc. may be more than your specified value of 4GB. If you see those symptoms you can cleanup pfile bounce Oracle with pfile and recreate spfile. In the same step you can also set to zero. 

When having a table in SQLite that has a data type, is there any implication if the length of the contained string differ significantly? 

OR do I need any transactions or locks for the DELETE vs INSERT/SELECT? It seems not but I was thinking I may be misunderstanding something Note: My question is not about transactions and their benefits.My question is if 2 statements from different processes on the same table SELECT/DELETE need for any reason to be synchronized 

What happens to a transaction when the client connection is lost. Example the client is a web application code that starts a transaction to a remote server instance. The client sends the sql code to the server and waits for the transaction to complete. What happens if the network connection fails? Does it depend on the point we are in a transaction? Is it irrelevant? Is it aborted? 

2nd step is to drop all the active connections and unfinished transactions so that node is free from user queries. And all the applications continued to work on the other SQL node. 

Also one small note for the multiple schemas approach - put different applications data into separate tablespaces. This will add just a few minutes while creating users but may save a lot of maintenance time later. Trust me. :) 

Even such query counts as scan if id field has btree index. In your case we almost all DELETE queries are running in loop with 'LIMIT 1000' or similar value. 

NOLOGGING. But as you noted not loged are only specific operations. So you have to plan and program around it. TEMPORARY TABLE. But in this case each session can only see and modify its own data. COMMIT WRITE BATCH NOWAIT. This feature was introduced in 10g and it affects redo size indirectly. Redo size is reduced because redo log buffer is not flushed immediately and less space is wasted in redo blocks. 

I read that I can have different isolation levels per connection and in the server. The default isolation level is REPEATABLE-READ. So if I have a transaction that issues a to the client connection, how does it interact with other transactions from other client connections? Do they all end up being serialized? From set transaction seems not: 

What I am doing is something like: This way I pick up each request one at a time. Then I do (the id=123 assume it is from the SELECT request). This works but I am not sure if this is another approach. What would be better? E.g. doing a LIMIT 100? My concern is that the order by is not a good idea as the data grow. Does it make an impact if I always process e.g. per 100? I still have to order by right? 

With any more complex data it is almost impossible to restore data in MySQL Cluster in one step. Usually one needs two steps: 

Now you can unconfigure and remove old machines from old cluster. Of course Oracle version on new machines has to be the same as on old machines. Or it is possible to do upgrade right away on new machines. You should test procedure of course. There are quite a few possible problems on the way. The idea is that Oracle DB does not store anything in "cluster". All the data is in datafiles, controlfiles, redo logs and spfile. Which is stored on ASM can can be mounted on another server.