Now when you need to fill the buffer all you need to do is go over each triangle (in order) and append it to your vertex buffer. You don't need to worry about the indicies as those are constant. This is very similar to how sprite particle system works (except the indicies are not constant). You can also skip filling the vertex buffer each frame by additionally passing in the , and into the vertex buffer. You can then have your vertex shader animate each individual vertex. 

Try using a 32bit index buffer instead of a 16bit one - it looks like you are running out of indicies. What you could look into is an engine with scene graph support. A scene graph is essentially a tree of nodes - where each node is attached to a parent via some matrix transform/translation (sound familiar?). If you are not allowed to use middleware a scene graph is somewhat trivial to implement; and there is a lot of reference code out there (e.g. CodePlex). 

Notice that the contributes to 50% of the value and the contributes 50% of the value. Taken slightly further: 

There are plenty of examples on the XNA creators club on how to get started with making games. In fact I remember them giving you the link while you are downloading XNA to the samples: rendering this question quite lazy indeed. The XNA community project on CodePlex also has a slew of examples. 

You need to remember that C++ is a multi-paradigmatic (OOP, functional, procedural, ..) language and you should use the programming paradigm that best solves your current issue. OOP doesn't lend itself well to this problem. In OOP you think about single objects in isolation (concept of "a tile"). But most of your algorithms will operate on a whole collection of tiles, not a single tile. 

Just turning a single water tile into ice for example is a special case then. The other way around, using a function that sets a single water tile to an ice tile does not scale well at all however. We can assume for example that most tiles on the map will not have some special script to execute (e.g. trap tiles). If you think about a single tile in isolation, then create a 2D array of tiles, every tile will have this functionality. If you think about a whole collection of tiles instead, you can have some separate container that has the scripts to execute only for tiles that actually do require this functionality (which will probably be few in comparison). This saves memory and performance (glampert mentioned better use of the CPU cache). It also makes your algorithms simpler and gives other advantages such as much easier and more efficient multi-threading. OOP has its place, but in this case even people who use OOP as a silver bullet will tell you to scale it down (advice such as composition over inheritance, have a Map class but no Tile class etc). 

How would you test that you have done things correctly and with the correct mindset? Insert a in your game loop and everything should still move and react the same way as the full framerate. Finally, please keep well away from 3D (and thereforce Quaternions and Matrices) until you have a good amount of experience with making 2D games work. I would venture to say that quite a few game developers don't actually know how Quaternions or Matrices work - but merely know how to use them - approach them much later on (or never, 2D games are a lot of fun and can be quite successful). You don't really need to know linear algebra and so forth to do this at the basic level (but it really does help, so go to some night classes if you can). Final bonus: One thing my art teacher always told me is "don't draw what you think you see, draw what you see." The same thing applies here "don't model what you think happens (), model what happens (`object.position += velocity * time)" - at least to reasonable limits (you are not modelling a perfectly accurate system, but make something that is a good immitation). 

You could also do a full 3D fluid dynamic simulation, but at a much lower resolution than the level and use marching cubes to generate the mesh; and then NURBS to smooth it out. Alternatively: 

Have you considered instancing? You will only need one draw call as well and use less VRAM. Have you taken a serious look at the picking algorithm? If your cubes have bounding volumes in a list instead of a tree for example that would explain a difference of that magnitude. 

I'm not quite sure whether I understand you correctly, but you can assign different buffer objects to different attributes. The buffer object currently bound while calling glVertexAttribPointer will be used as the data source: 

Instead of using a blocking function such as getline, you will want to get input without blocking. On Windows, you may implement the message loop and check for keyboard events. You then add the pressed key to your input buffer. When the user presses return you evaluate the input just as you do right now. When the user presses delete, you move back one position in your input buffer, etc. Wikipedia article about the message loop: $URL$ 

In your call to glViewport you are specifying less pixels in width and height. Your projection matrix is defined in terms of pixels. That's why you will have to calculate a new projection matrix after a glViewport call. 

The gl*Pointer calls will store the currently bound buffer object and use it as the data source no matter what buffer object is bound during glDrawArrays and other draw calls. So as long as you unbind the buffer object before you call glVertexAttribPointer it will interpret its source argument as an absolute address instead of an offset into a buffer object. EDIT: apparently it used to be possible because the extension was based on the APPLE VAO extension, but isn't anymore. Refer to the revision history in $URL$ I guess nowadays you should really just use streaming with buffer objects in a double buffer setup. 

Zacharmarz briefly touched on it in his comment; however, it's less about performance and more about configuration. In the first example the sampler state is left uninitialized and works just like any other parameter (e.g. your ) - this would allow your game to change the sampler at runtime. Conversely your shader won't work if you don't initialize the sampler. Most often you will see this syntax in conjunction with custom shaders as they would allow the to provide crucial configuration. In the second example the sampler state is 'hardcoded' into the shader. This means that once the shader is compiled it's set in stone - your game can't change it if it needs to. Both techniques are just as viable as the other - it all depends on how you intend to use the shader. If you need the sampler configurable use the first, if you require that the sampler is always in a predictable state use the second. It is much more likely that you will be using the second method. 

True 3D positional sound is entirely possible, however, not using the simple effects at your disposal. You need to be well versed in digital signal processing and a sub-field of that called head relational transfer function. HRTF basically seeks to emulate how sound is transferred through the air and your body, neck, ear lobes and head into your ears (your brain uses these subtle hints in order to position sound in 3-dimensional space). This works because, with headphones, the sound is delivered directly into your auditory canal: skipping any other mediums (such as your head). 5.1/7.1 headphones (which are complete snakeoil) might actually hinder HRTF because they introduce your outer ear into the equation. Unfortunately HRTF is prohibitively expensive to simulate. For example, I evaluated MyEars and while my machine (Intel Core2 Extreme) could cope with with the samples running alone (and yes, it was extremely convincing) I couldn't use it in games. Mid-range sound cards (such as Creative X-Fi's) have HRTF processors and this will give you true 3D positional sound in games that support it (DirectSound in Windows 7 dropped support for hardware acceleration, so you will need to use OpenAL). For a good example of what HRTF can actually achieve, this recording was done with a modelled head with embedded microphones (it isn't simulated HRTF, it's recorded): once again it will only work with headphones. If you wish to use 3D positional sound simply use OpenAL - don't bother trying to roll it yourself. Some more examples (made using simulated HRTF): 

What you're doing may not be the most efficient, but it is still quite efficient, simple, flexible and commonly done. A more efficient, but code-wise more complex and less flexible solution would be to use different draw `buckets' into which you put your drawables depending on the required state, e.g. you separate opaque from transparent meshes. With your approach I suggest packing the `draw description' into an integer. There is also no need for a tuple type (DrawDesc + geometry vector), just pack the geometry id right into that integer as well (what mesh needs to be drawn could be considered part of the DrawDesc, right?). You should put the most expensive state changes into the higher bits and the least expensive ones into the lower bits. Then you can simply sort your data container using this number (simple "greater than" check, no branches, no structs). So you will get a scheme roughly like this: 

TL;DR: SDL and CEGUI probably offer both Framework and static library versions. I suggest you use the static library for your application. 

You seem to be confused about the link process in general, so sorry if I go a bit on a tangent first. You need to differentiate between object files, static libraries and dynamic libraries. Apple adds "Frameworks" to the mix in OSX. They are just one possible way of adding foreign code into your executable. Roughly speaking: 

General Thoughts This is a problem that is best described using general relativity - take for instance the fact that you can see the effects of bullet time by just looking through your telescope at a satellite (the cause of the infamous neutrino travelling faster than light problem) - equations that are solvable exist under these conditions; meaning it's completely possible. Your game will need to be able to handle separate frames of reference - as far as I know the net effect of this would be ('player' being the person with bullet time active): 

Find the edges involved on the object we are colliding with. From get the vertices from those edges. Sum the normals of those vertices. Turn the resulting vector into a unit vector (normalise). 

From the deferred rendering articles/tutorials I read it's a limitation in XNA (likely because the XBox 360 probably discards the depth buffer when you change render targets, hence they apply the limitation to Windows as well). What you need to do is use MRT (Multiple Render Targets) and write/test the depth against another that is something along the lines of . When you have finished 'compositing' your depth buffer write it out to the real depth buffer using a screen quad and associated pixel shader. The first article in J.Coluna's (quite brilliant) series on LPP describes the problem briefly - and his code should provide a nice reference for working around the issue. 

In case of pre-installed, system-wide Frameworks (e.g. Cocoa) you may assume that they exist on the target machine and just link them, your user will also have them. SDL and CEGUI are not part of the OS X base system, so it is unlikely that your user installed them system-wide. Therefore you should add such libraries/Frameworks to your App Bundle and link to those local copies, or the application will crash on your users' machines at runtime. Even if the user installed those Frameworks (unlikely), then the version or configuration might not match. Many developers constantly break the ABI of their library during version updates (due to lack of knowledge, care or use of an ABI-unstable language such as C++). But even if that is not the case the configuration options used during compilation on your users' machines might bite you (e.g. you need GIF image support, but their SDL library was compiled without it). This is called "DLL Hell". So what to use? People are very religious about static vs dynamic libraries/Frameworks, so you will see very heated debates about this question on forums. In a perfect world we would all use the same library versions and configurations, the dynamic library would only be loaded once and we could all share their memory. In reality this is not the case, and how many of your running applications are using SDL and CEGUI right now anyway? Static libraries also offer the aforementioned optimization possibilities. So my rough suggestion to you would be: