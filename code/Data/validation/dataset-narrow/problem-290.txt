What Do You Do if You Find The Table Empty? Again, I am pretty sure that an empty table here is an indication that you, or some monitoring tool or process you are running emptied the table. Or possibly that there was an issue with a portion of the installation. If you find this table empty, there should be a file in your directory called This file creates some system tables and populates them. If you search for you'll see the create and insert script for this table. The reelinsert script (in SQL Server 2012 RTM) looks like this: 

I would say the best answer here is to just drop/recreate as often as you like - Only script out the permissions and run that. You can run the permissions script as many times as you like, it won't error. Trying to come up with an effective, safe and reliable hashing methodology to only modify the affected procedures isn't really necessary when you jut drop and recreate and run your permission script. You'll find a lot of shops that do deployments that way. 

A Final Word I'm not sure why you are using this table. I would suggest that you read up on each of the system functions above in books online. Some of them have notes of warning about various conditions where they can return incorrect data. For instance the note for talks about the data type and a warning that if it accumulates more than 49 days of time, it is no longer accurate or valid. Same with some of the other counters. I have actually never run on an instance of mine. It wasn't until this question that I dug deeper and learned about what it is and what it does. It looks interesting, it provides data on a host of counters (you can learn about each of those functions above here) that look to provide some useful data at first blush. But then when you look deeper, most of the information grabbed is probably best collected in other ways. For most of the information, I would look at a combination of perfmon counters, DMVs and virtual file stats. The counter is interesting but I can't really think of a useful case for the information presented that way. I want to get my disk write errors () and alerts as they happen and not worry about a cumulative count so much. Looking at total connections since a restart could be interesting and if you didn't want to track this information through perfmon or repeated queries at DMVs and adding it, that could be helpful. 

So basically says "if a divide by zero error happens or an arithmetic overflow happens abort the query" This is usually desirable behavior and is the default instance wide setting. If this causes issues with your vendor's queries, I would say that they may have been suffering from some coding issues to begin with. I would ask them for more details on why they are concerned here. Of all of the rules of indexed views, I would call this and many of the set options rules the least controversial. This would have to be set in the connections that interact with the view. So you would want to work with the vendor and really try and understand their reasoning and try and get them to commit to what they are thinking on the big disagreement here. That said - Indexed views are a bit of a big deal. They have other rules and they can impact the application and assumptions the vendor's developers had when building and performance testing. You should really have a conversation with them about the business problem you are trying to solve through indexed views and get them involved in the conversation about how to solve the problem. 

For your specific question - Yes. I think looking at , as you are doing is a good way to see which tables have the most reads or writes or the highest throughput of reads/writes. And I would say that looking at the returned data is good to look at in three angles. Look at the number of operations (reads or writes), look at the stalls and stall time so you can see who is waiting on IO stalls the most and look at the total bytes transferred. I would probably look at the total size last and focus on IO Operations/second IOPs number first for what you are looking for here. I think you should also look at and look at the amount of times your tables and indexes are written to () or read from (, , and ) to get an idea of the distribution. Steven brings up a good idea of looking to see the buffers as well. For The WRITELOG Waits I think you are heading in a good direction here to see which databases are your busiest in terms of writes. That said I would consider taking a look at a script like sp_whoisactive that Adam Machanic wrote. You can periodically run this and collect the results to a table and you can actually see which statements of yours are the ones most waiting on WRITELOG. I would argue that you may save some time in tracking down the read/write ratios of your databases (though this is great information to know and track) in this case and you can dive into a couple of other places to start. Presuming you are seeing significant WRITELOG waits and suffering issues from it (You can see these waits happening and not have them be a problem. You should ask what kind of averages you are seeing, what frequency you see the counter and if there are symptoms - are queries taking longer than expected in an app? Are users complaining? Or are you just being proactive), you could, for instance: 

Have a smoke test / check to verify all came over well. Change compatibility mode to SQL Server 2016 on all databases if testing proved that out to work. I see this getting missed a lot. Make sure maintenance and backups are in place. Run a post migration DBCC CHECKDB on the databases. Make sure your jobs are running. Configure monitoring and alerting so you don't miss alerts about this server Once you are satisfied with a limited test, turn the old off and make a CNAME change. 

You have a few options. There are third party tools that can help simulate a load against your database. There are open simple utilities like SQL Query Stress which can run whatever queries you supply it in whatever frequency you wish. There is also the combination of running server side traces and replaying those traces. This can be as simple as gathering scripts from a trace (as Dan rightly pointed out above don't use the Profiler GUI to capture events, but capture the trace to a file using server side trace) and then replaying the workload if you more care about individual query performance if you care more about individual query performance and less about concurrency. If you need to dig into concurrency and see the workload in real time from multiple angles - you would want to look at using the Distributed Replay features. This allows you to point to a trace in any version of SQL Server back to SQL Server 2005 and run it against any version of SQL Server, also back to SQL Server 2005. Jonathan Kehayias has a step by step walk through of how to setup Distributed Replay. This would allow you to configure the replay from multiple clients distributing the load - a single profiler replay would never allow you to scale. (Please note - you'll see the tool is a SQL Server 2012 tool. Don't let that scare you, the tool came out then, but can deal with your SQL Server 2008 R2 environment) All this said - there is complexity to this either way. Taking a trace for replay has some requirements and steps. Replaying it also has some steps to take to make sure you are starting from the same starting point, etc. This complexity is worth understanding, it can help perform upgrade readiness tests, help understand hardware changes on total application, etc. You could use the RML utilities to analyze the before and after traces also which can help show you how things improved or didn't after. If you just want to see if rebuilding indexes made any changes, you can do simpler tests also. You could take a large repeatable insert and run only that before and after. You could take a few queries with high scans and a few with high seeks and run that before and after. I would make sure you still rebuild statistics either way, though, since rebuilding your indexes does rebuild the index level statistics and that sometimes gives more of a benefit than rebuilding indexes and people attribute the benefit to the index rebuilds rather than the index statistics that got rebuilt. I tend to fall into the "rebuild" camp more often than not, but hardware changes in IO performance, workload patterns, etc all make that not a hard and fast rule necessarily for me. 

There really isn't a supported way to do what you are asking. You can use a backup/restore but that would be to a new database. You could try a filtered article where you only grab data after a certain create date. Or you could use a manual process such as an SSIS package or T-SQL Scripts to stage the schema and data. But the snapshot is more or less a required item. I would probably do a single article snapshot or maybe look to filter if you can filter create_date > getdate() and then add in rows manually after if you have such a column, or an identity column, etc. A few links: About manually preparing. About using backups or other methods. About filtering. 

How Do I see the full Error Message? My client just has a generic error message that says logon failed with no real information/state information. What if there is no error in the error log, but I still can't connect? How Do I read the logon failure (Error 18456) error messages? How Do I troubleshoot the various conditions found in the error messages? What is Kerberos, why do I care and how do I fix errors related to it? 

Just deleted my previous answer as I started a proposed canonical question for this type of question. In case that flagging doesn't work, a short answer for you here. You have a couple options. Big "Warning" here is - Shrinking isn't always a good thing, it fragments your indexes and takes some time. So proceed cautiously. 

Yes. Now my disclaimer is I'm not your licensing rep or Microsoft so you should check with them on questions. But the verbiage in the various license guides is typically "A Server running SQL Server - or any of its core components (SSRS, SSAS, SSIS)". So if you are using SSIS on a server standalone, by the licensing guidelines you are running one of the core components on another server and that should be licensed separately from any other machine running a core component. Depending on the load of your SSIS server, I have typically found that in many installations, just running the SSIS server on an already licensed machine - be it your warehouse server or a staging server or even your main production server works. When you get into complex SSIS workloads with lots of moving parts and pieces and high overhead, you may start considering a separate processing server that perhaps also contains the DB instance for your SSIS logging and reporting type of environment. I'd say that I have to consider that sort of separation for performance rarely lately, though I have in the past. Microsoft publishes several detailed and overview license guides. This one here is an example for SQL Server 2012.