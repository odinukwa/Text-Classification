Right now I just have an e-mail on my domain that I use for customer service and I answer all of the e-mails myself. However, my business has grown enough that I'm losing a lot of time answering all of these e-mails that others could be answering. I want to be able to give someone else an e-mail address to use for customer service e-mails, but allow them to pass off e-mails that he can't deal with. It wouldn't work for them to just forward the e-mail to me because it would then be from them and would make it inconvenient when trying to reply to the customer. I'm on a server with WHM/Cpanel, what would be the best way of doing this? 

We are in the process of moving from AWS where we have a highly available system setup using EC2's auto scaling feature. However, we aren't using this to change the size of the pool based on resource usage, we are simply using it to spin up new instances when one of them fails or becomes unresponsive. Without this auto scaling feature on other cloud providers (we are specifically looking at DigitalOcean, but it should apply anywhere), what are some options to achieve this setup? My first thought was to create an instance that monitors the others, but then that server becomes a single point of failure. Are there any services or established patterns to accomplish this whether automated or writing some scripts to the API without creating a single point of failure? 

I'm testing out a new CDN service ($URL$ and I'm trying to figure out if it is giving much of a gain. I have a very fast internet connection so I can't see any increase in speed (though I realize since I am fairly close to my server I shouldn't see a big gain anyway compared to international users), but I'd like to calculate how much gain there actually is. Is there anyway to do this? 

I was just looking around cPanel and noticed that the default mail account is taking up over a gigabyte of storage. I opened it up in webmail and saw there are over 200,000 spam e-mails sent to randomly generated e-mails like name@mydomain.com. I am unable to delete them because the webmail installed on cPanel can't handle such a large number of e-mails and won't load the inbox. How do I clear out these e-mails? If it matters, I am running CentOS 5.5 on a dedicated box, but I'd rather not have to delete from the command line if at all possible. 

I'm not sure what I should be changing to get this down under my 12G of RAM. I have a database with 110 tables, 10,000,000 rows (growing fairly quickly), and an average of 250 users online. My my.cnf has looks like: 

The issue turned out to be painfully obvious once I slept on it and took a second look (aren't they always). It was as simple as there being a typo in the . Since the IP was incorrect, no messages were going through on either server. I would have thought there'd be some error message for this, but everything started working 100% once this was fixed. 

I'm setting up my first website on Amazon EC2, and I'm trying to decide which distro to use. I've used Redhat and CentOS in the past, but I have no bias towards any system, I just want to use whatever is best (I also have had partially-managed servers in the past, so I haven't done too much server administration until recently). The website is just a web app written in PHP and MongoDB. I like the idea of having a lightweight OS that is described for Amazon Linux, but I worry that it could suffer in compatibility/updates compared to Ubuntu or other options that have teams focused exclusively on a server OS. Any advice? 

The DocumentRoot is what is confusing me. If ape.domain.com is supposed to point to the APE Server at port 443, what would the DocumentRoot be? I have it set to the folder that stores the JS, but it just shows those files in the browser when I try to connect. I tried changing httpd.conf's "Listen" to just the IP of the site and no the IP of the subdomain, but I still didn't see any difference, and this caused httpd to crash about every 30 seconds. I'm hoping I am just making some obvious mistake that I am overlooking. 

This is a purely theoretical question, but what if I had a site that would normally only get a couple thousand hits a day, but for a few days each month that could shoot to several hundred thousand or even several million hits over the period of 1-3 days. The site would be pretty bare-bones (as in, 2-3 total pages with 1-2 max MySQL queries on each page and some PHP), so bandwidth wouldn't be the issue, but sheer volume taking down the site would be the main concern. Cloud hosting seems like the best way to go, but would something like Amazon EC2, MediaTemple, or something else be the right choice in this case? 

I've been using MyISAM exclusively for several years now and know the ins-and-outs pretty well of how to optimize it, but I've just recently started using InnoDB for some of my tables and don't know that much about it. What are some general tips to help improve the performance of these InnoDB tables (they were converted from MyISAM and have anywhere from 100k - 2M rows and most won't use transactions). 

We ended up writing our own solution to somewhat mimic the behavior in EC2. We called it healthcare.js and open-sourced it at $URL$ Essentially, it uses the DigitalOcean API and tags for discovery, and then uses democracy.js to monitor which servers are running. This allows for a fully distributed self-healing system that will kill/re-build servers based on the passed server configs. 

I've got a single dedicated server with a MongoDB database of around 10GB. I need to do daily backups, but I can't have downtime with the database. Is it possible to use a replica set on a single disk (with 2 instances of mongod running on different ports), and simply take the secondary one offline and backup the data files to an offsite storage such as S3 (journaling is turned on)? Or would using master/slave be better than a replica set? Is this viable, and if so, what potential problems could I have? If not, how do I conceptualize this to work? 

I just converted a MyISAM table to InnoDB with around 1.4 million rows. When I converted it to InnoDB, it now shows -1.4 million rows. The table still works as expected, but why does it shows negative in the rows column? 

This is a completely hypothetical question that myself and a colleague have been wondering/debating about, and we thought some of the knowledgeable people at SF could shed some light. Say, for example, you have an ad server that delivers around 10 million ad impressions per day. Utilizing the linux, nginx, memcached, MongoDB (or similar NoSQL database), highly optimized code, and a CDN, could it be reasonable to assume that a dedicated server could handle this all by itself (we are talking about a fairly good sized dedi with anywhere from 10-16GB of RAM and a 12 core xeon processor)? The main activities here would be serving the static banner ads, and doing some quick calls to the database to select an ad and update impression and click stats. Any thoughts? 

For the past few weeks I've been getting more and more reports about lag on one of my sites. I've finally been experiencing it first hand over the last week, but I haven't been able to pinpoint the problem. The server load is never higher than about 0.5 out of 16 cores, and the memory usage tops out at around 12-13%. The issue isn't the database as the lag can happen on static resources. About 1 out of 10 page views gets a 502 error. About 1 in 5 pages takes 5-20 seconds to load. When looking at Chrome's network tab, it shows "waiting" for almost all of that time. I rebooted the server last night, and it seemed okay for a few hours, but less than 12 hours later it was back to the normal lag issues. Anyone have any tips on where I can look to try and figure out the problem? 

It turns out the issue was with OOMkiller killing the mongod process. As noted in the MongoDB production notes ($URL$ 

I've got a site with a LAMP stack and I'm looking at moving to a new hosting provider (currently on a dedicated server and looking to move to a cloud provider). What is the best/most reliable way to compare performance of my site on the different providers? Obviously I would spin up an instance and transfer of a copy of my site, but are there any tools or specific things I should be looking at to compare performance? 

How do I setup MongoDB so that it will start back up automatically after a server reboot? This is on CentOS 5.5. I start it with the following command, but if the server reboots, I have to login to SSH and run that again to get it going (not that big of a deal really, but would be nice to know that this isn't necessary if I don't have access to the server). 

I'm trying to setup keepalived + HAProxy as a redundant load balancer on an EC2 VPC (yes, I know that ELB is an option). I believe we have things configured correctly, but killing the master server doesn't seem to failover. Server A Config: 

I installed MongoDB on Ubuntu 14.04 as per the documentation. It runs and works fine, except that it mysteriously stops every few hours and requires a manual restart with to get going again. There's nothing in the mongod.log when this happens. In addition, when I restart and then check status with , it gives me event hough the application is able to connect to it. I've been searching around on how to debug this, but am stuck at this point. 

I'm comparing different cloud hosting offerings and am trying to do detailed cost comparisons. A lot of the other providers have most everything bundled, like bandwidth, but Amazon's is all scattered. Since I am only used to using dedicated servers, I'm not 100% sure on everything I would be paying for with EC2 (for example if I had 2 small nodes). I know that no bandwidth is included with an instance, so that is an extra $0.15/GB on top of the hourly instance cost. If I understand correctly, I need an elastic IP per instance ($0.01/hr). From what I gather it is also important to have EBS and possibly the elastic load balancing (however I'm not the exact needs for these so I'm not sure how to price them)? Any other costs involved? 

I am working on a new project that will be hosted with Amazon EC2. Speed of requests is of utmost importance, and the low memory footprint of Nginx is very compelling. I've always used Apache, but if significant advantages can be had with Nginx then I would like to switch to that. I see that a lot of people use Apache + Nginx, but are there any disadvantages as of 2011 to using Nginx with PHP-FPM to serve both static and dynamic content? I know that .htaccess is no longer an option with Nginx, but I also understand that there are ways around this. Basically, will you get the best performance out of Apache + Nginx, or just Nginx?