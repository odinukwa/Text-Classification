Aside: if you're seeing high CPU as a result of a new OR/M, I'd look at something called implicit conversion. What happens is that on string data types (, , , ), you might be seeing Unicode vs ASCII conversions between the database and your application code. The best way to look for this is to make sure your ORM matches the string data types correctly. If it's in the database, make sure the ORM doesn't submit parameters as . More reading here: Convert Implicit and the related performance issues with SQL Server. 

will tell the query where the is, but then you have to use the and operators respectively, to select that portion of the string. In the case of , it's straightforward, and you were almost there in your logic, if not in the implementation. For the , you need to subtract the position of the from full length of the string (that's what is doing). 

This is the cumulative total of all non-clustered indexes (keeping in mind that each of them contains a copy of the clustered index). 

Looking at your command, it appears you're missing credentials, so I'd give this a try (I've shortened the other parameters as well, and removed the TCP port): 

The SQL Server parser will not even parse the second statement. is not recognised as a valid table name or view. You'll get an error like this: 

This is probably a better option. As Kin suggested in the comment on your question, NUMA imbalance would be a factor by trying to balance CPU affinity across physical cores. 

SQL Server leaves out empty space when it backs up a database. If you want to know the size of the actual data and log files inside the backup file, you need to view the header of the file. From inside Management Studio, run the following query (you should fill in the correct value to the path of the backup file): 

If you downloaded SQL Server 2008 R2, the ISO you downloaded would begin with . Spanish version would start with . If you're installing off media like DVD, or just have the installer folder, you'll be looking for or similar, in the root of that folder. is the language code, and is the code. If you see that, you'll be installing that language. 

Without knowing too much about your environment, as you did not give us very much to go on, you may benefit from more RAM. For instance, your database is 400GB in size, and data access patterns require large scans to access lots of data. On the other hand, your indexes might need attention. For instance, your database is 400GB in size, and data access patterns require large scans, when in fact you only need to read one or two columns in that table. Instead of worrying about buying hardware, perhaps it would benefit you to start with the basics: 

According to the official documentation for on 2017 CTP 2.1, the command to export data and schema is as follows: 

Nothing will happen to your SQL Server installation if you change the Windows system locale, because it was installed with the specific collation . 

While this is possible to do with the Dedicated Admin Connection and a modified version of , it is unsupported. I would rather spend this time to restore properly on a new instance (on Developer Edition under a VM makes the most sense), and then access it from there. 

I'd consolidate the log files first, before worrying about the right size to set the log. To do that, you would need to use the clause in a operation, so that the second file can be emptied out and then deleted. Once you are back to a single log file, you can then shrink right back down to zero (to remove any fragmentation), and then resize it appropriately, to ensure good distribution of Virtual Log Files (VLFs) inside the transaction log file. There's no way to know what the "right" size is for your log file until you run a normal workload against the database, and see how big it gets. Once you know that, you shrink it down again, and then resize back to that value. Make sure you set a fixed growth setting appropriate for the performance of your underlying I/O subsystem, and not a percentage file growth. You might want to set it to 100 MB, or 1 GB. It's entirely up to you. Regarding , all it does is free any inactive portion, at the end of the transaction log file, back to the operating system. No data inside the log is moved. If there's no inactive portion of the log at the end of the file, no space will be released. The Simple Recovery Model does not allow point-in-time recovery, but I think you know that. If you want to do point in time recovery, you'll need the Full Recovery Model, and perform regular transaction log backups. 

There is no way to recover a transactionally-consistent database from only half of the backup media set. The problem is that, while a full backup contains the data file, there is also a portion of the transaction log, at the end of the backup file, to ensure the restored database is transactionally consistent. Even if you did manage to recreate portions of the MDF file from this backup, you would still be missing the transaction log that rolls back in-flight transactions and rolls forward completed transactions that took place after the backup started. Making matters worse, if the striped backup is written out as interleaved files (which I believe it is), you'd have one out of every two 8KB data pages, and a corrupted log at the end. There's no functional way of establishing any consistency with this kind of recovery. 

On SQL Server 2000, if you rebuilt your non-unique clustered index, all non-clustered indexes based on that non-unique clustered index would be automatically rebuilt. That's because the uniquifier changed during the rebuild. This is not the case on SQL Server 2005 and higher, or if they are just being reorganized. For index and statistics maintenance automation, I recommend using Ola Hallengren's Maintenance Solution, to manage this for you. You might also use Minionware's Reindex, also free. Side note: updating statistics regularly is (in my experience) more beneficial from a time and resource point of view than rebuilding indexes. Thus, I would do daily statistics updates, and defer heavy index maintenance to a longer schedule (weekly or monthly) if required. The level of fragmentation will dictate this, because rebuilding may not be necessary. 

Do this. Add the extra column. The cost of the additional memory is nothing compared to anything else you try. 

No. SQL Server Management Studio (version 17.x anyway) is based on the Visual Studio 2015 shell. I don't believe it's removable in Visual Studio either. 

This comprises an ordered set of backup media, which can be tape, disk files, Azure Blob Storage, etc., but (and this is important) not a combination of two or more of these. So if you create a media set, your backup media in that media set must be of the same kind. On to your question about the name of the media set. According to the same article (emphasis added): 

The index is a data page like any other page in the database. It has a header, a slot array, and data in the middle that points to other data pages, and eventually gets to the leaf node where the data sits. You can read more about data pages here. Once you understand that, you can read about index pages here. 

Yes I see that fragmentation all the time with GUID columns, due to the random inserts and page splits. In your case, this could be a result of a non-standard fill factor to account for GUID fields. A clustering key should be on an ever-increasing, narrow data type, like or , and the fill factor set as close to 100 (or 0) as possible. That all said, rebuilding a clustered index due to fragmentation is a time-consuming process, and it generates a lot of transaction log, so it's generally more accepted to simply update the statistics fairly frequently. You can worry about fragmentation less frequently, say once a month. 

Maybe. SQL Server does not care if you create duplicate indexes, but just having the same number of rows does not make it a duplicate. Read more here about determining if an index is a duplicate. The reason there are 830 rows in each index, is because the table has 830 rows, so each index contains the same number of rows, each of which maps back to the row in the clustered index (depending on which column(s) are indexed). 

Don't think of it as a backup, think of it as the fastest (RTO) way to recover the database to a consistent state, at a point in time (RPO) that satisfies the business requirements. Stated more plainly: 

Those numbers in your performance tests are accurate. Theoretical throughput does not match my experience, after migrating an environment to more than ten DS13v2 instances. I get between 220MB/s and 256MB/s, depending on the type of writes, even with P30s. To answer your question, the bottleneck is the network itself, which links the drives to the VM. 

I notice a case-sensitive difference between your declaration , and from the error message. Check to see if you're not using a case-sensitive collation. 

All this considered, we can make the following observation: All things being equal, at 12 bytes per row, we would be able to fit 674 rows in a single 8KB data page. That works out to be 741,840 pages in total, for 500 million rows. No doubt, this is where your 6GB figure comes from, or more specifically, 6,077,153,280 bytes. However, there's an overhead per row, as you discovered. Summarising from Paul's Anatomy of a Record: 

You will need to convert your as well, where you have it in the string. Watch out for quotation marks, and casting back to as well. Here's a proposed solution: 

Once you've done all that and monitored it, start focusing on performance issues using an index strategy. There are DMVs built into SQL Server you can use to query missing indexes (or sp_BlitzIndex from Brent Ozar Unlimited). Using appropriate human judgment (i.e. don't madly create every index it recommends, look for ways to create covering indexes), you should find some improvements. Once again, monitor performance against your current baseline. The reason we are going to all this trouble before answering the cloud question, is that a poorly performing system on-premises will also perform poorly in the cloud. This will result in you paying more than you need to. Remote access Your main motivation for moving to the cloud is to have remote access to your database. SQL Server running on a Windows (or Linux) VM By all means, check out SQL Server running on a virtual machine. Bear in mind that you'll pay for Windows (where applicable), SQL Server, and any data drives you assign to the VM. With a 1 TB database, you'll need at least two P30 premium storage drives, which isn't cheap. Then the size of the VM must also be taken into account, because SQL Server is licensed by number of CPU cores. Platform as a Service (Azure SQL Database)