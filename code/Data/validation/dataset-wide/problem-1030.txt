The answer is It Depends on exactly how much load your are putting on SQL Server, IIS, and so forth. (SSIS? SSRS?) All of these things add up. Jonathan Keheyias has a general formula that works pretty good for me: "Reserve 1 GB of RAM for the OS, 1 GB for each 4 GB of RAM installed from 4â€“16 GB, and then 1 GB for every 8 GB RAM installed above 16 GB RAM." $URL$ So this formula would say to limit SQL Server to about 5 GB and let the other code on the server also have some memory to use. (Or add more memory and recalculate.) You will need to experiment with the memory settings until you find your balance. Note: If you get your memory use balanced correctly, most of the memory will still be in use, but better shared across your applications. 

This transfers the table from the schema to the schema. Additionally the CREATE SCHEMA command can be used to create objects within the schema. (Again, this is at the database level.) $URL$ 

Regarding running reports from an Availability Group secondary server, you will need to license that instance of SQL Server. Since the AG requirements require identical equipment, your secondary's licensing cost will reflect the primary server's licensing cost Using the secondary server strictly for High Availability/Disaster Recovery does not require licensing the secondary server since it is only in use when the primary fails. Regarding the brief spikes in CPU utilization, is it causing you a problem now? Since it is hard for you to see who is responsible, consider loading sp_whoisactive from Adam Machanic's site: $URL$ This stored procedure has numerous options to control what information you can query. Brent Ozar also has an article on using sp_whoisactive to track issues within your system, logging the output to a file for later review. $URL$ Perhaps these tools will help you find out what is happening. 

The reason that your table is growing while only making updates is that PostgreSQL is that "tuples that are deleted or obsoleted by an update are not physically removed from their table; they remain present until a VACUUM is done." Of course, while you are updating 1000 rows per second your updates are in contention with and perhaps . You should probably not be using the option. See: $URL$ "Plain VACUUM (without FULL) simply reclaims space and makes it available for re-use. This form of the command can operate in parallel with normal reading and writing of the table, as an exclusive lock is not obtained." If that is not enough to resolve your problem, perhaps you could include the and in your job being driven by your application server. Then periodically switch from updating rows to run a . EDIT: Note that 3 comments have pointed out as a process that can run continually. However, dezso mentioned that configuring it for a heavy load may not be straightforward. 

If you want to delete the code just executed, you should Continue holding the key and then press the key. I suppose it is possible that someone has created a tool to do that automatically. 

I have had to bump up the timeout seconds in some environments, so do not be afraid to try this. (It is equally simple to set it back to the previous default.) You can try to configure a longer timeout, if needed. So far 10 seconds has almost always worked for my environments, but I have gone to 20 seconds a couple of times. 

Seems to work fine. If you are on a different SQL Server it might have different behavior. Or fluentMigrator may have a problem. 

I would use your current design over creating a table for each user role. Why create a new role, when your code may have to cycle through several tables to get an answer that could have come from one place. (Opinion: You would just be making more work for yourself.) Of course, a Team, a TeamManager, and a TeamMember may all have some attributes that are related only to their individual roles. If you are facing that challenge, then a supertype-subtype design can work well for this issue. All the common data is in the supertype table, but each subtype contains data unique to that role/task/etc. (Whatever you are trying to manage.) A Microsoft Tutorial give a basic description at Lesson 5: Supertypes and Subtypes at $URL$ 

See the link to learn other details about the SQLite triggers. Of course you could write your SQLite code to delete the rows that you no longer need in your database before inserting new rows into your database. I would prefer that approach over a trigger since your are just getting rid of unwanted data. 

Are my indexes well-designed for this workload? Do my queries require scanning all of the historic data? (Apparently not, according to your comments above.) Is the overhead of maintaining quarterly databases going to buy you significant query improvements? (Especially if you will need data from more than one database.) 

Because you reverted, you can repeat the designer steps with the database in exactly the same state as the first effort. If you are not using a version that supports SNAPSHOT databases, you can do the same thing (perhaps more slowly) using BACKUP DATABASE and RESTORE DATABASE. 

Yes, you can have several versions of SQL Server involved in a replication set up. You are limited (generally speaking) to the least common denominator of functuality. That may be just fine for you, but it is something to consider. SQL Server 2xxx Express can only be a subscriber in a replication landscape. So as long as data only needs to move to the Express database you can use replication. Microsoft has documented this for SQL Server 2008 R2 at: $URL$ 

Of course, you would need to have a periodic check that would determine that apparently all 6 jobs will not finish and report failure. This little snippet of code is just a toy, not a serious production program. But perhaps it offers you an idea or two about how you might approach your problem. 

A DELETE only affects one table at a time and removes all rows that are defined in the . Notice the syntax description: 

Since you are using SQL Server local logins, they cannot be grouped. Each login is independent and each login needs a user in the database. Where you can get some saving in specifying rights is to create roles, to which you , , or rights. (Note that rights override rights.) Then use these roles to give rights to the users. This way a role contains the specifications of permissions, rather than applying those to every user individually. After this you can add the users to the roles that user needs. For example: 

If you put three instances on the VM, you will have 3 copies of SQL Server running. If you put the 3 databases into a single instance, then you have only 1 copy of SQL Server running on your VM which makes more memory, processor, etc available to your processes. Assuming no serious processing issues, I would recommend the single instance. Why add overhead unless there is some other compelling reason to do so? 

Yes, initializing a large database through a snapshot can be very slow and time consuming. If it is appropriate to your case, you should start by restoring a copy of the database to the target machine. In setting up the subscription choose "allow initialization from backup files". Because of limitations in the user interface for setting up replication, you will need to create a script instead of relying on the UI tools. There is a blog post on this at: $URL$ The post from msdn includes the code sample: 

Depending on how you change the user context from the individual login to the service login, you might find that ORIGINAL_LOGIN() is helpful. $URL$ "This function can be useful in auditing the identity of the original connecting context. Whereas functions such as SESSION_USER and CURRENT_USER return the current executing context, ORIGINAL_LOGIN returns the identity of the login that first connected to the instance of SQL Server in that session. " 

One thing to note is that is a command, not really a permission that can be granted. Which is why your failed. In the SQL Server Books Online at $URL$ has pretty much said the same thing from SQL Server 2000 until now, namely: RESTORE permissions default to members of the and fixed server roles and the owner () of the database ... members of the fixed database role do not have permissions. If the , not , seems confusing it is like this. One login owns the database, it may be or . That login maps to the dbo user and should have rights to restore the database. Inside the database, many users may be in the role, but since those are inside the database to be restored, they do not get the permission. So, you can make logins members of the fixed server role or you can make one user the owner of a database and he should then be able to do a restore of that database. To test the consequences I created a login that is not a for these tests. TEST 1: Granted the server role and the role in MyDatabase. (It is necessary that have some rights inside the database to be restored.) ... was successful. NOTE: If the restored database did not already have as a user, lost access to the database once the restore was complete since the data was restored with the contents from the backup. So, needs to exist in the source backup if this user is to retain some internal rights after the restore. TEST 2: Revoked from the server role and the role in . Made the owner of the database. 

Regarding when a log backup can start: $URL$ This says: "A new log chain starts either with the first full database backup following database creation or after switching from the simple recovery model to the full or bulk-logged recovery model." So, I still believe that this will work as outlined. (Not identical, but I have used a differential backup to cover a gap when log files were lost, so as to establish a new origin for the log backups.) (Remember my disclaimer, of course.) 

Therefore, since Service Accounts are not people, you can use as many service accounts as you can make use of: SQL Server, SQL Agent, Full Text, and so on. 

As JamesZ mentioned, the memory is intended for the SQL Server to use, particularly for the data cache. Therefore SQL Server will attempt to use everything that has been made available to the SQL Server process. (Plus some operating overhead.) If you are concerned about reserving memory for Windows Operating System and any other processes that you run on the server you can adjust your maxservermemory to a lower number. I have had good success with Jonathan Kehayias's "formula" that he posted here: How Much Memory Does My SQL Server Actually Need? The first paragraph has a good outline of how Kehayias would by default configure the Maximum Memory. Of course, this is not absolute and you will need to observe your SQL Server and determine whether more or less memory needs to be reserved. Brent Ozar has a sanity check post at: Memory Dangerously Low or Max Memory Too High So, you should not have a script to tell you that there is too little memory available. You should configure appropriately for what your system needs. Brent Ozar's post for a 512 GB Server suggests: 

Whether you use one table or two, you should be able to insert all the data needed for in a single transaction. More importantly, you should not store the password, but instead store a password hash. Here is one post on using a password hash such as SQL Server itself uses. This is using SHA_512 hashing with a 32-bit salt. $URL$ This way you never store the actual password. At the time of the creation of the password, you need to share the password with your new user. And ideally force him to change the password at first use. Each time a new password is created, you must, of course, update the hash to match the current password. Which means that you must manage all password changes, not just the initial password. EDIT Re: "how can I am able to enter employee details in employee table and generate user id / password for user and store in from separate table in one action." You must do the work in a transaction. Here is a sketchy outline using 3 stored procedures. Of course, you need to write the actual code: