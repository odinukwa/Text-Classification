You can enhance error handling with Extended Events to overcome the TRY...CATCH shortcoming you are experiencing. The design pattern involves these steps: 

From there, you could build a TSQL string of commands and execute it. (I hardcoded for the schema. Presumably this will never change.): 

2nd Attempt Here I opted for a variable to store the total number of users (instead of a sub-query). The scan count increased from 1 to 17 compared to the 1st attempt. Logical reads stayed the same. However, elapsed time dropped considerably. 

Here's a blog post about handling events with DDL triggers with some further explanation: SQL Server Event Handling: DDL Events 

If the developers have individual logins, you might consider a DDL Trigger. Here's an example for the , , and events. 

Although I don't have a local copy of the Stack Overflow database, I was able to try out a couple of queries. My thought was to get a count of users from a system catalog view (as opposed to directly getting a count of rows from the underlying table). Then get a count of rows that do (or maybe do not) match Erik's criteria, and do some simple math. I used the Stack Exchange Data Explorer (Along with and ) to test the queries. For a point of reference, here are some queries and the CPU/IO statistics: QUERY 1 

For row it means: Service1 will be available for users of type 1 and Service2 will be available for users of type 2,3 and 4 only so in register form for users of type 1 I should only show the Service1 option and for users of type 2,3 or 4 then Service2 will be showed, I need some help building a query for get that data or help to change my model with better solution to that problem, any? 

I'm running in CentOS 6.7 server. I'm trying to get queries logged and I'm doing by redirecting log to files as shown below: 

But if I want to find "how many times the user using the player Lizard has won, lost and draw" then I need to run the following queries: 

are parameters coming from PHP and that's my only problem. How do I pass a parameter to a view if it's possible? I am using MySQL 5.6 at the moment. 

Between and tables exists the same relation twice: is a FK on and also is a FK and both points to maybe here is the problem and I don't see it yet. Anyway, I leave the SQL schema in this link because is a bit large, apologies for that but and I provide the same data I've for testing purpose. What I'm doing wrong? PS: You can take a look to data on this images I leave and as I said data is shared 

This could easily be put into a job step for a SQL Agent job. I tested this on SQL Server 2012 w/ SP3. I've also run similar code in SQL 2008 R2 and SQL 2014. I don't know for sure if it works in SQL 2016--please let us know if it does not. Other Notes The commands worked when I was logged in as a [sysadmin]. It also worked if the login was not a member of [sysadmin], but was the database owner. Membership in [db_owner] alone was not sufficient. (Again, this was on SQL 2012 w/ SP3. YMMV.) 

I'm having difficulty "optimizing" indexes as a minimally logged operation. Before the index maintenance is performed, the database recovery model is switched from FULL to BULK LOGGED. Depending on the fragmentation percent, each index is the recipient of a REBUILD or a REORGANIZE (or no action is taken). After index maintenance is finished, the recovery model is reverted to FULL. One database in particular is causing me some pain. The datafiles are about 64GB (including any free space). A defrag operation bloated the log file to 38GB, until it filled the logical drive. Then the level-17 alerts started rolling in. I tried duplicating this in a test environment. Numerous attempts were made with different recovery models, index REORG vs REBUILD, different transaction isolation levels, read committed snapshot ON vs OFF, different index fragmentation levels, etc. Index REBUILDs on DBâ€™s in the FULL recovery model always bloated the t-logs. No other testing variations did, however. This was frustrating because I could not duplicate what was happening in production. What am I missing? How can I optimize indexes without bloating the log files? UPDATE 09/23/2015 Not surprisingly, Ola Hallengren's name came up soon after I posted my question. Although I don't use his scripts, I am somewhat familiar with them. For those who are interested, there is a wonderful PASS session video--Ola is the presenter. Near the 48 min mark, an audience member asks a question similar to mine. 

Following Neil and user16484 suggestions also ypercube and jynus and good practices at DB modeling, I come with this model 

But I don't know if the using that will works and that is my doubt, how do I achieve an UPDATE from SELECT? Any time I try to run that SQL on I got this error: 

Can any give me some help? EDIT Because has duplicates (a table with no restrictions at all - see image below) I need to clean up a bit the trigger before execute. 

I am working in a PHP application and some "complex" queries are starting to appear in the code. Because of the complexity I am not able to use any ORM and the only resource I have is a plain SQL and PHP MySQL native functions which I don't like. Without more here is one of the queries I want to convert into a view: 

This should return any record from default_cmvblog where title, content or keywords is like 86a7 and also status is live, right? The same for default_users should return records where email or username like 86a7 and id != 1 and active = 1, right? But I get 0 as the COUNT why? What's wrong in my query? Cheers and thanks in advance 

I have a few "gaps" in my understanding of external memory pressure. I've found quite a bit of helpful information online, including this info from SQLSkills.com. 

An Event Notification can be used to both monitor for DBCC command events and respond to them. The script below creates all the necessary objects, which I would recommend be created in a separate database. I've added comments here and there where appropriate. Most of the script can probably be left intact, except for the stored proc, which you will want to tailor for your needs. If this solution doesn't fit your needs, you can simply drop the database. 

Create and start an Extended Events Session: it will capture events, filtered primarily by SPID. Execute a statement in a block. Within (or after) the block, read the XEvents session data. Use the available data to respond to the error(s) as appropriate. Stop and drop the XEvents session. 

Here are two queries I have used to compare permissions between database users. The first shows the database roles (if any) to which the database user has membership. The second shows individual GRANT and DENY permissions. 

Before restart MySQL service I did run the following command to create each file and assign proper permissions: 

So, I have this relations on my tables (see image) and I need to understand some things. I'll enumerate what I understand and what I need from yours is to say if I'm right or not and in case I'm not give me the good answer. If is FK on and tables if I set onUpdate and onDelete to what this mean? 1- When I delete a the cascade will delete all the records on and tables? 2- When I delete a what happen with the that points to that person? In order to maintain a good table structure, data consistency and integrity, what are you're suggestions regarding this topic? 

So for example, to find "how many times the user has won, lost and draw" I need to run the following queries: 

And I need to keep repeating the same queries all the time for get those types of statistics. Is there any other approach or a better way to achieve this? I am using MySQL 5.6. 

Here's an example that can be run manually in a single batch from SSMS. Just make sure to replace all occurrences of "2016" in the script with your SPID. 

This is a sad commentary on the state of software development and deployments that rely on an RDBMS such as SQL Server. The development and/or DevOps teams should know what level of authorization is needed by applications, both at runtime and at deployment time. It's very important to make the distinction between these two. Unfortunately, development and/or DevOps teams never figure this out because {reasons}. So here you are... You've mentioned that you are familiar with fixed server roles and fixed database roles. I don't want to insult your intelligence, but I think it's worth mentioning for those that are not familiar. At the server/instance level, the CONTROL SERVER permission is a much better option than membership in . At the database level, I would never let a non-dba own a database. They connect as -- DENY permissions have no effect. You can deny permissions to a database user that is a member of -- this is also a better option than membership in . But it is still usually overkill. I generally prefer to give a database user membership in , , and . Additionally, I'll GRANT EXECUTE, CREATE SCHEMA, and VIEW DEFINITION. Based on your needs, your mileage may vary. Another thing to note: you cannot DENY permission on DBCC commands. Members of can do a lot with DBCC, and to a lesser extent, members of can too. After all of that, I've still had to give authorization to non-DBA types on a regular basis for deployments. In those scenarios, the requestor had to submit their request to a Change Advisory Board for approval. Membership in was temporary and I always set up a SQL Agent job to remove group membership after an agreed upon amount of time (usually one week). I did this enough times that I wrote re-usable code to quickly create the necessary job, job steps, schedule, etc. Moving on... Whether it's temporary or permanent, you still have to deal with non-DBAs that have elevated permissions. SQL Server has some built-in features that can help you handle events that you might not want to happen: DDL Triggers: these are a great way to handle certain SQL Server events synchronously. Maybe you just want to know an event happened. Grab the and send an alert or log the info to a table for later inspection. If you're more hands on, you can inspect the as events occur and optionally ROLLBACK the bad stuff. It's really powerful. I kept an eye on ALTER_INSTANCE, ALTER_DATABASE, DROP_DATABASE, and a raft of authorization-related events. (Note: ROLLBACK doesn't work with the ALTER_DATABASE event.) EVENT NOTIFICATIONS: some events can't be handled synchronously with DDL triggers, but they can be handled with event notifications. ROLLBACK is not an option, but you can at least grab the for post-mortem analysis. This option involves Service Broker, and the learning curve to get started is much higher than working with DDL triggers. I think it's worth it, though. There's nothing to prevent a member of from "sidestepping" a DDL trigger or Event Notification (other than their own ignorance). But that's a severe offense IMO, worthy of disciplinary action. Another feature you may want to look at is Policy Management. I've not used it myself, but it appears to be quite powerful and flexible. It may also be a bit easier to use as compared to DDL triggers and Event Notifications. I'll let you be the judge of that. 

I want to get the rows on between and meaning the rows between last sync_time and previous sync_time for a given . I was doing this: 

What could be wrong here? Why logs are not being sent to those files? UPDATES @rick-james: I did check permissions by running: 

Now, regarding performance, disk space savings, query savings and so on, how would you do that? What is your recommendation on this edge case? Note: for the moment I am using MariaDB 10.1.x but this will be in a MySQL instance, probably 5.x or so I am not sure at all since I didn't got those details yet 

Take a look at the column which is a right now but it will hold a phone number. Since I don't know how many numbers a phone could have all over the world then I am trying to cover almost all of them. I want to keep database integrity as much as possible so I think is not a proper type for hold this kind of information - maybe I am wrong, you tell me - so I am thinking in change to or even . When I am defining a column in Workbench I should specify the number between parentheses not in all the cases but in those I mention previous I had to. So if I do this: I got this error: 

I'm a huge fan of the event-driven approach for a scenario like yours. Meaning, I don't want to sleuth who did it after the fact, I want SQL to tell me when it happens. For that reason, I'd suggest a DDL Trigger. Here's an example: 

I'm trying to tweak the "Server Activity" Data Collection set as outlined in this blog post (I wrote the post, btw. Sorry--it's kinda long). It is working in SQL 2008 R2 and also in SQL 2014. However, when I run on SQL 2012, I get this error: 

Marcello, if you absolutely have to be able to achieve your goal with a single script, you could try iterating through the databases and running TSQL in the context of each database. An example is below with sp_executesql running sp_executesql from/in each database in the cursor loop. Note that @Tsql is static here, so it's declared outside of the cursor loop. However it could just as easily be built dynamically within the loop. This approach isn't easy. It might even seem a little convoluted. I'll let you be the judge. 

But I'm getting repeated results. Is that query right? If I have two records pointing to the same record on tb_persona should the query return those two records? I've a test data if any can help me, just say me where to send and I will to solve this problem. Also it's possible to get only fields from default_tb_persona and not from both tables? 

I have a table called where I have a default record "always". Since I'm testing all the time and sometimes I need to clean that table I'm asking if there is any way to run the following query any time I run the TRUNCATE command on the table: 

I have 3 tables in 3 different databases which are exactly the same. I'm trying to sync from this 3 databases to a master database as shown in the picture below: 

I need to change the names on PK columns and that has relations with another tables meaning they are FK also so this is what I am doing: 

I have been able to update statistics on the underlying system tables. This has worked in my SQL 2008 R2 or newer environments: 

I have inferred that the Windows OS does not actively "push" the OS level memory availability status to SQL Server. Rather, the SQLOS Resource Monitor must actively ask for it via the Win API function. This would seem to be an asynchronous operation. Am I interpreting this correctly? If my reasoning for #1 is sound, what causes the Resource Monitor to get the OS level memory availability status? Is is performed on a schedule? Event driven? How much time might pass from the moment there is low memory at the OS level to the moment the SQL OS Resource Monitor becomes aware of it? When SQL Server reduces memory usage to free memory back to the OS, just how bad is that? Is this an "expensive" operation that should be avoided at all costs? 

1st Attempt This was slower than all of Erik's queries I listed here...at least in terms of elapsed time.