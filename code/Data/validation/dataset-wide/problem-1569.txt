i.e. if the product id exists in then it should be deleted. The implementation of may have to go through the array of ids but it should in most cases quickly discard an object based on its hash which should make it faster. I also find it more readable than an explicit loop. I'm not sure what the method does in your case but it's not safe to add or remove objects from an array while enumerating it. If that isn't the case for you then your code could can be simplified like this: 

If you're following along step-by-step you'll notice that the of the enumerated array is now unused, so you can change it to only loop over the : 

I previously published an article on a new method for analysing RNA-seq data as part of my PhD studies, and I'm now working on making this into an R package. I don't come from a programming background, so this is quite challenging and, thankfully, both fun as well as rewarding. There was a part of my code that was written in Python, since that particular part would be slow in R, but now I have to convert that part into R as well, in order for it to go into the R package. I have successfully done this conversion, but it is, as I guessed, horribly slow. I'm hoping this is only because of the way I did it, and that there is room for optimisation; hence, my question here. In essence, the script takes a VCF file (containing data on mutations from one or more samples), performs some filtering on data quality criteria and extracts the relevant information to another file. This is done to save time, as the resulting file is going to be analysed up to several hundreds of times (depending on the study) by other downstream scripts. This is an example record in the VCF file (long; scrolling needed): 

Suggestions will be much appreciated :) EDIT It seems I did not express my thoughts properly. I do not want repetitions during the random iteration. The idea is basically to cover all the array indices once, but do it randomly. The example linked above does this. It uses the same lexicographically sorted array of latin words on each run, and randomly iterates it and writes out each word to the outputstream. Randomness in the output can be defined as probability of occurrence of words with very small "distance" from each other. For eg. the words "quote" and "quoted" have very small distance. My requirement is that word with very small distance should not occur close to each other in the output (random). When I say the output is not truly random, I mean that I can observe groups of contiguous words in the output which have very small distance from each other. Hence the output gives the expression of not being truly random. 

The wonderful code provided by Gordon (above) does wonders, but I just found out that it doesn't work when the lists compared contain cases where one of the lists have zero unique entries. In such cases, the code snippet 

P.S. I would also welcome feedback on the general level of the code, style, legibility, etc., as it is my first R package and I don't have any previous experience with such. 

The main issue is the (mutation annotation) field in the 8th (extremely large) column. I can relatively easy get the following format, using the package: 

... does the trick. Just in case anybody happens to stumble upon this thread, the solution should now be more complete! 

I made this utility class to convert Closeable instances to AutoCloseable, so that I can make use of the try-with-resource exception handling mechanism of Java7. This is mainly useful for code which can not be altered. 

I wanted to use an async-cache to store URLs of images that I have to display in a list. The image URL is fetched (REST call) using a unique UUID associated with each item of the list. I store this URL locally and then use it to show images in the future. I came up with the following async-cache to make my life easier. 

The server is 3rd party and sends a no-cache header which I think does not make sense. It should be safe to cache conversion factors for some duration. 

The second one is a bit trickier and highlights some constraints in how can be extended to support other sorting algorithms. To make support any kind of collection, it would be natural to also make it generic over a type that is a Swift collection: 

The problem is that — because of array's value semantics — this doesn't account for changes that happen to the array since you started enumerating it. In short, you're swapping in a stale value from before you started sorting the array. Now, what you're really doing here is swapping the elements at two indices, and that is best done with . It's also documented to have no effect if the two indices are the same, so you can get rid of the surrounding if-statment: 

Just to close the question. Divide and conquer is not a good approach for the problem. I made a much simpler implementation and it is working fine. The idea is to keep a linked list of indexes and randomly remove them. We get random indexes and avoid repetition. Although we have to create this linked list every time we want to iterate, something which I wanted to avoid in the first place, but it seems like a pre mature optimisation hence going ahead with this only. 

I have created a utility class which allows random iteration over an array. The idea is pretty much a divide and conquer approach. 

Fails to list all the tuples, since at least one of the tuples does not exist in . I'm not sure if there's a nice and general workaround for this, but I found that simply removing the last two lines of the above and substituting 

There is thus several levels of per mutation, because a single mutation might effect several genes and transcripts. The goal is to separate all fields by , keep those mutations with the highest predicted effect on protein function (going in descending order from , , and ) and output them together with the data for the specific mutation. I need to go from this ... 

I was running some rightsizing tests and had to blast random objects of my various class types. Here is the code I came up with. 

I was making a simple currency converter utility and had to cache the conversion factors after fetching from server. Came up with the below interface. I have basically delegated the caching responsibility to OkHttp itself. Review appreciated. 

You can see this in action here. It works fine, but I feel its not truly random. The original array used in the link above is a lexicographically sorted array of 2000 latin words. As we can see in the output there are multiple groups of words which appear to be in order. For e.g.. a range like this occurred in one run. 

The gene annotation is gone, and each of the annotations now gets its own row with identical information (except the field). The script below works and does its job, yielding equivalent results to the previous Python script, but it is slower. A very small file containing only a hundred variants took around 30 seconds (1 second with Python), a normal-sized file up to 5 minutes (5-20 seconds with Python), and I gave up on a very large file after an hour (5 minutes with Python). So, how could I optimise my script? Am I doing it in a very slow manner? I didn't think that I could use , as I need to go from a single line into many, and thus went for a loop. But maybe I can? Or maybe there's something else? Any ideas are greatly appreciated!