It is quite common cross-linguistically to have ergativity marked optionally. McGregor (2010) talks about this in impressive cross-linguistic detail, arguing that while in some language ergativity may be obligatory, in other languages it can be used optionally. This optionality is not entirely arbitrary, and there are commonalities cross-lingustically. These include a tendency for ergativity to be more common in past tense costructions, with animate or human subjects or to indicate a high level of volitionality. Other articles in the Lingua special that McGregor's paper appeared in also discuss optional ergativity in specific languages. McGregor. 2010. Optional ergative case marking systems in a typological-semiotic perspective. Lingua 120:7. pp: 1610-1636 

I'm sure you've found enough information on Wikipedia and other places that make it clear that Albanian isn't closely related to any other living language, but for those who haven't the most concise summary I've found is at Krysstal's Language Families ( $URL$ "The Illyric Branch [of Indo European]. Only Albanian (called Shqip by its speakers) belongs to this branch. It has been written in the Latin script since 1909; this replaced a number of writing systems including Greek and Arabic scripts." "There are two dialects that have been diverging for 1000 years. They are mostly mutually intelligible. Geg is spoken in the north of Albania and Kosovo (Kosova). Tosk is spoken in southern Albania and north west Greece." "The ancient Illyric and Mesapian languages, spoken in parts of Italy, are considered by some to be an extinct member of this branch." So we have to go back to this Illyric branch, which it is argued is part of the Paleo-Balkan group. The only other surviving member of this group is Modern Greek ($URL$ Given how completely dissimilar Modern Greek and Albanian are this answer appears rather unsatisfying - until you realise how amazing it is that Albanian managed to survive this long when any closely related languages died out so long ago! 

The use of classifiers (mentioned by @Yellow Sky) The use of sentence-final particles (also mentioned by @Yellow Sky) Constructions using the -de (的) particle, which result in a word order that is ostensibly non-English. Often the particle behaves like a genitive -'s, but when the particle follows a larger noun phrase it's more natural to translate into English with of, which requires a different word order. Sometimes it behaves like a complementizer and, again, since it appears at the end of the relative clause, it results in a word order that is flipped from what English speakers are used to. Coming from Japanese, these heavy phrases preceding the head noun felt quite natural to me. 

It is standard to talk about the prosodic hierarchy, which is a theoretical construct that divides utterances into smaller, phonologically relevant constituents called phrases, which are in turn divided into smaller constituents called prosodic words, and so on. There is not an absolute consensus as to what the exact levels of the prosodic hierarchy are or even how many there are, and (as with many theoretical constructs in linguistics) slightly different models lend themselves to different languages. For example, the mora is a prosodic unit that is more motivated in some languages than in others. Here is a (non-exhaustive) list of levels in the hierarchy: 

I know WALS (World Atlas of Linguistic Typolology) www.wals.info has a chapter on the "Order of Adverbial Subordinator and Clause" $URL$ You could use this as a starting point because you know that all 660 languages in this survey cover a good area and genetic spread - and WALS almost always point you to a reference grammar. Might take some of the grunt-work out of your task! 

So by the seventh colour term they basically concede that there is no way to predict order of inclusion in a language. Given that their constraints have been even further relaxed since this analysis I would not find it surprising if there were a language where a separate lexical item for 'light blue' were found before others in rule 7 above. 

@jlawler in the comments of your question is right - but I'll try and expand a bit because I think it's a lovely example of how we often under-estimate the complexity of the languages we speak. A dictionary is a useful tool for what it's good at, giving people a basic idea of what a word means and its basic function. To integrate the data from a dictionary into a parsing machine requires the addition of a while lot more information. Firstly, a dictionary gives the parts of speech (which is a good start) but nowhere in a dictionary will you find sentence-building information like "subjects go before verbs in basic declarative sentences," or any of the hundreds of other syntactic rules that allow speakers to produce plausible utterances. So that's your syntactic limitation. Secondly, there's a lexico-semantic limitation. Think about a word like sand. We know form the dictionary that it's a noun, but we need more. We need to know it's a mass noun, not a count noun so putting a number with it is not good unless there's something to turn it into a count noun (eg. buckets of sand). Also, dictionaries don't include Proper Nouns so they're not going to be good at recognising large chunks of text ("Lauren mentioned Chomsky on Stack Exchange" - for example), not to mention the incredible creativity English-speakers show in creating neologisms ("I Chomsky'd that post"). Thirdly, there's a phonological limitation. To give a dull but illustrative example, when do you use a\an? You'll have to build a rule for that on top of your dictionary, but you'll need to include inter-dialect and inter-speaker variation (in fact, the same goes for all of the above). Dictionaries are great at what they do, and sometimes can be absorbed into computational processes in interesting ways, but it's just a small component of what you'll need to create a sentence checker! 

The use of shì (是) as a copula only for linking nouns to nouns. Over and over I would hear students trying to use the construction N + shì+ Adj (e.g., I am hungry). 

First off, a quick note about the use of / / vs. the use of [ ]. Usually the former is used for phonemes and the latter for phones. Since you are really talking about phones here, I'm going to shift to using [ ]. This is sort of a tricky question. In the examples you give, the state of both the tongue and the lips is expressed by the IPA symbol: 

A syllable is a phonological unit. Native speakers of two different languages can hear the same stimulus as having different numbers of syllables if their respective phonologies have different criteria for what can constitute a syllable. For example, a Hebrew speaker may hear a stream speech and perceive it as [pka] while an English speaker might hear the same stream of speech and perceive it as [pə'ka], since Hebrew phonotactics allow syllable onset kt-clusters while English phonotactics don't. What constitutes a syllable in a given language is deduced by linguists using phonological information gleaned from various criteria, such as stress/accent/tone assignment, word games, metrical divisions in poetry and music, and orthography (in cases where a language's orthography is syllable-based). Syllable counting is a phonological task that relies on learned knowledge about the phonotactics of the language in question. It requires words to be presented auditorily or in some unambiguous phonetic transcription, since orthography is often not a reliable or unambiguous representation of pronunciation (How many syllables in the English word peer? How about 911?). Phonetics is also relevant, of course, since interpreting a stream of speech and mapping it to its phonological representation involves knowledge/awareness of the phonetics of the language at hand. In English, for example, the location of a syllable boundary in relation to an [st] cluster is cued in part by how long the [t] closure is and how long the period of aspiration before the vowel is. In [as.ta] the [t] will have a longer closure and a longer period of aspiration than the [t] in [a.sta]. One last note: even within a single language, some words may be difficult to syllabify and different linguists may infer different syllabifications, since different phonological tests may yield conflicting results. In some dialects of English, for example, syllable counts for words like hire and flour are notoriously difficult to pin down. 

This is an example I always find is helpful for first year. Ignore the vowels just for now, and just focus on the 'p' sounds. If I were doing a phonetic transcription of English notice how all the 'p' sounds are different and they've got slightly different diacritics to show this: [pʰin] 'pin' - notice it's aspirated! [spin] 'spin' - notice it's not aspriated! [stop̚] 'stop' - notice that you didn't actually open your mouth at the end. I use those square brackets to show it's phonetic - it's the sounds as they really and objectively are. But that's not how English speakers really think of them - they don't think of these sounds being different, they all just think of them as 'p.' to show this we just write them all as /p/ and use angle brackets to show that people think of them all as the one sound - /pin/, /spin/ and /stop/. 

Here are some resources, in no particular order: The Wikipedia article on Speech Synthesis gives a pretty good overview on the topic, although it's a bit thin in its discussion of statistical parametric (i.e., HMM-based) synthesis. Dennis Klatt's History of Speech Synthesis gives example audio clips that exemplify the evolution of rule-based formant speech up until the 1980s. To learn more about developing HMM-based synthesis, you can visit the HTS Homepage. HTS is a free toolkit that, along with another free toolkit called HTK (a more general toolkit for building and manipulating hidden Markov Models), allows you to develop your own HMM-based synthesis system. The open-source speech analysis program Praat includes functions for doing speech synthesis, including its KlattGrid functions, which allow you to create formant synthesis output by specifying values for a bunch of parameters. It also has functions for creating and manipulating HMMs and neural nets, but I have never used those functions and know nothing about them. If all of the above are a bit daunting for you to approach (there's a steep learning curve when it comes to speech synthesis), try the online interactive Klatt synthesizer. The Simplified Vowel Synthesis Interface allows you to experiment with synthesizing vowels in isolation (by manipulating the first three formants, F0, duration and overall gain). On that page there are links to more complex interfaces, including the CV syllable interface and the full Klatt synthesis interface. I'm not aware of any site or free toolkit that allows you to develop your own waveform-based unit selection system. There are, of course, demo pages on sites of particular commercial systems, such as AT&T's Natural Voices system. There is also an open-source system called OpenMary. You can read an overview of unit selection synthesis in this presentation by Alan Black. 

ioling.org has the problems from the IOL comp - but individual countries that have the competition will also put their problem sets online. Here is the link for Australia's Linguistics Olympiad problems: $URL$ Over all they're slightly less daunting than the IOL level ones, so you may want to start there! 

Using epidemiology as way towards understanding language use and spread is a good start! See "Linguistic Epidemiology" (Amazon link) by Nick Enfield - he's an excellent field linguist using features of epidemiology to look at contact situations. There are many other ways sociology can help you in field linguistic, at all levels. Any grammatical description worthy of attention these days will include a fairly detailed description of the social context in which the language is spoken. That includes social dynamics, power structures and other features. From there, it really depends what you're looking at. These days field linguists can do a whole lot more than just study the grammar of a language. We're also interested in how migration affects language use, or the power relation between the local language and the national language (or even in some cases, the local dialect and the more common one). You may want to look at the rise of the use of small minority languages on the internet, in which case social networking theory might be useful. You might find there are less direct benefits, for example some areas of sociology put better emphasis on quantitative methods than most linguistics majors do, which might give you a better grounding in statistics, which might help with corpus analysis or phonetic analysis. Even if the study of sociology doesn't directly relate to any future linguistic study you do, the grounding in critical thinking, research methodology and the ability to think from the perspective of cultures other than your own are all going to help make you a better linguist! 

This may not be what you have in mind, but you could try language-specific sites. For example, if you go to... $URL$ and switch the input type to Pinyin, you can search for any syllable in Mandarin using Pinyin input. The results will provide hyperlinks to audio samples of a native speaker pronouncing the syllables. For example, if you type in 'xi', it will take you to the following page: $URL$ Click on any of the hyperlink-ified syllable names, and you will get to hear the voiceless alveo-palatal fricative pronounced by a native Mandarin speaker. The following syllables in Mandarin start with alveo-palatal consonants: 

The short answer is yes--schwa before a nasal in the same syllable tends to be nasalized. The more nuanced answer is that nasalization in English is not really as straightforward as is sometimes taught in Intro to Linguistics classes. There are a couple of issues to bear in mind: 

When you ask '[i]s the 12th Basic Color Term (BCT) always light blue' I assume you're referring to work from Berlin and Kay's 1969 book "Basic Color Terms: Their Universality and Evolution" (U Cal Press). This book and subsequent work argued that languages acquire colours in a systematic order, so a language with only 2 terms would have a light/dark or warm/cold distinction, and then the next addition would be red, followed by green or yellow, then the other, then blue, etc. Although Berlin and Kay's work was a real innovation, it has not stood up to scrutiny over the years. Not only have Berlin and Kay relaxed their finding, but they've been challenged by others. Some of the better reads on this topic are: 

The first thing to do here is separate out a child's knowledge from their production. The most well known and regarded example of this comes from J. Berko and R. Brown (1960). "Psycholinguistic Research Methods". In P. Mussen. Handbook of Research methods in Child Development. New York: John Wiley. pp. 517–557. They give this example of a child interacting with an adult. The child has a toy fish and says /fis/, the adult then replicates the child's pronunciation and says 'is that your /fis/?' The child says 'no, it's my /fis/" (and is rather ticked off at the idiocy of the adult). When the adult finally asks if it is a /fish/ the child accepts the question. What this example shows is that even though children often produce incorrect forms at various stages of their development this doesn't mean that the child can't perceive the difference between what they are saying at the correct form. So even though it may sound like the child in the example you give is merging two forms it is very likely that they're aware of the difference and it will resolve when they've got their tongue around that tricky consonant cluster. 

There are reasons specific to syllable structure and markedness that @Aerlinthe has done a great job of delineating (that answer got an upvote from me!), but I think it's also important to point out that we could replace the phrase "syllable complexity" in this question with almost any other grammatical feature: Why do languages have different vowel inventories from each other? Why do languages have different numbers of genders from each other? Why do languages have different word orders from each other? If we're comparing the phonological systems of two arbitrary languages, we might as well ask: Why does Zulu make use of clicks while French doesn't? Why does Cantonese distinguish six tones while Mandarin only distinguishes four (or five)? The human articulators are clearly capable of creating speech sounds with an ingressive airstream and the human ear is able to distinguish six distinct tunes, so why don't all human languages exploit these capabilities? The short answer is that they don't need to. Languages have evolved over time to meet the functional demands of spontaneous inter-human communication, and different phonological systems exploit different capabilities of the articulatory and perceptual systems of humans to meet these demands. Complexity, if it can be quantified, tends to pop up in different parts of the grammar in different languages. Sure, Japanese doesn't take advantage of consonant clusters to maximize the number of possible syllables at its disposal, but English doesn't make use of vowel length contrasts, which would otherwise increase the number of possible syllables in its syllable arsenal. It's this inherent property of languages that makes them beautiful, and it's what keeps us linguists in business!