You need to provide a reference to "Hale (1954)": it is virtually a certainty that the year is wrong, since this would have been an entirely different issue in 1954. There is, however, Morris Halle, who has written many things on the topic. The idea of having unified phonetics and phonology is, in fact, the theoretical starting point of pre-generative phonology Praguian and became a fundamental assumption of generative phonology as represented by Chomsky & Halle 1968, Postal 1968. We can contrast Ohala's view with that of Chomsky & Halle. Both deny that there are separate components of grammar: actually, it is most probable that Ohala does not have a concept of "grammar", in the sense that generative linguistics uses it. The C&H view is that all language-specific sound rules are implemented by a set of phonological rules, which operate on a universal set of features that can have integer coefficients (though which on can say that a stop in French is "more voiced" that a stop in Swahili). These rules produce a set of instructions to the articulators, and the production of physical sound procedes thereafter in a non-linguistic way. That is, languages do not differ in how nasality is produced in [ana], once the specific integer values of these phonemes are set by rule. Ohala on the other hand does not accomodate sound-system processes that aren't part of what have been called "postlexical phonology", thus there are no phonological rules of palatalization in Polish, there is no vowel alternation within the root in "dream ~ dreamt", Classical Arabic does not have rules deleting intervocalic glides. It is entirely unclear what his theory of language production is, since it appears that he holds that speakers have learned all of the words of their language and do not generate them from parts. The basic dichotomy between Ohala and Chomsky & Halle is that for Ohala, everything is historical change, and for Chomsky & Halle, everything is synchronic grammar. 

Such transpositions (metathesis) might be completely unsystematic. I don't know the specific example you cite, but I suspect that it is just a random example (being CVC metathesis). Other kinds of metathesis are more systematic, and you find it in Hebrew in the 5th binyan when the stem begins with a sibilant -- hitzaken → hizdaken. Some dialects of Arabic have metathesis in order to get guttural consonants out of the syllable coda The reason in that case is that metathesis results in a sonority-based improvement in syllable contact. This paper (the first in the file) covers some of the reasons, and applies to Persian. Most examples of metathesis are grammatically unsystematic, such as the common example of "ask" being pronounced "ax" in some dialects -- that only happens in the one word and assumes without reason that the underlying form is /æsk/ because that's the Standard English form. Sometimes, infixing is incorrectly labeled as metathesis. But there are some robust examples like Leti, so that one can't deny that the phenomenon exists -- the theoretical mechanism for accounting for such alternations is another matter. 

In the case of a "pharyngeal nasal", it depends on what you mean by "a nasal" and what you mean by "pharyngeal". In the case of [m] for example, there is a complete constriction at the lips and the velum is lowered [m] is a "nasal stop". There are also nasal non-stops. There is a lot of equivocation over what is "pharyngeal", since most "pharyngeals" are actually epiglottals. There do not appear to be any purely pharyngeal pure stops (for whatever reason), and if a "nasal" is necessarily a stop, there can't be a "pharyngeal stop". On the other hand, if you use "nasal" to include approximants and fricatives, and don't care strictly about pharyngeal vs. epiglottal, then the articulation can be made. I don't think we can compute whether such an articulation would have detectable acoustic consequences upon which a contrast could be founded, but there's a good chance that such a sound would be be acoustically distinct enough to be phonemic. Clicks require a constriction somewhere at the back of the mouth, plus some further forward constriction. The back constriction could be anywhere between on the soft palate back to the uvular region. Some of the Khoisan languages (esp. !Xóõ, Jo !'hoansi and N|uu) have uvular clicks, the latter language having been the subject of ultrasound studies by Miller. In fact, the back constriction on Zulu clicks moves from velar to uvular, and there is mounting evidence suggesting that the back constriction is clicks is commonly uvular. See Miller "The representation of clicks". 

There is a table of IPA performances by Peter Ladefoged here: they happen to be in AIFF format, but you can presumably work around that. Note however that only the vowels are produced with just the letter in question, and in the case of consonants you have a choice of "pa" or "apa", but not just "p". This is mainly because there's virtually nothing recordable with isolated stops, and formant transitions to a vowel are essential to identifying a consonant. The clearest case of the problem with trying to glue together isolated segments would be the attempt to glue together "tip" from the components "t", "ɪ" and "p". You can get a steady-state recording of "ɪ" (the snippet of "ɪ" on that page is 458 msc. long, which is abnormally long for speech). But "t" and "p" simply involve closing the lips or raising the tongue, and there is no actual sound during their production, so all you have is silence (the ability to discern "p" versus "t" comes from the formant transition effect that these articulations have on adjacent sonorants). This, "tick, tip, tit, pit, kit..." would all sound the same using this technique, namely [ɪ]. If you limit yourself to word without stops, you could piece together "s", "æ" and "ʃ" with steady-state recordings of those segments. If you do this, you also have to decide about the margins: do you leave an acoustic gap between "s" and "æ", or do you trim the samples so that you pick just the center of a performance? A no-tech way to decide this is to articulate the sounds "s" "æ" "ʃ" in rapid succession, and if this is satisfactory, you can go from there. If you aren't happy with that ("doesn't sound natural") you can trim the segments to eliminate the gaps and also make the durations more realistic, say vowels=100 msc, fricative=200msc. Piecing the parts together, I think you will find the result to be even more unnatural. There is no repository of isolated fricatives out there, so you would have to create it. A simple way to determine if it's worth your time is to either record your own samples or copy the Ladefoged recordings and edit out the surrounding vowel, then paste the parts together. If that does not dissuade you, then you might try extending the set of consonants to include nasals and voiced stops, which you can hyperarticulate (strong voicing, sustained duration) and use as the basis for extracting appropriate centers. I think that the Ladefoged samples of "b", "m" etc. would provide a decent basis, since the articulations are longer than usual and voicing / nasality is stronger (more controlled). One other impediment is that even if you have a transcription of a language using just IPA letters, the "exact" values employed in the transcription are often rather divergent from the actual sounds of the language. Supposing you had the word "cotton" in an IPA-transcribed database which gives you [katən] – that's one of the possible IPA transcriptions. But in American English it is usually pronounced something like [kʰɑʔn̩]. Unfortunately, all existing (professional) recordings of IPA letters omit the modifier-type letters such as "ʰ" and "̩ ", and you won't be able to find all of the aspirated, ejective, affricated and so on type modifications. And you will probably have to manually edit the transcriptions to apply allophonic details that are typically omitted, or to "correct" somewhat arbitrary choices in letter selection (the vowel of "hot" is not well-matched by either IPA "a" not "ɑ", so people typically pick one without denying the possibility of the other choice). One fairly simple way to overcome the problem of formant transitions is to use "diphone synthesis". This roughly involves recording all two-phoneme sequences (from real words), selecting an area around the transition, and then gluing them together, so "sunshine" could be generated from a stock of parts of "sʌ"+"ʌn+"nʃ"+... However, I think all methods of synthesis require a serious dose of programming ability plus knowledge of acoustics. There was a question about IPA and synthesis here, you could slog through this thread on xkcd, and this is also on the topic of IPA synthesis. And let us not forget the virtues (and vices) of Google synthesis, where you can play real and non-real words in English (and get credible results as long as it's phonotactically possible in English). This is Finnish gibberish, which sounds like Finnish. Swahili gibberish doesn't sound like Swahili; Icelandic gibberish is a bit better but not good. There is no synthesis for quite number of languages. 

The normal situation is that locations have a local name in the local language, and that is how the place is known. Certain places become sufficiently famous that they become regularly known to other people, who speak other languages, and the pronunciation may change for them because of the rules of that language. The city "Washington" [ˈwɑʃɪŋtən] is pronounced [wásindoni] by speakers of the language Logoori for that reason. Once a word is adopted into a language, it becomes subject to the rules of that language, and over time, the local pronunciation versus the "foreign" pronunciation can diverge. These kinds of independent developments explain the change of Londinium into "London", "Londres", "Londýn" (and likewise may explain the development of some other word into Latin Londinium, though we don't know what the ultimate source of the place name is). In some cases, there may be competing local names especially when there are two prominent ethnic groups, each with their own name. The town of Kautokeino has two local names, Kautokeino (in Norwegian) and Guovdageaidnu (in North Saami) – the Norwegian name (by which the town is generally known) is a phonological adaptation of the indigenous name. In the case of Vitoria-Gasteiz (now known with both names though one can find signs with just "Vitoria" and ones with "Vitoria" painted out), it seems to have to do with there being two places merged into one, with Vitoria having been a Spanish location and Gastehiz being a Basque one. 

Association lines are used for phonological representations, not processes (processes operate on representations). In the simplest kind of case, bábâ has two tiers, baba and HL, with the elements of the tiers being related by association lines. Without association lines, neither tier could be phonetically implemented – a vowel cannot be implemented without some tone, and a tone cannot be implemented without some vowel. The origin of the no-crossing condition is the origin of autosegmental phonology, Goldsmith’s dissertation, and is specifically laid out in section 2.3 “Excursus on Formalism”. The motivation is actually that some such statement is required in order to have a well-defined system, and the issue of whether there is an empirical need for the principle is not raised. The “No-Crossing Constraint” is a later restatement of a theorem from the Well-formedness condition which simply says “πi and inverse(π),i preserve connectedness” (discussion surrounding (28) specifically shows how non-crossing follows from that). The constraint (or what follows from the constraint) says that “this cannot be”, not “you may not do” – the former entails both “you cannot do” as well as “you cannot have such a thing underlyingly, or by any other means”. Subsequent research provided empirical support, albeit within the general autosegmental context, that NCC eliminates the need for conditions on association rules which would be of the form “associate this thing with the leftmost element, as long as no association lines are crossed”. Since no case has been detected where you need the lines to cross, this has been a fairly robust constraint on representations, whereas all of the other sub-cases of the WFC have proven to be language-specific. (That, incidentally, indicates that the above-mentioned simple formalization of the WFC can’t be right, since the requirement to have complete linkage between the elements on the various tiers cannot be separated, under that statement, from NCC, but only NCC is actually correct as a universal) 

The Latin term is a calque from Greek σύμφωνον "pronounced with". According to Dionysius Thrax, they "do not have a sound on their own, but, when arranged with vowels, they produce a sound". Aristotle (poetics) expressed the same view of "mutes" being without sound of their own: "Such sounds may be subdivided into vowel, semi-vowel, and mute. A vowel is that which without any addition has an audible sound; a semivowel needs the addition of another letter to give it audible sound, for instance S and R; a mute is that which with addition has no sound of its own but becomes audible when combined with some of the letters which have a sound. Examples of mutes are G and D." In Sanskrit grammatical tradition, vowels are svara meaning "sound", and consonants are vyañjana, meaning "ornamentation" (also "sauce" and a bunch of other things). So the view that you're pointing to probably originates with Aristotle, with terminology supplied by Dionysius. 

The explanation for the similarities that you observe is due to the fact that Ahirani derives from something like Sanskrit (a dialect of, or sister language). The Prakrits are probably well-enough attested that you would be better of comparing Ahirani and one of the intermediate languages like the Apabhraṃśas, in the same way that one could compare Latin and French (knowing that French descends from Latin -- so this isn't standard comparative linguistics where one compares the known Algonkian languages and hypothesizes what the original language looked like). However, before you can compare two Ahirani and anything else, you would need to have some materials on the language. Ahirani-shabdkosh seems to be large enough that you might actually be able to put words of the language next to words of some older language. It might be useful to first read some of The Indo-Aryan Languages edited by Cardona and Jain, which would put the development of the languages into perspective (it would tell you what is already known). [EDIT] One problem in understanding the development of any modern especially Indo-Aryan language is that languages can freely borrow from each other. Prakrits borrowed from Sanskrit as well as retaining original words which went through centuries of language change; Apabhraṃśas did likewise, and modern languages can borrow from earlier (literary) languages as well as modern languages such as Hindi and Gujurati. Romance languages often have words that descended in the normal way from Latin (for example Spanish leche "milk"), as well as words borrowed later from Latin (such as lactar "to lactate"). In order to know whether you are dealing with borrowing or direct descent, you have to know the regular historical rules. If a language has a word like ishwar for "god", that is almost certainly a later borrowing from Sanskrit, because shw was simplified millenia ago.