Even though it was logging in the event log via the trace flag, this also needs to set in order to trigger the emails. You can see the table here: 

and that works fine. the problem that I am having in that server is DatabaseMail didn't start when an email is queued when I query the sysmail_event_log I don't see any event related to the emails I have just sent (attempted to). 

this has been working very well as it is. the results and behaviour is as expected. question: 1) How could I have done this differently and more effectively? 2) have you noticed that instead of writing the procedure with EXECUTE AS I had to add this line 

I know this question might have already been asked, but I am still trying to figure it out how to query the xml data and I really need an example. I have the following XML code that is inside a variable called @x in sql server: 

Can I expect significative gains by partitioning all these indexes according to the imageSize? On what factors would it depend mainly (in this case)? Most queries use the ImageSize as one of the parameter, it is even in the clustered index. I could modify the indexes and table structure, this take is mostly used for reading. there is only 1 write a day, and that would not normally affect many records. 

You can always calculate higher-level information from lower-lever aggregate. For example if you had an aggregate on (app_id, day, collection_id) you could use it instead of (app_id, day). You can materialize your aggregates with feature. But this is not an only way. If old data is static, it could be enough to insert new rows daily, with something similar to 

If you want to see also query duration, you can set instead of . This is very useful for query tuning. Then reload config (restart or HUP) and collect enough log to estimate traffic. Note: neither method will include queries embedded in user-defined functions. 

Notes on performance With small tables (less than 1000000 rows), any solution will work. is slightly faster: 

In current version (Pg 9.0-9.3) there is no fast "fail-back" mechanism. You will have to follow official guidelines which state: 

All operating systems and all applications use a concept called "caching". It means - when the data is first read from a slow memory device (like, a hard disk), it is saved in a fast memory device (like, RAM) for some time, to facilitate faster lookups. The same applies to RDBMS. First time the data blocks that build up your query results are read from disk, second time they are read from memory. Details can be explored using OS and database tools. If you specify what RDBMS and what OS you are on, we can help you get the details. For PostgreSQL it's about EXPLAIN command. 

when I run my script it shows me the next running time for the job in general, but I would like to see it by schedule too. 

I have installed 2 instances of sql server plus SSIS on the following server. Note the amount of RAM is nearly 384 GB 

I know... I did not like it either, but it would not work with me using EXECUTE AS. 3) MAYBE if both my login and user were domain accounts, would this make a difference? I don't want to add any database ownership chain, or trustworthy in order for this work, unless really necessary. also: we are still on sql-2005 on these test machines: Microsoft SQL Server 2005 - 9.00.5000.00 (X64) Dec 10 2010 10:38:40 Copyright (c) 1988-2005 Microsoft Corporation Developer Edition (64-bit) on Windows NT 6.0 (Build 6002: Service Pack 2) 

I checked which .Net versions I had installed then I installed .NET 3.5 and all started to work nicely. 

I have a test environment where the AD group is currently . Recently I have had some issues, specially regarding to people restoring databases. My concerns are: 

I am trying to put this whole thing into a SSIS Package but I have my concerns and doubts. 1) what operation from the SSIS toolbox should I use to implement this within my package? 

RULEs or triggers are a performance overhead, and can be avoided. Consider something along these lines: 

In my opinion it is not related to or Windows issues. Per pg_basebackup docs, the main data directory will be placed in the target directory, but all other tablespaces will be placed in the same absolute path as they have on the server. 

You do not need any triggers / rules to maintain it. There are open ends here - that's just a draft... Some issues: 

Constraint-based exclusion [CBE] is performed on early stage of query planning, just after the query is parsed, mapped to actual relations and rewritten. (internals, Planner/Optimizer stage) The planner cannot assume any contents of "sensor_sample" table. So unless you have values hardcoded in the query, the planner will not exclude "partitions". I guess what happens with the CTE variant... the planner is restricted because you use TABLESAMPLE and the whole subquery may be treated as volatile even if literals in the subquery are static. (that's just my guess, I'm not expert on planner code) On the bright side, the index scan with negative result is blazingly fast. (single page scan at most!) so unless you have over 10000 partitions, I would not bother. So, to answer your question directly: 

I'm not yet sure if this is doable in pure SQL (probably - yes), but here is the brute force solution using cursors. This is just a (working) draft, with some twiddling and dynamic SQL you could add more parameterers, like function name. 

I have realised some people are not so easy to deal with, I rather reduce their power over that server. I don't want to do their jobs, but I would like to restrict them from creating new databases, or overwriting existing ones, either by restoring or by attaching files, etc. They still should be able to create and run jobs, create\alter any object withing user databases, create logins, grant permissions etc... Basically the question is: How to limit the power of a current sysadmin? I thought about not granting sysadmin but a set of server permissions (excluding those to create/alter databases) that would allow them to do everything else. what would that set of server permissions be? 

I have a process(es) hungry for tempdb, but I am struggling to identify this process. any ways I could achieve this? 

My problem is that I cannot retrieve the whole content of my script. I have tried to copy it using the mouse as you can see on the picture below: 

Missing Join Predicate the Missing Join Predicate has been discussed here and Here too. Missing Column Statistics I have fixed it, or at least got rid of the warning by simply running 

how can I find out where the backups are taking more time to run? I currently can find which backups are taking longer using the following script: 

Decide your vertexes (nodes, objects) and edges (relationships). Convert relational data to cypher, declaring all items and all relationships explicit. 

Make sure not only partitions are indexed, but also the master table is indexed in same way and ANALYZEd. This could make the planner include index-based estimates on a single partition, but ignore them on master table level. If expression index or statistics for master table is missing, the planner is not able to infer join cardinality from this condition - even if it has perfect statistics for partitions. It's just a guess because you did not provide full schema. Please let me know if this helps. 

Interesting question but also very open one. I'm putting a list of recommendations here - hope it helps. 

Exception blocks are meant for trapping errors, not checking conditions. In other words, if some condition can be handled at compile time, it should not be trapped as error but resolved by ordinary program logic. In Trapping Errors section of PL/PgSQL documentation you can find such tip: 

Update: Konrad corrected my misunderstanding of his question. The goal was to count queries, not transactions. How to count queries? Method 1 Use pg_stat_statements contrib. Method 2 Enable full logging of queries for a representative period of time. To enable full logging, for PostgreSQL 9.0 - 9.3, change following settings in