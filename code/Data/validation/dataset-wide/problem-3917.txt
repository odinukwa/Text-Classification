I'm no expert, but paging through "Linear Algebraic Monoids" by Lex Renner suggests to me that it has a lot of information you could use. 

I think it is worth pointing out that, from a purely mathematical point of view, economic general equilibrium theory is an exercise in fixed point theory. The same may be said of the theory of non-cooperative games. John Nash invented the solution concept now known as the Nash equilibrium in his thesis. VonNeumann dismissed Nash's result as "just a fixed point theorem" but Nash eventually received the nobel prize in economics for this work. The mathematical setup for economic general equilibrium theory focuses on constructing what is called the "excess demand correspondence". This is derived from assumptions about how consumers and producers formulate their plans to take best advantage of the prices they observe in the market place. The excess demand correspondence associates to any vector of market prices (one price for each commodity) the convex set of vectors of aggregate excess demands (one excess demand- possibly negative -for each commodity) that will arise when consumers and producers respond to that specified price vector. The main idea of the proof is then to find another convex set of price vectors, each of which can be interpreted as a supporting hyperplane to the given convex set of excess demands, and each of which maximizes the market value of the excess demand. This construction then can be shown to yield an upper semi-continuous, convex set valued function from the convex set of allowable vectors of market prices to the space of is convex subsets. One then applies an appropriate fixed point theory to deduce the existence of a price vector which, because of the structure of the excess demand correspondence, has the property that the value of excess demand in each market is zero. This is the market equilibrium price vector. 

I think the question raises a valid point. A very fruitful approach to affine problems was initiated by Iitaka in the 70's which is as follows: Suppose $V$ is an affine variety and $X$ is a projectivisation such that $D=X-V$ is a divisor with simple normal crossings (SNC). Look at the canonical divisor $K:=K_X$ and the divisor $L:=K+D$ on $X$. Just like, the now classical, theory of Kodaira and others of analysing the multicanonical systems $nK$ of a projective variety, Iitaka proposed to look at $n(K+D)$ to come up with the a kind of classification for the pair $(X,D)$ as one does for projective varieties. Of course the only complete success in classifying varieties until Iitaka's time was for curves and Surfaces (which is also available now for 3-folds), so he and others applied this idea for non-compact (in particular affine) surfaces. Below I shall talk only about surfaces since the appropriate theory for 3-folds has not yet been worked out (as far as I know) and the curve case is extremely well understood and presents no real difficulty, generally speaking. Just like a Kodaira dimension for projective surfaces, we can define the logarithmic Kodaira dimension of non-compact surfaces which is by definition the rate of growth of $n(K+D)$ as $n$ varies over positive integers. This number, called $\bar\kappa$ can take values $-\infty,0,1,2$ (or upto the dimension of the variety in the general case). At this stage one proves a theorem that this number is independent of the compactification $X$ chosen, as long as $D$ is SNC. This gets the theory started and we get a perfect gadget for studying the non-compact (in particular affine) surfaces. The whole project follows Kodaira's classification philosophy that one should develop enough classification theorems for the various $\bar\kappa$ classes and therefore (ideally) answer "all" questions about the non-compact (or affine) varieties. So if you want to answer a question like "are two affine varieties $A,B$ isomorphic or not" then the first thing to look at is their log-Kodaira dimensions. If they turn out to be different then we are done. If they are same then we have to look more closely into that particular $\bar\kappa$ class and either apply the appropriate classification theorems available or formulate and prove one, to decide. However, just like in the projective case, the general type surfaces are hardest to study and don't always admit any good structure like a fibration over a curve which might have helped in their systematic study. And, by and large, the greatest success story has been in the non general type cases where there is a detailed classification of projective surfaces. Similar difficulties are encountered in the affine case and the $\bar\kappa\leq{1}$ affine surfaces are amenable to detailed study. Of course, there are some strong results about general type surfaces also which are in spirit the same as in the case of surface geography problem. To find out more about these things one may look at Iitaka's book(GTM,76) and Miyanishi's book. 

Indirect method: Monoids are categories with one object. A simple calculation shows that the inclusion of categories N into Z has contractible homotopy fiber (which is the nerve of the category whose objects are the arrows of the one object category Z and whose arrows are commutative triangles of Z mediated by the elements of N). Thus Quillen's theorem A yields a homotopy equivalence of the corresponding nerves arising from the inclusion of underlying categories. Direct method: Consult this paper by Ken Brown: "The Geometry of Rewriting Systems" You need only the simplest version of his method. With it one can show that the nerve of N and the nerve of Z have cellular models which differ only by collapses of simplices and thus have the same (simple) homotopy type. 

I have uploaded a copy of Steinberg's Yale lecture notes on Chevalley groups to Google Docs. Here is the link 

Have you checked Shiota's 1997 book from Birkhauser: Geometry of Subanalytic and Semialgebraic Sets (Progress in Mathematics) 

I think it is important to remember that the Brown-Loday theorem concerns colimits of cat-n groups obtained from an n stage filtration of the underlying space. Moreover, cat-n groups can only provide information on the n-type. So, if you wanted to compute pi_200 of the two sphere, you would need a cat-200 group (at least). And if you wanted to apply the Brown_Loday VK theorem you would need a 200 stage topological filtration of the two sphere. Any thoughts ? or do I have this wrong?