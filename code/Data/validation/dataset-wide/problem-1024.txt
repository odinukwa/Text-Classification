Whenever you've got an overall general performance question like this, start by asking, "What's the server's primary wait type?" My favorite way to get that is with sp_BlitzFirst @SinceStartup = 1. (Disclaimer: I'm one of the authors of that open source script.) The first section of that will return your top waits since startup. If you post that output here, you'll get a lot better diagnostic answers. 

Occam's razor suggests starting with the obvious: If your script sometimes leaves a database in restoring state, then debug the script. Start by logging what you're doing in a table or a file. Then, when you end up with the database in restoring state, step back through your logs to see what went wrong. (If you want a second set of eyes from the community, try uploading your script as a Github Gist, but keep in mind that the bigger it is, the harder it might be for folks to spot bugs.) If you don't wanna do that, then try running a Profiler or Extended Events trace to track the restore events, but be warned - it's way harder than it looks. (Read the comments on that post for even more reader ideas that they've tried and failed.) 

By restarting SQL Server, you force SQL to give up memory, thereby letting Windows get more, and the paging stops temporarily. SQL will start again at near-zero memory use and gradually go up, and when the box runs out of memory again, the restart will help temporarily. By restarting the entire OS, you'll also force Windows' file cache use down. The real fix: stop copying files from the Windows server or cap the amount of file cache in use with the Dynamic File Cache Service as documented in those blog posts above. 

There's no such thing as the 100th-200th rows, because you don't specify an ORDER BY. Order isn't guaranteed unless you include the ORDER BY for a whole lot of interesting reasons, but that's not really the point here. So to illustrate your point, let's use a table - I'm going to use the Users table from the Stack Overflow data dump, and run this query: 

This isn't a really widely used tool for SQL Server database administrators. For better answers, I'd contact DBVisualizer's support. 

It depends on whether you need the search to be real-time. Typically in image search database environments, the images aren't changing THAT fast. You could run the search off 5-15 minute old data (and perhaps much older) without affecting the application experience. In that case, you're better off doing the image search outside of the database in a full text platform like Lucene. If you do need it to be real-time, then you have to be careful with indexing strategies. You want the right table indexes to support your queries, but none extra, because they'll slow down your application. 

Are you doing your backups to a neutral independent server like a file share? If you're doing them locally (like to a drive on SQL01A or SQL01B) then log shipping won't work since the file path would change whenever there's a failover. Instead, on SQL01A and SQL01B, write your backups to a UNC path (like \\myfileserver\myfileshare) and then SQL02 will always be able to find them there. 

SQL Server Management Studio sends a request to FORBISPRD08\BIS01, who sends a request to FORNSQPRD02\NAV01, who has the data, and sends it back to FORBISPRD08\BIS01, who forwards it on to SQL Server Management Studio 

That is the equivalent of shrinking. To prove it, check the box and click the Script button at the top to see the shrink commands that it produces. Generally speaking, thatâ€™s a great learning tool too - whenever you want to learn what something in the UI does, try that script button. 

If you can't add any extra storage for the data (as noted in your comment) then you simply can't do this. SQL Server doesn't track the old and modified rows for you. There's a duct-tape way of doing it temporarily: you can read the transaction log. When combined with the full chain of backups, you can build the old & new data together. Third party data recovery products like Quest LiteSpeed do this - for any transaction, they'll show you the old and new data by reading through the full backup and transaction log backups. 

You've got a couple of questions in here: 1) Any ideas on why the SQL Server process would not be following the limit set on the server? Max server memory refers to the buffer pool, but not everything that the server uses memory for. For example, SSIS isn't part of the SQL Server engine - it's another application that just happens to come free in the box with SQL Server. (Same thing with SSAS and SSRS.) Max server memory has no impact on SSIS. This is one of the reasons you'll often hear recommendations that you should separate SSIS onto its own instance - especially if you're using SQL Server Enterprise in virtualization. At $7k USD per core, you're talking about $28k worth of licensing here, and you're working with something like $500 of RAM. 2) Does anyone know a good way of profiling memory usage within SQL server? We can see that page life expectancy is dropping and more memory is being allocated to the process, but is there any way to see what SQL is allocating the memory to (i.e. procedure cache, cached results, etc). Yes, inside the SQL Server engine, you can use DMVs like sys.dm_exec_memory_clerks. If you're using the free Opserver tool from the StackExchange guys for SQL Server monitoring, look at the memory clerks listing. If not, you can start with the memory clerks query it uses. However, outside of the SQL Server engine - like with your SSIS packages - SQL Server DMVs can't help because this is happening out of process. You'll need to use conventional systems administration tools to do process monitoring and watch which processes use RAM. 

One of my favorite things about databases is that they can handle work in batches. If you need to read 1,000 rows, don't get them one at a time, each with their own query. Use a WHERE clause with a list of values that you're looking for, or a range comparison (like greater than a particular ID, and less than another ID.) If you need to write 1,000 rows, try working in a batch instead. Writing all 1,000 rows in a single statement can result in less blocking - whereas 1,000 statements trying to write to the same table, simultaneously, can be a blocking nightmare. (Especially when that table has multiple indexes.) 

If you control the T-SQL (like if you're dealing with views or a reporting system), then you're probably looking for a user-defined function. With UDFs, you can code your own logic into something like this: 

Use the Extended Events technique described here: From the DMVs, can you tell if a connection used ApplicationIntent=ReadOnly? Not necessarily an exact duplicate of this question, but the same techniques will be used to produce the answer. (I wish it was easier.) 

Jeff, Brent here. We put a lot of documentation in that URL - make sure you copy/paste the URL into your web browser to read the full documentation. If you're in simple recovery model, and you've got log files larger than the database, there are lots of possible causes: 

Yes, database mirroring works from one principle server to one mirror server. Only one mirror copy can be used. For multiple readable replicas, consider AlwaysOn Availability Groups, introduced in SQL Server 2012 Enterprise Edition. 

You could in theory do this, but it would require building the update statement with dynamic SQL. Basically, you declare a string variable, build it with metadata from the database, and then execute that string. Erland Sommarskog has an epic post on this: The Curse and Blessings of Dynamic SQL. However, that's gonna require a lot of work and a lot of debugging. Plus, for performance reasons, you don't really wanna fire a new OPENROWSET for every row of a table you're updating. Instead, I'd suggest doing a single OPENROWSET call to pull all of the file contents into a temp table, and then doing your update statement from the temp table.