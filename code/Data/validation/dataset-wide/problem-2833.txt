Lange discusses several approaches to justifying induction. The specific one referred to here is that Bayesian reasoning provides us with a mechanism for updating our credences in the light of new information and hence this constitutes a form of inductive reasoning. David Stove in his book The Rationality of Induction takes the argument even further and claims that an inductive skeptic would in effect be committed to believing that for any hypothesis H and evidence E, P(H | E) = P(H), which is absurd. Lange now considers a counter argument from the skeptic. Can one retain a commitment to Bayesian updating but deny that it leads in general to justifiable credences because Bayesian updating requires that one start from priors, and there is no objectively neutral way to assign priors in the absence of any information? Lange for some reason doesn't say much here about uninformative priors. It is a commonplace in Bayesianism that one must choose one's priors carefully in order to be 'epistemically modest' - that is, to avoid making assumptions for which one has no evidence and hence ensure one's priors are maximally uncertain or equivocal. There has been quite a lot of work in this area and it is surprising that Lange makes no reference to Jeffreys priors, or to Edwin Jaynes' approach using the maximum entropy principle, or to formal approaches to defining equivocation using Kullbackâ€“Leibler divergence. Given that (I would say) there are pretty good ways of assigning objective priors, at least in a wide variety of typical cases, the skeptic is checkmated here. They can of course refuse to assign any value to a prior, but such a refusal amounts to declining to allow that credences can be represented as probabilities. In effect that would be avoiding checkmate by saying, I refuse to play any more and I'm going home. Not only that, but convergence results have demonstrated that under a wide range of plausible conditions, posterior priors will converge when updated by information from a large number of independent trials. This means that even in the absence of objectively agreeable priors, one can still progress, inductively, to agreement on a posterior given enough evidence. 

I'm not sure what your example is trying to show. The contrapositive of "if A is true then B may be true" is "if B is impossible then A is false". This is not in general always true or always false. 

First off, we might ask, why does the difference matter? If we are using first order classical predicate logic, for example, the two agree. FOPL is sound (syntactic validity implies semantic validity) and complete (semantic validity implies syntactic validity). The question will only be important if we are either contemplating a non-classical logic, or an extension to the domain (e.g. arithmetic) or if we are asking an epistemological question about what justification there is for our logic. Proof theory reduces inference to deductive rules, and these are supposed to be intuitively obvious. But if this is so, which logic should we use? The intuitionist objects to LEM. The paraconsistent logician objects to LNC. The relevance logician objects to disjunction introduction. Why do they object? For semantic reasons: the rules lead to patterns of inference that they regard as objectionable when interpreted. It doesn't matter if you disagree with their arguments and consider classical logic to be correct; the mere fact that one can coherently argue about which logic is correct by appeal to its interpretation shows that it is the interpretation that matters. Constructing proofs by manipulating rules is just playing with symbols until you interpret them. The interpretation is where the rubber hits the road: logic must cohere with the empirical project of allowing us to make sense of the world around us, or else it is useless. To reinforce this point, ask yourself this question: would you prefer to use a logic that is sound but incomplete, or one that is complete but unsound? I would take the former every time. If my logic is unable to prove something that I consider true, that is unfortunate but I'll live with it. But if my logic is proving things that are false, what use is it? It is the semantics that is in the driving seat and the proof system had better agree with it, or the proof system will need fixing. Another consideration is that our understanding of logic is growing and it is doing so for semantic and empirical reasons. Classical logic does not cope well with vagueness or uncertainty, so we can extend it. Maybe classical logic does not correctly describe the logic of quantum theory, in which case we might have empirical grounds for changing the rules. Again, it is the semantics that is fundamental. Or with modal logics: they need different rules because modal contexts are referentially opaque and do not obey the normal quantification rules. How do we know? Because semantics. You say that appealing to interpretations has no value in identifying validity, but this is at best only half true. It is perfectly apt for identifying invalidity. Suppose someone asks you whether the following argument is valid: "all chimps are warm-blooded; all apes are warm-blooded; therefore all chimps are apes". I contend that rather than producing a proof that this in invalid, it is far easier to observe that substituting "dolphin" for "chimp" yields an argument with true premises and a false conclusion. 

One can find some plausible examples in natural language where 'or' and 'unless' are intuitively the same. For example, "hand over your wallet or I'll shoot you" is the same as "unless you hand over your wallet, I'll shoot you". But there are differences too: for example, 'or' is commutative: A or B is the same as B or A, but 'unless' is not generally so in natural language. One can hardly say that the two examples given earlier are equivalent to "unless I shoot you, you'll hand over your wallet". The reason seems to be that 'unless' is a kind of conditional, i.e. it is one of many words, including if, should, provided, barring, when, had, would, else, otherwise, suppose, assume, allow, imagine, etc. that are used to express conditional thoughts. Conditionals in natural language don't behave the way the logic textbooks tell you. They are not truth functions but pragmatic devices with various purposes including stating a rule, a hypothesis, an inference, an evidential claim, a proviso, a precondition, a supposition, an assumption, a disposition, an explanation, a negotiation, a promise, a threat, a plan, a restriction, a requirement, an offer, etc. 

This follows directly using the rule of existential generalization. If we assume our universe of discourse is men (this will save a little bit of space) then we could write the sentences as 

I would start with first order predicate logic, which is foundational to others. Then move on to non-classical logics and modal logics. If you want some book recommendations, I suggest Mendelson's Introduction to Mathematical Logic, Priest's Introduction to Non-Classical Logics, and Hughes and Cresswell's New Introduction to Modal Logic. 

Disclaimer: what follows is greatly simplified. Conventionalism in the philosophy of science can be understood as a rejection of foundationalism - the idea that statements about the world can be deduced (or even induced) directly from observations that are infallible. Conventionalists point out that observations are potentially ambiguous and require interpretation, and in any event, theories are under-determined by data so one cannot have a single theory or set of statements about the world that are uniquely deducible from one's observations. More extreme versions of conventionalism (LeRoy, Ajdukiewicz) reject objectivity entirely and hold that conventions of language, measurement and methodology mean that one cannot have any kind of objective facts, statements or theories. Conventionalism is different from instrumentalism, which in effect holds that scientific theories are not true at all, just instruments of prediction. Most conventionalists, at least of the moderate kind, hold that while we can have more or less good theories about the world, the best we can hope for is for these theories to be coherent and to do a good job of making sense of observations by their own lights. As such, conventionalism is often associated with a coherentist account of truth. 

This question needs quite a bit of unpacking. For a start, one must distinguish between truth and evidence. To a realist, something may be true without there being any evidence for it. I have no evidence as to whether it rained on Lands End in England at noon on May 1st of the year 1 CE, and I strongly suspect that no such evidence exists or will ever come to light. But I believe that proposition is either true or its negation is true. I have no evidence as to whether matter existed prior to the big bang, but I'm willing to believe that either it did, or it didn't. This means that the primitive idea that only verifiable sentences are meaningful is simply false. Furthermore, sentences, and even whole theories, typically are not verifiable at all, but only falsifiable. Even if we accept a kind of inductive reasoning, such as that of bayesianism, we cannot speak of verifying hypotheses or theories in any absolute sense, but only in comparison to rival hypotheses. If I have two rival hypotheses A and B, and A gives a better account of the data than B, then I'm willing to say that A is confirmed relative to B, but I cannot rule out that an even better hypothesis C might come along in future and trump both of them. I suspect that despite your question being framed in terms of truth, what matters more to you is whether one has evidence for a belief. This question might be phrased, should one believe anything without some evidence? Even this is a highly contentious claim. To begin with, you have to address Agrippa's Trilemma. The only thing that can provide evidence for a belief is apparently another belief, so where does evidence come from? Any evidential claim must either (1) be circular, (2) involve a regress, or (3) involve an appeal to a belief that is infallibly and indubitably true. None of these options is very appealing. Even if you think you can surmount this hurdle, one must address conventionalist objections that what counts as evidence depends on all kinds of methodological and linguistic assumptions. So you and your opponent may be working with different criteria of what counts as evidence with respect to some domain. What counts as evidence of the existence of God, for example? (Wittgenstein, and the later Carnap, regarded theological language as a distinct language game with its own rules that allowed assertions to be made that didn't admit of falsification in the usual way.) And even if, in straightforward scientific cases, all this can be resolved satisfactorily, does it generalise to other kinds of judgement, e.g. moral or aesthetic judgements? Is it true that one ought to do unto others as you would have them to do to you? Is it true that Adolf Hitler was a bad man and Albert Schweitzer was a good man? Is it true that wisdom is a virtue and greed is a vice? Is it true that Bach's music is beautiful and my tuneless whistling is not? One might reasonably have firm beliefs about any of these without being able to explain how it is supported by logic or empirical data.