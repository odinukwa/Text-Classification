Now there is a question we commonly use this technique to maintain the parent child relation i.e we store all the entities in one tables with a parent_id column and all top most parents have 0 in the parent_id column this is a good and normalized technique i agree but there is a disadvantage also, itâ€™s slow and inefficient. This is mainly caused by the recursion like for each parent we have to run the query again and again to make a tree 

I have looked at the solutions some might try to do it with any programming language by running query again and again which makes a loads on server , Some have provided the stored procedure but that also involves the recursion. So my question is can we do it with one database query for the tree(joins or subqueries) if we know the depth or if we don't know if it is possible so how can we get the top most parent(i.e parent_id=0) of any child if it is not possible then why this technique is so famous while it has the flaws or we have another solution for this? . i have added the sql fiddle but it only has the schema FIDDLE 

You can use LAST_INSERT_ID() to get the inserted id for example if you insert a row in your photos table and you need that auto incremented id for the detail table you can do so 

Here is the query which gets the accounts for a specific user the products are related to accounts in relation so for each product there are four permissions All the accounts which is not and also shows their permissions I got all accounts according to scenario but the problem is if one account has more than one products then it obviously shows account id more than one time *What i am looking for to the after the clause which checks the permissions * but no luck getting syntax error 

It sounds somewhat like you are hitting a bit of memory pressure. Dirty Buffers are pages in memory that have changes in them that have not been written to disk yet. Normally they would stay in memory until one of the various processes clean them up and/or write them to disk. There are some things you will want to look into but with only the information you have given here I have to ask, "How much ram have you reserved for the OS?" You are running a 64 bit version of Windows. Now, the 32 bit kernel will use 4 GB of RAM. However, we have found (the hard way) that 64 bit versions of the OS will use 8 - 12 GB of RAM dependent on how busy the Sql server is. By that I mean are you doing a lot of reading and writing to disk. If the OS can't allocate the memory it needs for this (because the Sql server has sucked it all up) you will see the CPU being consumed for the additional context switching it needs to do to write those buffers out to disk. When you added the additional RAM to the server did you reset the Sql Server Max Memory setting? When you only had 16 GB of RAM your Sql server was constrained as much as the windows kernel was and so you were likely using virtual disk (hard drive space) quite a bit. When you added more RAM you opened things up a bit and now the Sql server is attempting to keep more of the database(s) in memory. 

I have 5 tables that contain data split by quarter each table has a similar structure to the following: 

We were unable to start Service Broker on as SQL Server said it was already running. The databases were pointing to different & files. Why does the Service Broker use the logical names and not the database names? I always thought that the scope of logical name was restricted to a single database? EDIT This is the SQL Statement that was used to enable Service Broker 

There is also a CHECK constraint on each table to ensure that the is within the specified quarter. I have then created a View across these tables which I am querying against. I need to execute several queries which aggregate the date by week, month, 3 months and 12 months. When I look at the Query Plan for my query it is querying all tables regardless of the filter on the DateStamp column. I have a controller Stored Proc which in turns calls a Processing stored procedure. The processing stored procedure accepts the start and date as parameters, assigns them to local variables within the procedure and runs the query. The query is similar to the following: 

If I open up a command window and do this manually it works fine. e.g. Typing this in the command window 

I have multiple databases that I want to store in one data warehouse database. I am wondering how I design the import process to handle multiple lookup tables. For example, say I have 5 databases all with the lookup table CustomerState. In one datatabse it could look like this: 

Both multiple files in a filegroup and multiple filegroups are tools for managing your database. The first lets you manage your IO and both will let you manage your backups. It is possible to backup a single file of a database as well as a single filegroup. Be sure to backup the tail of the transaction log when you do if you are planning on restoring it somewhere. Database files allow your multi-core CPUs to have multiple read/write streams to the database without hitting higher disk queuing values. It may help to think of the filegroup as a logical division and the file as a physical division. If you have multiple filegroups you will automatically have multiple files as a file can only belong to one filegroup. It can also help (if you have enough cores on your server) to have multiple files in your filegroups. That can give you the best of both worlds. You assign database objects to a filegroup not a file. You can put the files on different physical disk arrays. When I first started doing database work it was common knowledge that you put your data and log on separate disks. If you had the budget you would put your non-clustered indexes on another disk. It's tough to get that these days with SAN technology everywhere. However, SAN is a management tool not a performance tool. As you pointed out having different filegroups will allow you to isolate high traffic tables from each other and from lower traffic tables. It will also allow you a limited additional protection from a corrupted database potentially limiting data corruption to a smaller part of the data.