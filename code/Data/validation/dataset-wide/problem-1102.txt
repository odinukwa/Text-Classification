It is a wrapper around PostgreSQL tool, that can be used to upgrade a database in place, hence not loosing anything, including between major versions. This article ($URL$ can give you a lot of information on how to upgrade PostgreSQL, and what happens when you use . 

so should be called only if directory is not already populated, so in your case the database just got initialized in a new directory, empty, while your data is still in another directory. But things have changed recently. Homebrew 1.5, released on January 19th 2018 has this in its changelog: 

But the algorithm by itself is a little dense. The gist of it is: Briefly stated, the Unicode Collation Algorithm takes an input Unicode string and a Collation Element Table, containing mapping data for characters. It produces a sort key, which is an array of unsigned 16-bit integers. Two or more sort keys so produced can then be binary-compared to give the correct comparison between the strings for which they were generated. You can view the specific Latin sorting rules here: $URL$ or more directly and specifically for MS SQL: $URL$ For the character it shows: 

step 4, Compare sort keys: Basically the third value determines the order, and it is in fact only based on the last digit, so the order should be: 

Based on the description, this is related to Amazon Aurora product, and not PostgreSQL by itself. See $URL$ 

If you want more, you need to increase the number of connections (backends) which default at 100, see this other part of the documentation 

In that way you do not "pollute" your table with input by users. It has the same "likes" as your own (you will just need an extra or similar) but not the dislikes. As the dislikes, it is another extra table, that has to be taken into account in your requests. You could even "merge" the 2 tables with just: if you specify in a constraint that either or are NOT NULL (but not both at the same time). You will then obviously need to accept having a lot of NULL values. But again, I recommend you to think about how you will query this data afterwards to find the model that most suits your needs. Also how often the "other" case could happen? It is more like 1%? 10%? 90%? The design and optimisations would depend on this too... 

Remove the column has it can be computed from other database information. Create a view over with something like: 

Now, the solution I have thought of is highly inelegant. I have considered saving the client details for every sale in the Invoice table, and the item details in InvoiceItems. Of course, that would be a great deal of duplicate data. Another solution might be to keep the defunct details of items and clients, which would mean polluting my ItemsDetails and Client table with records no longer valid. Of course, I can add a boolean IsValid column, but then again, the solution does not seem to be succinct. Any help would be heartily appreciated. 

So, in PURCHASES table, we record the Material we have bought and the Quantity of it. With each purchase, we need to make three modifications: 1. Insert the purchase details in PURCHASE 2. Modify CurrentStock in MATERIAL(increase it by the amount we have purchased) 3. Insert the amount of increase in Materials in STOCKHISTORY table. Please note that StockHistory does not record the cumulative stock, but just the amount of increase/decrease associated with each transaction. Similarly, for each sale, we need to make three modifications. 1. Insert the sales details in INVOICE. 2. Modify CurrentStock in MATERIAL(decrease it by the amount we have purchased) 3. Insert the amount of decrease in Materials in STOCKHISTORY table. Please note that INVOICE table here represents the invoices generated by us, i.e. the sales made from our end. Here is the schema: 

Pens x1 (nickname) --- Pen (MaterialType) ----- 1 (NumberOfPieces) Pens x2 (nickname) --- Pen (MaterialType) ------5 (NumberOfPieces) 

Disclaimer: I am a beginner in the field of database. I am trying to design a database in PostgresQL which will record the sales and purchases of items. I intend to use this database as the model with EF and WPF using MVVM approach. Here is the problem with the design. This is the simplified description of my schema. I have a Clients table, an ItemDetails table and a RateChart table, among others. The Clients table records details for each client, including their address. The ItemDetails table records details for each item we sell to our clients, including a short description for the items. We do update the description from time to time. The RateChart table has the different rates for items for each client. Now, I also have to save the details of each and every sale. So, I have an Invoice table, and InvoiceItems table. The Invoice table saves the invoice number (primary key), the date, the total amount of the order and the client name(foreign key, refers to client name in client table). The InvoiceItems table, on the other hand, records the items sold, the quantity of each item sold and the rate at which it is sold. Now, here's the problem. The items sold in InvoiceItems list is a reference to the ItemDetails table where details of all the items are saved. Now, the details of items are changed from time to time. For example, the description of items are changed. So, whenever I change those item details, those changes will cascade down to this InvoiceItems table, and will make the older invoice records erroneous, because that was not the description when the product was sold. Same problem lies with the Client field in sale table. Whenever I change an attribute any record in the client table, the changes will cascade down to the sale table. Here is the schema for illustration. 

So Amazon added some settings. You will need to contact them directly and ask since their documentation does not provide a result when doing a search, $URL$ gives Your search for "shared_heap_size" returned no results. 

This explains your results when ordering on except that ē does not exists in code page 1252, so I have absolutely no ideas what it does with it. Or if we do the Unicode algorithm by hand, using the keys value of DUCET at $URL$ : step 1: Normalization form D, so each case becomes: 

This question is not so related to databases but more on Unicode handling and rules. Based on $URL$ Latin1_General_100_CS_AS means: "Collation uses the Latin1 General dictionary sorting rules and maps to code page 1252" with the added CS = Case Sensitive and AS = Accent Sensitive. The mapping between Windows code page 1252 and Unicode ($URL$ show the same values for all characters we are dealing with (except e with macron that does not exist in Microsoft mapping, so no idea what it does with this case), so we can concentrate on Unicode tools and terminology for now. First, let us know precisely what we are dealing with, for all your strings: 

It is a very broad question, even more so as you give absolutely no details about your setup and kind of database (volume, type of queries, active connections, size of RAM, dedicated server or not, etc.) You can start by enabling PostgreSQL to log slow queries, see the in the configuration. That will give you historical data that you would then be able to analyze and maybe correlate with other things (like from the list of @Vérace) You will then have various tools to help, as described on $URL$ : 

It depends if you need something generic or if it can work with a "small" number of columns. In your specific example, this query: 

The Unicode Collation Algorithm is described here: $URL$ Have a look at section 1.3 "Contextual Sensitivity" that explains that the sorting can not depend on just one character after the other as some rules are context sensitive. Note also these points in 1.8: 

Indeed, you should not have uninstalled it and re-installed it, as PostgreSQL starts a new database store from scratch, hence empty. BTW it may happen that your data is still there, but in another old directory. See for example this article ($URL$ that shows an upgrade between 9.4 and 9.5 but it may apply in the same way in your case. If you look at the PostgreSQL formula you can see: 

Continuous integration for the database can be implanted through version control and merging. There are a number of ways continuous integration can be implemented depending on the configuration of your development team and the change management processes used. Regardless, DBmaestro (which I work for) has the ability to generate and or execute database changes in an automated fashion, thus becoming an integral part of any database oriented Continuous Integration solution. A build process can request updates from version control, merge automatically create merge scripts to update the target database change, deploy the database changes, and then execute any tests defined directly against the database, or include the application code as part of the process for a complete integration test. The problem is that simple compare & sync does not utilize the version control repository when performing the compare and generating the DDL script. For that reason the version control repository does not act as the single source of truth. Another problem that simple compare and sync runs into is, that because the information is stored in the ALM, CMS, or version control repository, which is external to the compare & sync tool, it compares the entire database and shows the differences, generating relevant and irrelevant deployment scripts. Most critically, simple compare and sync does not ensure the deployment script handles conflicts and merges them. Database enforced change management on the other hand, combines enforcement of version control processes on the database objects with generation of the deployment script when required, based on the version control repository and the structure of the environment at that time. I would suggest reading this whitepaper for more info 

I would suggest reading this whitepaper on database version control from DBmaestro TeamWork (where I work) Basically, it discuses how database enforced change management combines the enforcement of version control processes on the database objects with the generation of the deployment script when required based on the version control repository and the structure of the environment at that time. This approach uses build and deploy on-demand, which means the deploy script is built (generated) when needed, not as part of development. This allows for efficient handling of conflicts, merges, and out-of-process changes. It handles the challenges by: Ensuring all database code is covered – structure, business logic written in the database language, reference content, database permissions, and more are managed Ensuring the version control repository can act as the single source of truth – the enforced change policy prevents anyone (developers, DBAs) using any IDE (even command line) from modifying database objects which were not checked-out before and checked-in after the change. This guarantees that the version control repository will always be in sync with the definition of the object at check-in time. Ensuring the deployment script being executed is aware of the environment status when the script is executing – building (generating) the deployment script when needed (just before executing) guarantees it is aware of the current environment status. Ensuring the deployment script handles conflicts and merges them – by using baselines in the analysis, the nature of the change is known and the correct decision whether to promote the change, protect the target (ignore the change), or merge a conflict is easy. Generating deployment scripts for only relevant changes – the integration with application life-cycle management (ALM) and change management systems (CMS) enables you to assign a reason to the change, as is done in the file-based version control or task management system. Ensuring the deployment script is aware of the database dependencies – the sophisticated analysis and script generation algorithm ensures the DDLs, DCLs, and DMLs will be executed in the correct order based on the database dependencies, including inter-schema dependencies. 

Material records the material whose stock has changed. Amount records the amount by which the stock has changed. Invoice records the invoice number of the transaction. 

Now, if we need to delete or modify any record of PURCHASES or INVOICE, we also need to modify the CurrentStock in MATERIAL as well as the corresponding records in STOCKHISTORY table. Here is a rundown of STOCKHISTORY table: 

Now here is where it gets murky. And I need help with the schema here. The FlowDirection records whether it was a purchase or a sale. The PurchaseId records the invoice number of the purchase IF it was a purchase. The SalesId records the invoice number of the sale IF it was a sale. There is a constraint on the table where the nullity of PurchaseId and SalesId attributes are XOR'ed, i.e. One and only one of them will be null for a given record. I am thinking of getting rid of the FlowDirection as it is obviously redundant. Secondly, I also will get rid of the Invoice attribute, as it is already recorded in PurchaseId/SalesId. Or, I can point Invoice column to a union of Num attribute of the INVOICE table and the Invoice attribute of the Purchase table (I have got to rename all these tables and attributes too). Which one should be a better approach? 

Disclaimer: I am a beginner in the field of database. I am trying to design a database in PostgreSQL which will record the sales and purchases of items. I intend to use this database as the model with EF and WPF using MVVM and Repository approach. The simplified proforma is like this: Items: Details of items we deal with. Purchases: Records the details of items we purchased. Sales: Records the details of items we sold. Stock: Quantity of each Item in hand. Now, I also have a stock table where current stock of each item is maintained. The dilemma I am facing is this: When any Invoice is generated, it indicates that a sale has occurred, and the stock of each item in the Invoice is updated accordingly. Similarly, when a purchase from our side occurs, we record it in the purchases table, and the respective items in Stock table has their quantity updated. I would have to also consider the scenario of correcting the stock table if an existing invoice is modified or deleted. These are the only three scenarios when the stock table would change. Now, I can achieve this update of stock table using Triggers. I can also use stored procedures in order to encapsulate the entry in Purchases or Sales table along with updating the stock table. I can also do this in application logic using LINQ. Which approach would be the most pragmatic one? THE DETAILED STORY Please allow me to discuss the entities that matter in this current context. We are a reseller. We buy materials in bulk, we process the materials and repackage them and then sell them. In this table, the items we sell are marked as the entity ITEM. There is are two attributes in the ITEM table worth noting. One is MaterialType and the other NumberOfPieces. The entity MATERIAL is the raw material we buy. Now suppose, we buy 100 pieces of pens, and we resell them in units of 1 or 5. So, in this case, the records in the item table would be