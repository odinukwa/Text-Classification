I can understand that you feel some pieces that you are used to are missing. But that is only because they are missing. Nonetheless, SQL Server was being successfully used when Foreign Keys were just a concept (which we implemented through triggers in those days), not a physical implementation such as a constraint. Declarative Referential Integrity was there at least by SQL Server 7.0, but much weaker than the current implementation. Regarding the value of the Clustered ColumnStore Index it does provide an index and the rows are updateable. You might find this discussion valuable: $URL$ Manoj points out that there is a way to create an Indexed/Materialized View on top of this table, with Clustering Key as the PK (1st column of the table/view). Whether that suits you, of course, is a decision you have to make. But, as Aaron Bertrand and TomTom commented, this is all about better performance. If you can manage the other issues that concern you (and I believe that they are manageable) then you get quite a few benefits. So use the ColumnStore for what is is able to do and manage the missing features yourself. 

The restore will be followed by some upgrade steps to bring the SQL Server 2005 database format up to the SQL Server 2008 database format. (Note: You will not be able to restore the 2008 database back to 2005.) There are limits to how early a version of SQL Server can be restored to 2008, but 2005 should work fine. Here is a link to the supported upgrade paths to SQL Server 2008: $URL$ 

If the samples at boundary layer should not be reported, replace the comparison of with as mentioned by ypercubeᵀᴹ and is shown below: 

Then via Logins or a Domain group, granted users a membership in the role in . EDIT: The way we implement this permission is through Domain Group login(s) such as (). Then add the group login to 's role (for running ) and to 's or . After that we always add or remove logins from the to control the group membership for those rights. 

The lifespan of a login with enabled is set by Windows. I understand that in a Domain, the property is set at the domain level then propogated to the Windows account. I do not believe that you can directly change it. 

Having too many indexes can indeed cause performance problems. If many indexes have very similar statistics it is possible that the optimizer cannot reliably decide on the most useful choice of indexes. (I learned this when working with a database where almost every column was indexed.) In that case, we reduced the number of indexes significantly by removing indexes on columns that would seldom be used. This greatly improved the performance of our queries. In addition the too many indexes caused (1) more space to be used for little benefit and (2) consumed more server resources to keep all the excess indexes updated. So, yes, indexes can really help your performance, but you need to be reasonable in how many you create. Focus on the indexes that seem most useful to you. Additional Information: Many database vendors include tools to help you analyze the value and usage of the indexes. For example: 

This way, the and clearly identify not only whether the is active, but could also preview for you the approaching . Likewise, a future could also support setting up a that will not be activated for a few more weeks. The is not strictly needed, but can be a useful shortcut for querying active data without using dates. You just need a process to update the column setting as defined to be in the inclusive range of the and . Using a design like this keeps all the information in one table, which I assume will not overwhelm your query processing, so that you can simply filter the data according to your needs. Of course, if you feel that the would server you better for some other needs, then make the choice to also create that table. 

There are also scripts, including this powershell script, but they use methods that talk to msdb: $URL$ 

Since linked servers are used to communicate with another server, in this case a newer version, it will all probably work for you. However, you should check that you are not depending on an object (of any type) that has been modified and is no longer compatible. (E.g. in the database becomes .) 

However, my bigger concern is that the is not counting what you expect. EDIT: The poster confirmed below that this is returning what he expected. 

No SQL Server version mentioned, just in case that makes a difference. However, if you have sufficient rights you can connect to the server using the Object Explorer / Connect drop down and choose "Analysis Services..." rather than "Database Engine...". This will expand to allow you to see some details of the Analysis Services databases. Just by using the Object Explore you can explore some details for the Analysis Services databases. This is quite rudimentary, but may get you started. After that you need to learn how to query for the details. Here is some guidance, based on the soon to be released SQL Server 2016. $URL$ 

The Maintenance Plan task can reorganize indexes and that should have some benefit for you. Likely, however, you will reorganize more than necessary using that approach. You should look at Ola Hallengren's solution for Maintenance at: $URL$ 

On the question of the size of things, first of all you said that you shrank a database, so that would reduce the DB size of the space that was discarded during the shrink. John M also provided some clarification. Regarding the numbers in the bottom picture, they are: 

However it still exists in the stopped state. This is the state you are reporting as 'Paused'. If the stopped trace should be ended, then a second command is needed: 

The integer will provide a small key for use in joins, referential integrity, and so forth. This makes all the joined tables smaller than if you used the longer as the technical . By the way, the longest index in SQL Server is 900 bytes. You mentioned 60 characters in a column defined a . I assumed that was just over definition and it would be smaller. I suggest trimming that down to a more reasonable and usable length that SQL Server can absorb. So, as long as you can see that and are separate concepts, you can make the appropriate decision for each use case. 

Perhaps creating some denormalized data in advance would speed up the process. That, where possible, could reduce the number of rows involved in your ultimate query. This could mean a persistent table that you maintain. But it could also be implemented as a temp table (e.g. #criteria) that could then be joined more simply to get your final results. There is nothing special about those; it just reduces the complexity of the queries, giving the optimizer easier job steps to work on. Of course, if you are still suffering with the performance, you would need to analyze whether this is useful in your case. 

You state that "some of the columns and tables may be renamed or disappear completely." I would interpret the corollary to that is: most of the columns and tables remain as they were. Therefore, assuming that assessment is correct, rather than trying to identify everything you should build a test database to make the changes. Restore a copy of your database and name it . Then make your changes in the test database. Once you have changed the tables, views, (and aliases?) then use some software to compare the tables in with and report the changes. There are plenty of tools to do the compare. I happen to use Red Gate's SQL Compare, but there are plenty of other products. I even see a 'free' compare tool at $URL$ Using such a tool can report all the changes made and could simplify your work by focusing only on what changed. Of course, you will want to change your code to match the new table and column definitions. One simple, but tedious method is to paste the code (scripts, stored procedures, etc.) into the SSMS Query pane and run the Query / Parse command. I see that there is a Code Project to parse code at: $URL$ I have not used the code project, but perhaps you will find it useful. EDIT: I had suggested a copy of the database, but since the metadata is all that is needed for your test, perhaps the organization would be willing to create a database that contains no data. 

Selecting and deleting a large number of rows will lead to blocking, which is probably why you cannot use the system for a couple of hours. The easiest way in SQL Server Express to control blocking is to control how many rows you are deleting at one time. For example, you might read Aaron Bertrand's notes here: $URL$ He points out that deleting (or, in your case, copying and deleting) smaller numbers of rows at one time allows the deletes to succeed, then frees up the locks so that other work can progress, and so on. See Aaron's code samples and the approaches that you might use. As he notes, one size does not fit all.