The rationale of course is that if your productivity is very low, you have to work very long and exhausting hours to even survive. As your productivity raises, what you want is not to work even more for the extra income, but thankfully to get some rest. The empirical regularity is that most of workers in the world are away from the upper "backward" movement of labor supply - but in poor countries, they may very well be in the lower backward part. And I think this resonates with the information presented in @JohnL. answer. So chances are, you will find such preferences modeled in studies that focus on poor/developing economies. 

"Double-entry" bookkeeping is the resulting methodology of the fundamental approach of Accounting science to what a productive mechanism does (be it a company or whatever): Accounting perceives productive entities as mechanisms to transform capital from one form to another, irrespective of what their economic goal is for doing this (profits, charity, whatever). The Liabilities+Equity side shows "how much capital and from whom" has the firm sourced and put it under its control, and the Assets side shows to what forms the firm has transformed this same capital . But if it is the same capital, it necessarily has the same value : this is why accountants write the same number twice: they write it once and link it to a code that shows from whom did the firm get this amount of capital, and they write it a second time and link this to a code that represents to what has the firm transformed the capital. So I took a capital of 1,000 from a source called, say, Shareholders, (or Deposits, or a combination) and transformed it in another form of capital called "Loans to customers". I write the number down two times, one linking it to the code that Shareholders have, and a second time to link it to the code that "Loans to customer has". At the same time "Liabilities + Equity" shows how much capital must the firm return back -because in the theoretic Accounting model, the company itself "owns nothing", it is just a capital transformation mechanism. It may has capital under its control, but it has eventually to return it. Now the customer appears and says, "no payback". It is the same thing as a fire destroying a building - an amount of capital has been lost. So the company can no longer return it. Given the general legal/social arrangements, in such cases, irrespective of the actual source of capital (say, Deposits), those who will now must expect to get lower capital back, are the shareholders. So the double-entry write-off of the bad loan reflects exactly that fact: "I, the transformation mechanism, inform the world that a) I just lost an amount of capital and consequently that b) given the rules, my shareholders have now a lower claim from me, as regards the return of capital to them". Not doing this, won't "be better for everyone", because it would be misleading - by looking at the bank's book, the shareholders would still expect to receive back their 1,000 investment, but they would eventually find out that only 100 are there in reality. I hope that helps. 

Yes, if the Average Cost function has an interior stationary point. Considering the derivative of the Average Cost function with respect to quantity, we have $$\frac {\partial AC}{\partial y} = \frac {\partial (C/y)}{\partial y} =\frac {MC\cdot y - C}{y^2} = \frac {MC - AC}{y}$$ Then $$\frac {\partial AC}{\partial y} = 0 \implies MC = AC$$. If the Average Cost function has such a stationary point (i.e. where its first derivative equals zero), will it be a minimum? Denoting $C''$ the 2nd derivative of the total cost function, we need $$\frac {\partial^2 AC}{\partial y^2} |_{AC=MC} >0 \implies \big[C''-\big(\partial AC/\partial y\big)\big]\cdot y - (MC-AC) >0$$ and since we aveluate at $y: AC = MC, \;\; \partial AC/\partial y =0$ we get $$\frac {\partial^2 AC}{\partial y^2} |_{AC=MC} >0 \implies C''> 0$$ The Total Cost Function must be convex at the stationary point, for it to represent a minimum for the Average Cost function. The Total Cost function used in the OP's example is $TC = 50 + y^3$. The Average Cost function is $AC = (50/y) + y^2$, and the Marginal Cost is $MC = 3y^2$. So $$AC = MC \implies (50/y) + y^2 = 3y^2 \implies 25 = y^3 \implies y^* = 25^{1/3} \approx 2.92$$ At that point, the second derivative of the Total Cost function is $C''(y^*) = 6y^* >0$. So for this Cost function there is a point where Average Cost is minimum, and where it equals Marginal Cost. The Diagram is Indirectly, the thing to remember here is that the nice $J$-shaped curve for the Average Cost function is most easily produced if we allow for fixed costs and a convex Variable Cost. 

I find hard to believe that the textbook authors are that incompetent, so there must be more to the story -and the OP should provide this additional input, otherwise the question should rightfully be closed. 

This simply tells us that the mathematical object $f(x) = 1/x$ is not friendly to all the economic concepts we want to associate/obtain from its use as a demand function. So, it is not the right tool to represent a demand function, we should use something else. Mathematically, this relates to the harmonic series which is divergent (think of the integral as a discrete sum). 

Let's ignore for the moment the existence of the expected value. If this was a deterministic set-up, linearization through taking logs would be straightforward, and without the tricks of the links the OP provided. Taking natural logs on both sides of the first equation we obtain: $$0 = \theta \ln \delta-\frac {\theta}{\psi}\ln \left (\frac{C_{t+1}}{C_t} \right )-(1-\theta) \ln(1 + R_{m,t+1}) + \ln (1 + R_{i, t+1}) \tag{1}$$ Set $$\hat c_{t+1} = \frac{C_{t+1}-C_t}{C_t} \Rightarrow \frac{C_{t+1}}{C_t} = 1+\hat c_{t+1} \tag{2}$$ Also, note that it is standard approximation to write $\ln (1+a) \approx a$ at least for $|a|<0.1$. Usually this is the case with growth rates and financial rates so we obtain $$0 = \theta \ln \delta-\frac {\theta}{\psi}\hat c_{t+1} -(1-\theta) R_{m,t+1} + R_{i, t+1} \tag{3}$$ which is a clear dynamic relation that links the three variables present. If in the model, the steady-state is characterized by constant consumption and constant returns, then at it we will have $\hat c_{t+1} =0$ and so the steady-state relation will be $$R_{i} = - \theta \ln \delta + (1-\theta) R_{m} \tag{4}$$ But we did all these ignoring the expected value. Our expression is $E_t\left[f\left(C_t, C_{t+1},R_{m,t+1},R_{i,t+1} \right)\right]$, not just $f\left(C_t, C_{t+1},R_{m,t+1},R_{i,t+1} \right)$. Enter first-order Taylor expansion of $f()$. We need a center of expansion. Represent the four variables simply by $\mathbf z_{t+1}$ (it doesn't hurt that a variable with $t$-index is present in $\mathbf z_{t+1}$). We choose to expand the function around $E_t(\mathbf z_{t+1})$. So $$f\left(\mathbf z_{t+1}\right) \approx f\left(E_t[\mathbf z_{t+1}]\right) + \nabla f\left(E_t[\mathbf z_{t+1}]\right)\cdot \big(\mathbf z_{t+1}-E_t[\mathbf z_{t+1}]\big) \tag{5}$$ Then $$E_t\left[f\left(\mathbf z_{t+1}\right)\right] \approx f\left(E_t[\mathbf z_{t+1}]\right) \tag{6}$$ Obviously this is an approximation, i.e. it has error, even if only because of Jensen's inequality. But it is standard practice. Then we see that all the previous work we did on the deterministic version, can be applied in the stochastic version inserting conditional expected values in place of the variables. So eq. $(3)$ is written $$0 = \theta \ln \delta-\frac {\theta}{\psi}E_t[\hat c_{t+1}] -(1-\theta) E_t[R_{m,t+1}] + E_t[R_{i, t+1}] \tag{7}$$ But where are the steady-state values? Well, steady state values in a stochastic context are a bit tricky -are we arguing that our variables (which are now treated as random variables) become constants? Or is there another way to define a steady-state in a stochastic context? There are more than one ways. One of them, is the "perfect foresight steady state", where we forecast perfectly a not-necessarily constant value (this is the concept of "equilibrium as fulfilled expectations"). This is for example used in Jordi Gali's book mentioned in a comment. "Perfect-foresight steady state" is defined by $$E_t(x_{t+1}) = x_{t+1} \tag{8}$$ Under this concept, eq. $(7)$ becomes eq. $(3)$ which is now the "perfect-foresight stochastic steady state" equation of the economy. If we want a stronger condition, saying that variables become constant in the steady-state, then it is also reasonable to argue that, again, their forecast will eventually be perfect. In that case, the steady-state of the stochastic economy is the same as that of the deterministic economy, i.e. eq. $(4)$. 

The $u$-nullcline On p.8 the authors define $\lambda(\theta) \equiv M(1,\theta)$ (I believe they should have written for clarity, $\lambda(\theta) \equiv M/u = M(1,\theta)$) . On p. 25 they specify the matching function as $M(u,v) = uv(u^\phi + v^\phi)^{-1/\phi}$. Therefore we have $$\lambda(\theta) = \theta(1 + \theta^\phi)^{-1/\phi}$$ This means that whenever $\theta = 0 \implies \lambda(\theta) =0$. On p.14, eq. $(8)$ the give $k> J \implies \theta = 0$. In the calibration stage, $k=8.02$. The $u$-nullcline is defined as $$u_{SS} = \frac {\delta}{\delta + \lambda[\theta(J))]}$$ All these together imply that $$J < 8.02 \implies u_{SS} = 1$$ Using the other relations specified, after a little algebra, we get $$J>k=8.02 \implies \lambda[\theta(J)] = \left(1-(k/J)^{\phi}\right)^{1/\phi}$$ which is indeed your code, a bit less convoluted. And indeed the $u$-nullcline never crosses with the $J$-nullcline. For values of $u$ near unity see below. For low values of $u$ we obtain \begin{array}{| r | r |} \hline \hline \text {u} & \text {Jss} & \text{uSS(in J units)} \\ \hline 0.03 & 12.25 & 23.06 \\ 0.04 & 11.53 & 14.13 \\ 0.05 & 10.86 & 11.75 \\ 0.06 & 10.23 & 10.66 \\ 0.07 & 9.64 & 10.03\\ 0.08 & 9.09 & 9.62\\ 0.09 & 8.57 & 9.34\\ 0.10 & 8.09 & 9.14\\ \hline \end{array} The $u$-nullcline stays always above the $J$-nullcline. I also "pushed" the $J$-nullcline by the factor $(1+\rho)$, i.e. the discrete version of the equation. But $\rho=0.003$, too small a mark-up, and so again there was no crossing. Therefore it appears that what went into the simulations of the authors is at some point(s) different from what the equations and the calibrated parameters (Table 2) appearing in the paper give us. Or, we are missing something. The $J$-nullcline I checked the OP's code line by line, including the values of the calibrated parameters. I did not find any discrpeancy with what the paper gives. I then copy-pasted the code for the $J$-nullcline into Gretl, tweaking it only to match the local language. A note: the way various magnitudes are defined in the paper, we have $$\psi_u > \psi_e \implies b >1 \;\forall u \implies \sigma = s/b < 1, \;\forall u$$ $$\implies \text {muOfU}\equiv \mu(\sigma(u)) = \min(1/\sigma, 1) = 1,\; \forall u$$ $$\implies \text {nuOfU}\equiv \nu(\sigma(u)) = \min(\sigma, 1) = \sigma, \; \forall u$$