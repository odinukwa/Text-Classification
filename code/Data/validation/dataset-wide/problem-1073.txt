Check the database size & compare it previous size to find the exact growth of the database Find Tablespace Status, segment management, initial & Max Extents and Extent Management Check location of data file also check auto extendable or not Check default tablespace & temporary tablespace of each user Check the Indexes which is not used yet Check the Extents of each object and compare if any object extent are overridden which is define at tablespace level Tablespace need coalescing Check the overall database statistics Trend Analysis of objects with tablespace, last analyzed, no. of Rows, Growth in days & growth in KB 

Check the objects fragmented Check the Chaining & Migrated Rows Check the size of tables & check weather it need to partition or not Check for Block corruption Check the tables without PK Check the tables having no Indexes Check the tables having more Indexes Check the tables having FK but there is no Index Check the objects having the more extents Check the frequently pin objects & place them in separate tablespace & in cache Check the objects reload in memory many time Check the free space at O/s Level Check the CPU, Memory usage at O/s level define the threshold for the same. Check the used & free Block at object level as well as on tablespaces. Check the objects reaching to itâ€™s Max extents Check free Space in the tablespace Check invalid objects of the database Check open cursor not reaching to the max limit Check locks not reaching to the max lock Check free quota limited available of each user Check I/O of each data file 

I currently do not see an issue. It's a valid backup strategy, now where or not it meets your recovery SLA in a disaster is another question. 

It may... only if you have an unordinary small workload. Just the worker threads alone would make me assume that you're not going to have a good time. This coupled with having 4 instances per server, that don't communicate with each other, that are all trying to step on each others' toes (so to speak) leaves me thinking this, "I am really glad I won't have to admin this!". My personal take I would step away from thinking of AGs such as this. If you want to consolidate, that's great and I'm all for it! However, it needs to be done in a way that doesn't end up hurting you or your customers. If it were me, and I was tasked with the same thing, I'd immediately push back on "6" servers. I don't yet know how many I'll need... unless of course we're not going to do any scientific research or testing - in which case I'd escalate my concerns. We don't know things such as the number of total databases, how much log generation each database creates, etc., which we'd need to start gaining an understanding of what will be needed. Undoubtedly you can consolidate these. Is it even the right thing to put them all in a single cluster? I wouldn't. Each cluster is a fault domain, and while there are some really awesome features such as distributed availability groups that still ends up being more than a single cluster. Think of it this way, all of your AGs are in a single cluster - what happens if you have an issue with said cluster? Are all of the AGs now down? Maybe. Probably. Either way it's not something I'd want to be dealing with at 5 am. Some of the main things you'll see when you have too many AGs or databases in AGs: 

If I understand your requirements correctly.... I would just use the natural key, LanguageCode-CultureCode ("en-US," for example). It's small enough. (I'm using the entire "en-US" as the primary key to differentiate it from "en-GB," for example.) 

You could do log shipping with the standby option rather than norecovery. The database would be read-only, and the users would get kicked off during each restore, but it might suit your needs better. 

Then set up regular backups for those databases, both full and log backups. This is an article on log backups that might help. This is a more general article on SQL Server backups. Good luck! 

Operating system error 2 is a standard Windows operating system error--file not found. Check the permissions on the folder and make sure that the account that owns the agent job has access to the folder and is able to traverse the path to the folder where the backup is trying to write. Unfortunately, this is a Windows error message and not a SQL error. I found something on Microsoft Connect (related to restore, not backups) where they said they were not able to reproduce the problem and confirmed that this is an OS, not a SQL Server, error message. 

I don't believe so, and part of the reason why I included the quote about the password above. The API calls used to do this expect a plain text password to be given. Since one does not exist for the gMSA in this instance the password would be blank. GMSA's do have passwords, but they aren't available in plain text and are base don generated values that rotate every so often, specified at creation time. The code that does this expects a plain text password and does not check to see if the account is a gMSA or not, thus does not use any special API to retrieve the account password for storage. My best guess would be, if we traced this all the way back to the calls you'd actually be receiving error 1326: The user name or password is incorrect. Now What? If you have very strong feelings that this would be helpful or that others would also like to use it, I'd invite you to create a Connect Item. If you do create one, please update your answer with a link to the item. 

Oracle Database instance is running or not Database Listener is running or not. Check any session blocking the other session Check the alert log for an error Check is there any dbms jobs running & check the status of the same Check the Top session using more Physical I/O Check the number of log switch per hour How_much_redo_generated_per_hour.sql Run the statpack report Detect lock objects Check the SQL query consuming lot of resources. Check the usage of SGA Display database sessions using rollback segments State of all the DB Block Buffer 

Analyzed the objects routinely. Check the Index need to Rebuild Check the tablespace for respective Tables & Indexes 

Database user creation with required privileges Make the portal of Oracle Predefined error with possible solution. Check database startup time(if not 24X7) Check location of control file Check location of log file Prepare the Backup strategy and test all the recovery scenario 

I would probably just detach/attach via script. Drop a list of database names into a file ("control.txt"), which you can get from sys.databases (removing the system databases). The script itself is: 

(If it doesn't let you because that account is explicitly denied, close the window, run the command line as a different user, and try again.) Once you're connected, you should be able to create a new login 

This is expected behavior for a log shipping job set to norecovery. The logs are being restored with norecovery, which means that more logs can be restored on top, which is what you want. No other transactions can occur on the target database; it's a copy of the original. If something goes wrong with the original, you bring the target online with . I've never used log shipping with any options other than norecovery, but there's also a standby option that will allow the target database to be used read-only (but the users will all be kicked off during the next restore). If this doesn't suit your requirements, maybe you need to look at something other than log shipping. 

The one without the listener might be working fine... does it still work fine on a failover? Are they using some other DNS alias to "act" like the listener may. The one with the listener, what does their connection string look like? If it isn't using the listener name in the connection string then of course it isn't going to work... they need to point it to the right place, that's why the listener exists! The connection to the listener may also fail if there are multiple subnets, older client libraries are used, or certain keywords aren't in the connection string. If there are multiple subnets, the client driver should be something that supports the MultiSubnetFailover keyword and this should be set to TRUE. 

The reason that this fails outside of the internal Rackspace location is due to the URL Endpoints being set to a value that is not able to be connected through from your local environment. I discuss this process at a high level in this blog post, however to quickly recap the point that needs to be made here: The endpoint url is the address where the connection will be routed in order to connect and run their queries. [roughly speaking] 

It sounds like no one took log backups on that database for a very long time. Restore the MDF and try: 

You're using master. Like the error message says, you can't do that in master. Try using the TSQL2012 database. 

Like Aaron said, we don't know anything about the application or data (or number of users, or the hardware running the database, or...). I will say that in my experience, SQL Server is much faster and more robust than Access for things like web applications. Whether that means your application will run faster, well. Not enough information. 

I second Craig Ringer's recommendation of IMAP. It's already got all the relevant standards built in. If you're determined to proceed with creating a webmail client/email server architecture written as a web client with a database server rather than pre-written email server back end, I'd look at how Exchange has historically handled mail databases. Example links include this description of 5.5 (old), this newer description, and this overview.