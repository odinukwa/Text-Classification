A very lightweight solution is to utilize the services of $URL$ If you want total control, and have a spare machine to use as a server, the combination of Squid and SquidGuard is a pretty versatile solution. 

Sounds like billable busywork to me. Aside from the fact that many consumer appliances use the 192.168.x.x address space (which can be exploited, like anything else), I don't feel that really changes the security landscape of a corporate network. Things inside are locked down, or they aren't. Keep your machines/devices on current software/firmware, follow best practices for network security, and you'll be in good shape. 

It's a matter of configuring DNS correctly, combined with configuring virtual hosts on your web server. If you have the ability to manage your site's DNS entries and your web server configuration, this is easy enough to do. 

Sampling a random CD-ROM drive in the old junk box shows that it's rated at 29W. Even if it ran all the time, it wouldn't amount to much. I'm guessing that the power draw while idle would be negligible. EDIT: Worst case scenario example for my $0.08/kWh, my CD drive running at its fully-rated power draw of 29W would amount to $20.32/year or about $1.69/month. 

Not 100% certain about the first two questions, but there are lots of options for #3. I personally use CCleaner to scrub the cruft from the family PC automatically once a day. If you run it by hand, it should give you list of files found, depending on the function you're using. For #1 you could also try running "dir /s" while standing in that "Temporary Internet Files" from the command prompt. For #2 it could be from any software that does automatic updates (such as Adobe Acrobat viewer, Sun's Java, and MS's own Security Essentials and Automatic Updates). This is just a guess, as I'm not 100% certain where these apps store their downloads. 

I'd brute force this one: run tripwire on the entire device for a baseline, then run a check some time later and the offending directory will stick out like a sore thumb. 

Have you looked at Linux's network block device (nbd) driver? I don't know how well it handles high-latency (it's been years since I've played with it), but it may be worth looking at, assuming the project is still around. 

I use SSH keys, use the same one for all servers, and maintain a good password on my keyfile. Saves a lot of aggravation. For devices where this wouldn't work, I'd use a password that had a hard-to-guess core, then use the devices dns name, IP, or other common characteristic (such as OS or brand), to make the password unique for the device. This worked especially well for groups of similar devices. Just keep the pattern/mnemonic secret, along with the core, and you have a difficult, unique password that was easy to remember. 

I've done both. I once worked for a large shop (a university computing center), and transitioned to one day in-office and four days at home. It worked out very well for two years before I moved on. I currently only do remote work as a consultant/contractor and I routinely do work for people I've never met in person. It all depends on what is expected of you. 

I've tried adding in a new virtualhost to serve the old domain name and test pointing it to a different documentroot, e.g. 

RedHat 6.2 Apache 2.2.15 I've installed a new SSL certificate on my apache server and updated the /etc/httpd/conf.d/ssl.conf file to include the new details: 

I have 2 old servers, which I am not using any more, which have been replaced by 1 new system, and I want anyone who tries to go to the old domain names, to be redirected to the new one. So, for example, anyone who tries to go to a.mydomain.com or b.mydomain.com should be redirected to new.mydomain.com What was done was that a member of the network team simply changed the IP address associated with a.mydomain.com and b.mydomain.com names to point to the new server, where new.mydomain.com is being served. This works fine if you go to the top level, e.g. if I go to $URL$ it will redirect to $URL$ perfectly fine. However, if I try to go to a full file URL of the old name, e.g. $URL$ instead of redirecting to $URL$ it redirects to: $URL$ (Missing the slash "/" ) and therefore obviously fails, because that's not a valid url. They said it was a server configuration thing, so I've tried messing around with the virtualhost on the new server. I've tried using the old domains as ServerAliases, i've also tried actually just setting them up as separate virtualhosts to point to the same document root, but I am still experiencing the same problem each time. Does anyone know what might be causing this? And if it's something I need to change on the server, or if it's something the network team need to change in the DNS? (I am using CentOS and Apache) Thanks. Edit: This is what the virtualhost on the new server looked like originally: 

And I even commented out the new.mydomain.com virtualhost, but whichever domain you go to in the browser it goes to the same documentroot. 

After restarting httpd and visiting the website it is still using the old certificate which expires in a few weeks. (I also tested by running the domain through $URL$ I confirmed that it is defintely using that ssl.conf file, by removing it and then the server failed to load. And then confirmed that it was defintely loading those files, by changing to an incorrect path and the server again failed to load. I ran the following from the terminal: 

We have 2 separate VMWare environments, one is the main environment which has hundreds of virtual machines across lots of sites. The other is a much smaller one installed on one server, just for archiving old systems. What I would like to do is take a snapshot of the current state of one of our live VMs, and use that to copy across to the other VMWare environment and create a new machine there, using that as the archive of that system. Is this going to be possible/easy? 

Could anyone advise me what I need to change in the configuration so that accessing through the external IP address loads the new certificates? Thanks. 

A couple of other things to try would be to compact the database itself (I assume Access still has this function) and defrag the actual database file on the machine doing the sharing. For defragging single files, I recommend the sysinternals contig command line utility. You could also test for a bad network, by running ping for an extended time (I believe "ping -t" is the correct Windows incantation of the command), and seeing if you're dropping packets or seeing high network latency. 

Someone could be throttling your HTTP traffic (at the application layer) between your home and server. I'd be curious if any other protocols (besides SSH) have fast transfers while HTTP does not. You should try https, ftp, and maybe even something like rsync, git, or svn. Between A and B and the VPS, you should also try other protocols? From what you've said so far, it doesn't seem like a problem with the http server itself. Is your home connection also in Sweden, or does it cross any national boundaries? 

The "wget" utility has a --mirror option you can use. I've used it a lot for archiving sites, and it does the job well enough. 

This is an absolute lower bound on the cost of having the machine plugged in for a year. With typical use it will be much more. 

I think most any option will be "manual stuff". Under UNIX, it's pretty standard to unmount/remount a device before each benchmark run, often with a "newfs" thrown in for good measure. I don't know if you can use command-line tools under Windows to unmount/mount devices, but if automation is your goal, then it would be worth looking for such utilities. 

You should check out asciidoc. I've made a few short things with it, and the output it pretty sharp (and customizable, of course). The plain text is very readable by design, and you can have it output docbook, HTML and PDF easily. Using any number of other converters you can transform it to other formats, too, such as CHM. Very versatile package, though, being a UNIX-centric package, I don't know how the Windows support is. 

It's a little archaic, but you may be able to use something like kermit to use a modem-era protocol (zmodem, etc.). Looks like there's a program meant just for that purpose, too. I once needed to download a small-ish file from a remote unix server without any supporting tools, so I uuencoded the file, dumped it with cat to the terminal, and then captured the the resulting text with my local terminal program, where I uudecoded it. Sick, eh? :) 

This will give you a sorted list of all files, starting with the largest at the top, in KB and without crossing onto other mounted drives. If you only have a single, huge / partition, then replace "/foo" with "/". More often than not, you have a small number of large files that are eating up space, such as log files, core files, or crash dumps. It will really pound the server, so either nice it and/or run it when the machine can handle the extra load.