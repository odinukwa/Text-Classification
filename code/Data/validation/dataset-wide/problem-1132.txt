If you would now re insert record 3 and 4 on the source database you would get a PK violation error when these rows are replicated to the subscriber. That won't work. And I'm assuming that this is what you mean by "Replication don't work" But more important. If you wouldn't do that, you wouldn't notice anything. But you would have more records on the subscriber. 

The prefered method would be to alter the merge agent profile. See this link for a detailed explaination on how to do that: $URL$ The setting will take effect the next time the merge agent runs. 

You won't only see ENCRYPTION_SCAN resource in your wait list when Encryption (like TDE) is used. Certain operations will take a shared lock on this resource to make sure the database is not being encrypted during the operation. The moment you would encrypt a user database with TDE, the tempdb will also be encrypted (otherwise, you would have security risk when User data is used in temp db). Therefore, some operations will take a shared lock on ENCRYPTION_SCAN in Tempdb to prevent Tempdb from getting encrypted. Here are two examples: BULK INSERT 

Make sure that your session that you are using to do the restore is not using the database you want to restore. 

What where the exact steps you took, from the moment you found a problem until the moment you discovered that replication didn't work? What was broken, what was repaired, what was lost? All of this information is available in the output of . What exactly do you mean by: "Replication don't work?" Did you get any errors, or are tables not the same? Is data missing? 

Welcome to MS SQL :-) To start with your question: Yes there is. First you make a full database backup: 

start process explorer and find the SQL Server process. right click and select properties look at the thread tab. Sort on the CPU column and note the thread id (TID) that is consuming the most CPU. 

disclaimer: Hopefully you'll agree that this answers your question "How to prevent dropping a login" but in no way is this a proper root cause fix. However, I know that there are some pretty nasty legacy applications where it's not possible to change the code... I must admit, this one just made me smile.. 

Deadlocks and blocking locks are two different concepts that you need to understand. A deadlock is a situation where process/action 1 is waiting for process/action 2 to finish and at the same time process/action 2 is waiting for process/action 1 to finish. In other words. They would wait forever since they are waiting on each other. In your scenario, something else is happening: Process 1 is doing an action and has taken a lock on a resource to complete that action, Process 2 now wants to start a action that requires a lock on the same resource. Process 2 now has to wait for process 1 to complete and the lock is released. The key here is that at any given moment, none of the processes are waiting for each other (at the same time). One process is just waiting for the other process to finish an action on the same resource. They are not waiting for each other. I hope that's clear. On to how we fix your issue: Can you post the Table definition, the indexes on the table and the delete select statement. We could have a look to see if there are ways to make the likelyhood of blocking locks less. 

So every sunday, asuming you had changes every week, the diff backup will be bigger then the previous one. Untill one sunday, it's just as big as a full backup would be. That would imply that all data pages had changed. At that moment you want to do a new full backup. Since that will reset the differential bitmap and your next diff backup will be small again. (depending on the amount of changes) You could automate the process of making diff backups untill a certain percentage of the data has changed. Have a look at this blog post: $URL$ It explains a way on how to programatically look up the percentage of database changes. That way you could say.. make diff backups if percentage changed < 75% otherwise make a full backup. Now could you tell me why you want this? Because this isn't a setup I would advise a starter (with exceptions). So please elaborate a bit and we can see if this is the best solution for your setup. How big is your database? how much changes do you have? is batch based changes? Which version and Edition of SQL do you have do you have? 

It adds the switch /X86 to the Dtexec execution string. (please note that this switch will be ignored if you directly run it from the command line.) However, be aware that in that case, you are running the package in 32bit while not making use of the 64bit benefits. This shouldn't have to be a problem, just something to keep in mind. 

It's by design. I can understand why it could be confusing to be able to see partial meta data. However, the idea behind the data role db_datareader is to grant a user read access to both user data as well as column information. (you wouldn't be able to create a SELECT statement if you didn't have access to the column names as well.) So with that in mind, showing only the part of the index definition that contains column info would be perfectly within the boundaries of the datareader permission. The filtered index clause however is considered non column information so therefore you need more permission to be able to see this part of the definition as well. You need VIEW DEFINITION permission on that object, or VIEW ANY DEFINITION to be able to see that extra bit of meta data. I think it would be nice if they would give a warning when you generate a " incomplete" CREATE script. You can test it yourself: 

"Just" making a snapshot of volume on your Netapp can not be considered the correct way of making a backup of SQL server databases. However, Netapp offers Snap manager for SQL Server It's not a free Netapp option, so you need additional licenses. But with this solution you are able to create backups in the correct way. Since Snap manager interacts with SQL Server to coordinate an transactional consistent backup. Snap manager does have certain demands with the disk layout. You need seperate LUNS for your system databases, your temdb, your user db data files and your user db log files. Additionally you'll need an extra lun for snapinfo. So for existing SQL Servers that migrate to Netapp or for server where you want to use Snap manager as a backup strategy this might be an extra hurdle to take. However, especially when you have large databases, using storage based snapshots can be very benificial in regards to backup and restore times. Also consider scenario's where you need to refresh copies of acceptance and test. This can be a lot faster with snapshot technology. Further reading on how Snap manager works. 

In my pictures, _Total is highlighted (black) and both seperate CPUs are red and green. What I would expect to see is, one of the cores at 100%, another near 0% and the _Total around 50%. But instead I get this: 

Note that when you force encryption on the server side, the connection will be encrypted by using ssl. You can use any certficate for this. Your connection will be encrypted, However the client will not check if the identity of the SQL Server is in fact valid. Without this check you could be open for a man in the middle attack. If you choose force encryption on the client side, a full validity check will be performed. That way not only is the connection encrypted, also the client knows that the server it is connection to, is in fact the server mentioned in the certificate because it can trust that the certificate has not been tampered with. For this to work, the certificate used on the SQL Server must be issued by a certificate authority that the client trust. This can be any well known certificate authority or for example when you have a windows domain and a CA in that domain, you can have that CA issue one. You then have to import the root certificate of that CA into all clients and that will work too. to check if your connection is encrypted use the following: 

Regarding the time: Moving 1 TB , in worst case scenario (all random) is going to cost you 6.4 hours on a 10k IOPS system. 1tb/8kb pages x 2 (one time reading Kone time writing. That could take a lot of time. If you copy 20 x 1,5 TB why not consider replication? (replicating only the subset of data you need to a central database and copy that database to all your destinations) It's automated, saves you 66% of the copy payload. And saves you the 20 TB reallocating by shrink and the 20x 0.5 rebuilding of indexes. 

Initial size is not just 3MB, it is taken from the model database (if not specified during the creation of your user database.) So assuming you haven't specified a initial size during the creation of your user db and you haven't altered the model database file sizes after you have created your userdb you can do the following: 

Okay, there is a reason why tools that can do this are expensive. It's by no way easy to do. However this blog post can help you. How to create undo update statements Since you have found the transaction, you can filter the result by using your transaction id. 

Out of index order inserts. Those entry have to be placed in between data. Since the page is full, other records are forced of to a new page. you have a table schema with a large part of the row being variable data type. Updates altering the row size could cause page splits Updating index keys, making entries move in the index causes page splits. Snapshot versioning adds a 14byte tag to the row, this could cause page splits. 

Now from that moment on a so called differential bitmap is keeping track of all pages changed. From that moment every time you create a DIFFERENTIAL backup, only the pages that changed since your last FULL backup will be backupped. 

That makes using application roles not an option. However, here's the link anyway since it's good to know: Application roles Your second option would be using logon triggers. Link for more info What you do is, once somebody logs in, you check which application name they are using and if it's not your EXE, you deny access by aborting the logon.. However, this is not bullet proof since the "application" string can be spoofed. It might however be "good enough" for stopping the curious people. Security should be designed in such a way that people that are able to log in to a database can only do what they are allowed to do. So your best option (but the most complicated to implement at this stage) Would be to revoke the DBO permission an give more granular permissions. If you need business logic to be forced your only option is to implement that in stored procedures and only give execute permissions to those stored procedures.. However, that might again mean chaning the EXE. 

is only supported when both SQL Servers are the same version. You can only use An alternative if you need read access to the destination databases, is to use replication. 

If you could stay readonly than a good alternative to replication could be log shipping. The vendor would create transaction log backups. And send those backup to you. You then apply those backups.You can't restore differential only, you would still need to reply a full first. Another benefit of (delayed) logshipping is that, since the source is at the vendor (where you have no control over what they do with it, you might want to build in some buffer (in time) that when they make a huge mistake, you simply pause the logshipping redo, so you still have a good copy) For restoring filegroups/files you need transaction log restores as well. It can work but it highly depends on the growth rate vs the transaction log rate if I would prefer this above option 1, simply log shipping. Furhtermore, partitioning your database and table just for the sole purpose of solving your replica problem wouldn't be the route I would pick first but I have some questions for you later. Replication sounds like a very good option if you think the restoring of transaction log backups becomes to much work. What prohibits sql 2000 from doing Bulk Inserts? However, if you think logshipping is tedious, keeping track of updating every single object sounds like a potentially more troublesome strategy. I have no experience with cdc. somebody else has to help out on that. 

After running the script, start the snapshot agent and you should be done. Transaction Log corruption When you hit transaction log corruption and you rebuild the log file by using in Emergency mode you'll see that the publication and subscription are gone. You have to recreate these. 

No, SQL Server only writes to 1 LOG file at a time. So it doesn't help to create multiple log files for better IO performance. It will use the first file and only when that one is full and no VLFs are cleared it will continue in the second file, etc. etc. You might want to create a second log file if for example you run into an emergency situation where you run out of disk space on disk X and you want SQL server to continue logging in a second file on disk Y. 

Key is, that you hold an exclusive lock on the balance row from the moment you read it, until you update it. That way, as long as any statement that reads the balance table with a isolation level of read commited or higher, but NOT snapshot, will simply be blocked so you can't have a overdraw situation. If your business logic makes it possible to just update the BALANCE table it you could try: