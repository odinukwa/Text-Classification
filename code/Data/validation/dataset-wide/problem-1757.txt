The truth is that people don't use vocabulary consistently and I would agree with your assessment that the sentence is contradictory (or a mistake). Arguably the closest thing to a standard is the RFCs. In this case RFC793 where a segment and header are defined thusly: 

I would take this one step further. The way you describe it, your servers would have the potential to act as a bridge between the Internet and the rest of your network thus completely bypassing the DMZ in the event that there is a security breach. Your servers should remain on the DMZ and be accessible only through a point of control such as a firewall or VPN. They should not have a direct connection to anything on your internal network. That would invalidate the whole point of a DMZ. 

Maybe. You can create new Virtual Machine from existing Checkpoints by Exporting a VM from a Checkpoint (I believe this merges the differencing disk and the parent disk so you don't save any time). Checkpoints should really just be reserved for temporary point in time restores: 

It sounds like you are missing a route for 192.168.3.5/24 from the 10.30.0/24 subnet. You should add a network diagram and traceroutes for each network from each device. 

What you're really looking for is a load balancing solution that is conversant in Layer-7 protocols, namely HTTP. I think Squid could probably be configured to do this - it at the very least it is a good place to start. EDIT: Doesn't look like Squid will do what you want (it does reverse proxy load balancing for HTTP). I'm not sure why you'd want to Layer-7 aware outbound load balancing anyway - what problem are you actually trying to solve? 

Your first stop should be reading Citrix's How to Export a Virtual Machine to an External USB Drive. It will be best to try and recover the DomU's using the procedures linked - by booting the Dom0 and using to export the domU: 

I'm at a loss. I would really like to be able to setup custom reports without having to get our DBA involved (he is a busy man). Does anyone have any idea what I'm missing? 

In more detail we have a 48 port HP2610 (Switch A) and a 24 port HP2610 (Switch B). Switch B is "downstream" of Switch A by virtue of a DSL connection to one of Switch A ports. The dhcp server is connected to Switch A. The relevant bits are as follows: Switch A 

Now I cannot find anything in the client-side logs for what happened. Nothing in the , , logs and the which is supposed to record the Application Catalog action does not exist. If we deploy an application to a User Collection containing our regular users and they use the same account they are logged in to Windows as to log into the Application Catalog the same application that previously failed installs. This leads me to believe that you cannot use a separate account for the App Catalog as the current Windows session. Which is kind of a bummer. 

If you can successfully connect then you can confirm that you can reach the FTP server (i.e., there are no firewalls blocking your connection) and the FTP server is running. If this is the case then your problem lies somewhere in the Application Layer. As others have pointed the problem is more likely that there is either no FTP server listening and/or a firewall is blocking the connection. 

Windows Server 2012 R2 is all about PowerShell and frankly with the redesign of Server Manager I'm quite glad. 

Nine times out of ten you do not want to do this. The defaults are default for a reason. The IPv6 network stack is integrated into places you wouldn't normally expect it to be. There is the potential to break features in Remote Assistance, HomeGroup, DirectAccess, and Windows Mail (Source). There are plenty of reports of strange behavior or breakage in BranchCache, Exchange Server 2007 and 2010, and Microsoft Outlook that were resolved when IPv6 was re-enabled. If you are going to disable IPv6 realize that Microsoft does no testing with IPv6 disabled and that you are going against their recommendations. Make sure you have a good reason to do so. 

I would greatly appreciate more information about how these namespaces are used by SCCM and exactly how this fixed the issue if anyone has more detailed information. 

The Actual Question What kind of combination of technologies and architecture would work for this? While I know this sounds suspiciously like a shopping recommendation I am doing my best to frame it as an architectural question and avoid the X/Y trap, please feel free to edit as appropriate. 

Can someone explain how NetApp Snapshots can be considered backups? I'm looking for Good Subjective answers so please support your position with facts, references and experience. If my understanding the underlying technology is incorrect, please explain where and why that changes my conclusion. If your shop relies on NetApp Snapshots as backups, please include enough contextual information so that people can get a sense of what kind of recovery policy you have to meet. 

You need to run this as a regular user and not as an administrator since many of the Windows Apps are installed on a per-user basis. If you wanted to be a little more selective about which Windows Apps you uninstalled you could just add a . 

OK. It really does look like an issue with WMI namespaces. Somewhere in the depths of SCCM, something is telling the Software Updates Patch Downloader to connect to instead of . On a WAG, I checked likely tables in SQL database for references to to no avail.. 

SCCM used to be called SMS, which was lovingly nicknamed Slow Moving Software. SMS was designed to be deployed at a large scale (10s of 1000s of clients) in a geographically distributed network. This design choice means that it is implemented as pull technology with randomized client pulling times. When a new client is installed a number of things have to happen before it installs Software Updates: The client needs to discover the Management Point, pull Policy, run some Inventory tasks, download the Updates, install them if the advertisement deadline is elapsed and it is not in a Maintenance Window, wait for the reboot and so on. Things have to happen on the server as well, it needs to assign the newly discovered Resource Record into its various Collections (dynamic Collection membership) so it gets the SUP Deployment, run summarization on the state messages the client/s are sending back and so on. If all this is happening in under two hours your SCCM implementation just fine! That's about as fast as Slow Moving Software is going to move. Relax, grab a cup of coffee and sit back! If you want your freshly imaged machines to be immediately up-to-date, look at adding an Install Software Updates step into your Task Sequence or do offline updating of the .wim file periodically. TL;DR; SCCM is fine. It just likes to take its time. 

I'm willing to bet that the System Administrator running your Windows DHCP server is requiring secure Dynamic DNS updates. This means that you need to have a valid Active Directory account in the appropriate group in order to add or update DNS records in that zone. I believe there are two ways to solve this: You can either join your Linux machines to Active Directory using SAMBA and then they should be able to update their DNS records (if I recall correctly just being a domain member is enough to be able to update secured DNS zones) or your Windows admin can configure a "proxy-account" that will be used to authenticate when performing the secured updates. Create an Active Directory user and make sure that it is a member of the DnsUpdateProxy. Then in the use the DHCP RSAT tool to configure which credentials should be used to dynamically update DNS records. Here's an example I stole/borrowed from TechNet: 

From there you can save it an an .evt file or just use PowerShell to parse and filter it to your hearts extent. There are plenty of resources on using PowerShell to filter and manipulate Event Logs if you do a bit of research. 

In my experience the network is frequently blamed for "slowness" when the root cause of the problem is somewhere else entirely. A few examples: 

I'm not sure what you are talking about here. There are no Share permissions called in Windows Server 2003 and Windows Server 2008 R2: 

Aptitude logs to and apt-get logs to . If you've installed a new kernel the installation of the package should be recorded in one or both of those log files. 

The error is generally caused by trying to add a route that already exists when you restart networking services: From here: 

My advice is to only configure Auto Approval rules for a limited set of Windows Updates, generally anything that is classified a Security Update, and only to a limited set of your servers and workstations. You can then manually approve those updates for a wider set of your servers and workstations after a period of testing. As an aside, be aware that there is a distinction between Classifications vs. Severity. Microsoft has helpfully reused the term 'critical' to refer to both a classification of Windows Update and a Severity Rating so you have Critical Updates that fix a specific problem that addresses a critical, non-security-related bug (Classification) and you have Security Updates that have Severity Rating of Critical. You will notice the same applied to Updates with a Severity of 'Important'. My focus with Windows Update is primarily to ensure that security vulnerabilities are fixed, hence I only really have Auto Approval rules for Updates that are classified as Security Updates irregardless of their severity. If I find a Critical Update that needs to be deployed that is generally a one-off for our organization. I don't bother with any others. Also be aware that Service Packs and Feature Rollups contain Security Updates along with a host of other things. You need to think very, very carefully about how you want to handle these Classifications of updates because of how much other stuff they include. Again, my organization's focus is on security vulnerabilities so we do not approve Service Packs or Update Rollups on any automatic or wide-spread basis unless we have a specific need to do so. I would advise that you only auto approve Security Updates and you are more selective with Critical Updates but it is really what works best in your organization. WSUS Auto Approval Rules 

SuperMicro's Super Doctor III is the software I was looking for. It is SuperMicro's equivalent of IBM's ServerGuide or Dell's OpenManage. Unfortunately, while it supports hardware monitoring and SNMP or Email alerts it does not support running on Server Core. 

You should use dedicated hardware for the wireless bridge (the 0.6 mile shot between your offices) and then link that to whatever network infrastructure you think is appropriate at each office location. I would do this because of 1) functional separation and 2) the lack of devices to achieve specifically what you want which is both a point-to-point directional bridge and an omni-directional access point for clients. I have yet to come across anything that really does this. Buy a decent point-to-point directional wireless bridge (I like Airayas, but there are plenty of options out there), connect it to a "core" switch on the far side and then use cable runs to deploy your access points in the requisite locations required to get good coverage for clients. As far as your clients are concerned the wireless bridge should be transparent. 

Your firewall is misconfigured. Don't worry, it happens to the best of us. Good on you for testing! One do your network adapters is using a different network profile than your desired firewall profile. Network Location Awareness can sometimes play into this as well. NMap is erroneously detecting open ports. I have seen this happen rarely when testing from behind certain draconian firewall/proxy/IDS implementations. 

will allow you to shadow someone's RDP or Terminal Server session. Very useful for troubleshooting a box remotely with another party: 

It's not a recommended configuration to have a external root CA sign your RADIUS server's certificate. 

I can coordinate the Content IDs in the PatchDownloader.log back to the entries recorded in the ruleengine.log so, as mentioned previously, I'm pretty sure were looking at the same event generating all of these different errors but if someone knows better please correct me. If I use CMTrace's Error Lookup tool it tells me the following about hr=. 

WAN2: So as you can see from the Rrdtool graphs while WAN1 is hovering around 2.5-3Mbps during peak usage, WAN2 seems to always stay around 1-2Mbps range. The and values have been set the maximum value (65535) for WAN2 and set to 10 for WAN1. The manual has this to say about these configurations options: 

Which sounds equivalent to me (I am following up with support to confirm). I think a reasonable explanation at this point is that the Initiator can't complete the connection to the Target because the Group IP Address / Network Portal is on a different subnet. I really want to avoid a cutover and would prefer to run both subnets side-by-side until I can install and configure each Hyper-V host. 

Is this an unmanaged off the shelf commodity switch? I can't count how many weird "the network isn't working" problems I have eventually traced to an old Linksys hiding underneath someone's desk or up in the drop ceiling. Their power supplies especially like to slowly die, meaning the device powers off and on randomly or pukes CRC errors or all kinds of ugly. Of course, being strictly unmanaged it can be difficult to tell that switch is at fault. Either way commodity devices don't belong in most business settings. 

EDIT: A Microsoft NPS or NAP server is not really an option for my organization at this point due to cost issues. The best way to describe our environment is a centralized location running our core services with two dozen remote sites connected via WAN links of varying speeds and reliability. We have a varying ability and success of exercising positive physical or policy control over these remote sites, hence they are my primary focus for both wireless and eventually wired 802.1x authentication. If we loose a WAN link (which happens not infrequently) I still need clients at remote sites to be able to get network access, thus necessitating a RADIUS server at most of these locations. A request for another dozen Window Servers will be denied. Historically all of our Linux servers and network gear have been maintained as separate from our domain infrastructure. This means things like a split DNS scope with independent DNS services, independent authentication infrastructure and so-on. While I realize they're are some advantages to an domain integrated PKI infrastructure, I would need a good case as to why I should do it or or alternatively why I shouldn't use an independent PKI infrastructure. 

OpenNMS runs on a LAMP stack has both manual and automatic mapping capability. I can't say it is trivial to setup up, but it's an incredibly flexible and featureful platform with a very active community. You could setup Threshold Alerts for things like latency between nodes, dropped routes, lost paths, etc. 

Yep. I understand that, but please consider some of the technical limitations and dangers you might find yourself in down the road. A datacenter license of Windows Server looks like mighty affordable if SCCM has exploded your site's only domain controller. 

Lack of Manageability: Extranet or Extra Headache? The Extranet computers are on what is more or less an air-gapped DMZ, they are not managed with Active Directory and get their file, printer and WSUS services from a standalone rackmount server. This limits the manageability of their computers. Restriction #2 requires us to go to some contortions to perform backups and general administrative work. This was all well and good when that division had IT staff but now they do not (YAY! Budget Cuts!). My supervisor and I agree the best way to move forward would be to move them over to our existing platform/s wherever possible so we are only maintaining one system and not two or three. There is an additional Disaster Recovery and Business Continuity concern. The standing plan is essentially to take a LTO tape containing backups and go somewhere and then recover the data and away we go. My supervisor and I both agree there is some detail that is missing from this plan before it is workable. It would be nice to address this concern along with the file services at the same time. Last but not least... financial stuff is time sensitive and important. As in millions of dollars worth of important. Standardization, reliability and security of their computers and the Extranet LAN is a requirement, even more so now that we do not have IT staff that can be on-site during the initial hours of their shift who could immediately respond to an issue. The technical requirements of our Extranet users is pretty mundane: Windows 7 workstations, file, print and update services, internet access and a few 3rd party applications provided by our financial partner.