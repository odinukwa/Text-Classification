This shows the redshift distribution (The factor $G = \frac{E_{observed} }{ E_{rest-frame} }$) around the background Quasar accretion disc. Also shown is an example trace of a foreground object that could lens the emitted $K \alpha$ light from the disc. ** The rest of the problem can now be understood in a very visual way: If the passing foreground object can actually lens the light in a way that would be seen in the spectrum is now determined by the ratio of the SMBH Gravitational radius and the foreground's object lens radius. This works as follows: The gravitational radius is just the Schwarzschild radius $r_g = GM/c^2$ and sets the scale of the above image. The lens radius $r_l$ is a more involved function of cosmological distance and lens mass. One can imagine it however, being a circle traveling on the green trajectory in fig 1. that will lens all the background light that it hits into one observational data point. The latter point must be emphasized, as the situation in fig 1. will be vastly under-resolved in the actual observation. So if $r_l \gg r_g$ the lens will lens red- as well as blue-shifted $\rm K\alpha$-light into the image pixel and no net energyshift will be detected. If $r_l \le r_g$, then the lensed light will probably be very red or blue-shifted, according to the G-values it hits on it's trajectory. ** We're nearly done: Now the lensing configuration of RXJ1131-1231 and their cosmological distances conspire to make the case $r_g = r_l$ correspond to Jupiter mass lens masses. This means objects of Jupiter masses and below will be able to contribute to a measurable Fe $\rm K\alpha$ redshift, but only if they are not buried in the larger lensing radius of a star (which would have $r_l \le r_g$)! But, as we already said, the authors only can spectroscopic information about the lensed images. So they try to reproduce the redshift-distribution of the spectrae of their images with a star-only distribution, which fails. Subsequent tests they make, indicate that they have to inject a large number of objects with $r_g=r_l$ into their test-lensing distribution to reproduce their data. Summarizing: This is a purely statistical argument for microlensing by a certain number of planetary-mass bodies in the X-Ray spectrae of 3 of their 4 macrolensed images. It's not a discovery and as another Q&A pointed out, there are no planets to name or point to. It is even not the only explanation for the X-Ray data seen, as there are other competing ones. But the presented planetary explanation is the only one that can reach sufficient redshifts seen in the data. (P.S. I hope I didn't make it more complicated than the original paper, I've actually tried to summarize it...) 

Both methods can only give maximum masses and I'll just leave here fig. 5 from the original paper with the planet you're interested in: 

Now comes physics into play. The authors argue that they find anomalously high red- and blue-shifted $\rm K\alpha$-line distributions, that must be explained by planetary-mass objects. To understand how, let's consider fig. 1 from the paper: 

What you're asking about is not known at the moment, but I can briefly comment on how flawed our knowledge is. 1.) Theoretical predictions: As one comment mentioned, the question "where do planets end up?" is dependent on the answer to "where do planets form?" and the answer to "how fast do planets migrate?" which is not well understood at the moment. There are of course predictions for both those processes, but those essentially don't agree on anything and especially not with the observations. A recent overview and a very short video by the authors of that paper summarizing it. 2.) Observations: We have only very few multi-planet systems, and not a single one where we can claim we know all planets down to a certain size or mass limit. I suggest you take a look at the data on $URL$ and use the filter "NCOMP >= 5" (also sort by name), this way you get current candidate systems with equal to or more than 5 planets. You will see there are not many systems that we can base any systematic understanding on. Plots like the first one in the answer to this question that seem to indicate that we know quite many systems, hide the fact that those systems are all single or double planet-systems. This prohibits basing your generated systems based on data or planet formation theory. But I suggest that you can use a different method: 3.) Long-term stability: We don't know all systems that are possible, but we certainly know that all systems under certain conditions become unstable. There is an overview article or also a long video outlining that there should be a simple stability criterion for planetary systems: If any two planets come closer to each other than ~10 mutual Hill-radii, their orbits will become unstable and possible send the whole system into chaos. The mutual Hill radius of two planets with masses $m_1$ and $m_2$, and central star $m_s$ and semi-major axis distances $a_1$ and $a_2$ from the star is $$R_{hill,mutual} = \left( \frac{m_1+m_2}{3 \cdot m_s} \right)^{1/3} \cdot \frac{\left( a_1 + a_2 \right)}{2} $$ So my idea of a simple solar-system generator would be to choose some first planetary mass, place it somewhere. Then build up the star system from the inside out planet by planet, by chosing the next planetary mass from one of the above mentioned 1-or-2-planet distributions (this one could be a realistic one, like in Mayor et al. 2011, if you know how to re-shape random distributions). You calculate $a_2$ for them such that their mutual distance is $> 10 \cdot R_{Hill,mutual}$. In this way you'll probably get pretty packed systems, but you can of course take their mutual distances to be larger than $10 \cdot R_{mutual, Hill}$. So in this way the properties of your generated star systems would be: 

What you're possibly missing here, judging from your questions, is the link between the mass distribution of a galaxy and the rotation velocities. The simplest way to obtain a prediction for a tangential velocity component is to look at the Kepler problem, i.e. a central mass, dominating another smaller mass, orbiting it. Newton's force-law $\vec F = m \dot{\vec{v}}$ relates acceleration and acting force, which is given by gravity. All acting forces then sum up to change the velocity of a test-particle $\dot{\vec{v}} = \sum_i \frac{\vec F_i}{m}$. Then we get a number of cases, depending on how many summands there are, and how they are distributed spatially: 

This was in the news lately, as ESA managed to touch down and measure 67P's D/H ratio which gave another hint on the asteroidic origin of water on earth. This finding, however does not resolve the question: 

Now that we have established that solids exist in space, let us review the theoretical perspective of growth mechanisms: 

But all those terms have an obvious relation to the construction of the word 'atmosphere'. The only term I've heard for 'Earth minus atmosphere' is 'bulk Earth'. Those terms should be context free, so that you can directly, precisely refer to them when starting a conversation about them. 

Summarizing Yes, our galaxy had collisions before, and they were necessary to grow to it's current size. Scientists are searching for the relics of those collisions in a process called galactic archaeology (the link goes to a personal website, which I find neatly made). 

Atmospheric turbulence is known to scatter photons in a quasi-random way along their path throughout the atmosphere, resulting in lower imaging resolution than would have been anticipated by instrument-only considerations. I have been thinking whether the same effects can play a relevant role in limiting the sensitivites for photometry in transits or for spectrometry in radial velocity measurments. My thoughts so far: 

You 'feel' only acceleration, not velocity. Acceleration is a change in velocity per time. The forces on the surface of Earth on a human are in equilibrium, and therefore the accelerations are too, thus we don't fall off. 

First, I'll talk a little bit about the state of what is being done: (according to my knowledge) In Astronomy on-board processing is usually avoided if possible. The reason for this is to have the pure data as unspoiled as possible, because instrument effects, cosmic rays, the light from Earth's corona, etc. can introduce many systematic errors into the data. If that would be removed on-board and then sent to Earth, then there would be no control later on what the actual, original data was. This became relevant e.e. for the exoplanet-finding Kepler(see also the technical data there) space telescope where the photometric data was showing very unexpected behaviour at first, before the reasons for it were realized, obviously long after the spacecraft was designed and built. Thus, the image processing challenges are usually resolved on the ground stations and with the astronomers on their big computers. Also the interpretation of the data, after being cleaned, debiased, etc. is non-trivial. As however the downlink speed depends on the distance $d$ to Earth like $\sim \frac{1}{d^3}$ so satellites that orbit far away do have to apply some clever ways to reduce data sometimes. The Hubble is not a candidate for this, as it orbits Earth at only some 500 km a.s.l, its data can be transferred to Earth without problems. The Gaia satellite however, orbits the second Earth-Sun Lagrange point and is thus farther away from Earth, 1.5 million km. This mission is an astrometric mission, meaning it's there to measure positions and velocities of stars to extreme precision. If all the pixels of all onboard systems would continuously be read out and produce data, that would be too much to handle and some would be lost. So Gaia has a pre-detector that tells the main detector "Hey a star is coming". The main CCD then creates a moving window of 12x12 pixels from which all electrons are read-out and accepted as data. So that's some limited processing there. In terms of data transfer, missions like New Horizons (Pluto encounter last year) or the Voyagers, are too far out to simply send their data over. Therefore the Deep Space Network exists, to look out for radio signals from those probes and actively amplify them. What you could do: So now that I've spent time explaining that on real spacecraft it's not being done what you want to do, we can bend the rules and just try to think about what's being done on the ground (if you want to):