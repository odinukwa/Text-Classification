You could use PL/Proxy to set up a frontend, and then write some functions that run the queries that you need. It could be quite tricky, though. You should plan this architecture carefully. 

It looks like you are mixing Debian and OpenSCG packages. That won't work. Install PostgreSQL 9.1 from Debian. 

Copy all your configuration files to . Make any adjustments that you want to make as part of the upgrade. Then you should be able to start the new server using 

The intrepid release is simply not on the archive servers. I don't know the reason for that. But if you want to proceed, you will probably need to upgrade to something newer. 

If you have a lot of contention, there is some anecdotal evidence that performance doesn't get better or degrades after about 32 CPU cores. It is difficult to get definitive results in that area, though. If someone knew the answer, they would probably also be able to fix the problem. If you think your application will be pushing the limits on this, I suggest running tests yourself. pgbench might help you get started. It doesn't look as though it will happen anytime soon. Don't plan on it. This is highly dependent on what you are doing. If you have an OLTP application with lots of concurrent clients, it won't matter much. If you have more of an OLAP style usage, then it will suck because you might only be using one or two out of many CPUs, and especially your sorts could conceivably be faster. 

I would try the equivalent of to see whether and where it looks for and what it does with it. The option is unnecessary and unrelated to your problem. 

Since most file systems don't keep checksums (notable exception is ZFS), a checksum in the database would still be useful. In PostgreSQL, the WAL records are checksummed, but not the data pages, so you won't be able to detect if the OS or hardware has shredded your data. Implementing a checksum for data pages is a planned feature in PostgreSQL, but there are some very difficult problems with the implementation if concurrency is to be maintained. Search for "postgresql block level crc" or something like that to learn more (or become totally confused). 

In case of a failure, you have to restore your database from a backup. It will not (in general) recover by itself. 

Unless you want to become a hardware expert, I'd first of all suggest software RAID. It's much easier to set up and manage than hardware RAID, and for a medium-traffic site like you describe it certainly performs just as well. The appropriate RAID level depends on your data volume. If you can afford it and the data fits, you can't do much wrong with RAID 10. If you need to go higher volume, then RAID 5 is the next best option, although you will sacrifice some write speed, especially for your database. If you're concerned about that, it'd be best to try it out with your hardware and software. 

You can also use to discover what hosts are up, do a reverse DNS lookup on the hosts that it finds, and then filter on that. has a bunch of options to fine tune the output, but perhaps start with 

Vacuum and autovacuum replicate like any other write operation. (Well, they are obviously somewhat special internally, but as far as your question is concerned, they are normal write operations.) Running vacuum or autovacuum on the slave doesn't do anything and is not necessary. 

This works on Linux/GNU. On other systems you might need to go a more complicated route, or install the GNU coreutils. 

Follow the instructions on $URL$ to set up your APT sources (you may have already done this), then do 

If you want to make this a permanent configuration in you will have to use the older alias interface syntax (e.g., ), because that configuration file doesn't support multiple addresses per (primary) interface yet. Add something like this: 

contains a lot of information. Many source files under that directory also contain big comments. That's about as up-to-date and relevant as you can hope for. 

The background of this is: The old Postgres system used the PostQUEL language and used a data type named (because someone thought that was a good name for a type that stores text). Then, Postgres was converted to use SQL as its language. To achieve SQL compatibility, instead of renaming the type, a new type was added. But both type use the same C routines internally. Now, to some degree and in some places, is hardcoded as a default type, in case nothing else can be derived. Also, most functions are only available as taking a argument or returning . The two types are binary compatible, so casting is a trivial parse-time operation. But using is still overall more natural to the system. But aside from these fine points, there is no noticeable difference. Use whichever one looks prettier to you. ;-) 

to download the source package. Inside the source package, the configure options are in the file . Edit that, and then rebuild (with for example). 

(In normal netstat, but not in busybox netstat, the option also gives you that extra information.) and 

I suggest you take the specific issues to the respective authors/communities of the software packages you are dealing with, try to resolve them there, and learn from those experiences. While there is various literature around to learn about "Linux", issues like in your examples are quite specific and can't really be learned a priori. 

The OOM killer on Linux wreaks havoc with various applications every so often, and it appears that not much is really done on the kernel development side to improve this. Would it not be better, as a best practice when setting up a new server, to reverse the default on the memory overcommitting, that is, turn it off () unless you know you want it on for your particular use? And what would those use cases be where you know you want the overcommitting on? As a bonus, since the behavior in case of depends on and swap space, what would be a good rule of thumb for sizing the latter two so that this whole setup keeps working reasonably? 

The first three steps are definitely within Puppet's realm. Installation is a resource, initdb can be done with if the package doesn't do it itself, and starting the service is a resource. But populating the database is probably not a good use of Puppet. You could probably do it with a lot of custom code (e.g, a separate resource type for a table, a function, etc.), but it would be a lot of work and somewhat unchartered territory. I suggest you look for a different way to do that. 

That basically means the security of the box itself becomes your last line of defense. So make sure your logins are secured and the applications that are exposed to the outside of the box don't have any exploits. It's possible to do this; after all you have to put the fences somewhere, but usually having more than once fence is a good idea. It depends on the nature of the applications and the data, who the attackers might be, and what value all of this has to them. 

If you don't want to actually check if the hosts exist, is to just loop through the IP addresses and do a reverse lookup. The following recipe works on Linux and FreeBSD; I don't know how the program on Mac OS X behaves: 

An antivirus program should handle this case, and there are of course plenty of those for Windows. Since PDF files are nowadays a pretty common attack vector, PDF file scanning is a standard feature of these products. It won't give you a contains code/does not contain code answer, but it will block or remove files that contain known or potential exploits. 

This is probably not going to work without some installation routine that patches the pg_hba.conf file for the local situation. You could look for existing lines that initdb created in the initial file and just modify those, if necessary. initdb won't put in an IPv6 line if the system doesn't support IPv6. In PostgreSQL 9.1 (not yet released), you will be able to put host names into pg_hba.conf. So just writing might work on most of your target systems. 

You cannot (portably) use more than nine capturing parentheses in sed. You are probably better off writing your script in Perl. 

Have you considered Bucardo? It's asynchronous multimaster. It hasn't completely caught on and is not a general solution, but it might be worth a try. 

I probably wouldn't go for this without further evidence of MySQL-specific knowledge or qualifications. You wouldn't hire an experienced Windows admin to take care of your Linux boxes either, would you? With experience in a different environment, you would know most of what to do, but probably not much about how to do it. That said, MySQL offers a number of trainings and certifications, so maybe the candidate should take a round of those. That should get him up to speed with the product specifics. 

The canonical answer to this is: Use what you are familiar with. Since you are not familiar with either, it's a really close call. So here is a semi-philosophical argument: Debian is an original community distribution. Do you value a direct feedback channel and the possibility to shape the future of what you use? CentOS is a community distribution based on a commercial distribution based on a community distribution. Do you value something that has been polished three times but where the channel to the original author is perhaps more murky? But really, it's very hard to distinguish these options. 

One thing I do on Debian all the time is use a 64-bit kernel with a 32-bit userland for my normal system, and then create a chroot with 64-bit userland to build amd64 packages. The structure of the kernel packages in Ubuntu is a bit different, so I don't know if this is possible or recommendable there. Other than that, the clean solution is to use a virtual machine. QEMU is the obvious choice, because it can emulate just about anything on anything (and it's free). You can use qemubuilder (packaged in Ubuntu) to integrate the build with pbuilder. This can be a bit tricky to set up in my experience, but it might be worth it if you need to do this repeatedly. For a few packages, just fire up QEMU and log in by hand. If you are very adventurous you can also try cross-compiling, but this is probably much more difficult and doesn't work with all packages. Look for dpkg-cross to get started. Or how about you create a Personal Package Archive (PPA) on Launchpad and have your package be autobuilt there. 

If you just upgrade from etch to lenny, your installed postgresql-8.2 package isn't going to disappear. That's how it's designed. (Check the aptitude output when you're upgrading to be sure. You could set the package to hold if needed, but it shouldn't be needed.) 

This sends a mail to root whenever there is a package to be upgraded, which in a stable release means mostly security updates. As a general matter, you should of course redirect the root mail to somewhere you can read it. I have been using this for some years in a personal environment, and it works great. 

You can use SSH's feature for that. If you already have SSH set up (very likely) then this will be much easier than the alternatives for just connecting two hosts. 

In order to add SSL support to your installation, you need to rerun the , , sequence with the appropriate options. But you don't have to delete anything. Just install the new binaries over the old ones and restart the server. 

1) If you insist on storing backups in subversion, then there is nothing wrong with this approach. It is strange, though. 2) You should keep a checkout around, place the dump into the working directory, and run and as appropriate before committing. 3) If you run the commands as shown from a shell script, there should be no overlap. 

This is looking for the client certificate files. The server certificate files belong in , as you correctly pointed out. The client certificate files are looked for in . You are probably logged in as the user, who happens to have a home directory of , so that's how you got that path. So either put the client certificate files where they are looked for, or log in as the correct user, or reconfigure the server so that it doesn't require client certificates. 

You can find the equivalent information in slightly uglier form (a.k.a. hexadecimal) in . There, you can find the inode of the connection, which you can look up under . For example: 

PostgreSQL data files are operating system and CPU architecture specific. So first of all your Linux and Windows systems would need to be of the same architecture. And then the data layout that the compilers use would need to be the same as well. I have tried this in the past between Linux and Cygwin, and it did not work. So I would be very surprised if it worked between Linux and native Windows. In general, don't count on it. 

Why not just set up a cron job that runs some variant of ? Perhaps you mean that you only want the behavior for one of your packages, but I think it's not worthwhile to address that specially. In a stable release of Ubuntu, should very rarely pull in new packages, and if it does, there is probably a very good reason for it, such as a security update. 

You need root access for the second step. Not as convenient as the option, obviously, but works in a bind. Could be scripted, if necessary. 

You can tweak this a little to start automatically or in the background. See the documentation. Then you can connect: 

You will probably want to apply one of the suggested workarounds, but the answer to your question is no(t easily). If you want to change how sorts, and none of the special sort orders offered by the command-line options suit you, you will need to define your own locale. See . 

If the two servers have the same architecture, then you can just copy the data directory. Normally, you'd have to shut down the server to do that or set up WAL archiving. But for creating a testing database, it might be sufficient to do it anyway and clean up your copy with afterwards. You can use rsync to reduce the amount of data to be copied in subsequent runs. If the two servers have a different architecture, then is the only route. 

Nothing production grade, but for the curious, there is/was a research project to implement something like that. Search for "PostgreSQL index advisor". 

Aside from the valuable specific advice given in other answers, the canonical answer to this sort of question is to read the release notes. For example, in an upgrade from 8.2 to 8.4 you should read the release notes 

Use the built-in binary replication. Unless you have a reason not to use it, that should be the default choice. Reasons not to use it include the requirement to do partial replication and cross-version or cross-platform replication. I don't think that applies to you.