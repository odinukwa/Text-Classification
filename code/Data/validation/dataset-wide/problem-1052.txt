It's likely that your Test-User account has different SIDS on the primary and replica servers. The best way to resolve this is drop the Test User on the Secondary, generate a create user statement from the primary with the proper SID (e.g. sp_revlogin, dbatools.io, or another tool of your choice) and then run that generated CREATE USER statment against the secondary. This will create a user that has the same SID on both replicas so you won't get this error going forward. Another option, and one that I would recommend instead, is to convert this database to a Partially Contained Database so that all SQL Logins you create with the database are automatically transferred over to any replicas you setup. This is a very handy feature (if the limitations don't adversely affect you) for databases in Availability Groups and will help minimize the administrations headaches that come with them. 

In spirit, this question is very similar to Why would I NOT use the SQL Server option “optimize for ad hoc workloads”? Quoting from Peter Schofield's accepted answer on that question: 

You will need to remove the statements if you want to nest this into a SP, but you can hopefully see that it's not nearly as difficult to bulk-generate scripts with this approach. If you want to generate statements (which will be easier to do), script out table statements and then do a search/replace on to replace it with . 

I'm speculating that since you were given RDP access and a username/account to the database server, the security credentials in question are Active Directory credentials. DBeaver on Linux likely cannot use AD credentials (unless the environment taps into AD for authentication on Linux, in which case this isn't your issue). I don't know if trying to authenticate via SQL Authentication using AD Credentials will throw the error you've provided, but it may be the cause. I don't use DBeaver, so again this is just speculation. HeidiSQL requires that you check the box next to Use Windows authentication as shown below: 

Perform a Log Backup (e.g. ) against the database Perform a DBCC SHRINKFILE operation on the transaction log file specifying 1 MB as the target size Perform another Log Backup against the database Perform another DBCC SHRINKFILE operation on the transaction log file specifying 1 MB as the target size Manually increase the size of the log file, incrementally per Kimberly Tripp's article, to grow the TLog back to an appropriate size. 

Since no one else has mentioned this as a possibility, you can do this all directly via TSQL commands. First, you can directly write out to a file via either a linked server (if the file will have a static path) or via OPENROWSET or OPENDATASOURCE functions if the files are more dynamic in nature. Details on how to do this for excel files can be found here: $URL$ Details for flat files such as text or csvs can be found here: $URL$ Once you have created the file(s) to your liking, you can send an email via sp_send_dbmail, using the @file_attachments parameter to specify the file. Note that by default there's a 1MB file attachment limit, so if these files will be larger, you will want to adjust the mail profile accordingly (e.g. using sysmail_configure_sp passing the MaxFileSize parameter). I'm not endorsing this approach over any others, but here's another option for you to chew on. John 

UOW is the request_owner_guid returned in the above query. I've run into this situation with both linked server issues and jobs that are making calls externally from the SQL Server process. In either case, I would find that an external process would interfere (e.g. network drop, process is killed or errors unexpectedly, etc.) and then the transaction wouldn't properly terminate at the SQL Server. SQL Server won't know that the transaction has actually failed and things will just sit out there until you either walk through this process or bounce the server as you are already doing now. Hope that helps. 

See if TCP port 135 to the server is open or not on your firewall? If it's not, opening it up may resolve this icon, but understand that there are also some special considerations to be aware of when enabling this port. Normally, this isn't a big deal if the server is not exposed to the perimeter of your network, but if your firewall rules are very restrictive or this server sits in a DMZ, this may be something you have to live with. Your user may not have proper permissions to view the service state remotely. To enable those permissions, follow the instructions here. WMI services are not running on the server hosting the database engine. Start the WMI services via services.msc, etc. 

Precision and Scale are based on the mathematical concept of Significant Figures. You're not alone in that the current definition/usage for these terms is confusing. 

Some general disclaimers, this script does not include any clauses or other customizations (such as restoring from a striped backup set). This won't be an issue for the TLog restores, but it may require that you adjust the Full (and potentially Differential) backup restore statement(s) to fit your needs. I use this often when setting up a new server as an AG replica as the file paths between servers should match up to make life easier. 

Correct. If you never modify a record after it's added to the end of the table, you'd really only ever need to update statistics in order to avoid the Ascending Key Problem as identified by Scott Hodgin's comment below. 

I've not found any references indicating this is expected behavior, but before I submit a connect item, I first wanted to reach out and confirm this isn't a localized problem. Can someone either point me to documentation identifying this is expected behavior or alternatively confirm this is, in-fact a bug? EDIT: In response to the comments about not including an clause, I was always under the assumption the TOP keyword returned the data in the order in which it was inserted, which should, in this case, be the order dictated by the clustered key. When running the same statement against a formal table, the expected behavior is returned: 

Try the JDBC driver or ODBC driver(Install/Config Info per platform) supplied by Microsoft for Linux clients. I've found that I'm often more successful when using the drivers provided by the vendor than those that are included with the product (e.g. in this case DBeaver). I would think all the other settings remain the same, just swap out the driver url to point the MS supplied driver instead. 

For the changes you made locally, that you now want to replicate to the remote server, you'll need to either recreate them manually on the remote server or use a tool to identify the differences, such as RedGate's SQL Compare or Visual Studio's Schema Compare tool. There are other methods, but I think these are likely going to be the easiest approach for you. Finally, the "proper" way to handle changes like this is to tap into some release management methodologies. This topic can get pretty complex depending on your needs, but a good primer can be found here along with a listing of some of the tools available on the market. 

To set the hotkeys, navigate to -> -> -> -> . Type in into the Show commands containing: box. Then assign a new shortcut key combinations for and and press : 

Regardless what I've tried, any new Control Flow task always defaults this property to False which has caused some false positives when reviewing successful package executions. I'm developing within SSIS 2016, but this behavior doesn't seem to be version specific from what I've seen. 

While not ideal, an alternative approach here could be to setup your Azure database as a subscriber to replication publications created from your on-premise database(s). Replication will allow you to limit the articles within the publication, filter the types of records, etc. which hopefully means you can minimize the publications to only send the data necessary to serve the traditional four-part queries. If you still want to utilize cross-database queries, you can always create a second Azure DB which would act as the subscriber to these publications and then utilize cross-database queries from your current Azure DB which may allow you to minimize any rewrites of existing functionality. 

You are correct; the fact that you have a partition function defined for a datatype of is preventing you from altering the data type of your column... Sadly, you're kind of stuck with the current table because there's no way, to my knowledge, to adjust the datatype of a partition function without dropping/recreating it.... which you can't do because you have tables dependent upon it. So basically this is the chicken-and-egg scenario. 

For most typical implementations of SQL Server (e.g. all objects are stored within the schema), I cannot think of a permission set that will allow you to do this without the inclusion of some DDL trigger that prevents certain operations against a user, just as @dbaduck alludes to. While permissions are always prioritized over permissions (ref), this scenario is a bit of a paradox because the moment you grant a user the ability to control permissions on schema objects is the same moment that user can now any restrictions you would have imposed upon him/her as is related to said schema objects. Convoluted, right? If you love confusing graphs, I'm going to point you to MS's Permissions (Database Engine) article. Within it, you'll see the way that permission inheritance is laid out. What this shed's light on is that permissions on a user are not the same as permissions on the a schema, and because of this there's really no clean way of doing what you want without some DDL trigger tomfoolery. 

One concrete reason that comes to mind as to why you wouldn't enable this by default is the extra overhead associated with maintaining the row versioning. Paul White's article on Read Committed Snapshot Isolation provides a pretty comprehensive rundown of the feature and I would suggest you read it if you haven't already. In all honesty, I'm sure that if this feature was included with one of the first versions of SQL Server, it would be enabled by default. This isolation level is a very close approximation to how Oracle runs out of the box, so under different circumstances this may have been enabled by default instead, but since it's something new (with SQL 2005) that hadn't existed before, the Principle of Least Surprise is what we see. 

You will need to dig into query plans to identify if a particular query is performing a scan instead a seek, but you can (relatively) quickly identify what queries are using a particular index by running the following: 

Finally if this doesn't work, you may have to switch to a different SQL client. It sounds like the functionality you want is similar to executing an Oracle PL/SQL block via within Oracle SQL Developer. You can use Oracle SQL Developer to connect to SQL Server using Third Party Database JDBC drivers (ref), but I don't recommend it. 

Depending on your version of SSMS (newer versions obviously being preferred in this case), there are also some keyboard shortcuts you can use to collapse/expand a section. The keyboard shortcut link provided by Simon Hellings' earlier response alludes to hotkeys already being set, but I cannot confirm they actually work. However, you can set some custom hotkeys to do this as follows: 

The only way you can pass Windows Credentials between servers to my knowledge is via delegation as you've already taken a look at. If there's truly no way to get delegation working in your environment, an alternative to delegation is to fall back to mapping your Domain accounts to SQL authenticated accounts on the remote server that are setup with permissions that are equivalent to your needs. The downside here is any query executed remotely will be logged to the SQL Authentication account and not your AD Account, so audits won't be as transparent unless you take pains to map a one-to-one SQL Authenticated account to AD Account. To map AD accounts to remote SQL Authenticated accounts, you first need to create the SQL Authenticated accounts on the remote server and then configure local logins to map to them on your linked server. If you're happy to use the GUI, you can find this window under Server Objects → Linked Servers → Linked Server Name → Properties → Security Page: