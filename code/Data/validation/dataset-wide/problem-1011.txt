I'm also not quite sure why you are posting this without just attempting it, but yes, this will work without problem. The only issues you might have are: 

You don't specify what "first" means (or provide any table structure or example data), but let's assume it's the earliest completion timestamp: 

Has your database been explicitly activated (with )? DB2 will not evaluate whether a database is a candidate for automatic backups if it is not active. Relying on having at least 1 connection to the database to keep the database activated is a recipe for pain. That said, I moved away from relying on automatic backups a long time ago, instead relying on the consistency and control you get when using a scheduler like . 

shows the amount of uncompressed data in this buffer. The line above shows data from an SMS tablespace; DMS tablespaces will also show lines like this, but the will show . So, if you look for lines where starts with , you can add up the of these lines, and find the additional storage space that all SMS tablespaces will use. 

It almost certainly doesn't really matter. The only significant I/O going to files in the database path would be to the recovery history file (and even that isn't much). Considering that DB2 uses the first storage path by default if you do not specify , I would put it on if that was the only choice. My SOP is to keep DBPATH in the instance owner home directory. This is obviously another important location and perhaps makes more sense than the other two locations. 

It is relatively complicated to do, but it's possible. If your database has only DMS tablespaces that contain table data, you can use alone to get this information: 

Online backups require that the database be enabled for rollforward recovery. However, this is not the default when you create a database. In order to do this, you need to set the database configuration parameter. Once you have done this, you'll need to take one offline backup (i.e., no users can be connected). Once you've completed these steps, you'll be able to run online backups as you wish. You may want to spend some time reading the Data Recovery section of the DB2 Database Administration guide to help familiarize yourself with DB2 Backup/Recovery. 

There's no restriction on the specific name, other than it must be 8 characters or less. So works just fine as a database name: 

For the second problem you may want to look at using the function so you can calculate the delta in the log record: 

The issue here is that you're reading documentation for Db2 on z/OS, but you're running tests on Db2 for Linux, UNIX & Windows. When you perform this particular alter on Db2 for Linux, UNIX and Windows, it will succeed, but the table will be placed immediately into reorg pending state, which you can see in (see the column). will still show as 'N', even though performing a REORG on the table will rebuild the index. 

However, for remote connections to also be unable to connect, you'll have to restart the DB2 instance. This is required because the DB2 TCP Connection Manager (db2tcpcm) will cache the the database catalog entries, so until the EDU is restarted it will still "know" about the database. After restarting the DB2 instance, remote connections will get the error: 

The easiest way to do this would be to simply uncatalog and then re-catalog the database with a different name (perform this as the instance owner on the database server): 

When you enable archive logging in a database, you're telling DB2 that you want to retain a copy of all transaction logs that have been created. LOGPRIMARY and LOGSECOND only control how many active log files can be in use in the active log path at any given time. Please read the documentation on database logging for more details. 

DB2 does not implement the module in its Oracle compatibility code. DB2 does still offer the and UDFs, but I would recommend against these as they use a weak encryption algorithm and require the use of passwords in SQL. So, you would have to implement your own solution (via UDFs) for application-level encryption like this. It might make more sense to implement this in your application code rather than in the database to avoid having to send encryption keys across the wire to the DBMS. This would certainly be a more platform-independent solution. 

You need to use the explain facilities in DB2 to answer this question, it's highly specific to the actual queries you are executing, and you can't get a general answer. IBM i 7.2 (and previous versions) have Visual Explain. If you are restricted from doing this, then you need to speak to the system or database administrator for your environment. 

The files in are not to be used for a specific instance. When you run the command, it copies a series of files and directories into , and sets the permissions accordingly. (Some of the directories in will be soft links back to directories in ). For example, you should find that has permissions set to (i.e. ), owned by , and have the group owner set to the instance owner's primary group. You should not be executing things from . That is why the permissions of the files in that directory are set the way they are – should be . So, to start your instance, you should be executing . Be aware that you also need to initialize the user environment prior to starting the instance, so you should use the option for to handle this: 

Although you could solve the problem by making sure that the ID has sufficient privileges for the instance, it would be less confusing to use a different script for each instance and run the script as each instance owner. 

The first trigger is necessary to handle the outer join (i.e. where a user hasn't made any calls yet). The second trigger inserts the calls, and also deletes the records inserted by the first trigger (as they are no longer necessary once a record in PHONECALL exists). You would need to decide how you want to handle deletes in either table (either through triggers or foreign keys with cascading deletes). Again, I want to stress that this is a possible solution, but I would only use it as a last resort. Databases are designed to perform joins efficiently, but this might require some proper design and tuning. The table structures above should absolutely not require this kind of solution, but I'm working on the assumption that this is not your actual schema. I am only describing this solution because I can imagine a few edge cases where something like this solution might be necessary. 

requires Administrator authority on Windows, in the command shell you are executing it. Setting your ID as a member of the Administrators group on the local machine is not sufficient. IBM adds "DB2 Command Window - Administrator" in your Start Menu to give you a DB2 Command Window that has administrator privileges (which is the recommended method). Alternatively if you are using a normal command window (cmd.exe), you would have to start it by right clicking "Command Prompt" and choosing "Run as Administrator". It might be possible to use to execute , but it's probably easier to just start the command window directly with administrator privileges. 

You could specify similar options for the system catalog tablespace and the default system temporary tablespace, but I would not recommend doing so. 

This looks at only records in the last 6 weeks, and then finds records that occurred on Sunday () at 12:00. (Using the predicate should handle minor differences in clocks on your database servers and/or instances when your script doesn't execute exactly on the minute. It may also be possible to write this using OLAP windowing functions, but I'm not sure it would be any more efficient. 

This applies to DB2 versions that have storage groups – DB2 10.1 and newer. For DB2 9.7 (this will work with DB2 10.1 and later, too), you could extract the storage path(s) for the database and see if any container names do not start with the storage path(s). You could also consider the container name; automatic storage tablespaces will have a name that looks like: 

You can't "estimate" the size of a buffer pool, because the answer is, "it depends". Try using the command to get started, and enable STMM. Together these will go a long way towards getting your database running more efficiently. I would also recommend you spend some time reading about DB2 Performance Tuning. There is a ton of information available on the web, starting from the entire Performance Tuning section of the manual. 

In DB2, a tablespace is the object that serves as a layer of abstraction between tables (and indexes) and disk. It is similar to a file group in MS SQL Server. In DB2, a database is roughly equivalent to a SQL Server instance – a database contains multiple tablespaces, transaction logs, one or more bufferpools and other memory areas. A "database" in SQL server is much closer to a schema in DB2. 

You won't get any specific answer to this question, because as you observed, it depends. The answer depends on your company's tolerance for having one DB2 instance adversely impact the performance of another's. You may be able to control or limit this impact if your AIX admins are willing to learn and set up WLM policies at the operating system level (not DB2 WLM). This is really a question of capacity management. Understanding the workloads in your environment is key here, and being able to identify (or predict) when you'll run into limits will influence your decisions about how many instance(s) to put on a single server. 

If the database manager configuration parameter is small enough that you can't activate both databases at the same time (in addition to whatever other databases are activated within the DB2 instance). If you use the option for with some silly default values so you exhaust system memory System resource contention (disk, CPU) slowing the process down. 

The message you include from your db2diag.log shows that there is a potential space problem (i.e. the file system holding your data is filling up). The monitoring data here shows that you are ranging between 89% and 94% over a 90 minute period with increases and decreases – this is probably associated with system temporary tables in the database. It may be worthwhile to increase the size of the filesystem(s) holding the data since you're getting very close to running out of space. This space utilization, however, has nothing to do with how much I/O activity (IOPS) the database is generating. Generally when you have an I/O problem where you see a much different usage pattern than normal. You mention that normally there the system is generating 300 IOPS, but lately it's been generating 5000 IOPS. Assuming that the I/O activity can be traced to the file system(s) holding DB2 data, then you need to monitor the database during these periods of high I/O to find out what queries are active and are causing lots of physical I/O, and then do some investigation to figure out why. You can use the utility as well as many other monitoring tools to figure this out. At that point it's a tuning exercise. Are your bufferpools too small? Was an index dropped? Or is the cause a new query that needs to be reviewed / tuned? 

This is completely normal. Each of these directories are for each unique log chain. If you are familiar with software version control, each log chain is like a branch. A new log chain is created each time you restore a database and roll forward to a point in time other than . Here is why: You have a database, SAMPLE. It has reached the log file S007500.LOG. If you restore the database back to the point in time that corresponds to the log file S007000.LOG, what should DB2 do with the fact that you now have 2 sets of log files with the names S007001.LOG ... S007500.LOG? They represent 2 unique sets of transactions, and they are called log chains. So, when you create a database, DB2 creates the first log chain, C0000001. If you restore the database, it creates C0000002, etc. By preserving these log chains, DB2 gives you the ability to restore each unique series of transactions for a database. 

If the storage group ID is , then the tablespace is not using automatic storage. Don't consider and . Check (this works on Linux/UNIX, but not Windows - sorry): 

This is really an odd question – it's hard to image a scenario where you would need to create multiple databases simultaneously. Are you coming from a different DBMS (like MySQL or SQL Server), where the concept of a database is more like a schema in DB2? 

This is a matter of looking at physical writes per buffer pool or tablespace over time. will tell you this if you are in Delta mode and look at the "Delta p_reads/s" column. 

You are correct in your assumption – there is nothing done at the database level that would track the source of a row. You would need to modify your history table definition and trigger logic to add this kind of tracking. 

You can't do this in a single step. The locking required to truncate the table precludes you querying the table at the same time. The best option you would have is to declare a global temporary table (DGTT) and insert the rows you want into it, truncate the source table, and then insert the rows from the DGTT back into the source table. Something like: 

The reason for this behavior is that DB2 must have enough space for the largest possible LOB locator in the column for the row in the table. INLINE LENGTH is not like a VARCHAR where space is used only according to the amount of data. It's like CHAR – the size you specify is reserved in every single row of the table. From the documentation (scroll down to description of ): 

If you are asking if there's any way to determine whether existing rows that have a value were updated as a result of an existing trigger, then the answer is no. There is nothing inherent to triggers that would allow you to identify them as the source of the data in the column. The trigger could contain some logic to identify itself as the source of the value, but that would be stored in some other column or table. 

If you are asking if you can determine if the table in question has any triggers defined, then yes - you can either 

[This answer was also posted to the duplicate question on Stack Overflow] When you perform an online restore, DB2 must lock the tablespace(s) you are trying to restore. The restore process essentially overwrites the file on disk containing the tablespaces' data. This is incompatible with applications using data in the same tablespace while the restore occurs. If your database has all data in a single tablespace, then an online restore is not particularly useful. If you have multiple tablespaces in the database, applications may be able to continue functioning while the corrupted tablespace(s) are restored, but of course this requires some planning in your application and database design. 

Yes. Tablespaces in DB2 have a attribute that you can use. You can specify these size attributes in the statement (or when creating additional tablespaces), or use to set them later. 

Using and from the main section of the output, you can calculate the total space. (You can ignore the sections that give details for each container in the tablespace). If you have SMS tablespaces that store table data (i.e. not just system- or user-temporary tablespaces), you'll see tablespaces in this output that show . To calculate the amount of data used by these SMS tablespaces, you will need to dump out all of the information from the backup image using . You will get lines like this: 

In your case, this results in . The maximum transaction log size limits 2 things: 1) the absolute size of a single transaction 2) the "timespan" between the oldest active transaction and the newest transaction. You need to make it large enough to handle both scenarios, although if you're running into problems relating to #2 then you really have a problem with a poorly-behaved app that isn't committing. Choosing an appropriate value for should be based on how regularly you want to archive log files, as this will have an effect on your recoverability in the event of a problem. If your database archives log files only once every hour (or less), you will potentially lose much more data in the event of a serious problem than if your database archives a log file every 2-3 minutes (because every time a log file is archived it should be copied to NetBackup). 

Sounds like you are not properly deallocating objects like , , etc., so your application is holding open statement handles in the database. This will eventually exhaust "CLI Packages" in the database, resulting in the error. The proper solution is to make sure that you close these as soon as you're done reading them. You may also find documents suggesting that you rebind to increase the number of CLI packages at the database level, but keep in mind that this is just a band-aid.