A form of the anthropic principle is as follows: "We can observe the universe only because we can exist within it in some way such that we can observe it, and it exists such that we can observe it." What mathematical consequence does this have? I know it's broadly a problem of Bayesian probability, and we must consider all that we see from the perspective P(A|B), A = some aspect of observed reality, B = we think, therefore we are. Can this be formulated in some useful and general way to answer questions about the universe, existential, cosmological or otherwise, or do the mathematics here give us little information? NOTE: I know that the anthropic principle is often stated in a much more specific way and looked at from the perspective of cosmology, but that's not what I'm looking for here. Edit: To clarify the mathematical content of this question I'll give two examples (one from a comment below). 1) I've seen claims like "the anthropic principle indicates that we most likely live at a time such that half of all people that have ever been born have been born". I want to know if a statement like this is at all reasonable or not. 2) Consider it in these (not entirely sufficient) terms: You have a vague outline of a set of prior distributions in addition to some error-prone observations whose errors depend on the prior distribution. How can you glean information about the prior distribution. 

Christos Papadimitriou is in his late 50's now (I can't find his exact age, which is a little strange), and in just the past few years he's done major work in algorithmic game theory, a field at least somewhat removed from the one he made his career in. Technically, he's a theoretical computer scientist - I say this is close enough though. 

I am trying to speed up a sparse signal recovery algorithms. My sensing matrix is a set of Toeplitz Blocks, M = [T1,T2,T3,...,Tk] The objective is min ||Mx - b||_2^2 + ||x||1 What I'm actually doing is trying to encode an image with multiple patch-like bases, each of which can be centered anywhere on the image. Is there any structure of my M = [T1,...,Tk] sensing matrix that I can take advantage of? For instance, can I efficiently compute (MM')^(-1/2) or do any other useful structure I can take advantage of to speed this up beyond naive applications (adapted to us the convolution operator) of other sparse recovery algorithms? 

Many of the methods of physics are vastly more general than their use in that discipline. For example, information theory overlaps with a lot of statistical mechanics, and the latter actually developed first. ET Jaynes wrote a famous paper illustrating the connections. However, each is comprehensible without the language and intuition of the other (though I do not deny that a richer understanding comes from knowing both). What other methods of physics (particularly those with a statistical or computational bent) have interpretations (Please mention useful introductory texts!) that are completely physics free? I understand that various field theories meet this criterion; any good non-physics introductions? 

I wonder what you all think are some questions that our current understanding of math can't really answer but which, in the reasonably near future (Let's call this "before you retire"), we will be able to answer? This is a deliberately broad question. You can speculate on particular conjectures, on developing interrelations between disciplines, on methods, applications or anything you want. Try to describe a little bit what sort of program you see leading up to these advances. The only area I can really attest to is that of complex systems (large networks of interacting agents, in general)). Right now, this isn't really a mathematical discipline. Non-linear dynamics and applied probability sometimes touch on questions of complex systems, but right now nearly all study in the field is of a holistic and decidedly non-rigorous sort. NOTE: If you're an applied probabilist, don't think I'm forgetting about you - it's just that there's generally a huge gulf between our ability to simulate these systems very approximately and our ability to define their behavior rigorously. I believe that, within the next 20 years, we'll see a modest revolution in this field. We'll be able to bring in tools from the fields of probability and statistical mechanics and create new tools such that complex systems will allow us to understand certain collective human networks (genetic networks, contagion, traffic, urban growth are probably some of the most achievable) on the same level that we understand systems of particles today (or even that we understood systems of particles 50 years ago). In its early stages, this probably still won't be rigorous, we'll rely on hybrid techniques from machine learning, stochastic processes and domain-specific techniques to deal with each sort of problem. By the end of a couple more decades, however, we'll have made leaps forward in our ability to rigorously describe, understand and predict complex systems. 

Or another way to put it: Could the axiom of choice, or any other set-theoretic axiom/formulation which we normally think of as undecidable, be somehow empirically testable? If you have a particular scheme for testing it, that's great, but even the existence or non-existence of a proof regarding potential testability is wonderful. How about something a little simpler: can we even test the Peano axioms? Are there experiments that can empirically verify theorems not provable by them? This is a slightly fuzzy question, so to clarify what I mean, consider this: the parallel postulate produces good, useful geometry, yet beyond its inapplicability to the sphere, there's evidence to suggest that the universe is actually hyperbolic - this can be considered an experimental evidence "against" the parallel postulate in our universe. Edit: Thanks to all the people who answered - I understand the concerns of those who don't like this questions, and I appreciate all those who answered a more modest interpretation that I should, in retrospect, have stated. That is, "Is the axiom of choice agreeable with testable theories of mathematical physics, is it completely and forever irrelevant, or is it conceivably relevant but in a way not yet known," to which I got several compelling answers indicating the former. 

A huge amount of financial mathematics assumes Gaussian distributions of risks and Brownian movement of prices. What efforts have there been to replace these with heavy-tailed distributions? For example, could Black-Scholes be adjusted to assume heavy-tailed distribution of price movements, or is this too mathematically difficult? 

Or rather, what function can be parametrized with some value t in [0,1] such that f(x, t= 0) = x, f(x, t = 1) = e^x, and f(x, 0 < t < 1) is a principled interpolation between those two, kind of like the gamma function is a principled interpolation for the discrete factorial. Obviously, many functions fit the bill, like f(x,t) = x^(1 + t*(x/ln(x)-1) ), but that seems kind of arbitrary. What mathematically useful/elegant function exists? 

I have finite set of geolocation point data, and I'd like to estimate the fractal dimension. I know there are several ways to do this, and some of them give different numbers. What is the most appropriate fractal dimension to look at and what method do you recommend I use to estimate it numerically? Thanks 

I have a set of data, each instance in the real $[0,1]^{d}$. However, it's actually all in a relatively small range around 0.5, clustered into classes in even smaller ranges. The actual origin of the data is the output of an untrained neural network, but don't worry about that. The distribution is correlated between variables in an unknown but not ridiculously ill-conditioned way, and I have no guarantee that the intra-cluster distance is dramatically smaller than the inter-cluster distance (it may be of the same order of magnitude). I would like a method for embedding into d-dimensional Hamming Space that: 1) Preserves clusters as best as possible 2) Maximizes inter-cluster distance 3) Maintains relative inter-cluster distances 4) Minimizes intra-cluster distance In that order. The obvious solution is some sort of machine learning method, but since i'm actually trying to apply this to improve a different machine learning method, I want a method that's rather quick and simple instead. What I was doing was just rounding, but that failed #2 spectacularly. Rounding based on centroids or mediods of the data instead would be a little more sophisticated, but still wouldn't do a great job of #2, #3, and sometimes #1. 

Think of the sunflower as a series of almost-triangles of increasing size centered on the central point, with integers at each of their points. It's a couple steps more complicated, but try starting with a tetrahedral arrangement around the initial point. This arrangement has four natural gaps, which 5 through 8 can go into. I'm sure there's some rational way, from here, that consecutively larger almost-tetrahedrons can be put together around the central point. It might not have some of the more interesting properties of the sunflower sequence, and it's possible that it has arbitrary (or just wrong) elements, but it has the benefit of being scalable to n dimensions (if it works at all). 

The first part of my question is simple: Is every game continuous in time and strategy-space also a game of perfect information with a good equilibrium? For example, consider rock-paper-scissors. The discrete version has no nash equilibrium - a perfectly uniform random mixed strategy is the best option. Continuous rock-paper-scissors, by contrast, allows players to move at some limited velocity (consider 2 cases, acceleration is limited and acceleration is infinite) through a "strategy space" s.t. R+P+S = 1, and (0.5, 0.5, 0) vs. (0,1,0) returns 0.5 to player one and -0.5 to player 2, while (1,0,0) returns 1 to player one, -1 to player 2. To avoid the "go directly to the middle" strategy, it's fine to remove (1/3,1/3,1/3) or some disk around it from the strategy plane. So, is continuous RPS effectively a game of perfect information? For a more dramatic example, consider the stock market as a game. If it were continuous, would randomness essentially be removed? Would a player also need to explicitly know the strategies of all other players as individuals, or only the end result of those strategies (i.e. value of stocks at a given point in time) in order to play perfectly? For a more realistic example, consider a hunt between a dog and hare. Strategies for them are the direction they choose to run in the pursuit. The dog has reflexes r, the time it takes him to notice the hare's change in direction. The rabbit has acceleration a. Ignore the dog's acceleration for now. As r*a becomes extremely small (i.e. the dog's reflexes are swift relative to the hare's acceleration), does this effectively converge to a continuous, perfect-information game (specifically the game of the homicidal chaffeur), or is the difference still important? Specifically, suppose that the dog can only make decisions on pursuit directions at increments equal to r - I don't want it to be a continuous game with a lagging signal. 

In general, for generating extra-regular but not-too-regular distributions of points (in a technical sense, "low discrepancy", meaning that the variance in the length of gaps between points is smaller than a uniform distribution), you can use a class of methods called quasi monte carlo methods. There are libraries in MATLAB. $URL$ $URL$ Though if you want a totally uniform set of points, these won't help you. 

When going off on a tangent from your regular area, where, presumably, you have such mastery of all cutting-edge research from your routine reading that you hardly need to do any extra (if this is false, please correct me), how much do you try to familiarize yourself with that area before beginning to directly attack your problem? Do you read just a few canonical papers and surveys, look thoroughly over a dozen and glance at a couple dozen more, or do enough to write a whole survey article of your own?