Then you will get the mysterious error. This means, of course, that the file isn't provided by any package. The reason for that is that the file is not in the kernel version shipped by CentOS 6; the kernel feature your program depends on only exists in newer kernels. You will need a newer kernel in order to use this software. The obvious options are to use CentOS 7, or if you won't upgrade for some reason, a third party kernel such as elrepo. 

"Content restrictions" suggests that something in the email content is the problem. The prime suspect, of course, is the file attachment. You would be well advised to not send these as attachments, but to provide a download link to the file in your web site instead. 

The destination unreachable ICMP packet is a standard Internet control message which indicates that there is no service available on that port. There's generally no reason to disable it (and you didn't give one). But if you really want to disable it, you can just change your firewall rules to drop packets rather than rejecting them. (Wait, you are firewalled, right?) 

Bsaed on the fact that it starts with a single backslash in the error message, it looks like an escaping problem. PowerShell has an annoying habit of treating anything that begins with a backslash as a regular expression. Try escaping that parameter. Three possible ways come to mind: 

The packages included in these Red Hat add-ons are already present in CentOS. You don't need to do anything to begin using them. 

Normally I'd just point you at the docs, but they turned out to be rather difficult to locate, so I'll point you at the docs instead: Google says you need to set up both SPF and DKIM records to avoid the "via" whatever domain being displayed. 

As far as I've ever known, SSL certificates are not restricted to use on specific IP addresses or blocks of addresses. Most likely Comodo is asking for your region for some billing purpose (e.g. if you live in Europe they may have to do some EU-specific billing). In fact, I can see that by changing the region, they change the currency in which your price is quoted (e.g. dollars for the US, pounds for the UK, euros for the EU, etc.). 

You installed PHP 5.4, but the PHP functions you are trying to use were introduced in PHP 5.5. Use the remi and remi-php55 repositories to update your system to PHP 5.5 (or choose a later version). It should be sufficient to install the appropriate remi-release RPM, enable the and repositories, and then . 

Supposing you are using the tool to manage your Let's Encrypt certificates, which most people do, it is sufficient to copy the entire directory from one server to another. All of your certificates, as well as certbot configuration, are in there. So you can continue on the new server exactly as you were before. 

You're trying to proxy requests to example.com. But you haven't defined example.com as an , so nginx treats it as a hostname to look up and connect to. But you don't have a DNS defined anywhere. To fix the problem, do one of those two things. 

If you're running PHP as CGI/FastCGI then you don't get the (also known as ) function. However... As of 5.4.0, PHP includes a version of this function which works in FastCGI. For previous versions of PHP you can roll your own. Though this approach loses case sensitivity for header names (CGI/FastCGI capitalize all header names) it's generally good enough for 99.9% of possible purposes. This is my code for doing so, written long ago and published under LGPLv3: 

Your does not include your own domain, thus Postfix doesn't know it is meant to deliver the mail locally. This is why you see errors like "Relay access denied" and "Mail loops back to myself". It appears you had already set this up, but had commented it out. 

This section should be removed entirely. First, nginx should be daemonizing. Second, if such messages about being unable to bind() appear, it is not because nginx was running as a daemon, it's because nginx was already running when someone tried to start a second copy. 

With the old CPUs you're using, it seems most likely that you're CPU bound; compression eats a lot of CPU (and it can vary depending on the data being compressed) and is almost certainly the immediate cause of the issue. If you can compress with a faster CPU somehow, perhaps by piping the data to a faster machine first, then that should help. You can also try using a multi-core capable compression utility such as pbzip2, which should increase the rate of compression on your dual-core system, at least. It won't do anything for you on the single-CPU VM, though. pbzip2 writes bzip2-compatible files and is already packaged for most Linux distributions. 

The script will identify many different types of hypervisors and container technologies. It's pre-packaged for most major Linux distributions (e.g. or ) and its output is suitable for use in shell scripts. It correctly performs its detection from inside the virtual guest. 

So, queries are only allowed from the local host! All others get refused. For a proper authoritative name server, you should allow access from anywhere and turn off recursion. 

Using matches traffic that enters the system on the named interface. Instead, you want to match traffic leaving the system on the named interface, which is done with . 

Good news, everyone! It started trying to delete all my files! But unfortunately it ran into an error. I'll leave fixing that and getting the playbook to destroy everything using the module as an exercise to the reader. 

where is present. The problem is that by default the network is considered "up" if IPv4 configuration completes, even if IPv6 configuration does not complete. 

Because of the use of I would not recommend using nginx on Windows in production due to the performance issues it will cause. For a development box it would be fine, though. 

At this point it found candidate drivers and I just clicked Add Drivers. I did not find it necessary to select anything in the list. 

That rule was my idea, so here is my thinking on it. What I observed over a very large sample of requests is that actual web browsers (IE, Firefox, Chrome, Konqueror, Opera, and others) always sent the Accept header. But robots did not always do so, depending on a variety of factors such as the HTTP library, the programmer's competence, etc. Then, robots pretending to be web browsers with a fake User-Agent string would sometimes be missing the Accept header. So, the idea here was to catch out robots pretending to be web browsers and doing it poorly, by not sending the Accept header. To implement this rule, you must also check that the User-Agent string starts with "Mozilla". It's not appropriate to block unless both of these conditions are met: The user agent claims to be a web browser, and the Accept header is missing. 

Close port 3000 in your firewall. Then make sure you start thin in such a way that it is bound only to the localhost. This absolutely prevents inbound connections not originating from the local machine itself. For instance: 

Finally, it's best practice to use instead of unless you really absolutely need to pass the command through a shell. 

If what you get after putting the .tar.gz back together and unpacking it results in a disk image, then you should be able to recover files from it. To begin with, for best results you should be working from a Linux system, with plenty of drive space available (50GB or more). I'm not sure if this is even possible with Windows. First put the file back together: 

The module path will vary depending on the Linux distribution you're using. Some distros (e.g. Debian-based) may put it in strange places. To find the module path on your system, run and look for the in the output. For example: 

I don't think you want to find all the PHP files, just the malicious ones. Linux Malware Detect is a good tool for this. Of course, what you really should be doing is securing your server. Keep everything up to date, remove unnecessary modules/plugins, harden WordPress and Drupal, etc. 

Everyone needs to be able to read and write to . So these are not an issue. The kernel version is not really an issue either, provided you're actually installing updates, using Ksplice, etc. As for making outbound network connections, you can restrict this, as some web sites do not need to make such connections. But sometimes they do, for instance most web sites will need to connect to a database. You can manage this with SELinux booleans, such as: 

The log priority is not stored in at all. Like its predecessors, uses the priority as a hint to determine where to route any given log message. By default, it does not log the priority itself. However, the systemd journal does log the priority (and keeps a lot of other interesting metadata). You can use the option to to narrow down log messages by priority. 

If you ever put a computer in that domain, you will get bizarre DNS failures, where when you attempt to visit some random site on the Internet, you arrive at yours instead. Consider: You own the domain . You set up your workstation and name it. ... let's say, . Now you will notice in its it has the line: 

will give you a URL for connecting to a graphical console for a guest, if the guest doesn't have a serial console enabled. will connect to the guest's serial console. If all of this command line crap just annoys you, install the GUI and you can point and click your way around managing your guests. (Note that cannot yet install LXC guests.) 

IPsec in transport mode would do this, but this really isn't scalable. Four nodes is about the most at which I would even think about it. I'm currently using strongSwan for IPsec on Linux, which is easy enough to set up this way. 

You're missing DNS glue records for your domain. Since you're using GoDaddy you can set these up in the GoDaddy control panel. After clicking your domain and "Launch", look in the lower left corner for this and add the records for ns1 and ns2 there. 

Because of this, only URL paths which start with will be processed by the enclosing . This is not what you want. Instead, you should have all URLs processed, with 

SELinux is not namespaced, so individual containers cannot have their own separate SELinux policies. SELinux will always appear to be "disabled" in a container, though it is running on the host. For more information, see Introduction to Linux Containers. If it is SELinux policies that you are testing, use a full virtual machine. 

Just re-enable the default virtual host and ignore anything that hits it (as such requests are querying the IP directly, or are malicious). For example, as seen in the nginx 1.12.x :