From the sound of the error message, it seems like I also need to dedicate eth0 (PCI ID 01:04.0) to the domU. Am I correct? If not, what am I doing wrong? Thanks! 

This solution still doesn't allow different processes to bind to the same port. And I'm not even sure that I'm on the right track here by looking for an iptables solution. Any suggestions? Is there maybe a hack that be applied in userland? Thanks! 

Yes. VPC paired with a IPSec VPN connection to your corporate network. This will allow your AWS resources to function as an extension of your existing network. Be warned though: it's not for the faint of heart to configure. 

I commented on that blog article which was subsequently deleted by the author. You may wish to consider that level of professionalism as you read his post. What I wrote: AWS has never had a region-wide failure. If the blog author had taken the time to read the two incident reports that he linked to, he would have learned that. A properly designed AWS deployment always uses multiple availability zones. The worst impact that I've experienced through both events was slightly degraded performance or intermittent API access; neither of which were a slow stopper. What I didn't add to my original comment: 99% of the flak thrown at AWS is by people who don't understand how to properly use it. They treat it as just another VPS or just another co-lo server. You have to build your application with a cloud-based deployment in mind. It must tolerate random node failures, use stateless application servers, and seamlessly work across multiple instances and subnets behind one or more load balancers. You should be using loosely-coupled components and communicating between tiers using message queues. In short, this blog post was just another long-winded rant by someone who doesn't understand how to use the tools they've chosen. 

This design does not have any single point-of-failure*, will automatically self-heal if an instance dies, and is smart enough to scale up or down according to resource demand. Total cost: about $200/mo. if you opt to purchase 1-year reserved instances. I'm working on putting this configuration into a combination of a CloudFormation template and cloud-init/Python scripts that are automatically pulled from Github on boot. Basically it will allow anyone to pretty much push a button, wait for about an hour, and then come back and this whole environment will be waiting. I hope to have this completed by the end of the year. If you'd be interested in getting a copy of the template, send me an email to "jamie" at the website listed in my profile. * The NAT instance is a SPoF, but that's primarily a design limitation of VPC. And it's a non-critical component that could fail and not affect the application. 

I'm planning on stacking Dell PowerConnect 6248 switches together. I know that I need one stacking module in each switch, but I'm uncertain about the cabling. Since I only have two switches, do I need to loop them using two cables or will one stacking cable suffice? Thanks! 

I have successfully set up OpenVPN connection between my firewall at home and my EC2 instances. It works quite well, but I'm using Linux at both endpoints, so my experience won't directly translate to your config. As much as I hate IPSec VPNs, you might find that an easier route to try. Have you looked into Amazon's new "Virtual Private Cloud" service yet? This sounds ideal for your needs. 

No, instance states have to be consistent (need to be identical) to properly work behind a load-balancer. The proper solution to this problem is to upload your photos directly to S3. 

Someone installed the 32-bit version of MS SQL Server 2005 on a 64-bit OS with 16GB of RAM. Two instances of SQL Server are running, but each process is only using about 1.7GB of RAM. Combined, this equals about 3.2GB, or the hard limit of 32-bit applications. I'm trying to figure out why each instance isn't using it's own 3.2GB max address space? 

I am looking at rolling out a new configuration management tool to replace our home-grown solution. The defacto standards are Chef and Puppet, both of which are Ruby-centric (though can be used to deploy non-Ruby environment, obviously). The vast majority of our development is done in Python and our in-house deployment tools make heavy use of Fabric. Therefore I am learning towards Salt since it too is Python, even though it is not as mature as Chef or Puppet. But since I'm not familiar enough with the options, I'm finding it difficult to compare apples-to-apples. Other than the smaller community, would I be giving up anything signifcant by using Salt rather than Puppet/Chef? Update It's been six months since I posted this question. And despite it being closed, it's been viewed over 1,000 times so I thought I'd comment on my experiences. I eventually decided on Puppet since it had a bigger community. However, it was an immensely frustrating experience, mainly due to the convoluted Puppet configuration syntax. Since I now had a frame of reference to compare the two, I recently took another look at Salt--I'm not going back. It is very, very cool. The things I like best: 

The obvious answer is to do what the error message suggests and boot with the "irqpoll" option. However that has no affect, regardless of whether I boot the dom0 or the domU with "irqpoll". Does anyone have any suggestions? I'm getting somewhat desperate here... Additional Technical Details Truncated "lspci -vv" output on dom0: 

I have a Dell PowerEdge with a 8x backplane and a PERC 5i RAID controller. I'd like to configure two disks in a RAID1 array and four other disks in a RAID5+1 array. I'm pretty sure this is possible using a single controller, but I wanted to make sure first. Thanks! 

I get a similar story in the Dom0's log. But as you can see, it completely disables the NIC's IRQ and shuts down the interface. 

Yes, if you have reserved instances purchased in , for example, then you explicitly want to specify at launch. RI's are purely a backend billing thing that function like an invoice credit. The EC2 instances themselves have no awareness of whether they're an RI or not. The way RI's are handled is actually one of my top five complaints about AWS. I'm hoping that they improve on this soon. 

If you take care to replicate the records exactly, then it will not affect your users at all. Many providers will let you export your "zone file" so it can be reimported elsewhere. If this is an option, it would save you some typing and help avoid possible typos. 

First things first, if you only have two instances (one web and one DB), you're doing it wrong. You should be setting up an elastic load balancer with a minimum of two application server instances behind it. Instances can (and do) fail from time-to-time. And you really should be using RDS for your persistent data store. 

As previously mentioned, the robots.txt spec is pretty simple. However, one thing that I've done is create a dynamic script (PHP, Python, whatever) that's simply named "robots.txt" and have it smartly generate the expected, simple structure using the more intelligent logic of the script. You can walk subdirectories, use regular expressions, etc. You might have to tweak your web server a bit so it executes "robots.txt" as a script rather than just serving up the file contents. Alternatively, you can have a script run via a cron job that regenerates your robots.txt once a night (or however often it needs updating) 

Your stated requirements are extremely vague so expect to get only extremely vague replies. Having said that, look into Amazon EC2 and CloudFront Streaming. 

You don't want to be doing this. It could break the upgrade path of Amazon's AMI since it's a rolling release. Instead of wholesale replacing the entire file, specify the changes that you need in a custom .ini file and put it in . If a setting in the custom .ini conflicts with one specified in , the custom setting will take precedence. 

No, there is no way to automatically do this. You could write a simple script in your language of choice to manually replicate keys to a different region. I'm not really sure why you'd want to do this though? The network latency introduced from traveling half-way around the world would negate any benefits of memcache. You're better off replicating your persistent datastore and then having a separate memcache cluster in from of that data in each region. 

Solved! I happened to notice this line in on my NFS server when I was attempting to mount an export from the remote client: 

I'm using CentOS 5.4 on my dom0 with a stock Xen kernel. I'm attempting to use the pciback module to hide some of the Ethernet ports from the host and reserve them for a domU I intend to use for a firewall (process described here and here). However, when I launch the domU, I get the following error message: 

Just tossing this out since it hasn't been mentioned yet. But MS's own PowerShell is actually a really good CLI for Windows. It has some Bash-like features that should make it feel somewhat familiar. In my opinion, it's worth learning if you do any type of scripting or automation with Windows, even if you're primarily a *nix guy (like myself). You'll be able to do more in less time compared to trying to hack together something that runs atop Cygwin. 

Yes, this is possible. Here is the design pattern that you'll want to implement. The short version: You'll create an IAM user account for each Alice and Bob. They get their own AWS access keys that will be used to access the S3 bucket. You'll then apply a S3 policy to the bucket in question. Where most people get confused is that "IAM policies" and "S3 bucket policies" are two, different things and both need to be set up. 

AWS publishes their public IP address space. You could probably try pinging some random IPs in those ranges and see if your packets get killed at the firewall. 

This is expected behavior of EC2. If you want more granular control over your IP addressing, you're going to have to create instances inside of a VPC. 

Problem solved! Oddly enough, disabling UDMA in the BIOS fixed the problem. Thanks goes to this Ubuntu thread for having the answer. And thanks to everyone here for their suggestions as well! 

If your database is MySQL, MS SQL, or Oracle, you're much better off using an RDS instance rather than trying to roll your own using EC2 and EBS. RDS functions like a cloud-based version of a DB server and greatly simplifies configuring and maintaining a scalable, fault-tolerant database. The underlying OS is completely managed for you so you can't SSH into it, but you can connect to it with your normal client tools to manage your database. 

This will vary considerably: ball bearings vs sleeve bearings, made in Japan or China, ambient temperature, number of on/off cycles, etc. But this is kinda like asking if you should be changing out hard drives periodically as they age. You don't do that because there's no reliable way to predict failure. So the proper way to do this is to gracefully handle failures when they do occur. This would be a RAID array for hard drives, or using redundant fans for cooling. 

Edit: It's taking about 5 minutes to just get past the "GRUB loading stage 2..." message. I've come to the conclusion that the problem is likely either: bootloader, BIOS/firmware, or a hardware issue. I've played with all relevant BIOS settings without any luck and replaced my IDE cable in case I had a bad one. I have a new compact flash card (a different brand) on order, but it'll be a few days before it gets here. I'm also currently running Memtest86 to see if I have an odd issue with my RAM. Once that's done, I'll see if there's a firmware update available for my motherboard. 

As part of a pilot project, I am attempting to set-up a thin client environment for a team of developers using NoMachine. Each developer will login to the same Linux box and do development via an X session. Currently, each developer runs their own HTTP daemon on their local workstation that listens on 127.0.0.1:5000. However, if I move everyone onto the same machine this obviously creates a problem with port conflicts. Ideally, I'd like to keep their workflow the same. If I have to assign everyone a unique port, it's just going to create a lot of grief and confusion. Is there a way to do this? Can different processes bind to the same port on a per-user basis? I discovered a way to use iptables to do port redirection on a per-user basis, but this only solves part the problem: 

The problem was a faulty NIC driver. My research indicates that I could have patched and recompiled the driver, but it was easier to just swap in a different NIC. 

is the most reliable way to get a user's IP address when your app is behind a reverse proxy/load balancer. However I don't know what you mean by "trust"? is an HTTP header that can be easily modified by the client or any system between you and the client. You don't want to use it for anything like authentication. 

will show you which version of the httpd (Apache) package you have installed. Adjust the grep accordingly to get the other package versions. 

You never want to run a single EC2 instance in a production environment. They aren't durable and can permanently go offline at any time (taking all the data stored on them with it). Use a minimum of two application server instances behind a load balancer. Since you're new to this, I'd recommend that you read up on the documentation for Amazon Elastic Beanstalk. It's a PaaS-like configuration framework that makes it easier to get a properly designed AWS infrastructure online. 

In this instance, the OpenVPN would be used to logically extend your LAN. This would allow your devs to connect to the web server, database, etc. as if the EC2 instance was a computer plugged into your network. The alternative is to tunnel everything through a SSH connection using port-forwarding. Each developer would have to manually set up their SSH client to do this and for each TCP port they wanted to connect to. Functionally, these two options are equivalent (and even use the same encryption technology). But a VPN is generally easier for users to use because it "just works". 

This tells you who ARIN assigned my netblock to, which is Online Technologies, my datacenter provider. But there's no mention of me or my company. It's because I don't have my own assignment from ARIN, so I'm subletting address space. But all the equipment is owned an managed by me. Does this mean I'm a reseller of the datacenter? As part of our hosting, we also provide CPanel accounts. A feature of WHM/CPanel is the ability to create sub-accounts. I have one business partner who resells accounts that he sets up and provides first-level support. But I've available for second-level support, which I provide on a routine basis. Is he considered a reseller? I'm a bit confused on how you can be writing a book on the web hosting business without understanding how nebulous your question is? 

This allows traffic to/from behind the peer. However, I also have another subnet, behind the same peer that I would like to allow access. So I've had to modify my ACL to include : 

Wow... you're taking me way back. I used to work front line support at an ISP back when softmodems hit the shelves. Soft modems were sometimes a problem because they didn't do any of the DSP stuff on the modem itself, but within the OS driver. This was cool because you could add new features to the modem with a driver upgrade. It was also the source of many problems because you were relying on Windows to do the job previously left to dedicated hardware. More info at Wikipedia: $URL$ 

My company currently uses Google Apps for email. I can configure Google Apps to forward all outgoing email through a relay server. I can also point my MX records at this server and have it forward incoming mail onto Google. Therefore I can configure Postfix act as a proxy for all email that is both sent to and from our users. This part is done and works fine. However, I'm not sure how to retain a full copy of each message that passes through it. I'm interested in doing this so that all email to/from customers can be dumped into our CRM system so they're searchable by everyone in our company who might talk to a customer. The most common way of doing this is to have a "always BCC" setting specified in Postfix. However, this would only archive email sent to customers, not email received from them. Does anyone have any suggestions? Thank you!