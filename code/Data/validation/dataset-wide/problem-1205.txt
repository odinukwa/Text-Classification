That's a pass through (1 million) rows to inject a column. Then, you have to perform a join of the ugliest kind 

In fact, by doing this, you are actually taking the and changing it on . Looking back at your first query 

Look for . It should be 'Yes'. Look for . It should be 0. If it greater than 0, keep running until is 0. 

STEP 02 When calling the Stored Procedure, retrieve the role_id and pass it into the Stored Procedure: 

SUPPLEMENTAL INFO There are also table-specific grants in and column-specific grants in . These grants are stored in columns. Just run and and see. 

Prevent mysql from looking through fragments in a table in an attempt to load data into the right sized fragments. Eliminating these fragments will reduce this operation. Having the index statistics recomputed helps the MySQL Query Optimizer construct better EXPLAIN plans. Otherwise, queries may deteriorate in execution time because the MySQL Query Optimizer decided to take bad guesses at the EXPLAIN plan. This would be a definite symptom of a table that has had a high volume of UPDATEs and DELETEs. 

In most cases, crashing a MyISAM table is nothing more than throwing mud on the MyISAM file header. It keeps a running count of the number of times the file has been opened. It increments upon opening and decrements upon closing. You may want to look at the value for open-files-limit. This is an option that you cannot dynamically set with mysqld running. It can be configured at startup. What is interesting is its explanation in the MySQL Documentation: 

things like these. DISCLAIMER: I AM NOT ENDORSING GIVING OUT SOCIAL SECURITY NUMBERS. YOU SHOULD WORK WITHIN THE MEDICAL COMMUNITY, AND NOT WORK LIKE A TELEMARKETER. 

OPTION #3 You may want to consider just customizing a solution with ComputeEngine instances with MySQL manually installed and create frequent snapshots of your data folder (which should contain InnoDB tables only along with binary logs). Additional instances (at least 1) for setting up MySQL Replication slaves may be need to still have additional HA. 

Second, I would use periodic mysqldumps. Since the database is very small, you could run the following script every 24 hours: 

Your mission, should you decide to except it, it to carry out these steps with the hopes that Step 04 is not needed. As always, should any of your data be caught or killed, the DBA StackExchange will disavow any knowledge of your actions. This is not a tape, so it will not self-destruct in 5 seconds. UPDATE 2011-10-19 12:50 EDT I like your option 3. It seems the quickest and dirtiest. Here is how you can do this: Step 01) mysqldump everything to /root/MySQLData.sql Step 02) Drop all databases Step 03) add this to /etc/my.cnf 

You will note that the will have an blank username and some host. GIVE IT A TRY !!! UPDATE 2015-11-17 16:24 EST The column known as in no longer exists in MySQL 5.7. It was renamed . Proper Approach What you should have done is run 

STEP 05 : Setup replication from InnoDB Server to MyISAM server That way, you can change back to MyISAM by just failing over to the server that's MyISAM 

copy db1:/etc/my.cnf to db2:/etc/my.cnf change to in db2:/etc/my.cnf on db1 on db1 rm -f /var/lib/mysql/mysql-bin.* on db1 rsync db1:/var/lib/mysql to db2:/var/lib/mysql on db2 on db2 (Note : MASTER_LOG_POS is 98 before MySQL 5.1, 106 for MySQL 5.1, and 107 for MySQL 5.5) on db2 on db2 and let replication catchup (Seconds_Behind_Master=0) on db1 on db1 and let replication catchup (Seconds_Behind_Master=0) 

In both cases, once you done, simply drop the new Instance (saves money) at your discretion. If you combine the two ideas, you could load your PHP script's output into the point-time instance. Any needed data to retrieve from production could be read from the Read Slave to prevent further load issues with production. WARNING Bringing up instances can be time-consuming. So, plan to have the instance brought up early enough for report and import time. This is more ranting and brainstorming than a concrete answer. 

These steps handle the creation of the temp table when restarting mysqld. STORED PROCEDURE You need to retrieve the Connection ID using the CONNECTION_ID() function: 

This index might get selected and never have to touch the table because all columns (, , ) are in the index. GIVE IT A TRY !!! 

Perhaps you could add a counter to the loop so that every 100 rows, you run SLEEP for one second. I will add that code: 

This will not dump all the rows. It will sample the data and come back with a recommedation for the best column definitions given the data. Why is that important? 

The only way you can do that is if you had every binlog since you loaded master1 aznd never erased any of the binlogs. Most people rotate their binlogs, making this impossible. I need to setup a "replicator" user (with replication perms) on both master1 and master2? Yes EPILOGUE I have setup ucarp over the years as the failover mechanism for the floating IP. Most use Linux Heartbeat, Pacemaker, and others. So, you are in the ballpark of what needs to be done. 

If you see that means there is a text version of the slow query log. You will have to get your SysAdmin to get you the desired slow log info (Starbucks GiftCard might be required for this one. If the slow log file contains the same messages, take your Starbucks GitftCard back.) 

Once you do that, going forward, any INSERT within the same 10-second interval gets immediately rejected. That way, you do not have to code it. It will be part of the table design. Naturally, you will need a trigger to populate . Here is the trigger: 

If stays at zero, CONGRATULATIONS !!! If goes back to 7720 immediately, check the date and time set in the OS on the Master against the the date and time set in the OS on the Slave. Give it a Try !!! 

Download that ZIP and try running that . You could then collect all the files with something like this: 

Scaling up (beefing up hardware [CPU, spindles, etc]) Scaling out (splitting read/writes,provisioning replication and high availability) Tuning of DB Configuration Query Optimization Combination of one or more of the these 

Of course, you cannot import that into MySQL until you want the string NULL to be the value to be imported. 

Yes, you are right. I have setup Master-Master setups for years with one Write Master. I never touched the auto_increment options. Never had an incident. 

There is no implicit shared lock in REPEATABLE READ isolation level. You need to change the transaction sequence as follows: ISOLATION LEVEL You could switch to the SERIALIZABLE isolation level and disable autocommit. That way, the row is locked in shared mode implicitly. If you want to stay with REPEATABLE READ or READ COMMITTED, you will have to call SELECT ... FOR UPDATE manually before doing the UPDATE. QUERY (Optional Suggestion) You should also experiment with 

You could do this setup for DSN if you want, but you do not have to go that far yet. Why? The default is the processlist. This simply means that you did not run on the port 3307 instance. You cannot use processlist on a Master where all the Slaves are stopped. When you run on port 3307, a DB Connection should appear in the processlist of the Master (3306 instance) from the Slave (3307 instance). In light of this, you should just run on port 3307 and then run the . Give it a Try !!! 

Even if you defragment the InnoDB with , there is no way of knowing if a single row occupies two or more pages. ALTERNATIVE APPROACH You could probably force the index statistics to give you a round figure. You could just run 

Please take note of the row number . If that row number comes back in the error message every single time your mysqldump fails, then maybe you can suspect data corruption. However, if your get a different row number with every mysqldump attempt, this suggests mysqldump connection is able to move along the large table and just timeout and some random point. Please go set those values and try the mysqldump again. Then, come back back tell us what happened. 

What is the Query doing ? Since the column appears before column in the index, the query optimizer is actually saying: 

Running this query now will give you all of 2015. Running this query next year will give you all of 2016. NOTES gives you the first day of last year gives you the last day of last year The reason I use 

That way, after failing over your application to the Slave to become the Master, you do not have to restart mysql to enable it later. You could then setup replication to the new Master. 

I tweaked that query to display the Process ID of the Blocking Transaction, the Process ID of the Requesting Transactions blocked and their respective values from : 

This would not be in your best interest to do this unless this is a one-time reordering. My advice would be to just stick with creating the index and let MySQL do all the necessary heavy lifting. In addition, you will need to run once a week to ensure the latest index statistics before running large date scans.