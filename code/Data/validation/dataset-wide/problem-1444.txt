First there is the OP's original algorithm. Then the linear search proposed by 200_success. We see that the linear search is much faster than a binary search for such a small array. The overhead of the binary search is just too much here. Then izomorphius suggested using a array to avoid the searching altogether and this is again much faster than even the linear search. But at this stage, branch prediction on the boolean array is starting to become an issue. Because the data is random the branch is poorly predicted. I propose using an array of instead of and simply add the contents of the array to a counter to avoid the branch ( statement) and avoid the branch-prediction fails. The result is by far the fastest algorithm this far. Beating the boolean array with a factor 11x, and the original algorithm by a factor 27.5x. Finally, I'll attach the source code for the comparison (requires the excellent Âµbench): 

As you can see there are some attempts at using Dependency Injection, which I learned of about 1 year into the project. There exists a factory for building loadouts, but it's not that well developed. 

In addition to @mdfst13's excellent answer. I see nothing that could leak memory. However you are allocating a lot of new objects in a tight loop and then throwing them away. These must sooner or later be garbage collected by the VM. The GC sees that it has plenty of memory available in the system and is thus postponing its work of cleaning up so that your program can run quicker, it will clean up the memory when it feels that there is a need to do so. In other words, things are working as normal. Do not be alarmed by the memory usage. 

If you evaluate 300 000 nodes, we can estimate a upper bound of the chance to not get a collision anywhere in the table by: 

I have some legacy classes written without thread safety in mind. Instances of these classes are now being accessed in a multithreaded context in a thread-un-safe manner. Cue chaos. To fix this I make a wrapper class that enforces the user to take a transaction lock to serialize access to the instance. 

because both the and 's destructors will clear them and if each of them only contain objects that clean up them selves like then you don't need to do anything. I will refrain from reviewing high level concepts of the implementation because I think it is too hard get an overview of your code due to how it is formatted. 

to clean up after you. Please note that if you do this, you need to completely detach a node from the tree in your methods before deleting it, otherwise the children will go too. But I find this to be better style anyway; detach, re-link tree, then delete. Also you're being inconsistent with your naming, you have and . Either should be (first letter of first word is lower case), or should be (first letter of all words is upper case). Personally, I prefer the later. 

then the picture changes a bit. The class is an active class that performs some function in the program, other than just holding a bunch of data. Note that this type of object is rarely shown in the UI, although its function may be triggered from an action in the UI. If this interface would contain data it would have methods called for example . Which in an interface class would look odd as it prescribes the interface to have certain data members in which case it might just as well be an abstract class. I find through experience that separating behavior and aggregate calculations from your data model will avoid a lot of troubles such as classes growing too large or having mixed concerns. And it helps in keeping classes small and with good cohesion. Especially when you have anything but a trivial data model. In closing I want to say that accessors are not automatically bad by them selves, they only become a problem when they are used blindly without first considering if the functionality they are being used by should be a part of the class itself. When used to access attributes of your data model for display or use by the behavior, they are perfectly fine. 

Return value You are returning a vector by value, a naive compiler may cause the entire vector contents to be copied and if you have a vector of vectors, this is really bad. However most modern compilers perform Return Value Optimization(RVO). However if you don't want to be at the mercy of a possibly dodgy compiler, just pass a reference to a vector where you want to store the results as an argument to the function. Time complexity So you do this: 

The idea is that a long contiguous vector is nicer on your CPU cache, is easier to move around and has less overhead in memory usage and also contributes less to memory fragmentation. There are also portability implications just about all software that I know of that processes images or matrices use this kind of linearisation and you should too if you want to interoperate with them. Too see the (non) effects of this I have written a benchmark here on ideone. And I'm posting the results here: from Ideone: 

In addition to what has already been mentioned, the string representation of the name is a derived property of the registration number and as such I believe that it should be computed internally in the constructor. This saves you one argument to the constructor and avoids code duplication if you have multiple factories. Like this: 

Note that for the random access iterator the sum and sum of squares are calculated before the check. I'm expecting an optimizing compiler to see that distance doesn't depend on the value of the loop and move the check earlier. Even if it doesn't the < 2 case should be rare in a well designed application. I am also wondering if the there is an actual measurable performance difference between the two implementations to warrant the extra complexity. I'm thinking that the distance incrementation is performed simultaneously with the floating point ops in the ALU on a super scalar CPU, so the increment doesn't affect the loop latency. Asssuming a typical x86 super scalar CPU. Measure, Measure, Measure! 

The result must be calculated as if optimized to the result will differ. And in your case you actually want as the correct answer. In your case you should do: 

Your algorithm does 5 passes over the input string with several instructions per character. If you instead just iterate over input string once and keep track of what characters occur in the string (assuming ASCII here) then you can check if all vowels occurred easily, and without any branches. 

In addition to @mdfst13's excellent answer I just want to point out one small thing. When you are comparing euclidian distances or radii of objects you can do the comparison squared and get the same result. Formally, \$d \gt \ell \Leftrightarrow f\left( d\right) \gt f\left(\ell\right) \$ for any monotonic function \$f\$. The same is true for other inequalities such as \$\lt, \ge, =\$ etc. Which means that: \$\sqrt{s_x^2 + s_y^2} < \sqrt{v_x^2 + v_y^2} \Leftrightarrow s_x^2 + s_y^2 < v_x^2 + v_y^2 \$ for two vectors \$s\$ and \$v\$. In short, when comparing distances you can always compare the squared distance for the same result, as squaring is a monotonic functions iff the argument is larger than zero, but this is rarely a problem when comparing distances (or radiuses). This means you can avoid doing a costly square root operation in tight loops in many game related applications. Also I do believe that is slower than because is much more general and handles non integer powers. 

Jolly. In summary I can see how this can have some uses in template code but I believe that your implementation is too complicated. My advice is to stop trying to be fancy with and just make something that works. I would probably have implemented it like this: 

you are swapping the iterators, not the values they point to. Which means that the code doesn't even work at all. It gets stuck in an infinite loop, always. Not only that, but the order in which the arguments are evaluated is undefined so this type of code might not work if the variable is decremented first. You would have noticed all of the above if you tried to use this code in any way before posting to review. For future reference, we only review working code and this question is actually off-topic due to it not working at all. Cleaning it all up and fixing the bugs we get: 

If the lamda throws then you might have problems on your hands as destructors throwing exceptions is kind of a bad situation throwing from destructor. Not to mention that destructors are only for destroying the object, releasing any resources held and cleaning up. Further since you're not removing the commands from the queue, when your application shuts down the array will be destructed and each of the destructors on the elements will be executed again. Do I need to say this is bad? Not to mention that you never remove the objects from the queue when you're done with them. I feel that you are overworking this in an effort to make it fast but in practice you're introducing problems and not actually improving speed. If you want your lamda to be 16 byte aligned so that your captures will work then just say so. 

However I do not see the point of prohibiting this use at the cost of making the implementation harder to read (I had to reason about those functions for quite some while before I was sure they work and why they work). You could just as well have 

Broken for more reasons Without even looking at the details, I can tell that the mutex is not safe. The compiler is allowed to re-order your instructions which means that your writes and reads may not occur in the order you have written them. The compiler may actually even omit them totally... Not to mention there is no guarantee on memory ordering semantics at all. Writing threading primitives correctly is VERY difficult, I do not recommend that you try to write your own unless you are very knowledgeable about the pitfalls. (And if you are, then you know you don't want to do this unless absolutely necesscary). 

Matrix multiplication, while it seems trivial to implement from the definition, the naive implementation you are using is actually slow for anything but small matrices. You really should look into more efficient algorithms for matrix multiplication, a good place to start is the Wikipedia page here: Matrix multiplication algorithm. 

Calling will result in at most recursive calls, which is still a lot, but less than your code. But here is the catch, is very simple in structure and the compiler can expand the function in a way similar to loop-unrolling which removes a lot of iteration and overhead. This is a case of K.I.S.Silly. One way that both of the solutions can be speed up dramatically is by using Dynamic Programming, like this: 

Here and are both \$\mathcal{O}(n)\$ where \$n\$ is the number of spots. This means that for example the inner most loop of : 

Because you are creating a generator in each iteration with the same seed, the value you get from your distribution will be the same. So no, this is not the correct way to use the new random subsystem in C++11. You need to pass in a distribution by reference to . Also personally I don't feel that random numbers should be a part of a sorting algorithm because performance becomes non-deterministic. I would suggest you use one of the other pivot selection strategies. Also your algorithm requires the use of random access iterators. Might be worth noting. 

are using r-value references (NOTE: They are not universal references). Note that const r-value reference doesn't make any sense. Anyway, I assume that this is because you only want to allow usage on the form: 

Firstly, we cannot rely on arguments that the "compiler isn't smart enough to do that" because that may change tomorrow. We need to base our assessment of the code on what the compiler is and isn't allowed to do. Given two input strings and a yes/no return if the string match, any program performing the function can be transformed into the trivial short-circuited for-loop. Provided that there isn't anything prohibiting the compiler from doing the transform. Will writing the sum of xor differences to a volatile prevent the transform? Maybe, it depends on the execution model and guarantees of volatile in Java. In C++ under the as-if rule all writes and reads to/from volatile memory regions must occur in the same order as-if the program was executed according to the wording in the standard (somewhat simplified but that's the gist of it). So for C++ this would work; However I'm unsure if Java has the same guarantee. For C++ this limitation is natural because it must be able to interface with device drivers and bus addresses where writes and reads cannot be re-ordered or omitted. But Java does none of the kind and as such I wouldn't be terribly surprised if volatile has a more lax meaning in Java. What about ? Well it is really just volatile reads from in disguise to get the seed, the rest is deterministic which the compiler that compiled the JVM isn't allowed to reorder or remove, but what the JIT is allowed to do in this context is beyond me. It is conceivable that the JIT can deduce that the read from will not affect the program in an observable way and it will remove the code all together. If the volatile keyword doesn't have the same meaning in Java as in C++ I believe that doing this kind of processing in Java in a reliable and guaranteed way is difficult. I would consider a hand written loop in assembler in a JNI library, this is guaranteed by your C/C++/assembler to not be fudged in any way. I'm sorry this isn't an authoritative yes/no answer on correctness, but I hope that it is of some help any way.