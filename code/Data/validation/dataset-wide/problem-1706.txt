Mirroring data will have some impact, but that impact may already exit. Writing the log files incurs a performance penalty to ensure database integrity and recoverability. Hot backups populated from log files have been used for a long time. There will be a small load to transfer the file to the remote site. Depending on the log file size and count you might get some blocking under high load. There will be latency on the updates. This may be significant depending on when the and how the the log data is copied. In case of disaster you will likely loose data from the latency period. 

In your example, email destined for is deliverer either or . Which ever MX receives the mail forwards it to when possible (usually immediately). stores the mail until the user's UA connects and reads the mail. Depending on the setting email is deleted when read. Neither nor needs to be up when email is read. However, as is the MDA and has the email to be read, it must be up when you want to read email. The user will read only email which has been delivered to and will not be fetching or gathering email from or . 

Try closing your browser and then trying to connect. You may have cached credentials that are being used. If you have multiple secured directories with different passwords, use different values for the . EDIT: Try moving the Require outside the Limit statement. I always group the Auth definitions with the Require statement in the same block. Your error seems to indicate the requested resource does not exist or is not readable. Try removing your rewrite modifications for . The standard rewrite rules work well with directories and files mixed into the Wordpress installation. You may want to use a LimitExcept block instead of a Limit block to prevent access other than GET or POST. This is my working .htaccess file. 

It looks like an incoming request rather than an outgoing request. It is unlikely the server would create an outgoing connection from port 25. As the comments note, the FIN packet just indicates the connection is being closed. It is possible the other server sent a message, or tried to send a message and did not wait for the response before closing the connection. If the delay is long enough on your side, then the close can look to the router like a new conversation with invalid flags. Your mail logs may show an incoming connection a few minutes before the times in your firewall logs. 

It appears you want to provide access to a userid which acts as a dropbox for files. You can restrict what the users can do in the authorized keys file. As you only want them to connect to this user id on the server, you can provide them with a .ssh/config file which maps them directly to the correct userid. 

The simplest solution is to list all three IP addresses in DNS. The DNS servers will rotate the IP addresses as they resolve the name. This will give rough load balancing. Otherwise, you need some sort of load balancer in front of the servers. These may give better load balancing than DNS. Load balancing with sessions is more difficult, and there are a number of solutions. 

Spamhaus will block your site if it is listed as dynamic address. If you are running an Internet connected email server you must have a static address if you want to be trusted as not sending Spam. Check your IP address at spamhaus.org. You will get the reason you are blocked, and what can be done to correct it. I use Spamhaus as my primary block list as it is highly effective at blocking spambots which are usually on a dynamic address. EDIT: As for security, you need to ensure that Incoming email from the Internet is only accepted if it is for your domain or domains you relay to. If you have mobile users connecting from the Internet force them to use TLS on the Submission port with authentication. Also follow the recommendations from rihatum. 

The effect is that your SPF record is ignored. Please do the following. Your SPF record requires 6 lookups excluding any MX and A records included in the record. (An MX record lookup may result in 2 or more lookups.) 

If you are running virtual hosts on the the server you may want to pin the CPU(s) for the host. You may want to do the same for high CPU single threaded processes. Spread the load across CPUs if you do so. 

If your software follows the RFCs no delay is required. You should be able to send multiple emails in the same connection without delays. If your software just sends messages and relies on delays rather than following the RFCs, there is no correct answer. EDIT: If you read the RFCs you will see mail is sent using a conversational protocol. If your software follows that protocol, there is no need to insert delays between messages. Some software just uses delays where it should be following the conversation. Software that does that is very fragile and may break if the other end does not respond within a reasonable time. Adding delays between messages when you send a bunch may or may not be a good thing. Delays give the server time to process the last message before a new message arrives. Not having a delay increases the chance that multiple messages can be sent upstream with one connection. Unless you overload your mail server I would opt for no delay. 

EDIT: To test the redirection you will need to accept the certificate error. You should then end up on the desired site. This is what your clients will see until you get Google to reindex your site. Logon to the Google Webmaster Tools site and resubmit your sitemap. This should result in your site being crawled fairly soon. I don't know how fast the links on the search pages will be updated. If you haven't already registered your site, you will need to add specified content on the site to prove you have control of the site. There is information on setting the preferred site name as well. 

It appears you have failed to configure your dependencies correctly. The services should depend on the host. There is also an inherits dependencies flag which may make dependency configuration easier. It is common for one service to depend on another, often on a different host. These dependencies need to added. Hosts may be unreachable due to failure of intervening routers/host/firewalls. These should be added to the configuration. Only the last hop dependencies need to be specified. However, you need to specify all dependencies. Nagios will respond appropriately to hosts/services that are unreachable/failing due to network/server/service outages.. It appears your service was CRITICAL before the HOST was discovered as down. This won't clear until the host recovers. You may be able to change its state by re-configuring it. The CRITICAL state is usually correct when the service's host is down. Use scheduled outage when the HOST is expected to be out. 

Are they using a proxy service on the firewall? It may be possible that facebook related pages containing session cookies are getting cached. Depending on the order pages are loaded it could replace the session cookie with someone else's cookie. It should be less of a problem with connections which shouldn't be cached. 

Depending on the configuration, LDAP does not require a shadow database. There are two common options: 

The easiest way would to be to copy the configuration from the master. A short perl script should be able to replace the master keyword with slave and add the pointer to the master. As Christopher noted, you will also need to modify the configuration of the master to allow zone transfers from the master. I also noticed a security notice for bind which indicates that you should ensure all your zones have an allow-query clause. 

I dump ufw and use Shorewall instead. It is installable as a package. The /usr/share/doc/shorewall/examples directory has good starting configurations. It is will documented both on the Shoreline Firewall site and in the shorewall-doc packagage. 

When you are setting up Ubuntu the DNS servers that are being asked for are the upstream nameservers. These should be the IP addresses of the nameservers provided by your ISP. You may want to use Google's nameservers, or those provided by OpenDNS. If you have a home router, you can usually use its IP address as a nameserver. Hosts that use DHCP to get their IP address usually get their DNS nameservers that way. All domains have their own nameservers which host the names for that domain. Unless you understand DNS quite well, it is best to use the nameservers provided by your registrar for this purpose. In your case with Hover, you want to use Hover's DNS servers. Add an A record for your domain with the external IP address of your router. Your router will then handle routing connections to the server using your port forwarding. 

Nagios has a plugin for NTP. It will connect to the NTP server on the target system, and compare its time to the local time. You can set warning and critical levels for the time difference. 

You could use a single name for mail transfer on both interfaces. The two PTR records would return the same name of your MX in this case. 

Ubuntu allow desktop sharing, and I expect most distributions now have it. This can be invitation based or password based. In your case I expect you will want password based. Connection is via a VNC client. There is a browser based client that can be used when you don't have a client on the system you are connecting from. 

If you put a short sleep in the writer loop, you should see this much less frequently. You can avoid this by using an atomic action to create the file such as: 

Configure both LBs to listen on the same public IP address. The passive LB will watch for the active LB to fail and start handling requests when the active LB goes down. Points to remember: 

Verify that that you are using the transport. This should be listed after in on the lines containing . Checking DKIM on locally delivered email will not work as this transport is not used. Use a verification service to check whether your mail is signed. Verify the permissions on your private key. It must be readable by the use Exim runs as which Debian-exim for Debian and Ubuntu installations. If your transport is to , it will requeue messages if it cannot sign the message. It will log the failure causes to the and the . It may be easier to find the message in the . These are the setting that are required to get DKIM working. You seem to be missing some. (I sign for multiple domains with the same key. Try getting signing with a single key working before trying to get fancy and use seperate keys for different domains.) This configuration should prevent unsigned email from being sent by the transport. 

Your model opens up a raft of routing and configuration issues. Having distinct DNS entries for each service all resolving to the same IP address would be much more workable approach. It is common for DNS names like smtp.example.com, ftp.example.com, ssh.example.com, and others all returning the same IP address. These services all use distinct ports which allow them to run on the same interface. can firewall by service, and if need be destination address. The addresses can be on the same interface or different interfaces. I haven't found a solution that would allow DHCP to provide multiple addresses for one interface. 

The place to look is your DNS data. Your DNS administrator(s) should be able to to tell you which A records exist for the the address. As noted above you also need to get them to find all the CNAME records which point to one of these CNAME records. If you know all the domains your company owns, you may be able fetch the data using zone transfers. 

When connecting to clients networks, you should firewall their networks. At a minimum you will need to ensure their networks don't interconnect. This applies even if their addresses don't overlap. Routing rules will need to be across your network. IPv6 addressing avoids this problem. Each device which can connect off of the LAN will have a unique Internet address. IPv6 private network addresses have been deprecated. NAT is not required except to connect to IPv4 addresses. Routing is much simpler. 

You can change and just use and to configure and unconfigure the addresses. Using the command is a more robust restart. If you have a separate management port or are working from the console you can skip the in the above command. Using on the primary configuration (eth0) should only be done if you are not connected over that interface. 

Native domain keys support is available in Exim since version 4.70. See my notes on Implementing DKIM with Exim If you want to use a filter you may want to review the Exim Documentation for filters. It includes instructions on how to test your filters. 

Normally, all traffic will be routed over the VPN. You are being assigned a point to point adddress. It should have a route to another address on the remote network side. All traffic other that that required by the VPN connection itself should be routed over this address. The private addresses (such as your DC) that can be accessed will depend on the configuration of the network you are connected to via your VPN connection. This will depend on the routing and firewall configuration on that end.