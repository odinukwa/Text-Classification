ANTLRWorks is an editor for ANTLR grammars. ANTLR allows you to write BNF context free grammars (which also produces code). ANTLRWorks allows you to try examples strings on the grammar and displays the parsing. As to natural language, if you have a BNF grammar for one (I think there is a non-trivial but still non-inclusive grammar for English in the javacc parser code), then you can use it with some changes with ANTLR. 

To add to all the other excellent answers, there is a simple practical explanation of the proximal cause (the others explain well the academic reasons). Checking the sources of the two sets, there is a great amount of overlap. Both Etymonline and Wiktionary list as sources Calvert Watkins, The American Heritage Dictionary of Indo-European roots, Carl Darling Buck, A dictionary of selected synonyms in the principal Indo-European languages, and Julius Pokorny, Indogermanisches Etymologisches WÃ¶rterbuch. These three all use the Pokorny style of writing (as others have mentioned describing a more recent PIE. But Etymonline does not and Wiktionary does list Rix et al., Lexikon der Indogermanischen Verben (1998), which uses a more modern transcription, directed at an older PIE. And on the whole Wiktionary seems to use the latter Rix transcription. 

There are many factors, time spent in each language, consistency and exposure to each (at school, at home, at work, with friends/family). There seems to be only one change that makes it considerably hard to change ones accent and that is puberty. Before puberty it is very easy to get and maintain a native speaker accent in a second language, but afterwards it takes lots of time and training (with study and practice, grammar and vocabulary don't seem to be a problem after puberty). There is no biological mechanism yet discovered that comes close to explaining this. If you spoke a language fluently as a child, even if you stop using and hearing it before puberty, you shouldn't have much problem with re-establishing fluency later in life and getting rid of any accent from the in-between language. If you entirely quit the language at 5 years old, because that is so young, you may have some difficulty, but it won't be like entirely learning a new language sound-wise. One anecdote that shows this is Henry Kissinger, a former US Secretary of State. He moved with his family to the US from Germany at age 15. He has a very noticeable German accent. His brother, Walter Kissinger, is two years younger and has virtually no accent in English. 

The standard rule in English is if the subject has more than one entity then it is plural and the agreeing verb should be plural. Often in practice, the rule is ignored and the verb agrees in plurality with the most recent noun, presumably out of performance issues. My question is, isn't this a language universal (for those languages with subject-verb number agreement)? That is, it seems like a logical (or rather cognitive) application of a rule rather than a purely grammatical one: the speaker is judging the plurality of the subject based on a cognitive assessment. 

There are two related theories of lexical semantics that seem amenable to this kind of thinking. One is feature semantics, where a word can be assigned a number of positive or negative features (by analogy with phonemes having phonological features). A word's semantics is defined by a selection of binary features so 'man' might be [+human][-female] and 'table' might be [+furniture][+flat top][+has legs]. If an item doesn't have a positive or negative version of a feature, then that feature is just not relevant. Another model is 'ontology'. This is not the philosophical ontology (metaphysics of being) but rather the specification of semantics by relation to other entities. You can think of any concept as a node and how it is related to other concepts by labeled arrows to other nodes. (you can implement feature semantics in an ontology by having all male things point to a 'male' concept) You ask if there is a finite set of things here from which the seemingly infinite set of all words or concepts can be derived. Mathematically, yes, this could be done. With a set of 100 features, there are 2^100 (way more than the number of physical atoms in the universe). However I don't think this is possible usefully. Real life has so many possibilities to it, new things appear all the time. There might be a finite number of arrow labels ('is-a', 'has-a', 'is similar to', etc), but it seems reasonable to think that there are no terminal nodes (no outgoing arrows); there could easily be cycles. Just to mention quite an opposite direction, axiomatics in mathematical logic depends on treating axioms as the atoms for which there is no proof. All proofs bottom out at these axioms. Whether you consider them syntactic or semantic atoms is another discussion. 

That's it. In fact the vast majority of linguistically produced sequences are so arbitrary and so distant from imitation, that there is no specialword for the larger set of non-onomatopoeic words. And there is no real 'in-between' stage of 'somewhat sounding like the thing it stands for, but not quite', because onomatopoeia includes that too. For example, in a bee buzzing, have you ever heard a voiced bilabial stop? That is 'in between' but is still considered onomatopoeia. 

To make an analogy, spoken language is like a hand and written language is like a hammer. We are born with the capability to speak/hear (specific anatomical structures in the ear, mouth/throat/lungs, and brain) and we spontaneously learn spoken language by its presence. But writing systems are artificial, man-made contrivances, a coding system to record the spoken language, a tool that attempts to extend the usefulness of spoken language. One naturally learns language growing up around people, but one needs instruction in reading and writing to do it. People communicated for thousands of years quite well without writing. The rules of writing are totally dependent on those of language that came before it. To understand the arbitrariness of writing consider that Mandarin is normally expressed in logograms (the complicated picture-like collection of strokes for one syllable), but can also be expressed in pinyin, a romanization, roughly one letter per phoneme. Similarly, Persian is normally written in an Arabic script (leaving out most vowels) where Arabic is a very different structured language, but before the Arabic invasion was written in cuneiform, a syllabic system, but can be transcribed to a roman lettering. All these forms are with no change in meaning. The point is that writing systems are arbitrary inventions. Writing is merely an attempt at encoding natural language (the non-written, primary, spoken thing). So, studying writing is studying the arbitrary man-made patterns of orthography, and these structures are very different from (or, if not, depend entirely on) the patterns of spoken language. Studying writing (an interesting topic) is like studying a hammer, a very different thing than studying the hand that uses it. 

I won't answer this directly but rather give resources that I think can answer it. The WALS (World Atlas of Language Structures) has three chapters and accompanying maps on inflection types, 20, 21, and 22. They have a broad inventory of languages. They do not use the same terminology as the classes agglutinative, isolating, etc. but do start from analytic and synthetic and refine it from there. But from those maps you can tell frequency almost directly; they give the number of languages in their inventory with each particular feature, with all the limitations of such an inventory (selection bias, essentialism (lack of vagueness), your favorite may not be in there, etc). You'll notice there there is no single map for SVO permutations either. 

I think you are using those terms a little loosely, and when you pin them down to the usual ideas they represent, that doesn't exactly make sense. But it does give a vague idea of what you're looking for. A language is a communication device and, especially here on linguistics.SE, one between humans to communicate ideas. Then there are recording devices, like writing or codes, that map one thing into another which has certain properties, such as permanence and replicability, like writing, or efficiency and organization, like codes (Morse code, ASCII code, ). Human languages have many possible recording technologies, one of which is writing. Usually these are linear codes that capture the pronunciation or meaning step by step over the course of an utterance. Human language communicate many concepts including numbers. If you had a writing system, you could possibly have writing elements that capture numbers, a small set of the concepts a human language wants to get across, but most of the elements of writing would probably represent better most other non-numerical concepts (like 'dog' or 'truth'). There are many writing systems for human languages and the ways they represent numbers is sometimes by having separate symbols for the numbers (that's the modern way), but in the past it is often by using existing symbols for other words or letters. But there are no natural writing systems that have symbols for numbers that are reused to represent phonology or concepts. For example, Ancient Greek had no separate set of characters to represent numerals by themselves. It used the pronunciation letters, in its alphabetic ordering, to represent numerals as a secondary use. That is, alpha represented the digit that is written '1' in English, beta for '2', ... iota for '10' and so on. So the basic Greek alphabet, used normally for pronunciation guide, could also be used for accounting/arithmetic. But there is no natural writing system that starts off with numbers and these get reused for the rest of the natural language. 

Even though nominally, statistical MT is distinct from syntactical MT, at its simplest, statistical MT deals with word order (syntax by another name), by n-grams, collecting statistics on pairs of words (in order), triples, etc., with diminishing returns for effort the longer the sequences. For example, 'of the' is a very common 2 word sequence, but 'the of' is (almost?) nonexistent, so if ever 'of' is translated to, then the word before it is not going to be 'the' and afterword a good chance of being 'the'. Of course, part of speech tagging (using a plain lookup with some sequential context), can then be used with very short range order constraints. 

On the other hand one analogy that does supply change is the concept 'mutation', which for biological speciation comes from molecular transcription errors in DNA; in language it comes from aural distortions, production errors, and mental processing mistakes. That all said, the major differences are that the mechanisms of change are very different (biology is about survival and reproduction, language is about either straightforward resolving noisy channels or sociological acceptance). 

As an aside, sign language needs to be accounted for (only because it is nominally not spoken), and that account (by analysis and experiment) seems to be that it has the same properties as spoken language in contrast to written ones. For reference, see the preface/first chapter of any introductory linguistics textbook. There is the speech and writing section of wikipedia, which supports what I say, but you should seen actual authority to be convinced if you need a trustable reference. 

because a PB&J is cognitively a single entity. I am wondering how language specific the pattern is or if most/all languages with subject/verb number agreement follow this same 'cognitive' pattern. 

Within each of these dialects, there is a continuum of intelligibility, but from the extremes may not be intelligible (the wiki page gives many examples of unintelligibility within a dialect). To say the differences are as much as the differences within say Romance is difficult. But it is often repeated that the Chinese varieties are as far apart as the Romance languages. What I think that means is that if you know one of the Chinese varieties, learning any of the other varieties is just as easy as say a French person learning Italian. Some notes about your question: 

This is a funny question because of 'machine learning' (ML), 'still', and 'better'. Presumably you mean 'machine learning methods in NLP' (Natural Language Processing), because I'm having a hard time thinking of linguistic theory that informs ML uses outside of NLP. 'Still' implies it has a 'better' one now, and 'better' implies a current one is not sufficient. That is a bit too motivated in one very particular direction, so I will just answer simply just how linguistic theory and NLP implementation are involved together. The latest popular and successful methods for things like [Seq2Seq] translation and chat ($URL$ models, such as RNN or LSTM, use barely any linguistic knowledge at all (little more than "here's a sequence of characters that might be whitespace-separated 'words'"). No parsing/phrase structure grammars, POS (parts of speech), anaphora resolution. Some NLP methods may use these linguistic ideas but they are almost entirely avoided in methods that are ML based. Historically, there was an attempt to use syntactic parsers, and those did well-enough, but the these latest statistical methods have been much more accurate. In the narrower field of speech-to-text (a common stage before NLU), some phonological theory is used, but its mostly years of incremental engineering that have produced the high quality you have today. There's a famous quip by Frederick Jelinek about speech processing: 

Forgetting all the research in to the Sapir-Whorf hypothesis, the answer is both obviously yes and obviously no, all depending on how you interpret 'language', 'shape' and 'thought'. I have a hard time thinking about building supplies because I don't have the vocabulary to express all the intricacies of hardware. So obviously yes, the hardware language allows a new way of thinking. (but then what is causing what here: I can have thoughts about things without the vocabulary items, the latter just make it easier and quicker, and beside which came first, the words or the thoughts). And on the other hand obviously no, language doesn't force thought, one can translate (once experienced from one language to another, one can be bilingual and converse the same thoughts in two different languages. But, it may take one longer in one language than another, or the experiences and cultured learned with one language may not have corresponding things to translate to in the other. But to balance these out properly, I think one can say (without cynicism) that the Sapir-Whorf hypothesis is superficially true: when you first visit a new language it always feels like you have to -think-differently to use it properly, and that that is a reflection of and caused by the language itself. But with experience, one can translate from one to the other. (and then there is all the experimental evidence that strong Sapir-Whorf is unfounded and even weak Sapir-Whorf has problems). 

Language is arbitrary as you say but it is also not created by authority. There's no one person or committee saying: "Let's start a new language, and we'll call all those four legged things we ride 'horse'" (except for the wildly out of the ordinary constructed languages like Esperanto). There are all sorts of theories about the true, way-back-when, something-out-of-nothing origins of words and syntax, but they're all very speculative and quasi-scientific (no one is there to record the early language speakers). Looking at the behavior of other communicating animals (dolphins/whales, birds) gives ideas, but tends to the "Gosh we think it could have been like this, maybe." A grunt here, a sigh there, eventually people, out of arbitrary association with other activities, modified and distinguished these sounds, not through any dedicated process or choice, created words associated with meaning. Well, that's a theory with little support other than plausible analogy.