I am unaware if these changes would also result in stronger or stranger weather patterns, but I wouldn't be surprised. 

Parting from the attempts at pessimism, I would suggest that a second neural mass could easily evolve to handle sensory input from a second (or more) location for some perfectly reasonable environmental factor. At first thought, I would suggest a creature that walks long, narrow tunnels. A second predatory species (or multiple) might hunt that species in those tunnels. The prey might easily develop sensory organs for the "front" and "back", and "brains" to handle both. Given the right circumstances, these brains might develop towards higher cognitive functioning simultaneously, eventually resulting in sentience. That, however, is just an example. Any environment requiring multiple distinct sensory locations could result in similar evolutionary tracks. Multiple locations needing sensory organs -> favoring neural masses to handle reaction speed -> raising of complexity of those neural masses. The end result depends heavily on the environment. In the above example, the physical environment and nature of the predators. They could be very similar, or one could be specialized for one task or methodology while another for another task or methodology. In the above example of the push me pull you-esque creature, maybe the front brain evolved for hunting food and results in a sentience good at engineering, math, the arts, etc. The back brain might be the sentry brain, and have evolved for predator awareness resulting in a sentience good at "fighting" or athletics or vision and spatial-awareness. As a note, your own octopus example is a good reference for this concept, and was the first example of which I thought when I read the question title. You say that the octopus' secondary neural areas aren't brains, but that doesn't mean they wouldn't evolve further if the need was present. A pressing evolutionary need to advance those neural masses in the arms, and the octopus might one day have a sentient head and sentient arms. Sentience is simply the evolution of incredible neural complexity. 

Space is dangerous A space station is basically a really, really tiny planet. So imagine you lived on an asteroid big enough to house a few thousand people. What would you need to survive? What would you do if something went wrong? Now, compare that to what would happen if you lived on an earth-sized planet and something went wrong. Flying rocks One of the biggest dangers in space is flying rocks. A tiny one can put a good sized hole in just about anything you can manufacture. Most machines that humans build don't work well with holes added to them willy-nilly. A space habitat needs passive or active micrometeorite defenses. If the active system fails, it's only a matter of time before the passive system (i.e., really thick hull) fails and you have a Very Bad Day(TM). A planet, on the other hand, only needs a passive defense, and it's pretty much always available: it's called "an atmosphere". If you lose that, you've screwed up something really big, and you have other problems to worry about. Cosmic rays Radiation is everywhere. If you're close to a star, the solar wind will bombard you with all kinds of high-energy particles that you really don't want going through your body. Again, you can have an active defense like a magnetic shield, or passive defense like a massive lead hull (very expensive). A habitable planet has two excellent passive defenses: that good ol' atmosphere, and a magnetosphere (well, not all planets have one, but you should modify it if it doesn't). Ecosystem A space hab must have a closed ecosystem. But the stability of an ecosystem is a function of its size. Trying to make a closed ecosystem on the ISS would be insane, and probably impossible. Even if it worked, it would undoubtedly operate on a razor-thin margin. A giant hab with thousands of people would be better, but if some disease started ripping through your food supply, how long would the inhabitants last? What if resupply ships were too far away? Very easy to make the Donner Pass in Space. An ecosystem covering millions of square miles will be thousands of times more stable than even a very large space hab. It would be able to host far more diversity in the food chain than is necessary to have a reliable supply. A space hab might be able to grow enough food to feed humans, but it is unlikely that it could grow an enjoyable diversity of food. The space available on even a small planet would explode the farming/animal husbandry options. Manufacturing How do you get new goods on a space hab? You can recycle, you can mine nearby asteroids, or you can import raw or manufactured goods from planets. If there is any kind of disruption in the flow of materials or goods, the space hab could be crippled for new manufacturing (if it even has a meaningful manufacturing capability). This is especially relevant for warfare. Imagine trying to build fleet from space habs. If I were your enemy, I would know that all I need to do is disrupt your supply lines, and thereby cut off your ship-building capability entirely. I could destroy your navy before it is even built! A planet can not only provide the raw materials for a space navy, it can provide a secure staging area for defensive weapons if you choose to attack my shipyards. I can build massive power generators in remote parts of my planet without putting my population at risk. If you build powerful generators near your space hab, they either need to be close enough to endanger your population, or far enough that I can attack them without dealing with your hab defenses. That limits the kinds of defensive weapons you can field. Defense In the event of a full-scale war, I would much rather use the natural deterrents of atmosphere and land to provide shelter than having to build every last shield in space. If my population relies on transported raw materials, my supply lines are a constant vulnerability. You could attack continuously, wearing down whatever defenses I have, until I run out of materials to rebuild. You have your choice of beam weapons, missiles, and kinetic kill vehicles to attack my space hab, which is presumably not very mobile, making it a sitting duck. And because volume is expensive in space (and makes me even more vulnerable), my people are crammed into a small area that is extremely trivial to target. Having to defend a space hab is a terrible military position. On a planet, I can disperse my population, making any concentrated attack a waste of resources. I can tunnel underground, forcing you to either waste energy blasting through the surface, or landing for an invasion. Beam weapons will be attenuated by the atmosphere/magnetosphere for certain frequencies and beam types. Missiles will have to contend with potential counter-measures from all over the planet's surface, or in any number of orbits. Kinetic bombardment is made more favorable by the planet's gravity well, and may be the best attack. But if you can launch something big enough to kill my planet, then you can surely blow up any space hab you like with the same rock. If you are forced to invade, then I will have the advantage of native soil. Conclusion If I'm the leader of a space-faring civilization, I will choose to colonize planets over space habs every day of the week and twice on Sundays. I would only build habs in remote strategic locations where I want a presence but no suitable rock exists. Habs might be a nice vacation spot if you want exotic properties (controllable gravity, day/night, etc.), but not a sound basis for a large civilization. It is risky, costly, and foolish. 

There are two major differences between a moon and a planet: 1) Distance to the sun rapidly changing If we take the largest moons in our solar system as an example, a decent sized moon would have an orbital radius of 1-2 million kilometers. On one side of the planet to the other, in a matter of about 1-2 weeks, we could be talking about a moon being 4 million km closer to the sun than it was a week ago. The differential between the Earth's closest and furthest points from the sun is about 5 million km. The thing is that the seasons are not based on distance from the sun, so I'm entirely unsure what effect this would produce. I imagine, at the very least, the weather would be slightly more chaotic than on Earth. With an axial tilt, you might end up with something akin to mini-seasons, though the effect might be very slight. 2) Much more powerful gravitational forces If we again take our system's largest moons as an example, we would have a habitable moon with, at the very least, a gas giant affecting it instead of our own Moon. That's almost 26,000 moons worth of mass, which is equal, given the masses and distances of Jupiter and Callisto or Titan and Saturn, 17-19 times the gravitational exertion between the moon and gas giant than our planet and moon have over each other. Thus, if we're talking about a truly habitable world that's fairly earth-like, the tidal forces we have on our planet would pale in comparison to the tidal forces a Callisto- or Titan-like moon would feel under the shadow of a Jupiter-like planet. This would result in possibilities like 

The other teeth would likely spread out to fill or partially fill the gap. It all depends on if the other teeth already have enough room in the jaw. As for loosing the tusks completely, that would be more dependent on chance. Ordinarily, if an Ork was born with the defect of no tusks, they would have a hard time finding a mate and be removed from the gene pool. If they have their tusks removed at a young age, then those with the tuskless "defect" may be marked so at birth and thus grow up having a higher status. Otherwise, they will look just like any other tuskless ork. In the latter case, the defect won't be selected out but it won't have selection preference either. 

If the enemy has any kind of force field / energy shield then lasers are a very good weapon. Any shield that they can see through allows you to shoot them. The only effective defense against lasers (in my current understanding) would be a gravity based "force field" that would deflect the beam through lensing. Though you would have to figure out how to keep from crushing the protected asset while the gravity field is being generated. Short term defenses would be a reflective aerosol that might scatter enough of the beam (before the tiny reflectors are vaporized) to reduce damage. 

Hopfield Networks Your best bet is to look at Hopfield networks. They are auto-associative artificial neural networks (ANNs) which model some aspects of human memory. That is, the behavior of Hopfield networks under certain learning rules is similar to the performance of certain human memory (the kind that would end up in the frontal cortex as opposed to, say, the cerebellum). Like human memory, Hopfield networks do not exhibit "perfect" recall for all input vectors. However, the recall will be strongest when the input is closest to the recall vector. Sometimes, a Hopfield network will converge to the "opposite" of the intended recall vector (something which doesn't seem to happen to humans, possibly because of additional filtering circuitry rejecting such recalls...or perhaps, this does happen, but manifests itself in subtle ways). The size of the vector stored in a Hopfield net is equal to the number of nodes in the network. That is, the nodes themselves represent both the inputs to the memory and the manifested outputs. This may or may not be similar to the way parts of the human brain are organized. If it is indeed similar, then we can derive a few bits of performance data. Performance The number of "storable" vectors in a Hopfield network appears to be about 14%. So let's say you have a network which stores the names of people and places you know. In order to reliably store 1000 names, you would need about 7000 neurons. Pretty good, huh? Now imagine you need to store their faces. Uh-oh. The human retina has more than 3 million photoreceptors, which is about 1 million pixels, accounting for color. If you tried to store images at full resolution, then, you would need a network of 3 million neurons. Then you could store about 400k distinct images (assuming the vectors are sufficiently different). However, it is very likely that visual memories are not stored in the brain in this way, due to the fact that the higher brain only has access to the output of the visual cortex, which does a tremendous amount of pre-processing (edge and segment detection along multiple angles, movement detection, etc.). So how many neurons do we have to work with? The raw total for humans is about 86b. But much of the brain is control circuitry (for operating your automatic functions like heart rate, digestion, etc., skeletal muscles, other organs). If we limit ourselves to the cerebral cortex, which is where "higher order memories" are most likely to be stored, we are looking at about 21b neurons, or about a quarter of the total. Definitions At this point, it could be tempting to work backwards, and say that the human brain could then store about 3 billion memories. But remember that a Hopfield network stores vectors which are the length of the number of nodes in the network, so these "memories" would be ~3 GB each!!! And we know by the wiring of the cerebral cortex that it cannot possibly be a single Hopfield network. A Hopfield network is also fully connected (every node connects to every other), and the cortex is highly layered. On average, human neurons connect to about 7000 neighbors. Thus, if the brain contains any Hopfield networks at all, they are likely to be fairly small. In the limiting case where the entire cortex is composed of Hopfield nets (not plausible), we would have ~3 million networks which can each store about 1000 vectors of ~1 KB each. Although that is still 3 billion total vectors, we now have the challenge of mapping these vectors onto memories. We must thus ask: "What is a memory?" The fact that Americans celebrate Thanksgiving day in November might be considered a memory. And the fact that they tend to travel to be with family might also be considered a memory. But what about last Thanksgiving? Is the smell of turkey a memory? How about the smell of turkey + sweet potatoes + apple pie? Are those distinct memories or pieces of the same memory? Is the football game part of the "last Thanksgiving" memory, or its own memory? The fact that memories are inherently fuzzy does't help matters any. They do not have crisp boundaries, and they can be hierarchical. Can you remember which part of the bird you ate? Light meat or dark? Which family members were present? What they said? How many of these questions are answered because you stored discrete facts vs. recalled an image and queried it? For these reasons, the very question of memory storage density is ill-posed. But if we agree that there is some smallest unit of recall for the human brain, then this would almost surely correspond to a vector in a Hopfield-like network. And as you can see above, the upper bound for those units is about 3 billion, for the average human brain. It may be that images require many such vectors to store, and that they are always stored with many vectors, making the total number of distinct "memories" much smaller. And some folks may object that humans can "only" store 3 billion distinct items in their head. So let me address that briefly. Implicit Knowledge Do you play tennis? How about ping pong? Can you make a complete novice into a good player using just words? No. At the very least, the novice must actually "go through the motions". The programming doesn't happen in the ears. It happens in the cerebellum. And while some folks will think of bodily-kinesthetic programming as a functional wiring of motor control circuits, there is definitely a memory capability involved. Someone who has learned to play tennis will have a leg up learning ping pong, and vice versa. That is partly because the control circuits will be wired to make similar motions, but also because the players will have memories of specific trajectories and responses which are activated in particular circumstances. When new players play against each other for the first time, they will often perform worse than they would against a familiar person of the same skill level. That is because skill is ultimately an ad-hoc covering of the state space for the game. If the other player spins the ball or attacks in a way that hasn't been seen before, your control circuits will not have a pre-made response, even if you were physically capable of one. The conscious brain is too slow to act decisively in competitive sports. Even if you know the appropriate response cognitively, if the cerebellum has not executed the program which covers that part of the state space (including your relative position, balance, momentum, etc.), you will likely fail to produce an adequate response. Much of competitive sports boils down to remembering the best move for a particular state. Low-level programs control fine details like which muscle fibers to activate, but high-level programs like "move right while swinging across the body" must be practiced to store to memory so that it can be activated automatically in the right circumstances. These kinds of memories generally cannot be named, and would not be thought of as discrete. They are implicit in the programming which constitutes "athletic skill". In the same way, verbal behavior can also be implicit. For instance, most English speakers can finish the phrase: "See you ____." They will generally not say "catfish" or "pulverize" or "flavorful". The fact that most speakers will finish the phrase in the same or similar way means that this bit of behavior has more to do with the mechanics of language than the personal memories of the speaker. Thus, this information is likely not stored in the pre-frontal cortex (because we know that language facility is primarily handled by Broca's and Wernick's areas). In the same way, athletic "memory" is most likely stored in the cerebellum. I presume that you mean to exclude these kinds of "implicit memory" in your calculations. If so, then limiting the analysis to the PFC is appropriate. Otherwise, you will also need to consider the "functional" areas in the remaining portions of the brain, which is much more difficult, given that we don't have any really good models of how these work. Conclusion So, I would say that it's safe to assume that humans can "remember" much less than 3 billion distinct [personal] memories, and that the smallest chunks are on the order of 1 KB of information. That puts an upper bound of about 3 TB of information that makes you a unique history of a human. Sobering thought, huh? 

For one thing, we would spend more energy catching them than we would get from eating them. For another, we can't catch them faster than they can breed. We could possibly make a targeted virus that kill them off. There's nothing that can go wrong with that plan.... 

You would need another 2 solar masses since the minimum size for a neutron star to collapse into a black hole is about 3 solar masses (2.7 to 3.2 solar masses as far as I have seen). The density of the potato isn't the issue, just the mass. The main problem that I can see is that you would have to act fast. As the potatoes burn up, they will be "poisoning" the Sun with carbon. When the ratio gets high enough, the carbon will get in the way of too many hydrogen or helium collisions. The might lead to a premature death of the Sun. 

Technically, firmware is "hardware", so I would expect it to remain. I assume that "disappearing binaries" is more about the information being removed than a physical calamity which struck all information storage devices but narrowly missed anything which didn't look like running machine code. If indeed firmware remained, then I think we would have a good head start. We should be able to boot any system which can run off firmware, and at least have some primitive OS and text editing capability. The interesting problem is that if we have source code, but most of it is electronic, then it is mostly useless to us until we can restore the systems that are able to read it. Since there is so much potentially usable software lying around, it would make the most sense to reproduce the compilers which could rebuild the software. This means society would most likely not try to reinvent programming languages from scratch until we had recovered the ones we just lost. Since we have lost the use of the internet and digital information storage, our best bet is to target the best-documented languages for which we have books. Without a doubt, a C compiler will be the first and most important high-level tool in the rebuilding. Once you have that, progress can be made very quickly. You can then rebuild entire OSes, many software tools, and compilers for a lot of languages. There is a reason that this 40 year old language still tops the TIOBE list. It is the "English" of the programming world: awkward, annoying, ubiquitous and powerful. Since there are so many C/C++ experts in the world, once you have a system which can enter text and store bytes on disk, building a compiler should actually not be that hard. Most likely, a bunch of folks would be improving the "IDE" through raw assembly/machine code, and probably re-inventing it from scratch just to improve productivity. Many parts of a minimal OS would be brute-forced just to get this first C compiler up and running. But I'm pretty sure that getting the first self-hosting build of the compiler would be the moral equivalent of the starter on a giant engine finally firing up the flywheel so it can be self-sustaining. In fact, this process would most likely happen in many places all over the world. It's entirely possible that Russia or Eastern Europe produces the first working C compiler "post-catastrophe" due to the number of hackers/virus writers who have to understand low-level code. Although China has a lot of hackers, they tend to take higher-level pathways into systems. I would be surprised if they created an early C compiler from scratch (although, a big group of enterprising university students may accomplish this through sheer force of will). The US and Western European hackers would have the advantage of the most C books and reference manuals available to them, and in a language they easily understand. Now, if firmware is also zapped, things get much, much harder, along the lines of switch toggling as described by others. That is so depressing I can't even contemplate it. But I assume that the threads merge once you get to a basic console (keyboard, monitor, persistent store...whether disk, tape, flash, etc.). Although many languages have self-hosting compilers, most of the compilers could be rebuilt from scratch in C, and most of the original language designers could aid in this effort. I think overall, the rebuild would proceed much faster than people might imagine (from basic console to self-hosting C compiler in 6 months or less). In almost all cases, folks would probably decide that it's better to simply replace what was lost and regain the functionality than to run off the rails and redesign things. A redesign would occur if you also lost the source code. Perhaps information is retained in books, but if all electronic executables and source were lost, then I think we would see a significant redesign and shortcut to more advanced techniques. I think C would still be rebuilt from scratch, because of its status as a kind of lingua franca. And possibly Java and a few other major languages would be revived (though obviously from clean-room implementations). On the other hand, it would be much harder to restore Linux or Windows or OS X without any source code, from just books. Interestingly, we could take this opportunity to eliminate a lot of nagging flaws from the languages, tools, and operating systems. Perhaps we wouldn't get C exactly, but a kind of enhanced C99 with a lot of legacy cruft removed. On the one hand, it would be to everyone's benefit to simply implement an exact C99 compiler, so that people from around the world could exchange C sources as the digital world was being rebuilt. This would discourage innovation. And for this reason, Linux would most likely become the de facto OS of the new era, simply because many portions of it could be restored from books and knowledge locked away in certain high-level wizards. Probably proprietary software would simply fail to compete until the majority of functionality would be replaced. So the rebuild would most likely occur under a very open model, unless some countries noticed that they were progressing much faster than others, and could gain competitive advantage by closing off their progress from the rest of the world. At the end of the day, global commerce would force countries to re-establish international standards, so it is hard to say how long such walls could survive. Although many failed languages would simply not be reproduced (unless by their loving creators), the most popular languages would surely be revived because of the stored value of programmers with proficiency in those languages. The same is true of tools. However, it would take a long time to rebuild something like Microsoft Office or Adobe Photoshop, let alone Windows Server 2012. These tools may never exist again, and perhaps there would be a new arms race to reinvent each software category from scratch. Every technology with a published standard would out-compete proprietary alternatives with no public standard, simply because the standards would represent intellectual effort preserved in text that does not need to be redone. But the weakest standard technologies may be displaced by better alternatives simply because the weight of legacy has been lifted and is no longer such a great advantage to bad old solutions.