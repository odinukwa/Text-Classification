Well, here's a theory. Some Romance languages distinguish two sorts of attribution: essential versus accidental. In Portuguese, that's the difference between "estar" (accidental) and "ser" (essential). Since both orders of modifier-modified are permitted in French (as they are also, marginally, in English), the order has been specialized to express this difference: pre-nominal position for essential and post-nominal position for accidental properties. This is an old way of distinguishing prenominal from postnominal. It is discussed in the 17th c. Port Royal Logic, a.k.a. The Art of Thinking, Port Royal Logic, using an example that also works in English (which I use here, since I've only read the book in English translation). The "visible stars" refers to those stars which are permanently classified as being visible to the naked eye under good seeing conditions, an essential attribute, while the "stars visible" refers to those stars you can see on some occasion of interest, an accidental attribute. When it's cloudy, the stars visible are only a small proportion of the visible stars. 

I don't personally believe that CFL are insufficient, but among linguists who care about weak generative capacity (probably most don't care about the issue), the consensus seems to be that they are. The usual reference given is an article by Peter Shieber, which is online here, and which is, in my opinion, an admirable work of scholarship. (Which doesn't make it right, of course.) So far as I know, the case rests on the grammaticality of unbounded cross-serial dependencies in certain natural languages, see Cross-serial dependencies. Or, one might refer to them as "respectively constructions". A conceptual example would be "John smoked, ate, and drank cigars, caviar, and brandy, respectively", supposing that is interpreted as meaning "John smoked cigars, John ate caviar, and John drank brandy". The problem is getting the verbs to pair up with the objects that they go with. What seems to be a corresponding problem with CFL is generating "copy" languages, which have sentences with two consecutive copies of the same string, which is provably impossible in general. So far (you're probably thinking) this seems to be an imaginary problem, since the English example I gave above is not actually grammatical. If by proposing CFL for natural languages, we predict that natural languages cannot have such constructions, that would seem to be a good thing. The problem is that some other languages do seem to have such constructions. Shieber described a Swiss-German dialect, and others have since been found. So even if it happens that English doesn't have cross-serial constructions, since we're interested in human language in general, we have to give up on CFL as a general model. I don't, myself, think this argument has any force, since the only real prediction that CFL makes is that the depth of cross-serial constructions must be bounded and that the greater the depth of the cross-serial construction the more complex the CFL must be to emulate it. That is the situation in Swiss-German, in Shieber's account. Two-depth CS constructions are accepted by all, three-depth ones by some, but four-depth ones are not accepted by anyone. So, in my view, the facts actually support the theory that natural languages are context free, rather than showing that they are not. On your last question, yes, quite a few people believe that natural language is r.e., recursively enumerable, i.e., since all the languages in the Chomsky hierarchy are r.e. One of the dissidents, I believe, would be C. F. Hockett, who proposed in The State of the Art, reviewed here by Chuck Fillmore, that human languages do not have definite systems of the sort that Chomsky assumes. 

I doubt that it is helpful to try comparing the "naturalness" of single sounds. But you you might be able to compare the naturalness of the sound systems of the protolanguage or the naturalness of sound changes, under various hypotheses of what the protolanguage was like. But you're not telling us anything else about the vowel systems of any of the languages involved. You could try to find instances elsewhere of context free vowel changes [a] > [e] versus [e] > [a]. The latter change applied in the development of Indo-Iranian from PIE. 

I believe that the lambda calculus is, as you say, a notation system for logic, and for other mathematics. Linguists need to be specially concerned with notation systems for logic, because natural languages are also notation systems for logic, inasmuch as we generally carry out our logical reasoning in a natural language. Thus we are drawn to the study of logical notation. Nouns and pronouns in natural languages correspond to variables in predicate logic, but some human languages have fewer nouns than others, and there might even be some human languages without nouns. That is a reason to be especially interested in logic systems that can do without variables, like the lambda calculus or combinatory logic. Here is a background article from Quine on eliminating variables from logical and other formal expressions: Variables Explained Away. It's interesting that he finds himself inventing operators that look a lot like reflexive and passive constructions in natural languages. 

where I've written the highest pitch as "1", next highest as "2", and so on. If a period of silence after "sorry" were simply an inserted hiatus, not affecting the syntactic structure, it ought not to affect the intonation: 

I believe that Chomsky and Halle's SPE theory predicts progressive assimilation of voice. For English morphologically simple forms, the only way you can have a weak word initial syllable followed immediately by a stressed syllable (as in "Moˌnonga'hela") is when the initial syllable has a secondary stress (as in "ˌMon'tana"). However, in words with Latinate prefixes (attached with an "=" boundary), we can have unstressed closed weak syllables before a stressed syllable, as in "con='sider". Because "observe" begins with an unstressed weak syllable followed immediately by a stressed syllable, we know that the "ob-" is a Latinate prefix: 

Well, I can suggest how to work out the answer. Take two examples using a given form of "have" in the syntactic environment [S NP __ ... ]. Assume what is in the position NP, the subject, makes no difference. The two examples have A and B following the form of "have", say it's "have", so they look like this : [S NP have A ] and [S NP have B ]. The grammatical categories of "have" in the two examples should depend only on the categories of A and B. Construct the example [S have [X A or B ] ], where X is some category we don't know about yet. Is the new example acceptable? Suppose it is. Then because the category of a grammatical conjunction of two phrases is the same as the categories of both the two phrases, A and B must have the same category X, whatever that is. So the categories of "have" in the two examples must the the same, since they depended only on the categories of A and B. Since the interpretation of a phrase is a function of the pronunciation and the syntactic category of the phrase, the interpretations of "have" in the two environments we are considering must be the same. So, it all depends on the grammaticality of [S NP have [ A or B ] ]. If it's grammatical, the "have"s mean the same. If in general, we can find grammatical ways of conjoining the complements of the various "have"s of interest, they must all have a single interpretation. 

Semantics is the study of meaning, as, for instance, optics is the study of light, or arithmetic is the study of numbers. They are just different sorts of things -- it's hard to explain. Your study of something is different from the thing that you're studying. If I'm trying to figure out how to add numbers, my figuring that out is not the same as addition. 

Picard, I guess (but not before /a/). See Joret line and Picard language. But maybe you're asking about earlier history? 

Before I retired (7 years ago) I taught a linguistics course in articulatory phonetics from time to time. About half my class were undergraduate majors in Speech Pathology. So that course was clearly a requirement for Speech Path at that time. I used IPA in the course, but there was never a requirement to learn IPA. Linguistics is not about learning notations. You can just look at a chart for that. 

I question whether the premise of your question is true about language structure, though it is surely true about the way some people talk about language. Why think there is any difference between single word and "corresponding" multiple word constituents? It's easy to think of counterexamples. "Small" is a single word prenominal modifier -- an adjective. Then how about "very small"? Does having two words make it an adjective phrase? I see no evidence for a difference in category. In fact, if there were a difference in category, it should not be possible to coordinate an adjective and an adjective modified by an adverb. Yet it is: "wobbly and very small". 

A well known approach in linguistics to formalizing semantics is Montague Semantics, see Stanford's Plato and jump to section 3.4 for a very brief summary and some references on pragmatics. 

These rules also generate other trees, an infinite number of them, and here is a sampling of the sentences corresponding to these trees. 

Well, there are a lot of answers here, but I don't see the right one. We classify languages not by similarities of any sort other than shared sound changes in their histories. We inherit this geneological model from the 19th century neogrammarians, along with the assumption that sound changes apply without exception. Then what sound changes does English share with the Romance languages that other Germanic languages do not share? To my knowledge, none at all. There is no evidence to support classifying English as a Romance language. Perhaps a case can be made that English shares some morphology and some phonology with Romance due to a large influx of loan words, including sets of morphologically related borrowed forms. But I don't know of any phonological rule of English traceable to Romance that could plausibly be classed as an exceptionless sound change. Maybe the clarity of this issue is hard to see because so much doubt has been cast on the neogrammarian hypothesis of exceptionless sound changes. We could discuss that, I suppose, but I don't see it as an issue, here. That's the assumption that underlies the genetic classification under discussion. 

The example Men, women and children are people could have a compound noun men, women, and children, but it would be unwise to refer to it that way, using the term "compound noun", because of confusion with another quite different construction, also a "compound noun", which is morphological rather than syntactic and does not use a conjunction. For instance, woman child is a compound noun, stressed on the first part of the compound, which refers to a person who is both a woman and a child. So, although in the example, men, women, and children might be a compound noun, it shouldn't be called that, because it would be confusing. I'll refer to it as a conjoined noun instead, to avoid that confusion. I notice in other comments that men, women, and children is referred to as a conjunction of three noun phrases (or DPs, or subjects), rather than three nouns. That's misleading, though it could be true. The general rule in English is that several constituents of the same category can be conjoined, and since a NP (noun phrase) can be absent any overt determiner, in the example, we could be dealing with either a conjunction of three nouns or a conjunction of three NPs. If we add a determiner or an adjective to the example, the situation becomes clear: 

Yes. I was deeply impressed by the reports of vertical constructions by Ron and Suzy Scollon in the '70s. Here is a discussion by Charlene Sato of that and other relevant work: The Syntax of Conversation. 

It's not a fortition; it's an assimilation. The r becomes a stop after the stop n. Assimilations are lenitions. In my understanding of the term "morphophonemic", it refers to a change whose effect is phonemic. Since /d/ is a phoneme, that makes this change morphophonemic. Probably in the SPE version of generative phonology, such an example would be accommodated by making the rule subject to the linking convention of Chapter 9 in SPE (so that once r becomes a stop, it also ceases to be retroflex or liquid). 

A root is the form to which derivational affixes are added to form a stem. (Sometimes stems are formed by derivational processes other than affixation.) A stem is the form to which inflectional affixes are added to form a word. In English, many words have no affixes, in which case there is no point in distinguishing word from stem or stem from root. Derivation, at least for English suffixes, is conditioned by part of speech, while inflection is conditioned by syntactic environment. 

And this example should show what is wrong with your statement that a gerund has the syntactic role of subject. It doesn't. In the above example, "swimming the Atlantic" is the subject, not "swimming". In your example, it is the noun phrase made up of the gerund "swimming" which is the subject, not the gerund. Verbs can't be subjects. But this is not obvious when you consider just simple one word subjects, because then you can't see the difference between the noun phrase subject and the contents of that noun phrase. 

could mean either (1) for each voter among 25% of voters, it is probable that that person voted for Trump, or (2) the proportion of Trump voters is probably 25%. If the subject and the adverb arguments could be applied to in one step, there would be just one possible constituent structure for the example and therefore no ambiguity of structure. 

I agree with your classification, but I also think it is trivial. Two binary features always give 4 classes, and considering only things to which at least one of the features is applicable, you'll always get a three-way classification. 

I don't know of research, or whether the following is true, but phonological alternations might be due to phonological processes in the speech of native speakers, but due instead to phonological rules in the speech of nonnative speakers. The distinction between rule and process is made in David Stampe's theory Natural Phonology. Rules have to be learned, and it's an error not to apply them when a language has them; processes are due to the arrangement of articulatory organs in humans, so they do not have to be learned, and it's an error to apply them when a language forbids them. Here is a hypothetical example. Many languages devoice word final obstruents (oral stops and fricatives), e.g., German. This is a natural process. English forbids this -- we have to be able to say word final b/d/g/z/v in English. Japanese does not forbid this devoicing -- there is no reason to, since Japanese has no word final obstruents. However, it's been observed that Japanese speakers learning English do often at first devoice English word final obstruents. This error of devoicing where you shouldn't cannot be something Japanese speakers have learned about Japanese -- it is not language interference. Now, I don't know what adult English speakers do when they learn German, but it seems plausible that they learn to devoice word final obstruents as a rule, since they must have suppressed the process of word final devoicing when they first learned English (though some children learning English do apply this process, for a time). So, then, I'm guessing that a person speaking German who occasionally forgets to devoice a word final obstruent can be identified as a nonnative speaker of German. 

Syllabification refers to a phonological process in which a consonant becomes a vowel, or at least more like a vowel. Languages favor a syllable structure alternating between non-syllabics (consonants) and syllabic sounds (vowels): CVCVCV... Consequently, when a vowel stands next to another vowel, it sometimes becomes more consonant like, and when a consonant does not stand next to a vowel, it sometimes becomes more vowel like, or that is, it syllabifies. Due to the changes in syllable structure produced by Indo-European ablaut, syllabification can cause a /v/ next to a vowel in Sanskrit to become /u/ when it is not next to a vowel, for instance. This was noticed and called samprasāraṇa by Indian grammarians. Another famous instance of syllabification is the development of the syllabified nasal in the first syllable of centum "hundred" to /a/ in the satem IE languages. 

If I understand Walker, he's saying the regular case is words like "quiESCence", with accent on the penult. If "concupiscence" were stressed on the penult, it would be pronounced "concuPISSence", which sounds vaguely obscene. 

These are allophonic changes, not phonemic ones, which is why I use square brackets. When I aim for word final /n/, the preceding vowel does not tense. So, when I see Dorothy Sayers writing final "n'", for instance "peelin'", for Lord Peter's dialogue, I would most naturally say that as [ˈpilɪn], since it ends in /n/, not /ŋ/.