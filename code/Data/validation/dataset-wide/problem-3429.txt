I'll formalize what the EMH says, use this formulation to describe the "joint hypothesis theorem," talk about the difference between prices "containing all information" and prices "reflecting all information," and talk about how conditional probabilities relate to random walks. I'll also discuss how the concept of a random walk does show up when we make additional assumptions. However, as we'll see, this still does not imply that, say, stock returns are unpredictable. 

The correct approximation is $f(x) \approx E[f(x)] + E[f'(x)] (x - E[x])$. This is unbiased, whereas $f(x) \approx E[f(x)] + f'(E[x]) (x - E[x])$ is not. To see this, project $f(x) - \overline{f(x)}$ on $x - \bar x$, where the "bar" represents the expectation operator. Then, approximate $$ \frac{\text{Cov}(f(x), x)}{\text{Var(x)}} \approx E[f'(x)]. $$ This approximation is exact when $x$ is normally distributed (by Stein's lemma). EDIT: For clarification, see that the projection of $f(x) - \overline{f(x)}$ on $x - \bar x$ gives us $f(x) - \overline{f(x)} = \beta (x - \bar x) + \epsilon$, where $E[\epsilon] = E[\epsilon x] = 0$ and $\beta = \frac{\text{Cov}(f(x), x)}{\text{Var(x)}}$. If we use Stein's lemma to approximate $\beta$ as described above, we are left with $$ f(x) \approx E[f(x)] + E[f'(x)] (x - \bar x) + \epsilon, $$ which is unbiased, $$ E[\epsilon] = 0. $$ On the other hand, $$ E[f(E[x]) + f'(E[X]) (x - E[x])] = f(E[x]) \neq E[f(x)]. $$ 

I'm still not sure if I'm doing something wrong. However, it is useful to note that I get the same results in R. 

(More details about the procedure are given below.) Now, if I wanted to take a bunch of random stocks at some point in time and look at them over some time period, what should I do with the values ($Z_it$) for these stocks that aren't listed at time $t$. Should I use the de-listing returns where appropriate and then fill in zeros everywhere else? But this would do weird things to the beta of the stock. Should I try to constrain the beta (the factor loadings) of the stocks to be zero in all the places where the stock is unlisted? This would require me to change the model (requiring a model that somehow allows for time varying factor loadings). How do people usually handle this problem? Is there an easy way (even if it is slightly more incorrect)? Some Detail about the Estimation Procedure For concreteness, suppose I wanted to test the CAPM using the time series regression framework outlined in chapter 6 of Campbell, Lo, and MacKinley (The Econometrics of Financial Markets). Some of the assumptions are listed in this image: 

I have heard multiple times that a consistent definition of liquidity does not exist. The two wikipedia articles describing liquidity (Market liquidity and accounting liquidity) lack a discussion of this apparent problem within economics. Could someone perhaps provide a reference to a discussion of the problem of defining liquidity and maybe provide a brief summary of the issue? 

Sorry I'm not giving the full details, but you can read more about them in chapters 4 and 9 of "Recursive Methods in Economic Dynamics" by Stokey, Lucas, and Prescott. The point that I'm making is that the assumptions of these kinds of models usually imply that the value function is concave. Therefore, if you check the assumptions and they indeed do so restrict the value function, then you can restrict your search to look only for matrices P that are negative semi-definite. 

One common criticism in the optimal taxation literature is the specification of the social welfare function. The optimal taxation literature (I have the Mirrlees framework in mind) relies on first specifying a social welfare function. However, because we never observe the social welfare function (SWF) and because we're not even sure if a reasonable specification of social welfare can be represented by a utilitarian-like SWF, what kind of policy prescriptions can we get out of these types of models? How much of a problem is this? Are there potential ways in which this problem can be made less offensive? 

Opinion of Economists What makes you think that economists are so aligned against the minimum wage? Take a look at the IGM Forum that polls top academic economists. There is substantial disagreement about the effects and welfare implications of a minimum wage hike. 

I'm interested in income and social mobility. I understand that Piketty's research deals with the the distribution of income over a long time period. I have also seen some of Chetty and Saez's research concerning income and social mobility in the US. I'm wondering, what data do Chetty and Saez work use? What other data is potentially available and how far back does it go? It would be nice to see who could find the data that goes back the farthest. (Also, the data does not necessarily have to be US data.) 

I think Kamien and Schwartz's Dynamic Optimization: The Calculus of Variations and Optimal Control in Economics and Management is pretty well known. 

Because this question will ultimately produce subjective answers, and given the wise advice (given in the above comments) to avoid learning economics by studying "schools of thought," I decided to answer by telling you what books were assigned in my introductory economics class as an undergraduate. In my first undergraduate class, our textbook was "Principles of Economics" by Mankiw (which is actually very nice to read even though it's a textbook). Along with this textbook we were required to read these two more popularly oriented books: "Naked Economics: Undressing the Dismal Science" (which has some fun discussion of economic issues) and "The Armchair Economist: Economics and Everyday Life" (which gives a great introduction to economics thinking). These three books give a good beginner's overview of economics and economic thinking in general. I think this would be a nice way to start. 

Here's some information with regard to the intuition of each. The Kolmogorov backward equation is often referred to as the Fokker-Planck equation. It is a partial differential equation (PDE) that describes the time evolution of the probability density function of a variable over a state. That is, suppose we have information about the state $x$ at a time $t$. The forward equation tells us the distribution of $x$ at a time $s>t$. The Kolmogorov backward equation on the other hand is used to understand the probability of a state ending up in a set $B$ at some time $s$. Define a function of the state, $u_s(x)$. At the final time $s$, $u_s(x) = 1$ if $x \in B$. Otherwise, it is zero. Then, for any time $t < s$, $u_t(x)$ describes the probability that $X_s$ given $X_t = x_t$ will end up in the set $B$. The Kolmogorov backward equation described the evolution of the probability function. $u_s(x)$ then is the final condition of a this PDE. I've taken this information mostly from Wikipedia: 

I'm trying to figure out the commands necessary to replicate the following table in Stata. This table is taken from Chapter 11, p. 357 of Econometric Analysis of Cross Section and Panel Data, Second Edition by Jeffrey M Wooldridge. Here I'm specifically trying to figure out how to obtain the robust standard errors (shown in square brackets) in column (2). I'm trying to do this in Stata. I was able to to get the conventional standard errors using the command 

How do I use the Malliavin calculus to solve for the optimal trading strategy in the classic Merton problem? In Duffie's book "Dynamic Asset Pricing," he outlines the "Martingale method" of solving stochastic control problems. I won't reproduce the whole outline or notation here, but the the essentials are given on p.217 of his third edition book: