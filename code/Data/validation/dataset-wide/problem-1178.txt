Now, the solution I have thought of is highly inelegant. I have considered saving the client details for every sale in the Invoice table, and the item details in InvoiceItems. Of course, that would be a great deal of duplicate data. Another solution might be to keep the defunct details of items and clients, which would mean polluting my ItemsDetails and Client table with records no longer valid. Of course, I can add a boolean IsValid column, but then again, the solution does not seem to be succinct. Any help would be heartily appreciated. 

Material records the material whose stock has changed. Amount records the amount by which the stock has changed. Invoice records the invoice number of the transaction. 

Now, if we need to delete or modify any record of PURCHASES or INVOICE, we also need to modify the CurrentStock in MATERIAL as well as the corresponding records in STOCKHISTORY table. Here is a rundown of STOCKHISTORY table: 

I am trying to troubleshoot locking behavior and the READ_COMMITTED_SNAPSHOT isolation level while attempting to resolve concurrency issues. Background: Assume an online ordering system (ecommerce). Product price changes are calculated minimum monthly, and this results in around 600,000 records that must be changed in the database. When posting the price change updates to the database (SQL Server 2008R2 Web Edition) the site becomes unusable due to the significant levels of locking in the primary ProductDetails table when using READ_COMMITTED transaction isolation level. To resolve this, READ_COMMITTED_SNAPSHOT is enabled, however other transactions are still being blocked during the price updates. Investigation of sys.dm_tran_locks shows the blocked session is caused by a waiting Sch-S lock. As I understand it, Sch-S locks (schema stability) are taken while a schema-dependent query is being compiled and executed (aren't they all schema-dependent?). But sys.dm_tran_locks also shows a series of Sch-M locks (schema modification), which are not compatible with any outside operations per BOL. I assume this is caused by the fact that the 3rd party tool used to replicate data changes drops foreign keys during the update process and recreates them after the update is completed. And so, in spite of READ_COMMITTED_SNAPSHOT, other queries are still blocked, not by the update, but by the Sch-M locks cause by the changes to foreign key relationships. This theory was confirmed by eliminating the setting that dropped/recreated the foreign keys. Now the update process no longer takes Sch-M locks (sys.dm_tran_locks only shows X, IX, S locks), and other transactions are not blocked from using the version store to satisfy their queries. However, when executed using this process, the price changes take approx 1 hour to process (vs. 1-2 minutes) and sys.dm_tran_locks shows the transaction taking almost 90,000 different locks, compared to around 100-150 when foreign keys were being dropped/recreated. Can anyone explain this behavior and offer suggestions on how concurrency could be maintained without exponentially increasing the maintenance time for price changes? 

Also if you are getting a lot of records from first procedure, consider using temp table instead, table variable is not the best choice. And lastly you can always diagnose procedure execution using Extended Events, and log it to a table/file if it exceeds certain threshold, along with execution plan,and waits so you can compare it with the regular executions. 

I used examples on value and node, because you provided the code with those two functions only, if you want to read more about it please visit this Hope this simple examples give you an idea of how to query xml types 

will show you the most CPU extensive queries which you might want to optimize further. Optimizing these queries you might find missing indexes, outdated statistics, Non sarg-able queries which are real issues that are behind high CPU usage. Its not the only blueprint how to fix CPU problems but i hope it gives you a good start! 

Pens x1 (nickname) --- Pen (MaterialType) ----- 1 (NumberOfPieces) Pens x2 (nickname) --- Pen (MaterialType) ------5 (NumberOfPieces) 

Now here is where it gets murky. And I need help with the schema here. The FlowDirection records whether it was a purchase or a sale. The PurchaseId records the invoice number of the purchase IF it was a purchase. The SalesId records the invoice number of the sale IF it was a sale. There is a constraint on the table where the nullity of PurchaseId and SalesId attributes are XOR'ed, i.e. One and only one of them will be null for a given record. I am thinking of getting rid of the FlowDirection as it is obviously redundant. Secondly, I also will get rid of the Invoice attribute, as it is already recorded in PurchaseId/SalesId. Or, I can point Invoice column to a union of Num attribute of the INVOICE table and the Invoice attribute of the Purchase table (I have got to rename all these tables and attributes too). Which one should be a better approach? 

When you are all set and done, you can transfer those views from ViewSchema to dbo or some other more meaningful schema with command: 

Your second query runs within a second because it does not have to go through each record in (130 million record table) and compare whether it matches the record from a temp table. And there is not much you can do when you are using a temp table with a single record within. One solution would be to save it within a variable and use it in where condition without joining it, but you said temp table will contain more records. Note that ,it does not necessarily mean that more records will increase your execution time. With more rows in temp table, query optimizer will use Hash Join which could possibly give you even better results. However you could optimize your query like this: 

Monitoring CPU usage using task manager is not really a reliable source. There are many other(such as core OS activity,device drivers) non-sql processes running in the background that could be adding extra overhead without you even knowing. PerfMon is the tool you should reach for in these cases. Processor/%Privileged Time, Processor/ %User Time, Process (sqlservr.exe)/ %Processor Time Will give you an idea of what is actually happening with your SQL server, without explaining each of these counters, turn on description checkbox and read from there, but it will essentially show you the ratio of SQL Server vs Other processes usage. Even though its easy to spot, it is not so easy to diagnose. There could be other "hidden" issues that are indicating that the processor is the problem. Such as having lot of compilations/recompilations, which are issues related to non-parametrized queries or forced recompilations. You can find these metrics in Perfmon: SQLServer:SQL Statistics/SQL Compilations/sec, SQLServer:SQL Statistics/SQL Re-Compilations/sec. SQLServer:Plan Cache/Cache hit Ratio Indicates memory problem, but excessive page flushing in/out of memory also add extra CPU usage. DMVs can also help you diagnosing the problem. 

So, in PURCHASES table, we record the Material we have bought and the Quantity of it. With each purchase, we need to make three modifications: 1. Insert the purchase details in PURCHASE 2. Modify CurrentStock in MATERIAL(increase it by the amount we have purchased) 3. Insert the amount of increase in Materials in STOCKHISTORY table. Please note that StockHistory does not record the cumulative stock, but just the amount of increase/decrease associated with each transaction. Similarly, for each sale, we need to make three modifications. 1. Insert the sales details in INVOICE. 2. Modify CurrentStock in MATERIAL(decrease it by the amount we have purchased) 3. Insert the amount of decrease in Materials in STOCKHISTORY table. Please note that INVOICE table here represents the invoices generated by us, i.e. the sales made from our end. Here is the schema: 

Disclaimer: I am a beginner in the field of database. I am trying to design a database in PostgresQL which will record the sales and purchases of items. I intend to use this database as the model with EF and WPF using MVVM approach. Here is the problem with the design. This is the simplified description of my schema. I have a Clients table, an ItemDetails table and a RateChart table, among others. The Clients table records details for each client, including their address. The ItemDetails table records details for each item we sell to our clients, including a short description for the items. We do update the description from time to time. The RateChart table has the different rates for items for each client. Now, I also have to save the details of each and every sale. So, I have an Invoice table, and InvoiceItems table. The Invoice table saves the invoice number (primary key), the date, the total amount of the order and the client name(foreign key, refers to client name in client table). The InvoiceItems table, on the other hand, records the items sold, the quantity of each item sold and the rate at which it is sold. Now, here's the problem. The items sold in InvoiceItems list is a reference to the ItemDetails table where details of all the items are saved. Now, the details of items are changed from time to time. For example, the description of items are changed. So, whenever I change those item details, those changes will cascade down to this InvoiceItems table, and will make the older invoice records erroneous, because that was not the description when the product was sold. Same problem lies with the Client field in sale table. Whenever I change an attribute any record in the client table, the changes will cascade down to the sale table. Here is the schema for illustration. 

The service account you use to run SQL server (what you enter in configuration manager) must also be a member of the sysadmin role within SQL server. If you have not done so already, log in using the sa account and grant the Windows service account the sysadmin server role. 

As an alternative, I also could have simply inserted EVENTDATA() as an XML LOB into a table, rather than parsing it out into columns. Because I would not be manipulating the XML, the SET options do not matter. Then I would just build an XML index for querying performance, and construct a view to use for audit log reporting that parses the XML in the view definition, in the same manner as I am doing above in my INSERT statement. Thanks to Max for pointing me in a different research direction, and @AaronBertrand on #sqlhelp who helped me with correct SET options within the body of the trigger. 

Disclaimer: I am a beginner in the field of database. I am trying to design a database in PostgreSQL which will record the sales and purchases of items. I intend to use this database as the model with EF and WPF using MVVM and Repository approach. The simplified proforma is like this: Items: Details of items we deal with. Purchases: Records the details of items we purchased. Sales: Records the details of items we sold. Stock: Quantity of each Item in hand. Now, I also have a stock table where current stock of each item is maintained. The dilemma I am facing is this: When any Invoice is generated, it indicates that a sale has occurred, and the stock of each item in the Invoice is updated accordingly. Similarly, when a purchase from our side occurs, we record it in the purchases table, and the respective items in Stock table has their quantity updated. I would have to also consider the scenario of correcting the stock table if an existing invoice is modified or deleted. These are the only three scenarios when the stock table would change. Now, I can achieve this update of stock table using Triggers. I can also use stored procedures in order to encapsulate the entry in Purchases or Sales table along with updating the stock table. I can also do this in application logic using LINQ. Which approach would be the most pragmatic one? THE DETAILED STORY Please allow me to discuss the entities that matter in this current context. We are a reseller. We buy materials in bulk, we process the materials and repackage them and then sell them. In this table, the items we sell are marked as the entity ITEM. There is are two attributes in the ITEM table worth noting. One is MaterialType and the other NumberOfPieces. The entity MATERIAL is the raw material we buy. Now suppose, we buy 100 pieces of pens, and we resell them in units of 1 or 5. So, in this case, the records in the item table would be 

Dynamic quorum basically dynamically adjusts votes depending on available servers. Each time when one of nodes goes down, dynamic quorum will remove the vote from that node. In your scenario you have 2 nodes only and dynamic quorum will automatically remove the vote from your passive node, so the 1st node will have the majority of votes. In planned maintenance scenario when you are shutting down the first node quorum will transfer the vote from first to the second, and remove it from the first node. However in scenario where first node just crashes quorum does not have time to transfer the vote and your second node wont get to vote, which basically will just shut down your cluster. Therefore in scenario with 2 nodes only, it is recommended to have a witness. 

Note that specifying database name you are limiting user only on that particular database. To build your own permissions on a user/role take a look at this pdf: SQL Server Permissions Map