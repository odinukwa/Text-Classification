According to the HP DL360 G6 User Guide, the fan 2 is only necessary when the second processor is present: 

Here is an excelent article explaining in detail how to use this feature: Exchange 2010 Administrator Logging walk-through 

You can give selective DNS responses based on location with BIND Views if you are using BIND as your external DNS server. The Technical Preview of the new version of Windows Server also has a feature called DNS Policies which looks very promising. To serve content based on client location and other criteria such as User Agent or schedules, F5 has an appliance called Global Traffic Manager which used in conjuntion with their load balancers achieves what you are looking for. In Cloud environments, Amazon's Route 53 can accomplish the same. In order to keep the data in sync you must have a storage backend capable of do synchoronous replication, or use the replication provided by MySQL, which will keep the replicated data consistent. 

It's a good practice to create a dedicated account for some services, specially when they are accessible over the internet or any untrusted network. So if somebody exploits a security flaw in Tomcat it's not going to compromise the whole system. (Unless he manages to escalate privileges, but that's another story) From the Security Considerations section of Tomcat documentation: 

Asterisk already has builtin methods to handle NAT related issues. Take a look at this article. If you still wan't to proxify, you can try SaRP, which seems to work for both SIP and RTP. 

Mbox is fast when searching a text inside a mailbox and appending a new mail into the mail file, but it's prone to mail corruption and may have problems with network file systems. Also, File locking may be an issue. With Maildir, retrieving specific mails is faster, and has no issues due to file locking or network file systems (Example: With Maildir, you can store different mail folders in different NFS shares, this cannot be achieved with Mbox). You can have issues with Maildir when a filesystem is not efficient handling a large number of files. I understand that you need an scalable solution, before you choose one or another, be sure to give a read to this document: (Even if you are not using Courier) Benchmarking mbox versus maildir 

The fact is when you copy back a backup of the partition it keep (as you notice) the size it was because the filesystem has some field for that. What I suggest you is not to copy the entire partition, but reformat in ext3 your partiton of 98G and copy the files into that new partion. To avoid some permission problem use the command cp -pr (p flag for permission keep and r for recursive) 

As they are block these connections are not harmful for a DDOS. Logs are only there to let you know who tried to attack you. What is harmful is when the number of half connection opened gets near the limit you specified in your config. What I suggest you is to follow the advice of Zypher and have your firewall to moderate these connections. 

I accidently create a "\" file into my linux how to revert it. I tried rm \ (as it's the escaping character it didn't work), rm '\' and rm \\ nothing worked. 

Try to see documentation about iptables there should be some config about caping bandwith. But on the other end you should try to configure iptables to ban bad IPs so it will clear the problem and also you will be able to report IPs and time of the attack to authorities. 

It's not because you pay for something it is good, this is what I trust. But alot of people think the converse. 1 000 000 million people can be wrong. 

Yes it's possible to do so put a htaccess in the file you want to moderate the access. Put the same text as in the configuration file. Make sure that your conf files will read .htaccess. 

The article that you pointed out does not necessarily apply only to domain environments. As noted at the end: 

*1.2.3.4 = I replaced my IP address with this fake IP. I don't need any more strange packets knocking my door :) 

A couple of things that you can try with Group Policy are the following: -Set up a fake proxy, and add the internal servers URLS in the "Bypass Proxy Server for Local Addresses" list. This can be easily circunvent if your users manage to install another browser that is not affected by group policies. -You can add a bogus route to 0.0.0.0, and then appropiate routes for all your internal subnets. You can achieve this using Group policy preferences and adding to the registry the list of routes under or by adding a logon script with the apropiate commands. You can also enforce a fake DNS server and distribute a custom hosts file with the required servers from a network share, however, this may be become hard to manage in the long term. 

Check this resource for a deeper understanding of SSH tunneling: $URL$ UPDATE: If you need multiple clients accessing Machine A's port 5900 via the server like this: 

The iptables rule is fine, but according to nmap's output I don't think that you have any service running in that port. Confirm that by running the following command: 

Besides BIND Views, the new version of Windows Server (Which is in a technical preview currently) also has a feature called DNS Policies which looks very promising for achieving what you are looking for. 

Normally it will be an unsigned 32 bit (on 32-bit system) so 2^32 = 4294967296. So the range is 0-4294967295. 

It seems to be an client problem why it's declining the address. Is it well configured to use a DHCP address. Try runing dhclient manually. 

No I checked at the man page of netstat and there is no way of knowing the time of an established connection using netstat. And I don't think it is stored anywhere because connection are so dynamic. 

First what would you install apt instead of having yum? And second you'll need to find a source to had to yum so it can install apt, but because the two programs (apt and yum) are doing the same job but on different platform I don't think you will find one. So I think you'll need to build apt from source (see on Source Forge). 

The ~ means the home directory of the current user (or the user you try to connect to), but watch out adding the authorized_keys file to a user permits the one with the private key to connect to your server (without any password). 

What I find strange is it that client's routing tables are pointing to nowhere (0.0.0.0). It is ok for local networks but for 10.2.10.1 that passes trough a tunnel it might be a problem. 

Do you want myapp.example.com to point to prod.wip.example.com or myapp.prod.wip.example.com? In both case if myapp.example.com is another zone than example.com you must type: 

When you add an IP address to a specific interface (let it be by DHCP or manually with ifconfig), linux kernel will add this IP's subnet's route as being on that specific interface. This is because logically you are part of the subnet. Possible issues would be: 

You can use Nagios plugins to monitor the server via IPMI. Also, Supermicro has it's own monitoring software called SuperDoctor, which also relies on IPMI. More Details: $URL$ $URL$ ftp://ftp.supermicro.com/utility/SuperDoctor_II/Linux/ 

Although is not what you are asking for, I will suggest another approach to test the new server. If you put a load balancer in front of both servers and play with the load balancing algorithms you can at the same time test the new server and gradually replace the old one. You can send 99% of the requests to the old server and the remaining one percent of requests will go to the new one where you can closely review if the service is working as expected. If everything works fine you can increase the load gradually; 90%-10%, 80%-20%, and so on. Hint: Check haproxy and the and options. 

Well, disabling SELinux it's just a temporary workaround, not a solution. We don't wan't to make Dan Walsh cry ;) Try one of these options: 

Mod_Security may be what you are looking for. The default rules blocks this kind of requests, here is an example of a reconnaissance being blocked found in my debug logs: Message: Access denied with code 403 (phase 2). Pattern match "^[\d.:]+$" at REQUEST_HEADERS:Host. [file "/etc/httpd/modsecurity.d/activated_rules/modsecurity_crs_21_protocol_anomalies.conf"] [line "98"] [id "960017"] [rev "2"] [msg "Host header is a numeric IP address"] [data "1.2.3.4"] [severity "WARNING"] [ver "OWASP_CRS/2.2.6"] [maturity "9"] [accuracy "9"] [tag "OWASP_CRS/PROTOCOL_VIOLATION/IP_HOST"] [tag "WASCTC/WASC-21"] [tag "OWASP_TOP_10/A7"] [tag "PCI/6.5.10"] [tag "$URL$ 

Check with if SELinux context is the same in /etc/named.conf and /var/named/chroot/etc/named.conf. If is not, use chcon with the --reference option to set the chrooted named.conf the same as the regular named.conf, with something like this: 

I have now solved the problem. It's a bug in the mysql replication command "change master" and the argument "MASTER_PORT=3307". It doesn't care about that argument, and always uses port 3306. 

My solution Currently I'm using rsnapshot to make one local backup every day. Then storing weekly, monthly backups in the rsnapshot way. Then doing a full backup every month with tar, which I encrypt with gpg2 before sending to my friend. Then I do an incremental level 1 backup using the snapshot file from the full backup. But unfortunately this doesn't really work. Because each time rsnapshot is executed, my files get new dates and maybe other stuff. And it looks like tar only uses date to verify if the file has changed or not. Questions 

I have now got everything to work and I was very close to the solution. It was a permission problem. This is how you add the auditlog to openldap installed on Centos 6. First enable the module. 

Where should I load iptables modules, for example ip_conntrack and ip_conntrack_ftp. Places I have found that might be candidates, but are they? 

I have recently upgraded from Centos 5.7 to 6.3, and by that to a newer httpd version. I have always made my ssl virtualhost configurations like below. Where all domains that share the same certificate (mostly/always wildcard certs) share the same ip. But never got this error message before (or have I, maybe I haven't looked to enough in my logs?) From what I have learned this should work without SNI (Server Name Indication) Here is relevant parts of my httpd.conf file. Without this VirtualHost I don't get the error message. 

So I would check if your receiving end has any delay to fetch mail or that the server that receives the mail did not put in a long anti-spam check. I would pull out the logs from your mail server at that moment to see what happened wrong. If the header is right you should look at 2015-09-08 03:19 -0600 (your servers local time). 

As shown in the email and assuming all relay actually appended a Received: field. The mail was properly received by your bluehost server on 03:19:55 -0600 within seconds of being sent. As shown by line: 

An apple's airport express can do do the job as well as any other wireless router. You'll need to configure the router in a browser type: 192.168.1.1 or 192.168.0.1 and follow what you need to. 

auditd is a good utility to know what have been done to every file. It might not be what you need tough. 

The problem with well constructed rootkit is that they modify your system's command; like ps and top to not show the processes of the rootkit and ls to not show files of the rootkit. So what you'll need to do is to get these command possibly from source or in binairies form. (Be sure to be well signed). But the trick of a root kit (I've seen it) is that they maybe corrupted your compiler too. So when the compiler knows he is compiling ls or ps or any command he infectet them as well. When I saw this problem I said fine lets recompile gcc, but no what I need to have to compile gcc...the infecte gcc....so when he knows he is compiling itself he infect it so it can infect the command. You will say that this come big and difficult to detect, yes but rare are the root kit that are so bulletproof I just gave youthe worse case. Seriously, if you are sure that there is a root kit in your server, reinstall it!