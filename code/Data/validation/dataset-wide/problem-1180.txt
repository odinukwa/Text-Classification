I would simply use the views if they perform. No need to make copies of data unnecessarily, and many database platforms allow indexed or materialized views and filtered views etc. HOWEVER, MySQL does not support indexed views. So unless the underlying indexes on your table support the various ways you are accessing the data, it might be worthwhile materializing a version of the view yourself. 

Better in what way? The optimizer may pick a table scan if an index is not covering, because an index scan would still have to be augmented with a bookmark lookup. In those cases, a poorly indexed table is no better than an unindexed table. $URL$ 

"This means that the query engine must take an additional step in order to locate the actual data." Not necessarily - if the index is covering for a given query, no trip has to be made to the data pages. Also, with included columns, additional columns can be added to a non-clustered index to make it covering without altering the key size. So the ultimate answer is - It Depends (on a lot more information than you can really cover in a single question) - you need to understand all the capabilities of the indexes and the execution plan for a given query may diverge from your expectations. A general rule of thumb I have is that a table always has a clustered index (and usually on an identity or sequential GUID), but non-clustered indexes are added for performance. But there are always exceptions - heap tables have a place, wider clustered indexes have a place. Seemingly redundant indexes which are narrower to fit more rows per page have a place. etc. etc. And I wouldn't worry about the limits on the various indexes allowed - that's almost certainly not going to come into play in many real-world examples. 

(This query by itself doesn't look too bad, but you are going to use it everywhere you would use a single table name in JOINs and so on). 

If for some row the JSON in the comment column is an object, but which does not have any such key, you simply get back a NULL. However, if there is even one scalar in the table, the query fails with the error 

@CoderAbsolute's answer gives you a good design for your tables. Since he or she did not go into detail about why this approach is better, I thought it was worth adding another answer. First of all, design your table structure in accordance with how your data fits together. Don't try to smoosh different types of things into one table, don't add several tables for the same kind of records. Try to 'normalize' - if a table will have the same info repeated many times, then move that info into a different table, and link it from the first table using a foreign key. You should be aware of what first normal form, second normal form and third normal form are. Many real-world databases do not match these standards completely, but being aware of what you should aim for will help you make a much cleaner design. I would say, don't worry about optimization until you've already got a correct design. First of all, you don't yet know how many entries you will have in your tables. Secondly, database engines are designed to make queries as fast as possible, even if you have a lot of entries. Don't second guess the developers of your database software. When you've figured out that you really do have a bottleneck, you should look first at indexing. We think of a database table as being something like an array. In reality, it could be stored as a bunch of 'blobs', one for each row, which might all have different locations on a disk, not necessarily in order. (This is not a super accurate technical description of DB internals. But it should give you a clearer picture of how the pieces fit together.) How does the database find all the blobs? It has a somewhere, a list of 'pointers' which tell it where to find each blob. So typically, finding every single row of a table is an efficient process. It goes through the list and records each one. Now, suppose that you most commonly retrieve all the photos for a given user. This might be a slow process, since it has to go through every single row of the table, and look at the field. In this case, you should add an Index on that field. An index is something like a lookup table, which allows the software to quickly find the location of all the rows with a given . So it doesn't have to check every row in turn. There are different kinds of indexes. Some are optimized for matching a particular value. This is probably the type you want for the column. Some are optimized for finding things greater or smaller than some value. This might make sense for timestamps. (Give me all the photos from the last month.) You can have an index on multiple columns if this is what you regularly query on, or even on some function of one or more columns. Some indexes mean that similar items are stored close to each other on disk, to take advantage of caching to retrieve them more quickly. You should familiarize yourself with the possibilities for your DBMS. Experimentation can also be very valuable here, since the exact speedups depend on your settings and also your hardware configuration. 

I would link at all/most levels. This denormalized star means that yes, the data is redundant, but it typically makes the reporting and analysis a lot easier. Note that this is very different from OLTP normalization, and you don't typically have to worry about redundant data getting out of sync because in a DW scenario data never changes. New facts get added and dimensions get expired and new ones created. I don't see a Dim_Folder. I would assume that the actual path of the folder would be an attribute of the Dim_Folder. Only the numeric quantity and any degenerate dimensions ($URL$ would be in the fact table. I wouldn't think of the folder path as a degenerate dimension because it keeps coming back in each snapshot (an a folder isn't a transaction). So you could do something like this: 

Partitioning is only at the table level and is for managing partitions of that table. Typically the advantage of partitioning is in swapping in and out data and for getting additional control of the granularity of storage and backup. To some extent it can also help with performance if there are shared partition columns in a join, but that probably shouldn't be a reason to partition. I understand your thinking about the child tables, but if you want that data also partitioned by country (for some of the above reasons), you would have to add a column because the partition function is restricted. What is your thinking about why you want to partition in the first place? $URL$ 

One approach is to check the string form of the JSON column. An object should start with '{', or maybe by some spaces followed by that character. So 

that one day some other part number such as 9 would be acceptable. You can quickly and transparently add it to the table, rather than changing a bunch of constraints on different tables. that you might want to store some extra data alongside each part number, for example the name of the part. You can add a column to the table which you already have. 

I am not sure if this is a very clean solution, but it might work. Suppose that you have a table and a table. Create a third table, . This should have three columns, , , and . and are both foreign keys to the respective tables. for the column, restricting the integer values that it can take, for example from 1 to 8. Now impose two constraints on the table. First a constraint on , which ensures that a member can only have one leader. Then a constraint on the combination of columns . This will mean that each leader can only have as many members as there are distinct values possible for . I can see a couple of drawbacks with this - first of all inserting a new is now a little tricky, as you have to find a value of which has not been used. Bear in mind you have to make a check at this point anyway - to see if the in question already has enough members. Secondly the type for the might suggest that there is some difference between the members which have the different values - that the numbers are meaningful. However, I think it is a reasonably clean and normalized design. 

So the qa box would have 10 clusters of 1db rather than 1 cluster with 10 DBS. I'm sure that's hell for sharing resources, but it's qa, so that's not the priority. Is this a good strategy? Is there a better strategy that an experienced psql dba would go to instead? 

Only one user Db per cluster Snapshot the whole data directory Copy snapshot data to another drive and mount as another data directory, where I can bring it online as another cluster. Details for configuring user and settings files and maybe renaming the Db. Renaming May not be needed on its own private cluster. 

I have a 30 gb db in PostgreSQL 9.2. Running On Amazon Linux on an ec2 (web virtual machine), with the data directory mounted from its own drive (EBS). I'll call the database db30. On production it's the only user db. In qa/dev we get the call for multiple versions. Ex: 

I've been a mssql dba on physical servers for nearly a decade so I'm totally thinking with a SQL Server mindset. Right now, I'll pg_dump to a gzip file. 7gb. Might take a while. To restore I would create the new db name, the psql run the SQL file against it, which promptly takes a few hours to apply all of the SQL and build indexes. In aws and on sans, you have the option of a disk snapshot, which I'm assuming should be like copying the files and attaching them to the server. But the docs say you must grab both Db files and WAL in the snapshot. Because the WAL is an object of the postmaster, all db's share it. This snapshot method seems only valid for backing up and resorting the whole cluster (what I'd call a running instance of PostgreSQL). It doesn't seem possible to directly snap and restore only a single Db. So my rookie psql notion would be: 

This is a pain, as we have had strings written to this column some of the time, mainly due to errors. Is there any reasonable way of either filtering these out in a query, or identifying them all so that they can be removed in one go? 

should catch most cases. However, unlike the column, columns store JSON in its original form, not in a canonical form. So this method doesn't exclude the possibility that there will be some weird JSON, for example starting with another space character, which won't be matched. 

The only situation where I would not do it this way, was if 1 to 8 is something which could never ever change. For example, you might be sure that there will only ever by 7 days of the week (but who knows?). In this case I would create a TYPE with this constraint. Then you can use the type in multiple tables. If the days of the week (or whatever) DOES change, you can change the TYPE. 

We are using Postgres 9.3 and making use of the JSON column which was new to that version. Version 9.4 added a LOT of utility functions for handling JSON, as well as the new JSONB column. You can see this by comparing $URL$ with $URL$ 9.4 has a function, which returns the type of a JSON object (as a string: 'string', 'number', 'array', 'object', etc.) I would like to know if there is any practical way in 9.3 of retrieving all rows which are scalars (or, equally useful for us, which are not objects). We would like the column of a given table to always be a JSON object. This means that we can do queries like: