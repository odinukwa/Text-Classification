This is not a "deviation" at all. Contrary to your understanding, English employs tons of nasalization in its vowels. What it lacks is phonemic nasalization, that is, phonologically nasalized vowels whose distribution is not predictable. Generally, vowels (and sonorant consonants) in English get phonetically nasalized (i.e. they are pronounced with the velum lowered so that air can escape through the nose) when they are adjacent to nasal consonants. So, the [i] in bead is usually not nasalized, but the [i] in mean is nasalized because it is flanked by nasal consonants. The nasal passage is open for the [m] and stays open through the duration of the vowel and the final [n]. A vowel that is between a nasal consonant and a non-nasal consonant will usually get partially nasalized (more nasalized on the side next to the nasal). This explains why you are hearing the vowels in Marge's and under as nasalized. Many speakers leave their vela slightly lowered for a majority of the time while they are speaking, resulting in rampant partial nasalization of most of their vowels and sonorants, regardless of whether they are abutted by nasal consonants or not. As an American, I have noticed this to be a pervasive tendency in many dialects of British English (such as the one in your link). This would explain nasalization in words like see (although in this particular case that vowel does not sound nasalized to my ears). But the important point here is that, in all the cases above, the nasalization of the English vowels does not contribute to the phonological distinctiveness of those vowels. That's why speakers who leave their vela lowered most of the time have no trouble communicating with speakers of the same dialect that don't leave their vela lowered. 

In the second experiment, subjects were presented with a series of synthetic speech samples, which varied along various dimensions including degree of jitter, degree of shimmer, and degree of noise. Overall, subjects did quite poorly and distinguishing jitter and shimmer. They did marginally better when noise was absent (not a likely scenario in the real world), but even then their sensitivity was still "limited". The presentation has many clickable sound clips, so you can hear examples of what the subjects heard. 

I think the answer may be more mundane than you were thinking. Using the 'biometrics' tag for this question may be misleading, because it might imply to some people that the answer has to do with automated speaker recognition. But you are just asking how you, as a person, might identify your friends via visual representations of their speech. This is definitely doable, as long as you know what to look out for, you are choosing from a small set (as you mentioned), and the recordings include enough speech to provide you with the requisite information for telling your friends apart. I look at speech all day for a living, and I have gotten to know the voices of a few individuals pretty well. I can tell who's who based on various factors, including pitch range, intonational patterns, voice quality, and dialectal traits, all of which can be gleaned from a spectrogram. @Dominik is right about automated voice biometrics not being a widely available feature in open source speech analysis tools, but as a human I have a couple of advantages over machines--I am a native speaker of human languages and I have knowledge about phonetics and phonology--that enable me to distinguish the important parts of the signal from the unimportant noise. 

In some cases, final voiceless fricatives in English nouns become voiced with the addition of the plural morpheme -s. Some of these examples are more dialect-specific than others: 

I would submit that prosody could play a role in disambiguating the two readings, at least in English. As a native speaker, if I heard someone say (out of the blue): 

EDIT: In response to a request for further explanation... Note that '50' is represented by two characters, and '1' is represented by one character (so far so good), but '51' is represented by a concatenation of '50' and '1', resulting in three characters and, thus, a non-one-to-one mapping from numerals to characters. In addition, each successive power of ten gets its own unique character, trivially resulting in several other non-one-to-one mappings. 

If you know beforehand the content of the speech signal but you just don't know which parts of the waveform correspond to which parts of the utterance, then your problem is reduced to an alignment problem--much easier to deal with than a full-fledged speech recognition (plus alignment) problem. You already know what language it is, and you know what consonants and vowels to look out for and the order in which they occur, so you just need to figure out how to align them. Check out the UPenn Forced Aligner: $URL$ They also have a modified web-interface version: $URL$ It's not 100% accurate--it needs human correction here and there--but it might at least save you some time. 

I think you need to think about the theoretical implications of what you are asking. The question presupposes that you know going into an analysis that a certain sequence of speech sounds in a language is the realization of multiple phonemes in that language. But the usual criterion for assigning phonemic status to the subparts of a sequence of speech sounds is precisely what you are saying would be lacking for this hypothetical language--the observation that the subparts (or sounds that could conceivably be realizations of those subparts) occur independently elsewhere in the language. Imagine a language that is just like English except that it doesn't have [t] or [ʃ] in isolation (while still having [tʃ]). The most sensible phonemic analysis of such data would be that [tʃ] represents a single phoneme, /tʃ/. Now substitute any other sounds for [t] and [ʃ] in this example. How would you ever argue that the constituent sounds represent individual phonemes if they never appear independently? 

The standard system of transcription used widely by phoneticians and phonologists (IPA) is useful and convenient but not perfect. And it is certainly not universal. How could it be either? The speech signal is continuous, but IPA is an attempt at breaking down that continuous signal into discrete acoustic "events". And speech is gradient along most of the dimensions codified by IPA into categorical contrasts. An example: in English, a small amount of weak aspiration often follows the (phonologically) voiced velar onset stops in stressed syllables, but when that occurs it is usually not transcribed as [kʰ] or [gʰ] but rather just [g]. Phoneticians have agreed that aspiration is worth transcribing in English when it shows up with enough intensity and duration to meet a certain threshold. Going in the other direction, the same symbol may be used to transcribe many different sounds or acoustic events, as long as a majority of phoneticians agree that those variations are similar enough to be grouped into a single category. The bounds of these categories are different from language to language, so if you are working with a language for the first time, you have to learn where other phoneticians working on that language have agreed those bounds lie. Example: a vowel that you might transcribe as [e] in one language, based on its formant values, may be transcribed as [ɪ] (or even [ɪː]) in the context of a different language. Because of this lack of universality in the transcription system and the non-absolute nature of acoustic manifestations of phonemic contrasts, knowing when to use one label vs. another is a learned skill, as implied by user6726's suggestion of getting trained in making the distinctions auditorily. But I would argue that even making the distinctions visually by analyzing the spectrogram also requires training. As mach suggests, so many things can effect the absolute duration of a vowel, even within a single language. Below is a none-exhaustive list: Prosodic factors 

It is practically impossible to make a "comparison set" (again, even for a single language) that controls for all of these factors and can be used with data from different speakers and different recording sessions. But even if you had one, you'd have to know when the tokens you were looking at were "close enough" in duration to the respectively relevant example vowels in the comparison set to be given the same respective labels. And that, in turn, would require training and experience. 

(The choice of transcription of these Cʲ onsets varies; some choose to use [ç] instead of [hʲ], for example.) There are some gaps--the alveolar stops ([t], [d]), alveolar fricatives ([s], and [z]), and the bilabial approximant [w] do not have palatalized counterparts. The [w] gap is not an unexpected one, since [w] is the only approximant (besides [j] itself). Historically, the palatalized counterparts of [t], [d], [s], and [z] became what are now the alveolo-palatal consonants [tɕ], [dʑ], [ɕ], and [ʑ], respectively. Further, these alveolo-palatal consonants do not have palatalized counterparts (there is no [ɕ]/[ɕʲ] contrast, for example). Due to the above facts, some consider it reasonable to adopt a synchronic analysis in which all surface occurrences of [tɕ] and [dʑ] are underlyingly /tʲ/ and /dʲ/, respectively, and all surface occurrences of [ɕ] and [ʑ] are underlyingly /sʲ/ and /tʲ/ (or even /sj/ and /tj/), respectively. 

That being said, they contend that what we phonologists have been doing for decades is codifying sets of cultural conventions shared among communities of speakers rather than any kind of internal grammars. I am familiar with three main types of argument against the extreme exemplar view (i.e. the complete abandonment of rule-based approaches to grammar): 

There was one other thing that constantly tripped English speakers up--in this case my knowledge of Japanese didn't really help me other than to keep me less biased towards one particular language's (see what I did there?) way of doing things: 

However, in all such cases, there are "rules" that govern what pitch contours are given to the words and syllables that are analyzed as tonally "unmarked". They are analyzed by phonologists as "unmarked" only because their pitch contours are predictable. In other words, a language learner need not memorize any inherent tone shape associated with them, but the learner does need to be aware of the "rules" that govern their pronunciation, which is invariably dependent on the inherent tonal features of neighboring syllables that are marked for tone. For example, in Mandarin, the fact that the particle -le is unmarked for tone doesn't mean that you can get away with pronouncing it with whatever pitch contour you feel like. What pitch contour is appropriate for the particle depends entirely on the tone of the syllable before it in the sentence. If that penultimate syllable is high or falling, the particle must be lower relative to the penultimate syllable. If the penultimate syllable is low or rising, the particle must be higher. In African languages that are analyzed as marking select syllables as "high", usually all the rest of the syllables are pronounced with pitches that are lower compared to those that are tonally marked. Those other syllables could often just as easily be analyzed as being marked as "low", but to do so would be redundant. In Japanese, the traditional analysis of its tonal system assumes that accented words include a syllable that is marked for a locus of a tonal shift from high to low. Syllables leading up to the marked syllable are given a relatively high pitch, and syllables following the marked syllable are given a relatively low pitch (it's a bit more complicated than that, but that's the basic system). In words that lack an accent-marked syllable, the pitch gradually rises through the end of the word. I raise all these examples to illustrate the fact that getting the pitch of "unmarked" elements right is just as important as getting the pitch of marked elements right in tone languages. If anything, it can be even trickier for a non-native speaker to learn how to pronounce such unmarked elements. I've witnessed this firsthand watching American college students attempting to learn how to pronounce sentence-final particles in Mandarin--even after getting the basic four tones down, they often struggled with that elusive "neutral" tone. So I know I didn't directly answer your question in terms of helping you find a tone language with a relatively large number of tonally "unmarked" words, but my point is that you may not want to use that criterion to predict which tone languages will be easier for you to learn. EDIT in response to comments under the original question: After reading some of the comments above, I think it's important to point out two things. One--in the languages you mention, tone is contrastive at the level of the syllable, and while most "words" (in the morphological sense) are monosyllabic, many vocabulary items that you as an English speaker would consider to be "words" are compounds comprising multiple syllables. In these polysyllabic compounds, the tone of each syllable must be taken into consideration. I am not an expert in Thai or Vietnamese, and I know next to nothing about Burmese, but in Mandarin, there are almost no vocabulary items that lack tonal specifications. Either they are monosyllabic and the tone on that single syllable is specified, or they are compounds and just a subset (usually just one) of the syllables in the compound gets "stripped" of its underlying tone. The exception to this is monosyllabic particles, but they can't occur in isolation either. So the likelihood of encountering a standalone vocabulary item that is entirely unspecified for tone in Mandarin is pretty much nil. My limited understanding of Thai is that every syllable bears a tonal specification. In Thai orthography, which marks tones, some syllables don't bear a tonal marking, but (as in the examples above) there are rules that govern what tone such unmarked syllables bear. I found this Thai-language-learning website that lays out those rules. Two--just because a word happens not to have any alternate-toned near-homophones (what linguists call minimal pairs), it does not mean that it's not important to be aware of its tonal specifications. There's not a perfect analogy in English, but think about a word like beige. I can't think of any other monosyllabic words in English that start with [b] and end with [ʒ]. But that doesn't mean that it doesn't matter how I pronounce the vowel in that word. A learner of English would have to learn how to pronounce the word with the correct vowel.