I like to know what the answer to a problem is before I try to prove that it is right, so immediately I went to my computer and generated some data. So for one thing, it looks like the eigenvectors have rational entries (when normalized so that their first entry is 1). Furthermore, if you're a little creative and don't always write these rational numbers in lowest terms, it seems like you can express the denominators in the eigenvector associated to the eigenvalue 1/i (or -1/i) are a polynomial of degree (i-1) in n. For the 1 eigenvector (the perron-frobenius one, obviously) you get the constant function 1 (obviously). For the -1/2 eigenvector, for instance, the denominators are 2, 4, 6, 8, .... and for the 1/3 eigenvector, they appear to be the values of the polynomial 3/2(n-1)(n-2) Edit: It looks like the correct denominator for the ith eigenvector in the n by n matrix is probably i/(i-1)! (n-1)(n-2)...(n-i+1) - that is, the eigenvectors are integer vectors, if this is their first coordinate. 

(edit) Okay, so as far as i can see you want to find a replacement for the mobius transform, but for a $\sigma$-algebra. In fact I'm going to guess that your $\sigma$-algebra is the measurable sets in the unit interval, based on what you've said. The most general setting I know of in which you can define a Möbius function is a locally finite, partially ordered set (see, for example, $URL$ So it sounds like you're out of luck. The measurable sets definitely don't form a locally finite poset. However, I really don't think you've asked the right question yet. You probably would get better answers than mine if you frame your question in terms of measure theory, rather than Möbius inversion. For instance, the wikipedia article seems to imply that I should think of Möbius inversion as "analagous to differentiation", and convolution with the zeta function as "analagous to integration". I don't really find this too helpful, but that's what it says. Maybe you're looking for some kind of derivative? Just a guess. 

Are you sure you've posed the question correctly? You see, sometimes you can't even solve for $x_1$. In particular, this happens if $y_1$ is such that $y_1 + {x_m}^2$ is a quadratic nonresidue. So there is no such maximum $n$; even when $n=1$ there exist situations where there is no solution. That can't be what you meant. In an attempt to discover a version of the question with a more interesting answer, let's further suppose that the $y_i$ are chosen in such a way that $y_i + {x_m}^2$ are all quadratic residues. But this is boring for the opposite reason: you can always solve the system. Since you've forced all the $y_i$ to be distinct, then there is no obstruction to finding solutions until you run out of values of $y_i + {x_m}^2$ which are quadratic residues. The maximum $n$ where there's a solution is when you take $n$ to be the number of distinct nonzero quadratic residues mod $p$... I am still left with the sense that I haven't told you anything you didn't already know. 

That is, let A(n) be the matrix for sets of size n, where the rows and columns are in lex order, and M(n) be its inverse. Then conjecturally M(n) has the following block structure: 

Well, it has now, since I just sank my morning into studying it. I sure am a sucker for a naive combinatorics problem. Here's what I know, or can conjecture: 

You get all possible $E_2$ by starting with an $(n-m) \times (n-m)$ permutation matrix, and expand it to being $(n-m) \times n$ by inserting columns of zeros under each of the ones of $E_1$. That is, there's an easy bijection between your set and the order $(n-m)$ permutation matrices. I think you knew this already. I guess what I'm driving at is that even if there is some canonical name for this object in some field of mathematics, the set that you want to name is simple enough that you can describe it in one sentence. If you need to give this set a name, just call it $percomp_n(E_1)$, like you wanted to do. People do this all the time. Indeed, most of us would be even more confused if you dig up an obscure, but technically correct, name. 

I don't think so. You really mean maxiMUM, not maxiMAL, right? If so, take a complete bipartite graph $K_{m,n}$ with $m$ less than $n$, so the maximum critical set is the big part in the bipartition and is of size n. This graph isn't $\alpha$-critical, clearly, but its maximum independent set is so big that there's a pigeonhole problem with finding a disjoint second one. It seems to me that if you delete edges from G you can only increase the independence number, so (*) do this iteratively without disconnecting G until the result is $\alpha$-critical. This should give you a counterexample unless this last step (*) is flawed somehow, which might well be the case, I confess - I haven't thought it through. But rather than agonizing about whether it's possible, you can implement this randomly with a computer program, for explicit small m,n, until you have either generated an explicit counterexample, or waited for such a long time as to convince yourself that it's worth trying to find the error. I might do it if I have a chance today. EDIT -- I was wrong - step (*) is in fact flawed, because it disconnects the graph before it becomes $\alpha$-critical. Interesting! 

(Q1) I think it is still open though I don't know with complete certainty. Roughly a year ago, Nordenstam told me it was still open; he has since left academic mathematics. I'd add, though, that several related proccesses, including Nordenstam's process and a couple of processes of Warren's, are treated in Borodin-Ferrari. It is possible that the convergence is proven implicitly there, or in a sequel. (Q2) Note that in Nordenstam's coordinates, one corner of the Aztec diamond stays fixed, which is not the way you want to scale things in the limit you want. But remember that the Aztec diamond is growing, too. So if you fix the center of the line of particles instead, then particles either jump a half unit "up" or a half unit "down" at each step. 

I'd be very surprised if one can do better than just reading part of the proof of the classification, which is a beautiful combinatorial argument already, in my opinion. For example, in the proof of the classification of irreducible root systems given in Fulton and Harris (of Theorem 21.11), part (iv) says that you can collapse a long string of nodes in the diagram if the endpoints are connected to any other nodes. This will inductively force diagrams with more than one set of multiple lines (i.e. non-right angles) to be quite small. 

Many computer-generated proofs use this technique. For instance, there is an entire euclidean geometry textbook written this way! It is available on Doron Zeilberger's web page. It is titled "Plane Geometry: an elementary textbook", and it is attributed thus: "By Shalosh B. Ekhad, XIV (Circa 2050), downloaded from the future by Doron Zeilberger". Zeilberger, it seems, always names his current computer Shalosh B. Ekhad and frequently cites them as his coauthors. $URL$ All the theorems are proven by computer-generated proofs, and they rely upon this property of polynomials. As such, the proofs are most easily read if one understands Maple's programming language, so whether they'd work as a good math education tool would strongly depend on your audience. okay, that was embarassing. These proofs actually do not use this mechanism at all, they rely instead on the correctness of Maple's symbolic algebra code (specifically the solve function). The proof I was thinking of was in section 1 of A=B, by Marko Petkovsek, Herbert Wilf and Doron Zeilberger; it's a proof that the angle bisectors of a triangle meet in a point. There are other, non-geometric theorems proved there using the same trick. The book is available online: $URL$ Remove those upvotes please, people... 

One glance at the third row will tell any combinatorist that these are Eulerian numbers (at least, for odd n). See sequence A008292 at oeis.org. Also, wikipedia has a perfectly reasonable page on the Eulerian numbers: $URL$ There you can find a recursive formula. I'll use E(n,m) since A is taken already: $E(n,m) = (n-m)E(n-1,m-1) + (m+1)E(n-1,m)$. Of course this notation is different than yours; I think your numbers are $E(2n+1, m-n)$, You should be able to see this by applying the above recursive formula twice and doing the above change of variables to recover your own formula, though I haven't done it and may have made an error. There's lots of formulas for the Eulerian numbers and there's a lot known about them. 

This is a partial answer only - but there is a clear place to start. The Cauchy-Binet theorem gives the answer as a sum of products of all maximal minors of the two matrices, where the minors are taken with matching sets of rows/columns. All of these minors are themselves ordinary, N by N Vandermonde determinants. So the answer is the symmetrization of the squared Vandermonde determinant, over all possible choices of N of the k variables. This is probably not simple enough for you, but at least it's a formula. I haven't assumed anything about the z variables, nor attempted to do any simplification. 

Edit: ok, now that I have more than 5 minutes to spare I can clean this up a bit and add a wikipedia reference. I'm going to write A(n,k) for $A_n(k)$. First of all, note that it's easy to see that A(n,k) = A(n,-k) by induction on n, and that the A(n,k) are zero unless -n <= k <= n. So we may as well just start computing these things (with dynamic programming, for good practice) before we start thinking terribly hard: Sage code: 

I doubt it... as far as I know, the RSK correspondence isn't very well-behaved on the set of Young tableaux that you need. This doesn't entirely rule out the possibility of other "nice" proofs, though with current technology, I think you'll need to do a determinant evaluation at some point for this particular problem. That's not necessarily a bad thing, as there are some rather astonishing modern ways of evaluating determinants, especially those coming from tiling problems, plane partitions and the like. I'd suggest you take a look at C. Krattenthaler's inspiring papers "Advanced Determinant Calculus", $URL$ and "Advanced Determinant Calculus: A Complement", $URL$ 

Depends on what you call enlightening. I've got a different viewpoint on this than most other mathematicians I've met. To prove that this matrix A is invertible, you should guess its inverse M explicitly, and then prove that AM=I. This is certainly enough to prove that it's invertible! It's also potentially enlightening (or at least interesting) because now you get to try and think of an interpretation for the elements of the inverse. Anyway, the point is that the guessing part is really, really easy in this instance, because there's an obvious structure in the inverse of the matrix. Here's the inverse for n=3, computed in sage: 

Pedantic answer: take $G$ to be a finitely presented group on $n+1$ generators $x_i$, with relations of the form $x_{n+1}^{-1}x_{\sigma(1)}...x_{\sigma(n)}$. Here, $\sigma$ ranges over your favorite set of $n!-k+1$ permutations. Take $g_i=x_i$. If you really want a finite group $G$, then I don't know. 

Not an expert, but you caught my interest. Looks like you may need to be a little careful in higher dimensions, but there are versions of the theorem which work. See, for example: $URL$