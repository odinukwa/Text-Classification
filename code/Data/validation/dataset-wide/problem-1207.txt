Not really, there'll be a minor performance hit but the simplified logic and compatibility more than make up for this. I believe the SELECT * gets replaced with a list of all fields in the table which the DB engine has to look up before it can run the SQL, this is where the performance hit comes from. However, I've never seen an noticeable performance decrease and in my opinion the simplified maintenance and compatibility make up for this. 

Arguments can be made either way depending on whether you want to handle populating these tables using triggers or the application(s). I think this depends on what exactly you want to store in these logging tables, if you're logging anything specific to your 3 "data" tables then you'd be better off with 3 logging tables (1 logging table per "data" table which will be easier to populate using triggers), otherwise you can store everything in 1 table (with 1 field to reference each data table) which will make reporting easier. As regards referential integrity, if you're actually deleting the original data record (rather than just marking it as deleted) then your Foreign Key will have nothing to link to anyway. This is one of these situations where I de-normalize to gain centralized reporting and application logic (though triggers could be used for this, depending what you're storing). 

As recommended by Mikael Eriksson, I'd do the import without dropping/disabling the Foreign Keys. The downside to this is if you want to delete any records who's values are referenced by an FK or want to update values such that the original value is no longer in the table and referenced by an FK. In these cases you'd need to keep track of any such issues and decide how to handle each "conflict" (don't update/delete, alter the data in the other table or some other solution). This will be quicker, easier and ensure data integrity compared to dropping/disabling the FKs, importing and then restoring the FKs. 

I'm looking to store GPS co-ordinates in an MS SQL Server DB for location matching of users with GPS-enabled mobile phones. I've read about STDistance on MSDN, but it only appears to exist in SQL Server 2008 (& later) along with the geometry and geography field types. However, I'll need to support older versions of SQL Server as well (at least 2005). What's the best way of storing GPS co-ordinates and calculating distances between 2 GPS locations in SQL Server 2005? 

This seems to be one of those "rules" that have come about to stop people reading an entire 400+ column tables into memory. In my experience, as long as the table is fairly small (in terms of columns) and you really do need all the data, let the highly-optimized DB take the hit while you concentrate on maintainability and compatibility and other benefits to yourself and your users. 

Yes, if you want to reuse an image between multiple entities and a single entity can have multiple images you'll need to make it a many-to-many relationship. And yes, you would want to create multiple associative tables for each many-to-many relationship. The only way around this would be to create one Entity table that all of the other tables pulled their IDs from, but I think what you currently have is easier to maintain going forward. 

No, you must create a table variable, insert the records into that table variable, then pass the table variable to the procedure. The way that people used to do what you are describing is with pipe delimited text strings. For example, passing '1|Tyler|Smith|2|Jack|Blade|3|Someother|Guy' and then having the stored procedure unpack it. But that methodology has completely fallen out of favor compared to using table valued parameters. 

One easy solution is to include all possible attributes as columns on the main clothes table, and make all of the brand specific columns nullable. This solution breaks database normalization, but is very easy to implement. 

I can only speak definitively regarding SQL Server, and it appears this is not consistent across all database implementations. But in SQL Server, functions may not produce side effects. This is a hard and fast rule I've tried to circumvent a number of times with no success. If you are thinking about functions in the general sense, there are SQL modules that allow side effects (for example stored procedures) but user-defined functions do not. There is a saying "Clever Solutions Don't Scale" which is especially true with Microsoft products. I've seen a number of clever workarounds in early versions of SQL Server that became obsolete in later versions, because MS added them as true features. The ones that never became features, honestly, never became features because they fundamentally broke some aspect of T-SQL development. Side effects in functions is one of them. 

If you can compact the SQL making these types of adjustments the SQL becomes a lot more readable, will fit on one page, and you'll be much more likely to see how to fix the functional parts. As for the functional parts, one red-flag that jumps out is that you are calling DISTINCT in scenarios where you are using a GROUP BY clause. Calling DISTINCT and GROUP BY in the same select is redundant, since GROUP BY will already ensure the results are distinct. I'm not sure if removing the DISTINCTs will speed up the query, but since they are redundant it is worth a shot. Also, the generic advice of "run your script in SSMS, turn on the actual execution plan and add any suggested missing indexes" is usually helpful in these situations, you could be missing an index, and if it is an important one you could get significant performance gains without much analysis. 

Both multiple files in a filegroup and multiple filegroups are tools for managing your database. The first lets you manage your IO and both will let you manage your backups. It is possible to backup a single file of a database as well as a single filegroup. Be sure to backup the tail of the transaction log when you do if you are planning on restoring it somewhere. Database files allow your multi-core CPUs to have multiple read/write streams to the database without hitting higher disk queuing values. It may help to think of the filegroup as a logical division and the file as a physical division. If you have multiple filegroups you will automatically have multiple files as a file can only belong to one filegroup. It can also help (if you have enough cores on your server) to have multiple files in your filegroups. That can give you the best of both worlds. You assign database objects to a filegroup not a file. You can put the files on different physical disk arrays. When I first started doing database work it was common knowledge that you put your data and log on separate disks. If you had the budget you would put your non-clustered indexes on another disk. It's tough to get that these days with SAN technology everywhere. However, SAN is a management tool not a performance tool. As you pointed out having different filegroups will allow you to isolate high traffic tables from each other and from lower traffic tables. It will also allow you a limited additional protection from a corrupted database potentially limiting data corruption to a smaller part of the data. 

What you're describing sounds like Microsoft Sql Server HADR commonly called AlwaysOn Availability Groups. It combines traditional Windows clustering with Database Mirroring to allow you a primary replica and up to 4 (Sql Server 2012) secondary replicas. Once set up correctly it will provide you with a single IP address and port # to use as a connection target for your application. Setting up the mirroring portion is a bit more than just deploying your database to the connection though there are ways to automate that also. The replicas may be geographically separated as well as residing on different sub-nets and domains. 

It sounds somewhat like you are hitting a bit of memory pressure. Dirty Buffers are pages in memory that have changes in them that have not been written to disk yet. Normally they would stay in memory until one of the various processes clean them up and/or write them to disk. There are some things you will want to look into but with only the information you have given here I have to ask, "How much ram have you reserved for the OS?" You are running a 64 bit version of Windows. Now, the 32 bit kernel will use 4 GB of RAM. However, we have found (the hard way) that 64 bit versions of the OS will use 8 - 12 GB of RAM dependent on how busy the Sql server is. By that I mean are you doing a lot of reading and writing to disk. If the OS can't allocate the memory it needs for this (because the Sql server has sucked it all up) you will see the CPU being consumed for the additional context switching it needs to do to write those buffers out to disk. When you added the additional RAM to the server did you reset the Sql Server Max Memory setting? When you only had 16 GB of RAM your Sql server was constrained as much as the windows kernel was and so you were likely using virtual disk (hard drive space) quite a bit. When you added more RAM you opened things up a bit and now the Sql server is attempting to keep more of the database(s) in memory. 

I would stay away from LOOP joins, or specifying any specific joins. The SQL Server query optimizer is very good at picking the best join method to use, and it will pick LOOP on its own if that is the right one. Also, if there is a missing non-clustered index that could improve the performance of the query, SQL Profiler will find it and suggest it to you. One thing that I noticed missing from your query is the use of common table expressions. I haven't had a pivot yet where I haven't needed to use common table expressions to get the data ready to pivot. I would rewrite your query using common table expressions (particularly the correlated subquery) and see if that improves performance. If it doesn't another thing you could consider doing is changing the order of columns in the clustered index on the tblRespostaINT to better support that query. Finally I believe that what you are doing in the inner select can be accomplished with a windowed function, and if it can that should be faster. 

Check the "On Success" for the last step of your job. It should say "Quit the Job Indicating Success" It won't close the window you used to start running the job, but it will commit the job history once all job steps are complete. If the On Success is already correct it seems likely that one of the steps never completes, but with newer versions of SSMS you should see all completed steps regardless of whether the entire job completes. 

Then you can check the value saved in the table against the value for the person changing the record. And no, this does not break database normalization. As a practice it is quite common to include metadata like AddTime, AddUserID, LastChangeTime, LastChangeUserID. In fact in some environments I have worked in, it was a requirement that all tables have these columns for audit-ability. 

You shouldn't have issues altering the columns to a lower size. You can only alter column types one at a time with ALTER TABLE statements anyway. While the total data file size will grow intermittently, it can't grow bigger than the original with corrected column types. So here are the steps I would take: 1) Ensure that there is a primary key, or at the very least a clustered index on the table. 2) Run the ALTER TABLE ALTER COLUMN statements one at a time 3) If the database runs out of space, run a DBCC SHRINKFILE on the appropriate file. 4) Continue with steps 2 and 3 until all columns are their new appropriate types 5) Rebuild all indexes on the database to remove the fragmentation caused by the previous steps. Very easy. I've also gone the route of creating a new table with correct definitions, copying the entirety of the existing table to the new one, dropping the original table and renaming the new one to the original. However to do this you'd need more space at least intermittently. You could try putting one of the SQL Server data files on a network drive in the interim, but I think just altering the existing columns as outlined above will be easier.