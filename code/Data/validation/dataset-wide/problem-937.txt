The observation of even a few exceptional storms can provide quantitative evidence for climate change. Doing so however requires observing and learning as much as possible about how storms work, and not merely counting storms. With some understanding of how atmospheric systems and storms operate, we have other observational information from physics, chemistry, and planetary science that we can also apply to the question. We should use all the information available. Bayes rule can help us do this objectively. Here, P(A|B) can be the posterior probability that the climate has changed (A), given the observation of the exceptional storm (B). P(A) is the prior probability for climate-change. P(B|A) is the likelihood that storm B occurs given a changed climate (e.g. warmer). k1 and k2 are constants of proportionality. Let (!A) represent no change of climate. Then we can write two equations for Bayes Rule. P(A|B) = k1 P(A) P(B|A) P(!A|B) = k2 P(!A) P(B|!A) The prior odds for a change in climate is P(A):P(!A), and let's assume the prior odds for and against climate change are even, 1:1. Now if we use all the available information we have about atmosphere physics and chemistry, and we observe storm B in detail, we can make an informed estimate of the ratio of likelihoods P(B|A):P(B|!A). Let's assume that B is an exceptional storm and ten times more likely to occur when the atmosphere is warmer. If a storm of type B is observed in actuality, the posterior odds can be applied, and the odds should be updated in favor of climate change to 10:1. What this means is that the observation of exceptional (extreme) events should inform our opinion on climate change. This approach is most successful however when we have many, and many types, of information on how the atmosphere and climate works. Measurements taken on a day during extreme weather will of course be outliers, but could also provide important information about how In a Warming World, Storms May Be Fewer but Stronger. We should not assume outliers always represent 'noise' that needs to be averaged away. It does not seem unreasonable to ask the question whether or not we are seeing effects of climate change in the weather. Thursday morning I read the following on the US National Weather Service forecast discussion page: CLIMATE...THERE IS A SMALL CHANCE THAT SEATTLE WILL GET TO 90 DEGREES ON SUNDAY WHICH WOULD TIE THE RECORD FOR THE DAY. SINCE RECORDS STARTED IN SEATTLE AT THE FEDERAL BUILDING DOWNTOWN IN 1891 THERE HAVE BEEN ONLY SIX DAYS IN THE FIRST WEEK OF JUNE WITH A HIGH TEMPERATURE OF 90 DEGREES OR MORE. THE LAST TIME IT HAPPENED WAS JUNE 4 2009 WITH A HIGH OF 91 DEGREES. FELTON Whether or not the temperature exceeds 90 degrees next Sunday, I wouldn't dismiss the question of what mechanisms might be operating out-of-hand. We should try to estimate how much what happens supports (or not) hypotheses based upon physical processes. For example, use Bayes rule reasoning to estimate the change in posterior odds for a mechanism A that increases the likelihood of P(B|A) and P(!B|!A) by 15%, and decreases the likelihood of P(!B|A) and P(B|!A) by 15%. $$\delta = 0.15$$ Then the likelihood is given by the following, where the record is exceeded for a years and not exceeded for b years. $$ k \times\left[ \frac{P(B \parallel A)}{P(B \parallel !A)} \right] ^{a}\times \left[ \frac{P(!B \parallel A)}{P(!B \parallel !A)} \right] ^{b} $$ $$ k \times\left[ \frac{1 + \delta}{1 - \delta} \right] ^{a-b} $$ Let's also look at the support if the record is also exceeded in 2016 and 2017. Change in posterior odds in favor of A(0.15) (a) Record Not Exceeded 2015 - Posterior odds decrease from prior odds by 35%. By this method it is possible that additional observations eventually discredit the hypothesis. (b) Record Exceeded in 2015 - Posterior odds increase by 35%. (c) Record Exceeded in 2015 & 2016 - Posterior odds increase by 83%. (d) Record Exceeded in 2015 & 2016 & 2017 - Odds increased by 148%. Finally, the advantages of using this approach, rather than a frequentist approach, can be more easily understood by considering how it could be applied in practice. For example, how a Penn Cove shellfish business might use these calculated changes of climate-change probability to self-insure their farm. The owner of a shellfish farm may understand that climate change poses a risk to her business, and has hedged for the cost of the odd bad year due to this by putting an extra 100 dollars into an account each month. She has found this has worked well in the past, with the account growing to be large enough to cover costs in bad years, without ballooning too large. How might she use the information that Seattle is breaking temperature records (and the probability of A may be changing) to adjust this amount? If the temperature record is exceeded in 2015, she may decide to increase the amount to 135 dollars per month, and if the record is exceeded again in 2016 she may decide to increase it to 183 dollars, and if it is exceeded again in 2017 increase it to 248 dollars. The advantage is the Bayes method helps her make a decision to act sooner than by using a frequentist approach. This way she may be able to prepare for future costs. 

I think the greatest impact may be upon the careers of the investigators on the project. Decades of work can be invested in the planning and building of a submersible or space probe, it an inherently risky undertaking, if it gets destroyed in the process, who takes up the job of learning from the experience and trying again? Hopefully, institutions support people through the inevitable setbacks, but sometimes not. The instrument itself is relatively easy to replace, but the expertise to use it properly is not so easy to come by. We wish all the persons involved the best. 

References Machol, J. L., J. C. Green, R. J. Redmon, R. A. Viereck, and P. T. Newell (2012), Evaluation of OVATION Prime as a forecast model for visible aurorae, Space Weather, 10, S03005, doi:10.1029/2011SW000746. Newell, P. T., T. Sotirelis, and S. Wing (2010a), Seasonal variations in diffuse, monoenergetic, and broadband aurora, J. Geophys. Res., 115, A03216, doi:10.1029/2009JA014805. 

It is cristobalite, the main alteration product of obsidian. Obsidian, a siliceous glass, is only metastable — all known obsidian is younger than the Cretaceous. Over time, it devitrifies, forming cristobalite, a polymorph of silica. So there's no change in chemistry, just in the arrangement of atoms in the mineral. It's not an overgrowth or a deposition. Quoting Jim Miller at Oregon State: 

SEG-D is a specialized format, while SEG-Y is a general-purpose format. In general, SEG-D is intended for field recordings of seismic data, and SEG-Y is intended for 'seismic data exchange'. Having said this, SEG-Y is so general-purpose, and so ubiquitous, that I'm not surprised to hear (anecdotally) that people are using it for data acquisition. Certainly, the recent release of Revision 2 — which allows for high sample rates, variable trace lengths, long records, and large files — facilitates this. As an interpreter, I use SEG-Y all the time, whereas I've never come into contact with SEG-D. Thus, I don't know enough about it to go into any detail, but a quick look over the SEG-D specification suggests it has better support for describing receivers, channels, sources, and including things like observer logs. Which is what you'd expect in an acquisition format. SEG's formats are notoriously, er, flexible. So converting directly between the two formats may or may not be possible, depending on which fields have been used, and the lengths to which you're prepared to go to 'adapt' the target format to your use case. The usual way to convert, as it were, would be to process (i.e. demux, sort, image, and maybe stack) the field data in the SEG-D file(s), and write out one or more SEG-Y files. Good luck! References 

It's the same story at an active margin: the plate boundary at a subduction zone is buried several kilometres beneath a wedge of sediment: 

The building material comes from minerals dissolved in the ocean, mainly $\mathrm{Ca}^{2+}$, $\mathrm{Mg}^{2+}$, and $\mathrm{HCO}_3^{–}$, but of course there are all sorts of biochemical details. Most limestone originated as the skeletons of micro- and macro-organisms, such as plankton and coral. The minerals come in turn from the erosion of older rocks (among other places). For example, here are the famous white cliffs near Dover, UK, where massive amounts of calcium carbonate, mostly stable since the Cretaceous, are falling into the sea, where it will be re-used: 

It depends on the implementation, but bare-bones reverse time migration is usually not amplitude friendly. The problem is that the ideal imaging condition — deconvolution — is difficult to apply or unstable in the time domain. So cross-correlation is used instead, and this loses the relative amplitude information... so amplitudes are no longer necessarily related to reflection coefficients. Since this is the assumption of inversion (e.g. AVO inversion), an RTM volume could give spurious results. Several vendors have devised ways to compensate for this. For example, Zhang & Sun (2009; First Break, vol 26) described one of CGG's methods. Zhang et al. have written more recently (2013; EAGE London) about how this processing has evolved. Cogan et al (2011; SEG Annual Meeting) have written about Schlumberger's normalization algorithms, which aim to achieve the same goal. Those papers also contain nice explanations of why RTM is not ordinarily amplitude friendly. My advice is to talk at length to your vendor about the processing applied, and the attention paid to phase and amplitude. Ask lots of questions. Most processors (generalizing horribly) are either over-enthusiatic about treatments, especially new and/or proprietary ones, or ignorant of your needs as an interpreter–analyst. So if you're actually in the middle of processing now, test everything, ideally against geological data (like a synthetic). Lots of processing steps can hurt amplitudes, and inversion can work in surprisingly poor data... so assume nothing, and never give up! 

So it's not really that "when using RMS amplitude one must integrate over a window", more that if you want to measure amplitude over a window, you must use RMS. Because the values are squared, RMS has two other features (or bugs, depending on your point of view): large values are emphasized, and noise may therefore be emphasized. An alternative is to use the envelope of the trace (sometimes also called 'energy' or 'absolute amplitude'), which is the magnitude of the Hilbert transform (also called 'complex trace'). It is always positive and has the added benefit of being phase-independent. Read more about envelope.. Which window to use? Let the expected geology and basic geophysics and statistics guide all of your decisions. Use a window that captures the interval of interest (look at the wells!) without too much non-interesting stratigraphy. But use a big enough window that you don't see a lot of artifacts from large amplitude values coming in and out of the window (this also depends on the quality of the horizons and smoothness of the geology). Windows don't have to be symmetrical about a horizon. It depends where the features you're interested in are. In my experience, stratigraphic windows are often useful — from one horizon to the next. If you're interested in the horizon itself, consider just using its amplitude directly. If it's too noisy, try improving the pick or smoothing the amplitude map, before confusing things by throwing more geology in there. What does that caption mean? Who knows what that caption means? Vague captions plague the geophysics literature. Don't be part of the problem! If only for the sake of your future self, record the exact statistic and its parameters on every image — even put it all in the filename. Remember what you're trying to do You're trying to relate the seismic to reservoir properties you care about. This means proving that the property is related to the seismic in as quantitative way as possible. A nice way to do this is to crossplot the property with the seismic attribute — that way you also know the error of the estimate. A map showing wiggly channels is interesting, but nowhere near enough. I tend not to believe attribute analyses that don't include a crossplot.