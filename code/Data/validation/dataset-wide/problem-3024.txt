There may be a connection between model theory and type theory if you dig enough, but I don't see the analogy between assigning types to variables and assigning objects to variables. My knowledge of type theory is a bit fragmented, but I would say that, if anything, type theory has closer ties to computability theory or proof theory than it does to model theory. For one thing, types/terms can be systematically combined in meaningful ways to produce other types/terms, whereas objects in model theory don't combine in this way. Moreover, in model theory, the focus is on models, and which theories are satisfied in which models; it's more concerned with worldly structure, you might say, and less concerned about the particulars of the language (though which language a model is in certainly matters). By contrast, type theory is more concerned with the particulars of the language, and less concerned with the way the world is set up. That's a very rough (and possibly misleading) characterization, but for instance, in model theory, you might be concerned about determining what the models of the theory of algebraically-closed fields or the theory of divisible ordered groups look like (are there saturated models, prime models, homogeneous models, etc.) or what properties these theories have (are they complete, decidable, categorical, do they have quantifier elimination, etc.). But you wouldn't bother yourself with proof systems in model theory, and in fact you can easily get in model theory by without ever mentioning them! By contrast, type theory might be concerned with naming proofs and showing how to systematically code proofs into terms of a certain type. You wouldn't need to talk about the way the world is, in type theory, but you would need to talk about syntactic functions. It can also be confusing, because model theory uses the word "type" in a very different unrelated sense from type theory. A type over a parameter set X with respect to a theory T is a set of formulae consistent with T which may use members of X as parameters in the formulae. It then becomes an interesting question in model theory whether or not there are models of T that omit certain "types", or how many types there are with respect to T, or in what way the different types with respect to T are related, etc. 

With regards to the bracket notation, you're basically correct. It's easiest to state the idea as follows. Let's say, for concreteness, we're dealing with a sentence "A or not-B". This is logically equivalent to "not-(not-A and B)" and "if B, then A" (at least in classical logic). But all three of these are distinct sentences (one's a disjunction, another's a negation of a conjunction, another a conditional). The notation "[A]" or sometimes "[[A]]" can be thought of as conveniently denoting the "propositional content" of A (which can be made mathematically more precise). For instance, in our original example, [A or not-B] = [not-(not-A and B)] = [if B, then A]. (I don't think there's a significant difference between single brackets and double brackets; as far as I know, it's merely notational preference.) For instance, in the context of modal logic, [A] can denote the set of states in some model at which A is true. In the context of lattices or algebraic logic, [A] can be thought of as a point on a lattice which A (and all its logical equivalents) denotes. Corner quotes, on the other hand, are more syntactic in nature. They're most often seen in the context of nonstandard models of PA, set theory, or whenever you're dealing with systems which are powerful enough to code sentences of their language (as you sometimes see, for instance, in debates about the Liar's paradox). Corner quotes placed around a proposition form a term in the language you're dealing with, and hence can appear and be referred to in formulae directly. Usually, though, corner quotes around logically equivalent propositions will not denote the same terms. They're also sometimes used in order to properly account for the use-mention distinction with complex formulae, though this is more of a philosophical device/concern than a mathematical one. (See John MacFarlane's handout on Substitutional Quantifiers for an explanation.) 

I won't repeat the previous answers, but I imagine the quip with (b), i.e. "I won't buy both the pants and not the shirt" is that it's very common for beginning students in logic to simply (and thoughtlessly) replace every wedge with "and" and every negation with "not". But this doesn't always yield the best (or even good) translations from formal logic to English, and most authors want to emphasize you can't just do that. 

Kripke hesitates a little bit when it comes to fictional entities. The issue partially boils down to the following question: "Could fictional entities like Sherlock Holmes and Mickey Mouse exist?" On the one hand, you might think "Yeah, of course! Sherlock Holmes isn't contradictory or anything; surely there's a possible world where he could have existed." This was something like Kripke's old view. On the other hand, you might think, "No, Sherlock Holmes is a purely ficitonal character. Even if we were to discover that there was a man who actually did all the same things that Holmes did, and was even called Holmes, but (let's say) was completely unknown to Doyle, still the Sherlock Holmes of Doyle's stories would not be that person. Hence no possible person could be that Sherlock Holmes." This is more like Kripke's revised view. In Naming and Necessity, Kripke has the following quote: 

The idea is that, in set theory, one is developing a formal language to talk about sets. In particular, one is quantifying over sets in set theory. Second-order logic, when it introduces predicate variables, seems to be doing something very similar, since the interpretation of these predicates is meant to be a set. You can't quantify over sets of sets in second-order logic, but as you ascend into higher and higher orders, you come closer and closer to set theory. Quine's objection, then, is that second-order "logic" isn't a logic at all: it's a restricted theory of sets. If you want to learn more about these things, you really ought to read Shapiro's book Foundations without Foundationalism, which is a fantastic introductory source for both the technical and philosophical material regarding second-order logic. I would also suggest looking at Quine's thoughts on these things in Philosophy of Logic. Boolos also has some interesting articles defending (monadic) second-order logic philosophically as a kind of plural logic (i.e. a logic with plural terms). With regard to using second-order logic in mathematics, you might want to look into the field of reverse mathematics which deals a lot with second-order PA (I don't know much about it myself). The issue of finding proofs that use second-order logic is a bit tricky, as I understand it, since when you do want to use second-order logic, you can often just get away with using first-order set theory. 

The cool thing about SEP is not just that it has nice summary articles, but is also generally has comprehensive bibliographies for those interested in learning more. You should probably look through some of those. In particular, just scanning through some of them, there's Casta√±eda, "The Paradoxes of Deontic Logic: The Simplest Solution to All of Them in One Fell Swoop" (1981), which might sound promising. Also, I've heard very good things about Hilpinen's Deontic Logic: Introductory and Systematic Readings as an introduction to the subject, so it might contain some such list. 

Henkin semantics is equivalent to first-order logic + comprehension schema in expressive power. So if there's any reason to accept Henkin semantics over first-order logic, it will be insofar as one takes the comprehension schema to be logical truths. There's a subtlety in the interpretation of these schema, viz. the quantified predicate variables are better thought of as sets as opposed to properties. So for instance, the sentence "Beethoven has all the properties that make for a great musician," if schematized in second-order logic, would say (roughly) "Beethoven is in every set that every great musician is in", or equivalently "Beethoven is in the set of great musicians," which doesn't quite have the intended meaning you suggested. Perhaps this may make the comprehension schema look more plausible. But even if they're true, that doesn't yet mean they are logically true. Similarly, "Beethoven has all the properties that make for a great musician" doesn't yet give us reason to adopt Henkin semantics over, say, full semantics. For while it's natural to assume these properties are human properties (technically, the quantifiers range over the power set of the set of humans), that may not be something which is determined by our logic. If anything, our logic should be neutral to whether or not these properties being referred to are ones that only apply to humans (surely it's logically possible for the moon to sing, right?). But now, for comprehension schema, if they're true, are they true by virtue of logic? That will depend on your view regarding more fundamental philosophical issues. 

This sounds sort of like Kolondy & MacFarlane's "Ifs and Oughts" (though it's not quite FD). They present a scenario where ten miners are trapped in either shaft A or shaft B (we don't know which), and flood waters might drown them. We can either block shaft A, block shaft B, or do neither. If we block the right shaft, all miners are safe. If we block the wrong shaft, all miners drown. If we do neither, one miner dies. In this case, it seems we have the following: (0) (Given) The miners are either in shaft A or shaft B. (1) If the miners are in shaft A, we should block shaft A. (2) If the miners are in shaft B, we should block shaft B. (3) We shouldn't block either shaft. It's not quite FD, but it's closely related. You might want to read that paper for more details (though to be honest, SEP is not a bad place for references either; and those references most likely have more references). In fact, you could modify the example and just stipulate that the miner's are in shaft A. Then by FD, you'd get that we should block shaft A. But if we don't know they're in shaft A, that seems somewhat implausible. So you could probably generate failures to FD just by introducing epistemic elements of this sort. (Again, I'd suggest reading Kolondy & MacFarlane's paper to see how they treat this).