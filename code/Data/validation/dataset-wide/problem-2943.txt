Rationale is developed via our subjective collection of causes and effects over the course of our lifetime. If someone suspects based on their experience that what we are doing will be negative, it is possible they will claim you are being irrational. This may be due to a lack of empathy, as there is a tendency for people to assume that others should have the same conclusions as they do, despite no two people living identical lives, and so each drawing their own conclusions about what is a risk and what is not. The same would apply to being illogical, and when this is stated it is common for the person being criticized to explain why - to them - it is entirely logical. The question of whose logic is correct is a tricky one, as even the most logical and intelligent person in the world may not have the correct set of data to reach a more optimal conclusion than the least intelligent person, who happens to have experienced something which massively increases the odds of their conclusion being optimal. Person A likely does mean that they believe A has a 75% chance for success, whilst B has only a 25% chance for success, but this is not knowledge, it is prediction, and that is notoriously subjective. To answer the question in the title - it depends on the value placed on the results of the risk, and the consequences of things going badly. A rational risk can be taken, if the potential positives outweigh the potential negatives, but that is down to the individual - it's also entirely possible to take irrational risks. 

It's difficult to answer your question about postmodernism, for a couple of reasons. First, the term "postmodernism" is used extremely equivocally: sometimes it refers strictly to a movement of late twentieth-century French philosophers (Derrida, Deleuze, Lyotard, Irigaray); sometimes it also includes Heidegger, Nietzsche, Freud, certain readings of Marx and Hegel, or any feminist philosopher. So it's not clear whose views of science you're asking about. Second, even the strict-sense postmodernists had very different views on science. Deleuze saw himself as providing a new metaphysics for science, following Bergson; when Lyotard rejects "metanarratives" in The Postmodern Condition, he's primarily targeting science. If I recall correctly, Gary Gutting's French Philosophy in the Twentieth Century discusses both of their views (and has a great chapter on prewar French philosophy of science). The view that the value of science comes from the way it lets us do things is historically associated with Francis Bacon. If anything Bacon was just barely pre-modern! In the nineteenth and twentieth century, it's associated with logical empiricism — especially Otto Neurath — and American pragmatism — especially John Dewey. For a discussion of the relationship between logical empiricism and pragmatism, check out George Reisch's How the Cold War Transformed Philosophy of Science. For work by contemporary philosophers of science, I recommend Nancy Cartwright's "Well-Ordered Science: Evidence for Use" and Matt Brown's book-in-progress. 

I would prefer to say that "comes in clear denominations" supports "is easier to count" which in turn supports "is convenient as a form of money". You might like to start from the conclusion and work backwards by asking, what reason is there to accept this? Why are cigarettes used as money? They are convenient for this purpose. Why? They are easy to count. Why? They come in clear denominations. As Aristotle observed, what we desire in a form of money is that it should serve as a medium of exchange, a unit of account, and a store of value. To this end, it needs to be portable, durable, divisible, fungible and have some intrinsic value. Cigarettes do a fairly good job of meeting these requirements. 

I think we should understand Occam's razor as being a methodological principle rather than an epistemological one. What I mean is this: given a bunch of data and two candidate explanations, would I prefer the simpler explanation? Yes. Simpler is better; I don't want my explanations to be more complex than they need to be. But does this mean that the simpler explanation is more likely to be correct? Not necessarily. Plenty of simple explanations turn out to be false in the light of further data. We are still a long way from having a theory that accounts for everything in the universe. Our knowledge is provisional and fallible, and as it grows we may well come to find that our simple explanations are wrong and more complex ones are needed. As such, if someone offers me an explanation and urges that it is desirable because it is simple, that's fine. But if they try to argue that it is more likely to be true because it is simple, this is dubious to say the least. 

Your question is unclear. You're dissatisfied with "causes necessitate effects" and "all effects have a cause," but you don't explain why. You do point to some problems that concern you ("two effects of the same cause," "two causes of the same effect," something like correlation vs. causation), though you present them so briefly that it's hard to tell what you already know about the topic or how you're thinking about these problems. (It's also not clear to me what they have to do with "causes necessitate effects" and "all effects have a cause.") Since it's not clear what you're asking, it's hard to give a good answer. So let me recommend a few major ideas on causation from academic philosophy. Maybe doing some more reading on the topic will help you refine your question. First, Mackie's "Causes and Conditions". Mackie proposed that causes can be understood as INUS conditions: insufficient but necessary conditions for some sufficient but not necessary condition for the effect. For example, a spark by itself won't light a candle — there might not be oxygen in the air to burn. So the spark is insufficient to light the candle; but it is necessary for a complex condition (the presence of both the spark and oxygen close to the candle wick, sheltered from the wind, etc.) that is sufficient for the candle to light. However, this complex condition is not necessary for the candle to light: some other complex (involving touching the wick with an already-burning flame rather than a spark, for example) would also be sufficient. Mackie's account takes care of one of the problems with the idea that "causes necessitate effects"; namely, that sometimes you can have the cause without the effect. Again, if there's no oxygen, the spark won't light the candle. Mackie's account also takes care of cases where there are two (or more) causes of the same effect: both oxygen and the spark are causes of the candle lighting. Another major innovation is probabilistic causation. This is another way to deal with the problem of causes that don't necessitate effects. Consider smoking and lung cancer. Not everyone who smokes develops lung cancer; but smoking is associated with a large increase in probability or risk of developing lung cancer. Roughly, according to probabilistic theories of causation, X is a cause of Y if and only if X increases the probability of Y. A third important idea is interventionist or manipulationist accounts of causation. These accounts are designed to tackle the problem of distinguishing correlation and causation. Suppose X and Y are correlated. Basically, if (a) when we manipulate or change the value of X, Y also changes value; and (b) when we manipulate or change the value of Y, X does not change value; then we can say that X causes Y. This is the basic logic of a scientific experiment: we change the value of one variable, and look to see if there are also changes in the other variable. 

QM being unpredictable is likely due to the limits of human understanding, and so using it as the lynchpin in any philosophical discussion would be the same as using the sun disappearing and reappearing to confirm the existence of a higher power. The case for free will hinges upon the human mind being able to deviate from the otherwise seemingly deterministic nature of the universe, and quantum mechanics is a nice get out clause for that, however then we have to argue that the human brain can affect quantum mechanics, whilst being driven by something other than quantum mechanics, otherwise it is just QM affecting itself, and leaves us no more free than in a deterministic universe. 

Knowing is speculation distilled into certainty, via a process of inferred causal relationships and confirmation bias. 'knowing' one thing will encourage us to 'know' other things of a similar ilk due to their close proximity and seeming relationship, but it is worth remembering that at the root of all knowledge is faith in something, rendering all facts merely educated guesses that are pragmatically useful at any given time. This inability to forget knowledge - as you define it - is potentially unhealthy, as it can lead to a shock to the system if previously assumed facts are shown to be false, and so I believe it is always worth remembering that certainty does not imply objectivity, only a desire for such to be the case. Of course, I could be wrong, but pragmatically speaking I am making educated guesses based on my observations to date. 

Making sense of conditionals has spawned an enormous literature and there is still no generally accepted account of their meaning. 

I rather like this exercise. You are right to say it is abstract and difficult, but that is why it makes for a good test. .1 This is OK. As you say, it is an explicit premise. It is also scientifically dubious since what is sent from the mountain to us are photons and it is only when these impact the eye and are processed by the visual cortex that an image forms. The photons don't shrink. Nor is it possible to speak sensibly of the size of the image in comparison to the size of the mountain. The mountain has dimensions that might be measured in metres; the image can only be measured as a solid angle in our visual field. .2 The argument does not say "inaccurate", and I'm not convinced this is really implicit. .3 I don't think this is implied in any way. I would replace your 2 and 3 with .2' If the image we have of a mountain is smaller than the mountain itself then there is no way to judge the size of the mountain. Implicit premise. Also false, because as noted above, one cannot compare things and images like this. .3' If the mountain sends us an image, we would be unable to judge its size. This is your 4, but expressed as a conditional. This follows from 1 and 2' by hypothetical syllogism. .4' We are able to judge the size of a mountain by sight. Implicit premise. Also false. We can't judge the size of anything by sight unless we know how far away it is. .4A The mountain does not send us an image. This is your 6. I prefer it here because it follows from 3' and 4' by rule of modus tollens. .5 This is OK. As you say, explicit premise. Potentially dubious because it may be a false dichotomy: are we sure there are no other explanations of perception? .7 Again, OK. This follows from 5 and 4A by disjunctive syllogism. In other words, our 'unpacked' version of the argument shows that it is valid but proceeds from at least three false premises. 

We shouldn't put peer reviewed studies on a pedestal. The replication crisis unfolding in biomedical research, neuroscience, psychology, and other fields is largely about researchers trying to game the peer review system. At the same time, researchers in physics, biology, and social science frequently post working drafts on preprint sites, such as $URL$ $URL$ and $URL$ Some of these sites are event experimenting with postpublication review, where reviewers post comments and recommend changes after the paper has gone public. (More here: $URL$ In general, "researchers at university X found that ..." is going to overstate things, unless the "..." is about the particular observations that were made: "researchers at university X found that 7 of 13 participants preferred chocolate with peanut butter instead of jelly." Generalizations should be supported by several studies, preferably done by different groups of researchers over a period of time. I would recommend that journalists emphasize methods and context rather than findings. In other words, first, science journalists should help readers understand how a study was done. Was this an experiment, a field study, or based on historical data? What techniques were used to produce the data, and how were the data analyzed? What assumptions did the study rely on? What limitations did it have? Second, science journalists should help readers understand the scientific and social significance of the study. What other research has been done in the area? Does this study conflict or agree with that research? What disagreements do researchers in the field have? What arguments do they give for their views? Does the study have immediate commercial, policy, clinical, or social implications, or are the implications of the research more vague or long-term? (Very little "cancer research" is designed to directly produce new treatments.) If the research is socially controversial, what other factors ("besides the science") are contributing to the controversy? 

Quine's attack on the analytic/synthetic distinction is contained in a series of papers: Truth by Convention, Two Dogmas of Empiricism, Carnap on Logical Truth, and in the early chapters of Word and Object. In broad brush terms he argues: 

I'm not sure how a B-theory would help. Inductive reasoning can only be based on the information I possess. Whether I regard future events and observations as non-existent until they happen, or existing in reality on a par with the past and present doesn't affect the fact that some information is known to me and some not. The problem of induction is am I justified in projecting generalisations about what I know across to cases that I haven't observed? To take the example of the swans, on a B-theory we might say that my life is a squiggly timeline in a block universe and there are a bunch of timelines for all the swans that exist. At various points these lines intersect and I make an observation about a swan. But I can't move myself outside the universe and see all the swans. I can only make inductive inferences about the swans I have intersected with and then make projections about other swans that I haven't. As to whether this projection is justified, the law of large numbers helps here. Suppose we have an urn with 3 million marbles in it, 1 million each of red, green and blue. Now suppose I draw a random sample of 1000 marbles from it. A large majority of such possible samples will be approximately one third red, one third green and one third blue. So from any given sample, it is reasonable to suppose that the distribution of the population of the urn is similar to my sample. Another way to express this is that I am not assuming that my sample is representative, I am only assuming it is random, and the law of large numbers is carrying me from random to (probably) representative. Of course my sample might fail to be random. For a long time Europeans thought all swans were white and then they visited Australia and found black ones - in other words they discovered that their previous samples were geographically biased. In practice things are never simple. We need to worry about how big our samples are and how to avoid bias, and how our projections fit in with other things we know: in other words all the things that statisticians worry about all the time. But the inductive skeptic is engaging in an absurdity if he claims that observed samples provide no reliable information at all about unobserved ones. 

He's stating that if we are capable of making decisions, determinism cannot be true, and that if determinism is true we are incapable of making decisions. This is a clear link between determinism and free will - the two suppositions cannot exist together without contradicting each other. 

Every mind would be imprisoned if we could take hallucinogens freely, as well, it would just be a cell with a TV. 

The first one states that it isn't possible to go beyond what is possible, ie. You cannot create people without them adding to the maximum definition of how many could be made. You could call this the 'blatantly obvious' rule, if there are 100 people then having 100 people must be possible. The second one states that you could be in an environment where 100 people can survive indefinitely, and that is how many you could have there 'for very long'. You can go over this number, but you will not be able to remain above it forever. Think of a spaceship creating enough water for 100 people. You could put 200 people on there, but it would eventually decrease to 100 before stabilizing. 

Later economists added a fourth function, as a standard of deferred payments. As to the form of money, Aristotle noted that while different materials have been used, metals seem well suited for the purpose. He identified that money should be durable, divisible, fungible, portable and have intrinsic value. Gold and especially silver have proved popular choices for money over the centuries because they meet these requirements pretty well. As to the other things you mention that could be money... Cows are not durable (they die), not divisible (two halves of a cow are much less valuable than a whole cow), are not fungible (my cow and yours may not be the same weight, or equally healthy, or equally fertile). Diamonds are not easily divisible and are not fungible (every diamond is slightly different, so you would need to weigh them and examine them for flaws, clarity, etc.) Many other things have been used as money, including sea shells, wooden sticks and chocolate. Today the world almost exclusively uses fiat currencies, which have no intrinsic value and are only considered valuable because governments enact laws that require that they can be used to settle debts.