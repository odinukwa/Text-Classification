Yes, one example is Noam Chomsky's Poverty of Stimulus thesis in psychological linguistics, which suggests that the data language learners have access to about grammar underdetermined those general rules. Chomsky uses this claim as a premise for defending the idea that we must be born with mental structures for handling language, since otherwise we would not be able to do it as well as we can. 

That said, much of the best work in current philosophy of biology—which is currently the largest part of philosophy of science—arises from intimate familiarity with current biology. Note that this is a hard thing to achieve! We might even expect the results to be uneven. It is daunting, as a graduate student, to try to develop expertise at current philosophy, the history of philosophy, general philosophy of science and its history, philosophy of some particular scientific field, some of the history of that field, some of the History of Science literature on that field, and the current state of knowledge in that scientific field. Not easy. Here are some examples from the last twenty years of philosophers of biology contributing to biological knowledge: 

Peter Singer's One World: The Ethics of Globalization (Worldcat link) is a good, brief and thoughtful read on this topic. It discusses humanitarian intervention and moral responsibility and the challenges of those in an interconnected world. 

Yes, you might usefully think of the set of problems in the domain of philosophy as the set of problems which we can't resolve empirically, by observation. However, this allows that many questions which we could not research empirically have come to be able to be researched this way. "What is matter?", for instance, has shifted over time from being a largely philosophical question to mostly in the domain of physics, because we can learn so much about it using the methods of physics. Over time, as science has improved, many questions have shifted mostly or entirely out of the realm of philosophy over to science. Of course, science has generate new philosophical problems, too! Any way of roughly marking out the current domain of philosophy should also take into consideration that philosophers often refer to empirical results and theory when discussing problems. A huge portion of contemporary Philosophy of Mind, for instance, is engaged with neuroscience. But, as of now, philosophers of mind have questions neuroscience doesn't have the tools to answer. 

The word “contingent” in broader usage means subject to chance or depending on the way things are. However, philosophers use the word “contingent” and “necessary” specifically to describe ways of being or existing (for both things and properties). Something exists contingently if and only if it does not exist necessarily. Something exists necessarily if and only if it must exist—if it cannot fail to exist. Something necessarily has a property if and only if it must have that property and cannot fail to. So, in contrast, something exists contingently if and only if it can fail to exist, under some circumstances. Philosophers say of such things that they are contingent. The term implies that the things existing or failing to exist depends on other things, on circumstances, or the like. If nothing is contingent, then everything is necessary, meaning that everything that exists must necessarily exist and could not have failed to exist. This is equivalent to the thesis called “determinism.” (In turn the thesis of determinism is often thought to have the implication that actual free choices are impossible—that there is, that is to say, no free will.) If you are interested in learning more about Determinism, you might try the SEP article on causal determinism. 

You ask what the implications are for formal modal logic of there being differences among languages in what modal terms they have — or differences with respect to terms we might treat as modal, since it depends on how you define modality. There are no such implications, because the question mixes together two things which logicians keep separate. Terms like "must" and "should" are not operators in modal logic. The strong and weak modal operators of typical modal logic are defined formally, in relation to other symbols. "Must" and "should" are part of interpretations sometimes assigned to these formal operators for particular purposes. It is only in interpretations that the modal operators are connected with words in natural languages. And no interpretation is part of the formal definition of the modal operators. 

We can feel comforted by the existence of truths which are not merely subjective because the alternative that there are none is quite uncomfortable. Were there no truths, and nothing other than subjective perceptions, we should find ourselves with at least these two very uncomfortable thoughts: 

No, and we should not want it to be falsifiable, nor expect it to be on its own terms. Philosophers of science generally — though not universally — dispute the idea that there is a scientific method, as opposed to many scientific methods. But leaving that point aside, if there were a single scientific method ... How could a scientific method be falsifiable, and why should it be? To be falsifiable it would need to make a lot of predictions (or retrodictions) about what we should observe. I don't think any scientific-method candidate itself makes predictions. Let's take Bayesianism as an example. It offers a method for updating beliefs in light of new evidence. What does it say we should observe? Nothing. It is silent about that. Indeed why would we want a method to make predictions? It is a category mistake to want this from a method. Recipes for cooking pie or soup don't generally make predictions (or if they do, it's not essential to what they are). They offer advice about what to do. It may be right that the scientific method "advocates the use of falsifiable hypotheses," (though theoretical science involves hypotheses not open to falsification). But if so, it advocates using them as scientific hypotheses, not as exhausting the kinds of useful or meaningful sentences more generally. In Popper's case in particular, he did not describe falsifiability as a criterion of meaning — not even in the narrow sense of "cognitive significance" used by some positivists. So, no scientific method, including any form of falsificationism fails to be meaningful, or could fail to be meaningful, by virtue of unfalsifiability. So, in short, no candidate for being a scientific method is likely to be falsifiable, and I can't think of any reason that should worry anyone. 

Alexander Rosenberg, “Hume and the Philosophy of Science” in Norton, ed. The Cambridge Companion to Hume. Cambridge UP, 1993. Describes the theory with useful intellectual context. 

Sophie's World: A Novel About the History of Philosophy (Worldcat link) might be better for an 11 or 12 year-old, but is worth mentioning. It follows a 14 year-old girl who starts wondering about philosophical questions and engages with a philosophy teacher to discuss in an accessible way ideas from early modern philosophy. 

One way to put the difference is that Emotivism and Quasi-Realism differ in how a reasonable person should interpret the moral claims of others, like "stealing is wrong." If Jill says "stealing is wrong," a reasonable person should, according to: 

The Hackett English editions/translations of Plato are reliably good ones, and some of the best presented. Their edition of Phaedrus, translated by Nehemas and Woodruff, is no exception. It doesn't offer line-by-line commentary, but contains an introduction, outline, footnotes, and a couple essays at the end. I think you won't have trouble reading it with those aids. 

No, it's not. The antecedent of the third line affirms the consequent of the second, and so doesn't have any implications concerning the first line. 

For Kuhn the demarcation between science and non-science is institutional. It parallels institutional theories in other areas of philosophy, like aesthetics. In short, science is what is undertaken by the body of workers called scientists—especially professional scientists. The question of whether something is or isn't science is a matter of whether it's part of the human institution of science. An institution in this sense is collection of things in the world, typically recognizable to insiders and outsiders. Among the examples of institutions in this sense are a country's legal system and higher education. We can recognize the institution of science as collection of people, organizations, activities, and events, inputs, and outputs, including things like researchers, laboratories, journals, books, courses, experiments, techniques, specialized terminology, theories, hypotheses, bodies of data, physical tools, field-stations, articles, and so forth. Defining science institutionally of course stands in contrast to other preceding philosophers' efforts to define science in terms of its internal characteristics like verifiability (Logical Empiricists) or falsifiability (Popper). Kuhn would not argue this way. At the same time, he would not allow that anything goes as science, because he thinks the institution of science approaches the world with certain values that rule out some activities or claims as unscientific. One of the characteristics of the institution of science is normal science, and normal science involves puzzle-solving. Puzzle-solving is what you may be remembering as the typical activity of the institution of science. It is the characteristic activity of normal science, on Kuhn's view. Not all science is a matter of puzzle-solving, though, since some scientific activity is revolutionary—reevaluating the basic assumptions about what the major questions and puzzles for the discipline are. 

Whether it's useful to start with a paraphrase or the original text of historical philosophical works depends entirely, I am sorry to say, on your goals and your background. More helpfully, however: if you are a relative newcomer to Philosophy or to an area or period of Philosophy, it is usually preferable to start with an overview. Philosophers generally write to participate in a larger conversation, and generally speaking you will miss a lot if you don't have some sense of how that conversation has gone before the point when they enter it. Of course, there are goals for which I would recommend the opposite. For example: If you have as a goal understanding what Jeremy Bentham meant by "pleasure," read Bentham. He's a clear writer in English, on a relatively non-technical topic, and you'd need to see the details of his phrasing to fulfill your goal. If you have little background in Philosophy, but want to know what Kant thinks about cognition, by all means start with an overview or paraphrase. (Challenging writer, originally in German, on a technical topic, about which you don't necessarily need to see the text to get a good sense.) Incidentally, if you happen to be interested in early modern and modern Philosophy, especially European Philosophy from 1500–1900, I strongly recommend looking at the Early Modern Texts site developed by Philosopher Jonathan Bennett. He has translated or smoothed historical texts into contemporary English, vastly improving legibility, generally — and especially for introductory purposes — with little loss or alteration of meaning. They do not pretend to be definitive versions for scholars, but they generally make a first experience of a historical text much more pleasant. 

Rule T here allows one to write any tautological consequence of input sentences. Every sentential logic inference rule describes a particular tautological consequence. You can see that if you write out the inference rules as sentences. For example, a rule sometimes called “reiteration” can be written as “If P then P.” That sentence is logically true, and “tautology” is another way of saying “logically true.” So, in this case, since the law of hypothetical syllogism is logically true, it can be invoked with the rule T. Any other sentence logic rule can also be invoked this way, as can a whole series of them. To put it in different terms, any sentence logic proof can be done with rule T in one step, using this rule, once you know what’s provable in sentence logic. (As for what he’s doing with the unbound variables, I’m not sure. I’d have to see how he defines well-formed formulas, or what the context of this example is.) 

This is actually a valid deductive argument, but the premise assumes the conclusion, and yet is itself controversial. Anyone who doesn't believe that I'm not the best at tennis is really not going to believe that I'm the best at every sport including tennis. It is related to the ordinary usage because it offers a premise which invites or requires or begs an interlocutor to ask a further question: "but how do you know THAT?" or "why should I believe THAT?" Deductive arguments are practically good when they not only are deductively valid and have true premises but also are useful for persuading someone of something. An argument that begs the question is not generally going to persuade anyone of anything they don't already believe. It could only do so in very specific and unsual contexts (like if you didn't understand that tennis is a sport). Mozibur Ullah's answer is very close. It's right that it's a matter of there being a premise that is neither self-evident nor argued for, and which therefore requires a further argument. But it's worth pointing out that begging the question is not a formal fallacy, meaning that it does not make arguments invalid (and it isn't typical to describe sentences like conclusions as valid or invalid).