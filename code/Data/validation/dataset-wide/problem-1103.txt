$URL$ - which works on both SQL Server and Oracle, of course My objection to the interior aliasing is that it is not as good for maintenance, obviously, if new sections are added above that section, the aliasing gets whacked. 

For SQL Server, if performance is poor and is declining, the size of the database is really only an indirect cause. The fact is, all databases can perform reasonably when they are small, but relational databases are typically designed to perform well with large amounts of data through indexing. As the size of the data grows, the right choice of indexing minimizes the performance effects. Profiling your application to know which queries are slow and then reviewing the execution plans of the poorest performing queries should be the first step. It's possible that the queries are written poorly or that the database design is poor or that the application/system design is poor. But I would start to see if there are just queries which can simply be a lot faster with better indexes. 

Failing intermittently but more frequently. Package just calls a couple stored procs and exports the results to two Excel spreadsheets. Where to go from here: 

3m rows is not really a lot to pull in daily, that can be just one daily feed in a data warehouse. And with a traditional DW extraction of just the new and changed rows, it wouldn't be a significant load to bring into a datawarehouse on a daily if not hourly basis. And a (dimensional) data warehouse schema is ideal for running a lot of ad hoc queries where data is handled different ways. I would definitely advocate for remodeling your data into a dimensional model and start bringing those disparate data sources into a data warehouse. You're already pulling from different servers and you're already expecting data to only be as of last day, so this is the perfect opportunity to standardize and start your data warehouse. 

With your sample data, there are some unmatched pairs - this solution only gives the interval for the matched pairs, and more than two entries for each event, which can cause an cross join between the start and two stops. $URL$ 

I expect this simple solution would work fine in many cases although it doesn't particularly scale (but neither does the example in your question using CHARINDEX): 

If your ITVFs are very complex and expensive, you run into the danger of people building on them and incurring costs they don't need to incur because it's the only way to get some data - i.e. something which might be less expensive as a persisted computed column or something which is being computed in the function but most users don't really need - expensive things which few people need should obviously be in the last layer or a separate function. 

I don't know of a single good comprehensive solution to this. Local development means that developers don't break other people working on their own code against a shared database. However, when you get latest code, you also need to get the database into the right state to match the code changes you've merged in. If two people are making changes at the same time, merging can be difficult since database upgrade scripts can be incompatible. Column order doesn't normally matter in a database, but it can be a bit annoying for databases to be different. There are good tools to compare schema and data and apply changes. I would say schemas should aim to be identical. However, typically you want lookup-type data updated between developers but not regular application data (new customer types, but not new customers). Configuration data you might want updated, BUT sometimes only a subset (new printer options but not file path settings). You would think that ideally, you could JUST rebuild your local database completely once you've each merged changes. If you've set up a bunch of test scenarios through the application for testing (instead of in the build scripts), you now don't have a script to get those changes back into the database. And this is more difficult as the scope of the database schema increases - with surrogate keys and parent-child relationships which all might have complex dependencies. In the ideal scenario for a central database, you would have a developer DBA managed the database interfaces for the app and control that so that the exposed interfaces would consistently evolve over time and all developers about that level would see the same interface at the same time. But then you have two separate groups coordinating their different feature timelines. Which I think goes a long way to showing why people are still attracted to a variety of other approaches which put more emphasis on code and less on the database. Ultimately, I think that just shifts the problem around. 

Seems to me that this could be naively refactored (without fooling with any other issues it has like the implicit joins and other garbage) simply as: 

You can do it with multiple databases, but it will be more difficult to manage multiple schemas (rollouts, upgrades, etc) when there are changes. The single database design is a kind of multi-tenant (now that you have the right term, you should find a lot of material about these designs) and you would need to work on the design of these grouping structures. It's certainly possible to structure search across tenants very much more easily in a single database. In separate databases, you would effectively have to query across databases. This is possible in mysql, but isn't supported within the SQL language to pick up database names out of a table to do your joins - you'd have to generate dynamic SQL. So instead of simply: 

I'll just summarize all my comments here as an answer. You should read this: $URL$ The SQL Server Service is the SQL Server engine and runs under an account specified for the service and linked servers which use a file share will necessarily use those permissions, since there is no setting to have the Server Service impersonate another account when it connects through the filesystem. The SQL Server Agent is a separate process which runs scheduled jobs. If you use this and it connects to network shares on its own (not just telling SQL Server engine to run some statement against a linked server which would be under the engine permissions above), it would need to run under an appropriate domain account. In your case, that doesn't appear to be running, so I wouldn't bother fooling with it. Running the SQL Server Express has certain database size and functionality limitations, and I wouldn't consider it for general server-class production use except for high numbers of lightweight servers - for instance a deployment of an small inhouse application to several worksites - where you wouldn't want to pay for licensing and the requirements fit the product. The built in network account is "NT AUTHORITY\NETWORK SERVICE" and you can just put that in and blank out the passwords to restore it. If you switch the SQL Server Service to run as a domain account, you will need to give it sufficient permissions to access the OS on the database server - i.e. folder access to where the database files are if the account doesn't have access. 

Typically in relational databases, we have columns and rows, not fields and records. In relational database theory, the term relation is used to refer to a table. Rows in a table are equivalent to tuples of attributes (equivalent to columns) which are "related" to various candidate keys, one of which is identified as the primary key. 

With the change to package configurations in 2008 compared to 2005 when I specify /ConfigFile something.dtsConfig on the command line, variables defined in the package are keeping their design-time values instead of using the settings from the config file. I'm not quite sure I understand HOW to get the external config file to be used at all. I've read articles that say that only design-time configurations that are set will overwrite the load of the external file. Does this mean I can change the variables to blank strings and then they will get overwritten? I can't delete the variable completely! What about integers? I've seen articles that mention turning OFF using package configurations in the package. I can use the SSIS Package Editor or an XML editor to change the configuration file path in the package, and then it will use that file's settings "last" (regardless of the external /ConfigFile option), but I don't want to be changing the package. I want one package with Test.dtsConfig and Production.dtsConfig and be able to swap back and forth without changing the package. What is the recommended way to do this now? 

I don't seem to have a script like this in my toolbox, but found some others in my search. Have you already seen this script? $URL$ It uses a cursor, which is probably not strictly necessary. But I typically can clean them up to avoid a cursor by using the FOR XML and potentially a quirky update to concatenate the strings. Also found these: $URL$ $URL$ 

isn't going to affect the @result in the outer part - these dynamic SQL parts have their own scope and can't get to the variables in the calling part this way And then it really comes down to the error you are currently getting - which is sp_executesql can't be run inside a function anyway. Anyway, to aid you a little in making progress, may I suggest you read the following two articles which are are tangentially related (and I have used recently) regarding parsing CSV and JSON data. Both use his hierarchy idea (an unpivoted name/value thing) and repivoting and you might find some useful techniques there: $URL$ $URL$ If you can use the CLR, this article which was published might also be useful: $URL$ 

That one X locks the row in the CI (on CREATED column) and then attempts to X lock on the NCI which includes the status column. 

You could make the view add a ROW_NUMBER with different ORDER BY for different users and then the outer select (or your reporting tool's sort setting) would always (explicitly) sort by that new ORDERBYTHIS column. i.e. make the view as: 

Yes, you can make more and more database servers and you can also make a larger and larger database server (but the scale up limit can be reached very quickly), but in the scale out scenario are the servers you mention supposed to cooperate in any way or are they completely independent. The CAP theorem doesn't just apply to "database" servers, but since web servers aren't really considered as a "write" part of a distributed data store, they are usually considered to be easily scalable by themselves: $URL$ The fundamental problem for scalability in database is in the consistency part of the CAP theorem - all those ACID guarantees that databases have. 

Based on everything you have said in the question and the comment, I don't think you need to worry about hardware yet unless you are trying to just get a ballpark price estimate for feasibility. Since you are a software person, I would build the prototype on commodity hardware like your ordinary laptop, analyze and understand the problem and then spend money once you know more about the profile of the software and the problem space. If your laptop is not up to it, then pick up a refurbished server for a few hundred dollars to get it closer to being able to test. It sounds like a small amount of data and there is no indication of the large amount of ongoing data analysis (which can greatly be affected by the data model - some data models can make analysis many orders of magnitude faster). If you are trying to test at production loads, you will need a production spec machine - so the problem is a chicken and egg one. To have a production machine, by your own admission, you would need an expert. And I know this is relatively off topic from your question, but: The things you need to be looking at on your software design are the rate at which the data is coming in, the amount of processing (parsing, de-duping) you will need to do, the model for the data, the size estimates, the way the reads are going to work (whether you have multiple models - one for writing and one for reading, like data warehousing), and the complexities of the analysis and whether this will be performed by SQL Server in your architecture or by client code (and whether you intend the client code to run on the same server or whether you will also have an application server). This will tell you a lot more about what you want out of your SQL Server than your point 4 (the only thing you have given us which has anything to do with determining the nature of the server configuration you will want in addition to the $2000-$6000 budget). And this would be information that your expert would need to be useful to you. In my experience, the right time to make these kind of decisions is as late as possible - and a decision on this hardware can be deferred. 

A clustered index seek or scan could be improved to a non-clustered index seek or scan which should be more efficient. Since it looks like your problem is Products, I would see about adding an index which would be covering on that table (or perhaps an indexed view since you already have: Id ManufacturerId Active MemberPrice Because some of your other columns don't have prefixes, I can't tell where they come from, but I expect some of them also come from Products, so this might not be feasible to make this index covering. However, but having Active and MemberPrice in the non-clustered index, this might help. It might be enough to tip the plan in favor of a NCI with a lookup to the clustered index to get the remaining columns (like FamilyImageName) 

I don't think it's a significant amount of data in these tables - about 26000 rows on each side, and I don't think the set of differences is that big of a subset of the two. But I was wondering if there are issues with DB Links which make cursors preferable to regular set-oriented methods? This operation is part of a stored procedure which is run from an Oracle job on a daily basis. 

Indications from my searches are that this is some kind of connection issue or timeout. I'm working to improve the proc performance, but the timeout is not set on the Linux machines (so should default to 0). Is there something I can do to stop this error? Here is the log from FreeTDS of a bad run: 

Does the data actually need to be replicated in the same model? What will you be doing on the MySQL side with the data? I would strongly consider putting either another database or instance which you control on the SQL Server side to be able to take snapshots on that side so you pull less data down (comparing the snapshot to live data should be quick) and also look at what you want to do with the data on the MySQL side, since it is possible that you actually want to transform the data during the extraction to better facilitate the load. I think you will find that having a sandbox database or instance which you do control collocated with the vendor database will give you a tremendous amount of power, while still allowing the vendor to operate without interference. This can also help in reducing the bandwidth (unneeded columns, denormalizing usually increases data size, but not always) and reducing the load times or the complexity of the load process. A different model on the MySQL side which corresponds to the queries you will actually be running can also improve performance of reporting or even the load. Having views against the source data can also help mitigate against changes - although you might not be able to use DDL triggers and schemabinding to stop changes from breaking dependencies, you could possibly catch schema changes when the extraction fails instead of after the extraction has neglected to bring down some new columns.