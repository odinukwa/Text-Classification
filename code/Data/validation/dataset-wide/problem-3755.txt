Consider two mean centered multivariate normal densities $N(0,\Sigma_{1})$ and $N(0,\Sigma_{2})$. Are there known expressions (as opposed to bounds provided by the Pinsker inequality) for the total variation distance between such densities? Let $P_{1}=\Sigma_{1}^{-1}$ and $P_{2}=\Sigma_{2}^{-1}$. I am looking for either a general expression for the total variation distance or for expressions in special cases such as when (i) $P_{1}-P_{2}$ is positive definite or (ii) when $P_{1} =k P_{2}$ for some positive constant $k>1$. In special case (ii), the distance should be expressible using multivariate generalizations of the error function but I don't have a numeric approximation for the multivariate generalization of the error function. 

After some work, I have come up with the following answer. In the general case, the following holds for any two multivariate normal densities: \begin{array}{rcl} f(\mathbf{x}|\mathbf{0},\mathbf{\Sigma}_{1}) & \geq & f(\mathbf{x}|\mathbf{0},\mathbf{\Sigma}_{2}) \\ &\Updownarrow & \\ -\frac{1}{2} x^{T} \mathbf{P}_{1}x + \frac{1}{2} log [|\mathbf{P}_{1}|] & \geq & -\frac{1}{2} x^{T} \mathbf{P}_{2}x + \frac{1}{2} log [|\mathbf{P}_{2}|] \\ & \Updownarrow & \\ x^{T}[ \mathbf{P}_{1}-\mathbf{P}_{2} ]x & \leq & log [|\mathbf{P}_{1}|] - log [|\mathbf{P}_{2}|] \end{array} The set $A:=\{x| f(x|\mathbf{0},\mathbf{\Sigma_{1}})\geq f(x|\mathbf{0},\mathbf{\Sigma}_{2})\}$ is a compact and convex set if $\mathbf{P}_{1}-\mathbf{P}_{2}$ is a positive definite matrix as in special cases (i). Moreover, if $\mathbf{P}_{1} = k \times \mathbf{P}_{2}$ for $k>1$ then $ \begin{array}{rcl} f(x|\mathbf{0},\mathbf{\Sigma}_{1}) & \geq & f(x|\mathbf{0},\mathbf{\Sigma}_{2}) \\ &\Updownarrow & \\ x^{T} \mathbf{P}_{1} x & \leq & \frac{k}{k-1} n \times log [k ] \\ &\Updownarrow & \\ x^{T} \mathbf{P}_{2} x & \leq & \frac{1}{k-1} n \times log [k ]. \end{array} $ In A Generalized Error Function in n-dimensions, M. Brown defines an n-dimensional generalized error function $erf_{n}(.): \mathbf{R}_{+} \rightarrow \mathbf{R}$ by $ erf_{n}(x) = \frac{\int_{0}^{x} e^{-u^{2}} u^{n-1} d u }{\int_{0}^{\infty} e^{-u^{2}} u^{n-1} d u} $ Equation [47] in the paper concerns diagonal variance-covariance matrices and states that $Prob[\sum_{i=1}^{n} \frac{x_{i}^{2}}{\sigma_{i}^{2}} \leq \beta^{2}] = erf_{n}(\frac{\beta}{\sqrt{2}}). $ If this equations extends to the general case so that $ Prob[ x^{T} \Sigma^{-1} x \leq \beta^{2}] = erf_{n}(\frac{\beta}{\sqrt{2}}) $ then the total variation distance under special case (ii) can be expressed as $\begin{array}{rcl} ||f(.|\mathbf{0},\mathbf{\Sigma}_{1}) - f(.|\mathbf{0},\mathbf{\Sigma}_{2})||_{TV} & = & erf_{n}(\frac{\sqrt{ \frac{k}{k-1} n \times \ln [k ]}}{\sqrt{2}}) - erf_{n}(\frac{\sqrt{ \frac{1}{k-1} n \times \ln [k ] }}{\sqrt{2}}). \end{array} $ As per the Brown paper, the error function is given specifically by $ erf_{2m}(x) = 1 -e^{-x^{2}}[1 + \frac{x^{2}}{1!} +\frac{x^{4}}{2!} + \ldots+\frac{x^{2(m-1)}}{(m-1)!}] $ if $n$ is of even dimensions and by $erf_{2m+1}(x) = erf_{1}(x) -\frac{e^{-x^{2}}}{\sqrt{\pi}}[\frac{(2x)0!}{1!} + \frac{(2x)^{3}1!}{3!} + \ldots+\frac{(2x)^{2m-1}(m-1)!}{(2m-1)!}] $ if $n$ is of odd dimension. Considering the one-dimensional case and letting $k=(1+\epsilon)^{2}$, we get $ \begin{array}{rcl} ||f(.|\mathbf{0},\mathbf{\Sigma}_{1}) - f(.|\mathbf{0},\mathbf{\Sigma}_{2})||_{TV} & = & erf_{1}(\frac{\sqrt{ \frac{k}{k-1} \times \ln [k ]}}{\sqrt{2}}) - erf_{1}(\frac{\sqrt{ \frac{1}{k-1} \times \ln [k ] }}{\sqrt{2}}) \\ & = & erf_{1}(\frac{\sqrt{ \frac{(1+\epsilon)^{2}}{(1+\epsilon)^{2}-1} \times \ln [(1+\epsilon)^{2} ]}}{\sqrt{2}}) \\ & & - erf_{1}(\frac{\sqrt{ \frac{1}{(1+\epsilon)^{2}-1} \times \ln [(1+\epsilon)^{2} ] }}{\sqrt{2}}) \\ & = & erf_{1}(\frac{(1+\epsilon) \sqrt{ \frac{1}{\epsilon(2+\epsilon)} \times 2\times \ln [(1+\epsilon) ]}}{\sqrt{2}}) \\ & & - erf_{1}(\frac{\sqrt{ \frac{1}{\epsilon(2+\epsilon)} \times 2 \times \ln [(1+\epsilon) ] }}{\sqrt{2}}) \\ & = & erf_{1}(\frac{(1+\epsilon) \sqrt{ \ln (1+\epsilon) }}{\sqrt{\epsilon(2+\epsilon)}}) \\ & & - erf_{1}(\frac{ \sqrt{ \ln (1+\epsilon) }}{\sqrt{\epsilon(2+\epsilon)}}) \end{array} $ which corresponds to one of the answers to a univariate version of this question. The above can be partially extended to all of special case (i) to provide a bound on the total variation distance in terms of error functions. Define a function $g_{1}(.): A \rightarrow \mathbf{R}_{+}$ by $g_{1}(x)=x^{T}P_{1}x$. Let $b_{1} =\max_{x\in A} g_{1}(x)$. Since $A$ is a compact convex set and the function $g(.)$ is continous, this maximum is well defined under the present assumptions. We we can hence define a set $A_{1}^{*}:=\{x \in \mathbf{R}^{n}| x^{T}P_{1} x \leq b_{1}\}$. Clearly, $A \subseteq A_{1}^{*}$. Similarly, define a function $g_{2}(.): A \rightarrow \mathbf{R}_{+}$ by $g_{2}(x)=x^{T}P_{2}x$ and define a parameterized family of sets $A_{2}(.) : g_{2}(A) \rightarrow \mathbf{R}_{+}^{n}$ by $A_{2}(b)=\{x \in \mathbf{R}^{n}| x^{T}P_{2} x \leq b\}$. Let $A_{2}^{*}= \{x \in A|A_{2}(g_{2}(x)) \subseteq A)\}$ and $b_{2}=\max_{x\in A_{2}^{*}} g_{2}(x)$. This maximum exists under the present assumptions since the set $A_{2}^{*}$ can be shown to the non-empty, convex, and compact. Clearly, $A_{2}^{*} \subseteq A$. We can now provide a bound for the total variation distance in terms of error functions that generalizes the expressions for the total variation in special case (ii): $ \begin{array}{rcl} ||f(.|\mathbf{0},\mathbf{\Sigma}_{1}) - f(.|\mathbf{0},\mathbf{\Sigma}_{2})||_{TV} & = & \int_{x \in A} f(x|\mathbf{0},\mathbf{\Sigma}_{1}) dx -\int_{x \in A} f(x|\mathbf{0},\mathbf{\Sigma}_{2}) dx \\ & \leq & \int_{x \in A_{1}^{*}} f(x|\mathbf{0},\mathbf{\Sigma}_{1}) dx -\int_{x \in A_{2}^{*}} f(x|\mathbf{0},\mathbf{\Sigma}_{2}) dx \\ & = & erf_{n}(\frac{\sqrt{ b_{1}}}{\sqrt{2}}) - erf_{n}(\frac{\sqrt{b_{2} }}{\sqrt{2}}). \end{array} $ In special case (ii), $b_{1}=\frac{k}{k-1} n \times \ln [k]$ and $b_{2}=\frac{1}{k-1} n \times \ln [k]$. While not generally available in closed form solution, increasingly good estimates for $b_{1}$ and $b_{2}$ can be generated through repeated simulation from centered multivariate normal densities with precision matrices $P_{1}-P_{2}$. To generate estimates for $b_{1}$ and $b_{2}$, let $B=\{x^{(i)}\}_{i=1}^{L}$ be a set of (non-zero) random draws from a mean centered multivariate normal with precision matrix $P_{1}-P_{2}$. For each $x^{(i)} \in B$, define $\begin{array}{rcccccl} t(x^{(i)}) &= & \sqrt{\frac{log [|\mathbf{P}_{1}|] - log [|\mathbf{P}_{2}|]}{(x^{(i)})^{T}[ \mathbf{P}_{1}-\mathbf{P}_{2} ]x^{(i)} }} & , & \tilde{x}(x^{(i)}) & = & t(x^{(i)}) \times x^{(i)} \\ \tilde{b}_{1}(x^{(i)}) &= & ( \tilde{x}(x^{(i)}))^{T} P_{1}\tilde{x}(x^{(i)}) & , & \tilde{b}_{2}(x^{(i)}) & = & ( \tilde{x}(x^{(i)}))^{T} P_{2}\tilde{x}(x^{(i)}) \end{array}$ and estimate $b_{1}$ and $b_{2}$ by $\hat{b}_{1} = \max_{x^{(i)} \in B}\tilde{b}_{1}(x^{(i)})$ and $\hat{b}_{2} = \min_{x^{(i)} \in B} \tilde{b}_{2}(x^{(i)}) $ respectively. As the number of draws from the multivariate normal density increase, $E[\hat{b}_{1}] \rightarrow b_{1}$ and $E[\hat{b}_{2}] \rightarrow b_{2}$ where the former convergence is from the below and the latter from above. 

One has to note that in many countries there is a ministry that combines art and science. Clearly, society has decided that pure science research should be funded using taxpayer's money for similar reasons why art is funded. But funding research is only one way of promoting it. Another task of the government is to make sure that the public is educated well enough to be able to enjoy and benefit from the results of the research. Here most countries treat science and art differently. The educational program is quite minimalistic as far as science and math is concerned, you only learn what you need to know to get a job. If you want to learn more you need to study at university. We don't take this attitude when it comes to art or literature. This difference in attitude toward art and science explains why the beauty of math isn't as easily accessible to the wider public compared to the beauty of literature. But beyond not being able to appreciate the beauty of the subject, this has negative consequences for society. How can a democratic society choose what it should do to curb climate change if most people don't have enough scientific skills to separate expert opinion from nonsense? So, perhaps we are now paying the price of not having taken math as serious as other subjects. 

The Tonelliâ€“Shanks algorithm for finding square roots modulo a prime number. Today this is well known, it is e.g. used in the quadratic sieve method to factor large numbers. Alberto Tonelli discovered the algorithm in 1891, it was re-discovered by Daniel Shanks in 1973. 

The fundamental issue here is that mathematics took the wrong direction way back in the mid 1800s when it started to formalize intuitive notions of a continuum using set theory. A lot of work was done to get to a mathematically consistent framework that culminated in modern set theory. From a modern physics point of view, it makes more sense to work within a finitist framework. The formalism of quantum field theory (QFT) involves computing a path integral over all possible field configurations, but this has to be regularized such that it only involves a finite number of degrees of freedom. At the end of the calculations the continuum limit is taken. One can e.g. put the fields on a lattice and then imagine that we exist on such a large scale that we cannot see the lattice spacing. To me this suggests that if QFT (and particularly the way it is used in statistical physics where we start with an underlying discrete model that then leads to a field theory) had been invented a century earlier, mathematics would have taken a different turn. While the intricate details have never been worked out, it's clear that in practice when doing calculus, this would amount to having to take more elaborate limits (e.g. a replacement for continuous functions from $\mathbb{R}\to \mathbb{R}$ will have to involve a coarse graining procedure to get to smooth functions in the continuum limit). This may look more complicated, but it's more natural from a physics point of view. 

Mathematicians and all the tools they can use are subject to the laws of physics which impose certain constraints, e.g. that you can only ever perform a finite number of elementary operations. In a universe governed by classical mechanics this is not true, there you could construct a machine whose clock cycle can increase exponentially fast. Such a machine could perform an infinite amount of computations in a finite time. It is because we don't live in such a universe that polynomials are useful to us.