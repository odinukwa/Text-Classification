I've recently suffered from a power outage on one of my monitoring servers at the office. The result of that outage caused for some database tables to get corrupted. I've successfully repaired 3-4 tables by using the "use_frm" option however there are still 3 that seem to be badly corrupted and are not responding to the mysql REPAIR command (with or without use_frm) 

I'd like to know if I were to add additional drives to an existing Raid 10 if this would increase the speed of the entire array? 

Alright guys (and girls?), After a long search I finally found out what this issue was being caused by. There is a third party module that can be installed on php called suhosin. More information about the project can be found here $URL$ Anyway you need to open up your php.ini and find these following 3 line. If they do not exist then you need to create them with the proper variables 

Python will be reinstalled with all of its prerequisites which is what I'm assuming was all the associated programs your speaking about? 

I recently installed unison version 2.32.52 on a one of my CentOS 5.x boxes. With unison its important to note that all servers that will be synchronizing together will need to be running the exact same version. Unison won't work when linked to other unison versions older and newer. For myself I found that this rpm $URL$ worked for me running on centos 5.8 As for lsync I wouldn't be able to tell you much about it as I have never personally used it. I can however say that Unison when used properly is a fantastic tool. You can set Unison on a crontab to check for changes every 1-10 minutes. 

I have two internal networks that are connected to each other 10.10.10.1 and 10.10.11.1 I am trying to make devices on the 10.10.10.1 network be able to access the devices on the 10.10.11.1 network and vice versa. My questions is what is the networking term used to "bridge/route" traffic between the two networks making each other available to one another. Also what is the process in getting this going using iptables? 

I currently have Unison installed as a one way sync (mirroring) between a CentOS/Win7 box. When configured as a one way sync what happens if I change a filename on the source box? Will unison delete the file on the mirroring box and then recopy the file over or is it smart enough to simply rename the file? Also, what happens if I rename a file on the Mirroring box? Will Unison simply rename that file back to its proper name or will it delete it and resync the file from the "master" box? I am currently running unison version 2.32.52 on both boxes. Thanks 

However, when the user tries to upload a file using SCP, they can't, as the dev_login script is interrupting the process. Is it possible to use the 

Thanks to CtrlDot for the lead. The link in that answer might as well be in arabic if you are not familiar with Active Directory and Microsoft authentication regime, which must new Azure users will not be. I was able to piece this together from the following link: $URL$ Steps: 

Another proposed solution here: $URL$ Use a generic certificate for all agents connecting to the puppet master 

Your terminology isn't very accurate so its hard to tell what you are trying to do. Cloudformation creates "stacks". You can create a stack that contains an RDS instance. If you want to update a stack, you can upload an updated JSON template to that stack. The only changes that will be applied to the stack are the changes from the original JSON template that was used to create the stack. So, if you have an RDS instance in the stack, and your updated template does not include any changes to that RDS instance, applying an updated template (which changes other parts of your stack) will not impact your RDS instance. If your updated template contains changes to your RDS instance, then, depending on what those changes are, your existing RDS instance could be deleted. I would advise testing on a dummy stack first. 

Basically, the IAM documentation is totally unreliable when it comes to doing anything other than set global admin or read-only policies. This is the policy I eventually got to work (for the subnet bit at least): 

A wildcard is the best option, but I'm guessing you didn't buy one of these, and that you have a cert that is specific to "sitename.com" 

I set up a single Route Table that is associated with both of my public subnets. this includes route entries as follows: 

I am running an OpenSwan server to facilitate client-server connections into a secure data centre. I have a problem with the standard L2TP over IPSEC client in MacOS, specifically when using WIFI. When I connect for the first time, it works fine. When I disconnect and try to connect again, it fails at the authentication step (shared secret). From what I can see, when the MAC is using WIFI, it doesn't get time to send a DELETE signal to OpenSwan, so as far as OpenSwan is concerned the peer still exists. I can see this in the OpenSwan logs: 

I have a Siteground cloud server, and I know nothing about servers. It hosts 8 websites, although 3 of them have their own dedicated IP addresses and cPanels. It's on a Linux server, but there's something w/ NGinx mixed in there. For instance, sometimes they have to restart NGinx for me ... some type of combo or something. The basic details on the server are: 

They're all WordPress installations, and recently -- despite no major changes -- my administrative areas in all Siteground websites began lagging extremely slowly and eventually timing out. The techs are usually good, but this guy just told me to purchase 2 additional cores; he said I didn't need to purchase additional storage space or an extra GB RAM. The Sites: They're basically low maintenance sites and not too heavy w/ traffic. USS Vision, WebPrezence LLC (SSLd), and then my nonprofit organization, National Center For Due Process (SSLd, and is the largest and most visited of any of the 8 websites. As server experts, is it possible for anybody here to tell if this tech is correct? Will adding 2 CPUs help, so I don't get locked out of my FTP, cPanel, and back-end of my websites? Is it possible to tell with the little amount of information I'm knowledgeable enough to provide? Or am I way off here and should I be doing something else, leery of the tech's advice? Thank you for any guidance anybody can offer! 

Did I do the right thing in purchasing the 2 extra cores? Should I ask for a refund? Should I ask for something else, like more Terabytes of bandwidth? More than the 30GB of SAN storage I currently have on my Linux? Should I just wait until I have exhausted this billing cycle, which ends in September, and go with another host? 

I'm currently paying $30/month for this VPS -- and then an extra $18/month for the two additional cores. Thank you for any guidance anybody can offer! Update: Thank you for everyone who took the time to read this question and answer. Unfortunately, BlueHost has had one of their infamous blackouts that occur monthly since they were acquired by Endurance International Group (EIG) in August 2014. So I'm unable to tinker with any of the solutions posed, as I have no websites, no FTP, no e-mail, and no cPanel access. 

I recently purchased a premium WordPres plugin called Hide my WP. I planned on using this plugin for my nonprofit organization's website, which is a multisite WordPress installation with an SSL. Upon implementing it, page load times went from 4-6 seconds (already pretty bad, but I have a lot of content) to anywhere from 30-60 minutes. I should add that 95% of the page load times deals with the server response time (you'd see "waiting for nationalcdp.org" in the lower-left when using Firefox); there must be bottlenecks in there, but I can't figure it out. Once the browser connects to nationalcdp.org, the actual pages load quite quickly. Initially, the BlueHost techs blamed me that my sites weren't optimized. But they are optimized for speed ... they get good grades on gtmetrix, pingdom, page speed insights, and others. All caching mechanisms are in place, images are losslessly compressed, javascripts are deferred, html and css are minified, etc. etc. I explained that while I'm not knowledgeable with servers, I'm quite familiar on how to speed up page load times, especially when it comes to WordPress and SEO / speed optimization. I'm using a VPS from BlueHost, and the details of my VPS are: 

I'm trying to centralise logging in an environment that using multiple application technologies (Java, Rails and various DBs). We want to developers to bring up stacks with Docker Compose, but we want to them to refer to a central log source (ELK) to debug issues, rather than trying to open shells into running Docker containers. The applications all write to the file system rather than to STDOUT/STDERR, which removes all of the options associated with the Docker logging driver, and logspout too. What we have done is configure the containers to have rsyslog include the application log files and forward those to logstash which has a syslog input. This works in terms of moving the logs from A to B, but managing multi-technology logs in ELK based on the syslog input is horrible (eg trying to capture multine Java stacktraces, or MySQL slow queries). Is there a better way to do this? Should I be running logstash in each container, so that I can apply filters and codecs directly to the log files, so that I don't have to rely on the syslog input? Of is there some way to use the Docker logging driver with application log files that are written to the file system? 

Use a zip archive as your application version Include a Dockerfile.aws.json file in the archive Include a Dockerfile file in the archive Include a .ebextensions folder in your archive 

There is nothing there, and the server is telling me the /var/log/btmp file starts 14 days ago. There is one other archived btmp log files in /var/log/ 

Have managed to test this and I can confirm that it does work. Whereas ElasticBeanstalk assumes that your Dockerfile will build on and add to the base image, it is not actually necessary to do this. The Dockerfile can simple refer to the base image your want to use (eg in the AWS ECR) and the port you want to expose when it is deployed to the ElasticBeankstalk instance. This then allows you to use ebextensions in a Single Container Docker environment. 

If you have configured a VirtualHost, and Apache is giving you a warning that there are no VirtualHosts configured, the problem is with your VirtualHosts config, not your SSL certs. I'd check you VirtualHosts config for syntax errors that may have occurred while you were updating it to take account of your new certificate. 

I am currently migrating a DNS zone from one DNS server provider to another. I am trying to estimate how long it will take for the change to propagate, and to understand what the delay might be if I chose to rollback mid-stream. Previously, I thought I could do: