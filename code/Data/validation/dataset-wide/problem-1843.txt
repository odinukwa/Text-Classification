FTP works, but I'd make sure the client and server are both reasonably efficient and - most critically - offer the ability to resume transfers. I would also suggest taking a look at rsync, as it's quite efficient and offers a bunch of options for controlling bandwidth and ongoing synchronization (if that's necessary). 

Have you considered using the IP-MROUTE-STD-MIB rather than the IGMP MIB? You can get statistics on a per-mroute basis - which will give you a much better view of the source in particular. There's also a set of Cisco extensions to this MIB that can provide more in the way of platform-specific info. One item you can potentially look for is a substantial difference in counters on your various routers through the path of the mroute. Some delta is to be expected but this is would be a good place to track thresholds. For tracking streams freezing there's a pretty easy answer: ip multicast heartbeat ($URL$ You can configure a given router to throw an SNMP trap if no packets are seen on a configured multicast group for 10 seconds. There is also a feature called mrm (multicast route monitor) that can be called from the Cisco CLI to set up and track synthetic multicast groups. You'd likely want to use EEM or similar to call it periodically and then throw a trap or syslog if it doesn't behave normally. This is also a good troubleshooting tool. Also - just as you (should) monitor for changes in IGP adjacency, so too should you track on PIM. Events like neighbor state changes, elections, etc can indicate instability in the tree. It's not -necessarily- a big deal in all cases but should generally be quiet on a stable network. I'm not sure which supervisor you're running in your 4500's, but some of the more recent models support netflow for multicast. This would give you a much more granular and global view of multicast performance and would naturally lend itself to statistical trending, storage, etc.. definitely a good way to go. I hope this helps- 

There's nothing inherently special about an uplink port. You can use those ports to connect to switches or to individual servers (or NAS appliances). The ports can be configured to support VLAN tagging - or, in your case, can be untagged ports in the same VLAN as the 48 GE ports. Ports 51 and 52 are configurable as either stack ports or uplink (read: normal) ports. The stacking mode is just a proprietary mechanism to make up to 6 of these switches show up as a single device. If you buy more of these switches this might be a handy way of growing your network. If you don't, they're just ports. 

It's generally not recommended to run copper directly between buildings as individual structures tend to be separately grounded (or earthed, if you prefer). Even a relatively small difference in potential can destroy equipment and even create a hazardous condition for people working on the equipment. Perhaps more to the (immediate) point connection integrity can be significantly degraded - especially if the method/path used to run the cable between buildings is subject to excessive interference. This can manifest as poor performance even on in-spec runs. The right way to do this is with fiber. Distance limitations cease to be a practical issue, electrical isolation is assured and the network will perform as it should. Go for a simple media converter or a switch with native capability for fiber ports - either will be fine for your application. If you don't have need for additional fiber capacity, the media converter is probably the cheaper/less intrusive mechanism. PS - Don't be concerned with what "ought" to work. The distance certifications on network media exist for a reason, a reason which will become abundantly clear when a previously working over-spec installation ceases to function for no apparent reason. Saving a bit of money up front doesn't seem like much of a deal when whole buildings are cut off. 

Actually, the IP network works over MPLS. The idea of MPLS is that a series of labels can be applied to a given packet (or frame) that can subsequently be used to switch it through a network. In the case of an L3VPN that means that rather than the traditional mechanism of looking at the destination IP address, routing devices look at one or more previously applied labels to make forwarding decisions. The key point in the above is that the actual -contents- of the packet aren't actually considered. Once a given packet is labeled the intervening devices simply forward it based on whatever LSP has been signaled. In the case of an L3VPN, the packet is a fully formed IP packet. In the case of an L2VPN a frame from a particular interface has a label added and is forwarded. This might be a full Ethernet frame (with or without an 802.1q header), an HDLC frame from a serial link, one or more cells from an ATM PVC, etc.. One of the contrasts between L2 and L3 VPN is the mechanism used to signal and set up the overlay network. L3VPN (RFC2547bis) extends the BGP protocol to allow PE's to signal which routes are available within which VPN's. There are more possible ways to put together a layer 2 network (i.e. straight point-to-point links, multipoint, translational, etc) and there are also more mechanisms in use in the industry used to signal these various topologies. As for links - google is obviously your friend, but if there's some specific aspect of L2VPN design, architecture or setup it would make things easier for us to direct you. 

You need different tools. The best you can hope for with a basic multimeter is basic continuity, which is better than nothing..but not by much. There are relatively cheap units that can confirm appropriate pin-outs, isolation, etc and there are higher dollar TDR-type units (Fluke makes some good stuff, but not cheap) that can actually certify the actual performance levels of the cable or - in the event of a fault - where along the cable the issue has occurred. 

Dump the other two interfaces and just hook up a single port to a switch and then hang the hosts (and possibly wifi in the same subnet) off of that switch. It's possible to join all three of your internal interfaces into a single bond or team but this basically takes all three physical interfaces and turns them into a single logical interface. Create a Linux bridge interface. Give the IP, DHCP and other configuration elements to the bridge and then tie the three physical interfaces in as members of the bridge. Your firewall sees the single bridge interface as the inside (...even though it ties to three segments). Again - from an IP perspective you have one interface in the inside subnet, not three. Put the three interfaces in three non-overlapping IP subnets and configure accordingly. 

Worth noting - to catch the private address range (RFC1918) you probably want 192.168.0.0/16, -not- /8. 

In theory compiling your own source could take advantage of particular compiler optimizations that leverage your hardware. In practice for most applications it doesn't make a tangible difference - particularly when compared to the issues associated with dependencies, updates, testing, etc. This advantage also presupposes a compiler that can optimize for the features in question which is by no means a given. There's a semi-famous quote by Donald Knuth: "Programmers waste enormous amounts of time thinking about, or worrying about, the speed of noncritical parts of their programs, and these attempts at efficiency actually have a strong negative impact when debugging and maintenance are considered. We should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil. Yet we should not pass up our opportunities in that critical 3%." At this point rolling your own binaries for the purposes of performance would definitely fall into this category. There are times when particular features that aren't in distributions might be needed (for example) but these cases should be few and far between. 

The priority setting only comes into play when a resource is limited - so if both 1G servers were transmitting at full speed to another 1G host there would be twice as much traffic as the destination link could handle. The switch would then, in theory, reference the relative priority of the frames to determine which would be transmitted and which would be dropped. Note that there may be multiple mechanisms that you can select to perform evaluation and drops and that each may yield different results (not very familiar with HP's network gear) - for example a 2:1 ratio of high to low priority packets (vs all high and no low) 

There isn't really an accepted standard for icons. The Cisco icons themselves have changed substantially over the years based on rebranding, acquisitions, new product lines, etc (and will absolutely change again). Being consistent with icon usage, including legends and clearly labeling are each as- or more- important than the icon itself. A well thought out selection of boxes and circles with appropriate labels and some kind of consistent flow is infinitely more useful than a jumbled collection of images from a stencil. 

1.) Before basically anything else get your IP addressing plan straightened out. It's painful to renumber but it's the necessary step to arrive at a workable infrastructure. Set aside comfortably large, easily summarized supernets for workstations, servers, remote sites (with unique IP's, naturally), management networks, loopbacks, etc. There's a lot of RFC1918 space and the price is right. 2.) It's hard to get a sense of how to lay out L2 in your network based on the diagram above. VLAN's may not be necessary if you've got sufficient numbers of interfaces in your various gateways as well as sufficient numbers of switches. Once you've got a sense of #1 it might make sense to reapproach the L2 question separately. That said, VLAN's aren't an especially complex or novel set of technologies and needn't be that complicated. A certain amount of basic training is in order, but at a minimum the ability to separate a standard switch into several groups of ports (i.e. without trunking) can save a lot of money. 3.) The DMZ hosts should probably be placed onto their own L2/L3 networks, not merged in with workstations. Ideally you'd have your border routers connected to a L3 device (another set of routers? L3 switch?) which, in turn, would connect a network containing your externally facing server interfaces (SMTP host, etc). These hosts would likely connect back to a distinct network or (less optimally) to a common server subnet. If you've laid out your subnets appropriately then the static routes required to direct inbound traffic should be very simple. 3a.) Try to keep the VPN networks separate from other inbound services. This will make things easier as far as security monitoring, troubleshooting, accounting, etc. 4.) Short of consolidating your Internet connections and/or routing a single subnet via several carriers (read: BGP) you'll need the intermediate hop before your border routers to be able to redirect in- and out- bound traffic appropriately (as I suspect you're doing at the moment). This seems like a bigger headache than VLAN's, but I suppose it's all relative. 

Yes - this is expected. A given sequence number can be either a permit or deny and changing from one to the other doesn't eliminate the contents. As to the contents themselves (i.e. match statements) you're essentially telling the route-map to perform a logical OR on the two tags. If you want to only match on 20 then you should issue "no route-map s1 permit 10" 

So try adding something like this to the top of snmpd.conf: Listen for connections on all interfaces (both IPv4 and IPv6) agentAddress udp:161,udp6:[::1]:161 

On the commercial side there's Cisco Security Manager that can handle ACL's on IOS boxes, ASA, etc. There's a 90 day free eval and it runs in a VM. That might be worth looking at. There's also fwbuilder that offers multi-platform ACL management (including IOS), but I haven't spent much time with it. 

This is usually accomplished with LUN masking (usually on the array) or some combination of zoning and/or VSAN's on the switch (depending on vendor, topology, etc). Generally it's not a great idea to expose all of your LUN's to all of your hosts. 

Setting up Netflow would fit the bill. While normally generated by network equipment there are open source packages ($URL$ to generate Netflow based on libpcap. This data can then be exported to a number of both freely available and commercial collectors and analyzers (either on the same host or separate) that can then provide detailed information about individual hosts, protocols, etc. 

1.) Bandwidth != latency, and jitter matters. A dedicated leased line's latency is constant and is generally substantially lower than a DSL or FIOS connection. This can be a critical point for certain types of applications. The upstream bandwidth from a given POP is certainly a point of variability, but generally far less so than consumer-oriented concentrators which can suffer in non-intuitive ways under periods of congestion. 2.) Consistency of bandwidth - If you lease a DS-3 then you have that bandwidth available at all times and under all circumstances. Even business-grade DSL/cable/similar services are subject to variations in available bandwidth based on usage. There's more bandwidth available under most conditions, but potentially less under others. 3.) Availability - At least in the US, traditional leased lines can be had anywhere. If your facility happens to be in an industrial park or located somewhat remotely then cable/DSL/FIOS may simply not be available at all. This is especially true in many commercial settings where a SONET mux may already be present but other services aren't economically viable for SP's. 4.) Mixture of services - Many SP's will offer Internet transit bandwidth as well as private WAN service on the same physical pipe. While this can be accomplished on a shared medium, better results are often had on dedicated pipes. Also, as antiquated as it may sound there is still a lot of use of standard PRI's rather than SIP trunks for a lot of phone systems. These can be mapped into a timeslot along with other services. 5.) To your first point, ALL Internet service is multiplexed. That's kind of the point. Without oversubscription networks don't really make a lot of sense. Predictability of the behavior of oversubscription is the point - and this isn't something that has been a major design goal of transports dedicated to consumer transport. In practice I'd generally rather see a straight Ethernet connection run natively or mapped over a SONET circuit for sites with substantial bandwidth requirements but there are plenty of instances where traditional SONET framing is really the only practical solution. For edge sites the consumer edge services (inclusive of so-called business grade) are generally both sufficient and attractively priced.