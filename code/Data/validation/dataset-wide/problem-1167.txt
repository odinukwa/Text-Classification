In this case the SID of the SQL Login () will match the SID of the database user (). Restore Behaviour When you restore a database from a source SQL Server the database users (Native SQL Server Logins and local Windows Accounts only) in the source database will have different SIDs than the SQL Server Logins on the target server. This is because the SID is unique for the source and target Native SQL Server Login or Local Windows Account. The SQL Server Logins that are based on Windows Domain Accounts will always have the same SID, because SQL Server will retrieve these values from Active Directory. When you restore the database from the source to the target SQL Server the SIDs of the Native SQL Server Logins will be mismatched, even though a user might be listed in the system management catalog of the SQL Server instance and in the system management catalog of the restored database. Solution To rectify this and allow you to navigate the "SQL Server Login | permissions" and/or the "Database Properties | Permissions" you can relink these orphaned database users to the SQL Server Login. Switch to your user database and query the orphaned database users: 

Enabling Mixed Mode (aka, SQL Server and Windows Authentication mode) will get rid of this error message. 

Which Authentication Mode? Your server might have been configured to run in Windows Authentication mode only. This can be checked by opening up the Properties of your SQL Server instance in SQL Server Management Studio and looking at the Security section: 

If you are sysadmin on the SQL Server instance, then you can run the following script to see which options are available: 

I just had a look at the data in the Higher Education Statistics Agency (HESA) zipped file and the files are all different. This means you will either have to create an individual file for each import or create a single file using conditions as described in the following article: Reference: How to use multiple csv files in logstash (Elastic Discuss Forum) Expanding on your first by one level: 

Click on the Options >> button to display the login windows with Advanced Properties Advanced SSMS Login window Enter your Login and Password for a SQL Server Authentication login or select Windows Authenticated from the pull-down menu to login using your windows account. 

Truncating the logs allows for the VLFs to be reused. That's all. This process can have the benefit of preventing the TLog file from growing IF auto-growth settings have been set. If no auto-growth settings have been set, because your requirements engineering process determined that the TLog file would have a fixed size, then the worst case here is that the TLog fills up because no TLog backup occurs and hence no VLFs are being freed. The TLog can't grow and the VLFs are not freed up to allow for further transactions to be written to the TLog file (or VLFs internally). 

If I weren't using Simpana for the backup, who would (have to) delete the *.mmmmmmmm.backup files in the WAL Archive directory? The command? Why is there still a full WAL file in the Archive Log directory, when it should have been deleted like all the other WAL files in the Archive Log directory? Why isn't there a file for the still existing WAL file in the archive log directory? 

And when you run the above statements against your database, you will get a summary of the expected decompressed data for each object. Following an example output for an object: 

Description The above mentioned solution is a PHP page which allows you to enter your connection details and then just queries your database for the string. The PHP program iterates through the tables () stores this information in an array and then iterates through the of each table to retrieve the columns to search. It then performs a simple . The standard version searches only in fields, but this can be altered in the source code. 

Reduce the amount of monitored objects in EM Resize the SYSAUX tablespace according to the recommendations in above table. 

Estimate size of decompressed data With the following script you can create an output which you can then use to query all the objects that have compression turned on: 

Bypass Startup Options Alternatively try opening the Access database while bypassing any startup options: 

Normally an Access Database file is protected with the account and no password. Solution Enter into the User name: field and leave the Password: field empty. This should work. 

In order to retrieve data for the view you would have to for all the above objects. Sadly, this is not possible, because of the following restrictions: 

In the above example you are connecting to (as the source) and (as the target) and are then copying the permissions from to . So you could use: 

This is not a bug, but the way the optimizer was developed to perform, as was explained by Timour Katchaounov on the 28th February 2005: 

In my past years of being a SQL Server DBA this formula has been so far sufficient to leave enough free memory for SQL Server by-products that don't pull from the MIN and MAX memory settings and leave enough memory for the Operating System. With 32 GB of RAM the recommended setting would be 25 GB or 25600 MB for the MAX memory setting for SQL Server. 

What exactly does is probably an internal call to an object in the sqlserver.exe process or one of its linked libraries. 

The view sys.columns is a system view. This view is provided to you by the sys user in the master database, which is a database user without login. The sys user has its own schema sys which is then used to link all the sys.* objects. Permissions to select from the sys.* objects are granted to you via membership in the public SQL Server role. The SQL Server role public has SELECT permissions on all sys.* schema objects. This is how you are granted the permission to . If you are not member of the public server role, then you don't have access to any of the sys.* objects. sys user (script) 

You can't. The query plans belong in the query plan cache. You can either clear the QPC or leave the DBE to do its best. (Selective deletion of query plans can be achieved with DBCC FREEPROCCACHE(, but I wouldn't recommend this.) Determine what query plans are stored in the QPC and optimise the application and/or the memory settings for the SQL Server instance. (See script in Cigar Lounge and join with sys.dm_exec_sql_text and/or other DMV according to description for plan_handle column in sys.dm_exec_cached_plans documentation) 

You shouldn't mix file extensions with file types. You have to differentiate between file extensions and what the files actually contain. Create Database (non-standard) It is possible to create a database that has *.txt as the file extension for the Data files ("ROWS Data" type) which are otherwise created with an *.mdf or *.ndf extension. It is equally possible to create a database that has a *.txt extension for the TransactionLog file ("Log" type) which is otherwise created with an *.ldf file extension. Script to create a non-standard database 

You could force the SQL Server instance to cache a query plan for the statement or procedure that you think is missing from the query plan cache. Or you could verify that it is indeed a query plan cache issue or not. Caching a Query Plan This is accomplished with the stored procedure sp_create_plan_guide (Transact-SQL) takes the following parameters: 

Part II of II (had to split my answer) Doing it better Considering your RPO and RTO defined by your business, you now might want to alter the parameter of the to something higher than . Why? Because the value will only work together with the built-in fail-safe mechanism. Let's use the following timeline: 

If T1 decides to begin a transaction R1(X) and the commits the transaction W1(X) then everything is still in memory. At a certain point in time the DBMS will decide to write the contents of memory to disk (Transaction Log in SQL Server/UNDO - REDO Tablespace in Oracle). The Transaction Log then has knowledge of the transaction that were previously in memory. They are on disk. This includes the start of the transaction T2 with its Read R2(Y) When the system crashes and no writes can be performed, then everything in memory is lost. Your disk timeline (TLog / Redo - Undo TS) will look like this at the point of the crash: 

Addendum regarding primary key indexes On the assumption that you are using the InnoDB engine and that the PK on the tables is based on the , then all other secondary indexes will be based on this primary key as stated in the article 14.11.9 Clustered and Secondary Indexes in the official documentation. Even if you didn't create a primary clustered index yourself, MySQL will do it for you to ensure speedy queries. This is documented as: 

This will reveal some vital information. The view relies on the system function. Here Mark hits a wall and has to circle round via the view (deprecated) to reveal: 

Issue 2 This error can also occur on Windows 64-Bit systems where the Desktop Application Heap is too small. Fix for Issue 2 

In Solution Explorer, right-click the Shared Data Sources folder in the report server project, and then click Add Existing Item. The Add Existing Item dialog box opens. Navigate to an existing Report Definition Shared data source (rds) file and then click Open. Click OK. 

... implies that they probably do not know what they are doing. Turning on the Resource Governor in a productive environment could have a negative impact on the production. Blitz Result: Resource Governor Enabled If you have to enable the Resource Governor please ensure you read the documentation and understand the implications and impacts this can have. Resource Governor (2012) (Microsoft MSDN) The easiest way is to restrict their ability to log in to the server. 

Right now we don't know the specification of your "Server" so we can't tell if space might become an issue. Apart from that, the answers you are searching for are just a case of interpolating the available data along a time line. Answering your questions: 

Let's follow on with the simplistic model of my TLog file and the situation where the TLog file has to grow. Two transactions modify data. Before 

sys.dm_os_memory_clerks But instead of looking at individual caches, you might want to consider querying the view: sys.dm_os_memory_clerks 

No. And then yes. Well, it depends. What are your requirements? Backup / Restore Capabilities Do you want a backup of the database ... 

Create a SQL file with the script required to run your cleanup job Run the script with sqlcmd.exe and any required parameters Create a Windows Scheduled Task and add the command with all the required parameters 

Sometimes it helps to de-install the enterprise manager and then re-install it using the command. We'll leave that for the worst case. Using EMCA After Changing the Database Listener Port 

The answer is in the bold portion of my answer. Starters You have a table of Lead Managers () and a table for Companies (). A Lead Manager can be in multiple companies and can have multiple Lead Manager roles in one company. Inner Query The inner query retrieves the number of () Lead Manager's Codes () where the Lead_Manager's company code matches ... Outer Query ... the company code () in the Company () table. Result The result should be a list of Company Codes, Founders and distinct count of Lead Managers for each given company. The will only select unique Lead Manager', or put another way: Even if a lead manager has multiple Lead Manager Roles in a company only 1 count will be made for that company. 

VLF Calculations The number of the VLFs is not different for a manual or automatic growth of the transaction log file. SQL Server < 2014 Up to version 2014 of SQL Server the algorithm for the number of VLFs is as follows when you create, grow or auto-grow the TLog file: 

If the database backup history has the flag set to then you know that this backup was performed using a 3rd-party software that triggered the SQL Server Writer (VSS Service for SQL Server) which allowed the 3rd-party software to backup the database almost instantaneously. From the official documentation on what Snapshot Backups are: 

You might want to check out my answer here which was posted in response to the question How can I backup an SQL Server database using Windows Server Backup?. It is related to Windows Server Backup and VSS, but touches on some points you might encounter with the VSS backup strategy that has been implemented at your shop. 

(emphasis mine) Reference: msdb Database (Microsoft Docs) Conclusion If you backup the master database, then all the login information for the instance is safe. However, if you want to separately back up (as in: used to transfer logins to different instances, used to reset passwords) the SQL Logins of the instance, then you would have to apply a different solution. Individual SQL Server Login Backup Solutions Tool Reference List 

Oracle explains the V$SQL_MONITOR view in its official documentation. Looks like it only keeps the record there for a minute or two, or until it needs the space to show the other queries. 

If you are using strings in your SQL script (C:\DailyPatch\run.sql) then I would just ensure you put a N before the single quoted strings. Use this: 

There are some third-party tool from NirSoft that might help in finding the culprit. And then there is the following list that tells you which tools could possibly open the file for you: .mdmp Extension - List of programs that can open .mdmp files (Extension NirSoft) 

Filegroup 'xyz' Is Default If you receive an error message when trying to remove the filegroup's logical file that looks like this: