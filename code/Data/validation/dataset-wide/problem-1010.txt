I normalized the tables to avoid using NULLs. The problem is that some of these tables depend on each other due to business processes. Some devices must be sanitized, and some are tracked in another system. All devices will eventually be disposed in the Disposal table. The issue is that I need to perform checks, such as if the boolean field is true, then the cannot be entered until the fields are entered. Also, if the boolean value is true, then the fields must be entered before the can be entered. If I merge all of these columns into the table then I will have NULL fields, but I will be able to manage all of the business rules using CHECK constraints. The alternative is to leave the tables as they are, and manage the business logic in the stored procedure by selecting from the tables to check if records exist and then throw appropriate errors. Is this a case where NULL can be used appropriately? The boolean fields and basically give meaning to the NULL fields. If is then the device is not tracked in the other system and and are NULL, and I know that they should be NULL becuase it is not tracked in the other system. Likewise, and I know will be aswell, and a can be entered at any time. If is , then and will be required, and if and are NULL, then I know they have not been officially removed from that system yet and thus cannot have a until they are entered. So it's a question between separate tables/no NULLs/enforce rules in stored procedures vs combined table/NULLs/enforce rules in CHECK constraints. I understand that querying with NULLs in the picture can be complex and have somewhat undefined behavior, so separate tables and stored procedures seem beneficial in that sense. Alternatively, being able to use CHECK constraints and have the rules built into the table seems equally beneficial. Any thoughts? Thanks for reading. Please ask for clarification where needed. Update Example table if they were merged and I allowed NULLs. 

I have a very simple "inventory management" update that under load causes InnoDB to deadlock and rollback one of the threads. There doesn't appear to be any ordering issue here, so I'm at a loss how to resolve it. MySQL version is 5.6.33-0 

When I run mysqldump --routines I'm seeing multiple collations for my stored procs. Neither my DB nor columns use utf8mb (I understand the benefits of utf8mb over utf8 but let's leave it for now). 

During a series of statements in my stored procedure I wish to log an entry by inserting a record. That record needs to survive even if the overall SP ends up rolling back. Is this possible? 

I'm aware of the limitation "every unique key on the table must use every column in the table's partitioning expression". So given that my primary key is a UUID, it's going to require a change. Background: I have a transaction history table that grows at about 1.5G per month (more so than I can keep upgrading server memory). The table has a PRIMARY key (uuid_short) and a datetime column, among others. There are two types of queries hitting the table: those that return a list of transactions in a date-range, and those that refer to a specific transaction through its primary key. Question: Since vast majority of queries don't care for transactions more than 18 months old I thought datetime could be a candidate for range partitioning. Does this mean I'm better off dropping the PRIMARY key and converting it to a non-unique key? (dups are not a problem, the app servers guarantee there cannot be any). The system is currently a hybrid of OLTP and OLAP scenarios which is probably why it doesn't scale. Is partitioning going to make a positive impact on memory footprint? If not, then I may be better off reorganizing the system by moving historical transactions to a data warehouse like AWS Redshift. It would be quite a refactoring effort, but if that's the preferred solution then I'd like to plan early. This is MySQL 5.6. 

The table looks like this in this setup (Notice the addition of record with id #4, and the field which specifies that this status is for use with devices that can can connect to a network): 

I put check constraints on the table as follows to ensure a network status and network information can only be provided if the device is capable of connecting to a network. 

This question regards the proper use of NULL and utilizing CHECK constraints for business logic vs stored procedures. I have the following tables setup. 

The issue I have with this design is I'm not sure if the relationship with and / is a good or bad design decision. Is propagating a non-key field like to other tables a bad design? I don't have enough experience with database design to make an informed decision. 

Slave_SQL_Running_State: Reading event... Suggests that it's working on a large event. If you have MIXED or Row based binary logging and have changed a large amount of data on the master you might end up with a large event or series of large events. Use the mysqlbinlog tool to see what's going on with your relaylog. The master might show a concentration of binary logs over a small period of time (on the filesystem) 

You can write a simple script externally or use the MySQL event scheduler. Both will work just fine. The benefit of the MySQL event scheduler is that it will be part of your database backups. 

I suspect you're suffering 'out-of-the-box' syndrome. For the whole world to know... MySQL config out of the box sucks. Go to $URL$ and create better configuration based on the questions asked in the form. This should provide you with a better starting point. Once the output is generated, add it to your configuration file (/etc/my.cnf) and restart MySQL; review the mysql error log to ensure that your mysql instance started cleanly (beware of paths, permissions and the innodb_log_file_size size differences since that will require disposal of the old ib_log* files before new ones can be created). Once done, try your import again. 

Is this correct that a "CREATE TEMPORARY TABLE IF NOT EXISTS.." does not commit current transaction (I like that), but a TRUNCATE TABLE, which is temporary, does implicitly commit any transaction? Does this mean that the only way to have a fresh temporary table without committing the current transaction is to run (after above DDL statement): DELETE FROM temp-table-name; This TRUNCATE auto-commit behavior caused a rather nasty bug in my app -- the rollback didn't go all the way up because of the "TRUNCATE barrier" ;) 

Is there a way to "roll up" duplicate records such as time-series data in MySQL? I'm currently doing this in the application code after retrieving full time-series but the transmission of mostly redundant rows seems a nuisance. Here is an example: Original data: 

From what I'm reading, the interaction between a shared (S) lock and an exclusive (X) lock causes my concurrent updates to deadlock. Is there anything I can do to avoid this? Update: I believe this is a bug in MySQL. Two concurrent threads acquire Shared-locks and try to acquire eXclusive locks to do the update. They wait for the other to release the Shared-lock first. There is no ordering issue in the way locks are acquired; the mixing of types of locks is the issue. MySQL should be able to "upgrade" the existing shared-lock to eXclusive. If another thread wants to do the same, the first thread should make progress by obtaining eXclusive lock, do the update, then release the lock so the other Shared-lock can be upgraded, and so on. Workaround My UPDATE statement was modified to use the PRIMARY KEY (from previous UNIQUE KEY). In addition, I'm first calling a SELECT matching the condition of the UPDATE and skip the UPDATE entirely if the SELECT comes back empty, thus avoiding the eXclusive lock whenever possible. All of this is in a stored proc so clients aren't affected. 

This design eliminates the need to propagate across the tables. The issue I see with this design is that every device that has network capability will have records in paired with ids 1, 2 and 3, while devices that can't connect to a network will be paired only with id 4. It seems like a lot of extra records that all mean the same thing: devices that can be networked can only use statuses 1, 2 and 3, and devices that can't network only use 4. This design seems to be more "relationally correct", but also smells a bit. Update The following update proposes variations on Design 1. I come across situations like this often, where there are many ways to achieve the same end result. I never know how to tell if there are hidden problems with the designs, and I can't judge when to normalize or denormalize. Is one of these designs preferred over the other and why? Design 1.1 

I'm trying to configure external slave connecting to Aurora RDS. To my delight, I run into a silly problem where the Aurora cluster name - assigned by AWS - exceeds permissible MASTER_HOST limit of 60 characters. I want to use the cluster name (not IP) since it can be dynamically changed by Route53 on failover. Can anyone suggest a solution? 

The way I resolved it is I clicked on Modify in AWS/RDS console and shortened the DB Cluster Identifier so the resulting cluster endpoint fit in 60 characters. 

The last 48 hours or so I spend on trying to determine the cause for sporadic crashes of my MySQL (Aurora) DB. Turns out they were attributed to my mixing of database, table, column and stored procedure collation/charsets. My bad, or is it? Aurora aside, what were the practical reasons for all the ways to configure charset/collation in MySQL? Why couldn't utf8 just become utf8mb4 as part of a major release? Why changing charset or collation of a database/table doesn't actually change the database or table, only future additions to the schema? The last one provides for an opportunity to have a mix of columns charset/collations in your tables, for instance. What sane person would want to have different charsets/collations within the same DB? And if there is a rationale for all this madness, why not make it the exception rather than the norm when configuring mysqld? In my case, to resolve mysqld crashes I had to ensure all of below return consistent charset/collation values: 

This is an inventory database for IT assets. The models used are trimmed in order to focus on the problem at hand. Using SQL Server 2008. Thanks for taking the time to read and for any input you can provide. My design includes a table which holds the various devices that can be entered into inventory. Each device has a boolean flag, which states whether a device has network capability, e.g., for most computers , for hard drives ; some printers will be true, and others will be false. You get the idea. The field determines if network-related information is relevant when an inventory record is created. Design 1 My first design uses an index on and to use in a foreign key constraint with the table. 

With this setup, devices are archived by setting field to true and entering an . I could query any device easily whether it is active or archived. (Please ignore the field, as this is used for an unrelated concept). Take notice of the Phone subtype table, where I had to propagate an index of DeviceID and IsArchived flag because phone numbers must be unique for active devices. I have to do this with other subtype tables as well. I don't know if this is a good or bad design. This part really confuses me... What are best practices for handling soft deletes where foreign key values can be marked as deleted. The only thing I can think of is create a routine that searches for all records that are related to deleted data, and create a report for users to resolve the discrepancies. For example, if a table of locations is related to the devices table, and some locations are soft deleted, then the devices refer to locations that no longer exist and must be moved. By the way, I'm using MS SQL Server 2008 R2, and I plan on using Entity Framework 4 in my application. I value database maintainability over performance. Thank you for reading.