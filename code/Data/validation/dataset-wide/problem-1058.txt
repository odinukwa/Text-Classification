I'm conducting time trials on a new SSD array running both SQLIO tests and a real-world workload of DB restores and DBCC CHECKDB calls. I'm seeing a major discrepancy between the IOPS and throughput generated with my SQLIO batches and what I'm observing with the workload, with the workload only requesting a fraction of what I was able to observe with SQLIO, usually in the 5,000 IOPS range and generating no more than 400 MB/s throughput. Is there an inherent limitation as to how many resources DBCC CHECKDB will consume event if the hardware has more than sufficient capacity to handle the load? What settings can I experiment with to expand DBCC CHECKDBs usage of CPU and disk resources? Here are the specifics... From 

Is this particular expression not supported by PBM or is there alternate syntax I should use in the condition? 

One option may be to add individual sysadmins as users to the databases in question, add those users to a custom role with no explicit permissions (effectively like public), and setup the database audit specification to track actions for the custom role. 

C:\SQLIO>sqlio -kR -t8 -s120 -o8 -fsequential -b64 -BH -LS -Fparam.txt sqlio v1.5.SG using system counter for latency timings, 2211143 counts per second parameter file used: param.txt file L:\testfile.dat with 8 threads (0-7) using mask 0x0 (0) 8 threads reading for 120 secs from file L:\testfile.dat using 64KB sequential IOs enabling multiple I/Os per thread with 8 outstanding buffering set to use hardware disk cache (but not file cache) using specified size: 40000 MB for file: L:\testfile.dat initialization done CUMULATIVE DATA: throughput metrics: IOs/sec: 25160.07 MBs/sec: 1572.50 latency metrics: Min_Latency(ms): 0 Avg_Latency(ms): 2 Max_Latency(ms): 8 histogram: ms: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24+ %: 24 33 12 7 7 9 6 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 C:\SQLIO>sqlio -kW -t8 -s120 -o8 -frandom -b8 -BH -LS -Fparam.txt sqlio v1.5.SG using system counter for latency timings, 2211143 counts per second parameter file used: param.txt file L:\testfile.dat with 8 threads (0-7) using mask 0x0 (0) 8 threads writing for 120 secs to file L:\testfile.dat using 8KB random IOs enabling multiple I/Os per thread with 8 outstanding buffering set to use hardware disk cache (but not file cache) using specified size: 40000 MB for file: L:\testfile.dat initialization done CUMULATIVE DATA: throughput metrics: IOs/sec: 153634.35 MBs/sec: 1200.26 latency metrics: Min_Latency(ms): 0 Avg_Latency(ms): 0 Max_Latency(ms): 1 histogram: ms: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24+ %: 100 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 C:\SQLIO>sqlio -kR -t8 -s120 -o8 -frandom -b8 -BH -LS -Fparam.txt sqlio v1.5.SG using system counter for latency timings, 2211143 counts per second parameter file used: param.txt file L:\testfile.dat with 8 threads (0-7) using mask 0x0 (0) 8 threads reading for 120 secs from file L:\testfile.dat using 8KB random IOs enabling multiple I/Os per thread with 8 outstanding buffering set to use hardware disk cache (but not file cache) using specified size: 40000 MB for file: L:\testfile.dat initialization done CUMULATIVE DATA: throughput metrics: IOs/sec: 181107.89 MBs/sec: 1414.90 latency metrics: Min_Latency(ms): 0 Avg_Latency(ms): 0 Max_Latency(ms): 5 histogram: ms: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24+ %: 100 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 

You can also look at the application event log. For SQL 2008 R2 instances, the event ID to filter on would be 8957 (at least for success codes). 

I'm practicing splitting a partition function on a QA box to add new partitions for the upcoming calendar year. I'm running SQL Server Enterprise edition, patch level 12.0.4100. Here is the current function definition... 

I have a handful of archive DBs located on an Isilon NAS share that SQL lost sight of during a heavy I/O event, resulting in 823 errors like the one below. 

Two options come to mind. If you go with the local copy approach you can manually use the AutoFill feature to fill in the blank values. For an automated approach, the one approach I would think of in T-SQL is to use a cursor to loop through all the rows, capture the non-NULL Dates and Types into variables and when you encounter a NULL value, UPDATE the current record with the last non-NULL value. An integer identity may be needed in the table you're importing to confirm the rows have been inserted in the order as found in the file. 

On larger DDL scripts for deployments, usage of SET NOEXEC ON as part of error handling is commonplace to prevent execution of downstream code. If coupled with SET NOCOUNT ON, there's a chance code execution may be inadvertently suppressed unless the proper post-deployment checks are done. I checked sys.dm_exec_sessions and there is no attribute for either SET statement. Is there an alternate method of checking the status of either statement for the current session or any user's session from the DBA perspective? Here's a quick example... 

I'm testing creating DBs on an Isilon NAS for potential future use to relocate read-only archive DBs. The NAS admin created a file share with my sysadmin domain group and the SQL Service account having Full Control via a common domain group. My SQL Server 2014 instance creates the DB successfully and I can see DB files in the subdir in the context of the service account, but not under my sysadmin context. I read in a related topic that the service account has Full Control and the file owner is the local Administrators group, so I'm assuming it's a missing attribute assigned the DB files that should allow the domain group (including the sysadmins) to see the files in the directory tree. What file system command should be issued by the service account to allow the file to be visible to other members of the domain group? 

Check for any external connections connecting on the same port # using . Sometimes application services that automatically connect to the instance keep connecting even if you've declared a maintenance window like this. Once identified, disable them at the source, or temporarily block them by firewall rule, change in port # or disabling the external protocols in Configuration Manager (Named Pipes and TCP/IP). 

Three partition schemes were defined at the time of function creation but only the first one is utilized with data files added. 

I've tried both approaches for my data warehouse, but I've gone with option B, creating quarterly databases to hold historical data. In my situation, my source messages are written once, read once for ETL purposes and archived for legal purposes. By archiving them into quarterly DBs, I've been able to 1) deploy them to inexpensive SATA storage, 2) flag them as read-only for data protection and performance since it cuts the locking required, and 3) improve my RTO as I can restore a smaller 'current' DB faster than having to wait for all my historical DB to restore as well before recovery. Instead of the view using , you may want to use some application-based logic to from the correct DB depending on the criteria. 

In the above example, if the filegroup doesn't exist, preventing table creation, the check will call the statement which will prevent actual execution of the subsequent statement. I've seen this type of code generation with tools such as RedGate SQL Compare. The catch is, if the person executing the code neglects to run to reset the session during debugging, they may get a message regardless of the actual outcome. 

There shouldn't be a performance gain by the third option over the second. When you think about it, it's almost the same as having all your user objects in the PRIMARY filegroup. One performance advantage you can achieve with the second option is to flag the older historical filegroups as read-only once you've confirm no additional data modifications are expected, indexes are rebuilt and statistics updated. Once read-only has been implemented, locking doesn't become an issue when querying data in those filegroups. 

I'm running time trials on a new Windows 2012 Server host with PureStorage FlashArray SSD LUNs attached. I conduct an initial test with CrystalDiskMark and discover write performance is substantially better than read performance. 

By any chance are multiple instances of SQL Server running on this host machine? Check the Services console to see if more than one service of SQL Server is running. If so, the newly discovered one may be running the older service pack. 

My initial thoughts would be solution of two DBs per client, one for the datamart and one for the ODS. The ODS would be in full recovery mode as I don't think you can recreate the data here without retrieving it again from the upstream sources at your clients, if at all. The datamart may be in simple recovery mode as all the source data is coming from the ODS and can reloaded from scratch if needed, but the trickle-in fact table makes it complicated. I'm inclined to move the fact table into the ODS to have better options to enforce RI and other data cleansing needs before it goes to the datamart (via a DML trigger or seperate scheduled ETL perhaps). Also, if you can make the datamart DB read only in between ETL cycles, you'll benefit from not having SQL have to manage locks during the report requests. I don't think you have an option to use a central schema/DB for the non-sensitive content. From a scalability perspective, having each client's data structures in their own containers allows you migration options to faster disks or another host if needed without impacting the other clients or sustaining a larger downtime. From a business logic perspective, if a particular client needs a business rule change that impacts the schema, will the central DB be able to accomodate it without negative impact? Also from scalability, think of the risks caused by locks on a table caused by one client impacting other clients concurrently connected. 

I'm attempting to upgrade a SQL 2005 SP3 clustered instance to SP4. During installation, a rollback event took place with the error indicating there was insufficient space on the the disk. The system volume and application binary volume both have adequate space. On examination of the hotfix logfile, the config.msi target folder for the rollback files was the volume used as a mountpoint folder (in this case, M: was the target but it's only 128 MB in size, hosting only folders to be used as mountpoints for other physical disks, like M:\DATA00). Why would the SP install routine use that volume instead of the system volume or SQL binary volume for the rollback file location and can the target path for the rollback file be altered? 

My SQL Server instance is hosted on an internal network server (10.x.x.x address). It's a named instance currently using dynamic ports. An external application at a third-party hosting location connects to the instance through use of the IP address and port #. After each maintenance window I have to confirm the dynamic port # in use is the same as before as the firewall rule is set to allow connections between application host and database host on that specific port #. Is this the correct way to ensure the externally-hosted app can connect to the host without having to change port numbers in the connection string or should another port forwarding method be used (i.e., port 9999 on application host is mapped to port 1434 on SQL host)? What configuration manager changes in the Network Configuration section should be applied to allow the external application to use the same port # but internal applications to rely on the Browser server to make the correct mapping? 

I was able to get guidance from the following MSDN link: Configure Windows Service Accounts and Permissions. The pertinent section (bookmarked in the link) is "File System Permissions Granted to SQL Server Per-service SIDs or Local Windows Groups". The short version for the solution is that either Read, Execute or Full Control are needed on the leaf-level subdirs under the system root (DATA, LOG, FTDATA, etc). On my host, NTFS permissions weren't propagating from the mountpoint volume root or the folder by which the SYSDB LUN were mounted. Applying NTFS permissions on the leaf-level corrected the issue. 

On Server B, review of the execution plan indicates that the criteria for table [a] at bottom is part of an index seek operation at the start of the execution plan, which makes the subquery [c] resultset execute rapidly. On Server A, review of the same execution plan indicates that subquery [c] is executing first with full index scans due to outer criteria not being applied. Indexes utilized in both execution plans are identical. Table rows counts are slightly larger on Server A since the restore due to normal operations but index fragmentation is nearly identical. A participating index on table [t] in the subquery on Server B has double the number of pages as on Server A but identical row count. Statistics are updated nightly at the same time on both servers. I've attempted index rebuilds on table [t] and manually updating statistics to attempt to get both execution plans to match. What other factors may be causing this change in the order of execution? I suggested to the developer replacing the subquery with a UDF that takes the EID and UniqueDocumentNumber fields from [doc] as arguments. What other options can I explore with the developer to increase the probability of the execution plans on QA being utilized on Prod? 

Reading up on Kimball dimensional modeling, use of a surrogate key, in particular an IDENTITY, will help reduce fragmentation caused by page splits as you'll be appending rows at the end of leaf pages vs. attempting to insert them in the middle if the keys are not in the order of the index (ascending or descending depending on how the index was defined). But, if the surrogate key will not be used in joins with other tables or in WHERE criteria, use of the surrogate key as a clustered index might not provide any additional benefit after the initial data import. If you keep the composite clustered key, sorting the source data in the order of the six-column composite key prior to import should avoid the page split fragmentation you're witnessing. As for a choice of clustered index, the best candidate may be the column(s) that would be used in your most common/critical queries for WHERE criteria or joins. Use of a NCI will require a bookmark lookup to obtain the remaining values in the table if needed in the resultset. 

It may be due to the fact that you specified in your connection string. This will have the connection be established using Windows authentication using the domain account you're currently logged in as at the client, effectively ignoring the UID parameter. Try removing that parameter and rerunning. You may need to provide the password in the connection string, but since that's not secure, you could also keep the Trusted Connection parameter, put your users' Windows logins in a domain group and grant the domain group SELECT privs to the appropriate objects. 

Two suggestions come to mind. One is that the clustered index on both ApplicationEvent and ApplicationFault above be on the LogTime column. Assuming the data is posted to the table in chronological order, you'll have reduced fragementation from page splits and benefit from range scans when purging out older time periods. The second builds on the first to implement parititioning based on the LogTime column. Instead of the relatively expensive I/O during the delete operations, you can implement a sliding window which would 'slide' older time periods out the main table with ALTER TABLE commands, effectively dropping unneeded dates via essentially meta commands.