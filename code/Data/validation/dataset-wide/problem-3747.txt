I'm not sure if the converse i.e. a finite supersolvable group is a $B$-group, holds in general, as it is not mentioned by the author of the paper here. I tried to prove the assertion but it eludes me. The next best thing would be finding a counterexample of a finite supersolvable group $G$ with some $p$-subgroup $H$ that is neither normal nor abnormal in $G$. Any help with finding this counterexample would be greatly appreciated. 

Now $X'^{\langle x_i \rangle} = X'$. Also $\bar\omega(G) \unlhd G$ and $\bar\omega(G) \subseteq P_G(X')$. Letting $H = X'$ and $K = \langle x_i \rangle$ in Lemma 2, we deduce that $\bar\omega(G) \subseteq P_G(\langle x_i \rangle X' = P_G( \langle x_i, X' \rangle)$ 

The following is Theorem and proof of a research paper which I am having a problem trying to understand: Theorem: Let $G$ be a finite solvable group. Then $P(G) = \bar \omega(G)$ I have managed to show the inclusion $P(G) \subseteq \bar \omega(G)$. The reverse inclusion goes like this 

I have managed to show that (a) implies (b) and (c) implies (a). I am having a problem showing that (b) implies (c). Asumme that $H \leq K \unlhd L \leq G$. We will show that $H$ is weakly pronormal in $G$. It suffices to show that $L \leq N_L(H)K$. 

Let $U_1,U_2,\ldots,U_n$ be $n\geq 2$ mutually independent Bernoulli random variables. There are two cases of interest: $1.$ The random variables $U_1,U_2,\ldots,U_n$ are identically distributed; 

Let $\mathscr P _0$ and $\mathscr P _1$ be two non-overlapping sets of probability distributions defined on $(\Omega,\mathcal{A})$. Consider the distance defined as $$D_u(P_0,P_1)=\int_\Omega \left(\frac{p_1}{p_0}\right)^u p_0 \mathrm{d}\mu<\infty.$$ Two distributions are chosen from each set $Q_0\in\mathscr P _0$ and $Q_1\in\mathscr P _1$ such that $$D_u(Q_0,Q_1)\geq D_u(P_0,P_1)\quad \forall (P_0,P_1)\in \mathscr P _0\times \mathscr P _1,\forall u\in[0,1]$$ 

$\mu$ can be the Lebesgue measure, although I believe the same holds for the counting measure and the discrete sets. The set $\Omega$ can be $\mathbb{R}$ or an interval of real numbers. I am especially interested in the last ''not'' case, and for this case if necessary $g_U$ and $f_U$ can be assumed to be integrable over $\Omega$. I had previously asked this question at math.stackexchange but with no answers. Addendum: For $f_U=\infty$ and $g_U=\infty$ I know that $$\frac{g}{f}(y)=\begin{cases}c_1\quad\mbox{if}\quad g/f<c_1\\ h(y)\quad\mbox{if}\quad c_1\leq g/f \leq c_2\\ c_2\quad\mbox{if}\quad g/f>c_2\\\end{cases}$$ where $c_1$ and $c_2$ are some constants and $h$ is a function of the bounding functions for example $g_L/f_L$. I think the same is true for the case $f_L=0$ and $g_L=0$. I have some work on the solution of the problem with KKT multipliers, which may help solving this problem too. 

Let $A\subset \mathbb{R^2}$ be a finite set such that $|A|=k^2$. Let $x_i\in \mathbb{R^2}$, $i=1,2,3,4$, be four points in the plane in general position (no three lie on any line). Let us form the multiset of cardinality $4k^2$ out of the four translates $A+x_i$ and call it $M$. Question: Can we partition $M$ into four sets (not multisets) $A_i$, $i=1,2,3,4$ with cardinalities $(k+1)^2$, $(k-1)(k+3)$, $(k-1)^2$, $(k-1)^2$, respectively? Question: Are there any good references for problems of this kind? 

Let $\varepsilon_1, \ldots, \varepsilon_n$ be independent random variables taking values $0,1$ each with probability $1/2$. It is well known that $R_n=\varepsilon_1+ \cdots+ \varepsilon_n$ modulo a prime $p$ tends to the uniform distribution on $\mathbb{Z}_p$ (say, in total variation distance, but also in a lot of other senses). Is it known what the speed of convergance is? Let us say I want to bound $\delta_n=\sup_{k}|\mathbb{P}(\varepsilon_1+ \cdots+ \varepsilon_n=k)-1/p|$ in terms of $n$. Is it true that $\delta_n$ converges to $0$ exponentially? If this is true, could you provide a decent reference? Many thanks for the attention. 

Let us say have a sequence of $n$ 2-$D$ random variables $X_i=(\varepsilon_i/\sqrt{n},i\varepsilon_{i}\sqrt{6}/n^{3/2})$, where $\varepsilon_{i}$ are independent random variables such that $\mathbb{P}(\varepsilon_i =\pm 1)=1/2$. Denote by $S_n$ their sum and take a $2$-dimensional Gaussian random variable $Z$ with the same covariance matrix as $S_n$. Any standard Berry-Esseen theorem (say by Gotze or Bhachattarya or Bentkus) gives us that for any convex set $C$ we have \begin{equation} |\mathbb{P}(S_n\in C)-\mathbb{P}(Z\in C)|\leq c\gamma, \end{equation} where gamma is the sum of third absolute moments of $X_i$, which in this case behaves like $n^{-1/2}$. Question: is there a standard way to pass from the distance between distribution functions to expectations of Lipschitz functions? That is, suppose $f$ is Lipschitz, can we still bound \begin{equation} |\mathbb{E}f(S_n)-\mathbb{E}f(Z)| \end{equation} in therms of $\gamma$? If so, does the same bound of magnitude $\gamma$ still apply? 

Please feel free to comment and post a new anwer based on mine. Because still there are missing points. 

There are two sets defined: $$\mathcal{S}_0=\left\{\{u_1,\ldots,u_n\}:\prod_{k=1}^n P_1(U_k=u_k)\leq \prod_{k=1}^n P_0(U_k=u_k)\right\}$$ $$\mathcal{S}_1=\left\{\{u_1,\ldots,u_n\}:\prod_{k=1}^n P_1(U_k=u_k)> \prod_{k=1}^n P_0(U_k=u_k)\right\}$$ and the corresponding objective function: $$R(n,(p,q))=\frac{1}{2}\left(\sum_{\{u_1,\ldots,u_n\}\in \mathcal{S}_0} \prod_{k=1}^n P_1(U_k=u_k)+\sum_{\{u_1,\ldots,u_n\}\in \mathcal{S}_1} \prod_{k=1}^n P_0(U_k=u_k)\right)$$ $2.$ The random variables $U_1,U_2,\ldots,U_n$ are not necessarily identically distributed; 

Here are some remarks: $1.$ It is known that the inequalities above hold for example if $$\mathscr P_0=\{P_0|\,|P_0(A)-F_0(A)|\leq\epsilon_0\,\,\forall A\in\mathscr A\}$$ $$\mathscr P_1=\{P_1|\,|P_1(A)-F_1(A)|\leq\epsilon_1\,\,\forall A\in\mathscr A\}$$ where $F_0$ and $F_1$ are some predefined reference measures. However, in this case all $P_0$ and $P_1$ are not necessarily absolutely continuous w.r.t. a common measure. $2.$ In the paper $\Omega$ is called as an infinite set. I wonder if there is something different when $\Omega$ is countable or uncountable. I am more interested in the case when it is uncountable, and for simplicity it can be chosen $\mathbb{R}$ or any interval of it. $3.$ If necessary one can consider the following sets: $$\mathscr P_0=\{P_0|D(P_0,F_0)\leq\epsilon_0\}\quad and\quad \mathscr P_1=\{P_1|D(P_1,F_1)\leq \epsilon_1\}$$ where $$D(P,F)=\int_\Omega p\log(p/f)\mathrm d\mu$$ and here every $P$ is absolutely continuous w.r.t. $F$, and $\epsilon_0$ and $\epsilon_1$ are some positive numbers such that $\mathscr P_0\cap \mathscr P_1=\emptyset$. 

Here $\max_{\theta\in(0,0.5)}\max_{C_\theta\in\mathcal{C}_\theta}$ corresponds to all such convex curves. One can just put a single $\max$. 

Let $G = A \times B$. Suppose that $H \leq G$ such that $N_G(H) = N_A(\pi_A(H)) \times N_B(\pi_B(H))$ where $\pi_A$ and $\pi_B$ are the respective projection homomorphisms For simplicity and convenience, we can identify $G$ as being an internal direct product of $A$ and $B$ Proposition 1: $N \unlhd A \times B$ if and only if $[N, X] = [\pi_X(N), X] \leq N \cap X$ for $X = A, B$ Definition For $H \leq A \times B$, we define $C_X = \{x \in X \,|\, [\,x, \pi_X(H)\,] \leq X \cap H \}$ for $X = A, B$. Proposition 2: If $H \leq A \times B$ then $N_G(H) \cap X = C_X$ for $X =A, B$. 

Definition: Let $G$ be a finite solvable group and $\Sigma \in \text{H}(G)$, the set of Hall systems of $G$. The normaliser of $\Sigma$ is defined as $$ N_G(\Sigma) = \{ g\in G \,|\, H=H^g \text{ for all} H \in \Sigma \}.$$ A system normaliser of $G$ is a subgroup of the form $N_G(\Sigma)$ for some $\Sigma \in \text{H}(G)$. I have shown that a system normalizer of a finite solvable group covers the central chief factors and avoids the eccentric chief factors of $G$. Lemma: Let $U$ be a subgroup of a finite group $G$. Let $W \leq V \leq G$. Then $V/W$ is covered by $U$ $\iff$ $[U \cap V: U \cap W]=[V:W]$ 

Notes: $\bullet$ One can consider A or B since both conditions are equivalent. $\bullet$ $p_0$ and $p_1$ are densities of $P_0$ and $P_1$ and the same goes to $q_0$ and $q_1$ with $Q_0$ and $Q_1$. What I know: From Huber's paper (pages 260-261) Theorem 6.1 I know that if the distance is the $f$-divergence, i.e. $D_f$, then A and B are correct. Additionally, if A and B are correct, then $Q_0$ and $Q_1$ minimize $D_f$ (iff condition). Huber considers $$Q_{jt}=(1-t)Q_{0t}+t Q_{1t}\\q_{jt}=(1-t)q_{0t}+t q_{1t}$$ and finds the first and second derivatives of $D_f(Q_{0t},Q_{1t})$. He then shows that the second derivative is $\geq 0$ (convex) and hence $(Q_{00},Q_{10})$ minimizes $D_f$ if and only if the first derivative evaluated at $t=0$ is $\geq 0$ for all $(Q_{01},Q_{11})\in(\mathscr P _0\times\mathscr P _1)$. He shows that this is really the case, hence the claim is true. I think that this result can be strenghtened, i.e. if $(Q_0,Q_1)$ maximizes $D_u$ for all $u\in[0,1]$, then it should satisfy A or equivalently B. I dont know how to proceed. Addendum: It seems that the question eventually boils down to finding $(Q_0,Q_1)$ which maximizes $D_u$ for all $u\in[0,1]$ and fails to minimize $D_f$ for at least one $f$. This will be a counterexample to the claim (of course if there exists such a pair). 

Let $G=(V,E)$ be a finite graph and let $f$ be any positive function defined on the vertices. Put weights on the vertices $v_{i}$, way $w_{i}$ so that $\sum_{i=1}^{n}w_{i}\leq 1$. Assume that every independent set of vertices, say $I$, satisfies $\sum_{v_i\in I}w_{i}\leq 1/2$. I would like to maximize over all choices of the weights the following expression (the average of f):$\\$ $$\sum_{i=1}^{n}f(v_i)w_{i}.\\$$ Question: is it true, that at least one global maximum is achieved by either i) putting weights $1/2$ on a pair of two neighboring vertices or ii) putting a weight $1/2$ on one vertex and $0$ on all of the others? Remark: the second situation can arise, for example, in the case $G$ is the empty graph and the value of $f$ at one vertex is strictly larger than on the other vertices. 

Let $P$ be a finite poset. Assign the following diagram to it - put the maximal element of $P$ on the first level, the maximal of the rest to the second level, etc. Assume that the diagram is connected, that is, that between each pair of elements in the diagram we have a chain connecting them. Let $L_n=P\times P\times\cdots\times P$ ($n$-times) to be the product poset, that is, one element in $L_n$ is larger than another if we have all inequalities coordinate-wise. Is it true that there exists and $N=N(P)$ such that for $n>N$ the following is true: In the diagram of L_n there is a perfect matching between all consecutive levels (from the one with smaller size to the one with larger size). Note: due to the useful feedback of Andres Caicedo and David Speyer the question was altered. 

I have the following convex optimization problem: $$\begin{array}{ll} \text{maximize}_{{f,g}} & \displaystyle\int_{\Omega} g^u{f}^{1-u}\mathrm{d}\mu\\ \text{subject to} & \displaystyle\int_{\Omega} f \mathrm{d}\mu= 1,\quad \displaystyle\int_{\Omega} g\mathrm{d}\mu =1 \\ & f_L \leq {f} \leq f_U\\ & g_L \leq g \leq g_U\end{array}$$ where $u\in(0,1) $ and $$\int_{\Omega}f_L \mathrm{d}\mu< 1,\quad\int_{\Omega}g_L \mathrm{d}\mu< 1$$ $$\int_{\Omega}f_U \mathrm{d}\mu> 1,\quad\int_{\Omega}g_U \mathrm{d}\mu> 1$$ Here, $f$ and $g$ are distinct density functions and $f_L,f_U,g_L,g_U$ are some known positive functions on $\Omega$. 

My Own Work: For case $1$, I am able to simplify the problem considerably as follows: Assume we are given a certain $\{u_1,\ldots,u_n\}$. It will have $r$ times $1$s and $n-r$ times $0$s. The condition to assign it to either $\mathcal{S}_0$ or $\mathcal{S}_1$ is $$\prod_{k=1}^n \frac{P_1(U_k=u_k)}{P_0(U_k=u_k)}\lessgtr 1\Longrightarrow \sum_{k=1}^n \log\frac{P_1(U_k=u_k)}{P_0(U_k=u_k)}\lessgtr 0$$ Since $U_k$ are identically distributed above given condition can be written as $$r\log\frac{P_1(U_k=1)}{P_0(U_k=1)}+(n-r)\log\frac{P_1(U_k=0)}{P_0(U_k=0)}\lessgtr 0$$ which can be rewritten as $$n\log\frac{P_1(U_k=0)}{P_0(U_k=0)}+r\left(\log\frac{P_1(U_k=1)}{P_0(U_k=1)}-\log\frac{P_1(U_k=0)}{P_0(U_k=0)}\right)\lessgtr 0$$ Hence, we have $$r\lessgtr \frac{n\log\frac{P_1(U_k=0)}{P_0(U_k=0)}}{\log\frac{P_1(U_k=0)}{P_0(U_k=0)}-\log\frac{P_1(U_k=1)}{P_0(U_k=1)}}\Longrightarrow r\lessgtr t=\frac{n\log\frac{q}{1-p}}{\log\frac{pq}{(1-p)(1-q)}}$$ Here we have $r=\sum_{k=1}^n U_k\sim \operatorname{Binomial}(n)$. Hence, $$\mathcal{S}_0=\{\{u_1,\ldots,u_n\}:r\leq t\}\\ \mathcal{S}_1=\{\{u_1,\ldots,u_n\}:r> t\}$$ and as a result of above $$\sum_{\{u_1,\ldots,u_n\}\in \mathcal{S}_0}\prod_{k=1}^n P_1(U_k=u_k)=P[r\leq t\mid U_1\sim\operatorname{Bernoulli}(1-q)]=\sum_{k=0}^t\binom{n}{k}(1-q)^k q^{n-k}\\ \sum_{\{u_1,\ldots,u_n\}\in \mathcal{S}_1}\prod_{k=1}^n P_0(U_k=u_k)=P[r>t\mid U_1\sim\operatorname{Bernoulli}(p)]=1-\sum_{k=0}^t\binom{n}{k}p^k (1-p)^{n-k}$$ Consequently, $$R(n,(p,q))=\frac{1}{2}+\frac{1}{2} \sum_{k=0}^t\binom{n}{k} \left[(1-q)^k q^{n-k}+p^k(1-p)^{n-k}\right]$$ where $$t=\frac{n\log\frac{q}{1-p}}{\log\frac{pq}{(1-p)(1-q)}}$$ as found above. For case $2$, the things are getting complicated because each $U_k$ is Bernoulli with different success probabilities. What I have is the following: $$\{u_1,\ldots,u_k=0,\ldots,u_n\}\in\mathcal{S}_1^*\Longrightarrow \{u_1,\ldots,u_k=1,\ldots,u_n\}\in\mathcal{S}_1^*$$ Similarly, $$\{u_1,\ldots,u_k=1,\ldots,u_n\}\in\mathcal{S}_0^*\Longrightarrow \{u_1,\ldots,u_k=0,\ldots,u_n\}\in\mathcal{S}_0^*$$ This is because of the convexity of $C_\theta$, i.e. $$\frac{P_1(U_k=1)}{P_0(U_k=1)}=\frac{1-q}{p}\geq \frac{P_1(U_k=0)}{P_0(U_k=0)}=\frac{q}{1-p}$$ as $$(1-p)(1-q)\geq pq\Longrightarrow 1-p-q\geq 0$$ is true due to convexity of $C_\theta$. This says that one can populate the sets $\mathcal{S}_0^*$ and $\mathcal{S}_1^*$ with (much) less than $2^n$ computations. I can also write $R^*(n,(\mathbf{p},\mathbf{q}))$ in terms of $p_k$s and $q_k$s as follows: $$R^*(n,(\mathbf{p},\mathbf{q}))=\frac{1}{2}\left(\sum_{\{u_1,\ldots,u_n\}\in \mathcal{S}_0^*}\prod_{u_k=0}q_k \prod_{u_k=1}(1-q_k)+\sum_{\{u_1,\ldots,u_n\}\in \mathcal{S}_1^*}\prod_{u_k=0}(1-p_k)\prod_{u_k=1}p_k \right)$$ Accordingly,