Background Let $a,b,c,d$ be nonnegative constants and consider the map $T\colon [0,1]\times[0,1] \rightarrow [0,1]\times[0,1]$ defined by $$ T(x,y) := \left( \frac{1}{1 + ax + by}, \frac{1}{1 + cx + dy} \right), \quad (x,y) \in [0,1] \times [0,1]. $$ I'm interested in this map (and other similar ones) in the context of monotone dynamical systems. They come up, for instance, as the input to output characteristic of controlled dynamical systems with outputs modeling simple biochemical networks. Question By Brower Fixed Point Theorem, $T^2:= T \circ T$ has a fixed point. Now is this fixed point unique? If it is not unique, is there a counterexample? Are there reasonable hypotheses on $a,b,c,d$ which would guarantee that to be true? Of course I'm also interested in the general finite-dimensional case $$ T(x_1, \ldots, x_n) := \left( \frac{1}{1 + a_{11}x_1 + \cdots + a_{1n}x_n}, \ldots, \frac{1}{1 + a_{n1}x_1 + \cdots + a_{nn}x_n} \right), $$ as well as replacing the $1$'s in the numerator and denominator by general constants. But I'm hoping to find something conceptual in the simplest case which could then be applied in the general scenario. Progress (1) Computer simulations with randomly generated coefficients $a,b,c,d$ seem to indicate that this is true in arbitrary finite dimensions, with any nonnegative $a,b,c,d$. (2) If $a + c < 1$ and $b + d < 1$, then $T$ is a contraction (with respect to the sum-norm $|(x,y)| := |x| + |y|$. In particular, it has no period-2 points, and so $T^2$ has a unique equilibrium. But these hypotheses seem too restrictive. In fact, taking into consideration where $a,b,c,d$ come from, this is not always true. (3) This is also true if $b = c$ and $a = d = 0$. This makes me wonder whether there could be some sort of diagonalisation argument. Though I have no idea what to do with the nonlinearity. (4) I'm also attempting to approach this as a global optimization problem. More precisely, I'm looking at the map $$ (x,y) \longmapsto \|T^2(x,y) - (x,y)\|^2 $$ and tried to compute the Hessian using Maple but haven't gotten anywhere so far. 

As a lazy heuristic, one can consider the following construction. Consider the following operation $F$ on sequences. Given a sequence $S$, we identify in $S$ the first place $q$ where the partial sum leaves $\pm t$. We identify the last place $r$ preceding $q$ in which it remains within $\pm t/2$. Then we let $F(S)$ be the sequence obtained from $S$ by swapping the signs of all elements from the $r$th place. Of course, if we apply $F$ sufficiently many times to any sequence we will obtain a sequence whose partial sums are bounded in $\pm t$. The question is how many times must we apply $F$ to a typical sequence? We expect that for a random $S$ the value of $q-r$ is about ${t^2}/4$. Furthermore, by definition, if $q$ is the place at which the partial sums of $F$ first leave $\pm t$, and $q'$ is the first place at which the partial sums of $F(S)$ leave $\pm t$, then the last place $r'$ preceding $q'$ in which the partial sums of $F(S)$ remain within $\pm t/2$ satisfies $r'\geq q$. It follows that we expect to apply $F$ about $4n/t^2$ times to a randomly generated sequence $S$ in order to obtain a sequence whose partial sums are bounded within $\pm t$. It follows that we should expect that the probability that a random sequence has partial sums bounded by $\pm t$ is about $2^{-4n/t^2}$. It should not be so hard to turn this into a good argument that the probability is $2^{-c_tn}$ for some $c_t$ growing roughly like $t^{-2}$ (perhaps with some log factors..). 

Background Let $f: [0, \infty) \times {\mathbb R}^n \rightarrow {\mathbb R}^n$ be a jointly measurable function satisfying, 

$f(t, \cdot)$ is locally Lipschitz for every $t \geqslant 0$, for every compact $K \subseteq {\mathbb R}^n$ and every $b > a \geqslant 0$, $$ \int_a^b \|f(t,\cdot)\|_K\,dt < \infty\,, $$ where $$ \|f(t,\cdot)\|_K := \sup_{x \in K} |f(t,x)| + \sup_{\substack{x,y \in K \\ x \neq y}} \frac{|f(x) - f(y)|}{|x - y|}\,, $$ and there exist locally integrable $\alpha, \beta: [0, \infty) \rightarrow [0, \infty)$ such that $$|f(t,x)| \leqslant \alpha(t)|x| + \beta(t)$$ for every $(t,x) \in [0,\infty) \times {\mathbb R}^n$. 

I've been using the representation result below, from Krasnosel'skij/Lifshits/Sobolev; Positive Linear Systems---The Method of Positive Linear Operators. Heldermann Verlag, 1989. Theorem. Let $E$ be a real Banach space partially ordered by a solid, normal and minihedral cone $E_+$. Then there exist a compact Hausdorff space $Q$ and a linear homeomorphism $\Phi: E \rightarrow C(Q)$ such that $\Phi(E_+) = C_+(Q)$. In this statement, $C(Q)$ stands for the space of continuous, real-valued functions on $Q$, while $C_+(Q)$ consists of the nonnegative ones. This is stated as Theorem 6.6 on page 64 in the book. I'm wondering if anybody knows of a better presentation of this result. It's not the first time that I've been having problems with this book, which has many mistakes. The original paper of Kakutani from 1941 doesn't seem much easier to read, and Google searches haven't yielded much on that direction. Thanks! 

One may assume without loss of generality that $f(t,\cdot)$ is compactly supported uniformly in $t$; that is, there exists a compact $K \subseteq {\mathbb R}^n$ such that $f(t,x) = 0$ whenever $x \notin K$. Setting $\varphi_0(t,x) := x$, then $$ \varphi_k(t,x) := x + \int_0^t f(s, \varphi_{k-1}(s,x))\,ds\,, \quad k = 1, 2, 3, \ldots\,, $$ recursively, as in the standard proof of existence, doesn't seem to work. Euler's proof of existence seems to almost work. But the construction is not "canonical" in the sense that the subsequence along which convergence occurs via Arzela-Ascoli will depend on $f$. I would like to have a sequence $(\varphi_k)_{k \in {\mathbb N}}$ which could be defined by a procedure independent of $f$. This is because I'm ultimately interested in the flow of parametrized differential equations, and I don't want to have to choose a different subsequence for each parameter. 

The easiest way to do this is as said the probabilistic method. However, for those who prefer non-random constructions, here is a greedy method. Choose some large $n$ (you can calculate what is needed easily enough). Start with the empty $n$-vertex graph. Add edges greedily between vertices subject to two conditions. First, the vertices you join should always have distance at least $K$ in the current graph (if not connected, then assume distance is infinite). Second, both vertices should have degree at most $K-1$. When this procedure is forced to terminate for lack of such pairs, you have a graph with maximum degree $K$ and girth at least $K$. Now take any vertex $v$ of degree less than $K$. Look at all the vertices at distance less than $K$ from $v$ (including $v$). This set must include all the vertices of degree less than $K$, or you would not have terminated. But the set has at most $1+K+K(K-1)+K(K-1)^2+..+K(K-1)^{K-1}=C$ vertices, since the maximum degree is $K$. Similarly, the set of vertices at distance at most $2K$ from $v$ is bounded, and we can presume there are at least $KC^2$ vertices at distance more than $2K$ from $v$. Now joining greedily vertices of degree less than $K$ to these far-away vertices greedily without creating short cycles must succeed (each edge added blocks at most $C$ vertices, and there are certainly not more than $CK$ edges required). To make this have girth exactly $K$, start with a $K$-cycle. To make it regular is a little harder: one option is to run the first procedure (starting with a $K$-cycle which we insist on preserving forever, to fix the girth) with a much higher distance requirement to join two edges (say $3K$), then after termination, identify a low-degree vertex $u$ and adding an edge to some far-away $v$ (as before) then removing some edge $vw$. Now $w$ cannot have been (before edge removal) a low degree vertex (it is too far away from the first low degree vertex) and furthermore it cannot be within distance $K$ of any remaining low degree vertex, so you can join it to a remaining low degree vertex. Rinse, repeat, assuming $n$ satisfies the parity condition for a $K$-regular graph to exist (and is large enough) you will succeed. 

I think I got it! I turned out to be mistaken about item 2 in my Progress Notes. Lemma 1. Suppose $f \colon {\mathbb R}_{\geqslant 0} \rightarrow {\mathbb R}_{\geqslant 0}$ is locally integrable, and let $F\colon {\mathbb R}_{\geqslant 0} \rightarrow {\mathbb R}_{\geqslant 0}$ be the primitive given by $$ F(t) := \int_0^t f(s)\,ds\,, \quad t \geqslant 0\,. $$ For any positive integer $m$, set $f\colon {\mathbb R}_{\geqslant 0}^m \rightarrow {\mathbb R}_{\geqslant 0}$ by $$ f_m(t) := \prod_{i = 1}^m f(t_j)\,, \quad t = (t_1,\ldots,t_m) \in {\mathbb R}_{\geqslant 0}^m\,, $$ and set $$ S_m(T) := \{t \in {\mathbb R}_{\geqslant 0}^m\,;\ 0 \leqslant t_1 \leqslant \cdots \leqslant t_m \leqslant T\}\,, \quad T \geqslant 0\,. $$ Then $$ \int_{S_m(T)} f_m(t)\,dt = \frac{[F(T)]^m}{m!}\,, \quad \forall T \geqslant 0\,, \quad \forall m = 1, 2, 3, \ldots\,. $$ Proof. Follows by Fubini and integration by substitution. (Details/clarification upon request.) Assuming that $f(t,\cdot)$ is compactly supported uniformly in $t$ as described in item 1 of my Progress Notes, we may apply Lemma 1 to show that $(\varphi_k(\cdot,x))_{k \geqslant 0}$ is a Cauchy sequence on $[0,T]$ for each $T \geqslant 0$. Thus $$ \varphi(t,x) = \lim_{k \to \infty} \varphi_k(t,x)\,, \quad \forall (t,x) \in {\mathbb R}_{\geqslant 0} \times {\mathbb R}^n\,. $$ 

We know that, under these hypotheses, the ordinary differential equation $$ x' = f(t,x) $$ generates a unique global flow $$ \varphi: [0,\infty) \times {\mathbb R}^n \rightarrow {\mathbb R}^n\,. $$ Question I'm wondering whether there is a canonical way of realizing $\varphi$ as the limit of globally defined maps $$ \varphi_k: [0,\infty) \times {\mathbb R}^n \rightarrow {\mathbb R}^n\,, \quad k \in {\mathbb N}\,. $$ Progress