can't benefit from indexes, so it may only be an efficient option if wouldn't use indexes either. With multiple statements that could individually access an index (like in your case if column is indexed and the like has a wildcard at the end), performing of individual queries with a single would probably be most effective in a large dataset. However, if your query filter primarily by, say, a range of dates, then a single may be faster than multiple s ed. For more information about in MySQL see this and this. 

PostgreSQL can only make use of a function index when the comparison is against the results of the function, e.g.: 

If you sort by dates formatted in DD/MM/YYYY, then the sorting will put all the first of month first (of every month), then the seconds of month, etc. You can't sort by a column not included in a select distinct, though. To sort by ascending date, group by and use the actual date column instead of the formatted date via to_char: 

This was hard! I don't know if this is simpler, but at least it doesn't use window function nor produce rows that require being filtered out. 

It looks like the join between and is the culprit (it appears to be taking most of the time doing a filtered indexed scan on ja_jobs for each ja_feedlog resulting row). In cases like this where there are two joined heavily filtered large tables, I find it useful identifying which set of filter conditions will render the least rows on one of the tables, and separate that/those one/s into an indexed temporary table(s) for joining later. I have assumed that the table ja_jobs, with those filter conditions, will produce a reasonably small rowset in a reasonably small time. If that is the case, you may want to try the following: 

Your database is likely corrupted, especially the indexes. You can rebuild all indexes and check all tables on a database by: 

You can also set it as a default for any user or role (takes effect after a new connection is established): 

That's because your query is filtered by a single supplier, then by a range of parts, and finally the results are grouped together by year. In my personal experience, using explicit joins often helps with identifying these sorts of things, and also in reducing the chances of missing a joining condition and causing a cross table join as a result. 

@Serg's answer is effective and fast for a small set of attributes to compare, but it is limited to 30, i.e. hitting the maximum 61 tables that can be referenced in a query (MySQL 5.x). The following is another equivalent but arguably more costly plan, however this one is not limited in the number of attribute tests (you would build dynamically the table of UNION ALL rows, and leave the rest of the query as is): 

Finally, the following function will produce a list of all file names (i.e. the blobs' MD5 hashes) within a folder (in this case ): 

Response to the sequence question in comments: To create a sequence over a partition, your best bet is self-join with a grouped-and-sorted: 

The field in your table does not seem to be defined as a serial, so it is expected that will return nothing. The duplicate you get relates to one of the records in your is returning two rows with the same : 

INSERTs have lower impact in affected indexes than UPDATEs, however INSERTs affect all indexes (except for conditional indexes that do not meet the condition). Indexes are necessary if you need to retrieve data often and quickly. You must weigh the impact of one (indexes in updates/inserts) or the other (full scan searches): you can't have it both ways! An compromise is datawarehousing, which enable fast inserts/updates, infrequent full scans (copying to datawarehouse), and fast searches in not-up-to-date data. 

They're vastly different when it comes to other features, such as security, programming languages, data types, tools, end user support, requirements, etc. However, they're both compliant with basic SQL92 (and perhaps other SQL standards), so one can do the same basic schema creation and data querying and manipulation. You should be able to do the basics from java applications without noticing where you're connected to (and this applies to several other databases). 

I had a PostgreSQL database with loads of scanned documents, as a column in the table , with hundreds of thousands of documents that was large and inconvenient to backup. Also, there was a high rate of duplication, not major but 5-10%. I wanted to store those documents outside of the database so they could be backed-up via incremental tar, and reduce the size of the pg_dump database backup. I came up with a solution I want to share below, using plperlu. Any comments or further optimisation ideas will be appreciated! 

How many unique values do you have for ? How feasible it is that you'll hit an value greater than 2 billion (in the next 5-10 years)? Note that is the same as (because is not nullable), and that is effectively along with . What you're doing not only complicates understanding the query, but may also cause PostgreSQL to select a sub-optimal strategy. By using instead of , PostgreSQL can compute the whole thing on an index that does not include , and without the need for reading the table at all, so it should process much faster. Let's assume you currently have values in the low millions, and it is unlikely you'll hit the value 2 billion in the near future. You could speed it up by creating an index and adjusting your query as follows: 

You can use NOT IN in such situations, or LEFT JOIN, as follows (not sure what you mean by [CourseID] in your select list, though): NOT IN This may not be efficient for very large numbers of students. 

The size of the blob can be queried without having to read the file to memory and calculate the size of the blob, by simply getting the file size from the filesystem: 

This can be added to the function that handles the logon (replace and with point and user in logon function parameters) 

PostgreSQL database dumps are normally produced by the command line tool , and expected to be used via either or simply command line tools. However, you can both backup and restore from PgAdminIII. The backup option is available via right-clicking either the server or any database or schema. The Restore option is available via right-clicking a database or schema. Here's a step-by-step example of how dump or restore a database using PgAdmin. 

You'll have your reasons for going this route, where it seems you're duplicating information (since is implied from ). However, unless what you want to achieve is having with an optional product OR variant (i.e. a CHECK constraint ensuring one and only one of these IDs is not null), you must also ensure that you don't get into an invalid combination in in order to ensure you don't break referential integrity. For this, you need a UNIQUE key on and a FOREIGN KEY on . 

Via PostgreSQL FDW you can connect to any other PostgreSQL (as well as other databases), regardless of the version of the location (remote, local, or same) or version (older, same, or newer) of that other PostgreSQL. 

You could reduce that index even further by using a smallint calculated as the difference between a set date (e.g. 1/1/2010 +- 32768 days = Apr-1920 to Sep-2099): 

Note that casting to is faster than the slution by @Sole021, but it is not UTF8 compatible (or any other encoding for that matter), returning simply the first byte, so should only be used in cases where the comparison is against plain old 7-bit ASCII characters. 

DROP SCHEMA would attempt obtaining exclusive use of the schema first, so it would only actually manage to drop the schema when PostgreSQL is done retrieving data from ongoing queries. Further queries would likely be locked until the end of the transaction in your question. Queries accessing tables in the schema may fail after this transaction is finished if they (queries or functions procedures) were compiled prior to the dropping and renaming of the schemas, but should be safe for queries from newly established connections. 

The two analyses differ in the text comparison, the first shows , i.e. case insensitive (ILIKE) whilst the second shows only , i.e. case sensitive (check if you didn't run LIKE by mistake the second time). 

For this you could keep a table for general use, it can be quickly created with 10,000 rows like this: 

You could do the following, provided that your sensor 6511 takes measurements reliably every 30 minutes (use instead the one that is most reliable). This will calculate measurements based on the recordings of that primary sensor used (in the main FROM, in this case 6511), show the exact date and time of such recording, also show an adjusted time in 30 min slots, and use any available measurements for all other sensors, whichever latest available up to 10 minutes after the primary measurement: 

Note that timestamp ranges cannot be specified backwards (ending timestamp must be equal or higher than the starting one), and their boundaries are either inclusive () or exclusive () boundaries, being the default , i.e. an inclusive starting and exclusive ending time. 

Just join by i and j, and multiply val. If the samples are incomplete in either matrix, you may want to grab the union of both matrices and left join. 

You could create local copies of the remote tables, and run periodic processes that would add new records, remove deleted records, and update changed records. How large are the remote tables? I guess not too big since you were planning on creating local materialized views from them. Normal tables that you keep synchronised would have a similar impact. Once you have local copies of your remote tables, then you can implement foreign keys to them on the tables you need foreign keys. 

For checking of initials, I often use casting to (with the double quotes). It's not portable, but very fast. Internally, it simply detoasts the text and returns the first character, and "char" comparison operations are very fast because the type is 1-byte fixed length: 

If there is no guaranteed correlation between older ids and older jobs by creation time, the query is trickier: 

Testing This alternative seems to be less efficient for very small sets, but more or less linear in performance, whilst the original seems to be exponentially more sluggish. 

When designing a database, I find it convenient sticking to a single naming convention, either plurals or singulars (i.e. routes and stops, or route and stop). Here I opted for all plurals. Also, suffixes work better than prefixes for listing tables alphabetically (it doesn't really matter in this case because there are so few tables, but it still is good practice). This design allows you to have the same stop shared in different routes. Note I've included a in . This was just a suggestion, since sometimes bus tables have different schedules for weekdays and weekends. The query would be significantly more complex for finding a route from a stop to another one using two routes, but it can be done. It also allows you easily calculate travel time, and stop times at subsequent bus stops. 

With your datasets, MySQL has to obtain those 450,000 records from posts (in 1000 little chunks from each matching source_id), sort it, and then return the top 10. It is a costly exercise. You could resort to using a stored procedure, and accumulate results going back in time, say daily or weekly, looping until obtaining at least 10 records, and then returning the 10 most recent ones. You'll need an index on by . It would return quickly for the most recently active users, but take much longer for users without recent posts. Something like the following: 

As @JonathanFite stated in the comments, date_utc is a system function, so your queries will seem to work when your field happens to have today's date, but will fail for any others (because it's not reading the field, but returning the value of the system function instead). You'll always have to use backticks enclosing utc_date to make it work. Keeping this name, however, it is a recipe for trouble, you may want to consider renaming the column. 

This is due to relation attributes (defined in and , or defined dynamically from a statement) supporting modifiers (via ), whilst function parameters do not. Modifiers are lost when processed through functions, and since all operators are handled via functions, modifiers are lost when processed by operators as well. Functions with output values, or that return sets of record, or the equivalent are also unable to retain any modifiers included in the definition. However, tables that will retain (actually, probably typecast to) any modifiers defined for in . 

Also, for inner joins, it is clearer if filter conditions that are not strictly part of the joining condition are specified in the where clause rather than the join, for clarity and for making debugging easier. Finally, even if it doesn't affect the final plan, for readability and clarity I place in the FROM whatever table it will be the "carrier" table, i.e. the one that will take the biggest filtering effort, and "hang" from it all the dependent ones, like small master tables, since I find it easier commenting these out for debugging. 

De-normalisation and certain kinds of data duplication can be a convenient way to speed-up processes. Examples are caching, datawarehousing, and materialised views. Meanwhile the duplicated data is relied upon as a read-only snapshot (i.e. point-in-time copy of the actual data), or the system can guarantee consistency (like in the case of caches), it is safe to rely on this practice. 

You're mixing grouping criteria twice while creating junk_test, first in the GROUP BY subselect by having fewer conditions in the WHERE, and then in the PARTITION BY by having one extra partitioning field (). If you can assume that older ids are older jobs, then you can identify your duplicates by joining grouped table with itself, like this: 

(Moved here from an edit on the question: Reddit helped me out with this already. Eventually I used this code) 

You could create a query that first lists or groups elements as in your first example. Then select on that (as a subquery) grouping by basket and crating a string per basket through GROUP_CONCAT(). Then you can use those results (perhaps as another subquery) grouping by the group_concat string and having count(*) > 1 to find baskets with equal contents. 

PostgreSQL will optimise both cases with different internal strategies according to relationships, available indexes, and collected statistics, so which strategy is best will depend on your particular scenario. 

There are several ways for determining if the trigger is recursing. Checking the trigger depth () is not one of them, because you don't know if the trigger itself is recursing or is being fired from another trigger. The only direct way to know analysing the execution stack during an exception: 

PostgreSQL can't do this natively, but you can convert the CSV output to an XLSX file with a simple perl script. : For this script to work, install and perl modules, either through apt-get/yum, or through and . 

I resort to maintenance scripts like this all the time (in PostgreSQL). What you can do is using where supported (e.g. ), and conditionals otherwise. MySQL doesn't support for , so you can resort to a conditional. In your specific example, your script could be: 

Your subquery will likely return several rows, but will be expected to return only one and will give an error. Also, the sql operator is intended to work on sets rather than strings. If your has an ID, you can try the following query instead: 

To check if an index is working, use rather than , otherwise you're not only measuring index performance but also the time it takes to transfer all the data across your client application, which in this case is a whooping 50,000 rows! You can significantly simplify your query and index by date datatype instead of timestamp, which is smaller and therefore it should perform better: 

You could run the following bash script. It will expect the list of schemas in the file, and your script in . It will run the script on each schema (as a default schema) in , and will produce, for each schema, a file named . For Unix, the script is: 

Backups from a normal user are a nightmare. If you're using PostgreSQL on Linux, I'd recommend relying on (and the file) to accomplish this, so that a given specific user can with specific parameters, so that's all they can do as user (and therefore have no write access to the database).