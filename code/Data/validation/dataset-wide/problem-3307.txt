I have found a paper that addresses this question directly (finally!). Svenonius & Ramchand's 2014 paper (here) offers an explanation for universal "grammatical zones" that appeals both to innate grammatical principles and properties of extralinguistic cognition. From the abstract: 

In brief, whether speech error data is relevant to the question of derivational order in syntax depends on just how much explanation syntactic theory owes to a theory of linguistic performance. 

I think your question may reflect an instance of a much more general pattern. That is, suppose you find an expression X in a language, and another expression Y in that language, and you believe X and Y to stand in a "privileged" relation (two types elaborated below). It seems that, across languages, one is likely to find that X and Y occur as simplex (i.e., nondecomposable) lexical items asymmetrically: i.e., if Y occurs as a simplex lexical item in the language, so does X. But simplex X occurring in a language does not guarantee expression of simplex Y. One concrete example to explain one notion of a "privileged" relation between two expressions is privileged antonymy. Consider the adjectives tall and short: we "feel" that these expressions are related in a way that makes the inferences in (1) and (2) valid, but not that in (3) (despite long on occasion meaning something very similar to tall): 

Phonetics - studies the physical speech signal. Sound is interpreted in the brain, which leads us to: Phonology - studies the how speech signals are perceived, as well as mental categorization of sounds. These conceptual sound categories may combine to form morphemes, which leads us to: Morphology - the study of the smallest units of meaning (e.g. "book"+"-s"="books"). Morphemes may combine to form words, and those words combine leading us to... Syntax - the study of "sentence level" structure--how words combine to form phrases, and those phrases combine to form sentences. Of course, words and phrases just so happen to have meaning, this leads us to... Semantics - the study of meaning, both at the lexical and phrasal level. But of course one word may have slightly different meanings in different contexts, which brings us to: Pragmatics - the study of meaning in context. 

For future people who may need this, I ended up writing a webscraper and compiling everything from www.minpairs.talktalk.net. You can view/download the corpus here: $URL$ 

Turkish has only one suffix for plural (* ler) and with the number of objects It doesn't change. But as it is written many times above, Turkish has tendency to drop the plurals in most cases. This feature can be observed through the many proposed Altaic languages (Japanese, Mongolian etc.) 

the house is nice is a sentence. Its equivalant is Ev güzeldir which is a sentence too. For convinience people say ev güzel. On the other hand güzel ev is not a complete sentence. Bu güzel evdir is a complete sentence. People can omit -dır for convenience. I think this is why you are confused. Adjectives come before nouns all the time except when it is used for profession or honorifics. Ahmet Bey, Ayşe Öğretmen etc. 

The examples you gave are indeed Turkish, but they are not greeting words. Instead they are questions basically "How are you". You can say that in Turkish "Nasılsınız?". The root Esen is used for greetings in old texts (Kutadgu Bilig, Divan-ı Lügati-t Türk), as more or less as it is used in modern Turkish Esenlikler olsun. 

Does any language express the less form using bound morphology the way we do with -er in (1) and (2)? 

Linguistics is the scientific study of language. A language is narrowly defined as the set of rules that "speakers" (speaking or signing) acquire when they are very, very young. There is evidence for processes of language acquisition underway at the very youngest testable ages (under a year old). A speaker with a grammar like mine knows, without ever being told, that strings of words like colorless green ideas sleep furiously are well-formed, but strings like furiously sleep ideas green colorless are not. I can say that the first sentence would be true if (i) there existed some things that were colorless, green, and ideas, (ii) such things could sleep, and (iii) such sleepings could happen furiously. The other string supports no such interpretation. Since I don't have to look up these two strings in some kind of book to tell whether one is good or bad, we say that my grammatical rules are internalized. The rules of grammar (again, in the linguist's sense) are learned implicitly. They are not taught, as the rule "i before e except after c" has to be learned when one learns how to spell, but rather inferred on the basis of experience in a linguistic environment (i.e., a community of speakers who do some speaking in the presence of the child). Lastly, knowledge of language is universal. Barring severe cognitive deficits or social deprivation, every normally-developing child acquires a native tongue. Written language stands in sharp contrast: children learn to read and write much later (usually once they start attending school) than they learn to understand and produce spoken language; written language has no internalized rules, it reflects only the speakers' internal rules and, possibly, a speaker's regard for the prescriptive rules of style; written language has to be learned explicitly (often laboriously); and, written language is not universal—there are many, many people on this planet who have knowledge of language but can't read or write. "Written language" is an artifact of (some) human cultures who already had spoken languages. Spoken language is definitional of our species, and it is this kind of species-specific capacity that linguists study. This is not to say that the capacity for writing is not complex; it is in fact more complex than the capacity for language, in that explaining how we can have writing systems requires in part an explanation of what the language we're writing down is in the first place. 

I hear that only complements and specifiers can contain the argument for a verb. But there are certain structures with ditransitive verbs I believe you can represent as an adjunct. Here's an example: "Bill gave Jessica a gift" 

I often see a noun such as "Jimmy" in my textbook represented as [DP_Jimmy]. But there is no more detail provided as to its internal structure. This means either the D is null such as: 

This may be the wrong place to be asking this, but I'm not entirely sure what kind of advantage using a language like Prolog gives you over an Object-Oriented language like Python. Also, is there a reason why Prolog is popular in both linguistics AND ai? I assume the two fields are connected. 

I'm looking for a database of english minimal pairs that is at least somewhat organized by some principle such as features or phonemes similar to : $URL$ $URL$ These are fine, but I would need to write a web scraper to parse through all the data. Is there any publicly available minimal data in a more accessible format like a .csv? 

The main question you're asking has been addressed directly in a recent paper on the relationship between morphological and semantic markedness. 

Extensive typological study doesn't appear to have been done on this. However, this is probably so because it has been difficult to say precisely what the mass/count distinction is about. What we can understand is that the mass/count distinction tracks (imperfectly) an important human cognitive distinction between objects (or, countables) and substances (or, uncountables, "fake mass nouns" like furniture aside). As with many conceptual distinctions, different languages may mark the difference overtly, others may not. To give a different example, a language like West Greenlandic has a host of "pluractional" morphemes that attach to verbs, one of which indicates whether an event description picks out a series of iterative events, or whether it picks out a single event of continuous extent. English doesn't have such morphemes, but interestingly in this language we see certain event descriptions as underdetermining the two uses (e.g., John ran more than Mary last year, if true, can mean he ran on more occasions (iterative), or for a greater temporal or spatial extent (continuative)). So a conceptual distinction that is overtly marked in West Greenlandic is left implicit in English. Similarly, English does not mark mass occurrences of nouns, but marks individual (with the indefinite article a(n)) and plural (with the -s suffix) occurrences of nouns. The bare form of a noun is often understood on a material reading (which is why the gruesome interpretation in There was boy all over the road, as if uttered after a horrific accident), and singular/plural occurrences are understood as picking out a singularity or multiplicity of individuals that fall under the concept named by the noun. If a language, for example, doesn't have a singular/plural distinction, one might think that noun occurrences there are understood via a combination of the preferred conceptualizations of things falling under the concept (e.g., we think of boy best as describing individual boys, and water best as describing an unindividuated substance), and extralinguistic context. The grammar doesn't force such speakers to express which conception they intend. An oft-discussed counterexample to the idea that the distinction is universal, is the claim that all nouns are actually mass in Chinese---"count" nouns don't exist except in the sense that they are "built up" in combination with classifiers. However, as Cheng and Sybesma argue, and Chierchia accepts, even this language has reflexes of the conceptual distinction---i.e., it uses only "classifiers" for nouns that are understood as "count" (i.e., highly individuated), and only "massifiers" for those understood as "mass" (i.e., low individuation). In sum, the role of grammar seems to be specifying what kind of conceptualization we're after. In English, there are a lot of nouns that are comfortable occurring as either mass or count, the difference ending up in whether we want to express a notion of the material, rock, or that of some quantity of individuals comprised of that material, rocks. Other occurrences, like muds seem less acceptable, but this could just be a matter of the frequency with which we encounter individuated MUD; if we can talk about kinds of muds or jars that contain some mud in a context, so that I bought three muds at the mud store isn't really so bad. Extralinguistic context can help us individuate even under the concept MUD. Whether a language uses grammar to express the mass/count distinction will be a fact about particular languages, but the conceptual distinction is likely universal. (Hat tip to Dustin Chacón for discussion of Chinese.) 

Because your english example is compound sentence. You just add different words to each other. But Turkish example is like this Çekoslavakya-lı-laş.... If we inspect this, -lı and -laş are suffixes not words. -lı creates a new word which gives the meaning of Inclusiveness (like Czechoslovakia-n). -laş is a suffix that creates verb from a noun which gives the meaning of to become that noun. This is agglutination. It is common to have one word sentences in Turkish, you can express lots of things with suffixes. There are examples similar to your English example. Buzdolabı means fridge, literally Ice+closet. In here 2 words are added together no suffixes. 

Any general purpose dictionary will help, generally the ones that are published by the language associations. 

The root is not known. But for etymology I would recommend Misalli Büyük Türkçe Sözlük (It is online on kubbealtilugati.com . According to it, bakır has been used since the old Turkic, but there are some claims that it can be loan word from an Iranian Language (Sogdian maybe?) Source: $URL$ 

I've seen [+constricted glottis] described as encapsulating ejectives and implosives, but the feature matrix according to Hayes (2009) (that I pulled from his personal website over here: $URL$ only lists the glottal stop as being [+c.g.]. Is there disagreement here? Or an error? 

What I did here is extend the scope of the existential to encapsulate the BEATS predicate. Next, I included a conjunction that included another instance of the predicate DONKEY to make the formula more rigorous (because it would evaluate as true if we interpreted y as a pig/non-donkey object). Finally, I moved the conditional after the MAN predicate to "tidy up" the universal and existential quantifiers. From my intuition, the sentence "every man who owns a donkey beats it" doesn't suffer from ambiguity unless you interpret "beats it" as masturbating. So what's wrong with my translation into formal logic? Am I missing the point of donkey sentences? Edit: I suspect that this stems from the basic fact that it's discouraged to say ∀x(Px&Qx) compared to ∀x(Px->Qx), but I'm not completely sure why asides from the fact that it's "too strong a claim".