Just as an aside to the good answers you have already been given: Consider handling leap years During a leap year the dates for the different zodiacs change. 

In phase 1 I determine the max and min of both dimensions. And assume a hypothetical rectangle that goes between min and max of the points. Next in phase 2 I devise a indexing scheme that maps every point on the assumed rectangle to a unique index and any point not on the rectangle to -1. I also allocate a score vector that is as long as the indexes will go. Then I iterate over all the points again and if any point has a negative index, it is not on the rectangle so I terminate with false. Otherwise I take the index and mark it on the score chart so that I know I have visited that point (necessary to make sure the rectangle is closed). Finally in phase 3, I iterate over the score card to make sure every point on the rectangle was covered by at least one point. This neatly covers double points as well. The running time is more accurately \$\mathcal{O}(2n + w+h)\$ where \$w\cdot h=n\$. 

I don't know what framework you are using for your tests but gtest has type parametrised tests that you can use to test all the implementationsâ€‹ with the same tests. By using this feature you wouldn't even need the code to begin with, which in my book is the best code: the code you never wrote :) 

which makes it a bit easier to reason about the state of the class. There are other nitpicks but I'm not going to go into that as I'm sure you're aware of most of them already. Performance - Function call overhead in DFS I haven't benchmarked so take this with a grain of salt. The only place that I see that could be done faster is the method. Currently it is a recursive method that recurses at most N times. If you instead chose to implement an iterative BFS you could get rid of a lot of the function call overhead in what I believe is the most time consuming part of the code. Edit: Fixed performance where a node could appear twice in the fringe Something like this (untested pseudo java-ish code): 

To find the kth smallest/largest value you do not need to sort all values! Simply keep track of the k smallest/largest encountered values in a (sorted) buffer. This will take your run time from O(nlog(n)) to O(nlog(k)). And will also avoid the unnecessary copy and iteration, simply keep the index with k. The following pseudocode illustrates the algorithm 

It encapsulates the shorthand with the context in which it is used. As an added benefit you can now change the implementation from to if need be, without affecting the rest of the application. The C++11 keyword will save your clients typing . Also is shorter to type than ;) I agree with RubberDuck on the naming. Prefer as class name. Also I'm leaving the indepth review for some one with more time on their hands. 

I'm going to assume your target machine is a super scalar CPU with some variation of Tomasulo architecture. I'm also going to assume that there is no significant cache miss ratio at cause for your performance (you should verify this). When compiling optimised code, the compiler will rearrange operations to reduce stalls due to availability of input data to the instructions. Hence you cannot completely trust the line numbers in the profiler. As the CPU is super scalar some instructions may execute and complete out of order (but are committed in order). I see that you are computing the reciprocal of and to save yourself two divisions. It is possible (but doubtful) that this could cause a delay stage as the two dependent operations could end up stalling until the result is available instead of executing in parallel on different execution units. Contrived example: 

Do you care if the sequence is "seemingly random" instead of actually random? If the answer is "no" then there is a really fast, simple and cool trick you can use: "Fake shuffle" If you are shuffling between \$k\$ items. Pick any prime \$p>k\$ (use a sieve at program start, or hard-code a list of big primes). Finally pick any random starting index \$0\le i_0<k\$. Then the next index to visit is always \$i_j=(i_{j-1}+p) \mod k\$. This deceivingly simple algorithm will visit all the \$k\$ items in a seemingly random order without visiting any item twice (in any loop over all items). Example Don't believe me? Try this: \$k=8\$, \$p=11\$, 

(Note: the iterator itself never becomes invalidated even after C++17) However (Since C++11) may or may not invalidate the iterators depending on if a rehashing occurred or not. So if stable iterators are important for you then you should not use it. As for your question it is hard for us to help you because you have not clearly specified the requirements on the container you're trying to design. Depending on how you want to use it (we don't know) you may or may not be better of just using . But my gut feeling here is that requiring stable iterators indicates that you have a problem elsewhere in your code and that your efforts are better spent rethinking your design to get rid of that requirement. 

It doesn't handle negative h. You're basically just changing the lower bits, the high bits are mostly unaffected. A better (and faster) way is to simply multiply by a sizeable prime number: 

Purpose You never stated why you need an object pool, but I'm going to assume that it is for performance reasons, as it most often is. If you don't have a solid reason for using an object pool, you shouldn't. It just complicates things for no gain. With that in mind the performance gain from an object pool is for two reasons: 

Zero is twice as likely as any other outcome. This is clearly not uniform! Even for larger RAND_MAX and range values the problem persists. There are some special cases where it works but in general it doesn't. So how should you do it? Unfortunately though you see the approach with modulo shown everywhere on the internet because it's easier to understand. Or because people don't know better. Whatever the reason using modulo is not the way to limit the range of a random number generator. There are a few different ways: Use all the bits - Floating point rescale As rand() returns a some what uniform random number in the range [0,RAND_MAX] and we want all numbers to be used; So we can re-scale this range to [0,1[ and multiply it by our desired range and quantize it. This requires a bit of care so that we don't introduce additional bias around the edges of the range: 

The optimization gives you maybe 10% speed gain which isn't much. And I say maybe, it depends on if your CPU correctly predicts the branch so it's probably a bit less than 10%. These kind of problems very rarely depend on how you optimize your code, but rather that you have the correct time complexity, see the above. 

Performance - Unnecessary work creating adjacency lists Here you create a bunch of adjacency lists to represent the connectivity of the graph, depending on the input, some (or all) may never be used: 

If the constructor was called with a temporary like so where and are arguments, then arg would deduce to . Which is an r-value reference, you are then storing the pointer to this temporary into which refers to... you guessed it something that might be dangling as soon as the constructor returns. The other constructor: 

I'm in a bit of a hurry so I'll only touch on the big high level stuff that I noticed. Abuse of SFINAE I think that the way you are designing two different use cases into one template class and switching between them is an abuse of SFINAE. The most obvious indication of this is the fact that you define two aliases and to act as if they were two separate templates. I understand that this is a way to avoid code duplication but I do not approve of it. The added clutter and the extra overhead of a mutex and atomic variables etc on the single threaded queue is also less than ideal. What I would do instead is to make a shared, hidden base class which holds the shared functionality. Something like this perhaps: 

which matches the formula for sample variance. But I don't know how your application, you need to think about it and figure out if you need the population or sample variance. Gosh look at what you did? You made me do maths lol :) 

Forgetting to inject the CRTP parameter into a class that requires it will not give a compile error. It may not even give a run-time error. In this example it will just silently give you a memory leak that is a royal pain in the rear end to debug. This will happily compile and give you a memory leak, even if C inherits properly from B with the CRTP parameters passed around: 

As you have not provided any profiling data I can only speculate in what I see in the code you have provided. Guesstimating expected performance First of lets consider some numbers, you say millions of points so I'm going to assume 10 million points. You say 2GB file then for 10 million points, that's around 200 bytes per point sounds reasonable. Lets assume a typical HDD with read speed somewhere around 80MB/s, then reading 2GB of data should take 2000/80 = 25 seconds or so. Lets assume a modern 3GHz CPU with average instructions per cycle (IPC) of 5 (which is a bit pessimistic). That means that you can do around 3000*5/10 = 1500 instructions per point, per second per core. I would reasonably expect to be able to build the tree (ignoring the delay to read from disk) in a few, maybe tens of seconds. All in all to read the entire thing into memory and build a quadtree, I would expect around a minute. You say 5 minutes which is a bit slower than what should be possible. Looking at the code I see no major blunders that would be an obvious culprit but I do see a LOT of branching on data. One reason CPUs can have such high IPC is that they do something called "Speculative Execution". When the CPU encounters a branch (if-else) it will take an educated guess using sophisticated algorithms to try to predict which branch will be taken and start executing that branch before it has actually computed the branching condition. Pretty neat stuff. As long as this branch prediction goes well (and it does so quite frequently) you gain a lot of performance, but if the CPU on the other hand miss-predicted the branch, it has to back out of the work it already did and then go back and execute the correct branch. This is called, "branch prediction failure" and is very well illustrated in this SO question. In that question the OP has an array of random characters and sums all values larger than 128 (i.e. 50% of the values). Then the op does additional work and sorts the array first, and then runs the exactly same code. Intuitively you would expect it to be slower because you have done an additional sort before hand. But this is not the case, doing the extra work of the sorting actually makes the entire code 5x faster. This 5x speedup is entirely due to avoiding branch miss-predictions as the data is now sorted and the branch predictor in the CPU can correctly predict almost all branches while the random array is impossible to predict. The 5x speedup factor uncannily matches my guesstimate above (1min vs 5min) and your code appears to be branching on random data (as the points are not sorted geometrically, whatever that means). I believe that you can gain some significant performance by rethinking how you branch in your insert method. For example here: