You could use a tool like Magical Jelly Bean Key Finder ($URL$ to confirm what license key is registered by your system. If you installed and used a license key which is now invalid, Microsoft does provide license key update utilities. I have used the utility for Windows XP. 

I would recommend against using wireshark to monitor traffic. You'll just get too much data, but you have a hard time analyzing the data. If you need to look at/troubleshoot the interaction between a couple machines, wireshark is great. As a monitoring tool, IMHO, wireshark is not quite the tool you need. 

At least at the present time, DNS only maps a human convenient name to a particular host, i.e., IP Address. The TCP Ports was supposed to be handled by Well Known Ports. 

Virtualize your WinXP installation Install Ubuntu 9.04 Install Ubuntu package vpnc (vpn cisco) Install VMWare Run WinXP inside VMWare with NAT interface (not bridging) Establish your VPN with vpnc in Ubuntu, and the Virtual Machine will use that VPN connection. 

On the old machine: $ dpkg --get-selections > installed-software On the new machine: $ dpkg --set-selections Finally, on the new machine: $ dselect and the packages will be installed. 

You could try scripting rsync ($URL$ to run in some kind of "pushbutton" way: As soon as you edit or create a file on the Windows machine, push the button, and away it goes. You could also create a script (batch or Powershell) that sleeps for some period of time in between rsync runs. I suggest rsync, because my understanding is that the protocol is more efficient (data / protocol overhead) than either CIFS or NFS. 

The distributions I have seen clearly state the minimum requirements for your white box. Go 1.5x to 2x those recommendations. 

As mentioned, this configuration was working until recently. If I look at the Windows Firewall Security Logging, I can see that the TCP SYN packets from my client to this server's port 21 are being dropped. How should I configure Windows Firewall to allow the packets? One solution that works is to use the "Advanced" tab -> "Local Area Connection" settings and enable "FTP Server", but the dialog box states that opens the exception for Internet access. I am looking for a solution that allows access for only the local subnet. 

Push policy to the firewall on the firewall Reboot the firewall (Windows and Check Point FW-1, both love the reboots) 

Echoing wombie's concern, I don't think you want the server trying to do big data copy jobs in parallel. Whether you are trying to copy multiple partitions, which wombie predicts would cause the disk heads to thrash and slow things down, or to trying to copy multiple disks over a usb bus, in which each data stream may cause interrupts that would slow each other down, unless you are dealing with a transmission technology specifically designed to handle high throughput from multiple clients, you are going to slow things down if you try to do them in parallel. For example, trying to ftp a single file over 10BaseT Ethernet, I could get over 1 MByte/sec (over 8Mbit/sec) throughput, but if I tried to ftp two files from different machines, even to the same server, the throughput would fall to about 150 KByte/sec/per transfer (i.e., about 300 KByte/sec, 2.4MBit/sec). (This is from memory, and it may have taken 3 transmitting stations to get the 10BaseT throughput to drop from ~90% to ~30%. Still, adding a second station did decrease the overall efficiency, due to collisions.) Besides, its a catch-22: the protocols that can gracefully handle multiplexing high throughput streams generally introduce high overhead. Classic examples of networking protocols that gracefully handle multiplexing high throughput streams: Token-Ring, FDDI, ATM. For example, ATM introduces a minimum 10% overhead (of the 53 bytes in a cell, 5 are header) to the transmission. Whether you use dd, partimage, or clonezilla, I would suggest: 

My experience has been that the configuration files are in one place, and as long as you backup that one place, restoring the application works fine. For user applications, that one place is usually the user's home directory. For server application, you read the man or info page to find out where the application stores its config, and then you back that up. Usual locations are in /etc or in a sub-directory of /usr. Porting app configurations between different physical, or even virtual, machines, has generally been pretty easy. Install the app, and then overwrite the config file or config directory in the new target. Compared to Windows, my personal experience is that restoring configuration settings is much easier in practice on Unix-style systems. 

There are also manual pages online, so if your particular system does not have the man pages installed, you can still get the needed info. Many modern utilities have built-in help info. Try 

I have been managing Debian, and now Ubuntu, desktop systems for a small shop for several years now. I am very comfortable with apt for package management and distribution upgrades. The Ubuntu installer has worked for every system I have needed to add. I have not touched Red Hat for many years and have never really managed any RHEL systems. I have an opportunity to take over a RHEL and CentOS shop. I am looking for a list of issues / management concerns specific to rolling out RHEL and CentOS as desktop environments for multiple users who have a mix of Dell desktop and laptop systems. What issues should I watch for when rolling out CentOS as a desktop environment? 

I then looked at the isolinux directory on the CD, and all the boot menus had something like , so I tried: 

I connected the Ubuntu 9.10 vpnc to a cisco VPN device by first: 1) Installing the Cisco VPN client for a Window's host (Win XP) in my case, installing the config file provided by the network/security administrator, and having it working on Windows 2) Comparing the needed Network Connections -> VPN options with various screens of the Window's client. I did not try to compile the Cisco Linux client - I used the built-in vpnc with Ubuntu and Ubuntu 9.10's standard network configuration. For our network configuration, it took about 15 minutes per box (on the windows side, the client installation took the most time. On the Ubuntu side, the cross comparison took the most time. 

Zoredache comment about sniffer is, I think, the gold standard. Another options: use the command to see if the number of transmitted and received packets on the interface is increasing. Also, can see if any other interfaces are being used. For shell scripts, you may be able to query the proc data structures directly. 

WinXP SP2 and higher has a built-in firewall. That software can be configured to block all kinds of things, and it is already paid for. Take a look at the configure and edit your question to be more specific, I'm sure you will get more specific answers 

write a script that sequential checks to see if there is a disk to copy copies one disk at a time loop 

I'm going to keep trying, but I would welcome any advice on how to get Ubuntu 10.10 installed on my headless server. 

Both SMB or FTP would require you to manage users on the NAS device. I suspect, however, that your users will be more familiar with "Shared Folders" and would welcome the relatively familiar interface. Unless there are more variables at play, I would suggest configuring SMB rather than FTP. 

I would suggest not attaching a machine directly to the Internet. Place some kind of firewall between the machine and the Internet. This allows you to do security and network monitoring without putting more load on the server. Personally, I find network and function segmentation frequently simplifies network troubleshooting, although on occasion, the additional complexity does make analysis more difficult. The safest, but most annoying to manage, firewall policy is to deny all and explicitly allow only the traffic you must allow. This is annoying, because one frequently needs update the firewall policy as the network needs change. I would also suggest using some kind of interface firewalling on the server - defense in depth is the key. Using non-standard ports for administration related services doesn't hurt. fail2ban is fine. Pursue the more specific questions about security applications on Serverfault to find more ideas. Security is like the joke about the two hikers and the bear - while one can never achieve perfect security, it is helpful to be a more difficult target than the other guys. 

Decision Trees look at lot like mindmaps. Freemind is a great tool for creating mind maps very quickly, and the maps can be easily exported to the web. 

I have seen MTBF reported on company support sites. Talk with your sales person or SE to obtain the information. 

In my experience, you sometimes get lucky by trying some obvious things first, but once you exhaust the truly quick fixes, you need to get methodical quickly. This method is compatible with Scientific Method and Test One Thing At A Time. 

Does the device have an internal fan? From what I could find on the Internet, I can't tell, but even without a fan, the device may have accumulated dust inside which is preventing airflow. 

These purpose built distributions take a lot less of your time to get locked down and running Management drudgery can be handed off more easily 

Eventually, you will see some kind of traffic sent by the Windows box which is not sent by the Linux box. I wonder if WINS is somehow involved. 

An idea, not really a solution: Try L2TP or a 4in4 tunneling protocol (GRE?), and then run that over IPv6. It seems to me that most of the 6in4 tunneling issues was more about setting up manageable routing and less about the protocol specification of the tunnel. The "protocol" in "protocol" encapsulation seems to be done pretty much the same way, it was always the management piece or the gui configuration piece that tripped people up. 

The InQuira Intelligent Search product allows for the creation of decision trees - they call them "Process Wizards" in part because they include some dynamic functionality. The InQuira product is geared for call centers and includes natural language search, content management, and analytics. 

Compare Problem Stat to a Known Good State and look for the discrepancies. A Known Good State can be an actual documented state. It can also be based on a standard of expected behavior, such as known expected behavior of networking protocols or such as rules of thumb about appropriate average CPU usage. Examples: Using Wireshark or other network sniffer tool, you repeatedly see duplicate packets. Now you can delve in to try an figure out why you are seeing the same IP packet on the wire. Perhaps you have a "local router" scenario, or perhaps something is fragmenting IP packets. Average CPU usage is at 90%. If the average is 90%, then the server is likely maxing out CPU frequently, causing everything to back up. 

Given the budget concerns, I would suggest "Last Season's Model" of server from the likes of Dell or HP. By watching the various dealsite RSS feeds for "Small Business" related feeds, I have found that these standard servers cost less than roughly equivalent servers I have built myself, and they require a lot less time to install. My only complaint with the stock servers is the fan noise, which in a few, very limited situations, has been a problem. Also, waiting for a manufacturer's special can be annoying, but sometimes one has more time than budget. Farseeker's suggestions for App servers seem pretty good. 

Then, when you add a disk to the chain, it will get copied. Like some bittorrent clients that periodically check for a torrent in some folder and then process the torrent automatically. I would also suggest not using USB, if you can, or at least getting multiple USB cards so each disk can have its own USB bus. 

I have configured my built-in Windows Firewall (Windows XP SP3) to allow this box to serve as a local FTP server (FileZilla server). The configuration was working until <insert some recent date>. Specifically, I had configured these exceptions on the Windows Firewall -> Exceptions tab: 

With regards to clonezilla, presumably, the client and the server could reside on the same machine. Install the server, perhaps testing with a separate machine, and then install the client and have it connect to localhost or to an assigned IP of the server. 

I have enjoyed the Security Now podcast ($URL$ which, over the years, has covered many aspects of the TCP/IP family of protocols. Once you have learned the theory, nothing beats installing a protocol analyzer, such as wireshark ($URL$ and getting some hands on.