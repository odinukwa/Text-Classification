Not really; but if the goal is to improve the performance of the query that aggregates information in the GTT, you could set OPTIMIZER_DYNAMIC_SAMPLING to maybe 5 or higher, if your Oracle version is 10g or higher. That would encourage Oracle's optimizer to work harder to determine the best plan by running some sampling queries. You can use that via a hint: 

The Oracle database instance has several background sessions it creates in order to function, and those count against the number of sessions. For example, here is an Oracle 12c instance, one user connected: 

You may need to add additional fields to the group by clause, depending on what your definition of a location is. (Does HoleID uniquely identify a location? Or is it a combination of HoleID and SampleID?) For your sample output, you would need to include as your group by fields. 

If you use DataPump and commands, you can include the parameter in the impdp command parameter file. The log file will then time-stamp each phase. 

In your $ORACLE_HOME directory, there is a sudbdirectory Opatch and there is the command Opatch there to run with a couple parameters redirected to a date/timestamped file that should satisfy audits of Oracle software: 

command and see what LANG or LC_ALL is set to; you may need to set export NLS_OS_CHARSET=UTF-8 or the like. You can list the available locales in Unix with 

If I understand correctly, you are trying to copy the new Database (database4) onto a slave that already has 3 databases replicating (these are all on the same MySQL Database). The problem then is that replication will need to be paused whilst you do this. The process I would use would be: 1) MASTER - restart the master with new | remove all in order to start populating the binary log with database4's data 2) SLAVE - Check the slave Database's replication is up to date ( > ). If not wait till it is 3) MASTER - To stop new data entering the master 4) SLAVE - Check replication is fully caught up and no data is replicating ( and should be the same and not changing (in )) 5) SLAVE - to stop any data replicating into the database once you unlock the master shortly. 6) Run with option 7) Once MySQLDUMP has started running unlock the master Database In this way your systems can continue reading and writing to the master, hopefully keeping downtime to a minimum n.b. that MySQLDUMP may lock the database4 schema while it works depending on your system 8) Once MySQLDUMP has completed, import it into the slave Database 9) check the newly imported database4 looks correct 10) Restart the slave Database, adding to the my.cnf file as you go Once it comes back on, it should continue replicating from where it left off, but including database4. 

To me using the extra variable but getting an easier to read insert command is a good tradeoff and the debug.sql file makes finding sql errors much easier. HTH 

I’ve been given 2 dtsx packages to replace 2 in production. This is for a SQL 2012 server with the packages deployed in the project model. The only way I see to do the update is to export the project, replace the existing packages with the revised versions, then redeploy the project to the same “path” in the server. My question is will the current SQL Agent jobs, calling individual packages, continue to work or are there guid issues or anything else that would cause the jobs to fail? 

If you have Powershell this will get it for you. If you have SSMS 2005 installed you can run it as is after replacing "YourServerNameHere". If you're running SSMS 2008 delete the first line and un-comment the second. 

For SQL 2008 a Server trigger would do what you want but that's not an option on SQL 2005. Event Notification will do for either. The script below was tested on SQL 2005 Standard and 2008R2 Enterprise. As written it will enter a row in the msdb..auditRoles table when a login is created and added to the sysadmin server role. To get a row for both the AUDIT_ADDLOGIN_EVENT and the AUDIT_ADD_LOGIN_TO_SERVER_ROLE_EVENT remove the "WITH FAN_IN" line in the create event notification step. 

The main errors I saw were: and - no need to use - no need for I included the line as this caused an error on my side otherwise. I believe this is related to replication, so depending on your set up you may not need it. 

The config items you added in should have little to no impact on the situation. n.b. you will probably also want , (details) but leave that out until after you've imported the data, of you'll have 100's of GB's of binary logs created. I think the real question is whether your OS is happy with a single file that size, bearing in mind that it wont get smaller, but may get larger. If it's going to be an issue you may want to look at (details) which 'basically' splits the data between and a file for each table. (This involves reloading the dumpfile to take affect) (this is on by default from MySQL 5.6.6) There are pros and cons for both options. 

I have the below Data / Query. Is this really the most efficient way of doing this? As it seems like a lot of work to get the result. I've tried reducing it / combining the parts but this is the only way I can get it to work. (n.b. The actual table has a lot more entries over multiple days - hence the WHERE filters at the end) 

Is this a one-time update, or are you planning on running that query after inserts, where the insert statement is not setting SSC.NUMAR_CHEST? If you want ascending numbers, it sounds like you need to use an Oracle sequence; your subquery will return the max value at the point in time of the start of your query. To use a sequence, create it once (using the name in the examples below), then reference it in your update; every time you access you will get a unique number: 

RMAN command to see what can be deleted. Then do a to delete them. Do you have any guaranteed restore points defined? 

If you have access to the server, use SQLPlus running on the server to invoke your query; that will eliminate the network delay. To get somewhat precise timings, invoke these SQL*Plus commands first: 

Try removing the from the configuration of the channel, and use on the backup command. The following is working for me: 

Cases like this can usually be solved for certain by enabling trace events 10046 (if you have authority to do so), and run tkprof on the resulting trace files: 

Going forward, you could implement auditing of commands which would capture when a user ID is unlocked, but would not capture when it got locked due to number of unsuccessful login attempts. Take a look at $URL$ for details. If you really need to know when it was locked, etc. as Balazs suggests, you could invoke SQL every hour/day/week that would insert into a table a copy of the columns of interest from dba_users: 

The only way for a 'slave' (e.g DB1) to update the 'master' (DB0) is if the 'slave1' (DB1) is also acting as a master to the 'master' (DB0). (commonly known as Replication) You can check if this is the case by connecting to your 'master' (DB0) database and running The result 'should' be blank, most likely it isn't. Look for the fields: and - this is the database from which it is receiving data. It will probably relate to one of your 'slaves' (DB1). and - if these are both 'Yes' then replication is up and running, and any changes made to the database above will appear on your 'master' The quickest way to stop it is to then issue on your 'master' (DB0). Depending on what version of MySQL you are running there are then some things you can do to erase the settings. $URL$ (I recomend making a note of the settings in before issuing any kind of RESET, just in case) Also you may want to add: to your my.cnf file on your master, which will prevent the database starting replication again if the server is restarted. As @ypercube mentions above, you should avoid writing to a 'slave' database, unless you have specifically configured it as part of a master to master setup. 

for testing this, but you would probably want a , , datatype depending on the data you return and how accurate you need it. Also this doesn't include the or fields. You could use which would give you the timestamp field. The database name is tricky, as you would need to carry two variables across, and my script (above) only allows for one (if you are only looking at one database you could just type it directly into the query / set a default value). 

Here's what I've used when updating SQL 2005 to SP4 but I was updating all insances so you might need to adjust if you have multiple instances on a server and only want to update one. 

If you want to use Powershell this is what I have. If you have SQL 2008 SMO DDLs on your computer use line 1. If you have SQL 2005 SMO delete line 1, remove the # and use line 2. If all else fails delete both lines 1 and 2, remove the # and use line 3. Replace Your_Server_Name_Here with the name of the server you're checking. 

While I will never contradict nor intentionally dis MrDenny in any way (too much respect for his knowledge and contributions to the community) I've noticed the MS documentation recommending turning auto update stats off. At Storage and SQL Server capacity planning and configuration (SharePoint Server 2010) MS recommends setting it off. I've also read the same recommendation by MS for earlier versions of SP but can't find the links just now. But I'm NOT a SP Expert DBA so you might also want to check what Chun Liu, MS Permier Field Engineer, has to say about it at Chun Liu on SharePoint or just ignore this post and leave it on. 

Don't know if you're still looking for a solution to this but I'd change the Powershell for the insert command just a little. Add this after the line setting $size = $_.Size 

An index-organized table (IOT) is just that; an index with no "real" table. All data besides the primary key are just "tacked on" to the primary key, so for example if you have an IOT with 6 columns, 2 of which make up the primary key, behind the scenes you just have an index with 6 columns, of which the first 2 columns make it unique. So, sorry, only way to fix it is to recreate the table; to improve the rebuild time temporarily disable logging. You cannot use an hint on inserts to an IOT table to force a direct path insert: 

I get OK results from from server to both instances on , and I can connect to a non-dba user from server to both instances. I recreated the Oracle password file using on server instance and that did not help. Instance names are and on server . Here are my password files on that server: 

You can use the clause on an alter table statement for things like partitioned tables, or tables you know will not have any rows. 

Using a different blocksize would probably be the last thing to try, and only if there is a good reason to do so. A good reason might include 

Unless your development or testing team already has a set of regression test cases, you are out of luck. The possible things to test are astronomical. Now, as others have pointed out, there are some things to look out for, like more restrictive privileges, and reviewing the "What's New" for both 11g and 12c. One big example from the past that burnt a lot of people (including me) was upgrading from 9i to 10g; where developers relied on a "group by" clause in a select would always return the results sorted by the group by columns in 9i, but not always in 10g. Not a bug; the SQL standard for group by says nothing about ordering of results; so a lot of code broke. But again, only way to flush out those types of things are good regression test cases. Now, after all that, I have not seen or heard about any 10g to 12c surprises, but there are a lot of wonderful new features that should be used when moving to 12c. 

(-sorry if I have misunderstood) (-assuming you are using binlog file and pos). The key things to make sure of are that your servers have unique (if you have replication running already then this should be ok). You also probably want to set: 

I'm trying to install the Percona Audit_log plugin for my instance of MySQL. I ran it previously so I know it works ok with MySQL (I'm sure it says so in the documentation anyway). I have MySQL 5.5.47, so I downloaded the files for Percona server 5.5.47-37.7 and extracted the file. I put that file into my plugin directory on the server (debian), but when I try to install it I get: 

I'm just testing MySQL CLuster 7.3.8. According to the documentation, after adding new Data Nodes I need to ALTER my tables to spread the data to the new Data Nodes. So, I run: 

I've just started testing Row Based Replication on my MySQL 5.5. databases. I have now got the following error: 

The IP Addresses in your variables need to be enclosed in single quotes in order to make them character strings (as they have been Declared). The variables need to be moved below all the statements. You also need to add a clause above and the matching clause before your final clause. 

Unfortunately you can't use a trigger to update the table that called the trigger: from the documentation: