I would personally go for the historic table approach with the supertype/subtype design because it will allow you to track the changes to each table independently while still being able to efficiently query the latest data from the non-historic table. But keep in mind to update the main tables with the latest info, i.e. update Printer with the values to be inserted in PrinterHistory. Use datetime instead of date alone so that you can accommodate for changes done on the same day, and instead of joining the historical tables, just view them separately to avoid conflict of joining them in the case that the supertype is updated independently from the subtype. But if splitting the hisotry table of the supertype and the subtype is not possible, then create a slightly denormalized history table for each device type. i.e. 

As to what dezso said, ledgers are quite special depending on the business that is using them. You can try looking at other opensource applications with the same features and functions as your program if you're looking for a basis for your database. You can look for some opensource projects at Ohloh.net and Github.com But as a fellow application developer, I would suggest you take the time to read up on some basic database design best practices and principles. And a bit of normalization, at least up to the Third Normal Form. Having a faulty database can give you a lot of headaches in the long run when maintaining your application. Refactoring database changes aren't that easy with a mature program. 

This is just some supplemental information from G-Nuget's answer since I'm still unable to post comments. The passwords are for database users. The users have a set of permissions(SELECT, INSERT, EXEC...), Roles(Root,DBA,...) and access to tables/views/databases which can also be configured to be on able to be accessible only from a specific ip address/range. For example, you can restrict root to only be accessible from localhost, meaning that any remote device trying to remotely login to your db server as root will be denied. Also, you should always use multiple users on your database/tables in order to lock down permissions even if you only have one database being accessed by one application. Since you're using phpmyAdmin, its safe to assume that you are creating a website or web application. Database and web apps mean that you allow input and output from forms for most cases, in the case of form inputs, you are susceptible to SQL Injection attacks which could possibly pull out a drop table from what should be just a simple and harmless select or insert query. Of course, proper code sanitation can also prevent this, but its still better safe than sorry. Even if you are only on a dev environment, still practice using multiple users to lock down database and table permissions in order to keep it as a best practice and reduce the effort of moving from a single user database to a multi-user locked down database. Doing so will add some complexity to the program, but its better than having your data stolen, or worse, destroyed. 

Consider wrapping the statement in the block inside of so that it bypasses the syntax checking until runtime. 

In this situation you are never clearing your differential base, this means that the differential backup will get larger and larger over time (you can only run copy_only backups of databases from secondary replicas). Eventually the number of pages changed in the database will reach a sufficiently high number that the differential is as large as the full backup. Not an ideal situation. I would either go with running full backups nightly, off of your secondary, or take once a week (or more often as needed) full backups off your primary, and use differentials on the other days. 

You can break down the vendors using a CTE, prior to using the COALESCE function to get a list of emails, with one per vendor. 

There are many more switches available (MSDN has a full reference) so you might need to adjust things to get what you need. If all else fails you could create format files to ensure that you get the data in the exact method you need it. 

Check that your SQL Server allows SQL authentication. By default it will be windows authentication only. You can check the SQL Server errorlog which will provide the login failure reason. The message I you would see in the log in that situation would be 

You need to add the @job_par parameter to your calling procedure so that you can then pass that down the line. Using your example: 

I'm not sure what "Microsoft and Microsoft MVP standards" are, but my recommendation would be to find yourself some extra storage and to grow those logs out, and let them be at the larger size. Shrinking the logs is going to significantly slow down your month end processing, as each growth of the log is going to cause a stall in the processing while the new VLFs are zeroed out. Depending on the performance of your storage a 1GB log file growth could take 1-20 seconds (or maybe even longer). I'm pretty sure everyone would be super happy if it meant that the processing was completed earlier than it is now. There's the additional concern that your log shrinking routine would make the log smaller than it needs to be for general (not month end) usage. That would mean that you would have impact during regular work at those times when the log needs to grow. 

I've been using AGs for 3 years now, with table compression, and have not seen any material impact. The data shunted between the servers is all log transport, and that itself is compressed, the compression of the pages themselves doesn't really factor in. Your only concern would be if you were under CPU pressure already and then added on that little extra for the table compression. 

Note that this query is already set up and runs on SQL Fiddle (where you can also see the table structures, etc.). Here's the problem: the SupportTicket can be on either end of the link. Currently this only counts those where the SupportTicket is on the first link end (Entity_ID1). This should also count it when the SupportTicket is on the second link end (Entity_ID2). How do I fix this? 

Suppose I have the following three tables: SupportTicket, Device, and Link. I've created these in SQL Fiddle. I want to display a list of support tickets within a time range that meet specified criteria and show a count of the types of devices linked to it. The problem is that my current query is duplicating Support Tickets rather than counting all the devices linked to it on one row. Here is my query: 

Here is the SQL Fiddle instance where you can see what is currently returned by this query. A support ticket should only occupy one row with a count of how many devices (by type, as shown) were linked to it in the columns on that row. Below is the desired output: 

For various reasons, I must use a SQLite database with a local application. However, I wrote a Flask app that enables users to more easily classify items in the database, and it runs very slow on pythonanywhere hosting (which I believe is hosting its services on AWS) because of the SQLite. They support MySQL databases, and I believe it would be a significant performance boost for my users to migrate the data to MySQL. However, I need to then pull the data back out of MySQL and write it back into SQLite to work with the local applications. My concern is that the BLOB data would get messed up by creating a dump script (it's binary pickled data that contains Python data structures the application works with once unpickled). Currently updating the local copy is as easy as downloading the SQLite database from the server. What is an optimal way to migrate this data to MySQL in such a way that it will be easy to migrate it back to SQLite? Should I? 

I have a table with columns and . It has been collecting information about the type of browsers users log into the system with for years. Obviously many of the users have since upgraded their browsers. I do not want to consider data for old browsers when users have upgraded. Here is the query I currently have to get a count of how many people use which browser: 

I need to also pull up an image for each person when one is available (but display 'None' when no image is available). There are potentially multiple images for each Person - I only want to select the first one. However, there are also possibly no images on file, in which case a default value should be populated. Here is the current SQL statement to fetch images for Persons: 

Alternatively you could look at using a trigger to ensure that the value does not get set to something different. 

There is no metadata view that is going to clearly give you this information. Your best bet would be to have a scheduled process that runs every few minutes and looks at and evaluates if the is less than the prior , in which case you would know that it had cycled. You can also compare the to the to see how close you are to hitting the cycle point. A very basic view of this would be the following with a small sequence, you can see the initial values remaining, what it looks like after using a few sequences, and then how it gets reset as the sequence cycles. 

Yes, this is absolutely possible, and a great use of resources. You would get your OLTP databases added to an Availability Group, and then add the second machine as your secondary replica. For both replicas be sure to allow reads from secondary. Then do the same with your DW databases. You will end up with two AGs, and will probably want to run with each as the primary on different servers, so be sure that you include monitoring to let you know when the AGs failover, and what AG is running as the primary on each server. 

Extra quotes around dates are a problem (as an aside you cannot pass character strings into float columns, which is why your table variable insert also fails). Using extra quotes can be done at the dynamic layer, and using CHAR(39) rather than embedding a whole load of quotes can make the code a lot more readable don the line. As an addition, initially selecting out the dynamic SQL so that you can run the generated statement in another window to test your results is also a good thing when starting out. 

While Merge is not the best performing solution, based on your requirements this would handle what you requested (as it would insert rows that did not exist on the target). I put a quick schema together that may not match yours, but gives you the idea. 

This is an issue external to SQL Server. You should check that your authenticating AD servers for this machine are working correctly, and that you are not accidentally attempting to authenticate to AD in another data center (presuming that you have one). Additionally look for problems on the AD servers, such as high CPU utilization. For more details on this wait type look at $URL$ 

It does not handle expiration checks, you need to have the check expiration set for that to happen. Reference MSDN Create Login 

It is always a recommended practice to keep the parameters to be the same data type. This is to prevent potential implicit conversion issues with predicates which can lead to the query optimizer not being able to utilize statistics, or indexes, which leads to poorer performance, and can lead to serious CPU and storage performance issues.