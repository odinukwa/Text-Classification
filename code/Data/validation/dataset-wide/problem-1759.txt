shame that so many monitors are 'widescreen' these days... I wish the 4:3 form factors were more common (and cheaper at resolutions of 1600x1200 or higher)... not everyone wants to watch a movie on their PC. I have a 1200x1600 (portrait mode) 20" and while I enjoy (and depend) upon the vertical size, it still feels a wee bit cramped horizontally. 

I use this to keep two machines in sync, or to keep to subdirs in sync (like backing up to a USB drive). As one of the other posts stated earlier, the 'checksum' may actually be forced OFF if you are dealing with local drives. In some rare instances, I've had to add additional parameters to account for changes in login accounts across remote machines, changing ports, and even specifying where 'rsync' lives on the remote host... but those are not directly applicable to your question. 

I would recommend against using 'tcsh' as a shell. It tends to make you think that writing shell scripts in tcsh is ok. It's not. The real attraction seems to be the 'up-arrow' command line ease-of-use, but with bash you get that anyway. Also, coding scripts is much easier in 'sh' and it's derivatives (like bash and ksh)rather than csh and tcsh. I've also found that sh is on ever flavor of unix, and bash is easily obtainable as a first choice add-on. I'd warn against using the features of ksh and bash (like variable arrays and hashs) unless you can guarantee it's existance throughout the enterprise. 

wiki's tend to have revision control built-in, and many are file-based (vs stored in a database), so rsync should work perfectly fine. I know folks who do this for 'TWiki' for replicating their installations to mulitple servers. Perhaps you only have 'ftp' access to your wiki files? you might consider 'wget' to pull from ftp (rather than the http interface) with the recursive (-r) and timestamping (-N) flags set so that it only transfers file that are 'newer' (which isn't exactly a diff). Once you have a 'copy' of what is out on the ftp server, you'd mark the update time somehow (often with just a 'touch' of a specific marker file). You would then edit normally via your local installation of the same wiki, then use 'find $dir --newer touchmarkerfile' to identify the updates for ftp and transfer them over via a script around an ftp delivery tool. I have used such a solution before (though I had the advantage of sucking the changes back to the main server via 'wget', so just used the recursive timestamping approach again. In hindsight, if I had 'ssh' access (I didn't), I would have simply used 'rsync -globtru[n]cv' to simply pull (or push) the files in each direction. 

.MDB is MS Access, right? I doubt pgAdmin 3 being able to do that. If Access can handle SQL then you can probably dump the Postgres databases to plain text SQL and load it into Access (with small changes, I'm guessing). If not, then you probably have to write a small program that reads from Postgres and writes to Access/MDB. Maybe there exists one already. Most converters I found using Google go in the other direction. 

Basically those can be mounted by anything that is run during system startup. Have you grepped for and under ? Are you running automounter (autofs)? 

When Network Manager is disabled for sure we can take a look at . Interface aliases are no longer recommended, but (8) can add more than one address to one interface. This can be done in as follows: 

Can you find on your system? Use to find the package that provides that file, install that package and retry removing. 

GSSAPI or the Generic Security Services Application Programming Interface is used by NFS (versions 3 and 4) when using Kerberos for authentication and encryption. The ubuntu config file has a few lines about this: 

What kind of connection do you have between the peers? Is your network stack doing retransmits because some ACKs are delayed? 

Please do yourself a favor and use something designed for this. You already know about mcollective, but you and I both know that it needs some infrastructure to work. As do puppet and chef. clusterssh, parallel ssh and dancer shell are small simple improvements over a shell for loop. They don't need more infrastructure. But there's also ansible, which let's you do that, but also write reusable "playbooks" with several steps. It does need python installed in addition to sshd, but in practice I've never had to separately install that, it's always been available. Ansible is the only configuration management system I've tried that also works well as a deployment and orchestration tool (puppet needs mcollective and maybe capistrano/fabric for that, ...) (Yes, Puppet and Chef and all the rest can be run without central servers, but you need to install packages on the hosts to manage, which ansible doesn't need) 

AFAIK netfilter does not "understand" HTTP well enough to make choices based on hostname in the HTTP request. You should do it in the HTTP server instead. 

Protect your SSH key with a passphrase and use an SSH Agent so that you only have to type the passphrase when loading the key to the agent. See man ssh-agent, ssh-agent on wikipedia and Using Pageant 

Not saying I know specifically how tomcat performs shutdowns... I would expect a pidfile associated with the process, or a control port that tells the application to shutdown. Barring those, however, it's common for scripts to 'hunt-and-kill' by looking at 'ps -ef' output (or similar). In these cases, it's easy for the kill scripts to be too aggressive and kill off all matching pids (or just the parents of those pids). Cant tell you how many times I've been editing a script in 'vi' only to be killed off by an aggressive 'stop' command someplace. 

You might consider using `date +%w' as part of your tarfile, so you have a tar file for each of the last 7 days and dont have to worry about purging old copies. 

I always run the above to make sure it works, then remove the 'n' flag that once I'm happy with the results. The key features of the above combinations: 

looking for spikes is an interesting problem in general. is your data noisy? Try using the TREND modifier like this 

7.5% for a single child doesn't sound too abnormal, but it all depends on what the child is supposed to do... I run systems with apache and mod_perl and those children get huge. Watch your apache children memory footprints over time to see if they stabilize. If not, use MaxRequestsPerChild to control how often they are restarted. Use MaxClients to limit how many concurrent children you have (to avoid swap or out of virtual memory issues). In my experience, it's generally a memory bottleneck on the webservers. 

to get a rolling 1hour average (or whatever looks like a good smoothing interval). use RPN to compare that to your current value and use another CDEF to highlight the spike. I have found limited value in percentage based banding around a moving metric. I've built interactive cgi's to help explore up/down spike detection using RRDtool and jQuery. 

Sounds like it's not using basic authentication... perhaps cookie session based... what are you hitting it with? wget? curl? perl? There are perl modules that can emulate a browser and you can write a script to navigate thru the login and fetch the XML you want. Google 'perldoc WWW::Mechanize' Would love to hear of solutions that require less module installation, however. 

Then you must force authentication for your restricted area by denying all, then allowing just those subnets, followed by any requirements for how they actually would authenticate. Satisfy all is used to insure both policies of access are required. 

Specifically I do not fill up the entire VG, as that gives me more room to play as requirements change. And growing filesystems can be done online using LVM. 

Sure it is possible. You might have to compile another copy of pam_* whatever to look into another set of config files, but it is doable. Another way would be to use the proxying capabilities of OpenLDAP server (or another server that does proxying). 

Analyze the traffic of the other VPN software. Use tcpdump or wireshark or something similar. Don't guess, look. 

GRUB2 can handle /boot on LVM. I use it on some of my machines. But it's probably safer to have /boot on a partition outside LVM. 

Linux software RAID1 is fine. And in some ways better than hardware RAID. I hated not being able to upgrade the kernel of a RHEL installation because the binary drivers for the hardware RAID weren't updated for the newer kernel. And if the RAID card dies you need to get another one to get at the data (well, not necessarily with RAID1, but with RAID5 you would), but with software RAID any machine will do. 

You might also want to check out Augeas, tool for programmatically modifying config files. It is very powerful in combination with Puppet (a configuration management system), but can also be used by itself. 

Why not use rsync, rsnapshot, rdiff-backup or something else that does syncing. That way you don't have to wait for the whole 1TB to copy each time. 

Check out pGina. It doesn't have a Kerberos plugin, so you'll have to write one. Alternatively you can use OpenLDAP as a proxy and use the pGina LDAP plugin. 

We use DRBD at work. It works pretty well, but we only use it in a two node configuration. I wouldn't really consider it for anything more complicated. 

Have you looked at your firewall log to see if packets are being rejected? Which client/browser are you using? If chome, can you see how connecting to turn works from chrome://webrtc-internals ? 

Another possibility is that you have misunderstood how ssh keypairs are supposed to work. Have you installed your public key on the server? If you have ssh-copy-id available, this is very simple. On the client: 

As others have stated, look into your shell history control for hiding the information from history. But one thing nobody seems to have suggested yet is to mount with the parameter. Try modifying your line in to include , like this: 

It depends on your application and tolerance for failures. If you are running an oracle database for a financial business, you want expensive servers with hot-swappable parts and built-in redundancy (power supplies, disks, even cpu and memory). If it's a web server or compute servers with NAS storage, go cheap (on the server, not the NAS) as long as you can tolerate the loss of a box without much impact. Dont go so cheap that you are constantly replacing bad hardware. The general rule of thumb for me has been to use raid to protect your important disk-based data, but buy cheap commodity hardware for compute and web farms. Get a good load balancer that can detect when a webserver is not responding and mark it offline. Real life experiences: Bad: Running oracle on commodity hardware was a cheap solution that we were able to put together very quickly, but a bad CPU fan caused a server crash which forced us to restore Oracle from tape (ugh!). Good: We replaced 2 high-end heavily redundant machines with 70 commodity rackmount servers. We were able to drop maintenance on the 2 machines and started just buying $2500 'spares'. Over about 2 years, I think we only ever used about 6 of the 'spares' (the real challenge was avoiding deployment of spares for other purposes). 

The number one tool I wish I had when running a small site is 'push-button' builds. It makes patching, updates, and rebuilds easier, which can address a myriad of other problems in the future. No ssh properly installed on all boxes? no curl/wget/vim either? what about other in-house tools you'd like to have on each box? Having central management of your servers is one of the first tools you should have working to make future efforts much easier. 

GomezNetworks or Keynote are good 3rd party payware services that can provide performance data and handle the javascript nature of the website. dotcom-monitor.com seems to be another service that might help, but the key point is that you likely want 'transaction' monitoring, not just 'hit this URL' (though dotcom-monitor can do a form POST directly), and you want them to have full browser emulation (ie: javascript), not just simple HTML POST/GET etc try googling 'web transaction monitoring service' 

First Things First by Covey was good for me.... isolating tasks into the 2D grid of 'important (or not)' and 'urgent (or not)' was insightful. Some of the urgent 'feeling' stuff is just not important.