(online demo) Both will return the same results assuming that there is exactly one Driver record per and is not nullable. (DDL for online demos borrowed from jyao's answer) 

SSMS always truncates long strings unless typed as XML in which case you can set it to allow unlimited. 

Computed column references get expanded out to the underlying definition then matched back to the column later. This allows computed columns to be matched without referencing them by name at all and also allows simplification to operate on the underlying definitions. returns the datatype of the first parameter ( in my example). The return type of will be here too but it seems to be evaluated differently in a way that avoids the problem. In the cases where the query succeeds the trace flag output includes the following 

Just to summarise the experimental findings in the comments this seems to be an edge case that occurs when you have two computed columns in the same table, one and one not persisted and they both have the same definition. In the plan for the query 

is 56 in decimal. This indicates in ( column) is the version number of the format (always 1 in SQL Server 2008) This is the way the literal is always interpreted in SQL Server. e.g. will create a new column of integer datatype. Use an explicit cast if you want it to be treated as a different datatype. This is part of the product source code. You cannot view the definition. 

Think of the way telephone directories are (typically) ordered. If they were generated by an SQL query it would be 

For each student it checks with whether there are any courses that student_id = 1 has failed that are not in the set of failed courses for the student under focus. If there are no such courses then the evaluates to true and the student is returned. student_id = 1 will be returned as they have obviously failed at least as many courses as themselves so you may want to filter that out. 

Yes, you can use a and call if you want to deny the logon. Alternatively if you are on Enterprise Edition you could use these in a resource governor classifier function to route them to a workload group in a resource pool with limited resources. Note the is potentially easily settable from the client if they are making adhoc connections, e.g. via SSMS, or can edit the connection string used by your app. 

Updating system tables directly is unsupported and unsafe. The structure of them has changed since 2000 so the code you have won't work against them anyway. It is sometimes possible to use to do this as a metadata only change however. A full worked example below. 

No SQL Server won't split a single index access into a multiple index access in some sort of attempt to parallelise the work. 

The read uncommitted isn't required for the above in the case that a table lock is taken, there is a hint for that but a less obvious way would be to change the above table definition to 

The position of the on clauses means that the outer join is carried out between the two virtual tables resulting from and rather than just on a single table. In your second query conceptually the virtual table is left joined onto B preserving the entire cartesian product then the result of that is joined onto with the predicate . This eliminates any rows from the final result that do not inner join between and meaning it is equivalent to 

Because if that would incorrectly bring back duplicate rows. So it would need an operator that removed duplicates from these first. From a quick test this end it appears to be dependant upon the size of the table whether or not you get that. In the test below / rows is the cut off point between plans (this was also the cut off point between the index fitting all on one page and it becoming 2 leaf pages and a root page). 

I'd use a surrogate product id. This avoids the need to store potentially lengthy strings multiple times across every order they are used in. Additionally if the product name is ever updated you only need to do this in one place. It is also more flexible as it allows you to have multiple different products (perhaps over time) with the same product name. It is trivial to join from Products to Orders to retrieve the name in the times that you do need it. You could even create a view that encapsulates this. 

To provide information to the optimiser in the event that the temp table contains only rows for a specific client and one branch in the view can be ignored. 

If you were to change to and actually store dates before 1753 you would need to change the lower bound condition to . This also won't bring back dates matching the higher bound of - but in the real world this is unlikely to cause a problem. 

This populated 205,009 ingredient rows and 42,613 recipes. This will be slightly different each time due to the random element. It assumes relatively few dupes (output after an example run was 217 duplicate recipe groups with two or three recipes per group). The most pathological case based on the figures in the OP would be 48,000 exact duplicates. A script to set that up is 

Ctrl + H to open the Find And Replace dialogue Find What: Replace With Tick Click Replace/ Replace All Un-tick to avoid confusing results next time you use that dialogue. 

Why not install SSRS (or check if there is an existing installation available) and schedule a report? The stored procedure would just be responsible for returning the data. The converting to CSV and emailing would be done by SSRS. Once the schedule is set up you can run the report and email subscription on demand if needed by executing the relevant procedure in the ReportingServices database. (If the recipient email addresses are entirely dynamic and unknown in advance this will require a data driven subscription though, which is Enterprise Edition. If this is not available you'd probably need a different approach.) 

No it isn't. Or at least not in its entirety. Part of the transaction log will be included though reflecting concurrent activity during the backup. This is to allow the database to be restored to a transactionally consistent state. Much more detail in this article by Paul Randal Understanding SQL Server Backups 

I imagine as firstly it updated all rows in the table. I'm not sure what the execution plans for this would be like in Postgres too. In the worst case it might have been selecting all rows matching the from , passing in the correlated parameter, then performing on the result for each row in the outer table. 

Within each colour the ids are ordered but the ids across different colours may well not be. As a result SQL Server can no longer perform a merge join index intersection (without adding a blocking sort operator) and it opts to perform a hash join instead. Hash Join is blocking on the build input so now the cost reflects the fact that all matching rows will need to be processed from the build input rather than assuming it will only have to scan 1,000 as in the first plan. The probe input is non blocking however and it still incorrectly estimates that it will be able to stop probing after processing 987 rows from that. (Further info on Non-blocking vs. blocking iterators here) Given the increased costs of the extra estimated rows and the hash join the partial clustered index scan looks cheaper. In practice of course the "partial" clustered index scan is not partial at all and it needs to chug through the whole 20 million rows rather than the 100 thousand assumed when comparing the plans. Increasing the value of the (or removing it entirely) eventually encounters a tipping point where the number of rows it estimates the CI scan will need to cover makes that plan look more expensive and it reverts to the index intersection plan. For me the cut off point between the two plans is vs . For you it may well differ as it depends how wide the clustered index is. Removing the and forcing the CI scan 

The mechanism behind the sargability of casting to date is called dynamic seek. SQL Server calls an internal function to get the start and end of the range. Somewhat surprisingly this is not the same range as your literal values. Creating a table with a row per page and 1440 rows per day 

Showing that the procedure with of was executed 36 times for example. The is memory only so it would be best to set up something that polls this every so often and saves to persistent storage. 

Then when you execute it you supply the desired value. If you definitely want the procedure to end up with replaced with a hardcoded you can use. 

Since the Itzik Ben Gan article was written the hardcoded cache size of 10 for seems to have been changed. From the comments on this connect item 

with no warning about . From which I conclude the overflow happened in this case during the aggregation itself before row 2,150,000,000 was reached. 

The below is a simplified version of something I came across in production (where the plan got catastrophically worse on a day where an unusually high number of batches were processed). The repro has been tested against 2014 and 2016 with the new cardinality estimator. 

Because you have an equality predicate on . With the reversed order it is able to seek into exactly the rows matching the status and these can then feed into a stream aggregate to do the grouping. Your original index supports the but not the - meaning the whole index will need to be scanned and only rows matching the retained. 

Generally your current code shouldn't allow two concurrent transactions to process the same simultaneously. Two concurrent threads can't both acquire locks on the rows matching . If two threads simultaneously call with the same when there are no matching rows whilst meanwhile a third transaction inserts a row for that I can see how this would fail however. One or both of the calls would need to get past the statement before the insert and thus fail to lock anything that would block the other one. A better way of doing this anyway would be with the clause. 

Takes about 7 seconds on my machine. The actual and estimated rows are perfect for all operators in the plan. 

Not yet. You need to use with a style parameter or hack something together with or . SQL Server 2012 will have the function though that accepts a .NET Framework format string Syntax: 

I don't like with sentinel values, it requires picking values that can't ever legitimately appear in the data now or forever after and personally I find expressions containing these more difficult to reason about. For your test rig I tried four different ways of expressing the query and got the stated results. 

The file information was erroneously removed from the system tables in the user database. The file row should have been removed from the database but for some reason wasn't (or maybe was restored from backup at some point which brought back in information about a previously deleted file). 

And has a very high datatype precedence (only beaten by user-defined data types and they would prevent the method from working anyway). Personally I don't find this any more understandable then just finding the conjunction of the individual results though. 

Because this is valid SQL. First of all consider that it is valid to run a against the table that doesn't reference any columns from that table. 

but now should be interpreted as a unitless measure of overall cost. The execution plan is a tree. Each iterator in the tree is given an estimated CPU cost and an estimated IO cost and these are added together to get the overall cost (the relative weightings can be adjusted with a couple of undocumented DBCC commands). The estimated subtree cost includes the cost for the iterator itself and all its descendants. To see an example of the costing formulas used you can look at this article. To determine the estimated cost for an entire query in SSMS select the root iterator (e.g. the iterator) to the left of the graphical plan and look at this metric in the SSMS properties window. When running multiple queries (whether in the same batch or not) the percentage is calculated by adding up all of these values and calculating the percentage as you would expect. You should be aware that even in actual execution plans this cost figure is based on estimates and using this to compare the relative merits of two different queries can be woefully wrong in cases where the estimates are inaccurate. 

If you connect via the DAC then the plans seem to be neither used from the cache nor saved to the cache without this optimisation side effect of (which may be a practical solution if you are developing against your own instance). Or failing that on 2008 you can at least be more surgical about just removing the specific plan as shown below. 

This is not logged. Logically from my description of events the row ought in fact to be different here as the column count should be increased to but no such change actually happens in practice. Some possible reasons as to why this can occur in a fixed length column are 

is a built in function not a UDF so isn't allowed. It accepts no parameters to specify the database. I would imagine that either of the following would work. The first calls and executes the function. The second calls in the current database and adds an explicit statement to set the context. I haven't set up change tracking and tested explicitly but both approaches work as desired with