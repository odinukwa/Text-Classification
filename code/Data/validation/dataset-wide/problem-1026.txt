I have a software that generates transactions and executes them. I would like to asynchronously inspect about the transaction progress in term of the simple proportion of queries executed over total number of queries. I'm able to read both and generated by , so I was thinking that a solution could be printing to some custom messages like "Progress: 3/8" (or another custom text string). I've thought about executing a "logging query" which stores in a proper table the informations about query progress, but this table wouldn't be available until the transaction completes, making it unuseful for me to inspect transaction progress. At the moment I tried the following (suppose to have 3 queries in the transaction which do some stuff): 

I hope this post will help other people in the future. After reading about a lot of digressions about kernel, virtual memory, RAID controllers, disk cache, WAL and other tech stuff I never found someone talking about collations. 

You can play with to obtain the merging concept you have in mind. Here's a link to the official documentation (PostgreSQL 9.4): UNION clause. I think that you would like to remove duplicates (if there are duplicate entries in both tables), so probably the is right for you. 

where and are 50% and 75% of total RAM size respectively. should lay between and for your use case.Test it and give us a feedback. There are also two other moves you should make: 

After setting collation on T420 "C" the A transaction went from 195 seconds to 33 seconds against 40 seconds on Mac Mini; B type transaction went from 141 seconds to 78 seconds against 101 seconds on Mac Mini. This is the best performance improvement after modifing BIOS settings. Many kernel adjustments didn't provide significant improvements. So, running the following command will initialize a new database with collation C and encoding UTF8: 

I am trying to track down a cause for a difference in oracle impdp processing, and I am not finding anything, so I am wondering if anyone here can explain the cause of the differences. At times I have seen times where using the parallel=x parameter in oracle datapump will cause either multiple tables to be inserted at once, up to the value of 'x', or other times will use parallel threads to import a single table. I have not been able to track down what might be causing the difference in performance, and I am wondering if anyone has an explanation, or even just a direction to point me to. It would be helpful to determine why it runs in a given matter. For instance right now I am monitoring a single table, data_only import that was started with parallel=8 in the command, but the import job is only using a single thread to do the import, as shown by the following hint the insert query /+ PARALLEL("XXXXX",1)+/. If the process would use the maximum 8 threads specified to should run much faster. 

I did some reading, and opening the master key was only part of what I had to do. I had to completely configure the new server for SSIS. I found the following blog post helpful, $URL$ The following microsoft documentation was also a good second source of information, as a check against the blog posting. 

I am hoping someone can shed some light on this. When running an import into an oracle database, I sometimes see different behavior based on the parallel option. Some times, I will see multiple data pump workers all running insert commands, with (parallel 1) query hint in them. Other times I have seen a single, or just a few data pump workers, running insert commands with (parallel X) [where x is more then 1] table hint in the queries. I have seen this when running imports that essentially identical. The imports are using different dump files, but where created from the same nightly job, just done on different days. I am using the following options SCHEMAS=XXXXXXXX parallel=32 cluster=y DIRECTORY= DUMPFILE=XXXXXXX_%U.dpdmp CONTENT=DATA_ONLY TABLE_EXISTS_ACTION=APPEND DATA_OPTIONS=SKIP_CONSTRAINT_ERRORS LOGFILE=XXXXXXXXXX.log 

It subtracts 12 months from and checks if the resulting date is greater or equal to . This query works too: 

After asking on pgsql-performance list, Jeff Janes figured out that the cause was associated to the default collation used by Postgres (see this link for more informations). MacMini was using the much performing collation while Dell T420 was using the en/US collation. T420 (Postgres 9.4.1) 

PART2 - UPDATING In order to update you must specify values, so you have to know them a priori. This works perfectly, but probably there are other naive solutions: 

PART1 - INSERTING You don't have to use but and add square brackets to the JSON data structure. I sugget you using a JSON validator website like JSONlint to test the correctness of your JSON data. This codes inserts 3 new records in : 

This query will return the times a student retook an exam after more than 12 months from the first take: 

The problem is that I can't real-time access the content of variable. I can instead access the content of and , so the usage of doesn't make sense at all because it doesn't appear in both and but only in . Should I stick to parse or in order to find the output value from the function (removing the statement in that function because of it's uselessness) or there's an alternative method? Am I missing something? Thank you 

As @a_horse_with_no_name said, without a plan made on the 100.000.000 rows it's difficult to give other advices. Try the previous query and tell us if it's faster (and correct too). 

Remember that must be owned by user, so run . This way you can use wathever to analyze the table (if you need to). 

if you are running the deletes in batches, do you have an index on column that you are using to chunk up the process? IF there is no index and the query is resorting to full table scans to try and find the data that should be deleted, you could be adding a lot of time to the process. Unfortunately backing up the good data then dropping/truncating the table is probably your best bet. You could always rename the existing table and create a new table to do this rather then trying to extract just the data you need to keep. This way if you found your initial load of data was lacking, you still have everything to go back to for a second look. 

I am trying to setup a new sql server 2008 cluster, on windows 2012R2, and the installer is failing on the cluster shaed disk availability check. I have verified that there are 5 disks assigned to "Available Storage" when viewed in the fail over cluster manager. Some background, this is my second attempt to install sql server on this cluster. The first time, the cluster object was unable to create the new computer object during the cluster installation. This caused the installer to fail do to lack of permissions. I have since resolved this, and have run "Remove node from cluster" to uninstall sql server from the node. I am now trying again to run the installer. About the environment, OS: windows 2012R2 SQL Version/Edition: 2008/Enterprise I am running the installer from the current cluster host(node1), and all storage is owned by node1. This includes the quorum, as well as the 5 disks assigned to the available storage group. Both cluster nodes are up and available, and accessible either through the node names, or through the windows cluster name. There are no cluster validation warnings that I know of, but I have asked the windows admin to rerun the validation tool to confirm that, that is still the case. SQL 2008 is required by the front end application (I pushed for at least 2012, but was told it was a no go) 

I know this topic is pretty complex and involves a lot of different factors. Let's say I have several similar queries running at the same time. These queries involves only read operation and several ordering and windowing. If I'm not wrong, with I can get the memory used by some operations like . I need to know how a single query impacts in order to calibrate (and kind of predict) the amount of queries I can run at the same time. Is it possible to get the maximum amount (not the total amount) of memory used by a single query? Should I get it from the output (which gives me only some memory usage infos) or there's a better way? I'm using Postgres 9.5 and 9.6 on different Unix-like environments like Red Hat 6.7, Ubuntu 16.0.4 and macOS Sierra. 

The function is documented here: $URL$ ATTENTION Be aware that those two queries can give different results for periods that are exactly . The following returns : 

It handles models that has the same rank, displaying more than 5 rows per device type just in case. In order to test it, create a table and fill it with data: 

I've searched through the Internet but found nothing that solves my problem (for example issuing and adding to ). returns: 

I know by how these tables were populated that this result corresponds exactly to simple column binding, so that and can be omitted. The problem is, as far as I know, RDBMSs can operate only projections and row binding. Column binding isn't a typical operation. So the question is the following: is it possible to bind columns "as they are" without specifying any join criteria? For example, the following query, which is syntactically wrong, should explain what I mean: 

Well, it looks like when the first sql server install ran, it also configured the file server services, which were still trying to hold the disk and the network name. Odd thing is, the disks where all showing 0 dependencies. This probably could have been fixed if we opened a case with microsoft, maybe cleaned up some registry entries, etc, but the guys from out windows team wanted to just rebuild from bare metal, as it would be quicker and easier. I was on board with that, so that is the solution I am implementing 

I have noticed differences in executions between data pump imports lately, and I am trying to determine what is causing the differences. I am using impdp to move data between databases, and remap the schemas. It seems like I have seen two different things happen when I run the import in parallel, and I am not sure what is causing the differences. Sometimes I see the job running and it is importing a bunch of tables side by side, other times the import process is only importing a few tables at a time, but using parallel inserts for each table. Using Parallel inserts on just a few tables at a time seems to cut my overall run time substantially, but I can't figure out why it only runs like that sometimes and not others. 

I currently have a impdp job running for a fairly large schema (2TB) and ran into an error. The source database has one column that is larger then the destination database. This is causing ORA-02374 and ORA-12899 errors. This has caused the import job to come to a crawl on this one table. I am trying to get past the errors, and the best solutions I can come up with are to either fix the table definition, or tell the import job to skip the current table. I can not currently fix the table definition because the data pump job has the table locked. Is there a way to pause the job, make the column modification, and then resume the import? Alternatively is there a way to tell the job to skip the current table, and move on? This would allow to to come back once the job is finished, fix the table definition, and then re-import just the one table. ETA: This is one of the last few tables in the import, I would rather not kill the whole thing and start over.