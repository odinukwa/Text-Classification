Note that you have to use varchar columns one character wider than your input. Now run this command, note the lack of , but you can keep that there if you desire. It does't recreate an existing table: 

Is there not a SQL Server 2008 R2 specific option? Is this because SQL 2008 R2 shares the same compatibility level of 100 with SQL Server 2008? 

However, note that Returns 2 and 5. Tracing the bulk insert with extended events shows that things are being paramaterized as follows so I don't know what's going on: (@Param000004 char(2),@Param000005 char(5))INSERT [LogparserTest].[dbo].ZipCodes VALUES(@Param000004,@Param000005) So at this point you still need to perform an ALTER TABLE ALTER COLUMN command to alter each column, so it doesn't actually buy you much. 

I have a windows 10 box with SSMS 17. I had SSMS 2016 installed, but it is now uninstalled. When I go to install the SqlServer module I get this warning: 

I have a development VM running Windows 2012-R2 and Sql Server 2014. I have one disk from a SAN holding the OS and SQL. My backup file is stored locally on the disk. I want to restore database as quickly as possible as part of a "reset my dev environment" process. The system is mostly idle when I do this. The system runs SQL Server and Tomcat 7. I usually have an RDP session which has one or more of the following open Perfmon, SSMS, Visual Studio 2013, Visual Studio 2010. Process Monitor. I'm willing to tune this restore to the point where this RDP session becomes completely unresponsive during the restore process. No one but me is using this server. Using Nic Cain's post I was able to get my restore process from 6xx seconds to ~100-120 seconds with the following parameters: 

Do you know exactly what the INSERT statement is and what line of EF code generates it? Just wrap whatever calls that around some logging code and log all the SQL to a text file (preferably using log4net, nlog, ETW etc. Just wrap the code that calls the insert code with a call to set and unset the delegate described here. e.g. 

Assuming I'm running SQL Server 2008 or later, is there any reason to ever prefer a tracing session as opposed to extended events here? Does doing tracing through logman eliminate any of the performance issues of a traditional server side trace, or am I still just running a server side trace and just writing the results to an etl file with logman? 

I am making a SQLCLR stored procedure in Visual Studio 2012. I have these options for target platform: 

I have enabled Driver Tracing using these directions. I am running SQL Server 2014, which uses the 2012 native driver. I run the start batch file then the following LogParser command: 

I have an existing database with everything in schema . I have an SSDT project with objects I am adding to it with schema I have a table that looks like this in the project: 

tl;dr: I'm looking for the equivilant of in a SSDT post deploy script, but doesn't seem to be supported. I have a SSDT database project with tables of "static data" that I populate via MERGE statements in the post deploy script. Many of these tables are reverse engineered from I found this article about using to have multiple files for post deploy. I'd like to take this another step. I'd like generate the the merge statement by calling from a reference database on a remote server, and then executing the resulting merge statement on my database. I could use in sqlcmd or SSMS with SQLCMD mode enables, but that does not work in SSDT. What are my options? 

I read Paul Randal's article on getting the symbols for SQL Server. I have a theory that I could get the extended events to read the symbols from the symbol server catch directory by setting the _NT_SYMBOL_PATH environment variable. The only problem is I can't create the right conditions for any data to show up when I create an extended events session with sqlos.spinlock_backoff. I tried the script in this pdf. I tried HammerDB and I tried this script with adventure works. I don't get any spinlocks. I tried setting max server memory at 256 megs (my default setting on my laptop) and I tried it at 2 gigs. The Extended event session was running, but no data showed up. 

If you create the correct length columns as char or varchar you get the following error for every row: 

I can make a certificate in the windows certificate store for the master column key easily enough with: 

I know database instance names are stored in the registry in subkeys of and the path to master.mdf is stored in . However, I don't see a table in master.mdf or a registry key where the names of the other attached databases are stored. I know I can look in sys.databases for the databases. However, what I want to know is how to find this information out without sql server running. Is it stored in the master.mdf? In the registry? Can I get that information with a hex editor, OrcaMDF or something else? How does Sql server know what databases to attach at startup? 

In SQL Server, each session has its own spid. Spids are unique at any given notice, but spids, like process and thread identifiers in the OS are recycled. However sys.dm_exec_sessions has other columns with session metadata. Is there a combination of columns that is guaranteed to be unique for a server instance? 

Is this normal or did something not uninstall right? I'm wondering if this is causing some issues I'm having with Set-ColumnEncryption. I had trouble with the initial install of 2017, but then I had to delete a folder that the SSMS 2016 uninstaller did not install. I'm not sure what that folder was and can't' find a link to the fix I originally found. I don't appear to have an old version of SqlServer installed as two people have suggested. 

If I create Always Encrypted columns in Microsoft SQL Server from the SSMS gui, it makes a self signed certificate. Is there any value in making a CSR and paying a certificate authority (or using letsencrypt.org) to make a certificate? Does that chain of trust add and value in this case? 

The general understanding of monitoring SQL Server activity is that using SQL Profiler is very heavy, server side traces are less heavy, but extended events are even less heavy. Today I discovered you can capture SQL traces with logman.exe 

I am trying to examine some T-SQL with extended events like I used to with SQL profiler. I have the following event session: 

How do I tell SSMS to look in the folder that the current script is saved to without explicitly declaring the path? 

You can associate a spid with its start time via the login_time column. Other than a spid being recycled at a rate faster than the resolution of login_time, this is unique per server instance. 

It appears to be long column name versus short column name. Upon closer inspection the hidden column always had the same value as an unhidden one. Since the long name in this case was abbreviated and cryptic, I didn't make that connection right away. 

I thought the DYNAMIC keyword would work but it doesn't seem to. Basically I have an ASP.NET Application using the log4net AdoNetAppender. I'd like to be able to SELECT * FROM Logging.Logging in a cursor and have it keep reading new columns inserted after I opened the cursor. I know I could just keep sending and sleeping. That seems like not the optimal method. Do I need to set transaction isolation to something other than the default for the cursor to see the changes? This is my current code: 

By adding MAXTRANSFERSIZE and BUFFERCOUNT I was able to peg the and counters to the top of the perfmon chart. Before that, bytes/sec would spike to the top, but spend more time at the bottom of the scale, and operations/sec was spikey in the middle of the scale. Now I know I can tweak with the numbers and run the restores 100 times at different numbers and measure the restore time. While, there is a certain scientific vigor in that approach, I'd like something more data-centric. I know I don't want SQL Server page faulting during the restore, because hitting virtual memory is a big slow down. However, what other perfmon counters should I be looking at? 

It creates the following table: CREATE TABLE [dbo].[ZipCodes]( [CountryCode] varchar NULL, [Zip] varchar NULL ); So creates . My goal however, is for the resulting table to be: 

On the IBM i (as/400) you can have short and long column names on a table. The short names are listed in the field of and the ODBC/JDBC view. Is it possible to get these short column names from the INFORMATION_SCHEMA view or is it not possible since its not a feature supported in most RDBMSes?