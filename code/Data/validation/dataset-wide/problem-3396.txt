As far as I know, there isn't an optimal useful electronic thing like you seem to be looking for. For starters, I recommend Ladefoged A course in phonetics. This page is another IPA reference chart (by Peter Ladefoged), and contains this link which assembles the online material for the book. This set of vowels is useful from the perspective of auditory training, because it gives 3 real masters of proper IPA standards, so that you can hear what "ɑ" is supposed to sound like (the performers do differ, so it's useful to understand concretely that the symbols have approximate values, not exact values). Another resource is this page from SIL, where you can navigate to this link. This is probably the best bet from a pedagogical perspective. One problem with learning IPA is that it's impossible to both use actual language data and to control variations in speaker anatomy. If you combine the SIL samples and the Ladefoged samples, you can hear actual speakers of languages of the relevant languages so you hear how it actually is; but then you get all sorts of confusing differences in how things sound simply because one sample comes from an 80 year old 5 ft. tall woman, and another comes from a 7 foot tall 25 year old man. There are some IPA flash cards here, which are useful only for their comic value. As for the phoneme/allophone business, it's hard to suggest anything since it's not clear what problem you're facing (how do you know you have a problem?). 450 in summer would not be a bad way to overcome IPA struggles. 

I suggest Ultan's typological study for an overview, and the metathesis database for a few examples. I'm extremely skeptical about the prospects for finding a general case where just place features are moved but not voicing and manner, but place is certainly relevant as a conditioning factor. 

It would depend on two fundamental issues, the cognitive system of the aliens and the nature of the language database. The main issue is whether they could figure out the referent of "apple" -- i.e. will they ever be able to figure out an association between the letter string and a real thing. That's where photographs with labels are invaluable. The problem of alien cognition is that their perceptual apparatus may be wildly different from ours, and they may not grasp the concept of "multiple instances of the same type". Sci-fi authors typically don't veer too far on the weird side, and if your aliens are typical meat aliens that interact with the physical universe in a manner similar to us, then you can assume that it's possible. For example, can they see? do they reside in an alternative physical universe (such as the extra-dimensional aliens who created the Outsiders (Known Space)). Using a words-only corpus, you could not understand the meaning of "The dog chased the boy" vs. "The dog ate the biscuit". You would probably be able to discern some amount of word-relatedness such as "apple" vs. "apples", but probably not "think" vs. "thought" unless you mistakenly make "apple" and "apply" also be related. Without knowing what utterances refer to, you wouldn't "know" the language. This was the one valid point in what is otherwise the stupidest instance of sci-fi language-issues, in the Star Trek "Darmok" episode. 

Your question isn't entirely clear, and Greg Lee has implicitly answered one version, namely how do we determine the subgrouping of languages that we know to be related, for example how do we know that Hindi and Farsi are more closely related that Hindi and English – shared grammatical innovations. Typically there are more innovations in the form of phonological rules, but subgrouping can be based on morphological or syntactic innovations. But this starts from the assumption that the languages in question are related: then the question is, how do we know whether two language are in fact related. As you point out with the case of Telugu, there is a lot of similarity (of a type) between Telugu and Sanskrit, so one could hypothesize that the languages descend from some common ancestor. This is where the role of grammar becomes especially important. There is actually very little grammatical similarity between Telugu and Sanskrit. If you compare Sanskrit, Avestan, Ancient Greek, Latin, Gothic and so on, you find massive similarity in the actual case suffixes and verb inflections, to the point that we can reconstruct grammatical morphemes for the proto-language and we can see that regular sound changes apply in the development of these case suffixes. Comparing the case suffixes of Sanskrit and Telugu, well, Sanskrit has them, Telugu doesn't, and the Telugu postpositions don't at all resemble the Sanskrit case suffixes. (The existence of sandhi is not particularly probative since many languages have rules of sentence phonology; there is little similarity between the languages in the actual rules of sandhi -- although, the Sanskrit sandhi system has been taken wholesale as part of compounding, so this is actually an area where aspects of grammar can be borrowed). The underlying premise, which has been a standard assumption in linguistics for centuries, is that it is easy to borrow words, and hard (though not impossible) to borrow grammar. English now has a bunch of words borrowed from Hindi and various Dravidian languages, and has not borrowed a single item of grammar. One of the most important early works on reconstruction of Indo-European by Franz Bopp was his 1816 On the Conjugation System of Sanskrit in comparison with that of Greek, Latin, Persian and Germanic, which preceded his work on sound laws. It is true that historically speaking, the idea of language-relatedness is often based on lexical similarity and the ancient idea of there being a connection between Greek and Latin, Arabic and Hebrew, Saami and Hungarian was based on word similarity. Nevertheless, it is well-recognized that similarity on lexicon can be quasi-accidental (specifically, due to borrowing of words rather than genetic relationship), and it is a standard methodological principle that only via a comparison of aspects of grammar, rather that just lexicon, can genetic relatedness be established. 

It more or less corresponds to the plane defined by the frontal bone. If you wear a garland or crown, that is where it sits. 

You characterisation is basically right, but could be emended a bit, to reflect different ideologies of phonology (since there isn't just one). The most important is whether or not there are "phones" (or whether phones have a radically different status). A "phone" might be a real mental unit (not physically tangible), or it might be a conventionalised representation of an part of an acoustic waveform. Substance-free theories especially favor the latter interpretation, and SPE-era generativists favor the former (though neither particularly favors the use of the term "phone"). SPE theoreticians do hold that "phones" are comparable across languages and you can say that this "m" and that one "are "the same", but the substance-free response to that claim is the same as a phonetician's response would be, namely producing a graph of the differences between the waveforms. I don't think the "change the meaning of words" characterization of phoneme is right, in that it misses the essential property of phonemes and focuses on a possible consequence. Phoneme are whatever underlying sounds exist in a language, which can be the building blocks of lexical and grammatical formatives. Other sounds come into existence by application of rules. That is it. The question arises as to how we know what those underlying sounds are, and a number of operational tests have been called on: one of them is asking whether there are any two words that differ in just the choice of a particular pair of sounds (i.e. a "minimal pair"). Typically, people conceptualize "two words" as being about having different meanings. So, the "change meaning" characterization is sufficient for phoneme identification, but it isn't necessary. I would not agree with your second interpretation. Actual sound, which is studied in phonetics, is both produced and perceived in real life. Perception involves hearing, and perception is broader than the concept of "phoneme" (e.g. also applies to cognitive states in humans that arise from being exposed to non-linguistic and non-human sound). Interpretation likewise is something that we do to anything that we have perceived. In ordinary language, we would talk of how speakers "interpret" a given acoustic waveform as referring to "do" vs. "two". What you are saying in point 2 most closely resembles the distinction between phonetics and phonology, but that's more at the level of introductory linguistics where we teach sound bites that are memorable but not really true. Phonologists and phoneticians generally (though not universally) recognize a distinction between the discrete, symbolic vs. continuous physical aspects of speech: formant measurments are an example of the latter, and the vowel distinction [i] vs. [ɪ] is an example of the former. Any transcription is, by necessity, a discrete symbolic representation. Up to a point, a transcription can give you more details about the physical sound, but there is a huge limit on the precision possible with a transcription, which numeric measurements go far beyond. Use of square vs. slash brackets is also problematic. I will disregard the problem that many people just don't care what brackets are used and use them indiscriminately. Square brackets represent something that is closer to the physical output, and slash brackets are used to represent something that is further from the physical output. The standard generative convention is that slash brackets go around underlying forms and square brackets go around surface forms. Underlying is not the same as phonemic (note that generative phonology a la Halle rejects the concept of "phoneme" as a distinct representational level). This does lead to a quandry in representing intermediate forms, for example /apa/ → aba → [ab], where aba is not the underlying form and not the surface form. You may have notices that in American English there is a difference in the pronunciation of /t/ in "militaristic" vs. in "capitalistic", where it is aspirated in "militaristic" and flapped in "capitalistic". The governing factor is stress and syllable position, and reduces to the fact that "militaristic" is from "military" vs. "capitalistic" being from "capital" -- the stress patterns of those words differ. So the distinction is derived by applying the relevant rules to get ˈmɪlɪˌtɛri+ɪstɪk whence ˈmɪlɪˌtʰɛri+ɪstɪk, vs. ˈkæpɪtl̩+ɪstɪk whence ˈkʰæpɪtl̩+ɪstɪk. But -istic also causes a stress shift, and thus you get a surface contrast in aspiration vs. flapping. The phonetic outputs are [ˌkʰæpɪtl̩ˈɪstɪk] and [ˌmɪlɪtʰɛˈrɪstɪk]. The intermediate form contains a non-phoneme so shouldn't be in slash brackets, but it isn't an actually pronounced form, so shouldn't be in square brackets either. If "phones" or "allophones" i.e. actual physical outputs are the things in square brackets, then rules of grammar cannot refer to them, because rules of grammar are mental operations on mental objects. A reasonable amount of experimental evidence is emerging to show that the notion of "allophone" or "phone" is suspect, in that it conflates two different things. One is that languages can have distinct sets of sounds which are not yet exploited to make lexical distinctions (so that one sound is a rule-derived variant of the other). Vowel nasalization in Sundanese is a prime example. The other is that languages can have physical variations of sounds which are gradient in degree and timing, which resemble categorial distinctions in sounds in other languages -- patterns of nasal airflow in English are a prime example. Nasal airflow patterns in English really require physiological investigation because you can't hear the point at which air flows through the nose, or when it reaches its peak. You can, however, hear in Sundanese whether a vowel is nasal or oral, just as you can hear that distinction in French (the difference being that in French, vowel nasalization is phonemic, but it is not in Sundanese).