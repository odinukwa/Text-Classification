Like you, I think most uses of the terms 'probable' and 'random' are just epistemic, i.e. they relate to how much information we have. We say of a toss of a coin that it is random, and that there is a probability of (approximately) one half of it falling heads, but this just reflects the information we possess. Tell me more about the force and vector of the impetus given to the coin, its mass, etc., and I may be able to provide a certain answer about whether it will fall heads. But the situation changes when we talk about quantum mechanics. There are processes such that the best we can do is to say that certain outcomes have a probability, and this remains true no matter how much information we have. Does this mean that God plays dice with the cosmos? Could there be hidden variables that are entirely deterministic that we just don't have access to? There is evidence that there are no local hidden variables that could be appealed to in order to restore determinism. It is possible, though entirely speculative, that hidden variables could be dispersed throughout the universe, but this would have strange consequences. Otherwise, it looks like the randomness is just a rock bottom fact that we need to deal with. Even then, one's preferred interpretation of QM makes a difference. On a many-worlds interpretation, all possible results of a quantum event occur through the universe splitting and the only randomness for us is that we can't predict which of the split universes we will end up in. As to your question of where randomness comes from, I find this odd. Randomness is not a thing that causes other things. In fact it is a common error to reify randomness and you find people making strange statements such as "this was caused by random events" or "this is attributable to random variation". Randomness doesn't cause things. Randomness is fundamentally a way of saying we don't know what caused things, and in the case of QM events, no such certain knowledge exists. 

The key word in your first quote is "certain". There is a widely held position, called fallibilism, under which no proposition, hypothesis, theory, or whatever, can ever be called truly certain, because it is always possible for us to be mistaken, or to be in a position where we would revise our claim that it is true if we came to possess new evidence. Several pragmatist philosophers, including Peirce and Dewey maintained this position, and also, in a slightly different form, did Popper and Quine. This is not a skeptical position: it is not saying we don't know anything, or have no reasons to believe what we believe, only that certainty is not achievable. One way to think about fallibilism is that if we supposed the opposite, i.e. that some proposition was infallibly and indubitably true, it would imply that no amount of evidence could possibly persuade us to change our minds about it. Historically, some rationalist philosophers have attempted to establish some propositions as indubitably true, e.g. Descartes with the Cogito, and some empiricists have attempted to build a kind of foundationalism from experiences that are supposedly primitive and incorrigible, e.g. early Russell and Ayer with the sense-data theory. These attempts have not achieved any wide acceptance, though both still have their defenders, and the question is still debated today. 

Then there are those who think that knowledge is a fundamental concept and is unanalysable. Also, bear in mind that we are talking here of only one kind of knowledge - propositional knowledge, or knowledge that. We also have knowledge how, and self-knowledge, and possibly a noumenal kind of knowledge. 

Update in response to further question from Stratos: The main point here is that bayesian updating is a process in which adding information in the form of evidence terms, E, allows one to replace a prior P(H) with a posterior P(H | E) and so to progress towards an assessment of the probability of H that reliably reflects the evidence. That is what inductive reasoning is all about. The skeptic is committed to denying that this process works. One way they could do that is to say that a different choice of priors would lead to a different posterior, hence the issue of objective priors. I said above that for a wide variety of typical cases there are some pretty good ways of assigning objective priors. This doesn't contradict the response on cross-validated that there is no completely general way to assign priors that assumes no information at all. If you would like some more reading on this, there is a good paper here and you might also like Jon Williamson's book, In Defence of Objective Bayesianism. 

The literature on the analysis and meaning of conditionals is enormous. I have studied this a fair bit, and I can tell you there are at least 20 books and over 20,000 papers on the subject. Try going to JSTOR and searching for conditionals to get an idea. Conditionals in natural languages cannot simply be captured by the material conditional. Frege introduced the material conditional, but his main concern was to describe logical, mathematical and scientific relationships. Natural language conditionals do not generally make a good fit, which is why the paradoxes of material implication exist. This is not to say that logic is unhelpful in reasoning, just that you need to be careful how to use it when analysing natural language expressions. When we say "if p then q" in ordinary usage, we assert something much stronger than a truth function. We are typically saying that we are willing to infer q from p. Inferences can be of different kinds, e.g. deductive, inductive or abductive, but any can form the grounds for asserting a conditional. To change your example slightly, it seems reasonable to say A:"If Dave is in London then he is in England" but not B:"If Dave is in London then he is in France". If the material conditional were used here, both statements would be true in the event that Dave is not in London. It might seem possible to rescue the logic by replacing Dave with a universally quantified variable. A':"If anyone is in London then he is England" and B':"If anyone is in London then he is France" but even this doesn't work. The point is more subtle, but in the event that London was uninhabited, both A' and B' would still be true. The reason we find them different is because we are willing to draw an inference from "Dave is in London" to "Dave is in England", but not from "Dave is in London" to "Dave is in France". Another general reason for the difference between the material conditional and ordinary language conditionals is that in the real world we lack certainty and have to reason with defaults that allow exceptions. C:"If the switch is in the down position then the current flows". Except it doesn't if there's been a power outage to the building that houses the circuit. This is why the logical rule of strengthening generally doesn't work in natural language. Strengthing has it that "p => q" entails "p and d => q" which is true for the material conditional but easy to find counterexamples for with real world conditionals. Speaking of uncertainty, very commonly when we make ordinary conditional assertions we are really claiming only that the relationship is probable. In other words, much of the time "if p then q" is better represented as "Pr(q | p) is high". This is why, for example, the rule of hypothetical syllogism doesn't always work with real conditionals. Hypothetical syllogism has it that "if p then q" and "if q then r" entails "if p then r". Again, true for the material conditional, but easy to find counterexamples for with real world conditionals. There are many other differences between the material conditional and natural language conditionals, but my response has become very long already. You'll need to dig into some of the literature to understand it better. Jonathan Bennett's book A Philosophical Guide to Conditionals" is a good start. 

Critical thinking is as much about psychology as it is about logic. We are constantly tempted to overestimate our cognitive powers and underestimate our capacity for error. Overcoming this requires constant vigilance and self-examination. For example, when we believe something or have an idea, our first thought is often to look for ways to confirm that it is true. This quickly leads to confirmation bias, or cherry picking of data. The appropriate thing is to look for ways to falsify it, but this does not come naturally. Also, when evidence is ambiguous, we tend to interpret it in a way that is most favourable for our own views. When evidence comes along that contradicts what we believe, we have a tendency to hold it to a higher standard of acceptance. We also cut corners a lot when thinking: we rely on useful heuristics to save time, but this can often lead to blind spots. We also frequently overestimate the veracity of our memory and underestimate our susceptibility to suggestion. As well as cognitive biases, we tend to suffer from motivational biases, such as wishful thinking. People are often quick to accuse others of wishful thinking, but the fact is we are all vulnerable to it. We also don't like to admit errors or change our minds about things, because we naturally tend to see this as an act of weakness, but it leads to entrenched positions and gets in the way of accepting things as true. Another very common problem is that we tend to congregate with others who agree with us, which leads to reinforcement of what we already believe. It is always a good idea to spend more time reading and conversing with those whose views diverge from our own. As to books, Steven Novella's audio book, Your Deceptive Mind, is good on the psychological side of things. 

This puzzle has been discussed by many philosophers, mostly under two different names: the surprise examination, and the unexpected hanging. A search for "surprise examination paradox" at philpapers.org shows dozens of papers. Unexpected hanging is the name of the Wikipedia article on the subject. 

There is no general agreement on the axiomatisation of probability. Kolmogorov was a frequentist and his approach proceeds by supposing the existence of an event space, or possibility space, defining the probabilities of propositions in terms of their frequency relative to the total size of the space, then defining negation, conjunction and disjunction in terms of these, and finally defining conditional probability in terms of conjunction. Many since have found this approach unsatisfactory and have looked for others. Popper developed an axiomatisation in which conditional probabilities are considered as fundamental, because he came to think of probabilities as dispositions and these are more naturally interpreted as conditionals. John Maynard Keynes and Rudolf Carnap developed an approach to probability in which it is a logical relation: a kind of degree of partial entailment. This approach was taken further by Richard Cox (The Algebra of Probable Inference) who derived axioms for probability based on fundamental postulates about what qualifies as a plausible inference. Edwin Jaynes developed the idea further (in the first few chapters of his book, Probability Theory: the Logic of Science) and showed how the concept of epistemic probability can be combined with information theory to extend the scope of the Bayesian approach to probabilistic reasoning. Bruno de Finetti showed how the axioms of probability theory can be derived from decision theory if we understand probabities to be degrees of rational belief. His approach takes a bet as a paradigm example of a decision made under uncertainty, and shows that if we take as a minimal criterion of rationality that it would be bad if you allowed a Dutch book to be made against you (i.e. a combination of bets such that you are bound to lose whatever happens) then if you are to avoid this state of affairs, your bets must conform to the calculus of probability theory. There is a good summary of this material in Alan Hajek's article, Probability, Logic and Probability Logic. $URL$ Hajek also wrote an interesting paper called What Conditional Probability Could Not Be. $URL$ in which he argues against the Kolmogorov axiomatisation and in favour of the view that conditional probabilities are fundamental. 

In both your examples, the right-nested conditional is the correct interpretation, i.e. 1.2 and 3.1. One way to identify a right-nested conditional easily is to employ the import-export rule by seeing whether the sentence makes equal sense when you replace the first conditional with "and". So 1 is the same as "if Hyperion and Starbucks are both closed and you insist on having coffee, you're going to go to MacDonald’s but you’ll wish you hadn’t." And 3 is the same as "if Hyperion is closed and you insist on having coffee, you're going to wish you hadn't." One can, of course, construct examples where the meaning is genuinely ambiguous, but in practice right-nesting is usually right. 

It is not impossible in principle that there might be a logic of explanation, which is to say, a logic that answers "why?" questions. Different logics, such as intuitionistic, relevance, linear, etc., have different natural semantics. It is not unthinkable that one could have a logic whose natural semantics was that of explanation. Asserting a proposition "A" would then be interpreted as "A is explicable", and a conditional "A → B" would be interpreted as "A explains B", or possibly "an explanation of A can be manipulated into an explanation of B". The main problem is that I expect it would be fiendishly hard to come up with satisfactory general rules for such a logic. Carl Hempel attempted to describe a logic of scientific explanation, but it is widely regarded as unsuccessful. Different branches of science seem to use different paradigms for explanations of phenomena, so the rules would be difficult to generalize. In some specialized contexts one might be more successful. In computer programming, for example, running a debugging tool might be interpreted as enquiring why one obtained a particular result from a program. Depending on how the program is structured, it might be possible to produce debugging output that takes the form of a formal logic.