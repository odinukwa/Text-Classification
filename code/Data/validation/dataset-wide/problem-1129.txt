i'm trying to update a remote table using linked server, based on id of my local table. I'm trying this: 

Please, if this is not place to post questions like these, let me know and I will delete it. Inside the Glenn berry's Diagnostic queries, there is a query to show how much CPU a database is using. This is the query: 

As Seen Here ( Microsoft's website ), this field is a field. Ok. Converting bytes to KB I have: 8,309,834 Kilobytes. but the database size inside the folder, has: 

When I righ click the job + view history, I can only see 2 days of logs. I'm sure this is a really noob question but I can't find the answer. 

I'm in a process to change database collations. I could create ( and copy ) scripts to create/drop FKS, PKS, but i'm having problems with unique keys and check keys. I got this query: 

How can I sum, for example, LAUDOS_EXPEDIDOS1 + LAUDOS_EXPEDIDOS2 Into a 4ยบ column? like this: I'm pretty sure it's something really easy. I'm trying to use sum with the aliases, with no success. 

If Oracle cannot start even in NOMOUNT mode then you can fetch configuration information from spfile: 

It is possible to restore backup on a different cluster. Did that many times myself. Just backup and restore procedures are not so simple in the case of NDB. 

Then you would not get wasted space in tablespace and also even in case your data grows fast and catches up with autoextend operation - there will be only small delay in writes because 10GB will not take a long time to format. And Oracle will autoextend tablespace proactively. It will not wait for the last blocks to fill up. 

For Oracle client you can use Installation using Response Files. Your C# program can generate response file and run installer with particular command line options. 

Take backup of NDB tables structure with mysqldump. Make NDB backup. Restore mysqldump backup on the new cluster. Restore NDB data using ndb_restore (on both datanodes). Rebuild indexes (command has to run just on one datanode). 

This instruction is for Oracle Linux. And you are using Red Hat Enterprise Linux. Even though Oracle Linux is compiled from RHEL sources but they have separate base repositories. You will have to resolve dependencies manually. I tried to download preinstall RPM and install it with yum but it requires RPM which is not available on RHEL. Even if I am not the fan of what Oracle does with RHEL but for Oracle databases I switched from RHEL to Oracle Linux. Less resistance this way. Oracle ASM kernel modules are not available for RHEL and Flash Cache feature is available only on Oracle Linux. 

You can see I got a lot of ( Blank ) values. What would be the best way to update this ( lets say I have more than 100 columns ) to ? I made a script on excel, with for all columns. But I'm curious to know if there's a better way to do this. 

But the restore never completes. using the complete_percentage is null ( for like, 10 minutes ). In my own machine, I could restore the database, this is my server: 

If I don't get calls, I just drop the database. Just make a backup of the database and save it for x months. EDIT1: You can kill all connections of a database with this: 

inserts into an table that already exists. put data in a table that you will create in the same action ( whitout the need to create column by column. You should use something like this: 

I just found a database, and I have no Idea what to do. I can't drop, update, or anything. The database is in this state: 

is there a way to make this query, a unique query? and, is there a way to name these columns? because I know I need to alias them, but they stay with that (no column name) above it. 

Everybody here are missing one point - SQLs in question are retrieving 1000 rows. And one cannot retrieve 1000 rows fast unless most of the data is cached. If data is not cached one has to do 1000 random reads sequentially to retrieve the data. Good disk based storage with fast disks will give you up to ~200 reads per second. Common disks even in RAID I doubt that manage even 100. That means 10+ seconds to get results even with the best indexes. So in the longer run such data model and queries won't work. Now the queries run fast when you hit cached data. 

Not exactly an answer to your question but amount of changes generated by the session can be found using such query. 

For that you should configure SQL*Plus output. Documentation what can be done and how to do that is here. In your case it is: 

To import data into a different schema you need to grant role to the user. Oracle MOS Doc ID 351598.1. Or you can use SYS user: 

Ultimate source for such answers is Oracle Database Licensing Information. Sadly to downgrade from Enterprise to Standard Edition you have to export all the data install Oracle Standard Edition and import data (Doc ID 139642.1). From your list "Automatic SQL Tuning Advisor" is Enterprise Edition only feature. As a rule of thumb - features which needs AWR are Enterprise Edition only plus you need to buy . 

According to one of the Microsoft's support pages, there is this query to show if there are gaps between logs. there are none: 

After some time searching for an answer, I got a LOT of question saying to use ,, and etc, but what I need is: Table: 

Edit 1: With James Anderson's query, I got this too ( filtering with ) Again, I don't have backups from 2015-09-15, only 2015-09-14. 

I'm thinking in something like execute sql, But Im having problems with files with spaces in name ( jhon vacation.mp3 ) for example. Any Ideas? Edit2: Query done, now I need to fix the Spaces in files name. 

I could fix this by running the query pointing to a driver inside the server I'm running. How can I give permissions to a SQL Login/User to a network folder? Should I map the driver first and then run it? Like the tip they have in this site: $URL$ I'm trying it right now. Question: Is my BCP running under SQL Service Account ? or it's using the -U -P account? 

A simple trace with . But, how can I know the query that has the problem listed here? I don't think is appropriated to this. EDIT 1: Well, I'm using: STORED PROCEDURES: 

I was using the following steps to perform backup and restore. In the first step I generate dump script to make schemes structures backup. 

Apparently you dropped the object already. Now to calm you down - corrupted blocks now most probably are in free space and do no harm. They just annoy you during backups. To check that: 

Yes you can do that. There's nothing dynamically changeable here in normal circumstances. All the files here are not touched except in case you apply some patch. Files specific to the particular database resides in: 

Even such query counts as scan if id field has btree index. In your case we almost all DELETE queries are running in loop with 'LIMIT 1000' or similar value. 

I believe your DBA told you to use temporary tablespace for temporary data? If so using temporary tables have the following advantages: 

If it really shows here you have two options. You can simply ignore it. Once the block will be assigned to some object it will be reformated and corruption will go away. If you want to fix corrupted blocks then you will have to create the object which would occupy your corrupted blocks. Let's say your corrupted block exists in tablespace and datafile . First you have to make datafiles of tablespace not autoextendable. That is not to inflate datafile size. And then you'll create filler table. 

I tried with both configurations, by using services account, and leaving them blank. I checked with that the server is listening on this ports: 

EDIT1: Each "Image" has a binary code, and it has 43679 characters ). 1 millions rows for this table ( 1.141.947 images ( with coduser, image, and etc ) 

What can cause this problem? the database is 140GB. Is there a place where I should verify? With the file is valid. I'm using T-SQL to the restore: 

is configured this way: LSBACKUP_MyDatabase - every 25min : LSCOPY_MyDatabase - every 1 minute LSRESTORE_MyDatabase - every 10 minutes 

Maybe this question is confusing to understand ( as I'm not fluent in English ) but, I need to select for example, but this needs to have the following rule: Part 1: it needs to be ALL cods, except for . Part2: Another column, with another , but only with cod 21. it's not the entire query, But after this, I think I can do it. I tried this: 

We are implementing Zabbix, monitoring a lot of processes. But I think it would be great to monitor the SQL service ( if it's running, stopped...)