Very interesting question .. Based on my understanding, I will try to answer this. Other experts can correct me wherever I am wrong. 

I would recommend you to study and implement Ola Hallengren's SQL Server Backup solution. Its free and is widely tested and recognized in the community. e.g : Back up all user databases, using checksums and compression; verify the backup; and delete old backup files 

UDP 1434 is used for Named SQL Server instances and SQL Browser service listens on this port for any incoming requests to a named sql server instance. The browser service will respond to the client with TCP port no. for the requested named instance. From BOL : 

EDIT : To make my answer more meaningful, I am adding more details Test any scenario that you are going to implement to avoid any surprises !! 

Obviously, Enterprise edition can take advantage of Parallel execution of DBCC statements, but look out for MAXDOP setting as it might end up taking all your CPU. This can be hard limited by Resource Governor. Note: If you are having SPARSE column, then your CHECKDB will be dead slow as described here. Finally, its how to prevent database corruption by utilizing all available tool set + your faith in your database server hardware system and most importantly the value of your data. Some excellent references : 

Refer to BOL for more details. You can Lists the accounts associated with one or more Database Mail profiles using sysmail_help_profileaccount_sp Below is the t-sql that will give you all the details : 

This is done using transactions (honors ACID properties) & concurrency models. SQL server supports Optimistic concurrency as well as Pessimistic concurrency (default concurrency model) which inturn uses different isolation levels (read committed is the default isolation level ) and locking mechanism to allow multiple users access the data at the same time. Refer to : SQL Server Transaction Locking and Row Versioning Guide 

ANSI_NULLS { ON | OFF }: should be ON. This will be a default behavior in future versions of SQL Server. When is set to then NULLs follow the ISO compliant behavior of the Equals (=) and Not Equal To (<>) comparison operators. 

What you are telling is not entirely possible out of box (keeping in mind your 3 bullet points). If the above suggestion - does not work (provided you do proper testing), then you can use the . Do not use - see kb/308886, SQL Server Read-Consistency Problems by Itzik Ben-Gan, Putting NOLOCK everywhere - By Aaron Bertrand and SQL Server NOLOCK Hint & other poor ideas. hint will help in your scenario. The gist of hint is - if there is a row level lock then SQL server wont read it. 

AlwaysON will provide failover in terms of seconds, but it depends on your n/w. EDIT : If you implement AlwaysON (which fits in your situation), below is my recommendation: Assuming (from your post) DB1 is the main database and DB2 is for reporting. Configure DB1 as primary in AlwaysON. You get a warm standby database on the secondary server (say DB1_Replica). Configure DB1_Replica as readable and have asynchronous-Commit mode configured. 

Note: I have highlighted in the image what each column means and what you should look for. Basically, you have to monitor your autogrowth events for a duration of time e.g. during high activity or for you entire business cycle and then averaging it will give you a some what exact value that you can choose for autogrowth settings. Now, for Log file, you also have to consider factors like Index maintenance, CHECKDB running, etc. So size the log file to support the volume of data changes occurring in the database and take log backups frequently so as to allow rapid reuse of space within the log file. Also, worth to mention that you should enable Instant File Initialization as well. Works only for Data files ! Refer to Importance of data file size management esp the Data file growth and Data file shrinking by Paul Randal. Note: Do not shrink your database, unless you do a massive purge of your data and you are sure that the database is not going to grow that big again. It causes fragmentation and databases are meant to grow ! 

is an intermediate view allowing to join the DMV's that are primarily identified on - DMV's with other DMV's. This view will tell if the transaction is a user transaction = 1 or a system transaction with = 0. On the other hand, - is the DMV that will store transactional information showing status, state of each transaction - initiated but not completed, type, etc on the sql server instance. It also gives info on distributed transactions as well. This DMV will give results for all databases on the server instance and it is a point-in-time snapshot of currently active transactions - results will change each time the query is executed as the sate of the individual transaction will change. Refer to sys.dm_tran_session_transactions and sys.dm_tran_active_transactions for column lists and what each column means. 

Log shipping : with delay of log restore on secondary and choosing the secondary to be read only with disconnect users when restoring the log. This will allow you to have read from secondary server. you just have to balance between the restore frequency on the secondary. log shipping will be much more efficient as compared to your approach and it works out of the box and you can even monitor it. Plus you get a warm standby server which will act as a DR if the primary goes down (cavet is -- depending on the frequency of your log restores, there will be some amount of dataloss) Transactional replication : if your main database is not undergoing schema changes and you want near-to-real time reporting, you can use T-rep. Just make sure that you are replicating ONLY objects that are required for your reports. The initial snapshot will have some penalty on your main database. 

For third party tools, highly recommend to check out (there are many third party tools out there, but below ones I have used and they are great): 

==> You can use it with dynamic sql to connect to different servers for backup and restores. ==> There are tons of scripts found on internet that will tell you how to do it. SimpleTalk has -- Backup and Restore SQL Server with the SQL Server 2012 PowerShell ==> Automate Database Restore to Remote Instance with SSIS 

I normally set database in single_user and then waitfor delay and then set the database back in multiuser as below : 

What you are telling is possible. Since you are running Standard Edition of SQL Server you have to bear following things : 

--- now check if the dll is loaded or not SELECT name, description FROM sys.dm_os_loaded_modules WHERE description = 'XTP Native DLL' 

Logshipping is a death tested technique that has been around since many ages. Looking at your specific error sequence on secondary server 

Create another table with Name_staging and columns with correct datatype create Indexes, FK, etc Insert the data into the Name_staging table Rename the Name_staging to Original table. 

Ideally, you should not be restarting the server instance, just manual checkpoint and offline/online of the database will clear the files. e.g. Repro : 

means that SQL server will use to allocate the temporary space as opposed to allocating space in the user database whose index is being rebuild. This means you will need less free space in your user database during an index rebuild operation and more free space in tempdb. It gives you better advantage when tempdb is on a different set of disks (LUNs) from the user database. From SORT_IN_TEMPDB Option - BOL : 

because the activation execution context is trusted only in the database, not in the whole server. Anything related to the whole server, like a server level view or a dynamic management view or a linked server, acts as if you logged in as [Public]. Refer thoroughly blog post by Remus Rusanu : 

There is no way to convert a windows login to sql server. You can create a login with password and assign appropriate rights. 

You can schedule the log reader agent frequency to 30 mins (or as per your needs). As a side note, I will suggest you to read this article by Kendra Little. It explains T-Rep and compares it with other technologies (AlwaysON, CDC, etc). 

The above will result : (click here to enlarge) Note: As Aaron commented, you should remove CityID if not required in output list. 

Use SQL Data Sync which is still in preview state. You can set the frequency on how often you want to sync the data to Azure. Use SSIS to migrate your data. You can even schedule your SSIS package to run as per your needs, e.g. every 12 hrs. Possibly use SQL Azure Migration Wizard which essentially uses BCP (Bulk Copy) to get data out and BULK INSERT to push data back in Azure. 

I would recommend using SSIS. As it has the flexibility to allow you to save the package and reuse it for subsequent runs and it can even be scheduled using sql agent job. You are also compromising security by allowing ad-hoc distributed queries to run (changing it to 1). I have tried the exact same query that you posted and it works for me. 

You can download SSMS 2012 or 2014 which is a full seperate download and then use it to access tables, etc link: ssms 2012 can't comment on what/ how your plugin is. 

- Setting it to - it will change the computer settings to include the latest updates when you scan for Windows Update. I would suggest to set it and you decide what updates you want for SQL Server in-terms of SPs/CUs. From BOL : 

Couple of points to note : In sp_addsubscription make sure that And the article properties should be set to : 

Yes, if you know what data to insert after corrupt data pages are deleted, you should be fine. make sure you find out the root of corruption to prevent it from happening. also, once everything is fixed, then do a full back and restore it on a different server to make sure everything is fine. Your plan sounds good to me. 

Its very difficult to reverse engineer and interpret SQL T-log records. As mentioned there are 3rd party tools like APEX SQL Log , RedGate's LOG Rescue, Log Explorer Also, there is an undocumented command fn_dblog that can be used as below, but you wont have the flexibility like 3rd party tools. 

Alternatively, you can use a mix of Event Notification or Profiler with Blocked process report to detect blocking on your database server. 

Remember: Auto-close is deprecated, so one more reason to avoid it. Seems like you should invest in reading - Multi-Tenant Data Architecture 

It starts when you initialize the secondary database from the full backup of primary database restored with NO RECOVERY on the secondary server. Logshipping proces relies on the concept called Log-sequence number (LSN's) to link which log backup will be restored after the last log backup. A picture is more than a thousand words ! 

There is no other way to enable service broker except taking that database out of AlwaysON Availability group, enabling service broker and then joining it back. If you know that your source database has service broker enabled, then when you restore the database, use , so that you dont have to go with such trouble again. If you try to enable service broker for a database involved in AG, you will get below error :