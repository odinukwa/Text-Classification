The clause tells SQL to process all the rows with the same value - category_id in your case - as a block, separate from other blocks with a different value for category_id. The will calculate its total for each of these blocks in turn. Without the SQL treats all the rows in the table as a single block. The calculates a total for the whole table and returns a single row of output. 

Assuming your DBMS is using BTree indexes, there is a chance that there may be a small speed-up in certain situations. To perform a lookup on a BTree the system starts at the root node, follows links through the intermediate nodes to the leaf node. The fewer columns there are in the index the more index keys will fit in a page and the fewer levels will be needed in the BTree. With fewer levels there will be fewer page reads for each lookup. There are a lot of caveats to this. For example, in most realistic scenarios the fanout in a BTree is such that for medium-large table the index becomes 3 levels deep and stays that way for a very long time, irrespective of the keys in the index. Also it is likely most of your index pages are in memory already so the additional read time is trivial. Against the slim possibility of increased read speed you must balance the absolute certainty of decreased write speed. Each row inserted must now be written to the the table, the {a} index and the {a,b} index. Most optimisers will consider indexes where the leading keys are matched in the query. Say you define and index on {a,b,c,d}. That would be considered for queries that have predicates on a, a and b, a and b and c, or all four columns. (I say "consider" and not "used". Choosing which index to actually use is a whole different discussion.) So for a wide class of queries you get some potential benefit without the addition write overhead. Here's more detail on that. How much storage will an index take? Order-of-magnitude I'd say the size of the columns times the number of rows. So if you have an integer (4bytes) and 1 million rows, the index will take about 4MB. It might be 3MB, it might be 10MB, depending on a lot of things. But it won't be 1KB and it won't be 1GB. As a rule of thumb I would avoid keys with duplicate leading indexes. If you identify a very specific use case, after careful testing, and having considered the load on the system as a whole, then you may think of adding some. Not as a matter of course, however. 

Looking at Relationship 1 as an example. The primary key of this table is ID_PhoneNumber and ID_Company. The CompanyName, however depends only on ID_Company alone. The table is not in second normal form. For the design to be normalised it would have to be: 

A graph database would be a good candidate. They specialise at retrieving interconnected data and navigating the relationships between objects. The ones I'm familiar with allow dynamic schema so different objects can have different values. Some allow classes of objects to be constructed so some consistency can be enforced. The links between objects are integral to query processing. A document database has no explicit concept of a connection between one record / row / object / node and another. Your queries would reduce to step-by-step retrieval of an object, then following the "foreign key" pointers to other objects in the application. At least in an RDBMS the referential integrity could be enforced. 

Yes, Excel can query data from a database. On the Data tab there are tools to create and maintain connections. IIRC it uses ODBC so all major database products can be referenced. 

In July 2017 Microsoft announced Azure Stack, which makes their cloud management software available for on-premises deployment. Included in this is SQL databases. 

Problems that can be solved by splitting include Validation Any one part of the name can be compared to a master list. Those which do not match can be rejected. Postcode / zipcode is an obvious example. These are issued and maintained by an independent authority. The only valid ones are those issued by that authority. Sorting and Selection I have seen cases where postal charges are reduced if mail is handed to the delivery service already organised to some extent. Having the corresponding columns produces tangible business value. Analysis It can be useful to know where your orders are going, in a geographically hierarchical way. This may drive sales initiatives, product development or commission payments etc. Code Duplication By having all applications in an organisation adopt the same data model (that of the most complex consumer), a single code base can be adopted enterprise-wide and maintained consistently. Endlessly duplicated hair splitting can be avoided, or at least delegated to the propellerheads. Addresses held by different parts of the organisation can be updated consistently. Customer service and satisfaction can be increased. Development effort can concentrate on the unique, high value parts of a system. Legal Issues Laws and taxes vary by jurisdiction. By capturing the detailed address values separately it is easier to cross-reference transactional data to compliance requirements. Duplication It is simple to spoof addresses held as text by moving one element to the next line or resequencing some parts. Fully parsed addresses are easier to compare. This may be a simple data quality issue, or may have compliance or credit implications if, say, multiple shell companies make large orders to the same delivery address, or a credit card is used to deliver to many dispersed locations in a short period. Formatting Parts held separately can be combined in whatever fashion suits the current need. If, say, long thin print labels become cheap you can reformat to use them. Of course none of these may apply to any specific application. Data of this type is much easier to parse and validate at source, when collected, than it ever will be in post analysis. So even if YAGNI it may be better to put the extra effort in up front for little cost and a potential large future saving. Finally, I wouldn't dismiss the human factor. The data model is produced by data modellers. It's what they do. That's their profession. They're not going to tell you to just dump it in a BLOB, are they? 

Query plan generation is complicated. The number of possible plans grows exponentially with the complexity of the query. Each possible plan will be optimal within a small range of data counts and distributions. Change any one of these, however, and another plan becomes optimal. Say you add an index. This can be used for some range of counts and distributions. For a different range the optimiser may chose a plan which does not use the index, so the index may as well not exist. (Indeed, it's causing harm through slower INSERTs.) This is why it is always best to test code against realistic production data. In practice, however, most development occurs against "big enough" data sets with the expectation that the plans seen will be close enough to those produced by full-scale production data that the differences will not matter. This seems to hold in practice, by-and-large. There can be nasty surprises on launch day when it doesn't, however. Of course if your code change is in the procedural application rather than the invoked SQL then most likey this will scale linearly and 30% relative improvement will be seen however many rows are pushed through. The PICASSO tool has helped me visualise the stability of plans with respect to changing statistics. 

Nothing will happen .. initially. The database's collation is only a default that is copied to new columns when they are created. Once created they retain that collation. See sys.columns.collation_name. If you change the database collation, then create new columns, those columns have the new collation. This may or may not be collation compatible with the pre-existing columns. If you adopt a case sensitive collation for the DB then SQL becomes case sensitive. Object names will have to match the declared strings exactly. This may break the application. I went through this process many years ago. I seem to remember I had to explicitly ALTER every character column in the DB to the new collation, and then update the data for the collation change to kick in (update table set charcol1 = charcol1, charcol2 = charcol2... ; non-string columns were not needed). This was many versions ago so things may be easier now. Were I to do it again, depending on size and complexity, I'd be tempted to script all objects, edit the file to remove collations, then build a new DB from scratch and transfer the data and permissions etc. Good luck. 

The only thing that I can think of is to split the date into day/month and year - having one or the other as NULL as necessary. This is one of the only times I consider a split year/month/day structure, with separate fields, to be OK. 

Using avoids having to jump through hoops to find a join condition for an clause. Next you need something to count. This is where the statement comes in. The case returns an integer value per pair of first names, which is what gets counted. (If I'm reading your question correctly you want where the LastNames match so that's the comparison I have. Hopefully it's obvious how to modify this if I'm wrong.) 

Neo4j is a schema-optional graph database. You could choose to enforce a full schema and reap the benefits. MongoDB v3.2 has validation of structure and data type. 

If you are trying to measure this by, say, running it in SSMS and looking at the elapsed time in the status bar, be aware that is also measuring the network time to transfer the results from the database engine to the client and can include a significant amount of network variability. You could and . This will show how much work the DB engine itself is doing. Minimising these numbers, and analysing the query plan, should give faster-running queries. 

If used as-is this could produce misleading or confusing results. For example, using an interval of YEAR would show 1999-12-31 (YYYY-MM-DD) and 2000-01-01 to be one year apart whereas common sense would say these dates are separated by only 1 day. Conversely using an interval of DAY 1999-12-31 and 2010-12-31 are separated by 4,018 days while most people would see "11 years" as a better description. Starting from the number of days and calculating months and years from there would be prone to leap year and size-of-month errors. I got to wondering how this could be implemented in the various SQL dialects? Example output includes: 

The two examples are not equivalent and interchangable implementations; they embody different semantics. In the first, the three-way table imposes no conditions on the participants. The example shows this. Alex is enrolled in Biology, Emma is assigned to Alex and Emma knows how to tutor Physics. The tutor's subject is Physics but has a student who's enrolled in Biology and there's nothing in the model to stop them discussing, say, Economics. In the second, the tutor_id can only be associated with a pre-existing student_course_id. So Alex has to be enrolled in Biology before Emma can become his tutor. Since there's only one course_id (through the associated student_course) we can assume this is what they will discuss in their tutorials. However, there is still no way of asserting that Emma is part of Alex's faculty. If you're still working out the logical data model I would suggest you skip the surrogate keys and use only the natural keys for now i.e. drop the various "id" columns and only use course.name, student.name and tutor.name. Surrogate keys are great performance enhancers in real DBMS implementation but are not required when understanding and documenting the problem. They can be substituted in later when you're confident you have solved your problems. Next you need to understand the constraints on the data and the questions the DB has to answer. For example a constraint may be that a tutor can only work in the subject which she is employed. A question may be "who's available to tutor a Chemistry student?" Once you have these, the tables to enforce and answer them will emerge, as will the foreign key constraints. You may be content with any tutor teaching any subject to any student. That's OK if it is so, I can't tell form the question. Or you may choose to enforce this outside of the database - in the application, say, or through a written policy enforced by management. The important thing is to understand the rules as they are, and how they're implemented, so that when they change the appropriate adjustments can be made. Do not be afraid of having lots of tables with a few rows if this is what the data demands. Would you bolt on a new clickstream half way through an existing web site just because you "were going to have it anyway"? 

This will work if your values genuinely are all form shown i.e. two digits, a slash and one digit. If you have other formats the more involved solutions will be needed. 

There is no benefit to including the ORDER BY columns in the SELECT list. On the contrary, having unrequired columns in the SELECT list incurs a fractional overhead in run time and a larger one in maintenance. 

The method must start at the root of your xml. It will then return one "row" per matching node for the clause to work on. It should look like this: 

SQL Server 2016 introduced the STRING_SPLIT() function. It has two parameters - the string to be chopped up and the separator. The output is one row per value returned. For the given example 

Additional Detail A party is anyone or anything who has in interest in a payment. This would be a proprietor, customer or company, distinguished by a type: 

If you wrote the query statically with all the passed values in the WHERE, and added , this will cause the optimiser to do parameter embedding. The redundant predicates will be stripped. Only the needed query operators will be in the plan. You are probably suffering the cost of a statement compilation with each invocation anyway. The statement will be much easier to write correctly and maintain. 

In this post the writer runs a query several times. I notice the logical reads vary a little across executions. There is a difference of about 2 pages in a total of a few thousand pages read. It seems clear to me from the context that there would be no write activity in between times. If the plan had changed I would have expected a larger variation than a fraction of a percent. Q: what factors will cause SQL Server to report different logical read counts for the same query in the absence of data writes? 

It will be very difficult to guess in advance exactly how big each of your virtual drives should be. If you get it wrong you will run out of space for, say, data files while log and TempDB have plenty of unused space on their logical disks. You will not be able to assign this unused space to data files without creating a complicated mess. So I'd suggest one virtual drive and allow the free space to be used by whichever process needs it the most at runtime. 

Having thought it through some more, I think the algoritm is actually O(log N). For each message receipt the algorithm will have to read where that message lies in the current log and UPDATE, INSERT or DELETE as appropriate. Assuming a BTree index exists these are O(log N) operations (even though there may be several operations per read receipt). In the best case scenario read receipts would arrive in exactly primary key sequence (massively unlikely). Then the logging table would only ever have one row. In worst case read receipts would arrive in alternate sequece e.g. all the odd numbered keys followed by all the even numbered keys. This will cause the log table to grow to 50% of the number of posts sent. A rough prototype with read recipts arriving in random sequence saw the logging table grow to a maximum of 25% of the number or posts. So for a sample of one million posts the log table would start with a single row; this would grow to about 250,000 rows; and then shrink to zero rows when the last read receipt was processed. This was consistent across several orders of magnitude. Some day I'll sit and work out the maths of why this should be so, but not today. 

It will be the difference between a system function execution to get the date versus the overhead to pass the explicit value through the call chain. In my opinion, not significant. Bugs could be introduced either way. With the explicit value, a programmer will, eventually, forget to populate the parameter and the SQL will fail at run-time. Making the column NOT NULL hides the error further and requires thorough checking on top of thorough testing. It is difficult to enforce sensible, system-enforced checks on the provided value without invoking other system functions or hard-coding values and making the system brittle With a default, the value used is that at the server at the instant of execution. Time zones can play havoc here, especially if the client and server are in different zones. Writing and immediately reading a value can, in theory, produce any result from twenty six hours in the future or past. This is not intuitive. If the server observes daylight saving values may be duplicated, or be in the future immediately after the autumn step backwards. If the value is business-significant (say, the payment date on an invoice) and the user is supposed to enter an historical date (when the invoice was received), but the application fails to send it (a bug) the server will default the current date. That could have big legal and financial implications for the business. To me these approaches have different semantics. Application-provided values hold meaning from the business and user. System-defaulted values have a context internal to the database, data lineage and maintenance. They do not encode business-meaningful information. Use each as needed. The performance differences are negligible but the confusion could be immense.