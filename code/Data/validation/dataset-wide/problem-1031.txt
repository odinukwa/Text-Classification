Less of a race condition and more of an issue with order of operations and dependencies. The way you stated your question, in steps, is how you want to think of the SSIS package. First it has to do this, then it has to do that. Since the line items are dependent on the latest information from the header, that should always come second. Letting them go simultaneously, you don't really know what happens first. 

This cannot be done without a subselect as you state. There are two steps involved. 1. For each name, find the record with the MIN(qty). 2. Return the ID for that record. There are other approaches but the two steps remain. It appears the query you posted has some missing info. This should be the whole thing. 

Assuming you are looking for the Top 50 distinct combinations of C1-C4, Steve is close. However, having the Top 50 inside of, or before, the DISTINCT may cause fewer than 50 returned results if there are any duplicates found. Instead, you would want to return a full list of all distinct value combinations, then limit it down to only the top 50. When doing things like this, I generally avoid using the DISTINCT command. Instead, use GROUP BY. It will do essentially the same thing and will add more control if you want to expand results later. As for your query, I'm not seeing a closing parenthesis where there should be one so I don't know where the WHERE and ORDER BY clause are actually supposed to be. I would suggest putting them in your inner query to limit the results returned for the TOP 50. 

ItemValues: In Top 12 of indexes with most latch contention. Page latch wait in ms = 45,635, with wait count 247 ItemValues: 2nd place of indexes with most lock promotion count - 953 instances ItemTable 2nd place for index under most row locking pressure, with 405,759 lock wait ms in last 3 weeks ItemValues table #1 for most page io latch contention, with 3188168 count and 810432461 ms in last 3 weeks. ItemValues #3 for most page splits with 68528 leaf allocation count and 42,594 nonleaf allocation count. 

Bigint datatype. These columns are approx 10GB in storage cost alone. They could be reduced to INT and save a lot of space Float takes up additional storage when we might only need to use a 5-6 decimal precision level. I'm thinking to identify our max requirement here and then switch to decimal if possible to reduce size of the float columns. Column14 on ItemTableValues is purely for clustering the related Column15 together in close promoximity. This was done before I got here, since all Column15 rows are "grouped" when queried. Not sure that the cost of having this column purely for clustering records together offsets the cost of maintaining and storing. Refactoring the key and table to provide a narrower table with artificial key and a separate mapping to these keys. 

A related note, not necessarily an answer, but it could be a bit clearer for your code, .Net has a class for SQLConnectionStringBuilder which might come in handy here. Instead of just concatenating your string together, you might want to look into passing the values to that object and use the output connection string property. .Net SQL Connection String Builder 

I've worked at places that never expire service account passwords and other places that do it as a requirement. I personally don't feel it's necessary in most environments because the service account passwords are generally not known to anyone except admins and can be made to be very strong. Also, if you are managing the service accounts security, they should each have limited access to do exactly what it needs and no more. If that is the case, the risk associated with such accounts is very low. At one point, I actually designed an application to handle the security aspects of sharing passwords within the IT organization. With hundreds of environments and potentially thousands of service accounts, managing it all with a spreadsheet was getting out of hand, not to mention the lax security associated with Excel. I'm hoping to add some new features like PW expiration warnings to the app at some point but its not really a priority now. My current employer never expires service account passwords. 

Trying to evaluate architectural changes to improve the performance of an app that calculates a lot of individual values and stores each of these results as a row. The storage size for one of the usages is 80 GB. I'm used to a artificial key driven warehouse, and this utilizes a natural key, so I'm considering various approaches. The table structure is like this. 

Any other thoughts ideas? This has a huge performance impact when dealing with millions of rows being inserted, queried, and then deleted. I'd like to see if I could optimize to reduce the IO thrashing this causes. Some of the indications of possible issues: (dmv's 25 days since last reboot) 

Insert & Delete upon the core AccountCalculations being updated. At this time, it completely deletes and then reinserts the data with no difference comparison between the two. I have a goal to get this difference working, but it requires some big app changes. At one time the insert/delete could range from 8000 rows to 36,000 rows. This is a full insert/delete. (I will be examining a first step of using merge to try and reduce the activity on this) A significant number of reports query these details. All the accounts pulled are typically from the same Column14 grouping, and a child to Column15 (Account). All 8000-3600 rows may end up needing to be loaded by the application for calculations/display. 

I think given the requirements and that at some point you may want additional information that wasn't listed here, I would go with a fully normalized approach. 3 Tables: Guests, Episodes, Episode_Guests Then depending on if you want to do this for more than one show, another table for Shows(or series). As Paparazzi mentioned, the Guests table should contain sex. The Episode table should contain a date. Also, if you are going to do this for multiple shows, the Episode table should also have a foreign key back to the Shows table. The Episode_Guests table should record every instance of a Guest appearing on an Episode so all it would need is a foreign key relationship back to guests and another for Episode. 

100% agree with Marcin Gminski. The SA account is definitely not supposed to be used this way. Windows domain credentials certainly are preferable although not necessary. If you are going to use a single login account for all users, at least create a new login that is not a part of the System Admin role and make sure it has a strong password. As for your performance issue, we would need to know more. First off, what is your method used for data access in the MVC app. Are you using Entity Framework, Linq or something else? Next, have you debugged the app to see if it is making excessive, unnecessary calls to the database. Next, have you attempted to run a SQL Profiler trace to capture the activity on the server? Those steps should at least provide a little more insight in to where the problem exists.