The others are combinations of the documented flags, e.g. 24 is 16 and 8. This is a method for simulating optional parameters and is used in e.g. C iirc, the numbers and so on are structured like that because they correspond to binary values that combined in any unique way create a unique number. The function that accepts them then use bitmasking to extract each individual number. E.g. if you send the value 6 to a function, then we know that this is a combination of 4 and 2, to find this out using a bit mask we would do: 

and after that the call from the web server also completed in time just fine. I would suspect that the same plan should be used since I was using the exact same parameters from both connections, but I'm not sure. My question is: Can there be different plans or buffers for different connections? Or could it have been some other side effect caused by me running the above dbcc-commands? 

In SQL Server there is a separate thread that periodically (default 5 seconds, lower interval if a deadlock has just been detected) checks a list of waits for any cycles. I.e. it identifies the resource a thread is waiting for, then it finds the owner of that resource and recursively finds which resource that thread is in turn waiting for, thereby identifying threads that are waiting for each others resources. If a deadlock is found then a victim is chosen to be killed using this algorithm: 

Sometimes when inserting data in a table with many columns it could be useful to know which columns must be specified if the insert-statement shouldn't fail. I wrote this query to find out which columns are not nullable, identity, computed, timestamp and have no default value. 

I usually use mysqlcheck with --optimize and --databases combination. mysqlcheck is a command-line interface for administrators to check, optimize and repair tables. 

3. ALTER TABLE (ONLINE) The problem of and that it locks table during operation. You can use the Percona tool : pt-online-schema-change (from Percona Toolkit : link ). pt-online-schema... will construct a mecanism with triggers and temp table that you permit original table to be available for read and writes during the operation. I use this tool in production for the big it's pretty cool. Note that you should have referenced to your table, FK and triggers risk to produce a mess. To check this prereqs, query: 

WARNING NOTICE If you enable the audit_plugin (it's right with all logging processes like SLOW LOW, GENERAL LOG...) be careful to not saturate your disks in term of I/O. Depends of your workload, prefer write these types of logs on dedicated disks to minimize the impact on your MySQL instances. You can also throw them to syslog but you should have the architecture to handle them (ELK for instance). Max. 

Note regarding log-slave-updates: by default if you enable Binary Logs on Slave, the Slave will only writes events executed on the Slave directly, none of the events coming from its Master will be written in the Slave's Binary Logs. If you want to setup a chained replication (M -> S/M -> S), you need to tell the Slave to logs the Master events on its Binary Logs to replicate them on its own Slaves. This options is log-slave-updates. If you need to enable Binary Logs on Slave the command to see the curent position of the SlaveÂ´s Binary Logs is you will see the position coresponding to your files on your directory (on slave). Note on Binary Logs managment: Do not forget to set a "purge strategy" for your Binary Logs if you don't want to saturate your disks. The simplest way is to use the expire_logs_days variable which tell to MySQL to purge its Binary Logs older than this variable. I hope I was clear... Best Regards 

This topic is very interesting for me now, so because it Up, some last information: Replication is good, but what if You need replicate data to different technology Target System (for example Redshift) I found and test few implementations for change data capture: 1) MariaDB MaxScale CDC - my personal test was unsuccessful (and builded from source and binary distribution return link for missed plugin). Ask a questions, wait for answer. 2) StreamSets - simulate slave, ship changes to Kafka MQ. Tested on real loading, work perfect 3) Yelp 4) Maxwell Daemon - under test now, example with NiFi - NiFI examples 5) Python Library - also simulate a slave for remote server 6) Lapidus - NodeJS implementations Few other links also founded, some under active maintenance, some look like forgotten. From the tests, StreamSets look very good, Maxwell also very interesting MaxScale - sure it work, but may be available only under subscription support. From "other" side - Talend as primary data integration tools, Redshift, memSQL. 

unfortunately it is impossible, maximum possible restore structure InnoDB data stored in *.ibd files if set innodb_file_per_table=1 or in idbata1 if it 0. for success repair even when innodb_file_per_table = 1, ibdata1 need as well. if You have only from file = no chances restore data at all 

the logic of LEFT JOIN - take all rows from LEFT table because You have not any WHERE conditions - MySQL do not found any reason to use index for tb1 - with index or without index it still will read all rows. so, general answer for Your question - it will not increase speed, and possible will be slower (depending from data size), when sort operations will require use of temporary tables. You can make a few tests - for example change LEFT JOIN to INNER, it not correct for logic test, but it will show You different plan. Also You can add WHERE condition for tb1 and see - what happens. 

The original log file might be truncated so the space can be reused, but it's size might stay the same. 

Both statements would be evaluated to , thereby we have found out the individual values. Note: In the real world, we would iterate through all values from 1, 2, 4, 8, 16 and so on to the maximum value of an integer (or whatever datatype the parameter was set to). 

Identify threads that are not unkillable (e.g. a thread that is rolling back a transaction is unkillable). Find the thread with the lowest deadlock priority. Chose the one that is cheapest to roll back, i.e. the one that has done the least work so far. 

We had an issue in our dev environment where a procedure call timed out from the web server after 30 seconds. I traced the query and ran it manually (same params and all) from SSMS and it executed in about 2 seconds. I then ran 

I'm using Red Gate SQL Compare to create a release script based on differences between SVN and a database. This results in a script containing a bunch of table- and procedure-changes and it works fine. However, one thing puzzles me, it's using transaction isolation level serializable. I know what it does to dml-statements, but I'm not sure what it means for ddl. Can someone enlighten me, perhaps with an example? 

Make sure to keep transactions as short as possible. E.g. don't show a login form after starting a transaction and wait for user input, instead gather all the info you need and then run the transaction. Use the lowest possible isolation level, e.g. don't set serializable when you just want to temporarily show some values to the user. Please note that setting correct isolation level is a science in itself and out of scope in this answer. If you are the victim of a deadlock, i.e. you get error# 1205, then re-run your transaction transparently to your user. Since the other, competing, transaction has now hopefully acquired the resources it was waiting for and finished, it is unlikey that you will encounter the same deadlock again. 

In your mysqldump, did you put the mysql schema in it? The mysql schema contains the information_schema datas so when the mysql is restored, the past infos are loaded... understand? Max. 

If you are sure that your MySQL is a standalone instance, there is no know issues with the statement and I see that your binary log is 1.5Mb so i'm sure your system can manage that purge without pain. In a Master/Slave infrastructure, you can purge binary logs before they been "played" on slaves, the result is the slaves replication goes down (with impact on the apps). Max. 

Last note, we force the auto_increment value to 300 in our transaction so the 300 is no more available, the next one will be 301 so don't forget to force it with your_value - 1. Max. 

You'r lucky you have only one secondary index, the is a clustered index so it isn't included in the index statistics. You can find the answer with the field of the information_schema.tables view (result in bytes): 

For DDL (ALTER TABLE): I stop the replication, run my DDL commands, run a rollbacked script and restart replication: 

Note that is a cumulativ counter the first line shows the nominal counter value and at each iteration of mysqladmin, a substraction is made to show you how there were insertions during the time you setted (here is each 1 second). By the way, if you want some infos for your error (1205) it's the InnoDB Engine that should check because it's the only way to see which of your query (or queries) is/are stucking your DB. You can find infos on current and past transactions that lock your DB with: 

Now, if you don't use the Binary Logs on Slaves (for point in time recovery or chained replication) you can disable it and remove them: 

Native replication not help, and in this case better to leave both database on same server and just install trigger for INSER/UPDATE/DELETE on Master database, which will check conditions and make direct changes on second database. This is simplest way. There are many other possible. 

next could be (with steps from above): create not the View, but MySQL analog of "materialised view" - temp table with indexes over rid and year. Pivoting could be solution, but again it could be not fasted way 

Do not trust (rely only on it) - for mysqltunner, all settings must be adjusted based on real monitoring and loading. Log File size from one side - it recommended size 0.5-1Hr of all transactions during this period but from other side - if it more than 1-2Gb restart after crash could take ages. As bigger logs - as longer start. So it always balance between. Start from 512M per file (Total 1G), then if loading high - increase up to 1024Gb better to check what inside: 

Stop MySQL Server and backup (just copy all files to some location) - folder c:/ProgramData/MySQL/data Make a full dump of all databases mysqldump -u root -p --all-databases > c:/backups/all_db.sql 

I use ETL Job (Talend) which open flow from source table and write to destination. From the MySQL point of view - it is just select to external client Live usage example - on monthly basis, we are prepare data-set for past moth, the reason - by adding few columns on the fly we increase speed of calculation more than hundred times. The source table under high regular INSERT/UPDATE loading but for current period only. "manual" alternative - store result of query into local (on client machine or S3) csv file, than bulk insert from csv. ETL just automate this process and exclude intermediate csv file. Many developer tools like MySQL Workbench, Navicat, DBVisualizer (any serious tools) help You to store result of query to local file with different formats. The benefits of this way: