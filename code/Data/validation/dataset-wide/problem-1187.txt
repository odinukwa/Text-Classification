You should add an indexes for each table having in the of . Columns used for index should be the same used for s. In case of table you have to add an index for field while corresponding tables and should be indexed on the fields each. The common rule is: When you join two tables with condition, each table should have indexed columns used for . 

Here we can see that there is no penalties for using s with proper index. Here is the suggestion: lat-lon coordinates have the anomaly near to the prime meridian. Every rectangle dissected by it should be splitted into two different rectangles to the left and to the right of PM and checked separately. 

The problem is that values of the UDVs is restricted to the statement an do not passed across different s and s. If you want to refer to the values calculated in the one from another you have to use values stored in the resulting table from the previous statement. Try the next form of the second operand of the : 

You should not use the same table for entities of the different nature. In the RDBMS paradygm table represents the type or class. For each specific type/class you have, you should use the separate table. Sure you can mangle the table by bells and whistles like . Sure you can cheat and define your entities in common as "something that have an address(es)". But the direct and inevitable result of such approach is the complexity and non-obviousness of the queries on that tables. Each time you will be forced to check what exact "subtype" of the entities you deal with. More conditions in the s, more complex s and their 's and so forth. Indeed, there are lot of cases when we are forced to denormalize our databases or to break the ACIDity for sake of performance. But if your primary goal is the clear and comprehensible design - you have to avoid violations of the basic principles. Pretty good explanation you can find in the foundational book "An Introduction to Database Systems" by C.J.Date, part II chapter 5. As mentioned by Gypsy Spellweaver, you need an auxilliary "pure relationship tables": 

Insertion/deletion ratio is set to 1:3 just to ensure I get the reasonable removal rate even when incoming data rate become low due to daily/weekly/monthly oscillations. It is acceptable for established bases with low expired records count. If you want to perform initial cleanup, then you have to set the to the value that do not insult your server performance. If you have low incoming data rate then you can create the special routine ad hoc: 

The other advice is to simplify your calculations as possible. Do not calculate again and again those things you have calculated already, like . My final code now is such simple: 

It is amazing but your query returns the wrong result. of the returned row isn't associated with the row containing the minimal . But you can get the desired result such way: 

When you run some INSERT or UPDATE query on the trigger will replace submitted value of the by string stored in the variable. Sure you can do as complex and randomized transformation as you want. 

Here I suppose your table have some autoincremented column . If no you have to add it and fill with 1..n INTs. If there is no plain dupes then column is not necessary and query can be simplified like that: 

You have to filter out what you need from the whole product. That can be done in two ways. You can use or . There is significant difference between this approaches. generates the full product and then filter out only those rows that match conditions. create the product that initially contain only pairs of rows that meet requirements. 

It is obvious because with left joins you'll get alot of rows where and will be NULL and clause have no sense but make heavy load. 

Each row in the 's output having in the 'key' column should be investigated. Those row marked as aren't the culprits but rather the victims. In your case table need the indexes: , , and . I can't predict which one of the last two will be choosed by optimizer. Then derived tables produced by s will get the suitable inherited indexes for further proceeding. 

Accidentally I have the DB with table that contain 50+ million of NMEA rows in it. The table have the complex index (timestamp, lat, lon). Simple query has been launched: 

You have to test which index get better result. Keep in mind that indices are expensive so create only those indices you really need. The other suggestion is to precalculate all values that are constant within the query. In fact will be recalculated as many times as many records have . If you precalculate the value into the variable, then calculation will be performed only once: 

Some of fields are already indexed as primary keys so do not duplicate indices, that can impact RAM consumption as well as performance. First you have to do now - is to launch your query with prefix and show the result. N.B. Always use aliases for joined tables, like third one in your code. Always specify explicitly table you refer to, like or . 

There is one entity - , that can be in a various states. Completed, scheduled, postponed, rolled back etc. State is the property of the entity, only part of the whole, like the field is a part of the row. When the state is just a field of the table it is possible to create DB structure and code that are invariant regardless of how many states are possible. In the opposite case each new state produce the new table and lot of code should be rewritten to reflect the changes. 

Docker containers are intended to isolate environments used by different users and/or versions of software to minimize spends during the development time. Production server shouldn't be the home for everyone and shouldn't have different versions of software "just for testing". Therefore there is no reasons to put services into containers unless you are not a VPS-hoster. 

First you have to change the type of from text to the INT. Numeric search way more faster than string search because of no necessity to apply collations to the multibyte characters. Then you have to create complex index that include all the fields you need to distinguish record(s) from the set. Here the trick - the order you list the fields in the index definition is very important and depends on queries you want to speed up. For query 

You have to choose one of the terminal stations as and store the distances from to the each station on the route: 

Ordering should be performed with step greater than one. I prefer the step equal to 10 for better visual representation. Main idea that new item always can be inserted with SO=N+1 that is normally unused. Say, you have inserted something in between 30 and 40, then your sequence now look like that: 10-20-30-31-40-50. Then you have to the table that way: 

Indexes are not the part of relational math at all. They are part of RDBMS implementations only to speed up data proceeding. Keys in opposite are r/math constraints that are applied on the sets that should conform some restrictions like uniqness or existence. In practice primary key is the table-wide always-existent unique identifier, sometimes implicit and automatic. Primary index can be built over the primary key but that is not necessary. Moreover, table can have more than one key suitable to be primary key at the same time. And that alternatives can become ad hoc. 

This three queries can be combined into the equivalent single one with complex condition with lot of s, s and braces. But for better comprehension of principle splitted queries are preferrable. UPDATE My strong advice is to replace by even on the testing base until result will be acceptable. 

User-defined variables can't be used as identifiers in the function or procedure definitions. Use plain variables and initialize UDV like that: 

There is no performance difference between single base and number of bases, because "schema" is just the domain of authority. Grouped together or aparted, tables stored and processed in the same way. From the administrative point of view multiple bases are bit preferrable as far as you can dump the whole data for single client by DB name, not the pattern for the mangled tablenames. Sure your experience may vary. 

There are three tricks here. First is that (UDVs) are persistent through the rows proceeding. Once assigned they will store the value until reassigned. Second is that in context of UDVs there is difference between comparison and assignment that have different precedence. Here 

You have to disable general logging and enable it again to recreate the log file. When you enable logging existing file is used or new one is created and its inode is used as filedescriptor for writing. If you delete already opened file filedescriptor is pointing to the nonexisting file. You should close file via and reopen it again via 

If you want to delete duplicates you have to join the table with itself. First, let's delete those duplicates where at least one of them have proper CategoryID: 

The result of is that field is filled by evenly distributed numbers with step of 10: 10-20-30-40-50-60. Now 31 becomes 40, 40 becomes 50 and so forth. Indeed it is better to incapsulate all the insertion/renumeration into the for sake of data consistency. Two simultaneous with the same =N+1 can drive to problems. Also an index is recommended for field. Later when you fetch the data from the table you have to divide values by 10 to get the plain sequence of the natural numbers: 

If you want to get the latest records from for each then this can't be performed by queries only in an easy way (at least within MySQL dialect). The simplest way is to create the copy of the table but with UNIQUE index on the and syntax. Each record written into the should be written into too. IODKU ensure that old data will be overwritten by new ones and when you you'll get all and only latest log lines for each . 

Here I suppose that , and columns contains the unique values therefore proposed indexes have that specific orders of columns. But for other cases the different indexes like can be more efficient. The actual order of columns in the index depend on the size of table(s), type of columns and the ratio value. Also for best efficiency you have to avoid literal values and replace them by IDs from the corresponding reference tables, if possible. 

To test whether the query selects only desired rows you can replace by and check the resulting set of rows. 

If you can't for some reason update master records then you can do the same by joining table with itself: 

InnoDB engine autocreate the internal PK if it isn't declared explicitly. That PK is autoincremented integer anyway so there is no performance difference between table with or without explicit field. Single-column index is faster then multicolumn ones and INT-based index is faster than other types. The only difference is that you can refer to the explicit field and can't refer to the hidden field created by engine. 

Normalization is not always necessary because theoretical sets are relatively poorly emulated by RDBMSes. There are always some limitations, overheads, slowdowns and bottlenecks that can be avoided by some denormalizations. Sure for educational purposes it is preferrable to represent each data type by separate table. When your scheme have types and then they have to be declared as two different tables. But in the real production projects sometimes we have to denormalize for sake of performance for example. Unfortunately, each specific case need some specific approach that can't be advised without some analysis. For example table can represent the type "timestamped sensors readouts" like that: 

Primary keys are always unique thus you can't create the pair seller-customer that already exists. Foreign keys with restrictions prevents creation of pair with nonexistent seller/customer/both. 

means: IF previously assigned value of is equal to the freshly assigned value of THEN ... Third is that rows are ordered in the ASC-DESC order. If any row with the same have assigned, it will be proceeded first and propagate its value to the all consequent rows. If you have assigned two different values to the any two rows with the same by mistake the alphabetically bigger value wiil be propagated across the group. 

Reorder fields in the index definition placing more selective fields first. To determine selectivity run the next query: 

stands for one-dimensional distance while is standing for two-dimensional one. Let's the data is the timestamp. It is one-dimensional and you can define part of data "in range" or . If your data represents the plane of points x:y then you can define part of them as "points closer than radius Z to the given point" or . 

appears when column(s) used for grouping does not have an appropriate index. As mentioned above, results returned by are ordered by the same column(s). If you get for sorting you also have filesort for grouping. That insult performance in the same way. Therefore you have to create the index, not to suppress the sorting. 

For each table involved you need an index that mentioned all the used columns in the clause and in the as well: