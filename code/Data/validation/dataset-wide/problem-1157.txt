the code for pivoting would read the first 40 lines into an array then just set the output columns mapping to the array's index. consideration may need to be given to insure the key remains unique on subsequent runs and may need initialising to the max(key) + 1 from exiting data, use an sql task for this. 

I am following along with this video on SQL Server query optimization and I would like to check my understanding of the results I am seeing. I am using SQL Server 2012 with the AdventureWorks 2012 database. Testing the first three examples Example one: 

A problem could be you are trying to store the measurements in a single table that you are 'navigating' to via a suitability table. It makes sense but you may want to consider separate tables 

But this seems more simple then managing hierarchies. It could also be redundant if you chose to use s rather than an 'entity id' table. n.b. I have used the term 'entity' as what you have refered to as an 'object' 

Unlike the video, there is no difference between the number of logical reads between any of the queries as is. But, after playing around with the criteria and comparing results my reasoning for not seeing a difference comes down to 

from your form, it looks like a measurement may have multiple ranges for the same suitability? you may need to add a sequence like attribute or further normalise if this is the case. 

The first query in this example utilizes a seek but the second query in this has to scan and thus read more (all?) of the index. Questions: 

Is my understanding and explanation of what is happening correct? Are there any occasions when an indexed column can be involved in expression and still have its index 'seeked'? Does not having a clustered index on a table have any effect on the ability to use a seek? 

If the user just enters the busTime etc in minutes you can use an int, if you wanted to let the user enter in you can probably convert that to (int) mins, you are rather vague on how the user will interface with the database. To me this also makes more sense when reading the data i.e. rather then and makes it a tiny bit easier to query and aggregate. 

I believe this could be done more simply by using globally unique identities for all entities. Avoiding any need to manage heirarchies while maintaining referential integrity between authorisation and entities. Create a table for globally unique id's of all entities 

If you're using must_change or password policies you'll have an event raised or a message returned about this and can do it programmatically. Ex: $URL$ 

There is no delaying. In Availability Groups we don't ship individual transactions. We also don't wait for them to commit. The unit of transport is a LOG BLOCK which is a collection of LOG RECORDS, in order. These log blocks hold log records from many different transactions in the database and not just the ETL ones you have going on. Additionally, the log blocks are sent when they are closed and flushed to disk, which happens for a few different reasons and does not necessarily need to be from a commit. This means the secondary receives information on all transactions that are going on in the database that are making changes. In this case, the secondary has some portion, if not all, of the information about the earlier transactions that are still open. Now, this hits on another point. Readable secondary servers aren't using isolation level, in fact it is mapped to snapshot isolation under the covers. This is the reason you cannot see any information on the secondary pertaining to the earlier 3 transactions. You cannot add to the query, it won't let you see the data. Since snapshot isolation is used, we're going to use the version store. This means if you started a transaction on the secondary to read information and it is still running when the open transactions on the primary are committed, it will not "see" them until the transaction is over. This is part of the isolation of snapshot. To summarize: 

So you're upgrading versions but REMOVING high availability and disaster recovery? Clusterless AGs are called "Read-Scale" AGs and do not give high availability and you can argue on the disaster recovery part... 

A COPY_ONLY full backup is still a full backup. The only difference between the two is the COPY_ONLY will not reset certain tracking bitmaps. This means your differential backups will eventually be the size of your database. $URL$ 

Don't use sp_msforeachdb. If you're dead set on using it, though (given the many known drawbacks and issues) then you'll need to set the database context before attempting to run checkdb without a database name, or use the database name parameter. Assume: The output of this will be 'master' however many times as the number of databases that reside on the instance. Simple put, the stored procedure gives you the database name and will run it (hopefully for each database, though known to skip) but not set the context. Output: master, master, master, master,... Assume: The output of this will be the names of the actual databases (again, hopefully it didn't skip any) on the instance. If you'd like to run CheckDB this way, you could use: 

This might not be the answer but a general tip for designing is to remember that for your initial logical design. A logical relational DB schema is only made up of one-to-many relations. e.g. One Park has many Photos and One User has many photos. That allows for a many-to-many such as; A user may log photos for many parks and a park may have photos logged by many users. These are called business rules. You want to always go for just one-to-many relations in your model of the business rules. Later you might add a few one-to-one entities but rarely. As for many-to-many , just exclude them from your model and represent them as one-to-many-to-one ( i.e. as two one-to-many relations). Which you have done. Your schema design looks fairly close to reasonable but you have included some one-to-one relations. They are normally not part of a new schema design. They are basically extension tables/entities and done as a way to optimise performance in cases where some fields are rarely used. For now, I would exclude modelling them. But actually in your particular model, I think you just made a mistake with the notation. So you probably want to keep those entities but correct the relation types. e.g. Park-to-Favorite Park should be one-to-many ( 1:M ) . The Park Type to Park should be either 1:M or M:M ( via two 1:M ) , depending on your business rule. Can a Park be of more than one type; I expect a ParkType can be used by several Parks. If so then the relation is 1:M ( ParkType:Park ). If a Park can also be of several types then you'll need another entity and create the relation 1:M:1 via ParkType:ParkTypePark:Park . Now, location is tricky. Example data would help, as you list Address, long, lat as fields I assume that a location record is only usable by exactly one Park. If location is like a region where many Parks can be associated with one Regional Location then the entity makes sense to me (as a 1 Location: M Parks relation ) . But if the Location is very precise so only one Park can ever be there, then you don't need the Location entity, store that data in the Park entity. However, for optimisation you may put it back in if you think that searching for Parks in a Location area would be faster using a separate table with only that in it. But I doubt you need to do that. Not sure what ParkSubmission is, I'm guessing it is suggestions for new parks. If so, possibly you don't need it and can instead have a status flag on the Park record to show that it is a Suggested, Draft, Approved, Rejected, etc.