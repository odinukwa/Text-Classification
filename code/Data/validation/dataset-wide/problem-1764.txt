First, I suggest you verify that the application behavior matches the documentation: type in an elevated command-line and you'll get a list of open ports with the associated executable (if possible). Once you have that, make sure your rules matches the actual listening pattern of your application. An alternative would be to open the ports not based on port number but based on the executable itself. That might make it easier to manage. 

You can always create a new zone in your DNS and enter the DNS entries there but, honestly, it makes a lot more sense to simply connect the DNS servers. Ask them to allow you DNS to forward the request for their domain to their DNS servers: that way, they don't even have to open traffic to your whole network, they only need to allow DNS from your own servers. 

I have a rather unexpected situation here. I have a small application that connects to a WebDAV server (Sharepoint) using the standard WebDAV windows client (WebClient service, part of the "desktop experience" feature), walk through a number of folder and download all PDF files it finds. The overall process works. However, I'm experiencing a strange issue: when I run the application directly from the server's console or RDP desktop, it runs in about 10 seconds (assuming there is no file to download). When I run the same task from the same user but from a scheduled task, then it takes 5 minutes to complete and yield exactly the same result (files are downloaded if present). I've added debug to the application to see where the latency happens and the result of this is that the operation of listing the file directory seems to take forever, even if the file filter used causes no file to be returned. The same behavior can be observed when using ProcMon: the latency is within the Win32 calls and ourtside of the application alltogether. Now, this isn't really critical since the process works, but the more directories the are added the slower it gets and there will be a point where I will have to decrease the frequency at which the script is run so it can actually complete (or change the application so it start to process the directory in parallel). Would anyone have some kind of explanation why the WebDAV client seems to be several orders of magnitude slower when running from a scheduled task ? In case it matters, the connection is made with an explicit password each time and it goes over SSL. 

Technically, you could write an update script that copies the new data from the production to the test system but not only is that going to be complex, you're also going to run into problems that are far from trivial to solve: 

It all depends on the nature of your application, really. If all you're supplying is an HTTPS-based interface (SOAP, JSON or web app) and you do not use certificate-based authentication, then you can use some form of reverse proxy system. In that case, the proxy would have a public interface that uses the public (your customer's) URL and forward requests to your internal system. It can still go through your load balancer as a client. 

PHP run with the same permissions as the user that you assigned to your web site. Which user exactly is taken into account depends on many things. Here is a short list that should allow you to identify the account in question: 

Another option is to setup a source control repository somewhere and have the server automatically pull new revisions in is production tree. Mercurial seems well fitted for that. For the security side of things, you can setup HTTPS as transport and require logon for accessing the source control. 

You need to combine @MDarra and @LarsWA answers to make it work: setup both sites to use host headers in your binding as well as port 80. basically, you'll have the following bindings: 

Not in a generic manner. You can restrict the access to the database server to trusted users but trying to achieve perfect (or even descent) security using code obfuscation is a pointless waste of time and resources in most (if not all) cases. Edit: there is one thing that you can do to make it a tinny little bit harder to see exactly what is happening in you code is to write it as a CLR stored procedure. It's not going to slow someone competent very much but it'll make it harder to follow what's going on just using the SQL debugger. 

As it turns out, we have useful information stored in the "description" attribute of the source OUs. Unfortunately, it doesn't seem that this information is part of the data exported by the Get-ADOrganizationalUnit commandlet. So, could anyone suggest a way in which I could get the same information from the source AD but including the Description ? In truth, I can work very well if the only attributes I get back are , and so if you have another way to list all OUs under a specific OU in AD that includes these attributes, (and can be piped to ), it'll work as well. 

Yes, you can perform this, but it isn't exactly trivial. First, you need to establish a certificate authority that trusted by the client. Once oyu have done that, you can use the sslbump feature of Squid to perform the decryption (see $URL$ While you do not explain the context of what you want to do, it is probably worthwhile mentioning that if what you want to do is debug outgoing HTTPS connections coming from a windows machine, you can perform this in a much simpler way by using the Fiddler web debugger ($URL$ 

KB 2823324 has a bug that causes some win7 machines to BSOD and others to launch chkdsk at every reboot. It seems to be more specifically linked linked to Kaspersky AV (although I've heard of issues with machines running NOD32) See this from Kaspersky: $URL$ 

No. FTP doesn't have the equivalent of HTTP's "host" header. If you want the same FTP server to respond differently when the user connects using different DNS domain names, you will need to assign each host name its own IP 

I wouldn't do it that way: it will be difficult to assert whether your reverse proxy is actually working or whether you're hitting the final interface directly. You'll need to check the content of the data over the wire to be sure. Why not setup a couple of VMs for this instead ? 

Shared hosting means your web site will be located on the same machine as other customer's. It has repercussions with performances (you'0re sharing them with other apps) and security (a flaw in someone else web server can affect your own). "SSL included" probably refers to having a SSL certificate issued to your host name by your hosting provider and installed in your (virtual) web root. That will secure the communication between your clients and your server. So the two aren't directly linked and being on shared hosting shouldn't have any impact on your decision to purchase your own certificate or use the included one. Personally, I wouldn't use shared hosting for a commercial web site, however. 

The proper practice is to use your local domain as sender, possibly using a non-existing address as user name. 

It looks like the client certificate you receive does not have the expected properties. Specifically, it looks like it's subject canonical name field isn't matching the expected "proxy.salesforce.com" In your situation, I would setup a tcpdump on the external interface of your reverse proxy waiting for a connection from 96.43.148.8. I'd then feed the result of that trace into wireshark so that it would parse the SSL handshake and allow you to grab the subject.cn of the certificate used for SSL client authentication. That should give you a good indication of what is failing. 

Under "Ethernet adapter Local Area Connection" you should have an IPv4 address listed (and not an auto-discovered one like 169.254.xxx.xxx) If that works, the start nslookup (still from the command line and type 

(or sometimes straight ) Which tells the mail client that there is multiple parts in the message. At this stage, each part must indicate what type it is supposed to be in (in addition to several other parameters). All that to say that, if you mail application sends the message as plain text and you want to convert that as HTML, you will have to, at the very least, change the message's content-type header in addition to change the message body itself.