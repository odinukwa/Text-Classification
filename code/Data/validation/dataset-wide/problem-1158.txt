the database would have to scan the entire table (or the entire index) and evaluate the expression against the full value. Obviously, that's very expensive. Most of the better relational databases have facilities to do full-text search in a more efficient manner by constructing different sorts of indexes and text catalogs but these don't use the LIKE keyword. For example, here's a nice article that discusses full-text search in PostgreSQL. 

Unless you have some reason to believe that your query should require more than 4 GB of space, something that would be very unusual unless you are doing something like building aggregates from very very large tables, it sounds like your query has a bug. If I had a query that contained "lots of joins" and consistently exhausted the tablespace, I would wager that the query was missing one or more join conditions which is causing Oracle to do one or more Cartesian joins (meaning that N rows in one table and M rows in another table becomes N*M rows in the result set). With even moderately large tables, that causes a massive amount of temporary space to be required. If you look at the query plan, I would wager that you would see one or more operations. If you can identify the tables that are Cartesian joined, that should help you pinpoint which join condition(s) are missing. If you are using the old join syntax where all the join conditions appear in the clause 

The larger the on the sequence, the fewer times that you'll need to wait for Oracle to acquire the latch to be able to update the data dictionary when a new set of values need to be generated. There is definitely at least the potential for some performance benefit by increasing the size of the cache. On the other hand, if the is already at 10,000, you're only going to need to update the data dictionary 20,000 times in the course of a 200 million row data load (assuming one call to the sequence.nextval per row). Without seeing your system, it seems unlikely that these 20,000 updates would account for a meaningful fraction of the time you're spending in a 200 million row data load. You would potentially shave a couple of seconds off the load by increasing the size of the cache but that probably isn't meaningful given the amount of time you're likely spending on I/O. 

Alternately, you can create a synonym (public or private) that provides the mapping. As Alice, for example, you can create a private synonym 

Possible? Probably. Realistic? Probably not depending on what "without affecting the way my application works" means to you. As with anything else, if you build a piece of software on shaky foundations, it's likely that if you decide that you need to re-engineer some component down the road, it will be relatively painful and will likely involve changes to other places in your application. If you build your application with a language or framework that needs to be changed out in the future, you'll likely need to make changes to the way the application works. If you build an application that doesn't scale, going in to improve performance later will be more difficult will likely involve substantial changes throughout the code base. If you build an application on a database that isn't properly designed, re-designing it, converting all the data, and propagating the changes to the application will be time consuming. That doesn't, of course, mean that you shouldn't build anything until you're an expert in everything. A large part of becoming an expert is making mistakes and learning from them. If your site gets popular enough and you learn enough about databases to conclude that your initial data model was flawed and that it is worth reworking the entire data model, that probably implies that you've been quite successful and that having to work on a "version 2 architecture" is a good problem to have. On the other hand, it's very likely worth picking up a book on basic data modeling reading up a bit on normalization as you design your data model to hopefully avoid the most easily avoided pitfalls. 

I agree with everything gbn said. The critical bit in his comment, is "if it wasn't critical for performance". For the vast majority of OLTP operations, it really doesn't matter how many levels of nesting you have (assuming that your data model and procedural layer is designed intelligently so that you've got a normalized data model and that you're not adding layers of calls for no good reason) because it really doesn't matter if it takes an extra couple hundredths of a second to insert a new row and all the associated and rows. The ability to reuse the logic later on more than makes up for the extra overhead of calling a few more stored procedures. When you are talking about data warehousing, however, where you need to quickly process millions of rows in a relatively tight load window, procedural logic can become problematic. A hundredth of a second per row, multiplied by 10 million rows, for example, is 100,000 seconds= 1667 minutes= 27.8 hours. In that sort of environment, performance is much more critical and you're far more likely to choose a set-based approach rather than calling a lot of smaller, more modular procedures. 

If you don't want a user to create a database link, don't grant them the or privileges. My guess is that you've granted the developers a role that you didn't realize had that privilege-- the role in at least some versions of Oracle includes . I think that's been removed in more recent versions but I'd need to validate that. This is one of the many reasons that using the predefined roles and is highly discouraged in a production environment-- the privileges associated with them are generally much more expansive than you want (or than the name implies) and they tend to change over time. You're much better off creating your own roles, granting those roles whatever privileges your users actually need, and granting those custom roles to your users. As for the underlying issue of user queries bogging down a data warehouse, I would tend to expect that you want to use Resource Manager on the data warehouse to limit the resources that lower-priority users get when there is contention for resources. You might be able to get away with a lighter weight solution based on profiles as well to put a hard cap on the amount of resources any single query can consume before it is killed. Resource Manager, though, is a much more robust set of tools to prioritize requests. 

There is no way around it in Oracle 9i. If you were using 11g, you could define the table to be interval partitioned in order to avoid having to create a new partition every day. But that's not going to help you on an ancient version of Oracle. 

SQL Developer the client application is free of charge. Of course, the Oracle database would need to be licensed in a way that allowed you to connect with any tool. PL/SQL Developer is a completely different tool produced by a different company that you would need to license. Define "secure". Assuming a default database install, using any tool to connect to the database is secure in the sense that, say, the password isn't sent in clear text over the network. But all the data is, by default, sent in clear text unless you configure Advanced Security. But there are many, many ways that a database could be relatively secure or relatively insecure because there are many, many ways to set up security and many types of things that people may want to prevent.