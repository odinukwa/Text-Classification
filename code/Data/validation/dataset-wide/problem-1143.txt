Configure MySQL Workbench to not require SSL encryption If you get the MySQL Workbench error of "SSL connection error: SSL is required but the server doesn't support it" then you likely just need to change a setting that's defined within the MySQL Connections within MySQL Workbench. 

Members of this role in the database you create it on should only have access to the applicable metadata as long as they don't have other permissions to database objects such as explicit SELECT access to the a table or a member of the db_datareader fixed database role. 

Per the below site and quoted text it does state minimal logging is more effecient than full logging, but it does NOT state that NO logging occurs at all in recovery mode. 

I'm not suggesting making a bunch of changes before trying to find and fix the specific problem beforehand but there are many factors to consider I would think and it's just something you have to troubleshoot one step at a time unless there's something obvious causing the issue. In my setup, I'm the domain admin as well as the DBA so I have access to everything and all servers (two domains with two-way trusts) to troubleshoot at all levels whereas you may be a bit more restricted with what you can review, etc. on each server in the loop of the issue. I usually start with the simple items first that don't require changes and see if there are any obvious issues (e.g. event viewers on all servers, DNS testing on all servers, review all server configurations, etc.). This could be an issue at the OS level of the SQL Server for the Windows version it is on. This could also be an issue with the NIC of the SQL Server or a mis-configuration of TCP/IP or DNS settings on the SQL Server or any of your DCs. Gather information on or ensure the SQL Server is fully updated with Windows Updates, and ensure server firmware is up-to-date if applicable with BIOS version as well as any hardware device firmware updates. The domain admins could also do the same on the DCs but use caution with making a bunch of changes and do so one step at a time. I'm not sure how your AD and DCs are configured topology wise, but best practices should be followed as best as possible and a DC should be close to the network where both domains are physically rather than going across a slower network pipe or routers, etc. which adds another level where there could be issues or configurations to review. The DCs should have their TCP/IP settings configured so that their DNS settings point to other DNS servers or follow best practices otherwise as per Microsoft for the DCs configurations applicable in your environment. You'll need to do basic troubleshooting to find the issues to resolve, ask questions, see what all could be updated and patched or changed, and get your domain admins to help you troubleshoot or disclose domain configurations to you if you're not sure to see if everything is configured as best it could be for optimum performance and per best practices. That's not to say there could be some very simple reason this is happening suddenly but if nothing has changed anywhere (i.e. OS on DCs or SQL Server, network configurations, AD or forest functional levels, hardware upgrades, Windows Updates, etc.) then you just have to troubleshoot for the reason why one step at a time. For your #2 question I think the OS has an issue talking with the DC (for whatever reason(s) you determine) and when this happens, it just gets out of sync somehow with DC, AD, etc. and a quick fix is to reboot the SQL Server OS when this occurs and there is no network communication with the DC at the time of the reboot, and all comes up fine with everything back in sync at that point. To accurately answer this, you probably need to find the reason first. 

Below is a script of the T-SQL part of this process which I run after doing the above 4 steps, and it always works in my environment to accomplish what seems to be similar to what you've explained that you're trying to accomplish. I also have to ensure the AD account has a strong/complex password, and set it to never expire. 

If you really want to know why it's failing, here are some things to check with the package and how to troubleshoot to ensure it's not a connection or authentication issue. Look at the that appears just before the and change the properties for the FTP connection. This should include the , the the FTP server listens on, the , and . Ensure that all FTP attributes in these connection string properties are set correctly, and test from command line or an FTP client tool to ensure that whatever you have value wise in there also allows connection via that method to ensure it's not a password or incorrect value issue of what you're connecting to. 

SQL Server get drive space information, get volume space information, get disk space information, get disk free space, disk space allocation, report disk space, report storage space 

Use the keyword with the statement to get the data to load with MySQL without needing to adjust the file parameters 

In the SQL Agent job that you use for backing up your DBs with backup devices, just have each step move onto the next step whether they fail or succeed. However, have the very last step have the below logic in it and tell it to report failure on failure and to report success on success. This will email you the failure detail from the msdb tables for that job as you'd see in the SQL Agent job history area for the current date records. You could always check the SQL Agent history manually when you get the email notification as well but this works well for the need where I've set it up. I'm sure you could use the logic and make some changes to suite your need further too. In my case, each backup job to backup device is it's own separate step with each moving onto the next step regardless of the results, and the very last step is as I indicated above and is sufficient for failure monitoring. If I'm not in the office or by VPN connection, the emailed HTML table data will give me an idea if this needs attention and so on checking Outlook Web Mail. 

Since you're in full recovery and just need to keep the "support" DB up-to-date from production as-needed or once-a-week, then is log shipping not an acceptable option? You could have it restore once-a-day during off hours if it being in standby for the triage of issues would work. The transaction logs could copy over every hour, or whatever you feel is appropriate. Otherwise, you may be able to setup a custom SQL Agent job (with TSQL scripting or maybe xp_cmdshell) to copy over the transaction logs from prod but have a disabled SQL Agent job (on support instance) to kick off ad hoc to as-needed to apply the copied over transaction logs to the support DB. This would allow you to have ad hoc log shipping essentially without standby assuming you setup the restores correctly to not be in standby. The once-a-week backup file from prod could be copied over to the "support" server and overwrite or delete the existing file depending on naming conventions. You could then schedule that with SQL Agent job to do the once-a-week full backup and still let the transaction logs copy over without restore until you need to kick it off ad hoc as-needed. You would setup a job to purge TRN files that are older than a certain time frame once the full backups complete so the old ones are deleted from the "support" server. This may be step 2 of your "support" full backup file restore and just plugin the age of files to purge as-needed. See below example and change arguments and variables as-needed per your requirements and test of course. 

Continued Trouble If you continue to have trouble you might set the Use SSL field to a value of No and also ensure all the SSL fields below that are blank and then test the connection again. 

Wouldn't you want to ensure the secondary logs are restored to the secondary server at certain intervals? With log shipping, I thought you ship the logs over to your secondary DB and then restore from those to keep the secondary up-to-date as close as possible to your primary. Do you see any reason why you would not want to automate the log shipping restore on your secondary DB in the log shipping configuration? Once those log files are applied to secondary, then the job will purge the logs that are not needed any longer since those are reflected in the secondary DB at that time. Test it out and see how it goes. EDIT: @nulldotzero Okay, since you cannot run restore operations on primary or secondary in an AlwaysOn configuration, and because you are also copying the TRN log files to the seconary as well as the DR for redundancy, then you could just setup a SQL Agent job on the secondary instance to do the below for whatever hours you feel need to be purged. 

The people or security contexts running the expensive queries on the wrong instance should be identified and then notified to change their processes on the instance and DB to start running their stuff on the instance you want those run on. Since the secondary is read-only/standby, we would assume these expensive queries are SELECT statements only where the primary DB that's not in standby would have to run queries that update data or modify objects regardless of the expense. You could also look into why some of these queries are so expensive in the first place, see if adding indexes would help, rewriting queries for performance tuning, etc. I think that'd be a good root cause type solution. It seems like it'd be tricky to do this with one of more AD groups though as the changes are replicated from primary to secondary and secondary would be in standby to accept transaction log changes from primary when those are applied so I'm not sure how you would accomplish this in that sort of configuration with log shipping. I'm also not certain how often your tran logs are being restored to secondary as I thought when LSRestore jobs run, it disconnects all session on secondary until those transactions are committed, so I assume it's once a day or not too often or people would be screaming about their queries, etc. getting disconnected during normal hours. If once-a-day restores are occurring for LSRestore jobs on secondary, then this would mean the data gotten from that server is 24 hours old or however long between your LSBackup, LSCopy, and LSRestore jobs. So who can use which DB with this regard may depend on how fresh their query results need to be from the business side, etc. Lots of factors to consider here but getting to the root cause and having bad performing queries tuned, adding indexes, etc. may be the best solution as well as having the people with access take responsibility with their processes to not hose up the performance for others when they run their stuff. 

Getting the SID Once a SQL login is created you can get the value of its SID by simply using and pass the SQL login name as an argument, and it will return the SID value for that principal. Example T-SQL 

Afterwards, as timed out with some testing, what you already know about the timing, etc. the LSBackup, LSCopy, and LSRestore jobs should start working again. I don't remember exactly, but you may need to purge the LSBackup and LSCopy TRN files on both primary and secondary after step #1 above to ensure SQL doesn't try to apply the broken chain TRN files to the secondary DB. This can be done and I've done it before. It may not be "best practice" but if there's a business need, that should be enough justification if it works, gets the job done, and allows you to put some automation around it. 

Since you control the logic that creates the dynamic SQL Agent jobs you should be able to test with additional logic that will delete the dynamic SQL Agent job and execute sp_delete_job if the date is equal to a particular date (e.g. day eight or day nine). It may be best to use logic that will execute sp_delete_job if the date is greater than or equal to day eight or nine in case it doesn't run on the exact date due to system issues, etc. with logic such as: 

To Change Secondary DB with Log Shipping in Standby Mode The changes must occur on the primary DB first, and then those changes must be in a transaction log that is then applied to the secondary DB before the changes are effective on the secondary DB. 

Note, sometimes with the SQL Server and Visual Studio and other components, software, etc. as such, you have to install them in the order based on their version or things in the Windows registry, DLL component, etc. just get out of wack. For example, if you have some component (or software) of the 2005 and 2008 version you need, it's best to install the oldest first (2005) and then once that is complete install the newest (2008). I've seen this too many times where someone installs some old component or software as such on top of newer software, etc. that's already installed then the system start having strange problems. If all else fails, you can backup your important data, and then do a fresh wipe of your HD and install a fresh copy of Windows. Microsoft has no way of emulating all software on everyone computers in the whole world, so perhaps when you upgraded and then downgraded, something just got corrupt and there was something on your system that they didn't test with, etc. I typically always do fresh system upgrades rather than in-place upgrades but I'm in a business environment mainly too. 

You can script this whole process out with TSQL and SQL Agent jobs depending on the your cleanup process -- I assume you can or already have it scripted but here are the basics how I've dealt with similar issues in environments I maintain and support. 

According to this StackOverflow post, if you have the server-level settings (referenced below) configured with ForceEncryption set to Yes, then that will enforce an encrypted connection regardless of the SSMS GUI option being checked or not prior to making connections to that server. This may be a sufficient workaround for people where the "encrypted connection" is of more importance than the actual option being checked within the SSMS GUI. 

Press the Test Connection option from the bottom of the Manage Server Connections window after you make the to confirm the error goes away. 

Check the Event Viewer on the Windows Server which SQL Server is installed for more detail of issues as well Restart SQL Agent Service Check Service Broker if applicable Check DB mail settings, etc. $URL$ $URL$ $URL$ 

So this means there IS still SQL transaction logging that occurs when the database is in recovery mode but it's minimal logging when compared to recovery mode. 

Due to the fact that you have a configuration of in an of , I can only assume this is the ROOT cause of the issue (see notes and resource link below). There could be latency issues with committed transactions in this configuration due to the primary replica waiting for acknowledgement from the secondary replica that it's hardened its transaction logs before it commits its transaction on the primary. So, not knowing all the business and infrastructure detail on your side, you may want to consider or perhaps test changing the to since it commits its transactions without waiting for acknowledgment from the secondary replica that it has hardened its transaction logs. You may want to check and confirm if there are issues with transaction log hardening on the secondary replica server when this occurs. If you're going over a slower WAN or MAN link perhaps, confirm no issues at network level hops, or any general server issues, etc. If there is an issue found at one of these other levels, then fixing that should fix your original issue I would think since the primary could acknowledge quicker that the secondary replica hardened its logs and then it'd commit its transaction.