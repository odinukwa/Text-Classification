AFAIK rsyslog doesn't support wildcards in filenames. nxlog does. In addition it can parse apache logs and can send it to elasticsearch over http. You might want to take a look at it. (Disclaimer: I'm affiliated with the project.) 

You need to tell the compiler/linker where to find the zlib library. This is usually done with if it is autotools based. 

I'm not sure how this is possible within ossec, but you can do it with nxlog (disclaimer: I'm affiliated with the project). It can write the fields to database using the om_dbi module. 

I think the issue is that exec_async() should not receive the arguments concatenated to the command, i.e. you need this instead: 

The fields (name, number, naam, group) should be available in graylog when sent via GELF. Note that the original data won't be rewritten unless you explicitly tell it to do so with something like this: 

I think SEC can do this, but it needs to be monitoring it in real time. I can also recommend you nxlog (disclaimer: I'm affiliated) for checking such situations. This example could be tweaked to fit your needs. On the other hand writing such a script in perl wouldn't be that complex, but you need to take a few things into account in order to not generate false positives or miss an alert, and once more rules need to be added, you can easily find yourself stuck with development. 

There is an example in the manual: Example 6.9. Parsing URL request parameters in Apache access logs 

The cause of the syntax error is that your brackets are not correctly paired. It should be like this: 

You might be also interested in nxlog as it provides a wealth of features such as rfc5424 syslog format, TLS/SSL, filtering, rewrite, etc. (note: I'm affiliated with the project). 

I'm trying to fix a problem in a Chef recipe for installing / configuring ClamAV which is caused by the recipe using the wrong default user/group name for the clam service account on some platforms. It seems that: 

Is it good practice to version control the nodes and roles when using chef? If so, what is a good way to do it? It looks likes one should be able to take a tree of JSON files created using and simply check it into VC. Are there better alternatives? Update It turns out that checking in the JSON produced by is not a good solution. The problem is that the JSON produced by the script is not stable. Each time the hashes come out in a different order and the resulting files are totally different to the previous versions ... even though they means the same thing when parsed as JSON. I would be better of just saving compress tarballs. (But if I could canonicalize the JSON files by ordering the attributes ... ) 

Have I over-simplified this? Does anyone know of any examples that contradict this? (Lets assume that we are talking about the "recommended" RPMs / DEBs for the respective platforms ...) 

The system has one externally visible IP address, and Dom0 runs an Apache httpd configured with a number of virtual hosts each of which reverse proxies to web servers running on the virtuals. (The virtuals have to be NAT'ed, primarily because we don't have enough allocated public IP addresses.) The symptoms: 

I have a Chef Node that I'm initially setting up on one network that I need to (physically) move to another network. When this happens, I want the IP address and domain name to change, and the Node name and simple hostname to stay the same. I know I can do this by deleting the Chef Node and recreating it. Could I also do it by editing Node attributes? Or would that break the client keys or (worse still) the server? (I'm using Chef 10.16.2 ...) 

It's a little unclear whether there is a " character in your input or not. Try setting QuoteChar to something other than the double-quote. 

The message in the event is rendered by the EvtFormatMessage function. As far as I remember there was a limit of around 32k characters for this so this shouldn't be causing the truncation. This works via a format string that is identified by the event id and a set of values that are stored with the event. The piece is such. The EVENTDATA_DESCRIPTOR structure that is used to write this value can also store larger data. My bet is that the event provider has an internal limit (5120) for this. The reason behind this is probably due to the limitation noted in the documentation: 

You should check for any errors. To me it looks like the line is incorrect as there should be a linebreak after each . 

This is a bug in the module of the NXLog Community Edition. We have already fixed this in the EE and the fix will be available in the next CE release also. 

I can't say if it is a bug or is intended to work this way. The file can be monitored even if it isn't opened. Calling stat(2) and checking mtime/size will give hints whether there is any change since the last read. At least this is how nxlog works and avoids running out of file descriptors if you need to monitor a lot more than 28. 

Not sure which type you are referring to as the "event doesn't happen" can take different forms, it can be either conditional or unconditional. Examples: 

If you are shipping it as plain syslog (i.e. using in ) then the value of $SourceName (=) will be in the message in the part what RFC3164 calls : 

It depends on the console software involved. It could be because the console is more secure; i.e. it is easier for them to manage what you can / can't do. It could be that it is easier for them to provide the average user with management functionality for the virtual ... without having to take a course in Linux/UNIX system administration. It could be a combination of these and other things. (Whether is is actually more secure for them probably depends on the technical ability of their admin staff.) But the question is moot: they have chosen. If you really don't like it, your best option is to change to another provider. 

We've got a CentOS server running a cluster of virtuals. Occasionally the cluster's internal network drops out for a minute or so ... and then comes back. The problem is somehow related to the actual network traffic, but it is not a simple load issue. (The system is generally lightly loaded, and the problem occurs irrespective of actual load.) The setup: 

Your general question is not really answerable. There is no clear difference between "a shell" and "a console" in functional terms. There are nuances ... As conventionally used (in the UNIX / Linux) a shell deals with interpreting the shell language, running commands, maintaining session state, and leaves the mechanics of user input and output to something else. The something else might be the OS / hardware, or it might be something like a local or remote xterm. One usage of the term "console" is for the thing that does that bit. But the term "console" is also used to refer to something that provides higher level functionality; i.e. something that extends or subsumes the functionality of a classical shell, to provide something closer to what the user is presumed to want to do ... most of the time. I suspect that this is the case here. So to answer your question: 

Yes, but localhost (which is normally 127.0.0.1) is not accessible from the outside so you need to make it listen on an address that is accessible. Your im_tcp input instance needs this: 

To send syslog from NXLog to a syslog server you'd need to use the xm_syslog extension module and invoke one of the formatters (, , ) depending on the desired format that your syslog server supports. For more details see the Syslog section in the User Guide. While some USB events stored in the Windows Eventlog, there are other data sources for USB events: 

Can you try removing from your ? It will be still running as . Afair there was a bug with this in the CE. 

Probably you should send the message without the hostname (foo) and in rfc3164 format (not rfc5424 as the above) to get it parsed. 

If you are after the first case and need an open source tool, there is a Pairwithwindow rule in SEC and an Absence rule in nxlog.(Note that I'm affiliated with the latter). The second type is simpler and both tools can handle that too afaik. 

Doing a regexp match on $raw_event is a little ugly and inefficient. I suggest using the following form: 

Take a look at nxlog. It can ship your eventlogs securely, there is TLS/SSL and HTTPs support. You could also format your logs in JSON and entirely skip the syslog format. I think these features will help integrate better with loggly. (Disclaimer: I'm affiliated with the project.) 

You should investigate further why logstash is not closing the connections properly (e.g. check the logs). Perhaps you have some network gear/proxy firewall between the two that terminates the TCP session? 

If you are asking if you can set things up so that one machine can talk to another machine's loopback IP (e.g. ) then the answer is that it cannot be done directly, and it is probably a bad idea. 

I'm trying to set up Chef-managed accounts for a group of machines with the following characteristics: 

I need to check this at work, but I think I could add a "pam_exec" entry to the stack after "pam_krb5" entry. This could run a script that uses "passwd -S" to check that the password entry for the user has not been locked. Or I could do it by placing the user a "blocked" group and using pam_succeed_if like this: 

We eventually did find the problem. It turned out to be a caused by a problem in our virtual network configuration. For some reason that I can no longer remember exactly, network traffic for that particular download was taking an extra loop through the virtual networks. When a user tried to upload a large file, the download was tying down all available kernel network buffers. That was causing the entire network to freeze ... until something timed out and it all unjammed. I'm sorry that this is all a bit vague, but there may offer some hints for people who run into a similar problem. 

The evidence you have provided suggests that the 405's occur when a PUT request is directed to the default request handler. (The PUT behavior is different for the PHP handler.) The default handler is checking the request method before it checks whether the resource exists and has the correct permissions. Since the default handler doesn't support PUT unless this is enabled, you are seeing 405's rather than 404's. The PUT method is typically enabled by listing it in the config.