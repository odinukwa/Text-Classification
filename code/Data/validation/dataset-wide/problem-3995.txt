Quillens' paper on the Adams conjecture (doi:10.1016/0040-9383(71)90018-8) almost gives an answer. He maps a limit of spaces BGL_n(F_q) to BU and shows that it is not far from an isomorphism. This is related to the plus construction, but cant remember the details offhand. The space BU in turn can be described in terms of Grassmannians. 

The connection between the bimonster and moonshine and the 26 dimensions of string theory is still mysterious (at least to me), though there are several intriguing hints that there is something going on. Some papers discussing this are as follows: The paper by Miyamoto "21 involutions acting on the Moonshine module" J . Algebra 175 (1995), no. 3, 941–965. gives a relation between the 26 involutions generating the bimonster and the natural module of the monster. His construction involves the 26 dimensional even unimodular Lorentzian lattice that also appears in string theory and moonshine. Basak "The complex Lorentzian Leech lattice and the Bimonster" J. Algebra 309 (2007), no. 1, 32–56 gives a complex reflection group generated by 26 complex reflections similar to the bimonster, except that the generators have order 3 rather than 2. 

A few of the more obvious ones: * Resolution of singularities in characteristic p *Hodge conjecture * Standard conjectures on algebraic cycles (though these are not so urgent since Deligne proved the Weil conjectures). *Proving finite generation of the canonical ring for general type used to be open though I think it was recently solved; I'm not sure about the details. For vector bundles, a longstanding open problem is the classification of vector bundles over projective spaces. (Added later) A very old major problem is that of finding which moduli spaces of curves are unirational. It is classical that the moduli space is unirational for genus at most 10, and I think this has more recently been pushed to genus about 13. Mumford and Harris showed that it is of general type for genus at least 24. As far as I know most of the remaining cases are still open. 

[Emil Jeřábek posted a similar comment while I was writing this…] Probably linear programming is the simplest way: Let's say that $I_i = [l_i,u_i]$. Now plug the following linear program into any linear programming solver: $$ \min_{x,y} 1\quad\text{such that}\quad l_i\leq x_i \leq u_i,\quad \begin{bmatrix} X & -I\end{bmatrix}\begin{bmatrix} y\\x\end{bmatrix} = 0. $$ If the solver reports infeasibility, then there is no $x$ in the range of $X$ that lies within the bounds given by the intervals, otherwise it should return some feasible vector. Alternatively you also use alternating projections: Start with some vector $x$ and then perform projections onto $I_1\times\cdots\times I_n$ and the range of $X$ alternatively (the first is a simple clipping while the second is basic linear algebra). If the intersection is not empty, this will converge to something, otherwise, the method will end up alternating between two point that realize the distance of the two sets. 

In words: The minimal distance of a point to a convex sets is equal to the maximum distance of said point to any hyperplane separating the point and the set. So minimizing distances to a set of points is equal to maximizing distances to a set of hyperplanes (in certain situations, of course…). 

In general, small perturbations of the objective may change the set of local maxima drastically. Just think of a flat local maximum and adding a small wiggling (so also uniform approximation does not really help). However, there is a notion of convergence of functions, that is build in a way to ensure convergence of extreme points and this is the so-called Gamma convergence which works for functions defined on topological spaces and its specialization to Banach spaces, which is Mosco convergence. 

If one iterates the map z -> z^2 + c there is obviously a simple formula for the sequence one gets if c=0. Less obviously, there is also a simple formula when c = -2 (use the identity 2 cos(2x) = (2cos(x))^2 - 2). Are there any other values of c for which one can solve this recurrence explicitly? (For all initial values of course: there are many trivial explicit solutions for special initial values, such as fixed points.) Related links: $URL$ (the points c where 0 remains bounded under iteration of this map: this strongly suggests that there is no simple exact solution for general c). $URL$ (gives the explicit solutions above, after a change of variable) Motivation: I once used the map with c=-2 in a lecture to show that one could prove limits exist even without a formula for the exact solution. A first year calculus student pointed out the non-obvious exact solution above, and I don't want to be caught out like this again. 

The universal enveloping W* algebra of a C* algebra is discussed in detail in chapter III.2 of volume 1 of Takesaki's work on "Theory of operator algebra". It is universal in the sense that any map to an W* algebra factors through it (modulo some mumbling about topologies), and as mentioned above is given by the double dual of the C* algebra. This is called the Sherman-Takeda theorem, and was announced by Sherman in 1950 and proved by Takeda in 1954. 

Knuth, the art of computer programming vol II theorem A section 3.2.1.2 gives conditions for a linear congruential sequence x -> ax+c mod m to cycle through all numbers mod m, which may be what you want as these are often reasonable pseudorandom number generators. The conditions are that c is coprime to m, and a-1 is a multiple of all divisors of m that are prime or 4. 

Consider a non-planar quadrilateral in three dimensions, i.e. four points $x_1,\dots,x_4$ in $\mathbb{R}^3$ that do not lie on a plane and connected by straight lines. Then, by general theory of minimal surfaces and the Plateau problem there exists a surface of minimal area with this lines as boundary. The situation looks like this: 

If you map the boundary of the unit circle conformally, then the image will not have kinks, but an optimal tour should have kinks. Probably I just did not get what you had in mind... 

Sure you can't - but somehow you can. Obviously, $x\mapsto h(\theta\cdot x)$ is not an integrable function (if not $\equiv 0$) since it is constant along lines perpendicular to $\theta$. However, if $f$ is not $L^2$ but $L^1$, then you can view the integral as duality pairing: If $h$ is bounded, i.e. $h\in L^\infty$, then it makes sense to write $$ \int f(x) h(\theta\cdot x)dx = \langle f,h(\theta\cdot\ )\rangle_{L^1\times L^\infty}. $$ The situation is somehow similar to the situation for the Fourier transform: Of course, the integration against a harmonic plane wave is not an inner product in $L^2$ but makes sense in an $L^1$-way. But one can intuitively think of it like an inner product against an "uncountable orthonormal basis" to keep the analogy with Fourier series or discrete Fourier transforms. 

I am not sure if there is a standard name for these type of problems. I would call problems of this type sparse approximation problems because you want to solve a linear equation approximately (assuming that $\epsilon$ is small) and sparsely. In statistics these problems (where one really aims to minimize the number of nonzero entries in the solution) are called subset selection problems, since you want to select a subset of the columns of $A$ that should be used to represent $b$ (approximately). In general, the problem you gave is not equivalent to a linear program and in fact is NP hard. Also it is not convex and may posses a huge number of different minima. However, the links already given in the comments to sparse approximation or the Dantzig selector (other buzzwords are Basis Pursuit are Compressed sensing) show that the problem admits a convex relaxation namely, you interpret the objective function $\# \text{nonzero entries in $x$}$ as $\sum |x_k|^0$ (with the convention that $0^0=0$) and then replace the exponent $0$ by the smallest one that makes the objective convex, and this is $1$, i.e. you end up with $\sum |x_k|$. The resulting problem (with the constraint in the $\infty$-norm) can then be casted as a linear program. Remarkably, this relaxation is frequently found to be exact, i.e. the optimum of the relaxed problem is (one of) the optima of the original problem. This can be explained by some theory, but happens much more often than theory predicts (and also, in most practical cases I have seen, the theory does not apply but the predicted results hold (approximately) nonetheless). 

There is such a bound due to Minkowski. More precisely he shows for any symmetric convex region C, one can find a basis in xC for sufficiently large x. I cant remember the exact bound offhand, but it is probably somewhere in "An Introduction to the Geometry of Numbers" by Cassels (or any other book on the geometry of numbers). Addendum: on checking I realized that I misremembered Minkowski's result. He does indeed gives a bound for the product of the lengths of a basis in the lattice (in terms of successive minima), but it seems to be a basis for the real vector space rather than a basis for the lattice as you asked for. 

The analogues of Schwartz functions on general locally compact abelian groups are called Schwartz-Bruhat functions, and are mapped to Schwartz-Bruhat functions under Fourier transforms. Their dual spaces are spaces of tempered distributions on such groups which are mapped to other tempered distributions under Fourier transforms. The tempered distributions on these groups include most functions you might want to take a Fourier transform of. 

Some intuition might be given by the following informal analogy with measure theory: if we have a measure space of measure 1, then club sets are analogous to subsets of measure 1, while stationary sets are analogous to sets of positive measure. In other words, club sets contain "almost all" ordinals, while stationary sets contain "a positive proportion" of them. 

Mumford in Rational equivalence of 0-cycles on surfaces gave an example where an intuitive result of Severi, who claimed the space of rational equivalence classes was finite dimensional, was just completely wrong: it is infinite dimensional for most surfaces. This is a typical example of why the informal non-rigorous style of algebraic geometry was abandoned: too many of the "obvious" but unproved results turned out to be incorrect. 

Pretty late to the party here but Kantorovich's "On the translocation of masses" from 1942 is two pages. It gave a radically new look on the Monge problem of optimal transportation and can be seen as the starting point of an immense body of work on optimal transport and distances in probability spaces. 

Methods of these type sometimes go under the name "Kaczmarz method". Kaczmarz method is a method for solving $Ax=b$ by iteratively (e.g. cyclic") projection onto the solutions of the equations given by the rows of the system. If the system is underdetermined and initialized at zero you'll end up minimizing $\|Ax-b\|_2^2$ and then you are precisely in the case of your question. Although the method works pretty well in practice, at least for some problems (and for some it is even competitive to conjugate gradients for the normal equations), the convergence theory is not very nice. For example, the convergence rate depends on the order of the rows (or the $f_i$'s in your case). This seem most easily in two dimensions: If $A$ has two orthogonal rows, the methods finds the exact solution after projecting onto these two rows one successively. However, if there would be projections onto some other rows inbetween, you do not get convergence in finite time anymore. You get some convergence rate (in expectation) if you choose the rows not cyclically but randomly (see the Strohmer, Vershynin paper on the Wikipedia page). The method is also related to the class of POCS-methods (projection onto convex sets). Perhaps even more closely related are incremental (sub)gradient methods (check "Incremental subgradient methods for nondifferentiable optimization" by Nedic and Bertsekas) and "stochastic gradient methods". 

See On numbers and endgames: Combinatorial game theory in chess endgames by Elkies for some chess positions with non-integer values. 

This is a result due to Godel in his 1936 paper "Uber die Lange von Beweisen" (on the lengths of proofs), republished in English translation in volume 1 of his collected works. Roughly speaking, he shows that there are short theorems that have extremely long shortest proofs in certain formal systems, but much shorter proofs in more powerful systems. Harvey Friedman found some rather dramatic explicit examples, described by Smorynski in The varieties of arboreal experience 

A one line answer: A modular form is a highest weight vector of a discrete series summand of L2(SL2(Z)\SL2(R)). There are numerous variations of this: one can replace the reals by the adeles, or SL2 by another group, or replace discrete series representations by principal series to get Maass wave forms, and so on. This is explained in detail in Automorphic forms on adele groups by Gelbart. An introduction to the Langlands program by Bernstein and others is also good. 

This is an answer to the part of the question about why these terms are not translated into English. The reason is that words such as "nullstellensatz", "Schadenfreude" and so on that you mistakenly think are German are in fact perfectly good English words and so do not need translation. (Look up Schadenfreude in the Oxford English Dictionary if you do not believe it is an English word, though they have not yet caught up with nullstellensatz.) The point is that unlike languages such as French and German that try to remain pure, English has been happily looting terms from other languages for centuries, and the only difference between "nullstellensatz" and "house" is that "house" was stolen so long ago that we have forgotten about it.