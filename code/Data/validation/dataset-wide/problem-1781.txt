I would not expect that Apache would stop serving during transfer. I can think of four things that could slow down Apache. 

Looks good. I would drop the accept on port 20 as it should be handled as a related packet to an FTP connection. I usually put port 123 as the first check to minimize latency for NTP. For the OUTPUT chain only DROP or REJECT rules would make sense. The existing output rules don't do anything but duplicate the policy. You may want to look at your counters and adjust the ordering of rules accordingly. Consider using different chains for new connections on each interface. Consider logging non-accepted packets. Using a tool like Shorewall to build the firewall might make it easier to get everything in place. 

vncserver and tightvnc both have Java applets which can be added to your configuration. These are usually servered from a in the 5800 range instead of the 5900 range. For VNV port 1 you would browse to $URL$ This will trigger the applet to download and connect. You will need to setup regular VNC access first. The README file included with the tightvnc-java package includes setup directions. The applet can be served up from an existing web server such as apache. The java code can be run standalone which you may want to use to test your configuration if you are embedding it in a web page. 

I would consider using a firewall builder to generate your firewall. Something like Shorewall which is well documented and has some good example configurations could make your life easier. On a quick examination the three interface (zone) configuration might be a good starting point for you. The key files for shorewall are zones, interfaces, hosts, policy, and rules. There are a number of macros which generate commented rules by name rather than port. Normally your rules would be at the top of the list. Using separate chains for various traffic flow might help understand what is happening. It appears only two addresses have open access for port 80 which would explain your browsing problem. One should be the content-filter. The other may be the address from which you were able to browse while bypassing the content-filter. 

Your problem may well be your DNS configuration. Start with a static DNS address and get reverse DNS working. DKIM and SPF both have DNS components which need to be configured correctly. I don't believe either DKIM or SPF will solve your problem, but do get them working. I have posted on Detecting Email Server Forgery. This is written from the perspective of the recipient. However, it specifies the DNS entries that your need. If you are having problems sending to Gmail and Yahoo, it is likely one of the rules in this document that is causing problem. My post on Securing your Email Reputation with SPF outlines the various SPF records you may want on various servers. You should protect your domain, web domain, as well as your mail server domain. My post on Implementing DKIM with Exim also covers key management and the DNS entries for bind. You will need these if you want your DKIM signatures to be valid. 

Install the logcheck package. It will scan the logs once an hour and email you anything it doesn't consider normal. Essentially, it emails anything that entered the logs in the last hour that it doesn't have a rule for ignoring. There are additional attack rules than include things which shouldn't be in the log. The email subject line varies depending on the reason things were picked up. I generally build a local ignore file for it as I discover things which I consider normal, but don't have existing ignore rules. The various syslog alternatives all support server consolidation, so you can forward the logs to a single server. However, I haven't been in the habit of doing it. The only system I forward logs off of is my OpenWRT firewall. EDIT: I do use Splunk at work to search log files, although if I known the particular log I am looking for I am more likely to use less. It does have alert capabilities, but we don't use them. I expect they would alert on a match to a known record. This can lead to a lot of false negatives if you have new problems without an alert rule. I prefer to have false positives like I get from logcheck. Splunk may have better timeliness on alerts though. I do get timely alerts from fail2ban on cases that cause it to trigger. It also maintains blacklist entries for the originating source. 

This will run the commands in a sub-shell and run those in the background. If you run into problems with zombie processes, the spawned process with the command after the backgrounded commands. This assumes you are using as the scripting shell. 

Authorized keys are well covered in the ssh man page. Try 'man -k ssh' to get a list of man pages related to ssh. ssh-keygen it the tool to create the keys for you. There is a helper program ssh-copy-id to assist in securely copying the the public key to another system. If you want to understand how they work, then look for articles on public key encryption. SSH always uses one key from the server. Authorized keys adds using a key from the client to identify the client. Authorized keys are a copy of the clients key stored in ~/.ssh/authorized_keys. As such they are only valid for the user id for which they are authorized. The man page documents options used to restrict what the key can be used for: program, source IP, port forwarded, etc. Authorized keys are a good option if you want to run programs across systems in batch mode. It is a good idea to at least limit the systems which can use a given key. This will make it more difficult to use a stolen key. Putty (the ssh client for windows) uses a different storage format for the keys. It will provide the public key in the appropriate format for openssh. This is covered in the putty documentation. 

It may be possible to generate a test configuration that delivers messages to a file. See the above mailing list thread. The amavis-new documentation indicates these is some documentation in the distribution. In particular, 'test-messages/README'. Amavis works by re-injecting mail back into your MTA. This can result in back-scatter spam. 

This is a complicated setup. You may find it simpler to move the two Internet connections to one router, and add an additional VLAN and zone for the second internet connection. I would look at the documentation for multiple ISPS on the Shorewall site. I believe Tomato should run Shorewall-lite (which will require another server capable of building the rules. If not I believe the documentation explains the rules being set up. You may be able to run the split connection using the two routers using the gateway address of the second router as the second routing destination. If I understand your requirements, I think you only need to configure router 1. 

Addresses between 0.0.0.0 and 223.255.255.255 are unicast. Addresses between 224.0.0.0 and 239.255.255.255 are multicast. Addresses from 240.0.0.0 and up are reserved. Anycasting is done with Unicast addresses in the routing tables of multiple addresses. It appears to be much easier to implement than multicast. Without looking at these routing tables, there is no way to differentiate an anycast address from a unicast address. This is inherent in the design. Anycasting relies strictly on the the routing tables and does not require special address ranges as multicasting does. RFC 4768 which covert the Operation of Anycast Services may help you understand how it works. As the comment thread on the questions notes pings and traceroutes, can be indicative that and address is being anycasted locally. Now the IPv4 address range has been fully allocated anycasting is likley to become more popular for organizations providing content to geographically spread clients. At that scale it requires cooperation with ISPs and network providers. Checking the address on netblock assignment hints at the location of the IP, but really just tells you where to contact the organization to which the address has been assigned. The IANA IPv4 Address Space Registry or other GeoIP address databases indicate the continent or country to which the IP address is expected to be routed. If it appears to be routed within a different continent, then it is likely anycasted there. IANA maintain the Multicast Address Space Registry along with several other important Internet number registries. RFC 6890 contains the specification for the Special-Purpose IP Adresses. 

It is very easy to impersonate someone else in mysql. Given a user id with no password (the weakest security) just use . If it has a password it is a little more difficult, but a weak password makes it easy. If root has no password, I can access root as . I can then do anything inside the database that root can. Use of host specifications in the security is also a good idea, especially if remote access is or may be available. Passwords in files can and should be somewhat secured by permissions. Access by root, or the owner of the password file is trivial. If possible encryption of the password on disk should be used. This makes it slightly more difficult to access, but still vulnerable. 

If you are using cookies for persistence you need to provide a cookie jar for curl to use. Otherwise each request looks like it is coming from a new user. Try a command like: 

The most reliable situation would be to setup a for a subdomain of (host on) your domain pointing to the domain you are updating with DynDNS. I'll assume your domain is below. There are a variety of services that will support dynamically updating domains. Using a for your domain is not a supported configuration. If you want it for access to your home system you could setup as a pointing the name updated by DynDNS. You may be able to find a redirection server which can redirect web requests for your domain to your web subdomain. Requests for would be redirected to . If you want a reliable web server, you likely need to find a hosting provider. If you have a hosting provider, you may be able to configure your domain's record to the same IP address as you use for your web service. Another alternative is not to publish any addresses for your domain, and just have addresses for your subdomains. Browsers may try for a subdomain in this case. DynDNS no longer offers free services. When it did, I had a CNAME in my domain's DNS like which pointed to my DynDNS entry (). I've had a fixed IP address for years, so I haven't worked with the paid service. My and subdomains were hosted elsewhere and I provided records for those servers. My web provider supported my domain as well as the subdomain, so I provided an record for my domain with the same address as the subdomain. If you want email services, you will have problems using dynamic IP address. Many domains will refuse to accept mail from you, and you risk loosing incoming mail. 

If you don't provide your mail server name(s), you won't get much in the way of expert help. Temporary blocks are common for your first email to a recipient from a server. Barring rules that bypass greylisting for your server, servers using greylisting will temporarily reject your messages. However, this should be with 450 (temporary) rejection rather than a 550 (permanent) rejection. If you are too agressive at retrying, you may trigger the receiving server to locally blacklist your server. It is possible that you managed to get onto local blacklists when you were first setting your server up. This may be from greylisting as noted ablove, or another mechanism. My server includes some automatic blacklisting of poorly configured servers. These kinds of blacklisting are typically temporary, although often for a period of months. The "550 recipient address rejected" typically means the recipient is no longer at the address. (Mailing lists often don't get cleaned, so they can generate a lot of these messages.) As you have setup DMARC, your reporting address should be giving you some indication of why major mailer hosts are rejecting your mail. If you have not setup reporting, do so. Smaller mail servers likely don't support DMARC, so they will not report. You should be able to use your logs to gather enough data to attempt a reasonable replay of rejected messages using telnet. This may provide more details on why the mail was rejected. You can also use this data to contact the postmaster on the receiving server to see if they can provide assistance. 

captures the packets by tapping into the packet stream. It gets all packets even those that will be dropped by the firewall rules. If you want to verify the traffic is being blocked, you should check on the network for outgoing traffic. You can test incoming traffic by setting up a listener and trying to send it traffic from the network. If your rules work, you should get different results with or without the rules in place. However, if you have a DROP policy, the results will be the same. In you could add logging or accept rules after the drop rules. Checking the the packet counts for the rules will tell you if anything is getting by the drop rules. Are you sure you want to drop all UDP traffic? DNS tries UDP first and falls back to TCP only if necessary. 

A quick review of the documentation leads me to believe that squid will accept and cache compressed data from the servers. Compressing images will likely be counter productive. Most image formats are well compressed already and attempts to compress them usually increase the image size by the overhead of the compression algorithm. You could use apache as proxy. You should provide lots of disk space for squid to cache data. Review the caching options carefully and watch your cache statistics. I have found certain sites don't cache very well at all. 

You can use to do deliveries. It is very programmable and can do multiple deliveries. You will need a file in the home directory of the user that you need to filter messages for. There are examples for a number of use cases. I suspect you want to filter mail that meets some criteria and then deliver it. The filter language allows you to provide multiple selection criteria and filter the headers, body or both. should be available as a YUM package. Normally, it is enabled in the Exim configuration so that it will be used as the MDA if the file exists. The documentation should be readily available on line if you want to read it first. 

You will want to ensure you host is up and your various services are running. It is also helpful to monitor that your service is accessible from outside the site. I have used Nagios, and it does the job well. It is rather complex to setup for a single server though. For graphing status I have used MRTG, and an rrd plugin for Nagios. I just recently started using munin. The advantage of munin is you can configure warnings when you go outside a boundary on something you are monitoring. If required you can run munin over ssh. 

Overnight is when most infrastructure changes occur. Networks and other resources may go down. It you are using remote monitoring, you will see your site go down because it is not reachable. Knowing the maintenance windows for your various resources will help eliminate these outages from actual outages. As other have noted, on average outages are more likely outside of office hours based on hours on the clock. Given weekday availability and an 8 hour workday, only 1/3 of the outages should occur during office hours. Add in weekends and even less of the outages occur during workdays. Track the reasons for the outages, and how they were detected. You will find some outages due to resources like the network being down. These may appear as mysterious outages where the site disappeared for a few minutes and came back without intervention. I would expect many of your overnight outages were infrastructure changes. Infrastructure changes are usually scheduled, so you should be able to arrange to be notified of them. You can then adjust your response accordingly. Your outage log should reflect that the outage was due to the change. Also record any intervention that was required. You may need to add recovery code to your application to handle database restarts or other such resource changes. Knowing the maintenance windows for various resources can help identify which resources are causing unplanned outages. You may need to trace your resource dependencies, as networked disk and databases will depend on the network infrastructure. Likewise, the database may depend on networked disk storage.