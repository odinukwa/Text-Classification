Introduction There are different ways of estimating the parameters of a production function. For example, single-equation and system equation techniques are both possible. Another difference among methods is that estimable forms may either involve direct or indirect estimation of the parameters. For instance, rather than estimate the production function as is, one may instead estimate the derived demand functions or, say, use other relations such as factor shares. Also to be considered are the underlying economic assumptions; such possibilities being perfect competition, constant returns to scale, profit maximization, etc. Question Bearing the above in mind, my question is what is the standard (accepted?) method of estimating normalized CES (constant elasticity of substitution) production functions? For information, a normalized CES production function can be written as $$ Y = Y_{0} \lbrace \pi_{0} K_{0}^{\frac{1-\sigma}{\sigma}}(K_{t}\cdot e^{\gamma_{K}(t-t_{0})})^{\frac{\sigma-1}{\sigma}} + (1-\pi_{0})N_{0}^{\frac{1-\sigma}{\sigma}}(N_{t}\cdot e^{\gamma_{N}(t-t_{0})})^{\frac{\sigma-1}{\sigma}}\rbrace $$ following the notation found in Klump, McAdam, and Willman (2011, p.22). Importantly, what data is required to estimate the parameters of this particular production function? And, what is the exact procedure involved? Demonstrations and references are most welcome! Reference: Rainer Klump, Peter McAdam, and Alpo Willman, The normalized nested CES production function, theory and empirics, ECB Working Paper Series, No. 1294. (Feb., 2011), 2011. 

The question about using mathematics to model economic phenomena is a rather old one dating back to at least the marginal revolution (19th Century). A couple of the key figures of this era were William Stanley Jevons and Leon Walras. I don't aim to provide a complete, or terribly accurate, history, but let's just focus on Walras. In the preface to his Elements of Pure Economics, Walras paints a picture that suggests mathematics is a necessary input (basic ingredient) of economic modelling. He says: 

If we now fast-forward to the present day, we see that mathematics is everywhere in economics. Thinking of doing a PhD in Economics? Well, mathematics (and statistics) is going to be served up first - no question about that. So, what can go wrong when using mathematics in economic modelling? The main problem, I believe, is not being able to see the woods for the trees. This point is illustrated in Klemperer (2003) Using and Abusing Economic Theory. In this paper, Klemperer gives the following example: 

Say you have two distributions, $G$ and $H$ with equal means supported on (say) $[0,1]$ and $H$ is second-order stochastically dominated by $G$. Then, for any concave increasing $u$, we'll have $$\int udG \geq \int udH$$ This would correspond to Aigner and Cain, where the risk averse firm would prefer to hire an agent from distribution G. However, now suppose that $G$ and $H$ are distributions of talent (with the same stochastic ordering i.e. $H \succ_{C} G$). One equivalent notion to this idea is that distribution $G$ can be obtained by ``fusing" together part of the mass of $H$, i.e. by collapsing part of the mass to its respective barycenter. Given this, it is easy to see that there is always a cutoff $\eta \in [0,1]$ such that $H(\eta) \geq G(\eta)$. This corresponds to Heckman's result--if the job is sufficiently demanding, then it will be (at least weakly better) to be part of the high variance population. Moreover, note that this by no means depends on the expected quality of the agent. There are simply more of the highest quality people in the group with distribution $H$ than in the group with distribution $G$. They are not just more likely to exceed the cutoff, there are more of them that exceed the cutoff. Another way to think about the hiring problem is as follows: say a firm wants to hire one person but receives applications from $10$ applicants, $5$ from group $A$ and $5$ from group $B$. A candidate's quality is unknown and it it ranges from $[0,1]$ continuously. Suppose also for the sake of simplicity that the firm may observe the candidate's group (and so knows what distribution he/she is drawn from): the distribution of quality for group $A$ is given by $G$ and the distribution of quality for group $B$ is given by $H$. Interviewing a candidate fully reveals a candidate's type, but the firm has the budget to interview only $4$ candidates. Whom should they interview? The answer is simple: all four candidates interviewed should be from group $B$ (which has distribution $H$). Why? Because we can think about a candidate's quality as a random variable $X_{i}$ and thus the firm wants to maximize $\mathbb{E}[Z]$ where $$Z:=\max\big\{X_{1},X_{2},X_{3},X_{4}\big\}$$ Since $H \succ_{C} G$, $$\mathbb{E}_{H^{4}}[Z] \geq \mathbb{E}_{G^{4}}[Z]$$ Note also that $H \succ_{C} G$ $\Rightarrow$ $Var(H) \geq Var(G)$ but not $\Leftarrow$. In this sort of thing, I think it is stochastic dominance and not variance that is the apposite metric. 

Question Are there any simulated datasets that have been designed specifically to represent macroeconomic data? In particular, are there any such datasets that can be used in benchmark studies? Background To give an analogy, in optimization, one may be interested in assessing the quality of a given algorithm. To do this, there are a number of test functions for optimization, such as the well-known banana function, that can be used to evaluate the performance of an algorithm; for example, by analysing if and how fast the algorithm can find a global minimum. I am interested in working in this controlled experimental setting using simulated data that has a macroeconomic interpretation. With data simulated from a known data generating process (this would be my control), it would be possible to investigate, say, the performance of algorithms designed to detect structural breaks and to assess techniques/models used for forecasting. I would greatly appreciate if someone could help: either by pointing me to some simulated datasets that can be used for the above purposes or by providing some guidance on how to simulate data that carries a macroeconomic interpretation. It's possible for me to simulate a variety of ARMA processes (or VARMA model), but I am really interested in something that goes beyond that; the simulated data ought to have similar properties to observed macroeconomic data. Obviously, I am trying to avoid using actual data because my control (of knowing the data generating process) would be lost. Update For one of the purposes I had in mind, a quick read of Castle, Doornik, and Hendry (2013) suggests that it is, perhaps, not that difficult. Their "experimental design" is based on the following equations $$ y_{t} = \beta_{0} + \gamma y_{t-1} + \beta_{1}x_{1,t} + \cdots + \beta_{10} x_{10,t} + \epsilon_{t}\\ x_{i,t} = \rho x_{i,t-1} + v_{i,t}, v_{i,t} \sim IN[0,1], i=1,\ldots,10,\\ \epsilon_{t} \sim IN[0,1], t=1,\ldots ,10. $$ along with some further qualifications. So, it would appear that for one of my examples (the case of evaluating structural break algorithms), relatively simple (although by no means, "not tricky") DGPs are all that's required. Reference: Castle, Doornik, and Hendry (2013) Model Selection when there are Multiple Breaks 

As a caveat, disclaimer, I am not too familiar with Evolutionary Game Theory and it is quite outside my research area/main areas of expertise. However, I can show here that range of $(a,b)$ for which $B$ is a best response is strictly greater than for either of the other two strategies. To that end, You can characterize the range of $(a,b)$ for which $B$ is a (strict) best response by solving the system $$\begin{split} 4−2b−3a &\> b+2-2a\\ 4−2b−3a &> 1+a-b\\ \end{split}$$ Or, $$\begin{split} 2 &> 3b + a\\ 3 &> 4a + b\\ \end{split}$$ Or, $$\begin{split} a &> 7/11, b<3-4a\\ a &\leq 7/11, b<\frac{2-a}{3}\\ \end{split}$$ 

Hence, $$\begin{split} M &= 1 - \int_{0}^{7/11}\frac{2-a}{3}da - \int_{7/11}^{1}\frac{3a-1}{2}da\\ &= 1 - \frac{259}{726} - \frac{32}{121} = \frac{25}{66} \approx .3\bar{78} \end{split}$$ Finally, $$T=1-\frac{25}{66}-\frac{101}{264} = \frac{21}{88} \approx .238\bar{63}$$ Since $$\begin{split} \frac{101}{264} &> \frac{25}{66}\\ \frac{101}{264} &> \frac{21}{88} \end{split}$$ the result is shown. 

You're correct on the definition of the strategies: Albus' strategy set is $S_{A} = \big\{N,E,S\big\}$, and Minerva's is $S_{M} = \big\{a, b\big\} \times \big\{a, b\big\} \times \big\{a, b\big\}$. Remember, a strategy is a contingent plan that prescribes an action at every possible node at which a player plays. Then, the unique equilibrium (via Backward Induction) is $(N, b, a, b)$. The others you mention are not equilibria, because they involve incredible threats, e.g. if Minerva found herself at the decision node following a choice of $E$ by Albus, she would never choose $b$. 

I agree with the comments that suggest that the question is almost indecipherable. However, it should be easy, given the cdf, to derive the bounds. Why? Well we know for a cdf $F$, we must have $F(\underline{S}) = 0$ and $F(\overline{S})=1$. We can use the latter to work backwards from the upper bound i.e. We guess that $F(s)$ is of the form $$F(s) = k\bigg(\frac{S-v_l}{v_h-S}\bigg)$$ We must have $F(\overline{S}) = 1$ and using your solution, we have $$\begin{split}F(\overline{S}) = F\bigg(\frac{7v_h+3v_l}{10}\bigg) = k\bigg(\frac{\big(\frac{7v_h+3v_l}{10}\big)-v_l}{v_h-\big(\frac{7v_h+3v_l}{10}\big)}\bigg) &= 1\\ k\bigg(\frac{7v_h+3v_l}{10}\bigg)-kv_l &= v_h-\bigg(\frac{7v_h+3v_l}{10}\bigg)\\ \frac{7kv_h - 7kv_l}{10} &= \frac{3v_h-3v_l}{10}\\ 7kv_h - 7kv_l &= 3v_h-3v_l\\ k &= \frac{3}{7}\end{split}$$ Thus, I suspect that your mixed strategy for the high type is given by the cdf $$F(s) = \frac{3}{7}\bigg(\frac{S-v_l}{v_h-S}\bigg)$$ 

I think the main alternative to the Johansen (statistical) approach is the methodology propounded by Pesaran and Shin in their so-called long-run structural modelling (economic) approach. The main formal reference is Pesaran and Shin (2002). The methodology is also presented to a wider audience in Garratt et al. (2012). Although referred to as the long-run structural modelling approach, you will also read about the type of models associated with the methodology, which are called VARX* models. The distinguishing feature is that the cointegrated models are estimated using reduced-rank regression and (over-)identifying restrictions are derived from economic a priori then tested. The sequencing is slightly different to Johansen since economic theory is given priority. In the same school of thought is the cointegrated VAR approach associated with Juselius (2006). Again, Juselius propounds an economic approach to cointegration as opposed to the statistical approach by Johansen (and others like Phillips). Pesaran's approach has been programmed and is available in the Microfit software. Juselius' approach has also been programmed and is available in the RATS (CATS) software. To my understanding, you can find MATLAB code in the GVAR toolbox, and this should give an indication of what's required for the Pesaran approach. In terms of programming, I haven't seen much difference when compared to Johansen (although I just quickly investigated this). Note that the GVAR approach can be thought of as an extension to the VARX approach (the difference lies in stacking individual country models, but estimation of the individual models is the same). References Pesaran, M and Shin, Yongcheol, (2002), LONG-RUN STRUCTURAL MODELLING, Econometric Reviews, 21, issue 1, p. 49-87. Garratt, Anthony, Lee, Kevin, Pesaran, M and Shin, Yongcheol, (2012), Global and National Macroeconometric Modelling: A Long-Run Structural Approach, Oxford University Press. Juselius, Katarina (2006) The Cointegrated VAR Model: Methodology and Applications (Advanced Texts in Econometrics). GVAR Toolbox 

In your basic introduction to information economics, the two classic interpretations of screening with hidden information correspond to monopolistic screening or a principal agent problem. Viewing the situation through the lens of the principal agent problem; there is a firm who wants an agent to produce a good, and the agent has private information as to how costly it is to her to produce a good of a certain quality. That is there are multiple types of agents--if there is just one type, this is just the full information (first-best) case, and the screening problem is trivial. The producer's problem is to design a contract in order to maximize expected social value while minimizing the agent’s rent. The basic problem with moral hazard, on the other hand, need not concern an agent with multiple types. Instead, the agent has multiple choices of effort. Now, the principal's problem is to choose the optimal level of effort and the best contract with which to elicit that effort level. To sum up, in the hidden information (screening problem), there is an agent with a private type, and the principal designs a contract in order to maximize some objective, given this information asymmetry. In the moral hazard problem the principal chooses a contract in order to elicit the optimal level of effort. Finally, as a caveat, this distinction is only clear at the introductory level, where the problems are very basic. One can easily have more complicated mechanism design problems that incorporate elements of both. 

Nope, every pure strategy equilibrium can be characterized as a degenerate mixed strategy equilibrium. That is, it is a mixed strategy in which a pure strategy is played with probability $1$. 

History $(D,D)$ (Yes, this is with some abuse of notation.): On Path: Each player gets an average payoff of $4$. Deviating: Say player $1$ deviates and instead chooses $H$. Then the resulting sequence of actions is the alternating sequence, $(H,D)$, $(D,H)$, $(H,D)$,... Accordingly, player $1$’s sequence of payoffs is $(8,0)$, $(0,8)$, $8,0$... Player $1$'s average payoff is $(1-\delta)\big(8 + 0 + \delta^{2}8 + 0 + \delta^{4}8 + \cdots\big)$, which is $$(1-\delta)\frac{8}{1-\delta^{2}}=(1-\delta)\frac{8}{(1-\delta)(1+\delta)}=\frac{8}{1+\delta}$$ Hence, there is no profitable deviation here iff $$\begin{split} 4 &\geq \frac{8}{1+\delta}\\ 4\delta &\geq 4\\ \delta &\geq 1 \end{split}$$ Thus, $\delta \geq 1$ is a necessary condition (not necessarily sufficient because we still need to check that following tit-for-tat is still optimal for the other histories. I'll leave that to you!) 

The point is concluded pointing out that too much detail (i.e. advanced mathematics) can distract from what's really important in the wider context: 

However, Marshall provides a more muted opinion about mathematics being a necessary output of economic modelling: 

Let's begin by fleshing out what the EViews code is saying. The EViews code says that the variable CLOSE depends on its first difference and there is also an ARMA(3,3) error term included in the model. Supposedly, the ARMA error process is to whiten the residuals, which can be, and has been, argued against as a modelling strategy. It's also worth noting that there is no constant term in the equation. The reference to BACKCAST indicates how the MA process is initialized - the details of which can be found in the EViews documentation. ESTSMPL typically refers to the estimation sample. Having understood the EViews code, the question is now: how does one estimate a linear regression model with ARMA errors using MS-Excel? The easiest solution would be to find an existing add-in that provides this feature. An alternative method would be to write the code yourself using the MS-Excel programming language (VBA). Beware though, what makes this application tricky is the MA term; AR processes are easier to program and more common in certain fields, e.g. VAR models appear more frequently in economics than VARMA models. Another approach, if you are willing to delve into some code, would be to look at some R source code (either base code or from some time-series package) to get a sense of the task ahead if you do decide to program from scratch. Programming this from scratch seems like overkill, though, as this sort of estimation has been around for decades. Why bother if all you're interested in are the estimated parameters and not the underlying routines? It doesn't answer your question, but my advice would be to use R if EViews is not available to you. Or, seek out an MS-Excel add-in that will do the job for you. A useful thread from the EViews forum is here: $URL$ Note: "Estimation, and forecasting of MA processes is complicated, especially when backcasting is used to obtain starting observations for the error terms." Good luck!