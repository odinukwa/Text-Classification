A collection of trees is called a forest, unsurprisingly. Your graphs are considered trees only if the edges are undirected, i.e. the relationships described are "two-way", e.g. X is related to Y can be restated validly as Y is related to X. This is known as a symmetric binary relationship. If your edges are directed, then the graphs are not correctly called trees. The relationships described above are asymmetric, creating links which are also not symmetric. You might consider searching for "forest mathematics" on wikipedia and in mathematical journals to get some other terms to consider for evaluating and describing your collection of graphs. 

So for the $4 \times 4$ configuration, assuming that $A$ plays intelligently, there are $4 \times 4 = 16$ first moves for $A$ $16 - 1 = 15$ first moves for $B$, if $B$ plays stupidly, or either $2, 3,$ or $4$ moves if $B$ plays intelligently $4 \times 1 + 8 \times 2 + 4 \times 3 = 4+16+12=32$ second moves for $A$ which win the game and end the game ($A$ could have picked one of the $4$ corners, $B$ blocks one way leaving $1$ way for $A$ to play, if $A$ picks one of the non-corner edge pieces (8 ways to do that), then $B$ blocks one way leaving two ways for $A$'s second winning move, and if $A$'s first move is a center piece, $B$ can block one of the $4$ adjacent squares, leaving $3$ adjacent squares for $A$ to pick and win. So if $A$ and $B$ both play intelligently, the sum of these are the possible games for a $4 \times 4$ grid: $4 \cdot 2 \cdot 1 + 8 \cdot 3 \cdot 2 + 4 \cdot 4 \cdot 3 = 8 + 48 + 48 = 104$ If $A$ and $B$ do not play intelligently, then the number of possible games is much larger, and the number of "winning configurations" or "end positions" is much larger. 

Notice that the {1x2} rectangle is duplicated vertically in the first line and horizontally in the 10th and 11th lines (marked with an asterisk). This yields $29$ different rectangles, with only one duplication, whereas Scott Carnahan's approach has much more than one duplication of the {1x1} square} So of course, the answer depends exactly on what you mean specifically by the question. 

For example, for $n=4$: Take the $12$ possible un-drected Hamiltonian paths of length $4$ on a graph over four labeled vertices. The complement of each of these paths is also a hamiltonian path. Since we know that the complement of a disconnected graph is obviously connected for $n>3$, then the number of connected graphs is at least equal to the number of disconnected graphs. Hoewever, since for $n>3$, the complements of at least some of the connected graphs are also connected graphs, that means that there must be more connected graphs than there are unconnected graphs. The $12$ Hamiltonian paths are those connected graphs over $4$ vertices whose complements are also connect: thus the remaining $2^6 - 12 = 52$ graphs are divided into pairs of complement graphs which are connected and disconnected, yielding a total of $26$ disconnected graphs, and $26+12=38$ connected graphs over the set of $64$ labeled graphs over $4$ labeled vertices. The path graphs of length $n$ on the set of $n$ vertices are the canonical example of connected graphs whose complements are also connected graphs (for $n>3$). 

However, the binary matrix for the undirected graph "in the same cluster" using distance $d<50$ as the threshold results in 

The best model depends upon the hardware being considered. I agree with Joel David Hamkin's answer and with Mark Sapir's answer. The point of computability models, such as Turing machines, regular languages, push-down automata with stacks, etc., is to show the equivalence of these models. THe reason for the big-O notation order of complexity is to show that, within a small additive constant, complexity can be defined in terms of a linear, polynomial, or exponential (or otherish) relation to a particular characteristic factor of the input (usually the size of the input). The only thing that changes for that computation on different systems is a multiplicative factor, or the additive constant. Kolmogorov talks about quantitating this type of complexity based on abstract state machines, and coming up with a minimal-descriptor length type of explanation of complexity, and prefix-complexity. Complexity can refer to 

This is essentially ordering the lattice points intersected by a plane in three space in the all-positive-or-zero octant where the sum of the $x,y,z$-coordinates is $n$. (I may be mistaken but your example for n=1,2,3 show the values for n=0,1,2, and your example for n=4 does show the answer for 4, so I think you're off by one for part of your example. And you use the word permutation, but your description of the problem is more aptly states as combination of three integers.) This is essentially a simple geometric problem in 3-space over the integers, unless I am misunderstanding your question. If you look at it geometrically, you are looking at the points on the lattice $\mathbb {Z}^3$ and finding the points on the plane $x+y+z=n$ in the all positive-or-zero-octant $x\ge0, y\ge0, z\ge0$. You can then order the lattice points which satisfy these constraints ordinally in whatever order you prefer, say numerically with $x$-coordinate taking precedence over $y$ taking precedence over $z$. In this case, it is easy to read off the coordinates of the triangle in the all-positive octant of 3-space. 

What do you have to lose by submitting an article for publication? You'll have an even better record/credentialing/verification of the work you've put into it by being published in a Journal of good reputation. In the worst case, you will get a rejection letter, perhaps with a good explanation of why they are rejecting the article. The in-between case is pretty good too: you'll receive a referee report which may criticize your approach, suggest particular points to be polished and corrected, perhaps suggest a different approach to take, perhaps suggest that some of this work has been done by others who you shoud read and study or perhaps refer to in your work. If you're lucky enough to be asked to rewrite and resubmit for consideration, you're possibly on your way to being published. If not, you'll at least have made some progress and educated yourself about the academic publication culture, and will be more prepared for the next article which you prepare for submission. May I recommend that you find out what your activation energy level is, exceed it, and go ahead and clean up your paper and submit it for publication. Best wishes and good luck. Go for it! Would you mind sharing the arxiv link to your work? 

If it is possible to separate the elements of your set $S$ into disjunct partitions, there is a minimum distance $d_{min}$ that you can use. For example, setting the distance threshold to $40$ allows the partitioning of $S$ into disjunct (non-intersecting) subsets: 

Any periodic function which contains a scaled or translated version of its own derivative, for example sine or cosine , or any finite or infinite sum of multiple periodic functions which also yields a periodic function which is its own derivative, can be expressed in the format which you are asking for. (Assuming that you are considering the sine solution you listed as a non-trivial solution) For example, for $k=1$, you can transform the domain and range of the function $y=sin(x)$ to $y=2\pi sin(2\pi x -\pi/2)$, or for arbitrary $k$, $$y=k sin(\frac{2\pi x}{k} - \pi/2)$$ You can create a similar function for cosine by adding a different phase shift to the domain. So for arbitary $k$, you could use sine or cosine with the domain scaled and translated and the range scaled as necessary to get a solution of the format you'd like. $$ y = c g(\frac{a x}{k}+b) $$ If you are looking for a non-periodic trivial solution, then it's a different story and answer, delayed differential equations, as pointed out by Denis Serre above. 

Note that I have broken up the three steps into a few extra substeps to ease in the understanding of the visualization. Also note that the in toto transformation animation consists of a concatenation of multiple piece-wise linear sub-transformations. I think that is the best way to visualize or animate it. Attempting to create a single flowing transformation will falsely blur together the distinction of some of the topological procedures, in my opinion. A similar problem arises in graphical animation morphing: what is the transform of a square $ABCD$ onto the same square mapping $A \to B, B \to C, C \to D$, and $D \to A$? 

Alex, don't feel as if the weight of the burden of proof (of concept generalization) has to rest completely on your shoulders. I realize you already agree that curiousity and your own interest can be enough reason to pursue a topic or generalization, but... Isn't it the same as asking a question on mathoverflow about a topic which is interesting to you on its own merits, and finding out about the existence of either a longer history of it based on a parallel set of definitions or other possible applications of it in other branches of mathematics or physics? I had been working on a particular topic, but having approached it from one direction I could only perceive the question from my point of view. Even my attempts to research it found nothing initially because I was using the wrong key-words to look for similar work on my topic. It turned out that there was a long history of work on the topic using different terminology which I had not been aware of. Perhaps giving a short summary on mathoverflow (as a different question) of the generalization which you are working on would provide you some different points of view from other mathematicians. As to the utility of a generalization or of a particular approach, it is not possible to predict or find all of, many of, or even more than a few of, the possible applications of a mathematical technique on your own because you cannot survey the entirety of it yourself. It's often the intersection of multiple disparate interests that creates the application of a technique onto a problem, and every individual (and every individual mathematician) has a different set of disparate interests. (As long as the number of categories of possible interests is greater than the logarithm in base two of the size of the population under consideration; otherwise the pigeonhole principle requires that there must be at least two individuals with exactly the same interests. :) ) 

And the 3-d probability distribution at time step $t$ is the $t$-th convolution of $M_3 \div 6$ with itself. If you try a few steps of the $3-d$ convolution, you'll see that the probability density at the center quickly goes to zero. Once the number of dimensions is greater than $2$, the unbiased random walker is more likely to move further away in other dimensions where it's closer to the origin, rather than get closer to the origin in the dimensions where it's already further away. 

I wonder if the software code and hardware is vetted as well there as it is in Las Vegas by the Nevada Gaming Commision which oversees gambling and the electronic machinery for slots and electronic poker, etc. 

Perhaps I'm not following what you're asking perfectly, but if you have $N$ points, and a distance matrix which is $N \times N$ in size, you could use an $N$-dimensional hypercube. This hypercube would have the $N$ points at the $N$ vertices defined by the vectors $P_1=(1,0,...,0)$, $P_2=(0,1,0,...,0)$, ..., $P_N=(0,0,...,0,1)$. Thus the $N$ points all exist at Hamming distance $=1$ from the origin (0,0,...,0). In this embedding, all of the $N$ points are at the vertices of the $N$-dimenional unit hypercube. In the unit hypercube embedding, the Hamming distance is between each pair of points is $2$, and the Minkowski distance between each pair of points if $\sqrt(2)$ The pair-wise distances which you already have as a given are used to define the length of the distance between two vertices and thus give the separation between each pair of points. Of course, if the distances are not in a Euclidean space, you can't use a Minkowski metric to define the distances. If you want to have this be a euclidean space, then you can use different techniques to get the appropriate desired pair-wise Euclidean distance, $$d_{a,b}={(\sum_{i=1}^{i=N}} (a_i-b_i)^2) ^{0.5}$$ or you can use whatever other metric may be appropriate in your case. For the euclidean distance, you can set each point to be at a distance $d_n, 1 \le n \le N$, then calculate the pairwise distances for all of the data points and try to move the points around in order to get closer to approximating your original distance metric. Thus now you would have each point $P_n, 1\le n \le N$ at $(v_1,v_2,...,v_N)$ where $v_i=0$ for $i \ne n$, and $v_i=d_i$ for $i=n$. This embedding now places each of the points at a vertex of a hyper-rectangle in $N$-dimensional space, rather than in the unit hypercube. You could use an annealing method or a genetic algorithm with multiple candidates to mutate and cross over, or try to move one point at a time to optimize that point's pair-wise distance to all of the other points. What is the order of magnitude of your data set's $N$?