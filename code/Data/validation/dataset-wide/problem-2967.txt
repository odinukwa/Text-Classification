University research is not a guarantee for accuracy of theory. Any theory can get to be widely accepted and then something else comes in that invalidates the initial theory. It's how science evolves. Given this and how today's science and specially physics tends to work, I'd rather trust an independent research of a small group of scientists than that of an university's research. The logic is most of the time university projects are bound to comply by the very restrictive already established theories (there are exceptions, but they do not form a majority) while independent research can practically dig in any direction unrestricted. So far this approach worked very well for my research teams. 

The above is a 4D object represented on 2D. Quite the loss of visual information here. A 3D holographic projection could offer way more clues as to how a hypercube may look like. In that case. you would a 4D object represented on 3D. Much more visual info to get from that. But what humans lack in this case is the initial image of how the 4D hypercube looks in 4D. Due to that, there is no end point for the extrapolation to work, so in most cases humans will not be able to extrapolate at all a 4D object. 

I think this is not about randomness vs objectiveness but rather randomness vs nothing being actually random. Objectivity has it's own opposite: relativity. Because of relativity we have big problems determining what's what, therefore objectivity is recommended specially when dealing with advanced physics, universal laws and such things. 1+1 will always equal 2 only if the correct rules are applied, just as the author said. Otherwise, 1+1 = 2 is not true. As a rule-based example: 1+1 = 10 if the base rule is that we deal with a binary system. To discover if there is any true randomness somewhere in the Universe may not be that easy. For example anything natural is related to Golden Ratio and/or Fibonacci, so there's nothing random in living organisms or plants (not even at DNA level). Going from DNA level and atoms to a very large scale like galaxies, we will notice that even the stars have a non-random arrangement in galaxies. So from an Universal point of view, the vote goes against randomness (note: not to be confused with chaos). That being said, what's left to do is try to generate our own synthetic randomness, just to prove that it can be done. But in this situation, we may find that this isn't easy either. In the case of computers, it's well known that numbers generated using them aren't genuinely random. Any piece of software needs some kind of unpredictable physical input to be able to generate from there. But what about humans ? Well, most of the choices they make are based on something quite pre-determined so we don't get much randomness from there. but as we trick the computer giving it physical input, we could find a way to bypass the human pre-determined concepts so we get something random out of it. Let's design a situation. We have the drawing of a cube. Any sane mature human should understand that those lines drawn in 2D are the representation of a 3D cube. But the orientation of the cube cannot be determined from the 2D drawing, so each human would have to pick a way he thinks the cube orientation is. 

I've found what I was looking for. In response to one of the commentors, I reviewed Brian Green's The Hidden Reality which had prompted my thinking on this concept recently. I found right in the text the name I sought, complete with its own wikipedia entry: Max Tegmark's "Mathematical Universe Hypothesis". This is a great starting point for me, but I'd welcome any additions. Thanks to all for helping fine-tune my question. 

Surveillance is an ever hotter topic - surveillance of communications and/or metadata and of physical spaces (i.e. cameras). I'm strongly sympathetic to the privacy advocacy movement - not because I think I particularly have anything to hide but because (theoretically) the underdog politicians/leaders that I might support now or in the future very likely would need privacy from whatever institution they oppose. And then there are the people who work to protect those leaders and the ones who help organize those protectors and then the ones who might fund the entire outfit. Someone in power with access to surveillance materials would have an additional advantages (besides incumbency) to interfere with contenders personally, professionally, legally, etc. Still, not all mysteries can be solved by old-fashioned investigative work - some crimes will be solved by surveillance materials that would not be solved without them. And it would be the very unfortunate case that falsely-compelling surveillance evidence would lead to an honest but unjust judgement. Rather than choosing between two futures - one where surveillance is prevalent and available for review by government powers, and the other where government surveillance is minimal (or owned privately at direct private expense) and those crimes go unsolved - is there a practical middle ground where the responsibility to provide surveillance infrastructure is isolated from the ability to review those recordings? Instead of CCTV cameras being operated and reviewable by the police, for example, what if the police or other civilians operate the cameras yet the camera hardware actually encrypts directly on hardware to public keys owned by the judicial branch and exposed only with merit. For phone metadata, I've heard of schemes using anonymized metadata and network analysis for useful preventive or investigative work - but then I've also heard of researchers triangulating real identities out of seemingly anonymized social networks. But again, what if the stuff got recorded into a lockbox that only the judiciary can open under a warrant (whether or not they actually physically store the stuff)? No matter what the digital evidence, criminal penalties could be severe for people or companies intercepting/redirecting/copying these confidential materials (captured by publicly owned cameras, for example). And the judiciary, of course, could also be corrupt, but ideally their whole mission and purpose and structure - if society has any hope - is to always seek and provide for justice - so while enforcers are out finding better ways to enforce, justices are focused on practices/procedures that protect rights and preserve fairness. (Also, it isn't so far-fetched to me that a judicial branch of government would be charged with hosting the secure technology infrastructure for this purpose. Every branch of government has a supporting technology infrastructure and should have a competent staff capable of running it. And the hope is that the mission and culture and precautionary measures of that organization can keep its privileged agents in line). I realize that surveillance can be used for preventing crime in a direct, anticipatory way or it can be used for catching culprits of a crime that already took place. This mechanism probably moves too slowly for the preventive kind. Though perhaps if you have enough exposed surveillance evidence and enough muscle you can deter even the most evil adversary. Is there a surveillance scheme that all sides (among the informed and benevolent) could find just, useful, sustainable, and balanced? Is there something fundamentally useless about a separation of powers scheme? Is there something logically, technologically, game-theoretically, economically infeasible about securing surveillance (in the large) from abuses? 

Here's a example: We do an experiment and get valid results. We say X causes A to happen. We do another experiment and get valid results. We say Y causes A to happen. In both cases, we have math and observations backing up our claims. But X and Y are self-exclusive (a xor if you will). So what can we do in this case ? We can do experiments of another nature and see what was actually correct. But if that's not possible, we should select the cause that actually can explain more. I encountered this in physics many times, where theories were validated both by observation and math and in the math part we had a constant "c" and a variable "v" because in this manner the observations could be explained. But re-thinking everything, one could see that in the math formulas, the same valid result is obtained if "c" varies and "v" is declared a constant. Both cannot be variable or constant in the same time because it would invalidate the math part supporting the theory. So we got 2 options leading to the same result, confirming the same theory but we do not know which is a variable and which is a constant and we have no observable/experimental way to determine this. What did in such a case was choosing the option that can explain more. In the current example, let's say if we had a constant "c" and a variable "v" we can perfectly explain how a car engine works, but cannot determine anything related to the car wheels. If "c" varies and "v" is constant and we explain how the engine works just as in the 1st theory but we can also explain how the wheels work, we select the second option as the valid one.