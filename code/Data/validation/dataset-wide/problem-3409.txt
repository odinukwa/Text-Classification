is the definition of grammar. But this is not the case. There are two kinds of grammatical theory, generative enumerative (which fit this definition), and model theoretic. Model theoretic approaches to grammar do not postulate rules which build grammatical structure, but rather constraints. 

The answer is definitely No. However, not everyone agrees. You can read Keith Chen's article on the subject. He basically claims that the way languages use or not use future markers affects the way they deal with money. But also see the replies here and here. 

In general terms, yes, ditransitives are universal as far as we know. You can take a look at this paper by Haspelmath 

A grammatical constraint is a "rule" that prohibits speakers from building a particular kind of sentence. For example, you can't say in standard English he go to school, or to he goes school, etc. What they are asking you (if I understand the question correctly) is to provide examples of these kinds of constraints that a n-gram based grammar would miss. An example that comes to mind is that of long distance relations and structural closeness. If you need specific examples I could give you a couple. 

What I've seen people do, and what I would do, is to get more annotators. Get two or three more people and have them go over the problematic items, and then make a decision based on majority consensus. 

It doesn't, not directly. Those categories are more political and cultural than any other thing. It's not because of language, but because of social factors that we say "genderqueer". So, socio political and cultural factors affect the way we use language. Was that your question? 

This is not the case outside of indo-european languages, as has been pointed out to you. The most likely candidate for the origin of the nV forms is the PIE form *ne, which was a negation. But it could also be coincidence that different families converged there. I do not think there is a good explanation as to "why" the current situation came to be like it is. 

'Generative Grammar' is an ill defined term. You will find linguists using it to mean 'Chomskian Grammar', and linguist who think it can also refer to certain construction grammars. You mention that 

I would certainly count it as a sort of evidentiality since it expresess how the speaker knows what he is communicating, ie. source of evidence. In this case you claim it expresses second hand information as apposed to first hand evidence, so yes. 

If you are not just talking about phonology, the most complete graphic system known to man at this point is 

I hesitate to call the glide-like phone in a diphthong a full fledged glide only out of tradition, even though they are seemingly identical articulations. The primary difference then is that a diphthong includes a glide as one of two constituent parts. 

As usual, WALS is a great start, explaining the different modifiers: genitives, adjectives, and relative clauses. And there are number of articles on the order of modifiers: 

But given all this, there is one thing that English is not particularly unique in and that is in being unique. Wait, that doesn't sound right. What I mean to say is that though English is certainly special in having all the above rare idiosyncracies all in one place, most languages have a number of similarly idiosyncratic facts about them that distinguish them from all other languages, just not the same ones listed above that are special to English. French has the rare nasal vowels and some adjectives that go before nouns and some after. So English is not unique in being unique. 

In the mathematical theory of communication, which is an enterprise of engineering mostly electronic, noise reduction, detection and correction of errors, robustness (to use your word) under conditions of noise, all these are very important, maybe even the most important issue in the human designing of engineering artifacts (codes) to facilitate machine communication. But, for whatever reason, all that very important research, while discussed philosophically by intellectuals in an informal manner, never really had a discipline transfer to linguists. There is surely some research into confusion of phonemes and other phenomena within particular languages, but I don't know of much into comparing different languages. This sounds similar to comparisons of language length: the size of a text that has been translated into a number of languages, and which is the shortest and which the longest. There may well be succinct and, likewise, verbose human languages but the experiment to do so will be difficult because of the bias and artifice involved in both human composition of narratives and in the translation process. Average document length is actually one possible technical metric for comparing coding mechanisms so this is not just an analogy but also one very narrow example of what you are looking for. So to your first question, no, I don't think there is any established technical literature (or much of it) that experimentally compares spoken language and content transfer under noisy circumstances. I could easily be wrong; it's just that linguistics, despite being a natural science, is not as interested in experimental methods as say psychology. To your second question, very informally I would guess that it all depends on under what context you're comparing robustness. If you are comparing yelling but/sell commands in a noisy stock market environment, one could experimentally determine which language is more accurate, but still allow that there may actually be either 1) No significant difference, or 2) a statistically significant difference that in the long run has little substantial effect. I think there are too many social variables to account for before saying that one human language is more accurate under noise than another. 

G is called G for the German Grundstamme. D is called D because of Doubling of the middle radical (/qatal/ > /qittel/). C is called C for Causativity. It is also called Å  sometimes which better reflects phonology (although in Hebrew the marking consonant became /h/). Dt is often called tD because the /t/ became a prefix rather than an infix elsewhere. Verbal plurality can be of subject, object or action, e.g. "they broke the glass" in G becomes "they shattered the glass (in many pieces) in D via object plurality, or "they repeatedly broke the glass" (action plurality) or "they all broke the glass" (subject plurality). Causativity is e.g. to cause to be seen, i.e. "to reveal". Thus you see that these distinctions are in English covered by lexicon rather than grammar (i.e., there are different verbs for different Aktionsarts, "broke"-"shattered" and "see"-"reveal"). Besides this nice symmetric system there was an N-stem, the original semantics of which are still debated but which probably had to do something with middle voice. In Central Semitic, "internal passives" are developed for the three core stems G, D and C. In Arabic grammar these are usually not treated as separate stems, but in Hebrew and other languages they are: 

I often hear people mention in passing that grammatical features are more reliable than lexical features in diachronic research, specifically when detecting pseudepigraphs, because it is relatively easy to fake old lexicon but difficult to fake old grammar (as is evidenced anecdotally by people imitating old English but mixing "thee", "thou" and "thy"). If I were to mention this in my paper, who should I cite? Where can I read more about this hypothesis? Has there been any statistical analysis on it? 

A term often used synonymously to "generalisation" is bleaching. Joan Bybee in his Cambridge Textbook in Linguistics on Language Change defines it as "a meaning change in which specific features of meaning are lost" (p. 267). 

You are implementing the Jaccard coefficient whereas the library has the Jaccard distance. The coefficient tells how related two sets are (it is high when they are similar), whereas the distance does the opposite; it is low when they are similar. In fact, they are each other's complement, i.e. d = 1-c and c = 1-d. This is also explained on the Wikipedia article you linked: 

An older version of this answer just used the qtree package (instead of tikz-qtree) and draws something that looks like your tree. The only trick we need is to use and , because otherwise the bar over the T is touched by the edge. The differences with the original: