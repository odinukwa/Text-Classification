This might be your last resort to retrieving the information from your files in your broken MySQL instance. 

If you follow the packages you will find addtional dependencies. The postgesql-9.3 package for example depends on: 

The documentation for MySQL has the following bit of information regarding the EXPLAIN Output Format: 

You will receive a list of SQL Server logins for your SQL Server instance. Note: These are the SQL Server Logins and not the database users. Database Users Database Users can be queried by querying the system catalog view of each user database by issuing the following query: 

Could this be the issue? Is there a version of SQL Serer out there that has an issue with more than 2048 bytes in the comment? 

You shouldn't encounter any issues any more. (Or at least until your java application consumes all the processes again). 

Reference: What is the difference between Rollback and Undo Tablespace?, section Undo vs. Rollback Advisor Framework If you are running in Undo Segements mode then you can retrieve advisory information about the Undo configuration from the system with the following script (Advisor Framework): 

In the SQL Server Configuration Manager open up the branch for SQL Server-network configuration and select your instance. (E.g. ) Right click and open up the properties. Verify that you don't have any settings that could inhibit a connection. Close the setting when you have finished. In the right hand pane for the protocol settings verify that the TCP/IP protocol is . Now right click the TCP/IP setting and open up the properties. In the Protocol tab... a) Verify again that is set to . b) Verify that the setting is set to . c) The screen should look like this: 

The trick is to avoid the orphaned logins in the first place, not to synchronize them afterwards. As you probably know, 'orphaned' users are due to the SID associated with the user in the database not matching the SID associated with the login on the server. When a login is created on a server, the value for the SID is either taken from Active Directory (if you are using "integrated" or "windows" logins) or generated by SQL Server (in the case of SQL logins). With SQL Server logins, the value of the SID is generated by SQL Server. The value will vary each time you create a login [hint] unless you take special care. IOW, if you create a login named "foo" on two servers, each login will have a different SID value. If you create a login named "foo", drop it and create it again, each occurrence of "foo" will have a different SID value. Strategy #1: use integrated security With Integrated security, the SID used by SQL Server for the login on the server and the user in the database is taken from the Active Directory login. Because of that, the values can't differ. Changing from SQL logins to integrated logins is usually a hassle, developers generally don't like to spend time on infrastructure work and it's probably a non-starter. Strategy #2: use the "SID" parameter when creating logins on the destination server. If you create a login using TSQL code, you can specify a specific value for a SID. You can construct a set of CREATE LOGIN statements (including SIDs and exported passwords) so that the SIDs match on both servers. That way, when you restore the database (via shipped log backups or any other way), the SIDs will match and people can get to the data. Microsoft provides code that will help you create these CREATE LOGIN statements based on what exists in the source server. You can then copypaste those statements over to the destination server and run them. The name of the procedure is sp_help_revlogin. The code varies slightly between SQL Server versions, but the version for SQL2008R2 is available from Microsoft here TLDR: I'd recommend using sp_help_revlogin on the source server to create new CREATE LOGIN statements for the logins that have to sync up. (You don't need to do every login, just the ones that have problems.) Then, use those statements to drop and recreate the logins on the destination server. Since you will be dropping logins, you should be aware of any database permissions that those logins already have (maybe there are other databases on the destination server that the developers already have permissions in?) and you should be able to recreate those permissions after you get the login recreated. 

Restore the most recent backups of the system databases: , , (): When restoring the database start the SQL Server instance with the Trace Flag 3608: 

Reference: SQL Server Configuration Manager (Microsoft Docs) Actual Steps To Perform In your case you might have performed several of the steps, but not in the correct order. Try the following: 

Reference: Snapshot Backups (Microsoft Technet) A backup created using this feature can also be restored almost instantaneously. Summary The 3rd-party backups should be marked as and . These backups will not conflict with additional backup steps/procedures performed using native SQL Server , and statements. The 3rd-party database backups are not part of an existing backup set. Answering your questions The vendor stated correctly, that during the (quick) snapshot backup, that other backups should not run. 

The sys.database_files view The table as pointed out by SQL_Underworld is only for the database you are currently connected to. Attaching database files as database You could attach the database files you have found on your disk with the following command: 

You can't. SQL Server Profiler is a bit restricted in the way it displays and filters data. Solution You would be better off storing the traced data into a separate table on the same server and then querying the data at given intervals. The big benefit being you have the data in the table and can perform any complex statement you wish. Ensure you don't capture too much data by specifying one or two filter criteria. You could also consider running the SQL Server Profiler from the command prompt which will reduce the overhead of displaying the data constantly in an interface. The parameters are: 

There are third party products, such as SQL Lightspeed, that provide backup compression for Standard editions of SQL Server. There is a good chance that the purchase and installation of such a product will allow you to back up locally. The size of the nacho file depend on how much data is actually stored in the database and it's compress ability. 

A small chunk should complete in a reasonable time. You may find that chunks of the same size can take wildly different times to complete. (Some might take a few seconds, others may take minutes.) Normally, until I get a good handle on how things actually behave, I will run a couple of these at a time. Once I have an understanding of how it behaves, I would wrap it up in a template script that I can reuse. As you know, SHRINKFILE can cause index fragmentation. You should plan for reindexing before letting the developers into the database. If you regularly move databases from prod to test, it should be possible to come up with a script that handles the shrinking and reindexing. I suggest you have a look at Ola Hallengren's reindexing stored procedure, which is very popular with DBAs. 

UPDATE STATISTICS does not have any sort of internal parallelism. It does not matter if you are running with either FULLSCAN or SAMPLING. Of course, you can run several UPDATE STATISTICS commands at once, each on a different connection, through multiple SQL Agent jobs or some other contrivance. Depending on your exact situation with hardware and data, you may find that simply reindexing the tables is faster than UPDATE STATISTICS with FULLSCAN and possibly a better option. 

No, database mirroring is not dependent on the SQL Server Agent running. There is however a job that is scheduled to run every 1 minute to update the monitoring status of the mirrored databases. This is all it does: 

I'm not in a hurry and I do have some time to spend. I'm just curious if anybody has been in the same situation as myself and how you/they came to deal with the situation. Thanks for your time. 

In General I would consider reading up on what information the master database contains and why you need to backup the system databases (master, msdb, ...). Reference: Back Up and Restore of System Databases (SQL Server) (Microsoft Docs) master Database The master database contains the following information for each SQL Server instance: 

This is an issue that wasn't accounted for in the "Requirements Engineering" phase of the project. It shouldn't be looked at as an issue of the database system, because the database is performing as it should do. The mail is sent because it is not (yet) part of the correct business logic. It is called a Business Logic Flaw or possibly even a Business Logic Issue. Business Logic 

Because it is Microsoft Access, I would recommend sticking with the supported options rather than trying another solution that might or might NOT work. 

I think that the deadlock is due to the engine acquiring row locks on each of those itemid values, one at at time. Each query is working on an unsorted list of itemid values and acquires those locks in a different order, allowing them to get intertwined and, eventually, each query winds up with a lock the other query already has and has a lock the other query wants. The other thing is that SQL might be trying to promote the locks to a table lock. SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED on the connection that SELECTS might help, or you might get rid of the deadlock by dropping the locking granularity to page or even table. 

The "bitness" of your workstation should not prevent you from performing a task on a SQL Server, regardless of what the "bitness" of the server is. I've used 32 bit workstations (and therefore 32 bit SSMS installations) to administer 64 bit servers for something like twelve years. I would suspect that SSMS 2005 might have problems when administering SQL 2012. I would want to upgrade my workstation's SSMS to the same version as whatever the newest server that I have to administer is. SSMS compatibility usually works "backwards", meaning that SSMS 2012 can handle SQL Server 2005, but not the other way around. 

If you go ahead and install the AdventureWorksDW database from Github and have a look at the two tables in question you will find the following: Query 

The depicted slide is showing you the situation as it is at the time when the vertical axis is at the checkpoint. 

Moving even further forward (in the reference) the examined time (vertical axis) moves forward and reaches the Failure. 

If the instance can start with the pfile then you can create an SPFILE from the PFILE with the following command: 

Ola Hallengren's backup script does not support backing up a database and/or transaction log to a device/directory. You might want to ask him to add this feature in a future release. Both and result in the following error message: 

Size column in sys.master_files There is a delay in the size reported in sys.master_files according to the MSDN article sys.master_files (Transact-SQL) 

Objects created The solution is deployed as one script named , which itself will create stored procedures, tables and jobs . Following is a list of objects that are created: Table 

Summary Ola's script works as designed. We have verified that a and an additional will work together. The fail-safe feature ensures you can restore your database with the last full backup (be it COPY_ONLY or not) and the available Transaction Log backups. (Please read Part II of II for more answers) 

If all you need is a key/value store, you might want to look at some of the NOSQL options as an RDBMS may be overkill. If you stay with SQL Server, you should investigate the "table partitions" feature, which can help organize very large tables, and table/data compression feature, which can reduce the amount of space that data takes on disk and in memory. 

Within reason (I mean "less than thousands", and you are no where near that) the number of databases should not affect the performance of the server. I have seen people claim to run thousands of databases on a single instance of SQL Server, and the performance of the user databases hasn't been a problem. (On the other hand, looking at those databases with SSMS is a problem, as is trying to manually manage all of them.) Very roughly speaking the performance of the server is governed by the amount of data that is accessed in those databases and how much memory the server has. Assuming that you have enough disk space to store both databases and that you never access the "test" database, the production database performance should be unaffected. If you "sometimes" access the test database, the performance of the server could be affected at those times. You can mitigate that by testing when the server isn't busy, like in the evenings or on weekends. All of the data in a database might not be regularly accessed. It is very common to have old records, "archives" and other sorts of stuff which remains in the production database by isn't regularly returned by queries on a day-to-day basis. In many databases, this "cold" data might be much larger than the amount of "hot" data that is regularly accessed. If the amount of hot data is very small, you might not notice any performance problems when using the "testing" database even when the production database is being used full-blast. To make things a bit more complex, poorly designed queries and database tables might be inefficient, and return data that really should be "cold", or cause table scans. In short, if the server can hold all of the hot data (from production and testing) in RAM, then you will probably be ok, or at least not much worse off than you are with only the production database in use. Back to your direct question: Will adding more load be a problem? The first thing to do would be to look at the performance of the server now. Do you have performance problems? Are those problems caused by a lack of RAM, a lack of disk I/O capacity or a lack of cpu power? There are many, many guides on the internet and StackExchange to help diagnose where your currently bottlenecks are.