Then I join it to my table and add a rownumber based on the priority: So here's the output of my subselect (or derived query) named "res": 

The problem I have got an issue setting up Management Data Warehouse on SQL Server 2017 CU 5. The job "collection_set_1_noncached_collect_and_upload" keeps failing. It is related to the "Disk Usage" collection set. Error Messages are the following (I highlighted the part which is most relevant IMHO): 

So nothing of this really worked. Could you think of an alternative how to keep the trigger doing its job but also have the data collector work properly? Resources Here's the original sourcecode of the database level trigger: 

Eventually I have found a solution to my problem. As on a StackOverflow question described your can hack you way around the problem using . So I added these two lines to my code (within the IF clause and before doing other things) and got things to work: 

The procedure "AUDIT_TO_OTHER_FG" is a database level trigger. Its purpose is to put audit tables (with history data) into another filegroup. Our Java Application running on top of the database is using Hibernate and doesn't bother specifying filegroups. However all of these audit tables follow a certain naming convention. Thus the trigger fires at a CREATE_TABLE event, rolls back the table creation and creates the table again on a different filegroup. Maybe this is not the most elegant version to put the tables onto a different than the default filegroup...however it has worked fine in the past and has never been a problem until now. I had Management Data Warehouse data collectors set up before for that environment as it was running on SQL Server 2008. There haven't been any problem regarding these triggers in that version. Recently we moved to SQL Server 2017 and now I am experiencing these issues. I dropped the trigger temporarily and the data collector worked fine. So somehow it appears to be interfering with the actions of the data collector and the problem is the dynamic SQL used. However I do not get why this causes a problem as the data collector seems not to create any table in my user databases and the trigger doesn't fire while the data collector is run. Workarounds tried I have read a bit into "WITH RESULT SETS" and tried to change my trigger as follows: 

I admit that this solution is not pretty and presumably it won't work if another dynamic sql comes around with another resultset...however it works for me now. Martin 

as fas as I understand you would like to control the order of execution if x is not present search for y and so on. Therefore a simple IN clause or the case expressions probably won't work if you've got entries for x as well as for y and z....by result returning multiple rows. The best way I can think of would be to introduce a priority factor within a cte and use this to return only the relevant row. Here is an example. For simplicity I use a table with the following structure 

By the way: Having a time as datatype varchar(10) sounds a little bit odd. Datetime would seem to be more fitting. Have a look here for advice. 

Is it possible to restore a differential backup separately? The production server runs the following every sunday 

So I have looked up many other people trying to get Transaction Replication Without Snapshot working and I'm stuck. I've followed what everyone has said so I don't know what I'm missing. Prod server and Report server. On the prod server I configured the Distributor from the UI. Then I setup a transnational publication from the UI. I made sure not to check either snapshot options. Created the publication. Changed 'Allowed initialization from backup file' to true in the Subscription options in the UI. Now I create a full backup of the database on the prod server and restore it to the report server. I disable all triggers and constraints on the report server. Now also on the prod server I want to create the subscription for the report server. I choose to run all agents at the Distributor. I choose 'Add Subscriber' and point it to the report server. Agent Schedule is set to Location: Distributor and Schedule: Run continuously Subscription Properties are set as follows Initialize: checked Initialize When: At first synchronization I generate the script file and edit it as below 

Log Reader Agent Status once in awhile says 'x transaction(s) with x command(s) were delivered' Now looking at the logs this morning I see the following error 'Cannot insert an explicit value into a timestamp columns. Use INSERT with a column list to exclude the timestamp column, or a default into the timestamp column' Now I have read some posts saying this is because the row timestamp column. I have 1,000's of tables. If this is the issue what is the fastest way to change correct it? Also after the Synchronization status errors and starts back up, Why does it always say Initializing? This alone takes over an hour before it errors out again. Update I found in article properties that I can change timestamp to binary(8). Do I need to take another backup and restore it after this change? How will this effect the production server application along with the reporting application? 

user defined datatypes, i'm looking at some execution plans and seeing implicit conversions happening in the background when a var is declared of type user defined data type and pulling a value into that var from a column that uses that user defined data type, it's showing it as an implicit conversion. Is this how it functions normally? Added ok so under Programmability -> Types -> User-Defined Data Types: there is 

New test I can make an implicit conversion happen by doing the following reasonforcreatetype is char(1) in the database 

Now I create a new table with a char(1) and a bool. Run the same query and I don't get the implicit conversion. Update 2 Ok when adding a new column under the same table having the issue I get the following 

I keep getting the following error The log or differential backup cannot be restored because no files are ready to rollforward. Anyone help explain what i'm doing wrong? UPDATE I had to use WITH MOVE because the paths are different 

I'm assuming this has something to do with it now but I'm not sure the proper way to fix or correct these issues. 

I have still got all the rows but assigned a rownumber based on my priority. Then the last thing I have to do is to filter only the most relevant rows which are the one having row number 1 . Presto...here's the result of the whole query: 

Well the problem is that your CASE statement is ambigious. If the value is @p or @p1 both case when expressions will be true. In that case just the first one hit (returns 1) is going to be evaluated. There are two alternative ways I could think of: 

As you described we'd like to return X first, then possibly Y (if no X present) and at last Z and only one row per student if the applicable subject code. XYZ is easy because it is alphabetical but let's introduce some additional complexity guessing that there could be also an A which we would like to return if there is no X. Here is a SQL Fiddle with my example I would use the following query to achieve this: 

First of all thank you guys for helping me to get on track with your comments. I have now worked through an example and have a better understanding what's going on. The problem arises with moving LOB-Data (such as VARCHAR(MAX), XML and so on) to another filegroup. When you rebuild a clustered index on another filegroup the LOB-Data stays at it's former place (set by the command in the CREATE TABLE statement). One classic way to move the LOB-Data is to create a second table with the same structure in the new filegroup, copy data over, drop the old table and rename the new one. However this brings in all sorts of possible issues like lost data, invalidated data (because of missing check constraints) and error handling is quite tough. I have done this for one table in the past but IMHO it doesn't scale well and consider the nightmare of having to transfer 100 tables and you got errors for table 15, 33, 88 and 99 to fix. Therefore I use a well-known trick regarding partitioning: As described by Kimberly Tripp LOB-Data does move to the new filegroup when you put partitioning on it. As I do not plan to use partitioning in the long run but just as a helper for moving that LOBs, the partition scheme is quite dull (throwing all partitions into one filegroup): I don't even care, which partition the data is on as I just want to get them moved. Actually this technique and the implementation is not invented by myself...I use a formidable script by Mark White. My mistake was to not fully understand what this script does and what the implications are....which I have now: For LOB-Data it is necessary to rebuild (or recreate) the table (mostly the clustered index) twice: first with putting partitioning on it and second with removing the partitioning. Whether you use or not this results in having to provide the space of the original data TWICE: if your original table has 100MB, you need to provide 200MB for the operation to succeed. At the beginning I was quite puzzled, ending up with my new data files which had a lot of free space after the operation was finished. Now I accepted that I can't cheat around avoiding the free space. However I could avoid the necessity to shrink files afterwards. Therefore my solution is to do the first rebuild on a temporary filegroup and the second rebuild (removing partioning) on the destination filegroup. The temporary filegroup can be removed afterwards (if hopefully I don't hit the error message "The filegroup cannot be removed" (have a look at my question here) anymore. Thanks for reading and your help Martin Here is a repro script for my problem which includes the solution I have come up for it: