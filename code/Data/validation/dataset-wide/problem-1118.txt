show you? If you have 26 extents, you should see that extents 0-15 are 64 kb in size and extents 16-25 are 1 MB in size. 

for example, would tell Oracle that the current session is 8 hours before GMT (currently the Pacific time zone), if the data is stored in a column, you'd need something like 

If the query plan involves a full table scan, Oracle has to read every block from the table up to its high water mark (HWM). If there are 800 MB of blocks below the HWM, it would make perfect sense that it would take 30 seconds to read all that data. The number of blocks that actually have data in them, in this case, is irrelevant. If you truncated the table, however, you should have reset the HWM unless you specified which is not the default. The fact that the table is still 800 MB implies that you either explicitly had a in your command or that you didn't actually truncate the table-- perhaps you just did a to remove all the data, for example. A will not reset the HWM. 

Why would you want to index if you're already partitioning on ? If you are regularly querying the data looking for date ranges much smaller than your partition grain, that is, you're regularly querying for date ranges of a couple hours, you probably want to adjust your partitioning strategy to create partitions more frequently. If you query recent data in smaller intervals than old data-- for example, you regularly aggregate by hour over the past day, aggregate by day over the past week, and aggregate by month over the past year-- you may want to merge smaller partitions together as they age (potentially in addition to compressing older partitions). Additionally, are you sure that you would want to index rather than creating a materialized view that pre-aggregates the data by some smaller interval than the partition? If you're aggregating data in various queries by hour, day, and month, for example, you'd generally be better served by creating a materialized view that pre-aggregated the data at the smallest grain (hour) and an Oracle dimension object that allowed query rewrite to use the materialized view to aggregate the hourly rows into daily or monthly results rather than trying to read all the data from the base table. 

Read a row from Evaluate the expression If , evaluate the Assign the to the row that has been returned. Since no other rows have satisfied all the criteria yet, the will be 1 Evaluate the predicate. If is anything other than 1, the row is rejected Go back to step 1 and read the next row 

Changing the size of your redo logs won't affect how much space is occupied by archived redo logs. If you reduce the size of your redo logs from 200 MB to 100 MB, you'll end up with twice as many archived redo logs each of which is half the size. The volume of archived logs that you generate is dependent on the amount of your system generates not on the size of your redo logs. In general, you want the redo logs to be large enough that you are not constantly swapping logs (generally not more than every 15 or 20 minutes) and small enough that you aren't risking too much data should there be a catastrophic server failure (assuming that archived redo logs are written to a different server and that you aren't using something like DataGuard to replicate the redo to a backup server in real time). 

One of the biggest benefit of using a materialized view is that Oracle takes care of keeping the data in sync. If you have a separate aggregate table, you are responsible for keeping the data synchronized. That generally requires a reasonable amount of code and a decent amount of testing and most organizations manage to make mistakes that leave holes that cause the aggregate table to get out of sync. This is particularly true when you try to implement incremental refreshes of the aggregate table. Another major benefit is that, depending on the settings, Oracle can use query rewrite to use materialized views when users issue queries against base tables. So, for example, if you have a bunch of existing reports against a detail table that produce daily, monthly, and yearly aggregate results, you can create a materialized view on the base table that aggregates the data at a daily level and the optimizer can utilize that materialized view for all your existing queries. This makes it much easier to optimize reporting workloads in a data warehouse without trying to go and rewrite dozens of reports to use your new aggregate table or to mess with to force your own rewrites of the queries. 

where is 11 and is 20, Oracle would fetch the first row, give it a of 1, then discard it because it didn't satisfy the predicate. It would then fetch the second row from the inner query, give it a of 1 (since no rows have been successfully returned yet), then discard it because it didn't satisfy the predicate. That would repeat until every row had been fetched, assigned a of 1, and discarded. So the query would return 0 rows. The extra layer of nesting ensures that the outer query with sees rows with values between 1 and and that the full query returns the expected set of rows. If you're using 12.1 or later, you can use the syntax as well 

The page you linked to is, quite simply, wrong. It makes no sense for a ZIP code to be a primary key in an address table. You would expect to have multiple addresses in the same ZIP code, as you said. ZIP codes cross city, county, and state lines which makes it uniquely poor as a key (postal codes in other countries may not be as problematic but data modelers always have to worry about the worst case). Plus, ZIP codes change over time for an address which is not something you want in your primary keys. In general, given that addresses tend not to have anything that works as a natural primary key, your address table will need a synthetic primary key () that has no meaning and simply acts as a primary key. Databases have different ways to generate synthetic primary keys, sequences and auto-incrementing columns are common approaches. 

This lets me indicate that a particular retailer offered a particular product for sale at a particular price for the period between the two timestamps. If the product is sold at the same price for months at a time, I only need one row in the table for that. If I want to change the price of the product on a minute-by-minute basis, I can do that as well. If you are capturing prices rather than controlling price changes, it might make sense to write 1 row to the pricing table for every product for every retailer for every day (or whatever frequency you are polling for pricing on). If you were building a price comparison site, you'll likely want a different data model than if you are building an Amazon site that allows different retailers to sell goods and manage their own pricing. If you're intending to poll the actual source of truth once a day and want to have a row for every product for every retailer for every day, something like this would be reasonable. 

If you have a simple b-tree index on , then yes, you would need to avoid calling functions on that column in order to be able to use the index to filter rows. In this case, it would seem to make much more sense to convert your numeric literals to timestamps than to do the reverse 

Then, you'll need to go through each child table dropping the old foreign key constraint and creating the new constraint, i.e. 

From a performance standpoint, it is unlikely that there will be a meaningful difference between using one, two, or three subqueries. If you are going to multiple sets together, since you know there are no overlaps, you would want to use a rather than a to avoid an unnecessary sort and ( has to remove duplicates, does not). 

Depending on the edition (enterprise, standard, express, etc.), the licensed options (particularly the Performance and Tuning Pack), and whether you have installed Statspack, I would start by taking an AWR or Statspack report for both system when they are executing a similar workload and compare them. If you are licensed to use the AWR (note that querying the AWR tables violates your license agreement if you aren't), use an AWR report. Otherwise, assuming you (or the prior DBA) installed Statspack, use that. Otherwise, you'll have to install Statspack in both systems. If your guess is correct, I/O is the bottleneck, and the problem is that the I/O subsystem on the new server is slower than the I/O subsystem on the old server, you would expect that the top wait events in the report on the new server would be primarily I/O, that the amount of I/O would be consistent across the reports, and that you'd see that individual I/O operations on the various tablespaces and data files were consistently slower on the new system. Otherwise, comparing the reports should tell you what the bottleneck is (though an AWR report has a ton of information and a comprehensive discussion of interpreting an AWR report is a book not a forum post). 

A fast-refreshable materialized view cannot contain a non-deterministic function like . So if you want to materialize the data from the last 24 hours in a materialized view, the materialized view would need to do a complete refresh every time. Do you need a materialized view? Could you maintain your own staging table and create a custom job that runs every few minutes, deletes the data that is now more than 24 hours old, and inserts the new data (either by directly querying the table or creating a trigger that writes the new data to a different table)? That's likely a non-starter if you need to use the query rewrite functionality of a materialized view but if you just want the benefit of having a smaller table to query, custom code may be more efficient. Or could you partition the table by day so that queries for the past 24 hours always just have to hit the two most recent partitions? 

I certainly wouldn't want to include the server name in the TNS alias. It would seem highly probably that this would change over time as databases move from one server to another and as organizations move to things like RAC where there would be multiple servers. Assuming that your service names are chosen meaningfully, I would expect that the service name would match the TNS alias since both are logical names for the same thing. That's not a hard and fast rule, of course. Some organizations may have reasons to have two separate logical names for a single service. For example, you may want a single TNS alias that points to the service for users in the US and the service for users in France. But for the vast majority of situations, if the TNS alias would be , the service name ought to be as well. 

It appears that something like 95% of your wait time is on CPU. Even if you completely eliminated the direct path reads, you'd eliminate less than 2% of your total wait time. It seems unlikely that a 2% reduction in your wait time would even register as an improvement. Taking a look at what is consuming all that CPU-- you have a handful of very CPU intensive queries. 0dp0c8wux0jnt and 3g2kw67v3v0qs are consuming 60% of the CPU your system is using. Have you looked at optimizing those queries so that they use less CPU? 3g2kw67v3v0qs in particular takes nearly 5 seconds per execution. What else do you have running on this machine? A total SGA & PGA that is 6% of the memory available on the machine implies that you probably have a number of other things running (perhaps many database instances). 

The problem is one of name resolution, not privileges. The table exists in some schema other than the schema. When runs the query 

It is perfectly valid to increase the SGA_MAX_SIZE without changing the SGA_TARGET though it probably isn't going to change your performance. Setting the SGA_MAX_SIZE to a larger value allows you to adjust the SGA_TARGET upward without restarting the database. It won't affect the actual amount of RAM allocated to the SGA, that's controlled by the SGA_TARGET. If your intention was to increase the size of the SGA from 2 GB to 4 GB, you need to adjust the SGA_TARGET. 

In response to Vincent's comment-- A does adjust automatically to the session's time zone regardless of the server's time zone. As I adjust my session's time zone, the results change as well. 

should correctly invoke your procedure. Second, you have issues with the naming of your local variables. Normally, you would not create local variables like and that are the same as the names of columns in tables in your database. That makes it far too easy to introduce errors in your code where you intend to refer to the local variable but scope resolution rules mean that you are really referring to the column name. For example, if you write the perfectly valid function 

Oracle has no information about what value I might pass in for so it does a very generic estimate. If there are 20 distinct values, for example, it will probably guess that the query would need to access 5% of the rows in the table. If you actually execute this statement and pass in a value, on the other hand, Oracle has a lot more information-- it may know from a histogram that the value you passed in will actually require it to access 7% of the rows in the table. If the actual query plan remains unchanged, it's entirely plausible that the would increase by 40% since the expected amount of work grew by 40%. A complete list of everything that causes details from an estimated query plan to differ from details from an actual query plan and an explanation of how those things interact would be much too long for this format (particularly since a lot of the items get very complicated very quickly). There are situations where statistics on an object are missing, for example, where the optimizer has to do a random sample of the data to infer the statistics at compile time that will differ a bit every time the query is compiled. There are a number of situations where the optimizer has some sort of feedback mechanism that kicks in when a query is being run that it doesn't have when a query plan is being estimated-- it may choose a degree of parallelism based on available resources, it may change the cost of a sort depending on how much PGA space it can get, depending on the version it may be able to change course if an operation retrieves substantially more or less data than it expected. There are effects do to plans being cached or different technologies that try to ensure plan stability kicking in when queries are actually compiled. 

In general, you can move named user licenses around so long as at any given point in time you can identify the 20 humans (or non-human licensed entities) that have access to the system. In your case, so long as there is never a point where all 40 users have access to the system, it sounds like you would be OK. Of course, as with any licensing question, you would really need to talk with Oracle Sales and/or the folks that manage your licenses to get a definitive answer. Unlike technical suggestions, licensing suggestions can't really be tried out and verified in your environment before implementation. And if you get audited and it is determined that there is some issue with how you've managed your licenses, explaining that some guy on the internet told you it was OK is a career-limiting maneuver. 

If you want to return every row in if there are any rows in with an , you can eliminate the correlation using the key. But that would be very odd 

Beyond that, I completely second @BillThor's suggestions for how to create the various lookup tables. 

I would tend to be very suspicious of any set of universal best practices because, for most of these fields, the devil is in the details. Just because the information is relatively common doesn't mean that your application uses the data in exactly the same way that other applications use it. That means your data model may need to be slightly different. 

Posting the query plan for the two versions of the query (and the exact SQL for the other version just for clarity's sake) would certainly be helpful. I would guess that doing so would prove out the following theory. Without it, though, I can guess what is likely going on but I can't be sure. In general, the database is free to evaluate the elements of a query in whatever order it expects to be most efficient. That may mean that it executes the inline view first in its entirety and then applies the outer projection which would include calling the and functions. Or it may mean that it transforms the query so that the outer functions get called for every row and that outer predicates, if any, get pushed into the inline view. If you add a to the inline view, in any current version of Oracle, you prevent the optimizer from pushing logic into the inline view since the optimizer can't be sure that it won't impact the results that are returned. In theory, some version of the future optimizer might be smart enough to figure out that evaluating the and functions in the inline view wouldn't actually change the that is generated but that pushing the could change the result so some transforms would be allowed and others are rejected. But in the present, adding a acts to basically force the optimizer's hand to leave the inline view "as is". Most likely, the optimizer's estimate for the cost and the selectivity of the various function calls are off. If the function is less expensive than the optimizer expects and filters out much more data than the optimizer expects, it may decide that it is more efficient to call and on every row rather than filtering all the data first and then taking a second pass through it to call the functions and do the sorting. Ideally, you'd give the optimizer better statistics about the various functions so that it could come up with the more efficient plan without you needing to add a (particularly since that trick might suddenly stop working at some point in the future) but I've certainly been known to add a like this a time or two (along with a comment explaining why) if I needed a quick-and-dirty fix.