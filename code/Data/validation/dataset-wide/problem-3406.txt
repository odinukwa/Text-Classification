There is no good introductory textbook on dependency grammar (DG) in English that I am aware of; certainly nothing at the level of Linda Thomas' book, which is really very basic. The books linked to in the question are not appropriate: Kahane's book is a collection of essays that are not accessible to aspiring linguists. Liu's book is in Chinese. There are, however, a couple of good introductory texts on DG for those who know German, especially Eroms (2000). But unless stated otherwise, I will assume that you do not read German. There is another point that is worth noting in this area. When a syntax textbook mentions the word dependency or dependencies, it does not mean that it is a DG. Phrase structure grammars (= constituency grammars) also acknowledge dependencies, especially when they are discussing "long distance dependencies". Dependency grammars tend to use the terms dependency-based or simply dependency grammar in order to distinguish themselves from constituency grammars. Thus many of the books lower down on the page that is linked to in the question are actually constituency grammars, not DGs. Here's another interesting point that is worth acknowledging when investigating DG. All DGs are hyper-aware of the fact that they are dependency-based, whereas many constituency grammars (which is by far the majority of grammars) are ignorant of dependency-based syntax. With only a couple of exceptions (Matthews 1981, van Valin 2001, Carnie 2010), most syntax textbooks ignore DG (almost) entirely. The situation is now changing, though, especially in computational linguistics. There are, however, now a couple of accessible encyclopedia articles on DG that have just very recently appeared, both of which I have written: Routledge Encyclopedia of Syntax $URL$ HSK Handbook Syntax -- Theory and Analysis $URL$ If you are interested in taking a look at these articles, send me an email (tjo3ya@yahoo.com); I will hook you up with them. Finally, if there is a real interest in DG that develops, then time spent with Tesniere's book Elements of Structural Syntax would be a good investment. The history of modern DG begins with Tesniere's monumental work (1959). Tesniere's book remained untranslated into English for decades. But now that situation has just changed (since February 11th), since Sylvain Kahane and I translated the book to English: $URL$ I may be able to hook you up with a copy of this tome, too, if you contact me. Note that Tesniere's book begins at a rather basic level, and builds, and builds, and builds, for almost 700 pages. 

In (a'-b'), a politician can be viewed as a predicate in every sense. In (c'), in contrast, a politician is not a predicate insofar as it is not predicated of another part of the sentence. For the sake of putting names on the distinction, a politician is both a semantic and syntactic predicate in (1a-b), but it is only a semantic predicate in (c'); it is not a syntactic predicate in (c'). The terminology I have just used is unique to my understanding. What I would like to find out is whether there is already an established terminology for this distinction. Note that the distinction in predicate types is important for predicting the distrubtion of, for instance, reflexive pronouns (e.g. Susan likes our picture of herself vs. *Susan said that we like herself) and negative polarity items (e.g. The discussion of no problem did anyone find interesting vs. *Discussing no problem did anyone find interesting). The distribution of these items is sensitive to the difference in predicate types. 

A central distinction between dependency grammars (DGs) and phrase structure grammars (PSGs, also known as constituency grammars) is the understanding of the initial division of the clause. Traditionally, PSGs divide the clause into a subject NP and a predicate VP. The predicate VP corresponds to a finite VP constituent - there are also nonfinite VP constituents, of course. DGs reject this initial binary division, which means they reject the notion that finite VP exists as a constituent (but they acknowledge nonfinite VP constituents). Thus in order to evaluate the two competing approaches to syntactic structure, the empirical evidence that can be brought to bear on the (non)existence of a finite VP constituent is central. My question is as follows: What theory-neutral evidence can be produced to either verify or refute the existence of a finite VP constituent? I already have a solid opinion about this matter. I am interested, however, in learning how others respond when exposed to the question. 

the adjective drunk is predicative (i.e. non-attributive) because it appears outside of the nominal expression to which it assigns its property (he), and it is an adjunct insofar as it represents additional information that is not necessary to express a complete thought; the sentence He wrote most of his poetry can appear without drunk (versus, for instance, just **He wrote*). In the big picture, there are significant differences from one grammarian to the next in the use of these terms. There is therefore some freedom for the student of grammar and linguistics to develop a nomenclature that best fits his or her own understanding of the phenomena at hand. 

Singular demonstrative pronouns (this and that) are behaving like the definite personal pronouns; they cannot take dependents. The combination plural demonstrative pronoun + restrictive relative clause can actually be viewed as a particular construction in English and related languages. That is, it is a combination that occurs relatively frequently and has therefore been lexicalized. German has a very similar construction, e.g. 

I am not directly aware of a source that produces phrase structure rules for coordinate structures, but I can imagine a notation like the following: 

Kiyoshigaang points to the direct answer to the question. On a GB-style analysis (Government and Binding), who is in spec of CP and will is in C of CP; both move to those positions from lower in the tree. But the question references T of TP. Thus the question seems to be located in the generative tradition after about 1995, since the generative tradition did not start acknowledging TP until about that time. Producing an analysis that can be agreed upon becomes more difficult in this regard, since as the generative tradition has progressed, more and more functional categories have been added, e.g. AgrP, FocP, TopP, etc. A "correct" analysis and direct answer to the question is therefore increasingly debatable. This site is inundated by this type of parse tree question. In my view, the questions bear witness to a major problem with the state of mainstream Chomskyan syntax and modern syntax courses in linguistics in general. Students are being required to produce parses of sentences using an increasingly complex system; they are therefore understandably frustrated and thus seeking help where they might find it. But due to the complexity of the system, it is often difficult to produce coherent answers to the questions. The experts often disagree. This is of course not the answer that the question is seeking. Producing a clear answer would, though, be difficult, and drawing a modern X-bar-style tree that acknowledges TP is laborious. Perhaps this answer will, however, help generate skepticism about the value of producing such parses to begin with, in a system of syntax that is (in my view) overly complex. 

The forum linked to in the question provides the key points that answer the question. Irregular forms like those asociated with irregular verbs occur frequently in a language. They have to occur frequently because if they did not, they would disappear, becoming regular. A vivid example of this principle is provided by the strong verbs in Germanic languages (which includes English). The strong verbs are irregular in their conjugation pattern, e.g. 

The dependency parse contains 7 nodes and 6 edges, whereas the constituency parse contains 13 nodes and 12 edges. Compared to constituency, the pasimony of dependency is undeniable. This parsimony reaches through the entire theoretical apparatus. If dependency can do everything that constituency can do, Occam's Razor requires that constituency be ejected in favor of dependency. The dependency vs. constituency distinction is addressed in the following two Wikipedia articles: $URL$ and $URL$ I actually believe that constituency is necessary, but only for the analysis of coordinate structures. Otherwise, constituency makes the study of syntax more complex and complicated than it should be. My question in this area is therefore as follows: What can one do with constituency that one cannot do with dependency? Why is constituency needed? 

The first person pronoun ich is indisputably the subject in this sentence. First person pronouns trump third person NPs when it comes to choosing the subject (second person pronouns also trump third person NPs). English cannot rely on morphological markers to identify the subject, since it lacks the inflectional richness of German (and many other languages). English relies much on word order instead. There is thus a very strong tendency to take the NP that immediately precedes the finite verb to be the subject, regardless of the factors that influence subject choice in a language like German (singular vs. plural, first or second person vs. third person, definiteness). But English and German are closely related, which means the principles of subject choice in German are not entirely dead in English; they survive in a weakened state and are influencing our sense of acceptability. They are responsible for the awkwardness of the sentence 

Hence keep is unlike have of perfect aspect, which is a clearly an auxiliary. Keep is, rather, a subject-to-subject raising verb like begin (She began helping), quit (He quit eating), start (You started drinking), and stop (They stopped protesting). See the aticle on raising here: $URL$ This class of raising verbs takes the progresssive participle as complement. Each of these verbs expresses aspect: begin and start express inchoative aspect, stop and quit terminative aspect. As a class, they might all be called aspectual (subject-to-suject) raising verbs. In sum, keep is an aspectual (subject-to-subject) raising verb that expresses iterative/durative meaning. 

I have encountered the term partitive phrase used to denote such phrases. I think, however, that the best designation for phrases such as a lot of interest, two of them, a bit of time, etc. is quantifier phrase (not determiner phrase). A non-determiner quantifier (lot, two, bit) is the head of these phrases, not the/a determiner. This choice is supported by the appearance of the preposition of in each case. This preposition typically appears in English as the direct dependent of a noun. Thus since one assumes that in a phrase such as loads of interest, the noun loads is the head, it makes sense to view bit as the head in a bit of time. Concerning the impact of such quantifier phrases on the NP vs. DP debate (an issue mentioned in the comments), I think they support the traditional NP analysis more than the DP analysis. A DP-analysis of such phrases would have to view a in a lot of time as the head. An example like two of them allows the determiner to appear, e.g. the two of them. To be consistent, the DP-analysis of such phrases has to assume the presence of a null determiner if an overt determiner is absent. I do not see that there is any empirical evidence supporting the presence of a null determiner for phrases such as two of them, three of us, etc. The Wikipedia article on determiner phrases has some interesting arguments for and against the DP-analysis: $URL$ 

The present progressive participle happening and the past passive participle done are predicative expressions. The following article provides a good overview of what predicates and predicative expressions can be interpreted as being: $URL$ I think real insight about the nature of predicates comes with knowledge of predicate-argument structures. Modern theories of syntax and grammar draw a three-way distinction; they acknowledge predicates, the arguments of predicates, and the adjuncts on predicates. Consider the following example in this regard: 

The tree might be correct if one assumes a GB (Government and Binding) theoretical framework from about 1990. The tree is adhering to the standard X-bar schema quite closely, consistently positing three projections for each head (minimal, bar-level, and maximal). But there are some aspects of the tree that are unusual or not clear. The analysis of the C-C'-CP heads is strange. C' should branch downards into C and IP, but it looks instead as though C' is appearing as a second specifier of IP. Anyone who is learning/drawing these sort of tree structures should be aware of a couple of things about them. The tree appears to be assuming that all branching is binary. This is a widespread assumption in the GB/MP tradition, but there are many frameworks of syntax and grammar that reject the strictly binary branching structures (e.g. HPSG, CxG, DGs). These frameworks assume that branching can also be n-ary at times. Furthermore, a strict adherence to the X-bar schema is no longer consistent with more recent work in the MP (Minimalist Program). The MP assumes bare phrase structure instead of the X-bar schema, which means that the strict adherence to the three levels of a head is no longer necessary. My overall comment is therefore that one should not take one particular type of tree-analysis for a given sentence too seriously. The analysis will vary significantly based upon the theory of syntax one assumes.