Yes you can hide those messages in the log. In the calling session, before running the statement, issue this statement: 

I'm not yet sure if this is doable in pure SQL (probably - yes), but here is the brute force solution using cursors. This is just a (working) draft, with some twiddling and dynamic SQL you could add more parameterers, like function name. 

Constraint-based exclusion [CBE] is performed on early stage of query planning, just after the query is parsed, mapped to actual relations and rewritten. (internals, Planner/Optimizer stage) The planner cannot assume any contents of "sensor_sample" table. So unless you have values hardcoded in the query, the planner will not exclude "partitions". I guess what happens with the CTE variant... the planner is restricted because you use TABLESAMPLE and the whole subquery may be treated as volatile even if literals in the subquery are static. (that's just my guess, I'm not expert on planner code) On the bright side, the index scan with negative result is blazingly fast. (single page scan at most!) so unless you have over 10000 partitions, I would not bother. So, to answer your question directly: 

Update: Konrad corrected my misunderstanding of his question. The goal was to count queries, not transactions. How to count queries? Method 1 Use pg_stat_statements contrib. Method 2 Enable full logging of queries for a representative period of time. To enable full logging, for PostgreSQL 9.0 - 9.3, change following settings in 

You can check if the return value for the command, if 0 then OK else failure. Put may put it in shell script as follows: 

As you already mentioned documentation that you cannot use table level filtering on replication. You might like to try another approach instead. Replicate whole database and change all tables except those you need to use to engine. 

Active-Passive master-master is a good setup but I have seen SUPER (humans)users writing on slave without setting sql_log_bin. (Though super_read_only in 5.7 will change things around this.) Anyways, following is possible and works. 

If you're using MyISAM, convert to InnoDB and you might see some drop in this counts. There is no other way to reduce this unless you stop querying them. Increase the table_open_cache and table_definition_cache & observe the stats...Raise this as much as you can! May be you need to increase system file handlers too and that is a way to go. 

(Consider backing up binary-logs if you want point in time restores.) For backups you can use traditional mysqldump or mydumper/loader. If your data size is large, it'd be better to go /w physical-backups, follow settingup xtrabackup for mysql with Holland framework. 

In my opinion it is not related to or Windows issues. Per pg_basebackup docs, the main data directory will be placed in the target directory, but all other tablespaces will be placed in the same absolute path as they have on the server. 

Yes, it makes sense to materialize. The analysis of large datasets (see: OLAP, dimensional modeling) includes the concept of aggregations - which can be implemented as materialized views. You should design what aggregates will you keep. In my opinion you need at least two: 

There is a lot of good sources on to partition or not to partition. If you are going to store 2 years and more, daily partitions could be optimal. But remember that very large number of partitions will make query planning longer. Threshold depends on CPU speed / queries used. PS. I assume that you ran out of normal ways to optimize: 

Internal representation of larger attributes will be sometimes compressed. More specifically, what works here is the TOAST (Oversized Attribute Storage component used in PostgreSQL). The threshold when values are considered for compression is 2000 bytes. is not a logical length, but the size (in bytes) of actual internal representation of the column/variable. It is documented. PostgreSQL stores array values in a custom, internal, binary format. Command line example below. Details also here. 

Because the flush to disk operation only occurs approximately once per second, you can lose up to a second of transactions in an operating system crash or a power outage. Well I just copied all of the above from documentation ;) Anyways, now about your points. 

are you sure you're doing what you want? In first query you specified localhost while in second you specified %... so for % you will have to connect to server remotely or using ip. Check below: 

So yes as you have mentioned about SQL injection the other advantage is what you guessed. Quoting from documentation: 

You can generate these SQLs from information_schema database and source it to mysql to quick action. 

So you know now that another master-master setup is not possible as DRMaster cannot replicate from two masters. I doubt multi-source replication is a solution here as it will create more trouble than help. You need to make sure to keep your DRSlave, "master-ready"... setting auto_increment% and binlog + logslaveupdates settings. 

If you need to loop over the output of one query to another you can use cursors. For eg, in this stored routine to find in all tables databases we have looped over: 

Why async? I would avoid doing this in trigger due to locking issues under high load. Also, easy to DDOS so permissions should be separate for form insert and form create. 

I understand that you want to go with single database (as it is good from management & maintenance point of view), but maybe it's too much integration. I am assuming that: 

Make sure not only partitions are indexed, but also the master table is indexed in same way and ANALYZEd. This could make the planner include index-based estimates on a single partition, but ignore them on master table level. If expression index or statistics for master table is missing, the planner is not able to infer join cardinality from this condition - even if it has perfect statistics for partitions. It's just a guess because you did not provide full schema. Please let me know if this helps. 

No it's not. Unless your field set is very dynamic (no single authority, people can invent fields on the fly). 

To redirect traffic from primary to standby you need some external tool which will do the failover procedure - using either dns-based or IP-based or other failover method. PostgreSQL itself does not know how to redirect traffic or do anything outside the database scope. Popular tools are pgpool (in layer 7) or Linux HA or corosync and friends (in lower layers). 

This will let the slave catch-up with latest available changes from master. If your binary logs were not yet shipped (via replication) to slave, then you might want to ship them manually and play it on slave. (Provided they're available - as you say you have co-ordinates) 

Also if not and you're looking to create slave of RDS slave then, there is an old update blog which says it's possible. Hope this helps. 

Before doing any changes and attempts, take complete backup of your databases. If you have physical backup of your database: 

With larger records and data-size you might see increased margin of difference. If those are MyISAM tables then you might want to review following notes from documentation: 

Yes, if your mysqldump is backing up all databases! If not, for future make sure you use with mysqldump. Alert but this will take your system to the state what it was earlier... If you know what your java programs are connecting using (user-name and host) then you can choose to fix the permissions for that!! 

So there's tuner's logic, if the (Created_tmp_disk_tables/Created_tmp_tables)*100 is more than 25(%) then increase your tmp-table size (upto max 256M). Again, mysqltuner is for reference, if you don't see performance issues you don't need to blindly follow the suggestions. Also note that tmp table has nothing to do with InnoDB vs MyISAM (if you mean that in your last line). You might want to read about internal temporary tables. 

I try to saturate the server using pgbouncer. I was running a Select-only test, with 1000 clients for 5 minutes. (). PgBouncer was initializet with scale=100 (but a SELECT-only test should not suffer on it). During the test: 

(note: tool might be hidden in default debian/ubuntu setup. Look in to see it) When it's promoted, replication stops and slave is disconnected from primary. See relevant fragments on command in pg_ctl documentation and failover docs. Question 2 

You can use string escape syntax and function like below. Please note it's heavily dependent on setting. Should be . You can find details here, here and here. 

For better effect please post results of and . Also let us know the dataset size and resource configuration parameters (, ) as well as database host parameters (OS, memory, disks). 

in pg_hba.conf, in the lines with localhost IP, replace "ident" by "md5" and restart - then you will be able to use password logins if (1) is not acceptable, sudo to "postgres" user and create another superuser login - then use this login to connect from PgAdmin. 

To calculate TPS (transactions per second), run the query several times and calculate difference over time interval. There are ready made tools for that, one of them is $URL$ More info: $URL$ 

When you cannot identify a primary key for a table you need to use surrogate key; auto_increment columns are most common surrogate keys which database internally provides and hence you should use them then. You may alternatively have sequences or programmatically handle uniqueness... 

So in anycase statement/row, you don't need to be worrying about the data on slave and it should match the master. but still if you want and can fit-the-logic on an event, this might be a possibility on slave at the risk of inconsistency!! As you said this is dw slave, would you consider a separate process to do the task you're looking to do, say a procedure? 

Though this is very much prone to corrupt your data and/or introduce inconsistency! Not recommended! There can be multiple solutions to one problem, if you could share the scenario you would get a better solution. For eg: 

I donot think there is --ask-pass in xtrabackup! You might want to write a wrapper shell script which will 

mysql will write rollback to binary logs provided the transaction has mixture of engines (myisam and innodb tables). As the myisam can not be rollbacked. I guess you'd like this piece of documentation... 

All operating systems and all applications use a concept called "caching". It means - when the data is first read from a slow memory device (like, a hard disk), it is saved in a fast memory device (like, RAM) for some time, to facilitate faster lookups. The same applies to RDBMS. First time the data blocks that build up your query results are read from disk, second time they are read from memory. Details can be explored using OS and database tools. If you specify what RDBMS and what OS you are on, we can help you get the details. For PostgreSQL it's about EXPLAIN command. 

Decide your vertexes (nodes, objects) and edges (relationships). Convert relational data to cypher, declaring all items and all relationships explicit. 

Interesting question but also very open one. I'm putting a list of recommendations here - hope it helps. 

In current version (Pg 9.0-9.3) there is no fast "fail-back" mechanism. You will have to follow official guidelines which state: 

If you want to see also query duration, you can set instead of . This is very useful for query tuning. Then reload config (restart or HUP) and collect enough log to estimate traffic. Note: neither method will include queries embedded in user-defined functions. 

When you're comparing, RAID-10 is still a better architecture than RAID-5. My opinion is that regardless of hardware improvement the underlying functionality still remains the same for RAID-5. (Even though Ronado's answer has "If you do not use innodb_file_per_table...", he'd still recommend the same.) 

Looks like in the name of alises you also seem to ignoring the join ! So my guess is non-existence of index is causing the delay, but you can confirm by posting explain plan and/or table definitions. 

Well you can review what mysqldump does on git and try working on it!! Though I'd not go this way... Few things to suggest: 

Last line explains the case. Referred: Dealing with MySQL case-sensitivity Update: For views and case-sensitivity, refer this bug report. 

Now, if you need slave's changes to be reflected on master, you need your master to be replicating from your slave as well. That's known as master-master replication. You can find plenty of posts around that. By default, all the databases are replicated. If you want to replicate only few of the objects (DB / tables) there are replication filters for that. 

No. Grants are not defined per database (as in mongodb 2.4). mysql database is a centralized system of records for authentication etc... (just like admin database in mongodb 2.6+). You might want to refer the document for mysql privilege system.