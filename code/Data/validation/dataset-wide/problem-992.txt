The where negates the left Why make it hard on the optimizer? At 3 or more joins the optimizer will TEND to go defensive and into loop joins as that protects memory An or condition in the join it will also tend to go into a loop join - do I have hard evidence it will happen every time - no - still a reality With multiple joins pull conditions from the where into the join when you can 

This is a bit of a rant but I had an application that could have 100 people making edits and loading over 100,000 rows during working hours. It was smoking fast. I only gave them a next prev and they insisted on a page number and I told them page number was typically only valid for a few seconds when the system was active. They said but our old system did. I told them your old system took a static snapshot of 100,000 and a user did not see any updates. The said we need a goto page so I did it. Sure enough a user blew up when goto page 60 was not the same after lunch. I just take the values of the sort for the last row and then fetch the next X. 

Not the stated question but may get better query plans with better queries You are killing the left outer with the where where profiledmo1_.Id=@P0 turns that into a join On indexes just the first two 

just 9 joins if ID is indexed the join will be efficient if you have gaps in ID that needs to be compressed then this does not work 

Do it at the application level Even .NET only goes down to ±5.0 × 10−324 Multiple by 10^300 and insert After you select you can scale If you are dividing then you don't need to scale A * B would be A * B * 10^300 * 10^300 and you are going to be outside the range You might be better off with BigInt, SmallInt with SmallInt for the exponent 

I would not worry about the 100% The big number are the big number A lot repeats so start optimizing just one This is just a subset of your query 

One possible scenario is let's say you have affiliates that have unique ID and you know they will not duplicate across affiliates as they have unique starting character. The affiliates load data to a master table. There records are processed and then assigned a master ID. Users need access to the records as soon as they are loaded even if they are not yet processed. You want the master ID to be based on the order processed and you will not always process in the order the records were loaded. I know a bit fabricated. 

You will need to fashion this into an update. You don't supply the table. Basically create a table with the valid bit values and use a cross join. 

go with case insensitive collation full DATETIME will get rid of some overhead if that is a char field then that is a problem and you should fix the data 

If recordID is your PK and it is not fragmenting then you should flop and make recordID the clustered index. With IX_AmtoteAccountActivity as a non clustered index give it a fill factor of less then 100%. This will also speed up inserts. Since it is a big table don't go crazy bet even 90% will slow down fragmentation. And just schedule a rebuild of that index every night. 

Yes fragmentation will negatively impact performance. Anything over 30% is bad but on a small table still would not have a major impact. But on a small table defrag is fast so might as well do it. That report you posted is 20 or so rows and starts on 96. You could also have > 30% on some big indexes. In rows 1-95 I can pretty much guarantee you have some problem indexes. This is a 24 gb database and you state indexes have not been maintained. What you list is not a problem but where there is smoke there is fire. Rebuild all is a bit much. The guildline from Microsoft is 

The limit is only going to help with exactly 100 so it is probably not worth the overhead. If you put an index on the hash then it should know to stop (and where to start). 

Here is a situation you might use top without a sort but it is a delete You would do it to keep the transaction log from filling up 

I bet a full outer join and look for null would be simpler but I have not tested it. I think it would also be less efficient. 

Looks like the delete itself is what is taking the time. You should test disable, delete, and then rebuild the indexes. I know you said you tried a join but this would be a quick test 

I don't know if there is a best design but I have been under the covers on a few and they went with separate tables for each datatype. And even separate tables for single-value versus multi-value. I would certainly go with multiple tables. You have to select a column name or table name / join so not really much more complex. Can even use a view. An update takes a more specific lock. Consider multi-value. What if you want to support email to both joe and sue. A specific table is overhead but it is hard to do a lot of stuff with a simple data model. There is spec out there you should consider supporting CMIS. Clearly you don't have to support the spec but if you want interoperability that is the way to go. But also have some players that don't support CMIS and have a document (no SQL) type back end. There is some great free open source CMS out there. 

With no to be sure you are not feeding in a hard query Start and only drop it when you need clean data 

Good that you went with the view row_number() solution If you were stuck on hard coding I know this works in MSSQL but have not tested with postgresql 

You have a whole lot of looping going on Give this a try The exists will be faster then count And bring stuff up into the join can help the optimizer 

Like '%' + [Bad_Phrase].[PHRASE] is killing you That cannot use an index The data design is not optimal for speed Can you break the [Bad_Phrase].[PHRASE] up into single phrase(s) / word? If the same phrase / word appears more than one you can enter it more than once if you want it to have a higher count So the number of rows in bad pharase would go up If you can then this will be much much faster 

Not sure about this but I think it will work If they both have values then I think it is different from your case statement 

With one table you would need to limit it to a single row. I think a trigger on InUse would be a better solution. 

because your new index does not have the sort try without the sort on Vit_DataAberturaReal the index on the sort does not include all the fields in the select 

Not sure if it will improve performance but you are killing a number of left join with the where WHERE ipx.ProviderType IN ('IRF') turns that into a join 

Maybe I am missing something but why do you need the sub-query? OP said from entity framework and even the better query had a sub query. I wanted to break down the query and it was more than I bargained for. Worth a try: 

To consider Remove words that contain a smaller word There is no purpose to search on 'indent' if you have searched on 'in' 

The FK would still need to check with the empty table to see if the key was used on a delete operation. But this would be a quick check and never deny if the table is empty. 

I know you said with (rowlock) did not appear to be working Are you including a <> for the set as then it will avoid an update lock 

I cannot know why they are not present. The purpose of a FK constraint is to enforce the FK is present. Without the constraint you could have many without a one.