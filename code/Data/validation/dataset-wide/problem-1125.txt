I recently inherited a SQL Cluster (2008R2) which for the most part behaves itself impeccably. The windows cluster is made up of two nodes running Active/Passive, Node1 and Node2 are dedicated blades in two different data centers. There are 3 SQL instances all running on Node1. Quorum is established by a File Share Witness and we have a heartbeat between the two nodes. The other day someone switched off the file share witness by mistake, and the windows server failed over from Node1 to Node2. Or should I say, in Failover Cluster Manager, Node2 was now specified as the active node by Windows. However, the SQL Cluster didn't do anything. All the instances stayed up and hosted on Node1. I would have expected them to move Nodes, but no. There was no adverse affect on the databases at all. Once power was resumed to the File Share Witness I brought it online again and the Windows Cluster failed back to Node1. Our Windows Technicians are looking into why the cluster failed over, and I'm left scratching my head with the SQL bit. All I can think of is that the heartbeat kept the SQL instances on Node1 and losing the witness wasn't important. I'm still learning the small details of Windows Clustering, being much more used to Log Shippping and Mirroring when it comes to HA solutions, so any insight into why the SQL Instances didn't failover would be appreciated. 

This comes from this MSDN link found almost instantly after a quick web search. Bear in mind there are a few obvious and not-so-obvious functional reasons why you would not want to failover from Enterprise to Standard as some features are not supported in both. The idea of a clustered environment should be that you maintain integrity of the database environment upon failover, so why compromise that in any way, whether talking about support or features? 

Setting the database to single user mode will close/rollback any existing connections except the current session Set it back to multi user afterwards to return it to normal. 

As far as I can see from checking gpedit, Lock Pages in Memory is not enabled on the server. Each instance has the following memory settings: Min: 1024 Max: 7168 I'm getting performance issues with Instance 2. Slow queries etc. Instance 1 is fine. To do a basic check for memory usage I'm running: 

I'm not a database programmer. I really don't plan on becoming one; I dabble. My skills and work lie elsewhere, and these keep me busy enough. Nevertheless, I find myself needing, and begging, for some quick help at times in an area in which I have no time or need to learn fluently. I have officially attempted some trials at building a couple of new tables for this, but to no avail. I do most of my learning in a hands-on setting, and will know how this particular schema works once I've received some direction (which is all I'm asking for, I think) and implemented it. That said, the following was suggested by @RolandoMySQLDBA four years ago, and I've found it terrific for a project I'm building. I'm hoping he will see this and that he, or someone, can help me extend this to include a table of "Organizations" along with a table of Organization rel_types, to be associated with any of the persons... 

I just have 2 questions about this. Is it going to be wiser to incorporate a linking table to avoid duplicating "names" for each parent in one table, or is there some other way to avoid this, or is this even an issue? The output I'll be getting for each page will be a long, fairly complex outline layout, which would probably mean that I'd need to run this kind of query sometimes up to 30 or 40 times per page. Is this kind of query something I can run that many times to load a page without a problem, or do I need to come up with something more efficient? 

I have this hierarchical table that will end up containing 5 or 600 unique names, many of which will be assigned to multiple parents, such as "Hunt" below... 

This fantastic snippet/query works well - almost. What I need to do is either find a new way to define the associations between 2 persons in my ASSOCS table in such a way that a new branch is defined, or get this query and loop to do so in some fashion. It would probably take both, I'm thinking, but I'm not good with arrays or this depth of php. (I hope php is appropriate here - it seems to me to still be administration of a db.) There are also my (commented) attempts at including the new "info" field. 

Yeah you can do by using It's going to be fiddly to do what you want... simply because you'll need to unpick the stored procedure and re-engineer it, so to speak. I have sort of done the same thing myself for our in-house SQL inventory app that the DBA's use. The following produces a single result set which brings together the two result sets in and also adds in some info from It uses dynamic SQL to get the data for all databases in the instance. It might not be exactly what you want, but it might do something for you. Note that sometimes we have to explicitly COLLATE the joins so let me know if you get COLLATION errors and I'll try to help somehow. The script: 

I've never tried this but I'm reliably informed it is 100% not possible. It is certainly not supported by Microsoft, for good reason, so why would you do it? 

I know people will recommend Ola Halengren's scripts but I've always sidestepped them (as good of a resource as they are) simply because I want to be the architect of our maintenance scripts and creating them by myself furthered my understanding of indexes and maintenance plans in general. So with that in mind, here is a pretty flexible and lightweight script that will rebuild (or reorganize) indexes only if necessary, as defined by the percentage fragmentation thresholds. Using a more selective script as per the below, I have been able to reduce index maintenance times massively. We've seen reductions of 15 hours in some of old MP's designed in Management Studio on large databases. 

This is possibly a dumb question, but it has been triggered by a conversation I'm having with a colleague in our Dev team. My understanding is that the 'Last Modified Date' of the MDF file (in Windows) is only changed when the Database is Closed/Reopened or data is written to the MDF file causing it to grow (eg. by a Full Backup or Transaction Log Backup, assuming database recovery mode is full) However I've just noticed our hourly log backups are not changing the date of my .mdf files - they are stuck at the date/time of the last full backup. Shouldn't they be changing hourly for active databases? What if there was no write activity captured in the log during that hour? Would it not affect the .mdf modified date? And if this is the case, would the modified date for readonly databases (databases that are defunct for write purposes but still used for read purposes) ever change?