New Records to the New Table At this point, new records should be getting inserted into the new table. I'll simulate this by running the following: 

The first TLog backup simply forces a CHECKPOINT operation and flushes the log to disk. The first DBCC Shrinkfile operation shrinks the physical TLOG down to where the current logical log is located The second Log Backup issues another checkpoint and will likely backup/clear any resident log operations located in front of the logical log. These were never flushed in the past for whatever reason, but should be cleared at this point. This step is critical to fixing your issue and sometimes this backup can take a bit of time depending on how much data needs to be written down to disk. The second DBCC SHRINKFILE operation moves the logical log file to the head of the physical log file, eliminates excessive VLFs that are likely there now, and reduces the TLOG file back down to a very small size, in this case 1MB. It is imperative to manually grow the file in step 5 so you don't run into vlf issues again down the road Manually increasing the size of the tlog will allow you to control the size and number of VLFs within your physical log file. This is an opportune time to configure this properly, so take advantage. 

The SSIS user can now truncate whatever table you wish to allow simply by calling the stored procedure from within a Execute SQL Task step within the package, as follows: 

To my knowledge, vanilla Postgres does not natively offer these features within the engine. Per the Priorities PostgreSQL wiki page: 

Why don't you just use an actual which was introduced to SQL Server with 2012? The following approach overloads the value where the leading 10 digits represent the server IP address after being converted to a (via the method outlined in this blog post by SQLDenis) and the right-most 10 digits represent the CustomerID local to the instance. A can have up to a maximum of 19 digits, but because the leftmost digit will never be higher than to start the sequence this allows for far more customer numbers than the max value of an allows for (the comments in the code explain it in a little more detail). The only requirement for this approach is that this solution only be deployed to, at most, one instance per server (because IP won't be unique across instances hosted on the same server). As stated above, this approach still allows up to the maximum of 2,147,483,647 customers per instance in your solution. Because the column in the tables is defined as a in your question, I'm assuming this is acceptable. It's still about as fast of an approach as you'll find anywhere else. 

It may be too late now, but while you're tuning the new query execution plan, you may be able to grab an old plan from cache and force it's use via the USE PLAN query hint. This wouldn't be something I would recommend you do as a permanent solution, but it can act as a Band-Aid until you optimize the current execution plan. If you're interested in this, let me know and I'll expand this answer with some additional instructions. 

This will provide a workaround that effectively equates a in one column to a in another. So to work that into your Merge, it would be as follows: 

Because I've not seen either of the following approaches listed yet, I figured I'd include them. If you're running SQL 2016 or later, take a look at the STRING_SPLIT function. If you're running SQL 2014 or earlier, you can utilize some XML trickery to split up the string into a table. Both approaches are as follows: 

The Cross Apply is being used in conjunction with the XQuery call. This is basically getting every element for the XML arrays constructed within the CTE statement. The outer XQuery call then extracts that value and casts it to a , returning a record for each code within each group. I find it easier to digest if I break up the query. Maybe you will as well. 

Taking this approach allows the Data Flow tasks to remain unchanged, and now you should be able to reuse the SSIS package without much more work. If you're finding it annoying to create a stored procedure that controls the creation/truncation of 144 separate tables, there are various tricks you can employ. If you're not opposed to using the UI, you can multi-click items in the Object Explorer Details window and script out actions, so for instance if I want to generate out a bunch of drop/create scripts for a lot of tables, it's as easy as clicking on the table folder for a given database, multi-selecting the tables I want to drop/recreate in the Object Explorer Details window, and then right-click on them and select the option I want: 

My initial suspicion is that the initial setup didn't include the fully qualified name, but check for certain with the following statement: 

I don't know if you can run this without acquiring any locks, per se, but you may be able to reduce locks this transaction acquires by utilizing an optimistic locking isolation level such as either or isolation levels. This may have adverse affects on your database in general, so I wouldn't recommend enabling either of these options without proper testing.... and frankly, if you're going to be testing this, you may as well test your long-running transaction somewhere other than Production as well. 

While this implies only drops and/or truncates won't be tracked in real-time, I just ran the same query against one of my environments where nothing is ever dropped or deleted and am getting similar behavior to your results. I suspect this view just is updated in the background and whatever it is that has to trigger to spawn the update hasn't happened yet. Hopefully someone else has a better answer as I'd be interested to know for certain as well. 

Edit your SP to include a call to sp_send_dbmail upon successful completion of whatever logic you have. I would suggest nesting this into some TRY/CATCH logic. This will be the most direct route and will allow the notification(s) to occur regardless how the SP is called. Call your SP via a SQL Agent Job, and configure it to send a notification to an operator upon conclusion of the job (e.g. completion, success, or failure). Call your SP via a SQL Agent Job and include a secondary step calling sp_send_dbmail. This is helpful if there are more than one party or email distribution list you want to notify as the built in notification methods within a SQL Agent job are rather limited. 

The first Log Backup simply generates a backup of the completed transactions that have been flushed to disk and deletes inactive virtual log files from the logical transaction log, freeing space for reuse up to the logical log header. The first DBCC Shrinkfile operation shrinks the physical TLOG down to where the current logical log header is located, freeing as much space as possible. The second Log Backup operation is now able to loop back around within the logical log and backup/clear any residual inactive VLFs located before the logical log header. These were probably never flushed in the past for whatever reason, but should be cleared at this point. This step is critical to fixing your issue and sometimes this backup can take a bit of time depending on how much data needs to be written down to disk. The second DBCC SHRINKFILE operation moves the logical log header to the front of the physical log file, eliminates excessive VLFs that are likely there now, and reduces the TLOG file back down to a very small size, in this case 1MB. It is imperative to manually grow the file in step 5 so you don't run into VLF issues again down the road Manually increasing the size of the tlog will allow you to control the size and number of VLFs within your physical log file. This is an opportune time to configure this properly, so take advantage. 

We run RMAN backups via CRON jobs, but find the out-of-the-box emails with CRON to almost do what we want. I know we can channel the RMAN output to /dev/null so no email is sent, which for any successful run of RMAN is the desired outcome, but if the RMAN job runs into an error, I would like the entirety of the standard output sent via email to the DBA team for review. I suspect I can wrap my RMAN scripts with a bash shell script that will pipe the standard output to cron only in the event of a failure, but is there a better way to do this? To summarize: 

You're on the right track, but I suspect you're still running into the double-hop issue. SPNs are a step in the right direction to resolve this, but next you need to setup delegation for your SQL Server Instance so that it can properly pass along the AD token. I will recommend that you setup constrained delegation for SQL Server, but any delegation adjustments will require an account with Domain Admin privileges on your network, so you may need to loop in another person or team depending on your company's structure. The link above is for the older versions of Active directory, but the same approach will likely apply if you're running a newer version. Steps 3 - 6 on this article about configuring delegation for SSRS are also a good reference point and SSIS is no different from SSRS where these steps are concerned. 

As I was putting some test sets of data together, I noticed some funny behavior with temp tables. When working with large sets of data in clustered temp tables that are populated via a parallel execution plan, the clustered key does not look to be honored when selecting data. This issue also seems to affect all versions of SQL Server that I've tested (include vNext). Here's a dbfiddle.uk example of the test. You may have to execute it a couple of times to get the result I am finding, but it shouldn't take more than one or two executions to yield the same results. Additionally, this is the local execution plan I'm getting on my environment which shows that the only difference between the large and small data sets is the way data is fed into the tables (e.g. parallel vs serial plan). If you want to play-at-home, here's the test I'm running: 

Here's a sample of one of the results from the column which implies this isn't related to temporary objects which is what I would normally guess to be the cause. 

There are a number of US government regulations that, depending on how your company interprets them, require this separation of duties. Sarbanes–Oxley (SOX) is the big one for publicly traded companies. HIPAA for healthcare organizations, etc. The first question I have is, of those tasks you outlined, how many are you doing directly from the server? Arguably many of those tasks don't require local server access, but I understand this would be frustrating if you are used to performing this work locally on the server. Regarding workarounds, my suggestion is to be an adopter of this policy. Do this by making sure that your security team is properly locking down user accounts, but creates clearly defined policies for service accounts and that those accounts are setup appropriately. I would even encourage you to look at Group Managed Service Accounts as this makes everyone's life easier if you can make them work in your environment. Follow this up with moving whatever processes you can off of the servers. There are real security concerns with users actively logging into servers regularly, and I would say you should avoid doing this unless absolutely necessary. Argenis Fernandez, during one of the most entertaining PASS sessions I've attended, even showed that anyone with access to a local admin account on a server can get elevated access to the SQL Server regardless how "locked down" it is. Honestly, you could even go so far as to request all of the servers get configured to run as Windows Server Core, which is about as secure a Windows server as you can have. Finally, document those processes that cannot be moved off or need to have exceptions created for them. This will likely be your biggest headache in the short term, but proper documentation should allow your organization to bend the rules to fit the business needs. Look at it this way. If anything goes bad, such as a breach of information, and it's traced back to unauthorized access to a server, this policy may save your behind without you knowing. I've worked for years in the healthcare and insurance industries and as much as a headache as this sort of policy may seem to be, it's really a good policy to adopt. This process will be frustrating... at least it has been the number of times I walked a similar path, but don't make it a we vs. them issue, instead approach road blocks with the following statement "this needs to happen so the business can run, how do we accomplish that within these restrictions?" With this approach, it's not about you or your job, it's about your company staying in business which makes it a we vs. the policy issue.