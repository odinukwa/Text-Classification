This could be operationalised by imagining that you compile another corpus (with texts from the same registers!). What is then the likelihood that in the new corpus the frequency of the word is y? Let's say in corpus x the word has a frequency of 2 pmw and you want to know how likely it is that in the population it is 20 pmw. Assuming your first corpus has 1,000,000 words, we imagine that you compile another corpus of 1,000,000 words and you find the word in question 20 times in that corpus. Using the log likelihood calculator, you get a log likelihood (also called G2) of 17.09. If you scroll down on the webpage you find a key for the log likelihood values, which tells you 

What theories explain the transfer of phonetic and phonological features from the first language to a second or foreign language? How do these theories differ from each other? Such theories should refer to both transfer of segmental features (phonemes) and suprasegmental features (prosody) and be grounded in empirical research. 

Q: Can anybody confirm Polish influence on (East) German dialects, in general and in particular for these vowels? Influence of Polish on varieties of German spoken in the Ruhr region has been documented, but this influence was more restricted in time. There was Polish migration to the Ruhr region in western Germany in the 19th c., and influence also seems to be restricted to some lexical items. 

Although this appears to be a rather recent loanword it is readily integrated into Polish morphology. German A while ago I heard a shop assistant praise another's hard work: 

It really depends on the relationship between the categories and what you want to use your agreement scores for. Let's say you want to annotate verb phrases for tense and aspect, such as 

By the 14th c. people started making fun of the French spoken by the Norman Aristocracy. Chaucer, in the Prioress's Tale in the Canterbury Tales says about the Prioress (a nun): 

This converts your recording from stereo to mono by overlaying the two channels. Then draw as usual. If the two channels differ substantially and you want only one of them (instead of a mixture of both), you can load the recording into Audacity and split the stereo track into two mono tracks. 

The most widely used reading passage in research on English phonetics and phonology is The North Wind and the Sun. It includes most English phonemes and is used, for example, in the Illustrations of the IPA (translated where necessary, although it is then not guaranteed to include all phonemes of the language in question). David Deterding argued that the Wolf passage is a better alternative to The North Wind and the Sun. It includes all English phonemes and is better in terms of the occurrence of phonemes in syllable onsets and codas. A reading passage designed specifically to record learners of English is "The Tiger and the Mouse". It was used in the compilation of the Learning Prosody Corpus (the corpus manual includes the text). 

Other terms for assertive and non-assertive in this context are realis and irrealis as well as positive and negative polarity. When using some, somebody etc. the existence of the entity in question is asserted. There is somebody hiding behind the box means that the speaker assumes that there is a person hiding there, although they are not aware who it is. The existence of the person is real/realis. By contrast, Is there anybody behind the box? does not make the assertion, but simply raises the possibility. The existence of the person is hypothetical/irrealis. The same is true of negation: There isn't anybody behind the box. A complication arises from the fact that, as you mentioned, assertive words can sometimes be used in questions or negated contexts. We can, for example, contrast: 

(Note that the latter spectrogram is wide-band and the first in Praat narrow-band, but changing wide-band to narrow-band doesn't solve the problem in Audacity). 

The etymology appears to be 'without difficulty, easily'. The second part of the word, leicht, continues to be used on its own in the sense of 'easy/easily' - as in mit Leichtigkeit, English with ease, easily. The dictionary by Jacob and Wilhelm Grimm provides the following etymology: 

I have noted this stress pattern in a number of examples, and my impression is that it's more common in American than in British English. Each of them might be interpreted contrastively. In the second example the speaker seems to imply that Chinese food is somehow more fun or enticing than, say, Italian food. Similarly, in the first example that information that the speaker is talking about black men, not white men seems to be key. But I've always found this stress pattern surprising, to me the context in these examples doesn't warrant this kind of contrastive focus (as in "No, I want ChiNESE food, not ITAlian food."). Then again, I'm not a native speaker. So my questions are: 1) Does this stress pattern strike anyone else as remarkable, or is it just me? 2) Is "contrastive stress/focus" the best analysis of these examples? 3) Is it more frequent in American English than other varieties, such as British English (note that all the examples are from American English)? 4) Has this stress pattern become more frequent in the last decades or so? If this stress pattern is something more than just ordinary contrastive focus, references would be appreciated (but not strictly necessary ;) 

acattle's comment that there is a trade-off between inflectional complexity and strict word order is a good answer to your question. I think it would really help to illustrate this with some examples. First of all, let's look at English word order (actually it's the order of constituents, not words): 

However, in spoken German this is quite uncommon and only used in formal contexts. The colloquial variant has natural gender trump grammatical gender: 

Yes, every word is changed as time goes on (which is a big problem for linguists trying to learn anything about the prehistory of human languages), but they don't all change at the same rate. Historical linguists trying to determine how various languages are related exploit this by comparing words that are least susceptible to change - see for example the famous Swadesh list (although perceived cultural universality was an important consideration in choosing this list). Note that this list contains many personal and interrogative pronouns, negative particles and determiners. It also contains many verbs and nouns, but again these are frequent words referring to basic concepts. So personal and interrogative pronouns, quantifiers, negative particles and determiners - as a group - are less likely to change over time than nouns, adjectives, and verbs - again, as a group. But this is not due to any inherent quality of pronouns and the like, but because of frequency of occurrence. 

There is a technique in NLP called shallow semantic parsing that you might find useful. As others have pointed out in the comments, it's a very demanding task. Humans use a lot of information that's not readily available to an automatic semantic parser, not only from the context, but also real world knowledge that prompts us to exclude very unlikely interpretations (and only reconsider them if other more likely interpretations don't seem to make sense). The SEMAFOR project by the Stanford NLP group has built such a semantic parser and they provide a demo. You can probably download it and use it on your corpus. I haven't used this tool, but others from the same group. If you aren't scared by using a command line and figuring out a few things (it's rarely as easy as just installing some software and running it) you will likely be able to run it on your computer. They also provide further references on their website. I ran some of your sentences in the demo, and results are mixed. The verb live seems to be recognised only in the sense of *Live*1 = to reside. The adjective is treated as something different from the verb. Phrasal verbs and idioms such as live it up seem to be beyond the current abilities of the parser. 

I would recommend that you use Praat's pitch tracking function to provide a visualisation of pitch (see footnote below). The blue line in the figure below shows first a rise in pitch, followed by a long trough of low pitch, and finally another rise. 

Evidence on first language acquisition can be acquired in a controlled or naturalistic setting (and in other ways too, but this is a major distinction). In a controlled setting all factors other than the one(s) you are interested in are held constant. Participants should have the same age, sex, socio-economic background or at least you include roughly equal numbers of both sexes etc. and check whether it makes a difference. Then you present the same stimuli in the same way and see what happens, or vary them in a specific manner between gropus. From an experimental and extremely unethical point of view, the ideal experiment would be taking children a birth, assigning them to different groups and presenting each group with input that differs in some way you are interested in. Needless to say, nobody will do exactly that. In a naturalistic setting the researcher tries to influence the result as little as possible. You try to generalise from what you see happening 'in real life' - for example that 90 % of all children acquiring English in the UK from monolingual patterns have mastered irregular past tense verbs by age 3 (I'm making up the numbers here). One problem here is that the presence of a researcher might change the outcome. 

Whether you normalise by 100,000 or 1,000,000 words is essentially a question of style and/or common practice in the journal or sub-field you're in. Mathematically, it makes not difference at all whether you say feature x occurs with a frequency of 10 per 100,000 words or 100 per 1,000,000 words. But you should be consistent, so if you provide other normalised frequencies or compare your results with previous findings, use the same normalisation index. In my experience, most corpus linguists give relative frequencies per million words, this is usually abbreviated as pmw. Some authors prefer normalisation per 100,000 words and call this the Mossé index, but this is in my experience relatively rare. The only reason you might want to normalise by a smaller figure (e.g. per 1,000 words) is that some readers might interpret a frequency pmw as suggesting that you actually had a corpus of 1 M. words, not a (relatively small) 5,000 word corpus. So if you give no other normalised figures (from your own or other people's studies), it might be preferable to normalise by 1,000 words - especially if your audience is not very well acquainted with corpus linguistic methods. 

It's much harder to show that it's not relevant to any branch of linguistics than to show that there is at least one application of the P-NP problem in linguistics. But I haven't heard of any. It's probably obvious why it is not relevant to branches of linguistics that don't have any computational component, such as historical linguistics. But other, rather more computational branches, of linguistics also seem to have a lot of other problems to care about. In corpus linguistics, important questions are 

The World Atlas of Language Structures (WALS) provides information that is relevant to this question. Feature 68A (The Perfect) is divided into perfect from possessive (i.e. from have or the like), from finish or already, as well as a category for other perfects and one for languages that don't have a perfect. The perfect from 'have' is relatively rare with only seven related and exclusively European languages. It developed from structures like I have the work done, which were later reanalysed as having the meaning of the perfect. Of course not all languages are part of WALS, but Mandarin is, and it is categorised as not having a perfect. I'm not saying that your example from Mandarin Chinese is wrong. It's possible - though perhaps a bit unlikely - that WALS is wrong, or that your example is not a perfect after all. The perfect from 'finish' or 'already' is in fact more frequent, with 21 languages, and distributed over different language families. 

Q: I'm looking for a description of the Polish vowel system that is based on formant measurements (published sources please). Despite my best attempts I have been unable to find any. 

They are also in some ways complementary, so it's worth it to read both. Some background in linguistics will be useful to get the most out of them but they don't require too much previous knowledge. 

There is a free web-service where you can try out the semantic tagger. Here is an example of the sentence "My dog loves biting the postman." It's a bit cryptic, but that's because it's supposed to be machine-readable. 

As the Brown Corpus shows, this was in fact possible in the 1960s. Also, being selective about what you include is a good approach in empirical research. Rather than including just any language samples that were available, the Brown Corpus samples published material from a range of genres. The corpus make-up mirrors closely what was published in 1961 in the US, so that it can claim to be representative of 1961 written American English. This is basically the same approach as carefully choosing a group of 1,000 Americans (based on socio-economic and geographical criteria) to determine voting patterns, rather than asking 10,000 completely randomly selected Americans. The "completely random" approach is also used nowadays, for example to compare web-based megacorpora. If you want to investigate low-frequency phenomena this is great, but a huge, automatically compiled corpus comes with its own set of problems - and depending on the research question, sometimes a web-based mega-corpus might be appropriate, and sometimes a carefully compiled smaller corpus. 

Since you asked whether there is a corpus with such a classification, I'm going to give you an example of a semantic tagger. The UCREL Semantic Analysis System (USAS) can be run on a corpus just like a parts of speech tagger and relies on a taxonomy of 21 general levels of classification: 

The Oxford Dictionary of English (sport, disport), the Online Etymology Dictionary, and the Grimm Brothers Dictionary of German all give variants of < Middle English disport(en) < Old French desporter < from des- 'away' + porter 'carry' (from Latin portare) 

Footnote: Actually, pitch is a perceptual phenomenon, and fundamental frequency is its acoustic basis, but depending on the previous knowledge of your students you might not want to dwell on that for too long. 

(Not 100 % sure about the French example since I'm not a native speaker, but I think it's grammatical). The only difference is that German and French use a definite article, whereas English has the indefinite article. In neither of these languages, I think, do articles/determiners double as prepositions. 

Using formants, it is possible to produce an acoustic description of vowel quality. Basically, the first formant (F1) corresponds to vowel height, and the second formant (F2) to vowel backness, as shown in the following example for English: 

(apparently a quote by Groucho Marx) The joke rests upon the fact that in my pyjamas can be an adjective phrase that qualifies elephant or an adverbial phrase that qualifies the verb. Most people's initial interpretation is the latter because elephants rarely wear anything. We usually don't even notice that the former alternative (adjective phrase qualifying elephant) is a possible parse. Humans integrate syntactic and semantic parsing, but AFAIK current NLP applications can't do that. Semantic parsing is an area of active research, and the Stanford parser is not the only around. An alternative is the UCREL Semantic Analysis System. 

The latter is not primarily about your topic but they give a good description of and more references for segmentation criteria. Before you start writing your own program you might also want to consider using or adapting existing solutions. There are some tools that use phonemic forced alignment, such as HTK. Together with an acoustic model of the language you are working on and an orthographic transcription of the text, this produces a phonemic time-aligned transcription of an audio recording. Together with P2FA, which provides a wrapper and an acoustic model of American English, and outputs Praat TextGrids, I have achieved good results even for other varieties of English. You could also take a look at MAUS, which provides a web interface for a small number of languages and also produces Praat TextGrids. 

When dealing with a sequence of a sonorant (a consonant such as ) these criteria might be less or not at all useful. But a change in intensity and change in formant pattern will usually be observable. For example, approximant (such as in standard American and British English) usually has a low third formant, so the boundary could be set at the midpoint of the trajectory of the third formant in a . Here are two references you might find useful: 

As the other answers have pointed out, "natural" is not a category that is used in linguistics. However, there is a cross-linguistic tendency for plurals to be formed by the addition of a morpheme to the singular - i.e. plurals tend to be longer than singulars. Apart from that, as a Czech speaker you might find some plural forms in other Indo-European languages familiar because they have similarities that go back to the earliest form of Indo-European (proto Indo-European). Also, the umlaut/ablaut pattern to form plurals is shared by (some) Germanic languages, i.e. 'Mann' ~ 'Männer', 'man' ~ 'men'.