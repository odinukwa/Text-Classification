If you ask the question at this level of generality, the answer is probably "yes". This is not necessarily very profound; if you get general enough you would expect the two to have something in common: "Are science and religion comparable in that both are human endeavors?"; "Are science and religion comparable in that both are intended to convey information?" In particular, yes, scientists do engage in their pursuit because they believe that it will lead them to a better understanding of the world. The question then is whether it's apt to call it "faith". Scientists have accomplished breathtaking things, so there's certainly abundant evidence that science leads them to a more powerful position from which to manipulate the world. But there's always doubt that running that RNAseq experiment will really tell you something meaningful about the interaction partners of CREB, and generally as a personal matter it's faith that this whole thing's eventually going to work and turn out that keeps scientists going. You cannot conclude from this that science and religion have the same sorts of guarantees behind the robustness of their claims, only that people like to do things that they believe in, and scientists are people. 

Every non-alive thing that we know about "certainly" is not conscious. (In quotes because all knowledge is tentative.) We don't have a great scientific definition of consciousness, and philosophical definitions are disputed, but in almost every conception it has something to do with an ongoing awareness of events beyond the raw computation of their properties and immediate selection of an action. Tables, shoes, and even iPhones and computers are right out simply because they don't meet even these minimal requirements. (Computers might if we programmed them the right way, but so far it is extremely doubtful that we're using the right algorithms to even admit the possibility.) However, it is equally certain that non-alive things could be conscious if only because all evidence is that brains generate consciousness, and brains appear to be sufficiently well-described by deterministic properties plus stochastic noise that in principle a computational model could be made and implemented on a non-living computer. Then one is tempted to ask: what needs to happen to bridge the gap from the "certainly not now" and the "certainly possible"? We then fall afoul of our lack of scientific understanding: we don't know what is needed (or even quite what we're talking about) biologically to the extent necessary to even hazard an informed guess about when or how we might get there computationally. 

The conclusion is that Oscar's "water" is not Twin-Oscar's "water", but you can only know that by looking outside their heads, because they don't know it (at least not yet); yet you know that they mean different things because if Oscar knew that "H2O" and "XYZ" both existed and were not the same, he would (presumably) insist that only H2O was actually what he meant by "water". (He meant his water, not Twin-Oscar's twin-water.) (Is that the step that you were finding problematic?) 

Living our lives requires very little faith these days. Life just isn't that dangerous. Sensory input is reliable. Life expectancies reach into the 8th decade. Cats provide feedback about whether they like to be petted, and if you make a mistake the consequences are minor (maybe a scratch). Airplane crash statistics are readily available, as are the ingredients of your food, whether your unborn baby has a genetic defect, and astoundingly much else. The only little bit of faith we require to function normally is that we are not victims of Descartes' evil demon. That is, it is not the case that all our perceptions are an illusion which will be whisked away from under us at some point. So I don't think your premise is correct. Deciding to investigate a hypothesis does not require any faith. You can instead, for instance, calculate that it is not easy to reject that hypothesis, and collect more data that may invalidate it or further recommend it. We seem to like to add a lot more faith as a species beyond this, but then we have a surplus of knowledge and certainty these days as opposed to most of our written and evolutionary history, so it's not too surprising that we might reach for faith in cases which aren't really needed any more. (Kind of like inflammation--with modern medicine and lifestyles it's almost always an excessive response to injury.) 

You're just making a simple but intuitively appealing error in conflating a posteriori and a priori probabilities. I have a handful of dice next to my desk. Look, I just rolled 1,1,1,3,3,4,5,5,6! The chance of this is 0.15%. Amazing! Except...anything I would have rolled would be amazing. Likewise, the exact number of air molecules that enter your lungs in each breath is amazingly unlikely to happen, but it happens every breath. So, the chance for you to exist was very, very low. But the chance for someone to exist was very high--it could have been any one of gazillions of people, but it just happens to be you. It's really no more interesting mathematically than that you might have just breathed in exactly 10,392,210,415,215,602,289,076 molecules of N2 in your last breath. That collection of molecules must be feeling awfully special right now. What was the chance for exactly them to be the special ones? 

This is the Sam Harris route to ignoring the difficulties with defining an objective morality (I assign it to him as he was, as far as I can tell, the most vocal and prominent early advocate of this position). It's really easy to define an objective morality, actually. It's just really difficult to justify it. Here's an objective morality: that which takes humans further away from the center of the earth is good. That which takes them closer is bad. (So, obviously, we ought all live as high as we can on mountains, and treat scuba diving as a grievous sin.) There's no doubt that science is a wonderful tool for providing us with information about many things. That it would have a lot to say about well being of humans is unsurprising. The problems come when you start asking why: why well-being instead of happiness? Why just humans? How can you quantify it in a way that is correct, not just easy / measurable? How do you combine scores from different humans? Harris dodges the question, essentially saying, "Wait, wait, wait. There are easy cases--malaria unambiguously decreases human well-being, and science will tell us that!" This also ignores the point. Yes, there are easy cases, and they're already easy without this supposed framework for morality. Almost nobody seriously advocates for letting malaria run rampant or for spreading it. But there are other common problems, like increasing wealth disparities or the conflict between economic growth and environmental degradation or whether it is noble or evil to publicize the plight of starving children in Africa where you simply must answer many of these why questions. So, science is an awesome tool, and we can apply it to help us answer questions of morality, but it doesn't tell us that the metric should be "human well-being" any more than it tells us it should be "distance from the earth's core". It does however, tell us some things about morality that we tend to ignore. For example: 

I haven't viewed Heidegger's critique as important enough to invest much time in it, so I can't provide information on what other philosophers have said regarding his views. But the obvious defects to me seem to be that 

You probably really ought to ask somewhere that deals with statistics. But the brief answer is as follows: yes, anecdotes can be informative. They can't be tested since there's only one of them, but they can still be highly suggestive. However, this is only true in Sherlock Holmes style: when you have eliminated the impossible, whatever remains, however improbable, must be the truth. This also works if "impossible" just means "way, way less probable than 'however improbable'". The intuition is pretty clear. Swallowing a battery-sized model rocket engine ought not make someone explode. It might make them sick (it's probably poisonous) or hurt them, but not explode. But people really really don't go around exploding much. Suppose you, one time, see someone gulp down a model rocket engine and then explode. Heck--you don't even need to see it, as long as you know it happened by some reliable source (i.e. it's much more likely that it happened, weird as it is, than that your source is wrong). Now, you think: well, people don't go around exploding under normal circumstances; this would be incredibly incredibly weird that just in those few seconds after swallowing a model rocket engine he'd explode for some other bizarre reason. So, maybe model rocket engines actually can make someone explode. You can quantify this with Bayesian statistics (and find that it is valid, at least with reasonable distributions of priors). The problem with anecdotes is that people habitually underestimate the chance that the account is wrong, or fail to realize that the world has seven billion people in it and all sorts of stuff happens by chance and we select out the weird-seeming things to pay attention to. So if you want a rule of thumb: ignore anecdotes. But if you want to be statistically accurate: yes, they contain information (and can possibly justify you changing your model of what is likely by quite a bit*). *(Addendum - the clearest case where an anecdote has huge power is a single instance of something happening that is said to be completely impossible. For example, if "all ravens are black", and you see a white raven, that's pretty good evidence against "all ravens are black". Of course it might not actually be a raven so without a lot of investigation it's not conclusive, but single examples can go a long way towards falsifying statements.) 

My favorite example of a moral philosopher helping resolve--or at least helping to clarify the discourse on--an issue of morality is Peter Singer's involvement in animal rights. He makes a strong (though not inarguable) case for treating animals with much more compassion than we typically do, but without referring to any deities. So it's certainly possible to reach at least the same level of resolution as one gets from including a religious leader. 

The "problem" is a practical one: it is hard to organize large groups of children of drastically different ages, due to differing attention spans and interests, with only a small number of adults. Although it is possible for the older children to take some responsibility for the younger, this generally doesn't work in a system with high turnover or where people have years of training that younger children are inferior or not to be associated with. So I think it's an awesome idea for home-schooling (actually, it's how things tend to work anyway) or for small-group instruction. With a large group there's nothing immoral about it, but you may be unable to find a practical path to a system that works like that. (Keeping in mind that public schooling is presently at least as much about babysitting as learning.) 

To expand a little further: my usual sources for these sorts of questions have not been as helpful as usual. Wikipedia summarizes the content, but has only a very sparse coverage of critiques and modern perspective. IEP covers Nietzsche in great depth, but barely mentions On the Genealogy of Morals. SEP has an article on Nietzsche's moral and political philosophy, but the discussion is so mixed between On the Genealogy of Morals and other works that I can scarcely recognize any of the Genealogy in it; this nonetheless makes me wonder whether when viewed as a whole the works of Nietzsche paint a clearer picture of which G.M. is an essential part. And in every case, the focus is more on what he said than was it true; a discussion or defense of the latter is what I am most sorely lacking because so often, even when attempting to take Nietzsche's use of terms like "aristocrat" into account, his claims seem so often blatantly wrong that I wonder why G.M. continues to be viewed with more than historical interest. Or maybe it is merely historical interest. Or maybe most everyone agrees that it was in large part blatantly wrong, but it was blatantly wrong in such interestingly different ways than intuitively obvious yet actually wrong views that came before (and keep arising) that it has value as sort of a buffer, a counter-narrative that undermines a tempting yet misleading view of human morality. *To clarify what I mean about lasting value, since this is apparently not an intuitively obvious term: philosophy as a field attempts to study things the way they are or should be, both as the primary field for several types of inquiry (morality, comprehensibility of the world, etc) and as meta-analysis for others (philosophy of mathematics and science, for instance). Philosophy is not merely an expression of human creativity or artistry (we have art and literature and music and so on for that). Therefore, to study philosophy, one needs to be familiar with what philosophy studies, and what progress has been made in that study: what are the most natural questions to ask, and what compelling answers have been given? Are there counters to those answers, and so on? In this vein, for a philosophical work to have lasting value, it must either demonstrate something that is (at least approximately) true and relevant--either for the first time or as one of the best explanations yet, or it must raise a question or open up a new branch of philosophy (or close off an old one) and do so in one of the most compelling and clear ways that has been devised. It is not of lasting value, in the sense I mean, if it was merely part of a historical trend of moving in a new direction. For example, Giotto's painting Christ before Caiaphas was part of a trend towards improved perspective in paintings, but it is of no "lasting value" for accuracy in painting because the method used is wrong--wrong enough so that one shouldn't duplicate it--and because it is not terribly clear from looking at the painting what it was that he was doing. 

which is the statement that there's a best and universally-agreed-upon system of morality. Counterexample by example It intuitively seems that moral systems must return certain answers, e.g. 

Try looking up "visual illusions" or various results from psychophysics. There's all sorts of stuff going on that you're not perceiving. I'm not sure philosophy has caught up with these observations enough so that it is even speaking about qualia and perception using the right terms. But, anyway, in some sense you're asking a definitional question (do we call it an "experience" if it is unperceived, or only if it is perceived--and what do we mean by "perceived" anyway?). But it is certainly the case that inputs you are unaware of can considerably change your mood and actions (or can change your mood in actions in ways that seem unconnected to the input, and you can be unaware of the impact). "Unconscious priming" is the term typically used to describe these phenomena. (If you want to read something even more disconcerting, look up retrodiction or implanted memories--temporal continuity might be happening, or it might just be invented by your brain afterwards to make sense of before-and-after, and you probably would have no way to introspect which was which....)