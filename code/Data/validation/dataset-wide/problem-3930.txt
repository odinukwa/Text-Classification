Start by fixing invertible matrics $A_1, \ldots, A_m \in \mathbb{Z}^{n \times n}$. For a sequence $i_1, \ldots, i_k$ we construct $A = A_{i_1} \cdots A_{i_k}$. We would like to know "Is 1 an eigenvalue of $A$?". As we are doing this for a large number of sequences (the naive computations when $n \sim 6$, $m \sim 16$, $k \sim 12$ take days) we can assume that any information (for example LU decomposition) wanted about $A_i$ is essentially free. 

Suppose that $M$ is an $n \times n$ matrix where each entry is a positive integer. Then $M$ is Perron-Frobenius and so has unique largest real eigenvalue $\lambda_{\textrm{PF}}$. 

First a little background. In racing it is possible for a player to win a tournament without winning a single race, however, how bad can a tournament winner actually be? Can a player win a tournament without even doing better than coming third? Or even fourth? Obviously this depends on the scoring method used for awarding points for each race. More formally, suppose $p$ players, named $\alpha_1, \alpha_2, \ldots, \alpha_p$, play a game consisting of $n$ races (with no possibility of ties for a position). Suppse that player $\alpha_i$ finishes race $j$ in position $\beta_{i,j} \in \lbrace 1, 2, \ldots p \rbrace$ (with $\beta_{i,j} = 1$ being the best possible result for player $\alpha_i$). And that for each race the points scored by a player are given by a non-negative, strictly decreasing function called a scoring function $f : \lbrace 1,2, \ldots, p \rbrace \to \mathbb{N}$, i.e. the player coming first receives $f(1)$ points, the player coming second receives $f(2)$ points and the player coming last receives $f(p)$ points. Let $\text{score}(\alpha_i) = \sum_{j = 1}^{n} f(\beta_{i,j})$ be the total score obtained by player $\alpha_i$. Let $\text{best}(\alpha_i) = \min_{1 \leq j \leq n} \lbrace \beta_{i,j} \rbrace$, be the best position that player $\alpha_i$ came in. We say that player $\alpha_i$ is a winner iff $\forall j \in \lbrace 1, 2, \ldots, p \rbrace$ $\text{score}(\alpha_i) \geq \text{score}(\alpha_j)$, note there may be more than one winner of a game. 

While studying the proof of Bott periodicity for operator $ K $-theory in this set of notes, I learned this fact: 

The proof that $ \Pi_{\mathscr{B}({L^{2}}(\mathbb{T})),U,V}: A_{\theta} \to {C^{*}}(U,V) $ is a $ * $-isomorphism follows from the non-trivial fact that $ A_{\theta} $ is a simple $ C^{*} $-algebra (i.e., it has no non-trivial closed proper two-sided ideals). The main idea behind the proof is to use the so-called trace function on $ A_{\theta} $. This trace function does wonders for us. Firstly, it shows that $ A_{\theta} $ contains a non-trivial projection element. Secondly, it shows that $ \mathcal{A}_{\theta} $ is faithfully represented as a $ \mathbb{C} $-algebra in $ A_{\theta} $, i.e., $ \mathcal{N} = \{ 0_{\mathcal{A}_{\theta}} \} $. Observe that in defining the $ C^{*} $-semi-norm $ \| \cdot \|_{0} $, there was no guarantee that each non-zero element of $ \mathcal{A}_{\theta} $ would not be sent by $ \| \cdot \|_{0} $ to $ 0 $. Playing around with the trace function shows that this is indeed the case. 

Let $ A $ be a non-trivial $ C^{*} $-algebra and $ n \in \mathbb{N} $. Setting $ \mathcal{D} \stackrel{\text{df}}{=} A^{n} \setminus \{ (0_{A},\ldots,0_{A}) \} $, we can define a function $ f: \mathcal{D} \to \mathbb{R}^{+} $ by $$ f(a_{1},\ldots,a_{n}) \stackrel{\text{df}}{=} \frac {\displaystyle \left\| \sum_{j = 1}^{n} a_{j}^{*} a_{j}^{\phantom{*}} \right\|} {\displaystyle \sum_{j = 1}^{n} \| a_{j} \|^{2}}. $$ Observe that $ \operatorname{Range}(f) \subseteq \left[ \dfrac{1}{n},1 \right] $. The upper bound of $ 1 $ is pretty obvious, and it is attained if we let $ a_{1},\ldots,a_{n} $ be equal to any non-zero element of $ A $. The lower bound of $ \dfrac{1}{n} $ is gotten as follows: 

Define a mapping $ \| \cdot \|_{0}: \mathcal{A}_{\theta} \to [0,\infty] $ by $$ \| a \|_{0} \stackrel{\text{df}}{=} \sup (\{ \| {\pi_{A,s,t}}(a) \|_{A} \in \mathbb{R}_{\geq 0} \mid \text{$ (A,s,t) $ is a $ C^{*} $-representation of $ \mathcal{A}_{\theta} $} \}) $$ for each $ a \in \mathcal{A}_{\theta} $. Proof sketch that $ \| \cdot \|_{0} $ is a $ \mathbb{C} $-algebra semi-norm: 

I've produced a table of monodromies for about 63% of the hyperbolic, fibred knots listed on knotinfo. This is available at: $URL$ $URL$ (this link now contains significantly more data - for the origional data look under /source/Fibred Knots/). This was done by producing a triangulation of every possible surface bundle over the circle for the surfaces $S_{1,1}, \ldots, S_{5,1}$ made from a composition of at most 15 Dehn twists about generators. Non-hyperbolic and non-knot complement manifolds were discarded and for each pair of isometric triangulations the short-lex later one was also discarded. Finally, for each hyperbolic, fibred knot complement listed on knotinfo, SnapPy was used to find a bundle on this list isometric to it if it existed. As Sam points out, there is no canonical choice of generating set for $\mathop{Mod}(S_{g,1})$ so I used the Humphries generating set in each case. However, the monodromies obtained are the short lex earliest for each knot with respect to this generating set and the ordering of the generators shown at the bottom of the page. This ordering was chosen to minimise the running time; a different ordering can run several orders of magnitude slower. I should point out that these results don't show the millions of knot complements that were also found but that don't (yet) appear on the knot tables. This simply comes from the fact that my tables are ordered by monodromy length whereas knotinfo's is ordered by crossing number. 

Leys, Ghys & Alvarez have also made a video series in a similar style about dynamical systems called "Chaos". The nine chapters are all available under a Creative Commons license. 

i.e. For any bound $n$ there is a word $w$ that must be made more than $n$ letters longer during any sequence of group actions that take it to it's 'first' word. 

Your matrix $B$ is the Hadamard product of $A$ and $A$ which uses the notation $B = A \circ A$. However I don't know of any others, particularly for expressing $y$. See: $URL$ 

Due to the recent spate of detections of gravitational waves by LIGO, my amateurish interest in the mathematics of general relativity has been revived. 

Notes: If Condition (1) is violated, i.e., $ A $ has a two-sided identity $ e $, then the inequality is false because then $$ 1 \nleq \sup_{b \in A, ~ \| b \|_{A} \leq 1} \| e b - 1 \cdot b \|_{A} = 0. $$ I do not know what happens if Condition (2) is violated, however. If $ A $ is a $ C^{*} $-algebra, then we only need Condition (1) because Condition (2) automatically holds. This can be misleading, however, because it is not at all obvious that non-unital $ C^{*} $-algebras should have an r.a.i. norm-bounded by $ 1 $. Fortunately, it turns out that in proving Claims 1 and 2, the $ C^{*} $-identity is all that is needed as it takes over the pivotal role played by Condition (2). 

Additional Information: The inequality is valid if we only assume that $ A $ is a Banach algebra that satisfies the following conditions: 

To accomplish Step 1, there are several methods. I understand that you have read Davidson’s book, so let me describe an approach different from his that is more algebraic in nature. 

I believe that one can avoid theorems about positive linear functionals on $ C^{*} $-algebras and invent a proof that is mostly Banach $ * $-algebraic in nature, with the finishing blow provided by the $ C^{*} $-identity. However, I do not see the light. I hope that my request is not too vague. Thanks! 

$ A $ has no two-sided identity. $ A $ has a right approximate identity (r.a.i.) norm-bounded by $ 1 $, which we denote by the net $ (e_{i})_{i \in I} $. 

I am guessing that the answer is ‘yes’, and my reasoning is as follows: If $ T $ is integrable to a strongly continuous unitary representation $ \pi $ of $ G $ on $ \mathcal{H} $, then the Gårding space $ \mathcal{G}(\pi) $ of $ \pi $ is contained in $ D $ because $ \mathcal{G}(\pi) $ is invariant under $ T[{\frak{g}}] $. By the Dixmier-Malliavin Theorem, $ \mathcal{G}(\pi) $ is the set of all vectors $ v \in \mathcal{H} $ such that the function $ g \mapsto [\pi(g)](v) $ from $ G $ to $ \mathcal{H} $ is smooth, so $ \mathcal{G}(\pi) $ contains $ \mathcal{A}(\pi) $, which denotes the set of all vectors $ v \in \mathcal{H} $ such that the function $ g \mapsto [\pi(g)](v) $ from $ G $ to $ \mathcal{H} $ is analytic, i.e., has a power series expansion in some local analytic chart around each point of $ G $. Lemma 7.1 of Nelson’s paper then says that $ \mathcal{A}(\pi) = D_{T,\beta} $, so we obtain $ D_{T,\beta} \subseteq \mathcal{G}(\pi) \subseteq D $. This argument also shows that $ D_{T,\beta} $ does not depend on the ordered basis $ \beta $ chosen. I might have made a mistake in my reasoning, so I would appreciate any critique. Even if my argument is correct, any suggestion toward a simpler proof is warmly welcome. 

Is deciding if an integer square matrix has determinant $\pm 1$ faster that calculating the determinant of the matrix? 

Suppose $G$ is a finitely presented group with generators $a_1, \ldots, a_n$. Suppose $f \colon G \to G$ is a group endomorphism specified by defining $f(a_1), \ldots, f(a_n)$. As expected, we define a fixed point of $f$ to be any element $g \in G$ such that $f(g) = g$ and, as $f(\mathop{id}) = \mathop{id}$, we say that $\mathop{id}$ is the trivial fixed point. For example, let $G = \langle a | \rangle$ and $f$ and $g$ be defined by $f(a) = \mathop{id}$ and $g(a) = a^2$. Note in both cases $f$ and $g$ have no non-trivial fixed points and for this particular group we can determine that an endomorphism $f$ has a non-trivial fixed point if and only if $f(a) = a$. 

I would like to use the Lenstra–Lenstra–Lovász lattice basis reduction algorithm (LLL algorithm) to compute the minimal polynomial of a (real) algebraic number $\alpha$ from a decimal approximation $a$ (and bounds on its degree and height). A fairly standard idea for how to do this is to pick a large constant $N$ and apply the LLL algorithm to the lattice: \begin{eqnarray} &(1 \; 0 \; 0 \; \cdots \; 0 \; \lfloor N \rfloor),& \\ &(0 \; 1 \; 0 \; \cdots \; 0 \; \lfloor Na \rfloor),& \\ &(0 \; 0 \; 1 \; 0 \; \cdots \; 0 \; \lfloor Na^2 \rfloor),& \\ &\vdots& \\ &(0 \; \cdots \; 0 \; 1 \; \; \lfloor Na^d \rfloor)& \\ \end{eqnarray} The first vector of the basis returned by the LLL algorithm is $$ \left(a_0 \; a_1 \; \cdots \; a_d \; N \sum a_i a^i \right) $$ As all of these entries are small, we obtain a small integer relation between $a^0, \ldots, a^d$. See wikipedia for an example of doing this to compute a quadratic that has a root close to $1.618034$ using $N = 10000$. However this process depends on the choice of $N$. Too small will result in underfitting while too large will result in overfitting.