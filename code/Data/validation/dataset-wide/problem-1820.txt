At this point, unless your email system is providing your business some impressive value-add, it's probably best to think of it as a commodity. It's going to be hard to find an ROI that is better than an outsourced solution unless you have a very compelling argument as to why you need to bring it in-house. With the Google Apps, you have some flexibility should you change your mind down the road. But without dedicated sysadmin support and some specific value-add that you can justify in savings, I don't think you'll beat Google's price once you factor in hosting, hardware, and the opportunity cost of your sysadmin not working on revenue-producing activities because your mail server's IP is suddenly on a blacklist. 

We're just trying to bring our Macs into the fold here. My original plan was to use Backup Exec's Mac agent. Then I found out that the agent doesn't support 10.9, or even 10.8. So if you're keeping the OS up-to-date, that's out. I've heard legend tell that the next SP will get it up to speed, but I'm not holding my breath. It has been a few years, but Retrospect used to be the gold (and only) standard for Mac backup. Install the agent and you could set a schedule so the Macs would back up once connected to the network. I don't have recent experience with it, though it did work via VPN many moons ago. You'd then want to have it save the backup sets to storage that you would sweep into your existing backup environment. If you get a Mac Mini with OS X Server, you can redirect Time Machine on the laptops to the network, then sweep that connection up with another disk backup tool. I don't know if there's any granularity to Time Machine, though -- I believe it grabs the entire disk, or nothing. I know you mentioned cloud may not be an option, but if that is because of the VMs (which are now out of scope?), then perhaps that makes your CrashPlan/BackBlaze/Carbonite options more palatable. If you do want to bring the VMs in scope, you could install a Windows-based agent in the VM, and treat that as you would anything else. 

I'm trying to build a single GPO which will add "Log on as a service" on all servers in a system to all accounts under the corresponding service accounts OU. So, and should be able to log in as a service on and , while and should be able to log in as a service on and . I think I should be able to do this using Item Level Targeting, but I haven't figured out how to parameterize it. Am I correct in this? And is it possible? The domain is currently 2008R2 functional level, but we should be able to raise it to 2012R2 "quite soon", if needed. 

It's been a few weeks since we upgraded to SBS 2011, and I'm trying to keep a good look at the server logs at all times. I'm alarmed however, that I get quite a lot of errors and warnings, so I've been trying to resolve the underlying problems I percieve must cause these alerts. But it seems some of them aren't really supposed to be fixed? For instance here: $URL$ Microsoft seems to recommend not doing anything about quite a few problems, 

We have a multi-site domain with several sites in a hub and spoke-design. From the hub site, we have established a one-way trust to another domain though a VPN tunnel. Now, when someone from the external domain tries to authenticate, our domain controllers will return a referral to the other domain. However, the tunnel is only open for the domain controllers in the hub site. Can we somehow put use a proxy in this situation? I've been reading up on AD LDS, which comes up in searches on the topic, but it is not apparent to me how/if it would work. It seems OpenLDAP also has some proxy functionality, but I'd rather not introduce it unless it is the only option. Any other options? 

This is a bit of a broad question, so this is a broad-stroked answer. In general, a NAS is more of an appliance than a server. The OS is not as full featured as a dedicated server OS, but it has an interface to let you share files. A common feature is to have different RAID levels (0 & 1 almost always, 10 and 5 common) available for data availability. This increases uptime, since a single drive failure (other than RAID 0) will allow you to continue to work. It is not backup, since if you lose the NAS in a fire, you've lost everything. There are plenty of other features, but I'm sure that there are other better descriptions of NAS elsewhere. The good news is that less features generally means less care and feeding over time. But there's also less functionality as well. You won't necessarily be able to install any arbitrary application on it, like SQL Server. In your case, since you're talking about a file-server replacing NT (!), a simple NAS may be worth considering. The big questions that only you can answer are how much downtime are you willing to accept, and how much are you willing to spend to mitigate that risk. 

By request, I'm breaking this out of a comment... WesleyDavid's solution (using PowerShell, presumably in combination with Group Policy/Preferences) is the best solution to fit the problem as described, but that may not be the best solution overall. The questioner should probably consider a dedicated systems management package. If management wants to prioritize ease-of-use for the tech, then it may make more sense to use Altiris, Microsoft System Center, KACE, etc. to accomplish this goal. They're more likely to give reporting that management will like (X% of workstations had program Y installed) and help the less-skilled techs. Also, while a bespoke collection of scripts will give maximum flexibility, the system management packages can probably get you most of the way to your goal in a fraction of the time. If you should run into some particular issue, there's more likely to be a community of support and consultants that can help you out. The trade-off, of course, is money. But given that the questioner is mid-level IT, it may be more useful for him to work on other issues with a more direct business need than to work in an area that has is more of a commodity. My network is smaller than many here, I'm sure: around 120 PCs, 20 servers. I've done scripts and GPOs because I've had to, but we're looking to get one of these up just for the patch management and easier reporting to the powers-that-be. 

However, name resolution works from the command line with every utility I've tried (, , ...). The server is also running a , but it seems to have been shifted over to using another name server at some point ( has the server IP listed, but commented out, and instead points at the router, which forwards to the ISP's DNS servers). Does have some internal way of doing name resolution? I have never looked at a file before today (what has been seen cannot be unseen) but couldn't make much out of it. It did not seem to mention name resolution. Any ideas what I should check? EDIT: The requested config files: : (192.168.0.25 is the server, 192.168.0.1 is the gateway/router) 

I followed the technet article here: $URL$ (linked from the SBS 2003 -> 2011 migration guide). When uninstalling Exchange, I got a warning about NNTP not being found in the registry, but that didn't seem relevant, and the uninstall continued. The server was subsequently removed from the domain and shut down, as per the instructions. If I open the Public Folder Management console on the Exchange 2010 server, the public folders \NON_IPM_SUBTREE\EFORMS_REGISTRY and \Archived mails gives an error on "Update content". I haven't found anything else which indicates something is wrong. We never really used the public folders on the old server, so there isn't really anything lost. Can I just remove these folders and let them be created anew? 

I've got an issue with a user whose Outlook runs really slow. Her Exchange mailbox is ~4.6 Gb, which while large should be alright (she's got a 3.2 GHz i5 with 4 Gb RAM)? However, the .ost-file is 20.2 Gb. Why is it so much larger? I've run scanpst/scanost, which reported it repaired a few errors, but the size didn't shrink. I've turned off Cached mode since it was causing thousands of conflict-copies, which was also filling up this user's mailbox. Any suggestions on what to do? We're running Outlook 2007 on Windows 7, connected to Exchange 2003 on SBS 2003. 

The short question: can I share file and block level traffic on the same SAN? Perhaps more importantly, should I? The gory details are below... I'm hopefully putting the finishing touches on a new SAN design, and our new planned storage (EMC VNXe3100) will support being an iSCSI target, our original goal. It also supports file-level storage as well via CIFS and NFS. Some of the features we hope to use (particularly deduplication) are only available via file-level shares. The VNXe3100 has 2 controllers with 2 NICs per controller. Each NIC is going to a different switch, so either the controller or the switch can fail, and we should still be in business. This means that both file and block traffic would need to be enabled on each NIC. I'm assured by our rep that this is possible. My plan is to put the VNXe and the 5 host servers on the same VLAN and subnet (call it 192.168.1.x). This should keep my block-level iSCSI stuff only in that VLAN with no route out. But I would have a route out to the rest of the network for the file-level traffic on a different subnet (192.168.55.x). So each NIC would have an IP address for block traffic in the 1.x range and another for file traffic in the 55.x range. Since we are new to the world of iSCSI and the world of SAN/NAS devices, I want to make sure this isn't some horrible intermingling. But it would be really nice to expose our VMWare as NFS and get the VMs deduplicated on our hardware, and not having to maintain another file server would also be a bonus. If there's something else I'm overlooking, I'm all ears. 

This seems to indicate that only LDAP is needed? Checking open TCP connections while attempting to use an external account, I can see an established connection on port 389 to a domain controller in their domain. Any ideas what to try? I've seen some recommendations to add the NPS server to the "RAS and IAS Servers" group, but that would seem to require a two-way trust. 

I have a server with a LSI MegaRAID 9261-8i controller. Recently I started getting alerts like this one: 

There are plenty of google results for the same event id but different error ("access denied to Program files\Common files\Microsoft shared\Web server extensions\14\LOGS", had that error, but fixed it following the instructions given on the web), but I cannot find any reference at all to this error. Any ideas? 

One of our hosting providers has been setting the "Turn off Automatic Root Certificates Update" underlying registry key as part of their default Windows installation. After the servers join our domain, we'd like to allow the servers to run Automatic Root Certificate Update. However, simply creating a GPO and setting "Turn off Automatic Root Certificates Update" to Disabled does not actually flip this back. Is there some GPO way to do this? Or do I have to push a script which actually flips the setting? We're running 2012R2 and 2016, if that matters. Domain functional level is 2012, though (for obscure compatibility reasons, sigh). 

I'm buying an UPS for the new server. As always, money is short, so I need to buy one which is sufficient, but not needlessly overpowered. The server has a 900 W power supply, but the retailer says he doubts it would ever draw over 300 watts. I went out and bought an "energy meter" today, which can show the consumption, but now I need to test how much I can get it to use. When starting up, the max usage was around 230 W. How can I test the max usage? Is there software with the only usage of making the computer work a lot? The server has a single Xeon W3565 (3.2 GHz) processor, 24 GB RAM and 6 SAS disks. 

VMware best practice now is to install vCenter on a VM with HA. That's from a VMware training class when 5 originally came out. HA doesn't require vCenter to actually be working once set up, as the hosts know what to do. I have Essentials Plus, use this setup, and can attest that it works well for us. Just make sure that you have enough capacity on your hosts to accommodate HA. 

Try reducing your jumbo frame size to 8000. If I remember correctly, 9000 is the maximum standard for jumbo frames, and sizes that are over what a switch can handle get broken down. By going to 8000, you won't have the overhead break your maximum frame size. Once you've got that working, then you can start tuning to get you as close as you can to the largest packet size. 

To strictly answer your question, there's always good old mistakes with or . Mistakes with those tools are not fun. I (ahem) have a friend who may have accidentally shut down a server when I -- errr, he -- meant to just logout. But it's not a bad idea if the person legitimately needs those credentials. I log in with mine many times a day, but as a sysadmin, I need them all over. That said, I don't need admin credentials to read email and browse the web. If you're in a regulated industry (PCI DSS, SarbOx, HIPAA), you may be required to separate your duties out as much as possible, so an admin can be putting the company (and possibly him- or herself personally) in legal jeopardy. Frankly, that's what finally got us over to being better with our admin credentials. So the real takeaway, I believe, is to find out why the user is using the domain admin credentials. If the user is creating resources, installing software, etc., then perhaps that's what they need. If you have the time, you can always delegate out lots of AD individual privileges -- we let our helpdesk guy join PCs to the domain and change passwords, but that's about it. But just logging in with domain admin credentials, if you're an admin, doesn't necessarily mean there's cause for alarm. 

We're planning to migrate from SBS 2003 to 2011 in a month or so, and I want to try out the upgrade process before I do it on the real server. I haven't bought the 2011 licence yet, but noticed that there is a trial version at Technet. Is the trial version complete enough to try out a migration? I also don't have a spare 2003 licence, is there a trial version of 2003 still availible? And in that case, is it upgradeable? If not, any other way of trying it out? I suppose I could use the existing licence, but that isn't really ok, is it? I'm not good enough at legalese to understand the EULAs, maybe it is considered fair use (if there is such a thing)? Any good advice in general in trying these things? I intend to do it virtually since I don't have heaps of spare hardware, but that shouldn't be an issue, right? 

Yes. Very much so. You need to generate the key as the user you will be running the ssh client as, on the machine you will be running it from. The key is tied to that identity. As for the authorized_keys file, you need to make sure you add it to the directory specified by the sshd_config file. The default is ~/.ssh/, so look in the .ssh directory in your home on the remote computer. If there is no authorized_keys file, just create one containing the key you generated on the client. Otherwise, append it. If it still does not work, you're either not trying to authenticate with the key (more probable), or the server does not accept key login (less probable). In PuTTY, have you set the private key setting under Connection > SSH > Auth?