If you check data.workdbank.org, then go to the "databank", you can select "World Development Indicators" (WDIs). Then you can select China, and searching for "value added" within your browser you can get a variety of measures of agriculture, manufacturing, services and industry (e.g., % of GDP, in USD). The time series goes back long before 1990. Even if not disaggregated enough for what you want to do, it could be useful as a point of comparison. 

If a biotechnology advance lowers the production price of corn from 3.50 dollars a bushel to 3 dollars, then farmers who do not have access to this technology now must sell at 3 dollars without enjoying any of the productivity gain, a reduction in income of 50 cents a bushel. In considering the potential truth in the statement in the textbook (with respect to the context of 2018), one must focus on the question of "who is hungry?" and not "how much food is there?" There would be more food, but the distributional effects would be negative on those most likely to be hungry. Who is hungry? Many of the people who are most likely to be hungry are poor farmers, who would simply receive a lower price and thus have less economic resources to obtain goods and services on the market. 

Sources Campbell, J. Y., 1993. Intertemporal asset pricing without consumption data. American Economic Review, 83, 487-512. Campbell, J. Y., 1996. Understanding risk and return. Journal of Political Economy, 104, 298-345. 

A representative agent chooses a consumption time-path to maximize expected discounted utility $E\left[\sum_{i=0}^\infty\beta^i U(c_{t+i})|\Omega_t\right]$, where $\Omega_t$ is the information set at time $t$, subject to the inter-temporal budget constraint, $c_t + p_tq_t = r_tq_{t-1} + w_t$, for all $t$. The optimal consumption path satisfies: $p_tU'(c_t) = \beta E[r_{t+1}U'(c_{t+1})|\Omega_t]$, for all $t$ Which gives the Euler equation: $E[\beta(r_{t+1}/p_t)[U'(c_{t+1}) / U'(c_t)]|\Omega_t] - 1 = 0$. He then imposes the CRRA utility function $U(c_t) = c_t^{1-\gamma}/(1-\gamma)$, making the Euler equation become $E[\beta(r_{t+1}/p_t)(c_{t+1}/c_t)^{-\gamma}|\Omega_t] - 1 = 0$. This yields the moment equations: $E[\{\beta(r_{t+1} / p_t)(c_{t+1} / c_t)^{-\gamma} - 1\}z_t] = E[E[\beta(r_{t+1} / p_t)(c_{t+1} / c_t)^{-\gamma} - 1|\Omega_t]z_t] = 0$ with $z_t$ a vector of instruments belonging to $\Omega_t$. 

First, it is important to note that there is a substantial literature both developing and using nonlinear VAR estimation (for example, see papers here, here, here, and here). The reasons linear VARs are seen a lot, though, are similar to the reasons that least squares (in its varying forms) is seen a lot. The model is very transparent and analytically tractable. This is a benefit for both pedagogy (analysis can be done on linear VARs just using lag polynomials) and for interpretation of results. You also don't run in to the concavity and global vs. local solution issues you do with big nonlinear problems, which can make your results more credible in many circles. 

A great many things can be said on the question. But it might be worth pointing out that people have been building stuff for a very very long time, so maybe 1% productivity growth is not a bad result. The most difficult is anti-corruption. You could compare the free press + independent judiciary + empowered investigative authorities within specified limits + free elections ... with the approach of President Xi, whose name is now entrenched in the Chinese constitution. However, it seems that you're more interested to motivate a specific theory. So perhaps you could develop that and come back with a question that is more aligned to what you want to know. For example, you could be additionally descriptive about what it is about Tesla's operations that might have broader lessons in construction? (Note that Tesla makes cars, not ports, not bridges and not roads.) 

You don't have to apply a single continuous function as required in many textbook applications. Theoretically, you could say something like "beyond production level x, some other factor kicks in which begins to apply pressure". So, below production level x, f(x) is the function, and above production level x, the total cost function is the sum of f(x) + g(x). This is still a somewhat simplistic and theoretical representation, but for practical applications could be updated with some variety of parametric or non-parametric approaches. For example, perhaps some social media platform succeeds quite a lot in reaching 1 billion users under economies of scale production conditions, but then its per unit costs rise a lot beyond some threshold, for example due to: concerns related to privacy, data localization, spread of fake news, speech which promotes violence or hate against individuals based on ethnicity or religion, etc. Each of those concerns would enter as an essentially separate cost function into the overall production activities of the company. (In terms of pro-competition concerns, it would be arbitrarily easy to not apply such rules to start-ups with size under one thousand or one million users, etc). 

The only reason I can see right now could possibly be pedagogical. While that term does indeed just equal zero, there could be generalizations where the agent works a high amount with probability $\tau$ and a low, but positive, amount with probability $1-\tau$. In this case, the term would still go through. There are also plenty of functional forms that would result in that term not being zero (taking away the -1, for example, which is often done in applied work). At any rate, this is an idiosyncratic enough case that the professor might have thought it best to show the process of rearranging terms for future reference and to give help with other scenarios. On the other hand, he may just not have noticed it. :) 

Actually, Section 3 of the paper you linked is probably one of the most comprehensive lists you could find of the movers and the shakers with regard to predictive game theory. I know that many of them have both published and working papers addressing these issues. Fudenberg even addresses those specific two topics. A few selections from the paper: 

GDP is a statistical indicator which is an estimate of production that has already occurred, or an estimate of the current or future level of production. The units are generally nominal, or adjusted by some variety of methods to correct for different changes in price levels across a changing production structure (e.g., more smartphones, less typewriters). So I'm not sure if this involves confusion or disagreement about the precise meaning of "GDP", or a misunderstanding about the sequential order of logic. Specifically, the order in time is 1) production occurs, 2) there is a measurement indicator intended to reflect the production which actually occurred. For an example of how this could be important, consider the following example. Production increases equally across all sectors by 5%. However, a change in price levels can differ from one sector to another. So using some methods the statistical indicator called "GDP" will increase by precisely the same 5%, and according to other methods of calculating GDP it could be different from 5%. Conceptually, differences in these methods might be straightforward to understand in principle, but implementing the methods can become complicated, and there is liable to be some "art" in determining which method is most appropriate for measurement, or for the use of the statistical indicator called "GDP" depending on the specific application. For example, if exchange rates are very important, such as in trade analysis, "real GDP" could be less important for knowing the effects on the overall budget situation of the government or the cost of capital in the financial system. 

The simple answer is they estimate the demand curves for each product and, using their cost structure and market characteristics (competition structure, etc.) set price to maximize profits. This is standard for any firm, though. How Google in particular and these big firms in general (Amazon, Microsoft, etc.) estimate demand curves is somewhat different than the usual economist might do it. For usual demand estimation, a researcher would have to make use of market idiosyncrasies to identify demand. For example, using supply shifters with 2SLS for basic demand estimation, BLP for discrete choice with heterogeneous products, etc. Identification is such a big issue for demand estimation because a researcher generally just observes equilibrium (p, q) combinations, not the actual demand curve. We are also often constrained purely by the amount of data available. For a big firm like Google, however, they 1) have the ability to enact exogenous perturbation in price to see how sales change and 2) have access to tons and tons of data. Using 1) they are constantly running little experiments to see how consumer behavior changes. They can then use the results to actually trace out the demand curve. In these experiments, the firm could easily take into account things like movie popularity, genre, etc. With respect to 2), Pat Bajari, chief economist at Amazon and one of the biggest names in modern empirical IO, has a (at this point) working paper with Nekipelov, Ryan, and Yang on how to use machine learning to estimate demand curves across products with lots of sample points bunches of characteristics (think thousands of product characteristics). As a "fledgling computer science researcher," you'd probably be in to this. This approach is especially relevant for people/firms with access to tons of data (like Google, Amazon, etc.) 

This terminology is used for cases which may constitute a violation of civil and/or criminal law, depending on the specifics. I don't think "fraud" would necessarily apply, because the facts of a "bait and switch" tactic does not imply that a "fraud" (in the legal sense) would necessarily follow, even if the consumer had ultimately been manipulated in some way that was illegal under laws pertaining to (among other possibilities) misleading advertising. Ethical applications: I don't see that there is a need for a specific term involving ethical applications (or applications where the ethics are ambiguous, as opposed to unambiguously unethical). For example, a fundamentally honest communication can be made to the target market without implying that they are lying or misrepresenting themselves to the target audience. Maybe something under "information economics" would lead to some theoretical structures or vocabulary which could assist in achieving an appropriate level of directness and/or nuance with respect to both a) very generalized treatment and also b) more discerning typologies of retailers/sellers that are in some manner not forthright in their overt presentation in order to gain access to a market. 

The primary literature concerned with this type of question (at least where classical results break down) is behavioral economics. There's a great general compilation of papers put together by the Russell Sage Foundation called the "Behavioral Economics Reading List" that includes, among other things, a General Introduction section with overview papers by some of the big movers and shakers (Camerer, Kahneman, Laibson, etc.). Many of the papers you will find through the Russell Sage paper list will be on alternative methods to classical consumer theory. If you want just the tests of assumptions/predictions, I would recommend looking through the abstracts of John Lists's papers on Testing Economic Theory. List is one of the most prolific experimentalist authors on the subject and has weighed in on most questions that other people work on in this field as well. Just reading his abstracts on that page should give you a pretty good idea of the state of the literature up through 2011 (the website isn't updated). His CV is updated, but doesn't give you the abstracts without looking up the papers individually.