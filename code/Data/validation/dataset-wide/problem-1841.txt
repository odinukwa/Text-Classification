Look at NUT. It handles this well. Define number of power supplies from each UPS and number of required power supplies. Shutdown will not be triggered as long as there are sufficient power supplies not on UPS. 

Without modifying your job so that more work can be done in parallel, you will likely see worse performance if you manage to even out the load. The two CPUs which are at 100% are likely dedicated to two parallel streams, and moving them around will cause additional overhead for context switching and loading CPU memory caches. The other two may be sharing additional load which may be bottle-necked by the first two processes. If you can increase the degree to which the job can run in parallel you might be able to shave time, but it should still take over half an hour. (current runtime * 260/400 should be your best case improvement.) Bottle-necks are common in cases like this. If you solve one bottle-neck, you will likely discover another. Some database problems can be solved by modifying the queries involved. Given that CPU appears to be the bottleneck, it may be possible to improve the database code. This would require tuning the query using the most CPU. Profiling the application may yield information which would help in optimizing it. 

All the other answers given handle package installations. Without root level access there are few places a user can install software: 

Copying messages to a folder is normally done by the Mail User Agent. I am not aware of a switch to that would do this. However, you could add you userid to the list of addresses on the command line to get a copy of the message. There should be a switch to deliver the copy as a blind carbon-copy. The copy will contain a subset of the headers that exist on the message when it is finally delivered. It will contain the key headers such as , , , and . Other headers get added as the message is passed from server to server for delivery. 

Filter by host name. (Each appliance should have its own hostname). If you want you can listen on multiple ports and handle each port separately. Facilities are designed to handle message categories like authorization, mail, printer, ftp, etc. As UUCP isn't used much anymore you could likely use it for your own uses. Their may be other unused facilities in your configuration. However, you are better off using standard facilities values, and filtering by other data. There are 24 facilities as they are names for bits in a bit mask. This makes aggregating arbitrary sets of facilities in the same log. The protocol is specified in RFC 5424. The other field is severity. Normally logs contain all logs at or above a given severity. (More severe priorities have lower values, so the normal comparison is less than or equal to the selected priority.) However messages of a given facility can be selected, as is often done for the debug log which collects debug messages for all facilities. You may want to aggregate data for some facilities in the same log regardless of originating appliance. It is common to log the same message to multiple log files. 

If you are using Linux for your firewall, look at which is how the firewall is implemented. The Shorewall documentation will point you in the direction of . It may be possible to get to add the appropriate entry when it sees an login success message in a logfile. In my case I use authentication per service: 

How are you maintaining the time on your server. If you have a poorly running clock, then the time may need to be jumped periodically. This may cause sudo to notice future timestamps and try to fix them. However, I would expect different message. If you are using with poor conectivity or a large step setting time may jump far enough that sudo would be concerned. 

If you want only one domain accessible via , configure the virtual host listening on port 443 to only handle that domain. The virtualhosts I would use are: 

Add an alias for the address that should be delivered to the Exchange server with the domain of that server. This would go in . The alias should be something like: 

The purpose of the selector key is to allow multiple keys to be used. The design of DKIM assumes that you will have multiple keys for various reasons. There is a requirement that each active key have a different selector key. It is recommended to replace the signing keys periodically. When doing so, you create the record for the new key, and retain the old record until you can reasonably assume all mail signed with the key has been validated. When using third parties to deliver mail (common for businesses), the third parry should be provided their own signing key. The public key would then be added to DNS as an additional record. When using multiple MTAs to sign mail, it may be more secure to generate the keys locally. While the private key needs to be kept secure, the public key can be safely transferred. The public key, would be sent to the DNS maintainer to add the appropriate selector record. 

The nodes do not send data to the master. The master pulls data from the client. You need to add the node to the , and restart the master. The master will then pull data from the node. Normally, the master will be one of the nodes that is monitored. You can telnet to port 4949 on the node from the master to test. The protocol is text based. I documented my experience Monitoring with Munin. 

EDIT Something like this should forward all unhandled mail for local domains to notlocal.host.ref.example. Mail for root is not forwarded. This router has not been verified, and does not handle address validation. You may need to remove the cannot_route_messages from a prior router. 

Look into clustering support for your server. Properly configured clustering will replicate your session data between the two servers. 

I believe Dovecot uses the Date: header. I've run into this issue when moving messages without a Date header. Some were from an old mail system, but most were from the Sent folder. Relatively few were sent by servers that didn't ensure the Date header was present. You may be able to resolve this by adding the header and forcing it to reindex the folder. You may be able to use the utility to add the Date header. You may need to chain commands and create a new directory for the fixed email. If you have a Maildir format store, you may be able to move the cur subdirectory to a new name like old, and rewrite back to cur. 

You may want to consider using Puppet or CFEngine to make changes to the servers. Tools like these allow you make controlled changes to all servers. They help a lot keeping the configuration of multiple servers in sync. 

Try looking at the output without the . The pattern '10.241.169.7' matches 11 addresses. You may want a pattern like which reduces the number of commands required. You don't need nor all the netstat options. Try a command like: 

Any or all of these responses may be cached by the name server you are using. The first lookup is most likely a cached lookup. You may want to change the NS records for your domain on the old nameservers point to the new nameservers. This will add an additional lookup, but may get the correct nameservers responding. 

Use followed by the value you want. Zero is considered success, non-zero failure. This is typically used within a script, and terminates the (sub)shell it is exited from. If you want to capture the status of a command, assign to a variable. This allows you to save the value after displaying it. 

The basic password authentication will do what you want. However, the authentication tokens pass in clear text on an http connection. You probably should require https for remote connections. This is also available. Using an ssh tunnel will also do what you want. You may find that port 80 is blocked if you are outside your ISPs network. I generally configure access without a password from the LAN and add security as required outside the LAN. 

It looks like you are getting --one-file-system behavior. This may be the default on freebsd. I also see that your destination is not a root account which should break retaining permissions and ownership. 

It may be an ARP issue. See the Shorewall One-to-one Nat notes. I tend to prefer a firewall builder tool like Shorewall for things like this. 

It seems that the new version has improved security. Without encryption you are passing user ids and passwords in clear text or a near equivalent. If things work with SSL enabled, you shouldn't have much of a problem. If you are looking for documentation on how to enable harvesting of your passwords, you likely want to undo some documented steps. Look for documentation on how to enforce SSL on connections. Find where that has been done and remove that configuration. I would not do so or allow an administrator reporting to me to do so. 

If you have access to your DNS configuration, it is rather trivial to discover this data. However, anyone can have a CNAME pointing to your server. You won't be able to trace these. As @wombie has pointed out you can't do a reverse lookup for CNAMES. There is no PTR equivalent for CNAMES, and even if there where it is likely only some records would exist. A quick check of a random selection of domains would show PTR records often don't point back to the A record. Likewise, doing reverse lookups of PTR records for random addresses often doesn't find the corresponding A record. EDIT: CNAMEs are not the only way to alias a system. DNS allows multiple A records to point the the same address. Functionally this is the same as adding a CNAME but the method is different. Same problems apply outside your domain. To search for the various A records you would search for the IP address(es) of the system in question.