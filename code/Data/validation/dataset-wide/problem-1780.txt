We are migrating from SVN to Mercurial and this question has arised practically from day one. So, here is the given: 

I have just started to learn DSC and have no runbooks. So far I only used DSC configurations, no runbooks. So, is it only possible for runbooks? Does it mean that nobody works with just DSC configurations? P.S. Knowing that DSC stands for Desired State Configuration I find the term DSC configurations a bit funny. 

I am trying to grant a read-only access to my temp directory to another user from the command line using icacls.exe. Tried the following (PowerShell): 

I actually tried a different approach, which yields the same result (probably because both "sit" on the same data): 

As you can see, I am OK with having multiple heads in the backup repository, unless you convince me otherwise. So, the question is about REV and URL, namely I see the following options for them: 

It should be added to the Sql Server as a login account in sysadmin role. Visual Studio extensions that I want every developer to have must be installed from a session started by the owner account (not the SYSTEM account used to apply the DSC configurations) The TFS workspaces must be associated with the owner account. 

Just FYI, the case sensitivity is dependent on the collation. There are case-insensitive colations you could use as a workaround, perhaps temporarily. 

So my understanding of one scenario that ZFS addresses is where a RAID5 drive fails, and then during a rebuild it encountered some corrupt blocks of data and thus cannot restore that data. From Googling around I don't see this failure scenario demonstrated; either articles on a disk failure, or articles on healing data corruption, but not both. 1) Is ZFS using 3 drive raidz1 susceptible to this problem? I.e. if one drive is lost, replaced, and data corruption is encountered when reading/rebuilding, then there is no redundancy to repair this data. My understanding is that the corrupted data will be lost, correct? (I do understand that periodic scrubbing will minimize the risk, but lets assume some tiny amount of corruption occurred on one disk since the last scrubbing, and a different disk also failed, and thus the corruption is detected during the rebuild) 2) Does raidz2 4 drive setup protect against this scenario? 3) Does a two drive mirrored setup with copies=2 would protect against this scenario? I.e. one drive fails, but the other drive contains 2 copies of all data, so if corruption is encountered during rebuild, there is a redundant copy on that disk to restore from? It's appealing to me because it uses half as many disks as the raidz2 setup, even though I'd need larger disks. I am not committed to ZFS, but it is what I've read the most about off and on for a couple years now. It would be really nice if there were something similar to par archive/reed-solomon that generates some amount of parity that protects up to 10% data corruption and only uses an amount of space proportional to how much x% corruption protection you want. Then I'd just use a mirror setup and each disk in the mirror would contain a copy of that parity, which would be relatively small when compared to option #3 above. Unfortunately I don't think reed-solomon fits this scenario very well. I've been reading an old NASA document on implementing reed-solomon(the only comprehensive explanation I could find that didn't require buying a journal articular) and as far as I my understanding goes, the set of parity data would need to be completely regenerated for each incremental change to the source data. I.e. there's not an easy way to do incremental changes to the reed-solomon parity data in response to small incremental changes to the source data. I'm wondering though if there's something similar in concept(proportionally small amount of parity data protecting X% corruption ANYWHERE in the source data) out there that someone is aware of, but I think that's probably a pipe dream. 

Alright. The problem is solved. Here is what I did: I switched off my ESX. I started it again: DHCP started I tried the PXE install, got the 4011 error. Double-checked if my WDS is running: WDS failed to start. Why? Because port 67 wass already used. I opened WDS and then set the "Do not use port 67" option and thats it. Everything worked. So at the end it was just a simple restart to recognise, that I need to start the DHCP first so it gets assigned the port 67 for sure and that WDS does not care about that. 

You could use PHP: First that comes up to my mind is the use of "$_SERVER['HTTP_REFERER']" however, this can be disabled by the user on the client side if that is shown or not as far as I am concerned, please correct me if I am wrong. So you could do something like Pseudo code: 

So basically the script checks in the beginning if the variable is set, if it is not set than it must be new visitor, therefore it should redirect to the homepage. If that is actually set then it should not redirect. EDIT: Recognised, my first solution was wrong since it could not match if he actually came from a subpage. 

Scoped to a specific filesystem name :No header so that first line is a snapshot name : List snapshots (list can list other things like pools and volumes) : Display the snapshot name property. : Capital denotes descending sort, based on creation time. This puts most recent snapshot as the first line. : Says include children, which seems confusing but its because as far as this command is concerned, snapshots of TestOne are children. This will NOT list snapshots of volumes within TestOne such as . : Pipe to head and only return first line. 

We are doing nightly full backups and noon differential backups. We use Full recovery model with SQL Server 2005, but logs are never backed up and are truncated(TRUNCATE_ONLY) after the full backup. Restoring to a point in time is not a requirement, restoring to one of the nightly or noon backups is sufficient (Not my decision). So the question at hand is, since they are throwing away the logs every night, is there any reason to not use Simple Recovery model? Is there any benefit to using Full Recovery model if we are throwing out the logs every night? Thanks. 

Server Manager → Manage → Server Manager Properties "Do not Start manager automatically at logon" Server Manager → Local Server → IE Enhanced Security Configuration → Off 

The problem is that the system temp directory is required when machine installs updates first thing after a restart. My RAM disk, however, starts through , which is probably read later in the process. My question is this - is there a way to redirect the system temp directory to a RAM disk in a way that this redirection is realized before windows installs the updates after restart? Use case: I have installed a RAM disk that gets initialized on startup. Everything works fine, including rebooting the server (it is possible to save the RAM disk image, making it survive the reboot). However, when I reboot the server as a result of Windows Update the update is undone, because Windows is unable to access the TEMP directory - the RAM disk has not been loaded yet. Truth to be told the problem is only with the system TEMP directory and there is not much harm not to redirect the system TEMP and leave it as is. Still, if I could load the RAM disk before the Windows Update resumes then I could place the system TEMP on the RAM disk as well. My question - is it possible? 

$URL$ "Set max_connections to the number of concurrent connections you need. The default value is only 100 connections, which is very small." So, yes you can set "max_connections" 

Its a USB drive? Have you tried to safely remove it and unplug it and restart the computer? Then reconnect it and try to share it again? Sometimes these little things help on windows. You could also try the advance sharing rather than the normal sharing - or the other way around. Put the permissions on groups rather than on single users. Thats what I would try first. 

Thats not the default one, however it is in the allowed range. The UDP port range is the default since it is not advised to change them. I tried to change the "networkprofile" from 100mbits/1gbits and custom. I am running a 1gbit network with CAT6 cables and 1gbit netgear switch 5 ports. Everything is configured to use 1gbit. The WDS is authorised for the DHCP server. My ISA 2006 configuration: For the internal networking i have configured the following policy array: Allow protocols on internal network including the w2k3 host: 

Routing and Remote Access I tried the DHCP relay agent configuration that was suggested as well, but that did not work I would highly appreciate anykind of help because I am pretty much done here with my nerves. Thank you very much in advance. 

If I have a mirrored pair of 250GB drives in a pool, and I later buy two more drives and add another mirrored pair to the same pool, can that second mirrored pair be 500gb? Such that my total usable space would be 750GB? Or do all the mirrored pairs in a pool need to be the same size? 

I am running pfsense 2.0.3 nanobsd 4g i386 on virtualbox. VM configured with 4gb ram, there's 8 gb total on host system, with two net interfaces configured as host only. This will go on an SSD mini atx box, but for now I am just running on VM for learning pfsense. I assigned interfaces, em0 to WAN, and em1 to LAN. From the windows host(hosting the VM) I brought up the browser and tried to connect to the LAN IP. I was intermettently getting timeouts and I would reboot the server or use the reboot web configurator option, and sometimes I could get the login screen but after logging in with default user/pass, I'd get a blank page. Absolutely no error messages or feedback of any kind. I typed password carefully, thinking maybe it was doing anonymous authentication, since according to their documentation provides a blank page by-design. After many tries and reboots I finally got the wizard screen. I completed the wizard and the final page indicated it was going to redirect after a few moments, after a few minutes it redirected but failed to retrieve the next page. From there the web configurator again was not responsive, timing out. I rebooted and still same thing. How do you troubleshoot something that gives you absolutely no feedback or error messages? Any ideas about what might be wrong would be welcome, but primarily: How do I troubleshoot failures in the web configurator? Is there logs specific to the web configurator, or do I need to poke around in the web server logs, pfsense logs, etc.? Is there any documentation on directory structure that would help me find these? I've found from distribution to distribution, that each has it's own idea of where user programs, logs, etc are stored. 

I am currently building a storage unit for our office. It is rather low budget at the moment, but it needs to be extendable. basically we have a huge database that will grow over the next few months quite heavily. Therefore, ideally we would just like to throw hard discs at our new server. We have not purchased the server yet, but going through some details. However, I would like to get an answer to a question first. How easy is it to expand existing RAID systems? We will start with two HDD 4TBs WD black. But after about 1 month we will need to add another 2 4TB disks. The server we are going to get has 12 bays. Mirroring is important, However RAID 1 only works with 2 disks. Raid 10, would already allow us to mirror a RAID 0. And from what I have seen even the raid 10 can be installed with two disks. However, what happens after that ? Is there any recommendation to achieve a flexible RAID system ? On the OS layer I would just like to build a LVM, that recognises once there is space added to the "disk" so that it can be expanded. But in fact, it lies on several disks which are managed by the RAID controller. 

EveryDNS used to have this, but I switched to DynDNS because I found out my router has support for DynDNS. 

Intel AMT is a motherboard feature found in certain chipsets such as Q67. So if you are looking for a superMicro board with this, then just find out which intel chipsets support Intel AMT, and then look for supermicro boards with that intel chipset. There are some various other requirements as far as what CPU you use, etc. In a nutshell the motherboard keeps one of the ram chips powered, the northbridge powered(which runs the management interface), the onboard ethernet connection, and an extra flash chip to store settings. So you can connect to the system over ethernet and access the remote managmeent features/power-on/power-off etc. I am not sure though if this is available in any chipsets targeting servers though, but it is really powerful and would seem to be well suited for servers. 

Where can the pfsense log files be located and viewed? I have searched the documentation and it doesn't indicate the log files location for the various components of pfsense. 

I installed SQL Server 2008 and typed in an instance name of but it seems to have been isntalled as a default isntance. Trying to connect using fails but using just the computer name succeeds. The service is listed with as if it is the default instance, but the data directories all have the .sql2008 instance name suffix. So not only is it not what I wanted, but the data directories and service names have inconsistent suffixes. Was there something else I needed to do besides specifying an instance name during installation? Maybe a checkbox I missed? Is there a way to change from a default instance to a named instance?