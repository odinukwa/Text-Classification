I don't know exactly how the numbers would work out, but you would expect an equilibrium but it will depend on many factors. A complication is that you have public companies taken private and private companies that never go public. It may be there is some equilibrium between these as they are unlikely to be equivalent. Ex public companies are often highly levered, while never public are often debt free and cash rich. Particular important factors would be interest rates and credit spreads. This is because it boils down to the distribution of capital. If there is lots of money available to lend then LBOs become attractive and companies can be taken private. If there is strong demand for equity then IPOs become easy. I think an economy with a shortage of public companies would show overvalued equities and hence IPOs are encouraged (think dot com bubble - shortage of public web companies vs demand). If debt is cheap and there are too many public companies then you will see either take overs or LBOs. I suspect the effect is most visible within sectors rather than the whole economy. 

The other answers are correct in respect of what would happen if the money supply of a currency was kept constant, however there is nothing in bitcoins to ensure that money supply will stay fixed This is such a common misconception so I'll repeat it. Bitcoins limit the money base, but that does not limit the money supply. The money supply is the money base times the money multiplier, and the money multiplier comes from fractional reserve banking. There is nothing in bitcoins to prevent fractional reserve banking, and we are already starting to see website that allow you to lend or invest bitcoins. Someone could, if they wished, set up a bitcoin bank. Savers would deposit bitcoins and earn interest, while borrowers could take out bitcoin loans. As soon as you do this the money supply (the $M$ in $MV=PY$) increases. So, if bitcoins became a global currency, then you would almost certainly find that the money supply was much larger than the base limit - the money multiplier would expand as more banks entered the system. We don't have any concrete evidence to suggest what the limit on the money multiplier would be, but even in "normal" currencies the money multiplier can exceed 10x. The legal limit in the EU is 50x. With no regulatory capital or reserve requirements, who knows what the practical limit could be. So what does fixed money base do? Having a fixed money base means that there is no central bank to control the money supply. If the economy heats up, you would normally raise rate, which would reduce the money base and tighten the money supply. In this case there would be nothing anyone could do - the world would simply rely on the "free banking sector" to do the right thing. This was actually not far off the way the US ran it's currency during the period when it had no central bank (between 1836 and 1913). What you could do is control the money multiplier directly. This would mean passing laws and regulations forbidding fractional banking above a certain fraction. For bitcoins doing so would be almost impossible, not to mention against the whole "free" concept. You would have reintroduced a central controller for the currency. The free banking era saw wild swings in money supply (e.g. more than doubling in a year) and hence huge inflation or deflation as banks grew and failed. It is anyone's guess what would be in store if bitcoins became a major currency, but there is no reason to believe they wouldn't have inflation. 

The only reason I can see right now could possibly be pedagogical. While that term does indeed just equal zero, there could be generalizations where the agent works a high amount with probability $\tau$ and a low, but positive, amount with probability $1-\tau$. In this case, the term would still go through. There are also plenty of functional forms that would result in that term not being zero (taking away the -1, for example, which is often done in applied work). At any rate, this is an idiosyncratic enough case that the professor might have thought it best to show the process of rearranging terms for future reference and to give help with other scenarios. On the other hand, he may just not have noticed it. :) 

First, it is important to note that there is a substantial literature both developing and using nonlinear VAR estimation (for example, see papers here, here, here, and here). The reasons linear VARs are seen a lot, though, are similar to the reasons that least squares (in its varying forms) is seen a lot. The model is very transparent and analytically tractable. This is a benefit for both pedagogy (analysis can be done on linear VARs just using lag polynomials) and for interpretation of results. You also don't run in to the concavity and global vs. local solution issues you do with big nonlinear problems, which can make your results more credible in many circles. 

Inspired by the answer to a different question I wondered: Can anyone think of a case study example, past or present, when capital gains are inflating adjusted before being taxed. I could imagine this making a lot of sense in high inflation currencies. If there is an example, what prompted the introduction and was it considered a success? 

Firstly there are services like this in Spotify, and even radio and tv, but it sounds like you are talking about downloading the material with ads in. That causes a problem. Revenue from ads relies on giving many ads to many people. Each time you listen to a song the provider needs to be able to provide a new ad. If you download a song or book with ads then those ads are fixed. So you might listen to it 100 times but no advertiser will pay 100x as much because it is not valuable to give the same ad to the same person many times. Secondly you would need some added security on the files to make it hard to simply remove the ads. That is easily as difficult and expensive as DRM anyway. The third problem is ads are annoying. I think for many people an ad would be more incentive to pirate than a small cost. Early music DRM eventually failed (in part) because it was to motivate piracy in itself. Xkcd had a nice comic on that idea. And that leads to the forth point. Much of economics is about equilibria and long run expectation. In reality everything takes time and big bold change is even harder to start. Initially music was DRM protected but they eventually realised most people will pay 99c to avoid the hassle of piracy, but will pirate to avoid DRM Nothing changed in the economics, it just took time for the industry to realise that was a better way yo do things. There may well be a book publisher out there working on an ad supported eReader library, but it will take time. To put the price of a song in context: The US median hourly wage is \$17.091, the median song length of the top 100 iTunes songs is 223 seconds2. So 50% of US workers earn \$1.06 in the time it takes to play a song. 

Sources Campbell, J. Y., 1993. Intertemporal asset pricing without consumption data. American Economic Review, 83, 487-512. Campbell, J. Y., 1996. Understanding risk and return. Journal of Political Economy, 104, 298-345. 

The simple answer is they estimate the demand curves for each product and, using their cost structure and market characteristics (competition structure, etc.) set price to maximize profits. This is standard for any firm, though. How Google in particular and these big firms in general (Amazon, Microsoft, etc.) estimate demand curves is somewhat different than the usual economist might do it. For usual demand estimation, a researcher would have to make use of market idiosyncrasies to identify demand. For example, using supply shifters with 2SLS for basic demand estimation, BLP for discrete choice with heterogeneous products, etc. Identification is such a big issue for demand estimation because a researcher generally just observes equilibrium (p, q) combinations, not the actual demand curve. We are also often constrained purely by the amount of data available. For a big firm like Google, however, they 1) have the ability to enact exogenous perturbation in price to see how sales change and 2) have access to tons and tons of data. Using 1) they are constantly running little experiments to see how consumer behavior changes. They can then use the results to actually trace out the demand curve. In these experiments, the firm could easily take into account things like movie popularity, genre, etc. With respect to 2), Pat Bajari, chief economist at Amazon and one of the biggest names in modern empirical IO, has a (at this point) working paper with Nekipelov, Ryan, and Yang on how to use machine learning to estimate demand curves across products with lots of sample points bunches of characteristics (think thousands of product characteristics). As a "fledgling computer science researcher," you'd probably be in to this. This approach is especially relevant for people/firms with access to tons of data (like Google, Amazon, etc.) 

The microstructure noise is, roughly speaking, the small-scale noise introduced into the market as a result of the way the market is designed. For example, suppose that there is an asset and the "real" price for this, is 126.6. That is, if we magically knew everyone's honest, perfect maximum buy and minimum sell prices and could match them off and arrive at the equilibrium price, then it would be 126.6. However, imagine that the market only quotes in whole numbers. That means you can either buy at 127 or sell at 126, but it is physically impossible to trade at 126.6. Intuitively we expect to see a sequence of trades switching between 126 and 127. This is called "bid ask bounce" and is perhaps the simplest example of microstructure. Even with such a trivial example, we can start to do interesting things. For example, since the price is closer to 127 than 126, we might expect to see more 127s than 126s. We could use a simple logit function or something to select the probability the next tick is on the near or far side. Also we can "what-if" the market has a different tick size? Clearly we expect in our model the bid ask bounce to reduce. In our example the latent price is a constant, but that is not very realistic. For a better model one would usually assume some dynamic model for this hidden price process. Perhaps Brownian motion, for example. Now you can see how with our model from above, we will see trade prices bouncing either side of our hidden price. As the price gets closer to a whole number, more trades are done on that, until it crosses and then we start seeing trades on the next tick along. As you can imagine, you can continue to play this game, making the latent price process more complex, and adding more complex microstructure models to generate the trades around that price. And all along we have assumed independence of latent price and microstructure noise. That is to say, our microstructure process does not impact the latent price, and the microstructure process stays the same, regardless of what the price is doing. This is a useful assumption to make, since it separates the problem out - without it you probably wouldn't be able to solve much and the usefulness may be limited. That said, it is easy enough to think up possible models that would break this: e.g. 

The primary literature concerned with this type of question (at least where classical results break down) is behavioral economics. There's a great general compilation of papers put together by the Russell Sage Foundation called the "Behavioral Economics Reading List" that includes, among other things, a General Introduction section with overview papers by some of the big movers and shakers (Camerer, Kahneman, Laibson, etc.). Many of the papers you will find through the Russell Sage paper list will be on alternative methods to classical consumer theory. If you want just the tests of assumptions/predictions, I would recommend looking through the abstracts of John Lists's papers on Testing Economic Theory. List is one of the most prolific experimentalist authors on the subject and has weighed in on most questions that other people work on in this field as well. Just reading his abstracts on that page should give you a pretty good idea of the state of the literature up through 2011 (the website isn't updated). His CV is updated, but doesn't give you the abstracts without looking up the papers individually. 

A representative agent chooses a consumption time-path to maximize expected discounted utility $E\left[\sum_{i=0}^\infty\beta^i U(c_{t+i})|\Omega_t\right]$, where $\Omega_t$ is the information set at time $t$, subject to the inter-temporal budget constraint, $c_t + p_tq_t = r_tq_{t-1} + w_t$, for all $t$. The optimal consumption path satisfies: $p_tU'(c_t) = \beta E[r_{t+1}U'(c_{t+1})|\Omega_t]$, for all $t$ Which gives the Euler equation: $E[\beta(r_{t+1}/p_t)[U'(c_{t+1}) / U'(c_t)]|\Omega_t] - 1 = 0$. He then imposes the CRRA utility function $U(c_t) = c_t^{1-\gamma}/(1-\gamma)$, making the Euler equation become $E[\beta(r_{t+1}/p_t)(c_{t+1}/c_t)^{-\gamma}|\Omega_t] - 1 = 0$. This yields the moment equations: $E[\{\beta(r_{t+1} / p_t)(c_{t+1} / c_t)^{-\gamma} - 1\}z_t] = E[E[\beta(r_{t+1} / p_t)(c_{t+1} / c_t)^{-\gamma} - 1|\Omega_t]z_t] = 0$ with $z_t$ a vector of instruments belonging to $\Omega_t$. 

Just to synthesize the other two answers together a bit and "join-the-dots". Setting the Scene Since 2011 SNB had decided that it wanted to prevent the CHF from strengthening past 1.2 CHF per EUR. In order to do this it had to do either or both of: 

Keeping rates low at the short end is easy enough - that's the base rate. But that was scuppered by the low EUR short rates. Keeping rates low at the long end is trickier, but can be done by not issuing too many bonds. Selling CHF is fine, but that means buying other currencies. By the end of December the SNB had bought 85% of GDP worth of foreign currencies. At this point the SNB decided that it did not want to keep expanding the balance sheet. Why it decided this is a different question (as the question points out). The question is how to end a cap once you hold a huge Euro position. The Sudden Act Once the SNB had decided it could not defend the cap, it had three ways it could act. 

An exchange can handle more than one currency, but this would be uncommon for equities. An example is the ICE futures exchange in Europe. Multiple currencies doesn't cause any particular problems - transactions are carried out in currency and it is up to you to convert to whatever currency you prefer. Usually done by fx sweeps by your custodian bank. For margin purposes combined exposure (e.g risk netting) can be calculated at prevailing fx rates. Each security only trades in one currency, and each security trades basically like a mini exchange so there isn't really any problem. Think of it like two exchanges run by the same company. 

It looks like what you're really after is determinants of housing mobility or migration. You get that, you can then just apply it to a specific city and aggregate to predict. One of the earliest and most seminal works in this field was Rossi's Why Families Move, originally published in '56 and redone in '80. The book is here and a review is here. Another giant paper in the literature is Massey "Social Structure, Household Strategies, and the Cumulative Causation of Migration". It is also a little older (1990), but brings together a lot of the previous literature (great paper for lit review) and formalizes the decision making process of migration. Newer papers in the literature look more at specific issues related to relocation, like eviction, fertility and housing decisions, and connection with life-course events. 

To get Quah and Vahey's method, you should probably get a good handle on the Wold Representation Theorem first. Christiano has good notes on it here. Then, you can apply it to VAR. There's a pretty good basic explanation in Greene's Econometric Analysis. In the Sixth Edition, it's in Chapter 20 (Models with Lagged Variables) starting with Section 20.6, "Vector Autoregressions." It should be pretty straight forward to apply the Wold Theorem to VARs with Greene's notation. If it's still not quite clear to you, there's a pretty close example on page 704, subsection 20.6.8.b, "The Sacrifice Ratio." The basic idea is that you just end up sort of inverting the matrix of lag operators and coefficients (the "inversion" part) so that you have your main variable on the left side and a lag polynomial in your errors on the right side.