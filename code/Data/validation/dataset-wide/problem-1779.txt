I'm not familiar with Pingdom, but I think Smokeping will do everything you want. If you specifically want to monitor an HTTP/HTTPS server and not just general connectivity (by way of ICMP), check out the EchoPingHttp and EchoPingHttps probes. There are also probes that will monitor DNS and SSH services. We use smokeping for general latency and "network health" monitoring all over our network with good results. I just used it last week to help diagnose a duplex mismatch at a remote site. Setup is pretty minimal, especially if you are familiar with Linux. 

It means that if you decide to not just remove the slapd package but purge it instead it, aptitude/apt-get will also remove the the actual database as well. The difference between just removing a package and purging it, is that if you purge it apt will remove all the associated configuration files (even if you have made changes to them). From the aptitude manpage: 

Because that's how it was configured by a previous administrator and while it makes conceptual sense that a single subnet is "mapped" to a single VLAN there's no technical constraint that I am aware of that makes this have to be the case. 

No. It is not necessary to generate the CSR on the machine that you want to host the resulting certificate on. The CSR does need to be generated either using the existing private key that the certificate will be eventually paired with or its matching private key is generated as part of the CSR creation process. What's important is not so much the originating host but that the private key and resulting public key are a matching pair. 

It has been a while since I worked in the network administration side of things but here is what I think about this: 

Your service is bound only on and not to the loopback addresses (, ). You need to reconfigure IIS to listen on the loopback address of your choice, either IPv4 or IPv6, or listen on all available addresses () as appropriate. 

I'm going to offer a counter point to the prevailing answers to this thread. I don't think you should be running anti-virus software on most of your servers, with file servers being the exception. All it takes is one bad definition update and your anti-virus software could easily break an important application or stop authentication in your domain entirely. And, while AV software has made substantial progress in its performance impact over the years, certain types of scans can have a negative effect on I/O or memory sensitive applications. I think there are pretty well documented downsides to running anti-virus software on servers, so what's the upside? Ostensibly, you have protected your servers from whatever nasty-ness that filters in through your edge firewalls or is introduced into your network. But really are you protected? It's not entirely clear and here's why. It seems like most successful malware has attack vectors that fall into three categories: a) relying on an ignorant end user to accidentally download it, b) relying a vulnerability that exists in the operating system, application or service or c) it's a zero day exploit. None of these should be realistic or relevant attack vectors for servers in a well run organization. a) Thou Shalt Not Surf the Internet on Thy Server. Done and done. Seriously, just don't do it. b) Remember NIMDA? Code Red? Most of their propagation strategies relied on either social engineering (the end user clicking yes) or on known vulnerabilities that patches were already released for. You can significantly mitigate this attack vector by making sure you stay current with security updates. c) Zero day exploits are hard to deal with. If it's zero day, by definition your anti-virus vendor will not have definitions out for it yet. Exercising defense in depth, the principle of least privilege and having the smallest attack surface possible really helps. In short, there's not much AV can do for these types of vulnerabilities. You have to do the risk analysis yourself, but in my environment I think the benefits of AV are not significant enough to make up for the risk. 

If your goal is to build a Collection based on specific usernames, I think the easiest and maintainable way to do this is add those users to a security group and create a User Collection. For Example: We have a User Collection for a specific group of contractors. 

Wait WUT? That line was from another client's ServiceWindowManager.log and it certainly believed that 19:00 was the appropriate time to start. I checked a few others. Guess what. Not a single mention of starting at 20:00... but if I look at the what is listed in the database AND in the Configuration Manager Console the Thurs. Night Maintenance Window is listed as starting at 20:00. Zoinks! It's not a mystery maintenance window! It's a masked maintenance window! It looks like that for whatever reason is configured to start at 20:00. The Configuration Manager Console reports that and so does the database but if I look at a handful of clients some go 19:00 and others at 20:00. Two WAG (Wild Ass Guesses): 1) We have old Machine Policies hanging around from a previously implemented ConfigMgr 2007 Site. or 2) the Maintenance Window policy got changed from 19:00 to 20:00 at some point and the not every machine got the news. Whatever. I have no idea what I'm doing here. Resolution I created a new Maintenance Window to replace and assigned it to the appropriate Collection. I waited an hour or two for the clients to do their policy pulls and guess what: 

Have you considered using nping? You should be able to generate and send packets quite rapidly. If you're trying to identify periodic packet loss or latency you could try a tool like smokeping. I've had excellent results using smokeping for trying to correlate network events over time - especially for wireless point-to-point links. There's no MySQL probe but smokeping is modular so you could write your own if you so desired. These tools, along with Wireshark, have been very useful in eliminating Layer-3/4 issues from the equation when troubleshooting. 

Well that's not good. It looks like the WSUS site has magically migrated back to its standalone site because... FUN! If the SCCM SUP is looking for WSUS on 80/443 and it's no longer there no wonder it doesn't work. If I look at the registry key () that WSUSUtil.exe is manipulating I see that it still thinks WSUS should be running on 80. Maybe I just need to run more than once for extra... FUN? 

If I look through the ruleengine.log (which is presumably the log file that the higher level SMS_RULE_ENGINE log within SCCM gets generated from) and coordinate the Package ID for the relevant Deployment Packages that the Automatic Deployment Rules are supposed to place these updates in I find the following: 

Hmmm. Good question. The client should just run the installation string but it would not be terribly surprising to me if it did some deeper, darker magic. The only thing I can think of in your situation that might be causing the difference is the bit-ness of process you are running the installer under. I think the SCCM client almost always uses 64-bit but cmd.exe is 32-bit right? Take a look at my answer here for other general advice in dealing with software install issues. Best of luck. 

As per "Best Practices" staff in our IT department have two accounts. An unprivileged account and an account that is a member of the global Domain Admins ($DOMAIN\Domain Admins) group. On our file servers the Domain Admins group is added to the local Administrators ($SERVER\Administrators) group. The local Administrator group has Full Control granted on these directories. Pretty standard. However, if I login to the server with my Domain Admin account in order to descend into that directory I need to approve a UAC prompt that says, "You don't currently have permission to access this folder. Click continue to permanently get access to this folder." Clicking continue gives my Domain Admin account permissions on that folder and anything else underneath despite $SERVER\Administrators (of which I am a member of via the Domain Admins group) already having Fully Control. Can someone explain this behavior and what the appropriate way to manage NTFS permissions for file shares is regarding Administrative rights with Server 2008 R2 and UAC? 

I see you are using DD-WRT. That's unfortunate. I (and others here) have found it to be notoriously unreliable. Insure that DNS is working. From your local network client (192.168.1.104) do a DNS lookup on your domain and make sure the correct 222.111.111.001 address is returned. This will verify that DNS is functioning appropriately for your local clients. Then see if you can access your server from a local client using its "external" IP address (i.e., 222.111.111.001). If you cannot access your server this way, then I suspect your router is the problem. If you can access your server, I suspect DNS or possibly a bad virtual hosting configuration (you mentioned a webserver) could be at fault. 

We noticed our Automatic Deployment Rules for Software Updates failed to automatically download and apply this month's patches from Microsoft although they are correctly listed in the Catalog. 

It's difficult to comment on what ports are open on a "typical" installation, because due to the differences between distributions and myriad of situations Linux can be used in there's really no golden standard here other than run only as many programs as you absolutely have too. I suggest you start with Fedora Core 14's Security document to help make that determination yourself. 

If you continue to use dd-wrt be advised that the "official" recommend builds are largely considered broken. The current recommend build is 14929. Read the wonderfully organized "peacock" thread for more information. I suspect using higher quality access points will largely resolve your problem, but I doubt you'll ever see speeds above 16Mbps. 

This is a bit of a WAG but have you tried recompiling the iSCSI MOF files? I've come across similar behavior where NetApp's SnapDrive can't enumerate all the of the iSCSI drives on a particular server. This blog pointed me at this KB2001997. Try comparing the results of the following WMI query on this server to another known good one: 

There is no real way you can mitigate a DDOS from the host that is getting attacked. By the time the traffic has reached your host it has already passed through your local network stack and consumed local resources. No amount of hackery or gyrations can change this. You need to work with your upstream provider to prevent the traffic from reaching your machine in the first place. If they don't provide that service you can likely purchase it from them or you can find a provider that does. 

Telnet will only work for TCP services, so if you're trying to see if your DHCP server (UDP/68) is running on a remote machine it won't work. Likewise nmap defaults to only scanning TCP ports. For UDP ports use: 

This is more or less true. Back before switching technology subnetting to account for the total amount of broadcast traffic was a serious concern. Now days though switches are much more efficient both in terms of general architecture compared to hubs (i.e., they do not forward EVERY packet to EVERY port) and they are better with the broadcast traffic they do have. I have heard WAG numbers that 500 clients per subnet is about where you should start to consider subnetting based solely on traffic and broadcast domain concerns but I would not be surprised if enterprise-grade switching hardware could handle much more. Obviously, test and test again as everyone's workload is different. 

Installing and running Windows Server 2012 R2 and other previous versions of Windows Server on a Compact Flash card or USB is un-supported. The only supported installation of a Windows Server operating system onto a USB device is Hyper-V Server 2012 R2, the dedicated hypervisor of Windows Server which is basically just Hyper-V and the Server Core "management" virtual machine running in the parent partition (Run Hyper-V Server from a USB Flash Drive). As far as I am aware, there is no supported installation path for Compact Flash devices. Big Important Red Flag: 

Ah. The joys of automated software installation in a Windows environment. SCCM can only do so much with terrible vendor provided installers as you have discovered. My first stop when trying to build a Application-Program or a Package-Program is to take a look at ITNinja which used to be called App Deploy before Dell decided to buy and rebrand it. Unfortunately there's not much there for Lenovo ThinkVantage System Update. If you are on your own there is a few general steps you can try: 

I think you have some misconceptions about IPv4 exhaustion. Just because there are not more IPv4 addresses to be allocated does not mean there are no IPv4 addresses available. ISPs (generally Tier-1 providers) and many older established companies still have lots of IPv4 space left they can assign to customers. What it does mean is that RIRs have no more addressing space left to assign. This means ISPs and companies will have to make do with what they already have. The United States and Europe have managed to snarf up the lions share of the IPv4 space so the shortage is going to be felt much sooner and harder in Southeast Asia. On top of this, there is always NAT which lets your stretch your existing IPv4 space even further. When there are no more IPv4 address to be allocated (already happened!) it is not like IPv4 immediately stops working. The existing IPv4 customers will still be able to use their existing IPv4 addresses and traverse the existing IPv4 internet. It's not like a magical switch gets thrown and IPv4 becomes broken. This is whole point of running dual stack. 

You need to contact your upstream provider and have them block, filter or otherwise prevent the packets from this DDOS from reaching you. There's really not much you can do by the time they arrive on your server. You can try to block the source IP addresses using iptables, but likely the (computational) damage is already done by the time iptables actually looks at the packets and decides to drop it. EDIT: Your comment doesn't make sense to me. You have DDOS caused by lots of or specially crafted packets (or both) causing too much computational overhead due to interrupt switching. Are they coming from one or a few sources? Have your upstream provider drop packets from that source/s? Profit! If they never get to your machine - they'll never cause the problem in the first place. There's no magic you can do on the host to prevent this from happening once the packets already arrive. You need to have dropped the packets before they arrive on your machine. Contact your service provider or hosting company. 

I think your only real advantage of using physically discrete networks in this situation is conceptual simplicity. Sometimes it's just simpler to look at two physically discrete devices. I can also imagine that there are situations where you are required for legal or security reasons to have physically discrete networks. I'm thinking along the lines of three-letter agencies that require separate rooms, infrastructure and levels of clearance. The disadvantages are multitude: It's twice as expensive, takes up twice as much space, there's twice as many pieces of hardware to fail, you trade logical complexity for physical complexity, and it's not very scalable. Unless you have a good reason to need physically discrete infrastructure for these roles you're best off using VLANs. 

or will list what files are using which IPv4 ports and which IPv6 ports respectively. The file contains a list of which programs are generally known to run on which ports. However, there is nothing forcing a particular program to use a particular port, so don't take as the one and only way you'll ever find things setup. People, either on accident or purposely, move programs to "non-standard" ports... sometimes for malicious reasons, sometimes not. 

The difference between 0.0.0.0 and the loopback address 127.0.0.1 is that the loopback address is designed to allow a fully functioning IP interface within the host itself, regardless of what the rest of the networking setup, if any, looks like. Any traffic sent to the loopback device is immediately received on it. It's not so much that the loopback network "refers" to your own host... it's more of like you have a mini network segment in your host that devices, processes and sockets and can open and connect to. 

Without seeing your entire ruleset or really knowing what you're trying to accomplish I can only offer a proof-of-concept example. One final note: I would really avoid doing this. There is a tendency (at least I have it) to want to write the fewest rules possible by having one rule do more than "one thing". This is bad. Firewalls are already frightfully complicated and seemly always misconfigured; why make it harder on yourself by crafting a byzantine ruleset? A longer ruleset with simpler rules will be easier to understand, maintenance and debug. Avoid the temptation to be overly clever.