Start with Tara Kizer's post on Getting Help with a Slow SQL Server. (Disclaimer: it's on my company's blog.) She walks you through: 

You don't have a choice as to whether or not to have quorum: Always On Availability Groups are built on top of Windows Server Failover Clustering, which requires quorum voting to understand who the primary is for a given AG at a given time. I think what you might be asking is whether or not you need an old-school quorum drive (like a Q drive on a SAN). These days, Windows Server Failover Clustering (WSFC) can either use a quorum drive witness, OR a file share witness. Windows Server 2012 R2 includes dynamic witness capabilities: if you have an even number of voters (like if 1 of your 3 cluster nodes goes down), then Windows will automatically add in your witness in order to be a tiebreaker. That's why you want to configure the witness (either a disk or a file share) ahead of time - Windows will automatically manage its voting rights whenever you have node failures or network failures. I hate telling people to read the manual, but in this case, Books Online's quorum section is actually fantastic. Just make sure to read the appropriate one for your version of Windows, because Win2008R2, 2012, and 2012R2 all have different quorum options. 

Only if your indexes are also partitioned by UserId, and that's rarely the case. Usually you'd want indexes arranged in a different manner - but again, that's covered in the book. 

In SQL Server 2017, after applying Cumulative Update 7, there's a new system stored procedure in master called sp_restore_filelistonly. What's it for? 

The Compute Scalars are related to the computed field LineTotal. Script out the table, and you'll see that field defined as: 

Read more: $URL$ What she means by high availability is that you don't have the ability to run index maintenance jobs that would lock the table. Personally, I wouldn't recommend ever setting this by default - leave it to DBAs who find that they're unable to manage index maintenance appropriately with any other methods, like using Ola Hallengren's maintenance scripts during off hours. His scripts are genius - they'll do as much as possible online. 

Your tested and proven settings. Some folks have enough time to do repeated benchmarks of their own applications on their own hardware. That's fairly unusual, though. Microsoft's Fast Track Reference Architectures. These are very specific documented setups done in partnership with hardware vendors. They include everything from firmware versions to SQL config settings. Unfortunately, they're not available for all combinations of hardware. (Don't worry too much about the SQL Server version - SQL IO hasn't changed much since SQL 2000, with the exception of SQL 2014's new in-memory OLTP.) Your storage vendor's SQL-specific guidelines. Major storage vendors like Dell, EMC, and HP provide detailed documentation for SQL Server as well as other database platforms. Make sure to get the docs for your exact make/model of storage - different models can have totally different recommendations. General advice from some bozo on the web. Myself included - my work was quoted in one of the other answers. General advice just doesn't hold true for all combinations of storage hardware. This is a last resort. 

The Availability Group Listener name is a Computer Name Object (CNO) in Active Directory, not just a DNS entry: $URL$ Its location in AD determines where it gets registered. If you want to create a different one somewhere else, you can create a DNS CNAME (alias) pointing to the real Availability Group Listener name: $URL$ 

That's going to result in long downtimes for a 200GB database. To shorten it, you can use tools like Red Gate's Data Compare or build your own comparison tool to detect what changes have been made to your database. For example, on one project, the developers added LastUpdated timestamp fields to all the tables, and maintained those with triggers. Then, after our restore, we could shut down the apps, copy across the specific records that had been updated, and go live with only a few minutes of downtime. Q: What time zone should I use for my EC2 servers? Just as RDS was configured with UTC, so should your SQL Servers. Just set your EC2 boxes to be in UTC time zone regardless of their location, and then you won't have problems when you fail over from one region to another. 

Bad news: rolling work forward in SQL Server is multi-threaded, but rollbacks are single-threaded. If you had lots of cores doing work for 11 hours, it's entirely possible that it could take dozens of hours (or several days) to do the rollback. Try using sp_BlitzFirst @ExpertMode = 1 (disclaimer: I'm one of the authors of that open source tool), and look at the file stats section. It'll show you which data & log files have had physical reads & writes in that time span. That'll show you if your rollback is continuing to move along. You mentioned "impacting other processes" - keep in mind that your rollback speed can be reduced by other processes running on the server. You may also want to keep an eye on your drive free space and transaction log sizes. Your transaction was a logged operation, and SQL Server will need that transaction log to stay online even after you've taken log backups. It's very conceivable that you'll run out of drive space during the rollback. Given how big that rollback was, and depending on your RPO/RTO goals, I'd consider failing over to your disaster recovery servers or restoring from backups. I hate to lecture you now, but this is a good time to learn the lesson that you should try everything in a development environment first to measure how long it'll take. You would have been better prepared to identify that you can't move a multi-terabyte object from one filegroup to another without a lot of planning. 

If you're using good ol' fashioned spinning rusty frisbees, also known as magnetic hard drives, this isn't an unusual number to see when you're doing random reads and writes. You can outpace those drives with a simple USB 3 flash drive. To find out for sure, run the portable edition of CrystalDiskMark on your server during a maintenance window. Test the C drive, test where data is stored, where logs are stored, etc - and then compare it to your laptop's SSD. You might be surprised at how much slower random reads & writes are on this array of spinning rusty frisbees. 

I order 'em that way because that's usually cheapest/easiest up to hardest/most-expensive. If your wait type is something else, then don't worry about disk queue length. (Actually, don't worry about disk queue length, regardless.) 

Generally, I wouldn't worry about these unless you're designing a system for very high insert loads - say, consistently 1,000 inserts per second around the clock. (That isn't even a high insert number, but I'm just trying to get you a rough idea of the kind of scale we're talking about here.) If you need to load hundreds or thousands of records in a single transaction, from a single source, then getting a batch of identities using a SEQUENCE can be faster then using IDENTITY or a separate key table. For more precise advise on your particular scenario, expand your question to include: 

If the job is still running, check what command it's currently executing with Adam Machanic's excellent free stored procedure, sp_WhoIsActive. After installing it (typically in the master database), you can run sp_WhoIsActive to list the running queries and see what command they're executing right now. There's even a Percent Complete column that gets populated for backups and restores. You might have a particularly large database (or log file) that's just in the midst of getting backed up. For further followups & clarification, try taking a picture or copy/pasting the sp_WhoIsActive results in, showing what the maintenance plan's currently doing. 

SQL Server just uses the operating system's time. However, if your application code relies on functions like GETDATE(), then your code needs to be able to handle jumps forward (or backward) in time. For example, if your have code that does something like this: 

I'm going to be brutally honest here: those are not the right metrics to look at, and wherever you're looking that gives you those metrics is wildly outdated. Those are not the usual suspects. In the year 2018 (and indeed, for the last several years), the way you diagnose a slow SQL Server is by using wait stats. SQL Server is constantly tracking what queries are waiting on, and you can see it in the DMV sys.dm_os_wait_stats. My favorite way to visualize that is with sp_BlitzFirst @SinceStartup = 1. (Disclaimer: it's my favorite because I wrote it, and it's free & open source.) Try editing your answer to include a screenshot of your wait stats from sp_BlitzFirst, and we may be able to get you a lot closer. 

It's not a weird situation at all - lots of project managers say they want this until they discover just how hard it is. This is way, way harder than it sounds. Let's take a simplified scenario to illustrate it: CustomersV1 table, original bad design you're moving away from: 

Yes, any corruption alerts can make it to the end user. To see it yourself, grab one of the corrupt databases from Steve Stedman's Corruption Challenge. Attach the corrupt database to one of your development environments, and run the query that Steve describes for that database. You'll see corruption errors - just as your end users would. Your application (C#, Java, whatever) can be coded to trap incoming errors and display different things to the users, but that's an exercise left for the reader. 

Answering since I'm one of the people you're quoting, heh. 1. What should I set max memory to when I also run Analysis Services? You won't find guidance out there because SSAS is just like any other app that gets installed on your SQL Server: we just don't know how much memory you're going to use. Treat SSAS/SSIS/SSRS as third party applications - they just happen to be included as "free" in the SQL Server box. They have totally different CPU, memory, and storage needs. 2. What should I set the page file size to? My recommendations are so that Windows can do a mini-dump. If you run into a problem where you need to call Microsoft support, you can start with the mini-dump. If the problem recurs, and they can't figure it out from the mini-dump (or other sources of information, like the error log), then they may ask you to go to 1.5x RAM size. Microsoft's 1.5x RAM recommendations are so that Windows can do a FULL dump of memory when your system crashes. These days, with servers having 64-128-256GB RAM, it's not usually a good idea to have Windows pause to write out the entire contents of memory during a crash. You'd rather have the SQL Server back up and running, and deal with the mini-dump contents rather than the full. Save the full dumps for later - you can usually go your whole career without needing to upload a 64+GB dump file to Microsoft. (Good luck on that one.) 3. What do I set MAXDOP to for SharePoint? What you call "my" recommendations are from Microsoft KB 2806535. Start there in general, but when a specific application tells you something different, then they know something about their application that's different from SQL Server in general - follow that.