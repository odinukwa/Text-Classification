My personal preference is to create triggers by functionality, not by DML operation. So I have one handling auditing for insert/update/delete, one doing some kind of business rules or referential integrity (again, for insert/update/delete), another doing some ugly hack to work around undesirable behavior in our client software (possibly only for insert or update), etc. Keeping each "behavior" in one place reduces the chances of forgetting to update logic that's been duplicated across three separate insert, update, and delete triggers. The biggest pitfall with triggers is that the INSERTED and DELETED tables are not indexed, and you can't index them directly. Joining them against large data sets performs horribly, and things get really out of hand when you've got many triggers on a table, especially when trigger cascading starts coming into play. Most triggers I write start out by creating table variables @i and @d with the columns from INSERTED and DELETED that will be needed, giving them both a clustered primary key (since that's the only way you can index a table variable), then filling them with the data from INSERTED and DELETED. I wrote a short (noncommercial) article about triggers, and it also covers a few other points. $URL$ 

However, the example you posted does not match the column names you gave later. This means you are not posting your actual code, and this makes everybody’s job harder. Maybe this is only a toy example to learn how to use MySQL triggers. As such, it’s fine. In production, the very idea to change the price in the main table upon insertion of a new promotion is questionable. A better design would be not to change the table, add and columns to the table, and read the current prices either from a view which applies the active promotions for the day, or, if you have millions of products and performance becomes an issue, from a temporary table regenerated each day (and possibly replicated to slave servers... you know your scale). 

Suppose I want to log all severity 16 (and higher) user error messages to a database table in order to allow some retrospective application troubleshooting (since the answer to "Can you send me the error message?" is quite often "No, I closed it.") Is a server-side trace going to be the most effective option, or would there be an even better approach? 

I'd probably stick with triggers, for the same reasons gbn described, but if performance becomes an issue, you may want to consider using Service Broker to perform the work asynchronously (if business requirements will allow that). The typical design would be to have a trigger call a stored procedure that sends a Service Broker message, and the Service Broker queue handles the work where latency is occurring. Here's a short (noncommercial) article I wrote that might help keep any triggers from getting out of hand: Writing Well-Behaved Triggers 

which outputs the SQL definition of the table or view, even with the original comments (exactly the same as , with less typing. works too.) 

The complete list of characters in need of a special treatment is at the above link. However, it is best to have the intermediate language deal with such problems. For instance, in PHP you would call ; other platforms and languages provide similar facilities. 

To learn all you want to know (and what you’d rather not know) about sequences in PostgreSQL, read the docs. 

Well, it all depends on your scenario. If it is a one-off import, you create a junction table and use it for your joins. 

Having such junction table might improve performance, and it allows creating indexes. If the junction data is dynamic in such a way that keeping a junction table in sync is too difficult (a very unusual situation, I’d say), we can avoid the junction table but, at the very least, using instead of arrays is way simpler: 

There's (a lot) more detail in this article, as well as a nice example using the AdventureWorks database: $URL$ If it's a relatively simple query that's only performing poorly because of a lot of data and (inner) joins, this could be an easy way to improve it. 

The final step in every case would be setting the database to read-only mode. What other good/better options are there for doing this? My concern is moving the data over in such a way to preserve a high fill factor, and in a logically contiguous fashion. Edit: I should mention that about 75% of the data seems to be stored in image (LOB) columns. 

The simple way around this is to change some options in Management Studio. Go to Tools, Options, Designers, Table and Database Designers. Uncheck "Prevent saving changes that require table re-creation". Generally speaking, Management Studio will properly handle dependencies when you modify a table in a way that requires it to be recreated (it will do assorted tricks with temp tables behind the scenes, which you can see if you tell it to generate a change script instead of actually making the changes). However, this may involve modifications/changes to related tables or foreign keys. In other words, don't do this on a live system if you can help it, and make sure you've got backups before you do it. 

The safest way is to define product_id's with the finest reasonable granularity, to the point that if you decide to sell half the stock at a discounted price, you should define a new product_id for the items in promotion, all things being equal but the price. In this way you will manage to balance sales and returns without too many corrections. So I basically agree with blobbles's answer. You will have many ways to group your products together, e.g. same name, same size, same producer, same provider, and so on, and you will do that in separate tables associating the product_id with those features. 

However, this is an interface question, not a DB engine question, and it is rather strange. I suspect the interface settings have been changed from the ones used when asking the first question, because now the strings which the interface thinks are binary are automatically displayed as . Giving the first query to a commandline interface does not produce the results shown here. To get them from a commandline interface I have to write the second field as ; similarly, the supposedly wrong result of the second query is simply instead of because the interface does not know it is actually readable text. Casting it to should work. Please be aware that all those conversions the interface is forcing you to perform are not needed when data is exchanged between different parts of your application. 

If any of those is NULL, that row's result will be NULL. So then you're summing up those terms, potentially with NULLs along for the ride, meaning if any of those columns happened to be null, that whole row would be left out of the sum (you may see a warning about an aggregate ignoring NULL values at this point): 

In this case, dbo.first_day_of_month is a user-defined function. I don't know why you would ever invoke a function that way, but I would speculate that they required EXECUTE permission rather than SELECT to maintain consistency. Nowadays it's probably just holding on as compatibility baggage. 

You'll want to edit the Agent job (or whatever scheduled job is running them) and modify the value it's passing in for the parameter. e.g.: 

You should read about string literals in MySQL. There you’ll learn that a backslash character is used to escape some special strings, and to have literal backslashes in your strings (as in Windows pathnames), you have to double them: 

To resume the situation for people who don’t want to follow the link to the original question on SO: the OP is querying a Mysql DB via an unspecified interface which (sensibly) refuses to show the values stored in fields with encrypted data, and (rather dumb-mindedly) still shows a label for the value returned by calling on them. The answer given on SO was to cast those results as , which from a DB perspective makes little sense, but in this way the interface shows the results as text and everybody is happy. The question here is: if I have to use , how can I handle longer values? My answer is: you don’t say which interface you are using (your screenshots are not enough for me to recognize it), but I’d bet that if you cast as instead of it still works and you have no size limits that you should care of. So: