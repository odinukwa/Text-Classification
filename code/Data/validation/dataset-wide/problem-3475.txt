For clarification, I presumed that your outcome variable is in $\{0,1 \}$ (not in $[0,1]$). You observe $y_i$ for some $i$ but not for all, you fitted a logit regression, and then you want to predict the probability for every $i$. Please let me know if I misinterpreted your question. 

The question is about conditional expectation (expectation given information at time $t$). You have $z_{t+j} = \rho^j z_t + \sum_{k=0}^{j-1} \rho^k \eta_{t+j-k}$, and thus $E_t(z_{t+j}) = \rho^j z_t \to 0$ as $j\to\infty$ if $|\rho|<1$. The equation for $x_t$ is more complicated, but it can be written as $(x_t - \rho x_{t-1}) = (x_{t-1} - \rho x_{t-2}) + \epsilon_t$. That is, $x_t - \rho x_{t-1}$ is a random walk. When $w_t$ is a random walk $E_t(w_{t+j}) = w_t$. Substituting $w_t = x_t - \rho x_{t-1}$, you have $E_t (x_{t+j} - \rho x_{t+j-1}) = x_t - \rho x_{t-1}$. 

For prediction, yes you can consider the models $$ y = \beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p + \gamma_2 \hat{y}^2 + \cdots + \gamma_m \hat{y}^m + error, $$ where $\hat{y}$ represents the first-step OLS fitted values and $m$ is chosen by something like cross validation. I haven't seen this approach used before. My personal guess is that this approach is not as useful as other commonly used ones (SVM, splines, GAM, etc.). For example, if $p$ is large (in comparison to the number of observations $n$), the first-step OLS may already be overfitting so including $\hat{y}$ is not practical. (Yes, you can use lasso residuals but that's a different story.) If $p$ is small, nonlinearity can perhaps be better handled by splines or even by simply augmenting the equation with quadratic and cubic terms of the features. Some generalized additive models (GAM) are already there too. My personal experience is that nonlinearity is not so important (for prediction using economic data). It is usually a lot more important to avoid overfitting nicely. To me, your suggestion seems to be useful in some cases but not in many. That said, I do not want to dissuade you from pursuing this issue, although there is a (high) chance of ending up with the conclusion that it is not very useful given the availability of other methods. BTW, you would already know it, but just in case, Hastie, Tibshirani and Friedman's book (The Elements of Statistical Learning) is helpful. 

Point prediction and CI are different. For point prediction, we are better off by correcting the bias as much as possible. For CI, what is required from the beginning is that the probability equals $100(1-\alpha)\%$. When $[a,b]$ is the 95% CI for $\ln(y_0)$ for example, $[e^a,e^b]$ is certainly a 95% CI for $y_0$ because $P(a\le \ln X\le b) = P(e^a \le X \le e^b)$. So your $[e^{7.1563}, e^{7.2175}]$ is certainly a valid CI. But the center of this CI is neither the naive predictor (exp[predictor of $\ln y_0$]) nor the corrected predictor of $y_0$ (a correction factor times the naive predictor) due to Jensen's inequality, but it does not really matter. In some cases (not always), you may be able to change the CI to $[e^{a-p}, e^{b-q}]$ for some $p$ and $q$ so that the probability is still 95% and its center is the bias-corrected predictor, but I don't see the point in it. What you suggested, i.e., $[e^{s^2/2}e^a,e^{s^2/2}e^b]$ is not a 95% CI. To see why, let the correction factor be $h$ (nonrandom and perfectly known, for simplicity), so the bias-corrected predictor is $he^{\theta}$, where $\theta$ is the unbiased predictor of $\ln y_0$ ($\hat\beta_0 + \hat\beta_2 \ln x_2 + \hat\beta_3 x_3$ in your example). This "$h$" can be estimated by $e^{s^2/2}$ for example, but while the latter is random, $h$ is assumed nonrandom in order to make it simple. Let $[a,b]$ be the 95% CI for $\ln y_0$, i.e., $P(a\le \ln y_0\le b)=0.95$. Then, $$ P(he^a \le y_0 \le he^b) = P(\ln h+a \le \ln y_0 \le \ln h+b), $$ which is not equal to $P(a\le \ln y_0\le b)=0.95$ unless the distribution of $\ln y_0$ is uniform, which is usually not. EDIT The above is about the CI of $y_0$, not of $E(y|X=x_0)$. The original question is about the CI for $E(y|X=x_0)$. Let $E(y|X=x_0) = h\exp(x_0\beta)$, which is estimated by $\hat{h} \exp(x_0 \hat{\beta})$. In that case, I think the Delta method is a useful option (see luchonacho's answer). To be rigorous, we need the joint distribution of $\hat{h}$ and $\hat\beta$, or to be precise, the asymptotic distribution of the vector $\sqrt{n}[(\hat\beta-\beta)', \hat{h}-h]'$. Then the limit distribution of $\sqrt{n}[\hat{h} \exp(x_0\hat{\beta}) - h\exp(x_0\beta)]$ is derived using the Delta method and then CI's for $h\exp(x_0\beta)$ can be constructed. 

You will see that FE gives a negative trend, RE gives a positive trend, and the Hausman test is very significant. (Above I included a linear trend for simplicity. The results are similar when is used instead.) I think it is trends in X and the presence of fixed effects. 

Model (1) is a regular linear probability model. Your results say that $x_1$ is significantly correlated with $y$. Model (2) is strange. It means that the probability is quadratic in $x_1$ and the turning point is exactly $x_1=0$. You will have difficulty justifying the restriction. The coefficients in Model (3) being insignificant may be a symptom of multicollinearity, i.e., $x_1$ and $x_1^2$ are strongly correlated. To see if that's the case, you can test $H_0: \beta_1 = \beta_2 = 0$. I am quite confident they are jointly significant considering the significant results in (1) and (2). If they are jointly significant but individually insignificant, it's a typical symptom of multicollinearity. Plotting the fitted probabilities for (1) and (3) often helps. Also see if the range of $x_1$ contains the turning point ($-\frac{1}{2} \beta_1/\beta_2$) in Model (3). You usually have insignificant quadratic terms if the turning point is not in data range, because $y$ is already explained well by the linear model, and then the linear term also becomes insignificant due to multicollinearity. 

That can happen and is not odd. For your model the partial effect of a change in $x_1$ on the average price equals $\beta_1 + 2\beta_2 x_1$ (use calculus for simple derivation). Thus, $\beta_1$ captures the effect of an increase in $x_1$ from zero. The coefficient of $x_1$ being insignificant just means this effect of changing from zero is insignificant. In other words, $x_1=0$ happens to be close to the turning point, which can happen in some applications (why not?). We wouldn't worry if the turning point is $age=43$ for a smoking regression. There can be other applications where the turning point is close to zero. In many applications, $x_1=0$ is not very meaningful (e.g., $x_1$ is years of schooling), though I don't know it for your data. Often we subtract the sample mean (or any particular value) from $x_1$ so that the coefficient of the transformed variable ($x_1-c$) indicates the effect of a change of $x_1$ from $x_1=c$. This way, you can also compare easily the results from the linear model and the quadratic model. 

General remarks: The BG test under homoskedasticity can be done using the command in the package of R. The $(n-p)R_{aux}^2$ version mentioned in link works only under homoskedasticity. In the presence of heteroskedasticity, Wooldridge (1991, JoE) gives a discussion (as noted in the Wooldridge textbook you mentioned). What I think: I guess that what Wooldridge does is to use a heteroskedasticity-robust variance estimator. For this, (i) get the OLS residuals, (ii) regress e(t) on e(t-1), ..., e(t-p) and X, and test the joint significance of e(t-1), ..., e(t-p) using a heteroskedasticity-robust covariance estimate. If you want to use R, do the following for AR(2): 

This is an interesting result, not a bad result. If there are no regressors other than time dummies, then I think OLS = RE = FE. (I've done a few experiments with , , and , but I have not proved.) If $X_{it}$ have trends and are correlated with fixed effects, anything can happen. For example, run the following Stata script (copy & paste): 

In the exactly identified case, it is natural to assume that a unique $\theta$ satisfies $\bar{g}(\theta) = 0$, because the number of parameters is equal to the number of equations. Let $\hat\theta$ denote such $\theta$ value. When $W$ is positive definite, $\bar{g}(\theta)' W \bar{g}(\theta) \ge 0$ (because $W$ is positive definite), and $\bar{g}(\theta)' W \bar{g}(\theta)$ attains zero if and only if $\bar{g}(\theta)=0$. That is, the global minimizer of $\bar{g}(\theta)' W \bar{g}(\theta)$ equals the solution to $\bar{g}(\theta)=0$, which is $\hat\theta$, for whatever positive definite $W$ matrix. Thus, $W$ is irrelevant. Note that this argument does not hold if $W$ is not positive definite. For example, if $W$ is positive semi-definite but not positive definite, then the GMM estimator may not be unique. 

You have done two different things. Your fixed-effects model captures the within-group over-time functional relationship between $debt_{it}$ and $y_{it}$ (that is, how much average difference in $y_{it}$ is there between two periods with a 1-unit difference in $debt_{it}$ within a country). In your data, there is limited within-group variability in $debt_{it}$, which probably lead to a large standard error and a resulting insignificance. Your reasoning about the correlation between the fixed effects and the explanatory variable is about the cross-sectional functional relationship. You found that countries with higher $debt_{it}$ have higher $y_{it}$ values (as opposed to periods with higher $debt_{it}$ having higher $y_{it}$ within a country). This is not what you originally intended to investigate when you set up your fixed-effects model. Your Hausman test says that the within-group functional relationship and the cross-sectional functional relationship are different. That said, yes, it is true that "a higher debt does result in higher interest rate spreads", but it is true only in the specific sense that "countries with higher debts show higher interest rate spreads". You should be careful when saying "result in". Overcoming the issue of limited over-time variation and having significant results is hard. There is no magic. Abandon the fixed effects model, and try to control for many time-varying and time-invariant regressors, enough for you to argue that you controlled for most country-specific factors. (You can use RE or POLS estimation.) People might still criticize that you didn't control for enough factors, but you will need to defend yourself somehow. You could also move over to dynamic models or others, but that's a different story. 

Good question. You observe $y_{ijt}$ ($i$: firm, $j$: country, $t$: time). Let us not consider time effects as they are irrelevant to our discussion. You are considering identifying "country effects" using two approaches. In short, the two are as different as RE vs FE in standard 2D panel models, with the main difference being in what you assume about the correlation of the explanatory variables and the firm effects within country groups. (i) Dropping firm-effects and using country dummies gives consistency if firms within the same country have no fixed effects [1]. (ii) Defining country effects by the averages of firm fixed-effects allows for firm-level fixed effects [1] correlated with the regressor levels. More detailed discussions follow. Your first approach (of having country dummies and no firm effects) is represented by the model $y_{ijt} = c_j + X_{ijt}\beta + v_{ijt}$, with $E(v_{ijt})=0$ and $X_{ijt}$ being uncorrelated with $v_{ijt}$. If there is no firm-level correlation of $X$ and $v$ within country groups (that is, if $X_{ijt}$ is exogenous to the error $v_{ijt}$), the random-effects regression (POLS or FGLS or whatever) of this model gives consistent estimators. However, if $v_{ijt}$ contains firm-level fixed effects (say, $\mu_{ij}$) that are correlated with the level of $X_{ijt}$, this approach leads to inconsistency. The model corresponding to your second approach is $y_{it} = c_j + \mu_{ij} + X_{ijt}\beta + e_{ijt}$, where $E(e_{ijt} | X_{ij1}, \ldots, X_{ijT})=0$ for all $t$. What is important here is that $c_j$ and $\mu_{ij}$ are not separately identified without further restrictions even under this strict exogeneity of $X_{ijt}$ (because you can add a constant to $\mu_j$ and subtract the same constant from $\mu_{ij}$). You are proposing to define $c_j$ by the further restriction that $n_j^{-1} \sum_{i=1}^{n_j} \mu_{ij}=0$, which is fine. What is the real difference between the two approaches? For the model $y_{it} = c_j + \mu_{ij} + X_{ijt}\beta + e_{ijt}$, the difference is that the first assumes that $X_{ijt}$ is exogenous to $\mu_{ij} + e_{ijt}$ while the second assumes that $X_{ijt}$ is exogenous to $e_{ijt}$. Under the assumption that $X_{ijt}$ is strictly exogenous to $e_{ijt}$, the difference is reduced to whether $X_{ijt}$ is correlated with $\mu_{ij}$. If, within a country, a firm with high $X_{ijt}$ level has higher $\mu_{ij}$, then only the second approach gives a consistent estimator. It is all the same as any RE vs FE considerations in standard (2D) panel data models. [1] "Fixed effects" above mean time-invariant effects that are possibly correlated with the levels of the explanatory variables.