Use BCDEdit to set truncatememory option. That will limit your memory. To use it first check what BCD entries you have with 

Is there some command line function within FreeNAS (FreeBSD) derivative which could return my external address? Since that same address is synchronized with DynDns (via router), in C# I retrieved that via DNS query like this: 

Try to install it in Windows Vista compatibility mode (right-click on installer and go on compatibility tab). Since this program does support Vista, I would expect that to work. Only installer is broken. 

It is not supported as such. However, you can use command to create new drive letter out of folder. If you do on root folder, that would give you desired results. E.g. 

I ended up going to Microsoft's support for this. Here's what was going on: At some previous point FrontPage was used to manage a site or sites within the main IIS directory () on the Development Server (). FrontPage keeps track of all its metadata with a number of files (see List of Special Files and Directories Maintained by FrontPage for details) that were left in the directory. This causes Expression to detect that the site or a portion of it is managed by FrontPage and therefore invalidates it as a File System Publishing target; however we can't publish to the directory using the FrontPage Server Extensions (FPSE) because while the metadata files are still there the actual FPSEs are not installed since FrontPage has been depreciated since 2006. The FPSE metadata files are not just limited to one "site" in the sense of your folder path. As I understand it, if somewhere further down the directory tree from there are leftover files, publishing to your site at will fail since the whole thing is contained in a single IIS Site. We discussed writing a script that recursively descended through the directories deleting the FPSE metadata files however there is the danger that the sites still depended upon code and other other information stored on them. You can "de-couple" individual folders in Microsoft Expresson by right clicking the site and selecting Convert to Folder. An additional concern is that Expression always tracks the full path for publishing so if we want to publish a file from to , what you actually end up with is your file being published to . The whole idea of working in your Development server and then publishing to your Testing server only works for subsites as long as the full path is identical. During our testing our support engineer and I noticed that Expression was slow when editing and manipulating files. Apparently every time you change directories, manipulate a site file or do some other action it "re-scans" the entire site directory. It sounds like it doesn't keep a cache and/or doesn't intelligently update it as you work with the site's files. Either way, he said for a site of our size (larger than 1GB) that we are pushing up against the performance limits of Expression and that we should not be surprised if it runs slow or crashes. Fantastic. The final nail in the coffin here is that Microsoft Expression Studio was deprecated in 2010 and it's functionality has been rolled into Visual Studio 2012. 

You may wish to enable offline files for that network location. This will not prevent disconnects but it will give you offline copy to work with even if connection is lost in middle of editing. Once connection is restored, file will be copied back to server and synchronized. Only issue here may be if another user modified that file while connection was down since that would cause synchronize error and you would need to resolve it automatically. 

You will not be able to cache Windows Updates in that manner. For cacheing them, check Microsoft's WSUS. 

P.S. I know that 89.172.197.320 is not real IP address. I wrote it like that in order not to share mine IP address (or IP address from someone else). P.P.S. Thanks for help to Kronick and Avery Payne since their ideas pushed me in right direction. 

Those are standard non-privileged users/groups created by packages so their programs can run without root privileges. If this is the only reason you suspect that your system has a security problem, then it is likely that you don't. Additionally, it is generally a Bad Idea (TM) to just start disabling things unless you really know what you are doing. The defaults are default for a reason. 

Is my understanding correct? I am a bit thrown by the terminology NetApp uses as well as the "snapshot is full" (does that mean one snapshot or the snapshot reserve)? What happens if the user data grows and competes with the snapshot spill space? Does the snapshot occupying it get deleted? What condition/s generally cause this to happen? It's my understanding that once the snapshot reserve is full or if a new snapshot will fill it, ONTAP just bumps the oldest snapshot/s out the reserve as necessary. In that case, how exactly can you even "fill the snapshot"? 

1) no 2) yes 3) maybe For home server, I would go with desktop motherboard since it is much cheaper. 64-bit drivers were problem for quite a while since desktop motherboard manufacturers didn't bother much. However, if you search for little bit better desktop motherboard with Intel chipset, you cannot go wrong. 

I have Asus WL-500g and I use DD-WRT. I currently have v24 SP1 installed and there are no issues with it. I find it really easy to configure and it supports Asus hardware quite well. Torrenting from it is little bit harder with DD-WRT, but there is support for installing additional packages. VPN is supported just fine. 

Can anyone confirm that when doing an online P2V using SCVMM located on another network segment than the source computer and the virtual host that no imaging traffic (or any bandwidth intensive) traffic is sent between the SCVMM server and either the source computer or the virtual host? What happens to the physical host if I cancel the P2V process? Will it fail safe (i.e., will the source machine be effected)? 

This depends on whether you have more time or more money. :D Like I said, there are ways to reduce the overhead of un-managed solution. The overhead may be less than you think. 

Wow. I would right-size your subnet, like right now! I am going to copy/paste this from my other answer but it is very relevant here: You are not managing hundreds of hosts. The complexity of your solution should reflect the complexity of environment. Resist the temptation to be overly clever. You will thank yourself later. Second, I am not sure how it would be more with regards to broadcast traffic since there's only 20 clients to broadcast. When you think about broadcast attack or broadcast fan-outs the limiting factor is generally not the broadcast domain but the the nodes generating the traffic so if your 20 nodes are attempting to broadcast to 252 IP addresses or 16,777,212 IP addresses ( 16,777,192 of which are unoccupied) the same number of broadcasts are going out. Now if the malware does some kind of amplification attack where it starts creating IP addresses, yeah, you have given your attacker a lot more room to play. Maybe that's what your security guy was getting at. Information Security is complicated and I only have a cursory knowledge of the field so if you want to explore this question in more detail perhaps Security.SE would be more appropriate. 

NTLDR error can arise for many different things, but fragmentation is not one of them. In this case I would go with "disk failing" reason since, as you noticed, even TrueImage couldn't read it. Since TrueImage reads everything sector-by-sector, it would not mind fragmentation. Try to backup everything you can since I assume that drive will completely fail very soon. Fragmentation will have effect here since it is much harder to recover data from fragmented drive than from non-fragmented one, but it is definitely not the cause. 

I would assume that this is issue with drivers. However, IDE has same size limit as SCSI on Hyper-V. Usually IDE disks are a LOT easier to use in Linux and that solves problems with visibility in most cases. In your case that is a problem since you use first Hyper-V Server 2008 release. If you can, I would recommend moving to Hyper-V Server 2008 R2 which will allow you to have big disk on IDE interface and thus avoid any driver issues with Linux and SCSI. P.S. You can just disconnect already existing VHD and re-attach it as IDE since format is not dependent on interface. 

A less common but still useful deployment is when you have application servers located in a DMZ that need read access to your internal Active Directory services. There's a few different security models but one involves extending your forest into the DMZ by placing an RODC there. See: Active Directory Domain Services in the Perimeter Network 

I've seen this issue manifest itself in a few different ways. It almost always goes back to either old printer drivers or specialized drivers that do not fully support UAC. A couple of general things to check: 

I have a functioning DHCP/DNS (ISC Bind 9.6, DHCP 3.1.1) server running on Debian that I would like to add DynamicDNS functionality to. I have a pretty simple question: Does DynamicDNS require (or recommend) separate sub-domains? I have seen a few tutorials where the the clients that are acquiring their IP addresses and other networking information via DHCP are on a different sub-domain as the servers which are statically configured (both in terms of IP, and DNS). For example: All the clients are on ws.example.org and the servers on example.org. Right now all of our servers and clients are in the same domain (example.org) but spread across different zone files (because we have multiple subnets). The clients are configured with DHCP and the servers are configured statically. If I want to setup DynamicDNS for the clients should I use a separate sub-domain? What's the best practice here (and why or why not would it be a bad idea to do otherwise)? Thanks. 

As far as I know, there are no provisioning in answer file to do it. Answer file contains only settings that you get asked during setup. Page file is not one of them. 

These conflicts are usually resolved through locks. It is upon application to ensure proper locking. That said, it needs to be noted that most of applications do tend to lock files, especially during writes. 

You can try DynDns. It will enable you to connect your dynamic IP with your DNS address (given to you by DynDns). It only requires small utility on your side at worst. If you have access to router, it gets even easier since almost all routers have support for it. 

@PSaul is mostly correct. You do not want to use or as your time source for your Domain Controller that is holding the PDC Emulator FSMO Role. As the default they are heavily used, often slow due to lack of locality and sometimes unavailable. Pick a NTP pool that is closer to you. However, do not disable Hyper-V Time Synchronization integration. It is required for certain functions like resetting the time after a reboot or when the virtual machine comes back from a saved state. What you want to do is to tell your virtualized Domain Controllers to ignore their Hyper-V host as a time source. This can be done as follows: