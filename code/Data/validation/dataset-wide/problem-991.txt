Looks like Tom Kyte has the answer: $URL$ Gandolf, you were technically correct according to what Tom responds with, but for whatever reason (bug? user error?) I couldn't get that to work. So I tried the workaround: 

I prefer to use the RMAN clone method. The overall method is to start with the PROD database, and then a DEV database that is in a NOMOUNT state with all datafiles/controlfiles/tempfiles/archivelogs removed: 

First, let me say that you haven't listed the version of your RDBMS, so I'm going to assume you're running a version that utilizes the Cost-Based Optimizer (CBO), as opposed to the Rule-Based Optimizer. The CBO was introduced in the 10g series of databases, so I think this is a safe assumption. If you're still running a 9i or earlier version, my answer won't help you. So your question is "will inserts stop my RDB from using an index?". The easy answer here is no, they won't. Your database will continue to use the index it used before, as a_horse_with_no_name mentioned in the comments that the rows will be maintained automatically in the index without any extra effort from you. However, there is a chance that the volume of changes could impact the performance of using that index, which is something that you need to be aware of. How? Take the total row count of all rows that could be included in an index (remember we cannot index nulls). Then consider the number of inserts you've done - is this number >= 10% of the existing rows? If the answer is yes, then your statistics are now considered out-of-date. Luckily, by default Oracle's later versions installs an overnight job (usually running around 10pm local server time) that will automatically gather statistics on any table & indexes where it senses a 10% or greater change in the count of rows. But, if you are doing something outside of ordinary processing, say adding a significant amount of rows due to a one-time data load, you may very well need to gather stats yourself sooner than the automatic job. Also, in my experience, the automatic job only has a certain window of time in which to gather stats, so it's possible that it won't be able to gather stats on all objects if large amounts of data was manipulated across many objects. My point here is that changing data volumes significantly in a short period of time can lead to performance problems not because your indexes will be bad, but because in a Cost-Based world, the Optimizer will have bad information, and will make bad decisions. I bring this up because you seem focused on indexes, which are only one tool in a much larger toolkit to having a well performing database. HTH. 

I have found a query that is a major performance issue in my environment. I looked at the actual execution plan for this query and found out the main problem is the remote scan (98%). Its remote because SQL Server is accessing its system objects (DMVs). If you look at the estimated number of rows it says 33, but the actual number of rows is over 16 000. If it was a regular table query I would look at the statistics for the columns but in this case I don't know what I can do. 

This way the engine knows when you want to use Column from TableA and when from TableB. It is a good practice - you should always use aliases when writing SQL queries. The query you are looking for might be: 

You might have to change the SQL Server database engine service account (local account is default). It's simple. I would recommend using Ola Hallengren's Maintenance scripts though. They give you much more versatility. 

This gives me a block-for-block recreation of the production level database, and depending on how many threads you give RMAN, can be completed pretty quickly. From there, I have a set of environment specific scripts I run that change passwords, null out email addresses, etc in the DEV database to configure it the way it needs to be. 

Don't get hung-up on "cloud" vs "local" - what you're doing is establishing a TNS network link between two databases. It is nothing more than that. In order to do this, you will need to have the appropriate tns entries configured in your tnsnames.ora (unless you are using LDAP or you pass in the entire connect string). If you don't know how to do this, the netca application will create one for you using a wizard. Once you have this tns entry, you can simply create the database link as you listed above, but replace the USING clause value you have with the tns entry alias. $URL$ 

I am looking for a better solution. I heard that SQL Server Integration Services have a MERGE JOIN and a LOOKUP transformations. I'm not sure if one of these provide the possibilities that I'm looking for. What tool can you recommend? I'm sure it can be done efficiently in SSIS but I just don't know the right solution. 

I have a query that collects customer data. I select a lot of correlated data from different tables. I want to have only ONE result row for each customer. The query looks like that: 

I'm sorry if this question breaks the rules of DBA StackExchange. I'm not sure if it is allowed in here. I am planning to lead a basic SQL Server training for my coworkers. The course should include some basic information about relational databases and the SQL querying. This is why I'm looking for some training materials. Obciously I could prepare PowerPoint presentations, SQL tasks and all the other stuff myself but I think since it is a well researched topic I should not reinvent the wheel. Do you know of any training materials available that I could use? I have the training materials for the MS Exam 70-461 - Querying SQL Server but I'm not sure if I would be allowed to use it and also they are pretty big. Can you recommend something? 

Technically, you could use Oracle's SUM function (good explanation here: $URL$ Your code would look something like this: 

Personally, I would start with DBA_HIST_ACTIVE_SESS_HISTORY, and look at all statments that contain that type of hint. From there, you can pull the index name coming from that hint, and then do a lookup on dba_indexes to see if the index exists, is valid, etc. You should be able to do this via PL/SQL if you want to make it really fancy and do it all in one step, otherwise a few pieces of SQL and a spreadsheet application can be your friend. 

It's not necessarily the number of transactions, but the timeout. The parameter is set to 60 seconds by default. The purpose of this parameter is to avoid having distributed transactions in a long running wait status while something else is performing work on that row; the transaction will wait 60 seconds, then Oracle kills it. You can modify this parameter (requires an instance restart) to whatever you want (in seconds). 

I don't want to wait the estimated 2 months for the rollback to complete. The transaction isolation level of the database is READ COMMITED. Is it possible to kill this session? How should you act if you find such a query in one of the mission critical databases? 

I am configuring transactional replication in SQL Server. The subscription is configured as push from 2008R2 publisher (distributor is the same server) to 2012 subscriber. The object I want to replicate is an indexed view. The base tables exist only on the publisher. The replication fails due to the following error: 

It is true that the view doesn't exist in the subscription database. How can i create it without the base tables? 

Using different users will not reduce or increase the performance. It is clearly a security issue. Usually you don't want users from company A to be able to access company B data. I would recommend to create separate users for each application and grant them only the necessary permissions. If you run the application using a user with admin rights you risk a lot when the application gets compromised. 

This might be over simplifying it, but could you use an block around the call to the extra procedure? As in, "check to see if the variable I just wrote to is null, if not, do more work, if it is, specifically call my custom error package? If you've built your own exception handler (and kudos to you for not falling into the "catch all the errors generically" trap), you don't HAVE to call that in an exception block. You can call it whenever you want, and trap the errors whenever they occur. 

I suspect this may have something to do with you starting impdp as sysdba. Please see the note in $URL$ "Do not start Export as SYSDBA, except at the request of Oracle technical support. SYSDBA is used internally and has specialized functions; its behavior is not the same as for general users." Try your command as system. 

Assuming you mean that the database is "missing" logs from P->R, and that your primary database no longer has those logs either, then the answer is that you'll have a functional database (upon standby activation as a primary) but you will be missing data from P-> the Activation point. The thing to remember here is that your standby is only as good as your last archive log that was fully applied. Without intervention on your part, your standby database is essentially frozen in time, and will never advance. If you're missing archive logs, and they are truly unrecoverable (did you check storage backups? Backups to tape?), then your standby, IMO, is worthless as a production level standby. You, and your business, may have a different opinion based on your usage of the database in question. But if this was truly a production level standby database that is meant to take over for production in a disaster scenario, personally, I would rebuild the standby immediately. 

I have recently upgraded a few SSRS projects and solutions created in Visual Studio 2010 to run correctly on VS 2017. I changed the target SQL Server version to 2012 (since this is the version that is used). One of the problems I face is whenever I upload any of the RDL files to the Report Server I get the rsInvalidReportDefinition error. This problem is further discussed here: Error while uploading a report I used the recommended solution and uploaded the rdl file from the bin folder instead. This time it works fine. The problem is that the files in the main folder still do not work correctly on the report server. I am wondering if I should maybe replace the files in the main catalogue with the files from the bin folder? Can you help me understand this issue better? 

I think it's a great news that Microsoft finally changed this default setting. The previous one was really bad - it caused problems with latch contention. Paul Randal's article on the topic 

I completely agree with dartonw's comments about reading through the guide. That's the only real way to start understanding the concept of indexing as it relates to Oracle and the Optimizer. One thing I will add to it though is this - Philosophically, as a DBA, I rarely decide what columns to index on my own. What I mean by this is that I go back to the developers and ask them for the use cases of searching that table. If the table has five columns, and the dev tells me "we'll be joining this table to that table by columns a and b to retrieve columns c,d,e", that tells me that I need the index on columns a and/or b. The Developers/business users and their SQL should dictate what columns are indexed, not what you have read about in a book. Each app is different, and your indexing should be built specifically around data storage and retrieval of that app. I hope that makes sense. 

Is it possible that the frequency of rebuilding indexes (which automatically triggers statistics update) can affect the query logic? One of the developers I work with claims that a stored procedure failed (there was a type conversion error) because the statistics have not been up to date during the query execution and it used the wrong query plan. He also claims that after having updated statistics the procedure works fine. Is it even possible? 

A few notes about both of them. If a server crashes and you can't repair it what should you do? Set up a new server from scratch? In both physical and virtual environments it's gonna take a lot of time. When the database server is a VM should you recover it from a VM backup? This operation can last multiple hours for bigger machines. Or maybe one should keep a clean virtual machine with database software installed and in case of disaster: - restore databases on the clean VM - change the IP address and start using the new database server Any ideas? Or maybe you can submit some useful links or refer any books? 

I can't personally think of anything better for on-the-fly resource throttling based on to-the-second stats than Resource Manager. It can be a bit of a hassle to set up, but I don't imagine how writing custom code wouldn't just be reinventing the wheel that is Resource Manager. I'm not aware of any reliable 3rd party vendors at this time. I do know that you don't have to do anything to your stored procedures. IMO the best way to use Resource Manager and stored procedures is to use DBMS_SCHEDULER to tie the resource plan in with a scheduled job. Documentation on dbms_scheduler is here: $URL$ Also, IIRC, remember when implementing that Resource Manager handles CPU and I/O, but not Memory utilization. 

This is working now. Of course, as you can see, I also had to change the inner sql to be a loop, because this wouldn't work if a user had logged in with 2+ session. Additionally, it still isn't working because the very act of querying is doing a parallel query, which means that there are a whole bunch of "ghost" session ids that only exist at the time of the query, resulting in this procedure returning: