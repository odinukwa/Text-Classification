Several philosophers have offered different ways of seeing that our intuition of time is not internally consistent, and may only be a construct for human interpretation, rather than something ultimately real. McTaggart directly attacks the notion of time as an ongoing process, what he calls the A-series of events. His actual argument is a bit abstruse, but special relativity, to some degree, captures it better and backs him up. If there is no way to declare events simultaneous, then our notion of time 'unfolding' faces severe obstacles. Kant points out in his second 'antinomy' that we do not have a firm grasp on the notion of time because we can logically prove that it both must and cannot have 'ends'. If we cannot get a handle on its basic topology, he proposes, perhaps we should back off from considering it too much of a fact. Loschmidt's paradox suggests that the direction of the flow of time has no consistent basis because we keep finding that the principles of physics are essentially reversible. Boltzmann included this notion in his initial notions of thermodynamics, suggesting that we experience consistent time only because we are occupying a part of space relatively near an event of extremely low entropy. 

The point of Schroedinger's cat is not just that you have not checked, the point is that the cat is killed based on an indeterminate event. Indeterminate events in normal physics have to be in one of the allowed states. Indeterminate events as we observe them in quantum dynamics can be in multiple states at once, and only decide what state they were in during the past when some result affects something measured. This ability to not have to write history until you hit another particle is the point. It seems insane. We like to believe history is written as time passes. But on a microscopic scale where individual particles may be far enough apart that we can separate out each interaction and determine its state, this just is not true. Past history is written when particles interact later. Of course there are so many particles, on any normal scale, that this almost never matters. Immediately after one interaction, there is another, and another. Fairly quickly some of those contribute to some noticeable effect on our shared reality, and things are decided. In a literal case of a dead cat in a box, that cat is going to rot or not, and you are or are not going to smell it rotting. No need to open the box. Even if you had an airtight box, you would have to isolate the cat so thoroughly that its body heat could not contribute to the temperature of the room around it, as on some subconscious level we all measure that. So this is not a realistic idea, just a hypothetical to make the point of how strange time is on the tiny scales where complete accounting is theoretically possible. This kind of leads one to accept a view of physics like Leibniz's, where the monads all 'commune' and 'decide' what happens, over a form of materialism where actions are independent and absolutely predictable. You can consider the distinction a word-game, but it seems to really matter. For instance, why should time run slower when there are a lot of particles present? (We observe the gravitational time dilation proportional to mass from general relativity, and macroscopically, mass is basically a particle count.) You can insist it is all about objectivity and relativity of measurements, but maybe that is just the effect. It makes comparable sense to consider that those particles, being more numerous and more intimately interconnected really might have to 'commune' more in order to 'decide' how to move on -- so time really passes faster for more 'more independent' particles more isolated in space. 

Pyrrho got there long before Descartes, and stayed in the vein for a longer time before betraying its basic principles. Sextus Empiricus' rearrangement of Pyrrho is the closest thing we still have. Descartes only wishes he could manage to be a radical skeptic. I don't think the natural exit point from true radical skepticism involves three separate proofs of God, and all the other trappings of Descartes. And as for all the disparaging comments. Bear in mind that major world religions, including branches of Hinduism and Buddhism think it is important to be radically skeptical, at least on a theoretical level. 

The only way out of a real logical paradox is a different form of logic. For instance, in a logic that is temporalist or accepts non-well-founded truth values, you can create a special category of truth value that changes or alternates, reflecting the mental state of someone initially considering the statements continued indefinitely. In a logic that is dialetheian, you can accept that some truth values simply remain in conflict to varying degrees, or in a logic that is intuitionistic, you can decide that unprovable statements just don't need truth values and are better off without them. But there is no way to maintain classical logic, retaining all of its strengths, and resolve all the problems that traditional traps like self-referential negations or actualized infinities can introduce. 

Right, propositional validity and rhetorical sense are different things. In propositional logic, any falsehood implies anything you wish. Whereas we know from experience that you can only get from certain lies to certain other lies without doing something obviously problematic. Likewise, in propositional logic anything true implies any established truth, whereas we know that specific truths have to be reached by defensible paths. The interpretation of "A implies B" as "B or not A" is an idealization of real rhetorical logic into a form that assumes all true deductions have already been established before the question is asked. It is tense-free and ignores the ordering of premises or logical steps. It is a safe way to learn to argue and to vet arguments because it is conservative in this fashion. Anything provable from a rhetorical point of view is provable in the idealized system, and vice versus, but some of its actual acceptable arguments will come across as unconvincing to humans. 

The short answer is "Enough not to be easily misled". And to me what that means is that one needs to be immersed in an academic culture that will allow others to care about your work and to contribute perspectives from outside your discipline. I do not think that the philosophy of mind, for instance requires any understanding of neurophysiology. But it requires the ability to capture a metaphor that will not be derailed by idiosyncratic or purposefully misleading interpretations of neurophysiology. You need a grasp, at a man-on-the-street level of someone who might watch Horizon or Nova. Most importantly, you need a good sense of who is making sense. To some degree, Kuhn can save you here: when it is most important, that is a sociological question, and need not be addressed by actual understanding of the field's internals. 

To treat someone merely as a means you need to be not considering the value of the outcome for them as a person. You have to be intending they should see no benefit or choice, despite their involvement and significance in the situation. When it is done ethically, imprisonment is meant to serve an reformative or an expiative effect through its restriction of the individual. The potential for reform or the affirmation of the person's individual responsibility and power of self-determination and the release of their emotional guilt means that this is not use of the prisoner as a mere means, as long as it is genuine. The latter is kind of subtle, but I think it is the aspect Kant would emphasize, because he minimizes the significance of predictions of the future. (No human skill should be a direct requirement for moral action. So the ability to understand likely outcomes should not matter. Guessing that reform is more or less likely is a computation -- which requires a skill.) So let me explain 'expiation'. Tying criminal acts to consequences means that one is legitimately making a decision when one acts as a criminal. Not following through on the deal under which that decision was made is unfair to the person making the decision. It undercuts their sense of structure and fairness in the world -- a world in which they live as much as everyone else. It handles them paternalistically, and reduces them to having a childish lack of traction on their future. So rules are good for everyone, including those who break them. Stating those rules, but then not following them, without making some other demand or plan that everyone involved would agree is in fact 'expiative' is bad for everyone involved, including the perpetrator, as it leaves the self-loathing of guilt and the fear of retribution on the table. (As usual, a genuine Kantian solution allows for both secular and religious interpretations. The notion of expiation allows for the Christian alternative of genuine contrition as an alternative to actual justice, but only, again, when it can be proved genuine to the victim, who gets a sense of righteousness from his forgiveness.) 

I am not in favor of vigilante violence against any political position, but, although your position is likely to be correct, your argument for it is not valid. It is the slippery slope fallacy to say if you allow X at some level of Y, then X is going to be possible at a different, unrelated level of Y. Violence that does not kill is not killing, so saying that to permit slapping your partner for embarrassing you in public facilitates murder is obviously not a well-founded argument. One might die from a slap, but that is not likely. I could make such a major miscalculation that I slap you hard enough you have a heart attack from shock and die. But making the call to slap you is still not my deciding to kill. To render those equally immoral is misplaced. "Well, who would define that level of Y?" does not dispel the basic fallacy either. People do just define things. There can be measures in a moral theory that simply appeal to convention or human intuition. We can set a standard and allow for a reasonable level of misunderstanding and ambiguity so that we know using that standard is generally safe. Consider the Aryans on which 'The Order' in the play God's Country are based. They propose a 'point system' that logically implies that to be a man in their society if there are any Black people left in this hemisphere, one must eventually kill one. And although its is a fictionalized amalgam, these rules did in fact actually exist for some Skinheads in the U.S. in the 1970's and 80's. Using physical violence against people who have openly sworn to kill is not necessarily a problem. If you capture them via violent intervention and deliver them to the police, you may be stopping them from killing. And if they are just being rhetorical and do not intend to kill, you would be dissuading them from targeted terrorist manipulation. If that is part of your definition of what Nazis have to do to be Nazis, it is a safe one to use in this circumstance. No? A label is not a criterion. So what someone is called is not part of a moral deduction. People lie, especially about their enemies, and especially when using hyperbolic labels. But what they are doing or have done, are saying or have said can be used as valid criteria to decide how to treat them. These people probably can't really choose a definition of Nazi that works and applies to Charlottesville. But if they could, they may well be able to support their position in some way. Your dismissal of all possible arguments they might make is highly likely to be right, but is still logically premature. 

To be a little sarcastic: Data are given, Information informs, and Knowledge is known. But the point is that their etymological roots are nouns made of verbs 'dare: to give', 'in/formare: into/to build', and "cnawan: to discern'. To be given means that the data simply exists, and may not even be processed, but it is available in some form. The number on the side of an instrument or a 3000-page list of numbers that no one will ever read are still data. To inform means that the information is being added, transmitted, acquired, etc. to form the mind. The 'in' is a direction, and information, therefore, has a dynamic quality. But in its modern sense, information does not have to go into a mind, it can go into a repository of data. But knowledge is specific to the mind. It is etymologically related to recognition, not storage. It bears the sense of internal perception. To know something is to have the emotional impression of having it, and having impressions or perceptions cause recognition of it, bringing it out of some passive state so that it can be experienced as present in the mind. 

It seems to be a matter of simple absence of degree -- black and white thinking where only perfection signifies. If something cannot be perfectly distinguished, then the law would be unfair to those where it is real but not distinguished. We should therefore just never consider it, lest we be unfair occasionally. Only perfectly fair laws should exist. 

There is no fallacy here. The primary pitfall here is not one of logic, but one of semantics. In abstract terms, your title describes a correct deduction. If A only occurs because B always causes it, then when A does not occur, B must not have happened. In an indicative mood, if we are being perfectly careful, this is what we mean, because we have "open quantification". Unfortunately, English does not make that a rule, because the subjunctive markers in English have evolved into a useless mess, and can therefore be omitted at any time. You can make the statement without implying the always. If B only sometimes causes A, then obviously in those cases when it fails to do this causing, you have B and not A. But in that case, you should say B may cause it, not that it does cause it. You can also make the statement without implying the only. But in that case you should say A may be caused by B, not that it is caused by B. It might occur for other reasons. But that omission still allows you to deduce that if you don't have A, B wasn't there. So this English statement can have three different meanings with very different logical implications. You can only tell if you know your speaker is an absolute pedant, or by context. And deduction in your title is only true for two of them. This ambiguity is one way of seeing what is wrong with your first example. 

Everyone has to believe things for which they have no evidence. Hume has still not been disproved. Science believes in causality, as a thing separate from mere correlation, but there can never be any real evidence that it exists. We can only note correlations and declare the ones that accord with a given theoretical mechanism as causes. So yes, this kind of absolute attack on baseless faith is hypocritical. We all have baseless faith in things like causation, that are grounded in our basic shared psychology, because they work for us. That said, probability based on observation is one of those things we tend to believe in. It is not hypocritical to witness a succession of failures and state that ultimate success is unlikely. On that basis, it is perfectly logical to look at the range of religions people have had, the strength of the associated faith, and their ultimate failure, and declare it likely that none of them is true, and that any new one is unlikely to be true. Dawkins is as justified in his several places of accuracy as anyone who disbelieves in perpetual motion, or unicorns to an equal degree. But his notion of 'failure' is probably wrong. Those religions sustained people, individually and in groups, whether or not they correctly predicted actual events or expressed actual history. We need to come around to a place were we can accept mythology for what it is -- not physics or history but still meaningful.