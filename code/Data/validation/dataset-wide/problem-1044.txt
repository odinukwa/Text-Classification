The hybrid approach is often used, with many of the main DB vendors having a complimentary in-memory database to synchronise with their traditional database: 

Whereas you can have multiple unique keys in a table and the referenced columns may allow null. By declaring a primary key, you are saying "this is the candidate/surrogate key that should be referenced by foreign keys". This is well understood by database developers who are unlikely to apply a foreign key to a unique constraint when a primary key is available. Unique constraints can also be used to enforce rules such as "a customer can only have one default contact number". A primary key can't be used for this purpose, as customers may have multiple non-default numbers, so the constraint can't uniquely identify all rows in the table. 

You could then have further child tables below hardware or clothing, if there's more specific details required or just add columns to these tables (which may be null for some clothing/hardware types). It's fine to have nullable columns, though if a large percentage of the columns will be null due to the product type most of the time, you should think about splitting the table into separate child tables, similar to above. If you need to say which products a vendor sells, you can link them via a table. 

1: In some cases it may be worth including a column in an index if this means all the columns in your query are in the index. This enables an index only scan, so you don't need to access the table. 2: If you're licensed for Diagnostics and Tuning, you could force the plan to a skip scan with SQL Plan Management ADDEDNDA PS - the docs you've quoted there are from 9i. That's reeeeeeally old. I'd stick with something more recent 

It looks like this is a bug with the handling of functions on collection variables. Given the following function: 

As you've identified, storing the price on the order makes the technical implementation easier. There are a number of business reasons why this may be beneficial though. In addition to web transactions, many businesses support sales through other channels, e.g.: 

As Phil says, you can't really to this retrospectively. You can create DDL triggers to capture this in the future however. These will fire before/after a DDL event, allowing you to capture the dependencies to a table: 

Note the ORA_ROWSCNs are the same in the second example. You can't index the ORA_ROWSCN, so filtering on this will result in full table scans. If combined with filtering on an insert timestamp you could overcome this however (which should also help prevent updated rows appearing, if that's the behaviour you want) 

When choosing index column order, the overriding concern is: Are there (equality) predicates against this column in my queries? If a column never appears in a where clause, it's not worth indexing(1) OK, so you've got a table and queries against each column. Sometimes more than one. How do you decide what to index? Let's look at an example. Here's a table with three columns. One holds 10 values, another 1,000, the last 10,000: 

We currently have a 100GB database which is in simple recovery mode, and it is running on 2008 R2. We have a new server with SQL 2016 Enterprise installed, ready to go - and we're building a migration plan, for how to do the upgrade. We've ran a test run using SQL 2016's Data Migration Wizard ($URL$ which we used to point to our current, live, production DB instance and to restore the DB to '16. This took 15 minutes, which is very quick considering the other approach (backup currently-running DB, copy over to new server) -- mainly because we can't back up / restore over a UNC path. We've realised that we have a lot of old/unneeded data in our DB, and have kicked off some scheduled tasks using the SQL Agent to perform some deletes, so by the time we're near to our "go live" date then we'll have less data residing in the DB, with the idea being that we should be able to restore the live instance over to '16 quicker with less data. However, obviously the actual size of the database FILE on disk will not shrink until we perform a DB shrink. We're thinking of shrinking the DB for these reasons: 

I'm aware that by shrinking the DB then the DB size will just grow again in some time, but I think we've previously overlooked some aspect of data growth, to which we've put some processes in place to reduce excessive DB writes. We currently have a 75MB autogrow which happens around 2-3 times a day. Will this be a good idea? We'll be restoring our DB over, upgrading the SQL compatibility level and then running an index rebuild (using Ola Hallengren's index script) & updating stats. I'm curious if the shrink will massively spike these index operation times as it'll leave the DB pretty fragmented from what I've understood. I'm not sure what the best approach is here - perhaps don't shrink and only rebuild the index on '16 after the data migration? Thanks 

Environment is SQL 2008 R2, 26GB RAM, 4-core Xeon E5-2650 @ 2.6GHz. Please see the image below, our maintenance plan which reorganises indexes (not rebuilds). This is run on a weekly basis, and within one week, this plan went from taking ~5 minutes way up to 1hr - 1hr:15, and it has not improved since. We've had to disable this plan and work around this in another way. I've looked at the amount of data in the database and it hasn't grown by a considerable enough volume to cause this issue. 

I've been in touch with our hosting company to see if anything has changed within our environment, but they are saying nothing's changed, there's no limitations/restrictions in place on our VMs, IO seems ok. We haven't made any changes to the underlying application neither. From what I can see, other things have started to fail within this week period too - some queries which ran perfectly fine are starting to timeout. I'm at a bit of a loss as to what could have caused this and am unsure how to retroactively find out. Could anyone point me in the direction of where to begin to discover what could have been wrong here? 

If you just want an overview of your system in a time period (including "heaviest" SQL statements), the AWR report itself gives this. You can find an intro to this on oracle-base. 

Then I would go with storing a row for every day each resource is assigned to a project. While these kinds of query are possible when using start/end date ranges, they're harder to write. Overlapping dates are also easier to prevent entirely using constraints or identify using queries. So your table would be like: 

This can be done in a single statement, but you have to wrap the sequence call in a function. By calling a function in the select (as opposed to the sequence directly), you overcome the problem of getting ORA-02287 errors. However, the function will be called once for each row in A, which is not what you want. This can be overcome by defining the function as deterministic. This allows Oracle to optimize the function so it is only executed once for each distinct parameter value passed in. To make this work in this case, you'll need to pass in : 

Now both indexes are back to the same size. Note this takes advantage of the fact there's a relationship between few and many. Again it's unlikely you'll see this kind of benefit in the real world. So far we've only talked about equality checks. Often with composite indexes you'll have an inequality against one of the columns. e.g. queries such as "get the orders/shipments/invoices for a customer in the past N days". If you have these kinds of queries, you want the equality against the first column of the index: 

As this solution implements the constraints in the SQL layer, it overcomes some of the concurrency issues discussed in the procedural solution however. UPDATE As pointed out by Vincent, the size of the MV can be reduced by only including the rows with stage_id = 1646. It may be possible to re-write the query to consume no rows, but I can't think how to do that right now: 

The row_number() clause is to enable the conditional insert (when r = 1). Without this, you'll insert into a for every value present in your outer_tab. 

By doing so, you'll make it much easier to answer questions such as "What are all the trains going to station X on 1 Oct?". It'll also makes "temporary gaps" when trains aren't running (e.g. Christmas day) possible to identify. A one-off train is now simply one with only one entry in SCHEDULE_DAYS. As the schedule can be different on weekends to weekdays, I think it's better to have separate rows for each day. This allows linking different schedules for every day of the week, should you ever need to do this. 

You can use the package to "re-organise" a table like this. It provides functionality for you to create a temporary table with modified contents of an existing table as you describe. It also allows you to copy over the grants, triggers, indexes etc to the new table, then switch the tables over so the temporary table becomes the live table. See this article for a worked demo.