I am using event notifications to capture data and log file autogrowth events for all databases on my servers. I'm using the data to for analysis of database storage configuration. In looking at the data I've noticed that the average duration for transaction log growth is well above anything I would expect which leads me to think that I'm either misinterpreting the data or overlooking something related to how transaction log autogrowth works. This is an example of a log file growth event that was captured today: 

It ensures that the constraint is enabled after it is created. Your statement includes which is the piece that says not to check for existing bad data during the creation of the constraint. As written, the existing data will not be checked against the constraint because of the in the first statement. Issuing the second statement will enable the check against the constraint for any future changes to the table that are covered by the constraint, up to the point that an is issued. The statements, as written, are basically saying "Create this foreign key constraint but don't check it against existing data. Make it active for any upcoming changes to the data." 

Since duration is reported in miliseconds I'm reading this as it taking 69.54 minutes to grow the file. Autogrowth for this log file is set to 512MB (limited to 2TB) 

Note: is the logical name of your data file, not the physical file name on the filesystem. You can find the logical file names of your database files with the system view: 

Having searched quite a bit and come up empty I decided to just go ahead and remove the registry keys for the IP address I wanted to delete from the configuration. Steps: 

You can set the database's max file size to 9GB, but you will need to do the purging yourself. This is not something that SQL Server supports. You could get notified of the limit being reached using extended events, but there is no way for SQL Server to know what records to delete without you telling it. SQL Server does not log or otherwise keep track of how old a row in a table is. You have to build it into the table schema and maintain your own aging logic. To set the file size in SQL Server Management Studio: 

I "inherited" a web application which is designed and implemented horribly (both the application and the database). For example, the main data is stored using a sort of emulated key-value storage in a Postgres 8.2 database, making it virtually impossible to extract useful data from it in a reasonable amount of time. Currently I'm working hard on replacing the entire application + database, however it will take a few months before the new application is finished. This is a problem since the website is really slow due to the extremely bad database design and even worse queries. Therefore I'm planning to fix the database design and the queries on the live site until the website has an acceptable load time as a temporary solution. I do however have a few limitations to work around, the most problematic ones are: 

Create initially empty tables to store the data in a sane way. Create triggers on the old tables which can sync the data with the new ones. Export the data from the old tables to the new tables & enable the triggers. Replace the queries one by one. 

So in fact I want to group by message_type and user_id, but instead of generating multiple rows per user, I want to create multiple columns, one for each message_type Can I achieve this without hardcoding the message types in my query? 

I have a messages table in a database, which include a sender id and a message type (and of course many more columns not relevant for this question). I try to create a query which counts how many messages of each type a user have send. e.g. if I have the following table: 

However, there is a problem with this design: a row should only be allowed to reference a connection when the education type matches. E.g. a row can only reference a row in the table that references a row in the table with type == master. Would it be possible to add a constraint which can check exactly that? If not, what other options are available? 

Using this strategy I will have 2 synced databases after step 3. Initially all queries will go to the old database, but while I'm updating the queries slowly the old database will be used less, and the new one more. Taking this "sub-optimal" situation in mind, is this a good strategy to fix some of the problems? What things should I take into consideration while doing this? Note that I fully understand that this is a very risky suicide mission. However, I'll have to do something in a short amount of time, otherwise the website becomes entirely unusable. 

When Windows tries to connect to the Shared Folder it uses the PC1 login (JhonDoe 123456) since this is your local login and then it compares with the remote login information and it matches exactly with the PC2 login (JhonDoe 123456). Just then, it let you access the shared folder without any prompt for password. However, when it faces this situation: 

Is there anything I am missing? Is there anything I can be doing better? Is there something I can make to be even more reliable since my Goal is to accomplish a self driven backup server that stores backup from all other Servers. 

Since I have to make this works, I have to make a plan and present it to the IT Department, so I need to be specific for what I need. What I believe is I need to change a few things: 

Make the Servers connect to the Domain and create a Domain Group named Servers. Add the Domain group Servers to the users allowed for the Shared Folder. (I don't know if it's necessary) Make the MS Sql Server service run by one standard user in all the Servers. Restart the Server and place the script in a Job. 

Still, remember that going full onto normalization has drawbacks on performance as well. It all depends on the business and how you are trying to manage the data. 

This is quite interesting since all permission are being overlapped again and again. Let me explain, Your Sql Job run as a different user you connect to the server (Some cases they are the same but that's the best scenario). You can check this by running this command on the Sql Server editor (enable the xp_cmdshell as well): 

I had a query that copies the backup to a Network Shared Folder hosted by the Backup Server but it fails to run since the SQL Server service is executed by a different user (sqlservice), I login to MS SQL Server with (sql_admin) and my Windows login is (DBAJhonDoe) I made a batch file and try to accomplish the same but nothing changes. It still fails for the same reason. I made a login for Sql Server with my Windows user with sys_admin privileges but the same thing happens.