The definitive guide for online credit card processing is the Payment Card Industry PCIDSS standard: $URL$ All of the things on your list (and much, much more) are in that document. Doing it right is a lot of work, are you sure there isn't a already written app/service that will meet the client's needs? 

Basically, in active mode during transfers the client will sometimes listen on a random port and have the server connect to that port and start sending data. If you are behind a firewall the inbound connection from the FTP server fails, unless the firewall is capable of snooping on the traffic, and opening the incoming port. Since the listening port is chosen by the client behind the firewall randomly, you would have to open every port for this to work. Obviously that is not a solution. However many FTP applications can be configured to use a small set of ports for active connections. You need to configure your FTP client to use a fixed range of ports and then add iptables rules to allow inbound connections to those ports. In the end, I strongly recommend passive mode FTP. The only exception is if you have a really old FTP server that doesn't support it. Mark 

AFAIK IPtables doesn't have enough information to do that effectively. You would probably need to write a script that uses netcat or data from netstat to decide if the port is listening, and if so do the redirect. Run that script from cron. 

Newegg and several other retailers stock a decent selection of generic rackmount cases, if you want to move your motherboards into new cases. The quality of the cases are generally good, but I've found the rails that are included are often very difficult to mount. You can also buy shelves that mount in standard racks and place the PC cases on those. $URL$ Or, if this just for home, skip the expense of a server rack and instead use custom shelving. The inexpensive restaurant wire shelving offered at many big box stores works great, and it is actually used by many hosting companies in their datacenters. $URL$ 

Could you add a script, installation step or group policy to change the permissions on that file to writeable by Everybody? 

I don't know of a prebuilt tool, but you can wrap a UI around Dell's omsa command line tools pretty easily and they run in Linux. In our case it was easy to do things like add a "identify" button and a data agent to our existing inventory system. Other tools like BIOS autoprovision script using PXE etc. Just setting up SNMPtraps and having something like Zenoss to capture them does most of what you really need. 

One solution is to use ksplice. If you use Ubuntu or CentOS kernels you can subscribe to the ksplice.com service, where for a small fee they will provide you with special kernel images that can be used to patch a running kernel. Reboots are not required for most updates. Pretty easy to use and setup. If you are particularly skilled you can use the ksplice patches to build your own enabled kernels without subscribing to the service, or for non-standard kernels. 

Anyone have suggestions on a basic, easy to use LDAP Authentication server package for greenfield deployments? I find myself at job after job fighting the battle of getting the schema setup correctly, writing a password change/recovery webpage and other tools to get the environment going. Most of the LDAP world is centered around tools flexible enough to fit into existing LDAP deployments but so flexible there is no "best practice" for new environments. There are of course commercial suites that do this well. Often AD is the simplest solution, being both common and working well for both Windows and Unix systems. 

Depending on your skill and hosting provider, you can generally setup host based firewall/port filtering (iptables on Linux, Windows Firewall, etc) for no charge. You would choose a hardware firewall for an additional measure of protection, or to mitigate certain types of attacks. Generally one is not required (I prefer hardware firewall for Windows, and generally stick to software firewalls and disabling services for Linux. YMMV.) If you go with a hardware firewall it should be matched to the bandwidth allocation and usage of the site. 10mb is reasonably large for a single server and a small to midsized site, depending on the site itself. 

Anyone know of a way to "boot" another Linux kernel from within Linux, discarding the original kernel/initrd? The idea is that I can boot my server from a small read only copy of Linux that does some housekeeping/data gathering tasks and then checks in with a management server to figure out what to do with the server. Most of the time that will be to just boot Linux/Windows off the local harddrives or image the box. Basically, I want a tiny copy of Linux to be the bootloader, instead of a specialized (and less flexible) bootloader package. The tiny copy of Linux could be on a USB key, or delivered by PXE. Loadlin used to do this for Windows, so I could create this environment in DOS... but networking can be tricky and I lose a lot of tools. Other options are virtualization. All the automation above is part of the host OS, and it checks in with the management server to figure out guests to launch and the like. How the cloud servers do it. Or I could do some magic on the PXE side: PXE boot the data gathering copy of Linux first, make a change to the PXE server and reboot it with the second time it PXE booting a remote kernel but a local OS drive. 

Google's antispam takes into account many more variables than just SPF records. You need to check with them to understand why your mail is being blocked. Start by reading their guidelines for bulk senders: $URL$ Then if you are still unsure contact their postmaster. Make sure your domain has feedback loops setup with all the major destination mail providers as well, so you'll be notified of issues. 

Could you run a "bookmarks" webpage somewhere that has a clickable link with the full Hostname/Port in a anchor tag ie: to make it easier for users? This only works if the browser or the OS shell has a handler for that URI protocol. (There are instructions online on how to create protocol handlers in Windows Shell) Mark 

I've never done it, but you can have two DHCP servers that both make offers to a client. It is up to the client to pick one of the offers I think the trick is that you need to add the "authoritative" command to the top of the DHCP configuration on the Clonezilla machine but not on the PFSense box. That way if Clonezilla offers the client a DHCP address the client will accept that one first, otherwise it will fail back to the PFSense offer. YMMV The article makes mention of this issue near the bottom: $URL$ 

You or support probably checked this already, but is the "Enable Ping from LAN to Management Interface" checkbox on the System->Administration page set? Not sure about not being able to reach the management page remotely. 

Generally the rule is highly limited access, generally only the IT facilities team and the Hardware Operations team. All people with unsupervised access should have background checks. Cameras and door logs should be used to audit employee access. System Administration teams should generally never need to touch the hardware (except where SysAd/Hardware/Operations are overlapping roles) These days everything is about "Lights Out" management. You should not need to enter a server room/datacenter for anything other than rack and stack and physical hardware maintenance (AC, servers etc.) It is not unreasonable these days for sensitive environments to require two people be present whenever work is being done. (Rack and stack should be done with a buddy anyway, and having someone double check your cabling work can reduce errors.) The volume and temperatures etc of datacenters are starting to reach the point where protective equipment and careful safety management need to be considered so moving to lights out management is a good long term plan anyway. 

We have a network of HP 2510G switches connected back to HP2912al for aggregation. We've noticed that long running connections like a MySQL DB dump start flooding out to all network ports once the mac-cache-timeout expires. Doing an "arping" against the destination IP stops the flooding (going back to port to port) until the cache timeout expires again. I can understand why this would happen for unidirectional UDP traffic, but I'm at a loss as to why it is happening for TCP. I would think the ACKs from the receiving machine would cause the Procurves to refresh the MAC address in their cache. Instead it seems like they only learn from ARPs. Any ideas? 

The way I understand it, 4 is for general authorization messages, and 10 is for "sensitive" messages. Generally it seems like logins/changes/sudo to the local machine end up facility 10, and remote stuff like IMAP/POP logins/out seem to end up in 4. YMMV depending on what OS the syslog stream ends up. $URL$ Mark 

You could also move the email addresses one at a time to the new site, leaving DNS pointed at the old site. Forward each mail address on the old site to @newserver.company.com as it gets moved. Once everyone is moved over, then move DNS to the new server. Give it another day or two for DNS to move, and you're done. 

I did this with qpsmtpd... it is a qmail SMTP frontend replacement written in Perl. It has a bunch of plugins you can experiment with, but the most helpful is a maildir delivery plugin that can send all of the mail into a directory, or subdirectories by recipient address. Just install qpsmptd and run as a daemon, no other mail application is required if you just want the mails to get dumped into a directory. $URL$ 

Nat would be the traditional solution when the customers have overlapping IPs. Pick an unused subnet on one side and NAT it to an unused subnet on the other side. This may cause problems with some protocols. Some information on ASA NAT here, but there are plenty of guides on the Internet. $URL$ Mark