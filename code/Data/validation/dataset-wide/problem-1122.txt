I was recently asked to take a look at at some SQL Servers for a friend and carry out a few config and security audits. There were a couple of servers belonging to the same application group which interested me because there were many inconsistencies and generally odd configuration options set. One thing which attracted my interest: Named instances configured on a dynamic default port (i.e tcp port 1433). SQL Browser was running but clients connected using ,. My thoughts on this were that this was totally wrong. I mean, why bother creating a named instance only to put it on a default port, and why make that port dynamic? I contacted the vendor who very assertively told me that their documentation and installation guide stated that instances should be named and configured on dynamic port 1433 ("As you know, this is the default port that connections expect SQL Server to be listening on and changing this would void your client's support contract with us...") I can't bring myself to fashion a reply at the moment. Part of me feels it's not worth the argument as I'm doing this for a favour really. To arrive at the point, my question is two-fold: When would it ever be useful to operate named instances on default ports, and why would it ever make sense for them to be dynamic? 

We are setting up a new SAAS application on Azure and while we have used Azure before, not at this level of multiple apps, etc. We plan to have both a web app and a cloud app in both a US and EU data center. They need to hit a common database because requests will go to the closest data center via traffic manager and a lot of the data works off of the customer table. And a customer (company) can have users in both the U.S. and E.U. Is there a way to set up Azure Sql Server so it has instances in both data centers, and Azure keeps them synchronized? If I understand sharding right, that is not what we need as someone hitting either data center could be requesting any of the data in the DB. thanks - dave 

In order to make sure that the trigger runs successfully, I tried it by entering an invalid value for screening_start_hh24 column: 

In order to display names of customers who have purchased all the DVD, this is the query that I have tried out: 

Then I have got the following assignment question: Create TR_validate_screening_time trigger which fires before inserting a row in the Screening table. The trigger should check if screening_start_hh24 is between plan_min_start_hh24 and plan_max_start_hh24. If not, the trigger should raise an application error with a meaningful message. [Note: screening_start_hh24 is in Screening table and plan_min_start_hh24, plan_min_start_hh24 are in Screening_Plan table] This is my trigger logic: 

This comes from this MSDN link found almost instantly after a quick web search. Bear in mind there are a few obvious and not-so-obvious functional reasons why you would not want to failover from Enterprise to Standard as some features are not supported in both. The idea of a clustered environment should be that you maintain integrity of the database environment upon failover, so why compromise that in any way, whether talking about support or features? 

At the moment I have around 125 production instances, each with a script-based maintenance plan running as an agent job. The tasks run are Index Reorg/Rebuild, Stats updates and Checkdb. Backups are looked after by Netbackup so they dont form part of the plans but for a couple of exceptions. I moved all the instances last year to script-based maintenance plans from plans created with the SSMS wizard (hate those) and they're efficient and effective so overall I'm pleased. I'm wondering whether it's feasible to take things a little further. I've recently been working on a powershell script that, when pointed at an instance, iterates through the databases on that instance and performs those three tasks on demand. My question is whether anyone can see any downside by doing this for all instances, I.e. Having a single powershell script on our DBA server that iterates through a list of instances on a windows schedule and executes the maintenance tasks. Any errors would be handled / written out to logs etc. The main benefit of this in my eyes that we won't be deploying mp jobs to new instances and configuring schedules. We will just be adding the name of any new instance to the instances the script must iterate through. I'd welcome your thoughts. 

Question 3: Write SQL query (equivalent to relational division of relational algebra) to list Item_Name and Quantity that are stored in all warehouses in Sydney All these 3 questions are assignment questions. However, I have tried and answered them and which is shown below: Answer for Question 1: 

But unfortunately, after executing this query it is displaying all the customer names. But I want to display a specific customer name who have purchased all the DVD. Can someone provide me the correct query ? 

I just want to confirm that if my SQL query logic for these 3 questions are correct or not. If it is correct, then I will able to proceed and convert them in relational algebra with relational division which I can do it by myself if my SQL query logic is correct. If it is not correct, then kindly correct the mistakes in my SQL query logic wherever applicable which would be very helpful for me. 

As Remus Rusanu says you do not need rights to run a trace, you need permissions. I don't know anything about your company, but as a DBA in a large public organisation I have much experience of users asking for rights because they want to run a trace to 'figure out what a query is doing...' When asked that question, I don't flatly say no, I explain why it isn't a good idea to run client side traces and to put SQL Profiler in the hands of the users. Sure one of the reasons is long traces can have a performance impact on production systems, which is of course a worry, but there's also the setting up of the profile and the interpreting of the output - you don't want any help with that? The fact that you may have never used Profiler before or understand its complexities and consequences would fill me with worry. I always engage in a bit of dialogue about why server side traces or extended events is potentially better. I ask why they're investigating what the query is doing - maybe I, or one of my team, can help without a trace. It works both ways though, I am wondering if you have fully explained what you want to do to your DBA's or IT Management Team. I think sometimes when people go guns-blazing asking for SysAdmin rights without effectively engaging in a bit of dialogue you end up with closed doors and brick walls (bureaucracy as you call it) rather than collaboration, co-operation and learning experiences. You may have done this of course, and your IT team may just be stubborn - but this is just my two cents. Plus, if I found out that third-party tools were being used in isolation without the authority to do so I would put that user in breach of our acceptable use policy and report it - so please be careful if you're going down that road. Doesn't matter what company you work for - one team, remember? 

Question 1: Write SQL query (equivalent to relational division of relational algebra) to retrieve the project name which is contributed by all employees Question 2: Write SQL query (equivalent to relational division of relational algebra) to retrieve the first name and last name of all employees who work on every project Then there are list of 6 tables 

The problem is that I am getting the error message displayed as ORA-01403: no data found. But I want the error message to be displayed as ORA-20001: Screening start time should be between planned minimum start hour and planned maximum start hour Where did I make the mistake in my trigger code ? It would be very helpful if the solution code is provided. 

You can not perform a downgrade in the same way SQL Server allows you to do an in-place upgrade, so you're stuck with having to manually move your database(s) between two installs. If you've got a named instance, you'll need to back it up, perform an uninstall of the Enterprise edition, and a fresh install of Standard - restoring the databases thereafter. Of course that's a basic theory, and it's not always that simple. Are you using any Enterprise specific features, such as partitioning, snapshots, some aspects of Always On Availability Groups, Resource / IO governance, compression...? You can see the full feature list by edition here here You can also run the following query to check for any version specific features you are currently using: 

Using Azure Sql Server, and in C# if that's relevant, is there a way to get an event when data in specific table changes? In one table we need an event if any column changes in a row. In a 2nd table we need an event only when either of 2 columns change (although any change would be ok). And we need to know which row. Is it possible to do this? If so, how? Update: We need a call into our C# code for this event. A database trigger where the database can do something doesn't help, our program needs to take action on a change. thanks - dave 

Firstly, forgive my poor question title. I couldn't think of the best way to summarise this. Essentially I'm putting together some scenario based DR documentation, with this specific scenario being a SAN issue where we have lost disks which hold either log or data files. I'm assuming that MASTER has been affected (ie. the drive where it's files exist isn't accessible) and SQL services won't therefore start. My specific questions are as follows: 

I've never tried this but I'm reliably informed it is 100% not possible. It is certainly not supported by Microsoft, for good reason, so why would you do it?