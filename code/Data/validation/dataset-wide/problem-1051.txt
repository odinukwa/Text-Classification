Note: Using AWR data requires you to have the Diagnostics Pack license option purchased. It may be possible to run the queries below without having the necessary license. Be sure to check with your DBA/licensing manager before using these. The Automatic Workload Repository (AWR) contains details of the activity on your database. These are stored in various views. The kind of information you're looking for is primarily available in the view. By joining this to you can get the information available in a given period: 

To help you fix this, if the query is static (doesn't change) and you've licensed the tuning pack, I'd advise looking at the SQL tuning advisor. With this you can create an SQL profile locking the query to a good plan. Hopefully the advisor will find the good plan for you automatically, but if not you may have to create it manually. Some links: Using the tuning advisor package: $URL$ Manually creating SQL profiles: $URL$ If you're not licensed for the tuning pack, then I think you're down to restructing your query, adding indexes and fiddling with hints. Finding out which changes will benefit will be a bit of trial-and-error; it's difficult to say exactly what you should do without access to the actual datasets. 

You can do something like this to dynamically generate a query against the tables with a nullable ID column to see whether they actually contain nulls or not: 

You need to choose which entity from you list above represents the grain. You could choose any of them, however looks the best candidate to me as this is at the lowest level (it is the only table with no children). If you make your grain, then it won't be possible to report on what the individual line items are. This may be what the business wants to track (and they don't care about line items). However, it'll be very difficult to change the grain to line items at some point in the future if they change their mind. Whereas you can make the grain line items and provide aggregations which show summaries at the claim level. For a more detailed discussion about this (again by Kimball) go here. 

There's a lot of restrictions on using long columns which make them tricky to work with. One way around this problem is to convert the data into an XML structure. Once you've done this you can use XPATH expressions on the XML compare the field values. The conversion to XML can be done using passing in your query as a string, like so: 

Is the number 0.01. However, if you create a function P01, the object reference will be picked up before number conversion. This allows you to execute functions on the database giving you increasing powers, as follows: Create a basic "get by id" function: 

I think what you're missing is a table. This will have an FK to the table and have and as children, like so: 

Before doing this you should verify that there aren't any objects still allocated to the tablespace for other users however. You can find this by running: 

I would go for option 1 possibly introducing option 3 if you have a large number of products with very different attributes. This will give you a "master" product table, holding attributes common across all (or most) products (e.g. name, price, etc.) and separate detail tables with more specific entries for each product type. From your question, it sounds like you have (at least) two main classes of product, "hardware" (padlocks, chains, etc.) and clothing (hats). So you would have tables something like this: 

But there's no FK defined :( So the optimizer doesn't know this. And it needs to access both tables when executing the query: 

In these cases the order may be entered into the system at some time after the transaction took place. In these circumstances it can be difficult to impossible to correctly identify which historical price record should be used - storing the unit price directly on the order is the only feasible option. Multiple channels often bring another challenge - different prices for the same product. Surcharges for phone orders are common - and some customers may negotiate themselves a discount. You may be able to represent all possible prices for all channels in your product schema, but incorporating this into your order tables can become (very) complex. Anywhere that negotiation is allowed it becomes very difficult to link price history to the order price agreed (unlesss agents have very narrow negotiation limits). You need to store the price on the order itself. Even if you only support web transactions and having a relatively simple pricing structure, there's still an interesting problem to overcome - how should price increases be handled for in flight transactions? Does the business insist that the customer must pay increases or do they honour the original price (when the product was added to the basket)? If it's the latter it the technical implementation is complicated - you need to find a way to ensure you're maintaining the price version in the session correctly. Finally, many businesses are starting to use highly dynamic pricing. There may not be one fixed price for a given product - it is always calculated at runtime based on factors such as time of day, demand for the product and so on. In these cases the price may not be stored against the product in the first place! 

The key to determining whether your table is in 3NF is the transitive dependencies. To figure out if you have any of these, the best question to ask is: If I change the value in column x, does that imply I also have to change the value in column y? for all non-prime columns. If the answer is yes, you're not in 3NF. In this case, you have to answer yes for the and columns. If line three was dog food instead of gas, that also implies you have to change to someone other than British Gas. To get this into 3NF, you need a separate table, which lists the name of the product and the vendor. You would then have a foreign key from to the primary key of this new table. 

Joins across database links can lead to sub-optimal execution plans as Oracle doesn't have all the information available about both sites. Queries across db links can (and do) perform just fine when joined to local tables though. If writing a new query with local and remote tables I'd start joining it all together (set-based) then only break it into separate queries if performance is unacceptable and other tuning hasn't worked. Given your query is just a simple minus with no joins to the remote site, I would expect the refactored set-based approach to be quicker. Make sure you test it though! 

Do you (or will you) have additional attributes of the city (e.g. population, county/state, display name etc.) Are you executing a query to generate a list of distinct cities to pick from (your comment suggests yes) 

I believe the important difference is to inform developers which of the available unique keys should be used when applying foreign keys to a table. The important features of a primary key are: 

You can then parse the date/timestamp into an actual date. Having done this you can use it for comparison against an actual date to return you a list of the partitions affected: 

If you find you want more (or less) data than 24 hours you can then just update your table as appropriate. It's worth looking into partitioning as well though, as Justin's suggested. 

While the names for these attributes may be the same, it's very likely that you'll want to store different values depending upon whether a person is a customer or an employee. For example, if I work for a company but also use their website to purchase their products (so I'm an employee and a customer), when purchasing products I'll want to enter my personal phone number, email address etc. Whereas my employee record will need to store my work issued email address, phone and so on. If you create a single table to store all the common fields, if I want to change my contact number for purchases I've made, then my employee record will also be updated. If HR/IT use this information to know which phone I should return when leaving the company it could cause issues down the line if I'm free to change it in the customer context. Therefore I'd recommend creating separate tables. The exception would be if you were in the unusual situation where your employees are your customers (which may be the case for an internal purchases application). 

So it's unlikely to be worth indexing on its own. Queries on lots_vals return few rows (just 1 in this case). So this is definitely worth indexing. But what about the queries against both columns? Should you index: 

As the where clause is evaluated before the columns are selected, the value passed to the function isn't set until after the context is read. The location of the sys_context call in your query (select, where, group by, etc.) will affect exactly when this value is set. 

A downside is that inserting data is slower, so you need to weigh up the costs and benefits. Ultimately, it comes down to knowing your data and understanding how it's to be used which should guide the decision. 

As you're on 11gR2 and suitably licensed (make sure you check!), I would recommend taking a look at the SQL monitor. This gives a detailed breakdown of all SQL statements that took more ~5s to execute in the past 24 hours. Using this, you can see the duration of each step in the execution plan, along with I/O and wait details. This will enable you to see why it's taking so long. The quick SQL executions won't appear by default, but you can add the hint to force them in. You can get a graphical view of the plan using Enterprise Manager or SQL developer. You can also get a text version using SQL. Have a read of this oracle-base article for more details on using the SQL monitor. It's possible that the reason for the differences is because the SQL is flipping between two execution plans. You can spot this by looking in the AWR to see this. To do this you'll need to find the sql_id for the statement(s) that are causing you issues. You can do this with: 

The issue is you're restricting on the instance field, which is resulting in the roles only appearing in one column or the other. To get around this, remove the restrictions on and the roles from the second database to the roles in the first, like so: 

However, if you're using to create a generic "open a query" function like the above you're probably doing something wrong! 

Datafiles are allocated to tablespaces, not users. Dropping the users will remove the tables etc. from the tablespace, but the underlying storage is not affected. Don't delete the dbf files from the filesystem directly, you'll get your database into a mess. To remove them, find which tablespace the files belong to using the following statement: 

A cursor is a pointer to a result set for a query. By returning a you allow the client to fetch as many or few of the rows from the query as it requires. In stateful applications this could be used to page through results. A cursor can allow more flexibility than writing a PL/SQL function that returns an array as it's completely up to the client how many rows to fetch and when to stop. That said, I've not found many cases where this additional flexibility is useful. It's worth noting that the is weakly typed, so you can return pointers to queries which not only have different from or where clauses, but different numbers and types of columns as well. Alternatively you can use strongly typed cursor where the columns in the result set are fixed. This enables you to write functions which return different queries, like so: 

Also create a function P01 which does something undesirable (in this case just creating a table, but you get the idea): 

You'll then need to extend your model/add further checks to ensure that resources are inserted into the correct allocation table depending upon their type. UPDATE To show the unassigned resources: 

Yes. The optimizer can remove redundant tables from a query when RI is enforced in the database. For example, here are two tables: 

If this returns anything, move the objects to another tablespace if you want to keep them before dropping the tablespace. 

Note that only "highest consuming" statements are retained in AWR. So if the query you're interested in isn't the slowest, most executed, most disk access, etc. it may not be in the AWR data. You can get around this by "coloring" sql_ids of interest. This ensures they stay in AWR for as long as your retention period is set. You can do this by running: 

Notice that the clustering factor for few_lots is 10x higher than for lots and lots_few! And this is in a demo table with perfect clustering to begin with. In real world databases the effect is likely to be worse. So what's so bad about that? The clustering factor is one of the key drivers determining how "attractive" an index is. The higher it is, the less likely the optimizer is to choose it. Particularly if lots_vals aren't actually unique, but still normally have few rows per value. If you're unlucky this could be enough to make the optimizer think a full scan is cheaper... OK, so composite indexes with few_vals and lots_vals only have edge case benefits. What about queries filtering few_vals and many_vals? Single columns indexes only give small benefits. But combined they return few values. So a composite index is a good idea. But which way round? If you place few first, compressing the leading column will make that smaller 

You also have standard functions to extract the various components of the time (hour, minute, etc.): 

Shared nothing typically refers to hardware, though it could also be used when describing an SOA architecture. From a hardware perspective, a "shared nothing" Oracle database simply means a machine using local disks, memory, etc. Oracle RAC (real application clusters) is a shared disk architecture, but not shared memory. Horizontal scaling is then achieved by adding more machines to the cluster. When talking in SOA terms, "shared nothing" means that each service has a corresponding database which is only accessed by that service. So the ACCOUNTS service accesses the ACCOUNTS_DB, ORDERS service the ORDERS_DB and so on. These databases could be shared nothing from a hardware perspective as well, or use RAC. Ensuring consistency of data and references which would normally be handled using foreign keys becomes a challenge in SOA shared nothing databases. Sharding typically refers to partitioning managed at the application level, rather than within the database. For example, you could partition accounts by email address and direct customers with address starting A-C to ACCOUNTS_DB01, D-F ACCOUNT_DB02 and so on. The shard mapping could be a simple range like this, a function on the input or a lookup database stating which database is stored in. The databases would be "hardware shared nothing" in this case as the idea is you use relatively cheap machines which are easily added and replaced. You could shard your databases at the application level and still have Oracle partitioning at the table level within the database itself. So you could shard your ORDERS database by customer, then partition the orders table by order date as well inside the database. The downside to both meanings of shared nothing comes if you frequently run queries that have to access several databases. In these cases your joins will be pushed into the application layer rather than the DB layer so are likely to be slower. Good governance is necessary to ensure this doesn't happen.