Build your using a variable that includes maybe the package runtime. Convert that timestamp to just something like where you full backup file name comes out to something like: You could then just add a process task in your package to execute a PowerShell command to delete files older than 3 days. Your process could be: 

If you are using sequential processing you will never know when one particular server is going to have a load from the index maintenance or stats maintenance. As well, it could also vary each time it runs based on other processes running on a given instance. Something to consider is if your maintenance window is large enough to handle that fluctuation. 

Something you might do that is just a thought, throwing ideas out... Create a single job that periodically checks the job table in msdb to see if any jobs show as failed, that can be done with a good T-SQL query. Then you could go into the sysjobsteps table and see if an output log is set for the job. Have a stored procedure send an email attaching that file to it. You would be able to see exactly what the job did from beginning to failure without having to touch the server. Then could also have PowerShell script check the event log for errors. It allows you to filter down a pretty good bit to get exactly what message types you are looking for. You could set that up as a SQL Agent job to run periodically. Then in the PowerShell script use the email cmdlet to send the message if it finds one. Far fetched ideas here, just some I thought about. 

From this you just need to build out your statement to execute via . You could use a simple cursor or while loop per your preference, below is a template for just a basic cursor that I keep handy: 

I would highly suggest you not assign a login to db_owner just "to have access to" a database unless you want them to have "admin" privileges in that database. You also have to consider what permissions the login has at the instance level first. Assigning a user to will have no affect if the login is sysadmin or securityadmin privileges. If we are talking about a "regular" login on the instance you prevent them from accessing the database by not mapping that login to a user in the database. If no database user exist in the database for the login then they cannot connect to it, as long as they do not have higher privileges at the instance level. 

Create a Scheduled Task on that server that contains a PowerShell script. The first part of that script contains a that would iterate over each instance and call the backup job on each instance. Use a would allow all the jobs to start at one time. You would then need to script out a while loop to wait and see that all jobs have completed. Then the end of that PowerShell script just call the code you need to run. Pick an instance and use it as a "central" location to actually run the backups against each instance. You could utilize linked servers to remotely call Ola' scripts. This would offer a bit more control and ease of management because you would know once the command finished the backup for that instance are complete. Your last step would just be your PowerShell script. Leave each backup job on the given instance. Add a scheduled task that calls your PowerShell script, but at the start of that script check to make sure a recent backup has been done. If not, just stop the script. Schedule that task to run between an the average end time of the backup jobs until a given a few hours later. This would ensure at some point it will find that the backups on all instances are current, then execute your code. 

In this example I am going to pull that query in from a file and then search for the value, replacing it with the database name I want to query for... 

"Reseting" the transaction log is basically what happens when SQL Server backups a database either during full or log. Read here to get more detail: $URL$ The backup basically takes all that used space in the transaction log and clears any inactive log space that it can, so SQL will be able to reuse it instead of growing the log file out. The more frequent you do log backups, the more it clears that inactive space. Most people will refer to this as log management, with regards to SQL Server. 

Edit for the nerds If you open up Activity Monitor, and then go into tempdb you will find all the temp objects (tables and all) that Activity Monitor is using. For the Expensive Queries it is using the object . You can open that procedure up and look at the exact query they use to pull it. Pretty cool. I did this for the activity section so I could get similar information without having to open up Activity Monitor all the time. 

It is much easier to modify maintenance plans directly than try to add additional flags within the SQL Agent job step. As well the flag you are referring to is regarding the space within the database itself, not just the physical log file. It is intended to shrink or release the unused space to the specified percentage. Not something you should be doing if you don't have to. Log truncation occurs upon completion of a full or log backup, or whenever SQL Server issues a checkpoint. You can read more on that process specific to SQL Server 2000 here. If you are trying to keep the log file under size due to disk space issues you need to increase the frequency of log backups that are occurring. If that is not something that can be done you either need to look at adding additional drive space, or find out specifically what is causing the log file to grow out of control. Shrinking the log file does not cause any breaks in the backup chain. It is however not something that is advised to automate. If the log file is growing out of control so much that it requires you to constantly shrink it down then all you are doing is covering up the bigger problem. You need to determine what is causing the log growth and take corrective action from there. 

I was going to just comment the link to the SQL Server 2012 Best Practice security white paper...but found out Microsoft took it off the Internet for some reason. You might be able to find a cached version of it some where online, but I will paste in the contents of contained database section of the document here. There is also a BOL article for Security Best Practices with Contained Databases, this may be why the white paper was taken off but not sure. Author of this whitepaper: Bob Beauchemin, SQLSkills; Published January 2012; Applies to SQL Server 2012 

I am not sure how dirty you want to get on this one but the only method is going to be opening up SQL Server Data Tools and building (or modifying) an SSIS package. One would be to import the maintenance plan into SQL Server 2012 Data Tools and modify the SSIS package. The other would be to create it from scratch. You have the same options as the maintenance plans when you create an SSIS package. The one thing you want to add is an Expression Task, which is an awesome addition to SQL Server 2012. You could use the DATEPART SSIS expression to determine whether it was a weekday or weekend based on the system variable for package start time. Now I am not the best with expressions but this would be the one I would use: 

The import/export wizard is not showing because it is not available for Azure SQL. Even if you downloaded the latest version of SSMS, you shouldn't see it. You are in a different environment that how on-premise SQL Server databases work. In that Azure SQL does not support using cross database queries. So doing an are not going to work. Depending on how big the table is, your easiest thing to do is using the script object wizard and just include the data. Then just run that under the destination database. If you need to do this on a routine basis then look at the data sync services Azure offers. 

To test with your third party application you will have to check with that vendor. I can tell you with something like Idera's SQL Safe you can likely create a backup policy for a temporary database, run the job once, set the database offline, and then try to run the job again. Idera's product has to see the database initially in order to setup the policy correctly. I executed the above T-SQL on my local instance that I have the alert configured on three times and you can see the results below for the alert: 

You can first check to make sure it actually is a valid MDF file using DBCC CHECKPRIMARYFILE. It is an undocumented command but can be useful at times. If that turns up nothing or shows that it is not a mdf file you can use the RESTORE HEADERONLY to see if it might be a backup file. To your specific question, yes you can attach this as a different database name as long as the physical file is not in the same directory on the server as the original. So in your case I would just create a sub-directory in your data folder for this file and the future LDF file that would be created. 

Without actually backing up the tail log of a database (don't have an test instance to try this on) you could logically conclude that the value returned in the column mentioned would be the first LSN of the next log backup, in your case the tail. So executing the following will return the value I believe you are looking for: 

The provider is the issue in most cases as it does not know how to interpret some characters/commands. If the PowerShell step type put you within the context of the PowerShell.exe host, our life would be so much easier and awesome things could happen. 

I just happen to have set this up for a database in production and was wondering the same thing cause I needed to compare the configuration to another mirroring session to make sure I matched it correctly. should provide you with you want. It contains the which is: 

Not knowing what all the steps of your job do, we cannot provide you a step-by-step instruction to deploy it. You can script out configured SQL Server agent jobs, which may be a starting point. I would suggest installing another instance on your local machine, unless you happen to have access to a test server. Then take note of everything you do to get your job up and running on that instance. If you can repeat it over and over again, then you have your deployment document completed. 

Microsoft provided, for free, SQL Server Management Studio (SSMS) starting with SQL Server 2005. Is it not exactly like SqlYog but there are similarities. With regards to table space I would direct you to the Standard Reports that were added to SSMS after SQL 2005 SP2. They provided a more eye friendly look at allocation of space, index usage, performance information, etc. 

This KB is the closest thing I can find regarding your error, that would come from SQL Server. I would see if you could find any other errors that may give you more explanation as to the issue. Since this seems to be dealing with XML data I cannot help you that much, have not worked with XML much. Other folks on this forum might be able to chime in though... 

Once you have the execution plan open, just right-click in an empty area and select . Once you have that you will see a section in the XML . In that node it will show you the : 

Now you would need to create a custom message using (BOL), so you can use it with the . Then your SQL Agent Alert would just check for that message text. When the alert is fired (meaning it caught the message) you configure it to notify you by email. 

To give information on the performance counter side... The only performance counters I know of that might help would be the ODBC counters for disconnects. SQL Server has a counter for killed connections but those are connections killed by SQL Server, not the client. You would have to capture these at the client which you could use command to do this remotely. SimpleTalk has a good article about using this command. The comment suggestion would be the more human way of doing things. 

I would first question why they need direct access to the database. You might ask your manager or legal department if the security policy for the company allows granting this type of access. Is it really needed if they are just going to execute a stored procedure on a regular basis. As you stated this is going to be on a regular basis I would setup an SSIS package to export the information to a file. Then either have the package email the file (if not to large) or put it on a UNC directory for them to come and download themselves. If they want to determine when the data is pulled down setup a script for them to execute outside of SQL Server that simply runs the package. 

Behavior 1 I can expect because there are some features that may or may not work as expected when using higher version of SSMS on lower version of instance. There could be things in the background that changed just slightly from 2014 to 2012 that you will probably never find any documentation on. I would actually create a connect item on this if you can recreate this scenario on another machine every time. Now, Behavior 2 failed the first time I tried, but then succeeded every time after that, even after opening/closing windows. However if you are trying this using just it is to be expected that some things will not function the exact same way when you are dealing with the provider. If you tried doing the same command within I would expect it will work every time, it does for me at least. Now I am not using SQL Server 2014, only 2012 is loaded on my machine right now. You can see the fact of how different and are by comparing the assemblies loaded in each one: