Again, these tables assume the simplification from Issue 1. Even without that simplification, these tables could be amended to use an analogous approach. 

I think if you look at your model and you consider the requirements that it doesn't handle (i.e. your questions) then you will find that you need to expand your model somewhat. Consider the following ERD: (Note I use the James Martin crows foot notation which is a little more compact than what you have used but should be pretty simple to understand. The only slightly distinct feature is the use of an upper case "I" to indicate that a relationship is part of the identifier of the entity) 

I take it that you are OK with these assumptions, since you have noted an index on in your question. Then your user's global ranking for their scores would be something like this: 

I've had to reverse engineer several existing complex data sets. The most important thing to establish are the keys and dependencies in the data. The problem is frankly NP hard, so some intuition and inspection will go a long way to getting you to a sensible answer, so don't count on a simple "turn the crank" solution unless you've got a lot of time on your hands. What you need to do is to query the data a column at a time and by combinations of columns. You want frequency distributions for column values (and column combination values). Columns (or combos) with maximum frequencies of 1 are candidate keys. You can also look at frequency distributions of combinations of columns to find potential hierarchies. In your example each value in only ever has one value in and so forth. When you identify candidate keys and dependencies between columns you can apply normalization. 

MySQL has geospatial extensions. You should examine these and see if they solve your problem more directly by allowing you to find services that are "near" (whatever that means in your context) the searcher. 

First off, your table doesn't appear to be in third normal form (3NF) because you have attributes that depend on only part of one of your unique keys. You should really have a minimum of three tables: 

2NF: Remove Partial Dependencies includes partial dependencies. D and E depend only on A, F depends only on B, G, H, I and J don't depend on the key (directly) at all. 

As Nick Chammas pointed out in his comment, your test tables should be combined if they have the same fields and segregated if they have very different fields. It is difficult to imagine 100,000 different test tables. If this is not complete hyperbole then you might need to consider other options. One option would be a document-oriented database. This would be efficient for storage of loosely structured data and easy retrieval of data for a particular patient. It would be pretty useless for rapid retrieval of data for similar tests across multiple patients. Another option would be Entity-Attribute-Value (EAV). EAV is considered an anti-pattern by most people, but yours may be a situation where it could be useful. EAV would be less efficient at retrieving the results of a particular test for a particular patient, but it would certainly be better at getting all results for a particular patient than joining thousands of different test tables. It would also be better than other approaches for getting the same test result values across multiple patients. 

You have to decide whether you want your world to be simple for the programmer or useful for the user. The most useful scenario is the one which treats things that happen in the real world most directly. In your case that is your second scenario: One table for check-ins and check-outs with different rows for each. The reason that this scenario is the most like the real-world is that it is modeling inventory events. Each row in your table records the physical processing of a piece of equipment by someone. You are either taking it out or puting it back. There are no null columns to have to work around. What is problematic with this approach is that outs and ins don't necessarily come in nice, tidy pairs. However, this is very likely going to be a fact in your actual operations. In the real world, paperwork sometimes goes missing or people get sloppy with procedures. If you build your system along the lines of your option #3, then at some point or another someone is going to ask for a business rule that says "don't check out things that aren't checked in". While this makes good sense in a textbook sample problem, it will just be a nuisance in a real-world application. This approach is also the most useful when it comes time to expand the requirements of your system. For example, let's say phase 2 of your system is to implement a physical inventory process. In your option #1 you need a whole new table. In your option #3 you need either another table or a new column. In option #2 you just need a new value for your "inventory movement type" column. Modeling close to the real world generally makes your database schema more future proof. Track all of your events as individual records and then build your application logic to interpret the event stream. 

You have a list of quesitons in the table. Each question can have two or more possible options for answers in the table. For each option there are one or more category score changes which are noted in . When a takes the quiz their answers (i.e. choices of options) are recorded in . Note that with this design you can change someone's score in multiple categories with a single answer (such as in your example of reading WIRED magazine) and you also have the option of changing someone's score in a category by an increment other than 1 (as in your example of not having taken an art class). You could even change someones score by more than 1, for example in your question 1 you could score +10 in the science category for prefering to read Philosophical Transactions of the Royal Society! 

This allows you to have any number of work periods per staff member and client and gives you the details of who worked for who and how long (not to mention when - which is also very important!) 

If you supply the missing superset entity, then every employee is one of these, as is every customer, and also therefore every user. The hard part of this is coming up with a reasonable name for this superset, but it solves the issue of having to figure out which foreign key to follow from user. It also makes a connection between employee and customer should one happen to exist, and should that connection be of interest. 

When someone makes a simple payment you record one and two records. The first record shows who paid and the second shows who was paid. The sum of the amounts for the two details must be zero, that means you need to pick a convention for positive and negative Amounts such that one means giving money and the other means getting money. If the transaction is more complicated, you just add more records, for example instead of one person getting paid, you get two people being paid (administrator and seller for example). While the sum of transaction detail amounts for any transaction is zero, the sum of amounts for any given may be non-zero, meaning you have some of their money in your system. To clear these amounts, you need a representing any payment settlement organization or bank that you deal with. These are the ultimate sources and sinks of funds and their balances don't need to be zero in the long run. All of this may seem complicated, especially compared to what you were thinking of, but the advantage of it is that it allows you to keep a detailed record of exactly what happened and when it happened. If you need to make an update, i.e. cancel and order or issue a credit note, just add more transactions and transaction details. You can always go back and see exactly what went on. This convention is how people have been handling and accounting for financial transactions for hundreds of years, so if nothing else, people will understand what you're doing. Here is an example with some data. In this example, Buyer X pays 100.00 for an order. The Agency collects 10.00 and pays Seller 1 40.00 and Seller 2 50.00. They buyer pays with PayPal and the Agency and both Sellers deposit their money in one of two banks. Note that this example is quite simple insofar as the agency and the sellers clear out their accounts after just one order. In a real-world scenario, payments for many orders might be batched up before settlement is made. 

From this master list of phone types, you can create distinct views for each legal entity phone type list (i.e. , , ). One of these views might look like this: 

If this is the case, your concern is that recording the "default" (duplicate) comment in the second table is wasteful or even dangerous since the data could get out of whack. In this situation, you can use the SQL function along with a to solve your problem. Using the outer join lets you use a single SQL statement that pulls together the two tables (when there are records in each) or just pull data from the first (mandatory) table if the second (optional) record is missing. No complicated branching, just a single SQL select. The coalesce allows you to pick the first non-NULL value in a list of values. This is useful because it lets you take the first comment as a default if the second comment is NULL. The second comment can be NULL because either (a) there is no matching record in the second table or (b) the comment within the second table is NULL. It seems like in your case there will always be an entry. Sometimes this entry is created by a user and sometimes it is the result of an unsatisfactory . In this case you want to select: 

Relational DBMS is not ideally suited to this type of application. RDBMS is great for tracking and enforcing referential integrity for relations between two entities, THING A and THING B. What you want to do is to track relationships between THING A and every other thing in your database (or lots of other things, at least). Your options, each of which include some form of compromise, include: 

By all means use an unsigned int surrogate key as your primary, clustered index. However, instead of using sequential values, build some padding into the sequence. This means that you'll have to assign the id manually instead of using . If you use unsigned int in MySQL, the max value is 4,294,967,295. If you expect to have at most 100,000 rows that means you could space each word out by more than 42,000. When you need to insert a word between two existing words, just plug it into the space half way between. Let's say you use 40,000 as your intitial padding value. If you have "house" at 800,000 and "dog" at 840,000 you can insert "nouveau" at 820,000. 

Computers don't get tired. Nobody has so many employees that the summation over a well-indexed set of rows of vacation actually taken would grind their servers to a halt. What is a problem is precalculated data getting out of whack with respect to your raw transactional data. If you're tracking vacation you're going to need to keep a list of days taken no matter what, for audit purposes, if nothing else. What happens if an employee challenges your calculation of their vacation taken? The only way you'll be able to justify your number is to go back and show the details. Since you'll already have the transactional details, there's not much point in keeping a precalculated sum of these details. From a code complexity/maintenance standpoint you're not really saving anything. You write the code that calculates the remaining vacation entitlement once. It's a single select statement that has three or four more keywords in it than the single select statement that would read a precalculated row. That's not much of a savings. What you're thinking about is often called pre-optimization and it's something to be avoided, because it actually makes things worse more often than it makes things better. 

One of the advantages of this approach is that you don't need to maintain foreign keys between your event data and your reporting period data. That's really convenient if you decide to change your reporting periods. 

EDIT: Entity Relationship Diagram This would be a logical data model that could support your business rules: 

Do not store dates in a separate table unless there is something about a particular date that has additional information that you need to store. In general the date is a simple attribute of some other entity, it isn't an entity in itself unless you have a very peculiar application. One exception to this might be a star schema data warehouse where there is a time dimension table that might use date as the primary key and have attributes that describe the way that date might roll up - assuming that some roll ups might not be trivial to calculate (like fiscal reporting period or some such). You can index a date column just as easily as an integer column, so there is nothing to be gained by abstracting out a date so that you can give it an integer alias value. You can also put a date column in a WHERE clause so searching for records by date is not at all onerous to do. 

A logical schema won't exist in your database. A logical schema is a design-centric database structure built to meet your business requirements. It is a model that exists on a white board or in a diagraming tool. It is like the architect's drawings of your database. A physical model is what is actually implemented in your DBMS. The two can be different for a variety of reasons and in several ways: 

Grade level represents the year/form/grade at which the subject is being presented. The material actually being taught is the subject. When certain material is taught to students at a given grade level, that is a course. When a particular instructor teaches a course to a given group of students in a certain place at a designated time (or set of times) that is a class. Whether you choose to use natural or surrogate keys is a separate discussion. The fundamental structure of your data model will probably end up looking like this ERD.