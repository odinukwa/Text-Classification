This might be fine for one change per module, but imagine how your code will look like after severol years of development. Before version control was universally adopted, there used to be modules that were 95% history log and only 5% actual code. Fortunately version control automated out that inconvenience long ago. I would completely get rid of change logs in comments - we are much more productive using modern tools. Besides, git and other version control tools allow you do do much more than see the log of changes if one file. For instance, they can will show you all other files changed in the same commit. Also they give you accurate information, while we cannot fully trust comments. 

Snapshot isolation is robust. It does work. I have been using it in my system for several years, and it does reduce the amount of deadlocks. However, snapshot isolation adds more workload to your tempdb, so you need a skilled DBA who can ensure that your system withstands the load. Before you start using snashot isolation in production, make sure you have a DBA who understands how it works. 

There is a very good blog post by Hugo Kornelis that explains it in good detail: Can you trust your constraints 

We've been doing this for almost five years, and we think that explicitly testing modifications is definitely doable, but it is quite slow. Besides, we cannot easily run such tests concurrently from several connections, unless we use separate databases. Instead, we should test modfications implicitly - we use them to build up at least some of the test data, and verify that our selects return expected results. I've written an article entitled Close Those Loopholes: Lessons learned from Unit Testing T-SQL, as well as some blog posts Regarding your question "Is there a treshold in complexity where it gets completely hopeless?", complex modules need tests much more than simple ones. To simplify maintenance, we generate expected results, and we store them in separate files - that makes a huge difference. 

This function is very easy to invoke from my Ado.Net client. All I need to do is pass the parameters. However, if I want to test this function from pgAdmin, the result set is open on screen only if I keep my transaction open. This is inconvenient. Of course, it is easy to expose the data as an HTML table or an Excel spreadsheet, but this is kind of a minor inconvenience. 2. Using setof records 

I would not bother with persisted columns at all. I would just define a computed column and have it recalculated every time we select, so that I save considerable storage and have my selects do less reads, at the expense of the negligible amount of CPU needed to calculate volume. Of course, I did some benchmarks in the past. 

To view the trace file contents in SMSS, I am using the following function: FN_TRACE_GETTABLE. To run FN_TRACE_GETTABLE, the users don't need any permissions on the server's file system. Only the account that is running the service must have the permissions to access the trace file, but it already has them, because it has created the trace file in the first place. I have an automated job that uploads a trace file into a table every weekend, and neither me nor the account running the job have any permissions on the server's file system. This is running 2008 R2. When I was developing that job, I had run FN_TRACE_GETTABLE from SSMS quite a few times. 

SQL Server optimizer is closed source, so we cannot see what exactly is going on under the hood. Of course, there are tons of documentation, and just as surely there are lots of cases when this thing does not behave exactly as documented. In my experience, all documentation from all vendors tends to get out of sync quite soon, sometimes before the first release ;). The only 100% accurate source of information is source code, pun intended. So, even if you observe some behavior right now, there is no guarantee it cannot change later on. The reason: we have no idea what is involved, how the optimizer is making its decisions. So, we do not know if we have tested all the situations. As such, I would keep it simple: 

According to MSDN, Getdate(), GetUtcDate(), and CURRENT_TIMESTAMP all return DATETIME. I ran a short test, which confirms that: 

This is an answer to Kirk's question 'why not use it (HierarchyId)'. As compared to materialized path, in some important cases HierarchyId seems to be both less performant and less convenient to work with. The reason is simple: quoting from Microsoft comment on Connect, "The problem is that CLR calls, including hierarchyID's methods, are opaque to the query optimizer. This is by design. However, it means that the cardinality estimate for them can sometimes be quite wrong." On the other hand, implementing materialized path is very easy the first time we need to do it, and next time it is essentially a copy-and-paste task. So, we get a more versatile and better performing solution with very little effort. So I completely agree with Paul Nielsen, who wrote in his excellent book entitled "Microsoft® SQL Server® 2008 Bible" as follows: "The new HierarchyID is not without controversy. It’s new and gets plenty of press and demo time, but I’m not sure it’s a problem that needed another solution." 

I've been doing Agile development for more than four years, including my SQL Server projects, and I really like it. I think it is important to understand why and when Agile is useful, from the perspective of the whole system, and general purpose Agile-related books such as the following fit the bill: " Agile Principles, Patterns, and Practices in C#", " Patterns of Enterprise Application Architecture". The following two books are focused on database development, as you asked, but I would read the general purpose ones first: "Agile database techniques", "Refactoring databases" 

As such, at the time of the original release of SQL 2008, I decided against using MERGE. I am using MERGE a lot now, on 2008 R2, and I think it is a really great feature. Edit: here is the list of defects in SQL 2012 that were recently fixed. Hope it helps. Another edit: I have chosen MERGE for more detailed analysis, because it is a very important improvement. In fact, it is a major step in catching up with Oracle, and it does improve our productivity. As such, MERGE has been marketed a lot at the time of SQL 2008 release. Yet it was not completely ready to use in serious production systems when it was originally released, and there was no easy way to know it from the presentations/articles/blog posts and such. Similarly, snapshot isolation is an awesome new feature which just works, but invoking scalar UDFs in CHECK constraints does not work in all cases and as such should not be used in production when we need data integrity. However, both new features were recommended in "What is new in SQL xxxx" presentations, as well as in books, articles etc, and with similar enthusiasm. We need to be very careful with new features - not all of them are going to be useful/reliable/performant. 

I need to let my users specify the list of columns they want to select. So far I know two ways of accomplishing that. 1. Using refcursors 

This is not a complete answer, I do not have the time now, so let me just share a few thoughts. The complete answer would be huge, and I am not sure you want to know the details. I have been working with various temporal queries for several years already, and learned a lot in the process. As such, I would rather not have to optimize your query in my production system. I would try very hard to avoid solving it with T-SQL. It is a complex problem. Itzik Ben-Gan has written about "gaps and islands" several times, including a chapter in his latest book on OLAP functions. Your problem is a variation of gaps and islands. First, I would consider reading all the data to the client and solve it there using loops. I know, it requires sending data over the network, but fast loops in Java/C++/C# work very well for me most of the time. For instance, once I was struggling with a query involving time series and temporal data. When I moved most of the logic to the client, the C# solution was several times shorter and it ran 20,000 times faster. That's not a typo - twenty thousand times faster. There is another problem with solving such problems in T-SQL - your performance may be unstable. If a query is complex, all of a sudden the optimizer can choose another plan and it will run many times slower, and we have to optimize it again. Alternatively, I would consider storing data differently. Right now I see two possible approaches. First, instead of storing intervals we could use this table: 

In T-SQL, you cannot modify any data in a function. There is no straightforward way around it. There are some obscure hacks, but I would not use them. Use a stored procedure. The hack, quoting from Erland Sommarskog: 

Anyway, it looks kind of hacky to me. Is there a better way to retrieve, for example, numeric[] as a type if my parameter is an array of numeric? 

This is why whenever we add a new index, we need to have some kind of baseline testing to verify that none of these issues happen. 

I have been doing unit testing T-SQL for more than four years so far. I think it much easier to use C#, which is more versatile. For example, in C# I have no problem unit testing a stored procedure that returns two or more result sets - it is plain impossible if you use T-SQL. I described the process in this article and in this blog post. 

Can a T-SQL solution for gaps and islands run faster than a C# solution running on the client? To be specific, let us provide some test data: 

I had a similar problem in the past. I needed to send up to 100K numbers to SQL Server 2005 as fast as possible. After lots of benchmarks I think the fastest approach in my environment was to pack numbers in binary format. That needed less network packets and parsed very fast using very little CPU. Erland Sommarskog included the C# code I used to pack numbers into a CLOB in his article Arrays and Lists in SQL Server 2005 , in the section entitled "Passing Numbers as Binary". 

That would guarantee that "if the LocationID is specified, then that LocationID should be a Location that belongs to the specified EventID". That done, your approach makes sense. 

Shutting the system down and doing all changes at once may be very risky. If something goes wrong, and frequently it does, there is no easy way back. As an Agile developer, I sometimes need to refactor tables without any downtime at all, as those tables are being modified and read from. The following approach has low risk, because the change is done in several low-risk steps that are very easy to roll back: 

Either way, only you can determine the impact of your solution on the performance. Typically we do not have deadlocks in our system at all, although we do have a lot of potential for having them. In 2011 we made a mistake in one deployment and had half a dozen of deadlocks occur in a few hours, all following the same scenario. I fixed that soon and that was all the deadlocks for the year. We are mostly using approach 1 in our system. It works really well for us. 

The reason: I believe that the result of @DateTime-7 is not documented. Even if it just happens to be equivalent to DATEADD(DAY, -7, @DateTime), it may break in a later release. 

Be careful with READ_COMMITTED_SNAPSHOT: if you set it on, it can cause lots of subtle bugs. Also READ_COMMITTED_SNAPSHOT is the default isolation level, which may be overridden by something. Run DBCC USEROPTIONS to determine the actual isolation level your select runs under. I would explicitly SET TRANSACTION ISOLATION LEVEL SNAPSHOT right before your select. That way you will be sure your select never embraces in deadlocks, and you do not break any other code, like READ_COMMITTED_SNAPSHOT might. 

Currently dbcc DATA_PURITY complains about subnormal real values in some of my columns, such as 7.561407E-42. I would rather keep them in my tables, and ask dbcc to ignore them. I am OK with the check on upper limit, and other checks on other column types. So I would rather keep running the utility, but have it stop complaining about legitimate very low values. Is there a way to accomplish that? 

This is doable, but inconvenient. Are there other ways to return dynamic column lists? Edit to protect against SQL injection, I typically split the comma-separated list and join it against a system view. Anything that is not an actual column name is not returned. I did not mention that originally, just to keep the question short. 

and you are all set, as long as all your constraints are trusted. I am not sure why would you need the third table CourseSize at all. 

** The constraint CHK_IntegerSettings_PreviousFinishedAt_NotAfter_StartedAt guarantees exactly that. See for yourself: 

As in my previous solution, we can use constraint to ensure the integrity of data (the sequence of dates must have no gaps or duplicates). It is relatively easy. 

Add CourseSizeMax column to your dbo.Session table, and add a UNIQUE constraint on [dbo].[Session]([ID], [CourseSizeMax]) - it is needed later. Add CourseSizeMax column, and BookingNumber column to your dbo.Booking table. Add a FK constraint on dbo.Booking(CourseId, CourseSizeMax) referring to Session Add a CHECK(BookingNumber BETWEEN 1 AND CourseSizeMax) Add a UNIQUE constraint on dbo.Booking(CourseId, BookingNumber) 

This is easy if you have a numbers table. The following example is cut and pasted from Erland Sommarskog's site: 

Your first statement creates a disabled constraint. It needs to be enabled and possibly trusted. The following strange syntax will make sure your constraint is enabled and trusted: 

** Again, the same constraint UNQ_IntegerSettings_SettingID_PreviousFinishedAt guarantees precisely that, as demonstrated below: 

There is no substitute for benchmarking. To answer the question, I would create and populate several possible tables. Then I would expose these tables to your typical workload, and benchmark. Including additional columns in you NCIs will slow down modifications and speed up selects. Based on frequency of both, we can choose which approach uses less resources. If a row is on average read twice per year, your conclusions might be different as compared to the case when every row is on average read twice per minute. Besides, not all queries are born equal. If some queries must complete in certain time no matter what, then you must make sure these requirements are met. Obviously such requirement trump the common good approach described above. Only you can know the actual requirements. 

Let us drill down one more level and consider just one command, the MERGE. It was released as part of SQL 2008 with several problems, described in the following links: 

The following documentation describes how to see the refcursor returned from a function, here, like this: 

The select keeps a shared lock on a non-clustered index on itemId, waits to acquire another shared lock on the clustered index, so that it can retrieve more columns. The update has modified a page of the clustered index and of course keeps an exclusive lock. It waits to modify the non-clustered index as well. 

You can store A and B columns in a separate table. Make sure your table has a primary key on A, and a unique constraint on (A,B) Refer to (A,B) from your table. The primary key on A will guarantee only one B per A. The foreign key without ON UPDATE CASCADE will make sure B does not change as long as the row is referred from the child table. 

You can store pre-calculated gaps, and use constraints to make sure that your pre-calcualted data is always up-to-date: Here is the table and the first interval 

The language keeps evolving. A few decades ago the literate people used "indices" instead of simpler "indexes". As we switched to "indexes", we eliminated an unnecessary complication and made the language more useful. The need to memorize a plural for "index" was pure overhead - it did not in any way help us communicate. Make no mistake, there used to be grammar Nazis who enjoyed correcting those who switched to "indexes". Of course, grammar Nazis lost. This is how Occam's razor eliminates useless details if the whole thing stays relevant long enough. So let us take it easy - knowing the difference between rows and records adds absolutely nothing to our ability to develop and maintain databases. Many excellent professionals use rows and records interchangeably, yet develop awesome systems. As such, Occam's razor should eventually eliminate the distinction, and the next generation will have to learn one less useless fact. If, of course, SQL is still relevant at that time.