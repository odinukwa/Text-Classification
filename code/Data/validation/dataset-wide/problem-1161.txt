I would like to expose a problem I often have when I work with database and I never knows what to do. I need to load data from a file into a table (table1) line by line with the help of a software. The file can be a csv or something similar. During the loading I also check the validity each line and each value in this line. If something is wrong I put a message another table (table2) with a reference to my line. A line can have more than one error. So in my table1 I can have valid and invalid line. To check if a line is valid or not I need to check if there is a message, a line, in my table2. I hope this is clear. My question now. Thinking about Normalization should I create a column valid (true/false) in my table one to avoid the have to check table2 each time I want to get value from table one. Example: 

I noticed in the last month one of my SQL Server instances is getting a daily SQL Dump saved off to the Log directory. Unfortunately the SQL Dump is getting large and is consuming the majority of disk space. I don't know exactly where to start investigating why I'm getting SQL Dumps. If I run a DBCC CheckDB against my databases it comes back without any errors. Where do I start investigating why I'm getting SQL Dumps into the log directory? What can cause the SQL Dumps? Will reading the SQL Dump help with my investigation? 

I will inserting into a database the signatures of 500,000,000 pictures. The signatures will be generated using libpuzzle. Each signature is 338 bytes. (so 160 GB) plus a table for searching (read more below). I would prefer to keep the main database on one VPS server with a standard HDD (No SSD because of cost issues). The most important aspect is search time, Insertion time does not matter. In the past I have attempted this all within MySQL (with way less records) and had one database for everything, the main searching happened with a scheme like: 

I will not go in details and explain everything. For some (not good) reason I need to maintain 2 databases synchronized together. The full database schema may be different but not on table I want to sync. After looking documentation and all built-in possibilities I think it's better for us to do it manually. Actually it's not better, it's more realistic and cheaper. We only need to sync few tables. I plan to create trigger on CREATE, DELETE and UPDATE on some table. I would like to log all these changes from my database A into a a log, sync or audit table. Then I plan to create a small software that read this log, sync or audit table to apply change from A to B table by table. I guess this is wrong and I would like to know why and what is the correct way to implement sync manually between table from 2 databases. I use SQL server 2008 R2 

ID 2 is currently set to the default but cycles after a max 5 files. This trace isn't the one filling up my hard drive. ID 1 path and trace file are the ones filling up. I've already turned off C2 auditing. What other troubleshooting can I do to see why the audittrace files are even being written to in the first place? EDIT: I've restarted the services. It would seem the correct solution would be to turn off this audit trace by using the sp_trace_setstatus 1,0 but when I run that command I get the error: 

I am using a solution of backblaze B2 / duplicity / duply to create backups of my databases. I upload full backups and incremental backups to B2 using duplicity via duply. The incrementals are done similar to rsync. My server is 100GB and my databases are currently 70GB, but I don't suspect them to grow much. I dont have a master/slave setup. I want a backup solution that allows me to backup the 70GB of databases but not require very much space during the backup process. I decided that I could just backup the whole directory instead of using or other backup methods which requires a full backup persist on disk. I read that in order to do this method I would have to completely turn off the mysql server, do the backup, and turn it back on. I am OK with that because duplicity does incremental backups which should only take a few minutes. I have ran in order to make shutdown/starup as fast as can be and verified via that hovers around 0. I also understand the caveat for InnoDB tables is that they require the exact same mysql version in order to properly restore. Is that still the case? Is there anything I am missing and will I have a near 100% confidence that if I use the exact mysql version (mariadb in my case) that at least one of my hundreds of incremental backups will restore. 

In table1 I have an item 001 with 4 versions. Version 4 is not valid. This information of validity come from table2. I always need to be able to request the latest valid reference. Get the last valid reference. To do so I need the check into 2 tables. Check last line of table1 then compare it with table2. If invalid ignore last invalid line then repeat the request. This seems so heavy... 

Is it possible to connect to a SQL server in SQL Server Authentication if the server has been installed with Windows Authentication? 

I need to remove records from this table that are older than 30 days. We only have 2 columns in this table. The first date after 'Printed_%' is what I will be using to determine if the record should be deleted. Notice some of the dates are in MM/DD/YYYY format while some appear in M/D/YYYY format in the string. How can extract the date so I can determine which rows are older than 30 days? 

We use a centralized backup location on a network share. Each instance uses the Ola Hallengren scripts to perform nightly backups. I have a team that requires folder permissions to the same location where backups are stored. While the team has permissions to the parent folder it seems they no longer have permissions to any of the sub-folders where the backups are stored. I have narrowed it down to the command which executes the procedure dbo.xp_create_subdir. However, I am unable to figure out why the permissions are overwritten everytime this command is run. How can I ensure the subfolder permissions are not being changed with the Ola Hallengren scrips? 

I'm not DBA at all but I need to do something to synchronize four SQL server databases. I just discover there is a Merge Replication system in SQL Server 2008. At this moment I'm reading the specification and the documentation but it's not clear if this tool is able to do real synchronization or is just merging data from my master database to my slave database. By real synchronization I mean the data is synchronized from my master to my slaves but also from my slaves to my master. $URL$ 

Where by nature of the libpizzle, you would search the table for many and then get all the from the table, do some math and spit back a score for each signature how similar it was for the search. Then for each similiarity that was above a threshold I would get the data I needed from by looking up the It took about 5 minutes for 1 search to search 40,000,000 pictures - so I think there is room for improvement. Especially because I want this to be fast up to 500 million records. Should I seperate all the non-essential data (only about 1% the size, which is everything that is related to the specific picture) in a seperate database? On a seperate server? Because it's just doing a massive search for and spitting back all the 's that could be a match, I assume that not having any type of relation with the data can help me pick a specific technology that will maximize my speed. Which technology is best for this?