You have 32G real memory on your system, use amm and have almost 25G for memory_max_target. That does not leave a lot of memory available for other tasks. Chances are that you SGA is swapped out of memory. Also the connection pool is a bit big. Can your system handle 350 concurrent running sessions? I would start with a number that is close to the number of CPU's of your system. AMM is OK, as long as your application is a perfect Oracle citizen. This means, it does use bind variables where possible and only uses literals in SQL for class selections. If your application is not such a perfect Oracle citizen, start with setting a fixed db_buffer_cache size to protect the database cache by being pushed down in size by the shared pool. If your app uses many literals, it won't re-use cursors -but still tries to cache them- and waste valuable shared pool space. Check v$sql .... do your cursors have high 'executions' or do you have many app SQL that have executions = 1? To prevent swapping out the SGA, use LOCK_SGA=true and use larger memory pages, say like 16MB page size. This gives better use of the memory and reduces CPU usage. As mentioned before .... the swapping in itself does not need to be a problem, if your app keeps running as intended it could be OK. It could run quicker if it would not happen. best tip: hire a real DBA to do some serious diagnosis. It is a job that requires a little more than just being able to install software and create an empty database. 

I'm not sure how to conclusively test which way it is. I'm not sure the Live Query Statistics output can be trusted in this way. Does anyone know how this works? 

Is this feature documented and reliable? The only official mention of it is on the page but it's buried in a code sample. This suggest that it might be intended to be undocumented. There are remarkably few Google matches for this feature as well. 

I used the column to find indexes/partitions that have ghost records. I found a few that have one such record. I then tried to queue them up for processing by scanning all pages: 

On my dev box I sometimes need to run very IO intensive queries such as index builds and . This can put so much load on the disk that it hardly can process anything else. This causes enormous lagging in other programs. For that reason I sometimes need to suspend using Process Explorer. If I do that for longer periods of time I sometimes get IO timeouts such as 

I'm in the middle of a huge . In order to speed it up I set . Will this affect the running statement? I watched IO and CPU numbers on the server and could not make out a difference. I'm also not sure if there would be any difference at all so I can't tell. 

This caused that even the simplest of all projects took months to even get started and in a time where time to market is extremely important, this is killing. For some reason this is missed by many, often even neglected for political reasons. The solution for this is quite simple. Create one serious database, give every project/application their own schema[s] and access user[s] and get running in hours, instead of months. If you are going to do something like this, it could prove beneficial to combine applications that have similar up-time requirements. Oracle gets more and more on-line maintenance options but for sometimes, getting a few hours downtime is a lot easier. Having time windows for this defined beforehand could prevent a lot of problems. You are going to need some downtime. Don't allow applications to connect to the database, make them connect to services dedicated to the application using their own tns-aliasses. Doing so enables you to move the application to an other database, without having to re-configure the application. BTW: the companies that used this way of consolidation saved a lot of cash yearly, more than the licenses required to start rolling. 

No. The words Secure Files should be interpreted that files can safely be stored in the database. This is often safer than storing them on the FileSystem with a reference from the database. If the file is in the database it is also protected by many of the database features. SecureFiles is the modern implementation of LOB's. The new implementation has a great performance boost compared to the old inplementation. dbfs is a fileystem that can be created inside the database and that can be mounted on Linux using dbfs_client. Regular operating system processes see it as a regular POSIX filesystem. The LOB's in a dbfs should be implemented as SecureFiles to have the best performance. The performance of dbfs is better than that of NFS, if using filesystem like logging. With normal logging dbfs has about the same performance as NFS, with the added possibility of replicating the data to standby databases. 

Tonight our server decided to install SQL Server 2014 CU7 for SP1. The update appears in the Windows Update history and I found downtime in our logs for it. The previous patch level was CU3. I was surprised by this because I am not used to Windows or SQL Server automatically installing CU's. I also never witnessed a service pack being installed automatically. What has changed? According to sqlserverbuilds.blogspot.de the CU7 was released today. It was installed right away after release. I am not categorically against this but I would like to understand what policies are driving the automatic update choices. Strangely, tonight was not a "patch Tuesday". No other updates were processed. The server did not reboot, either. I'm certain this was not a manual action. So far my plan was to keep the server at the CU3 level and upgrade to 2016 eventually. The new CU being installed caused unnecessary downtime and introduced risk which I normally was not willing to take. In particular I do not wish to install patches at a random time. So what's the automatic update policy and should I do something about that? 

I'd like to keep rows with the same together in the result set. The groups themselves should be ordered randomly, though. is supposed to be the secondary sort criterion. Like this: 

To me it looks like you did not start the listener but it is started. Your problem seems to be the tcp connection and not the ipc. So the listener is running, you can not reach it using tcp. Test this using If the telnet does not give a connection, a firewall is blocking you. Stop/edit the firewall. 

Normally it should happen quickly. There could be large transactions that take a while to rollback, or a long running transaction that takes ages to end. If the time is spent for the rollback, you could crash the instance with a good and solid kill -9 on the server procs and use parallel instance recovery. Normally this should not happen but parallel instance recovery can be quicker than a recovery by 1 process. see Oracle manual 

What needs to be done depends on where the connection initiative is done. If the initiative is done from the Oracle side, you could use dg4odbc and configure it with the odbc driver and connection details for the Postgress database. dg4odbc sets up a special kind of listener process that you refer to using a regular tns alias. This listener process has to be on the same machine is the ODBC driver but does not necessarily have to be on the database server. Ofcourse, the datbase server has a listener process but nothing -well maybe licensing- does prevent you from setting up a dedicated listener for dg4odbc on a separate macine. In your Oracle database you setup a database link that uses your tns alias that points to the special listener. 

I then waited at least 10 seconds to let the ghost cleanup task run. But the ghost records do not disappear. I also tried restarting the server. No ghost related trace flags are in use. This is not an actual problem that I'm having. I'm trying to understand ghost cleanup in general. Why do the counters not drop to zero in tables that have no writes? 

Watching queries execute with Live Query Statistics I noticed that it seems SQL Server is lazily constructing a hash table from the build input of a hash join. This is a meaningful difference in the case of 0 probe rows. It potentially saves the entire build side tree. I always thought a hash ran like this: 

The documentation calls out the behavior of and so I understand that part. But the operator seems to behave under a more complex set of rules. 

Does Azure SQL Database support indexed views? Does it support automatic indexed view matching without ? Does the answer depend on the pricing tier? I was unable to find definitive answers to these questions. The feature differences guide does not mention indexed views. The indexed view documentation does not contain the word "Azure". 

This sometimes causes the query to abort. How can I disable IO timeouts on my dev box? Also, in case the query does not abort I sometimes get dump files. How can I disable those? 

This would have taken care for cleanup of the old tablespaces. I also expect that you would have gotten errors during the import. When messing with databases and tablespaces, a little dba training would be very good. For 10g start here OracleÂ® Database 2 Day DBA 10g Release 2 (10.2) 

The biggest con's are those where those fields are misused for other purposes than where they were meant for. Normally dimensions are here for a reason. The question looks a bit like what often happened in the old COBOL days where developers asked: can't you just give 1 record type with 1 field of type character with a large size. Doing (the old cobol filler field way) so will make the meaning of a relational database less clear. An other one is: what about programmatic errors that are adding length to strings, without noticing. This will end up lots of extra space. Storage is still at a cost. If developers make no errors and the usage remains as is defined, I see no harm in setting length of [n]varchar2 fields a bit higher than currently needed. Especially if it is foreseeable that the size of the contents will grow, defining that upfront will save a lot of time.