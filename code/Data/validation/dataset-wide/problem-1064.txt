Internal representation of larger attributes will be sometimes compressed. More specifically, what works here is the TOAST (Oversized Attribute Storage component used in PostgreSQL). The threshold when values are considered for compression is 2000 bytes. is not a logical length, but the size (in bytes) of actual internal representation of the column/variable. It is documented. PostgreSQL stores array values in a custom, internal, binary format. Command line example below. Details also here. 

Yes you can hide those messages in the log. In the calling session, before running the statement, issue this statement: 

Decide your vertexes (nodes, objects) and edges (relationships). Convert relational data to cypher, declaring all items and all relationships explicit. 

If you really want two queries, you can use special FOUND variable to test if previous query gave any result: 

If you want to see also query duration, you can set instead of . This is very useful for query tuning. Then reload config (restart or HUP) and collect enough log to estimate traffic. Note: neither method will include queries embedded in user-defined functions. 

b-tree index is stored as a sorted index - entries are organized based on their value from left to right. every delete statement from the table is also deleting an entry from the index and balancing the tree - so not only table locks are obtains but also index locks. if you are deleting high pk values, lets say pk=1000, and inserting a new row with pk = 1001 - the right block of the index is becoming a hot spot and you have a locking problem. what you do in a case like that is making your index descending. 

the advantage here is that you keep all your phones in one table. the disadvantages here are that 1. if you want to retrieve a phone number there is an additional join to the relationship table 2. you would probably want to retrieve only an employees or only a company phone - so the join would process unnecessary data. 

There is a lot of good sources on to partition or not to partition. If you are going to store 2 years and more, daily partitions could be optimal. But remember that very large number of partitions will make query planning longer. Threshold depends on CPU speed / queries used. PS. I assume that you ran out of normal ways to optimize: 

Notes on performance With small tables (less than 1000000 rows), any solution will work. is slightly faster: 

In my opinion it is not related to or Windows issues. Per pg_basebackup docs, the main data directory will be placed in the target directory, but all other tablespaces will be placed in the same absolute path as they have on the server. 

I try to saturate the server using pgbouncer. I was running a Select-only test, with 1000 clients for 5 minutes. (). PgBouncer was initializet with scale=100 (but a SELECT-only test should not suffer on it). During the test: 

I just get a problem with a customer's SQLServer. I'm reading articles related to schema, owners and permissions, but I can't get it out. I have two DB : DB1 and DB2. I have a schema, which has the same name of it's owner (user1, which has "user1" as default schema). This schema exists on both databases, user1 is mapped as owner on both databases. So, some of my tables are in this format : user1.table1, user1.table2, dbo.table3... DB1 is not used anymore : I turned it offline. Just after that, the tables [user1].[tableXX] from DB2 were not usable... Turning back DB1 online solved the problem. How can this happens? How can I turn DB1 offline without blocking DB2? Thanks in advance! 

I just had a problem : days ago, on one of my client's databases, the LDF file suddenly grew up to 100% of the disk capacity. .TRN backups are correctly done every 15min (I don't think reducing this time will help, because this database is rarely used). I created this database from a backup, two months ago. Everything was OK. What I didn't understood, is why the "initial size" of the LDF file was... 512GB, which is the size of the disk... If the initial size had been like that since the first restore, i would have noticed it. So I'm certain that it was not the case. What I did : backup, simple mode, reducing initial size (it reduced the file size without shrink), full mode, backup. It's OK now, but i'd like to understand if the "initial size" can change automatically, and if yes, how to prevent it. Thanks! 

For better effect please post results of and . Also let us know the dataset size and resource configuration parameters (, ) as well as database host parameters (OS, memory, disks). 

Update: Konrad corrected my misunderstanding of his question. The goal was to count queries, not transactions. How to count queries? Method 1 Use pg_stat_statements contrib. Method 2 Enable full logging of queries for a representative period of time. To enable full logging, for PostgreSQL 9.0 - 9.3, change following settings in 

Why async? I would avoid doing this in trigger due to locking issues under high load. Also, easy to DDOS so permissions should be separate for form insert and form create. 

In current version (Pg 9.0-9.3) there is no fast "fail-back" mechanism. You will have to follow official guidelines which state: 

To calculate TPS (transactions per second), run the query several times and calculate difference over time interval. There are ready made tools for that, one of them is $URL$ More info: $URL$ 

what you are looking for is slowly changing dimension design. each row of data in the table will hold the opening date and the closing date for that record. this deisgn enables you to query the table as it was at any point in time in the past. example 

The script seems to work but nothing is written to /tmp. I usually use but I am unable to use this in a script. From what I understand using require having all the command on the same line which is not a viable option for me. Any idea why COPY report success but don`t write anything? P.S tried runnig with sudo, with psql, and tries granting mydb user superuser. 

The safest and simplest way to add a tns entry to an oracle client is to use the wizard. run on the command line and fill in the necessary fields step by step. if you need to add parameters edit what the wizard created.