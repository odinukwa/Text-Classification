And works fine. Except that the working file, the one with the name generated by tempfile() remains in the working R folder on the server. All the other working files get cleaned up at the end of the process. Im my case, I don't want the image file to remain on the server, as it potentially contains sensitive information. -- I'm pretty new to R, so I've been experimenting with the code in an interactive line-by-line fashion in R console: I find that before running that line with the call to readBin() to assign the contents to the output data set, I can simply use the R command 

Horses for courses. Works pretty much the same as your method. I'd write it your way in this instance if I were me. But it IS a good conceptual exercise to try to think of and compare different approaches like you're doing. Keep it up! :) One thing to note: you should really reference the source table in the SELECT clause. To avoid ambiguity or future complications: 

I think I've been able to fix what I think is broadly the same issue. First thing in the morning, if I go to our Report Server URL, it loads quick enough and I'm able to browse the menu straight away no problem. But as soon as I click on an actual report, there's that 2 minute wait before the report actually executes... [Quick test... it was only 75 seconds for me this time, but it felt like longer!] Running a report from a command line using the "RS" command seems to start up (wake up!) whetever part of the architecture isn't otherwise being started and the first report takes a normal amount of time to execute. I shall shedule that as a task and forget about it. RS is a bit fiddly to use, since you need to script a bit of VBA to execute the report, but I already needed to work that part out for another job I had.. Edit: Added script below I can't really take all the credit for this, nor cite references I'm afraid. It's cobbled together from 3 or 4 different resources already out there, and adapted to suit my needs (specifically, I wanted to generate a set of pdf exports for a range of inputs). And then simplified a little bit more for here: Script file "RunReport.rss" 

Is it crucial for the business to ensure that a large block records has been transferred in a large batch, such that anyone accessing the source table will not see them at the exact point in time anyone access the target time will be able to see them? Or, is it necessary for users of the target table to see large blocks of records appear in the target table all exactly at the same time? If no to both of those, I second Sp√∂rri's below statement... 

Once you have the private key over to the destination server, you can create a symmetric key using the certificate, and decrypt your columnar data using this. It's not necessary to move the master database key from server to server, however, you do need to have a master database key created on each server. The master key is needed to protect the private keys internally, and to support server bare metal recovery. You don't need the master database key password to copy a certificate. It's the certificate that you need, and you should be able to access this as , unless the certificate itself is protected locally, in which case you'd need the certificate's creation password to run the operation above. 

Note that regardless of whether you trigger a table lock, a 1000+ record INSERT transaction will lock many pages in your indexes, and has a high risk of causing deadlocking or other problems. The good news is that your post does not seem to provide a business case for taking on a such a large transactional operation. 

If I rm() the dataset containing the imagine data frame at this point, it still won't let me delete the working file. Has anybody solved this? Or anyone more familar with R offer any assistance? 

You might then be able to edit the datatype directly in that text stanza and trick SSRS into sorting itself out 

Here's the trick you're after - use the CTE to build a list of each region, cross referenced with every region which counts towards it: 

Might be a little quicker. Essentially it ought to behave much like your original working query did. 

A good trick to use when coding, which I tend to do automatically now, is as soon as you use an "OR" anywhere in your code, then add some parenthesis to make it totally clear. Even if your where clause is simply: 

since, you're future-proofing against someone adding to the code later and accidentally writing something like: 

and then when you look at the code your mind kind of fills in the parenthesis where you think it ought to be. Whereas order or precedence rules actually perform it thus: 

Yeah, SSRS can be a little cumbersome with its handling of empty/null values. Try adding an explicit isnothing() before the yellow option: 

OK, just to follow up my correct (yay!) comment answer - in my experience almost everything along the lines of "weird issue" involving data selection, involves a rogue, unparenthesised "OR" somewhere in the where clause. It's always the first place I'd look. Typically you write some code along the lines of: 

Which made sense since I had changed the account that was being used. I simply deleted encrypted content to address that since it was a new SSRS instance anyway. The second error was the one that took me for a loop 

That gap you see above prodkbp is in the output as well. Not sure if that matters either. My production server has that anomaly as well so I can't imagine that has anything to do with it. Basically everything looks right but something is still wrong. I cannot install Enterprise Manager Environment 

Windows 2008R2 with firewall disabled for testing (not that it changed anything). Listener.log doesn't show anything of worry. Just evidence of me restarting the listener many times. I am aware that I could configure a static listener with but I would like to understand why this is not working. This is the 4th time around that I have been doing this and I have not run into this problem yet. I am basically stuck since I am not sure where else I should be looking. 

Which also looks OK. Most of the solutions point to SERVICE_NAME mismatches. However I don't appear to have one. v$parameter output 

So it would seem the issue was not the service name specifically but that the request was going to the IPv6 address which was not set up in any of the required files. Looking at listener.log ( which for me was located D:\app\Administrator\diag\tnslsnr\dvp-oracle\listener\trace\listener.log) I found these entries associated to my connnection attempts. 

I've used SSRS for a subset of this kind of activity. It's not great for using as a full-on data input program for obvious reason, but there's no particular problem with running insert/update/delete statements inside datasets. What I've tended to use it for is more "clicky" kinds of updates. Where clicking on a hyperlink within a report causes something to happen - be it insert something in to the database, update a value somewhere, etc. And the activity that happens is controlled by the hyperlink passing in the various parameters to control that. I've tended to use a report where the hyperlink runs a new instance of the same report, but controls what gets updated by passing in new parameters. Mainly so that all the code is in a single report. I've made a quick demo report to demonstrate the technique in action, if that'll help? Available at: $URL$ You can do quite sophisticated stuff within the confines of this technique, but it's definitely not a data entry portal for entering entire rows of data. I'd be interested to hear if anyone's really used it for that in earnest. 

at that point, and the file will be deleted. But once I've read the results in to a data frame it's like it's now keeping a file handle open and calls to file.remove() result in an error message like this: 

Really new to Oracle and being a DBA in general. I am trying to set up a development environment so that I can play an learn oracle better. Enterprise Mananger failed to configure itself when I first created the database using the Database Configuration Assistant. No biggie. Just need to user emca.exe I had some issues with the Listener but those might have just been me being impatient in waiting for the service to register or the service not running. Right now my issue is this from the emca log: 

Much to my surprise that actually fixed the issue. My problem is that the documentation for account permissions on the SSRS account do not mention this permission is required. They only state that "Log On As A Service" is required. All my other SSRS environments, which I inherited, use a domain admin as the service account so I have nothing to compare to. I am trying to get away from that setup. Is this just a flaw in MS documentation or did I cover up the problem (XY)? I do not understand how "Bypass Traverse Checking" fixed "ERROR: Failed to initialize listener" 

According to post here and on SO it should just be an issue with the service name. Problem is it looks right to me. Listener.ora 

The important portion was . Since I would never need to support IPv6 I removed it from the network adapter and restarted the server to ensure all services were updated accordingly. While it might not be the ideal solution I was able to recreate the EM dbcontrol successfully after that change. 

(Note: I'm not sure that adding supplementary increments to questions is quite how stackexchange is supposed to work, and you haven't supplied an updated fiddle for this - so this is untested. But here goes anyway...) You just need to change a couple of lines in the body of the select, thus: 

You don't need to display any fields in this special group, it'll simply use the page break setting as per your requirements.. 

Another workaround, which seems to work quite nicely - place a parent group of the group you want to selectively break on, then in Group Properties for that, group on a derived expression of either the group field, or a fixed value. And then tick the box for that grouping as per your screenshot. e.g. Assuming a parameter called "BreakPages", and a grouping on a field called "MyGroupField", you group on: 

Knowing, and understanding, different options to obtain the same results is key to getting familiar with SQL, IMHO. No matter how obscure and esoteric the problem might be. Anyway, I know I'm late to the party, but here's a different take on a subquery method.. 

Like MguerraTorres suggests, SSRS has probably got confused somewhere, and the value suggests it's getting cast to some kind of floating point datatype somewhere along the line. Try checking the datatype SSRS is assigning by looking at the xml in the .rdl file raw text. for a simple bigint column I'm seeing an SSRS field definition something like this: 

It's possible to move a private key from one server to another, if the symmetric key was created using a certificate. The certificate is what allows you to create the same symmetric key on a different server. Looks like that is your approach so you are off to a good start. You have created a symmetric key, secured with a certificate, to encrypt a column of data. To move a symmetric key to another server, you need to follow the following process: 

You need to back up the certificate and move this to the other server. To do this, use the command. Note that this creates two files: the certificate itself, and its private key file. 

Now 64KB is not a very large value. If your records are each 1000 bytes, that means you are talking at most 64 records. As such, I recommend you do these transactions one record at a time. If the migration terminates on an error, one of these records will get rolled back, and you can certainly pick up where you left off because the source table and target table will be in alignment. This will ensure maximum availability for your users. 

When you create the certificate on the destination server, you specify both the certificate file, and the private key file, along with the password you used to back up the certificate on the source server. Here is the example from MSDN: