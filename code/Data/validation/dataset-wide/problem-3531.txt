There was a similar question in the Quant. Finance section (see here: $URL$ People suggested the following options: $URL$ (free for subscription), $URL$ (not free) I also guess that Bloomberg and Thomson Reuters databases should also contain historical CDS rates. Practicioners use the data source extensively. Though they are very expensive for a personal use, some universities buy a license to get access to it and it is worth asking people from the finance/banking/econ department if they have access to the database. 

I believe there is no comprehensive source of the information collected in a standardized way that you are looking for. If you want to have info on a single company only, I guess your best bet is to look at the companies' annual reports. Companies disclose a lot of information about their operations there. Some newspapers certainly did investigations on the topic and collected (spuriously, I guess) some info on that. Try to check the Wall Street Journal, the Financial Times and the Economist for the starter. If you look for the aggregated statistics by countries, try to search the web-site www.statista.com and the works done by scientists like Gabaix and Branco Milanovic. They both did much of empirical work in inequality and could gather much info on that. No guarantee they published their datasets free of charge. 

A conventional answer would be that interest rates represents the rate of return Russian banks agree upon to lend money to physical clients/enterprises. If you believe this, you can create the following explanations. Two things come to my mind. Option A: High Costs Russian banks are unproductive and poorly managed. Their operational business is so inefficient that forces the bank to add a high premium on the interest rate. Option B: High Risks Russian banks have to lend money to clients with high risks. What would you do as a bank if you face many clients with high risks? You charge a higher price so in case somebody fails you may cover the sunk costs by revenues generated from credits to other clients. This is a simple way to `compensate' for uncertainty. You can have a combination of two of course and that could already explain some of the differences you observe in the world and in Russia as well. Note for instance, that financial sanctions decreased stability of the Russian banks in a number of ways: a) enterprises (thus lenders) do not have access to external finance anymore and given the record low interest rates in Europe it means less access to cheap finance b) banks cannot finance on the external market themselves; thus if you have a liquidity problem you have less sources to take finance from. This is not the end of the story however, as banks usually differentiate the interest rates they charge depending on the type of product they offer and a type of client, who wants to lend. If they do a poor job at differentiating between good and bad borrowers, another option arises: Option C: Banks do poor job at separating the wheat from the chaff // Signalling does not work Imagine you have two type of borrowers: risky and non-risky. If you would have perfect information on everyone, you could easily charge interest rates according to how risky the business of clients is. Lower rate for a risk-free guy, higher rate for a risky guy. But reality is more complex: you do not know for sure. And here they sit and show their documents, balance sheets, profits and loss statements etc both giving an impression how brilliant their business is. Yet if you know (post factum) that half of you clients go bankrupt, what do you do? Well, you just charge a high interest rate for both of the guys! Maybe it will be the wrong guy who will take your loan in the very end but at least you'll get high interest payments before he goes bankrupt. Maybe it is exactly the case with Russian banks. That is, maybe they just do a horrible job at investigating how reliable business of their lenders is. That indeed might be the case because many of Russian banks expanded their business in mid-2000 on the boom of the consumer credits. And it is not a heavy job to do: broad consumer base, high diversification - just give money to everyone and be happy. Yet now, when income stagnates, the business model does not work that good anymore. What do you need to do? Well, you need to start working with the enterprises. But lending to them is more complicated if you want to know for sure: you need knowledge of how business operates, what business-risks are relevant, how responsible is the team, how the industry feels in general and the list goes on. Maybe most of Russian banks do not know how to do it good and therefore keep charging high interest rates. A Little Sidenote Keep in mind two things however: a) My answer comes from the micro-perspective. The macro guys would probably point out on the issues related to capital flows and macro regulation, b) You better be clear which numbers you are reffering to and where do they come from. I've just checked the web-site of Sberbank. The web-site reports the following borrowing rates (minimum interest rate charged per annum): consumer credit (12.9%), morgage for flats in built houses (8.9%), security-free SME loan (17%), securitized SME loan (14.52%), SME loan for investment purposes (12.2%). Yes, these are minimal rates meaning that the actual rates are higher. But note two things: they are very far from 25% you mention (and I bet that the rates in private banks have to be lower if they want to compete with Sberbank) and there is a high variation between them: the maximum difference is around 8%. And we are talking just about one bank. Thus, when you claim that the interest rates in Russia are way higher than in other developing countries you have to be sure that you compare apples with apples. And that you compare the averages of the apple sizes in both cases and do not take the maximum size on one side and the minimum size on the other. Hopefully you will find some of my explanations trustworthy ;) 

My view coincides with the introduction to your question. Namely, a) Econometrics is mostly concerned with causality b) Machine learning is mostly concerned with fit But for the remaining part, our views depart. Here is why: a) IV (and other quasi-experimental techniques) are not the only way to test for causality. The alternatives are i) experiments ii) structural estimations. In both cases you apply econometric machinery though you will mostly use a simple OLS in the first case and bayesian/GMM/Maximum Likelihood things in the second. Compared to experiments, it is probably less clear how structural estimations help and here I come to the second point; b) As in any science, economists build math models of how things work. The problem is, there are many models for many contexts. How to define, which one is an appropriate in given circumstances? Here is where the econometricians help, because econometrics helps to discriminate between working and non-working models. There are different ways to show it: with (the now so popular) quasi-experiments, you show that the (hopefully causal) link either exists or does not and its magnitude = $\beta_i$. Now, what if your data does not fit into the shoes of the approach and doing an experiment is impossible? You can go for structural estimation You say: "Hey! Let's assume that the model XYZ - e.g. the Cobb-Douglas production function - works. If this is true, what would be the estimated parameters?" So you take your non-experimental data and forcefully put them into the model you have and estimate the parameters. How does it help to establish the "truth"? You look at the parameters and try to understand how reasonable they are. For instance, if you investigated the parameters of the Cobb-Douglas production ($Y = AK^\alpha L^{1-\alpha}$ s.t. $\alpha < 1$) function but your coefficients for the log-log regressions are $\beta = [13, 0.8, 1.5]$ (the numbers are absolutely fictional) then you have a reason to conclude that the model does not fit the context (maybe the industry) you are studying because you got $\beta_2 = (1 - \alpha) = 1.5 > 0$. Nonsense, right? Maybe, because you can blame data, the fact that you observe the equilibrium outcomes and omitted variable bias. But it makes you think in both directions: anything wrong with the theory? anything wrong with my econometric model? People in Macro and IO frequently follow the approach (their tools differ though) because one has limited abilities to experiment in the field. Otherwise, when you do follow the prescriptions of the quasi-experimental literature, you have investigate only a little subset of problems, which are crucial for our understanding of how economies work. This is in my opinion the main point of the Deaton's critique with respect to the quasi-experimental approach to the causal inference. It turns researchers into people, who are looking for a problem that fits the tool and care about the context in asmuch as randomization is credible without digging into the field for real. They can publish studies on labor economics, make conclusions about political economy, analyze sports data and evaluate development policies in poor countries all at the same time and do not care about the underlying mechanisms. The quasi-experimentalist don't care about the econometric model as long as randomization works. Maybe the true relationship is linear, maybe not. But when instruments are strong and you use the right words in the section on identification strategy, it does not matter for the publication? This is probably not that bad per se, but Deaton is worried that the approach tell little about which models work and what are the values of the fundamental parameters (Check the response of Imbens though. Both things are good readings). Why are the parameters important? Let me give an analogy from physics (where my knowledge is limited by my high school classes). In physics you measure the things, make experiments and obtain the coefficients. Fine, now we want to make a prediction about how fast the stone falls in a new place. The natural appoach is to use the previously obtained parameter estimates to predict how fast a stone falls from a certain distance in a new environment. If you know how the new environment differs from the other one (defined by the model!) you can use the coefficients you got, plug them into a model and get a credible prediction. In economics, the values you get from the quasi-experiments won't help you to do the same thing. Think of a development program the World Bank started in Eastern Europe and want to apply in South Africa. Assume you had a credible inference with a super-fancy RDD strategy. Fine, you got your super-significant $\beta_i$. It is clear though that the impact of development program in Eastern Europe won't be the same as in South Africa because the context (environment) differs. So using the $\beta_i$ directly won't work. But can't we just somehow adjust the values and make a reasonable prediction? Well, since we don't know what on Earth the $\beta_i$ really is and how two models really differ, we don't know what kind of transformation to the $\beta$ we need to apply. So we know something for Eastern Europe but cannot use the numbers for other places. That's a pity right? Cause you did a good job, but cannot generalize it's results. Structural econometrics can be explicit on what the coefficients - in terms of the model - mean and how to use the values when you transfer them to another environment. The price of it are stricter assumptions on the relationship between the variables and the structure you - as a modeller - impose on the error term. c) In my opinion, machine learning is a valuable tool to gather the previously unavailable data and test things, which we could not test before. A good example is the current stream of papers on protests and political economy. With internet you get access to a lot of unstructured information. The ability to extract it using the machine learning techniques enables you, for instance, to evaluate the sentiments of the electorate and how it affects future political outcomes. So in a sense ML is a good tool to save your time to create datasets to study novel problems or tackle the omitted-variable bias (since now you can quantify things, which remained unmeasured - and staying part of an error term - before). 

I am working through the basic examples of the stochastic RBC models in the book by McCandless (2008): The ABCs of RBCs, pp. 71 - 75 A Standard Stochastic Dynamic Programming Problem Here is a formulation of a basic stochastic dynamic programming model: \begin{equation} y_t = A^t f(k_t) \end{equation} \begin{equation} A^t = \cases{A_1 \text{ with probability } p \\ A_2 \text{ with probability } (1 - p) } \end{equation} \begin{equation} k_{t+1} = A^tf(k_t) + (1 - \delta)k_t - c_t \end{equation} With an agent maximizing the expected utility function: \begin{equation} E_t \sum_{t}^\infty \beta^t u(c_t) \end{equation} Substituting consumption from the previous equation and using the recursive formulation of the problem gives the following problem: \begin{equation} V(k_t, A^t) = \max_{k_{t+1}} \left[u(A^tf(k_t) + (1 - \delta)k_t - k_{t+1}) + \beta E_t V(k_{t+1},A^{t+1})\right] \end{equation} Then McCandless proceeds saying that the algorithm to solve to the problem is almost identical to the deterministic case. One finds the first-order conditions (a derivative of the value function with respect to $k_{t+1}$) for the control variables, then does the same for $k_t$ and applies the Envelope theorem to get an analytical solution. Steady-states found, model written, paper submitted. Profit. A Lottery Augmented Version Now I want to investigate a little different case. Take the very same model but introduce another control variable. Call it $s_t$ for a security: \begin{equation} l_{t+1} = l_t + s_t \end{equation} And the $l_t$ enters the problem through the variable $A^t$: \begin{equation} A^t = \cases{A_1 \text{ with probability p} \\ A_2l_{t} \text{ with probability (1 - p)}} \end{equation} The main difference is easy to see if one writes the equation for income in a period $t$ explicitly opening up the expectation sign: \begin{equation} y_t = pA_1f(k_t) + (1-p)A_2l_tf(k_t) \end{equation} In this case we have a deterministic control variable $l$ "turning on" when a certain event happens (as if you would win in a lottery, which increases your income by the factor of how much you invested in it - yes the example makes little sense but I am interested in the principle itself). The Question: Does the following lottery augmentation changes the process of how to solve the model? If yes, what is the idea behind and what does change? If no, why is that? P.S. If someone could point me a paper with an example model, which is very close to the one I described, that would be brilliant.