Nested views are non-deterministic, so we cannot index them Each view references multiple s to build the strings Each UDF contains nested s to get the ISO codes for localized languages Views in the stack are using additional string builders returned from s as predicates Each view stack is treated as a table, meaning that there are / / triggers on each to write to the underlying tables These triggers on the views use that stored procedures which reference more of these string building s. 

Also, feel free to vent about your experiences with tasks like these. I'm sure I'm not the only one who has been handed down tasks like these. 

Here is why s are being used as predicates. The column is formed by concatenating: During testing of these, a simple from the view returns ~309 rows, and takes 900-1400ms to execute. If I dump the strings into another table and slap an index on it, the same select returns in 20-75ms. So, long story short (and I hope you appreciated some of this sillyness) I want to be a good Samaritan and re-design and re-write this for the 99% of clients running this product who do not use any localization at all--end users are expected to use the locale even when English is a 2nd/3rd language. Since this is an unofficial hack, I am thinking of the following: 

Is there a best practice method to handle localized strings via a view? Which alternatives exist for using a as a stub? (I can write a specific for each schema owner and hard-code the language instead of relying on a variety of stubs.) Can these views be simply made deterministic by fully qualifying the nested s and then schemabinding the view stacks? 

I have a table that stores version information in multi-dotted strings, ie '4.1.345.08', '5.0.1.100', etc. I am looking for the quickest way to find out which is larger. Is there a faster, or easier way on the eyes than what I have come up with below? One caveat is that I am 90% sure that we should ONLY find dotted numeric values here, but I can't guarantee that in the future. 

I have a client agent application that stores a replica of user metadata for the entire organization in a or earlier database. It gets accessed fairly often based on user activity so low latency is appreciated by everyone. Normally this is all well and fine, but some organizations have to store multiple orders of magnitude more metadata, which would total around 10-300 million rows, instead of 1 million or less that a typical agent uses. The Jet DB runs on each client workstation, and must operate locally on aging laptops without consistent network access. Full user data syncs get pushed to it every 24 hours or the next time it can reach the servers upstream at corp, and metadata stored while offline gets replicated back to corp whenever convenient. I have a copy of the schema that works for the server side agent which should have not changed since it was first implemented back in the early 2000's, should have very similar data access patterns, and runs on SQL Server 2008 R2 and up. The server agent does a substantial amount of reads and writes in a RBAR fashion, so I am going to assume the same access patterns are used in both. If I profile the activity for a few hours on the server agent to generate some queries to replay activity to benchmark, how can I run it against a Jet database? Are there testing or development utilities that can create a new Jet database and populate it with a schema and data? I am wondering if there would be any benefit in pushing for the development team to upgrade the client database engine to one of those new SQL Server engines that get loaded up only when the application code needs it. Not sure what the best options are out there, or if there is either a useful benefit either in performance or memory consumption. The client agent code runs in an old version of Java, .Net, and C++, but I have not been able to determine which language the data access layer is written in. 

If the function does not need to be called from SQL you could change it into a procedure with an OUT parameter, though this still requires a "junk" variable it might look very slightly cleaner? 

Earlier today we had a problem logging into a database that had just been created automatically. When on the actual box (Linux) , i.e. a local connection, was fine, though, it was occasionally impossible to do anything once connected. Otherwise, whether using a session on the box or not did not work at all. and DNS testing revealed nothing and there was no problem in . The Listener was accepting the connection but not handing it off to the database. This was confirmed by using on the listener log and watching it accept the connection. There is a logon trigger on the database that inserts the user details / time etc into a table, but this doesn't have any indexes and there cannot have been any locks. We finally traced the issue. There was a conflict in the file that creates the temporary tablespaces for the database. It resulted in there only being 5MB of tempspace instead of the normal 25GB. Disabling the logon trigger and then adding the additional temporary tablespaces, sorted everything out. There were some sessions already running that would have been using some of this tablespace. Apparently Oracle requires some temporary tablespace in order to accept an incoming connection. The specific version used on this DB was 9i, though I think this applies to all. Why and what is it used for? 

As far as I can tell this doesn't appear to be a permissions issue (though it obviously is). The system privileges of the 2 users are identical, the following queries return no results: 

The following are examples used to convey the point, there are multiple instances of each of these examples within your ERD. Naming Consistency is key. Some table names are pluralised () some are not (). Your table contains data about many tickets so I normally prefer pluralised table names. You're prefixing every column with the name of the table. This makes for column names like , which isn't instantly understandable to readers. If I named the primary key of then you store and you might access , which makes everything more understandable. Relations There's a couple of relations that don't exist, for example you've got in (makes sense if you have preassigned seating) but no relation to ? Why not? If your cinema/theatre doesn't have preassigned seating then you don't need this data. There are some duplicates within tables. Why does contain both and ? Data duplication There are some unnecessary (unenforced) relations that are allowing for data inconsistencies. If a must happen within a an a ticket must be at a then there's no need for to contain any information about the hall in which the movie is going to be shown. It's unclear what the difference between the and is. Maybe one of them is the number of minutes the screening runs for? This information is already within and so is another duplication. Data There's no need to persist the age as you've already got the date of birth. I've written about this on Stack Overflow a few times. It's quite an assumption that all staff and movie names will be 45 characters or less. I find the inclusion of in the staff table a little off-putting. What's the purpose of storing this information? Are the chromosomes of the person selling you a movie ticket that important in this cinema/theatre. I also don't understand why it's 10 characters long. How to start Think about exactly what information is required to go within each table. If you're not sure it's required then don't add it in. If you need to add something later then so be it, but you haven't included a lot of unnecessary data. I'd set-up some test tables with dummy data to see if what you're creating will actually work. Every relation to another table should be enforced. If you've got some unenforced relations then either add a foreign key constraint or remove the column. Your cinema probably has tickets, so removing the data will prevent you selling tickets. 

This seems pretty rotten to me, but I only have a few years experience with TSQL. It gets better, too! It appears the developer who decided that this was a great idea, did all this so that the few hundred strings that are stored can have a translation based on a string returned from a that is schema-specific. Here's one of the views in the stack, but they are all equally bad: 

Say for example, if a defect is reported in this spaghetti mess, for example a column may not be accurate in certain specific circumstances, what would be the best practices to start troubleshooting? If debugging this was a zen art, how should I prepare my mind? :) Are there any preferred SQL Profilier filter settings that have been found useful in debugging this? Any good tactics, or is it even possible to set breakpoints in nested procs when using debug mode? Tips or suggestions on providing meaningful debug logging when dealing with nested string builders? 

I recently inherited a MESS of a search. It's around 10,000 lines of code in the procs/functions alone. This search is aptly named the "Standard Search." It's a proc that calls about 6 other procs, which are entirely composed of string builders, in which each proc has between 109 and 130 parameters. These procs also call deeply nested functions which generate more strings to be assembled into a final query. Each proc can join up to 10 views, depending on the logged in user, which are abstracted from the table data by between 5 and 12 other views per primary view. So I am left with hundreds of views to comb through. On the plus side, it does have a parameter to PRINT out the final query, (unformatted of course!) It's a hot mess. The string builders also have no formatting, and they don't logically flow. Everything jumps around everywhere. Although I couid do an auto-format, to pretty print the code, that doesn't help with the formatting on the string builders, as that would involve refactoring the content of strings, which is a no-no. 

It sounds like you need to add another column in your data table to account for the quantity of ingredient. In the example you linked, it appears that the site calculates how many grams each ingredient adds to the total sum of the final product. But it will be up to you to decide how best to generate these. For example, if your site is all about home made juices, perhaps you can come up with a formula to determine how potent or how much flavor a specific ingredient adds to the final product. But I digress. If you have an additional column for grams you could take the weight of one ingredient, calculate the SUM(Weight) of all ingredients in your recipe and divide the two together, like in this Excel example. 

That seems like a complex plan for a view that has less than a thousand rows that may see a row or two change every few months. But it gets worse with the following other observances: 

While profiling a database I came across a view that is referencing some non-deterministic functions that get accessed 1000-2500 times per minute for each connection in this application's pool. A simple from the view yields the following execution plan: 

Create a new String table populated with a cleanly joined set of data from the original base tables Index the table. Create a replacement set of top-level views in the stack that include and columns for the and columns. Modify a handful of s that reference these views to avoid type conversions in some join predicates (our largest audit table is 500-2,000M rows and stores an in a column which is used to join against the column ().) Schemabind the views Add a few indexes to the views Rebuild the triggers on the views using set logic instead of cursors