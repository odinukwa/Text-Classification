From what I've read, a lot of languages that have nominative-accusative marking have only this type of marking. However, languages that have ergative-absolutive marking in some contexts typically also have nominative-accusative marking in other contexts. These are called "split ergative languages." (See the article in Wikipedia for more detail: $URL$ But a lot of people talk about "ergative languages." The article just cited states that "most of the so-called ergative languages are not pure but split-ergative." Are there really any languages that have ergative marking but no nominative-accusative marking? And why, in general, do people refer to split-ergative languages as "ergative languages"? 

Objective hyponasality, the kind that can be measured on a sound spectrograph, is almost always the result of a physical obstruction. $URL$ Even the speakers of languages that don't have phonemic nasal vowels speak with some nasality. So hyponasality is noticeable, for example, even in English. Henry Kissinger is famous for it. So are people suffering from colds and the consequent swelling or blockage of the nasal passages. So it's doubtful that marked hyponasality is a function of being a speaker of say, Cantonese vs. some other language. As for whether Cantonese speakers sound hyponasal to a given listener, that's a subjective issue. 

Apparently, languages differ when it comes to the prevalence of compound words in their lexicons. For example, the fact that compound words are more prevalent in Chinese than in French was mentioned in this article: $URL$ How much do languages vary when it comes to the prevalence of compound words in their lexicons? Are there extremes, such as languages with almost no compound words? (The article above describes Chinese as lying near the opposite extreme.) 

Across West Germanic Languages, what sound changes have been most common since 1000 CE? For example, has there been much epenthesis (vowel insertion) or syncope (dropping middle vowels) or metathesis (transposing sounds, as in flutterby becoming butterfly)? Other historical processes? 

I've been searching the internet for an answer to this question, but found only a reference to "cosubordination" being neither coordination nor subordination strictly speaking. Does "cosubordination" have any relation to clause-chaining? 

You have to remember that all the branches of linguistics are related merely by the fact that they each concern something that shows up in our every utterance. "I eat daisies in the Springtime," uttered by a native speaker, has phonological, morpho-syntactic, and semantic information, and the utterance and its communicative intent and context can be described in terms of pragmatics. // If you think of language as having three broad areas, form, content, and use, you've got phonology & morpho-syntax under "form," semantics under "content," and pragmatics under "use." – 

e.g. We were relieved when the man returned. But do any natural languages mark the verb as being modified by a clause subsequent to the one in which said verb occurs--a clause with the same form that an independent clause has? e.g. We were relieved (infix means "at the same time as" and also conveys that the following clause is adverbial to the one in which the infix occurs) the man returned. 

e.g. "I know that Howard must have eaten here. But do any natural languages mark the verb in the matrix clause as taking a complement clause, leaving the complement clause itself with the same from that an independent clause has, like so... e.g. "I know-[infix that indicates that "know" takes a complement clause] Howard must have eaten here. 

How do tone languages assign phonemic tones to loanwords from non-tone languages? For example, does such assignment vary according to the phonological context in each loanword? Alternatively, does each tone language have a typical pattern of tones that it assigns to such loanwords? Are there such things as strategies for such tone-assignment that are common across contour tone languages? Across register tone languages? Across both types? 

Do clause chaining languages have complement clauses? If so, what syntactic roles can they play? If not, what structures do clause chaining languages have that are commonly translated into English as complement clauses? I've found a fair amount of information about clause-chaining on the web, but almost no information about complement clauses, or structures that are commonly translated into English as complement clauses, that occur in clause-chaining languages. In this article, for example, I was able to find only one reference to one possible complement clause. $URL$ 

First, it's important to remember that the two sentences that you compare are not synonymous: they stand for different propositions. One concerns a plaintiff's claim about who is responsible for the destruction of the car, and another concerns the destruction of the car. To see how different these propositions are, consider this example: If John Doe destroyed the car during his first epileptic seizure, John would still have destroyed the car, but the plaintiff might not have claimed that John was responsible for its destruction. To sum up: you're not comparing sentences that say the same thing using different styles. Second, I think it's safe to say that persuasiveness of an utterance is connected to the listeners' beliefs about what constitutes evidence for or against it. An utterance may be persuasive (or not) depending on how the event described has been documented, how much the speaker trusts the source of information about the event, whether the listener has observed the event itself or similar events, etc. Third, to answer your question, you may wish to consult volumes on Rhetoric, which is the art of persuasive speaking and writing. 

What are some of the difficulties that linguists face in defining the notion of "word"? From what little I've been able to find out, the concept of the word is considered problematic among quite a few linguists. But though I've found some links about lexicology, I haven't been able to find a specific source that outlines some of the main problems. What makes "word" hard to define for the purposes of linguistics? 

"Are modern languages in any non-arbitrary way superior to ancient languages, e.g. easier to write, more expressive, more understandable, more consistent?" I can't address that last two parts of your question, for two reasons. First, I know of no instance in the speakers of language A have more trouble understanding each other than the speakers of language B owing to some grammatical inadequacies of language A. Second, the inconsistencies of a language are not a problem for the language's native speakers. Most people can use even rare and complex language constructions in their native tongues by the time they are five years old. See this article: $URL$ As for ease of writing, there doesn't seem to be a lot of evidence to support the notion that one type of orthography (say, logographic vs. alphabetic) is more easily written than another type. See this post on the Linguist List: $URL$ Ease of learning may be another matter, but I haven't found information about this. As for whether ancient languages are inferior to modern languages in any non-trivial way, we have no way of knowing what the most ancient languages were like. Our species is about 195,000 years old. See the Wikipedia article on homo sapiens: $URL$ Writing systems were devised only within the last 6,000 years. See the Wikipedia article on the History of Writing: $URL$ So, on the evolutionary time scale, so-called "ancient" languages like the first language to be written, Sumerian, might just as well be modern. We don't even know for sure whether language first developed in our species or among our pre-homo sapiens ancestors! Some anthropologists believe that the origin of modern language happened about 50,000 years ago, when humans first left traces of "behavioral modernity" (manufacture of keen-edged stone tools, use of pigment for art, burial of dead, and other behaviors). See the Wikipedia article on behavioral modernity: $URL$ However, other experts believe that "behavioral modernity" began long before 50,000 years ago, and that the evolution of language may have occurred before the human line evolved into anatomically modern humans. See this paper by Francesco D'Errico and a number of others: $URL$ (Disclaimer: I haven't slogged all the way through this last paper: I concentrated on the abstract and what I gleaned by using the finder to locate the word "language" in various places in the text.) So while it makes sense to assume that the most ancient natural languages were less sophisticated than the languages of modern humans, we have no way of knowing what those first, rudimentary languages were like, or how they became as complex and expressive as every language that linguists have studied. 

From what I've heard, syllable-timed languages have syllables of equal length throughout each breath-group (i.e. bit of spoken discourse said in one breath), and stress-timed languages have breath-groups of equal length. By this account, the former entails variation in the length of each breath-group and the latter entails variation in syllable length. However, I could have botched these definitions. Also, I've heard rumors that the timing scheme in a given language can change with rate of speech. I've also heard a rumor that this distinction isn't quite valid. Therefore I'm asking for a brief and authoritative statement about the difference between stress-timing and syllable timing, and whether this distinction is considered valid by linguists. 

I'm only beginning to review phrase structure rules, so let's take a very basic example: "A sentence consists of a noun phrase + a verb phrase." S --> NP + VP Now the NP can consist of "NP + NP," right? Or NP --> NP + NP + NP and so on ad infinitum. How is this noted in the most-used phrase structure grammar notation? 

What is the difference between assertive and non-assertive words? I haven't been able to find an answer in my online linguistics sources such as the SIL Glossary of Linguistics Terms. The only source I could find was an English Grammar site, namely $URL$ This source states that assertive words are generally used in declarative sentences. Examples of assertive words include "some, once, already, somebody, something, sometimes, somewhere, someone." The source also explains that non-assertive words are used " in questions and negatives. They are also used in if-clauses and with adverbs, adjectives, verbs, prepositions and determiners that have a negative meaning." Examples of non-assertive words include "any, anything, anybody, ever, & yet." However, as I read the source, I wasn't able to determine exactly what the semantic or pragmatic difference between assertive and non-assertive words is. I'm hoping that someone can spell this out for me, and/or recommend a source that explains the difference. Also, the link provides an example is given of an assertive word used in a question, namely "Did you want something? (Suggests ‘I think you want something’." 

It would be classified as neither. Icons literally resemble what they stand for. For example, a globe is an icon to the extent that it literally resembles Earth. No such resemblance exists between an acronym and that which it stands for. An index literally resembles an effect or thing affected by what it stands for. For example, a symbol that resembles smoke can be used to stand for fire. No such resemblance exists between an acronym and that which it stands for. Acronyms are as arbitrary as any other conventional symbols. For more information, read $URL$ 

For those who came in late, "perfect" and "perfective" aspects are not the same. Perfect aspect pertains to actions that have been completed at the time referenced by the tense. So English past perfect would be "had eaten," present perfect "has eaten," and future perfect "will have eaten." Perfective aspect, on the other hand, conveys that the action is conceived as a simple whole. $URL$ My question is, do the terms "imperfect aspect" and "imperfective aspects" also refer to distinct concepts? 

All the English speakers in this group are familiar with the use of "keep" to convey persistent action, whether the action is repeated (He kept knocking the ball off the table) or maintained (The ball kept rolling.) Correct me if I'm wrong, but I think that "keep" in this context is an aspectual auxiliary. But for the life of me, I can't find the name of the aspect that auxiliary "keep" conveys. This Wikipedia article, $URL$ names dozens of different aspects, but though it listed English aspects, it didn't contain an explicit referenced to auxiliary "keep". So what's the name of the aspect that auxiliary "keep" conveys? 

What factors affect the number of synonyms a language has? I'd like to leave aside sign languages for this question. When the production of a given sign can be varied in 3-D space, not a lot of synonyms are needed. Does English, which borrows and anglicizes loanwords galore and was spoken throughout a worldwide empire, have more synonyms than Danish, which is spoken over a smaller territory? 

Are complement clauses that contain finite verbs be noun phrases? Consider English's "that" complementizer. A. Clauses introduced with "that" can be be replaced with pronouns. (1) I know that the insects will rule the Earth. --> I know it. I know this. B. "That" clauses can also be subjects, objects, and complements of at least one copular verb. (2) That Erin left Joe bothered me. (3) I heard that Erin left Joe. (4) Sue told me that Erin left Joe. (5) The truth is that Erin left Joe. C. But I can't think of an example in which a "that" clause can serve as an object complement. (6) *I find the music that it sickens me. D. And the choice of prepositions that can take "that" clauses as objects seems restricted. (7) Everything was fine, except that Mary had fallen ill. (8) *The apartment was okay before that the rats showed up. E. And since when can a noun phrase have tense? Part of my quandary may lie in my mis-analysis of some of the preceding sentences. Part of my quandary may have to do with the possibility that the choice of finite vs. non-finite complement clauses in given contexts vary from language to language. 

Many of us know that the term "ontology" applies to the a priori philosophical study of the nature of existence. Ontology is a branch of metaphysics (the attempt to coherently characterize reality a priori in the most general terms possible). So, for example, a lot of rationalists have only one set of things in their ontological inventory (inventory of the kinds of things that exist), namely those states of affairs which can be modeled with the scientific method. Traditionally, such thinkers are known as metaphysical materialists. Apparently,"ontology" seems to have a different meaning in computer science, namely a shared specification of the types of concepts that can be represented, e.g. what kinds of classes, states of affairs, relationships, and I-don't-know-what-else. (See $URL$ What I'd like to know is, do some linguists attempt to specify ontologies (more or less as computer scientists use that term) for natural languages, either specific natural languages or natural languages in general? 

I found a good phrase structure grammar primer, namely "Beginning Syntax" by Thomas, which has been recommended by more than one person on this exchange. However, when I looked for any similarly elementary text for dependency grammar, I came up short. The list of books I found at the following link all look too technical for me. $URL$ I want to find a dependency grammar primer because I heard that it might simplify the description of my conlangs. Any suggested titles or sources? 

What characteristics of a language tend to maximize the average number of allophones per phoneme? Would a small phonemic inventory tend to increase the number of allophones, e.g. in a language with a phonemic inventory comprising /a/, /i/, and /u/, would /a/ more allophones than it would in a language with a 10 vowel system? Would a language with few or no free morphemes have more allophones on account of the greater number of phonetic contexts in which a give realization of a phoneme might occur? What other factors might maximize the average number of allophones per phoneme? 

Though there are some linguists who express hostility toward conlanging, most notably Yaguello in her book "Lunatic Lovers of Language," I haven't heard of any general outcry from the linguistics community against conlanging. However, I do believe that most linguists consider natural language to be the proper subject matter of linguistics. As a conlanger, I think that this is as it should be for several reasons. First, linguists discover, document, and interpret their data. In linguistics, inventing the data is strictly forbidden for obvious reasons. But when I devise a conlang, inventing the data is not only permitted--it is the whole point of my hobby. I think that this makes conlanging something other than a science. Second, a great deal of theoretical debate in linguistics is devoted to the degree to which linguistic knowledge is innate and to the existence of language universals. Since conlangs are almost never acquired naturally and can be designed with or without any of the proposed language universals, I don't see how the mere study of conlangs could have any bearing on such issues. For example, inventing a language without recursion would not, as far as I know, shed any light on the controversy about whether recursion occurs in Piraha. Third, although every good conlanger knows more about linguistics than most non-linguists do, conlanging involves making implicit or explicit whimsical or esthetic judgments about linguistic structures that have no place in linguistics. To a linguist, ergativity is interesting and worthy of study. To a conlanger, ergativity might be a cool feature to put in one's conlang, or conversely a trite feature since all the other conlangers are putting ergativity into their conlangs. A conlanger might also believe that copying natural language ergativity isn't weird enough, and might therefore look for ways of conditioning the ergative/acccusative split that don't occur in natural languages. Another conlanger might also think it would be fun to give ergative marking to some hypothetical relative of Latin. For conlangers, there is no limit to the whimsy. I have often likened the difference between conlanging and linguistics to the difference between painting monsters and studying zoology. There are, I suppose, some zoologists who take an occasional interest in depictions of monsters, but it would not surprise me if most zoologists didn't share this interest.