First guess, your script calls some other command and system startup has a different PATH environment variable. It would be really helpful if you could post the script. Otherwise, all anybody can do for you is guess. 

Other more maintainable methods aside like a domain or a peering WINS setup like joeqwerty is suggesting, I'd probably take the brute force approach. Set up a linux box connected to a trunk port so it can access all vlans. Then I'd script out this: 

What kind of video interface do you have in your system? Some on-board video chips use the system memory for their own uses, making it unavailable to the OS. It's most likely configurable in your bios. 

Restart GDM. Then try connecting again. You should see a login screen where you can enter your username and password. If that doesn't work, try changing to . It may be having trouble with IPv6. 

Ok, here's a quick and (very) hacky way to do this. Short story is, there isn't a way (that I know of) to dynamically do what your asking with the standard Apache tools. Extra modules or code is necessary. Someone out there may have already made a module that does what you want. I didn't go looking. Install and enable mod_perl in your Apache config, then put this block anywhere in your config after the LoadModule for perl. It doesn't have to be in any VirtualHost or Directory or anything like that. 

I haven't used procmail in a while honestly, so I appologize if this doesn't immediately work. First off, the nesting in the script you had was causing problems because it pretty much cut a recipe in half. They aren't necessary anyway, so I removed them. I also simplified the structure and set it up so it will never fall through to the local mailbox. 

I would suggest that you redirect requests rather than make the same site available in two places. Either create a new VirtualHost to redirect traffic: 

Take a look at the bonding driver. It has an active/passive mode that can do what you're describing. It shouldn't require any specific switch support. Here's an old document, but I think it should still be relevant: $URL$ 

Unless you intentionally removed then you do have cron jobs scheduled. Several are set up by various packages as part of their normal operation. You can find them in /etc/crontab, /etc/cron.d/, /etc/cron.daily/, /etc/cron.weekly/, and /etc/cron.monthly/. That sounds to me like was running. finds every file on your server and indexes them for to use to, well, locate files quickly. Crawling through the entire filesystem can take a while depending on what you have on there and it will keep the disk busy for that entire time. 

You might be able to duplicate the loop driver and use a different name and a different major number. That would give you another 256 each time you do that. You'd likely need to be pretty comfortable with at least modifying kernel modules to pull that off. 

Most of the time, e2fsck will not run unless it thinks it should. You can force it in a situation like this. Try adding the flag, like . 

Short answer: Use the flag on your RewriteRule. First, a little Apache backstory. When Apache receives a request, it goes though a process of mapping that request to the local storage. After that, Apache actually has two pieces of data on what you requested. The request url, and the file path. So a request for is translated into and that path is stored separately. mod_rewrite lets you tweak this behavior to route urls where you want. Now, the two RewriteRules you have there are implicitly doing different things. 

Note the difference of the back-reference in the replacement when using . In order to use back-references from both places, use %N for those from and $N for those in the . 

There is a page on Sun's website about this: $URL$ Here's the relevant information for what is likely your problem: 

Again, that's not the right way to do it really, but it looks like it works, at least on my Leopard box. Didn't try it with Snow Leopard. Hopefully they work similarly and I didn't just lead you down the wrong direction. 

will show you the target of a link. Is that what you need? Another option is . Oops, sorry, didn't read everything there. How about . The tells ls to dereference the link, so you'll see the target there instead of the link. 

Try using the switch with sudo. That option sets the $HOME environment variable to the home directory of the target user, typically /root. will use root's .nano_history file instead of your own. 

If you have your rules in a file, then the reason for the not allowed here is almost certainly the setting is too restrictive. Generally it is set by default to allow very little. In your site's configuration in /etc/apache2/sites-available/foobar, add this, adjusting the directory path of course: 

If possible I'd start with opening it while it's running and try to isolate an area. The fans are hot swappable as are the power supplies if you have duals. If it seems to be one of those, pull them until you find the culprit. If it's a fan, you now have a new desk toy. Seriously. I wouldn't even bother replacing it. There's something like a dozen in that box. If it's the power supply, ebay is your friend. HD, give it a shiny new SSD. I don't personally subscribe to "shoot it because it's old." If it is doing what you want in the way you want then keep it. Too much electronics in landfills as it is. 

Try setting RewriteLog and RewriteLogLevel. The log might give you a hint about where that value is coming from. 

You may get a better answer over at superuser.com. This isn't really the right forum for your question. 

This won't work for just any package, but ruby seems to be pretty clean as far as dependencies go. No promises that you won't run into some obscure issue with other packages you might install from apt though. 

I suspect it is the clock or timezone settings at the os level rather than a couchdb or erlang configuration issue. If your clock and timezone is correct (the date command I asked for show that for sure) then I would like to look into the environment that beam is running with. There could be a hint there. I know that's not the answer you're hoping for but from what I can tell, there isn't a configuration for couch or erlang that would control this. The date and time in the log is retrieved from the built in erlang library httpd_util using the rfc1123_date() function. I haven't been able to find anything that would make that return anything but the time in GMT. 

There is a standardized interface. RFC 2136 describes it. The program that comes with BIND implements it. As far as I know most DNS servers support it. You'll have to shop around carefully though unless you're running your own DNS servers, as a lot of providers don't enable it. $URL$ $URL$ 

Unless you have a somewhere, isn't going to match anything. Mod_rewrite looks at the entire url, including the leading /. Try: 

The macports apache comes with a LaunchDaemon file for launchd to use, so the "proper" thing to do is not use the web sharing option and instead run 

The command you tried is to run as an executable, which of course is impossible. The "Permission denied" you're getting is because that file is not set as executable, not because you can't change it. You can see the current setting by reading the file: 

Create the directory /etc/default/grub.d if it doesn't exist already. Create a file /etc/default/grub.d/myextraoption.cfg adding to the variable you want (Append to it only, with an extra space. You want to be careful to not clobber or mangle any existing data there.): 

Usually interface is en0 or en1. To see all the gory details, run before you run the above, then tail /var/log/system.log. After you're done, remember to run to turn it off again. 

Starting BIND with will enable debugging. Depending on your OS/Distro you may need to look for how to set startup command line arguments. You can increase the value if you need more information. If you want more guidance you should probably post your config files in your question, redacting secrets and replacing your names with examples. It is likely something that could be identified here by others looking at your setup. Update 1: I think the error might be that named cannot read the zone file, so it is ignoring the zone entirely, leaving it thinking that it should ask elsewhere but cannot because recursion is disabled. Look at the log file back to when named started and see if that has any hint as to what is happening. 

Are you using DHCP in this environment? If so, I believe you can specify one or more WINS servers as part of the DHCP lease. This is the option in ISC DHCP. This would configure them to use a wins server you specify without having to go touch each machine. 

Everything after is taken to be a command to run for each result, up to the , which is escaped here so that it will be passed to . The is replaced with the filename that would normally print. Once you've verified it does what you want, you can remove the . 

I thought there wasn't a way to do this, but turns out there might be. The CustomLog directive can look for the existence of an environment variable to decide to log the request or not. Combine that with mod_rewrite to set that environment variable, and I think you have what you're looking for. 

This is the one place where I think APT really sucks. To be honest, I'd suggest taking the lazy way out and just install the debs manually. 

Screen recognizes a few extra control sequences that the terminal inside a window can send to affect Screen. Of particular interest to you will probably be the \ek and \e\ sequences. Anything in between those two will be used by Screen as the window title. You can then put the window title in your caption line. $URL$ Give this a try: 

As Dennis pointed out above, it's gzip. Part of the gzip header is a mod time for whatever is compressed in the file. If you need gzip, you can compress the tarfile as an extra step outside of tar rather than using tar's internal gzip. The gzip command has a flag to suppress the saving of that modification time. 

I can't say much about the performance difference between BDB and FSFS, but I can definitely say that FSFS is far more stable. I'd suggest using it over BDB simply to preserve your sanity. When we had a largish repos running on BDB, we had to run recovery on it at least once a week, often several times. It was irritating. Now that we use FSFS, it's been rock solid. 

Is your RAM all the same? I had this happen after I bought more ram and got some that was faster than what was already in the box. According to the specs for the mobo it should have worked with mixed speeds, basically clocking to the lowest common denominator. Each set would work fine by themselves if I took out the other, but together something would happen and while the box would work for the most part, there were clearly problems. I did the checksums just like you described and had the same mismatches. Even ran memtest overnight and had the same result. I eventually wound up just taking the loss of the ram and scrapped the smaller of the two sets. 

First thing that comes to mind is that mod_rewrite isn't on. Just loading the module isn't enough. Somewhere in your config you also need to enable it. $URL$ 

That will ensure it starts on system boot and all that goodness. With that in place, just don't touch the one in the sharing panel. 

You should be able to safely include a yourpackage.cfg file in your package without risk of it being overwritten or clobbering something else. Any of those .cfg files are included after the main default file, so just be aware of that and plan accordingly. You will almost certainly also want a postinst script to run update-grub when your package is installed, and just to be safe since it is in /etc you should probably also include it in conffiles in your package. I think though that this will leave it behind unless a purge of the package is done, so dealer's choice on that part. For reference, /usr/sbin/grub-mkconfig on or around line 157 is what reads the default files, including anything matching /etc/default/grub.d/*.cfg. It seems likely to me that this situation is exactly why it does so. I wrote this based on Trusty. I don't know how far back in releases this is still applicable. I just checked Lucid and it is not there. It is there in Precise. 

I use that as my prompt on remote systems. It automatically sets the window title to be . The escape sequence for Screen is at the end of that command. 

This question's already been answered, but I'll toss in what I do anyway. If all you want is to see if files were created, removed, or changed, you can do this: 

To answer your question though: Httpd has a compiled in concept of where home is, so just moving the stock httpd out of the way and making a symlink to the macports httpd appears to work to trick the sharing panel to start the macports apache instead of the stock version. 

When you access /monitorix in a browser, you are accessing /var/lib/monitorix/www. Your Directory block allows access to /var/lib/monitorix/www/cgi, not /var/lib/monitorix/www. Either change the Alias to: 

The latest version if the ImageMagick port appears to include lcms, so it surprises me that it would build without it. First off, make sure your portfiles are up to date. Run and then . It's possible you have an old version of the portfile where it wasn't enabled. If that doesn't work, run then and watch for a hint in the log about why it's being excluded. If it looks like something in the portfile is responsible for it not building, you can modify it before it runs to change the options it uses. running will show you the path of the portfile. If you edit that file then run it will use the portfile with your changes. Beware, when you run again, it will overwrite your changes. If, after all that, it still doesn't work, I'd probably want to look at the output of and see if anything looks out of place. 

Not with the open source Xen. I don't know about XenServer, though I suspect not. Live migration only moves the memory and state, not the disk. You should shut it down to move it. 

There's no use to including it. If the client browser or library has it as a trusted certificate then it obviously doesn't need another copy, if it doesn't have it then including it isn't going to make it trust it. I have no idea why Namecheap would include it in their instructions. Abundance of caution? It's not an error or spec compliance violation to include it. Your site will work fine with it present. It will however add (very) slightly to the handshake processing time and serves no other practical purpose which is why Qualys includes it as a warning. $URL$ 

Have you upgraded the Mac OS since installing XCode? System updates, major ones anyway like 10.5 to 10.6, may remove those programs from /usr/bin. Download the latest Xcode and reinstall. It should put them back. 

Did you specify dom0_max_vcpus in grub? I saw this issue on my xen cluster for a while and I eventually discovered it was because I had limited dom0 to two vcpus. After I removed that I haven't seen it again. 

As for seeing the source of the cgi, it's probably mod_cgi's configuration. Either it's not enabled or it's not set to handle *.cgi files. You may need to add: 

Or add a set of rules above your others that will detect the presence of the www. prefix and redirect the user: 

Take a look at the ULOG target for iptables, combined with the owner match module. It's not quite what you asked for, but if you create a user specifically for your testing and run only the application you're trying to watch as that user, you should be pretty close. ULOGD, at least as far as I can tell, will write a file with raw packets. I haven't done it myself so I can't vouch for it but it looks like it will do what you want. 

Procmail has several options for delivery so definitely check out the procmailrc man page. It sounds like you are running a local mta in which case the ! and @localhost will pass the message there but you can also forward elsewhere, write to an mbox file like the last example, or even use a pipe to run a custom program to handle each message. 

If the name you're using doesn't match one of the virtualhost sections, it will default to using the first one. My guess would be that you're accessing www.nacc.biz. The www is significant. The name has to match exactly. Since apache doesn't have a virtualhost section for www.nacc.biz, it is using the first one (kadence.tv) as the default. Take a look at the ServerAlias directive, and use it to specify all of the server names you expect to use.