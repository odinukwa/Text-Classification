The only issue with encoding phonological alternations in CFG is unbounded dependencies: in whatever way GPSG manages that issue, that would be required for phonological relations as well. In explicating this, I will speak in terms of "words" though the term "signs" might be better, to generalize over words, larger-than-word chunks, and morphemes. The set of words in a language is finite, and each has a finite length. Therefore each word has a finite set of variants, Wα1, Wα2... Wαi... In the worst case, the set of rules introducing Wα have to be pretty specific, e.g. X → Wα2 Wβ5: there is a finite set of such rules. The realization rules then would be e.g. Wα1 → [don], Wα2 → [dom], Wβ1 → [pan], Wβ2 → [pam], Wγ1 → [kan] and the allomorph-selection rules would tell you to select Wα1 before Wβ1 and Wβ2 but not Wγ1. So X → Wα1 Wβ1; X → Wα1 Wβ2; X → Wα2 Wγ1. A minor complication arises when the triggering word is not adjacent to the target, but this simply requires a few more rules: X → Wα1 Wδ1 Wβ1; X → Wα1 Wδ1 Wβ2; X → Wα2 Wδ1 Wγ1. Phonology has unbounded dependencies. I can't say that I understand how unbounded dependencies are handled in GPSG, but whatever the technology is for obvious syntax, you do the same with phonology. There may be ways to tidy up the system of rules so that you have something which more directly expresses the generalization underlying vowel harmony, for example. Context sensitive metarules that generate sets of rules would be useful. The rules of this system conform to the requirements of a type-2 grammar: A → γ where A is a nonterminal and γ is a string of terminals and non-terminals. (Graphically I used sequences of letters such as Wα1 for convenience, rather than introducing many new letters like "䝿": hopefully it is obvious that the rule "Wβ1 → [pan]" is of type 2, as is obvious if I re-write it as "䝿 → [pan]"). This method can work for absolute, symbolic distinctions, but not for continuous real functions – i.e. phonetic implementation. For example, in the production of "extreme" in English, for dialects with rounding of r, lip protrusion starts at some point before r and increases over time, reaching a maximum at the end of r. Unless you settle for a close-enough finite set of time-instants, you can't expand the grammar to include selection of "...Wα1-at-40msc Wα1-at-41msc..." as an expansion of Wα1, since you skipped infinitely many times that exist between 40 and 41. But, the question asks about phonology and not phonetics, and phonology does not face the problem of infinitely subdividing time: it stops at the segment. In a related vein, Anderson in Organization of phonology revises the SPE theory of feature coefficients so that rather that mapping + and - to 1, 2, 3, 4..., phonetic values range from 0 to 1. There is no representation of time in this theory, so no issue of continuously subdividing time. If Plougrescant Breton has rules that assign [0nasal], [.3nasal], [.5nasal], [.7nasal] depending on context, the phrase structure rules can simply be expanded to select [.5nasal] in the relevant context. The number of actual real values is determined by the number of rules, and it will be small. though more than 2. In fact, there isn't a compelling argument for using reals, and if you multiply his phonetic reals by 10, you get integers – nothing that he says depends on phonetic feature coefficients being reals. 

It's a "tag". Here is a paper on tags. Armagost 1972 English declarative tags, intonation tags, and tag questions is a good introduction, IMO. Your examples are "declarative tags" (section I), and there are other types (You won't, will you?"; "You won't, wont you?")". Basically, tags convey pragmatic information, about speaker attitudes and presuppositions. 

The fundamental difference is that generative grammar purports to be a model of mental processes and (quasi-classical, non-Sapirian) structuralist linguistics denies that or is agnostic. Technically, GG is a perfectly explicit description of the competence of the ideal speaker-hearer (Aspects p. 4), but then there isn't much GG around, given the "perfectly explicit" desideratum. Also, bear in mind that many people equate GG with "Chomsky's current theory of syntax", which is a misunderstanding of the concept. 

Since "small person" doesn't mean "child", this indicates that you're willing to put up with phrasal idioms, like "kick the bucket", where the meaning of the phrase has to be learned separate from the meaning of the individual words. In that case, it does not matter what the meaning of the 20 words is. The words can therefore be ta, ti, te, to, tu and so on. The word "pu" uniquely means "end of phrasal idiom". Then, "ta pi ko pu" is an idiomatic phrase meaning "cow", "pi pi su pu" is an idiomatic phrase meaning "grass", and so on. If you want to impose a requirement of strict semantic compositionality at the phrasal level (a draconian requirement not obeyed by any actual language), then you wouldn't be describing a human language, since human languages don't have vocabularies of only 20 words. I don't know how the concept of "expressiveness" could apply at all to such a system. There's no metric of expressiveness, anyhow, other than "unbounded; infinite". 

This is a long-standing feature of various dialects outside the US. In the US, it was most strongly associated with Valleyspeak, although it is also a long-standing feature of North Dakota and Minnesota English (originating in Norwegian). It is now fairly standard across US dialects, but age- and gender-associated. 

This is a currently non-existent but potentially interesting specialization in linguistics. The design of the experiment would not be very difficult: make good quality recordings using a head mic, standardized recording parameters and multiple speakers, speaking normally (according to norms of that culture, which is the main thing that matters) – don't normalize amplitudes (since that would obscure a feature of interest). Then overlay environmental noise of differing characters, and see what the effect is on listener comprehension. If Bella Coola turns out to be harder to hear than Cantonese, Bella Coola speakers can adjust by talking louder, so there's no insurmountable problem regarding noise. Apart from cultural norms regarding how one should speak, a language like Hawaiian is probably one of the most robust against noise because it has only open syllables and a rather restricted phonemic inventory, so less chance of noise overriding the signal. A language with significant consonant clusters and lots of consonant types would be most at risk: candidates include Berber, Kartvelian and Salishan languages, Polish and Mongolian. In the case of Berber, some of the languages have epenthetic vowels and others don't, where the vowels can make the individual consonants more audible. Another possibility would be languages like Vietnamese and Hmong, which rely heavily on phonatory differences on vowels as part of their tonal system (vowel phonation contrasts are not very robust w.r.t. noise). 

The largest number of levels (actual level tones, not contours) is 6, in the language Chori, documented in his 1976 U. Wisconsin dissertation Aspects Of The Tonal Structure Of Chori. He proposes rules for deriving 3 of these tone levels from underlying tone sequences, but those rules are not surface allophonic rules (the conditioning tones are often not actually present), so the "other" 3 levels can't be dismissed as just the result of phonetic implementation. However, there are only 3 underlying tone levels associated with lexical items. 

Pharyngeal, epiglottal and glottal frictives are produced lower in the throat that the palatine tonsils, which is presumably what you're looking at. A uvular fricative would not be fully behind the tonsils, but would be at the tonsils. 

I think I have a bit better handle on what you're looking for. At a linguistic level, each segment is representable as an integer where the nth bit is the + or - specification of the nth feature. The difference between any two segments is thus computable as the number of bits difference between the segments, and the "distance" is the number of different bits. So [b] and [p] which differ in exactly one feature have a distance of 1; [b] and [φ] differ in two features so have a distance of two. From this, you could generate "all the pairs that differ by 1; all the pairs that differ by 2". You can also compute differences in classes, for example [ptk] compared to [bdg] by masking off the bits that are not identical within each class. For English spell-checking, this could be useless, but it might underlie the North Saami spell-checker, so ultimate utility depends on why you really care. [Addendum] Going beyond just answering the question that was asked, here are answers to related questions that maybe the OP wished he had asked. One is whether there is a network of segment confusability relations, which addresses a perceptual question. You can approach the question generally or language specifically. The general form of the question is "which two segments as pronounced in language X are most likely to be 'indistinguishable' for any human (with normal hearing)?". Thus you could inquire of speakers of all of the world's languages how hard it is to distinguish [l] and [l'] in Lushootseed. The language-specific form asks about indistinguishability where the subject pool is speakers of language Y (Y could = X). The most language-specific version, also the least fraught from an experimental perspective, is the case where Y=X – ask speakers of a language to distinguish sounds of their own language. This raw form of the question has an invalid assumption, which can be eliminated, that it's just the segment that matters – in fact, the implied problem is related to the context that segments appear in. You need to control for contexts, so that you compare "rip, rib; writ, rid" and not "rip, rib; tie, die". The standard way to do this is find word pairs differing in one sound, and fuzz the recordings with noise to obscure the segment in question – the test is whether speakers can correctly identify the spoken word. You need a big sample to cover the field of "contexts". The result will be a table of segment pairs in a specific context, and you can decide what kind of theory of "context" you want to adopt. (Example: a linguistically-motivated distinction for consonants would be "after a short vowel when at the end of a syllable", which might generalize to "after a vowel when at the end of a syllable" or maybe even "after a sonorant when at the end of a syllable"). This doesn't carry over to perception of X by speakers of Y, or to any perception of Y, and it doesn't work very well for English unless you narrow the population to a specific variety. You can't come up with anything valid for all languages, but you could do a reasonable job for one language. I don't think this has been done even half-definitively for English, but it should in principle be doable in less than a lifetime. 

The arguments for Feature Geometry are set forth in Clements' 1985 paper 'The Geometry of Phonological Features'. By way of conceptual background, the question of how/whether features are organized was current in much work in autosegmental phonology, once it was realized that in principle every feature could exhibit "autosegmental autonomy". A simple model (the 'bottlebrush model') had been contemplated, that all features link directly to the CV core. For the CV tier itself, see Clements & Keyser CV phonology and references therein. The problem is that features often act as a class, hence we find "place assimilation" rules in languages. The formal theory did not treat simultaneous assimilation of [anterior, coronal, back, distributed] as any different from simultaneous assimilation of [anterior, voice, continuant, lateral], but while the former is well-known, the latter is completely unattested. The theory also makes it more complicated to perform total assimilation of place in a language that has many place contrasts (such as Malayalam) and simpler to write a rule that only partially (and depending on formulation, bizarrely) assimilates place features. The defects were identified in an unpublished paper by Mohanan ('The structure of the melody'), who identifies the fact that features group together functionally, and proposes the essential concept of "class node" which he calls "feature block". Clements expands and refines on this in the 1985 paper, assembling a set of "functional unity" instances, for instance preaspiration in Icelandic, Klamath nasal-lateralization and debuccalization. By way of important prior results leading to the theory, it had been previously established in many papers that partially and fully assimilated consonant clusters exhibit a kind of special phonological integrity labeled integrity and inalterability, and there was a structural proposal (about crossing lines) which required having "an association line" between e.g. m and p in mp, whereby an epenthetic vowel could not cross that line (thus /mp/ does not become [mip] via a general cluster-busting epenthesis rule). Feature Geometry thus provided the last logical stop in deriving the integrity / inalterability facts. 

Another sense lake n.3 is "A small stream of running water", is attested as OE lacu, citing the attestation from 955 Charter of Edred in Earle Charters 382 Ðæt to Mægðe forda andlang lace ut on Temese. This they say is not from Latin lacus because of the meaning, and they derive the word from the "leak" root *leg. A half-vote also goes to Etymologyonline, which lists both roots, says that the modern word is borrowed, and gives both Old English lacu "stream, pool, pond" and lagu "sea flood, water, extent of the sea" (lagu being the OE form cited in Pokorny). In other words, the weight of evidence indicates that it is borrowed (in some fashion, including adaptation of semantics), the source would be French (hence Latin), and that it is the result of two distinct roots. 

You should take any such claims with a large grain of salt. While there is no question that Toda has a contrast, it is not evident that the phonetic realization is [ɕ] vs. [ʃ]. Ladefoged & Maddieson in The sounds of the world's languages 156ff list the Toda sibilants as [s̻ s̱ ʃ ʂ] and [ɕ] is not listed. They also list the Ubykh sibilants as [s ŝ ɕ s̥], without [ʃ] (they state that [ʃ] does not occur in Ubykh and Abkhaz). Although the wiki entry on ɕ lists Norwegian as having that sound, Kristofferson in The phonology of Norwegian lists it as [ʂ], and the wiki entry on Norwegian likewise does not include [ɕ]. Ladefoged & Maddieson put their charcoal where their informants' mouths are and substantiate the claim that Toda has [s̻ s̱ ʃ ʂ]. I argue that to make that specific a claim (that a language has [ɕ] rather than [ʃ] or [ʂ]), one must use some articulatory-measurement method, or, one must be Ian Catford or Peter Ladefoged (both of whom, alas, have another thing in common). Edit: corrected ɕ/ʂ typo 

If the same word can always be pronounced two distinct ways with sound X or sound Y, "tomato" and "to-mah-to", that is known as free variation. If two distinct words are distinguished by a single difference in sound, then the pair of words is a minimal pair, and the two sounds are distinct phonemes. But "tomato" and "to-mah-to" are not distinct words, they are the same word, pronounced different ways. (Actually that's a terrible example because it's a song lyric, not a linguistic example, but there are analogous examples like the first syllable of "economic" which can, for many people, be either [ɛ] or [ɪj]). The nuclei [ɛj] and [ɑ] are not in free variation in English, see for example "date" and "dot" or "sate", "sot" and "sought" (the latter two because the vowel of "tomahto" is either the vowel of "sot" or that of "sought", depending on how you neutralize the difference, if you do). Uncontroversially, [ɛj] and [ɑ] are distinct phonemes as established by minimals pairs. In fact "tomato" ~ "tomahto" is non-systematic and doesn't reflect a rule: they come from different linguistic systems. Likewise, "pasta, Mazda" are pronounced with [æ] north of the border and with [ɑ] south of the border, so these vowels are also not in free variation. There is no potato ~ potahto variation outside of humor. Such two-pronunciation cases generally reflect competing strategies for dealing with spelling when there are multiple possibilities for pronunciation. 

It is not arbitrary, but it is very theory-dependent. One popular criterion for affix-hood is that affixes tend to affix to a particular word-class, thus if you treat "an" as an affix, you would expect that it only attaches to nouns (for example); but in fact it attaches to anything that can be on the left edge of an NP ("an old apple; an enormously expensive apple", etc.). However, there are "edge-inflections", where a certain feature is realized on the left or right edge of some constituent. From what I can tell, word / affix distinction is nonexistent in contemporary Minimalist approaches. Arnold Zwicky has written a fair amount about the problem of "words" and affixes. 

Insofar as Linnean binomial nomenclature is in Latin and they are cooking up new names all the time, yes, and apparently it is expanding to galaxies. Here is a list of Latin computer terms (English to Latin translation). As with English, there isn't a single pronunciation (< principi > = [printʃipi, printsipi, priŋkipi]), so expect variation. For example, this newscast is Latin filtered through Finnish phonetics. 

I'm not aware of any work on this topic in linguistic phonetics, but there may be something out there in musical acoustics for voice. The main problem for quantifying inharmonicity is detecting harmonics exactly, and the main issues can be seen using Praat. Frequency information comes in fixed-width bins which is inversely related to the length of the analyzed signal: the longer the analysis window, the smaller the frequency step between successive frequency computations. Each glottal pulse is fairly short, say in the range of 8 msc, and an analysis that looks at the harmonic shape of just one glottal cycle will have a much larger frequency step. A large frequency step means higher uncertainty as to the actual frequency which a reported peak corresponds to. Comparing an 8 msc window to an 80 msc window, the shorter window has a frequency resolution of about 86 Hz whereas the longer window has a frequency resolution of about 11 Hz. In the former short window, it's not practical to try to identify harmonics. Even increasing the window length to 16 msc (about 2 glottal pulses), the identified "peaks" are way off from integer multiples of the fundamental (about twice the actual multiple). With an 80 msc window, frequency resolution is mathematically pretty decent, but even then you will get apparent inharmonicity, because of the limited frequency resolution of the apparent fundamental. If you look at a higher harmonic such as the 10th, the apparent peak should occur at 10 times the frequency of the fundamental, give or take one-tenth the frequency resolution (so about 1 Hz for an 80 msc window). The computed frequency of the fundamental, on the other hand, is that apparent peak, give or take 10 Hz. Autocorrelation methods for pitch extraction can solve that problem, as long as pitch changes negligibly within the window. What you will probably find is not that higher harmonics are "off" relative to the fundamental, rather, the harmonic peak of higher harmonics occurs a multiple of the fundamental, and the lowest peak that supposedly reflects the fundamental is off a bit. With a piano string or tuning fork, you can strike a note, sample it and get a long analysis window with excellent frequency resolution (assuming that you don't have gremlins jumping up and down on the vibrating string). Human speech is full of acoustic gremlins, so the longer the window of analysis, the more "noise" you're going to introduce (especially variation in the shape and duration of individual glottal pulses). This may be more controllable with professional musicians performing sustained notes, but that is not speech. Since perfect vs. imperfect harmonic series is not linguistically salient, I think you won't find any linguistic phonetic literature on this. You may find something in the realm of speech pathology and dysphonia. One way to overcome the frequency-bin problem is to (very carefully) select a small number of cycles – two – selecting the zero-crossing at the same point of two successive cycles, then concatenate a number of copies. You will get spurious harmonics (every even harmonic, I think, will be a lower-amplitude artifact) and also you have to define "peak" in a special way (it should be the midpoint in the series of "peak-ish" numbers, but that point may be a negative number with the set of dB values). But, the frequency resolution will be improved to the point that if F0 is 110, you'll get harmonics at every integer multiple of that. This minimizes the changes over time that characterize speech and introduces fewer artifacts (esp. compared to iterating a single cycle). I don't know of a published source on this, but experientially, I know that synthesized complex waves built from sine wave components of integer multiple periods sound mind of mechanical. A bit of variation in the components (within a cycle and between cycles) makes it sound somewhat more natural. I expect that too much variation has the opposite effect.