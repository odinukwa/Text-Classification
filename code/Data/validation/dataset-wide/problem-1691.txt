Regarding howto replace the controller (if it should be needed). The PERC5 does indeed store the volume configuration on the disks themself. When you replace the controller it will just show all drives as "foreign". Inside the RAID BIOS there is a menu "Foreign Config" which allows you to import the config from the drives. This process works pretty flawlessly IMHO. 

After some research I know that RDP atleast was unsafe in earlier Windows versions (2003) and AD seems generally to be considered unsafe. Is it still a wise idea to tunnel RDP over SSH (the servers all run cygwin)? Thank's for your advice! 

Coming from a Linux background and now having to manage some Windows servers (2008R2 & 2012R2) I'm wondering which services are safe to run directly over the internet. The servers are all internet facing without additional hardware firewall or a internal VPN management network. The services in doubt are: 

I guess you are already aware of it, but the setup you are looking at, is badly broken to begin with. How can a server with 5 physical disks be running two RAID5 arrays? You need atleast 6 disks for that. You are missing one disk to begin with. 

My interpretation of the above output is, that this domain should be resolved to my server after a maximum of 3600 seconds (as I'm changing both, the NS and A records). As I did move the above domain, 3 hours after receiving the TRANSFER ACK it is still resolving to the wrong DNS server: 

To give an anecdotal response; the 840 Pro did work on the H310 when I tried them. The problem is, especially the firmware of the H310 has a ridiculous low queue depth preset and because of this may (depending on your work load) suffer from a terrible I/O performance. You have a chance to change that, through cross-flashing the controller to get rid of the PERC firmware. The controller is an OEM version of the LSI SAS 2008 and can be reflashed to LSI mode (queue depth 600 if I remember correctly). Another possibility is to get a proper controller (PERC H700/H710 for example) - those also work with the 840. EDIT Here is a good comparison between various controller queue depths. The H310 has a queue depth of 25, while for example the H700 has a queue depth of 975. 

Adding to the other answers (they all have valid points - bandwidth is important here, too!). A quick websearch revealt that the NAS you mentioned seems to have a Marvell 5281 CPU running at 500MHz and 128 MB of RAM. While technically rsync can handle multiple connections, I would suggest you don't try to run all the jobs at the same time, but instead leave a gap between the jobs that is long enough for each job to complete before the next one starts, so that ideally no jobs run simultaneously. This hardware doesn't look strong enough for me to feature 20 simultanous connections. You'll of course need to calculate this offset based on your available bandwidth at that location and the overall write performance of your NAS. 

So as you said it still boots to the windows login screen I assume with the exception of the error message you posted the server is running fine? 

I now attached the USB drive to a VM with FreeNAS to mount / or repair the fs on the stick with fsck. After running once, the partition seems to be totally destroyed (can't even rerun fsck as it just gives ""). So I guess the fs can't be repaired. That's why I'm now trying to retrieve the config . I mounted "" but "" just gives: 

In the end nothing to worry about - pull a working BBU / controller from another server or get yourself one on ebay. Dead BBU batteries are a really common problem with any controller (the lithium ion cells just wear off with the years) and the reasoning why most RAID controllers use flash-based cache and no BBU nowadays. 

The certificate chain of your certificate is incomplete. Most likely your provider failed to install some intermediate certificate when installing the new certificate. Most times such intermediate certificates are provided by the SSL authority, to provide support for some older browsers and operating systems. That's the reason, that while it works for you, it doesn't work for some of your clients. An really great utility to check for SSL issues with your website is the SSL Server test by SSLlabs. As you can see in the link above, not only are you having a chain issue here, but also the signature algorithm used to create your cert is a weak one, your webserver is still vulnarable to the POODLE attack and still supports RC4, which is also considered unsecure ... I don't want to say anything against your webserver provider, but in your position I would mail them, that they fix all this issues ASAP, or change to another provider ... 

I'm trying to figure out how TTLs are handled in DNS (in the end I'm trying to figure out the fastest way to switch over a domain), but somehow I'm stuck in interpreting TTL values. For example I moved a domain today which had the following DNS records before moving it to me: host -a example.com ns.old-provider.net 

After migrating to a new VPS I had some users complaining about slow loading images on their sites. After creating some test files with dd I realized that I can download all files via sshfs with full speed while downloads via web are painfully slow. The larger the file is and the longer the transfer takes, the slower the transfer speed gets. I thought I had some problems with Apache and just spend the whole evening with replacing Apache2 against nginx for static file serving - with no effect at all. No I/O wait states in top. Tons of RAM free, no high CPU utilization and hdparm shows a decent I/O performance at all times. I just have no idea anymore, what's happening on this server. This is a link to a demo file: $URL$ Anybody an idea what I can check out? 

Finally found the problem. After having read so much about the Wordpress botnet, I don't know why I didn't thought about that in the first time. All of the pools are Wordpress sites and they are beeing hit at random times by the botnet all at the same time which makes the server freak out. I now installed (I'm pretty sure I also had this done on the old server ...) and also lowered and it seems the server can now survive an attack without the need to reboot. 

The bad news You are running a somewhat aged system. Your CPU is 6 years - or 9 generations - old and even at that time was the cheapest entry level model of it's series. Even with virtualization extensions (VT-x) enabled, I fear that your system will still be somewhat slow. That's because the first generation of virtualization instructions supported by your processor don't have a really huge performance impact anyway. The performance gain in using hardware features mostly came with the Nehalem CPU architecture (4 generations later) and the VT-x EPT instructions. So while it's generally favorable to use KVM over Qemu for the sake of hardware accelaration, I don't think you'll really gain a lot of performance switching to any of the other hypervisors. The good news As you are running an pretty old system the prices for spare & upgrade parts have basically reached their bottom. An decent upgrade for your Xeon 5110 is for example the Xeon 5160 (1,6 vs 3 Ghz). It's the fastest model of the Xeon 5100 series and sells for eBay at 2$. Matching memory upgrades (DDR2 FB-DIMM) for this processors are also dirt-cheap. Depending on the revision of your Precision (it will work on second generation 490 and the T5400) you might even be able to run the first generation Quadcores (Xeon 5300 series) - but the 5160 is a safe upgrade in any case IMHO. Disclaimer You should be aware that even with upgraded processors the system is still no rocket, but you should be able to see a 100% performance improvement over your current situation for almost no money. I'm running a similiar rig in the moment (with dual 3 Ghz Xeon 5160 + 12 GB DDR2) using VirtualBox to run the free modern.ie Windows images ontop of Ubuntu 12.04. It's pretty decent/useable, but very far from being "fast". I don't need it that often so that's ok for me. 

In a working install the config file is within that folder and is an ordinary subdir of the folder. Is the file lost? Any chance to get it back? 

RDP (in standard configuration)? MSSQL (2012)? Active Directory? WSUS (without domain if 3. is unsafe)? 

Finally got the file. Didn't realized /data is mounted from the 4th partition and is not part of the root filesystem. Unfortunately, it's corrupted and can't be opened and sqlite doesn't seem to have a good tool to repair corrupted databases. So I'm finally out of luck with this. 

I would assume your second Logical Drive is running in degraded state since a few month as someone already took a broken disk out without replacing it. The "INTERIM RECOVERY" state means a disk is missing/broken, but due to the redundancy provided by the RAID controller, the data is still accessable for now, until a second disk is going to break. If the data on the second Logical Drive is important to you, I would advise you to boot the system with a live CD and copy the data of to a safe location as soon as possible. In the long run, you'll need to add the third disk back to the server, so the array is able to rebuild and return to a healthy state. 

As you can read, the cache is used to further enhance the write-performance of the array, but this does not seem to have any impact on the performance of any subsequent write or read operations. Regarding disk-fragmentation, this is a file-system/OS level problem. The RAID controller - operating on the block level - isn't able to optimize file system fragmentation at all, so there is no difference if it operates in or mode. 

If you are running in synchronous mode (O_SYNC) your Result A seems therefore to be reasonable in terms of what can be delivered via soft/fake-RAID. 

That should be possibly if we assume the drives are really OK as shown in your screenshot. You could also try to start from your HP SmartStart CD and run the Array Diagnostic Utility. The ADU log will show more details then the screenshots you posted. It may be possible, that you are able to simply reenable the Logical Drive in the ADU, for example if the reason for the failed state is a temporary problem with detecting the drives. 

The main difference between your two choices is 3 year warranty vs 5 years and SOHO usage area vs. certification for datacenters. In terms of raw I/O performance, there is not much of a difference between 5400/5900 and 7200 rpm S-ATA disks nowadays (that was different a few years ago). In the end therefore the array with more disks will very likely show a superior performance, as RAID10 scales with the number of disks. What I would guess is more important in this setup, then the number of drives is, get a proper hardware RAID controller with BBU+Cache if you can afford it - or even better go with SSD only from the start on. Also please be aware, that ESXi natively only supports hardware RAID! While there are workarounds to use software RAID with ESXi, I personally wouldn't advice using these, if you are looking for a decent performance. Protip: If you are on a really tight budget to self-build this storage server and we are talking about a non mission-critical system, then go fetch yourself an OEM version of a proper RAID controller (IBM M5015, DELL PERC H710, etc.) on ebay. 

Regarding the measured 33x difference between your results, following up on our discussion in the comments, it turned out, that showed that setup B had the disk drive cache enabled by default, while it was disabled on setup A. Using resulted in both systems showing a similiar performance. Is it safe to run the RAID with disk drive cache enabled? Running a RAID with disk drive cache enabled is actually similiar to running a RAID controller with non BBU backed volatile cache with write caching enabled (forced write-back mode). It enhances the performance, but at the same time increases the possibility of data-loss and data-inconsistency in the event of a power failure. If you want to avoid this chance, while still having a decent I/O performance, it is advisable to have a controller with BBU backed-cache and to configure your volume to write-back mode with disk caching disabled. The difference between your two RAID controllers I don't know if you already knew, but there is more between software and hardware RAID (this is an interesting article regarding this). In the end the MegaRAID SAS 2008 is more or less an HBA or IO-Controller with added RAID capability, while the MegaRAID SAS 3108 is a real RAID Controller™ (also called ROC or RAID-on-Chip), which has a dedicated processor for handling the RAID calculations. The SAS 2008 is especially known for horrible write performance with some OEM firmwares (like the DELL one in the PERC H310 which I mentioned in the comment). Especially the synchronous mode in combination with your chosen record length and file size seems to result in really poor results with software/fake RAID. For reference, this is what I get on my workstation using 10k WD Velocity Raptors in software RAID1: