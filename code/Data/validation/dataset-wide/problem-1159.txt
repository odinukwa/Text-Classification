To import data into a different schema you need to grant role to the user. Oracle MOS Doc ID 351598.1. Or you can use SYS user: 

For that you should configure SQL*Plus output. Documentation what can be done and how to do that is here. In your case it is: 

Now you can unconfigure and remove old machines from old cluster. Of course Oracle version on new machines has to be the same as on old machines. Or it is possible to do upgrade right away on new machines. You should test procedure of course. There are quite a few possible problems on the way. The idea is that Oracle DB does not store anything in "cluster". All the data is in datafiles, controlfiles, redo logs and spfile. Which is stored on ASM can can be mounted on another server. 

I was using the following steps to perform backup and restore. In the first step I generate dump script to make schemes structures backup. 

For Oracle client you can use Installation using Response Files. Your C# program can generate response file and run installer with particular command line options. 

I have column of type bytea in PostgreSQL database. If I update a row, is it possible to chunk the data into the bytea column in several small steps? The size of one bytea entry will be mostly below 200MByte. The chunks will be about 1MByte. I use psycopg2 and Python to access the db. This is a follow-up question of Store HTTP response in PostgreSQL 

I want to understand a database layout. It has 180 tables. I only know the high level use case: Owncloud (File Hosting Server. Can be compared to DropBox). How to understand the database layout? How to identify the most important tables? Is there a tool which helps me to understand the layout? At the moment am interested in two facts: 

"Avoid redundancy" is important to me. I want to store the input data for configuration management in a relational database. My input data: 

$URL$ row2: I think every person has exactly one date of birth. I don't understand this row4: I think every person has exactly one birth place. I don't understand this. What am I missing? 

I have a simple table with columns col1, col2, col3. All not nullable. I want to delete all rows where the tuple (col1, col2) has several entries. Background: a unique constraint for (col1, col2) should be added. 

Then you would not get wasted space in tablespace and also even in case your data grows fast and catches up with autoextend operation - there will be only small delay in writes because 10GB will not take a long time to format. And Oracle will autoextend tablespace proactively. It will not wait for the last blocks to fill up. 

Performance will be very similar. GHz difference is not essential because e.g. E5-2690 has 3.8GHz Max Turbo Frequency vs 3.5GHz on E5-2643. You can find full comparision here: $URL$ The big thing may be licensing. If you are using Standard Edition or Standard Edition One per CPU licensing they are licensed per socket. So in case you buy second E5-2643 you will also have to buy one more Oracle license. Of course that it not the case with the Enterprise Edition. If licensing is also not an issue then you still have two decide between two choices: 

Maybe db_cache_size, shared_pool_size, sga_target or other memory related parameters are set to non zero? Remember that when using AMM those parameters specify minimum memory allocated for particular pool. So if sga_target is 6GB you will not be allowed to set memory_target to 4GB. Also sum of internal variables __sga_target, __db_cache_size, etc. may be more than your specified value of 4GB. If you see those symptoms you can cleanup pfile bounce Oracle with pfile and recreate spfile. In the same step you can also set to zero. 

I want to check if our code does use only indexed columns. I think it does, but I want to be sure. How can I configure/modify PostgreSQL to raise an exception if it needs to do an sequence scan? It is ok, if PostgreSQL wants to do an sequence scan, but an index exists. Then I run our software tests to see if an exception gets raised. 

After pg_upgrade one check fails. This SQL gets executed to list tables and their sequences. In comments to this question it is called "above sql". 

I read this article about PostgreSQL performance on SSD: $URL$ These two configurations seem to be important vs Since both parameters need to match the particular hardware I wonder if it is possible to automatically detect the matching values? Update I have these steps on my mind: 

The size of the our PostgreSQL data directory is 100 GByte. Up to now we use traditional hard disk drives and hardware RAID 10. Which hard disk drives and RAID setup would give us maximum performance? The budget is at maximum 5000$. The load is like in most average databases. Maybe 1/5 writing (update or inserts) 4/5 reading (select). 

What does alert log says at that moment? From my experience - in almost all of the cases when Oracle's takes very long time (10+ minutes) or does not finish in reasonable time running DBMS_JOBS jobs are the problem. Don't know if DBMS_SCHEDULER jobs would also inflict such behaviour. So the best thing before reboot is to disable jobs and kill running jobs. No problems with shutdown then. 

Now if you look into your output you can notice that only backupsets with archivelogs has tag FIRST. Backupset containing backup of datafiles has autogenerated tag TAG20150515T105436. 

If it really shows here you have two options. You can simply ignore it. Once the block will be assigned to some object it will be reformated and corruption will go away. If you want to fix corrupted blocks then you will have to create the object which would occupy your corrupted blocks. Let's say your corrupted block exists in tablespace and datafile . First you have to make datafiles of tablespace not autoextendable. That is not to inflate datafile size. And then you'll create filler table. 

Take backup of NDB tables structure with mysqldump. Make NDB backup. Restore mysqldump backup on the new cluster. Restore NDB data using ndb_restore (on both datanodes). Rebuild indexes (command has to run just on one datanode). 

Above works on PostgreSQL 10 but fails on older versions. Older versions tell me this error message: 

Unfortunately the result is not what I was expecting. Although there are more than 150 locks, pg_stat_activity shows only few (less then 10) queries. This has happened about twice a day during the last days, and every time only few lines where returned by pg_stat_activity. What is going on here? How to debug the current DB state if there are too many locks? 

We use check_postgres.pl to monitor our database. We use this to check the count of the locks: $URL$ We often see more than 150 locks. The question was: What is going on? We patched the script to output this sql statement, if the lock count was exceeded: 

One work-around: Change the port of PostgreSQL on . This way it is unlikely that clients connect to the DB during . 

If or gets updated, then the table should get filed. The ticket.id of the changed ticket should get inserted into the transfer table. Here is the working code: 

If I fetch only one row, the query takes much longer: 1433 ms vs 23 ms Is there a work around? Slow: 

Also I am guessing that you are running database on disks with write caching enabled. Or RAID controller with write cache enabled and no batteries or flash to preserve unwritten data. Do not do that if you value your data! Quite probably you will get some corruptions in case of crash reboot e.g. in case of power loss. 

Ultimate source for such answers is Oracle Database Licensing Information. Sadly to downgrade from Enterprise to Standard Edition you have to export all the data install Oracle Standard Edition and import data (Doc ID 139642.1). From your list "Automatic SQL Tuning Advisor" is Enterprise Edition only feature. As a rule of thumb - features which needs AWR are Enterprise Edition only plus you need to buy . 

Everybody here are missing one point - SQLs in question are retrieving 1000 rows. And one cannot retrieve 1000 rows fast unless most of the data is cached. If data is not cached one has to do 1000 random reads sequentially to retrieve the data. Good disk based storage with fast disks will give you up to ~200 reads per second. Common disks even in RAID I doubt that manage even 100. That means 10+ seconds to get results even with the best indexes. So in the longer run such data model and queries won't work. Now the queries run fast when you hit cached data. 

If I have a simple ascii list of both sides, I can use a diff tool to compare both db schemas. I found ways to output all columns like above but the solutions don't support ordering by the schema of the table. Question How to show the difference of the column order in two databases? Other strategies are welcome. Implementation: I use postgres 9.3 

How can I avoid redundancy if I store this in a relational database? Unfortunately these values are not always equal, only in most cases. I use PostgreSQL 

I am missing the ability to sort the output. I would like to avoid dirty shell scripting, and use SQL wisdom. How to get the N SQL queries into PostgreSQL, so that I can run ordinary SQL on it? (Sub question: Is there a matching tag for this kind of question? I mean questions where the SQL spans several databases?) 

Since I only used plpythonu up to now, I solved it like this. It works, but I see two things to improve: 

I search for a simple way to select all columns except one in psql. With I mean the interactive command line. I would be happy with a tool that expands to to a list of quoted column names. Then I could remove the column to remove by hand. My question is just about the interactive usage of psql. It is not a duplicate of questions of people unhappy with the sql standard and who want to execute something like "select *-foo". 

This instruction is for Oracle Linux. And you are using Red Hat Enterprise Linux. Even though Oracle Linux is compiled from RHEL sources but they have separate base repositories. You will have to resolve dependencies manually. I tried to download preinstall RPM and install it with yum but it requires RPM which is not available on RHEL. Even if I am not the fan of what Oracle does with RHEL but for Oracle databases I switched from RHEL to Oracle Linux. Less resistance this way. Oracle ASM kernel modules are not available for RHEL and Flash Cache feature is available only on Oracle Linux. 

Even such query counts as scan if id field has btree index. In your case we almost all DELETE queries are running in loop with 'LIMIT 1000' or similar value. 

30min for 1TB is quite normal. That all writing stops is also normal if your tablespace completely run out of space. When writes has nowhere to go they have to wait for the RESIZE operation to complete. If one extends datafile while there is still space in it database I/O does not stop. Just why you extended for such huge amount? Now those +1TB will add to your RMAN backup. Of course they will compress well but still not 100% and RMAN will need time to read all those blocks. I would setup autoextend on those tablespaces: 

In my environment most PostgreSQL queries are generated from http requests. In my case every http request has a unique header: X-Request-ID If there is a error or warning in the PostgreSQL logs, I would like to see the related X-Request-ID to see where the sql query came from. How to add an extra message like X-Request-ID to log messages of PostgreSQL? Related: $URL$ 

Is there a way to write this statement so that it is easier to understand? Yes, "easier" is a matter of taste. In this question easier is means: prefer sub-selects and union over joins. In this context it does not matter if the performance gets slower. 

Source: $URL$ The first query returns 140 rows (including strange results), the second 120 (without strange results). 

If I would store this data in YAML, then I could use variables to avoid repetition. AFAIK this does not work in relational databases. I could store this in DB, but AFAIK I need to evaluate it myself: 

Is the result of both SQL statements equal for all possible data sets? I use PostgreSQL, but AFAIK this should not matter for this question.