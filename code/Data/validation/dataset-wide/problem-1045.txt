I have a table that describes a set of properties (yes it's metadata, in this case there's a valid reason to have it in the database); among other things I tell the type of the data that can be assigned to them (Type) and a default value. The valid values for the default value are thus restricted by the Type column, but the default value is genuinely an attribute of the table, they are not uniquely determined by the Type column. I'm not sure though, should this be somehow normalized or is it right as it is? Edit: here's an approximate description of the current structure of the relevant part of the database, as requested. Don't mind the Values table, that's a separate issue. Just consider that ValueType restricts the set of DefaultValue permitted values. Also, consider this only as an example, I'm interested in the problem in general, the problem being namely columns that limit the set of valid values of another column but that don't determine its exact value, thus as far as I understand don't constitute multivalued dependencies (but, as an aside, it would be useful to have an integrity constraint that enforce the limit - I'm not sure that this issue can be separated from the former). 

When I use pgAdmin3 to create a column of type , why does it instead create ? Should we keep this type defined by pgAdmin? Manually create the table with SQL defining column as type causes the pgAdmin SQL pane to again display it as . pgAdmin SQL Pane 

I've just started using PostgreSQL 9.2 and my data consists of product prices at various points in time, usually a different price every month. Question: Because every product can have different prices over time, I am thinking of storing all the data in a single table, each row having full details of the product, its price at that time, and the time. 

I have a PostgreSQL table with existing data, and needs to import all the rows in a CSV file into that table. I am using pgadmin3's Import tool to do the import. 

I have a case when statement in a extract query which doesn't seem to be returning what i'm expecting 

Not sure this is something which people will know on here are not. We currently have a BI setup where we have Integration services on one SQL instance and the data warehouse database on a separate SQL Instance. My question is, as no users physically connect to the Integration services SQL instance and it is purely used to execute package from the integrations service catalog via SQL agent, do we need to License every user i.e. Cal's for that SQL instance. Basically i'm wanting to know if I could utilize SQL 2016 for an integration services instance and use features like Json manipulation etc and then load this data into the 2014 version instance...but what level of licensing will I need? 

At the end I avoided to use the solution with the Type column because it seems very unorthodox to put a foreign key that doesn't refer exactly the primary key of the other table; I did not make a table for each subtype though, only one for the specific one I needed with only one column with the appropriate list of the ids, leaving the general properties in the original table. This seems the best option since it is really, extremely unlikely I'll ever need to refer to the other subtypes. 

I have a short (15 rows) look-up table that lists the valid values for several columns in the database. I don't think it can be considered a One-True-Lookup-Table, it is about a single definite concept, but it may conceptually be partitioned in some subgroups (3). I now happen to have to add a column that actually needs to accept only the values from one of these subgroups. The proper thing to do now would probably to make one table for each subgroup and turn the original table into a simple list of IDs, from which the subgroup tables take their primary ids from. It is though very unlikely that I ever further need to refer to the subgroups, I will instead frequently use things for which all the values in the original table are valid. So the partitioning option would make things more complex for the vast part of the application just to support one case. The only other option I know of is to add a Type column to the original lookup table and a fixed-value column to the single table that need to refer to the subgroup, and use a two-columns foreign key (to ID + Type) in this single case. This is very ugly, and I'm not sure if it is theoretically correct to use columns that are not the primary key as foreign key referents, but given the context, is it acceptable, or even the best way? 

My server has been hacked and I only have FTP access to recover my data before reinstalling the OS. Which/where are the directories that needs to be backed up? Will MySQL be recovered back to its initial state if I were to copy these directories back to the freshly installed server? 

Why does pgAdmin3 automatically set the table owner to when its created and set ? I am new to PostgreSQL and want to know the rational behind this default behavior. 

My reason for storing everything in one table and have duplicate data in certain columns like is to facilitate doing queries like fetching all products' prices in a particular month (can return 10k rows) will avoid costly table joins if I have a table with the product details and a foreign key in another table with just the time-cost data. Is this a good reason for doing what I've described? Will using make it easier to do such queries? Or did I get it all wrong? Thanks!! 

I'd look at changing the union to union all to remove the overhead of making the data in the subquery distinct. Then use group by to remove the duplicates 

I'm expecting the value to be ReturnValue3 however it appears to be ignoring this whole line for some reason. Can anyone see a problem which I'm not spotting. 

Technically the restore verfyonly is like performing a restore, however there is no better check that actually restoring the database to check its a valid backup. We've had an instance where verifyonly passed however the database wouldn't restore due to corruption, my point is don't count on this as a verification the backup is ok. We've now developed a system which restores all our databases on a separate instance to check their validity, clearly that's not always possible so try sampling certain database a week. The long and short don't trust verifyonly 100%.