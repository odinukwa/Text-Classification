It has been like this for a number of days. The cpu and the network are showing no real movement indicating that nothing is happening. Obviously, I'd like to not lose data, what do I need to do to get this back to a healthy PRIMARY/SECONDARY/SECONDARY replica set. 

I have a mongo cluster with 6 replica sets. 5 are fine, one is not. Each replica set has three members. Here is the for it: 

I have a Mongo Cluster. It's sharded on 6 servers, with 6 replica sets, each replica set having one Primary, and two Secondaries. Additionally, there are 3 config servers. Though, I think the Mongo Cluster configuration is irrelevant to this question. I have many application servers (currently 100), with the intention of having many more as my volume grows. On each application server, I have a mongos process, 8 http servers (node.js) each bound to a port (using HAProxy as a reverse proxy to route traffic from port 80). Each http server connects to the local mongos process using connection pooling with a poolSize of 3. Pretty simple so far. The issue is that the number of connections seems to quite simply be: 

I had a 3 shard set up, and ran out of capacity, so I added 3 more shards. (Each shard is a replica set). But the data isn't spread across the cluster evenly. I have my chunkSize set to the standard 64mb: 

So it would seem that the number of connections scales linearly with the number of servers I'm running. If I scale up 5x, then the number of connections goes to 9,600 which is dangerously close to the 10,000 limit (as well as seeming unnecessarily wasteful). What about a 10x or 100x scale up? It is stated that "any connection count greater than 1000 â€“ 1500 connections should raise an eyebrow". How can I possibly maintain that number of connections with 100s of application servers? Is a mongos process per application server at this size still appropriate? 

I have downloaded the file - "win32_11gR2_database_1of2.zip" from oracle site. While installing Oracle 11g, I find this window in the first place: 

I have a question, how did we know that R1 was not in BCNF and needs to be decomposed? Since, is already applied in a decomposition, we don't need to use that again. Coz, it will produce the same result. (1) Finding the violating FDs: is violating the BCNF and it is also not applied yet. (2) Decomposision: Start with . (i) Find the closure of DE and decompose: . . So, our final BCNF decompositions are, 

What to do? What values should I put on the dialog box? Where can I get the related info very concisely? 

(1) Finding the violating FDs: and are the violations of BCNF. Coz, in these FDs, attributes are nor dependent on keys. (3) Decomposition: Start with any one of the invalid FDs: . (i) Find the closure of and decompose: 

HI is a key. So, H is a prime attribute. But, H is also in non-key part. What are the prime and non-prime attributes in this relation? 

Let us consider this example: $URL$ At the first decomposition step, it is using . On the next step, the same FD is not considered any more. Is it because, 

While designing ER Diagrams, how can we differentiate between a Multivalued Attribute and an Entity? How can we know that, what we are considering as an 'Entity', isn't actually an entity but a Multivalued attribute or vice-versa? 

Should "Type of match" be a separate entity? I have included some extra attributes to the entities which were not described in the original description of the case-study. I have used "Look Across" min-max notation. 

I am facing issues related to constantly growing log file due to which I am getting error. When I checked SQL Log I found below messages (error log is filled of these messages almost 90%) 

I've a table with 80 columns and that is a base table for most of the application. Daily load inserts almost 8,000 records and update upto 2,000 records. This table is now having more than 5 million records. Unfortunately, I can't change the architecture and for next few months I have to continue with this. Now, as I said there are multiple application connected to the table which are fetching data from it and almost all of them using different columns for their purpose. Example of few of those queries are: 

I have many SQL Server jobs on the server. I need to execute a SP whenever any job execution is done. One hard way is to create a step on every job which calls that SP. Is there any simple way to do that? Does, SQL Server execute any SP or anything after job completion? 

Now this make sense and I understand that SQL can not perform action cause its log file is filled up. I have two log files 

I have seen column store index and also tested it. I have no doubt admiring that it is lightning fast. But every tutorial and blogs I have considered, says about its limitation "table will become read only" and I won't be able to insert/update/delete data in a table. And at the same time it is recommended to use them in data warehousing projects where data is in huge amount and updated periodically. On based of that I have come up with the solution to