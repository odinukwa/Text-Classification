You may also like the following intriguing and embarrassingly simple illustration from Luenberger's book of the idea of duality in optimization: 

Since I experienced 1. myself and I am thinking about 2., I would like to hear anwers to this question: 

A good (but probably not up to date) reference on several abstractions of the notion of convexity is 

Another, not too mathematical, analogy comes from image processing. There you can consider an image $u$ as a real valued function on a rectangle, say. A basic method for edge detection is to calculate the absolute value of the gradient of $u$ and consider all points where this value is large enough as an edge point. If your image is a (smoothed) characteristic function, then this will give you an approximation to the boundary of the set by thresholding the absolute value of the gradient of the image. Similarly, the morphological gradient also gives the boundary of an object. Here, you consider the object as the support of an indication function. For this function you calculate the so-called closing with some parameter $\epsilon>0$ (which is nothing else as the indicator function of the $\epsilon$-enlaged object). The morphological gradient is then the difference of this closing and the function itself and encodes the boundary of the object. 

Some buzzwords that should lead to some non-textbook examples: In the fields of uncertainty quantification, statistical inverse problems or Bayesian inference one wants, for example, compute conditional expectations for posterior distributions. The domain of integration has as many dimensions as the the quantity of interest has degrees of freedom, and this can be a distributed parameter which, after discretization, may well be in the ten thousands up to millions. Before Monte Carlo methods can be applied one needs to think carefully, how to generate samples from the distribution and one often uses Markov chains to do so (leading to Markov chain Monte Carlo methods). To be a bit more concrete: Consider an inference problem where the quantity of interest $u$ is observed through an operator $A$. Moreover, take into account that the measurement $Au$ is not exact, i.e. we observe $v^\delta = Au + \eta$ with some noise $\eta$ and also consider the case where $A$ has no continuous inverse (or, discretized, a large condition number). Assuming that the distribution of the noise is known, we can write down the probability $$ p(v^\delta\mid u) = p(\eta). $$ If we further assume prior knowledge about the real $u$, formulated in a prior distribution $p(u)$ (could be Gaussian or something else - whatever makes sense in the application), then the distribution that is really of interest is the posterior $$ p(u\mid v^\delta) \propto p(v^\delta\mid u)p(u) $$ i.e. the one that answers the question "What is a probable $u$, now that we have seen the data $v^\delta$?". It's not easy to deal with the posterior, since it is a distribution on the space where $u$ comes from, so if the problem is a discretized problem and you want $u$ with $n$ degrees of freedom, then, it is a distribution over $\mathbb{R}^n$ - $n$ in millions. One quantity of interest for the posterior is the conditional mean, i.e. something like "the expected solution $u_{CM}$, given the current data $v^\delta$" and this is the integral $$ u_{CM} = \int u \, p(u\mid v^\delta) du $$ and this is an integral over $\mathbb{R}^n$. Here people (and especially companies) indeed use Monte Carlo integration. Even more concrete: $u$ is oil distribution under ground, $A$ takes this distribution and gives back the surface measurements under seismic stimulation and $v^\delta$ is the measurement you take. Oil companies invest a lot of time and money to infer as much as possible about the oil reservoirs and indeed use techniques as described above. They want a good resolution of the oil distribution and hence, $n$ as large as possible (but to have at least a crude picture of how it looks like under ground, you need at least some ten or hundred thousand…) This is probably not what Ulam and von Neumann had in mind but I think you can't get more real world than that. A starting point for the general idea is 

There are some examples given by Pjateckiĭ-Šapiro in Classification of bounded homogeneous regions in n-dimensional complex space. Dokl. Akad. Nauk SSSR 141 1961 316–319. and On bounded homogeneous domains in an n-dimensional complex space. Izv. Akad. Nauk SSSR Ser. Mat. 26 1962 107–124. (I have a vague memory that the smallest examples are 4-dimensional, but might have misremembered.) 

Since there are about 2k possible sums, with typical order of magnitude about kn, it seems reasonable to guess that the first case when one of these sums is 0 will occur when these 2 numbers are about equal, which is when k is about n log(n)/log(2). This incredibly crude estimate is somewhat smaller than the numerical data, but maybe suggests about the right growth rate, 

It is unlikely that there is any easy reason why a classification is possible, unless someone comes up with a completely new way to classify groups. One problem, as least with the current methods of classification via centralizers of involutions, is that every simple group has to be tested to see if it leads to new simple groups containing it in the centralizer of an involution. For example, when the baby monster was discovered, it had a double cover, which was a potential centralizer of an involution in a larger simple group, which turned out to be the monster. The monster happens to have no double cover so the process stopped there, but without checking every finite simple group there seems no obvious reason why one cannot have an infinite chain of larger and larger sporadic groups, each of which has a double cover that is a centralizer of an involution in the next one. Because of this problem (among others), it was unclear until quite late in the classification whether there would be a finite or infinite number of sporadics. Any easy way to get around this has been overlooked by about a hundred finite group theorists. 

Karl Marx. So you didn't know he was a mathematician? A book of his collected mathematical papers is in our math library, which is more than most mathematicians can claim. (They are mostly attempts to understand the definition of a derivative if I recall correctly.) They were quite popular during the cultural revolution, Chinese mathematicians presumably figuring that the study of dialectical calculus was better then a one-way trip to one of Mao's holiday resorts. 

First, you should restrict $x$ to be positive or use $|x|^\beta$ instead. Then I think that the answer is no: For $\beta\neq 1/2$ you can argue as follows: The special case of diagonal $\Omega = \mathrm{diag}(w_1,\dots,w_n)$ ($w_i>0$) is simpler as in this case you problem is $$ \min_x x^T\Lambda x - \alpha^T x + \sum_i w_i |x_i|^{2\beta}\quad\text{s.t. convex constraints} $$ i.e. is is a convex quadratic problem with a weighted $\ell^{p}$ regularizer with $1\leq p=2\beta <2$. As such it is a fairly simple convex problem. I am fairly sure that even this special case can not be cast as SOCP (if found this claim in "Mixed norm FIR filter optimization using second-order cone programming" by Dan P. Scholnik (ICASSP 2002) and the report "Second-Order Cone Formulations of Mixed-Norm Error Constraints for FIR Filter Optimization" by Dan P. Scholnik and Jeffrey O. Coleman but no reference is given). For $\beta=1/2$, i.e. $p=1$ one has to argue differently as the above case can be cast as an SOCP. The case including the spd matrices is equivalent to (neglecting the constraints) $$ \min_x \|Ax-b\|_2^2 + \|Lx^{1/2}\|_2^2 $$ with some $A$, $b$ and $L$. Here the penalty looks like $$ \|Lx^{1/2}\|_2^2 = \sum_i (\sum_j l_{i,j}x_j^{1/2})^2. $$ With $n=2$, and $L=\begin{bmatrix}1 & 0\\1 & 1\end{bmatrix}$ lead to $$ \|Lx^{1/2}\|_2^2 = |x_1| + (\sqrt{x_1}+\sqrt{x_2})^2 $$ which is not a convex function (simply check that the level sets are not convex), so also here, the answer is no. There is still a possibility that there may be a clever reformulation/substitution, but I doubt that. A prove that there is no such a reformulation seems very hard… 

David Mumford, a well known algebraic geometer, is responsible for the "Mumford-Shah segmentation" model in mathematical imaging. Besides being one of the most cited papers in this area, it also sparked in immense amount of work in geometric measure theory, "special functions of bounded variations", Gamma-convergent numerical approximations and calibration and functional lifting for numerical purposes. To get a glimpse, check the book Singular Sets of Minimizers for the Mumford-Shah Functional. 

More precisely, is Ricci flow with surgery on a 3-dimensional Riemannian manifold M given by the "constant-time" sections of some canonical smooth 4-dimensional Riemannian manifold? There would be a discrete set of times corresponding to the surgeries, but the 4-dimensional manifold might still be smooth at these points even though its sections would have singularities. The existence of such a 4-manifold is well known if there are no singularities: the problem is whether one can still construct it in the presence of singularities. Background: Ricci flow on M in general has finite time singularities. These are usually dealt with by a rather complicated procedure, where one stops the flow just before the singularities, then carefully cuts up M into smaller pieces and caps off the holes, and constructs a Riemannian metric on each of these pieces by modifying the metric on M, and then restarts the Ricci flow. This seems rather a mess: my impression is that it involves making several choices so is not really canonical, and has a discontinuity in the metric and topology on M. If the flow were given by sections of a canonical smooth 4-manifold as in the question this would give a cleaner way to look at the surgeries of Ricci flow. (Presumably if the answer to the question is "yes" this is not easy to show, otherwise people would not spend so much time on the complicated surgery procedure. But maybe Ricci flow experts know some reason why this does not work.) 

A student who has looked something up on google and copied it out has probably learned something from doing this, so why worry? In fact, may be better for them than copying the answer from a friend. 

Regular local rings are Noetherian by definition, but valuation rings are not unless they happen to be discrete valuation rings. So with the usual definitions, most valuation rings are not regular. (It might be possible to come up with a reasonable definition of regularity for non-Noetherian rings, but I have not heard of one.)