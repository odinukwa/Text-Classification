You have asked two different questions, Muze. The title of your question asks whether the Roche limit is dangerous to spacecraft, but the body asks whether the Roche limit has been observed having an effect on an object. The answers to these two very different questions are "no" and "yes". The main question first: The Roche limit applies to objects that are weakly bound by self gravitation. A good example was Comet Shoemakerâ€“Levy 9, which broke apart into a number of pieces before colliding with Jupiter in 1994. A prior close approach to Jupiter in 1992 tore the comet into pieces thanks to tidal effects. The collision in 1994 was a string of pearls hitting Jupiter. Spacecraft are held together via welds, nuts and bolts, screws, and inter-atomic and inter-molecular forces. Each of these is many orders of magnitude stronger than is gravitation. The Roche limit doesn't apply to spacecraft as spacecraft aren't held together by gravitation. 

By luck, there were no large solar energy bursts when men were in space on the way to the Moon, on the Moon, or coming back from the Moon. A large coronal mass ejection event did happen on August 7, 1972, but that was (by luck) sandwiched between the Apollo 16 and Apollo 17 lunar missions. 

There are different notions of mass, but they're equivalent. There are two distinct notions of mass: gravitational and inertial. The masses in Newton's law of gravitation, $F = \frac{Gm_1m_2}{r^2}$, are gravitational masses. The mass in Newton's second law of motion, $F=ma$, is inertial mass. Gravitational and inertial mass are implicitly assumed to be the same in Newtonian mechanics. General relativity makes this assumption explicit in the equivalence principle. But what if they're not equivalent? Unlike mathematics, where one can simply make an assumption and see where it leads, assumptions in physics need to be validated. This assumption has been tested with many kinds of materials, both on the ground and in space. Variations on the Cavendish experiment using different kinds of materials have been made. Within the limits of the rather lousy accuracy of the gravitational constant (one part per ten thousand, at best), every one of these is consistent with the null hypothesis (gravitational and inertial mass are the same) and inconsistent with the hypothesis that different materials have measurably different gravitational and inertial masses. The Earth's Moon, with its very different near-side and far-side, provides an even better mechanism for testing this equivalence. Rather than the one part per ten thousand (at best) accuracy available to Cavendish-style experiments, the Moon shows that gravitational and inertial mass for sodium and iron are equivalent to within about one part per ten trillion. So much for ordinary matter, but what about antimatter? That an ordinary matter particle and its antimatter equivalent have the same (positive) inertial mass has been tested over and over in particle colliders around the world. Whether the equivalence principle also applies to antimatter remains a somewhat open question. While there are many reasons to think that the equivalence principle applies to antimatter as well as normal matter, testing that this is the case is very hard. The best results to date are from the ALPHA experiment, which tests whether neutral antihydrogen (a antiproton and an positron) falls up or down. The results are that antihydrogen's gravitational mass lies somewhere between -65 and 120 times its inertial mass. This is not anywhere close to conclusive, but it does lean towards antimatter having a positive gravitational mass, consistent with the equivalence principle. 

That is an oversimplification, and a rather large one at that. There are multiple lines of evidence that all lead to the concept of the Big Bang, one of which is that galaxies tend to have a redshift that is strongly proportional to distance. While the correlation between distance and redshift is very strong, it is not perfect. Nearby galaxies do not exhibit this behavior. The reason is simple. That redshift is proportional to distance means that nearby galaxies would have a very small redshift due to recession. (Ordinary) gravitation can easily overwhelm this tiny recessional velocity, and that is exactly what happens. 

You are ignoring velocity and eccentricity, which do distinguish between comets that originated from within the gravitational bounds of the solar system versus comets from interstellar sources. This is the key reason Jan Oort hypothesized the Oort cloud. All comets observed to date are consistent with having an intra-solar system origin and are inconsistent with having an extra-solar system origin. An interstellar comet should have a large excess velocity. That is not what has been observed. The few comets that apparently are on a hyperbolic trajectory are deemed to have originated within the gravitational bounds of the solar system, with the hyperbolic orbit a result of non-gravitational forces and gravitational interactions with planets. If interstellar space is awash with ejected comets, estimates on how often we should see an interstellar comet range from once per 25 years (which has not been observed) to once every 450 years (which is consistent with not seeing any). Moreover, there are reasons to think that interstellar space is not awash with comets. If interstellar space was awash with comets, we should see a lot more gamma ray bursts than are observed, and we shouldn't see any white dwarfs depleted of hydrogen (which are observed; 16% of the white dwarfs are DB white dwarfs). This suggests to some that the solar system is an oddity, and that most stars don't produce Oort clouds. 

The answer is "it depends." It depends on what kinds of eclipses you count, and perhaps even more importantly, how likely you are to see the event. There are four types of solar eclipses: 

Gases in a planet's troposphere don't differentiate chemically; the turbulence driven by heating and planet rotation keep the atmosphere well-mixed. We can see this in our own atmosphere. Carbon dioxide and argon are considerably more dense than are the nitrogen and oxygen that form the bulk of the atmosphere. Yet we don't have a layer of carbon dioxide at the bottom of the atmosphere. The turbopause marks where an atmosphere shifts from being dominated by turbulent mixing to being dominated by diffusion. Chemical differentiation by atomic mass does occur above the turbopause, but even there, it's gradual. But what about rain? The answer is simple: It evaporates. That happens here on Earth, particularly in arid regions. Clouds form, and rain falls from those clouds, but the rain sometimes evaporates before it reaches the ground. This is called virga. Temperatures rise inside Jupiter due to compressional heating, at a rate of about 1.85 K per kilometer of increasing depth. That means the temperature reaches the water's critical temperature (647 K) about 240 kilometers below the 1 bar pressure level. So even if rain water could fall that far as rain before evaporating (which is dubious), it would cease to be a liquid. 

The first meeting of the AAS was held in September 1899 in Williams Bay, Wisconsin. The meetings were held once a year initially. The frequency varied somewhat for the next 60 or so years. The meetings appear to have settled down to twice a year about 40 years ago, one in January and another in late spring. Source: $URL$ 

They don't, and they can't. The best they can do is estimate the object's trajectory based on observations and based on techniques used to propagate the object (and other objects in the solar system) over time. These estimates are always imperfect: 

There are a number of different organizations that produce ephemeris models of the solar system. Even within one organization, there a number of different ephemeris models. The three leading organizations that produce solar system ephemerides are 

The other approach is to use numerical integration. I'll start with a discussion of how to solve for the value of a scalar function $x(t)$ at some time $t_1$ given an initial value $x(t_0) = x_0$ and some well-behaved (continuous and bounded) derivative function $f(x(t),t) = dx(t)/dt$ that describes the time evolution of $x$. This falls in the very broad category of initial value problems. Suppose the ordinary differential equation cannot be solved analytically and cannot be expressed terms of a useful power series. This doesn't mean nothing can be done. There are a number of techniques for solving this problem numerically. Note the dependence of the derivative function on the dependent variable $x$. This becomes the much simpler problem of numerical quadrature if the derivative function can be expressed independent of $x$. The discussion that follows assumes that the derivative function $f$ does indeed depend on $x$. Note well: Newtonian gravitation falls in this category. It also assumes the derivative function is well-behaved. Numerically integrating across a discontinuity is a bad idea. The foundation of the integration-based techniques for a scalar function is the mean value theorem, which says that at some time $t_c$ between $t_0$ and $t_1$, the value at $t_1$ is exactly $x(t_1) = x(t_0) + (t_1-t_0)\,f(x(t_c),t_c)$. If only we could find that magical $t_c$ and the derivative at that point. There's a chicken and egg problem here: that magical point in time is not known. Even if it was, the derivative function depends on state, and that too isn't known. A very simple approach around this problem is to assume that this magical point is the initial point: $$x(t_1) = x(t_0) + (t_1-t_0)f(x(t_0),t0)$$ This works quite nicely for values of $t_1$ that are very close to $t_0$. It doesn't work very well at all where $|t_1 - t_0|$ is not small. This suggests splitting the interval $(t_0, t_1)$ into a number of smaller intervals. This results in Euler's method: Apply the above to advance state to time $t_0+\Delta t$, then to $t_0+2\Delta t$, and so on, eventually reaching the desired time. Euler's method is rather lousy, even for a simple first order scalar ODE. We can do much better than this. The key reason for discussing Euler's method is that it is the basis for many other integration techniques. Learn how it works, then toss it. One approach to approving on Euler's method is to somehow correct the result from Euler's method. For example, take an Euler step and compute the derivative at the end point. Then use the average of those two derivative values (the original value used to make the Euler step, and the other from the end of the Euler step) to recompute the step from $t$ to $t+\Delta t$. THis is Heun's method. Another approach is to guess that the magical point $t_c$ lies somewhere between $t$ and $t+\Delta t$. Perhaps the middle? We can use Euler's method to advance state to the midpoint, and then use the derivative at that point to advance state from $t$ to $t+\Delta t$. This is the midpoint method. Both Heun's method and the midpoint method appear to be steps backwards, computationally. While Euler's method requires but one evaluation of the derivative function per time step, these improved methods require two. However, the error growth is in general so much smaller with either Heun's method or the midpoint method compared to that from Euler's method. This means that those "improvements" most definitely are improvements. The expense of calling the derivative function twice per step is more than offset by the fact that imprpovements enables take steps that are orders of magnitude larger than one can make with Euler's method. Both Heun's method and the midpoint method are simple improvements. This problem has been studied in many guises. There are many more advanced techniques. One is the class of Runge-Kutta integration techniques. Both Heun's method and the midpoint method fall into this class. The most popular of these, classical Runge-Kutta 4, is a significant improvement on those two methods. There are even higher order Runge-Kutta integrators than RK4. Heun's method also falls into the broad class of predictor-correctors, wherein one method (the predictor) advances state to the end of the interval and another method (the corrector) uses the derivative at this approximate endpoint to correct the guess made by the predictor.