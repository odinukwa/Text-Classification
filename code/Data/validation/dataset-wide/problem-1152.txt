After hours of trying to figure out what was going on I finally solved the issue. The issue in my particular case was that the import statement I was using for test: 

I'm setting up a development environment for a content management system with a database backend. The source for the dev environment is a virtual machine snapshot from production the target environment is a docker environment. We have the application code under git source control, but also want to source control the database content. This will enable us to migrate both code and content all at once and know what we are changing. I've written an import script and an export script. Everything is working except for about 7 tables that won't roundtrip data. Each export and import loop the data is growing for some odd characters. I have this data in an import statement 

This answer to the follow-up question works here too. The index definition's / + / combination (whether implied or specified) must either match the query exactly or be the exact opposite. E.g. and are not compatible, but and are exact opposites and the index of one can be used for the other. So it did not have to do with it being an expression index, after all. 

These are my settings: $URL$ According to the documentation, as far as I can tell, this combination of settings along with a value of 3600000 should mean that hour-long queries time out, and well before then, they and anything else longer than ~214ms should be producing EXPLAIN output in the logs. But they're not. 

I feel the problem is in the import because the data looks correct when I'm importing it, but I'm not sure what is up... 

How I figured it out was by viewing the binary at each stage in the process to ensure it was what I expected. First, the way I discovered there were discrepancies in the first place was by using git. By adding my sql file to git then when I do an export I can do and to see if the data changed. Using this method I isolated one table, then one row, and then one character that was failing to use as my test case. The rest of these steps focus on ensuring that character is right. How to find the character. Look in your data. In my case I had "Spartyâ€™s\" that would look incorrect after export and import. By using a hex viewer (discussed below) I used the y and s characters as the endpoints and looked at what was in the middle to determine the character. Starting with the database. I would select the one column from the one row with the issue as hex like this: 

I run a service where I deliver a lot of downloads that go over 1 extra hosts between the destination and origin host. They are represented by the integer interpretation of the 32 bit IP address. My system currently handles about 500 inserts/second during peak hours. I run a master-slave system. The master has an apache webserver with a PHP file that gets called from remote hosts and inserts a line into the log table. Then the changes get replicated to the slaves where queries happen. My queries are primarily aggregations over the mb_transferred field over a range in the time field filtered by client_id. 

The maser server runs an apache webserver with a simple php file that does the insert and is called by other servers. My server is now almost at the limit. I already upgraded to big hardware. I thought about using a GUID as primary key and using master master replication, that will for sure relieve something, but I think its short sighted, because it does not decrease the insert amount per server. I am expecting higher trough-puts in the future and I am also worried about database size. Also in future I plan to have a second table which defines "weights" for certain services. Something like: 

I've run ANALYZE on table as suggested here but the results are still as above. If I instead I get back in 150 milliseconds, or back in 12 milliseconds. Is there any way to make such a query work quickly whether there are results or not? Reproducible example Not quite the 5s, but 2s still vs 12-31ms, so you get the idea, at least: 

That seems like a really arbitrary number of milliseconds to use as a maximum. What can I do if I want to log even longer-running queries on a data warehouse? C.f. $URL$ 

already contained by default when I checked. The parameters above are applied to the server. I've restarted the instance after applying the parameters. My back end has reported an incidence of (which only started happening after I set ) but there is nothing in the logs (at all, let alone in the vicinity of the time reported) showing anything other than xlog starting and stopping. 

Then I needed a hex viewer. Sublime has one, but it didn't work well (search was input in hex and didn't seem to search the whole buffer for me, and I had to have another window open to help me compare the expected text to) so I used vim. To put vim in hex mode I used the command after opening utf8.sql. Vim is nice because it shows the hex on the left and unicode on the right. Additionally allows searching by unicode! So I can input my search term and quickly see the next character, looking for . If I didn't see and in the previous step it was then that means the problem was with the export. This is what the line looks like in vim 

This query takes 5 seconds if I search for something that doesn't exist, and 31 milliseconds if I search for something that does: 

If I leave off the , the results are reversed: In the non-existing case, it takes 12 milliseconds, but in the existing case, it takes > 30 seconds (naturally, because it's trying to return 50,000 rows.) Anyway, the query plan instead is: 

As I understand it, this separation is good from a security point of view, so that if a user somehow gains control of the user, their permissions are limited. And running a migration on AWS RDS (with users connected) does successfully boot them out. Yet, running it on a local Postgres 9.4.12 instance (with users connected) gives the error in the title in response to the call. Why? (I would expect the same behaviour locally and with RDS. After all, RDS's 's only difference is that it has , but that's not actually a superuser.) Also, if it's right that I get this error, how should a migration work if you need to boot users, but have no superuser permissions and have a separate login explicitly for the regular users whom you are trying to boot? 

Here we store the full username also in the adressdata table. To me this has the following advantages: 

In a lot of relational database designs there are fields that are referenced in other tables. For example consider a user table with a unique username and a second table storing address data. One possible layout, that I would say is the common approach, because I have observed in most software, is to use auto increment ids like this: 

In my opinion it is much easier to work with text fields and don't use increment ids, and the trade offs are minimal and in most applications not relevant. Of course some object ARE identified with an incrementing number by their nature (for example forum posts should receive an incrementing id because there probably is no other unique field like title or so). But I before I start designing my database layouts in a completely different way I would like to know whether there are things I did not think of. 

In my case this was useful for dealing with although I couldn't paste into my sql terminal (however it worked from save command files) 

I copy paste that line into Sublime to do my manual character comparison. This isn't necessary, but I'm not good at reading hex so it helps. 

After pasting the hex text into Sublime I would search for and see what characters were next. If it was I was in good shape, time to move on to the next step of the process. In the above example it is not the character i'm expecting () instead it is . This means the data is bad in the database. After ensuring the data is correct in the database the next place it goes is the filesystem. I used vim to follow a similar process to verify the character is what I need: First I export the data to a file on disk: 

I am currently running a MySQL Database for logging and analyzing those logs. My current table schema looks like this: 

Then I want to run join queries against this table and multiply the mb_transferred field with a weight factor. I also want to add fields like "transfer_duration" to the logs table to calculate the speed of downloads and run queries to get statistical data how how well/bad the connection between certain networks, or certain servers for certain hosters is. The point is. The data structure is simple, its just a huge amount of rows. I have a lot of aggregation functions. This makes a light bulb in the "map reduce" section of my brain flashing. I thougth about doing vertical shards and use client_id as a breaking point. For example if I have 10 server send every user to its userid mod 10 server. This would be easy and relieve the load. But scaling will probably be awkward. So i think with the size of the project that I am expecting to reach soon with the current growth I cannot do anything but turn towards a distributed database system. I already tried to examine cassandra, project voldemort, amazon dynamodb and hbase but no matter how much I read I seem to run against walls. I think the long years of relational thinking are somehow blockading my mind. Can someone point me into the right direction on that? What database system would be suited for my use case and why?