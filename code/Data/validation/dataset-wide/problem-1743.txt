I don't think you can put both timezones in , but you can make different files in each of which has a different and setting. For instance, : 

"NOQUEUE" means that Postfix did not queue an incoming message. "Reject" means that Postfix rejected an incoming message. "RCPT" means that Postfix rejected the message after the client sent RCPT. "454 4.7.1 Relay access denied" means that Postfix rejected the message because it was not addressed to any domain that Postfix serves. This is its default configuration, and prevents spam from being relayed. To ensure nobody uses your mail server to relay spam, you should: 

The and directives that you're trying to use were added in Apache 2.4 and are in the module. They are not available in Apache 2.2. 

You can use multiple names in and even repeat it. For instance, to answer to three hostnames, you could do something like: 

Docker themselves make this clear: You're expected to run a single process per container. But, their tools for dealing with linked containers leave much to be desired. They do offer docker-compose (which used to be known as fig) but my developers report that it is finicky and occasionally loses track of linked containers. It also doesn't scale well and is really only suitable for very small projects. Right now I think the best available solution is Kubernetes, a Google project. Kubernetes is also the basis of the latest version of Openshift Origin, a PaaS platform, as well as Google Container Engine, and probably other things by now. If you're using Kubernetes you'll be able to deploy to such platforms easily. 

There are a wide variety of problems that can cause NaN in Cacti graphs. The best thing to do is to work through the debugging checklist in the Cacti documentation. 

If you use the command line tool to update your system, it will tell you how much bandwidth you saved. For example: 

Stick the SSL certificates in a tiny Docker volume that's shared between your letsencrypt container and your nginx container. While you're at it, you may as well let nginx intercept Let's Encrypt challenges as well, which might simplify your architecture a bit. Use haproxy instead of nginx. Unlike nginx, haproxy is capable of passing through SSL connections (and doing a variety of other things that are useful in a containerized setup). Note that this requires SNI support, and so such sites won't be accessible to ancient web clients which can't support SNI. 

The problem you are having is that your web site works when accessed via IPv4, but returns too many redirects when accessed via IPv6. To resolve it: Your nginx configuration has errors that should cause it to fail to load. In particular, you've specified in your IPv6 directives, but also specified IPv4 directives along with them. The expected response to this is nginx returning an error and exiting (or refusing to reload the config). If nginx is still running, it is likely with an old config. You should first remove from each directive, as it is redundant and unnecessary. You should then add the IPv6 directives to the blocks which are missing them. 

The proxy server administrator can see every site you visit and every URL you download. It is trivial for the administrator to determine if you are violating policy, or worse, local laws. And you may find that many web sites not related to work are blocked. I recommend you do your home stuff at home, and your work stuff at work, and keep them physically separated. It sounds like you're just trying to use the work connection to download large files, since you only have dialup at home. In this case I would recommend you use an Internet cafe, until you get a broadband connection installed at home. 

Better yet, dump this firewall entirely, and go back to the system which was included with CentOS 7. It will configure a proper firewall without all of these traps for the unwary. 

I would remove from . Since you aren't going to be serving any files directly, there is no need for it to be present, and it is the immediate cause of this error, if not the root cause. Leaving you with: 

If the clock drifts more than 500ppm in either direction, ntpd considers it insane and stops updating it. You will see a single log message when this happens (if you go looking for it). In this case you will need to use an alternate ntp client, such as chrony, which can be configured to continue to work in this scenario. 

and is an otherwise empty directory, which only contains files when Let's Encrypt is being used to issue a challenge (with ). Let's Encrypt is then run with: 

For some reason MySQL is using an old-style init script, even on CentOS 7, which uses systemd. They should have distributed a proper systemd unit file, but chose not to. The proper way to enable an old-style init script is with , i.e.: 

libvirt is a programming API allowing for developing applications that manage hypervisors. It is completely hypervisor-independent, meaning it can manage Xen, KVM, VMware, Hyper-V, etc., through the same API. So if you want to build a system that is hypervisor-agonstic, or you're running different types of hypervisors, you would want to use libvirt to manage them. This way you use the same code to perform an operation regardless of whether the underlying hypervisor is Xen, Hyper-V, KVM, VMware, or something else. 

OK, here's a completely different answer. Place the offending hosts with crappy IPv6 connectivity in with their corresponding IPv4 address. For example: 

Your SPF record contains , which means that everyone on the Internet is permitted to send mail for your domain. Since this is obviously ridiculous, it's treated as though you don't have an SPF record. Perhaps you meant to use instead, meaning that no one except those you designated elsewhere in the SPF record can send mail for your domain. 

Rich rules aren't the way to go about this. They'll just create confusion, now and later. Understand that a firewalld zone corresponds to a set of services that you may wish to allow, and the sources of the traffic to those services. All you have to do is to set the services you want to allow in the zone (which you probably already have done) and then set the sources. Traffic sources can be designated in two ways: By interface, or by source IP address. Traffic that matches any source passes this check. So, what you want to do is to add the IP addresses allowed to reach the services, and then remove the interface (if any). 

The thing I see most commonly done in this circumstance is using jwilder/nginx-proxy to handle all incoming connections and then proxy them to the correct container. More complete PaaS solutions like Kubernetes and OpenShift have their own means of handling this (e.g. by running an internal haproxy container which does the same thing). 

Congratulations, you've found an Internet tutorial with bad advice. The problem with using a single keypair for multiple computers occurs when any one of the computers is compromised. Then you have no choice but to revoke the keypair everywhere and rekey every single computer which was using that keypair. You should always use unique keypairs per machine and per user, to limit the damage that a compromised key can do. As for that tutorial, it's amazingly bad advice to generate the keypair on the server and copy the private key to the client. This is entirely backward. Instead, the keypair should be generated on the client and the public key copied to the server. There is even a helper script which does exactly this, and along the way makes sure all permissions are correct, the client gets the server's host key, etc. There may indeed be situations where you want to centrally manage users' identity keys, e.g. for automated scripts, but in this case you really should do this from a third host, or ideally from a configuration management system such as puppet. 

We see from the audit log that SELinux denied access to traverse the directory, to descend into child directories. As far as I can tell, should have permission to traverse , so you're likely looking at a bug. As such, you should report it on the Red Hat bugzilla. As a workaround, while you wait for a fix, you can have run unconfined by SELinux (without disabling it system-wide). This is temporary and will be undone by a relabel or package update, so you may have to reapply it if a future update doesn't resolve the issue. 

Since you have varnish in front of nginx, it thinks all the requests are coming from 127.0.0.1, since technically they are. To resolve this, use the nginx real ip module to pick the client's IP address out of the header, which Varnish automatically adds to requests (unless you told it not to). An example nginx configuration would be: 

Change it to something a bit longer, say 86400 (1 day) or even 604800 (1 week). Keep in mind that this applies to negative caching of any nonexistent record, not just AAAA records, so you'll need to take additional steps when adding new records of any type (reduce this value, wait, add the new record, increase this value again).