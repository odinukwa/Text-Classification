I can't imagine telling someone I'd "refuse to admin" a system, unless I was given some kind of bizarre ultimatum along with a bizarre system. To fix this, just create a new database on a correctly formatted partition, and migrate the data to it. Should be relatively straightforward unless the database is ungodly huge, and if it was ungodly huge, I think this would have been a pretty pressing problem before now. In the admin world, systems that are working are seldom worthy of emergency responses. Plan it out, do your migration in an orderly manner, and you should be fine. 

The only good reason to go IPv6 internally is to be ready when the world switches to IPv6, and I think that's a pretty bad reason, given the rate of adoption. Since most internal IPs won't be externally reachable, it wouldn't be a big deal to translate the rest. My corporation will probably never switch to IPv6 internally. It would require a fundamental shift in policy so massive I can't honestly conceive how it could come about. A lot of people would have to get killed, and a lot of inexplicable hiring choices would have to be made. Likewise, any attempt by individual business units to switch to IPv6 on their LANs would be squashed with prejudice by the corporate networking overlords based on interoperability and maintainablity concerns (we allow a lot of leeway locally, but not that much.) Basically, if switching to IPv6 was painless, we'd have done it years ago. 

Check out Webmin for a nice web-driven remote management interface. It has module add-ons for a number of monitoring utilities (like Heartbeat, for example, as well as other multi-server monitoring tools). You can also link any number of servers running webmin into one panel, to put all your server monitoring in one place. For users, you probably want to set up something like OpenLDAP or a Samba domain, rather than copying users from one machine to the next. OpenLDAP can be a pain, but nothing like trying to sync users across multiple machines. 

(Though I tend to use tds version = 8.0 when I'm working with MS SQL Server, and it works fine) Edit: (Going to put my responses here so I can code-format them) Your /etc/odbcinst.ini should look like: 

If you're going to be using php, you might as well do LAMP. Php and IIS is a nightmare. Php and MSSQL is just kinda a waste, and more than a little annoying to configure (I actually use Php and MSSQL, just because the MSSQL is standard here, and the Php was inherited. I don't particularly enjoy it.) Still, for such a small setup, you can use whatever you're most comfortable with. I'd skip MSSQL partly because one of the things I enjoy most about MSSQL is Visual Studio, and you probably won't have that. And Apache/Php works decently well on Windows. I wouldn't worry about sticking your windows-using coworkers with a LAMP machine. As long as you install the GUI it should be easy enough for them to manage. 

I don't know of a way to restart individual processes without having multiple different apache installs (which is a perfectly sensible way to do it, especially if people are breaking things). However, you might want to look at the graceful restart directive...Basically, it's a restart that waits for running processes to complete, before it cycles the server. I work on a lot of really large websites, and this is commonly used in production with very little user impact. 

Looks fine...You don't want to be tooling along flatlined: that means you've paid too much for your hardware. And if it's topping out at 50%, then you've got plenty of spare capacity. The only thing I'd worry about is that Tuesday spike...Looks like you handled it fine, but spiky traffic is the worst to plan for. What kind of site is it? 

The only thing you really need it for is setting the hostname. Otherwise, you can add some network and gateway settings, but you can just as easily put that stuff in the /etc/sysconfig/network-scripts/ifcfg-ethX files. 

The answer is yes, but since we're talking about IIS, the answer isn't that easy. Depending on your application, the limit can be overridden in the code (goooo ASP.NET with maxConcurrentRequestsPerCPU/maxConcurrentThreadsPerCPU) or otherwise, you'll need to change the appConcurrentRequestLimit property in the IIS Settings Schema. The default is 5000, but some people recommend bumping that up substantially (like by a factor of 20. I only use IIS when I have to, so YMMV). 

Check your (could be tomcat(versionnumber).conf and make sure you're not over-riding the system value there. 

Meaningless question without ram clock speed/ front side bus numbers. Extra meaningless because the "R" in raid is for "redundancy" and there is no need for redundancy in volatile memory. Pick the one that's faster. 

Run a traceroute on one of the other machines and make sure it's not somehow being routed through your Mac Pro. That would be bizarre, but hey, easy to check. After that, I'd check to see if it's trying to sync something to the network before it shuts down. Start up ipTraf on one of the Linux boxes and see if it's getting any weird traffic. 

You need to talk to a local ISP and get a quote on a high-end connection. They can tell you how much it'll cost to get set up. Lot of times your regular "consumer" ISPs have high end data plans available, in which case the cost would probably be minimal. Cable and DSL use pretty much the same equipment for home/business connections. I have a satellite office that gets it's internet from a local cable company (Cox) and we get 15/3 for about what a T1 would cost (though it's less reliable). 

The first three can be done by modifying /etc/sshd_config The fourth depends on which firewall software you're using. 

I usually only do new domains for geographically separated business units, for the first reason echobeach2 mentioned: you want local admins to be able to admin their local domains, but you don't want them to have admin rights to ALL domains in the forest. It's also a pain to have AD replication constantly running across a VPN tunnel, or whatever. Otherwise, the fewer the better. 

The problem with textbooks in this context, is that most schools don't teach "SQL Server 2008 in Windows 2008" for the simple reason that, when you graduate, you'll need to know "SQL Server 2012, on Windows 12" and they'll have taught you nothing. I've never read the Murach book, but I'll tell you right now, any book that says you'll learn: "How to create complex inner and outer joins, summary queries, and subqueries..." has nothing that a professional would find useful in it...that doesn't even fall in the scope of a MSSQL server book, imho...That's DBA territory. You're going to need to go out and hit the "MSSQL 2008 for Dummies" section of your local bookstore. That's the sort of place you'll find tech manuals that deal with software-specific configuration issues. I'd recommend "Microsoft SQL Server 2008 Internals"; it's not a bad book. Don't buy a book on Windows Server 2008: unless you're setting up AD on it, you won't need it. What you will find, very quickly, is that it is very easy to do very easy things, and very difficult to do everything else. The online documentation is horrible; they hired savants who know the exact example that would help you understand their cryptic instructions, and they ruthlessly expunge all those examples from their site, choosing instead to use ones that are so simple, so wholly idiotic, you'd never have needed them, or so esoteric you don't know why ANYONE would have needed them. The visual studio tools (which damn well BETTER come with the software) are very nice. If you don't know a lot of VB scripting, you're going to need to learn to love the "Business Intelligence Development Studio": it's ornery and picky, but it's better than nothing. There is no substitute for just installing it and playing with it. You can't even have good questions until you've seen it running. It's very easy to set up: just stick the disks in and go. 

Switches generally don't show traffic for other addresses...That's pretty much the point of a switch. A lot of them can be switched to "promiscuous mode" for debugging though, so that'd be the first thing I'd try. A switch in promiscuous mode will broadcast every packet it gets, so you can see the full range of traffic. The second thing is to put a machine inline between the server and the switch (or just run wireshark on the server) so that you can see the whole of the traffic. 

I actually have a heavy rackmounted server that miraculously survived having one of it's "ears" fail when it was being racked. They absolutely WILL bend and break. I say no. (I'll post a picture later) 

I use something similar to this for tons of cron-based write jobs. Works like a charm. It's actually a similar method to the ALOHA communications protocol, which is very similar to a later network protocol called "TCP/IP". Try, try, and try again. 

You can try Thunderbird, but depending on how the Exchange server is configured, you may not have any luck. This is a pretty good how-to regarding Thunderbird and Exchange. Or, depending on how sensitive your mail is, you can just forward a copy to a gmail account and use gmail. 

Well the address thing can be done simply with NAT but only for machines OUTSIDE of the NAT'd subnet. To make every machine have the same IP on the same subnet, just set them all to the same IP. Won't work, of course, but what do you expect? They all have the same IP. 

if you want to make the route change permanent, add the -p flag. The metric doesn't really matter very much in this context, it's more of a statement of priority, so the system knows which route to choose, all other things being equal. 

You'll be using pkgadd instead of apt? You're not going to run into any huge gotchas. If anything, you may have a better experience, seeing how much of what you're installing is Sun software (apparently, all of it). The problems you can have with proprietary Unix are most notable when you move outside the proprietary comfort zone. If you like non-mainstream OSS, you're going to have to learn to love to compile from source, because Solaris binaries may not be available. Otherwise, what? Different gui? Sun hardware is solid and elegant. Support is responsive like you can't even imagine if you're used to Dell, et al. Solaris is stable and exhaustively tested. Imho, it's not worth the price for anything but a critical application, but if you've got it, it's very enjoyable. 

I've done it on "zero-downtime" systems. Really though, you're just as likely to lose a different drive when the RAID rebuilds...I swapped one out once, then ended up swapping it back in when another drive started throwing errors during the rebuild. It's a philosophy question really: if you believe in pro-active stress testing (both of the array and of your cardiovascular system) then you should swap your drives. But really, you're never going to know which drive is going to go bad next. It's not at all unlikely that you could lose the newly replaced drive before you lose any of the older, proven drives. That being said, I'd waste my time on stress-testing my backup solution, and leave the drives in peace until they start actually throwing errors.