you get 1,44115188075856E17. It is not the same value you entered, you have lost some precision. Try this 

The annoying thing is, that the button to change the connection ("Verbindung Ã¤ndern" for me) is inactive, and I have to open a new tab, open the connection and copy the contents of the inactive window to it. Is there a better way to handle this situation? 

I'm just testing my PowerShellScript to do bulkcopy of multiple tables between sql-server and oracle Databases. When the destination is Oracle, than I can monitor the progress by by executing select count(*) from Mytable the single batchsize chunkss individual committed. Using a sql-server destination, there seems to be a single transaction. Which was rolledback due to network error at first trial. The second is still in progress. Will the use of internal transaction commit the individual chunks? The use of bulkcopy seems a bit of an all or nothing, if it fails you can restart from the beginning. 

The most critical resource is RAM. Each running Oracle instance allocates some RAM for its own, when just started and not under load. We are running a 10g with 10 and 11g with 8 instances, but these are development servers. After restart of the OS some of the Oracle services don't start automatically and must be started manually: Oradim -startup -sid xxx. We are just beginning to use Automatic Memory Management, but the situation keeps different from SQL Server, where you can add databases as much as the disk space gives. I your case, with more instances on one machine the SGA for each instance becomes smaller, less precompiled sql can be cached and the machine has to do more sql compilation, which reduces performance. Adding RAM might help in your situation. 

Is there something like a directive which I can use in a script to force SSMS to enable/disable SQLCMD mode? 

For procedures whose definition are longer than 4000 Characters. Any proposals to handle these cases in a similar way? Edit: It is not only that I want to avoid flushing of cached plans. I also have to cope with different customers having different versions of a stored procedure where I only want to replace one of these variants by a newer version. 

Pens x1 (nickname) --- Pen (MaterialType) ----- 1 (NumberOfPieces) Pens x2 (nickname) --- Pen (MaterialType) ------5 (NumberOfPieces) 

Disclaimer: I am a beginner in the field of database. I am trying to design a database in PostgresQL which will record the sales and purchases of items. I intend to use this database as the model with EF and WPF using MVVM approach. Here is the problem with the design. This is the simplified description of my schema. I have a Clients table, an ItemDetails table and a RateChart table, among others. The Clients table records details for each client, including their address. The ItemDetails table records details for each item we sell to our clients, including a short description for the items. We do update the description from time to time. The RateChart table has the different rates for items for each client. Now, I also have to save the details of each and every sale. So, I have an Invoice table, and InvoiceItems table. The Invoice table saves the invoice number (primary key), the date, the total amount of the order and the client name(foreign key, refers to client name in client table). The InvoiceItems table, on the other hand, records the items sold, the quantity of each item sold and the rate at which it is sold. Now, here's the problem. The items sold in InvoiceItems list is a reference to the ItemDetails table where details of all the items are saved. Now, the details of items are changed from time to time. For example, the description of items are changed. So, whenever I change those item details, those changes will cascade down to this InvoiceItems table, and will make the older invoice records erroneous, because that was not the description when the product was sold. Same problem lies with the Client field in sale table. Whenever I change an attribute any record in the client table, the changes will cascade down to the sale table. Here is the schema for illustration. 

Disclaimer: I am a beginner in the field of database. I am trying to design a database in PostgreSQL which will record the sales and purchases of items. I intend to use this database as the model with EF and WPF using MVVM and Repository approach. The simplified proforma is like this: Items: Details of items we deal with. Purchases: Records the details of items we purchased. Sales: Records the details of items we sold. Stock: Quantity of each Item in hand. Now, I also have a stock table where current stock of each item is maintained. The dilemma I am facing is this: When any Invoice is generated, it indicates that a sale has occurred, and the stock of each item in the Invoice is updated accordingly. Similarly, when a purchase from our side occurs, we record it in the purchases table, and the respective items in Stock table has their quantity updated. I would have to also consider the scenario of correcting the stock table if an existing invoice is modified or deleted. These are the only three scenarios when the stock table would change. Now, I can achieve this update of stock table using Triggers. I can also use stored procedures in order to encapsulate the entry in Purchases or Sales table along with updating the stock table. I can also do this in application logic using LINQ. Which approach would be the most pragmatic one? THE DETAILED STORY Please allow me to discuss the entities that matter in this current context. We are a reseller. We buy materials in bulk, we process the materials and repackage them and then sell them. In this table, the items we sell are marked as the entity ITEM. There is are two attributes in the ITEM table worth noting. One is MaterialType and the other NumberOfPieces. The entity MATERIAL is the raw material we buy. Now suppose, we buy 100 pieces of pens, and we resell them in units of 1 or 5. So, in this case, the records in the item table would be 

When trying to use the SQL Tuning Advisor from SQL Developer I get this error. Tuning advisor was working some days before. 

OK its late and I'm a bit lazy, but filling in the remaining 12 cases is straight forward. Not using dynamic SQL has some advantages: 

In about 95 % of the queries I only want to select the non canceled rows. Further I want to be able implement unique constraints to some columns for the not canceled columns. What are the pros and cons of using NULL in some column as indicator for not canceled ? 

but at which levels session, database, server can this setting be influenced? Edit: I found this in msdn it seems to be possible to set this on server level. In the Server Properties dialog (Connections Page) I found the option changed and scripted it: implicit transactions on: 

Inspired by Erics answer, I found the following solution which only depends on the table names and doesn't use any specific column name : 

I think it is equally important to look at the explicit added denormalisations, either added aggregate values or some fields from a master table copied to a detail copy. The argument mostly being some performance argument. If you do so enforce, that those fields are updated by triggers and it's up to the database to keep them consistent. 

It is rather easy to find links describing a feature. In most cases there is no hint in which release the feature was introduced. Often the last resort is to google for the feature in combination with special releases and to compare the hit counts or something similar like old lists of new features etc. 

My feeling is, that this transformation could have been done by an optimizer. It seems to be straight forward, but how can I be sure if he does? Comment on Igor's answer: (comparison fixed thanks to Matts comment) This inspires me to the following: 

After executing your code, you can check if it worked by selecting from the database. The next step will be to use some begin end; block not returning any result sets. Generally Ado can't process whole scripts. Using sql-server it can process batches, that are parts of a script separated by go statements. As far as I know for Oracle it can only process a single sql-statements or a begin end block. Edit: Here is a litte PowerShell you can use to try what is possible: 

You know what's really funny? This is exactly the kind of thing SSIS is designed to do. You're probably going to run SSIS on your own desktop or laptop as you learn. SSIS is part of SQL Server Developer Edition, so you can install that for free on your laptop. It also comes with the SQL Server engine, obviously. So attach the AdventureWorks db to your local SQL Server Developer Edition, and then use SSIS to move the contents of the local tables you want over to the server run by your IT department. Congratulations, you have your first ETL project! Bad news though: you're also the client. You're probably a terrible boss. If you want to learn something easier, start by using SSIS to import the contents of a spreadsheet or a text file into that target SQL Server instead. 

No, but what the presenter was probably doing was using Grant Fritchey's technique to search the plan cache for a string. You can search the plan cache for your view name. That technique has a few drawbacks. It's very slow on a busy/large production server - it doesn't hold folks back by blocking, but it can just take a really long time to search, say, 10-100GB of query plans looking for a string. Also, it only searches the literal plan - if you have a view calling another view, the nested view's name may not show up in the query or the plan. 

Take a step back and ask, why are we copying log files? SQL Server LDF files contain transaction log data, but they're not terribly useful by themselves. To work with a database, you need both the data files and log files. In order to access those files directly via the file system, though, you'll either need to stop SQL Server or use snapshot technologies like VSS. If you're trying to synchronize transactions across multiple databases, check out technologies like transactional replication. If you're trying to back up the database, use the SQL Server BACKUP DATABASE command. 

Scaling out writes isn't that easy, and requires code changes just like it does in Azure SQL DB. Does either of those do true load balancing? No. Neither Azure SQL DB nor SQL Server Always On Availability Groups truly balance load across multiple secondaries. You can get into a situation where one replica is running a handful of awful queries, and the rest are sitting around idle. And in that situation, you can still end up getting new queries assigned equally to both the underworked and overworked servers. Doing true load balancing - keeping the work requirements similar across multiple replicas - is left as an exercise for the reader. 

Now, if we need to delete or modify any record of PURCHASES or INVOICE, we also need to modify the CurrentStock in MATERIAL as well as the corresponding records in STOCKHISTORY table. Here is a rundown of STOCKHISTORY table: 

Now here is where it gets murky. And I need help with the schema here. The FlowDirection records whether it was a purchase or a sale. The PurchaseId records the invoice number of the purchase IF it was a purchase. The SalesId records the invoice number of the sale IF it was a sale. There is a constraint on the table where the nullity of PurchaseId and SalesId attributes are XOR'ed, i.e. One and only one of them will be null for a given record. I am thinking of getting rid of the FlowDirection as it is obviously redundant. Secondly, I also will get rid of the Invoice attribute, as it is already recorded in PurchaseId/SalesId. Or, I can point Invoice column to a union of Num attribute of the INVOICE table and the Invoice attribute of the Purchase table (I have got to rename all these tables and attributes too). Which one should be a better approach? 

Now, the solution I have thought of is highly inelegant. I have considered saving the client details for every sale in the Invoice table, and the item details in InvoiceItems. Of course, that would be a great deal of duplicate data. Another solution might be to keep the defunct details of items and clients, which would mean polluting my ItemsDetails and Client table with records no longer valid. Of course, I can add a boolean IsValid column, but then again, the solution does not seem to be succinct. Any help would be heartily appreciated. 

Material records the material whose stock has changed. Amount records the amount by which the stock has changed. Invoice records the invoice number of the transaction. 

So, in PURCHASES table, we record the Material we have bought and the Quantity of it. With each purchase, we need to make three modifications: 1. Insert the purchase details in PURCHASE 2. Modify CurrentStock in MATERIAL(increase it by the amount we have purchased) 3. Insert the amount of increase in Materials in STOCKHISTORY table. Please note that StockHistory does not record the cumulative stock, but just the amount of increase/decrease associated with each transaction. Similarly, for each sale, we need to make three modifications. 1. Insert the sales details in INVOICE. 2. Modify CurrentStock in MATERIAL(decrease it by the amount we have purchased) 3. Insert the amount of decrease in Materials in STOCKHISTORY table. Please note that INVOICE table here represents the invoices generated by us, i.e. the sales made from our end. Here is the schema: