I think all that logic you are using to check which digits to multiply by 2 and which not to is a little overengineered. If you generate the digits from right to left, it's always the even positions that get doubled. You can also make your life simpler by generating the digits one at a time, rather than the whole number. If you do it like this, you don't even need to keep the digits in a list, as you can add them to the return number directly. Another possible optimization for the doubled digit result is to use a look-up table for the values. With these ideas in mind, I rewrote your code as: 

Your first setting of all the even numbers to 0 is not very efficient, the whole point of sieving is to avoid those costly modulo operations. Try the following: 

At the heart of your problem you are trying to solve a single linear Diophantine equation. Because 3 and 5 are coprime, their greatest common divisor is 1, and hence your equation, \$3a + 5b = n\$, will always have an integer solution. It is easy to check that \$a = 7n\$, \$b = -4n\$ is indeed one such solution. Of course you want both \$a\$ and \$b\$ to be positive, so if we take the more general solution to your equation, \$a = 7n - 5k\$, \$b = -4n + 3k\$, and impose \$b \ge 0\$, we get \$k = \lceil 4n / 3 \rceil\$. Let's put this into a Python function: 

We can do even better though: an alternative approach is to use something akin to a branch and bound approach. We know that the maximum distance that our made up chess piece can move is \$\sqrt 5\$, so we need not consider cells that are further from the destination more than that value times the number of moves left. A possible approach is to create a board holding minimum number of moves to get to the destination from any other cell: 

The cross product of two vectors is perpendicular to both vectors, unless both vectors are parallel. So you could simply take the cross product of your first vector with , unless it is parallel to , in which case you could use . If you use lists rather than dedicated classes with attributes and are willing to use numpy, this gets ridiculously short: 

If you test this, you will find that, while they both produce the same result, the performance is already 100x faster for this small example: 

If you want to produce correct results, you should simplify after each operation. Euclid's algorithm is very fast in finding the GCD, and very simple to code. If you have a function like: 

Now that you have a way of extending a prime list, you just need to extend it until it has sufficient items in it. A not too sophisticated way of going about it could be: 

The algorithm searching for the pair in the sorted list is \$O(n)\$, but the sorting is \$O(n \log n)\$, which dominates and is also the complexity of the full algorithm. 

I wouldn't be surprised though if, this being Python, the explicit looping took longer than the previous version. But this would be preferred in a lower level language. Another possible optimization is to scan the left half of the array prior to the copying into the buffer, to avoid moving around items that are already in the right place: 

Since we do constant work per entry, and keep a constant number of values as state, this solution uses \$O(1)\$ space and takes \$O(n)\$ time. For completeness: 

There is no native Python data structure really well suited to create a FIFO queue. There is in , but it is a little obscure language feature, and also not ideally suited, so I'm going to pass on using it. Your implementation using a list and removing items from the front will lead to a terrible worse case performance, probably quadratic in the number of nodes. At the cost of not releasing the memory early, I think it is better to never remove items from the queue, and use an indexing pointer. 

where is a function returning all primes smaller than . With a very straightforward implementation of the sieve of Erathostenes, my laptop takes about 6 seconds to compute all primes less than , and about 2 more seconds to iterate over them to give the right answer to the problem. 

It's not very relevant for traversal, but a dict-of-sets may be a better overall structure than a dict-of-lists, as it would let you check for edge existence between two random nodes in amortized constant time, as opposed to worst case linear on the number of edges. Using sets instead of lists for things like is bound to have a significant impact on performance for larger graphs though. Another point to consider, since you are building an iterator, is to make your function return, well, an iterator of course! You can always materialize them calling on the return, but for traversals of huge graphs it may spare you a generous amount of memory. 

You are actually scanning the whole dict twice for every word (two calls to ), so I would probably refactor even further, get rid of the and have the following single one: 

You would of course have a larger dictionary with all possible conversions. You could get fancy and store only half the conversions: 

Make sure you stress that you are not validating the input, you shouldn't use for that, but documenting the function's specification. I have also removed magic numbers from the above function: it is much clearer what stands for if you make it explicit that it is the number of letters in your alphabet. Try to stick to the conventions of the language, which for Python means PEP8: read it, learn it, love it. Especially if you end up coding on a whiteboard or a google doc, keeping your max. line width short is a great habit to have. And if you happen to be interviewed by a hardcore Python geek, it is probably good to avoid unnecessary parenthesis or missing spaces! While it is true that the worst case performance of look-up and insertion into hash tables, i.e. , is O(n), it is also true that they take constant time on average. The only practical drawbacks they have are malicious opponents (but this doesn't apply if your keys are single char ASCII strings), and the fact that dynamically resized hash tables will have the odd operation that does take linear time, so you probably don't want that data structure used in a blocking call in a pacemaker's firmware! But in many, many typical interview situations, the right answer to "can we do better?" is a hash table: for all practical purposes, your single-liner takes linear time and space, and you should make sure you convey that to the interviewer before trying anything more complicated.