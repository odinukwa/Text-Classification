Can you point the 'full path' to the UNC path i.e. instead and see if that works? Just like when you map the "X" drive to just use that in the full path of one of your packages and run to see if it'll work. If one of your jobs work like that (assuming most all are setup the same and this way, etc), you can probably script out the SQL Agent jobs through SSMS by pressing F7 (once SQL Agent jobs is highlighted), selecting them all from the right pane window, right click, then create to new query window, then do a mass CTRL+H and do a find and replace to replace with , and then run that. Just be sure the SSIS proxy account or the SQL Server Agent account has appropriate NTFS and SHARE permissions where the SSIS packages reside to read them. 

Yes, active transactions are included in transaction log backups and this is how the database restore option works. It's important to understand how database restore operations and options work in SQL Server that'll help you get a better understanding as well so I put another hyperlink at the bottom with nothing quoted for further reading on this topic. Below are some resources that give good explanations for this as well to help put all the pieces together. Some of the article may be a bit dated, but the explanations seems accurate and the concept is still the same for your inquiry. Transaction Log Backups 1 

Consider taking a look at sys.dm_exec_query_memory_grants and sys.dm_os_sys_memory to get your started on your troubleshooting journey. 

From the MySQL Connections screen click on the little wrench icon to the right to bring up Manage Server Connections window. 

Additional Resources (These articles seem to have some references to potential parameters in SSIS package areas to set for smaller transactions, etc. The titles or the steps may be for a different ultimate goal, but there is potentially applicable content in these for what you may need to change in your SSIS logic to rectify your issue so these may be worth a simple read.) 

Consider testing with the parameter too and see what results you get with your logic. This may work for your need as well. 

Create the [user account object] service account in AD () Create a SQL login i.e. the service account in #1 () Create a SQL credential with that same () account credentials, Create a SQL Server Agent Proxy account to use for the "Run As" from the SQL Agent job to execute PowerShell 

Make sure you have a full or log backup on your primary DB first, and then do the shrink operation from the primary DB. It'll probably grow again due huge transaction so you may want to see if throwing in a few log backups may help reduce the growth before your full backups occur or else determine what's causing the logs to grow so much bigger than the data file -- typically it's due to poorly written queries or HUGE transactions or maybe not doing enough backups to free up space since your in FULL recovery mode. 

Do a FULL backup of the primary DB (now or whenever), copy that backup file over to the secondary server, then restore that to the secondary DB in standby mode. Afterwards, see if your LS jobs run successfully on secondary once you get enough fresh logs to apply to it. 

It would seem the explanation would be that your your committed transaction sizes with the operations are HUGE. Simply make the commit\batch size parameters of your logic in SSIS or your execute TSQL in the package of a smaller size. Try testing with 100, 1000, 10000, 100000, and so on to see what gives you the best result and prevents the issue from occurring. If the transactions are smaller, once the committed transactions are committed in recovery mode, then the log space of the committed transactions can be reused by subsequent (or other) transactions. 

I have to agree with Max's comments about running the stuff that would normally run when the issue occurs as a simple step to confirm whether or not those items/processes are causing the issue -- process of elimination should be simple enough. Since you pretty much have the day and timing down to a science when the issue occurs, you could schedule a SQL profiler trace to run via a SQL agent job and give it a stop time (@stoptime) to stop the trace to see what details the trace will provide. Since you asked about where else you could start to troubleshoot, I think a SQL profiler trace would give you a lot of detail to go by actually. I'll paste what I have on this when I did it on SQL Server 2008 R2 below. SETUP DETAILS Follow the below instructions for scheduling a trace with a specific start time and a specific end time. You'll go to 'SQL Server Profiler' and build your trace criteria as usual but be sure to save to a file somewhere valid on the server itself, check the 'enable file rollover' option, and specify a trace stop time. Once all the criteria is selected and filtered for what you need to capture, run it, stop it, and then navigate. . . FILE | EXPORT | SCRIPT TRACE DEFINITION | 'select' the SQL Server 2005 - 2008 R2 | and then save the file somewhere you can open later. Go to the file, open it and you should get something similar to the below. KEY POINTS 

@JulienVavasseur is correct in that you need to format your queries using Ansi92 join syntax. If nothing else, it will make it easier for us to read and help you with your questions. However, looking at the problem, it seems as easy as adding a Distinct to the count of session.id to reduce it from 7882 to 752. When you ask for a count, you're going to get a count of the rows, not a distinct count of the sessionid (unless you ask for distinct). 

You should break the quantity ordered into two columns or at least one integer column (but I like two). MinQuantity and MaxQuantity. Note I don't usually allow for abbreviations in tables or columns, but I'll make exceptions if it's universally accepted like Min and Max. Second, name your table appropriately, something like PriceBreak is more descriptive in my opinion than Exception (that could mean anything). Once we have proper table names, column names and data types, then the solution becomes much easier. You will use the itemname and itemquantity to join to the proper row (itemquantity between minquantity and maxquantity). One of the reasons that it's a good idea to name everything well is that the code becomes self documenting and is easier to create and troubleshoot. In that scenario, you won't even need a case statement, just a LEFT JOIN and an ISNULL. 

From this link: $URL$ Cardinality and Modality are the indicators of the business rules around a relationship. Cardinality refers to the maximum number of times an instance in one entity can be associated with instances in the related entity. Modality refers to the minimum number of times an instance in one entity can be associated with an instance in the related entity. Cardinality can be 1 or Many and the symbol is placed on the outside ends of the relationship line, closest to the entity, Modality can be 1 or 0 and the symbol is placed on the inside, next to the cardinality symbol. For a cardinality of 1 a straight line is drawn. For a cardinality of Many a foot with three toes is drawn. For a modality of 1 a straight line is drawn. For a modality of 0 a circle is drawn. zero or more [b4] 1 or more [b5] 1 and only 1 (exactly 1) [b6] zero or 1 [b7] Cardinality and modality are indicated at both ends of the relationship line. Once this has been done, the relationships are read as being 1 to 1 (1:1), 1 to many (1:M), or many to many (M:M). 

You need a tally table or calendar table that includes all years (that you want to report on). Call the table Allyears or something like that. As a field, you could put something like yearstart datetime and populate it with January 1 of each year (consider yearend as well). Then simply join the Allyears table to TestDates and display the years you want. Note, you have to decide if they were a customer on January 1, do you count them for the year? Or did they have to be a customer on Dec 31st to be counted. That will determine how you should create the table and join it. Apparently, I'm the only one who has trouble posting Sql code, the site or my company's security doesn't allow the edits when they contain code (sometimes). 

You feel like they should be separated because that makes perfect sense in the relational world. But as you said in the NoSql world and in MongoDB in particular, you want to group like-items together. I've not done extensive research on Mongo, but have spent some time with the online classes and I believe the answer is to store them together. You probably realize there is no such thing as a join in Mongo and therefore if you wanted to get 100 rows and get their corresponding images, you'd have to get the IDs for the 100 rows and then get 100 rows by their identifier (object_id or whatever). In other words, you have to do manual joins. 

For the security credential/context which the application uses to authenticate to the DB, set an EXPLICIT DENY to it on the table to not allow it to access it at all (e.g. SELECT, UDPATE, DELETE, etc.). You should really have the application credential locked down in the DB which it uses anyway to ONLY allow it to ONLY have the EXPLICIT level of access it needs to each DB object specifically in the name of security. So why the user table is a concern now, wonder what other tables, etc. it has access to due to poor security design. 

Some things to check, read up on, or try not knowing the Server OS version, etc. (Sounds like a Windows Update broke something with the OS and SQL error log is catching it (extended event notifcation) but perhaps not able to send a notification, etc.) 

You can also run this adhoc this way without SQL Agent scheduling specifying the correct DateTime variable when you want it to stop. Once the process has run, you just go to the file and open it as usual. See script logic at bottom with comments on how to confirm this isn't running any longer, etc. -- any traces for that matter and how to stop them from running if they are. NOTE: In my case we scheduled this job start with SQL Agent job at 7 AM and then put the DateTime variable value for the @stoptime argument passed to the sp_trace_create object so it quit running at 8 AM on this day. Once in the office and reviewing this, we were able to sift through and determine the issue and correct. We filtered our trace criteria down as much as we could though beforehand when we built the script file that saves to disk to give the TSQL for scheduling. 

Give this a shot since you state brings up a NULL value, despite restarting the server. 1. Run the below with the local as the second argument. . . TSQL 

Condsiderations for security options in SQL Server (Two things to mention for typical simple configurations) 

Check the SIDs of the accounts in SQL and AD before the SQL login deletion to confirm they are indeed the same and the AD admins didn't really delete and recreate the AD user account just telling you otherwise. Articles: $URL$ and $URL$ There may be a way to get both from PowerShell but confirm what the AD domain and forest functional levels are and research with your version of SQL Server for known bugs -- maybe it's time to upgrade/update one or the other. 

Have you considered creating additional custom roles for more "all object" DB-level access that each person needs rather than granting them the role as that will probably give them more than they actually need to DB level objects as well. I usually give what's needed exactly and nothing more for them to do their job and if there's a "usual" or "standard" need for DB level object access to all objects in a DB, I create a custom DB role sort of like the the but see my below example. This way you can grant the people what they really need to ALL DB object in a particular DB if you're not getting object level explicit in your DBs for their security. 

So do a FULL backup on your primary DB Run the SHRINKFILE on the primary DB of the mirror without ALTERing to SIMPLE RECOVERY mode 

Noting that the. . . 1. will allow access to all tables 2. will allow , , and access to all tables 3. will allow access to all executable objects 

A few more suggestions—adding as an answer to accept more characters so I can include more. . . BE SURE TO RUN EVERYTHING AS ADMINISTRATOR 

Issues when Loading Data with MySQL LOAD DATA INFILE You appear to have a few issues going on here and to resolve you can make a few adjustments to get your data to load without error. I've source referenced and quoted the items in more detail below so you can read up on each for a more thorough explanation. In short though essentially you can: