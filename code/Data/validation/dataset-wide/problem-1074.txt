If both tables have the same columns and column order and you are trying to match them all. Or alternatively 

To divide by 1000 with integer division to get and then calculate modulo 10. Something similar should be possible in all DBMSs though the exact syntax may vary. E.g potentially something like 

Shows estimated rows of 1856. This is exactly what would be expected by getting the estimated rows for the 19 equality predicates individually and adding them together. 

The plan for this can have a seek on the table specified to the left of the like. i.e. would not be able to make use of the index on that was used above but could seek one on . The suggestion in the other answer of indexing a calculated column on both sides is more flexible however. As for a nested loops plan either table can be on the inside and it would also allow a many to many merge join without requiring a sort. 

2) Otherwise you could loop through all databases (and query in each) with something like this code for a more reliable and flexible sp_MSforeachdb 

So it appears that all the computed column definitions get expanded out then during the Project Normalization stage all the identical expressions get matched back to computed columns and it just happens to match in this case. i.e. it does not give any preference to the column. If the table is re-created with the following definition 

Which (if trace flag 3605 is enabled) dumps out limited information such as the below to the SQL Server error log. 

A similar documentation bug was filed by Aaron about . Another annoying aspect of that stored procedure is that the must be passed as 'bigint' and it doesn't accept a literal integer. I assume that this is also because it is an extended stored procedure. 

Your app could of course try and dynamically discover the limit by simply trying statements with ever greater amounts of columns but that is not a serious suggestion. 

For larger batch sizes the version seems generally faster. The TSQL Querying book also explains why can have a performance advantage over sequence. The is table specific and isn't. If disaster was to strike mid insert before the log buffer was flushed it doesn't matter if the recovered identity is an earlier one as the recovery process will also undo the insert, so SQL Server doesn't force flushing the log buffer on every identity cache related disc write. However for Sequence this is enforced as the value might be used for any purpose - including outside the database. So in the example above with a million inserts and cache size of 1,000 this is an additional thousand log flushes. Script to reproduce 

You can do it within the same transaction it just needs to be pushed down into a child scope so compiled separately. 

It has no clause so it must process and aggregate all 40 million rows. SQL Server will not take advantage of the index order and skip scan ahead to the next once it has found the for the current group but will continue processing the other rows in that group. Each group has an average of about 30,000 rows. As you have another table that lists the 1,386 distinct IdTag types then you could try the following instead. 

In order to apply this change the process will need to acquire a lock and once it's got that it should be pretty instant (a metadata only change.) The only risk is that if you have a long running transaction that is using the table whilst the statement waits to acquire the lock any other subsequent requests coming in that require a lock on the table will be queued up behind it to prevent your alter request from being starved. So you could end up blocking production queries for a while until the lock is granted and the change made. 

You can only use the keyword in place of an expression, not inside an expression. So you would either need to just use the value of the default directly (could be looked up from the system views to be dynamic) or spilt it into two update statements with appropriate mutually exclusive clauses - one using and the second using an expression - if you really want to use that keyword. 

The grammar for accepts a which allows the module name to be in a variable. This does accepts three part names so the below does the trick. 

Will be able to take account that the tables have 100K rows each and give a different plan. In SQL Server 2012 you can only add indexes to table variables via constraints. If the values are unique you could use 

So it would be simple to just scale the estimates down to adjust for the new table cardinality by multiplying by . The scaling down works fine for as 1,000 * 0.75 gives the correct result but if you try querying the 250 rows that you just deleted this will not show correct estimates. 

has higher datatype precedence than so you need to cast the explicitly to in the string concatenation to avoid the string being implicitly cast to an integer (which fails as the string is not numeric) 

For fact data you open the cube and can similarly set the storage mode by selecting a measure group and altering the storage mode to one of HOLAP/ROLAP/MOLAP in its properties window. 

As you are on SQL Server 2012 you could also create a sequence object per table and apply it to the desired auto incrementing column with a default constraint that calls . The two methods have some differences in behaviour which you may or may not find desirable. 

I agree that there is no point in creating an column clustered index here but I would just create the composite PK as rather than . There is no point having 2 copies of the data, One in the heap that never gets used and one in the NCI. In fact it is downright counter productive as shown below. Your NCI will still get page splits anyway if you are not inserting into the table in order of primary key so you have not avoided the problem. You might also consider a constraint or index on the reversed key order as well depending upon what queries you run against that table. In general NCIs can be slightly be more compact than the equivalent NCI as they do not have the status bits B or (if no nullable columns) but in this case this is more than outweighed by the necessity to store the 8 byte RID pointer to the row in the heap as can be seen from the below. Script 

You have an index on that can be seeked into. This satisfies the range predicate on and also supplies the required ordering for your row numbering. However it does not cover the query and lookups are needed to return the missing columns. The plan estimates that there will be 30,000 such lookups. There may in fact be more as the predicate on may mean some rows are discarded after being looked up (though not in this case as you say COLUMNF always has a value of 1). If the row numbering plan was to use a clustered index scan it would need to be a full scan followed by a sort of all rows matching the predicate. is considerably cheaper than the so the plan with lookups is chosen. You say that the plan with lookups is in fact 5 times faster than the clustered index scan. Likely a much greater proportion of the clustered index needed to be read to find 30,000 matching rows than was assumed in the costings. You are on SQL Server 2014 SP1 CU5. On SQL Server 2014 SP2 the actual execution plan now has a new attribute Actual Rows Read which would tell you how many rows it did actually read. On previous versions you can use to see the same information.