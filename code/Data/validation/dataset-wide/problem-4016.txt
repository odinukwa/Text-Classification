Yes, of course there are. Any result that is equivalent to extra induction axioms will require nonstandard models for a semantic separation. For example, Hirst proved that the pigeonhole principle $\mathsf{RT}^1_{<\infty}$ is equivalent to $B\Sigma^0_2$, and thus is not provable in $\mathsf{RCA}_0$. But $B\Sigma^0_2$ holds in every $\omega$-model. Similarly, there is a variation of $\mathsf{ACA}_0$, denoted $\mathsf{ACA}'_0$, which has an axiom asying that for all $n$, $0^{(n)}$ exists. In contrast, $\mathsf{ACA}_0$ only proves that $0^{(n)}$ exists for standard $n$. So every $\omega$-model of $\mathsf{ACA}_0$ is a model of $\mathsf{ACA}'_0$. For an example with non-arithmetical induction: $\mathsf{ACA}_0$ plus $\Sigma^1_1$ induction proves the consistency of $\mathsf{ACA}_0$, so $\mathsf{ACA}_0$ does not prove $\Sigma^1_1$ induction. 

I will try to answer two questions: 1: "But how does set theory "know" which $n$ is meant?" Within ZFC, we have a standard construction of the natural numbers. So for each natural number $n$ in the metatheory, and any model $M$ of ZFC, we can identify a set in $M$, say $n^M$, which plays the role of $n$ within $M$. Every model of Peano arithmetic starts with a copy of the standard natural numbers, possibly followed by some nonstandard "numbers". It's the other direction that's problematic: if $p$ is a nonstandard number in $M$, then clearly there is no standard $q$ such that $q^M = p$. 2: "Actually I don't even know if it is possible to give a formal definition of the set $\{s \in A^n : \phi^A(s(0),...,s(n-1))\}$" (in ZFC). Call that set $B$. The definition of $B$ uses the fact that $n$ is a standard number. It looks like this: $$ s \in B \leftrightarrow (\exists t_0, \ldots, t_{n-1})[ t_0 = s(\dot{0}) \land t_1 = s(\dot{1}) \land \cdots \land t_{n-1} = s(\dot{n-1}) \land \phi(t_0, \ldots, t_{n-1})] $$ To make this easier to follow, I put a dot over the numbers in that formula that actually represent fixed set parameters in the formula (like $\dot{0}$). In a particular model $M$, you would replace $\dot{0}$ with $0^M$, etc. The subscripts are not actually part of the formula; I could call those variables $a, b, c, d, \ldots$ instead. Each of the dotted parameters is found using my response to the first question. You could further expand the definition of $B$ by replacing each subformula of the form $t = s(\dot{m})$ with its own definition in ZFC. The fact that $n$ is a standard number is what allows us to write a formula that actually has $n$ different variables. 

Failures of the deduction theorem are one of the more mysterious topics in logic, in my experience. The motto is that axioms are stronger than rules. Here is the simplest nontrivial example that I know. Start with propositional logic with two variables $A$ and $B$. Add the single new rule of inference $A \vdash B$ to the usual Hilbert-style deductive system, with no new axioms. Note that this does not in any way change the collection of formulas that can be derived. (Proof: the first time you use the new rule, you already had to derive $A$ in the original system, but you cannot, because the original system only derives tautologies. So you can never use the new rule.) Thus the new system has the rule $A \vdash B$ but does not derive $A \to B$, and hence the deduction theorem fails. But this new system is not completely trivial. If we add $A$ as a new axiom, then we can derive $B$ in the expanded logic, which we cannot do in ordinary propositional logic. So there is an interplay between the rules of inference and the axioms of a given theory. The deduction theorem for first order logic shows that this interplay is very well behaved in that context: an arbitrary first-order theory $\Delta$ with the usual deductive system has the derived rule $\phi \vdash \psi$ if and only if it has the derived rule $\vdash \phi \to \psi$. In retrospect, there is no reason to expect this to hold for arbitrary sets of deduction rules, because new axioms may give additional strength to the existing rules. As François G. Dorais has mentioned in the comments, more complicated examples are known in proof theory. They are similar to the above example in that they weaken an axiom by replacing it with a rule. The general idea is that an extensionality axiom of the form $x = y \to f(x) = f(y)$ might be replaced with a rule $x = y \vdash f(x) = f(y)$. This suggests immediately how the deduction theorem can fail: if $x$ and $y$ are terms that are not provably equal, but are equal in some interpretation, then the extensionality axiom might fail in that interpretation even if the rule of inference is satisfied in some sense. But this is just a heuristic sketch of the argument. For a short, rigorous explanation, see "A note on Spector’s quantifier-free rule of extensionality" by Ulrich Kohlenbach, Archive for Mathematical Logic 40:2 (2001), pp 89-92. 

In general, when working in constructive mathematics, the strategy for proving $Q \lor R$ is to prove $Q$ or to prove $R$. In this case, just knowing abstractly that "there is an $n \not = 1$ that divides both $a$ and $b$" does not directly tell you whether $b$ is composite or whether $b$ divides $a$. So you will need more information to come up with a constructive proof. If you know more than the fact that there is such an $n$, and you actually know the value of $n$, that would help. In particular, if you could tell whether your value of $n$ is equal to $b$, then you can tell which side of the disjunction holds. So we could use the fact $(\forall k)[k = b \lor k \not = b]$ to finish the constructive proof. Many real-world constructive systems include additional facts like that about the natural numbers, which would not be true for other objects. In particular, many constructive systems prove the sentence $(\forall n,m \in \mathbb{N})[n = m \lor n \not = m]$, which says the natural numbers have decidable equality. The motivation for accepting this in constructive systems is that, given two concrete (terms for) natural numbers, we could in principle examine them to see if they are equal. This is not the case for other objects, such as real numbers, for which equality is not decidable. The fact that equality of natural numbers is decidable is provable, in many systems of constructive math, from induction axioms that are already included. With a little work, these constructive systems prove that each natural number is either composite or not composite. And, with that extra fact, the remainder of the original proof goes through constructively. This may be out of the scope of an introductory proofs class, but it is one way a constructivist could prove the result. 

$\mathsf{RCA}_0$ is a standard system of second-order arithmetic with induction for $\Sigma^0_1$ formulas with set parameters and with the $\Delta^0_1$ comprehension scheme. Therefore, there is some problem with the example that the question tries to construct. I don't see any problem with putting the defining formula $\phi(X)$ from $B$ into normal form. 

The problems with using "et al." have been discussed at length on this blog post [1] and probably elsewhere on mathoverflow. However, these authors have left you in a slightly ridiculous situation if you try to give a full citation in the body of your text. So it seems that biting your tongue and using "et al." is the best way forward in that (apparently very unusual) situation. It seems particularly bad to me to not list all the authors of a paper in the actual bibliography entry for the paper. Personally, I would list all of them in my submitted version and see whether the journal editor wants to force the matter. 1: $URL$ 

*: Traditionally, a "Lindenbaum algebra" or "Lindenbaum–Tarski algebra" should be defined with the dual ordering of the ordering I use. But the ordering in which $0=1$ corresponds to the top of the algebra matches better with the diagrams we create to illustrate relationships between different axiom systems, such as 1. People also use the reverse ordering in the context of set theory, where large cardinal axioms are sorted by consistency strength, e.g. 2. 

The point is that Principia was not intended simply to be a development of mathematics in type theory: it was intended to make a philosophical argument that mathematics could be carried out using only "logic". Thus translating PM so that the underlying mathematical principles are more clearly described would miss the point that there are not supposed to be any underlying mathematical principles, only "logical" ones. This differs sharply from Gödel's paper, which was intended to be mathematical (the result can be viewed as just a particular type of combinatorics or number theory) rather than philosophical. 

In contemporary practice, in formal arithmetic, it is normal practice to use the term "natural numbers" and the symbol $\mathbb{N}$ to refer to the standard natural numbers, i.e. to identify them with the informal counting numbers (e.g. this is Kaye's convention, and many others'). The need for a distinction between standard and nonstandard models is particularly evident in my own field of Reverse Mathematics; we have a different convention that $\omega$ refers to the standard numbers and $\mathbb{N}$ refers to an arbitrary model at hand (e.g. Simpson's Subsystems of Second Order Arithmetic). Part of the issue here may be that the meaning of the term "standard model" $\mathbb{N}$ can be interpreted in several ways. From the perspective of a certain kind of realism, it refers to the "actual" counting numbers. From the point of view of a certain kind of formalism, it refers to the natural numbers in whatever metatheory is being used at the moment, so that the "standard numbers" are the ones that are metafinite and "nonstandard models" have numbers that do not correspond to numbers in the metatheory. In any case, the notation $\mathbb{N} = \{0, 1,2, \ldots\}$ is intended to convey that $\mathbb{N}$ is identified with the usual counting numbers $0$, $1$, $2$, $\ldots$ from basic arithmetic, whatever we think those are. 

Robin Chapman's answer is very apropos. Here is a theoretical answer that points out a subtlety in the question. First, recall that the primitive recursive functions are the smallest class of functions on $\mathbb{N}$ that: 

The statement for the completeness theorem is due to Harvey Friedman, 1976, "Systems of second order arithmetic with restricted induction II", p. 558 of: Meeting of the Association for Symbolic Logic, John Baldwin, D. A. Martin, Robert I. Soare and W. W. Tait, The Journal of Symbolic Logic, Vol. 41, No. 2 (Jun., 1976), pp. 551-560, $URL$ Friedman had previously worked in systems without restricted induction. He stated the corresponding result for the completeness theorem for WKL in his paper "Some Systems of Second Order Arithmetic and Their Use", Proceedings of the International Congress of Mathematicians, Vancouver, 1974, pp. 235-242, $URL$ As Noah Schweber mentioned, the proof of the completeness theorem in WKL is essentially just a formalization of Henkin's proof of the completeness theorem from ZFC. However, Friedman's theorems show that the completeness theorem for countable first-order theories is equivalent to WKL over RCA (also to $\mathsf{WKL}_0$ over $\mathsf{RCA}_0$), which requires an additional proof for the reversal. 

Although the proof-theoretic ordinal of second-order arithmetic is very hard to determine, there is another standard method for the proving consistency of arithmetic: Gödel's Dialectica interpretation. This was originally used by Gödel to give a different relative consistency proof of Peano arithmetic by reducing its consistency to the consistency of a quantifier-free theory of functionals of finite type known as system $T$. This work was later extended by Spector and Howard to give a relative consistency proof for second-order arithmetic. The weaker system used is the same system $T$ augmented with bar recursion. The details are spelled out in section 6 of Gödel's Functional ("Dialectica") Interpretation by Jeremy Avigad and Solomon Feferman from the Handbook of Proof Theory. Although this is not a Gentzen-style analysis, it does have a certain analogy. Gentzen showed that the consistency of Peano Arithmetic reduces to that of a weak theory augmented with transfinite induction. The Dialactica-style relative consistency proof for second-order arithmetic reduces its consistency to that of a (different) weaker theory $T$ augmented with bar recursion, which can be seen as a scheme for constructing objects by transfinite recursions. The induction scheme dual to bar recursion, bar induction, is a kind of transfinite induction scheme. The proof also gives a characterization of the provably total computable functions of second-order arithmetic, much like the consistency proof for Peano arithmetic does. 

Cut elimination shows that if a sentence is provable in first-order logic, it is provable with a particularly nice type of proof in a natural deduction system without the "cut" rule, which is essentially modus ponens in that system. In particular these proofs have the subformula property – every formula in the entire proof is a subformula of the formula being proved. The cut elimination theorem and its generalizations are key tools in proof theory. Gentzen proved cut elimination in 1934 and used it as part of his consistency proof of Peano arithmetic; there is a nice survey article "The art of ordinal analysis" by Michael Rathjen in Proc. ICM 2006. The cut elimination theorem can be used to give nice proofs of the Craig interpolation theorem and other theorems from logic; one exposition is Chapter 6 of "Logic for Computer Science" by Jean Gallier. 

The use of $\mathsf{ATR}_0$ is not really necessary here. The key point is that there is a linear ordering $L$ of $\omega$, that is not a well ordering, for which there is a set $H^L$ that satisfies the arithmetical definition of being the iteration of the Turing jump along $L$ starting with the empty set. This kind of set $H^L$ is known more generally as a "pseudohierarchy". The existence of a suitable $L$ follows immediately from the standard fact that the set of well orderings of $\omega$ is not $\Sigma^1_1$ definable. The statement "$L$ is a linear order and $H(L)$ exists" is a $\Sigma^1_1$ formula with parameter $L$, which is satisfied by every well ordering of $\omega$, so it must also be satisfied by some other set $L$; this $L$ will satisfy the "key point" statement in the previous paragraph. 

Because there are nonzero r.e. Turing degrees strictly weaker than $K$, I think this may answer the question. The result is in the paper "Degrees of Unsolvability Associated with Classes of Formalized Theories", Solomon Feferman, The Journal of Symbolic Logic, Vol. 22, No. 2 (Jun., 1957), pp. 161-175. $URL$