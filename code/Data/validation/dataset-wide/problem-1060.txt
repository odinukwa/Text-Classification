MDX Drillthrough allows you to specify a limit for the row set returned. As far as I am aware it doesn't have a 'show how many rows this would produce' facility. You could fake it by having a count measure for each of your fact tables and selecting this for the slice in question before you execute the drillthrough. I'm not sure whether Excel 2010 has much in the way of facilities to manage this out of the box. Back when I were a lad Excel 2000-2003 didn't do drillthrough out of the box so you had to roll your own. This is an example of a VBA script to do a drillthrough action. You can hook the menu on the pivot table fairly easily to provide access to this sort of script. This would at least allow you to provide some sort of validation. Introspection into the pivot table through the API is possible if a little fiddly. You can get the dimension and slicer state out. Back in the SQL Server 2000 days I did a 'drill across' this way that allowed you to open another cube on the same slice (think insurance policies going across to claims). Now SSAS supports cubes with multiple fact tables (measure groups) this is less useful. However, if you could intercept the (IIRC) double click event that would kick off a drillthrough and knock that on the head then you could use the introspection to formulate a count query that could guard against over-large drillthroughs. This would then pop up a dialog box along the lines of 'You're going to drill through into 1,347,961 rows.' 

Create a cube with Date, Product and Reseller dimensions and a SalesTrends fact table with a single metric 'Price'. The MDX query below will produce counts of products for which the price is increasing, static or decreasing in a given period. 

Depends on your DBMS platform to some extent. On the SQL Server based development I did back in the late Jurassic period we used timestamps as these are automatically updated when the record is written. You load a record with: 

Stating that a B.I. project is a 'Business' initiative is something of a truism. Technically it is correct in that a project of this sort is likely to fail without effective business sponsorship. However, there are also a variety of technical critical success factors whose absence can seriously unseat a project. In practice, I suggest answering queries about a business intelligence project with something like: 

First, note that 1 billion rows can be handled effectively with a partitioned architecture on ordinary commodity server hardware. Exotic shared nothing architectures will not be necessary for this data volume, however, you will probably get significant benefits from table partitioning. Sharding is something different from horizontal partitioning, and implies a 'shared nothing' architecture, which is not supported by most versions of SQL Server1 SQL Server can support horizontal partitioning, and a shared disk architecture will be adequate for ~1 billon rows. In SQL Server, you create a partition function selects a partition based on values or ranges of values in a column on a table, e.g. 

Building temporary indexes for ETL jobs is not necessarily bad practice, as the index builds are fairly quick. Where it might not be so efficient is if you have relatively small incremental updates on very large tables, but it sounds like this is not the case here. The only caveat is if you expect the tables to grow substantially with time. If they are just work tables for the ETL then it may well be oK. If the tables are fact tables that will accumulate large data volumes over the next 5 years then the index rebuilds may get slower with time. For staging data, dropping indexes then loading will make bulk loads to staging much quicker, and you may well need to add indexes to the staging tables in order to support queries supplying the ETL process. 

Most B.I. staff tend to fall into one or more of these categories. The value to an organisation varies with individual circumstances. One common phenomenon that I observe is that people responsible for operational systems greatly underestimate the amount of work that actually takes place in these roles. I've seen one insurance company that had 170 staff just in the accounts department of their European operations. Most of their time was spend wrangling data extracts in spreadsheets and operating manual reconciliation and control processes. Management Information is very often a poor cousin in the priorities during development and operation of line-of-business applications. A poorly coordinated or non-existent data architecture strategy can cost a large amount of time and money. The default behaviour is to treat systems as silos with nobody having direct authority to fix cross-system data issues. Leave this for long enough and the net effect is back office operations employing hundreds of clerical staff (often qualified finance personnel) spending most of their time doing the work of a few stored procedures. 

QA on DB patches. On a large schema you may want to have a manual patching process for live systems, particularly if the patches involve data migration. For example, it may be desirable to have roll-forward and roll-back scripts to support backing out a change if necessary. In this case you will want to have a facility to test that the patch actually works properly. If you manage the database schema in a repository you can test the scripts by setting up before databases and generating a reference database from the repository. Running the patch script on the before database should synchronise it with the repositiry model. A schema compare tool can be used to test this. I've done a lot more data warehouse and integration work than bespoke application development lately, so I encounter this more often than a development team will in most cases. However, project management tools and methodologies do a very poor job of managing external stakeholders. In an integration project such as a data warehouse I really want to see a project management tool that really pushes external dependencies (i.e. ones I don't control) in the face of programme management. On any non-trivial integration project the external dependencies are by far the biggest drivers of wasted time. 

This is much more efficient than solutions involving multiple self-joins on the table, and will be somewhat efficient with a clustered index on the base entity and attribute type keys. You will probably also want to put type conversions into the view so you can sort the results correctly. 

Yes, the benefit comes when you want to query on a part of the index. If you put the part-used predicates first the index can be used for queries that involve those predicates but not all of the columns in the index. Also, unless you have other requirements it can help to put the most selective predicates first, as this can trim down index seek operations faster. In your case is not necessarily redundant depending on the nature of the queries on the table. However, it may not be necessary to include all of the columns. If, for example, you do a lot of queries by and then may be useful to help resolve those queries as is not in the right order to be useful for that. You might, however, find that is redundant on . From 9i, Oracle introduced a 'skip scan' operator where trailing index columns can be queried more efficiently, which can reduce the need for supplementary indexes of this sort. In a more specific case, if you are querying by and and don't need any other columns then the query could be completely resolved though the index without having to read anything from the table. You can also build covering indexes that have non-indexed columns attached. If all of the needed columns can be resolved from the covering index the query doesn't need to touch the main table at all. Finally, in answer to your last question: If you have a set of regularly used queries that are using up a lot of resources and could be tuned using an index then it's certainly worth considering. However, maintaining indexes comes with an overhead on inserts, so you will have to trade query performance off against the overhead that the indexes place on insert or update operations.