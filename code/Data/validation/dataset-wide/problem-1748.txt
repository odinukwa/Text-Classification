In this case your setup requires Server Name Indication (SNI) and somewhere you have configured another certificate for clients not able to do SNI, like MSIE8 on XP. 

Maybe or maybe not, depending if the path len is restricted to disallow a trust path of this length or not. 

The source IP is you, the source port is 53 (DNS). Looks like you have a DNS server, which is open for queries from outside. This can be intended use like to provide DNS for your own subdomains, but it also can be a misconfigured DNS server which can be used for DNS amplification attacks, which then can be used in (D)DOS attacks against other hosts. So better check if you need a local DNS server at all and if it should be open to queries from outside. If you need the openly accessible server to resolve your own (sub)domains, make at least sure it cannot be used for recursive queries. For more information have a look at $URL$ or $URL$ 

The request you want to have is not a bad one, but just a proxy request. To create it just specify your target host as a proxy. Apart from that curl might be not the right tool to generate real bad requests. 

Unfortunatly this will break, if otherdomain.com will contain pages which link to URLs with the hostname otherdomain.com (e.g. absolute URLs containing a hostname). If the user follows them it will connect directly to otherdomain.com, e.g. without your reverse proxy, and this will show up in the browser. Another way might be to use frames, e.g. frame otherdomain.com inside a page of mydomain.com. This will break if otherdomain.com sets HTTP headers for security, so that it does not get framed or if it contains which explicitly open a new window or tab. 

I don't think squid implements any logic to automatically retry and SSL downgrade if the connection failed. So you have only the following options: 

Usually not. DNS is only used to find the address for a specific name but the association (TCP connection) is between two IP addresses and not between two hostnames. It still might be that your application requires a different behavior. But this would be application logic then and will vary depending on the requirements of the specific application. 

Port 22 is ssh. Which means you are trying to use sftp (file transfer over ssh) and not ftps (ftp with ssl). But with proftpd you have to use either ftp or ftps. 

Browsers usually do a SSLv23 handshake. With this handshake they announce the best protocol version they support (mostly TLS1.2 today) but don't restrict the server to this version. Thus a server, which has a properly implemented and configured TLS stack but only supports TLS1.0, will simply reply with TLS1.0 and the connection will succeed on the first try. But, there are some bad TLS stacks or misconfigurations or some bad middleboxes (load balancer etc) on the servers side, which cause this SSLv23 handshake to fail. In this cases browsers downgrade the protocol used in the handshake, that is they try with an explicit TLS1.0 handshake followed by an explicit SSL3.0 handshake (some browsers have disabled SSL3.0 already and thus don't try this). Newer browsers will use the TLS_FALLBACK_SCSV pseudo-cipher if they do such a downgraded connection. If the server capable of TLS_FALLBACK_SCSV detects a downgraded connection with a lower protocol version it supports (e.g. downgrade uses TLS1.0 but server supports TLS1.2) than it assumes that a POODLE-like attack happens and will close the connection. But why might a client downgrade in the first place when contacting your server? 

With HTTP you have the User-Agent header. With FTP there is no such thing, which means that the server is not able to find out the client software and thus can not log it. 

Content encoded with can be easily decompressed with piping the content through . For deflate I don't know an utility but it can be done with some zlib programming. SDCH is no that easy because to decompress it you would need have access to the dictionary used for compression, which might be somewhere else in your packet capture or nowhere. Gzip and probably deflate too should be transparently handled by wireshark, so that you can see the decoded headers there. An the httpflow tool coming with the Net::Inspect perl package can also decode the gzip and deflate payload for you and can also be used to extract HTTP requests/response pairs from a pcap file and save each of these pairs as single pcap file with payload already decoded. 

You can't. This string just describes the cipher suite used for encryption and is independent from the certificate itself. You have to take a look at the certificate instead, like this: 

If you want to offer HTTPS from start you must provide a certificate accepted by the client from start. Because otherwise the client will not accept the SSL connection and you will not be able to redirect the client to a different site or a HTTP-only version. This means to support this case you 

The point of certificate chain validation is that you have locally trusted (root) certificates and from that you defer trust to certificates send by the peer. So the server should only send the leaf certificate and the intermediate certificates needed to built the trust chain from a local root certificate to the leaf certificate. Which means that you should not send the root certificate but if you don't it gets usually ignored. And you should make sure that you add the certificates in the correct order, that is first the leaf certificate and then the chain certificates in the correct signing order. Some servers or clients might work around a wrong order but you should not count on it. 

HSTS is used by the operator of a web server or web application as a better protection against man in the middle attacks. It tries this by addressing the following ways for man in the middle 

Yes, it can still be redirected. But the mail itself can still only be decrypted by the real recipient, i.e. the owner of the target key. 

Obviously, the order does not match. The first certificate #L is correctly the leaf certificate. But the following certificate #A does not sign #L as you can see from the fact that the subject of #A does not match the issuer of #L. Instead #B signs #L and #A signs #B and #R signs #A. #R then is the root certificate which should not be included at all. To fix: 

It is not really possible to do this if both sides should share the same hostname (e.g. and ), because this name will resolve to the same IP address independent of http or https. In theory you could forward all traffic on port 80 on your VPS to the shared hosting, but this would introduce lots of overhead (for forwarding data for port 443 from the shared host to the VPS you probably don't have the rights). But, if you could use different hostnames (like $URL$ vs. $URL$ this would be possible if you have control over the DNS settings. 

I doubt this. Maybe it was working, but I doubt it was working fine. Maybe you just did not realize before how insecure your configuration was. 

HPKP does not replace the normal validation. Instead it is additionally to the normal validation. Thus, if the normal validation fails since the certificate is self-signed HPKP will not help. 

The server cannot verify the client certificate you've sent because it does not find any path to the CA's trusted by the server. 

Yes, usually this is the case, like with all delivery errors. But like most of the other delivery errors only technically fit people might understand these error messages. 

It looks like that your server has a private IP but your client a public IP. From that I assume that your server is inside some local network behind a router which will forward most of the ports to the server so that FTP data connections work (at least , which are and in your configuration. But, the server still knows only its internal IP address of 172.30.0.248 and expects the FTP client to connect to this address (in this case 172.30.0.248 port 58382). If the FTP client only implements strict RFC959 it will try to connect to this address and will of course fail to connect because it is not reachable from his site. Other FTP clients might instead ignore the IP given inside the response to and instead connect to the IP address of the server. While this is against the standard it works around situations like yours and can also be more secure. In summary: you expect the FTP client to work around a configuration problem on your site. Some do, others don't. For correct configuration of vsftp behind a router see for example $URL$ (first hit with google). 

SPF only checks the sender address given in the SMTP envelope, i.e. what you call "Return-Path". SPF does not care about the contents of the mail itself including the mail header. This also means that SPF does not care about the 'From' header and thus does not deal with a spoofed 'From' header. Thus the SMTP envelope mail from ('return-path') must match your SPF policy. Allowing your users to use any mail server they want is exactly what SPF tries to prevent because in this case there is no longer a way to distinguish "your users" from "anybody else". Thus if you want to use SPF for your domain you should either have a wide open (and probably useless) policy which includes all the mail servers used by your users or make your users use your mail server (with authorization). The last option is much better because then you can use a tight SPF policy and this is what most providers do. 

Some strange problem at the server. Some deep inspection middlebox (like firewall) in between which actively interferes with the traffic either by policy or because of a bug. Some firewalls prohibit uploads as data leakage protection. Some firewall which rate limits the number of data connections. Some broken router which croaks on some TCP options used during transfer. 

No. According to the rules of the CA browser forum, RFC2818 and RFC6125 only one wildcard is allowed and only in the leftmost label. Which means there is no and no either. You need instead to add all the domains you need in the subject alternative name part of the certificate, but you could have multiple entries and and you can use wildcards, i.e. , etc. Such certificates with multiple wildcard names are common (look at the certificate for facebook) which means there are SSL providers which offer these certificates. But they will cost more than others. 

The server should send its leaf certificate and also the intermediate certificates which are needed to build the trust path to the trusted root CA. If the server only sends the chain but not the leaf certificate something is wrong. But maybe you did not notice that the server sends also the leaf certificate (should be the first) and not only the chain certificates. 

This kind of setup is not well supported by the servers. The most you can probably do is to allow each of these protocols but then check inside your web application which protocol is used by the client (environment variable SSL_PROTOCOL with apache) and display a warning message instead of the real page to all clients not meeting your minimal requirements. Of course any analyzers like SSLLabs will give you a bad rating because they will only see that you support these older protocols and not how you handle such legacy clients. 

because these are horribly insecure. Please consult $URL$ for a useful and secure configuration. Also the following error messages indicate that you certificate and/or private key is not in the correct format: 

There is no technical reason against installing the same certificate on multiple systems if the domains on the system are covered by the certificate. There might be usage restrictions by the CA which you need to check. And since all systems with the same certificate need to use the same private key the change of compromising the key gets larger. 

The symptoms you describe look very much contrary to this claim. Chrome (and IE?) download missing intermediate certificates by themselves while Firefox and most mobile applications do not. SSLLabs will not mark these intermediate certificates as missing but they are marked as "Extra Download". If this is really not the case check if your server has IPv4 and IPv6 enabled and if the setup for IPv6 is different. SSLLabs does not check the IPv6 setup. If IPv6 gets used depends on the OS, connectivity and preferences of the browser so this might explain such differences too. Other differences in this area are different IP address of the server depending on the location or different tests, i.e. sometimes and sometimes only . 

The option is for specifying your own certificate (client certificate). But it fails to verify the servers certificate. To specify this certificate use either or , depending on how you have the servers certificate/CA (see documentation of curl). Note that you usually don't have a private key for the servers certificate, so only the certificate w/o the key should be given. 

How should the client (browser) know, that .nl is an alias of .com? Because this is not defined anywhere you need to explicitly specify this by having this information inside the certificate, that is use a certificate which is valid for all your domains you own at the same time. 

Handling of idle connections has nothing to do with TCP keep alive but is only related to settings inside the server process. TCP keep alive cares only about detecting broken connections on time, i.e. where no packets can be exchanged because something in the middle is broken or one peer crashed. When a connection is idle no actual data will be exchanged, but TCP keep alive packets with zero payload still can be exchanged.