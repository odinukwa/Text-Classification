I'm using SQL Server 2008 R2. Someone else set everything up. I encountered an error today: Autogrow of file 'ASPState_log' in database 'ASPState' was cancelled by user or timed out after 1748 milliseconds. Use ALTER DATABASE to set a smaller FILEGROWTH value for this file or to explicitly set a new file size. This was a very large transaction log (much bigger than the table) trying to grow and as a result it shutdown our website. The recovery model is set to Full but the logs are not being backed up and truncated. But somehow the database is being backed up nightly as there is a timestamp under Last DB Backup. However, under Management there are no maintenance plans...so I'm not sure how the DB is getting backed up nightly unless there is an outside task or server doing it (which I couldn't find). Should I do a full backup including logs and then truncate them? And where can I find how I am currently being backed up? Thanks, my first question. 

EDIT After talking with the programmer he seems to think that passing parameters to one variable and assigning them to a declared variable helps with security AND it will cause queries to run faster. 

We use stored procedures and so far we haven't detected any injections that have been successful, but we see attempts all of the time. I started logging some of the data hoping to gain insight on what people were attempting and possibly use it to block any future attempts by new methods. Here is the code of something someone tried to run on our site today, they tried it with syntax 8 different ways. I can understand some of it, but not all of it. So I was hoping someone else who knows more could tell me how exactly this works and what it was trying to do (I believe it was trying to insert links to a page using hidden divs in MAX columns in all of our databases). 

Our senior programmer has been having me write stored procedures in the following format to protect against injection attacks. He says that the best practice is to take the parameter in, then declare a new variable in the body and assign that var the parameter, that this step is extra protection against injection attacks as it forces any injection attempts to be considered data and not taken literally. Is that true? I would think this would slow down the query and use extra memory and not add any extra protection, but I could be wrong. Example below. 

That's exactly how it was injected. They declare some variables to move data around, then get the name of each of the non-system databases. From there I'm not sure what it's doing but I'm guessing it goes through all the tables looking for max columns and if it finds one, it finds some rows and writes links to them. 

It simply refuses. It's worth mentioning that if I run the code without the parameter-part ( being the sole parameter to the function) it runs fine. I've tried other syntax alternatives, such as 

For the two former events, I'd store the type of event, the old value and the new value. This works fine as long as I stop here. However, if I want to log the 2 former-events, I would need to be able to store different data-types in the same column. The changes would look something like this: 

This is the information I am trying to recreate, using a TSQL query on the MSX server. I want to see the outcome and history of the jobs by all enlisted servers, similar to what is displayed by the GUI window. I've tried digging through the job-related tables and views of the database, but with no luck. The table on the MSX server contains no history from enlisted servers, and I can't seem to find any good documentation on how else I would go about gathering it. Is this data even accessible through TSQL? Any relevant resource is greatly appreciated. 

I've stared myself blind at this. It started as a more complex procedure, but I've stripped it down to the bare bones, trying to make it run. This is the current code: 

Question: In SQL Server 2016, does updating a column to the same value (e.g. updating a column from to ) produce the same amount of transaction-logging as when updating a column to a different value? Read below for more details. We have several SQL Agent jobs running on a schedule. These jobs select data from a source-table (replicated data, linked servers), transform it, then insert/update/deletes the rows of the local target-table accordingly. We have been through various strategies while trying to find the best way to achieve this. We've tried 

In Example #1, the SET-operation will update all columns, even if only 1 column has changed. In Example #2, the SET-operation will update all columns for all rows, falling back to the old value if the value is unchanged. In both examples, all columns are hit by the SET-operation, and, according to my seniors, this creates an unnecessary/problematic amount of transaction-logging when done frequently. The same applies for the -statement. Even if you check a matched row for changes, all columns are hit by the update. 

Our current database environment includes a cluster with 3 nodes(one primary and two read-only replicas), as well as a single, independent server standing next to the cluster. For brevity, I'll call the nodes N1, N2 and N3, and the independent server S1. Recently, we configured for our servers, using S1 as the MSX (master) server, and N1, N2 and N3 as TSX (target) servers. This means that SQL Agent Jobs that operate on the cluster-nodes are created and managed from S1. TSX servers report their state, outcome etc to the MSX server, which can be accessed using said . From the , the job-history of each enlisted server (TSX) can be accessed by selecting : 

EDIT: Clarifying based on kind input from Matt Lord: "Perhaps the writes being executed by your benchmark/load test are against a table with cascading FKs?" No, the output from (where mysqldump_gr.sql is the result of ) results in one huge text insert into mysql.help_topic. "[Can you give me a] MySQL error log snippet covering the relevant time period from the node(s) you're executing writes against[?]" As weird as it sounds, this varies. Either there is no output to the error log during the test or there are lines like this one: . I didn't write about this error message because I thought it was just a one-off the first time we tested and none of the google results had anything to do with GR, but now I did another test and here it is again... "[Can you give me] A basic definition of the load test: schema, queries, write pattern[?] (e.g. is each benchmark/client thread being executed against a different mysqld server?)" Unfortunately that's proprietary, but I can reiterate some info from above: The test is executed against a single node (i.e. a single server). Each thread gets its own rows to manipulate. "[Can you give me] The my.cnf being used on the mysql instances[?]" I've tried with two different ones, though with many similarities due to requirements. This is the latest one, anonymized a bit: 

I've set up a MySQL "cluster" (four CentOS 7 servers running MySQL 5.7.19) using Group Replication in multi-primary mode, but I can't get it to accept writes from more than one thread at a time. This mode is recommended only for "advanced users", according to Oracle, so I guess that's the source of my troubles. The group that I've set up works: I can write and read from it, it stays in sync, all good. However, we have a load test (in Java, running on Tomcat) that I'm running on the cluster, that consistently fails when launched with more than one thread. This load test runs a variety of transactions in as many threads as wanted as fast as it can towards a single node. What happens is that the transactions result in . (This is, as far as I can gather, what is printed any time the group replication plugin has determined that some transaction must be rolled back for whatever reason). This eventually kills all but one thread, which continues happily until completion. The odd thing is that this load test is made to never create contention on any row; each thread gets its own set of rows to manipulate. Stopping the group replication plugin or running in single-primary mode fixes the issue, allowing me to run concurrent threads with write transactions. Only having one writer at a time would be unacceptable in production, so this is a showstopper. I've tried all the isolation levels (including read-uncommitted). I've tried running the appliers in parallel. I've read the requirements and limitations in particular and the entire group replication dev documentation from Oracle in general. I've tried reading bad translations of Chinese open source forums... No luck. Has anyone gotten this to work, or knows how to? EDIT: It is possible to run more than one thread against the same server, if the transactions are timed so that they interleave. That is, more than one connection can execute transactions, but only one can execute a transaction at any given point in time, otherwise one of the transactions will fail. 

We have several tables which we "refresh" frequently by rebuilding them in a staging-table, then performing a metadata-switch to the production-table using the statement. These operations are causing a huge amount of logging, and we're not sure how to handle it. For now we just moved these tables and operations to a new database using the recovery model, but I would love to hear alternative solutions to this problem. How do you suggest we handle the massive log generation as a result of frequent table rebuilds? 

At this point, I would need to store the textual old/new value of the company name, as well as the id of the recently added user.You might see that I'm already headed off in the wrong direction, and this is where I ask for help. I have 2 questions: Should I just use -datatype, or if this is considered poor design then what would be a sensible way to store these log-events? Thanks in advance. 

I am trying to create a log-table for storing events to a -object, and I am afraid I might be taking the wrong route. I've arrived at the conclusion that I should log different data-types in the same column, and it doesn't feel right. I'll explain the basic use-case with 2 tables; and . 

I am attempting to construct TSQL queries to substitute various GUI tools provided by SQL Server Management Studio. One of these tools is the , accessible through the window. 

Now, I'm only a junior DBD and my understanding of how the transaction-log works is very limited. That being said, my seniors have concluded that we cannot use the MERGE or UPDATE statements where all columns are processed in the same statement since it creates excessive logging. The argument for this is that when you perform an -statement in SQL Server, when you set the a column-value and the new value equals the old value, it is still marked as an update in the transaction-log. This apparently becomes costly when you perform lots and lots of pointless SET-operations. In the following example, we update the and of the target-table using values from the source table, joined by . 

It feels like there must be a smarter way around this, and I would appreciate any clarification and corrections to the statements made in this post. Like mentioned earlier, I'm just trying my best to understand why it has to be this way. Apologies for the lengthy post and thanks in advance. 

Now, say I want to log different types of events on the -object. I'd make a log-table called and and store events there. These tables would look like this: 

This example does not check whether any values has actually changed. If we ignore -checking and sane fallbacks for a moment, this could be checked in one of the following ways: