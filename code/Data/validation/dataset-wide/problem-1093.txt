The virtual accounts tied to the service sid (in this case ) will retain any permissions given to them. This is because they don't go away, they are still tied to the service via the service sid. When you change the account, for the example you gave, other than a few house keeping items there is nothing else given. It is very well possible to change to a domain account that does not have the proper permissions to even start the SQL Server service. Nothing is transferred. For example, if you had an account that had full access to read and write to a directory not in the SQL Server standard locations (that are part of the install process) and changed the account, the permissions stay with that account. The new account would need to be granted those permissions in order for SQL Server to properly work with anything in that folder. The last part of being linked to the virtual account. No, they are not linked. The virtual account and service sid will stay with whatever they had. 

Install a supported OS for SQL Server 2016 on the old 2008 server. Install SQL Server 2016, then setup log shipping as you normally would per your above quote. 

This is letting you know that the Agent job was executed as the SQLAgent$MyAppDEV local account. This is most likely due to the job owner being sysadmin. 

You can have as many databases as your hardware will support which depends on workload, server size, and other general infrastructure items such as latency and bandwidth of the connections. I've witnessed AGs that are ~20TB in size across 4 databases, AGs that had over 1,100 databases, and all values in between. Completely depends on your workload, hardware, and what you'd like to accomplish. 

Your table doesn't start at page 0 of file 1, however that space in indeed occupied and used. This page is reserved and called the file header page, each file will have one of these. Additionally there are GAM intervals which happen roughly every 4 GB, PFS that happen every roughly 64 MB, etc. Finally there are the metadata tracking tables, such as sys.sysschobjs, which aren't readily available for consumption yet do need space in the database. 

Since the certificate and permissions are all included at the database level, there should be no reason to re-create the certificate. Always On will take care of transferring the data to the secondary, it should just be there if it's synchronized or synchronizing but matching log block numbers in the DMVs. If you wanted the certificate to "be" in the "C:\Certificates" folder and constantly use it (not clear as to why from the questions) then it'll need to be on each replica. I'd also say that this would be bad as the keys are on the same server as the databases, which sort of defeats the purpose. 

Standards - each database is setup the same way with the same standard names and tables. Flexibility - a new customer needs their language to be finnish and a different collation to be used, easy enough since it's in it's own database. Try dealing with that with a single table and everyone's information in it. Making a change for a request or feature but only for a single customer, you'll start to like a database for each customer. Security - depending on needs, a different login and user for each database to keep security tight. Automation - since each database is an exact copy (or base tables are exact copies at least), this makes it extremely easy to automate tasks. Upgrades/Changes - changes and updates can be rolled out for each customer on their specific schedules and needs. If a single upgrade or schema change goes wrong, only a single customer is down and not all customers. Granularity - ever need to restore older data that may have been accidentally deleted? Yep, much easier to do this when segregated by database rather tan all jumbled together in the same table or restoring a huge database for a few records in a different schema. Locking and Blocking - if all of the records are in the same table, some type of optimistic locking will need to be used if you don't want too many performance problems of all of the customers attempting to read/write their data at the same time. This is going to lead to more tempdb utilization and make a process much heavier in terms of resources than it should be. 

If you're going to be maintenance to a host, I'd move all roles (drain) off of that node. So, yes, everything should be moved from it. 

No, because you can't restore a database that is in an AG. Thus, to restore it'd have to be out of the AG and wouldn't be subject to database level health detection. 

This happens at the WSFC level. A failure is detected or SQL Server initiates a failover which moves the resource group (and all contained resources) logically to the new node (where ever and which ever that may be). The resources are then attempted to come online. Part of the process to come on-line for a network name (listener) is to have at least one dependent IP address online and register with DNS among other items. This may or may not be the same IP Address based on how your networking is setup and the location of the new node owning the resources. 

Except for a very specific scenario, this will not work. In all other scenarios you'll lose quorum before you'd ever be able to automatically fail over (which requires... quorum). The best answer would be to have the DR side be manual failover (it could still be synchronous) with the proper documentation on how to force quorum and bring the AG online. You could also invest more into VMWare and use their technologies but that assumes the infrastructure, licensing, and ability to implement those products for a specific service such as this. 

Move your infrastructure to the cloud - you won't have the issue of moving servers around, DR for hurricanes (I also live in FL so I get it), etc. Pick different regions (if applicable) to make sure you're covered. This is not as easy as it sounds but may pay-off for things such as this. 1.1. Have a dedicated DR site at co-lo facilities such as Rackspace, sungard, etc. Availability Groups - This would require a lot more infrastructure than I believe you are willing to invest due to the nature of AD integration with AGs and Clustering. While Windows Server 2012R2 gives dynamic quorum and ad-detached failover clustering, I wouldn't advise using it in this situation. Mirroring - Since AD, etc, would be an issue, it would be possible to use a local account on the hosted platform and setup the endpoints to use certificates. It also doesn't have to be part of your domain. You may have some latency issues that would need to be worked out or potentially not be acceptable. We don't know your SLAs. Replication - There are so many ins and outs to replication, but a simple transactional setup could work for you, assuming your database meets the requirements. There are many gotchas just like the other technologies. Log Shipping - A very simple DR method that may work for you close enough to when the server may possibly go down. This could be completed without being domain joined, but there would be a few extra hoops to jump through. 

Turning my comment into an answer. There seems to be some misunderstandings of how certain performance counters work when it comes to availability groups. Only Synchronous Commit replicas will be counted toward Asynchronous Commit replicas will not change this counter as they transfer no mirrored transactions (yes, this could have been called something better). Let me correct some of the assumptions in the thought process in the original question. 

Like Erik pointed out, AG Replicas are currently read only so it can't update the values here. Additionally, if it did update the values it would just be overwritten the next time anything was updated on the primary and the log block sent to the secondary. 

Even though traces are deprecated they still work well. If it's just a successful login needed then this can be fairly low overhead. It has well known functions to load into tables and do your analysis. $URL$ 

According to BOL it will not affect anything currently in your plan cache: "Setting the optimize for ad hoc workloads to 1 affects only new plans; plans that are already in the plan cache are unaffected." $URL$ You may see the potential (depending on eventual re-use) of cpu hits for plan compiles (since it would have to do it twice, once for the original execution and then one for the second when it is stored). Some other "weirdness" would include monitoring tools, especially if they are grabbing execution plans as plan stubs do not have any associated with them. Some odd results may come from tools that expect there to be one associated all the time. I'm not extremely familiar with Dynamics but IIRC, it has a specific Microsoft setup guide like SharePoint. I'd double check this won't invalidate your supportability for the product. 

Since you only need the successful logins, there is an event "sqlserver.login" that can work well for you. Send it to an event_file target and it's fairly simple as well. There are also functions to load the files and do your analysis. $URL$ 

Removing and re-creating is going to be your best bet in the current implementation... though I'd argue the need for a database in a Distributed Availaiblity Group which needs to be refreshed each day. The only other option is to fail the Distributed Availability Group over after removing the database from the global primary so that the forwarder becomes the new global primary. Then you will be able to remove the database from the (now primary) second availability group. That's less appealing of an approach due to the connections being killed, versus the remove + add approach you already have which will keep connections to the current global primary. 

The issue is around NTFS allocation maps and the amount of space they have for file allocations. In general, the more fragmented the file the more allocation mappings there needs to be. By default when a volume is formatted for NTFS, small allocation map size is chosen (1024 bytes) whereas with /L the large allocation map size is chosen (4096 bytes). Normally this doesn't come into play or show itself and any appreciable way... However once volumes get larger and larger or use technologies that create many small random allocations (such as database snapshots or checkdb) then it can rear its' ugly head. To get around this, a few different options can be chosen: 

Checked for blocked redo threads on the secondary. Make sure the AG is synchronized and not synchronizing Make sure the transactions have @@TRANCOUNT = 0 Make sure the secondary has enough disk IOPs to handle the load without growing the redo queue size 

What kind of changes are you looking to make? Personally, at the minimum I'd setup a listener and have the applications use that. Otherwise it's going to be an even more painful AG failover experience. 

If your answer to the above is, "Not worried" then I don't see any reason to use any AlwaysOn technology (FCIs or AGs). If your answer is, "Yes, one or more worry me" then I would consider looking into a set of technologies to meet your specific concerns and scenarios. 

Then you have quite a good bit of development work ahead of you. Seriously. Everyone asks this same question, but it is not an easy one to answer with any more resolution than "It failed over at this time." Things such as virtualization make this even harder as the tools and the hypervisor itself may do things that are outside the purview of your sandbox. However, to get you started, places to scrub for data: