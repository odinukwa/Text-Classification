If I have two tables that have identical constraints, datatypes and primary keys is there a way to 'import' the index from one table to another if they're on the same server, but in separate databases? 

There is another table in the database with around 25 million rows. This database will only be used on a single machine, and it will be designed to mine data that already exists and it under no circumstances will ever grow beyond its current size. For a situation such as this, what's the best way to tackle this so SQL Server doesn't complain? Will the solution matter that this DB won't be exposed to multiple users? 

I'm beginning to learn some about looking at execution plans and making queries more efficient Consider these two basic queries 

Even though the first query supposedly has the higher cost by a 4:1 margin it runs faster than the second one. Why is it that a simple distinct added to the query will add the (what I assume to always be bad, corrections are welcome) hash match operator? And why does it have the higher query cost relative to the second query if it runs faster. 

Using TCP/IP Sockets, when the SQL client is given a server name SERVER\INSTANCE, for example PRODDB\Payroll, it checks two sockets to work out what port number INSTANCE is listening on. 

Choosing the number could be automated by using a query and placing the returned number into a dynamic SQL string. This must be placed in a serializable transaction to make sure no other sessions insert new rows while this session is reseeding the value (thanks David). Also, the master must not be changed while reseeding is taking place. One way to deal with this might be to create non-overlapping identity domains. On ServerA, create the identity column as and on ServerB as . That way ServerA's inserts will be 1, 3, 5… and ServerB's inserts will be 2, 4, 6… Of course, if you ever want to create a third server then you are hosed. :-) Better still, don't use in a multi-master situation. It provides no prevention of duplicates. Use a uniqueidentifier or an application-generated identifier instead. 

I had some changes in that table and it got executed. mainly the primary key. pl go through the same, 

Check the insert query for teams field you have mentioned values as 1 and if you insert the same value for second record it will behave as below. 

Try the above query in terminal or check the below link to repair table or databases via phpmyadmin $URL$ 

WITH (NOLOCK) is the equivalent of using READ UNCOMMITTED as a transaction isolation level. So, you stand the risk of reading an uncommitted row that is subsequently rolled back, i.e. data that never made it into the database. So, while it can prevent reads being deadlocked by other operations, it comes with a risk. In any application with high transaction rates, it's probably not going to be the right solution to whatever problem you're trying to solve with it. SQL Server 2005,2008 and 2008 R2 will support Nolock. Pl look on the below link $URL$ 

Here is a higher-level answer, from a BI breakfast I attended two years ago. BI is what people in your organisation are already doing - making decisions based on information. The goal of BI tools is to allow those people to make those decisions faster and with more confidence. Another answer, the one I often use, is that BI tools are there to turn "data" into "information" in a timely fashion. Kimball Group uses the phrases "Deliver data to business users that is easy for them to understand and navigate" and "Deliver fast query performance". 

When you create the management data warehouse, the current login will be added to all three roles, making it an administrator of the database. The dialog above lets you add other logins to the roles, to make those people readers, writers or administrators of the database. 

Options Unique constraints support indexing options like and , though this hasn't been the case for every versions of SQL Server. Included Columns Nonclustered indexes can include non-indexed columns (termed a covering index, this is a major performance enhancement). The indexes behind PRIMARY KEY and UNIQUE constraints cannot include columns. Hat-tip @ypercube. Filtering A Unique constraint cannot be filtered. A unique index can be filtered. 

Hi I need to set auto increment for a table as S1. I tried by altering the table and also by creating by new table I am aware how to set auto_increment but when i try to set auto increment as S1 i am unable to do. since i need the primary key values should be S1,S2,S3 as following records. can any one please suggest the data type for that, 

This error happens when Full back up is not restored before attempting to restore differential backup or full backup is restored with WITH RECOVERY option. Make sure database is not in operational conditional when differential backup is attempted to be restored. Please have a look on the below blog. $URL$ 

Error Cause: The control file change sequence number in the log file is greater than the number in the control file. This implies that the wrong control file is being used. Note that repeatedly causing this error can make it stop happening without correcting the real problem. Every attempt to open the database will advance the control file change sequence number until it is great enough. 

"…but I'm curious as to whether there's a better way." Yes - use hardware or software RAID to create one RAID0 (or maybe RAID5, but write performance might suffer) array across the three 700 GB disks. Better still (as the commenter below says), add more disks and create a RAID array with fault tolerance. SQL Server databases are more manageable with fewer files, not more files. Is it possible to physically remove the hard disks from the old server and install them in the new server? If so then restoring the database into the new instance of SQL Server is just a matter of attaching the data files. Edit: The poster below makes an excellent comment about fault tolerance. A RAID0 array has no tolerance for errors. Then again, neither does splitting the database across three individual disks. I am OK with SQL Server data files being on RAID arrays with no tolerance, if and only if (1) there is a great DR solution, and (2) the log file is on an array with fault tolerance, and (3) the database is in Full recovery Model (so the tail of the log can be backed up). 

If the tables were dropped, ApexSQL Recover can recover them even from databases in the simple recovery model. ApexSQL Recover can recover both table structure and table records On the other hand, ApexSQL Log cannot recover records lost when a table was dropped, it can only recover the table structure In case the records that were lost using DELETE (not DROP TABLE), both ApexSQL Log and ApexSQL Recover can help The advantages of ApexSQL tools over recovery to a point in time is that ApexSQL will recover just the tables you specify (creates CREATE TABLE and INSERT INTO scripts) , while a point-in-time recovery will roll back all the transactions that happened in the meantime A DROP TABLE statement marks the MDF file pages used by the dropped table for deallocation. These pages are actually still in the MDF file until overwritten by new operations. To prevent new operations overwrite the data necessary for successful recovery, we recommend creating a copy of the original MDF and LDF files immediately 

If you want to give principal Alice the SELECT, INSERT, UPDATE and DELETE permissions to all table-valued objects in all schemas then use the following. 

The actual answer depends on your needs but generally "put data and log on different arrays" is of higher importance than "put tempdb on its own array". Given the number of drives you have available, my starting point would be: 

¹ I am sure I read somewhere that Codd considered implementing two null markers - one for unknown, one for inapplicable - but rejected it, but I can't find the reference. Am I remembering correctly? P.S. My favourite quote about null: Louis Davidson, "Professional SQL Server 2000 Database Design", Wrox Press, 2001, page 52. "Boiled down to a single sentence: NULL is evil." 

I only see one relationship: Parts are sourced from Suppliers with a particular price and a flag indicating whether or not this is our preferred supplier for this part. Table Parts: PartKey (PK), other atributes. Table Suppliers: SupplierKey (PK), other attributes. Table PartSourcing PartKey (PK, FK references Parts), SupplierKey (PK, FK references Suppliers), Price, IsPreferredSupplier. I would be inclined to create a filtered unique index on the PartsSourcing table to enforce there only being one preferred supplier for each part. 

I am getting the below error in mysql while trying to ping. i am using mysql version 5.5.37-0ubuntu0.12.04.1. when ever i use mysqladmin i am getting the below error. Please suggest how to clear this issue 

It will ask for password don't enter anything first time because it will use blank, n just press enter you are done. N later you can set password too...:) 

Your BHCR is 98.92% its perfectly Good and Great. If the value of the Buffer Cache Hit Ratio (BCHR) counter is "high", then SQL Server is efficiently caching the data pages in memory, reads from disk are relatively low, and so there is no memory bottleneck. Conversely, if the BCHR value is "low", then this is a sure sign sign that SQL Server is under memory pressure and doing lots of physical disk reads to retrieve the required data. Prevailing wisdom suggests that "low" is less than 95% for OLTP systems, or less than 90% for OLAP or data warehouse systems. You can also test the true nature of the BCHR 

Foreign Key Constraints A Foreign Key constraint cannot reference a filtered unique index, though it can reference a non-filtered unique index (I think this was added in SQL Server 2005). Naming When creating constraint, specifying a constraint name is optional (for all five types of constraints). If you don't specify a name then MSSQL will generate one for you. 

Thanks to the OP's clarification of the error, I'm going to post my comment as a suggested answer. In SQL Server, permissions are checked only when ownership changes. Consider the following tables: 

Alternatively, you could query the management view and find the table's object id. Use this to generate a dynamic SQL statement. Copy the object id from the results and paste it into the dynamic SQL. 

I think this is a routing question, not a DBA question. Your router needs to be configured so that if traffic comes in its external interface on port 1433 (for the default instance) or on whatever port your named instance is using then the router sends that traffic to the SQL Server machine on the internal network. This is known as "port forwarding" or "server publishing" or "virtual server", depending on the router (it looks like your router uses "virtual server"). Warning: If you are not familiar with configuring routers then be very careful. Misconfiguring a router can break an entire network. The big steps for what you ned to do are as follows. 

In create table you have set teams as primary key, and also you are aware that primary key does not allow duplicate values. 

You can create a username and password also provide grant access to them. so that all clients can access the mysql from their local by using the username and password provided by you. 

I had created table with engine BLACKHOLE basically the BLACKHOLE storage engine acts as a “black hole” that accepts data but throws it away and does not store it. Retrievals always return an empty result. I heard that we can retrieve the data by creating a new table same as the old table with storage engine as innodb or myisam. but i had tried that also but unable to get the result. Can any one pl help me on this issue to fix it. 

If you anticipate that your database file(s) will exceed more than 1TB in size you should configure the server to put the data files across multiple VHDs. How can spread my databases across multiple VHDs? Create one or more dynamic volumes consisting of multiple VHDs at the operating system level and put your database files on these volumes. Split your tables and indexes into separate database filegroups and then place those filegroups in different files (MDF + NDF) which are spread across separately attached VHDs. Note that this will not offer any benefit for transaction logs because SQL Server does not stripe across transaction log files but uses them sequentially. If you are migrated to Azure and want to compare the performance of your databases before and after the migration then take a SQL Server baseline(enter link description here)