Right now your only option for remote 3D rendering is MS RemoteFX with Session Virtualization. It has to be over a LAN, and your apps must be able to use Direct3d rather than OpenGL. On the server side you need to be stacking the server with Quadro cards on the hardware compatability list (there's only 3 or 4 of them). Be aware the solution isn't particularly cheap, scalable, or mature. 

Yep. Console onto the server OS itself and use the hponcfg app to dump and edit the iLo configuration. Best bet is to dump the current config to a file, edit, then re-up. 

Yep this is do-able. You just need to change the compatability mode for the installer to 'Windows XP'. No guarantee that you're not digging yourself into a hole with this, though, as future patches to your 2003 server are going to be layering on top of an unsupported state. If you're having issues connecting from 2003 to newer RDP servers, there are settings you can alter on the newer RDP hosts that will allow them to accept older clients that run lower levels of channel security. This would be the supported solution, if that's your issue. 

Typically load balancers are clustered together into a high-availability pair. If one load balancer fails, the secondary picks up the failure and becomes active. They have a heartbeat link between them that monitors status. If all load balancers fail (or are accidentally misconfigured), servers down-stream are knocked offline until the problem is resolved, or you manually route around them. 

Have all your clients been switched over for DHCP, DNS and WINS to the new Domain Controller? Are you using DFS and if so, have you migrated the namespace across to the new DC with dfsutil? (Assuming DFS is supported on SBS - Suspect it isn't) If you ping domainname from clients, do they all return the IP of the new server, and not the old one? 

Does the server have a battery-backed cache for the storage controller and if so, is the battery missing, disconnected or failed? Dramatic differences in throughput like you describe have been seen in instances where the cache is missing, malfunctioning or disabled on a server, vs an identical server where the cache is operating correctly. On a brand new machine, there may be a period when you first power it on where the battery-backed cache is charging, and caching will be disabled until it's ready to go. This can be up to 24h after it's initially powered up. Also, during RAID rebuilds, caching is often disabled by the controller. Check the perfomance again once the rebuild is complete. 

Openfiler can do what you require but you're probably going to have to get your hands dirty with the command line. The web gui won't have everything you need. You'd be looking at something like: 

You will need a separate network segment for your iSCSI traffic. Don't run data and disk traffic on the same network, and preferably run dedicated switches for the Storage Area Network. Whatever you do, never mix the two types of traffic on the same physical ports. 2 ESXi servers would be a good choice for reliability reasons. You can install ESXi onto an SD card on the host and boot from that, then run all of your disks out of the box you're configuring as a storage appliance. 

Here's the core of what you get from a business-grade server, that you don't get with consumer grade machines: 

vSphere has this new-fangled vBackup API that kinda does away with the VCB proxy if you want to do direct SAN-based data transfers on your backups. There are several vendors with products that support this, and my experiences so far a have been very positive. The main advantage of SAN/vBackup-based backup jobs are: 

The two domains will not interfere with each other on the same network. There will be no trust established between them unless you manually establish one. The DHCP issue is a valid point, and your potential fix is correct - You can hand out the DNS address of one domain via DHCP, and use a forwarder to resolve the other domain's namespace. An alternative fix would be to manually configure networking for the clients on one of the domains, and point their DNS manually at the correct domain controller. You can leave the other domain's client working from DHCP. We have a few subnets that are used for internal testing and have 5+ different domains running on them, no real issues to speak of. 

The VirtualCenter? Not an issue. It'll obtain SAN information via the hosts. The only non-ESX/ESXi box you might want to access your SAN is your backup host, if it's got a direct SAN backup option. 

Open SQL Management studio and connect to the instance Right-click the instance in Object Explorer and select 'Properties' Under 'Memory', see if someone has previously entered a static limit on the RAM usage ('Maximum Server memory in MB'). 

Bearing in mind that 24Gb is the most RAM that i7 chip can address, and it's non-ECC and at a lower memory bandwidth, and that you're not able to scale to multiple i7s.... i7 - 3.33Ghz * 6 cores = 19.98 E5620s - 2.4 * 8 cores = 19.2 Whether your application is highly parallelized or not, the i7 still looks like it will narrowly edge out the E5620s for raw processing. However, there are better E56xx chips available and they'll exceed the performance of the i7, especially in 2-chip setups. You also need to pay attention to the memory bandwidth as all that extra processing power becomes useless if you transfer the data at suitable speeds. Outside of these issues, also bear in mind that the intel server chips are designed for continuous operation and stress tested more completely than the i7s which are desktop targeted and generally no as robust. 

Gets users used familiar with OWA. Requires no migration dependency on client-side existence of Outlook. Is low-maintenance which means you don't get dragged away from the migration work to fix client issues. 

You can configure each host to support virtual machines. However each individual VM cannot execute on more than one host simultaneously. Each VM is restricted to the resources available within its host server. You can 'Live Migrate' a VM from one server to another without bringing the VM offline. However as above, at any one time you only have the resources of a single host available to the VM. With the resources you have, you may be better served by taking the RAM and CPUs from two of the hosts and using them to stack the other hosts as fully as possible (2 x dual-processor, 8Gb hosts). Then set those more powerful hosts up to host VMs. This relies on there being enough RAM and CPU slots spare within the hosts to expand them. Hope this explanation helps. If someone does figure out a way to aggregate multiple hosts into a unified VM-hosting platform, I'm pretty sure they'll clean up. It's pretty much a virtualization holy grail ;) 

It's your security team's responsibility to tell you the AV solution they'd like you to use, if they're going to mandate it. That said, Avast! offer a WinCE AV solution if you can't argue them around to a reasonable position, and McAfee may still offer one, too. Mobile devices generally don't run AV. Ask them what AV is running on the blackberry devices, or smartphones with Wifi. WinCE has an excellent security track record. It's pretty much baked onto the hardware, runs a mostly seperate architecture to other windows builds, and to my knowledge there's barely any examples, if any, of major WinCE exploits in the wild. 

Now, configure your public-facing router so that the public IP address you have assigned is NAT-ed back to the cluster IP you have assigned. Then configure NLB on the windows host as per the MS guide here: $URL$ 

Given your requirements, NTBackup to staging disk space on the backup server, then backing up on that server directly to tape. Since you're backing up remote machines, you'll need to get enough staging space on the backup server to host the backup files. Streaming directly from the remote servers to tape won't be feasible because data throughput won't be high enough to keep up with the tape speed. 

If you're after database performance in Amazon, you may find this article useful: $URL$ The general vibe from my own research (have been reading about this for a few weeks) is that EBS is the faster option. 

Anyways, my core point is that a sysadmin with programming experience is an asset as long as they don't try to solve all their sysadmin problems with code. 

If you can't find an app to handle it, you'll have to get messy with the messenger protocols to be able to pull the contact's information out before delivery. The protocols seem to be updated frequently to continuously cripple unofficial clients, so it's probably impractical. MS offers a corporate alternative to live messenger which was forked from Windows Messenger, MS Office Communicator. This is the non-hacky way to restrict your messenger use to business purposes. Alternatively, you could deploy a Jabber server or something similar if you're on a budget. Lastly, don't forget the maxim "You shouldn't attempt to solve people problems with technology alone" 

There's no internal ESX feature that'll help you with this, so you should treat the work as you would 2 physical servers (copy over the network with an appropriate tool). If the VMs are running on the same host, you may be able to create a dedicated vSwitch, purely for internal traffic, and achieve a quicker copy. One other approach which may be sustainable is to create a dedicated LUN for this data, and attach it directly into the VM to copy your data in. 

what PIX version? The feature you want to use is Port Redirection. The basic syntax should be: static (INTERFACE1,INTERFACE2) PROTOCOL IPADDRESS2 PORT2 IPADDRESS1 PORT1 netmask NETMASK so for example: static (INTERFACE1,INTERFACE2) tcp yourexternalip smtp 192.168.0.1 smtp netmask 255.255.255.255 And on your ACL you will need something like this: access-list outside-inbound permit tcp any host yourexternalip eq smtp (Disclaimer: I haven't touched Cisco for a while, but I'm 100% someone who has will be along shortly to confirm or correct!) 

The issue seemed to center mostly around uploading files to a host via an SSL secured session. MS says it's been fixed as of IE7. Mentioned at the end of this article: $URL$ If it's still there in the latest version of Apache, I'd err on the side of caution and keep it in there. The performance hit for your server and IE clients is not as bad as rendering them unable to access parts of the site. 

You must ensure your traffic is coming in on multiple IPs because load balancing is achieved on a per-connection basis. I don't know a way to load balance a single connection across 2 NICs. 

Short answer is that I think you'll spend more time, effort and money on chasing this than you ever would save by implementing it. There are few products that can unify management of a disparate collection of technologies, and of the few that exist I can't think of any that are inexpensive or straightforwards to implement. They're typically the kinds of products that starry-eyed execs at large companies roll out under the assumption that it'll magically make IT simpler and allow them to lay off some techs. Given the constraints/circumstances you listed, IMO your best approach would be to focus on reducing/unifying your systems to cut down on variety. Some examples: 

Any experience with home-made clusters? Some with OpenFiler around the same user load that you describe, but I haven't clustered it together. Didn't need to. You can run the kind of load you've described quite comfortably with a single well-specced SuperMicro-chassis build running OpenFiler or FreeNAS. As long as you ensure that you stack it with 8Gbs+ RAM (for fileIO caching), a competent controller with 1Gb Flash or Battery-Backed Write Cache, and sufficient spindles to serve the IO load with acceptable seek times, then you can shave a good chunk of cash from the big brands. I wouldn't focus on clustering so much for this level of usage. Just because you could purchase multiple identical SuperMicro builds for the same price as a single HP, doesn't mean you'd gain anything by doing so. However if you feel it's warranted, then DRDB is worth a look. Any arguments on the risk of using commodity hardware? The biggest problem I've found is getting a suitable monitoring solution to ensure you're notified if any components in the system fail. HP have Insight Agents, Dell have their OpenManage etc. This is really important because you need to know the second a hard drive fails. Also beware the peril of using non-enterprise drives for enterprise RAID sets, and the time-out problems that can occur due to a lack of TLER on the drives. Any points on the performance of such system? There's really nothing special in a branded system that puts it's performance above an unbranded one. The key is that they're usually specced by experts for particular workloads and ship with components that are guaranteed to work well together. Good quality array controllers and NICs with offload capabilities are advisable. Any recommendation on alternate solutions that gives place for growth? As above FreeNAS/OpenFiler are worth a peek. DFS on Windows 2008 R2 is also worth a look as it's been completely overhauled from previous versions and appears to be a much stronger solution than before. No reason you couldn't run 2008 R2 on a custom server build, if your components have appropriate drivers. 

Computers joined to a domain manage their membership by use of a shared secret. Basically, all computers on the domain have their very own account password, just like all your user accounts. The difference is that the computers maintain this themselves, without any involvement from you and without you ever seeing what that password is. The machine periodically goes through a password change where a new password is generated and updated on both the computer and the AD computer account object. What you're seeing is a situation where the password stored on the domain and the password the computer believes is valid, are not the same. This could happen for a multitude of reasons but the most common ones are: 

One useful principle that I try to apply, and see violated regularly, is that a sysadmin should understand the boundaries of their reponsibility, and take care not to overstep the mark. What I mean by this is that often in sysadmin duties, questions and problems will arise that actually require decisions to be made by other areas of the business, but IT may attempt to address the problem without seeking out those decisions from the business. Some good but by no means exhaustive examples of this are: