Letting $\eta = e^{1/e}$ where $e$ is Euler's constant, there exists a function $F(z)=\, ^z \eta$ with the following relevant properties. (I won't bother showing the existence of this function, or the construction, but it is there in the cosmos of function space.) Let $z= x+iy$ $$F:\mathbb{C}_{x>-2} \to \mathbb{C}$$ $$F(0) = 1$$ $$\eta^{F(z)} = F(z+1)$$ $$F:(-2,\infty) \to (-\infty,e)$$ $$\frac{d}{dz}F(z) \neq 0$$ $$\frac{d}{dx}F(x) > 0$$ This is tetration base $\eta$, and is a valuable function for many reasons. We are sticking to the regular iteration method. Now my question is difficult to state but revolves around extending this function to a larger domain than the half plane $\Re(z) > -2$. It is rather trivial to show $$\log_\eta(F(z)) = e\log(F(z)) = F(z-1) + e2\pi i k$$ for an arbitrary branch of $\log$, for some $k \in \mathbb{Z}$--locally about a point $z$. What I'm trying to understand is if the following is an extension of $F$ for $|\arg(z-2)| < \pi$. Namely if $F$ extends everywhere to the complex plane excepting $y=0,x\le-2$. It is difficult to state but the proof of an extension goes as follows. $F(-1) =0$ and therefore $\log(F(z))$ is not holomorphic in a neighborhood of $z=-1$ so that $F(z)$ is not holomorphic for $z$ in a neighborhood of $-2$ necessarily. So that the singularity at $-2$ is a logarithmic singularity--a branch point if you will. Using the functional equation of $F$: $\eta^{F(z)} = F(z+1)$ and by taking the implicit equation $$\eta^{\mu} - F(z-2)=0$$ for $0 < x < 1+\epsilon,\,y>0$ and arbitrary $\epsilon > 0$, we have a bunch of functions $\mu$ defined locally about each point $z = x+ i y$. Now $\mu + e2\pi i k $ is equally so a plausible function by the periodicity of $\eta^z$. What grounds do we need so that we can say there is $\mu$ holomorphic for all $0 < x < 1 + \epsilon,\,\,y>0$? As in, what do we need to show in order to say this covering of the domain with analytic functions $\mu$ about each point $z$ forms an analytic function on the entire domain? I assume the result requires some knowledge in Riemann surfaces (something I am a beginner in), and I assume it is not trivial to show this result. Multivalued functions have never been my strong suit, and so I hope the answer can be found in simpler terms than some advanced argument from complex manifolds. If $\mu$ is holomorphic in this domain, there is only one $\mu$ that agrees with $F(z-3)$ for $1 < x < 1 + \epsilon$ by the identity theorem. And therefore we have found an extension of $F(z)$. Repeating this procedure eventually shows $F(z)$ is holomorphic for all $y>0$, and an equal procedure works for $y<0$. This in the end shows $F(z)$ is holomorphic everywhere excluding $y=0,x\le-2$--i.e: when $|\arg(z-2)|<\pi$. 

See David Marker's "Model theory. An introduction." Although argument is model theoretic, you can easily transform it into syntactic one. 

I am interested in different contexts in which Gödel's incompleteness theorems arise. Besides traditional Gödelian proof via arithmetization and formalization of liar paradox it may also be obtained from undecidability results of Turing and Church. In the context of complexity theory, it is not hard to see that Gödel's theorem (as well as Turing's result) follows from the following Chaitin's result: there exists some natural number N such that for any program P of size more than N it is impossible to prove (say, in ZFC) that it is the smallest (in the sense of size, i. e. number of bits) of those programs which have the same inputs-outputs as P has. The number N depends on particular axiomatic system (say, ZFC) and is not very large so that it actually can be calculated. If You know other ways towards Gödel's incompleteness theorems, please, present them. Particularly, can Goedel's theorem be obtained without use of self-referential ideas? 

Works by Montague show that natural language is not SO MUCH different from formal languages, its syntax and semantics has strong structure. You may say that they are semiformal. That is why we may apply all the techniques and results from mathematical logic, in which specificaly mathematical languages are formalized and deeply studied. As a result we have a field of formal theory of natural language and its aspects (grammar, semantics, development and so on). You may see the broad range of topics presented on this conference: $URL$ This opens the way for doing philosophy of language by formalizing the problems and answer them by mathematical proof. The work of Hilary Putnam presents attempts to tackle the problems of philosophy of mind and philosophy of language by comparing with formal models. To quote from his "Models and reality" paper: "In this paper I want to take up Skolem's arguments, not with the aim of refuting them, but with the aim of extending them in somewhat the direction he seemed to be indicating. It is not my claim that the "Lowenheim-Skolem paradox" is an antinomy in formal logic; but I shall argue that it is an antinomy, or something close to it, in philosophy of language. Moreover, I shall argue that the resolution of the antinomy - the only resolution that I myself can see as making sense - has profound implications for the greate metaphysical dispute about realism which has always been the central dispute in the philosophy of language." See his collected papers. Edward Zalta in his Principia Metaphysica has formalization of general notions of abstract and concrete objects. In mathematics (today) the most basic objects are sets. In Leibniz' Monadology (which is pure philosophical theory) they are monads. Zalta formalizes the monads and does Monadology formally. As well as Plato's theory of forms, theory of meinongian objects, theory of situations, the theory of worlds, theory of times. Moreover, Zalta claims that his formalization enables to obtain new useful abstract notions (objects) automaticaly by mechanized theorem proving. See his home page." 

This question has been bogging me down lately. I'm not sure how to come up with an approach to tackle the proof exactly. I'm without a proof, butI think the result I'm searching for is true. Similarly, I do believe I'm not the only person to consider this question and I expect it is already answered somewhere in the vestibule of the internet. The $n$'th super root is defined as the inverse to the function $$^n x = F_n(x)=x^{x^{...n\,times...^x}}$$ so that if $\Psi_n(x)$ is the function in question then $$^n \Psi_n(x) = F_n(\Psi_n(x)) = x$$ Now $F_n(x)$ depends on the branch of the logarithm chosen to define it, and therefore it is multivalued. We will stick to the principal branch of the logarithm as it simplifies the question (and I assume solving for other branches only requires a modification of the proof). It is obvious $F_n(x) : \mathbb{C}/ (0,-\infty) \to \mathbb{C}^{\times}$. It also follows that for $\Re(y) > 1$ there exists $x \in \mathbb{C}$ such that $y = F_n(x)$. Now we can define an inverse through the analytic implicit function theorem granted that $\frac{d}{dx} F_n(x) \neq 0$ when $\Re(F_n(x)) > 1$--but showing this is rather daunting. It invovles showing when $\Re(F_n(x)) > 1$ we have $$\frac{F_{n-1}(x)}{x} + \log(x)F'_{n-1}(x) \neq 0$$ which is rather daunting to say the least. This brings me to my question which is more of a reference request than anything. Unless I'm completely missing something and the answer is right on my nose. 

The second question is based on related evidence and not on clear fact. I want to further understand how $G$ behaves. I'm assuming for this next question that $G$ is in fact continuous. Is it true that at most points $G(\xi)$ is a linear function, and that at some points we have a discontinuity in the first derivative? To explain this statement it helps to look at the fractional iteration of $\sin$. It is well known $\sin^{\circ t}(x) \to T(x)$ as $t \to 0$, where $x \in \mathbb{R}$ and $T(x)$ is a triangle wave with period $2\pi$. We are taking the analytic solution to $\sin^{\circ t}$ that is real valued. On $- \pi \le x \le \pi$ the function $T(x)$ looks like the following \begin{eqnarray*} T(x) &=& -\pi - x\,\,\,&\text{for}&\,\,\,-\pi \le x \le -\pi/2\\ &=& x\,\,\,&\text{for}&\,\,\,-\pi/2 \le x \le \pi/2\\ &=& \pi - x \,\,\, &\text{for}&\,\,\,\pi/2 \le x \le \pi\\ \end{eqnarray*} $T(x)$ is linear at most points, with discontinuities in the first derivative at $\pi/2 + k \pi$. $\sin^{\circ t}(x) \to T(x)$ is the standard example of a fractional iteration that does not tend to the identity as the iteration tends to zero. But still, it kind of looks like the identity. $\sin(T(x)) = \sin(x)$. Plus, least of all, it is linear almost everywhere. This leads me to ask, in a more sophisticated language 

I don't understand what people usually mean when they say "aspects of some mathematical field related to computer science" (how they know what is really related to CS?), but if you really want to study or teach first order logic, then, in my opinion, there is old but still the best book for that -- Joseph Shoenfield's Mathematical logic. 

Lindstrom's theorem states that any extension of FOL more expressible than FOL fails to have either compactness or Lowenheim-Skolem. When I first read Lindstrom's theorem my first reaction was: "Does it mean incompleteness of any more expressible extension of FOL? And why this obviously important question isn't discussed in standard logic texts?" Standard extensions (such as second order logic) in fact are incomplete. After some attempts of proving incompleteness I found reference to Vaught's paper in which he proves completeness of extension of FOL by adding quantifier Qx = "there are uncountably many x such that..." It would be very interesting (at least for me) to understand complete extensions in general. Such an extensions may be of great importance, for instance, for computer science, because FOL isn't enough expressive for its purposes. So, my questions are: 1) Do you know some other examples of complete FOL extensions? 2) Are there some results concerning characterization of complete FOL extensions? 2) Do you know any peoples, papers, books... studying general properties of FOL extensions? Thanks in advance. 

[Edition. In this answer I wrote about "philosophical logic" and "mathematical logic" in usual sense of these terms. But I also think that the difference between them is very conventional. For example, people studying knowledge formally by means of modal epistemic logics (as formal models) and those who studying provability logics to a large extent (if one forgets about actual motivations) are doing the same things -- they are investigating various modal logics.] "My question is about relations between logic as part of philosophy and mathematical logic from the second half of the 20th century when its seems that connections between these two areas have weakened." "I am quite curious also about the reasons for the much weaker connections between mathematical logic and formal models developed by philosophers at the later part of the 20th century." The main reason is that mathematical logic concerns with models of mathematical thought, but philosophical logic builds models for various parts of philosophy which are very different from mathematics (i. e. they may use modalities, analogy, induction and so on). To give one example of "formal models developed in philosophy that had become important in mathematical logic", Provability logic is such an example. To quote from Stanford Encyclopedia of philosophy: "Provability logic is a modal logic that is used to investigate what arithmetical theories can express in a restricted language about their provability predicates." So, provability logics are modal logics which capture provability in formal systems of mathematical logic. Provability logic is regarded as an area of mathematical logic (which represented by Robert M. Solovay, George Boolos, Sergei Artemov, Lev Beklemishev, Giorgi Japaridze, Dick de Jongh and so on) and it uses models, ideas and techniques from modal logic which is (in any case, was) part of philosophical logic. "Another thing I am curious about is to what extent for the applications to computer science formal models described by philosophers turned out to be useful." Again, the original papers by Kripke was by no means concerned with computer science. Kripke discovered another way of interpreting the modalities and the context was philosophical one. But today the whole field of program verification in CS is based on modal logic. A lot of specific modal logics are developed in CS which model specific aspects (temporal logics, dynamic logics and so on). Crucial results about them such as expresiveness, decidability and completeness are proved using essentialy the same ideas as those from original papers in philosophical context. Finally, to speak "about works in philosophical logic that were motivated or influenced by developments in mathematical logic", I quote from what I said in the meta thread: "A new interesting field in philosophy is growing in this century often called Formal Philosophy (this term I believe has its origin from the title of Richard Montague's collected papers book). People of this field (among them R. Montague, H. Putnam, E. Zalta, D. Bonnay...) are trying to solve philosophical problems using formal logic. Particularly, and this is important, many of them try to formalize such kinds of reasoning as modality, induction, analogy, simplicity, naturality, generalization and so on as well as concrete philosophical theories (which are of course in the domain of philosophical logic) using formalisms and methods of mathematical logic. 

But I'm doubting this because I think some extensions may be equivalent when adding another element. So, my gut also says to ask: 

So I think I found a way of proving $F \neq G$ using distribution theory. I'm not the best at distribution theory, it's definitely not my field, but I think I'm doing something right. This is as much rigor as I can produce in distribution theory, but I think I'm on the right track. I don't have a referential brain for distribution theory, I'm going off what I can remember, I apologize if I'm not explicitly stating the theorems I'm using. This idea works generally as to why the continuous Taylor series can never equal the discrete Taylor series, but I'll be a bit cavalier. Maybe this argument works in more general cases, but one would need to know more about distribution theory to produce such a result, sadly I don't. Let's just use $e^{-y^2}$ to be explanatory. If we take $$F(a,x) = \int_{-\infty}^\infty e^{-y^2} e^{-2\pi a y i} \frac{x^y}{\Gamma(y+1)}\,dy$$ Then we can look at the equivalency in a Fourier sense. Take the Poisson summation $$\sum_{n=-\infty}^\infty F(n,x) = \sum _{n=-\infty}^\infty e^{-n^2} \frac{x^n}{n!} = \sum _{n=0}^\infty e^{-n^2} \frac{x^n}{n!}$$ which follows because $\frac{1}{n!} = 0$ for $n<0$. Now I'm not certain about the following interchange, but I'm fairly certain it should be allowed in good enough scenarios (especially with $e^{-y^2}$). $$\sum_{n=-\infty}^\infty F(n,x) = \sum_{n=-\infty}^\infty \int_{-\infty}^\infty e^{-y^2} e^{-2\pi i n y} \frac{x^y}{\Gamma(y+1)}\,dy = \int_{-\infty}^\infty \chi(y)e^{-y^2} \frac{x^y}{\Gamma(y+1)}\,dy$$ where $\chi$ is the dirac comb. This is kind of nothing new to what Christian Remling said, except I'm phrasing it distributionally, which is to our advantage as we add the Fourier transform. As least, if I've not mistaken myself. The above expression can be thought of as an inner product. If the discrete and continuous Taylor series equal each other, then $(\chi, h) = (1,h)$ or $(\chi -1, h) =0$, where $$h(y) = e^{-y^2}\frac{x^y}{\Gamma(y+1)}$$ I'm kind of out of my element right here, if I'm wrong, definitely correct me. Let $\mathcal{F}$ denote the Fourier Transform. Let $\delta$ denote the dirac $\delta$ function $$\mathcal{F}\{\chi -1\} = \sum_{n=-\infty}^\infty e^{2\pi i n y} - \delta = \chi - \delta$$ satisfies $$(\mathcal{F}\{\chi-1\},h) = \sum_{n=1}^\infty e^{-n^2}\frac{x^n}{n!} \neq 0$$ Notice we remove the $n=0$ term. This produces the final result. It is straight forward from here! We just use the unitary properties of $\mathcal{F}$ under this inner product. $$(\chi-1,\mathcal{F}^{-1}h) \neq 0$$ Apply $\mathcal{F}$ again, using the unitary property again, and through the exact same argument $$(\chi-1,h) = (\mathcal{F}\{\chi -1\}, \mathcal{F}^{-1}h) = (\chi - \delta, \mathcal{F}^{-1} h)\neq 0 $$ therefore $$(\chi-1,h) \neq 0$$ The continuous Taylor series cannot equal the discrete Taylor series. Personally, I'm ashamed to ask, but for what functions does this actually work. I know this argument must work in a distributional sense, but for what functions does this actually work when written in a more classical sense?