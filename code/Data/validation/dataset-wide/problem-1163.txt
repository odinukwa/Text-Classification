Likely you have some data that doesn't fit the datetime column somehwere in the file. Insert to a staging table with a varchar or navachar filed for the column and look at the data. You may need to adjust some of it (or null out bad records) before loading to the real table. 

You don't need a diagram for it to work, you need one to train the future developers! However, if what you really have is a database with no actual formal PK/FK relationships set up (Which you can do when creating diagrams as well as through better ways such as SQL scripts), then that is a different story. That is something that will eventually cause your data to be inconsistent and is a real problem indeed to other users of the database. 

The only time I would ever consider a delimted list is if there is no chance you will want to look at the data separately. Then indeed it is faster. This is a very rare case, however. And in what you are doing, there is an approximately 100% chance you will want to look at individual payments separately. Delimited lists are harder to query (and updating if someone made a typo, ugh) for individual elements (and generally slower for this than a related table would be) and harder to maintain the data integrity. Databases can easily handle many millions of records in tables that join with correct indexing and design. If you need more performance at that point typically, you partition the database. Sit down and read some books on database design and performance tuning. The rules for good design are different in databases than applications. Please take the time to learn them. 

And of cousre you would want associated tables for the customer, Customer address, partslookup, labor tasks lookup, etc. You might also want to provide a place on your form to add a new part or labor task to the appropriate lookup table. Sometimes you may need to quote something that is not already in the system and the best place to do that is while you are creating the quote. 

It's just a generic example, if you want for specific one, please add your tables structures in your question. Max. Edit for question "At what point would it be a good idea to do something else? ": The INSERT SELECT statement locks (write lock) your table to enforce data consitency during operation. I can't answered to the question "at what point" it depends of your server, configs... But imagine your have a table with 1 Million rows, the operation could take some seconds, we'll say 4 seconds for example, so for 4 seconds, your users will not write in your table. 

For DDL (ALTER TABLE): I stop the replication, run my DDL commands, run a rollbacked script and restart replication: 

You'r lucky you have only one secondary index, the is a clustered index so it isn't included in the index statistics. You can find the answer with the field of the information_schema.tables view (result in bytes): 

To switch of MySQL engine you could : 1 Make a ALTER TABLE myTable ENGINE=InnoDB 2 Make a mysqldump of your table, then edit the CREATE TABLE statement to replace MyISAM by InnoDB and restore the dump in the new table (i called it myTable_InnoDB): 

--replicate-ignore-db applies only to the default database (determined by the USE statement). Because the database was specified explicitly in the statement, the statement has not been replicated. My advice is to change the context with a instead of explicitly specify the database in the statement. Max EDIT (for questions) 1. 

The error refers to the field name and the key name (not the value). You should have a on the field , can you paste the result of: 

I've just tested your solution about the "CREATE_TIME" field in information_schema and I think it's the good one, look : 

Now with my query you can retrieve one line in your dataset. This query will split the result and return one line, you just have to modify the @i variable to choose your line (0 is the first line) : 

In your mysqldump, did you put the mysql schema in it? The mysql schema contains the information_schema datas so when the mysql is restored, the past infos are loaded... understand? Max. 

Replication was made for small changes, so you're on the right track. To keep the systems in sync, at the database level, you will need to setup a mysql master-master replication scenario. I've used the tutorial from this linke ($URL$ to setup such configuration. Depending on how much data is changed, you can choose between [statement] or [row-based] replication. More info about the differences between the 2 types: $URL$ 

Also, I believe that the application that's using your database already knows what those values mean. So what is the real use case for your constants ? If you really want to go with your approach, without changing the existing column type, you can create a function that will return the STRING value for each integer code: 

... And so on. Best Bet: Partitioning the existing table by [station_id] and [created], you will have "A" partitions for each station, "B" partitions for each month, and a total of AxB posible number of partitions. Once you partition Table_A, do the same thing for Table_A_Archive, and on the end of each year, move the data from Table_A to Table_A_Archive. ** IMPORTANT:** After you make the partitioning schema, keep in mind that all queries should have in the WHERE clause the conditions necesarly so that the query will hit as little partitions as posible. Ex. 

Why would you want to use the DB to check for files ? Just write a shell script (if using Linux) or bat script (Windows) to check for the files, and use cron or Windows task scheduler to run that script. You can also create the INSERT query in the script (on Linux + MySQL that should be easy enough), and every time the files are checked, a new row should be inserted in your Frequency table. Don't try to use the DB for something that can be easily done at OS level. 

From docs: ($URL$ The Oracle Flashback Database feature, which provides an convenient alternative to point-in-time recovery, generates flashback logs, which are also considered transient files and must be stored in the flash recovery area. However, unlike other transient files, flashback logs cannot be backed up to other media. They are automatically deleted as space is needed for other files in the flash recovery area. When Files are Eligible for Deletion from the Flash Recovery Area There are relatively simple rules governing when files become eligible for deleteion from the flash recovery area: 

Use the option --no-create-db when you execute your mysqldump, this option avoid inserting CREATE DATABASE IF NOT EXISTS db_name; in dump file. Max. 

Last note, we force the auto_increment value to 300 in our transaction so the 300 is no more available, the next one will be 301 so don't forget to force it with your_value - 1. Max. 

By default, MySQL makes a DNS lookup to resolve the client hostnames. You can disable this lookup with the option , check if your server wasn't started with this option: 

In the table in . Use the field for reads and sum the fields , , , for writes. These values are cumulativ counters, you should subtract values between an interval: if t1=t0+60seconds, Com_select at t1 - Com_select at t0 = reads in 1 minute 

MySQL calls these section "Groups". The [mysqld] group contains the variables apply to the MySQL Server (the mysqld process). The [mysql] group contains the variables for the client program (mysql). The [client] option group is read by all client programs (but not by mysqld), so "mysql", "mysqldump" etc... None of these groups are mandatory but usually we set at least the [mysqld] because it's where we configure the server. About how MySQL interprets these variables, MySQL Documentation says: 

If you are on linux you can also make a ls -l on your datadir to show the creation date of your schemas folder : 

Now, if you don't use the Binary Logs on Slaves (for point in time recovery or chained replication) you can disable it and remove them: 

You're right it's a little bit confusing but it's not a bug. See MySQL Documentation for the TRUNCATE command: 

Operations and issues on slaves will not impact the Master. I dont't understand your concept of "the master -> slave replication running every 5 minutes". All transactions on Master will immediatelty be replicate on slaves. If your tables are InnoDB, your dump will not lock any tables (with the option --single-transaction) so you can have normal activity on master and slave too. To backup your slave you can also stop the replication, shutdown your MySQL (Slave) and copy your datafiles or use a hot backup solution like Percona XtraBackup. Max. 

In your first query, both valid values + null values are returned from the database, but because you use the "limit 50", and because NULLs are displayed last, you don't see the rows containing NULL Price. The correct for for the first query should be: 

2 questions: - Where is the IO problem ? On the master or on the replicas ? - If the IO contention is on a replica, can you create a TEST replica, replicating from the same master, but having innoDb engine ? The thing with MyISAM storage is that an UPDATE / INSERT query will lock the whole table. And the replication thread on each replica is also a serial process, running just one query at a time (event) from the master binlog. My suggestion would be change the storage engine for all tables, from MyISAM to InnoDb, then re-check the replication parameters. 

This can and will work (theoretically :) ...), but you will need a recursive function (in application or in the database) that can return the parent group id (ex. B-1), or the top group id (ex. A-1) for any group id value given as an input parameter (C-1). But ... like I said: The more info you give, the better responses you get. Hope that helps. 

Because both of them are virtual servers (you didn't mention the technology used (ESX/KVM...), you can try this: 

Your questions is a little foggy You don't mention how the archiving process works You're basically asking about a feature on the User Interface side ... not on the DB side. 

Because your hierarchy models is Group A -> Group B -> Group C, it's enough to store in [user_groups_data] table just the user_id, the last group id (in the groups hierarchy tree), and the info So, data in the tables can look like: 

The reason for which you first query didn't use the index was because the result of isnull(Price) returned an un-indexed value. 

FRA will be purged automatically when free space is needed Put the archived logs in FRA as well Use RMAN to make the backups and use RMAN commands (REPORT OBSOLETE / DELETE OBSOLETE) to manage the purging of backup pieces + archived logs from FRA