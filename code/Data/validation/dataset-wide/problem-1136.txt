If you want to do update Statistics manually you should first know When Statistics are updated automatically If the SQL Server query optimizer requires statistics for a particular column in a table that has undergone substantial update activity since the last time the statistics were created or updated, SQL Server automatically updates the statistics by sampling the column values (by using auto update statistics). The statistics auto update is triggered by query optimization or by execution of a compiled plan, and it involves only a subset of the columns referred to in the query. Statistics are updated before query compilation if AUTO_UPDATE_STATISTCS_ASYNC is OFF here are nice articles that speak about when update statistics is triggered in SQL server 

An Introduction to SQL Server FileStream To BLOB or Not To BLOB: Large Object Storage in a Database or a Filesystem FILESTREAM Storage in SQL Server 2008 

I am trying to create a Scalar-valued function in SQL server, without specifying the return_data_type i want to be able to return any datatype based on a parameter sent by the function caller e.g. 

This is a really tough problem in the US. Names are not unique and often change during a person's lifetime or are presented differntly (Rob versus Robert for instance), so they can never be used to identify the patient except in conjunction with some more realiable information. Health insurance number and provider changes much more frequently and may be the same for multiple members of the family. SSN is supposedly unique, but there is fraud around it. Same with Driver's liscense number which of course not everyone will have. Personally, I would start with insurance policy number and date of birth and name combination, then ssn and date of birth and name combination. I would check address and phone to give me additonal assurance when they match but not much weight if they don't. Additonally I would use blood type as a rule out factor if it is known (and we all know the hospital vampires will be taking blood samples) as that doesn't change. Name matching would have to be fuzzy match due to the name varaition problem. Other things should generally look for an exact match first themna fuzzy match if the name confidence is really high (could have been a typo entering the SSN). 

We have been experiencing an odd behavior in our application where various modules will begin to timeout in SQL Server 2012. Each time we stumble across this issue, we find that the statistics require update and that running fixes the issue. After updating index statistics, the timeouts go away. However, the frightening issue is the frequency in which we have been experiencing the need to update the statistics, and the fact that nearly all of the statistics are showing a need to be updated across all of our tables related to order processing. Due to the frequency, we have setup a job to run prior to business hours at 7:30 AM. This seemed to have calmed the issue while we continued to investigate until it occurred again today. Not 10 minutes after business opening, they were receiving timeouts. I immediately ran and the timeouts disappeared and application function returned to normal. The order volume since business opening was low (less than 20 rows added to the primary order table), yet the statistics became so bad between 7:30 AM and 8:10 AM that we began experiencing time outs. A couple of notes: 

Well we tend not to care what the originators table structure is, but only if it meets our requirements (which we send to them). If you are trying to figure out how to design a way to store the data permanently because you don't currently have a structure, then this is the method I use. Import the file into a staging table (not the final permanent table, I highly recommend you do that anyway, so you can clean the data before moving it to its final location) that has everything defined as varchar(max) or nvarchar(max). Now you can examine each field and see what is in in, look for the max lentgth of each field, check to see if numeric fields contain only numbers etc. Then you will know from the data what types of fields you need in the production table. Since this is only the first file, I would tend to create my final table with slightly larger fields than the data indicates, so it doesn't fail on the second file you get. After you have some history, you can tighten them. If you use SSIS instead of bcp, there is a data profiling task you can use to see what the data is really like. 

This returns: My clause clearly brings back only the rows that are explicitly , so it is not an issue of being an empty string and evaluating to . 

We have the following (unfortunate) scenario: we have a database where on-the-fly changes are being made by developers as quick remedies to dictionary tables (order types, document types, fee distributions, and so forth). This wouldn't be an issue if these particular developers didn't forget to check their changes into TFS, but alas, we face it. This causes an issue where the database project(s) in TFS do not properly reflect what is in the database. Essentially, I want receive an e-mail or other type of notification (table insert with necessary information) when a change is made to 15-20 tables, what the change was, and who committed the transaction. What is the best method to monitor this type of behavior? We are willing to invest in tools that allow for this ability. I've checked out RedGate SQL Monitor, but it doesn't seem to have this feature set (or I am missing it). I've also seen usage of triggers, but we have a large amount of tables that would require monitoring and I don't know the performance degradation that might incur and if we can collect the appropriate data. I do like the trigger approach where it would insert the information into a table ([dbo].[TABLE_UPDATES]) because it would allow us to build an SSRS report to run against and pull data as needed, but I'm not sure I 100% trust this as a solution. We currently use RedGate DLM Dashboard for monitoring schema changes, and it has proven quite invaluable to receive those updates. We just want to expand it to the data itself. Disclaimer: I know - terrible practice by developers to apply things directly to a database without checking in their changes, but we are currently having to deal with this hurdle and reign these individuals from wildly shooting at the hip. We are wanting a long-term solution for accountability purposes as our database and company grows. Thanks in advance. 

And of cousre you would want associated tables for the customer, Customer address, partslookup, labor tasks lookup, etc. You might also want to provide a place on your form to add a new part or labor task to the appropriate lookup table. Sometimes you may need to quote something that is not already in the system and the best place to do that is while you are creating the quote. 

We have a good sized Enterprise system that has many of SSIS packages taking data in and out of the system daily. Some of the strategies we have used are: Only process deltas especially in large files. To figure which records are deltas we use change tracking and send that data to tables in a separate database, so the process of figuring out the deltas when we receive a full file is pushed off onto a staging server. Only records that are new or changed go to the real production server. On our busiest server, we have moved all teh processing except the final load off to a completely separate server. If the file goes directly to table that does not also have transactional data changes or multiple data sources, we might have two tables (and A and B version) and a view that selects from the active table. So we do our processing on the inactive version (possibly including dropping and recreating indexes), switch the table in the view (meaning there is about 1 second of down time fromteh user perspective), then update the new inactive table. SSIS packages can be designed to run very quickly or not so quickly. So we may spend a lot of time performance tuning one that takes too long and affects production. Run the packages during the time period when there is the least usage on the server. 

I have an issue where within my SQL Server 2016 Standard instance, is set to but within the file, it is capturing . Once I observed the behavior, I thought that the reason for this was that I had an trigger enabled. This trigger was created to prevent usage of a SQL Server credential when connecting with SQL Server Management Studio that originates from a that has not been delegated access. This was added to allow access from specific terminals outside of the domain (but within the same network) that the SQL Server is hosted on. This is the trigger: 

The data it stores is enumerated based on a JavaScript method that uses information from their browser (I don't know much more than that, but could find out if needed): 

I am attempting to update a query that utilizes the operator within a clause predicate with to compare potential performance improvements and better understand what is happening behind the scenes when the two are interchanged. It is my understanding that in practice, the query optimizer treats and the same way whenever it can. I'm noticing that when the query is ran with the operator, it returns the desired result set. However, when I replace it with the equivalent, it pulls in all values from the primary table I want to filter. It is ignoring the provided input values passed to and returning all possible distinct values. The use case is relatively straightforward: the query accepts a pipe delimited string that can contain up to 4 values, e.g. or . From here I the input into a table variable to determine the corresponding internal for the . The result set returned is then filtered to exclude the that were not passed. To illustrate, here is a similar table definition: 

It's a good idea to havae database in any event. We use ours to store exception records, configurations, logging information on packages that have been run, staging table, etc. 

And a good bit of slow performance in databases is due to bad indexing or badly performing queries. No amount of tweaking the server itself will fix those. Findout what they are doing when it gets slow. Don't just randomly fix things and hope that things improve. Know what is causing the issue and measure before and after to make sure you have improved. 

Of course creating views that call other views is a performance killer too. Do not go down that route. Write the queries you need and don't use either TVFs or views if you want performance. It is the layering that is creating the problem, this is almost always a bad thing to do when querying a database and you can quickly end up hitting the limit of the number of tables you can reference too, especially since you often end up referencing the same tables in different layers. Further, while this seems as if it woudl be easier to maintain, it is not. Try debugging or adding a column due to a new requirement when the layer you need to fix is at the bottom.