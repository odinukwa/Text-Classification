Note that only NetworkManager pays attention to this setting, so if you've disabled NetworkManager and are using the old network script, it will be ignored. You can also make the equivalent setting in the NetworkManager GUI. 

Remember that specifies the document root; the URL path is appended to that. So this would attempt to access , which is not what you want. You can either correct the (preferred, if the directory structure matches) or use an . 

starts a shell unless you instruct it otherwise. It looks like you actually want to run this script as another user. To do that, try something like this: 

Nagios considers a host or service that is acting in the manner you describe to be flapping. You may wish to tweak your flap detection for this host/service. 

That's all. The only thing you really need to check is whether the user has a cookie indicating he's logged in, left a comment and wanted to be remembered, or entered a post password. Everything else either doesn't matter or doesn't apply to WordPress. 

Since you are bridging, you need to set the IP addresses in the container only, and not on the host. The host should only have its own IP address(es). 

That compiler setup may work for hhvm, but it's pretty useless for anything else. It's quite difficult to have two gcc versions on the same system. You could do something like use a Software Collection, but I personally don't like those as they are not very easy to use. You should be using CentOS 7, anyway, as it won't require you to replace the compiler, and more things will be current. Overall, basing the system on C7 will pretty much solve all your problems anyway and be more future proof. 

Because 204.51.178.49 has no PTR record, the reverse DNS check fails. The second step is to take the hostname that was returned, and look up its IP address. That IP address must match the IP address that made the connection to the server. This is what forward confirmed means. If it does not, then again we reject the connection. (So few make it this far that I don't even have an example in the last week's worth of logs.) 

Heroku itself only provides direct support for PostgreSQL. Many applications (especially those in Rails) can be migrated from MySQL to PostgreSQL with little issue. For situations in which you can't easily migrate, you can use a third party Heroku add-on. 

This should generally only be done when a software package is being renamed. If you try to use this to remove other software, you will break someone's system, and administrators will curse you forever... In this case, use the directive in your spec file. For instance, in an example package : 

That is not the version of perl which comes with CentOS. From looking at the output, it seems you have a second version of perl installed in and was built by God-only-knows-who. It also appears to be a pretty minimalist version of perl. As I mentioned before, you really should start over with a CentOS box that hasn't been corrupted by whatever breakage cPanel applies. You will continue to run into problems that are difficult to solve, as long as you use it. Anyway, try removing from your and then building the program. This should ensure that the locally installed Perl doesn't get run, and the system Perl (which works correctly) gets used instead. 

We build backup systems for one purpose: To enable restores. Nobody cares about backups; they care about restores. There are three reasons one might need to restore file(s): Accidental file deletion, hardware failure, or archival/legal reasons. A "complete" backup system would enable you to restore files in all of these scenarios. For accidental file deletion, things like Dropbox and RAID fail because they simply reflect all changes made to the filesystem, and a deleted file is gone in these scenarios. Your backup system should be able to restore a file to a recent point in time fairly quickly; preferably the restore would complete within seconds to minutes. For hardware failure, you should use solutions such as RAID and other high-availability approaches when possible to ensure that your service remains up and running, as a full restore of a system can take hours or possibly days due to the necessity of reading and writing to (relatively) slow media. Finally archives, or full backups (or equivalent) of the systems at a specific point in time, can serve restores in both legal and disaster recovery scenarios. These would typically be stored off-site, in case a stray meteor turns your data center into a smoking crater... Your complete backup system should be able to support restores for any of these three types, with varying levels of service (SLA). For instance, you may decide that a deleted file may be restored with one business day granularity for the last six months and one month granularity for the last three years; and that a disk failure should be capable of being restored within four hours with no more than two business days of data loss. The backup system must be able to implement the SLA in a backup schedule. Your backup system must be fully automated. This cannot be stressed enough. If the backups aren't fully automated, they simply won't happen. Your backup system must be capable of fully automated backups, out of the box, with little or no special configuration or scripting required. You must periodically test restores. Any backup system is utterly useless if restoring from backup fails to work. I think most of us have horror stories along these lines. Your backup system must be able to restore single files or whole systems within the SLA you're implementing. You must purchase backup media on an ongoing basis. Whether you're just doing on-site tape backup or going whole hog with off-site cloud backup, make sure you have it in the budget to pay for the gigabytes (or terabytes!) of space you will need. 

If you don't have a GUI then this will simply use the terminal as you expect, though it will take advantage of the GUI if you have one. Note that if you receive the message that "Application tried to create a window, but no driver could be loaded." it means your "console" application is actually a GUI application and requires a GUI. In this case consider running it in a virtual X server such as Xvfb. 

You installed the RPM Fusion release RPM for Fedora instead of the one for CentOS. I'm not entirely sure how you did that; it should reject attempts to do so. To fix the problem, remove the release files you installed, and then download and install the correct ones. 

It decided to kill the child with pid 20977, a shell script that was spawned by the process. If you want Linux to always kill the task which caused the out of memory condition, set the sysctl to 1. 

If you already use firewalld, then you should have fail2ban also use firewalld. There's no point in having it use iptables directly in this scenario. Not to mention that has much better performance for large ban lists than . 

Typically, Ubuntu and other Debian-based distros will start a service as soon as it's installed; Red Hat based distros do not. In either case it's not necessary to reboot; if the service hadn't been started automatically as part of the installation, you could have started it yourself. 

In future, be careful with manually removing files included with system packages, be careful with which third party repositories you use, and be especially careful if you mix packages from different Debian versions (e.g. stable and testing). 

You can see clearly that Drupal generated the 503 Service Unavailable error. When you put Drupal in maintenance mode, it serves this error to anyone who is not already logged in with sufficient rights. That includes Googlebot. To resolve the issue, take Drupal out of maintenance mode. 

What I found here is that the command to verify config was not actually changing from the root user to the toranon user before reading the configuration, so access to the directory was being denied, as systemd did not give the process CAP_DAC_OVERRIDE. You can see the user IDs were logged as . So to resolve this on my system, I decided to have systemd start Tor as toranon rather than starting as root and Tor changing its own uid/gid. 

Those package names are for Debian-based systems, and you aren't on such a system. Use the equivalents for Red Hat-based systems instead. (Which they really should have provided for you so that I wouldn't have to look them up.) 

By default non-root users cannot access libvirtd directly, unless explicitly granted authorization. I've done this using PolicyKit: 

While both distributions use the RPM package format, the similarities end there. Generally, the packages are not compatible between them. It's a particularly bad idea to substitute core system packages from the wrong distribution, as the inevitable problems which arise (such as, in this case, no one being able to log in) will be unsupportable by either vendor. As for your specific packages, it appears that the functionally equivalent packages on SuSE are named and respectively. If the person who wrote those instructions is available, you should have a long chat with him. 

Or you can use a virtual PC console instead of a virtual serial port, but you may need to reconfigure the virtual machine to do that. 

I'm assuming you've placed a file you want to include on the same DVD or USB media from which you are trying to install RHEL. The key here is that the installation media is not mounted at when you boot into it. Rather, is mounted from the read-only filesystem image on the installation media. The DVD/USB media itself gets mounted at in RHEL 7. So if you placed a file in the top level directory of the installation media, you will find it at . 

So far I can find no evidence that nginx supports this functionality, nor of any unofficial patches. Nor, from a quick look, did I find anything relevant for Apache or any other web server. I am sure it will be added eventually, but if you really need this soon, I would suggest asking on the nginx mailing list. 

You are probably not using a UTF-8 locale on your system. You can temporarily work around this in your shell by setting your locale to a UTF-8 locale, for instance: 

You need to open endpoints for the services you want to make accessible from the Internet. In the old Azure portal these are listed under Endpoints. In the new portal under Inbound Security Rules. 

Make sure you have the package installed properly. If it is installed, you may have accidentally deleted its files and need to reinstall it. 

Finally it loaded the drivers within Startup Repair and I was able to see the virtual hard drive. One problem solved. 

@mattdm notes in a comment below that a bug has been filed against FirewallD for exactly this behavior. The bug appears to have been fixed, by removing the options and and replacing them with and . This has been committed and will appear in a future release. 

There is only one place on a Linux system that is the source of truth regarding the system timezone for userland processes, that is . And indeed, is managing that file, creating a symbolic link to the appropriate zoneinfo file. 

You don't actually have MySQL's yum repository installed. As the directions advised you, you need to do that first. 

If you want to use a wildcard address (listen on all IP addresses) in a , you need your directive to be listening on all IP addresses. 

Don't bother with this proposed kludge. Just set the VM to have a fixed MAC address. Edit the VM settings in vSphere Client and set the MAC address to Manual (then specify an address). 

The wrapper is created in the directory, and then you call it from your init script after switching users. 

Since your host machine here is going to be a workstation here, rather than a server, I would suggest VMware Workstation or VirtualBox. Both have sufficient OpenGL/Direct3D support to do light gaming; I've even managed to run World of Warcraft in a Windows Vista VMware virtual machine on a Linux host. Both also have decent performance (after installing guest tools) and full screen mode for pretending it's the only OS on your computer at the time. Remember that the host machine must have the video card manufacturer's 3D drivers installed for best performance. This means AMD or NVIDIA video card; Intel stuff will do 3D desktop compositing, but won't give you decent gaming performance in this scenario. 

I found several references on the Internet that indicate that HostGator installs a custom firewall script on their VPS and dedicated servers. However I wasn't able to find any instructions on how it was installed or how to get rid of it. I would contact HostGator for further information, or consider switching to another provider. 

See the man page for more options such as specifying the server IP address if NetBIOS resolution fails, operations you can use to browse or get files, etc. 

So if your file is 1000 bytes, and the client requests bytes=501-2000, you will return a 206 and serve bytes 501-1000. (And remember that ranges start from zero, so watch out for off-by-one errors.) In the case where no part of the range is satisfiable, you send a 416 error, but you do not send any file content. The whole point of the error is that there is nothing to send! 

You are not supposed to configure the failover IP addresses on the host in this scenario. They must be configured only in the guests. Better yet, use your IPv6 addresses to access your VMs from the host. This is going to be more reliable anyway. 

Actually it sounds like you have a PC and a printer on network A, and you're printing from the terminal server on network B, via some proprietary network "thing" that's costing you $400 a month for something that should be approximately free (i.e. a VPN). The VPN approach would be easiest (and, as I said, approximately free since you already have the VPN infrastructure) and would only require you to adjust the relevant IP address/port numbers so the terminal server can continue to reach the printer. The simplest solution would probably be to install the label-printing program on a PC at site A and forget about running everything back and forth over the network twice.