Here is something which came up in my algebraic geometry class, and I'm wondering if it has a deeper explanation. Let $F(w,x,y,z) = w^3+x^3+y^3+z^3$ and let $X$ be the cubic surface in $\mathbb{P}^3$ where $F$ vanishes. As is well known, there are $27$ lines on $X$ (in characteristic $\neq 3$). I had my students verify that the obvious equations on the Grassmannian $G(2,4)$ define the $27$ points corresponding to these lines as a reduced scheme. This is easiest to do in an affine cover. For example, consider the affine chart of lines of the form $$\mathrm{RowSpan} \begin{pmatrix} 1 & 0 & p & q \\ 0 & 1 & r & s \end{pmatrix}.$$ This chart contains $18$ of the $27$ points in question. The obvious equations come from setting equal to $0$ the coefficients of $t^3$, $t^2 u$, $t u^2$ and $u^3$ in $$F(t,u,pt+ru,qt+su).$$ I.e. $$1 + r^3 + s^3 = 3 p r^2 + 3 q s^2= 3 p^2 r + 3 q^2 s= 1 + p^3 + q^3=0. \quad (\ast)$$ To verify that they define a radical ideal, one must check the Jacobian condition: I.e., that $$\det \begin{pmatrix} 0 & 0 & 3 r^2 & 3 s^2\\ 3 r^2 & 3 s^2 & 6pr & 6 qs \\ 6pr & 6 qs & 3 p^2 & 3 q^2 \\ 3 p^2 & 3 q^2 & 0 & 0 \\ \end{pmatrix} \neq 0$$ at each of the $18$ roots of $(\ast)$. (Actually, I only assigned them to do one root; then the large symmetry group of $X$ does the rest.) So, here is the thing I can't explain. Out of curiosity, I computed the above determinant and factored it. It turns out that $$\det \begin{pmatrix} 0 & 0 & 3 r^2 & 3 s^2\\ 3 r^2 & 3 s^2 & 6pr & 6 qs \\ 6pr & 6 qs & 3 p^2 & 3 q^2 \\ 3 p^2 & 3 q^2 & 0 & 0 \\ \end{pmatrix} = - 81 \det\begin{pmatrix} p & q \\ r & s \end{pmatrix}^4 .$$ Is there any deep reason for this? Does that determinant even have any significance away from the $18$ points which describe lines on $X$? 

Partizan generalizations of Sprague-Grundy are the subject of Conway's book On Numbers and Games. His later work, Winning Ways for your Mathematical Plays with Berkelamp and Guy, gives examples of how to apply the theory to games that human beings might actually want to play. I'm afraid that it's been too long since I've read it to remember any nontrivial examples. 

The growth rate is exponential. Specifically, I will prove the bound $J_n \leq \frac{(n+1)^{n} 2^n}{(n+1)!}$, which grows like $(2e)^n$. I can also show that $J_n \geq 2^n$. I would guess that neither of these are the true rate of growth. I'll also give an explicit recursion for the $J_n$, which makes it clear that they are all in $\mathbb{Q}[\pi]$ and should make it easy to compute them recursively. It is convenient to define $J_n(b)$ to be $J_n$ with every upper bound $1$ replaced by $b$. For example, $$J_2(b) = \int_{t_2=0}^b d t_2 \int_{t_3=0}^{t_2} d t_3 \frac{1}{\sqrt{b-t_2}} \left( \frac{1}{\sqrt{b-t_3}} + \frac{1}{\sqrt{t_2-t_3}} \right)$$ By homogeneity, $J_n(b) = J_n \cdot b^{n/2}$. We set $J_0(b)=1$. Here is the recursion in the form which is easiest to prove: $$J_n(b) = \sum_{k\geq 1} \frac{1}{k!} \sum_{\begin{matrix} n_1+n_2 + \cdots + n_k = n \\ n_1, n_2, \ldots, n_k >0 \end{matrix}} \prod_{i=1}^k \int_{c_i=0}^b \frac{J_{n_1-1}(c_i) d c_i}{\sqrt{b-c_i}}$$ Sketch of proof of recursion: The left hand side is a sum of $n!$ terms which are naturally indexed by functions $f: \{ 2,3,\ldots, n+1 \} \to \{1,2,\ldots, n \}$ obeying $f(i) \lt i$. The term corresponding to $f$ has integrand $1/\prod \sqrt{t_{f(i)} - t_i}$, where $t_1=b$. Group together those summands for which $f^{-1}(1)$ has size $k$; let $(u_1, u_2, \ldots, u_k) = f^{-1}(1)$. For any $x \in \{2,3, \ldots, n \}$, there is precisely one $u_i$ such that $u_i \in \{ x, f(x), f(f(x)), f^3(x), \ldots \}$ Say that "$x$ passes through $u_i$". Let $n_i$ be the number of $x$ which pass through $u_i$. We group together all terms on the left with the same unordered list of multiplicities $(n_1, n_2, \ldots, n_k)$. On the right, group together all terms which differ only by permuting $(n_1, n_2, \ldots, n_k)$. I claim that the corresponding sums on the two sides match up. An example is probably clearer than a proof here. Take $n=3$ and look at the partition $3=2+1$. On the left hand side, we have terms accounting for $(f(2), f(3), f(4))$ equal to: $(1, 1, 3)$, $(1,1,2)$ and $(1,2,1)$. On the right hand side, we have $2$ copies of $$\frac{1}{2!} \int_{c_1=0}^b \int_{c_2=0}^b \int_{t=0}^{c_1} \frac{d c_1 dc_2 dt}{\sqrt{b-c_1} \sqrt{b-c_2} \sqrt{c_1-t}}$$ The two copies cancel the $1/2!$. We are integrating over the region $0 \leq t \leq c_1$, $0 \leq c_2$. There are three possible orderings for the integrands: $0 \leq c_2 \leq t \leq c_1$, $0 \leq t \leq c_2 \leq c_1$ and $0 \leq t \leq c_1 \leq c_2$, and these correspond to the three functions $f$ above. The confusing thing in writing out a general proof is noticing that the $1/k!$ and the number of orderings of the partition $n_1+n_2+ \cdots + n_k$ exactly cancel to eliminate any double counting. Since you tagged this with "feynmann-integrals", I assume you are used to the way these sort of symmetry factors work. $\square$ Simplifying the recursion We first plug in $J_m(b) = J_m \cdot b^{m/2}$ to obtain $$J_n = \sum_{k\geq 1} \frac{1}{k!} \sum_{\begin{matrix} n_1+n_2 + \cdots + n_k = n \\ n_1, n_2, \ldots, n_k >0 \end{matrix}} \prod_{i=1}^k J_{n_1-1} \int_{c_i=0}^1 \frac{ d c_i \ c_i^{(n_i-1)/2}}{\sqrt{b-c_i}}$$ The integrals are now simple one dimensional integrals and can be evaluated: $$J_n = \sum_{k\geq 1} \frac{1}{k!} \sum_{\begin{matrix} n_1+n_2 + \cdots + n_k = n \\ n_1, n_2, \ldots, n_k >0 \end{matrix}} \prod_{i=1}^k \left( J_{n_1-1} \cdot \frac{\sqrt{\pi}\ \Gamma((n_i+1)/2)}{\Gamma((n_i+2)/2)} \right)$$ Note that $\left( \frac{\sqrt{\pi}\ \Gamma((n_i+1)/2)}{\Gamma((n_i+2)/2)} \right)$ is rational if $n_i$ is odd and is a rational multiple of $\pi$ if $n_i$ is even, so this shows that all your polynomials are polynomials in $\pi$. (I am feeling nicely superior to Mathematica now. ) Upper bounds We have $ \frac{\sqrt{\pi}\ \Gamma((n_i+1)/2)}{\Gamma((n_i+2)/2)} \leq 2$. So $J_n$ is bounded by $K_n$, where $K_n$ is defined by the recursion $$K_n = \sum_{k\geq 1} \frac{1}{k!} \sum_{\begin{matrix} n_1+n_2 + \cdots + n_k = n \\ n_1, n_2, \ldots, n_k >0 \end{matrix}} \prod_{i=1}^k \left( 2 \cdot K_{n_1-1} \right)$$ Define the generating function $g(x) = \sum_{n=0}^{\infty} K_n x^n$. Then this shows $$g(x) = \exp(2 x g(x))$$ We thus see that $g(x) = -W(-2x)/(2x)$ where $W$ is the Lambert $W$ function and so $$K_n = \frac{(n+1)^{n} 2^n}{(n+1)!}$$ as promised. Lower bound We can use a similar trick to get a lower bound. Note that $\frac{\sqrt{\pi} \Gamma((n+1)/2)}{\Gamma((n+2)/2)} \geq \frac{2}{n}$. So $J_n \geq L_n$ where $L$ is defined recursively by $L_0=1$ and $$L_n = \sum_{k\geq 1} \frac{1}{k!} \sum_{\begin{matrix} n_1+n_2 + \cdots + n_k = n \\ n_1, n_2, \ldots, n_k >0 \end{matrix}} \prod_{i=1}^k \left( \frac{2}{n} L_{n_1-1} \right)$$ Putting $L(x) = \sum_{n \geq 0} L_n x^n$, we see that $$L(x) = \exp\left( 2 \int_{t=0}^x L(t) dt \right).$$ This integral equation has the unique solution $L(x) = 1/(1-2x)$, so $L_n = 2^n$, as promised. 

Your sequence is Sloane A096368. Sloane links to this page, which has files of all examples up to $13$ vertices. MathSciNet has 30 papers with "regular tournament" in the title, none of which seem to know much about enumeration up to isomorphism. A quick scan of the papers with "balanced tournament" in the title suggests that that term means something else, so I would search on "regular tournament". McKay proves an asymptotic formula for the number of LABELED regular tournaments of the form $2^{\binom{n}{2}} e^{-O(n \log n)}$. (See his paper for a much more precise statement.) Since the size of $n!$ is "only" $e^{O(n \log n)}$, we can deduce that the number of isomorphism classes is also $2^{\binom{n}{2}} e^{-O(n \log n)}$, and in particular goes to $\infty$ as $n \to \infty$. 

Now, we need to show that our construction is reflected in properties of the group rings. Let $R_i = \FF_p[G_i]$. Our first goal is to show that $R_1 \not \cong R_2$. We must show how to canonically recover $V_i$, $Z_i$ and $\phi_i$ from $R_i$. The only $1$-dimensional representation of $G_i$ over $\FF_p$ is the trivial rep, since $G_i$ is a $p$-group. Therefore, $\FF_p$ is an $R_i$ module in only one way. Let $I_i$ be the kernel of the unique map $R_i \to \mathrm{End}(\FF_p)$. Explicitly, $I_i$ has $\FF_p$ basis given by the elements $g-1$, for $g \in G_i$. The center of $R_i$ is $\FF_p[Z_i]$. For $z$ and $z' \in Z_i$, we have $(a^{z+z'}-1) - (a^z-1) - (a^{z'}-1) = (a^z-1)(a^{z'}-1)$, so $(a^{z+z'}-1) \equiv (a^z-1)+(a^{z'}-1) \bmod (Z(R_i) \cap I_i)^2$. We see that the $\FF_p$ vector space $Z_i$ is canonically isomorphic to $(Z(R_i) \cap I_i)/(Z_i \cap I_i)^2$, by $z \mapsto a^z-1$. (Expressions like $(Z_i \cap I_i)^2$ mean the square as a two-sided ideal.) We claim similarly that $V_i \cong I_i /I_i^2$ by the map $v \mapsto a^w-1$, where $w$ is an arbitrary lift of $v\in V_i$ to $W_i$. To this end, we first must prove the formula is well defined. If $w$ and $w+z$ are two different lifts, then we must show that $a^{w+z} - a^w = (a^z-1) a^w \in I_i^2$. In other words, we must show that $a^z-1 \in I_i^2$ for $z \in Z_i$. Now, if $z = \phi_i(w, w')$, then $(a^w-1)(a^{w'}-1) - (a^{w'}-1)(a^w-1) = a^{w+w'+\phi_i(w,w')} - a^{w+w'+\phi_i(w',w)} = (a^{2\phi_i(w,w')}-1) a^{w+w'-\phi_i(w,w')} \in I_i^2$. Therefore, $a^{2 \phi_i(w,w')}-1 \in I_i^2$. Letting $w$ and $w'$ vary, we can obtain $a^z-1 \in I_i^2$ for any $z \in Z_i$. We have now checked that $v \mapsto a^w-1$ is a well defined map of sets $V_i \to I_i/I_i^2$. To see that it is a map of $\FF_p$ vector spaces, note that $$(a^{w+w'}-1) - (a^w-1)- (a^{w'}-1) = (a^w-1)(a^{w'}-1) + a^{w+w'} (1-a^{\phi_i(w,w')}).$$ Both summands on the right hand side are in $I_i^2$. A bit more work shows that this map of $\FF_p$ vector spaces is an isomorphism. Now, we must show that we can recover $\phi_i$. Let $\langle Z(R_i) \cap I_i \rangle$ be the two sided ideal of $R_i$ generated by $Z(R_i) \cap I_i$. Fix a section $V_i \to W_i$. Using this section, we can write any element $c$ of $R_i$ uniquely as $\sum_{V \in V_i} c_v a^v$. We have $c \in \langle Z(R_i) \cap I_i \rangle$ if and only if all the $c_v$ are in $Z(R_i) \cap I_i$. For $c \in \langle Z(R_i) \cap I_i \rangle$, define $\sigma(c) = \sum_{v \in V} c_v$. We check that $\sigma(c) \bmod (Z(R_i) \cap I_i)^2$ is independent of the choice of section. So $\sigma$ gives a well defined map $\langle Z(R_i) \cap I_i \rangle \longrightarrow (Z(R_i) \cap I_i)/(Z(R_i) \cap I_i)^2 \cong Z_i$, which we will also denote $\sigma$. For any $w$ and $w' \in W_i$, we have $a^w a^{w'} - a^{w'} a^w = (a^{\phi_i(w,w')} - a^{\phi_i(w',w)}) a^{w+w'} \in \langle Z(R_i) \cap I_i \rangle$. Thus, it makes sense to talk about $\sigma(x x' - x' x)$ for any $x$ and $x' \in R_i$. Suppose furthermore that $x \in I_i$ and $x' \in I_i^2$. Then I claim that $\sigma(x x' - x' x)=0$. By linearity, we may assume that $x=a^w-1$ and $x' = (a^{w'}-1) (a^{w''}-1)$. So $$x x' - x' x = (a^{\phi_i(w,w'+w'')} - a^{-\phi_i(w,w'+w'')}) a^{w+w'+w''+\phi_i(w', w'')} - (a^{\phi_i(w,w')} - a^{-\phi_i(w,w')}) a^{w+w'} - (a^{\phi_i(w,w'')} - a^{-\phi_i(w,w'')}) a^{w+w''}$$ and $\sigma(x x'-x' x) = \phi_i(w,w'+w'') - \phi_i(w,w') - \phi_i(w,w'')=0$. This proves the claim. Therefore, $(x,x') \mapsto \sigma(x x' - x' x)$ gives a well defined map $I_i/I_i^2 \times I_i/I_i^2 \to (Z(R_i) \cap I_i)/(Z(R_i) \cap I_i)^2$. Tracing through definitions, this map is $2 \phi_i$. 

Does "inert" mean that there is only one prime over $p$, or does it mean that there is only one prime over $p$ and that prime is unramified? As the other answers have explained, unramified primes can not remain inert. However, it is possible that there is only prime over $p$. For example, let $p$ be an odd prime, take $K = \mathbb{Q}$ and $L= \mathbb{Q}(\sqrt{p}, \sqrt{a})$ where $a$ is not a quadratic residue modulo $p$. 

A nice way to think about the action of $SP_{2g}(\mathbb{R})$ on $S$ is that $S$ is an $SP_{2g}(\mathbb{R})$-invariant open set in the Lagrangian grassmannian. Let $V$ be a $2g$-dimensional complex vector space equipped with a symplectic form. Let $L$ be the space of isotropic $g$-dimensional subspaces of $V$. Such a subspace is called a Lagrangian. Clearly, $L$ is a complex manifold equipped with a holomorphic action of $SP_{2g}(\mathbb{C})$. Choose a splitting of $V$ as $U \oplus U^{\vee}$, where $U$ and $U^{\vee}$ are complementary isotropic subspaces of $V$ and the symplectic pairing between $U$ and $U^{\vee}$ makes $U$ and $U^{\vee}$ into dual spaces. Let $\Omega \subset L$ be the open set consisting of those Lagrangians which are the graph of a map $\phi: U \to U^{\vee}$. The condition that the graph of $\phi$ be Lagrangian is equivalent to the condition that $\phi$ be self adjoint, so $\Omega$ is identified with the space of symmetric $g \times g$ complex matrices. The action of $Sp_{2g}(\mathbb{C})$ on $\Omega$ isn't fully defined, because $\Omega$ isn't $Sp_{2g}$-invariant, but when it is defined you can check that it is given by the formula $Z \mapsto (AZ+B)(CZ+D)^{-1}$. Choose a real $2g$-dimensional symplectic vector space $V_0$ and identify $V$ with $V_0 \otimes_{\mathbb{R}} \mathbb{C}$. Let $\Omega' \subset L$ be the open set of those Lagrangian's $X$ such that $X \cap V_0 = \{ 0 \}$. Clearly, $\Omega'$ has a holmorphic action of $Sp_{2g}(\mathbb{R})$. If we identify $\Omega$ with $g \times g$ symmetric complex matrices, then $\Omega'$ is those matrices $Z$ for which $\det \Im(Z) \neq 0$. Obviously, one connected component of $\Omega \cap \Omega'$ is $S$: Symmetric matrices for which $\Im(Z)$ is positive definite. And it turns out that this is actually a connected component of $\Omega'$, so it inherits an action of $Sp_{2g}(\mathbb{R})$. This is the action you asked about. 

As expected, the answer is "no". Define a graded set to be a set $X$ with a map $\mathrm{deg}: X \to \mathbb{Z}_{\geq 0}$. For $X$ and $Y$ graded sets, and $d \in \mathbb{Z}_{\geq 0}$, define a map $f: X \to Y$ to have degree $d$ if, for all $x \in X$, we have $\deg(f(x)) = \deg(x) + d$. Define $\mathcal{C}$ to be the category whose obects are graded sets and where $$\mathrm{Hom}_{\mathcal{C}}(X,Y) = \{ (f,d):\ f: X \to Y,\ f\ \mbox{of degree}\ d \}.$$ $$(f,d) \circ (g,e) = (f \circ g, d + e).$$ Let me emphasize that $\mathrm{Hom}_{\mathcal{C}}(\emptyset,Y)$ is $\mathbb{Z}_{\geq 0}$; we do not identify the degree $d$ map from $\emptyset \to Y$ with the degree $e$ map, for $d \neq e$. Put a monoidal structure on $\mathcal{C}$ where: $$X \otimes Y := X \times Y \quad \mathrm{deg}(x \times y) = \mathrm{deg}(x) + \mathrm{deg}(y)$$ $$1_{\mathcal{C}} = \{ \mathrm{pt} \} \quad \mathrm{deg}(\mathrm{pt}) =0.$$ $$(f,d) \otimes (g,e) = (f \times g, d+e).$$ If I am not confused, this structure is closed, with $Y^X = \mathrm{Hom}_{\mathcal{C}}(X,Y)$ and with $\mathrm{deg}$ taking a map to its degree. Then take $X = Y = \mathbb{Z}_{\geq 0}$ with $\mathrm{deg}(i) = i$. Note that $Y^X \cong X$, but $Y$ has an automorphism $i \mapsto i+1$ with no fixed points. 

What you are seeing is that the orthogonal matrices of determinant $-1$ swap the two spin representations. The first several parts of this argument will be valid for $(4n+2) \times (4n+2)$ matrices as well, so I'll rename the size of the matrix to $2m$ and specialize to $m$ even when it becomes relevant. In more detail: Let $S$ be the $2m \times 2m$ change of basis matrix $$\frac{1}{\sqrt{2}} \begin{pmatrix} \mathrm{Id} & i \cdot \mathrm{Id} \\ \mathrm{Id} & -i \cdot \mathrm{Id} \end{pmatrix}$$ Note that $O$ is orthogonal if and only if $SOS^{-1}$ preserves the split bilinear form $x_1 x_{m+1} + x_2 x_{m+2} + \cdots + x_{m} x_{2m}$. Explicitly, $$S O S^{-1} = \frac{1}{2} \begin{pmatrix} O_{11} - i O_{12} + i O_{21} + O_{22} & O_{11} + i O_{12} + i O_{21} - O_{22} \\ O_{11} - i O_{12} - i O_{21} - O_{22} & O_{11} + i O_{12} - i O_{21} + O_{22} \end{pmatrix}$$ We want to show that, for $m$ even: If $U$ preserves the split bilinear form, and has determinant $-1$, then the upper right $m \times m$ block of $U$ is singular. Consider the action of $U$ on $\bigwedge^{m} \mathbb{C}^{2m}$; this is a big $\binom{2m}{m} \times \binom{2m}{m}$ matrix which we will write $\bigwedge^m U$. The entries of $\bigwedge^m U$ are $m \times m$ minors of $U$. In particular, the entry we care about is the coefficient with which $e_1 \wedge e_2 \wedge \cdots \wedge e_m$ is taken to $e_{m+1} \wedge e_{m+2} \wedge \cdots e_{2m}$. Let $\alpha_{+}$ and $\alpha_{-}$ be the highest weights of the spin reps of $\mathrm{Spin}(2m)$. Let $V_{\pm}$ be the irrep with highest weight $2 \alpha_{\pm}$. Then $V_{+}$ and $V_{-}$ both inject into $\bigwedge^{m} \mathbb{C}^{2m}$. The vector $e_1 \wedge \cdots \wedge e_m$ is a high weight vector for $V_{+}$. The vector $e_{m+1} \wedge \cdots e_{2m}$ is a low weight vector for $V_{(-1)^m}$. In particular, when $m$ is even, Both these wedge products lie in $V_{+}$. An orthogonal matrix with determinant $-1$ switches the spin representations. So this entry of $\bigwedge^m U$ is $0$. 

UPDATE I can now do the general case. For $f(x)$ a polynomial with real coefficients, let $V(f(x))$ denote the number of sign alternations of the coefficients of $f$. Lemma $V((1-x) f(x)) \geq V(f(x))+1$. Proof This is a standard lemma proved in the course of proving Descartes rule of signs. See, for example, this proof. $\square$ Lemma For $m \leq n$, we have $V((1+x+\cdots + x^k)^n (1-x)^m)=m$. Proof Our proof is by reverse induction. For $m=n$, we have $(1+x+\cdots + x^k)^n (1-x)^m= (1-x^{k+1})^n$ which has $n$ sign alternations. Now assume the lemma for $m$ and we will show that it follows for $m-1$. By the previous lemma, $V((1+x+\cdots + x^k)^n (1-x)^m) \geq V((1+x+\cdots + x^k)^n (1-x)^{m-1})+1$. So $V((1+x+\cdots + x^k)^n (1-x)^{m-1}) \leq m-1$. But also, $V((1+x+\cdots + x^k)^n (1-x)^{m-1}) \geq V((1+x+\cdots+x^k)^n)+m-1=m-1$. So $V((1+x+\cdots + x^k)^n (1-x)^{m-1})=m-1$ as desired. $\square$ The rest of the argument follows as before. We don't need to go past $m=n$ because, when $m=n$, the truncated partial difference sequence is already shortened to length $1$. In fact, not only have we shown that these sequences are unimodal, we have shown that they are log convex. Set $h(x) = V((1+x+\cdots + x^k)^n (1-x)^m)$. We showed that the coefficients of $h$ are unimodal in any string of constant sign. But, in fact, the same argument shows $h(rx)$ has the same property for any $r>0$. Saying that $(h_i r^i, h_{i+1} r^{i+1}, \ldots, h_j r^j)$ is unimodal for all positive $r$ is the same as saying that $(h_i, h_{i+1}, \ldots, h_j)$ is log convex.