You need three tables - , and an associative table for the many-to-many relation between them (a can have many s and an will have many s), call it or something. and will have autoincrement fields as a surrogate primary key, will have FKs to both and , like so: 

1: If the column you're changing is part of the clustered index definition, then yes. If not, no. Any non-clustered indexes involving that column will have to be rebuilt when you change the column type, also. 2: A heap just means no clustered index, so nothing to rebuild. Same as answer #1 for non-clustered indexes. 

You use when a column isn't being filtered on (isn't in the clause) but is being selected. means that the column will only be included at the leaf level of the index (it's a small efficiency saving, essentially). Also as noted, if you have a statement without a where clause, indexes will never be used (faster to just scan the table rather than muck about with an index). Index seek is only used when you're returning a comparatively small number of rows, for your sample query if most of your and rows are marked as Active then the optimiser may well decide to index scan instead of seek. A query that would be more likely to get an index seek instead of scan would be something like: 

I gave up resolving this issue for a while now. Could someone please assist me on this? I need to create this Linked server!! What am I doing wrong all these days? 

I just by chance added alert for Error 825 and ran sp_Blitz again. And the message for Finding "No Alert for Corruption" was not displayed for Error 823 and 824 !! After Alert setup: 

Full Backup started on 4/21 12 AM. While backing up one of the database, it is stuck. sp_WhoIsActive shows following information 

We recently migrated to a new SQL Server and the SQL Services are running under a Service Account. I did observe that the Service Account do not have rights on G drive and the old files are not being cleaned up. Question: Why doesn't the procedure when it cannot delete a file OR simply move on instead of waiting for around 9.5 minutes? Also, doesn't the job error if it cannot delete the old backups? We will not know until we receive alert !! Here is the log details. 

Example below is, for each column in the table that starts with either 'A' or 'B', return all rows in the table that have a non-null value in any of those columns. Note: I don't have a Sybase install handy to test this. Chances of it actually working are very low, but it should hopefully be enough to make you realise what a bad idea this is. 

is a session-level command in Sybase ASE, it's not a server-level setting (if it was a server-level setting you'd be able to alter it via ). Can you run wireshark (or something similar) on the packets being sent from the JDBC client to see if it's setting showplan on as part of the session initialization? That said - showing the plan should not affect database CPU or memory usage, the plan is generated by the query optimiser anyway. However, showing the plan will increase network utilization. 

I didn't mention the PK on , for efficiency it would be a compound PK on , but there's also an argument to be made for a seperate surrogate primary key (which I personally think is a waste of space, unless you need to allow for multiple s between one and one ). For a purely associative table in a many-to-many relationship, though, the compound primary key should work fine. Edit: The complicated part of this isn't in the relational design, it'll be in the application code, because you'll need best-match/partial matching in order to show users the groups that most closely match their interests (rather than having accidental splinter groups all over the place). And if you want to out-Facebook Facebook, you'll need a smoother and better user experience than they offer. 

DatabaseBackup - USER_DATABASES - LOG: This job fails saying "Executed as user: Domain\XXXX-SVC. Unable to open Step output file. The step failed." The error is only with LOG backup job. The other DatabaseBackup jobs (FULL, DIFF) works just fine with same SVC account. So the service account have appropriate permissions. The Output File(Job Step properties-->Advanced) is F:\SQLAgentLog\ which is same for all jobs. Only problem is with LOG backup job. Has anyone else experienced this and is there any solution? Current environment: SQL Server: 2012 SP3 CU8 OS: Windows Server 2012 Note: This was working all good on a Windows Server 2008!! 

I had a situation where the Native Backups were being made on a Server. I happened to see in that there was a third party backup tool () that was also taking VSS (kind-of) . At some interval, the AppAssure (backup being made to ) was doing a and at some other interval it was doing a breaking the log chain. Is there any way() to know when a backup log chain is broken? Here is a screenshot of the situation from February. 

How often you need to run index maintenance/rebuild stats depends on your database load, specifically how often your data is modified (i.e. //). If you're modifying data all over the show (i.e. a staging table for a weekly batch process), you probably want to update stats/reorganize indexes nightly. If your data is rather more static you can probably make it a weekly or fortnightly schedule. 

(I'm assuming SQL Server in this answer, please clarify which RDBMS you're using if it's not SQL Server) The index you want is: 

For Aurora Postgres, there's two relevant cluster-level parameters (note they're not instance-level parameters): and . I haven't tested this myself but you should be able to modify them in the usual way using DB Parameter Groups. 

Note that if you have a front-end built in Access and just want to shift the database away from JET (Access's internal DB engine) onto a "proper" RDBMS, you can do so by migrating the data across and setting up linked tables inside Access to the new data source. 

Here is the scenario which I am working with IT team to get it right with no luck. Following are the users who work on a SQL Server. 

However, if the Domain1/User1 and Domain2\User1 are added as individual accounts then we could Login without issues. 

There is AD group on created on . The domains each other (per IT team). Now, we have a which is also trusted for both Domain1 and 2. We added AD group to . When either of Domain1\User1 or Domain2\User1 try to Login to SQL Server on Domain3 - we get Login Failed message. 

Since the command running indexoptimize was a deadlock victim, I am wondering the job succeeded instead of an error!! 

And I verified by only setting up 823 and ignoring 824 and 825, still the sp_Blitz does not report for other 2 missing (824 and 825) !! 

I have setup a SQL Agent job to delete files older than 7 days. The script does it job when run through windows powershell window. However the same script does not work from SQL Agent job 

So no it doesn't lock tables, rows or pages. However, any (even with set) will issue a (schema stability) lock, which basically means no schema changes can happen while the is executing (schema changes = add/drop column, change datatype of a column, change nullability of a column and a couple other operations I can't think of off the top of my head). A lock should not interfere with DML statements (///). 

(* I'm aware this would probably fail any security audit going, but to my mind if we've let an intruder into the server room that knows to look in the third drawer down for the unlabelled 'sa' password post-it, then we're screwed anyway.) 

Short version: It depends. Generally spoken Sybase SQL Server is smart enough to do things the fastest way, though. Long version: Sybase's query processor is, at it's core, very similar to the one used in MS SQL Server. It will create worktables (internal temporary tables; not visible to the user) if the result set is sufficiently large to overflow available memory (similar to a table spill in SQL Server). Otherwise, it'll do a pair of index scans (you do have indexes on and in both tables, right?), then a join and output, all in memory. Caveats: 

Is there a way to resolve this to make work on Domain3 SQL Server? Also, creating a group and adding the individual userID's from other domains worked too. We don't want a new group just for this. 

Server Config: RAM: 24GB - 21.5GB for SQL Server Processors: 8 Not much activity in this server - hardly 50 people access this SQL Server via Sharepoint. 

I am working with some old jobs and I found this code snippet. I am just wondering why would someone dump transaction log files from databases in to one file ? 

It turns out that the person migrated the jobs from old server to new server edited the jobs manually. One of the edit was to this was set to default log location on old server. On the migrated server it was edited to where the directory did not exist. (that person missed deleting the text in the path) On the other jobs (FULL and DIFF) the text had been removed so it was set like this and these both were working fine! 

I know I just spent this entire post detailing why EAV is a terrible idea in most cases - but there are a few cases where it's needed/unavoidable. however, most of the time (including the example above), it's going to be far more hassle than it's worth. If you have a requirement for wide support of EAV-type data input, you should look at storing them in a key-value system, e.g. Hadoop/HBase, CouchDB, MongoDB, Cassandra, BerkeleyDB. 

Re: Shrinking. I see so many people getting their claws out at the very mention of 'shrink' and 'database' in the same sentence, so time to clear things up a little. Shrinking data is baaaaad. It literally turns your indexes into quivering shells of their former glory. Don't do it. Shrinking log should not be done routinely, but if you have a ridiculously outsized log (i.e. in one case I saw a 40GB log for a 100MB database, due to whoever set it up putting recovery model to full then never dumping the transaction log), you can shrink the log (after ) to reclaim that space without any ill effects (although, obviously, the will chew up I/O while it's running). PS: Speaking of log files, check your log file size and autogrowth settings. Small autogrowth settings can lead to underperforming log I/O (due to a poorly-explained feature in SQL Server called Virtual Log Files), particularly on batch transactions. 

Is there any way to find who used Dedicated Admin Connection? Not active connection but the previous one which is already closed? 

P.S. The Service Account was granted rights and the old files are being cleaned up. Version Info: SQL Server 2012 SP3 CU8 / Windows Server 2012 

There is a SQL job scheduled with number of steps. Few steps are throwing Query timeout expired error but still the job step is being reported as Succeeded. I confirm that the steps are set to "Quit the job reporting failure" on failure. What could be the cause for the step to succeed? 

Also, I see another backup on the same database started 30mins later (Backup to VirtualDevice - I know this is AppAssure backup tool). From SQLSkills preemptive_os_waitforsingleobject, I see And this is blocked for 2 days - I don't think one of the session will terminate itself until I kill the other. However, LOG backups were happening without any issues. Only FULL backup and DIFF backup for that particular database was blocked. I killed the AppAssure session and all the long queue went away from AppAssure. I killed the DIFF backup as well. Now the only process left is this FULL backup with 100% complete with same and not willing to complete!! I had no other options but to kill the FULL db backup and restart backup jobs. And, the USER_DATABASES full refuses to start saying but showed nothing. Also, there was nothing in state. CurrentJobActivity in msdb showed that the job was active. Had to right-click-stop under jobs and then start the job again!! Any idea on how do we avoid this situation? (other than telling the IT-team to stop AppAssure?) [EDIT]: Adding the version 

(change table names as appropriate for Hashtags and Words) Or, slightly more complicated, get the top 10 words for all tweets mentioning a specific hashtag: 

There's no server-level 'read any database' permission and server-level roles can't be granted database-level permissions So yes, you will have to map users to databases individually. If you're using Active Directory, you can create a windows group, then give that group a login to SQL Server, then apply db_datareader in all databases for that group (you'll need to create users in each database though). 

Access is a perfectly fine database system for small scale individual-user apps. Here are some criteria for shifting: 

First: This isn't easy to implement and it's going to break very easily (by which I mean performance is going to be horrible, and it's a nice little route for SQL injection attacks if you're not very very careful). I strongly advise you to re-think what you're doing because there has to be a better way. Second: The question as it stands doesn't actually make sense - what condition are you checking on the columns? ? Also, do you want to or the conditions together? To actually answer the question, though, to do this you'll need to write some dynamic SQL: