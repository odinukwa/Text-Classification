You then calculate some figure of merit (e.g. chi-squared) for how closely the model and data agree and go through an iterative process to adjust the parameters and optimise the fit of model to data. A more sophisticated discussion can be found in this paper by Beauge et al. If you have the RV curves of both stars, then you can fit them both simultaneously. Obviously, they have $p$, $e$, $\gamma$ and $\omega$ in common, but their RV amplitudes $K_1$ and $K_2$ will be different. The ratio of $K_1/K_2$ gives you the ratio of the two stellar masses. If you only have one RV curve you are limited to estimating the mass function of the binary system. $$ \frac{M_2^{3} \sin^{3} i}{(M_1 + M_2)^2} = \frac{p K_1^{3}}{2\pi G},$$ where $i$ is the inclination of the orbit with respect to the line of sight. This can only give you a lower limit to $M_2$ unless $i$ is known. Taking your specific case study. If you know $M_1$ and $i$ (this could be the case for a transiting exoplanet, or maybe a binary featuring an eclipsed black hole candidate), then the primary radial velocity curve gives you $K_1$ and hence $M_2$. If the masses and $p$ are known then Kepler's laws give the orbital separation. There are a number of options if you want an off-the-shelf solution to fitting RV curves. Perhaps the best free one is Systemic Console. There is no fundamental difference between analysing the RV curves of stars with exoplanets and stars with unseen (stellar) companions. 

Early in the life of the universe it is thought that star formation in "metal-free" gas probably favoured larger stars. These Population III stars had short lives and very quickly enriched the interstellar and intergalactic medium with nucleosynthesis products. In fact the main enrichment of the interstellar medium (ISM) in our own Galaxy is thought to have taken place some billions of years before the Sun was born. Stars that are being formed now have basically the same composition as the Sun. The plot below shows the iron abundance (on a logarithmic scale) as a function of deduced age for stars in the Gaia-ESO spectroscopic survey (from Bergemann et al. 2014). As you can see, there has not been too much change in iron abundance over the last 10 billion years. 

Most type Ia supernovae are thought to arise from the thermonuclear detonation of white dwarfs that are composed almost entirely out of carbon and oxygen. These white dwarfs are the cores of relatively low-mass stars that have lived their lives, gone through stages of core hydrogen and helium burning, leaving behind degenerate carbon/oxygen cores that become cooling white dwarfs after the outer envelope has been shed during the asymptotic giant branch and planetary nebula phases. As such, their composition, at least to first order, is almost independent of the initial composition of the star from which they were formed. That is, even if the progenitor star had a very low initial metal content, the white dwarf produced would still be almost exclusively a carbon/oxygen mixture, which had a similar Chandrasekhar mass and a similar explosive potential. It is well known however that not all type Ia supernovae are the same. It has long been known that their light curves are subtley different and there is a so-called stretch factor that can be applied to get a "corrected" peak magnitude. a.k.a The width-luminosity relation. More recently there has been a realisation that type Ia supernovae could arise from both accretion or mergers and there is clear evidence that the amount of radioactive Ni varies from explosion to explosion. A very recent paper by Milne et al. (2015) has however challenged the view of metallicity independence. They claim there are two populations of type Ia SNe, connected with progenitor metallicity, and that these populations become more apparent at high redshift when looking at rest-frame ultraviolet emission. The gist of their conclusions is indeed, as your question supposes, that this may go some way to ameliorating (but not eliminating) the need for dark energy. 

Cosmic ray "hits" are artifacts caused when high energy particles (cosmic rays, often muons) slam into atoms in the CCD itself and liberate large numbers of electrons, which then show up as bright spots and streaks when the CCD data is read out. They can also occur as a result of radioactive decay processes in the material in the detector and instrument itself and also the immediate surroundings. They are a tremendous nuisance that can be removed to a certain extent by median stacking images - though this also fails if you get into a situation where cosmic ray hits are found in the same pixels of $>1$ images in your stack. This limits the lengths of exposures that can safely be made with astronomical CCDs. Depending on exact usage, this limit is usually between 30 minutes and an hour. The flux of muons and the distribution of angles at which they reach the detector is actually very well known already. It is largely unaffected by local shielding (unless your telescope is buried far underground!) It is possible to distinguish different types of events (muons, locally produced gamma rays etc.) from looking at the pattern made on the detector (e.g. Groom 2004), but I don't think you can tell much about their energies, unless it could be by looking at the flux as a function of how much shielding (lead?) or how far underground the detector was, since often only a small fraction of the cosmic ray energy ends up deposited in the CCD. 

The Sun is of course in motion with respect to other stars in our Galaxy, but it does not move quickly compared with the vast distances involved. For instance it takes about 220 million years for our Sun to orbit the Galaxy once, travelling at around 200 km/s. The stars that are closest to the Sun tend to be orbiting in more-or-less the same direction and at a similar speed (that is why they are in the vicinity of the Sun). Thinking specifically about Polaris. There are three components of its motion with respect to the Sun - two tangential directions on the plane of the sky and a line of sight velocity. Using the SIMBAD CDS database we see that Polaris has a line of sight velocity of 16 km/s towards the Sun and tangential motions of 28 km/s in the right ascension direction and 7 km/s in the decreasing declination direction. This sounds a lot, but a velocity of 1 km/s means it takes about 300,000 years for the star to move 1 light year, and Polaris is about 400 light years from Earth. So, given its net velocity, the position of Polaris with respect to us will show significant changes (of degrees) on timescales of hundreds of thousands to millions of years. It is heading southward in the sky, but will take around 25 million years to cross the equator at the rate it is travelling now. You say that the constellations are "constant". Yes, they are on human timescales, but the motions of the stars both radially and tangentially is routinely measured. If you waited some millions of years, the constellations would look very different. NB: I am ignoring the precession of the Earth's rotation axis, since this is just a rotation of the coordinate system and not a change of position of the stars with respect to the Earth. 

You are on the surface of the Earth now (I assume), as are almost all other humans. The surface of the Earth can be thought of as a two dimensional plane. OK, let's say you do something amazing and you want to tell everyone on Earth about it, so you set up a means of broadcasting your news, at the speed of light, across the surface of the Earth. The speed of light is quick, but it still takes nearly two tenths of a second for your signal to make it round the world. Before that signal gets to them, nobody knows your news. You can imagine a circle, defined by the speed of light times the travel time, spreading outward from your location, containing those regions that have heard the news. Now what you do is imagine those circles stacked on top of each other, each drawn after a small increment in time, with a larger radius, and separated vertically by an amount that represents that increment in time. The circles begin at your position, when you start to broadcast. This stack of circles of increasing radius forms a light cone. Inside the cone are regions of space and time that it is possible for you to communicate your news to. Outside the cone are regions (of space and time) that can never hear the news because no signal can get there faster than the speed of light. A concrete example. It is about 5000 km between London and New York. If the stock market crashes at 17:00 (UT) in New York, it is impossible for traders in London to hear about it for at least 0.016 seconds. For that period, they lie outside the light cone of a signal produced at 17:00 in New York. More distant cities take even longer to intercept the light cone. The light cone idea also works in reverse. You can invert the cone to mark regions of space and time from which you could have heard some news. In the example above, the inverted light cone of the London traders does not contain New York until 0.016 seconds after 17:00. A point of confusion is the idea of a cone, which is really only appropriate if space is defined as two dimensional - e.g. points on the surface of the Earth. Conceptually it is harder to work with in 3D (though mathematically equivalent). In 1D (points along a line), the light cone approximates to a light triangle. 

Short answer: Without tunnelling, stars like the Sun would never reach nuclear fusion temperatures; stars less massive than around $5M_{\odot}$ would become "hydrogen white dwarfs" supported by electron degeneracy pressure. More massive objects would contract to around a tenth of a solar radius and commence nuclear fusion. They would be hotter than "normal" stars of a similar mass, but my best estimate is that they have similar luminosities. Thus it would not be possible to get a stable nuclear burning star with 1 solar luminosity. Stars of 1 solar luminosity could exist, but they would be on cooling tracks, much like brown dwarfs are in the real universe. A very interesting hypothetical question. What would happen to a star if you "turned off" tunnelling. I think the answer to this is that the pre-main-sequence stage would become significantly longer. The star would continue to contract, releasing gravitational potential energy in the form of radiation and by heating the core of the star. The virial theorem tells us that the central temperature is roughly proportional to $M/R$ (mass/radius). So for a fixed mass, as the star contracts, its core gets hotter. There are then (at least) two possibilities. The core becomes hot enough for protons to overcome the Coulomb barrier and begin nuclear fusion. For this to happen, the protons need to get within about a nuclear radius of each other, let's say $10^{-15}$ m. The potential energy is $e^2/(4\pi \epsilon_0 r) = 1.44$ MeV or $2.3\times 10^{-13}$ J. The protons in the core will have a mean kinetic energy of $3kT/2$, but some small fraction will have energies much higher than this according to a Maxwell-Boltzmann distribution. Let's say (and this is a weak point in my calculation that I may need to revisit when I have more time) that fusion will take place when protons with energies of $10 kT$ exceed the Coulomb potential energy barrier. There will be a small numerical uncertainty on this, but because the reaction rate would be highly temperature sensitive it will not be an order of magnitude out. This means that fusion would not begin until the core temperature reached about $1.5 \times 10^{9}$ K. In the Sun, fusion happens at around $1.5\times 10^7$ K, so the virial theorem result tells us that stars would need to contract by about a factor of 100 for this to happen. Because the gravity and density of such a star would be much higher than the Sun, hydrostatic equlibrium would demand a very high pressure gradient, but the temperature gradient would be limited by convection, so there would need to be an extremely centrally concentrated core with a fluffy envelope. Working through some simple proportionalities I think that the luminosity would be almost unchanged (see luminosity-mass relation but consider how luminosity depends on radius at a fixed mass), but that means the temperature would have to be hotter by a factor of the square root of the radius contraction factor. However, this could be academic, since we need to consider the second possibility. (2) As the star shrinks, the electrons become degenerate and contribute degeneracy pressure. This becomes important when the phase space occupied by each electron approaches $h^3$. There is a standard bit of bookwork, which I am not going to repeat here - you can find it something like "The Physics of Stars" by Phillips - which shows that degeneracy sets in when $$\frac{ 4\pi \mu_e}{3h^3}\left(\frac{6G R\mu m_e}{5}\right)^{3/2} m_u^{5/2} M^{1/2} = 1,$$ where $\mu_e$ is the number of mass units per electron, $\mu$ is the number of mass units per particle, $m_e$ is the electron mass and $m_u$ is an atomic mass unit. If I've done my sums right this means for a hydrogen gas (let's assume) with $\mu_e=1$ and $\mu = 0.5$ that degeneracy sets in when $$ \left(\frac{R}{R_{\odot}}\right) \simeq 0.18 \left(\frac{M}{M_{\odot}}\right)^{-1/3}$$ In other words, when the star shrinks to the size of $\sim$ Jupiter, its interior will be governed by electron degeneracy pressure, not by perfect gas pressure. The significance of this is that electron degeneracy pressure is only weakly dependent (or independent for a completely degenerate gas) on temperature. This means that the star can cool whilst only decreasing its radius very slightly. The central temperature would never reach the high temperatures required for nuclear burning and the "star" would become a hydrogen white dwarf with a final radius of a few hundredths of a solar radius (or a bit smaller for more massive stars). The second possibility must be the fate of something the mass of the Sun. However, there is a cross-over point in mass where the first possibility becomes viable. To see this, we note that the radius at which degeneracy sets in depends on $M^{-1/3}$, but the radius the star needs to shrink to in order to begin nuclear burning is proportional to $M$. The cross-over takes place somewhere in the range 5-10 $M_{\odot}$. So stars more massive than this could commence nuclear burning at radii of about a tenth of a solar radius, without their cores being degenerate. An interesting possibility is that at a few solar masses there should be a class of object that contracts sufficiently that nuclear ignition is reached when the core is substantially degenerate. This might lead to a runaway "hydrogen flash", depending on whether the temperature dependence of the reaction rate is extreme enough. Best question of the year so far. I do hope that someone has run some simulations to test these ideas. Edit: As a postscript it is of course anomalous to neglect a quantum effect like tunnelling, whilst at the same time relying on degeneracy pressure to support the star! If one were to neglect quantum effects entirely and allow a star like the Sun to collapse, then the end result would surely be a classical black hole. A further point that would need further consideration is to what extent radiation pressure would offer support in stars that were smaller, but much hotter.