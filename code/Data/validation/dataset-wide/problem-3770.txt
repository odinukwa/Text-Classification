The proof of the incompleteness theorem can already be done syntactically, ignoring truth, if we remove the conclusion that the Gödel sentence is true and leave only that it is neither provable nor disprovable. In particular, the "usual" proof of the incompleteness theorem is syntactic once we move to Rosser's version. For Gödel's version, there is an extra hypothesis of $\omega$-consistency, which is directly about truth in the metatheory: $\omega$-consistency corresponds to the reflection scheme $\operatorname{Pvbl}_T((\exists x)\psi) \to (\exists x)\psi$ where $\psi$ is quantifier-free. The explicit use of this assumption was elided in the question, but it become more obvious if we write the formalized provability predicate $\operatorname{Pvbl}_T$ instead of "is provable". If we start asking what axioms are used in the metatheory, we need to move to a formal metatheory. One good reference for this and everything in the question is Smorynski's article in the Handbook of Mathematical Logic. He covers in detail the question of what metatheory is sufficient. The short version is that for an effectively axiomatized theory $T$ that meets the hypotheses of the incompleteness theorems (with Rosser's trick), PRA will prove $\operatorname{Con}(T) \to \operatorname{Con}(T + \lnot \operatorname{Con}(T))$. There is no notion of "truth" in the language of PRA to begin with, and this proof is just syntactic. In general, axiom schemes in the metatheory containing sentences of the form $\operatorname{Pvbl}_T(\phi) \to \phi$ are called "reflection" schemes in the context of arithmetic. They have been studied in detail, and Smorynski spends several pages on them in his article. Another reference, which I have been meaning to read but haven't had the chance yet, is Axiomatic Theories of Truth by Halbach. I think Halbach's book should be very related to the topics in this question. 

This is a side comment. There are several answers that explain why compactness is so important in model theory, and I agree with what they say. But I want to point out that the "in model theory" part is key here. In the overall study of logic, not restricted to model theory, both compactness and completeness are important, and each of those has areas of logic that favor it. Model theory, being a semantic field, naturally identifies more with semantic notions. In mathematics outside logic, I think there is more implicit use of completeness than of compactness. Every time I prove that an identify is derivable from the the axioms of a group by working semantically and showing that the identity holds in every group, I am implicitly using the completeness theorem. It is easy to miss this or take it for granted, because the completeness theorem is so well known. There are systems that do not have complete deduction systems; one example is second-order logic with full second-order semantics. In this system it is perfectly possible for something to be true in every model without being provable in our usual proof system. Therefore, when we study this system in logic, we have to keep a close watch on whether we have shown something is provable, or just shown that it is logically valid. Imagine the difficulties in an alternate world where mathematicians have to distinguish between "true in all groups" and "provable from the axioms of a group". The completeness theorem is what lets us ignore this. By comparison, it's more difficult to see reflections of the compactness theorem in everyday mathematics. 

I'm not sure what counts as an "easy" proof, but the following seems easy to me. Make a subtree of $\mathbb{N}^{<\mathbb{N}}$ by declaring that a sequence $\sigma$ is in the subtree if (1) $\sigma$ is injective and (2) whenever $\sigma(2n+2)$ is defined, it is equal to the least natural number not in the range of $\sigma \upharpoonright (2n+2)$. Then any path through this tree is a bijection of $\mathbb{N}$, and the tree is perfect because any sequence of even length has infinitely many immediate extensions in the tree (any number not already in the range is a candidate). So the tree has continuum-many paths. 

For higher-order analogues, it is still a question whether restricted induction or full induction is included. Kohlenbach [1] has used notation such as $\mathsf{ACA}_0^\omega$ to refer to the analogue of $\mathsf{ACA}_0 $ formalized in arithmetic in all finite types. In this context, though, there are many different ways in which induction can be restricted. So notation like $\widehat{\mathsf{E\text{-}HA}}^\omega_\upharpoonright $ is used in the literature, where the hat and the harpoon refer to different sorts of restrictions. These notations are explained in Kohlenbach's Applied Proof Theory or in Troelstra's Metamathematical Investigations. 1: Ulrich Kohlenbach, "Higher Order Reverse Mathematics", Reverse Mathematics 2001, Lecture Notes in Logic, 2005, ftp://ftp.daimi.au.dk/BRICS/RS/00/49/BRICS-RS-00-49.pdf 

If you begin with $\psi \equiv (\exists x)R(x) \land (\exists x)(\lnot R(x)) \land (\forall y)S(y)$, the usual prenex form will have depth 3, while the original formula has depth 1. So you are asking how to find an equivalent formula to $\psi$ that only has one quantifier. Let's assume our language only has two unary relation symbols $R$ and $S$. Then the formula $\psi$ above cannot be equivalent to any one quantifier formula $\phi$. First assume $\phi$ is existential, and true in some model. Then we can make a new model $M$ by adding one more element for which $S$ does not hold. Then $\phi$ is still true in $M$ (it will be preserved under extensions) but $\psi$ is not true in $M$ because $(\forall y)S(y)$ is not true. Suppose $\phi$ is universal. Take any model with more than 2 elements in which $\psi$ holds, and call one of the elements $c$. Take the submodel $N$ containing just $c$. Then $\phi$ is still true in $N$, because it is preserved under taking substructures, but $\psi$ is not true because $\psi$ requires at least two elements in the domain. 

Using this, we can (re)define $\phi(t)$ to mean $\phi'[t/x]$ where $\phi'$ is as in the theorem and $[t/x]$ is the syntactical substitution of $t$ for $x$. Then $\phi(t)$ is well defined up to provable equivalence for any formula $\phi(x)$ and any term $t$. 

You might also be interested in Graham Priest's article "The Structure of the Paradoxes of Self-Reference", Mind 103 (1994) pp. 25-34. (Journal page ; JStor) and similar work by Priest. He has a general framework that he argues captures the various self-referential paradoxes. I believe he also discusses this in some of his other work and monographs. 

Two points. The first is that one theory that is sufficient for the task is PRA - primitive recursive arithmetic - or any theory that interprets it. The definition in the question works fine in that setting. There are weaker theories where you can code things, as well. The key point is that in PRA there is a definable unary function Set($n)$ which says $n$ codes a "finite" set, and a definable binary relation Element($n$,$k)$ which says that $n$ is an element of the set coded by $k$. These have the property that for every $l$ in any model of PRA there is a $k$ which codes the "finite" set of numbers $n < l$ that are prime. This sort of coding requires quite a bit of work to set up, but it is viewed as completely routine unless you try to really plumb the depths of the weakest possible theory. PRA is already an extremely weak theory. Second, there is some benefit to rephrasing things before formalizing them. Rather than proving "no finite set of primes includes all the primes", it is more natural in arithmetic to prove "the set of primes is not bounded". These are equivalent in the real world, but the latter entirely avoids the issue of coding finite sets. We still have to prove, by induction, that for every $k$ there is an $n$ that is a multiple of every prime less than $k$ (using the method from the question to define $k!$); that if $a | n$ then $a \not | n+1$; and that every number has a prime factor. None of these requires coding finite sets. 

As the other answer points out, the subscript 0 means restricted induction. However, without the subscript 0, there are two conventions: 

Bishop's mathematics is compatible with classical mathematics. For example, if we look at set theory in Bishop's framework, each model V of ZFC is a model of Bishop's system, and if we look at second-order arithmetic in Bishop's system then the standard model of second-order arithmetic is a model of Bishop's system. So it is not true that, in Bishop's system, every function must be continuous. On the other hand, Bishop's system is compatible with the nonclassical axiom that every function is continuous. So you cannot prove the existence of a discontinuous function. Bishop's system is, in this sense, neutral between constructive and classical mathematics. It is weak enough, among constructive systems, to admit classical models, but it is also weak enough, among classical systems, to admit nonclassical models. One nice (and short) reference on Bishop's system is Varieties of Constructive Mathematics by Bridges and Richman. The authors contrast Bishop's system with other constructive systems. 

In the field of computability theory, the terms "decidable set", "computable set", and "recursive set" are all formally defined and they all mean the same thing. So, to put it gently, Wells is misusing terminology. There is an enormous amount of evidence that the formally-defined class of computable functions on the natural numbers is exactly the same as the informally-defined class of effectively calculable functions: the ones for which there is an effective procedure that could, in principle, be carried out by a human with unlimited time, resources, and patience. That is, there is an enormous amount of evidence that the Church-Turing thesis is true in the form "a function is effectively calculable (in the informal sense) if and only if it is computable (in the formal sense)." Regarding Wells' invocation of Tarski, we'll never really know what Tarski had in mind, because Tarski died within a year of the conversation Wells describes. But Wells' argument that an entire field should redefine the formal term "decidable" based on an off-the-cuff discussion with Tarski is not compelling. There is a research area known as "hypercomputation" that studies various models of computation that go beyond things computable by Turing machines. The reason that this work is not viewed as evidence against the Church-Turing thesis (as stated above) is that these models don't possess the essential property of being able to be carried out by a patient human using unlimited paper, pencil, and time. That kind of effectiveness is the heart of Turing computability, and the reason that we use the word "computable" to refer to the Turing computable functions rather than some broader or narrower class of functions. 

Once we look at computational content, rather than constructive content, things are easier to answer. When we work on the level of individual reals, all the representations are equivalent - a real is computable in one sense if and only if it is computable in the others. However, some of the conversions are not uniform (i.e. there is not usually an algorithm that can convert arbitrary reals from one representation to another). This lack of uniformity also presents issues in a constructive context. A very thorough treatment of the uniformity issue was given by Jeffry L. Hirst, "Representations of reals in reverse mathematics", Bulletin of the Polish Academy of Sciences, Mathematics 55 (2007), 303–316 (PDF). Hirst summarized the known relationships and verified several relationships that had not been treated in the literature. Here is a table from the paper. It shows the subsystem of second-order arithmetic necessary to (provably) convert a sequence of reals of one form to a sequence of reals of a second form. The use of sequences is a stand-in for uniformity, and has close connections to constructive analysis. 

There is one more well-known equivalence for $\forall \exists$ sentences. Theorem (Chang-Los-Suszko). A theory $T$ is preserved under taking unions of increasing chains of structures if and only if $T$ is equivalent to a set of $\forall \exists$ sentences. For a proof, see Keisler, "Fundamentals of model theory", Handbook of Mathematical Logic, p. 63. I found a related paper, which is older and doesn't quite answer your question but may be of interest. R. C. Lyndon, "Properties preserved under algebraic constructions", Bull. Amer. Math. Soc. 65 n. 5 (1959), 287-299, Project Euclid According to that paper, and MathSciNet, a general solution to your question should be contained in H. J. Keisler, "Theory of models with generalized atomic formulas", J. Symbolic Logic v. 25 (1960) 1-26, MathSciNet, JStor 

This is all in Stephen Simpson's book Subsystems of second order arithmetic. The completeness theorem "every consistent countable first-order theory has a model" is equivalent to WKL0 over RCA0, so no weaker system containing RCA0 can prove that theorem. However, the special case of the completeness theorem in which the theory is already closed under logical consequence is provable in RCA0. The incompleteness theorems are stated in the language of first-order arithmetic. They are provable in first-order primitive recursive arithmetic (PRA) and therefore they are provable in any subsystem of second order arithmetic whose first-order part includes PRA. This includes RCA0, for example. 

One counterintuitive aspect of the axiom of choice is a theorem of Diaconescu and independently Goodman and Myhill that, in some constructive set theories that don't begin with the law of the excluded middle, the axiom of choice implies the law of the excluded middle. But in other systems such as Martin-Löf type theory, the corresponding form of the axiom of choice is completely constructive and does not imply the law of the excluded middle. 

A slightly older list is from Steve Simpson: Open problems in Reverse Mathematics (1999) I will mention one specific and tantalizing problem: the strength of Hindman's theorem. Upper and lower bounds for this theorem were first established by Blass, Hirst, and Simpson in a joint paper from 1987. Since then, there have been several papers on the ultrafilters that are relevant to the proof, but the overall bounds have been only slightly improved, for example by 

A moment's reflection shows that this can only work if ZFC proves $(\exists X)\phi(X)$ in the first place. So it appears that we want to prove: 

In the context of this question, the philosophical meaning of this theorem is that it is not possible to come up with a completely explicit form for every computable function, in some fixed signature, because then by enumerating all possible forms we would get a numbering $\phi$ as in the theorem. The closest we can hope for in terms of an explicit form for all total computable functions is something like Kleene normal form, but this still includes an unbounded search. Kleene's normal form theorem says that there is a primitive recursive function $U$ and a primitive recursive relation $T$ such that for every computable function $f$ there is an $e$ such that for all $i$, $$ f(i) \simeq U(\mu s . T(e,i,s)). $$ where $\mu$ is the unbounded search operator. 

They show that BISH + CPF does prove every function from $\mathbb{R}$ to $\mathbb{N}$ is constant, and so BISH + CPF is not compatible with classical models of analysis. Nowadays, we sometimes identify BISH with formal systems of constructive arithmetic in all finite types (e.g. variants of $\text{HA}^\omega$). These systems are also compatible with classical logic, in the sense that the standard model of arithmetic is also a model of these theories. So these formal representations of BISH also don't prove every function is continuous. 

There are several issues here. The easiest one to resolve is the "contradiction" with Gödel's incompleteness theorem. This theorem does not concern arbitrary formal systems in your sense, but only arbitrary (effective) theories in first-order logic. These theories may specify axioms, but they cannot specify additional rules of deduction. The deeper issue is that a rule of deduction is not "valid" on its own, it can only be valid with respect to some semantics. A semantics for a general formal system is a collection of "interpretations" of the system. The essential property is that each sentence of the formal system is assigned a truth value by each interpretation. The usual semantics for first-order logic uses so-called "structures", each of which consists of a domain of discourse and interpretations of the non-logical symbols of the formal theory. Gödel's completeness theorem shows that the usual deduction rules for first-order logic are sound and complete for this class of interpretations: anything that is provable is true in every structure (soundness) and everything that is true in every structure is provable (completeness). Your proposed additional deduction rule "from K deduce Con(T(K))" is not sound under usual first-order semantics. Essentially, any model Robinson arithmetic, Q, that does not satisfy Con(Q) is a counterexample to your deduction rule. (The reason for chooing Robinson arithmetic is that it is finitely axiomatizable.) However, it is trivial to obtain a semantics for which your deduction rule is sound: simply limit the allowed interpretations to those that satisfy your deduction rule! This is a common technique in other settings. For example, when Liebniz's rule of equality is treated as a deduction rule rather than an axiom, we must restrict the class of allowable interpretations to the ones that validate Liebniz's rule to ensure that our deduction rules are sound. Liebniz's rule says, informally, that if an interpretation makes $\Phi(a)$ true and makes $a=b$ true then the interpretation must also make $\Phi(b)$ true. The downside of limiting the class of allowable interpretations is that, although it guarantees soundness, it can cause completeness to fail. For example, if I limit the interpretations of Peano arithmetic to just the standard model, the usual deductive rules are still sound, but they are no longer complete, because many things are true in the standard model but unprovable. There are other classes of interpretations used outside first-order logic. One important example is the class of Kripke models used to study modal logic and intuitionistic logic. There are completeness theorems in these contexts that say that certain deductive systems are sound and complete with respect to these semantics.