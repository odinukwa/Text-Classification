This last bit is why ISNULL is usually used because it's more predictable (?) and COALESCE can add unintended data type conversions: which is where the "it's slower" bit comes from 

If you need all data from the child rows, then you will always have multiple issueIDs. Assuming you want to page per issueID, some options: 

The difference is that Enterprise edition without the hint may decide not to use the indexed view but the base tables instead. My personal experience is that SQL Server is somewhat braindead in this. I almost always have to use the hint: the query is quicker with far less IO even though the plan "looks" worse with a scan on the view not index seeks on the base tables. And it runs more consistently too YMMV of course :-) So, to answer, it will (should?) work the same based on what I've seen. Other folk may have different experiences and I'm interested in other answers To avoid using the hint everywhere you can wrap the indexed view in another view with the hint: hints propoagte inwards to all outer queries will automatically have NOEXPAND. 

We've done a lot of this, and (administrative) users were allowed to fix the translations live. (You still might want a caching layer, but I'm totally down with driving this with a real database and not resource files - it gives you a ton of power to query and find things which need to be translated, etc). I think your schema is probably fine, so I'll just pass on some stuff we learned in the hope that it's useful. One thing you have left out is phrases with insertion points. In the example below, the order is reversed and the language is still English, but this could very easily be two different languages - pretend this is just two languages who normally put things in a different order. 

"This means that the query engine must take an additional step in order to locate the actual data." Not necessarily - if the index is covering for a given query, no trip has to be made to the data pages. Also, with included columns, additional columns can be added to a non-clustered index to make it covering without altering the key size. So the ultimate answer is - It Depends (on a lot more information than you can really cover in a single question) - you need to understand all the capabilities of the indexes and the execution plan for a given query may diverge from your expectations. A general rule of thumb I have is that a table always has a clustered index (and usually on an identity or sequential GUID), but non-clustered indexes are added for performance. But there are always exceptions - heap tables have a place, wider clustered indexes have a place. Seemingly redundant indexes which are narrower to fit more rows per page have a place. etc. etc. And I wouldn't worry about the limits on the various indexes allowed - that's almost certainly not going to come into play in many real-world examples. 

We are creating SAAS where we will at most have 50.000 customers. We are considering creating a user in the Postgres database for each customer. We will map each user that logs into our service to a user in the database in order to be very sure that they only have access to their own data. We also want to implement an audit trail directly in the database by this solutions, which utilizes triggers. If each customer has its own database user, then it would be very easy to see who did what, even if two customers would share the same data. Will we be running into some unexpected problems because we have 50.000 users in our database? Performance-wise or administration-wise. Maybe connection pooling would be more difficult, but I do not really know whether we would need it. 

I am considering a model where I use PostgreSQL COPY to copy data from a file into a table. I was wondering what kind of performance to expect on high-end hardware. An interval in MB/s would be nice so I can start estimating. If you just know the answer on other databases, I would be interested to compare. 

You should post the query, given that it filters on some types, I would expect an index involving that column would help. I expect an NCI on the transaction type and date would indeed help a lot based on the query in your comment. And INCLUDE the PlayerID and PointDelta columns - that index would be covering - also if it's just one (or a few) types for basically just this query - make a filtered index: $URL$ 

The data is being compared from the local version of the table to one in the contracts linked server. I'm not certain what the goal of this code is in the first place, but I'm trying to get a handle on simplifying it a little. 

I would link at all/most levels. This denormalized star means that yes, the data is redundant, but it typically makes the reporting and analysis a lot easier. Note that this is very different from OLTP normalization, and you don't typically have to worry about redundant data getting out of sync because in a DW scenario data never changes. New facts get added and dimensions get expired and new ones created. I don't see a Dim_Folder. I would assume that the actual path of the folder would be an attribute of the Dim_Folder. Only the numeric quantity and any degenerate dimensions ($URL$ would be in the fact table. I wouldn't think of the folder path as a degenerate dimension because it keeps coming back in each snapshot (an a folder isn't a transaction). So you could do something like this: 

I just spent one hour debugging a query which had previously worked but suddenly created strange results. The cause of this bug was that I have a table in one schema which I reference in the query. Someone had, however, added a view in the current schema which has the same name as the table in the other schema. This caused the new view in the current schema to "take over" the reference in the query causing it to reference the view instead of the table. Obviously this caused me to start looking in the wrong place for this bug and getting completely confused. My question is therefore, how can I prevent this from happening? For example by enforcing that all views an tables in the database needs unique names. By enforcing this, I mean that it will cause and error if one attempts to create two objects with identical names. Because I am using some external tools, I have set up postgres to search for tables in multiple schemas (I have forgot how I did this), which is probably partly to blame for this mistake, but I cannot change this. 

Because it's a system, you will have a LOT of interacting subsystems, including power, environmental controls, communications, hardware, and then on top of that the data management and everything else. There are facets which cut across all these layers, like security, redundancy, business continuity, etc. Just from the database side, you would need to consider the transactional load, the architecture for balancing the load, the storage requirements, the response time requirements. Everything depends upon the requirements - of which there will be many from regulatory bodies, clients, users, business partners. As to whether cloud-based databases, I don't think any of those can be viable if your organization has to be responsible for the system. Any time you outsource anything, you limit your ability to be responsive to requirements. Some things you delegate are easier to mitigate than others. For instance, you decide to use a software library as a component in your system - you can always modify the library if you have the source, or rewrite it yourself or swap it out. Or picking a RDBMS like Oracle or SQL Server where you can get a vendor to do a proof of concept and have a high level of support because the sale is large. That's not to say by relaxing your requirements, you couldn't use Google Apps BigTable or Amazon or Azure or whatever, but the simple fact is that even with a contract, you have a limited amount of control over another organization's system and you are relying on them. All systems are built by balancing tradeoffs. 

If I share a Google Spreadsheet with someone, I will share a link like this: $URL$ This means that I am free to edit the name of the file (for example to "Book - absolute final version really7"), the Google Drive folder it is located in etc. etc. while the link will still work. Would it be possible to simulate something similar in a database? The problem I am trying to solve is the intense pain and risk I have to go through every time I need to rename a column, table, view or stored procedure in the database, or if I want to move it to a different schema. Because of this pain, I currently have all sorts of naming of all these things so you will see a primary key lookup looking like this in my database: 

Now I would like to utilize the test view or any other view in a GUI which simply presents an editable table. This is possible with the recently introduced automatically updatable views. I want the table to contain drop-downs containing all possible values every time there is a column with constrains on it, like the column with a foreign key in this case. How can this be achieved? I know that I could utilize enums and I do that today, but on Amazon RDS they are unreasonably hard to modify. I had an idea of (ab)using the "EXPLAIN SELECT" query for each column and check which table and column is being queried. 

Assuming that you are talking about tables containing the same kinds of entities, you typically want to have one table. You would not have any performance differences and a whole lot of management differences between the two approaches, with the single table being easier to manage. Typically large tables do not have performance overheads compared to smaller versions of the same when properly indexed, since the index seek time grows slowly compared to the row growth. In a normalized design, you will have different tables for different entities or relations, but as far as partitioning (either supported with a DBMS feature or manually with separate tables), that usually is necessary when you have certain requirements, like granularity of backup or data loading/unloading on a partition basis. 

perhaps you have to use dynamic SQL build up this query as a UNION with queries where reservations is prefixed by the different database names for each database allowed for a web site from a similar hotel/site linkage table. Like I said, the grouping structures are going to be key, since it sounds like your "tenants" are going to have a little less than simple relations. However, it's certainly possible to build this using appropriate structures once you've thought about the allowed relationships between the hotel and web site entities. It does get difficult to scale this out where "tenants" get their own servers if they have high load if you have a lot of overlap in your tenant structures, but then again, if your tenants are allowed to see quite a bit of stuff between each other, that points even more strongly to a single database design.