In SQL Server, I'm getting the following error "The query has been canceled because the estimated cost of this query (5822) exceeds the configured threshold of 300. Contact the system administrator." This is the result of an execution of a stored procedure, which is pretty complex. I haven't run into this for other stored procedures, only this one. Is it possible to change the query cost for this one procedure somehow? Can I do that in the stored procedure itself upon execution? Or do I have to define this on the server only? I'm using ADO.NET command to execute the stored procedure. Thanks. 

I have a table in a database with three columns: ID, Key and Value. Everytime I update the record, I have a trigger that sends these updates to another table and creates a new record in it, who's schema is this: 

I was able to query all of the empty partitions (1, 59-101) without error. In addition, I was able to query partitions 25, 26, and 49-58 without error. When querying partition 39, I received the third error message each time. When querying any other partition, I typically received the first error message, but sporadically received the second error message. After repeating my queries several times, the second error message went away, and I only received the first and third error messages. [The numbering of partitions corresponds to the order in which data was inserted into them, with 2 being the first data-containing partition, and 1 being an empty partition reserved at the beginning.] DBCC CHECKDB also failed, and it returned the following: 

I have a database with its primary file and log file stored locally. One table is split into 101 partitions which are stored on a Windows share on another server accessed by a UNC path. Each partition filegroup file is 1GB in size. Yesterday, I ran a bulk process to sequentially load data into partitions 2 through 58. The partition schema ensured that none of the files were filled to the 1GB capacity. The load did not report any errors. Today, when I attempted to retrieve data, I began to see three distinct errors: 

After you run the Mofcomp tool, restart the WMI service for the changes to take effect. The service name is Windows management Instrumentation. Then you might need to register two .dll's In your case: 

The servers might be running SQLServer Express and/or LocalDB SCCM has a SQL Server backend with a runtime license, Domain Controllers might have a localdb instance and so forth, check in the services app on the servers for those or run the following on the command prompt 

The database server is being backed up by a third party tool not the maintenance plans. The following query will show you the backup vendor information 

and then truncate the log as above to get out of trouble but remember which databases were in full recovery model. Now read the article linked above and start making good backups. 

I'm making the assumption that 00 values only appear at the end of a classification and that they can be ignored (i.e. they are placeholders for null, or something along those lines). An explanation of how the query works is in the comments of the code. This will work whether or not the graph entries in classification are in order by sno. 

In my first attempt to fix things, I rebooted the remote storage server. This did not change any of the observed behavior. In my second attempt to fix things, I restarted the SQL Server instance. This eliminated all of the error messages mentioned above. After doing this, I was able to complete a DBCC CHECKDB successfully, and I was able to retrieve data from each partition of the table. Two things that stood out to me while researching this: 

I have to say yes. By blocking dev/test you are making sure that application changes are not accidentally updating production data. In many cases sensitive data on dev/test is scrubbed after restore and by blocking you are making certain that no one with access to test can access sensitive data in production. If you are using SQL Server with MSDN licensens you have to segment the Development servers from the production servers anyways. ($URL$ 

Yes you can. You open the FTP site in IIS and change the SSL Settings from Require to Allow. You have to check with the owners of the FTP site if this is allowed and I would recommend to use https if possible as this way you are transmitting the username and password using cleartext 

then create a secondary index for the { VALUE | PATH | PROPERTY } you are querying with Xpath $URL$ and $URL$ 

You have to setup the permissions so that only one SQL Server account has access, and only you know the password to that account. That's the only way. Or, use impersonation, and only give that one person's windows account access. 

Anytime you change Oracle's sqlnet.ora or tnsnames.ora files, does the system require a reboot? In my instance, I only have the Oracle client installed on the machine I'm referring to, but out of curiosity what would it mean for an Oracle server installation? 

So anytime the first table record's changed, it creates a new record in the second with the table everytime. For every record with a matching ID, I want to add a unique incremental version number. If a record with ID of 1 is entered into this table 5 times, those records should have version numbers of 1, 2, 3, 4, and 5. Is the way to accomplish this through a subquery for existing version numbers, or setting up a partition, or something else? Thanks. 

While examining logs on the virtual host of the storage server, I noticed two occurrences where device latency increased significantly and then later dropped down to normal. I lost these logs during the reboot. [If this problem occurs again, I will definitely check for a recurrence of this and update this question with the specific warning message.] 

Assuming that the Ageing values match the names of the columns in your expected output, then this should work: 

The database is now online on the second server, and I can query data from filegroups PRIMARY and C. The database is aware of filegroups A and B, but they are marked as offline. Is there a way to restore the remaining two read only filegroups directly from the .NDF files? 

I have a database running under the simple recovery model. The database contains three narrow tables. For two of the tables, data is populated by insert statements. The third table is initially populated by an insert statement, and then the record is completed by an update statement. On average, the database sees 4000 inserts per minute and 2000 updates per minute. It's active, but not exceedingly so. The database has 10GB of space allocated to the transaction log. According to the Disk Usage report / DBCC SQLPERF(LOGSPACE), the log space usage is steadily increasing. Yesterday, usage was at 33% and growing. Overnight, it has increased to 48% and continues to rise. DBCC OPENTRAN returns no open transactions on the database. Nothing stands out to me when I run sp_whoisactive. Another database on the same server, with similar activity, a slightly larger transaction log size, and simple recovery model, is cycling between 0% and 10% usage, which is the behavior that I am used to seeing for the first database. What other steps can I take to determine the cause of this behavior and ensure that the transaction log will not fill up entirely? *Given that I cannot justify the growth in log usage, please assume that "allocating more space to the transaction log" is not a viable solution. 

Enabling filestream by it self will not have any impact on the databases not using filestream. Queries on filestream data will be done using system cache not buffer cache so you might need to make sure that the SQL server service is not using all availible resources, see $URL$ 

run MMC.exe select File - Add remove snapin and find the SQL Server configuration manager and add it to the console root. Save the MMC console for later use with the extension.msc. If the SQL Server Configuration manager does not show up in the MMC console you have to register the DLL's and recompile the .mof for it Open a command prompt with run as admin, type the following command, and then press ENTER: 

You need to make certain that the quorum is set up according to your needs. In most cases when you have an odd number of nodes node majority is the best way to go so make certain that your quorum is set up accordingly. In a 3 node cluster you can setup node majority as your quorum mode and then you can survive the failure of a single node within the cluster . That is in most cases sufficient. If your cluster spans multiple sites you might want to fiddle with the quorum-vote weight depending on your failover plans.