I think you are right to be impressed with the Curry-Howard correspondence. It is a detailed and extensive rule-by-rule and feature-by-feature isomorphism. This strongly suggests that proof and computability are closely related. I also agree that it is under-appreciated within the philosophy of logic and that we can and should allow it to inform our understanding of logic. Logicians are fond of arguing about logic. They will disagree even over basic things such as what account to give of the concept of validity. Ask Frege, Quine, Tarski, Davidson, Lewis, Prawitz, Etchemendy, McGee, Brandom and MacFarlane and you will get ten different answers. They will also argue about whether there is a single "one logic to rule them all" and if so, which one it is, or whether logical pluralism is defensible. According to Dummett, intuitionism is the only way to go; for Read it is relevance logic, for Priest paraconsistent logic, for Quine classical logic. On the computability side of the fence, there is relatively little argument about what account to give of computability. There are some issues about the precise way to state the Church-Turing thesis, whether and how it applies to interactive computers, and whether considerations such as the laws of nature should be allowed to determine what we may call a computation. So since we apparently understand computability pretty well, and logic rather less well, it seems to make sense to allow our grasp of the former to help us with the latter. It is important to notice that the Curry-Howard correspondence extends to classical logic. Curry and Howard themselves were not aware of this when they formulated the correspondence. They started from the BHK interpretation of intuitionism and used the fact that intuitionistic proofs are constructive to read these proofs as recipes for a computation. But subsequent work by computer scientists, including Griffin, Parigot, Aschieri and others showed that even classical logic shares the correspondence. What this means in practice is that there are computational interpretations of classical systems that are normalizable and allow computations to be extracted from classical proofs. This does not mean that any classical sentence is computable: clearly there are any number of undecidable sentences. The full extent of what is classically computable is still an area of research. But it does mean we can dispense with the simplistic idea that classical logic is non-constructive while intuitionistic logic is constructive. Classical disjunctions, for example, can be the objects of cut-free proofs, as Girard noted in his paper A New Constructive Logic: Classical Logic. To address your specific questions: 

The question is indeed a little vague. My journey to work involves me getting in my car and driving. Does this imply that I always drive my car to work? In fact, I always have driven to work, but it seems reasonable to say that I needn't: I could walk or take a bus. If one is talking about processes and actions in the real world, there are always going to be such possibilities, i.e. exceptions or defeating conditions that mean we can't say for certain that something is always so. One might wish to embrace some kind of essentialism, i.e. to say that Y is essential to X and therefore must always be present. Plato and Aristotle taught this, though their versions of essentialism have few adherents these days. More recently Kripke and Putnam have adopted a kind of essentialism in respect of the origins of things. Incidentally, if you wish to read more about this, the Wikipedia articles on these subjects are pretty weak. You would do better to read the Stanford Encyclopedia entry on Natural Kinds, and if you are very brave, follow it up with the entry on two-dimensional semantics. 

Generally speaking, the meaning of sentences and words is a matter of syntax, semantics and pragmatics. Syntax is concerned with formal rules for forming and manipulating sentences and includes the rules of grammar and the rules of deduction that feature in logic. Semantics is concerned with conventional meaning and interpretation. Pragmatics is concerned with the more messy principles and guidelines that govern what a speaker is trying to achieve when making an utterance in a given context, on a given occasion and before a given audience. Mathematicians like formal proofs that are independent of context, so mathematics is big on syntax and avoids pragmatics. Ordinary language on the other hand is big on pragmatics: it is a major feature of understanding what people mean. Because of this, mathematical usage is not very typical of meanings as they feature in ordinary language, so it is best not to put too much weight on definitions found in math books. Here are some examples for illustration. Your math books defines "P and Q" to be simply P is true and Q is true. But the meaning of the utterance, "eat that apple and you'll die" clearly expresses something much more: eating that apple will result in your death, perhaps either because it is poisonous, or because the apple is mine and I will kill you if you eat it. "Alice drove home and drank a beer" has a different meaning from "Alice drank a beer and drove home". In both cases the difference lies in the pragmatics of language. In the first example, "you'll die" would be irrelevant unless the speaker intends the audience to understand that it is the consequence of eating the apple. In the second example, it would be disorderly of the speaker to state what Alice did in the wrong sequence. Many of these pragmatic features can be explained by Paul Grice's theory of the co-operative principle and its use in implicatures. $URL$ In the case of conditionals the matter is even more complex because ordinary language conditionals are quite extraordinarily messy and it is difficult to draw the line between semantics and pragmatics. The conditional in your math text, i.e. that "if P then Q" is false when P is true and Q is false, and true otherwise, goes back to the stoic philosopher Philo and is usually known as material implication, but it is not the only conditional, nor is it typical of ordinary English usage, though it definitely is useful in mathematics. Usually in English we utter a conditional in order to express a causal or evidential relation, or a constraint or a precondition, or any one of a number of other things. So logical connectives are not arbitrary and often they have conventional meanings that abstract away from the meanings of the sentences they connect, but these meanings do not exhaust their use in ordinary English. Formal logic is an attempt to formulate the rules that describe the conventional component of linguistic meaning, but it only approximates meanings within natural languages. 

Goldman is trying to achieve an account of knowledge that does not fall prey to simple Gettier cases. In such cases, the subject has a justified belief in some proposition, and it is true, but only by luck - there is a disconnect between the facts that ground the truth of the proposition and the reason for believing it. Goldman tries to capture this disconnect by saying that in such cases there is no appropriate causal connection between the facts and the belief. In the barn example that you mention, the farmer can see what he takes to be a barn, and is normally perfectly capable of identifying a barn when he sees one, so he has a justified belief that there is a barn in the field. As it happens, what he can see is just a facade, but there is a real barn hidden from his sight in a hollow. The farmer's belief that there is a barn in the field is true, but it is not knowledge. Goldman says this is because his belief is caused by the facade, not by the real barn, i.e. light coming from the facade into his eyes is the cause of his belief, while the barn, which grounds the truth of the proposition, is having no causal influence on the farmer. As you note, Goldman abandoned this account, because it fails to work for more complex cases, such as where there are lots of barns visible to the farmer; he points at one and says, I know that is a barn; but unbeknown to him, nearly all the items he can see are merely facades, though by luck, the one he points to is a genuine one. Here, the causal account would say the farmer knows this is a barn, because his belief is caused by the light coming from the real barn. But it fails to be knowledge because we don't count belief as knowledge if there is a large element of luck involved. Capturing the meaning of 'luck' here is tricky, which is why there are plenty of different attempts to explain knowledge in terms of reliability, or counterfactual tracking, or the absence of defeating conditions, etc. 

I think you are correct in what you say about Frege. In his logic, you can only quantify over things that exist, and all and only those things that exist have names. Therefore one cannot correctly predicate anything about things that do not exist. Your idea about existentially neutral objects that may or may not exist sounds like the ontology of Meinong. He held that it was OK to speak of objects that subsist, because we can imagine them, and that existence is a first order property of such objects. 

One should bear in mind that Searle writes a great deal about issues such as the relation between mind and brain, and the limits of artificial intelligence, and what these limits might tell us about that relation. He is not so much concerned with what is computable in completely abstract terms as with whether we can think of the brain as a computer, and if so, whether this has consequences for our understanding of minds. The issue of interpreting the significance of the Church-Turing thesis in this context is considerably more subtle than most accounts allow. Firstly, Turing's use of 'computable' really answers to the concept of 'effectively computable'. The distinction may seem a fine one, but it potentially masks a misinterpretation of the Turing thesis. Copeland has a useful discussion of this point in section 2 of the SEP article on the Church-Turing thesis, and there are a number of references there to follow up. Secondly, it is appropriate to impose a physical restriction on what counts as a computer. If we do not do so, there are any number of ways of specifying a hypercomputer, i.e. a computer that transcends the capabilities of a Turing machine. An analog neural network, for example, is capable of being a hypercomputer, but it would need to operate with arbitrarily fine precision. In our quantum universe, no such computer exists. So the definition of a computer has to be about physics and not just mathematics. Again, there are a number of useful references in the SEP article and in the Wikipedia article about hypercomputation. Thirdly, it is problematic to explain how the Turing thesis applies to an interactive computer. A Turing machine is conventionally envisaged as a black box device, with an indefinitely long tape and a machine head that moves up and down reading and writing numbers onto the tape. Input to the machine takes the form of the initial state of the tape and its output is the final state. If an external agent writes onto the tape while a computation is in progress, then this violates the specification of a Turing machine. But this is in effect exactly what happens when a computer interacts with its surroundings. This has led some theorists, such as Peter Wegner and Dina Goldin, to claim that what they refer to as the strong Church-Turing thesis is false: it does not apply to interactive computers. (For example, see Goldin D., Wegner P. (2005) The Church-Turing Thesis: Breaking the Myth. In: Cooper S.B., Löwe B., Torenvliet L. (eds) New Computational Paradigms. CiE 2005. Lecture Notes in Computer Science, vol 3526. Springer, Berlin, Heidelberg.) Wegner and Goldin are not entirely 'mainstream' in their views, and for my part, I would be unwilling to say that the thesis is false, but we must be careful about interpreting its significance. At any given instant in time we could take all the interactive input that a computer has received, encode it in some way and include it on the input tape of some Turing machine. But an instant later, when the computer has received some more interactive input, we would have a different specification. The interactive computer is not a single Turing machine but a long sequence of Turing machines that evolves over time in accordance with the input it receives from whatever it interacts with. Without a complete specification of the portion of the universe that the computer is capable of interacting with, we cannot specify what future Turing machine it will evolve into. This suggests that the concept of computable functions has relatively little to tell us about the capabilities and limitations of artificial intelligences, or for that matter human beings.