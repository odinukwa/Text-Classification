This still has the advantage of using the automatic detection of for determining how many rounds to do, and still get reliable results, whilst still maintaining a simple structure to calling it. My custom method Even though it worked nicely, I didn't really like my original test method since I had to decide how many runs it had to do, and since it summed up all the times into a total running time (instead of a time per loop). In addition you had to make the test function names, i.e. into strings. So I changed it into the following: 

I still left a few comments in there, so that it possible to have some idea on what is happening. I've also changed the name to "repeatedSubstringCount" to indicate both what the function does, but also what it returns. 

Before diving into these points, I would like to say that both alexwlchan and SuperBiasedMan has given good pointers related to other code smells and stuff you need to look into as well. Exponential memory usage There is only line which really stands out and will require a load of memory when the length of the text increases: 

Alternative cleanup handlers You want to ensure the calling of , which seems to be recommended way of business for the Raspberry PI as it otherwise would leave the output pins in the current state, which could risk shortcuts later on if you reconnect a high pin to ground. See How to Exit GPIO programs cleanly, avoid warnings and protect your Pi. This however is actually a general question on how to ensure calling of cleanup procedure when I'm done doing something. This has at least the following solutions: 

Two focus areas for this review, first some style and code comments, and then a little discussion on a more optimal solution. Code and Style Comments 

Return at first fail The easiest way to handle this, is to make your own , which in addition to the normal parameters has a flag variable initiated within , where the local variant never executes the if the flag variable is set. This removes the possibility for changing the error code, and re-executing the after failing. Skip later executions An alternate version I've used in production code for a company is to have code which essentially behaves like the following: 

Translating my alternate implementation to yours with your definition of and I trust you'll manage. Note that within the method the coordinates needs to be in the first dimension, whilst the coordinates are in the second dimension. In other words, keep your head straight on when handling dimension indexes. Update: Changed to non-static methods, which purely was a result of laziness in my test setup based upon the method, and not proper object initialization. 

Don't split reading from same file into different processes I haven't used my self, but it seems like you're a splitting the file read over 30 different processes, where each reads 30 lines each? If that is correct, you should seriously consider a different split tactic, as that will throttle your IO seriously. You'll have 30 different processess trying to read from 30 different places in the file at the same time. A better tactic would be to send each file to a different process, and then let that process handle that file completely. Choose number of processes wisely Another caveat would be the number of processes you use. You create 30 processes, but as long as you don't have a 30 actual processors available you wont see any major performance gain using this number of processes. Back in the days using various Unix based operating systems, we did compilations in batches and the general rule was that we would aim for approx 4 times the number of processors we had available. In other words on a quad-processor, we would aim for 16 processes. Any more and we started seeing congestion due to interprocess issues and IO related performance bottlenecks. Shorten the distance from file to server Another speedup can be found if you are able to avoid network traffic. That is if you are able to run this script directly on the server, so that you can use addresses and local connections instead of using the IP network. Establish a baseline Not so much a performance suggestion, but do you have good baselines for how long it takes to do a typical run using only a single process? This can be helpful, when you start dividing the load according to other metrics, to compare when you reach a threshold regarding what each server/client should do. 

My main question when reading your code is: Why do you count the number of spaces? You don't need the number of spaces, only to know whether the previous character was whitespace and if the current character is whitespace. In addition there are some style issues in your code: 

There isn't much code, so that leaves little to be reviewed. I have not considered the mathematical side of this. This is also a code stub, so when reviewed out of context it is hard to do a good review. 

Alternate solution You're asking for a faster solution, so let us analyze what is happening in your code currently: 

And then I find just to find that this is called before the other, and does much of the same stuff... Strange... You should aim for one major parser, and let is select which tokens you find along the way, and then afterwards parse the token list into something sensible. To get a proper token list you need to separate your string on any split between elements, which might be as simple as spaces and the delimiter list. Whenever you find either a delimiter or something else, add a token to the list. Afterwards, you can parse the token list, and consume tokens depending on the context you are in. For example if you have a , you consume all tokens until the matching , similar for and other tokens. The main point is that after you've built a proper token list (not a string), it is rather easy to define your language as a sequence of given . A final point on your main program is that you read the entire file into memory, whilst it could be better to read and parse it line by line when translating into tokens. You'll surely not get that large test files, but it is a better way to handle files normally. 

Details as to why the exception occured, and this your code does good. You make it easy to provide good details to the exception. Where the exception occured, and this you seem to bypass. You only have a generic reference to which exception was thrown and possibly a somewhat unique combination of parameters. This will make it hard for you to locate your exception when your code base is getting larger