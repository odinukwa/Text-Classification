This will need expansion by a more knowledgable person, but as memory serves, it was proved by Mayer and Mumford that the closure in Ag of the locus of traditional Jacobians is the set of products of Jacobians. This is probably exposed first in a talk in the 1964 Woods Hole talks on James Milne's site. (I see Mumford credits it there, on page 4 of his talk, in part three of the Woods Hole notes, to Matsusaka and Hoyt. Apparently Mayer and Mumford computed the closure in the Satake compactification.) But let us try to explain this more in dim two. A two diml ppav is a compact 2 torus A containing a curve C carrying the homology class a1xb1 + a2xb2, where the aj,bj are a basic symplectic homology basis of H1(A). It follows from the topological Pontrjagin product that the induced map from the Albanese variety of C to A, has topological degree one, hence is an isomorphism. (I.e. the map from the Cartesian product of C with itself g times to A, has image whose class is the g fold Pontrjagin product of [C], which equals g! times the fundamental class of A. Hence the induced map from the g fold symmetric product of C, has image with exactly the fundamental class of A. Hence this map has degree one as does that induced from the Jacobian.) Since it also induces the identity map on C, it also preserves the polarization. Let me speculate on the special cases. If C is reducible it is known (Complex abelian varieties and theta functions, George Kempf, p. 89, Cor. 10.4) that A is a product of elliptic curves. If C is irreducible and singular then I guess the normalization map extends to a map of the Albanese of C to A. But that seems to imply the image of C in A does not span, a contradiction. So it seems that any irreducible curve C contained in a two diml ppav A and carrying the class of a principal polarization, is smooth and induces an isomorphism from the Albanese (i.e. Jacobian) of the curve to the ppav. I hope there is some useful information in this. 

I can offer some general common sense that might apply; it is derived from life experience and not from the specific setting of my mentoring undergraduates or being mentored as an undergraduate. I find that practice is one way to develop ability in a particular skill set. I note that many of the more respected answers on this forum are not just those that are clear examples of communication: they have specific references and show quality of research and scholarship. Precision and clarity are important, but providing the links to the existing and relevant literature so that others can follow, repeat, and confirm or correct the argument presented is a hallmark of decent research; high school is not too early to start practicing such skills, even for those not destined to a profession in the sciences, engineering, or education. Even documenting and keeping journals on small projects is good practice for those aiming to produce good research. Mentors should do what they can to encourage such practice. Gerhard "Aiming To Produce Good Research" Paseman, 2013.03.03 

I am going to argue that Dieudonne’ is actually using limits of Riemann sums to define his “Cauchy” integral. Dieudonne’ hides his use of Riemann sums within the proof of existence of primitives of regulated functions. (Of course existence of primitives is exactly where the rest of us also appeal to the Riemann integral of continuous functions.) In section 8.7.2, on page 160 of my first edition of Dieudonne’, he writes down a primitive of a step function, i.e., a Riemann sum for a step function on a sub interval [a,x] of [a,b], which is just a piecewise linear function. Then he appeals to a prior theorem that in case the step functions under consideration converge uniformly to another function f, then these primitives also converge to a primitive of f. Now if f is continuous, the Riemann sums for f (on intervals [a,x] with variable right end point) are precisely step functions converging uniformly to f. Hence for continuous functions f, he is proving that the Riemann sums, do converge to a primitive of f. Thus the only difference between his integral and Riemann’s is the class of functions to which they apply. A weakness of Dieudonne’s approach to my mind, is the lack of a means of finding the uniformly convergent step functions needed in his definition (for arbitrary regulated functions). In Riemann’s approach the approximating step functions are easy to choose (even for arbitrary Riemann integrable functions). For any regulated function f, there exist Riemann sum functions that converge uniformly to f, but Riemann observes this is unnecessary, since L^1 convergence suffices. As long as the set of discontinuities has measure zero (which criterion Riemann proved in an equivalent form), then any choice of the Riemann sum functions will converge to the integral. In Dieudonne’s approach, one must find step functions which converge uniformly, which he accomplishes by a non constructive compactness argument in 7.6.1. Of course once these approximating step functions are found, he approximates the integral by their Riemann sum, as in 8.7.2. So the basic idea that an integral is approximated by Riemann sums is common to both treatments. The difference is that Riemann’s approach applies to a wider class of functions, and provides an easier way to choose the approximating sums. Added later: My hat is off to Harry for this question, and all who have answered. I learned a lot from them, especially short ones like Allen Knutson’s. Building on them, I throw out a possible combined approach. (It helps if the student read Euler’s precalculus book with infinite series but never mind.) I address those beginning students who care about understanding what is done and why, more than seeing it the absolute “best” way on first encounter. I.e. I suggest the first job is to motivate the best way, not present it in full. The basic problem is to define a way of adding up, or averaging, values of a function with an uncountable number of values. All competing definitions seem to use limits of functions with a finite set of values, or “simple functions”. Thus the only question is which simple functions to use and what method of taking a limit. One could begin with the integral of a monotone continuous function, as did Newton, and exhibit the two standard ways of approximating it by integrals of simple functions, namely adding up over columns (Riemann), and adding up over rows (Lebesgue). Here they are almost the same, in that one representative value is chosen for all points in a subinterval of the domain. From a computational viewpoint, Riemann’s method is superior since the length of the domain subintervals is more easily calculated. Thus for elementary approximations, Riemann’s method is worth knowing. Introducing more complicated functions one observes how the domain “level sets” become more complicated in Lebesgue’s approach, making the definition of their size a significant task, while Riemann’s approximations remain easy to calculate. One could then define the Riemann integral and state its fundamental theorem, and define “negligible sets” and state the Riemann-Lebesgue criterion for Riemann integrability, or just mention the special cases of continuous and piecewise monotone functions, possibly proving the latter, or the former assuming uniform continuity. One could also state and/or prove the convergence theorem involving uniform limits, possibly informally. The characteristic function of the rationals, whose integral should exist for anyone knowing infinite series, but does not in Riemann’s sense, shows one limitation of this approach, whetting the appetite for Lebesgue. It also explains the failure of convergence for pointwise limits. One might admit that much of the work in the first course will use the technique of antiderivatives rather than integrals, accepting in spirit Dieudonne’s argument, but that Riemann sums are crucial for approximations. For the technique of evaluating integrals by antiderivatives, one can observe the usual proof of the FTC works on continuous functions for any definition of the integral which is monotone and additive over subdivisions of the domain. Point out it is hard to give a full fundamental theorem for Riemann integration, i.e. to intrinsically characterize indefinite integrals of all Riemann integrable functions. One could mention the advantages of Lebesgue’s definition. In this context one could discuss a more flexible definition of antiderivative as Dieudonne’ does, at least for step functions, and remark that one can characterize indefinite integrals of bounded functions with only a finite (or countable) set of discontinuities. Since many instructors in the US omit proofs entirely in beginning calculus, the question of whether to present complete proofs for Riemann integration applies more to a beginning analysis course, where I think it can prove useful practice for many students. To the argument that presenting the most advanced and abstract version first is more efficient, I would point out that having been taught that way, it took me another 40 years to understand what is going on, hence not very efficient for me. 

As Eric Tressler mentioned in the comments, you could use bucket sort which essentially means (by somehow using knowledge of f()) for each b, place b in the list at hash index f(b), then starting at the max index and looping down, use the list to update the values needed. While this does not directly compare values of f, it does assume some external ordering to the set of values which is accessible to the loop, and for me seems like an implicit comparison. Further, there are f for which you will need to loop through all the values before getting enough info to fill in for all the v_n. Even if f satisfies your condition that it is injective, you may still need to go through half the values, which for bucket sort gets prohibitive even for n above 25. If you don't know enough about f, you have to look at all of the values to find the largest one. Once you have the set of b for which f(b) is largest, say it has s-many such b, you have enough information to fill in about n + lg(s) instances of the max subexpressions for v_n, where I use lg to be log in base 2 rounded down to the nearest integer. While it may be possible to memoize and save on f(b) comparisons, a straightforward algorithm gives the result after at most (n+1)2^n comparisons, and minor modifications might get the (n+1) factor down to (1+lgn), but not without a lot of additional comparisons of values of b or parts of b. I recommend tracking the k largest useful values to start, for some small value of k < 2n, and iterating through the necessary b. You can repeat this as often as needed to resolve unknown values of v_n. Like Douglas Zare, I would appreciate more context. I would like to know if this is homework-related before sharing any more on this problem. Gerhard "Ask Me About System Design" Paseman, 2013.04.22 

This is really a long remark. The concept of a general hyperplane is used here to prove that the degree of a curve is well defined. I.e. that there exists a non empty Zariski open set of hyperplanes whose intersection with the given curve consists of d distinct points of multiplicity one. But if one wants to actually compute the degree, this definition is insufficient. I.e. one can calculate only with a specific hyperplane, but the answer is correct only if it is also “general”. The knowledge that most hyperplanes are general is of little help. This is why one usually proves also that a hyperplane all of whose intersections have multiplicity one, is indeed general. This gives a computable criterion for generality. E.g. when Mumford computes the number of lines on a smooth cubic surface in his little yellow book, he first shows that most cubic surfaces have the same finite number of lines, and then proves that smoothness is a sufficient condition for a surface to have this number of lines. Then he checks that the Fermat surface is smooth and computes on that one. Sometimes it is so difficult to find a specific example that is general, that one is forced to compute with a non general one. In his book on algebraic curves, Fulton gives a formula for the local multiplicity of the intersection of two curves at a point, and shows that, at least theoretically, one can compute the Bezout number even if the curves do not intersect everywhere with multiplicity one. B. Segre gives in his book on cubic surfaces a lovely example where he computes the number of lines on a general cubic surface by using as example the highly non general case of the union of 3 general planes! Here a small deformation, to a general cubic, is defined by another cubic which meets each of the three lines of intersections of a pair of the original planes in 3 points. He then computes that a line lying in one of the 3 planes is the specialization of a line on a general deformation surface, if and only if it meets both of the other two planes at one of the distinguished points. Thus there are 9 such lines in each of the three planes, for a total of 27 lines on a general cubic surface. 

If you consider ways of going backwards, i.e. finding a point set that realizes a given shadow, I expect you will find many ways of getting non-inherited symmetries. Fascinated as I am by constellations, I sometimes remember that Orion's belt is not three stars almost in a row, but three stars that look to me like they are in a row. Similarly, take any 2-D symmetric configuration of points, and throw some of them straight up into 3-D at different heights, you will get a configuration of points which has few if any symmetries. About the only thing that you can depend on is that each such collection of symmetries for a point set will be a group. I expect many other possible things to be realized. If you place certain restrictions on the original point set, you may get a theorem. The only thing I see is that the full symmetric group contains the group of the shadow which in turn contains the group of the original point set. Gerhard "Ask Me About System Design" Paseman, 2010.12.05 

Euler's Elements of Algebra, Newton's Principia, and Riemann's works, seem rather obvious suggestions. 

Given any two points on a projective variety, blow them up and re embed the blownup variety in P^N. Then by Bertini, any general linear section of the right codimension will meet the variety in an irreducible curve which also meets both exceptional divisors. Then blowing back down gives an irreducible curve connecting the original two points. Normalizing that curve gives a map from just one smooth connected curve that connects your two points. (I learned this trick from David Mumford.) – roy smith 11 hours ago 

Something for Toby Bartels as well as the poster, I've decided to post as an answer. A (Universal) algebra A is Hamiltonian if for every subalgebra B of A there is a congruence of A in which B is a congruence class. This is a little stronger notion than ideal-supporting. Similarly, the algebra A has the CEP (congruence extension property) if for every subalgebra B the restriction map from congruences of A to those of B is surjective, in other words every congruence th of B can be extended to a congruence ph of A so that b th c iff b ph c for all b and c in B. This is also a little stronger property than ideal-supporting. Looking up Hamiltonian and congruence on a web search leads to a 1991 paper of Ralph McKenzie (Algebra Universalis 28, Congruence Extension, Hamiltonian and Abelian properties in locally finite varieties) on the subject. It may not be the best starting place on a quest for ideal supporting varieties, but you may find it helpful. Gerhard "Ask Me About General Algebra" Paseman, 2013.02.13