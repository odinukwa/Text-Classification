I get OK results from from server to both instances on , and I can connect to a non-dba user from server to both instances. I recreated the Oracle password file using on server instance and that did not help. Instance names are and on server . Here are my password files on that server: 

In the above example, compressing the index at level 3 would save 28% space. So if it is a good amount, you would 

I must put out a warning. We have now moved away from merge replication, and I would have to suggest that the scheme above could be a major performance issue. The exception to that would require you to have a small publication with either small amounts of filtering and/or filters that are not multiple levels deep. For instance 10 - 100 articles might work fine. If you push the above scheme too much you could have performance / locking issues. Every time you insert a record into the top filtering table the merge replication trigger has to process all the child tables. It also adds records to MSMerge_contents and MSMerge_genhistory for all the child tables. The more child tables you have, and if they have a big amount of records in them then the more processing power will be required. We had a problem with sp_MSsetupbelongs being too slow and timing out. In the end we came to the conclusion we were pushing merge replication too much and that this technology would not work for us. This leads me to suggest that if the filtering scheme in merge replication out of the box is not flexible enough for your situation then either don't filter, or don't use merge replication. Test Test Test though of course, every situation is different. 

So in your case using or for backup and pg_restore or psql to restore the dump is the way to go. Hope that helps. 

you should then be able to restore the dump file with your current restore command in the cron job. Hope that helps. 

My first shot to solve this problem was to setup the PgBouncer 1.5 connection pool and configure it as a simple proxy with exactly one connection to the target database (session pooling mode). My reasoning was that PgBouncer would establish this connection at start-up and that I can then connect/disconnect to/from the proxy with psql while the connection to the database keeps open. 

I am running merge replication in SQL2012 using web sync. Has anybody done any experiments to compare the performance of web sync to connecting to replication using a straight TCP/IP connection? Just wondering if it might work to open up a port for a TCP/IP connection over the web and not to use web sync. If it could work is it faster? 

I set up merge replication web sync using a pull subscription. When I try to start the merge agent at the client side I get this error, 

Any application that uses GPUs for processing has to be custom-written to access the GPU's processing power. Where GPUs shine are in floating point arithmetic, something that is low on the list of database operations, so no, no database software is likely to harness GPUs for processing. 

An index-organized table (IOT) is just that; an index with no "real" table. All data besides the primary key are just "tacked on" to the primary key, so for example if you have an IOT with 6 columns, 2 of which make up the primary key, behind the scenes you just have an index with 6 columns, of which the first 2 columns make it unique. So, sorry, only way to fix it is to recreate the table; to improve the rebuild time temporarily disable logging. You cannot use an hint on inserts to an IOT table to force a direct path insert: 

I am using identity columns in my merge replication solution to give people an id they can tell us over the phone (i.e. to support staff). A rowguid column would be too hard for this. I have noticed that int identity columns are replicated no trouble, and each client subscription has a range of identity values it can use. i.e. select * from MSmerge_identity_range 

I'm just wondering what happens if you do some kind of bulk insert for instance which inserted 5000 new records into this table. What would happen to the identity column then? 

I am running PostgreSQL 8.4 database server and psql terminal front-end on Ubuntu 10.04 Lucid Lynx and would like to span a single transaction over several sequential psql sessions. When I connect to my database with psql a new connection is established and a server backend process for this connection is created. When I disconnect the connection is released and the backend process terminates. A (non-XA*) transaction is bound to the scope of a connection, so obviously there is no straight forward way to span a single transaction over several psql sessions. What I would like to achieve is that the following sequence of commands can be run within a single transaction and therefore return the same transaction timestamp on each call of : 

Application Express, together with either ORDS (Oracle REST Data Services) or a similar web service such as Glassfish, is basically a set of processes that run in the database, usually connecting via internal connections. Application Express basically serves HTML web pages to allow access the database and display reports, and so end users only need a web browser to use an Application Express application, with reports, ability to modify data, etc. This is a huge simplification, but for your purposes, all you need to know is that end-users just need a fairly current web browser that supports HTML5 to get all the fancy web pages for accessing/modifying the data in the database. APEX comes with a security framework, or you can also use your own corporate security. 

UserRegion Table, (think of this as a security table, which region is a particular user allowed to see) 

When I browse to the address $URL$ it works fine when I specify the same login credentials as I did for the subscription. If we have a think about some of the possible causes, proxy server, we're not using that. URL is fine (as far as I can tell). Login credentials are OK as far as I can tell too. The other thing I noticed is that I can see the https traffic in fiddler when I put the ?diag address in my browser, but when I start the merge agent I don't see the traffic in fiddler. UPDATE: I used wireshark to look at the traffic going between my subscription PC, and the IIS server. When I run the subscription agent I get this traffic, 

But there is a little problem with this approach, as soon as I begin a transaction on the proxy connection and disconnect... 

Of course this makes perfect sense for a connection pool. Its job is to provide (a) shared connection(s) for several clients but to isolate the transactions of these clients. But for my use case a shared transaction is exactly what I would need... So my question is now, is there a way to configure PgBouncer (or another connection pool) to not release the connection upon disconnection after BEGIN/START TRANSACTION or is there another way to achieve what I would like to do? All further questions to this post, comments and of course answers appreciated!