The big thing about NoSQL is the concept of "eventual consistency" or "optimistic replication". Assuming nothing in the database is really dependent on the order of inserts, modifications or deletes, it vastly improves performance. After all, performance is a feature. 

The reason your process isn't working is that it's waiting to release the inactive portion of the log using a transaction log backup. Switching to Simple Recovery Model bypasses this issue. If you really need to shrink down the transaction log, so that you can regrow it (to reduce the number of VLFs, for instance), this is the process: 1) Switch to Simple Recovery Model 2) Run a CHECKPOINT and DBCC DROPCLEANBUFFERS (just in case) 3) Shrink the log file 4) Switch back to Full Recovery Model 5) Grow the log in reasonable increments (I use 4000MB increments) 6) Set autogrowth to a value your I/O subsystem can handle 7) Run a full or differential backup to restart the log backup chain. 

The point is, there's a lot you can do before considering purchasing more RAM. Figure out what your queries are doing, and help them make better life choices. Brent Ozar has just open-sourced his course on how the database engine works, and I reckon you would benefit from that, as well as many other free online resources. 

That's 2 + 2 + 12 + 2 + 1. Yes, it works out to 7 bytes as you calculated, but it could easily be more if there are more columns in the table. Assuming then a length of 19 bytes per row, you can fit 426 rows in a page. That works out to 1,173,709 pages, or 9,615,024,128 bytes. If we have a b-tree structure with 500,000,000 rows, your 1,173,709 pages will be at the leaf level. You'll need roughly 2,500 pages for the intermediate level (I don't know the exact length of the intermediate index record off the top of my head, so I guessed 16 bytes), which is an additional 200MB, give or take, and you may have two intermediate levels between root and leaf level. With this in mind, based on my terrible estimates for the b-tree structure, that would take you to around the 10GB range you see for your data. (All credit to Dan Guzman and Michael Green for their comments, and of course Paul Randal for his blog post.) 

You have a basic misunderstanding of what a blank database file is. In our parlance, that just means that there's no user data in the file. It doesn't mean that the file is zero bytes in size. If you create a new empty database using Microsoft Access today, you'll find that it is a certain size, even though there's nothing in it. No tables, queries, etc. However, under the covers, the file itself contains information to make it understood by Microsoft Access. This is called the file format. No file formats for complex applications start off at zero bytes. You'll find exactly the same case with a Word or Excel document. What I suspect is that your downloaded file contains empty database structures. This is the same as a "blank" Excel file containing, say, ten worksheets that are formatted with headers, but no data. It will also be a suspicious size if you're unfamiliar with the file format. Don't delete what's in the file you downloaded. See if it works. I hope that helps your understanding better. 

(or whatever). Then your metadata operation is the update of the view, as opposed to the switching in of the partitioned table. 

I would strongly suggest modifying this script to only write to one path, and then use an automated method to copy this backup set to another location. 

You can turn on "Optimize for Adhoc Workloads" with a high confidence. The only potential effect this has, is when a plan is compiled the second time, it takes a few microseconds longer. This is because the first time a plan is cached, it only records the stub. After that, the full plan is already cached, and you'll see the benefits. 

Things that will cause a DROP and CREATE, aside from your example of changing a data type, usually involve changes to the Primary Key / clustering key. Moving columns will do it, as will changing collation on text-based columns. I'm not sure what you're trying to avoid, per se. An ALTER may seem more elegant in code, but it might also be doing a whole lot of work in the background too, running the risk of truncating or failing, so dropping and creating is considered better. I think their rule of thumb is that in-place data conversions are too risky, especially with tables that already contain data, so it makes more sense to make a new one. 

(Full disclosure: I created the Max Server Memory Matrix and associated script that you are referring to in your question.) Per this excellent blog post, which contains a detailed explanation of the issue you're experiencing, you'll find this little quote: 

We refer to these as heaps in SQL Server. I understand that Non-Clustered Indexes complicate things, but that's their name. For what it's worth, on many RDBMS platforms, with auto statistics enabled, there's very little chance a table will not have statistics on it. I'd be comfortable calling these data structures unindexed heaps. 

Even if it is, go with fewer cores. On a virtual environment, this is preferred, especially with Standard Edition. Final note: if you do share the host with other VMs, please be wary of something called CPU lag. This can happen when VMs with different numbers of vCPUs are running on the same host. 

There are two options for this kind of scenario: 1) Shrink the data file, and then rebuild the indexes. As you say, this is time consuming and you're largely at the whim of the storage subsystem. 2) Create a new filegroup, and migrate all the data into the new filegroup, one table at a time. Shrink the old filegroup when you're done, and either move everything back again, or leave it as is. This second option adds some complexity, and additional maintenance for the extra FG, but is probably what you're after, since you can move one table at a time. You would do this by altering each table's clustered index to rebuild in the new filegroup. Adding a Filegroup: (See $URL$ 

Tough call. Look at it from a transactional RDBMS versus reporting-style OLAP perspective, at a high level. The IQ methodology is column-store, versus a traditional row-store for ASE. Column-stores are very fast when running aggregated queries over them, but that depends on how your reports are run, right? I think you almost nailed it with this: