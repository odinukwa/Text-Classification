The Connection timeout is always set by client. The timeout is normally configured when the connection is created. In order to prevent recompilation of client code the timeout interval is often stored in a config file, registry entry etc You cannot configure the timeout on the database itself. 

Take a user created Stored Procedure, MySP and as part of the SQL Script to create the SP add a SQL Statement to Grant execute permissions on the SP. 

If you do create a table using this syntax then you should be aware that Indexes, Constraints and Triggers are not added to the new table. 

SQL syntax. This syntax creates a new table and insert the resulting rows from the query into the new table. 

I have recently seen this exact same problem with an upgrade from SQL Server 2008 R2 to SQL Server 2014 where a very small number of Stored Procedures (SPs) (approx 5 out of 3,000 ) performed worse in SQL 2014 Compatibility Level 120 than in SQL 2014 Compatibility Level 110 or in SQL 2008. In SQL 2014 the Cardinality Estimator (CE) has changed which may impact how a SP performs in SQL 2014 Compatibility Level 120. There are a few possible solutions 

to the offending SP to force just that SP to use the prior CE. The SP should now perform in Compatibility Level 120 as it did with Compatibility Level 110. <-- I prefer this approach as your Database/System gains all of the other advantages of SQL 2014 while the "fix" only impacts the offending SP. Irrespective of the approach I would then profile the execution plan/read/writes etc of the offending SP on a test system in SQL Server 2014 Compatibility Level 120 and rewrite the SP to cater for the new CE. Once tested and verified I would then apply to production. Microsoft have a good article on the Cardinality Estimator changes in SQL 2014 

The Blocked process report (which you have) can be a little confusing at first. There are a few important sections Blocking-Process This is the process(SP, query etc) that is causing the blocking. The process is using a resource that is required by the Blocked-Process Blocked-Process This is the process(SP, query etc) that is blocked and is waiting on a a resource that is in use by the Blocking-Process. waitresource This details the resource that the process is waiting on. Taking your example of waitresource="OBJECT: 99:774293818:0 " The 99 refers to the Database ID of your data and the 774293818 refers to the table. Using the ID 774293818 and systables you should be able to find the table that is causing the blocking. I would also set up an Extended Events Trace to capture the parameters of the offending blocking process and review the execution plans. A good starting point is $URL$ 

I don't know of a specific product called "Amstrong Database" but I have heard of Armstrong's Axioms which are a set of axioms relating to the functional dependencies of relational databases. The axioms have been around a long time (mid 1970's). There is an IBM Research Paper available. I would think a Computer Science or Maths Department in your local University would be able to help. 

The use of Save Transaction provides you with a mechanism for rolling back portions of a transaction. For example SP A starts a transaction which then calls SP B. At the start of SP B, a Save Transaction Start Processing could be created. If an error then occurs in SP B you could just rollback the change in the SP B allowing the changes in SP A to be committed. 

Allowing even trusted users to login to what appears to be a production database is in my mind a big no-no. The solutions you mention above may provide you with a sense of security/relief but they their own have side effects. I would be particularly worried about your statement 

I am designing a calendar database that has a lot of booking and billing information. The database itself consists of form records, which are editable, and only when an event is confirmed by staff it appears on the web calendar. The web calendar does not have a separate table, it merely queries the database for recent confirmed events. Originally the submission form was used by both staff and public and went into one giant table. I've split the booking form data into the following tables in order to normalize it: 

(Not sure if worth it to handle edge cases, but I assume joins are not required to simply rebuild each form, if the following tables are strictly dependent. I.e. if rates were separated out, I would already know the form_id for a given or and simply SELECT for it, correct?) : describe calendar info pertinent to each event date. About 80% of events only have one date. Per the above: Should the "default" (i.e. primary) event_id for each series be placed in ? Should it be a boolean (is_primary, is_default) in ? Or should it be a field in that is UNIQUE foreign key ( = form_id), default NULL? (to mark it as the default, or primary record) I want to structure it so that the php can't accidentally cause data integrity issues by setting non-unique default child record ids (primary date, default billing) for each form. : speaker info unique to a given date or form (as submitted). The idea here is that old info from the date-of would be preserved even if the canonical info for a given speaker in the contacts db is updated. the form could AJAX query the contacts db to allow read-only hinting if a speaker appears multiple times, in which case the info listed in the form would be overwritten with the then-existing contact info for the speaker and a reference to the unique key. event_speakers would be many-to-one relational to each event date. If a speaker is overall host, then they would be listed as default for the entire form, but I am not sure where best to put this, as a flag or foreign key (same question as above.) Like primary event date, primary host info would be unique to each form, one-to-one, but would be in the same format / table as all other event_dates or speakers respectively. Perhaps I should separate them into a cloned table for primaries only? The problem with that is, the client may wish to allow a different speaker to be made primary, or allow a different event in a series to be the primary date without having to INSERT/DELETE anything. OK, so here's my main problem: : list all actual dates in which the event occurs, queried by the web calendar. I currently have as listing custom event info only, i.e. dates with something different going on than the primary event_date -- different lineup, different description, or different hours. (Different lineup would simply be a reference to different event_date_id in ). However, this leaves me with that simply repeat at intervals (some ad infinitum). I can put the repeat-interval (or date-mask, e.g. mTW) in , but this only helps to set dates, not retrieve them. And I realized I needed to track dates that are not in the calendar, such as "every second wednesday EXCEPT July 1st. ALSO INCLUDING July 20th." I figure the best way of handling this is to use the info in form_control to set or generate dates but then write a record in ongoing_dates upon-submit for every custom or blackout date, (otherwise scheduled dates for which it is reserved, or dark), with default = reserved. (reserved would be treated as a suggestion if the event is not yet confirmed, i.e. pending). That way, only primary, default, or custom dates (dates for which there is any data other than "it's happening again") would have their own record created. The rest would go in ongoing_dates.its_happening to be simply queried by the web calendar. Possible values for its_happening would be e.g. "it's happening again", "it's not happening this particular interval", or... "it may be happening", which can be represented as a simple TRUE-FALSE-NULL. ongoing_dates would also have a column for: event_start and end_time (using form_control start and end time as default, allowing custom times without creating a record) and, importantly, an optional event_dates foreign key. (i.e. every event_dates record would have a 1-1 relationship with ongoing_dates, but 99% of ongoing dates would not have a separate event_dates record, unless one is created by simply adding the ID.) This way the calendar can add and delete ongoing_dates at will without losing any information (e.g. if the exact same event is made once a month instead of twice a month) My question is, does this approach make sense from a normalization standpoint? Should I separate time and visibility info entirely into ongoing_dates, so that each event_dates record refers to a date that is only listed in ongoing_dates? Or must every date in the calendar have its own full record (with mostly-default information) and the calendar queries only that? Or does it not make sense to have a separate (small) record for potentially indefinite repeating dates? Those seem to to be my three options. I don't want the database to fill up with extraneous info simply because one event happens every day of the week. 

If you have users who are updating/insert records then I am sure they are also deleting records most likely with no or a very limited audit trail. If this was my database I would 

In order to get the desired result you need to join your table to itself and then ignore those rows where c3 is null. The SQL below produces the required result. I'm not a PostgreSQL person so there may be a better way to do this. 

Once you have encrypted the data using an AES key the data is encrypted (the point of using AES). The only way to change the key would be to decrypt the data with the old AES key and then re-encrypt the data using a new ASE key. Some good MySQL AES documentation. 

Using the above example a new table, #tmpTable will be created with the data from coulmnA, columnB and columnC from tableA. The data types in #tmpTable will match the datatypes of tableA. You can of course create a true physical table and have multiple tables in your select statement. 

The 'AUTO_UPDATE_STATISTICS' Option has two possible values, False(off) and True(on). When set to False, 'AUTO_UPDATE_STATISTICS' will not update any statistics. When set to True, 'AUTO_UPDATE_STATISTICS' will update statistics. The method chosen to update the statistics though depends on the value of the 'AUTO_UPDATE_STATISTICS_ASYNC' option. This option has two possible values, False(off) and True(on). By default, 'AUTO_UPDATE_STATISTICS_ASYNC' option is False(off) and the statistics update synchronously. When 'AUTO_UPDATE_STATISTICS_ASYNC' is True(on) the statistics update asynchronous. 

From reading the YQL Documentation and viewing the data sources it appears that while the yahoo.finance.quote does provide data on all the stock data, running a query on a large data set make take some time and the data may be returned in multiple pages which means that your application will have to handle the pages. It also appears that the YQL data requires a where clause in order to run so you would need to know the symbols for all of the stocks. There is a YQL Console which allows you to access the YQL data and run queries, view the output etc. I would suggest reading the YQL documentation and running some sample queries in the YQL Console to see if the data you require is available. To access the YQL Console you can 

Create a SQL Server Account and use those credentials to connect to the SQL Server <-- this might be the easiest Trust the domain in which your GIS Programme is running <--I would be loath to do this 

When the SQL is run the SP is created and stored in SQL Server. Once the SP is created the Grant execute statement is executed. When the SP is executed only the contents of the SP are run. The Grant execute statement is NOT stored with the SP in SQL Sever. This can be verified in a number of ways 

Immediately prevent all users from accessing the database tables and running any SQL statements such as Select, Insert, Delete, Update etc Sit down with the end users who are requesting data and determine exactly what data is required. Based on the end user requirements create a set of SPs that produce the required data. A DB account should be created that would only have execute permissions on the SPs If possible determine if the reports really have to be run in real time on a production database. If not I would suggest a snapshot be created on a regular basis on which the reports are run. I would also ensure that the sa account has a strong password and is not used on a day to day basis and that the password is know to only 1-2 people. Monitor the database over a period of time to ensure users have not gained additional privileges 

By opening SQL Enterprise Manager, locating the database in which the SP resides and viewing the SP under Programmability. Use sp_helptext 'MySP' which will return SQL Servers stored version of the SP