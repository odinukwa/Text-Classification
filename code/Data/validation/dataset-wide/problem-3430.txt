So there you have it, "a broad answer to a broad question." All that said, I really do like both Bayesian and frequentist approaches. A lot of things goes into deciding which to use. Some problems lend themselves well to one vs the other. Some audiences lend themselves to understanding one vs the other (recall that often effective communication of a concept can be very important). It's up to you to learn enough about both the methods and the problems you are trying to address to choose well. 

If you want to chew on the "thinking is costly so I won't do a lot of it until I need to" approach, Ariel Rubinstein's "Modeling Bounded Rationality" takes an interesting approach. He attacks the question from the perspective of how to actually model costly calculation, from some interesting perspectives (including computational theory from computer science). It certainly isn't the only reference on this topic, but an interesting read and a good place to start. I will warn you that this is not a field that has been widely explored (that I am aware of). You should also look into the experimental literature cited in the first paper I linked to above. Many interesting topics discussed there. 

To extend @Majoko's comment, you may be very interested in the book Poor Economics which discusses many of the issues you note. It specifically discusses issues with theory, and of course has a lot of empirical work to back it up. Perhaps on the other end of the spectrum is general equilibrium theory applied to poverty and the developing world. you should really look into Robert Townsend's work. His "Thai Project" is a massive dive into some of the questions you're asking. A reasonable place to start is probably with his books. As an aside, I was originally hooked on his work by his fantastic application of microeconomic general equilibrium theory in The Medieval Village Economy, which has sadly gone out of print (although I see there are now some used copies finally selling for less than $70, which is nice). He takes very sparse data and asks what theory might be able to tell us -- then very carefully and purposefully walks the reader from a most basic model through many extensions to explore important possible variations of the medieval experience. It's really an excellent example of applied general equilibrium work, and how theory might still be tested/explored with little data. This isn't my area of research, but I'm very interested to see other answers appear here. 

The paper itself does a great job of overviewing many options for "non-standard" preferences and utility. They introduce a preference, outline key features, and then apply them in a number of classic settings to give you a feel for what is going on. If you are interested in using non-traditional preferences, you should certainly read this. I can't give extensive insight further than that -- I currently do nearly all my work with more traditional CRRA preferences. 

This is only a partial answer, but I wanted to note and describe the theoretical literature which will come to mind for many research economists when observing (1), "eliminating the FDIC." The simple framework which is often first used to theoretically discuss this kind of question is the so-called Diamond–Dybvig model, which tries to set up a very simple theoretical world in which maturity transformation occurs in a straightforward way. (Maturity transformation is defined nicely in this interview, which I shamelessly lifted from the first reference on the current version of the [wikipedia page] :) Other quick references are a bit dry unfortunately.) There's something of an instability in maturity transformation which can lead to to-called bank runs; Diamond–Dybvig ("DD") try to capture this dynamic in a succinct model. I'll be careful to note that this model itself is very abstract -- the true financial system is very complicated. Regardless, it has been used as a framework for exploring theoretical ideas related to banking stability for a while. I'll start with the intuition for the model first. In the model, banks function by converting short-term accounts (savings accounts, perhaps money market accounts) into long-term loans. Banks take depositor money, retain a certain amount, and lend out a certain amount as long-term house loans, car loans, or small business loans. This means everyone gets something at the end of the day -- depositors get a safe place to keep their money with some modest return on savings; homeowners and business owners can smooth out the large costs of housing and capital over many years, and everyone gets a little bit of the slice of "added production" which occurs because the loans could be made. There's some investment projects which will fail (houses defaulted, businesses going bankrupt), but that can be planned for appropriately by the bank. Not all depositors need their money at the same time -- after all, that's why you put it in a safe spot to begin with. The bank knows (or guesses) how many people on average will need to access their funds, and holds on to that much (plus extra for a "safety buffer"). Everything else is lent out to achieve the above-mentioned benefits. This system works fine unless, for some reason, enough people start to believe that enough other people will want to withdraw their funds more frequently than usual -- perhaps all at once. Everyone generally understands the structure of the game: stage 1, if there are too many people who want to withdraw money, the bank will fail. Stage 2, if everyone realizes this, they will want to get their money first. Stage 3, I personally need to get my money out before anyone else does, and I know everyone else knows this, so now it's a race. Now this only happens if everyone believes that everyone else believes the bank will not have enough money for everyone. For a bank "standing alone" this is always a danger. However it also demonstrates a way to nip everything in the bud: institute an insurance scheme for individual depositors. You need to be able to credibly tell the borrowers that, "don't worry, even if the bank collapses, your deposits are insured and you'll get at least X back." This promise itself (and it must be a "good" promise; a promise no one trusts will be useless) will prevent many possible instances of bank runs which might occur when there is nothing intrinsically wrong with the bank. The theoretical illustration of the above points is the well-known Diamond–Dybvig model [wiki page here, which is actually quite good]. The model is really quite beautiful in laying out the constraints faced by everyone in the world -- depositors, banks, loan-takers. It does so in a very clean and clear way, which is one reason it is so popular. If you're ever deeply interested in these things, I'd definitely recommend working through the model at some point, ideally with someone you can "ping" for input (perhaps here, later). It's a great example of a framework one can use to tackle bigger questions (or alternatively, a great example of how to frame an observed phenomena in the simplest way possible, which is always how you want to start). This is one of those nice theoretical results which is so intuitive and easy to understand that it can be understood by anyone. In fact, it is literally the main plot device for the classic "It's a Wonderful Life," where [SPOILER!] the first bank run in the movie is quelled with the "insuring" of depositors' accounts with the couple's honeymoon fund. (The real-world equivalent of the honeymoon fund, of course, being the FDIC in the US.) NOW, all that said, I have no idea what the context of point (1), "eliminate the FDIC," in The Bankers' New Clothes. I suspect the authors are aware of everything outlined above, and they may perhaps have an alternative suggestion for how to handle the "bank runs" problem (aka the "Wonderful Life" problem) which emerges naturally without some deposit insurance system. If they are advocating a "clean sweep" of depositor insurance, that would be very extreme -- but I don't know if that is what they are actually advocating. There is an enormous empirical literature on bank runs -- see @Lumi's response, particularly the list of bank runs. I'm certain that if you were to choose a bank run from that list at random and search for it in Google Scholar, you would find plenty of empirical papers. (I'm not an expert on this literature so unfortunately cannot answer this more specifically.) 

This is a more "practical" answer, vs a deeper theoretical one, or even a specific one. Take it as "a broad answer to a broad question." Also, since it is a "Bayes vs frequentist" issue, at least the last few suggestions must be taken tongue-in-cheek. 

The Handbooks series has three very nice volumes that cover a broad range of "what computational economics is;" I'd highly recommend checking them out. At the time of this writing, I'm not aware of a nice central article in Palgrave for computational economics (which is a shame). However if you search the dictionary for articles related to computational economics, you'll get a lot of results. The first few pages of results are have a reasonable set of topics. I'd definitely recommend checking the Handbooks of Economics first. Peruse the tables of contents of each to get a reasonable overview of what people publish in these fields. You might also check out some of the journals of the Society for Computational Economics: Journal of Economics Dynamics and Control, Computational Economics. Flip through the table of contents for the journals. 

I strongly suspect that an emerging important area for applications of measure theory will be in approximate dynamic programming techniques. Approximate dynamic programming (aka "reinforcement learning" in the computer science literature) has been the direction of research work in the last ~10-20 years of the dynamic programming literature. Economics is only just now starting to adopt some of these advances. For example of the direction of the DP literature, see Bertsekas' most recent 4th edition expansion of his dynamic programming series, or Powell's Approximate DP: Solving the Curse of Dimensionality. Economists are just starting to pick up some of these tools, both directly and indirectly, and I suspect that they will have a growing impact on the literature over the next few years. Some of the analytical background for convergence of these methods is topology and dynamical systems. A good example of theoretical contribution to this type of literature from economists is Pál and Stachurski (2013), Fitted Value Function Iteration With Probability One Contractions (ungated version here). Peruse that paper and you can see the importance of a good grasp of measure theory. Stachurski's book Economic Dynamics is actually a very nice exposition of dynamic programming from this perspective, building at a pace which works for multiple levels of graduate student/professional (measure theory comes in formally at the end I believe -- I'm still working towards those insights). Hopefully this answers your question to some degree. I'm afraid that the phrase "post-1960s mathematics" is somewhat ambiguous to me (due to my own lack of knowledge of history of maths literature), so if I've completely missed the mark, my apologies!