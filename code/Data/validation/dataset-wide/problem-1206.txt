I have the following task: I want to execute fixed SELECT statement against SQL Server table with some fixed condition and I need to do it after specified period of time multiple times... Let's say N times with 1 minute interval between each execution with final goal to display only records which were returned all N times... Final goal is to check if some specific records persist in select results over time or maybe even order them by time they were returned - i.e. one record was returned in 3 first SELECT executions and interval was set to 1 minute hence I can say it was persistent for 3 minutes, another record was returned 5 times - so I can conclude it persisted / met my condition for 5 minutes. Nature of my condition/data dictates that records not meet this condition for more than one minute and I want to catch abnormal records which meet my condition for prolonged period of time. From explanation above it seems that it all involves some cycle, temp table and condition to put results in yet another temp table. But I'm posting it here to check if I maybe missing some simpler approach/solution which will allow to achieve the same result? 

SSIS is not really my forte. I noticed you have a table lock option on OLE DB destinations where the entire table becomes locked in what I assume is during the transaction of inserting data. What happens if you have a flat file data source with a conditional split that is parsing out data into 5 or more OLE DB destinations that are targeting the same table with table lock on? Would each OLE DB destination get blocked by one another in this scenario if data is being fast loaded (inserted) into the destinations themselves? I removed the table lock in my instance and everything seemed fine. It was splitting the data and inserting records at about 1 million records per minute. 

My understanding of this node: As explained there, the reads table blocks in sequential order, so it doesn't produce random-table-access overhead which happens as doing just . After has been done, PostgreSQL doesn't know how to fetch the rows optimally, to avoid unneccessary (or if there is a hot cache). So to figure it out it generates the structure () called which in my case is being generated by generating two bitmaps of the indexes and performing . Since the bitmap has been generated it now can read the table optimally in a sequential order, avoiding unnecessary . That's the place where a lot of questions come. QUESTION: We have just a bitmap. How does PostgreSQL knows by just a bitmap anything about rows' physical order? Or generates the bitmap so that any element of it can be mapped to the pointer to a page easily? If so, that explains everything, but it's just my guessing. So, can we say simply that the is like a sequential scan but only of the appropriate part of the table? 

I have a two part question really for anyone working with data warehousing where they have large fact tables. Question 1 Lets say you have a table that has 500 million or more records in it that is clustered on time. You are only posting incremental records for the last 24 hours on a daily basis to this table. How do you handle inserting delayed records that are a month or older to that table? Would you do nothing unless the insert caused a lot of fragmentation or would you attempt to drop the indexes, insert and rebuild? I do not have enterprise edition available to me for table partitioning. Question 2 If you have a fact table that is growing large like the above example, would it be wise to split the fact table up into multiple tables or would it be better to look towards adding additional files to the filegroup of said table if table partitioning is not an option? Thanks in advance. I'm dealing with some large growth and trying to approach it the right way. 

I have Windows Server 2008 R2 with Oracle Client 11.1.0.7.0 client installed. TNSPing to 100% available databases (accessible from other machines, i.e. I can do TNS ping to these databases from other machines) returns me following error: TNS-12535: TNS:operation timeout. At the same time I can telnet to database host/port and see that proper tnsnames file is being used. Environment variables are configured as appropriate (ORACLE_HOME, TNS_ADMIN, Path). Also I can connect via telnet using service/host name + port - so this is unlikely a firewall issue. After enabling Oracle client logging I can see following error related details in tnsping.trc: 

Is it possible to control permissions on a single stored procedure level in SQL Server 2012? It seems that SQL Server Manager doesn't expose such functionality via GUI. Can somebody advice me how to view/set permission on a single stored procedure level? In particular I'm interested in removing "Connect" right on one procedure for specified account while leaving Connect rights for this account on all the others procedures. Is it possible? 

I'll try to explain my misunderstandings by the following example. I didn't understand fundamentals of the . Consider the query the plan of which is this: 

Note that the cost is decreased more than 4 times ( now). My question is how exactly the cost of depends on the correlation? How is it computed? It would be good if you pointed out to some references to the postgresql documentation. 

PostgreSQL 9.4 I have a table called which has a column integer. After runnig against it I got the following statistic for the column: 

The tables are relatively small (~100 rows each). Now, I need to select all pairs for users who have s. That's a classic usage of . But the table has the following statistic: 

I'm looking for a way to create new schema from existing database selecting only subset of database objects. I.e. I want to have some "database representation" which only includes subset of tables explicitly specified. What is the easiest way to create schema which includes/has access not to all objects in the database. Is it possible at all? I'm trying to do this in MS SQL Server 2012 R2. 

The only strange/suspicious thing about this machine is that it had multiple versions of oracle client installed (10, 11.1, 11.2) - removal of all versions apart from 11.1 & reinstall/repair of 11.1 haven't resolved this issue. Also it seems to me that v10 of client was not removed properly - it disappeared from Programs and Features, but files in installation folder had not been deleted. Any advice on what to check / how to resolve this issue? 

I know there are a number of topics on this question, but I'm always seeking more insights. I have a large table with a billion+ records. The amount of records could be reduced and archived, but the size will still be large. My task is to change a existing data type of a single column where the old value of data is safe to convert into the type. Here are some of my approaches: 1 - Drop the constraints on the table that impact the targeted column, drop the indexes that also impact the targeted column, add a new column with NULL's at the end of the table, update the new column with the old column values in chunks of 10K, 50K or 100K increments, drop the old column when data has been copied and reapply indexes for that column only. 2 - Copy all data into a new table with the data type change in chunks as before, verify data is completed, drop the old table, rename the new table to the old and apply indexes. 3 - Import all data from another data source like a flat file to a new table with the data type change using BULK INSERT and MERGE SP's, basically like option 2 with having 2 duplicate tables, verify data, drop old to replace with new table and apply indexes. What would be the fastest and safest option? Are there other options I'm not considering? I've updated 100 million records for other tables really well with option 1. The bigger the table, the harder option 1 becomes due to the time duration of updating. 

I have application / service which connects to Oracle database (11g) which shows me "ORA-00942: table or view does not exist" in its log file without any indication of particular table/view with which it has problem. And there is no way on app side to increase log details to include this data. I have direct access to the Oracle database which is using by this app (not DBA level privileges, but quite high level of access) any advice on how to check with which table or view my app having issue? Probably I can use some query to check this? I'd like to find out object name so that I can check granted access and any locks on it. 

return . So, as long as the index scan 2 random page reading (The first one is to read an index, and the second one to read the actual table), it's not clear what the means in the plan? Could you explain how it's computed? 

Since the optimizer doesn't have statistic for the value , it made a wild guess for of the table size. So, the estimated row count should have been . But the optimizer returned the . Why? Maybe I didn't understand the estimating process of row counting of unknown values? Couldn't you explain it a bit? 

QUESTION: Why is there so different expected and actual rows? I ran right before the execution but it didn't help. How can I deal with such a distinction? 

What is a statistical database? At a high level, it's just a type of database that only stores statistical data. An example is a census database. Typically, access control for a pure SDB is straightforward: Certain users are authorized to access the entire database. It's about the data not the engine Keep in mind, this is about the data and not the type of relational database management system you chose such as MySQL, SQL Server, Oracle, etc. Any database can be a statistical database if you store statistical data in said database such as census information and so forth. Typical structures for statistical databases That being said, most statistical databases are Online Analytical Processing (OLAP) which is characterized by relatively low volume of transactions. Queries are often very complex and involve aggregations (e.g.: derived aggregated information and not derived individual information). OLAP vs OLTP Can statistical databases be created using SQL Yep. Any database engine can be used to create a statistical database if the data is one that provides data of a statistical nature, such as counts and averages and the users accessing the database are querying aggregate, or statistical, data from said database as opposed to individual records on the users the statistics are ultimately based on. Being most of the popular database engines use a form of SQL to get data to and from the underlying system, then the answer to the question is, "yes". Why is individual information restricted? This greatly depends on the business or organization using the statistical database for their needs. In general, individual records, what the statistics are based on are restricted for privacy reasons. For example, medical statistics based on medical records from the local hospital. Therefore, security is a big concern for statistical databases in order to help prohibit users from unearthing individual information from the use of aggregated statistical information. Examples of how individual records can be compromised with statistics