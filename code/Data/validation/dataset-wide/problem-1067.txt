If you work with Reporting service in order to create a report and you need to retrieve data from the relational database. The datamodel of the relational database can be structured in a certain way that can be very difficult to retrieve right data by using join and lots of SQL. In order to save time and cost, it is recommended to use analysis service to retrieve right data that takes less time and cost and the result of the analysis service can be used in reporting service? In other words, is it recommended to retrieve data in SSAS that will sent to SSRS instead of using SSRS to do lots of SQL coding in order to display right data in the report? // Fullmetalboy 

Is there any recommended plugin for SQL server 2008 to use when you are working as database administration, Business Intelligence developer or Database developer? 

Goal: Improve my sql ability by doing some coding and reviewing existing code. Problem: I have spent lots of hour to find a solution based of this question "3c. Some countries have populations more than three times that of any of their neighbours (in the same region). Give the countries and regions. " I tried finding different solution but unfortunately I can't solve this problem :( The link can be found "$URL$ Again, the purpose is to improve my ability in SQL! // Fullmetalboy 

Yes, certainly, assuming your controlfile and spfile are in ASM too and therefore survive the failure intact. This is the kind of thing you need to test in your own environment to give confidence you will be able to pull it off for real when disaster strikes - there really is no substitute for testing your DR procedures to make sure you are not making any invalid assumptions such as: 

You'll need to substitute your ordering for my s - and probably do something with the pattern to make it match with your pattern table :) 

Depending on your real-world data, you will probably want to vary the number of granules and the function used for putting rows into them. The actual distribution of frequencies is key here, as is the expected values for the clause and size of ranges sought. 

Firstly, if you really want to use the name for your table then you will need to enclose it in quotes: 

IOTs are an index-like structure. When deleting large contiguous chunks of data, how much undo is generated as compared to a similar contiguous delete from a heap? 

11.2.0.2 is a minor release so for most purposes it will not be much different to 11.2.0.1 depending on your platform, 11.2.0.2 may not be the latest available (eg 11.2.0.3 on Linux) 

EDIT: I've created a much simplified example, simply partitioning the original table using 3 views, but not involving other tables (and the query plans for both variants). In retrospect, this is what I should have used as an example in the first place. 

This query (query plan) returns the correct result, but does not use the trigram index. It takes around 330ms. Removing either of the match conditions (query plan), or having them point at the same table (query plan), removes the performance problem. Both of these use the trigram index and are executed in under 1ms, but do not solve the given task. How can I get PostgreSQL to use my index? I have simplified this example to the minimum necessary to demonstrate the effect. The actual query is much more complex (and partially auto-generated), so using a UNION of two queries with only one text match each would be very hard, if it's even possible. I'm using PostgreSQL 9.5.5. The schema is still open for modifications (to some degree). 

(edit: see end for a simpler example) I'm searching in a table named "cases" (135k rows, 29 columns). Some of the rows in this table have a type of parent-child relationship (of different types), which means that for these records a mix of parent/child fields must be used for filtering and display. I have identified four different parent-child relationships and created views for them: 

See here for the docs explaining this in more detail, but the simple solution is not to run at all - just . Then your table will probably settle into a steady state where 'holes' in the data are left and can be used by later updates. As for "insert time", I'm surprised at your results. My expectation would be that time would be slower after a - but if all the blocks are in the cache, the overhead of finding free space inside the current block might be higher than adding the new row at the end of the heap even if the number of blocks accessed is higher 

You are explicitly setting the role to so you will need to use before your to use the permissions from role (but I'm guessing you probably just want to ) From the docs: 

Timing can be turned on with at the psql prompt (as Caleb already said). If you are on 8.4 or above, you can add an optional on/off argument to , which can be helpful if you want to be able to set timing on in .psqlrc - you can then set explicitly in a script where plain would otherwise toggle it off 

Problem: Where can I get material to improve my ability in SQL coding? Is there any website for downloading a database with different task/assignment and solution? Where should I go to review lots of SQL code? 

Which websites, books etc. can you recommend me to improve my basic knowledge in Microsoft Business Intelligence 2008, SSAS, SSIS and SSRS? Please remember that I'm a newbie in technical BI tool. If possible, it would be great to retrieve any material for task work that would be similiar as doing lab task at university. // Fullmetalboy 

What measurement do you know if this sql code is efficient based on execution plans? How do you know if this sql code is efficient based on execution plans? 

I have read that you also can apply N-Tier for Businesss Intelligence based on these criteria: *Presentation *Functional logic *Data What is your experience when you are using N-Tier? // Fullmetalboy 

What book or webpage can I read the architecture and methodology for creating ETL process? In other words, I'm looking for "how to do it" to create a ETL process when you have many source system to be involved and you also have a data warehouse. 

On Oracle you basically have a choice between running a hot standby and using RAC. The main aim of High Availability is to remove single points of failure. RAC does this at the server level allowing failure of a server without any service interruption. You will need to achieve something similar at the storage end using ASM, mirroring, and two or more physically independent storage pools (or a SAN). Using a hot standby will mean service interruption in the event of a failure, but is simpler and has fewer "engineering trade offs" Good quality hardware is also essential, eg SAS not SATA, redundant PSUs, UPSs etc. There are also other aspects of High Availability you may need to consider (eg human error) - this white paper from Oracle discusses them in more depth. 

for the following data, I'd like to be able to delete some rows and insert others giving the result below. Is this possible with a single statement (eg with the statement)? 

Unless you take steps to make sure this kind of thing doesn't happen again, you will certainly have "problems in the future". I suggest you: 

edit: I've managed to get a simple-ish reproducible test case. To me this looks like a bug due to some sort of race condition. schema: 

The results from these views do not overlap and together cover 100% of the table. When I select a union of all of them, and separately filter each view, the query takes about 9 ms. Selecting a union of all views and filtering the result of that takes about 500 ms. I have also tested this without views, inlining the queries they contain, which did not produce a measurable improvement. This is the fast query (explain): 

With the added requirement that one of the other field values () must be identical between parent and child records, I thought a composite foreign key should do the trick. I changed the last line to 

Is it possible to somehow let PostgreSQL know that the filter/join on the outer query can be applied to the inner subquery? Or is there any other way to avoid filtering each view separately? The user-facing form for this query has over 20 filter fields, and there can be JOINs with up to 14 additional tables. PostgreSQL is version 9.4.7 running on Linux. 

and got ERROR: there is no unique constraint matching given keys for referenced table "foo". I can easily add this constraint, but I don't understand why it is necessary, when one of the referenced columns () is already guaranteed to be unique? The way I see it, the new constraint would be redundant.