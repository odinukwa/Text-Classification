This is a classic example of a bad comment. Anyone can see that the code increases precision by n, but what isn't obvious is why by n? I would think it makes sense to increase the precision by the logarithm of n to some base. 

makes the structure a bit clearer. is a special case, and the rest seem to be in an illogical order, so let's structure the order to be in ascending complexity. 

Yes. is one of those mistakes that everyone recognises is a mistake, but is so deeply embedded in so many libraries and tools that it's a pain in the neck to eradicate. 

I'd almost add a third, but it's more at the level of a question than a code smell. Why does care about the subject being ? I can't see an obvious reason why 's type isn't . 

The massive advantage is that you no longer need : it suffices to use (or maybe : it's a matter of taste because was chosen such that would work in languages like Java which don't have ). I infer from your comment on an earlier answer that you haven't studied modular arithmetic. This is quite understandable: I think it's safe to say that 99% of programmers will never use it in their careers. But although it is irrelevant for most line-of-business programming, it is quite important in code contests/challenges (interpreted broadly to include katas, Project Euler, etc.) because they frequently use modular arithmetic to make large test cases feasible. If you find katas either enjoyable or useful for your CV, it's probably worth your time to study. The properties I'm relying on are (treating as an operator on arbitrary-width integers and assuming that we're avoiding overflow, which is ): 

is easier to read and should be faster for non-trivial inputs. An alternative, which is arguably even easier to read, uses a single accumulator: 

There are two main tricks which you're missing: Caching In the extreme case that you're given and , you compute the 10000th prime 4000000 times. It hasn't changed. The method should use a persistent (maybe , but I suspect it would overflow) and only do any computation if the value isn't already found. Sieving Trial division by every number up to the square root makes sense in some contexts, but not in this one. Since you're computing primes in order, you can easily do trial division only by primes up to the square root. Note that since this means accessing the cache, it probably implies refactoring to inline into , since otherwise you have potential inconsistencies. Other notes 

Yes, the algorithm as described is sound, although not the most efficient use of the random number source. However, there are a few surprises in the code. Making the default capable of returning 2^31 - 1 distinct values is a bit unexpected, and slightly skews the distribution of the lower bits. It might be worth changing the names, too, in case you want to add more output types later. I would adjust as follows: 

This looks like a test wrapper rather than the "main" code. It would be nice to offset it in a separate file. There's also a good case to be made for writing unit tests before you write a TUI test wrapper, and I think you would find reviews of unit tests more useful than reviews of a TUI test wrapper. 

That leaves the question of whether it should be done at this level. You haven't said how you're getting the date from the database. Can that be configured? 

You have common code, which moreover has applications beyond this one, so should you not pull it out into a function? Then you can reduce to 

Take the first elements from the source, put them into an array , and sort it. Read elements from the source, discarding them if they're smaller than the smallest element of , and otherwise inserting them into the binary heap. Do this until the heap is full or you reach the end of the source. Combine the top elements of the binary heap with , either by insertion sort or by merge sort. If there are more elements, clear the heap and loop back to step 2. 

However, the approach of storing the full path for each visited element doesn't scale particularly well. All you really need is to find the predecessor: provided you store all of the predecessors, you can work your way back up the chain. One way to do that would be a dictionary (which could also take the place of ). 

The number of trailing zeros doesn't change when you mask away the ones above the lowest one, so I don't see that this is any clearer or simpler than 

Finally, to address the security: give me two passwords encrypted with the same key and I will give you both passwords in very short order by finding the greatest common divisor, which will either be the hashed key or a small multiple of it. I will be able to tell when I've found the right multiple because the salt padding will all be from the reduced set of printable characters. 

Algorithm You seem to essentially be doing linear regression with a least absolute deviations optimisation criterion. There are various standard approaches, although I'm not entirely sure about their applicability. Problem: the task implemented by this code is not well specified. The variable serves to quantise the gradient and intercept of the line, but it's impossible to tell from the given specification and comments whether this quantisation is inherent to the problem domain or whether it's there to control the runtime of the search. If the quantisation is inherent then some of the standard techniques will give starting points but you'll need to test the four nearest valid parameters (rounding each parameter up and down). Given the size of your data, the approach which tries all lines through two points is probably going to be the most efficient, but again I'm not sure whether the real scenario has a larger dataset. 

No. Just no. ISO 8601 is the only date format that should be used unless a locale parameter is supported. 

Why not reindex so that (rather than, at present, )? Then you can lose the special case in . And as a bonus the variable name will be more transparent. 

I see cropping up a lot, and it makes for some unreadable lines of code. That should definitely be pulled into a method as a starting point for possible refactoring to merge conversion methods which are always used together. provides an operator overload for . 

Looks like is left over from a refactor and is now completely unused. Why append to a string? Other methods in the same class use , which is a better way to do it. In this case, with a known fixed length, there's a good argument for too. 

And now the special case is really a microoptimisation, and could possibly be removed to leave a one-liner. 

So this code is executed up to 10000 times, and each time is filtering up to 10000 cameras and then sorting up to 10000 cameras. Given that each query includes at least one brand out of the five, there are only 31 possible brand combinations. So by pre-computing you could execute this code 31 times rather than 10000 times, and since it's the obvious candidate for the bottleneck that might give you a 300-times speedup. If that's still not fast enough, then you need to look at linear time median algorithms. But frankly if the queries are evenly distributed then ~300 queries per brand combination is much larger than \$\lg 10000 \approx 13\$, so it's probably faster to quicksort once per combination than to execute a linear time median once per query. If you really want to overkill it then you could create a custom data structure which supports multiple median queries by maintaining state of which ranges of the array have been partitioned around which pivot, but KISS, YAGNI. Alternatively, but still violating KISS, you could do just five sorts (one per camera) and then some complicated multiple binary chop. I'd still prefer one sort per brand combination. 

I'll give you one more hint about improving the complexity: the fact that they want the answer modulo 109 is a really big clue that you shouldn't compute it solely via . 

Check the spelling. What's the second skip condition about? I don't see that in the algorithm description, and it seems to reduce the ability to discriminate between multiple candidate ellipses. Why the inconsistent use of for two conditions and then a massive nested block for the third? It would seem more consistent to write 

Assuming that is correctly implemented and that and are guaranteed to both be non-negative, is equivalent to . That allows a further optimisation to a single loop, because the number of values in which are divisible by is : 

I can work out what and mean, but I think and would be clearer, especially since the method names do call them and . Sort-of. Actually, I would have guessed that meant and meant , not the other way round. 

An clause here would not change the asymptotic complexity of your solution, but it would surely make it much faster in many test cases. 

Too many people use and then rather than , so thumbs up for getting that right. I would observe, though, that is a common enough pattern that you could consider factoring it out as an extension method to and . 

Whoa! If this method is used much at all, it's going to be a major performance hit. IMO you need to revisit the way the data is stored: to do this efficiently you want a . Also: why return ? Under what circumstances would you call this method wanting anything other than ? 

Separate point on cost: since you're always going to call (if nothing else then in the selection), is there any benefit to making it lazy? 

That is a classic blunder on the level of invading Russia in the winter or going up against a Sicilian when death is on the line. bitmap.getPixels will probably give you a speed-up of about two orders of magnitude. The other thing which would probably give a moderate speedup is changing the way the image is divided into chunks. Image formats are compressed: you will get better performance if the way you access the image respects the compression format. For JPEGs that means using chunk sizes which are multiples of 8 pixels wide and high where possible, and I think (although a quick search has not sufficed to confirm) that you'd be better off with full-width small-height chunks than medium-width medium-height chunks. (I.e. I expect the 8x8 JPEG blocks to be laid out in English reading order). 

We're still in , so this looks like more copy-paste which either means that it's unnecessary code in both methods or that there's a bug. as a name implies to me that the variable is being used in a swap, or maybe as a factored out part of a complex expression. For a loop variable, why not ? (Or , since that's what the base class is called?) 

Why are these lists? is appended to in the parsing stage and iterated over in the output stage, so it could be any . is as reasonable as any; I personally would use , but I can see an argument that is more efficient. is appended to in the parsing stage and looked up by index when parsing the edges, so it has to be a . But the output stage calls in a loop, and there it would be more efficient to pre-compute a (or to store the index as a field of the ). is cleared, added to, and tested for . It should be a , because is always going to be a operation. Unused variables I don't see any reads of or except in their self-updates ( and respectively). Maybe they were for debugging or you changed your mind about the signatures of the methods, but they can be deleted now. Style Style is always disputable, so these are opinions, but I hope that they're at least majority opinions. 

Wrong question. It's not a matter of re-ordering the iteration: it's a matter of iterating over something much smaller. You're looking for \$s\$ such that \$(s+t) \mid st(s+1)(t+1)\$. But \$s(s+1) = (s+t)(s+1-t) + (t-1)t\$, so that's equivalent to looking for \$s\$ such that \$(s+t) \mid (t-1)t^2(t+1)\$. It's pretty easy to factor \$(t-1)t^2(t+1)\$, since it's already partly factored. To make it really efficient you can use a sieve (which you already need for the primality test anyway). I adapted some code I wrote previously to enumerate factors given a prime factorisation, and was able to run with in about 1.4 seconds. Online demo. The bottleneck (about 55% for ) was the sieve generation, and that would be a serious memory problem for . Since we use it for factorisation up to , and only for primality testing beyond that, I changed the sieve to only run up to and to case split. This gave a 70% speedup (presumably beating the obvious limit of 55% by improving the cache locality), down to just over 30 seconds for . Online demo. Then there are small speedups available by pushing the range tests on into the generator and not pushing onto the heap values of which are too large. I did think that there would be a speedup by observing that when you increment you can reuse the factorisations of and , but that actually doesn't seem to work. My fastest version takes 20 seconds to go up to : 

Looks buggy: will only be correct when or . Also a tad repetitive. Can't you merge the two counts earlier? 

This is not dynamic programming. A dynamic programming approach to this problem must work on the following basis: 

seems a bit heavyweight given that the only reason for calling it is to append a newline. I'd replace with 

Leave it as is. Replace with a class which takes scaled reciprocals of a general sequence, but keep the static for the name alias convenience. Factor that general class out of and make the latter a subclass whose only declared member is a constructor. As 2 but replace the static method with . (There's also a 4b option of having both static utility methods). As 4 but make the return type of all the static creator methods be . 

It is possible to calculate it in \$O(\lg N)\$ time, but it's a bit complicated. Consider how to count numbers \$1 \le i \le N\$ such that . That requires an odd number of digits such that . The valid digits are \$D_1 = \{1, 3, 5, 7, 9\}\$. Let \$c(n, D)\$ be the number of n-digit numbers (including those with leading zeroes) with an odd number of digits from the set \$D\$. Then the number of n-digit numbers with an even number of digits from the set \$D\$ is \$10^n - c(n, D)\$. Now, clearly \$c(1, D) = |D|\$; and $$\begin{eqnarray}c(n + 1, D) & = & (10 - |D|)c(n, D) + |D|(10^n - c(n, D)) \\ & = & (10 - 2|D|)c(n, D) + 10^n |D| \end{eqnarray}$$ which has closed form $$c(n, D) = \frac{10^n - (10 - 2|D|)^n}{2}$$ So how many numbers do we have with ? If \$N\$ has \$n\$ digits, it's \$c(n-1, D_1)\$ plus those between \$10^n\$ and \$N\$. Those can be separated into the ones with the same first digit as \$N\$ and those with a smaller first digit; the latter can be handled easily, and the former require a recursion. Once you've got that implemented, it should be easy to generalise it to handle sets \$D_2 = \{2, 3, 6, 7\}\$, \$D_4 = \{4, 5, 6, 7\}\$, and \$D_8 = \{8, 9\}\$. Finally you weight the four counts by the corresponding value of the bit and add them.