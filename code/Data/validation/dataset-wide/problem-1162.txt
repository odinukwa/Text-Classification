You can use a deferred constraint trigger, or (what I'd do in this case) replace assignment table with an array column in and a normal before update/insert per row trigger to sort the array, and and using a unique key on the array to ensure uniqueness. 

You could perform the pg_dump with the parameters and , so no permissions or links to any users are presents in the dump. 

The schema is for PostgreSQL, you would use the equivalent of for the auto_incremental implementation of your RDBMS of choice. 

Your query has many issues that difficult optimisation. You use in several tables where should suffice (e.g. all cases of and ). You include tables that you later never really require (e.g. all cases of ), so removing all of them wouldn't impact the result (replace all instances of from to in the where clause). You left join the table four times, when one should suffice with inner join: 

You could implement an ON INSERT trigger and delete, for the same user, any rows with a timestamp older than the oldest (MIN) of a subselect ordered by timestamp DESC limit 10. Edit, using an array in users: An alternative is using an array straight in the users table: Add a type and a new array in users: 

The above requires you modify to include a , since you could have more than one person with the same name and surname. 

With the query above, all phone numbers will show as although each area has specific formatting rules, which vary greatly, so that formatting will look funny for certain regions, such as Australia where formatting is . 

You could use to get a list of all equipment names sorted alphabetically for each growth space, and compare it with another sorted list of elements: 

NOTE: MySQL Workbench may not allow running the whole script at once. Run until first, then run the rest. 

For large volumes of data, perhaps you can cut down on the cost of fulltext search by adding a fulltext GIN index, either for the whole json () or the output of a function that would extract and concatenates all of its relevant values. Then you could use the operator to obtain a reduced subset of likely matches, and search within that subset using or . 

You could play with the intervals, reducing them to 1 day (more cycles with faster queries). Ensure you have an index on . 

Your query doesn't loop 10483 times, it loops once (loop=1) for each loop, but it has to sort 10483 rows first prior to discarding all but the first 5 rows. Your last query was almost correct, except for the fact that you filter user_account by the users with greatest follower_count regardless of being involved in the discussion or not. Also, isn't this query returning the same user accounts? Because you're not using DISTINCT nor GROUP BY, and user_account has a one-to-many relationship to user_post_comment. Perhaps this will return more accurate results? 

That will show you where the environment variable is pointing to when you start the service normally via , and all the parameters used for starting and stopping the service. 

If I execute via an TFDQuery component, the function itself fails (FireDAC executes it as a cursor), raising the same error stated above. After tearing my hear out debugging, I realised that the only way to avoid the error was avoid creating any temporary tables inside the function. Whereas creating the function as or as the error was raised. Without , no errors raised. Any clues will be highly appreciated! 

You may want to just leave it as users enter it, since you can safely use strings processed twice via . In the documentation, the example for phrases within ts queries is: 

SQLServer can't work without a master database at all as it stores essential configuration information in it (such as the table of databases in the system), so it wouldn't allow you to detach it. However, provided the SQLServers being the exact same version, you'll probably run into no problems if you stop the SQLServer services in both ends, copy the master database files from the one system to another (keep the old ones just in case), and start the services agan. It will be safer if neither server has any other databases in the system. 

You may want to consider using a single JSON field on your posts table to store all the miscellaneous details in a JSON object. You can have as many as you want, including arrays and nested objects. At this time MySQL is not great at indexing and accessing/manipulating JSON data, but it does provide a few functions like JSON_OBJECT, JSON_ARRAY, JSON_SET, JSON_INSERT, JSON_REMOVE, JSON_REPLACE, JSON_MERGE, and JSON_EXTRACT, and surely future version will have even greater support, such as indexing. MariaDB has also Dynamic Colums and also good (possibly better) support for JSON data than MySQL. 

MariaDB encryption wouldn't be able effectively deal with either, because stealing the drives would effectively provide root access to the system, including access to the encryption keys and complete unrestricted access to the database. This is also the case with cloud technicians, who can access backup copies of the system or peer into the filesystem by analysing the underlying datastore. MariaDB encryption can minimise the risk of intrusion, however in order to access the drive or mysql files, the attacker would have to have either root or at least mysql user privileges, and would access not only the files but also the encryption keys in that case. A good way to ensure encryption at rest is using dm-crypt (or equivalent) to have the OS encrypting the partition where the database files are located, and entering the password manually by an authorised operator at system or RDBMS startup time (i.e. no keys stored in the system at all). This system is not particularly effective against server breaches, but it makes the data completely secure in case of physical storage theft, or access to system backups or underlying datastores. 

I use gdrive (follow the installation and set-up guidelines from the link). Once gdrive is installed, I dump my own databases to a folder daily, and then upload my files to Google Drive's PGBAK folder using gdrive. I use the following script in /etc/cron.daily/: 

I found that your query had many redundancies in the conditions, and you used cross joins that were good candidates for simple joins. This might confuse the planner. Perhaps you could try the following rearrangement of the query (it is functionally exactly the same but uses joins and removes all the redundant comparisons) to see what the planner comes up with in both production and test? 

You could create a third table C, with a primary (or unique) key defined in it, and create before each row INSERT, UPDATE and DELETE triggers in both of your tables A and B that insert, update or delete their key into table C. You could use table C for foreign keys also, if you need to. 

Instead of , perhaps you should use . Instead of , if the parent exists in or any of the others, use that link instead of the constant. You could also use a single IN, and a UNION of both queries. Finally, if you use a filter condition in WHERE on a LEFT JOINed table, that defeats the purpose of a LEFT JOIN and might result in bugs. Either use JOIN or move the condition to the ON part of that LEFT JOIN. In this case, the condition in the LEFT JOIN was different than the one in WHERE for the same field (t2.id), so it could never work.