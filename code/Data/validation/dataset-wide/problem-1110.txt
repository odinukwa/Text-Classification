To get the most out of it, you would need to create indexes on both tables. Now depending on the query, you would have to decide which indexes to create in the first place. Blog where it just scratches the surface of the indexing can be found here You also need to be able to read execution plans, in order to understand whether your indexes are working properly, and whether you are even using those indexes. Blog about execution plans can be found here To answer your question: it makes no difference which table you initialize first, as query optimizer makes his own plan and order in which tables are joined, unless specified otherwise (with option(force order)), but it will give you no performance boost if you do. 

Would give you ID of the first customer from the first order In your particular case value('.') means give me all values from the shredded element( that i will talk about in a moment) Remember in using value function you`re moving through XML. Now to make it more easier ,not to 'move' too much you can use function node which shreds XML into relational data. It returns a row set with rows representing the nodes identified by the path expression. Example: 

This is very importaint key, and make sure you back it up using this guide Now you are going to create a certificate based on which you will create module signing user. Note it is only for singing modules and you cannot use it to test privileges etc. So first things first - Certificate : 

As Kris mentioned, you could create a stored procedure and run it as a job Here is a sample script that will accept Table name and Threshold(in KB) and send an email if table size exceeds its threshold 

.wrk files are just temporary extension names, while log files are being copied. If log is small enough you wont even notice that change, but in your case it is notable. Basically this is way of sql server ensuring that log that has being copied wont be accidentally restored by LSRestore job or by a manual restore. As soon as log has been copied successfully it will change into .trn. I dont know anything about your SQL, hardware and network specifications but having 570 transaction log backups in a span of one hour once in 24h could cause overhead on disk IO. Check the log file size, perhaps adding to the schedule one more LS during 24h span would be beneficial (lower tran log size). 

Which was NVARCHAR(500) in this case and SQL server does not need to check the underlying structure of referenced functions/tables as long as knows the return type. It will fail to execute tho, if usernames function (in this case) would return something other than NVARCHAR(500), which is expected On the other hand 

To add on my comment, you can create a trigger on a database level and add user to desired role automatically when you create a user. You can add a custom one, but for example sake i added db_datareader. Example : 

Havent really tried to debug the script, but i can see just from the parameters what it essentially does. There are many scripts online that you can use to do just that The one that i use pretty often is this: 

Since you are testing it on production,and the tables are busy, there are many reasons why "sometimes" its slow. I cannot give you exact details because i dont know logic and environment in the first procedure, but here are some things to watch for: 

Green rectangles are tables, red eclipses are attributes, blue triangles are tables that connect tables and a relationship descriptors so you could understand their relationship. Do not name triangle like tables as referred on the diagram, such as 'contains' , give them meaningful names. Purple eclipses are additional attributes of tables that connect other tables Numbers and letters next to triangles are telling you what kind of relationship it is Whether is : One to many - example Table Account has 1 and table Character has N which means one account can have many characters , but character can have only 1 account Many to many - example Character can "Ran into" many creeps and many creeps can ran into many characters What does it means? It means that depending on a relationship type, you will create foreign keys and constrains accordingly. If you are still in doubt what the foreign keys are, i suggest you to read some books/articles before you actually start designing database. Databases are base for any project, and it is not something to be taken lightly 

What you`re seeing is the XQuery implementation in SQL Server . Although XQuery uses its own parser and performs its own algebrarization during the query compilation stage, the results are combined and optimized together with the DML portion of the query, then combined into a single execution plan. SQL Server supports five different methods. value , exist , query , and nodes are used to access and transform the data. Last one, modify , uses XML DML to modify the data. The value() method returns a scalar value from the XML instance Lets say you have the xml : ` 

You can also use extended events and capture high resource consuming queries, with included user names 

You need a table between Cities -> Business, and a table between Categories and Business. This way since city can have many business keeping it all in the single table you are breaking the 2nd Normal Form which is suggesting that the groups of data shouldnt be repeated within a table. What that means is that for instance if you have City of New York that has multiple business you will have 2+ records with the same City name but different City ID. Therefore i would keep the table cities with metadata about the City only and add a new table CityBusiness with its own identity id and c_id and b_id OR since cities can have only one business (is it possible for one business to expand and open its office to another city?) you can have a primary key defined as combination of b_id and c_id. The same rule applies for the Categories. A new table BusinessCategory with ca_id and b_id, and a Category table with metadata related to category itself. 

Note that i filtered the view names retrieval query with some silly name, you should apply your own filtering. 

You should try to get a track of long running trans. especially during that time, so you could prevent it from happening again. Check whether you have some kind of bulk inserts, long running jobs that include some DMLs, or simply set the alert. 

When you use the nodes() method with the XML column from the table, you must use the APPLY operator. Example: 

Fields to note here that are useful for shrinking are File Size which is the size in bytes, and Status which can be either 0(unused) or 2(used). The amount of % or MBs that could be shrunken is sum of unused VLFs - 1VLF. Be aware that shrinking only happens in VLF amounts, you cannot shrink it in desired MBs such as 5,10,12 etc, unless it fits the summed size of used VLFs. Always make sure to check messages after shrinking occurs. 3) Shrinking does not remove indexes, since shrinking removes only unused(empty) VLFs. If you might have thought, if shrinking database files defragment indexes - yes it does but its off the topic. 

This DECRYPTION BY PASSWORD is password that you specified when you were making a certificate backup. And last step is creating an actual symmetric key, with the same KEY_SOURCE and IDENTITY_VALUE as the one on original DB, with certificate(that you just created) specified encryption. 

If you were still developing database, you could create views within a single schema, and add that user or group of users/role , permissions to alter that schema alongside permission to create view. Such as : 

There is a workaround for that issue. It will require an additional table,a trigger, and an agent job to be ran before transaction log backup job. So if you think its worth going through that than bare with me: You should create a table where you should add Database name,whether database was updated or not (you will need this for an agent), and some other info needed for backups such as : 

Based on your parameters it will defragment the indexes, either rebuilding or reorganizing depending on @MaxFragmentation. Online or Offline ,depending on edition. Hope it helps 

You should definitely separate it into multiple smaller procedures. Using just one procedure will cause a lot of locking and blocking, and generally degrade performance. As for optimizing: The first statement is update that updates data based on a @Code provided. Unless if its a clustered index created on it, you should create a nonclustered index on a Code column so you could prevent the whole table scan just to find the code. Same logic could be applied for deleting. Having an index on MainObjectID, and generally on any Foreign Key column would provide you decent performance benefits (Whether you delete, or join the table, or even update records in parent table). As you are inserting some data and deleting data based on ID of that insert, you could create a procedure that will do the first part of the code (Update or Insert if condition is met) and then return you the value of that ID(How to create objects with output parameters), which you could store in some int var in your c# app. Once you get a value you could execute the 2nd part of the procedure ,which includes deleting and some value inserting. Still it wont give you a best solution as you are inserting and deleting data in the same transaction - meaning it will most likely escalate into object lock, making you unable ( depending on transaction level) to make other select queries on those tables, but it will overall cut the % time of that update/insert initial statement. In summary keep in mind that smaller and faster transactions are much more effective than larger and complex transactions, for performance and concurrency standpoint. 

You can use it either way, however there is a one thing you should keep in mind EXEC() function is SQL injection prone Take a look at this: 

Simply granting VIEW DEFINITION and SELECT permissions on and schema wont give you rights to see the definition of the view. Permissions that you added will provide you information about all objects in sys schema and information_schema. I suppose you want to see the view definition which are created in some other schema, in which case you would have to provide the user with to that particular schema. Such as : 

It wont yield any results, but it wont execute truncate table, because like i said, it is only treated as a parameters. Other than that, performance vise, there is no difference. Note that you should call procedure with schema name included, and should be aware that dynamic SQL executions will create a new plan individually from stored procedure each time you call it. Update 

See if you can find SOS_SCHEDULER_YIELD & CXPACKET waits. If SOS_SCHEDULER_YIELD waits are high you might have some very CPU extensive queries, which you should pay attention to. This: 

Hope it gives you few startup points, in case your boss is not willing to invest in 3rd party tools, or you dont feel like setting up data collector tool 

As someone who is trying to use the DMVs as much as possible, there are few scripts that can give you a quick heads up 1) This one will give you an info about the full backups and tran log backups, how long it took it, what was the size of the backup,where it is located,expiration date(if it has),logical device (if it exists as well), and server name. Keep in mind its filtered for backups for past 7 days, and it displays all databases on the server, but you can sort it up as u please 

It has nothing to do with SCH-S. Select queries which acquire S and IS locks are compatible with SCH-S, which is called Schema Stability and all it does, it prevents table during query execution from being modified which means SCH-S and SCH-M are not compatible. SCH-S is compatible with all other locks. Your problem however is that you have auto update stats enabled, which are triggered whenever there is certain amount of data changed within a table (20% pre SQL2016 or with 2371 traceflag counts % depending on number of rows) and it does when you execute the query. So what might have happened is that your table was pretty busy and query optimizer decided it should update statistics, so you had to wait. You should disable auto update statistics and update it manually 

Next step is to create a job that will check (before tran log backup job): Step 1. Whether there is a database with value of 0 (meaning not backed up) Step 2. Back it up taking values from the table (DB name,BackupName) Step 3. Update column to 1. Optional: Send an email informing DBAs about newly backed up database. 

If views are already created, you can either add grant alter schema (name of the schema where the views reside) and add permission to user to create a view. But be aware that since you granted a user alter schema, he has all the rights on that schema, including truncating/droping tables etc.. Last option is creating a dynamic query that will grant a user/role to alter each of these views Update 

Once this is done, the next step is to actually sign this procedure, so whoever has rights to 'execute' the procedure will inherit rights from the certificate user. 

In order to decrypt the column that is encrypted by symmetric key you would have to create the exact same symmetric key on the new database. Since symmetric keys cannot be backed up, in order to use them on another database you would have to provide them 2 attributes that have to be specified when creating a new symmetric key on a different database to decrypt a column. Those two attributes are KEY_SOURCE and IDENTITY_VALUE After you have specified these attributes you need to encrypt that key, with one of the following: password, certificate, another symmetric key, asymmetric key, or some third party provider. In your case is certificate, therefore you need to create a certificate from certificate(original DB) that you supposedly backed up already. Such as: 

That happens when you have an open transaction during the database full backup. Whenever you make any DML data pages are hardened to disk, containing the earliest LSN that was part of that transaction.These are called dirty pages (since transaction is not committed), which helps SQL recover after an unexpected shutdown/restart, to restore itself to the point before non committed transaction. While you was doing a full backup you, it ignored the LSN of the transaction that was ongoing, and after it has been committed, you next transaction log backup read the latest committed transaction (which was the one that was active while your full backup was being processed). I managed to demonstrate the same situation on my computer with a simple test: leaving a transaction active, while taking a full backup: 

Just to expand on previous answer that was posted. Just like George said, shrinking in general is not something that should be part of maintenance job. Rather log size miscalculations, some unexpected scenarios (such as uncommitted transaction, large and intensive DMLs etc etc) or insufficient amount of log backups can cause excessive log growths. If your log size does not seem large enough, you should monitor it during busy hours, or during night time ETLs(If you have some) to see the average log size and change it if needed. Also make sure to set log size auto growth in specific MB size, which will mostly depend on your initial log size. More info could be found here Database log VLFs Now to answer your questions: 1) No in general. But in scenarios i mentioned above, it could be helpful. Which is the only time when it should be used - out of ordinary situations. 2) If you determined you want to shrink your log file, you should be aware that the log file is made out of VLFs(Virtual log files), which are gradually filled one at the time. Once all of them are filled,if you reached your log maximum size, log auto growth will happen and depending on size will grow in 4/8/16 VLFs. Once the log is backed up, these VLFs will become empty again (you will always have some in use, so it can track current LSN). To keep it short, once you backup you log, you can check used and unused VLFs using command 

You could also use window functions which are usually faster,to get OrderNumbers that contain two rows, meaning contain both 'ghi' & 'abc' 

You should setup a role with permissions you wanted, and assign user to that role,so it would be easier for you to manage that user and permissions in general. "Almost anything" would include alter database, but you have to be aware what the user can do. You could add a user permissions such as : 

There are couple ways to achieve what you are after. I will show you two, but there are other ways as well. The first one is signing a procedure, and allowing other users to use it with specifying only. First of all you need a master key, which i assume you already have, and you can check it using DMV, and looking for record that has '##' as a prefix. If not simply create a DB master key