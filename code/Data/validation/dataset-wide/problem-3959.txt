I think that in some ways you have answered your question yourself: we see that to prove properties about sets, say within the projective hierarchy, we need representations of those sets of reals as trees, but moreover nice trees with certain properties (and the homogeneous trees you mention in particular with measures attachable to them). To get the latter involves measurable cardinals at least; and to get the determinacy of PD or AD we need to be able to shift those trees around in more subtle ways to prove that complements of nice trees, and projections of those complements etc, also have nice properties, and this requires Woodin cardinals etc... Conversely we find descriptive set theoretic arguments involved in analysing the mouse components that go into the making of the higher inner models, in particular this is necessary for the so-called core model induction. To construct such models one has to have something of an inductive process to do so, and to start off, this involves an analysis of the lower levels of a putative model, and the description of those levels is, or can be seen as, descriptive set-theoretic on the sets of reals of the model. It just begins to look inevitable that DST and inner model theory will thus be inextricably linked. Thus the fact that the Kechris-Martin theorem has two styles of proof starts to look like two sides of the same coin. Whilst I am not really pretending that this is a comprehensive or full answer to your question, (I hesitated to answer this as I am not setting myself up as "an expert" who you ask for!) one could add that one aspect at least, is the following reply to the (simpler, side) question that is often asked "Why large cardinals" or "Why are such large trees, and concomitantly, large sets, measures etc needed for these analyses?" It is possible to see this, at least in the determinacy arena as simply an extension of what is needed to analyse Borel: Friedman showed that we would need iterations of the power set operation (and Collection to collect together the resulting sets) with roughly speaking the number of iterations proceeding stepwise through the complexity of the Borel sets to show their determinacy (thus for $\alpha \geq \omega$ one would need $\alpha$ many iterates of power set etc to get $Det(\Sigma^0_\alpha)$). Martin's proof of Borel determinacy showed that Friedman had it exactly right: we already need larger and larger trees, or spaces if you like, in which to unravel Borel sets at a certain level and show their clopen representation (and hence determined status) in a corresponding larger space. So: we exhaust the power of ZF to show there are enough good trees to establish Borel determinacy. To get higher levels of determinacy we are going to need to go beyond ZF and use strong axioms of infinity, i.e. large cardinals. Thus the whole enterprise spans a spectrum through building models of fragments of second order number theory second order number (up to $Det(\Sigma^0_3$) say, through models of fragments ZF of certain uncountable ordinal height (Borel Determinacy, so we use "weak" inner models of fragments of ZF of set height here), and then full height of the ordinals models of ZF, (for $Det(\Pi^1_1)$) and now comes the same with inner models for larger cardinals, albeit more sophisticated arguments. 

A proper class of measurables more than suffices. It suffices for the generic absoluteness to have X-sharp exists for every set of ordinals X. Then the Martin-Solovay tree can be constructed throughout On. One place to look is below. The definition of the Martin-Solovay tree (I seem to recall) is given for an arbitrary cardinal $\kappa$, but even if it is only given for $\omega_1$ just replace $\omega_1$ by $\kappa$, and take the union of the trees on all regular cardinals $\kappa$ to get a tree on $On$. A. Kechris Homogeneous Trees and Projective Scales. Cabal seminar 77-79: Proceedings of the Caltech-UCLA Logic Seminar 1977-1979 1981,Eds.A. Kechris, D.A. Martin, and Y. Moschovakis 839, Springer Lecture Notes in Mathematics Series, Pages 33-73 

Concerning Copy(K)-determinacy: you mention that every set of countable ordinals being climbable would lend plausibility to this in ZFC (that is with AC). We just saw that climbability fails under AC; of course $\forall K$Copy(K)-determinacy holds under AD; and if you make the game harder for II by insisting (I make it that II has the winning strategy, not I, by Boundedness) that II's wellorders code all of $K$ below their supremum, then one can conclude that $K$ is in $L[\tau]$ where $\tau$ is II's strategy. So for this slightly more rigorous game, its determinacy for all $K$ is inconsistent with choice and $\omega_1$ inaccessible to reals. 

Mathias has a paper where he corrects the flaws that occur in Devlin's theory BS (= Basic Set Theory). The theory has to be only slightly strengthened to be correct. (It is more than sufficent to add an axiom that asserts for any set and any $n \in \omega$ there is a set of all its sized $n$-subsets.) It is really only in dealing with syntax and showing that certain straightforward concepts are $\Delta_1$ that BS comes unstuck. I think the book can be safely read beyond a certain point. It is true that Devlin has not correctly proved that the satisfaction relation for $\Delta_0$ formulae is $\Delta_1$, but one can just take the attitude that the result is correct (as Jensen showed), it is just that that particular development in BS failed. BS needs another axiom and all would be well. Thus, the constructions of the $J$-hierarchy, the fine structural concepts of projecta, mastercodes these are all fine, as are the constructions of trees, $\Box$, Morasses etc, and the Covering Lemma can all be safely read since there one is past the point where these delicate matters are being considered. Mathias: Weak Systems of Gandy, Jensen and Devlin, Centre de Recerca Matem`atica, Barcelona, 2003--2004, Birkh\"auser Verlag, 2006, Eds: Bagaria, Joan and Todorcevic, Stevo, Trends in mathematics Series. 

As far as I understand it (although I am no historian) Cantor himself came to make an explicit distinction between "Absolute Infinities" or "Inconsistent Multiplicites" on the one hand and the things that could form a 'completed' whole, that is sets. Of the two former terms he seemed to change which term he used depending on who he was talking to. He also had the unfortunate habit on occasion of using the word "Menge" when he really meant one of these proper class forms. Nevertheless he was quite aware of the difference. (See his letter to Dedekind reprinted in the van Heijenoort "From Frege to Goedel" volume.) von Neumann made the distinction completely explicit: he gave a functional axiomatisation of what would become the "N" part in "NBG". This was in two papers in the late 20's. His was a thorough-going mathematisation of the concepts and had three types of function with arguments depending on whether they were 'sets' ,'classes' (or either). But earlier than this Zermelo's axiomatisation was about which classes could be regarded as sets. (The words 'set' and 'class' in English were used early on by Russell - pre-Principles of Mathematics. Cantor said that he like the idea of "ensemble" (French) rather than the German Menge - presumably because it carried overtones of being gathered together into a completed whole). 

My impression is that a reconstruction of L in NF is not very satisfactory. One version of L's construction, following Jensen, is to take the rudimentary set functions and iterate them under composition and unions along the (von Neumann) ordinals. However not all the rudimentary functions can be given a stratified definition. Thomas Forster remedies this and defines a list of strat.rud. functions and mimicks the Jensen process using these to build a strat.rud. closed model S. However choice fails in S, but more significantly, S has a canonical wellorder (that of construction) but initial segments of the graph of this well order are only unstratified, and thus are not in S. In short the model S cannot "construct itself" in the way that "V=L" is shown to hold in L. Worse still, there is no total order even of $V_\omega$ (let alone a wellorder) in S. The problem seems to be that one can iterate the strat.rud functions along wellorderings, but there is no way in general to compare the results: one does not have in general a Mostowski-Shepherdson Collapsing Lemma in NF to "transitivize and compare" the differing hierarchies so produced, because there is insufficient induction. (The latter problem would seem to raise its head whatever version of L's construction one used.) Forster's article is in Contemporary Mathematics, vol 36, 2004. You may want to check out his webpage, which lists publications of himself and his research group. Dang's thesis to be found there (I think) looks to be relevant.