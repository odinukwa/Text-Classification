There is a range of computer programs that can represent acoustic energy graphically in a spectrogram. I usually use Praat, which uses a black and white gradient to represent the intensity of energy at specific frequencies and time points. Here's an example: 

Then it would make sense to compute IAA for all features combined if what you need is accuracy in the annotation of all features. That IAA is lower overall is not a problem per se since whatever benchmark you are using (comparing two human raters as benchmark, and one automatic with one human rater as test case, for example) will also be lower. On the other hand you will lose information by computing combined IAA. It might be of interest whether there is more agreement for annotating perfect than for annotation progressive, for example. This will help in identifying categories that need to be clarified or need clearer guidelines for annotation. Unless you have dozens of features, it might make sense to compute both combined and single IAAs to get the whole picture. Another point is whether the features you want to annotate belong to the same category. If you rate syntactic and semantic features of verb phrases it will probably make less sense to calculate combined IAA: 

This is why I said pela might be interpreted as not-singular because it's also used in the dual (and trial where it exists). 3rd person dual conforms to the system: tupela But 3rd personal singular and plural have suppletive forms: em and ol Again, as far as I know pela is not used as a plural marker otherwise. But the meaning of the pronouns is still compositional in that there is a person morpheme (e.g. mi), a dual morpheme (tu), and a non-singular morpheme (pela) that combine to form the various pronouns. 

There are opens source tools that do that. They are called lemmatisers - Stanford Core NLP includes one. 

EDIT: So far, most comments and one answer focus exclusively on the concept of the "marked phoneme". Let me make it clear that (1) I think this is a relatively well established concept - If you don't believe me google "markedness in phonology" and "marked phoneme". If you doubt the usefulness of this concept you're not the only one, but please let us move one from "I don't believe you" and "It's wrong", and please provide at least one academic reference why you think it's wrong or useless. (2) More importantly, I couched my question in terms of markedness because I think it's a relatively uncontroversial concept, but it's really not central to my question. I'm asking whether there are phonemes that are acquired very late or never by a substantial number of speakers, whether you want to call them marked or articulatorily complex or challenging or hard. Answers should if possible be empirically sound such as quantitative claims from surveys, personal experience from teaching, child-rearing etc., with references where appropriate. 

Carnie makes the "no negative evidence" argument here. Note that he first calls corpora "invaluable" but then proceeds to say that to "really get at what we know about" language they are not useful. In other words, he doesn't seem to consider corpora all that useful and in fact never once mentions them again in the whole textbook. 

In Accents of English (1982), John C. Wells came up with a useful notation for English vowels that allows easy comparison of the pronunciation of English vowels in varieties of this language. This notation has been widely adopted, and it is now common to talk about the STRUT vowel to refer to the vowels in cub, rub, hum. In standard British and American English this vowel is pronounced as [ʌ], but in Northern England as [ʊ]. What makes this notation extremely useful is that mergers of two vowels can be easily referred to by shorthand, such as the NEAR - SQUARE merger. Q: Has anyone suggested an equivalent notation for consonants? Such as the TOOTH consonant when referring to what in British and American English is a voiceless interdental fricative? I couldn't find anything online or in the literature. I get why the lexical set notation was first devised for vowels since English spelling is particularly inconsistent when it comes to vowels. But I feel it's still inconsistent enough for consonants to make a lexical set notation in this area useful. Q: If not, what would be criteria by which to choose keywords to represent lexical sets for consonants? 

In spoken language intonation is at least as important as syntactic mechanisms. Hector receives a pitch accent (major intonation movement) in the following sentences, where Hector is extraposed. 

I assume you're dealing with audio data. Then your problem is that phonemes in actual speech are not discrete units and there is no single right answer to the question where the boundary between a consonant and a following vowel is located. For example, in the syllable , the articulators move away from the most consonant-like position where airflow is completely blocked (lips are closed) to a fully open position at the mid-point of . The articulators need time for that, so there is a range of mid-points between the two sounds that you might want to consider as potential boundaries. When dealing with a sequence of an obstruent (a consonant such as ) and a vowel, common criteria are: 

The actor in this Youtube comedy video seems to be imitating African American Vernacular English (AAVE). I wonder how successful he is. The grammatical features seem to be pretty accurate: 

To make it more interactive, you could have your students record themselves reading the same sentence as a statement, a neutral information-seeking question, a surprised question, an incredulous question, with different emotions etc. and then ask them to compare the visualisation of their intonation using Praat - first the different readings by the same speaker, then in group work similar pronunciations by different speakers. There are slight technical challenges here, depending on your and your students' technical expertise. They need to work with Praat's recording function (or other software), and preferably use headset microphones. Also, women have higher pitch than men (plus there are differences between individuals), so the upper and lower ceiling of the the pitch tracking algorithm might need to be adjusted individually in Praat. But that should be doable with most Bachelor level or more advanced students. Your students might also compare their intonation in a foreign language they speak/are learning with that of a native speaker. This idea comes from an an article by Ulrike Gut, which unfortunately is available only in German - but I think you get the idea.