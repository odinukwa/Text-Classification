The title is asking about reviving - this would mean making a dead language a live spoken language again. To that the answer would be a qualified yes. Look for instance at modern Hebrew. But that still requires some form of speech to be preserved. The body is asking about: 

The vast majority of examples will fit into this category. For instance, the word 'bread' / 'chleba' translates straightforwardly between Czech and English when applied to loaves, however, English also uses it as a category label for which Czech has a different word. There are countless examples of this in either direction - for instance in the expression of manner of motion. Czech 'pivo' means 'lager' by default where as the (Br) English 'beer' means 'ale'. Czech does not even have a good word for 'ale' (which would go in category 2). George Bush was mocked for saying something like 'The French don't have the word for entrepreneur.' but it is entirely possible that that is the case. In Czech, 'podnikatel' is a good translation but certainly does not have all the corresponding connotations or collocations. A very common manner of misalignment is in collocation and what has been called 'semantic prosody' or connotation. The word 'volunteer' is a good example. In my work with the Peace Corps (in 15 countries), this was a word that caused the most issues - since in many of these contexts (former Soviet Union) 'volunteer' had the semantic prosody with non-optional public service - ie a negative connotation. (Community is the other one but it seems to better fit in category 2 because of the frequent loans). Sometimes, it is the cultural classification of a shared cultural reality that is misaligned. For instance, the Czech 'hluboký talíř' which is used for soups only and can either be described as 'plate' or 'bowl' in English without any of the correct cultural connotations. A similar case is 'závora' which is a type of gate that goes up and down (often not fully covering the entrance - e.g. level crossings) which simply does not belong in the category of 'gate' in Czech. Both Czech and English 'have a word for it' but its use is more ambigous in English and will leave Czech speakers confused. (Many examples in the opposite direction exist.) To give an example of a misaligned construction, I would list lack of gender reference in anaphora. The English epicine 'they' makes translation very difficult when the gender of the referent is not known. The same for genderless profession names (secretary - Czech has a different word for things like 'Secretary General, doctor). Sometimes, etymology gets in the way. E.g. the English 'nurse' has mostly lost its association with gender in the medical context, so a 'male nurse' is fine. The Czech 'sestra' (sister) did not, so no 'male nurse' expression is possible. Going the other way, articles in English specify a lot of things that Czech leaves ambigous (even though it expresses definiteness in other ways - it often just does not make it clear). All of the above (and much more) make the claim for linguistic universalism problematic - and lies behind the constructionist reluctance to look for UG-like or even Greenbergian universals. While the reality behind these differences can be described in both languages (in most language pairs), can those languages be meaningfully described as lexically or constructionally equivalent? 

Morphological complexity as such as is not related to the level of schooling. Some of the most morphologically complex languages are spoken by people without any education. So, all Russian and German speakers (including those with no formal schooling) use the morphological cases in their respective languages. So did speakers of Vulgar Latin which was really just a vernacular counterpart of Standard Latin. What you're referring to is the difference between standard and non-standard dialects which will differ in the use of cases, morphology, etc. This may be identified by speakers of the standard as a 'lack of cases'. In short, what is related to schooling is the ability to speak the standard dialect not the ability to use complex morphology as such. 

What's interesting in this context is that the original historical linguists were driven by exactly the opposite motivation. They were convinced that languages were deteriorating over time and their mission was to record the perfect state (and if possible to halt the decline). Labov describes it in his Principles of Linguistic Change 

So it is worth to always specify exactly what you mean by your terms if the distinction between these three is important. Otherwise, pretty much any of these three terms will get the general sense across. 

For English, this has mostly been done and there are a lot of online tools. I've used Photransedit which gives decent (but not 100% reliable) results. But you can also get word lists annotated with IPA such as the (which uses a machine readable format with a 1-1 correspondence to IPA (done before SAMPA). 

which I suspect would also be treated by many as grammatical (in non-prescriptivist contexts). In both of these cases, we're dealing with a coordinate construction where 'me' and 'I' are interchangeable as long as there is some cognitive shielding to the infelicitous interpretation. Note, that sentences 

Of course, none of these produce a dual meaning because 'eating' something because of loving it, does not make sense in the blend of the frames of loving a living thing and eating a living thing. Constituency has nothing to do here. Thus: 

Your two examples both show a different kind of process. In German, you remove the diacritics and retain the information that they encode. And you do it in a way that is broadly consistent with the conventions of German orthography for removing diacritics. However, in French, you simply remove the diacritics and any information that may have been encoded by them is lost. You can do the latter for any language with diacritics. Simply, remove them and use the equivalent plain ASCII characters. You will increase the amount of ambiguity in the text but not beyond what native speakers would be able to cope with. So in Czech ěščřžýáí become escrzyai. You end up with things like pani which could be either paní (Mrs) or páni (gentlemen). This is typically not a problem in context and occurs less often than you'd think. In the early days of email before unicode, speakers of languages with diacritics outside ASCII often resorted to simply writing in the ASCII equivalents (and many still do). There were also many ad hoc systems for preserving the diacritics such as e^s^c^r^z^y'a'i' (or a" for umlauts) but they never really caught on because they made the text harder to read than removing the diacritics altogether. Also, remember that no all non-ASCII characters in European languages are simply formed by adding diacritics. For instance, ß in German which would be typically replaced with ss, ø in Danish or ł in Polish. Algorithms for doing this already exist. See for instance, $URL$ and has been implemented in many contexts in software. 

I'm very skeptical about the very notion of pro-drop it is not a feature of syntax but rather of text. However, a typology of subject marking across IE languages is not a bad idea. Slavonic languages generally have rich enough verbal morphology not to need explicit subject marking. Yet, they vary in the degree they mark subject external to the verb. In a language like Czech, using the pronoun is similar to raising in English, whereas in Russian it is pretty much obligatory. 

This is a very broad question, particularly since you put systemic in Systemic Functional Linguistics/Grammar in parentheses. If you think about the broader functionalist program, you can hardly move within linguistics without encountering some aspect of it. You could say that most introduction to linguistics are written from the functionalist perspective. Most of the 'applied' disciplines of linguistics (with the notable exception of Natural Language Processing, Machine Translation and related fields) are broadly functionalist. From lexicography to language acquisition and language teaching. So are sociolinguistics and historical linguistics. Things are a bit more murky when it comes to the Systemic Functional Linguistics. Again, you can see its direct impact in a number of fields. Most of the study of text-level and discourse-level phenomena is done (more or less explicitly) within the scope of the SFL approach. For example, Critical Discourse Analysis (as formulated, for instance, by Fairclough) explicitly relies on SFG. But you could also say that much of the work done in corpus linguistics is done under the umbrella of SFL. It would also be difficult to open a modern textbook of English as a second language and not see the impact of Haliday's work. Much of grammar writing is also done in that vein. What makes things a bit more difficult is, that Systemic Functional Grammar is different from many of the other "Something Grammars" in that it does not come with a very constraining descriptive framework expressed through rules but rather a set of principles and axioms. This makes it a much more useful foundation for useful linguistic work and also slightly impervious to the sort of questions you might ask about generative-type grammars. It does not require constant tweaking of 'rules' and formal tools. As a linguist, you can just get on with things. It also does not require formal allegiance for it to be useful. I often refer to Halliday's three metafunctions of language in my own work. As is so often the case, SFL is also subject to regional divisions. Most of the work on it is done in New Zealand and Australia (where Halliday relocated) with some remnants in the UK and around the world. It is therefore not as well known by many linguists who are actually feeling its impact in much of their daily work. Often in contrast to Chomsky whose work has almost no realistic impact on the work of a student of language but is widely known and cited. SFL has a similar problem in MAK Halliday whose stature perhaps makes people reluctant to declare their paradigmatic belonging to SFL for fear of being too constrained. As to some exciting new ideas within SFL, I'm very intrigued by Michael Hoey's theory of lexical priming and John Sinclair's theory of local grammars. As to further areas of research, here you're looking at all of language. Unlike the Chomskean paradigm, SFL does not constrain language to just what can be described through formal means. This makes it much more difficult to point to places with small gaps to fill. In my own work, I try to bring construction grammar and SFL closer together in the study of discourse level construction of arguments. But you can pick any area of language that interests you and keep SFG/SFL in mind as you work on it. You're certain to encounter some unexplored areas as you continue your research. 

Computers able to analyze millions of patterns in minutes Cryptographic techniques that use what is known about patterns of a language to discover regularities in the seemingly random noise of a cipher 

The definition of parts of speech is an uneasy mixture of semantic and formal properties. So nouns and verbs can be identified largely semantically with some important reference to formal properties whereas things like prepositions are defined purely formally. That's why they are not a very good foundational category. Of course, you cannot just say something like 'nouns' are things and 'verbs' are actions, but it's actually not a bad place to start. They also have different formal properties (more in some languages than others) but when you try to look at a language to see which formal properties map to which part of speech, you start with the semantics. So if I have a pattern like 'X(N) Verbed a Y(N) with a Z(N)', I can make certain assumptions about the meanings of the blanks knowing what part of speech they are as well as how they are typically used in a pattern like this. Hope this helps. It would be useful if you could clarify a bit more what you're trying to achieve by your question. 

I would start with building a collection of linguistics blogs you follow and engage in discussions there. For example, the comment threads on Language Log $URL$ are frequently very productive, as well. While Reddit is a bit free wheeling, it's also not a bad place: $URL$ The linguistics tag on Tumblr $URL$ also often has items that generate discussion. Start Tumbling and you may soon find yourself engaged in discussions. The thing about LinguistList is that it is a collection of mailing lists. Most lists are for specific subfields and you have to find one in which you are interested. $URL$ 

There is no universal technical meaning for 'hard' and 'soft' when it comes to sounds. You will not find it used by professional phoneticians. However, within many languages, there are pedagogic conventions for describing hard and soft sounds that do not translate across languages. This is often done in an overlap with orthography. For example, in English people speak about the 'hard' and 'soft' 'c' to differentiate between /k/ in 'cup' and /s/ in cell. This is very different from Slavic languages where 'soft' and 'hard' consonants describe pairs that are palatalized or not. As in /d/ vs. /dʲ/. Sometimes, this applies to vowels. In Czech, for instance, there is a traditional distinction between 'hard y' and 'soft i' which is now only relevant to orthography - the preceding consonant is palatalized for soft i. However, this refers to a perceived difference in the darkness of the sound which is still preserved in Russian. Czechs will also describe 'ü' in German as hard y. As you can see, there's no easy way to map the idea of 'hard' and 'soft' when it comes to sounds across languages. However, there seem to be certain tendencies in how people describe sounds across cultures. Certain voiced plosives are more likely to be seen as dark or hard whereas affricates are more likely to be seen as soft or light while voiceless stops may be seen as sharp. This is a whole area of study called 'sound symbolism' or 'phonosemantics'. The way you used 'hard' and 'soft' is very idiosyncratic but in the context of a conversation, it may be understood by people. But only if there's enough context. 

This is a very hard area to research because of so many confounding variables and the fundamental difficulty of measurement of both introversion (or similar personality traits) as well as progress in learning (both reliability and validity are problematic in most measures). However, there's been no end of research on personality and SLA (second language acquisition). Most of it has used a version of MBTI (Myers-Briggs). But given all the difficulties, there is no good reliable evidence of a strong correlation between personality traits and learning success. One suggestions made was that personality interacts with other factors (such as learning context) but is not in itself a predictor of learning outcomes. A good survey of the literature is in this book by Zoltán Dörnyei. However, it's not at all clear how this should be applied in practice. The most convincing case for using personality as a part of the mix in instructional design with a clear methodology was this book by Betty Lou Leaver. 

Many languages have a little subsystem that uses a combination of particles of no*, some*, any*, every* or similar to create related question and negation words. This is what the system roughly looks like this in English: With complete sets like: 

Will AntConc and Wordsmith deal with characters not present in English (sometimes referred to as non-ASCII)? This is really a question about encoding of text (not language). AntConc supports unicode UTF-8 which means it should deal with any script. WordSmith only supports a limited subset which means that texts in non-Latin scripts will have to be converted. But none of the examples you give will present any problems. This is a relatively easy issue to resolve depending on the encoding of the texts you provide. The question about dealing with morphology is one of lemmas (which are essentialy word stems without any inflection). Both Wordsmith and AntConc support lemmas but you will have to provide a lemma list for each language. There are guides online on how to do it and a pre-made lemma list for English also exists. A good source for data for generating a lemma list for other languages are the many open source spell checks - but it will require work and coding to create those.