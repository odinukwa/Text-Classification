I think this is a poor oversimplication of Robert Lucas's maxim. It's one thing that Robert Lucas models markets as if "always in equilibrium", it's quite another Robert Lucas claims it's an empirical truth that "market is always in equilibrium". Lucas(1978) explained briefly why he chose such methodology which doesn't faithfully reflects reality: 

What Ann believes implies Ann is rational. What Ann believes Bob believes implies that Ann believes Bob is rational. What Ann believes Bob believes Ann believes implies Ann believes Bob believes Ann is rational. $\ldots \ \ldots$ 

To understand the emergence of constitution, Myerson(2008) models a scernario that a political leader gathers supports from captains in order to defeat challengers whose arrival is modelled by a Poisson process. I found using Poisson process seems to be simple and reasonable, but it is the only model I know that utilizes Poisson process.Are there any other well-known models utilizing Poisson process in a similar way? If not, what's drawback of Poisson process? 

Can we have some other implication, like, If a game admits a unique Nash equilibirum, does common knowledge of rationality implies Nash equilibirum? 

In this video (from 7: 30 to 9: 00)on Youtube, Battigalli mentions the state of world for a simple three-legged centipede game, which, in his own word, is 

I assume that $r(t)$ is continuous. The idea is that, this Poisson process with time-varying parameter $r(t)$, as the limit of Bernoulli trial with time-varying probability of success, is memoryless: For a generic finite partition $\mathscr{P}$ of $[0, T]$ as $\{[0,t_1),[t_1,t_2),[t_2,t_3) \ldots,[t_{n-1},T]\}$, let the $n$th cell following a Poisson distribution with a fixed parameter $r(t_n)$, so $$P_{\mathscr{P}} = 1 - \underbrace{e^{\sum_{k=1}^n-r(t_k)(t_k-t_{k-1})}}_{\text{Probability of no accidents}}$$. So $P = 1 - e^{-\int_0^Tr(t)dt}$. Notice that the Riemann integral $\int_0^Tr(t)dt$ is the limit of a net indexed by the set of all finite partitions of $[0,T]$ with vanishing maximal length of cell with respect to number of cells. $f(x)=1-e^{-x}$ is continuous, so this limit preserves. 

You are overthinking the definitions of inflation and deflation. There is much simpler terminology for this effect: "getting richer" and "getting poorer". I won't repeat the definition of inflation; it appears you understand what it is. But it only refers to the changing nature of prices; it doesn't account for people's ability to afford things. Regarding your assumptions: 

It won't ever happen for all players. As a counter-example, consider an engineer who invents software that can predict when a jet engine needs maintenance. It's worth millions to the airlines, who will happily pay him for the software. He has no incentive at all to release this software as open-source. The same principle applies to all sorts of specialist software. In contrast, open-source works well for common building blocks. For example, there are all sorts of companies who need an operating system for their product - cloud providers, home network kit, super computing, all sorts. While they could build their own, it is much more efficient to have a common pool, like the Linux kernel. This environment means that companies can put paid engineers on open-source projects, and still run a profitable business. 

I'm from the UK, where there is an extensive welfare system and extra investment in education in deprived areas, called the Pupil premium. Although, not nearly as much as suggested by the paper you reference. Many people do not support the pupil premium. There is a large "squeezed middle" in the UK, not poor enough for extra support, not rich enough for private schools, and this group sees extra support for deprived areas as unfair. 

Most people's main asset is their labour, and automation threatens that. Automation may reduce the value of human labour. If that happens, some people will lose out massively. While society as a whole becomes richer, these individuals will become poorer, and probably much poorer, to the point of destitution. It is not clear whether this will actually happen. The rise in automation over the past 200 years has changed but not removed the value of human labour. But the idea that automation should be feared comes from this uncertainty. If labour is devalued, governments could cause wealth redistribution with policies like a universal basic income. However, if this is not done, then enormous social problems could result. 

This question arises from reading of Sannikov and Scrzypacz (2007) and Green and Porter(1984), which are modeling the same thing, dynamic collusion under imperfect monitoring, in a continuous-time version and a discrete-time version. Their choice of different noises term are crucial to derivation of their conclusions. My question is how to decide which one is dominant for a particular market(e.g. European banana market) regarding the modeling of noises. My guess is that multiplicative noise term is more relevant to shocks of weather, additive accumulative noise term(following Brownian motion in Sannikov and Scrzypacz's case) is more relevant to shocks related to institution, say, cultural revolution and Khomeini's return to Iran. 

What's the relationship among universal type space, Aumann's semantic knowledge model and Samet's syntactic knowledge model? Here's my confusion: Regarding universal type space and the model in Aumann's Agreeing to disagree, I used to think they're really the same thing from two different angles. The type space construction is a bottom-up construction. It starts with all the payoff relevant parameters from players' point of view, and generates a set of states of world in the end. Aumann's model is top-down. The set of states of world and players' knowledge as partitions are exogenous. Interesting results are derived by imposing some consistency rules. It seems to me, there's a one-to-one correspondence between a player's type in the former and a cell( or atom, block) in the selfsame player's partition in the latter, because a player's strategy has to be measurable with respect to them in these settings. But after having a skimming of an unpublished paper by Robert Simon,The Common Prior Assumption in Belief Spaces: An Example, I found it's not the case. In that paper, he actually imposed a partition on Merten and Zamir's type space(he claimed so) for each player, so there's no such correspondence. Another thing that is weird is that it seems to me he's actually not working on on a belief space. It looks like he's working on a model in Dov Samet's Ignoring ignorance and Agreeing to Disagree, which is homeomorphic to a Cantor set, a set which is generated by assigning truth values on underlying propositions and the use of non-repeated knowledge operators of the same player. It seems to me Dov Samet's model is not homeomorphic to Merten and Zamir's type space, because we might need to add operators of belief from probability 0 to probability 1 in Dov Samet's model to do that, and the generated set of state of state of world will have a cardinality strictly larger than a Cantor set. 

The vast majority of cyber crime is fraud. The criminals don't hack the bank directly; rather they hack the customers of the bank and commit fraudulent transactions. I strongly doubt there is active collaboration. Some criminals may have insider contacts, but that's different to a symbiotic relationship. There may be an effect somewhat like you mention. Increasing security is expensive for banks, so it is only worth doing if they are experiencing a certain level of losses. In theory it would make sense for criminals to restrict their level of activity, to avoid banks implementing additional security, which would make their job harder. In practice I don't think this happens, because their are multiple cyber gangs and I strongly doubt there is a gentleman's code between then. It is theoretically possible for someone to hack the bank itself, and directly modify balances - effectively creating money out of thin air. In fact, it is thought that some major nation states (mostly US and China) have penetrated many banks around the word. However, if they started to extract money, this is likely to be noticed, due to internal checks and balances. It is more valuable for these state-sponsored actors to silently remain present, capturing information. And one day, perhaps they will strike, as part of a larger war strategy. 

There are 22,000 Vietnamese Dong to a USD, making it one of the world's least valued currency units. But denominations under 1,000 are rarely used (in tourist areas at least). They might as well say it's 22 Dong to a dollar - the zeros make no practical difference. The zeros remain because back in 1985 they revalued the Dong (1 new dong worth 10 old dong) and this led to inflation. So I understand their reluctance to repeat this. There is a general trend that poorer countries have less valuable currency units. But it's not a strict rule. Vietnam, for example, is not the poorest country in the world. Before the introduction of the Euro, the Italian Lira was a notable example of a developed economy with a low-valued currency unit. You see the locals' attitude to money expressed in the currency. While in developed countries people throw small change into jars at home, this is not the case in poorer countries. In India, coins are rarely used, and you can see from the heavily worn low-denomination notes, how these have been carefully kept inside clothes and shoes. Money is more precious with extreme poverty. I find the Thai Baht to have an especially convenient value - about 35 to a dollar. A very pleasant currency to use. Although technically subdivided into 100 satang, the satang is not much used. That makes the Western practice of pounds and pence or dollars and cents look like unnecessary complexity.