The classsification of Korean and Japanese (including Ryukyuan) has always been a focus for academic debate. There are generally speaking five camps: 

The fifth camp believes that Japanese should belong in the Sino-Tibetan family under Tibeto-Burman. This theory is supported by a camp of Japanese scholars led by Nishida, who holds that Japanese word order is similar to that of Burmese, and also has vocabulary similar to those of Burmese and southern Chinese dialects; even the pitch accent has similarities with the tonal systems of these languages. However, other people believe that since Japan is very far from both Tibet and Burma and separated by Austronesian and Tungusic language areas, this theory should be ignored. However, yet others think that according to race migration history, around the Shang and Zhou dynasties of China, some dialects of Tibeto-Burman and Austronesian mixed, and [their speakers] used their advanced seafaring skills to migrate to Korea and Kyūshū under the pressure of the Sino-Tibetan speakers (Zhou dynasty). At that time, the Tungusics have yet to set up home in the eastern Mongolian plateau, so this theory is extremely probable. 

Some Native American languages obligatorily mark sentence perspective. Aymara is one of them and the following paper provides many examples: Pragmatic Structures in Aymara 

Ditransitives of the English type are very rare. Most languages use either case marking on nouns to signify grammatical relations or polypersonal head-marking. By "English type" I mean that both objects can be passivized. Your example can be paraphrased as "A book was given to Mary by John" or "Mary was given a book by John". 

You've correctly recognized that a predicate in syntax is not what's called predicate in formal logic. As has been mentioned above, appositives can be expressed as conjunctions in formal (first-order predicate) logic. In general, what's expressed by the logical predicate is the focus of a sentence whereas the argument is the topic. For example, "Melissa dormit" would be sleep(Melissa). But if you have "Dormit Melissa", you can't have Melissa(dormit), which is nonsense; formal logicians use lambda calculus to make Melissa a predicate: λf.f(Melissa). (I used a Latin example because it has free word order in order for both syntax trees to be identical.) Another possibility would be to use Davidsonian semantics which is very flexible and doesn't make you use lambda calculus. 

I'm curious why every language I've heard of has back or central vowels. Are there any languages that exclusively uses front vowels (say the phonemes /a/, /e/, /i/, /y/)? I want to know this more or less because I'm making a simple conlang that I want to only use front vowels - would this in real life be unstable and lead to language evolution that generates back vowels? Interestingly, shouting and screaming generally involve front vowels. Is there some natural vocalization resulting in back vowel sounds, which would lead to inclusion in language? 

The fourth camp believes that Japanese and Korean are both isolates, unrelated to other languages of the world. Scholars believing in this view support it with the "lexical similarity problem". 

Clausal arity can't be expressed structurally at the surface level. I'll modify the Catalan example (I'll use llegir "to read") and add two more phrases: 

There is no consensus on how participles should be classified as for POS. Different treebanks/taggers mark them as adjectives or verbs. Some theories would classify this example as a verb as for form and an adjective as for function. It's all about terminology. In any case, the question in the test was unclear. 

Applicable to what? Meaning assembly? For this, λ-calculus is used only in categorical grammars, which are binary-branching by definition. In syntactic frameworks with a context-free backbone, glue semantics (based on linear logic) is used at times, but it has problems, too. Meaning assembly via composition rules is best done using unification. If one uses Davidsonian (or neo-Davidsonian, i.e., Parsonsian) logical forms, every phrase (including preterminals) is associated with an LF fragment and an individual. If you have a ternary (or, in general, a nonbinary) rule, the individual of a subordinated phrase is unified with a variable in the LF fragment of the head. For example, the predicate of "give" is quaternary and the corresponding rule is VP -> V NP NP. Then the individual associated with V (the eventuality) is unified with that of VP. The individual associated with the indirect object is unified with the fourth argument of the predicate of the verb, etc. The subject variable remains open until it's unified later by another rule. Since (neo-)Davidsonian formulae are existentially closed conjunctions of literals, when the parsing is completed we take all literals used during parsing to be conjuncts and add a quantifier for each variable occurring in the LF. The above procedure can be informally described as a "relaxed" lambda calculus, but rather than replace variables we unify them. It can also be used in dependency grammars if we add unifications to ID rules. 

It seems like Indo-European languages are always stuck between throwing away complicated fusional grammar (like English) or retaining most of it (like Russian). Are there any Indo-European languages that reanalyzed the verb system into agglutination with series of suffixes rather than trying to stuff complex tense, aspect, and person information into a single arbitrarily chosen suffix? Or is it generally a rule that languages evolve away from agglutination and never towards it? It seems, for example, that Modern Japanese is a bit more fusional rather than agglutinative due to certain sound changes - for example in Classical Japanese "ikitari" went simplifying into "ikita" (which prevented further addition of affixes since -ta is not a verb citation form ending) and then fused into "itta", which obscures the root "ik", thus becoming somewhat fusional. 

It actually came form Late Latin (e.g., probatum habeo). It's a natural process, a similar construction with "to have" has developed, for example, in Northwest Russian which is very interesting because Russian has no corresponding verb so it had to resort to its "у+NP" construction (e.g., у него корова подоено "he has milked a/the cow", lit. "at him cow milked") which clearly shows the origin of this tense is semantic. 

Dependency parsing is constraint solving. I recommend you have a look at XDG, which is the only formally precise dependency grammar approach I'm aware of. 

You'll need a parser to identify objects automatically. There are a few online interfaces to natural language parsers. For example, you could use the Stanford parser (link) and look for "dobj" to find direct objects. 

Liquids do not appear in the first syllable of native words Vowel harmony Agglutination Conjugation for person (however, J/K does not have this one) 

[A typical Altaic languages] such as Tungusic (Nanai) Mini bithe-i "My book" Bi mini bithe-we-i hvla-i "I'm reading my book". (note: incomplete sentence in original) bithe-we-i's "we" is a patientive suffix, while "i" is a ferst person genitive (?) suffix. hvla-i's "i" is a first person conjugation for the verb. The remaining three characteristics do belong to Japanese, giving this theory significant support. However, Japanese shares very little vocabulary with other Altaic languages, so scholars against this viewpoint take this as strong evidence against this view. 

The sentence will be a CP in traditional GG: [CP [C have] [IP I seen the man]]. Note that constituent trees represent sentence topology rather than grammatical relations. In the example at hand, "have" and "seen" are coheads but they don't appear contiguously in the sentence. 

I suppose you mean a rule-based parser since nobody would think of developing his own statistical parser (there are so many good open-source libraries). Building a parser is quite complicated. The best way is to have a context-free grammar (CF parsing is trivial) and build up the dependency structures via constraint rules. This is how LFG works, whose f-structures are just plain old dependency trees (in general they are DAGs but can be thought of as trees with coreferences). If you don't want a context-free backbone in the parser (which doesn't make much sense for most Indo-European languages), you should devise syntax rules based on feature constraints. The dependency tree of a phrase or sentence is a rooted spanning tree over a graph whose nodes are the words of the phrase with edges representing possible dependencies that conform to the constraint rules. In Latin, for example, one would say that an adjective depends on a noun if they agree in case, gender, and number, such as puella pulchra (nominative), puellam pulchram (accusative), etc. Likewise, verb phrases would be constructed via constraints. In a sentence like tu pecuniam debes, the constraining rules must state that the subject of a verb is in nominative and its direct object in accusative. But note that in most languages you'd need a ton of word order rules (that's why it's better to use a context-free backbone). Moreover a good parser needs a lexicon with valency frames to resolve ambiguous structures. A simple parser can be quickly developed in Prolog. 

The first camp believes that Korean and Japanese do belong to this [Altaic] family, which could only be true if the Macro-Altaic theory is true, because Korean and Japanese have some Altaic characteristics. (note: this sentence is equally awkward in the original) 

The second camp believes Japanese and Korean belong to a new [i.e. separate] language family. Scholars believing in this theory think that Japanese grammar is shockingly similar to that of Korean - both use SOV - and both have had strong influence from Old Chinese. Thus, some people believe that Japanese and Korean is a mixed language family arising from the creolization of Sino-Tibetan and Altaic. However, the lack of common vocabulary between Japanese and Korean also becomes a arguing point for dissenters. 

In surface syntax it's "John - seemed". In deep syntax it depends on the theory. "John" would depend on "fall" but sometimes there are two heads and one would say that the subject of "seem" is athematic, i.e., it has no thematic role. XDG does it this way, for example. 

In Guarani, the derivational suffix -kue means former, ex-. It's also used in Jopara, the mix of Guarani and Spanish. 

The idea is that deep syntax structures should be as language independent as possible. In your sentence "will" is only a tense carrier and "to", though being a semantic preposition, functions as a case marker (illative). From a graph-theoretic perspective, to translate a surface tree into a deep dag is to remove all the nodes that represent function words from the graph. Thus at the level of deep syntax a sentence has the same structure in all languages unless there's a lexico-semantic mismatch (in which case the edge labels will be different). In grammars with feature structures, deep structures are constraint-based. In LFG, for example, c-structures are phrase structure trees but f-structures are isomorphic to (deep) dependency dags (if one substitutes thematic roles for grammatical functions). From a logical perspectives, deep syntax dags are "hierarchical logical forms". Lambda calculus could be used to incrementally assemble the logical form and there's a one-to-one relationship between logical forms and dags. Also worth mentioning is information structure which is sometimes added to dags (though it's not part of syntax sensu stricto). It may be expressed as an ordering on nodes (as in FGD). Note that to fully account for syntax one needs a LP component (such as topological trees) which means that surface dependency trees are uninformative since they can be reconstructed from structures at the other levels. They can be view as a blend of topological trees and deep syntax dags. Surface trees tell us little but are structurally closer to the phonetic form of the sentece. Update: A few definitions might be useful: