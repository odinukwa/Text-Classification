No, google apps for business is run from the same infrastructure as the rest of google apps. There are some that run on a separate infrastructure (early adopters, I believe), but these are being transitioned back into the main infrastructure. You can tell if a given google apps account is being run from the core infrastructure by trying to load one of the apps email accounts alongside a regular gmail account on the same browser session. If you can open both at once, they're on separate infrastructure. If you can only open one at a time, they're in the same infrastructure. 

If you're applying the script as part of the computer startup, you need to grant access to the computer object on the share (you can add a computer to a group just like a user account, as they are both valid objects that can authenticate on the domain). If you're running the script at user logon, it runs under the user's own security context, so you users will need to be able to access the remote location you host the file from, and have access to the local folder on the machine that you want to copy to. If you can't grant your user access to both of those locations, it's usually preferable to run the copy from the machine startup script, instead of the user logon. 

There's a group policy setting that explicitly sets this, located here: Computer Configuration -> Administrative Templates -> Windows Components -> Terminal Services -> "Allow users to connect remotely via Terminal Services" To fix the issue, you need to unset (not 'disable') this on the GPO. After a policy refresh you should get back the ability to control logon availability. 

Some OUs may have 'block inheritence' configured, which will prevent higher GPOs from being applied. On the face of it, this seems the most likely cause. You can find the setting under Group Policy Management, as shown here: GP Block Inheritence $URL$ You should be able to evaluate the Resultant Set of Policy for particular user/computer combinations by using the tools provided in the Group Policy Management MMC console. There's more information on this here: $URL$ This is where you find the wizard you need: GP Modeling Wizard $URL$ 

There were a number of changes to the file sharing system in 2008/Vista which may be causing you issues. 

If you backup server cannot acquire the data from the network fast enough, you're left with slack space all over your tape. This can add up to a significant loss of space on the tape. Investigate the throughput you're getting and whether there are any bottlenecks you can resolve. Also ensure that you have selected hardware compression on the job. The LTO3 tapes are 400Gb raw but are listed as 800Gb under ideal scenarios with compression. Depending on the data you're putting on the tape, you might get a compression ratio better than or significantly worse than 2:1 If you're struggling to get sufficient throughput from your client servers, you can look at shutting down some services during the backup period. For example, most file servers will back up far faster if the antivirus on-access scanner is disabled during backup. In my environment, use the pre & post commands in backup exec to send 'net stop avservice' and 'net start avservice' before/after a backupjob. You can do likewise to shut down other services depending on the server type. For example, a lotus domino server can be backed up much faster if you stop the LotusDominoServer service... this is often acceptable if a backup runs overnight. Another alternative if you have the resources is to stage your backups to a backup-to-disk volume, then later stage it off to the tapes. 

I'd recommend plugging it into the network with the network socket, allocating it a fixed IP, then creating a print queue on the server that points to the printer. Clients can then connect to the queue on the server (via a nice friendly network name), and they'll have the correct drivers served automatically (assuming this is a windows server). You'll be able to delegate permissions for the printer via the server which means being able to clear out jobs, etc. Using the ethernet connection on the printer means it's not tied in proximity to the server, and it's generally a more robust way of hanging it all together compared to USB. 

Microsoft had a software solution for this very purpose, Windows SteadyState. In Windows 7 it was rebranded 'Guest Mode' and was supposed to ship with the OS (yay) then got pulled (boo). It think the easiest, low-risk solution for you is to boot a Linux LiveCD and launch Firefox from there. If you still want a functional OS to boot into as a power user, use the BIOS to put a password on the hard drive, and boot from CD by default. Then, you can get onto Win7 any time you like by going to the boot menu and putting in the password. Here's a list: $URL$ PS if your BIOS doesn't have a HDD password option, you can truecrypt the drive and that'll have the same effect. 

Vendor support can be purchased for OpenFiler, and your hardware, if your company insists on these things. You can also configure multiple OpenFiler hosts with DRDB to give you high availability and redundancy options. We've run one of these in production for over a year now, and it's been very solid. 

The most reliable method of maintaining time in a virtualized domain environment is to keep your PDC physical, increase the sync frequency and drift tolerance on all your virtualized DC and domain members, and disable host-guest time synchronization for all VMs in question. Here's a pseudo-explanation of the problem (it's been a long time since I looked at this): When a machine (workstation, server, VM) starts up, it reads the time from the RTC (battery backed clock chip on the motherboard/BIOS), and works out the duration of each CPU tick. The OS then counts the number of clock ticks that have occurred since the initial reading was taken, and adds the time from that to the original time reading taken at boot. This gives you the current time. Problem is, hosts obfuscate the true clock cycles occurring from the VMs. A VM may have seen 100 clock cycles when 500 clock cycles have actually occurred on the host. So this method of calculating time breaks down, and time drifts out of whack on the VM. Host-Guest time synchronization via the installed vm tools/enhancements packages on vSphere and Hyper-V go some way to curing this, but they're not perfect (in some setups they can pull a VM forwards if it's drifted behind realtime, but they don't jump a VM forwards if it's drifted ahead of realtime). This complicated further by the way clock cycles are counted on multi-core setups (the timing counter is essentially emulated on each core) and on setups that can change clock speeds on the fly (I have no freaking idea how this is maintained). Factor in the idea that a VM can execute one clock cycle on one core, then jump to a different core on a different CPU for the next cycle, and it gets really horrible. So back to the original point: Domain time by default starts at the PDC, then trickles down to the other DCs, then out to the member servers and workstations from there. So if you ensure your PDC is a truly reliable time source (by keeping it physical), and disable host-guest sync on all other domain members, you'll ensure a stable and relatively accurate time infrastructure. Note that running your PDC as a physical server then enabling Hyper-V on that server and adding some guests to it is probably not a suitable fix either, as I believe that when you enable Hyper-V the 'base' OS actually becomes a virtualised OS as well (silently). So keep one physical server as a PDC, and keep Hyper-V off the box. Interesting side point to note: Microsoft's official stance on Windows Time Synchronization, even using the compliant NTP service that's been built into Windows since XP/2003, is 15 seconds. In practice, you can get it down to sub-100ms, but all they'll support is synchronization to within 15 seconds. Kinda makes sense, the only time-sensitive key component at the core of most MS environments is Kerberos, and by default that'll operate happily as long as you're within a 5 minute tolerance. 

You kinda get this, but only on paper. There's no advantage in doing it as your storage isn't really redundant and your performance can't be compared to a good RAID card. The only scenario where you should look to do this is for training/lab purposes, as it gives you something to mess around with as a storage area network. But for actually running stuff, you're better off serving the data straight up on Hyper-V to the guests. 

Assuming your requirements are low enough that none of the above is an issue, the final point to consider (IMO quite carefully) is the reflection it makes on yourself (and if you have one, your department) if you go ahead with this. While IT isn't always client-facing in the usual business sense, it's effectively a service to the rest of the business. How's the business take on running on unsupported configurations for these systems? How's it going to reflect on you professionally? IMO the only time a hokey system like this may be appropriate is in an extremely small business where there's simply no cash and you're trying to bootstrap the whole thing. I'm talking mom-and-pop's flower shop and their kid is keeping their workstations, website and internet connection going. Nowadays, for anything larger than that, you'd be better cobbling together a few cloud-hosted services (dropbox/skydrive, gmail/yahoo) simply because they'll offer small-scale solutions with a solid infrastructure you don't even have to think about. Of course all this is nonsense if you're just playing around with some servers at home. In which case some old laptops sound ideal.