There is a built in task for sending SMTP mail in SSIS. If you want to use a script task to send mail via SMTP the post below lists several ways to do what you are looking to do. $URL$ 

@@identity is not a good solution and will return incorrect results since . SCOPE_IDENTITY() is session based and will return the last inserted record for the session. There isn't a way to correct this without a code change. 

One solution, albeit kinda sucky, is the convert the Agent jobs from SSIS tasks to CMDExec tasks and then call DTExec directly. If you do this you can set the environment variable in the batch file before the package is executed. 

General rules for deployment would have kinks worked out when deploying to a TEST, QA or DEV system so there are no issues with the PROD deployment. If you are already running a cluster you can add an instance to the cluster at no licensing cost and use that as your TEST environment to iron out the deployment process. If the deployment to TEST goes south then you simply restore the database from the PROD backup, change the scripts and try again. 

The MSX/TSX is indeed related to the SQL Agent, allowing you to schedule jobs on a master server and have those jobs pushed out to slave servers. CMS is an extension on the registered servers concept and allows you register servers into the MSDB database of a single instance through the SSMS GUI. Once the servers are registered in the CMS you can logically group them by business unit, version, etc. Once you have them registered any DBA/user with permission to the instance can connect to the CMS and see the same grouping. CMS does not allow you to schedule agent jobs like MSX does, but you are able to run a script through SSMS on servers grouped in the same folder in CMS. For example, if you highligh a subfolder in CMS and hit New Query it will connect to all the instances registered in that folder and run the script on each one. Below is an example script you could run; CMS will return the instance name and the user running the script as columns by default. You can also use CMS for grouping Policy Based Management evaluations and policies. 

This is not possible using SSRS, but you can use powershell to read from CMS and then use the SQLCMD cmdlet to execute queries. 

Your steps to do this are pretty much right, I've added some steps and changed the wording on a few items. Some SAN vendors provide migration software so you only have to turn off the cluster nodes for a few minutes and they will migrate the data behind the scenes. You take a breif outage and a performance hit until the migration is complete, but you don't have to manually copy the data over or have to change the drive letters. It's worth looking into for having 13 instances. 

In order to take advantage of mirroring you have to have the database in FULL recovery mode and will need to take transaction log backups in order to keep the log file from growing. If you don't need the log backups then just purge them after x amount of hours with a maintenance plan, but they need to be taken. To clean up the environment you should remove the mirroring, switch the recovery mode to simple, get the log file size down via the recommended Paul Randal Way, switch back to full recovery mode, set up full and log backups, then re-initialize your mirror. You can try to get the log size down while the mirroring is in place but it will be way easier to remove it first. 1 GB shouldn't be too bad of a db to re-initialize. 

Here is the script it calls, in this example the script will enable remote errors on the SSRS instance. There 

If you want to restore a differential backup on top of a full backup you will need to change the options for the full backup restore so that it does not recover (WITH NORECOVERY) the database. This will leave the database in a restoring state and then you should be able to apply the differential backup and use the WITH RECOVERY option to recover it afterwards. 

If you're okay with using a script through SSMS the script below will search through user tables for a certain string. In a system with a lot of tables and columns it will take a long time. :) 

Local temporary tables should not have this problem, but if you want to ensure that the table creation doesn't cause a DDL conflict then just put a conditional drop statement at the top of the statement, see below. 

There is an undocumented procedure that will do this, code sample below with your query. Brad's answer is safer because undocumented procedures can change without notice. 

Data Driven Subscriptions is a feature that is only available in SQL Developer or Enterprise Edition. 

I think you need to specify an "/en" option in order to change the encryption level. See this link. $URL$ 

You can generate a script file and run this from SSMS in SQLCMD mode. Since you are just kicking off an asynchronous job the command should run quickly. Pretty much the same answer as @mrdenny though. :) 

If this is installed on a windows 2008 server then you will need to check that the firewall is either disabled (not recommened) or that there is a firewall rule for the SQL port (1433 for default instance, 1434 and whatever is configured for a named instance). 

Once the database is detached you will probably not be able to attach the data files. Since the file operation that occurred is an almost guaranteed data corruption I think your best option here is to perform a restore from a native SQL server backup or export of some kind, or recover any snapshots you might have taken of the environment. 

The schema change it taking so long because you are assigning a default value to the column during the change and enforcing that with a non-nullable column, and it has to populate the column for 60+ million rows, which is an incredibly expensive operation. I'm not sure what your application requirements are but an approach that would make the schema change faster is to add it in as a nullable column with no default value and then perform an update in batches to assign 0 as the value for the column. After your update are done then you can apply another schema change to change the column to non-nullable and assign the default value. 

Backup your primary database and restore that to your mirror instance, then backup your primary log and restore that to the mirror instance, both with norecovery. After that you need to configure mirroring (with or without a witness) netween the primary and the mirror. After the mirroring has been configured you can force a failover by running "alter database mydb set partner failover" on the primary. Once the database fails over the old primary should show the database as "restoring" and the old mirror should show the database as accessible. To fail the database back to the old primary run the same statement on the mirror. You can not "blow away" the witness if you want to have automatic failvoer, but if you don't want automatic failover then you don't need a witness anyway. 

A default instance install (instance name of MSSQLSERVER and on port 1433) will disable the SQL Browser service because 1433 is assumed. On a clustered SQL install the cluster service will try to connect tot he instance and run a "select @@servername" in order to show that the server is up, but because it's a default instance it tries to use 1433 by default. I've tried to re-enable the SQL Browser service after changing the port so that the mapping of instance:port happens but haven't had any luck. What you can try is to set up a client side alias (using SQL Server Configuration Manager) on each node with the virtual SQL name of your default install (the network name) and then put the port number in the alias so that SQLVIRTUAL points to SQLVIRTUAL,3876 or whatever port you are trying to change to. This way when the cluster service tries to poll the instance to see if it's up on 1433 it will be redirected to the actual port by the alias. You will not be able to use a dynamic port for this. 

This is sort of possible using the SQL 2012 AlwaysOn feature, which allows you to create a cluster between two nodes that do not share storage. The setup uses database mirroring behind the scenes to replicate the data to the remote nodes storage but I don't think automatic failover is possible, would have to be kicked off by a user or process. 

Try putting a GO on a newline between your procedure DDL and your grant statement and after the grant. 

When you switch the recovery mode to shrink the log file you are invalidating prior backups because it breaks the log chain. What you need to do it switch it to simple, shrink it down to what you want, then switch it back and set up regular log backups (I would recommend hourly) in addition to full backups. When a log backup is performed the log is truncated (note, not shrunk. it will remain the same size on disk) and then the database will be free to re-use the space at the beginning of the file instead of appending to the end of the log file and causing file growth. 

You should use database mirroring to replicate the database to the new server, then fail it over (alter database mydb set partner failover) and break the mirror (alter database mydb set partner off) when you are ready to make your DNS change since mirroring FROM 2008 TO 2005 will not work. The fail over for database takes around 5-30 seconds and the roll back plan would be to recover the database on the 2005 server (restore database mydb with recovery) and then re-point DNS back to the original server. 

Look into rs.exe (installed with SQL reporting services configuration). Is uses a vbscript syntax but it will allow you to do data source updates with command line/script. $URL$ Here is an example of the call to RS.EXE that you could put in a cmd or bat file. If there is only one instance of SSRS and it is in the default instance then exclude the "_instancename" from the URL.