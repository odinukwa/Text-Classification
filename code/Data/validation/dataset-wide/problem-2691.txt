There is a parallel between what W says in the Tractatus about ethics and what he says about logic. A central doctrine of the Tractatus is the idea that some things cannot be said but can be shown. The metaphysics of the Tractatus is like this, and you can approach what W is saying about ethics through his conception of logic. Nowadays people like to talk about two readings of the Tractatus, standard and resolute or however you like to call it. In the standard interpretation, the propositions of the Tractatus are intended to show something about logic and ethics which cannot be said. On the resolute interpretation, the whole work is ironic nonsense. Both perspectives agree that W is saying something about philosophy, and a certain philosophical conception of logic (and ethics) is being abandoned. Just as for W logic takes care of itself and ordinary language is in good logical order, so it may be he thinks it is possible to make ethical statements in an ordinary, non-philosophical way. The logical positivists who were heavily inspired by the Tractatus read it differently, analyzing ethical statements empirically. The best known positivist interpretation of ethical statements influenced by the Tractatus is that of A.J. Ayer who explained ethical statements in terms of emotivism, the doctrine that they do not express propositions but emotional attitudes. 

The idea that you cannot know what another person is thinking (a form of the so-called 'problem of other minds') is certainly unusual in Nyingma philosophy and almost certainly a local idiosyncrasy among Nepalese Nyingmapas. The main Nyingma philosophical writers are Rongzom (1012–1088), Longchenpa (1308-1364), Jamgon Ju Mipham (1846–1912) and Shenga (1871-1927). These writers for the most part held to the traditional madhyamaka views as expounded in India (especially Shenga) where there is no major theme that there is some problem in knowing the minds of others. Rongzom's writings are mostly lost now but there is no evidence he held such a theory. This is basically true of Tibetan Buddhism as a whole, where the problem of 'knowing other minds' and the thoughts (rtog pa) and intentions (bsam pa) of others is barely discussed. In many Buddhist texts, not unique to the Nyingma or Tibet, it is suggested that Buddhas know the thoughts of others. But the extent to which the minds of others are obscured to ordinary people is nowhere extensively discussed, and it is reasonable to assume minds are opaque just to the extent that, as is obvious, rational agents can hide their motivations or choose not to reveal their thoughts. But it is strange indeed to assume this is the norm, as Ortner seems to imply. In general, we have to assume we 'know' the reasons others have for acting all the time in order to make sense of their behaviour. It is only by doing so that we may consider them to be carrying out actions, whether verbal or physical, rather than behaving mechanically. e.g. See Davidson's 'Three Varieties of Knowledge' for a good discussion. If we did not get the motivations of others right most of the time, it is hard to see how we could have a coherent conception of a reason or motivation at all. The idea that the reasons of others cannot be known is nowadays widely regarded as philosophically idle in the West, like the idea that we cannot know the external world. It sets the standard of knowledge impossibly high and opens up the doors of global skepticism and solipsism. 

As in so many matters, a clarifying answer to precisely this question is given by Wittgenstein. Belief, like so many psychological attitudes, does not have genuine duration. It is more akin to an ability or a disposition than a state. For example, you car has 300 Horse Power. It has this regardless of whether it is moving or not. Its power is not a state but an ability. Understanding, belief and knowledge are similar. 

The Principles of Mathematics, section 434 I would add that when you say Kant 'showed' mathematics is synthetic a priori, you seem to imply this was definitively done, but Kant's, Frege's and Russell's conceptions of mathematics and logic have been disputed by Quine, Wittgenstein and others. 

It is important to understand with Wittgenstein that he has a particular understanding of 'explanation' which differs from that most commonly found in analytic philosophy. Commonly, 'explanations' are given by formal analysis - most typically, in recent decades, giving the structure of sentences or propositions in terms of a calculus (this holds true of logical positivism and truth conditional semantics, the two main strands of linguistic analysis in the 20th century). Wittgenstein, by contrast, sees such activities as providing not inner structures but representations of language - he stresses that there is no single true representation and nor need there be. A 'proposition' is a 'family resemblance' concept and has no single structure or essence. Explanation of language, for Wittgenstein, is given through showing the underlying grammar (which differs from the superficial or 'surface grammar') of sentences. Grammar, in this sense, is determined by how the sentence is really used (not only by philosophers!), and what connections it has with other sentences. It would be mistaken, on such an account, to think there is a grammar of thinking. If one wishes to understand what thinking is, one could look to the uses of the word 'thinking' and its cognates. Philosophy has no other means of explaining a term. If there is some other discipline, a science or empricial psychology, which can explicate thinking and any 'structure' it may have, then that is fine, but not the domain of Wittgenstein's investigations and not the domain of philosophy. 

The canonical figures are who they are mainly in virtue of their extensive influence on the tradition, so an alternative canon can only be of minority interest. It is a separate question whether there are any important figures in the history of philosophy which have been overlooked. I think Wittgenstein, while certainly not overlooked, has been widely misunderstood. He is frequently interpreted as belonging to one of the comfortable and familiar categories of philosophy ('logical behaviourism', 'antirealism', 'assertion conditions semantics') while in fact his views are misrepresented by such labels and his philosophy is radical and visionary. I would like to see serious Wittgenstein scholarship back on the agenda and at the centre of the Western canon. 

Wittgenstein's early and later view of logic entails that there cannot be multiple logics. A calculus is a means of transforming symbols, and there can be as many of these as one has transformation rules. Logic however, in Wittgenstein's sense of the word, is different. He regarded propositions as bipolar (Tractatus) and, later, bivalent (Investigations). He rejected Brouwer's view that there are propositions with no truth value and Lukasiewicz's multi-valued logics (in the 1932-1935 Cambridge lectures). Their systems are calculi which specify alternative transformation rules, but these are not the rules of language and not, therefore, the rules of logic in the required sense. A language can be governed by these rules only by altering what it means to be a proposition, and therefore what it means to have a language. Logic, proposition and language are all internally related concepts. 

Formalization of philosophy was a primary goal of Russell, the early Wittgenstein and the logical positivists. They applied formal systems of logic to a wide range of philosophical propositions and arguments and on the whole this led them in the direction of logical empiricism, a form of reductionism of sentences to a 'scientific' vocabulary (indeed, the logical positivists regarded themselves as scientists, not philosophers). From the 1940's onwards this approach to philosophy was widely abandoned as hopeless. Reductionism simply cannot be carried out as has been shown by Quine, Sellars, Hempel etc. The tradition of the formalization of language continues however, in the works of Tarski, Quine and Davidson and their successors, and one may say some kind of progress has been made in formalizing large parts of natural language. However formalization does not lead to universal agreement. For how does one choose the 'correct' formalization among different logics, models and notations? Formalization is as underdetermined as any scientific theory in need of evidence. Hence the later Wittgenstein and others such as Austin and Ryle argued that formalization cannot be the ultimate goal of philosophy. Rather, clarification and perspicuity are its ongoing and legitimate concern. 

The key difference in the eyes of Deutsch between himself and philosophers from Leibniz to Lewis is that he claims to derive his theory from the a posteriori evidence of Quantum Mechanics. Norris argues (convincingly in my view) that nevertheless, Deutsch does rely on a priori arguments and hence his theory has an essentially metaphysical component. So it comes down to the question of how we distinguish conceptual from evidential arguments that determines whether the multiverse belongs to science or metaphysics/philosophy. 

I think this is a VERY interest question that gets to the heart of what logical positivism, and empiricism in general, are really about. I remember Peter Hacker in his book 'Wittgenstein's Place in 20th Century Philosophy' explaining the Marxist leanings of some (but not all) of the leading positivists. Logical positivism was a epistemic reductive movement with a tendency to elevate the status of science culturally and strongly reject metaphysics/religion. In this respect logical positivism broadly belongs to empiricism. Ever since Hume, empiricism has had a strong connection with atheism, the rejection of religion, and the promotion of science in its place. Logical positivists tended to continue in this tradition. To some positivists, Marxism probably seemed more compatible with their empiricist dogmas than the free market based political systems that most Western democracies had implemented in the early 20th century. There was certainly a strong cultural match between the progressivism towards science inherent in logical positivism and the cultural progressivism of Marxist idealology. The positivist manifesto seems to be influenced by the earlier Communist Manifesto, for example. I think it is reasonable to say that to this day there persists a general cultural connection between scientific materialism (the successor to positivist scientism), atheism, and Marxist/socialist ideologies. But this is my own humble opinion which needs further argument. 

The answer to the main question asked is trivially yes; Russell was well aware of Kant's views on mathematics and was influenced by them. Kant, Frege and many others were forerunners to Russell's views on mathematics in a very general sense. The answer to the more interesting question in the body of the text - whether Russell's conception of mathematics is analytic - is definitely no. Russell held that mathematics and logic are both synthetic. Kant on the other hand held that logic is separate from mathematics; logic is analytic and mathematics is synthetic. As Russell says: 

Sam Harris makes a similar, confused point in some of his speeches. The 'experiment' you describe is, like most 'thought experiments', not an experiment at all, but a conceptual, a priori argument. If it was an experiment, it could be conducted empirically, the results observed, and conclusions drawn. In fact, this does not occur. Rather, you are setting up or presuming a conceptual equivalence between free-will and the ability to choose what one will be thinking about in the future. This is really a re-definition of what 'free will' normally means. The customary, layman's meaning of free will is an ability to make choices per se. So just because you sometimes cannot choose what to think about, does not by any means imply there is no free will whatsoever. 

Saying 'computers can think', meaning: thinking is really just mechanical. Or else, meaning: computers are becoming really sophisticated, having near human behaviour. Neither of these things is really true. Thinking, thoughtfulness is by definition not mechanical. And no matter how sophisticated or advanced we consider computer behaviour, can you even count how many human behaviours a computer lacks? Asking whether computers think is not an empirical question, but a question about what we will count as 'thinking'. 

Since you raise the question in terms of what makes sense, I think you are looking for a meaning theory as opposed to an ethical theory. Pragmatist theories of meaning are those which derive meaning from the consequences or commitments of assertions. Those consequences and commitments can themselves be linguistic (in which case they are inferential commitments) or can be commitments to non-linguistic actions. Meaning is constituted by these. Michael Dummett first identified the possibility of a pragmatic theory of meaning in his paper 'Truth and Meaning'. He contrasts the theory against a verificationist theory of meaning, a theory which derives meaning from what would be required to justify an assertion. The two are actually compatible, as long as they are in harmony in terms of the meanings that each one determines. Harmony is a desideratum of such theories. The problem with verificationist theories of meaning as they existed in positivism and operationalism was that they were inherently reductionist - verification was taken to be reducible to sensory or scientific terminology (what Carnap called 'protocol statements'), and such reduction is now thought impossible. More recent verificationist theories of meaning make no such attempt at reduction but define verifications in broader epistemological terms. Carnap et. al. wanted to show that metaphysical statements, like the ones about free will, are meaningless because they are unverifiable. The more recent verificationist and pragmatist theories explored by Dummett et. al. are inherently antirealist, Dummett argues, because they make truth dependent upon an act of verification or on the outcomes of assertions, i.e. dependent on human knowledge. Metaphysics now turns out to be a consequence of the theory of meaning, not meaningless. Dummett argues this at length in his brilliant book The Logical Basis of Metaphysics. In favour of pragmatist and verificationist theories of meaning, Dummett argues that other theories which make truth independent of human knowledge cannot explain what knowledge of language (and meaning) consists in. Against them, their antirealism suggests that the past is only as real as the imprints it leaves upon present observers and language users. Both positions are extremely problematic. Pragmatist theories of meaning are not to be confused with pragmatist theories of truth such as those proposed by William James and Richard Rorty. 

$URL$ "Kant characterized his critical philosophy as effecting a Copernican revolution. All previous philosophy had insisted that the mind must conform to the nature of things; but Kant suggested that phenomenal things [things as experienced] must conform to the nature of the human mind. In particular, phenomenal objects must conform to the ‘synthetic a priori principles’ which are conditions of the possibility of experience. Synthetic a priori principles are conceived to be propositions that are both necessary, and true of physical reality. Wittgenstein’s account of the nature of necessary propositions effects a second Copernican revolution. He argued that a priori propositions appear to describe how things necessarily are, but in fact they do no such thing. The necessary truths of logic are vacuous tautologies that one and all say nothing at all. But each such tautology is internally related to an inference rule. Other necessary truths, that appear to fit the category of the synthetic a priori in that they are necessary but not analytic – such as ‘nothing can be red and green all over simultaneously’ or ‘red is more like orange than it is like yellow’ are what he called ‘grammatical propositions’. They are, in effect, norms of representation – rules for the use of the constituent terms in the guise of descriptions. That red is more like orange than it is like yellow is actually no more than an inference rule: it entitles one to infer that if A is red, B orange and C yellow, then A is more like B in colour than it is like C. And these internal relations are partly constitutive of what it is to be red, or orange, or yellow. Similarly, the principle that every event has a cause is part of the grammar of ‘event’ in certain physical systems, eg Newtonian mechanics – it is a ‘norm of representation’. That every event is spatio-temporally related to every other event is not a description of a physical fact, nor yet of a metaphysical fact, but a ‘grammatical’ truth partly definitive of the term ‘event’: we would not count something as a genuine event if it were not simultaneous with, antecedent to, or subsequent to any other events you care to mention. We are prone to take such grammatical propositions to be descriptions of objective necessities in the world. That is indeed what they seem to be. But they seem so only because these grammatical propositions that are expressions of rules or conventions of representation, cast a shadow upon the world, and we confuse the shadow for a reality. I think Wittgenstein did more to demystify the nature of necessity than any other thinker."