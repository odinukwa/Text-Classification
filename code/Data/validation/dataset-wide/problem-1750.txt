Ended up using Kernel for Exchange server from Nucleus Technologies to export all the soft deleted mailboxes out to PST. Given the time constraints we had to get mail flowing it was decided to hurry up a scheduled google apps migration. While I was less than impressed with Google's import tools all in all the switch went well. Not exactly a pretty scenario but all is well in the end. 

I am having a very strange issue that I have been unable to nail down. When I send to a particular address with Outlook Web Access 2003, I get the NDR message shown below. When I email that same user the exact same email from Outlook 2007 through the same exchange server, I have no bounceback.The user's address is external to our mail server, completely different company. Reporting-MTA: dns;xxx.corp Final-Recipient: rfc822;gina@bxxx.com Action: failed Status: 5.7.1 Diagnostic-Code: smtp;554 5.7.1 The message from () with the subject of (Re: Wacom Tab/ Signature Pad) matches a profile the Internet community may consider spam. Please revise your message before resending. Does anyone have any idea what would make OWA2003 fail, yet the same message sent through the same server succeed? Thanks in Advance! -Nate 

I am looking for a way to use USB key on a guest OS running on a 5.6 Xen Server environment. The catch is that I need to actually detect in the Guest OS (Win2003 Server) like an actual USB Key. Attaching it as a storage drive doesnt work (It is a key with special attributes that servers as a licensing mechanism). Just wondering if anyone has had a similar need and found a good solution? Thanks, Nate 

We used the network usb hub for a couple of VM scenarios. We also found that you can map the USB controller from the physical machine directly to the operating system (windows server 2003 in our cases). This requires figuring out exactly which hub controller is using what usb ports but it was a great workaround for software using a USB HASP key. 

I have roaming profiles set up with additional folder redirection deployed by GPO in a 2008 R2 RDS farm environment. They are working well, but files added to folder locations in the profile (such as saving a file to the desktop) only replicate to the redirected location at logout. Is there any way I can set this up so files are immediatley saved to the network location while the user is working prior to logout? My concern is that in the event of a server crash a user could lose hours of working data. Thanks in advance, any relevant info one way or the other will get a vote. 

The situation I am facing is that we were in the middle of moving mailboxes from an exchange 2010 sp3 environment over to a server running exchange 2013 and the exchange 2013 server had a raid error and we lost all data on the 2013 box immediately after the migration (completely dead array, all VM's gone of course :) I am able to see the mailboxes in "soft deleted" state on the 2010 Exchange box, I have tried running microsoft's recommended command to restore the soft deleted mailboxes the command errors out indicating that user's mailbox is on the 2013 server. Unsure how to proceed and would love any pointers anyone may have. 

We have been experiencing a VERY odd error and I have no idea where to go from here. When users are logging in to a Standalone Windows 2008 R2 server in workgroup configuration, it causes the following error: Event Log: System Event Source: Disk Event ID: 11 Research Severity: Error Details: The driver detected a controller error on \Device\Harddisk2\DR(number of drive varies). There are multiple USB drives plugged in, and the drive that detects the error varies. I have tried 5 different drives, and 2 thumb drives, and all will throw this error periodically. It seems to be only 1 drive at a time that will throw this error. 1)THE DRIVES HAVE ALL BEEN VERIFIED CLEAN WITH MULTIPLE CHKDSKS 2)THE MOTHERBOARD ON THE SERVER HAS BEEN REPLACED TWICE 3)Switching ports does not affect the error. As I said, the MB was also replaced 3)The issue appears EXACTLY when a user logs in Has anyone out there seen ANYTHING similar? Thanks in advance! 

The virtual machine should use a bridged network, not the "default" NATted network (such VMs can talk to the Internet but cannot be reached from outside). 

Do not edit the default file , as it will be restored whenever the package is updated. That is why we put it in a local file to override defaults. After creating this file, tell systemd about it: 

You should be using the real IP module, so that the client's actual IP address is considered the remote IP address, rather than your load balancer's IP address. This way, you won't have to check X-Forwarded-For in your logs, nor in your application either. It is simple to enable it, just supply your load balancer IP address(es): 

Note that this allows authenticated users to send mail with any FROM address they wish. If you want users to be able to send mail only with the email address they login with, use instead. Also note that your configuration is incomplete. You will also need several other directives to complete the configuration, primarily . 

This is a vulnerability where a malicious ssh server can attack the client if the client has connected with ssh-agent forwarding, and has somehow gotten a malicious file installed on the client's filesystem. I also think TrustWave has vastly overestimated the importance of this issue. That said, the obvious workaround is to disable agent forwarding in . 

The SELinux policy included with RHEL includes definitions for ports (not to mention policies and booleans) from hundreds of services, not all of which are installed of course. There is no need to remove them. 

The problem here is that you used an online utility to "convert" from Apache to nginx, and none of these do very well. 

But wait, there's more! You haven't specified an optional flag for the directive. So, after rewriting the URL, nginx continues right along on its merry way through the config. It doesn't start over processing the request, or redirect the user agent. This could cause WordPress to get confused. If you want redirection (e.g. for SEO) then you should add the appropriate flag. 

The source code was so short and simple that it only took a moment to find the only place within FreeBSD's utility where that particular message is printed. Specifically, it occurs when PAM has failed to authenticate the user. In other words, one of these things: 

(Errno 12 is "Cannot allocate memory") So we see that MySQL tried to allocate 128 MB and failed to do so. That means your instance doesn't have enough memory left. It could be you just have something running that's leaking memory, and rebooting the instance will get you up and running. But it's more likely that you need to go through all your processes and see what is using up memory, and tune them appropriately to run in a micro instance, or upgrade the instance size. 

Looks like someone's abusing Google App Engine. Check your Apache access log for that IP address, and the user agent string should have their app ID. You can then report the abuse to Google. 

Established only means that the connection is fully open and data can be transmitted. It doesn't necessarily mean that any data has been transmitted! It doesn't imply anything about layer 7, whether someone has authenticated to your system or not. You can check your system logs to learn if someone has authenticated successfully. 

You're missing a rule to accept traffic based on existing traffic (the rule that makes iptables stateful). This should be your very first rule: 

Just use "DEVICE partitions", it will try all devices listed in /proc/partitions, and you wouldn't have to worry at all what the device names are. UUID of an array is stored on each device belonging to it, so each array will be assembled correctly even if you have several of them. 

You need 32-bit version of libraries to run 32-bit applications on 64-bit system. Unfortunately Redhat doesn't have package like ia32-libs which would install most of them, it is supposed that you should install all 32-bit applications with yum and it will install the appropriate libraries for you. If your application is third-party, try installing 32-bit version of each library it needs, they usually have .i586 suffix, so you execute something like "yum install libusb.i586". 

If you create backup using tar, it will be enough to copy all your files, however the problem will arise when you try to extract that archive into a live system. Some binaries or libraries may not update because they will be in use by the system, and you will end up in a mess. You should get some support from your hosting provider (for the new VPS). Usually they have some features for that case, for instance my hosting provider allows me to switch my VPS to "repair mode", in that mode new VPS is created with basic Linux software, and my VPS's disk is mounted there, allowing me to change it however I want and then leave repair mode. Or maybe you can just send your tar archive to the support team, asking them to extract it to your VPS when it's offline. 

You can setup transparent proxy on your client machine that will have your company's proxy as a parent and add authentication information when forwarding requests to the parent. You will need to install a proxy server that supports transparent proxying, I'd recommend squid; and you will need a firewall that will redirect your traffic to a proxy server, many windows firewalls can do that. Google for "squid transparent proxy", there are a lot of manuals. 

I can't think of any tool that would do that out of the box. This is quite rare scenario, as you can't create correct two-way NAT mapping if you only change port. Do you really need just one-way traffic ? However you can always write your own netfilter module (it's not that difficult) and alter packet headers in any way you want. 

I suppose you know how hashing generally works: it calculates some function out of the data (IP, pair of IPs, etc) and uses value of that function as an index in the table to locate the structures associated with that data. Each cell in the table (which corresponds to one possible value of hash function) is usually called hash bucket. Unfortunately different sets of data may produce the same value of hash function, and will be associated with the same hash bucket. That's why hash bucket may contain several hash entries, which are usually stored as a linked list. Thus, when a lookup is done, hash function is calculated first and a hash bucket is selected, and if it contains several hash entries, they are analyzed one by one to find the approriate hash entry. Thus hashlimit-htable-size limits the number of hash buckets (size of hash table itself), and hashlimit-htable-max limits the number of all hash entries (stored in all hash buckets). 

As you all probably know, ipv4 route cache has been removed in 3.6 Linux kernel series, which had serious impact on multipath routing. IPv4 routing code (unlike IPv6 one) selects next hop in a round-robin fashion, so packets from given source IP to given destination IP don't always go via the same next hop. Before 3.6 the routing cache was correcting that situation, as next hop, once selected, was staying in the cache, and all further packets from the same source to the same destination were going through that next hop. Now next hop is re-selected for each packet, which leads to strange things: with 2 equal cost default routes in the routing table, each pointing to one internet provider, I can't even establish TCP connection, because initial SYN and final ACK go via different routes, and because of NAT on each path they arrive to destination as packets from different sources. Is there any relatively easy way to restore normal behaviour of multipath routing, so that next hop is selected per-flow rather than per-packet? Are there patches around to make IPv4 next hop selection hash-based, like it is for IPv6? Or how do you all deal with it?