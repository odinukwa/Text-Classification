(1) and (2) can give rise to the same surface sentence. In sense (2) we predict synonymy with the passive, because the embedded sentence could be passivized before subject-raising. But the embedded sentence of (1) cannot be passivized, because such aspectual verbs require the subjects of the matrix and embedded sentences to be the same (if the matrix has a subject at all). So your example is problematic for the usual analysis only if your first example with "HP" as subject means unambiguously that HP was responsible for stopping the control of programs. If there is a reading in which the responsibility might lie elsewhere, then there is no problem here. 

Start with 2.: A unless B = A, if not B A, if not B = if not B then A In general, if x then y = not x or y (for the "material implication" of logic) So, if not B then A = not not B or A (from the above, with x as not B and y as A) In general, not not x = x So, not not B or A = B or A (substituting B for not not B) In general, x or y = y or x So, B or A = A or B Thus, A unless B = A or B 

This analysis is a little tricky for your example "nearly an hour", since "nearly an" cannot stand alone, and you have to consider that "an" is a reduced form of "one". But at least that makes sense of the construction, because "nearly an hour" does mean that the quantity of hours was almost one, and the full form "nearly one hour" is an acceptable paraphrase. 

A semantic theory for a natural language is a translation scheme for relating the pronunciations of the natural language to the expressions of a logic. A logic is more convenient for describing (correct) human reasoning, in general, and implication, in particular. If the translation goes from the natural language to the logical expressions, it's "interpretive", and if it goes the other way, it's "generative" (though one needn't necessarily distinguish the two directions). To supplement the references in user6726's answer, here is an online copy of a relevant influential article by David Lewis, General Semantics. 

GPSG, the book (Generalized Phrase Structure Grammar, by Gazdar, Klein, Pullum, and Sag) gives a context free psg theory of English which, from the standpoint of syntactic theory, is a great achievement, in my opinion. It covers all the main parts of English syntax that were analyzed in classical transformational grammar. It probably would never work as a practical parser, since it was not really intended for that, and it has a lot of rules. 

I never heard of shallow parsing, but I have a theory that sort of does it. I don't often see a question that seems to license me talking about it. Consider a variant of context free grammar in which I. heads are not assigned to grammatical types (N, A, V, P, ...) but are rather introduced directly by phrase structure rules (S1 -> NP1 loves NP2), and II. grammatical types are distinguished by their degree of embedding, which I note with a digit suffix (0, 1, 2, 3). My preceding illustration means that a finite clause (S1) consists of a nominative NP (NP1) followed by pronunciation "loves" followed by an accusative NP (NP2). The sense in which this gives a shallow parsing is that much of the embeddedness which requires huge tall trees in conventional syntactic theory is there in the psr. Much of the apparent branching structure accompanies lexical forms of verbs (which are psrs). I assign intrinsic embeddededness levels to the various auxiliaries of English and the various adverb types (Adv0 is performative, Adv1 is sentential, Adv2 is manner, Adv3 is degree). You can make tall trees if you want, though. Corresponding to the principle of the transformational cycle, psrs are expanded always starting with the most embedded parts. So if you think of parsing as associating psrs with strings of words and trees, there are many fewer associations required for a tree of a given depth, so the parse is shallow in that sense. 

I would assume these rules if there is no evidence against them, and there is no such contrary evidence in the forms that are listed. A good place to look for evidence for these rules is in loan phonology. If word initial r in other languages is borrowed as rr and/or word final rr is borrowed as r, that would corroborate the above two rules. If the language has prefixes or suffixes, those can also cause alternations between r and rr which could provide evidence for these rules. (In Natural Phonology, one would write "process" instead of "rule" in the above.) 

To do this exercise, not knowing Lakota, I would assume that English and Lakota are almost exactly alike, except that the morphemes sound different and the order of parts in each constituent may differ from English. That is, make a tree for the English sentence, replace each English morpheme by a Lakota morpheme that means the same, then shuffle the order of parts in each constituent as required to get the entire Lakota sentence. You may have to make some arbitrary adjustments to get this to work. If you suspect that the English sentence has some idiosyncracies that you wouldn't necessarily expect to see turn up in another language, feel free to change those parts first. The assumption of the exercise is that all human languages are pretty much the same except for the pronunciations of the morphemes and the order of constituents. If that's not true, it might not be possible to do the exercise. 

I'm not sure, but I think that "two o'clock" is a noun phrase, and in "nearly two o'clock" the "nearly" is an adverb modifying the determiner (Det) "two" within that noun phrase. The constituent structure is: 

I don't know how to understand your term "lexis" -- does it mean "vocabulary", or "morphology"? I don't know who could have ever thought that "vocabulary and grammar were unconnected". How could anyone say anything without both knowing some words and how to put them together to make grammatical phrases? You can't have grammar without vocabulary, any more than you can have molecules without atoms, or portfolios without stocks. There is a way to interpret a part of your question that interests me, so that's why I decided to attempt an answer. In the traditional view of language structure, grammars and dictionaries look different. Grammars describe phrases and dictionaries describe words. It is not obvious that we need to distinguish the kinds of items in a grammar from the kinds of items in a dictionary. A context free grammar (CFG) as described by Chomsky back in the 50s can describe the patterns of phrases and the categories of words in a uniform way. For instance, the phrase structure rule "S -> NP-subj watches NP-obj" describes both "watches" as a word that combines a subject on its left and an object on its right to create an S, and at the same time the rule describes a sentence type in English consisting of a subject followed by a specific word, then an object. It is not necessary to suppose that there are two books, one with an entry describing a transitive structure as a grammatical sentence type of English, and another book listing "watches" as one of the transitive verbs of English. There's just one book. 

If you'd like to transcribe how we perceive cat language (as opposed to how cats do), you might be interested in the corresponding problem for bird song, which has at least some literature. I found a reference to an LSA talk from 1977 by Donegan and Stampe, "Old Sam Peabody, Peabody, Peabody". 

The two most popular ways of presenting a logical system are the axiomatic method and the natural deduction method. As the name suggests, most people find the natural deduction method easiest to comprehend. In a natural deduction system, suppositions are made, and consequences are drawn from those suppositions, where the suppositions correspond to questions in actual conversations. The system gives a systematic way of going back and forth from the conversational forms of deduction (entailments) to the statement forms (implications).