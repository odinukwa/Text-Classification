This is an interesting question, and I have a similar one in myself. It is my belief that in certain situations, multiple files per file group can help with performance depending on your hardware configuration (number of cores) and physical I/O design (the files can be distributed across multiple drives) Although of course you will be using SSDs in Raid 10 configuration for the production data drive(s), and have a ton of server memory, so that the majority of pages are being read from disk. With that hardware I'm not sure how much the physical design will matter. 

1) Backups are typically compressed, they will be less than the size of your data and the size of your log. 2) If you run a database in full recovery model you need to back up your transaction logs or they will grow indefinitely. If you don't care about transaction log backups you should switch back to simple recovery. 

It depends on the type of authentication you are doing. A common practice in SQL Server is to use only windows authentication, then to use SUSER_SNAME() as the default when saving to a User_ID column. I am not certain which MySQL equivalent is correct, but based on a quick Google it is likely either 

At my current job the majority of database objects use (NO LOCK) or READ UNCOMMITTED isolation. I agree it is a bad solution, but if you go back far enough it is used quite often. I was not familiar with the acronym RCSI, but in googling it I found that it referred to using Read Committed Snapshot Isolation. Yes, if you are having issues with locking and dead-locking, you should use SNAPSHOT ISOLATION. I think the best way to prove this out would be to create two virtualized clones of your production system, use snapshot isolation on one of them, simulate a typical usage pattern on each, and verify that it improves performance. Or just do it in the lower environments and then once you've verified it is stable migrate the changes to production. No one ever got fired for using Snapshot Isolation. 

I work for a company that has a multi-tenant database model that doesn't currently use database partitioning. The ideal end-state would be adding a new TenantID column to every existing table, including this new column as the first column in every primary key, and rewriting all existing SQL definitions to filter and equi-join on TenantID. However, with thousands of existing SQL modules, this is a resource intensive solution, outside of a complete rewrite of the entire database. So my new plan is to make this update in phases. Phase I will be keeping the existing database schema exactly as is, and creating aligned database partitions that use the existing SupplierID column (which is in nearly every table) as a proxy for TenantID. SupplierID already lines up well to what I want to do with TenantID. Existing suppliers are clustered into supplier groups, so a group of suppliers can be thought of as a single tenant. So my question is. If I do this, create aligned database partitions on the existing tables without changing any table schema or the definitions of any existing SQL modules, will I see any performance benefits? You can ask for additional information in the comments, and I will update the question to reflect them. But for starters there are many existing queries that filter on SupplierID, but few that equi-join on it. 

I would try persisting the computed column. If you do this the cost of computing the column will be incurred on writes, persisted, and then it won't slow down reads. 

I believe any of these options could work, but I would go with option 1 if the number of conditions is truly fixed at 3, and option 2 if it is not. Occam's razor also works for database design, all other factors being equal the simplest design is usually the best. Although if you want to follow strict database normalization rules I believe you'd need to go with 2 regardless of whether the number of conditions is fixed. 

There are numerous restrictions to what can be an indexed view. In a data warehouse setting those restrictions will make it impossible for just about every standard star join view to be an indexed view. Which is really a shame, since despite what the comments say, [materialized] indexed views are magic! As for whether joining to the same table twice is an issue, it is not. In a data warehouse you'll often find yourself joining to time and date dimensions dozens of times. Also, I was interested to see that you made the primary key on the date dimension the date itself. In my last data warehouse we ended up needing to use an INT column as the primary key, because we needed to save indeterminate dates (like 2016/1/??) where all of the date parts were not known. Also, when writing your views, you should always check to verify whether a LEFT JOIN will have better performance than an INNER JOIN, even if they produce the same results. Back when our data warehouse was closer to a snowflake schema, the joins were into the hundreds, and many of the views did better with all LEFT JOINs as opposed to INNER JOINs where INNER JOINs were appropriate. 

If you entered in a @ConsoleName that didn't already exist in the Console table, it would INSERT it there. Next the stored procedure would look up the ID for the console name you provided it, and INSERT a record into the Game table with @GameName and the @ConsoleID. Then you'd call that stored procedure for each Game you were planning to enter: 

Yes, using ENABLE_BROKER in the RESTORE conflicts with NORECOVERY. If your database is in NORECOVERY you'll need to wait until it is recovered before you renable the broker. If you are having trouble, try renabling with rollback immediate: 

I don't believe there is any way to do what you are asking. At the point you start using schemas you should always include the schema prefixes to your naming. It is a best practice and helps to avoid the problems that you are describing. The only reason that SQL Server allows you to reference objects without specifying schema is that it is legacy functionality. Originally SQL Server had no concept of schema, so everything was effectively in the same one. 

When you define the foreign keys you can include ON DELETE CASCADE to enforce the delete rule. As for problems in the design. One immediate issue I see is that you've defined the Identity columns as "Id". This design pattern almost always leads to trouble. What you want to use instead is an ID column name that includes the table name. For example, Id in the User table should be named UserID. 

From a logging perspective, if you intend to delete more than 50% of the rows from a table, you should use the following methodology: 

What you are asking to do is pretty straightforward, and I see some good answers already, but here is the simplest version. You need to join the consumer_client table twice to the person table. So 

Using SELECT statements instead of SETs can improve performance and readability, and may get you around the stated error. So instead of: 

Sounds like there may be a maintenance job running intermittently in the background which is filling your log. Do you have access to the job system to look? On non-production systems you'll also want to put the database in simple recovery mode in most circumstances. 

I am going to suggest an alternate solution. Rather than trying to mock-up a database design in Visio, why don't you create the database design in the relational database? Once that is complete you can use software to create the database diagram from your actual database. Most relational databases have database diagrams built in, and if the built in diagrams don't fit your needs there are a variety of third-party alternatives, many of them free, or with trial periods. The advantages to this approach is that doing the actual design will help you think through your choices, and even if it doesn't end up being the final design, you can use the work as a starting point. 

You've written the SQL using an EXCEPT DISTINCT clause, when you would likely have much better luck adding a NOT EXISTS to your WHERE clause. Essentially you are adding rows to the destination table where they do not already exist there. 

And so on until you've inserted all of your games. Please note that the exact syntax I've used is for the database platform I am most familiar with, but the underlying logic should be usable in any database platform. Also, as I write this I am struck by the idea that what you really want is to support a many-to-many relationship between game titles and platforms, since a platform can have many games and a game can be available on multiple platforms. That would take a little more work, but would be helpful for efficient storage and would give you the ability to easily determine which platform(s) a given game was available for without doing a text-match. So, at a high level you'd need another table. The existing Game table would not have a ConsoleID column. Instead there would be a new table, likely called Console_Game, that would include only ConsoleID and GameID as keys. Then the stored procedure would need to check for both the existence of ConsoleName, and the existence of GameName. Inserting into both tables as necessary, and then finally write a record to Console_Game to show that a given game is available on a given console. This will be more work upfront, but it is a more scalable solution than what you currently have. If this is for a job and not a hobby you should probably go that route. 

I can't immediate see anything that will improve performance, but I can see dozens of things you can do to better format your SQL so that that someone on here can give you a performance improving answer. First of all, it is completely valid in SQL to write many words on the same line. In fact, SQL server will ignore all of the white space in parsing, so you can write as much or as little on a single line as you want. So the lines that currently read 

As the previous answer stated, it will drop and recreate objects in many situations where it does not need to. Your best bet is updating either the database or the source control definition for the objects to be in sync with each other, and not rely on the SSDT reconciliation tool, which makes very bad choices. For example, if there is a difference in a constraint name, for example, between source control and the database environment, you could rename it in the database environment to be consistent with SC, or rename it in SC to be consistent with the DB. Otherwise the SSDT will want to drop and recreate it because it always takes the path that will work in all situations, not the path of least resistance. I've seen it drop and recreate a table just to change the column definition on one column from VARCHAR(50) to VARCHAR(100). 

I've done a number of 2008 R2 -> 2014 migrations. I've never attempted doing an in-place update where both the edition and version changed, all have been Enterprise 2008 R2 -> Enterprise 2014. I would recommend doing the in-place update in two steps. One to change the version, the other to change the edition. 

Although you'll probably want to alias the columns in the select statement, since you'll get duplicate columns with a straight up SELECT *