Is it common to mark rows with a batch number? Yes it is. In accounting contexts I have seen this used widely for audit and reconciliation purposes. Is having a separate table to store batch-related values a good idea? Yes it is. Good practice in relational DB design is to have one table per entity type. Since your problem domain recognises "batch" as a concept, with its own values, it should have a table. Should this table use an autoincrement column? Well, this is very much an implementation detail. Does your problem domain allow for gaps, negatives, re-use after delete, out of order values etc. etc? Does your DBMS software guarantee the necessary features? If so then, sure, go for it. If not, write your own generator with the needed guarantees. Will denormalising timestamp from the "batch" table to the "data" table improve performance? Can't say. It will make the data table wider, which should cause more IO. There may be enough spare space in each page that this doesn't happen, however. If all the data, or at least the woking set, fits in memory, this consideration is void. If there are good indexes in place and they are being used I'd guess denormalisation won't change much. You could try rewriting the query. Sometimes if the optimizer starts from a different place it ends up with a different plan. Put in a subquery, then join that to to get the timestamp. You need to understand where the time is consumed and address that. And of course test, test, test. 

You could add another table just to hold the totals. It would have two columns - type and total_value. As a transaction is inserted the running total is updated, either in application code or by a trigger. At higher transaction rates this table rapidly becomes a bottleneck to higher throughput. Some relief can be had by adjusting fill factor so there is only one value per page. That will only go so far. Since you can tolerate some staleness the hotspot can be avoided by batching updates. Let's say you can tolerate 1 minute of lag between a transaction and the total showing it. Every 30 seconds or so read the highest id and the total transaction value. I each cycle record the highest id so each transactions is only processed once. A bit like this: 

Option 2 is the cannonical solution for the Order / OrderItem pattern, which a shopping cart is, in essence. It allows the cart to be treated as a whole and distinct from another cart that may be submitted by the same user an arbitrarily short time later. Fulfilment can be recorded against a cart rather than each item individually, if that is what is required. 

Ball park - it will have to read 60Gb out of the log file, write 60Gb to the backup file, do some metadata operations for the log truncation itself and (optionally) move what's left of the log file if you shrink the ldf. For your average disk subsystem I'd expect you would be done and dusted in couple of hours, tops. 

I have not yet found a notification or activation on job completion. The nearest I can think of is putting a tirgger on the job-related systems tables in MSDB. This is very dodgy, however, and not at all recommended, and may not be possible. In the past, when I've needed to merge two job streams that I can't edit, I've used a check-and-wait stored procedure like this: 

Often software design is a compromise between competing requirements. It is important to understand the relative merits, both for the system as a whole and each case locally. For example, you say writes outnumber reads. That would suggest the system as a whole should be optimised for writes. However, what are those reads for - do they prevent a vehicle collision or cardiac arrest? Perhaps those systems should be optimised for read. Do you have a index on the time column? Then a query like should use that index. Essentially, you pre-sort the data for all queries. The irony being that every write will have to maintain this index, doubling the cost each time. 

For your specific example scenario 1 keeps all the data on the server. Scenario 2 will require the data to be packaged, sent over the wire to the client, where is must be buffered (and, perhaps, spilled to disk) then un-buffered, re-packaged and sent back to the server, where it will finally be processed. This network time adds up. Do this often enough with large enough rowsets and you will see latency mount. Do it one row at a times (as your example shows) and you'll be ruing your decision for the rest of your natural life. Submitting a batch is usually faster than submitting statements individually. Scenario 1 is a single statement with an implicit transaction to keep the data consistent. Scenario 2 will require an explicit transaction to achieve the same consistency. Those locks will have to be held for the duration of the round trip to the client, which will cause blocking. If you split a complex query into simpler statements which are all submitted as a batch there are gains and losses. For example, splitting 

"Is this a good plan to copy the indexes." Indexes exist to improve the response time of read operations. Those operations arise from the workload. If the workload differs, the indexes should differ, too. For example, it is not uncommon to have one server handle the OLTP activity and replicate the data to a second server where analytic queries are processed. The first server's work will be mostly single row lookups. The second's range scans and aggregates. In this case having different indexes on each server makes good sense. If all indexes were on both servers there would be additional, unnecessary work to do during writes and maintenance. 

I'm going to use ASCII art. I'll label each page in the sequence they're created and show the keys/ pointers within the page. So page P containing key values k1 and k2 will be . Now key 3 comes along. According to Section 5.2.1 ... Insertion, the first task is to search. This determines key 3 should be on page A - the only page we have. Further "if [that node] is full, it will be split into two nodes." The page is full so it must split. We now have 

The usual design is to relocate each column with repeating values into its own table. You don't say, but I'll call your current table , and I'll assume the is the primary key. Next create a table for each repeating-value column and link them back to . For example 

The DMV columns will tell you which tables were accessed since the instance last started, assuming you have a typical indexing strategy. Other DMVs will tell you which SPs are in use, but only while their plans are in cache; again, only since the instance started. If you have a relatively small application and large memory this may be the whole workload (unlikely though). can help tease out the links between various objects. Running a trace can help, too. Many applications will have special-purpose routines which execute rarely. Examples are accountancy year-end or customer on-boarding. These will not show up in the caches or traces. Your life may become significantly more stressful should you delete them, however. A trawl of your source code may be the only solution. Dynamic SQL is your nemesis here (another good reason to avoid it). Garbage programmables are not much overhead in the grand scheme of things. Large, unused tables you would want to remove for speed of restores. Even then you could move them to a different filegroup and have a piecemeal restore strategy. If you have reliable source control in place that can help. There are various tools out there which will compare two databases (say, production and UAT), listing differences. This will help distinguish application-required objects from ad hoc cruft. Ultimately you can rename an object and wait to see what fails. It's not pretty, but it works. 

In accounting practice values are never altered once written. Instead secondary compensating transactions are created. In your circumstances this would mean one new row with a value that is the negative old, incorrect value and a second new row with the new, correct value. To work each data row will need its own datetime column so these compensating transactions can be post-processed into the correct buckets. Averages cannot be altered in post processing. Instead the sum and count are stored and average calculated at run time. 

You have an ingredient; let's call this "strawberry mix, packet, 100g". You also have a product; let's call this "strawberry mix". There is a row which maps the product "strawberry mix" to the ingredient "strawberry mix, packet, 100g" with a quantity of 1. No need to convert anything to anything. 

As the years tick over will a member's level change? Is it important to have history? For Example Chris was Secretary while at level 100 and became President while in level 300? If so you'll have to separate out of into a new table and make that the foreign key into . 

I use as an example. You use whatever's right for you. The list of monitors is more difficult because it may contain many values. For this you have to use a table-valued parameter and for that you will have to define a type: 

When creating a data model a good place to start is to have an entity type for each noun in the description of your problem. The attributes of these entity types will be the values you wish to store about those nouns and verbs in the description will become relationships in your data model. In the database the entity types become tables, the attributes are columns and relationships end up as foreign keys. Of course there are a lot of subtlties but this should get you going. Copying your description above into these terms you will have: 

Fill factor applies to the leaf pages. PAD_INDEX will determine what happens to non-leaf pages. From the same BoL page: 

Firstly, this is a Really Bad Design. Reusable columns will lead you into a world of pain. Amend the design to use normalised entities with properly-named attributes. That said, I acknowledge that there may be nothing you personally can do about this schema. Since this change is just about applying better labels to an existing resultset, can you get the presentation layer - GUI or reporting tool - to apply them? As a last resort I can think of one SQL based implementation. Apply an to the Table2. This will give: 

Treat the various processes as sub-types. There will then be entity , which contains all the common attributes, , , etc. for the process-specific attributes. Implement these as a table each. A view which combines them all together may simplify usage 

Creation and maintenance of PolyBase statistics is in the hands of the DBA. The "getting started" guide provides several steps to establish PolyBase, the last of which is CREATE STATISTICS referencing the external table. The documentation on CREATE STATISTICS states: 

You could use bcp to write the rows you wish to retain to a different server. Then truncate the table (or drop and re-create). Use bcp a second time to import the data previously exported. 

SQL Server does not de-escalate the lock. I investigated using a "Numbers" table with 100,000 rows. Empirically, updating 5,000 rows produced a corresponding number of RID locks in . Updating a further 10,000 rows caused escalation to a single table lock. This was consistently reproducible. To minimise the objects involved the table was a heap without indexes. I used Extended Event tracing to capture and events. For ease of analysis I used separate traces during the UPDATE and ROLLBACK phases. Two sessions (SSMS windows) were used - one for the DML statements and one for the trace DDL. I could not use as single session as I wanted to stop and start traces while the transaction is open and this is not permitted. The isolation level throughout is READCOMITTED. The result of is 

Your approach is reasonable. Put a check constraint on the table to ensure exactly one of your foreign keys is ever set at any point in time. The flag is redundant since the same information is captured by which of the foreign keys is . Keeping it just adds maintenance overhead to the application. Eventually it will become inconsistent and then you'll have a mess to sort out. An alternative design is to combine some of the information from and into a table. The schema then becomes 

If you write your queries properly and avoid then any relational database will allow the addition of further columns or tables without requiring adjustment to the application. You do, of course, have to declare each column to the DBMS before referencing it in SQL. I find the claim that NoSQL is "schema-less" to be slightly misleading. Applications using NoSQL persistance do, indeed, have a schema. The difference is that the schema is held in application code and it is the responsibility of every programmer who touches the code throughout the application's life to enforce that schema. With relational databases the data structure is declared to the service and it then takes the responsibility for enforcing those rules for ever after. The real flexibility of NoSQL is that two rows within the same "bucket" (the definition of which various depending on your DBMS) can have different structure. 

A requirement in a recent project was to report when a resource would be fully consumed. As well as the exhaustion calendar date I was asked to show the remaining time in English-like format, something like "1 year, 3 months to go." The built-in function 

Yes that is correct. Of course you no longer actually have a many-to-many relationship in your model. You are no longer "representing" the relationship, as you say in your question. You have, in fact, resolved the relationship into an association entity type with two one-to-many relationships. 

I specifically do not call this "user" since participants need not be people. You will have a participant record, too. 

If a single business can participate in several groups you have what is known as a graph. Your own and Shooter's solutions implement this scenario. However, these model a directed graph - there is a "from" and a "to" end to each combination. To find all possible links you must read the table twice - once using the Business_1 column and once using the Businesses_2 column - or double up each row - "A to B" and "B to A". (There are specialised graph database products which can handle this, but that's beyond the scope of this question.) An alternative implementation is to have a new Business_Links table. It has two columns Business_Name and Group_Identifier. Note that this in not the same as the sample data above. That is an extension of the Businesses table. It will have one row per business. This is a new table. There may be many rows per business. Let's say {A, B, C} form a group and so do {C, D, E}. "C" is in both groups. The data is: 

You can use SQL Server Management Objects (SMO). They are a set of classes and APIs for interaction with almost all aspects of SQL Server. There is support for scripting. The SMO dlls can be invoked from any of the usual programming languages, Powershell or even, if you're desperate enough, from within T-SQL. 

If an index can be defined on the search criteria then this will work OK at any scale. BTree indexes have O(log N) performance IIRC so working through 100,000 rows will be 5/3 the time needed for 1,000 rows. However .. most systems can't fulfill that criteria. Often a user can fill as many or few search fields as she chooses. Then things get complicated. Sometimes it may be best to define separate queries, and indexes, for each combination of search values. This quickly gets large. 5! = 120; that's a lot of indexes and queries. Some DBMS have optimisations for generic searches - OPTION (RECOMPILE) in SQL Server, for example, will cause parameter embedding. Probably some combination of often-used specific queries and a generic catch-all will work. As the list gets hugh users will expect ever fuzzier searches. At some stage a full text search on the product name and / or description may be useful. Maybe, eventually, you offload that task entirely.