As you can see, it is implemented as a single statement, which usually implies that it uses a set-based approach (which is good, because relational database systems are optimised for that). However, this query cannot really qualify as strictly set-based, because it uses a recursive common table expression (CTE) – a row-by-row device by nature, despite being called "expression". Anyway, here is a description of how the method works: 

This is an equivalent substitute because whatever will be passed to the IN subquery is going to match based on the filter in the query that passes the value. 

Note that the query does not specify an order in which the rows are read or returned. The solution will still work regardless of the row order but absence of an explicit ORDER BY ultimately means that the output might be different between different runs of the query. The issue is actually twofold. Because the rows are read without an explicit order, the query might produce different rows for the output, and the other problem is that they may also be returned in different order with each run of the query. To resolve this and make the output completely repeatable, you probably need to supply an ORDER BY both at the nested level and at the outer level, something like this: 

Within each group, use to put the master row after all its duplicates, and then to determine the order of the duplicate rows (assuming there can be more than one duplicate of the same master row). Those last two criteria are the same as in Kondybas's answer, by the way. The entire clause, therefore, would look like this: 

So, if table A has no match on Item and Date at all, the NOT EXISTS predicate will evaluate to True. If there is a match and its status is 0, the predicate will again be true because of the Status condition. 

For the second option, using an ID instead of a name to specify the user would mean a simpler left join: 

An EXISTS predicate is often faster than an equivalent join, though, so you might prefer the former option, but test in your environment to see which is actually better. If the user is specified by ID instead of a name, the query becomes simpler in each case. The EXISTS predicate would look like this: 

I currently have a impdp job running for a fairly large schema (2TB) and ran into an error. The source database has one column that is larger then the destination database. This is causing ORA-02374 and ORA-12899 errors. This has caused the import job to come to a crawl on this one table. I am trying to get past the errors, and the best solutions I can come up with are to either fix the table definition, or tell the import job to skip the current table. I can not currently fix the table definition because the data pump job has the table locked. Is there a way to pause the job, make the column modification, and then resume the import? Alternatively is there a way to tell the job to skip the current table, and move on? This would allow to to come back once the job is finished, fix the table definition, and then re-import just the one table. ETA: This is one of the last few tables in the import, I would rather not kill the whole thing and start over. 

I am trying to track down a cause for a difference in oracle impdp processing, and I am not finding anything, so I am wondering if anyone here can explain the cause of the differences. At times I have seen times where using the parallel=x parameter in oracle datapump will cause either multiple tables to be inserted at once, up to the value of 'x', or other times will use parallel threads to import a single table. I have not been able to track down what might be causing the difference in performance, and I am wondering if anyone has an explanation, or even just a direction to point me to. It would be helpful to determine why it runs in a given matter. For instance right now I am monitoring a single table, data_only import that was started with parallel=8 in the command, but the import job is only using a single thread to do the import, as shown by the following hint the insert query /+ PARALLEL("XXXXX",1)+/. If the process would use the maximum 8 threads specified to should run much faster. 

I have noticed differences in executions between data pump imports lately, and I am trying to determine what is causing the differences. I am using impdp to move data between databases, and remap the schemas. It seems like I have seen two different things happen when I run the import in parallel, and I am not sure what is causing the differences. Sometimes I see the job running and it is importing a bunch of tables side by side, other times the import process is only importing a few tables at a time, but using parallel inserts for each table. Using Parallel inserts on just a few tables at a time seems to cut my overall run time substantially, but I can't figure out why it only runs like that sometimes and not others. 

Since for user1 each will match the IN predicate, you will get the corresponding email returned twice. To resolve that, instead of this: 

Your solution seems good enough, I am not sure why you would need to ask for anything more. I would only like to note that if you want to skip the first 18 characters, then you should specify 19 as the second argument for , because in SQL the character positions in a string start from 1. So this makes perfect sense and should work well for you: 

(The keyword is unnecessary. I use it because in my opinion it improves readability, but you are free to omit it.) This series of correlated subqueries is pulling data from the same rowset as well, but each subquery has an additional condition, which makes this a different case from the previous one: 

You, however, want a user-centric report. So, first of all, you need to transform the original set to a user-centric one: 

In MySQL, variables can be used in queries both to be calculated from values in columns and to be used in expression for new, calculated columns. In this case, using a variable results in an efficient query: 

Another way would be to arrange the arguments and their respective sorting values as a (derived) table, join that table and use the sorting column in ORDER BY: 

In a / query, a clause filters a single sub- (the one immediately preceding it) rather than the entire UNIONed set – thus, each sub-select can have its own clause. (This is unlike e.g. , which you would be allowed to specify only once and it would apply to the combined set.) So, if you want products by the same maker B from each of the three category tables, you need to repeat the filter for each sub-select: 

In this case there is no need to derive a table, as columns you want to group by can be explicitly specified in the GROUP BY clause. ————————— Both methods produce the same output, identical to the one that your joining method generates, as can be seen in this demo at dbfiddle.uk. 

will only ever evaluate to 1 if the is , not when it's or or anything of the kind. By the way, when you want the function to return a substring from a certain position to the end of the string, you can specify any fairly large number as the length parameter, no need to use . Very often something like 

Well, it looks like when the first sql server install ran, it also configured the file server services, which were still trying to hold the disk and the network name. Odd thing is, the disks where all showing 0 dependencies. This probably could have been fixed if we opened a case with microsoft, maybe cleaned up some registry entries, etc, but the guys from out windows team wanted to just rebuild from bare metal, as it would be quicker and easier. I was on board with that, so that is the solution I am implementing 

I have a powershell script that I am trying to add in to a sql agent job, but the job step is failing with the following error 

I am hoping someone can shed some light on this. When running an import into an oracle database, I sometimes see different behavior based on the parallel option. Some times, I will see multiple data pump workers all running insert commands, with (parallel 1) query hint in them. Other times I have seen a single, or just a few data pump workers, running insert commands with (parallel X) [where x is more then 1] table hint in the queries. I have seen this when running imports that essentially identical. The imports are using different dump files, but where created from the same nightly job, just done on different days. I am using the following options SCHEMAS=XXXXXXXX parallel=32 cluster=y DIRECTORY= DUMPFILE=XXXXXXX_%U.dpdmp CONTENT=DATA_ONLY TABLE_EXISTS_ACTION=APPEND DATA_OPTIONS=SKIP_CONSTRAINT_ERRORS LOGFILE=XXXXXXXXXX.log 

I am trying to setup a new sql server 2008 cluster, on windows 2012R2, and the installer is failing on the cluster shaed disk availability check. I have verified that there are 5 disks assigned to "Available Storage" when viewed in the fail over cluster manager. Some background, this is my second attempt to install sql server on this cluster. The first time, the cluster object was unable to create the new computer object during the cluster installation. This caused the installer to fail do to lack of permissions. I have since resolved this, and have run "Remove node from cluster" to uninstall sql server from the node. I am now trying again to run the installer. About the environment, OS: windows 2012R2 SQL Version/Edition: 2008/Enterprise I am running the installer from the current cluster host(node1), and all storage is owned by node1. This includes the quorum, as well as the 5 disks assigned to the available storage group. Both cluster nodes are up and available, and accessible either through the node names, or through the windows cluster name. There are no cluster validation warnings that I know of, but I have asked the windows admin to rerun the validation tool to confirm that, that is still the case. SQL 2008 is required by the front end application (I pushed for at least 2012, but was told it was a no go)