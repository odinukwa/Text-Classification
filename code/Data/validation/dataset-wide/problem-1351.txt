In your loop, you have a list called which gets the same things as except its cleared for every . You don't actually use it, so you could just drop it completely. If you do want to keep track of which solutions go with which , you could change into a , and then do: 

Do put spaces between arguments (after the commands), don't between the function name () and the opening bracket. 

The rest of your code assumes that we will be working on the original, but the easiest fix for that is to make a copy at the earliest opportunity. If you do that early (and with an appropriate comment), and you don't need the original list anymore (you don't), then you can reuse the name without losing clarity. Unfortunately, copying the list is bad for performance, but readability needs to come first. But way down the bottom, I will point out a bigger improvement that will mean we don't actually have to choose between performance and readability here. 

Do the microsecond ones really need to be functions at all? They seem like they'd be used only rarely (probably at the end points, reading and writing to a database), and since they're one trivial calculation, a small comment at the call site would be sufficient. Some of these really seem like they belong as instance methods on the object. In fact, a lot of them are in newer stdlib, and the entire point of your code is to also support versions of Python from before that was the case. So, it might be a good idea to simply offer a compatible interface. There's a few ways to do that. First, the stdlib module is implemented in Python, so you could include a newer version of it wholesale and have your module do this: 

Second, you could inherit from the stdlib one and add the missing functionality. If you want to add directly to timedelta this way as well, you would also have to override and to turn any object they are about to return into your extended . I was going to suggest monkey patching as another option (this being one limited context where it does seem worth it), but it turns out you can't ( and use ). I only recommend adding the strict compatibility things this way. The idea is to set it up so that if you stop supporting 2.6 down the track, you can delete this code without having to adjust anything else, except maybe some imports. in particular ought to stay separate, especially because it fairly specifically enforces/assumes your local policy ("serialised dates will be in one of these two formats"). 

This will end up falling off the end of the function, and so returning . The rest of your script will continue, but eventually fail horribly. This probably isn't what you want. 

This emphasises that you don't do any work in that case. You might want to add a short comment about why you don't need to. From your title, it sounds like you might want to treat this as an error condition and do this: 

If you use , I wouldn't suggest using string constants like that. The typical convention is to use the object that is being mutated (thus make sure it is instantiated up-front, and then you can do ). Note, the provided code is not thread-safe. It has several issues: 

The performance might be improved if you used more efficient synchronization mechanism. The directive is notoriously inefficient. It's pretty convenient way of doing mutex lock, but there are more efficient alternatives. A common pattern is to use dedicated GCD serial queue to synchronize access. Even better, one can use the the reader-writer pattern: It's a concurrent GCD queue where you to read operations to enjoy concurrency on read operations, but use to write). See the discussion of this pattern in WWDC 2012 video Asynchronous Design Patterns with Blocks, GCD, and XPC (it's in the latter portion of the video). A couple of other reactions to the provided code snippet: 

You can improve performance by reducing the batch size. In my testing, if I saved these in batches of 1000-2000, I got the best performance (and it's also more memory efficient than doing all 220,000 objects at once). You might want to try different batch sizes and see if it differs given your object model. I also, personally, wrap each of these batches in an autorelease pool, which ensures that memory consumed by autorelease objects is recovered. I would not, though, generally have an autorelease pool for each object saved. Just one autorelease pool around each group. (The objects are held in memory until it encounters , anyway, so there is modest benefit in having more frequently than that.) 

This is really unrelated, but I wouldn't suggest three separate arrays for the , (?), and . I'd personally rather see an array of objects (either custom object or for which each object has its own , , and ). There are other approaches, too, but having three separate arrays doesn't seem very appealing. Also unrelated, but I'm surprised that if you call this method and the key/URL both exist, that you don't update the existing record. Right now, you leave the old values there. 

If all you're trying to synchronize is , and if that's a simple data type (e.g. an integer or float, not an object), you can just define it as , and it can be safely read and updated from multiple threads, with no custom getter required. This is one of the very few situations where can be useful for synchronization (and it's remarkably efficient). By the way, if your property has a public interface that is , make sure the private class extension redefines it as so the setter is synthesized for you. And make sure that you only set it using the setter (not bypassing it and using the instance variable directly). 

But the basic approach of creating custom objects is a very solid notion. Having said that, this seems more applicable when reading data from some web service. If you're really reading this from your bundle, I might be inclined to making my objects compliant, and then save this collection of objects using a and then the app could instantiate the with a single call to . See the Archives and Serializations Programming Guide for more information. Or Core Data is another approach. This approach of parsing the JSON and then reprocessing the array seems cumbersome if dealing with some resource in your bundle. 

The end result - after applying the changes in the first solution and the changes for these 3 points - should be something like the following: 

This approach calculates each result in \$O(n)\$ time and has a space complexity of \$O(1)\$ and, IMO, is the way to go if the function is calculated not so often. Cached-result processing In case the function is calculated often I'd suggest another approach. If you use an appropriate data structure (an arraylist should be ideal) you can implement the function in the following way: 

Regarding the complexity of the algorithm we have that time complexity is \$O(AvgBagCapacity^{NumOfBags})\$ (that is reduced a little by breaking the computation early). Regarding the space complexity depends on the approach used to expand the tree. Personally, I'd suggest to use a depth-first approach as it reduces the space complexity to \$O(AvgBagCapacity * NumOfBags)\$ (if you don't keep the already expanded nodes in memory). If you want to furtherly reduce the space complexity you could use lazy initializations of the nodes so you use only one node at a time. By doing so, the space complexity is \$O(NumOfBags)\$. For the sake of clarity I wrote some code which does what I described. Obviously it has a lot of room for improvement (starting with the naming :P ). I also included some tests (the first two are the same as two of your examples) so you can do comparisons and play with it. 

This approach has a time complexity of \$O(n)\$ in the worst case (when \$n\$ is much greater than the number of cached elements) and \$O(1)\$ in the best case (when \$n\$ is smaller than, or equal to, the number of cached elements). In the average case the number of calculations is reduced, but still remains \$O(n)\$. The space complexity, on the other hand, is \$O(n)\$ in all cases. Let me know if anything's unclear. P.S.: The same logic applies to \$G(n)\$ and \$W(n)\$. Also, all code is pseudo-code, it's there just to better explain the idea. 

In my opinion, there are a few improvements that can be made. I'll address only some readability issues here, not the performance or algorithmic ones. Remove regions There are a lot of discussions on this, just google it and you'll find a ton of discussions. The methods are too long Try to reduce them by using already existing object or methods. For example: 

Given that the sum of \$n\$ matrices is independent for each couple of matrices you can sum the \$n\$ matrices in groups of 2 in an async way. You just need a queue of objects. 

There are a few things that popped in my mind seeing this: Generalization The method can be used to split any enumerable/collection/array, but this is just a minor point. 

and let the management of special cases to the caller. In this way, the first line of the method becomes something like the following: 

Pick the element on top of . If this element is greater than or equal to the element on top of , or if is empty, push this element in . If this element is smaller than the element on top of then pop the element on top of and push it in , then go to step 2. If contains any element go to step 1.