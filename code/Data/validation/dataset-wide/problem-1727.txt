We have a 2 node Exchange 2010 server setup with a DAG. I really didn't want to use Public Folders, but we need them for a legacy app. I created two public folder databases, one on each node because you can't make PF databases part of the DAG. They replicate to each other. Right now, I have one of the PF databases selected in the DAG as the default public folder database. It doesn't look like you can select a failover. My question is: Is there any way to load balance across these two PF databases? Or if I can't do that, can I set them up to failover automatically if one goes down? It looks like right now I can only have one PF database on one node in use at a time, and if that node goes down, public folders go down as well. 

Like a lot of people in IT I sit at a desk for 8-10hrs/day working on stuff that needs to get done "now". That usually means eating unhealthy lunches at my desk, and sometimes dinner too. This does terrible things to your health. I have been trying to work some exercise into the workday but I was thinking it would be great if I worked somewhere where I was exercising all the time (walking around, lifting servers, etc.). Does anyone have a job in IT (or know someone) where you are actually moving around (not sitting) doing something for most of the day? 

It depends totally on what you're trying to do. A static website? Probably no problem at all? A website where each page requires a large amount of CPU & memory before it can output the results? You're going to have problems. Is it mainly plain text? Does it output a 1Mb graphic for each user? The best way to know is to use benchmarking software to find out what the site's requirements are, and by extension, if it can handle the load you're expecting. Here is a list of different programs. Loadrunner will without doubt do it for you, but I'm sure there are others which will work just as well. 

You can also use the screen program, and pressing ctrl-a h will write the screen to a file 'hardcopy.n'. 

It's the amount of physical memory being used for cache memory. Even though top lists it at the end of swap, it's really got nothing to do swap space. Red Hat has a nice explanation of all the different types of memory usage. 

It's confusing to try to understand what your problem is, you don't make it clear if you're checking a file on web1 on web1, or db1 on web1, or whatever. Please give a good description of what's you're actually seeing, without confusing the issue with copying and chmoding. Something simple like I create a file on db1, with permissions x:y and on web1 I see permissions a:b and on web2 I see permissions c:d. First thing, using NFS, any file which is owned by root will usually be shared so that it's owned by nobody. This means that if you have root on the client machine, you effectively don't have root on the server. I think that explains some of what you're seeing. Secondly, if you are running NFS, it's vital that the userid->username mappings are identical on all the servers. Unix filesystems only store a numeric id for userid & groupid, which are then mapped to usernames by programs like ls. Are you sure that they are all in sync? It could be that you've got a mismatch. Finally, tar p is an option for extracting, not creating tars. It's ignored when creating tars, and even when it's used, it's not going to set the ownership to what they originally were. -p basically means, ignore the umask. Tar will create files owned by you only, unless you're root. 

We currently host all of our clients in one datacenter. We would like to expand to two new datacenters by the end of the year. My job is to figure out how to extend our current network to those two locations. I have some basic designs drawn up (site to site VPNs between the datacenters, OSPF for internal routing, round-robin DNS to distribute the load), but I could really use some advice from someone who has done this before. My major concern is making a design choice that will constrain us or require a redesign in the future. We already have some design choices that will hamper us in the future (e.g. every client gets their own 10.x.x.x/24 subnet with their own VLAN, which will work great right up until our 4093rd client). Does anyone know some good resources on how to create a scalable network design? EDIT: Our business resembles VPS hosting, so as you can imagine, there's all types of traffic. Most of it is web (80,443) and mail though, so throughput is the usual priority followed by latency. The business reason we want to expand is so our clients can host their servers either on the East/West Coast US or South Pacific Asia. The IT reason is to give us a disaster recovery site in case of natural disaster. EDIT 2: Just to be clear I'm looking for resources on how to design the network. The details on how to do it (VPNs, routing, DNS, etc.) I can do. 

You could install encrypted home directories. This page has a writeup on how to install under Ubuntu. 

You have a bad installation somehow. It is conventional on Unix that a program exists in /usr/sbin/sendmail$, which acts as an interface to the local MTA. This MTA is rarely sendmail nowadays, but other MTA's have a compatible program which is installed here. sendmail.h would be part of the source code for sendmail, and sendmail.0 and sendmail.8 would be the man pages, so it looks like you've somehow got the source code for the right sendmail in /usr/sbin You need to clean this up, and get a proper sendmail program into /usr/sbin/sendmail to fix php. Since you've not told us the distribution, nor which MTA you're actually using, no-one else can help you. $ = Actually one or more of /usr/lib/sendmail and /usr/bin/sendmail as well as /usr/sbin/sendmail, programs will either search these paths, or have one or more hardcoded into them, or ask at installation time. It looks like your program is using /usr/sbin/sendmail. Regardless of which, the installation for the MTA will do it. 

ls -ld /DriveXT will tell you the permissions on /DriveXT. You can't get the permissions on the lower ones, they are probably '--x' for you for MMT, and '---' for you for LP2010. If you want to find out the permissions on the lower directories, you'll have to be root, or user with 'r' permission on those directories. 

Go to the Member’s Overview page and click on Reverse DNS Configuration Manager (look near the “Network Information” button — there is a link to it). Just make sure a corresponding A record exists for the PTR record (i.e. make sure you have an A record for foo.example.com first). 

We have a Dell Poweredge 1950 that came with Dell Rapid Rails. These are the tool-less mounting rails for 4 post square hole racks. We have some new Dell Poweredge R610s on the way. We were hoping to use the existing 1950 rails but it looks like Dell has a new type of rail for the 11G Poweredge series (yay) called the Ready Rail: $URL$ Does anyone know from first hand experience if the old Dell Rapid Rails work with the newer 11G servers? 

You're getting that error in the Exchange connectivity testing tool because the Network Solutions SSL certificate isn't trusted by default in Windows Mobile phones. The only ones that are trusted by default are: 

I usually go with GoDaddy certs for this reason (they purchased Valicert and their CA root awhile ago). That being said, iPhones do trust Network Solutions certs. Are you running Exchange 2003 SP2? iPhones need Exchange 2003 SP2 to sync. 

We have a Windows 2008 VM running IIS and SQL Server Express (it's an all-in-one web application). We need to have another copy at our secondary datacenter site. What is the best way to do this? It doesn't have to be running all the time but it has to have almost the latest copy of the current VM. I took a look at VMWare Fault Tolerance and after the heart attack at the price I starting looking for another solution. If need be I wouldn't mind copying it over to a cloud VM provider, if I can find one that lets me copy my own VMs up and start them up without any conversion process. 

You should give a full path, because you don't know the context that it will be executed in. If your program exits with 67, then this will be bounce the message as unknown user, 0 will drop the message. Anything else will be retried until the message times out and bounces. Be careful of security - you're basically allowing anyone on the Internet to run a program on your system, so don't trust user input, and sanitize it before you use it. 

Usually web servers are configured to only allow access to user files in a specific directory, traditionally the public_html directory. The url format would be $URL$ which would translate to /home/user/public_html/dir/file.html This is for security reasons. Imagine if anyone could access your .ssh_keys directory from the web, anyone could break into that account. For that reason, there isn't any way that you do what you want directly. On the other hand, there is nothing wrong with accessing a server by hostname. A virtual host is just another name for that same server, which usually has different content served out. There is nothing you can do with a virtual host which you can't also do with the correct hostname. 

You should approach it in the other way, instead of limiting these log files, work out a system where there is always enough log space so that they can never fill the disk. My primary way of doing this is simply calculating how much space is likely to be needed, and giving it lots of space, but I've also got a script which will check for diskspace in the logs directory, and if it's getting tight will automatically rotate the log files, compress old versions, and erase those which are old enough that they'll be on archived media. 

We are going to let one of our sysadmins go in a few weeks. He has access to our entire infrastructure, so we'll have to reset the passwords on everything. This is going to be really time consuming though, with the number of servers and devices out there we would have to reset. A few things are tied into Windows domains so that's easy enough, but problematic depending on whether any services run as that user. Is there any way to easily revoke access to everything for that user? I guess I'm asking for cross platform single sign-on...maybe RADIUS can do this? Or is there a turnkey solution out there like RSA SecureID? What have you guys used? 

This isn't a direct solution, but would it be possible to use a 3rd party email marketing company to send your emails? This way you don't have to worry about a lot of these issues. ConstantContact.com is one. Eloqua seems to be a popular one for large companies. 

We have a Dell Poweredge 2970 with a PERC 6/i RAID controller. We have a one drive RAID 0 array (we wanted to add the drive as a JBOD but the PERC forces you to create an array to access it from the PERC). Can we take the one drive RAID 0 and move it to a new server (one that doesn't have a PERC)? Since there's only one drive in the "array" there's no striping going on...the only issue would seem to be if the PERC has some metadata on the drive that would prevent Windows from reading it. Does anyone have any experience with this scenario?