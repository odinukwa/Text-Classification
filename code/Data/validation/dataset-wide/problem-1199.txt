Yes, you're correct; they do require Windows Server Failover Clustering. However, I don't see how that's a show stopper. Does it take a little extra configuration - yes. Does it require a little more understanding about your actual failure scenarios and overall architecture - sure... but it also beats having to deal with mirroring. In fact, there were many, many, updates done to availability groups in 2016 that it is much more performant than mirroring. 

Yep. If you wanted to do it inside of SQL Server you could write some CLR code, but I wouldn't advise it. 

This means, in order to OPEN the DEK to decrypt the database data you need to open the Server Certificate. To open the Server Certificate you'll need to open the Database Master Key (DMK). To open the DMK you'll need to open the SMK. This is normally done automatically for you as part of automatic key decryption if all levels of the hierarchy are setup properly. Thus, if we attempt to restore the database using TDE to another instance and that instance does not have the server certificate then it will not be able to read the database and cannot open the database. This is why the Server Certificate is required and we can open it properly. Encryption Hierarchy: $URL$ 

Worker thread exhaustion Slow DMV queries Slow Cluster API responses Slow or Unresponsive Instances of SQL Server 

This will cause SQL Server to use very little resources to read and write so as not to create too much contention on the files and volumes. To get the best backup/restore performance you'll need to backup and restore to/from multiple files and will probably have to change the buffercount and maxtransfersize options also. 

During the time the secondary replicas are removed from the AG, continue to take log backups as normal. This will facilitate the reuse of the log so that it doesn't grow out of control. Keep these log backups handy and ready for action. Once the affected secondary replicas are no longer affected, copy all of the log backups taken while the secondary replicas were out of the AG and apply the log backups to those databases. When applying the log backups make sure to keep the databases in a restoring state by choosing on each log restore. Finally, suspend the log backups and restore any final ones that were taken while restoring the older ones. This will bring the databases on the previously removed secondary replicas to the same time frame as the primary and any other secondary replicas. Once the final log backup has been applied and the databases still left in a restoring state, add the replicas back into the AG. When this happens, since the databases are still in a restoring state and have been restored to the last log backup the AG will be able to join the replicas and databases without issue. There will be a short period of time where the replicas will need to catch up. Once the secondary replicas and databases are rejoined, resume log backups as normal. This would be the ideal process as it keeps your AG intact (for any unaffected secondary replicas), continues to leverage the listener for your applications, can still provide HA and to some extent DR depending upon the replicas available, continues to allow for log backups and re-use, stays transparent to the end user. 

Sure, you log it manually when you figure out what the root cause is. There is nothing automatic that will magically give you a root cause for the failure - that's your job. 

That depends, but my gut instinct with the data you've given is - no. Sure, you'll potentially save some of that space as a plan stub will still take memory just not all that much (compared to your 1 MB plans). So you'll net memory, we get that. However, we don't know how many of those plans were executed a single time, and then some point later while still in cache executed again. This brings up the question about compilations/recompiles and the cpu utilization to go along with it. If you have a good bit of headroom then it may be a trivial issue (pun intended). If your server isn't under memory pressure, I would not expect to see too much of an improvement in terms of "performance" depending on how you want to classify that. If you're swapping and having some slight memory pressure this could alleviate it for a few moments - though upping the VM memory would have the same effect at a much faster implementation without negative side effect cost. 

Log blocks are shipped, not log records, and NOT just committed transactions Yes, there is information already on the secondary about those transactions Readable secondary servers have read committed (default) isolation level mapped to snapshot isolation level automatically If you need the data on the secondary, avoid large long running transactions 

There is no way to complete this with just SQL Server and the interfaces it exposes. In order to do this you'll either need an appliance between the clients and SQL Server (the users connect to the appliance endpoint and the appliance does the redirection) or you can implement your own through a cname and some type of configurable load balancer like an F5. There is nothing in 2016 to change this behavior AFAIK. 

You're correct on the first part and I don't know what you are considering the "usual" backup schedule. If you have the log backups, you can continue to roll forward through the bad full and covering differentials. You're definitely going to want more than a single recovery sequence. 

If you use the CHECKSUM option, then yes it should be found but only when using a or . If you don't check it, you'll never know. 

No, this is the name of a specific version of the filter driver. For example, here is a system with the 2016 one loaded . ReFS is a file system, this is a filter driver that sits between the filesystem and the miniport driver. It's actually quite disconcerting that this is a legacy filter driver as denoted by the .10 at the end of the altitude... hmm. You'll also notice it has quite a low altitude, which is generally not acceptable for 3rd party filter drivers. 

This is troubling as it should definitely not be the case. Certain workloads are not for in memory tables (SQL 2014) and some workloads lend themselves to it. In most situations there can be a minimal bump in performance just by migrating and choosing the proper indexes. Originally I was thinking very narrow about your questions regarding this: 

Then I wish you the best in your trial and error to figure out what works and what doesn't work for each specific type of interaction of DML, row store, etc. The only other recourse you may have is talking with Microsoft if you're a partner and attempting to obtain help with this endeavor. Disclaimer: I work for Microsoft. 

This is a "feeling" now you'll have to put this feeling into tangible, can be tracked and trended. This is how you can make predictions about your workload. What would this look like? There are three areas you'll need to document: What are the limits of my hosted solution? There may be more than this, it's just to get you started thinking. 

One of the use cases for Distributed Availability Groups is for extremely low downtime cross cluster migrations, you're correct. If you're using Windows Server 2012R2, you can do rolling cluster upgrades [NOT available in 2012, must be R2] without needing to do Distributed Availability Groups. 

Give the CNO Create computer objects, list properties, read properties, write properties over the OU it resides in. Create the listener through SSMS/TSQL/Powershell 

Unless you set the proper trace flag. I'm not going to reinvent the wheel, here, Paul already has a great post on this. 

The cluster detecting an issue The cluster arbitrating of resources SQL Server recovering the databases 

The error is saying, in plain English: You have a databases that has, inside of it, a database master key. That database master key is encrypted using a password. I need you to give me that password so that I can decrypt any objects that might be encrypted using it. Here is a super quick repro to give you the "message": 

Set the listener for port 1433. The listener uniquely identifies the instance of SQL Server and there can be multiple listeners all using 1433 as each listener has a unique IPv4 and/or IPv6 address. Note that there are other means to which SQL Server can be connected through when there are multiple instances on the same server which may or may be appropriate or correct ways of connecting. 

Depends on how much you like/dislike/abhor your SAN admins. If you want to piss them off, fire them all off at the same time and listen to them complain about their enterprise class SAN not being able to handle an enterprise class workload. Otherwise, yes, stagger them.