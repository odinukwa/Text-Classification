I've noticed that there appears to be a delay of ~ 3 seconds for Bob seeing events. The tail -f will echo a logline, then pauses for around 3 seconds, then it spits out 3 seconds worth of log lines, then another 3 seconds of nothing, then outputs another set etc. However, if on Bob I do: 

It seems to be waiting at that point for quite some time. I thought of using vnc to connect to the box to see what's going on. Even though I've used , my understanding is that the VNC display should still get created by default. However, this is the output of : 

We have a number of applications that generate fairly large (500Mb a day) logfiles that we need to archive/compress on a daily basis. Currently, the log rotation/moving/compressions is done either via custom bash scripts and scheduled via Cron, or in the application's code itself. What (if any) are the advantages of using a system daemon like logadm? (These are Solaris boxes). Cheers, Victor 

I have an IoT device that communicates with a remote server via HTTPS over port 443. I would like to intercept the HTTP/HTTPS communication - e.g. using Charles or mitmproxy. If it was a desktop or Android device, I could setup the proxy's SSL certificate in the certificate store. However, this device doesn't really have any HTTP settings - so I'd need to setup a transparent proxy right? However, how do I get it to trust the SSL certificates presented by Charles/mitmproxy? Thanks, Victor 

The site seems to go longer than before, but still dies. Checking the list of processes, I have (third column is physical mem, fourth column is virtual size): 

The dialog asks me for a realm, as well as the hostname of the server, both of which I enter. However, it then seems to hit an error: 

On the front indicator panel of the drive, there is a steady amber light (i.e. critical failure). I have a SATA-to-USB adapter, I know the physical adapter is the same for SAS, but I'm guessing this won't actually work for connecting to a SAS drive. From a quick search, it seems I'd need to get an enclosure and an external HBA. I'm wondering, what are the chances of any data recovery at this stage? My understanding is that in general enterprise drives are more prone to simply reporting as fatal, rather than returning possibly corrupt data. Would we be able to get any data at all from the failed drive? And if we can't, I suppose, with the striping, depending on the stripe size would you be able to get small chunks? 

Two way Intel E5504 @ 2GHz, 24GB RAM, 12x32GB Intel X25-E SSDs in RAID10. Intel Core2 6400 @ 2.12GHz, 3GB RAM, simple 80GB SATA drive. 

I have an Openbravo POS workstation with a customer display pole. The pole is VFD-860. When I click on the products on the Sales page of Openbravo, the display responds but displays garbage: 

So it is not exactly broken. Com port is correct because otherwise it would not respond at all. There is a configuration utility with the pole but it is either the wrong version (it says it's for VFD660_460 but the documentation at $URL$ says that VFD860 uses software which installs itself to Start menu > Programs > VFD-660_460) or there is some other problem because whatever choice I make there, after clicking Accept I always get Device Time Out error (even though I see another garbage character appearing onto the display the second I click Accept). How to fix that? 

I have a Strongswan IKEv2 server and I can connect to it from Windows 10 using built in VPN client but I cannot ping the subnet behind the vpn server. It only works when I manually add a route to the subnet with route add 192.168.12.0 mask 255.255.255.0 10.100.0.1. Basically the same issue that strongSwan server with Windows 7 clients doesn't route traffic. Is it possible to automate it from the server side (i.e. I don't have to create a bat file on every client desktop to add the route)? 

I need to configure Postfix to send for a specific domain. I have tried to google for returning custom error codes for a domain with no luck so far. 

I have a 2.7 TB virtual disk (LSI MegaRAID controller with ten 600GB SAS drives configured in RAID10) under Linux. I am sharing this disk to a remote ESX host via ISCSI. Unfortunately ESX will only make a 740GB VMFS partition if you present it LUN greater than 2TB. I could make a 6 disk RAID10 (which would be smaller than 2TB) but I really don't want to lose spindles (IOPS). Is there a way to split this big RAID10 virtual disk up (for ESX) in Linux? 

I'm attempting to follow the guide on the Ubuntu wiki for installing and setting up Kerberos. I am running Ubuntu 14.04 (LTS) 64-bit. I have setup avahi-daemon in order to provide .local DNS names. I have then run: 

I'm trying to install memory into a Dell R610 server, and am honestly at wit's end. The DIMMS I have are: 

I have a FreeNAS server setup at my parents place. It was previously running FreeNAS Coral. This had a single ZFS volume called 'datastore'. It's a RAIDZ-1 volume, comprised of 4 x Toshiba 5TB disks. For some reason, that installation seems to have borked itself. Anyhow, I re-installed the boot USB with the latest FreeNAS 11 and booted up. I then went through the wizard to import my old ZFS volume. It seemed to import the volume fine - the GUI also prompted me to update the ZFS pool version, which I did. However, I now notice that I seem to be missing several ZFS volumes - yet no errors are being reported. Output of : 

However, this didn't do anything - and from the port 631 - is this just for the CUPS control interface anyhow? (which actually worked via ZeroTier even without this line). Is there some other config that controls how IPP works via CUP? 

There should be other volumes - e.g. "datastore/joo", or "datastore/photos" etc. Any ideas of what's going on? Could the pool version somehow have borked something? =(. The fact it reports 12 TB free is pretty worrying - but I am hoping the data is on the disks there somehow....any suggestions on getting to root cause, and/or recovering? $URL$ Update - output of : 

Is there an easy way to find out what exactly is going on? My understanding of Apache's innards isn't that good, but I would have thought we wouldn't need this many concurrent processes to serve up a page like this, with this sort of traffic. We did inherit the app, so we don't know much about it's insides, but it's a fairly basic CMS-type site, showing a few search results, I didn't think it would need this sort of grunt. I did run ab against the site, I was getting a fairly lousy request rate (well under 50 a second), but that may have been my poor choice of settings - a lot of those requests seemed to fail. Where should I be looking for information on what's happening, or any troubleshooting tips I could try? Cheers, Victor 

That leaves the other redundancy features you've got here, which are the switch stack itself, HSRP, and anything on the Astaros. My bet is on the failure recovery mechanism on the Astaros. Since you mentioned that one is "master", that implies that only one is active at any one time. What kind of timers are setup on the Astaros devices for failover? Do you have any logs that indicate how long it takes the standby device to go active after the switch fails? Spanning-tree doesn't seem right because of the fact that all the STP is being done on one switch, and because of the downtime. The switch stack (at least on 3750 stacks) failover should be faster than that too, although you might hookup a console to the secondary switch to see if its taking a long time to take over as master. HSRP (assuming its running at the provider and not on your switches) will also fail a good bit faster than that, and shouldn't be affecting you. TL;DR -- I think it's the failover timers on your Linux boxes that are causing the delay. Second place goes to the switch stack taking a long time to have the secondary switch take over as master. 

log-input will cause the router to generate a syslog message every time the ACL entry is triggered, including the MAC address of the packet. For more detail, you could use "debug ip packet acl [detail]", which should be run with an ACL filter to keep it from cratering your router. Finally, you can do "debug ip packet acl dump", which is a hidden command. This will actually show you the full contents of the packets that match . Seriously un-recommend running this on a production router without a very specific ACL for a very small flow. The correct answer is to configure a SPAN port on the downstream switch that mirrors all traffic on the router port to a Wireshark (or similar) capture device. 

I have googled high and low but there doesn't seem to be any example doing raid10 with megaraid (only the syntax). Can anyone explain what is wrong? 

This will mount the CD disc, make it the only package repository and install a kernel that has support for more than just a handful of vmware/xen/etc virtual devices. After you get it working, restore /etc/apt/sources.list file from the backup and update aptitude again. 

Both machines run Windows Server 2008 R2 now and have 10Gbit Supermicro AOC-STGN-i2S (actually they are Intel 82599 bearing Supermicro logo) in PCIe x4 slots- with a SFP+ direct attached twin axial cable between them. The second server is for testing only. First I installed ESXi on the 2nd and used the 1st as a datastore. I noticed that according to CrystalDiskMark, a VM on ESX only got 325 MB/s seq transfer rate (tried with both NFS and ISCSI). I ran the same test on the first server locally and got ~1000 MB/s. I wondered if the network link really kills 2/3 of speed, so I replaced 2nd's hard drive and installed Windows Server 2008 R2 and tried Jperf and NTTtcp. Jperf showed 400 MB/s and NTttcp showed 4300-4600Mbit/s. Windows Task Manager showed some 600 000 000 bytes per interval which translates to 4.47 Gigabits. I verified that both ends had full duplex and tried toggling jumbo frames on and off both ends but the difference was only 580 000 000 vs 600 000 000 bytes per interval. Why the throughput I'm seeing is only about half the theoretical maximum of 10 gigabits? ADDENDUM NTTtcp command lines: 

Is it possible to use .htaccess file to rewrite this: $URL$ into this: $URL$ If it is then how? EDIT: The goal is to avoid any updates to .htaccess when a new customer is added EDIT2: I see that my question has been too vague. I have a CMS that is being used for lots of different customers (www.foo.com, www.bar.com, www.somerandomname.com). Each customer has it's unique files (design, uploads) in a directory /data/www/cms/customers/XXXXXX/ where XXXXXX is the name of the customer. Apache config file has default DocumentRoot set on /data/www/cms/. What I'm trying to do is to have the URL $URL$ to return a file from the given customers data directory, i.e. /data/www/cms/app/customers/foo.com/assets/css/bg.png.