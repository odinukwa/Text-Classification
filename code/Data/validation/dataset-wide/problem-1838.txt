rsync has a way of doing disconnected copies. In other words, rsync can (conceptually) diff a directory tree and produce a patch file which you then later can apply on any number of files that are identical to the original source. It requires that you invoke rsync with the master and mirror with ; it produces a file. You then transfer this file to any number of other targets, and you then apply the batch to each of those targets using . If you keep a local copy of the last rsynced state (i.e. a copy of what the mirrors look like right now) on the same machine as the master, you can generate this "patch" on the master without even contacting any mirror: On the master: 

Transfer the file from the master to all of your mirrors, and then on the mirrors, apply the patch so to speak: 

Varnish doesn't (yet) support gzip compression, so it might be an idea to swap it around with nginx in front to compress what varnish sends back. Since varnish and nginx don't fight for the same resources (nginx uses CPU for gzip compression, while varnish uses memory) they should run smoothly on the same machine. Varnish now supports gzip compression, so unless you need SSL termination (as suggested in the comments), I would suggest putting varnish directly in contact with the Internet. For http: 

If you want apache in there too (for the ubiquitous mod_foobar support), I'd put it between varnish and the application Update: Updated to include gzip support in varnish 3.0. Added ssl / esi as suggested in comments 

It will make change to reflect It will create a binary patch file (or batch file) called for later use. 

I think it would work if you specified the time period on the contact. Define the contact twice: once with notifications at night, and again with notifications only during the day. 

This should give you a good night's sleep even though an escalation has been running for a few days. Note: I haven't tested this myself ;-) 

tl;dr Think about the services that your software provides, send alerts when these services fail, or when the risk of a failure of these services increases. Service Level Agreements The theory behind monitoring strategies is to tie monitoring and alerts to some sort of service level agreement. After all, you want to be alerted to the fact that you're losing money, not necessarily that there's a spike in the number of TCP connections to nji0019.myserver.com. There are various tools that will give you tons of alerts, define dependencies between alerts, but many of these checks aren't directly relevant to the service you provide to someone. Breach of service Identify the important services that you provide, such as the ability to serve a web site, and the ability to modify that web site (e.g. a CMS of some sort). Those should be checked (e.g. by monitoring that you can get the web page, and that you can). The failure of these two Services (used here with a capital S) should trigger an alert to notify you. If it's important that the site responds within a reasonable amount of time, that too should trigger alerts. Sort of a "breach of SLA" if you will. Increased risk Usually there's an inherent risk of a Service failing, and often enough that risk is mitigated by the fact that you introduce redundancy, e.g. a second server, or a slave database, or extra network cards... When that redundancy is lost, the Service is still fine, but the risk of the Service failing just went up. This is the second major reason to trigger alerts; that redundancy is gone (e.g. that the second server died), or that there is an imminent danger that the risk will increase (e.g. disk only has 500Mb left, or disk trend indicates that the disk will go full in about 5 hours). What about all those indicators? But check_mk gives me 50-60 checks per host, are these all worthless? No. All this doesn't mean you want to ditch the plethora of automatic checks you get with e.g. check_mk, but it means you should try to categorize each of the checks into what Service(s) might be affected if something does fail. What Service would be affected if the /var/ partition fills up? What Service would be affected if the eth0 interface is down? ... if outbound TCP connections are blocked by some firewall? ... if the number of threads exceeds 800? ... if the database goes down? Example You have 2 web servers, and a database server serving a site behind a load balancer you don't own (e.g. the ISP). The Service you provide is port 80 on the two servers, and they have enormous caches that can survive e.g. database downtime (database on a third server). In this scenario, the complete failure of a web server would not result in the site being down. What has happened is that the redundancy is gone so that the risk of failure just went up. That should trigger an alert. The complete failure of the database might not affect the ability to serve the site at all, because of the well tuned caches in place; This then doesn't affect the Service of serving the web site, but it might affect a different Service, namely updating the web site, or accepting orders... Each Service would have its own level of service that designates how important it is to restore service or to avoid outages Be agile Every time you get an alert, you should do one of the following: - change the system being monitored to fix the problem that caused the alert (e.g. replace the drive or reconfigure logrotate or something) - change the monitoring system to avoid the alert being sent out the next time that situation arises. (e.g. change the levels for "disk free" so that the disk can fill up to 90% instead of just 80%) My own experience I'm mostly familiar with Nagios and its verbose configuration, and have since been hooked on Check-mk's multisite. I recently learned that check_mk has this concept of Business Intelligence (since 1.11) which seems to match this thinking well. You can define that checks in nagios are part of a larger service and have rules that define the state of the "Service" as being a function of the state of many checks, aggregating to the worst or best state. 

Consider also the path for Offline mail caching... You might want to rename the file if you're using two profiles, so that one profile does not overwrite the cached emails of the second profile and vice versa: 

I would your suggestions for an effective solution for a person, who needs to access resources in two Windows domains and wants to use one computer. It's about our CEO, who has accepted a second position in another company. Accessing files and folders isn't big problem. The greatest challenge I see is that he wants to conveniently access Exchange accounts in both companies; he would like to send and receive mail in single Outlook if possible (two profiles?) There is also a challenge with calendars: he would like to have one calendar for all activities from both Exchange accounts. Creating a POP3 account for accessing second Exchange server is a last resort, because obviously there is a problem with scheduling meetings and other calendar related tasks. Forwarding and receiving all mail/tasks on primary Exchange server is inconvenient because simple replying to original sender is disabled; and also when manually changing the recepient, he will receive mail from the wrong address. We were considering Virtualisation, that is setting up an instance of virtual machine inside existing installation and then joining this virtual computer to a second domain. Then installing another MS Outlook. This would of course mean two different Outlook accounts, two different calendars, but would at least enable our CEO to access all information from a single laptop. Does anyone have any other idea? I know setting up two domains on a single computer is a no-go (without much hacking at least), but effective workarounds are appreciate. The thing I am looking here is high usage/efficiency/productivity, but also as elegant solution from the administration point of view. Thank you very much (if you managed to read this through, this is a good sign ^_^ ) 

I have a user in a domain who has access to multiple subfolders in multiple folders. His rights were defined pretty granularly. Now he's leaving the company but will continue to work for a firm as a contracted resource. I need to find all folders he had access to and revoke his permissions, then set him up with a different set of access permissions. Is there any tool (freeware, preferably) that lists all NTFS permissions for a given user? I've tried with AccessEnum from Sysinternals, but the list cannot be filtered by username and is useless for me. I've looked at CACLS, too, but as far as I can tell it displays permissions ordered by file, not by user. Any ideas? 

Further comments: The larger company is planning to implement seconnd solution internally for accountants and other "office" PCs. The infrastructure isn't there yet, but people with experience in this area will work on it. The smaller company would then also have about a third of its computers virtualized and put up on terminal server. 

Next, I attached script to OU Group policy and voila! I wrote Instructions for users, and still helping some to finish setting up the profile. Still much easier as manually configuring all clients. Anyway, the following site was crucial for my breakthru, so it deserves to be linked here: Pay them a visit Thanks to Matthew, his post pointed me in right direction. 

Well, I found the solution that works perfectly. It uses a simple Logon script, so Avery Payne, one might say you were at least partially right :) Outlook (2003 and later, don't know about earlier versions) allows importing the profile info through PRF file. It is a plain text file, so you can write it from scratch, or preferably - export it from existing profile and edit as needed. I used the second variant. I downloaded Office Resource Kit - I was exporting PRF file from 2003 version, so I downloaded Outlook 2003 version (ORK.EXE, download it here). Installed it and got the Custom Installation Wizard, which enables you to export existing profile to PRF file. Needless to say, you first need to configure a working profile on the PC you're exporting from. After successful export, I edited the PRF file to lose unnecessary stuff and to generalize some settings with Windows variables, so that I get user specific info after the PRF file is imported: 

I also wanted to be sure that default user profiles won't get overwritten, so I changed corresponding settings to match: 

For details, read the Whitepaper: Configuring Outlook Profiles by Using a PRF File -> link I created a very simple logon script, almost too simple, actually. I could have checked whether the profile already exists and skip the procedure altogether, but sometimes quick and dirty works just as fine: 

I have a standalone Win 2003 server with Windows Sharepoint Services (WSS3) running on it. I had to rename the server and I had bunch of problems resulting from this. Note that the server is not in AD environment. I fixed the problems with Sharepoint (directly related to a rename) by using steps 1 & 3 from this site (TNX) Nevertheless, there are still strange references to old server name all over the place: in registry, in Windows Internal database, in Sharepoint Central Administration... Most disturbing problem is that Sharepoint isn't able to send email notifications to participants. This is IMO due to the fact, that Simple Mail Transfer Protocol service is still "answering" with old server name. When I try to telnet locally to the mail server (SMTP), I get a response: