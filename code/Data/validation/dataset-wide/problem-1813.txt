Still, all but 205 MB of memory are used in that example, even if it is by cache. As it was explained to me (or at least, as I understood it), if a program suddenly requests > 205 MB of memory, that chunk comes out of swap, so that the process can have the memory chunk, then it comes out of swap once it gets a chance to free memory being used by cache. As pfo mentioned, the underlying mechanisms are complex, and far beyond my understanding. I asked Ted Ts'o nearly this identical question at a class he was teaching at LISA, and I've given you essentially the same answer he gave me, or at least as I understood it. Incidentally, I suspect that the memory "used" in swap isn't actually used by anything at this point, but that it just hasn't been freed yet. 

I think it depends on what you're using for a monitoring system. Nagios has NSC++, you can always setup a Windows based snmpd, and various commercial monitoring software packages provide other agents for you to install. Do you have a monitoring system up and running yet, or is that what your goal is? 

Here's what I do Label each cable I have a brother P-Touch labeler that I use. Each cable gets a label on both ends. This is because if I unplug something from a switch, I want to know where to plug it back into, and vice versa on the server end. There are two methods that you can use to label your cables with a generic labeler. You can run the label along the cable, so that it can be read easily, or you can wrap it around the cable so that it meets itself and looks like a tag. The former is easier to read, the latter is either harder to read or uses twice as much label since you type the word twice to make sure it's read. Long labels on mine get the "along the cable" treatment, and shorter ones get the tag. You can also buy a specific cable labeler which provides plastic sleeves. I've never used it, so I can't offer any advice. Color code your cables I run each machine with bonded network cards. This means that I'm using both NICs in each server, and they go to different switches. I have a red switch and a blue switch. All of the eth0's go to red switch using red cables (and the cables are run to the right, and all eth1's go to the blue switch using blue cables (and the cables are run to the left). My network uplink cables are an off color, like yellow, so that they stand out. In addition, my racks have redundant power. I've got a vertical PDU on each side. The power cables plugged into the right side all have a ring of electrical tape matching the color of the side, again, red for right, blue for left. This makes sure that I don't overload the circuit accidentally if things go to hell in a hurry. Buy your cables This may ruffle some feathers. Some people say you should cut cables exactly to length so that there is no excess. I say "I'm not perfect, and some of my crimp jobs may not last as long as molded ends", and I don't want to find out at 3 in the morning some day in the future. So I buy in bulk. When I'm first planning a rack build, I determine where, in relation to the switches, my equipment will be. Then I buy cables in groups based on that distance. When the time comes for cable management, I work with bundles of cable, grouping them by physical proximity (which also groups them by length, since I planned this out beforehand). I use velcro zip ties to bind the cables together, and also to make larger groups out of smaller bundles. Don't use plastic zip ties on anything that you could see yourself replacing. Even if they re-open, the plastic will eventually wear down and not latch any more. Keep power cables as far from ethernet cables as possible Power cables, especially clumps of power cables, cause ElectroMagnetic Interference (EMI aka radio frequency interference (or RFI)) on any surrounding cables, including CAT-* cables (unless they're shielded, but if you're using STP cables in your rack, you're probably doing it wrong). Run your power cables away from the CAT5/6. And if you must bring them close, try to do it at right angles. Edit I forgot! I also did a HOWTO on this a long time ago: $URL$ 

I admin 75-100 servers (physical and virtual) in a 24x7x365 environment. It's a financial services company with worldwide clients, so the service itself can have no downtime. We've got 4 physical sites (2 datacenters, 2 offices) with a small userbase (around 20 users total, varying from very technical to very non-technical). There's around 40TB of SAN storage and 5 WAN links. I feel overworked. 

The best way to do network mapping on Linux is to use 'nmap', which is literally, network mapper. You can do it one of two ways. The first is a ping scan, which will just send a ping to IPs and report back the IPs that respond. For instance: 

I have heard great things about Netapp, but I haven't had the cash to pick one up. I have had two SNAP! appliances, and the performance was horrible on both. One has been junked, the other relegated to near-offline of probably-useless data. 

It depends on what's wrong. In some cases, the part that's "bad" can be replaced (in a clean room), or the platters can be moved from one drive to another (again, in a clean room). Worst case scenario, I have heard of microscopic evaluation of the platter surface, though I can't imagine what that would cost. 

wow, RAID5 over RAID5? Want to discuss performance problems? You'll have tons. The host you hang those off of will have kittens computing parity, writing that parity to 3 drives and then computing the parity of THAT parity and writing it to the 4th drive of that set. WOW! Lets talk about RAID10. It's essentially RAID 1, but you divide up your drives in half and mirror that. It's fault tolerant in that you can lose 2 drives and still be fine, plus the performance is outstanding. If you don't need an insane amount of space, but you've got a 24TB array sitting around with nothing better to do, but it absolutely positively has to be up, then you might consider RAID60. It's essentially RAID6 using mirrored sets of drives. You'll lose around half your drives, and performance will be bad, but you will be nearly guaranteed that the data will be there. Really, I'd go with RAID10. It performs well and works fine. I second Evan's opinion that you probably shouldn't make giant RAID sets out of that many disks, because as he says, things like fsck and chkdsk will take forever, and but more importantly in my mind, because the statistical likelihood of a read error goes up as individual disk size does. I'd recommend 7-10 disks per set. You could create 3 very decently sized RAID volumes with that number of spindles. Whichever you pick, remember to leave a couple of disks at hot spares, so that you can start rebuilding immediately, rather than having the array wait for you to replace them. As soon as a disk dies, the clock starts ticking for another one to go. 

This is a stab in the dark, but can you right click on the command icon, and select "Run as" and select an account with administrator privileges? 

Incidentally, since this is an enterprise network, have you given thought to providing desktops with CentOS, rather than Fedora? The length of support is much longer, and you're dealing with much more stable software. It's definitely possible to maintain an internal repo for CentOs as well. I do it for my servers, and if there are any packages that I need, I just put them on my local repo and install them, rather than using a large repo that I can't control, like EPEL. 

Again, I can't recommend RAID 0 ever, except in the most unimportant cases. I second Tom's idea of RAID 1 to mirror the system drive where you install ESX(i), but I don't see the need to RAID-6 your drives, and lose the 2nd drive to parity information. The reason that RAID-5 is falling out of favor is because of performance (due to the way the information is written across drives) and failure rate (because the likelihood of an unrecoverable read error goes up as drive capacity goes up). RAID-6 doesn't improve performance over RAID-5 when it comes to writes, and your 8 300GB drives aren't anywhere near the capacity where it starts to become statistically significant that you'll fail during a crash.