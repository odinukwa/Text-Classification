By focusing on (d) and (d') it's clear that the solution to the transformed problem is the transformed solution to the original problem. In the transformed problem, the primary bounding ray has slope $r/b$ with $b > r > 0$ (still in Case 1 here) so its slope is $< 1$; therefore as we start analysis of the transformed problem as before, we'll swap the coords so that its slope $b/r > 1$. Notice that what we've done so far is perform one iteration of the euclidean algorithm; that is, $a$ has been replaced by $b$, and $b$ has been replaced by the remainder of $a$ divided by $b$. Repeat as often as we find ourselves in Case 1 (keeping the "primary" designation on the same one of the two rays even as they get transformed). Since the $a,b$ values are following the euclidean algorithm starting with two of the original input numbers, well-known analysis of the euclidean algorithm tells us Case 1 can happen at most $O(n)$ times: that is, if we get all the way to the end of the euclidean algorithm, $b$ will become 0 which will take us out of Case 1 (if we haven't already left it before that). Case 2: The shear makes the primary bounding ray horizontal, and the sheared secondary ray still points into the first quadrant. Exactly as in Case 1, (a')=(b')=(c')=(d') and so the solution to the transformed problem is the transform of the solution to the original problem. Furthermore the transformed problem is now a case of EASY PROBLEM #1; solve that, inverse-transform the solution back to the original space; done. (When this is encountered recursively from Case 1, it means we've made it all the way to the end of the euclidean algorithm.) Case 3: The shear takes the secondary bounding ray direction out of the interior of the first quadrant, to either the $x$ axis or the fourth quadrant. (This is regardless of whether the primary bounding ray direction became horizontal or stayed in the first quadrant). This case is relatively easy to solve, but we have to be careful-- in this case the transform of the point inside the quarter closest to the intersection (i.e. satisfying (a)) is not necessarily the point in the transformed quarter closest to the transformed intersection (i.e. satisfying (a')). However, it is true that the transform of the point satisfying (d) is the point satisfying (d'). Therefore the transformed problem, though easy, is not in the form of the original problem. Instead, it's a case of: EASY PROBLEM #3: The bounding ray directions both point into the +x half plane, but do not both point into the interior of the same quadrant, and we are asked to find the point satisfying (d) rather than (a) (which are not necessarily the same in this case). So I've described an algorithm for transforming the original problem into one of three easy problems, in $O(n)$ iterations. Complexity analysis The number of iterations is $O(n)$, so the overall runtime is $O(n)$ times the cost per iteration. So how expensive is an iteration? Each iteration consists of a small constant number of arithmetic operations, each of which is at most $O(M(\mathrm{num\ digits\ in\ operands}))$ where $M$ stands for the cost of one multiplication (see the wikipedia article "Computational cost of mathematical operations"). But how many digits is that? Well, at first glance it appears that intermediate and final values could have up to $n^2$ digits which would make the overall runtime $O(n M(n^2)))$. Furthermore one might suspect the $n$-digit witness point could help decrease this bound. But the following closer analysis reveals it's not as bad as that, and the $n$-digit witness actually makes no difference at all. Each transform is a shear followed by a coord axis swap: $$ \begin{pmatrix} 0 & 1 \\ 1 & 0 \\ \end{pmatrix} \begin{pmatrix} 1 & 0 \\ - q_i & 1 \\ \end{pmatrix} = \begin{pmatrix} - q_i & 1 \\ 1 & 0 \\ \end{pmatrix} $$ where $q_0, q_1, ...$ is the sequence of quotients $q$ occurring in the euclidean algorithm (possibly cut short by Case 3). So the matrix norm of the cumulative matrix is $\leq$ the product of the $q_i$'s, which is at most the original value of $a$, which has $n$ digits. And the same can be said about the cumulative inverse matrix. That tells us the maximum number of digits occuring in the answer or any intermediate value is $O(n)$. Therefore the overall runtime is $O(n M(n))$. Incidentally, the sequence of 2x2 integer matrix computations being followed is actually well known as the EEA (Extended Euclidean Algorithm) for CRT (Chinese Remainder Theorem). If taken all the way to completion, the cumulative matrix represents an invertible linear transformation with integer coeffs that takes the original primary ray direction to a coordinate axis, which can be a useful building block for many applications (notably Project Euler problems :-) ). So one might think we could simply use a prepackaged EEA implementation as a building block; but I don't see how to make that work for this problem, due to the possibility of needing to stop early due to Case 3. 

The answer is no: it is not possible, in general, to reduce a matrix over a principal ideal domain (PID) to a diagonal (or trigonal) matrix by means of elementary row and column operations. (This topic has been discussed in this MO post). The following example is a result proved by P. M. Cohn [1, Theorem 6.1 and subsequent discussion] in 1966 and doesn't rely on some $SK_1$ obstruction. 

The commutative case mentioned in OP's question was solved by P. M. Cohn [2, Theorem 3], should $R$ be finite or infinite: 

This answer presents an alternate proof of users' negative answer by proving directly that a finite ring whose only unit is its identity must be a Boolean ring, hence commutative. The proof given below is based on a result by Melvin Henriksen. It doesn't rely on the Artin-Wedderburn Theorem and turns out to be fully elementary. Following Melvin Henriksen, we call $R$ a UI-ring if $R$ has an identity element $1$ and $ab = ba = 1$ for $a,b \in R$ implies $a = b = 1$. We have 

The statement "a local ring whose maximal ideal is principal is Noetherian" is (I think) false. The ring of germs about $0$ of $C^\infty$ functions on the real line seems to be a counterexample since $e^{-1/x^2}\in \left(x^n\right)$ for all $n\geq 1$. 

The only sneaky groups that I can think of are similar, in spirit, to the following example. Let $\langle a_1 \rangle \times \langle a_2\rangle = C_2\times C_2$ act on $\langle b\rangle \times \langle c \rangle = C_3\times C_5$ so that $a_1$ commutes with $C_5$ and acts fixed point freely on the $C_3$, and $a_2$ commutes with $C_3$ and acts fixed point freely on the $C_5$. We end up with $$\left(C_3\rtimes C_2\right)\times \left(C_5\rtimes C_2\right) = \left(\langle b \rangle\rtimes \langle a_1 \rangle\right)\times \left(\langle c \rangle \rtimes \langle a_2 \rangle\right)$$ which has elements $ca_1$ of order $5\times 2$, $ba_2$ of order $3\times 2$, and $bc$ of order $3\times 5$. But, since $a_1$ doesn't commute with $b$ and $a_2$ doesn't commute with $c$, we can't have any element of order $3\times 5 \times 2$. Sneaky. We can generalize this construction by replacing the $C_2\times C_2$ with a product of (solvable) Frobenius complements, and the $C_3\times C_5$ with a product of Frobenius kernels, with actions defined in such a way that each complement commutes with one kernel and acts fixed point freely on the others. Furthermore, we could expand one more time to using 2-Frobenius groups of type $(p,q,p)$ (see p.5 here for definition), for example by replacing the $C_3$ and one of the $C_2$s with an $S_4$. So, my question is, 

[1] "On the structure of the $GL_2$ of a ring", P. M. Cohn, 1966. [2] "Tits systems with affine Weyl groups in Chevalley groups over Dedekind domains", E. Abe and J. Morita, 1988. [3] "Subrings in quadratic fields which are not $GE_2$", H. Yu and S. Chen, 2003. [4] "The K-book", C. Weibel, 2013. 

The question as to whether $L(R)$ is complemented was settled by Robert Blair in [1, Theorems 1 and 2] in a broader setting: 

The following proposition answers OP's question regarding the upper bound of $$\tau(C) \Doteq f(C)/\lambda^2(C).$$ Let $B_n$ be the closed Euclidean unit ball of $\mathbb{R^n}$ centred at $0$, that is $$B_n = \{ (x_1,\dots, x_n) \in \mathbb{R^n} \, \vert \,\, x_1^2 + \cdots + x_n^2 \le 1\},$$ and let $\tau_n = \tau(B_n)$. 

Hint: If $\,^tASA = S$ with $\det(A)= -1$, then show that $\text{Tr}(A) = 0$ and $A^2 = \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix}$. Then reduce the case when $A$ is upper triangular. If the above conditions hold, the $\text{SL}_2(\mathbb{Z})$-class to which such an $S$ belongs is called an ambig class (sic). This is also what John Robertson calls an ambiguous class in [2]. From this source, we get 

Let $n$ be the input size, i.e. the number of input digits in some base, or the sum or max of the logs of the magnitudes of the input numbers-- these are all equivalent for the purposes of big-O. Strategy: apply at most $O(n)$ shears (area preserving linear transformations leaving one coord axis fixed) to get an easy problem, solve the easy problem, and then apply the inverse sequence of shears to get the solution back in the original coordinate space. EASY PROBLEM #1: when at least one of the two bounding rays is axis-aligned. EASY PROBLEM #2: when the two bounding ray directions point into the interiors of different quadrants of the plane. Details of the easy problems are omitted, since they are straightforward and not the interesting or challenging part of this problem. So assume we have a non-easy case of the problem: that is, both bounding ray directions point into the interior of the same quadrant; w.l.o.g. both point into the first quadrant. Choose one of the two bounding rays and call it "primary" and the other "secondary". If the primary ray has slope $< 1$, swap the coordinate axes, so that the primary ray has slope $\ge 1$. Let $a/b \ge 1$ be the slope of the primary bounding ray, with $a \ge b \gt 0$ taken directly from the input equations. Let $q,r$ be the quotient and remainder of dividing $a$ by $b$. That is, $q=\mathrm{floor}(a/b), r=a-q b, a \ge b \gt r \ge 0$. Apply to the problem geometry the shear that leaves the $y$ axis fixed and takes $(1,q)$ to $(1,0)$; in other words, the linear transformation that leaves $(0,1)$ fixed and takes $(1,0)$ to $(1,-q)$. Maintain the designation "primary" on the image of the primary bounding ray. This shear transformation decreases the primary ray's slope to r/b, either keeping its direction into the first quadrant (if $r>0$), or parallel to the $x$ axis (if $r=0$). Note that the solution before this shear transformation can be described by any of the following equivalent characterizations (equivalent because both bounding ray dirs point into the first quadrant): 

If I add to the hypothesis that the ring is a domain, then (I think) the statement is true. I'm trying to figure out if this must be true (I suspect not). Is there a nice example of a local Noetherian ring whose maximal ideal is principal that is not a domain? Is there a better, weaker condition to add to the hypothesis so that sufficiency holds? In other words, "if R is a local ring whose maximal ideal is principal, then R is Noetherian if and only if R is [what is the best thing to put here]?" 

For a $p$-group $P$, the number of maximal subgroups is $\sum_{k=0}^r p^k$ where $r$ is the minimum size of a generating set for $P$. You can see this from looking at the maximal subgroups of $P/\Phi(P)$, which is elementary abelian of order $p^r$. What I can tell you is that there is at least one normal subgroup for every power of $p$ up to the order of the group. Sylow theory style orbit counting gives us that the number of normal subgroups of each order $p^k$ is going to be congruent to $1 \mod{p}$, so the total number of normal subgroups in a $p$-group of order $p^n$ will then be congruent to $n+1 \mod{p}$. EDIT: I thought of a bound. $n+1$ is the lower bound, attained by the cyclic group of order $p^n$. There must be at least one normal subgroup for every prime power divisor, so this is the lowest it can go. On the other hand, I claim that elementary abelian groups $E_{p^n}$ contain the largest number of normal subgroups. This is because it has the maximum rank of all groups of order $p^n$. Thinking of $E_{p^n}$ as an $\mathbb{F_p}$-vector space, we obtain the number of subspaces by $$\mathcal{N}(E_{p^n})=\sum_{m=0}^{n}\prod_{k=0}^{m-1}\frac{p^n-p^k}{p^m-p^k}.$$ Here we count the number of ordered combinations of $m$ linearly independent vectors in $\mathbb{F_p}^n$, then divide by the number of possible bases of an $m$-dimensional subspace. Summing over $m$ we have the total number of normal subgroups in $E_{p^n}$. 

An ideal is said to be quasi-primary in the sense of Laszlo Fuchs and Edward Mosteig if its radical is prime. 

Here is a necessary and sufficient condition for $G^{-}(f)$ to be non-empty, taken from [1, Exercise 6.21]: 

You may turn the above proof into a one-liner if you refer to this more general result of Irving Kaplansky [1, Theorem 9.3]. 

[1] P. M. Cohn, "Rings of zero-divisors", 1984. [2] M. Henriksen, "Rings with a unique regular element", 1989. 

It follows from Claim 3 that the presentation $\langle a,b \, \vert r_i = 1,\, i = 1,\dots, n \rangle$ is minimal in the sense that removing any defining relations yields a non-isomorphic extension. Moreover, any presentation of $G_n$ must involve at least $n$ relators. 

If $\mathbb{Z}1_R \simeq \mathbb{Z}/n\mathbb{Z}$ for some $n > 1$, then the condition $\text{gdc}(k, n) = \text{gdc}(k^2, n)$, or equivalently $\text{gcd}(k, \frac{n}{\text{gcd}(n, k)}) = 1$, implies $kR = k^2R$. This is certainly a necessary condition when $R = \mathbb{Z}/n\mathbb{Z}$. YCor outlined the fact that every module over $\mathbb{Z}/6\mathbb{Z}$ is projective. He further extends this remark by mentioning below that every module over a commutative ring $R$ with identity is projective if and only if $R$ is the direct product of finitely many fields, see this MO post for references and a more general statement.