There's no one metric that tells you which of those solutions is the right one. Welcome to performance tuning. Ain't no easy button here. 

It's hard to know this for sure given the limited information here, but I don't think you're quite down to root cause yet. 

It gives you a prioritized list of server issues, and URLs to copy/paste into your browser to learn more about each problem. For example, in the screenshot above, the server's slowing down because some meatball is running a backup right now. You can click on the "ClickToSeeDetails" line to see, uh, details: 

Use the one you're most familiar with. You're better off making smart decisions on a database platform you know well than making first-time-user decisions on a strange platform. All RDBMS's can scale to millions of records these days. If you're going to scale to the billions of records, you'll want to hire a database administrator and architect who's done this kind of thing before. 

There's a lot more around server-level config problems, database schema problems, hardware problems, etc. I wrote a script to quickly analyze servers looking for these kinds of problems: $URL$ 

Yes, this was fixed recently in Service Pack 2, Cumulative Update 3. Here's the KB article: $URL$ "FIX: Log_Send_Rate column in sys.dm_hadr_database_replica_states cannot reflect the rate accurately in SQL Server 2012" 

There's a few questions you need to ask first. Are you using SQL Server Enterprise Edition? Mirroring can be done with either Standard or Enterprise, but snapshots can only be done with Enterprise. Are you already paying for licensing on the mirror? There's a lot of fine print, but you basically get the mirror for free - only as long as you're not querying it. However, if you're going to add snapshots on the mirror and offload the reporting work there, then you'll have to start paying for the mirror. Are you doing synchronous mirroring? If so, the mirror hardware needs to be as fast as possible in order to keep up with writes on the primary. Adding query loads to the mirror may slow down your primary's writes, and that may be a no-go for the business. Is the business okay with data being as-of-yesterday? No one can be running a query while you drop and recreate the snapshot, which generally means you can only refresh it after-hours. Do you have enough space for a growing snapshot? If you do high-volume activities like index rebuilds or reloading ETL tables, your snapshot can grow - by a lot. Ideally, if your database is 1TB (and nearly full), you'll need another 1TB free for the snapshot - not right away, but it can happen if you do things like rebuild all indexes. You'll want to know your change rate, design free space for that, and implement monitoring on both free space and database growth. 

Use those terms to describe which specific Always On feature you're using. Q: In a failover, will it be always on? Neither FCIs nor AGs are really always on. During a failover, your running transactions will fail, and connection retries can fail for 5-60 seconds (or more). It's up to you to build in graceful retry logic in your applications, or build in degraded capability tools like Stack Overflow does. Q: How do I configure Always On? It varies dramatically based on: 

If you have a read-intensive query running on one NUMA node (in this case, 0), then it can experience a lower page life expectancy relative to other NUMA nodes. That's totally normal. To see what queries are running right now, you can use Adam Machanic's excellent sp_WhoIsActive. It's totally free. Some folks even run it every X minutes and log the data into a table so they can go back to see what was running at the time PLE nosedived. 

The DMV sys.dm_os_buffer_descriptors will help you break down which tables from the master database (or any database) are being cached in memory. This query from that Books Online page can be run from the master database and give you a list of which objects are involved: 

Ideally, you do this in a different database so you don't bloat the data & log files with your temporary export work. Then, use SSMS's magical wizards to export the data from your staging table. (If you have to do this a lot, though, check out SSIS.) 

There's different things you can do with certificates: you can use them to encrypt endpoint communications (like mirroring or AlwaysOn Availability Groups), or you can use them to encrypt databases with Transparent Data Encryption (TDE). If you're using them to encrypt endpoint communications, just back up the certificates once and keep them in a safe place. I wouldn't expect shops to routinely rotate those certificates. If you're using them with Transparent Data Encryption, back up both the certificate and the key as described in Books Online: $URL$ With TDE, though, you should routinely rotate the keys. Otherwise, when anybody steals the certificate and key, then they can decrypt all of the backups going forward. When you rotate the key, you'll want to immediately back that up again. You can't run backups while the database is re-encrypting with the new key, but back it up again as soon as the re-encryption process is done. 

Then, consider adding network capacity. I'm guessing your servers have just one 1Gb Ethernet port each, and it's getting saturated during your index rebuilds. You could consider upgrading to teamed cards with load balancing, or to 10Gb cards, or to separate heartbeat network cards. I'm not usually a big fan of separate heartbeat networks because they increase complexity, but that might be the cheapest/easiest solution here. (Start with teaming first though.) The above steps will let you keep your automatic failover AND your index rebuilds. However, I'd be a bad database guy if I didn't suggest that maybe rebuilding every index, every night, is a little bit overboard. Not only is it causing your AG problems, but it's also inflating your tranaction logs, causing those backups to take longer too. For more on that, watch my GroupBy session, Why Defragmenting Your Indexes Isn't Helping. 

This means the cluster network name (tcsdb) can't be registered with Active Directory at the moment. It could be that your newly primary node can't see a domain controller, or can't see a writeable domain controller, may have a network interface down, may be having routing problems, etc. 

I assume you're talking about me, and I'm not sure what post you mean, but here is my thought: Patch your SQL Servers regularly. I'm such a huge fan of patches that I wanted to make it easier for people to find the most current updates for SQL Server, so I built SQLServerUpdates.com. If you ever want justification for why you should be patching, just go back through the list of hotfixes in each cumulative update that you've skipped. You'll find bugs that deliver incorrect query results, and that's usually all management needs to hear in terms of why we need to patch. Here are just a few fixed SQL Server 2017 bugs for incorrect query results - not an old, dusty version of SQL Server, but the brand new one they just released: 

Generally in our setup guide, I recommend that you leave 4GB or 10% free for the operating system, whichever is larger. In this case, 4GB is larger, so you'd set max server memory to 28000. The free memory is for: 

In AWS EC2, you don't get the ability to add additional network ADAPTERS to your VM. You can add additional network INTERFACES, which are just different IPs/subnets/routes/etc, but they're using the same underlying network adapters (cards). You can choose to use instance types with faster networking ports, though. The EC2 instance configuration page lists which ones have 10 Gigabit ports, although I like ec2instances.info for easier sorting and filtering. When you're doing Always On Availability Groups, it's especially important to use this faster networking because high-network operations (like backups to another machine) can result in SQL Server timeouts (disclaimer: that's my blog post) if you've only got GbE networking. (That causes AG failovers.) 

You can have rows in sys.stats without a corresponding row in sys.tables. Classic example: an indexed view. Watch this: 

Disclaimer: I'm one of the authors of sp_BlitzFirst and the First Responder Kit. The wait stats section is on top for a reason: you need to focus on your top wait types. It doesn't really matter how fast or slow your drives are if SQL Server isn't waiting on them. Great example: say you've got a read-only database hosted on a server with enough memory to cache the entire database, and none of your queries happen to hit TempDB. Once the data's up in cache, does it matter how slow your drives are? Kinda - for maintenance tasks like backups and index rebuilds - but not for end user queries. Now, look at your server's top 3 waits: