I would like to attempt to solve the given problem below. I am sorry that I dont know whether such a question is solvable at all, even for just one specific example. If the solution set is empty, I am also helpless about how to show it. I asked this question two years ago at math.stackexchange with no answer or helpful comment. Given two distinct and continuous probability density functions on real numbers, $f_0$ and $f_1$ consider the following set of density functions: $$\mathscr{G}_0=\left\{g_0:\int_{\mathbb{R}}\log\left(\frac{g_0(y)}{f_0(y)}\right)g_0(y)\mathrm{d}y\leq \epsilon_0\right\} $$ and $$\mathscr{G}_1=\left\{g_1:\int_{\mathbb{R}}\log\left(\frac{g_1(y)}{f_1(y)}\right)g_1(y)\mathrm{d}y\leq \epsilon_1\right\} $$ for some sufficiently small $\epsilon_0$ and $\epsilon_1$ such that $\mathscr{G}_0$ and $\mathscr{G}_1$ are also distinct. In other words any density $g_0\in \mathscr{G}_0$ is not an element of $\mathscr{G}_1$ and vice verse. Note that $f_0$ and $f_1$ are known and assumed to be positive so that the terms $\log(g_i/f_i)$, $i\in\{0,1\}$, are well defined. 

My Own Work: For case $1$, I am able to simplify the problem considerably as follows: Assume we are given a certain $\{u_1,\ldots,u_n\}$. It will have $r$ times $1$s and $n-r$ times $0$s. The condition to assign it to either $\mathcal{S}_0$ or $\mathcal{S}_1$ is $$\prod_{k=1}^n \frac{P_1(U_k=u_k)}{P_0(U_k=u_k)}\lessgtr 1\Longrightarrow \sum_{k=1}^n \log\frac{P_1(U_k=u_k)}{P_0(U_k=u_k)}\lessgtr 0$$ Since $U_k$ are identically distributed above given condition can be written as $$r\log\frac{P_1(U_k=1)}{P_0(U_k=1)}+(n-r)\log\frac{P_1(U_k=0)}{P_0(U_k=0)}\lessgtr 0$$ which can be rewritten as $$n\log\frac{P_1(U_k=0)}{P_0(U_k=0)}+r\left(\log\frac{P_1(U_k=1)}{P_0(U_k=1)}-\log\frac{P_1(U_k=0)}{P_0(U_k=0)}\right)\lessgtr 0$$ Hence, we have $$r\lessgtr \frac{n\log\frac{P_1(U_k=0)}{P_0(U_k=0)}}{\log\frac{P_1(U_k=0)}{P_0(U_k=0)}-\log\frac{P_1(U_k=1)}{P_0(U_k=1)}}\Longrightarrow r\lessgtr t=\frac{n\log\frac{q}{1-p}}{\log\frac{pq}{(1-p)(1-q)}}$$ Here we have $r=\sum_{k=1}^n U_k\sim \operatorname{Binomial}(n)$. Hence, $$\mathcal{S}_0=\{\{u_1,\ldots,u_n\}:r\leq t\}\\ \mathcal{S}_1=\{\{u_1,\ldots,u_n\}:r> t\}$$ and as a result of above $$\sum_{\{u_1,\ldots,u_n\}\in \mathcal{S}_0}\prod_{k=1}^n P_1(U_k=u_k)=P[r\leq t\mid U_1\sim\operatorname{Bernoulli}(1-q)]=\sum_{k=0}^t\binom{n}{k}(1-q)^k q^{n-k}\\ \sum_{\{u_1,\ldots,u_n\}\in \mathcal{S}_1}\prod_{k=1}^n P_0(U_k=u_k)=P[r>t\mid U_1\sim\operatorname{Bernoulli}(p)]=1-\sum_{k=0}^t\binom{n}{k}p^k (1-p)^{n-k}$$ Consequently, $$R(n,(p,q))=\frac{1}{2}+\frac{1}{2} \sum_{k=0}^t\binom{n}{k} \left[(1-q)^k q^{n-k}+p^k(1-p)^{n-k}\right]$$ where $$t=\frac{n\log\frac{q}{1-p}}{\log\frac{pq}{(1-p)(1-q)}}$$ as found above. For case $2$, the things are getting complicated because each $U_k$ is Bernoulli with different success probabilities. What I have is the following: $$\{u_1,\ldots,u_k=0,\ldots,u_n\}\in\mathcal{S}_1^*\Longrightarrow \{u_1,\ldots,u_k=1,\ldots,u_n\}\in\mathcal{S}_1^*$$ Similarly, $$\{u_1,\ldots,u_k=1,\ldots,u_n\}\in\mathcal{S}_0^*\Longrightarrow \{u_1,\ldots,u_k=0,\ldots,u_n\}\in\mathcal{S}_0^*$$ This is because of the convexity of $C_\theta$, i.e. $$\frac{P_1(U_k=1)}{P_0(U_k=1)}=\frac{1-q}{p}\geq \frac{P_1(U_k=0)}{P_0(U_k=0)}=\frac{q}{1-p}$$ as $$(1-p)(1-q)\geq pq\Longrightarrow 1-p-q\geq 0$$ is true due to convexity of $C_\theta$. This says that one can populate the sets $\mathcal{S}_0^*$ and $\mathcal{S}_1^*$ with (much) less than $2^n$ computations. I can also write $R^*(n,(\mathbf{p},\mathbf{q}))$ in terms of $p_k$s and $q_k$s as follows: $$R^*(n,(\mathbf{p},\mathbf{q}))=\frac{1}{2}\left(\sum_{\{u_1,\ldots,u_n\}\in \mathcal{S}_0^*}\prod_{u_k=0}q_k \prod_{u_k=1}(1-q_k)+\sum_{\{u_1,\ldots,u_n\}\in \mathcal{S}_1^*}\prod_{u_k=0}(1-p_k)\prod_{u_k=1}p_k \right)$$ Accordingly, 

There are two sets defined: $$\mathcal{S}_0^*=\left\{\{u_1,\ldots,u_n\}:\prod_{k=1}^n P_1^k(U_k=u_k)\leq \prod_{k=1}^n P_0^k(U_k=u_k)\right\}$$ $$\mathcal{S}_1^*=\left\{\{u_1,\ldots,u_n\}:\prod_{k=1}^n P_1^k(U_k=u_k)> \prod_{k=1}^n P_0^k(U_k=u_k)\right\}$$ and the corresponding objective function with $(\mathbf{p},\mathbf{q})=((p_1,q_1),(p_2,q_2),\ldots,(p_n,q_n))$ is: $$R^*(n,(\mathbf{p},\mathbf{q}))=\frac{1}{2}\left(\sum_{\{u_1,\ldots,u_n\}\in \mathcal{S}_0^*}\prod_{k=1}^n P_1^k(U_k=u_k)+\sum_{\{u_1,\ldots,u_n\}\in \mathcal{S}_1^*}\prod_{k=1}^n P_0^k(U_k=u_k)\right)$$ The success probability of the random variable $U_1$ for case $1$, i.e. $(p,1-q)$ and the success probabilities of the random variables $U_1,\ldots,U_n$, i.e. $(p_1,1-q_1),\ldots,(p_n,1-q_n)$ for case $2$ are defined on a continuous convex curve $C_\theta$, which passes through all the points $(p,q)$ as well as $(0,1), (\theta,\theta), (1,0)$. An example of $C_\theta$ is given below with a blue curve and $\theta=0.3$. The set of all such curves are denoted by $\mathcal{C}_\theta$ and any element of $\mathcal{C}_\theta$, i.e. any continuous convex curve $C_{\theta=0.3}$ must be in the orange area given in the figure, due to convexity. Notice that $C_\theta$ is a convex curve of the points $(p,q)$, whereas the success probabilities are $p$ and $1-q$, each with $1/2$ probability as defined above. 

The result of Sion's minimax theorem can be restated as $$\min_{h_l \in X}\sup_{h_k \in Y}f(h_l,h_k) = \sup_{h_k \in Y}\min_{h_l \in X}f(h_l,h_k)$$ where $h_l$ and $h_k$ are continuous functions on $X$ and $Y$, respectively, and $f$, is then a functional. Let $$ h_l^\kappa(x) = \begin{cases} 0, &{l}(x)<\rho \\ \kappa(x), & {l}(x)=\rho\\ 1, & {l}(x)> \rho \end{cases} $$ where $l$ is a positive continuous function on a closed interval of real numbers $I$, $\kappa\in [0,1]$ is an increasing function on an interval $A=\{x:l(x)=\rho\}$, $\lambda(A)>0$, where $\lambda$ is the Lebesgue measure, and $\min_x l(x)<\rho<\max_x l(x)$ is a positive number. Let $$X=\{h_l^\kappa:\forall l,\forall \kappa\}$$ 

My question is whether this is true for general locally compact groups. (Assume $G$ is unimdular and all irreducible unitary representations are admissible if this simplifies things.) The question can be generalized even further as follows: Suppose that $Z$ is just a normal amenable subgroup of $G$ and let $\mathrm{L}^2_\omega(G/Z)$ denote the set of $(Z,\omega)$-equivariant functions $\psi:G\to \mathbb{C}$ such that $|\psi|$ is square integrable when viewed as a function on $G/Z$. Is it true that any irreducible representation of $G$ that is weakly contained in $\mathrm{L}^2_\omega(G/Z)$ ($G$ acts by right translations) is weakly contained in $\mathrm{L}^2(G)$? 

Some hints in the literature led me to an answer, which I find a bit surprising: One can take $U'_\bullet=\mathrm{cosk}_0(U_\bullet)$ and the $1$-cocycle $\alpha\in Z^1(U_\bullet,A)\subseteq A(U_1)$ descends uniquely to a $1$-cocycle in $\alpha'\in Z^1(\mathrm{cosk}_0(U_\bullet),A)\subseteq A(U_0\times U_0)$ along $(d_0,d_1):U_1\to U_0\times U_0$. In other words: Proposition: For any hypercovering $U_\bullet$, the canonical map $Z^1(\mathrm{cosk}_0(U_\bullet),A)\to Z^1(U_\bullet,A)$ is an isomorphism. Consequently, the map $\mathrm{H}^1(\mathrm{cosk}_0(U_\bullet),A)\to \mathrm{H}^1(U_\bullet,A)$ is an isomorphism. This is a nontrivial statement so let me sketch the ad-hoc proof I have. Step 1: We may assume $U_\bullet=\mathrm{cosk}_1(U_\bullet)$. Indeed, since $U_\bullet$ is a hypercovering, the map $U_2\to \mathrm{cosk}_1(U_\bullet)_2$ is a covering, and this easily implies that $Z^1(\mathrm{cosk}_1(U_\bullet),A)\to Z^1(U_\bullet,A)$ is an isomorphism, so replace $U_\bullet$ with its $1$-coskeleton. One consequence of this assumption is that $U_2=\underline{\mathrm{Hom}}_{\mathrm{Simp}}(\partial \Delta^2, U_\bullet)$, where $\partial \Delta^2$ is the boundary of the $2$-simplex, realized as a constant simplicial object (i.e. sheaf) in $\mathbf{X}$, and $\underline{\mathrm{Hom}}$ denote internal $\mathrm{Hom}$ in $\mathbf{X}$. Step 2: Let $\alpha\in Z^1(U_\bullet,A)$. We claim that $a\in A(U_1)$ descends along $(d_0,d_1):U_1\to U_0\times U_0$ to some $\alpha'\in G(U_0\times U_0)$. Let $V=U_1\times_{U_0\times U_0}U_1$ and let $\pi_1,\pi_2:V\to U_1$ denote the first and second projections. Let $S$ denote the simplicial object of $\mathbf{X}$ obtained by gluing two copies of $\Delta^1$ along their vertices. Then $V=\underline{\mathrm{Hom}}_{\mathrm{Simp}}(S, U_\bullet)$. There is a simplicial map $\partial \Delta^2\to S$ which degenerate the edge $\{1,2\}$ into a vertex. This gives rise to a map $V\to U_2$, which in turn gives rise a map $A(U_2)\to A(V)$. Applying this map to the cocycle equation $d_0^2\alpha-d_1^2\alpha+d^2_2\alpha=0$ in $A(U_2)$ gives $0-\pi_2^*\alpha+\pi_1^*\alpha=0$ in $A(V)$, which means that $\alpha$ descends to $\alpha'\in G(U_0\times U_0)$. Step 3: We finally claim that $\alpha'$ lies in $Z^1(\mathrm{cosk}_0(U_\bullet),A)$, which proves the surjectivity of $Z^1(\mathrm{cosk}_0(U_\bullet),A)\to Z^1(U_\bullet,A)$. The injectivity follows easily from the fact that $(d_0,d_1):U_1\to\mathrm{cosk}_0(U_\bullet)_1=U_0\times U_0$ is a covering. To show the claim, it is enough to show that the canonical map $U_2\to \mathrm{cosk}_0(U_\bullet)_2=U_0\times U_0\times U_0$ is a covering. Indeed, if this holds, then the fact that the $1$-cocycle equation holds for $\alpha$ in $A(U_2)$ implies that it holds for $\alpha'$ in $A(U_0\times U_0\times U_0)$. Proving that $U_2\to U_0\times U_0\times U_0$ is a covering amounts to showing that any $3$ "vertices" in $U_0$ can be joined by a "triangle" in $U_2$ locally. But this follows from $U_2=\underline{\mathrm{Hom}}_{\mathrm{Simp}}(\partial \Delta^2, U_\bullet)$ and the fact that $(d_0,d_1):U_1\to U_0\times U_0$ is a covering. I would value references for this proposition in the literature, if you know them. 

I am looking for a reference or an ad-hoc proof of the following fact, which seems to be known to experts: Let $\mathbf{G}$ be a complex algebraic group with maximal (algebraic) torus $\mathbf{T}$ and Weyl group $W$. Let $G=\mathbf{G}(\mathbb{C})$ and let $\mathrm{B}G$ denote the classifying space of $G$; define $T$ and $\mathrm{B}T$ similarly. Then the canonical map between the rational cohomology rings $\mathrm{H}^*(\mathrm{B}G,\mathbb{Q})\to\mathrm{H}^*(\mathrm{B}T,\mathbb{Q})$ is injective and its image is the $W$-fixed elements in $\mathrm{H}^*(\mathrm{B}T)$. There are numerous proofs of the analogous result for compact (real) Lie groups, i.e. when $G$ is a compact Lie group and $T$ is maximal compact torus in $G$. It should be possible to deduce the complex case from the real one because every complex reductive Lie group $G$ contains a maximal compact real Lie group $G_0$ which is a retract, but this fact alone is not sufficient as one has to take into consideration the corresponding maximal tori, and make sure that the corresponding Weyl groups of $G$ and $G_0$ relative to these tori are canonically isomorphic. 

There are two sets defined: $$\mathcal{S}_0=\left\{\{u_1,\ldots,u_n\}:\prod_{k=1}^n P_1(U_k=u_k)\leq \prod_{k=1}^n P_0(U_k=u_k)\right\}$$ $$\mathcal{S}_1=\left\{\{u_1,\ldots,u_n\}:\prod_{k=1}^n P_1(U_k=u_k)> \prod_{k=1}^n P_0(U_k=u_k)\right\}$$ and the corresponding objective function: $$R(n,(p,q))=\frac{1}{2}\left(\sum_{\{u_1,\ldots,u_n\}\in \mathcal{S}_0} \prod_{k=1}^n P_1(U_k=u_k)+\sum_{\{u_1,\ldots,u_n\}\in \mathcal{S}_1} \prod_{k=1}^n P_0(U_k=u_k)\right)$$ $2.$ The random variables $U_1,U_2,\ldots,U_n$ are not necessarily identically distributed; 

Here $\max_{\theta\in(0,0.5)}\max_{C_\theta\in\mathcal{C}_\theta}$ corresponds to all such convex curves. One can just put a single $\max$. 

I tried to work on the problem and I think I am able to resolve some points. Here is my work: Consider the Lagrangians: $$L_0=\int_{\mathbb{R}} g^u{f}^{1-u}\mathrm{d}\mu+\int_{\mathbb{R}}\left(\lambda_0(f-f_L)+\lambda_{00}(f_U-f)\right)\mathrm{d}\mu+\mu_0\left(\int_{\mathbb{R}} f\mathrm{d}\mu-1\right)$$ $$L_1=\int_{\mathbb{R}} g^u{f}^{1-u}\mathrm{d}\mu+\int_{\mathbb{R}}\left(\lambda_1(g-g_L)+\lambda_{11}(g_U-g)\right)\mathrm{d}\mu+\mu_1\left(\int_{\mathbb{R}} g\mathrm{d}\mu-1\right)$$ Taking the Gateux derivatives of the Lagrangians, at the direction of funtions $\psi_0$ and $\psi_1$, respectively, leads to $$\frac{\partial L_0}{\partial f}=\int\left((1-u)\left(\frac{g}{f}\right)^u+\lambda_0-\lambda_{00}+\mu_0\right)\psi_0\mathrm{d}\mu=0\quad\quad(1)$$ $$\frac{\partial L_1}{\partial g}=\int\left(u\left(\frac{g}{f}\right)^{u-1}+\lambda_1-\lambda_{11}+\mu_1\right)\psi_1\mathrm{d}\mu=0\quad\quad\quad\,\,\,(2)$$ Here according to Gateux derivative, $\psi_0$ and $\psi_1$ are arbitrary functions. I take them as integrable functions with $\int \psi_0 \mathrm{d}\mu=1$ and $\int \psi_1 \mathrm{d}\mu=1$ There are actually $3$ cases for each Lagrangian $L_0$ and $L_1$. For $L_0$ we have $$f=f_L, \quad f=f_u, \quad f_L<f<f_U$$ and for $L_1$ we have $$g=g_L, \quad g=g_u, \quad g_L<g<g_U$$ The conditions above $\partial L_0/\partial f=0$ and $\partial L_1/\partial g=0$ make sense only for the conditions $f_L<f<f_U$ and $g_L<g<g_U$. Hence, I can write the maximizing functions as $$f(y)=\begin{cases}f_L\quad y\in E_0\\ f_U\quad y\in E_1\\h_0\quad y\in E_2\\\end{cases}\quad g(y)=\begin{cases}g_L\quad y\in E_3\\ g_U\quad y\in E_4\\h_1\quad y\in E_5\\\end{cases}$$ Based on this result I have following conclusions: 

This question is about a claim given in this paper (page 261, the remark), but without any proof. It simply says that if two sets of probability distributions, $\mathscr{P}_0$ and $\mathscr{P}_1$ (each having infintely many elements and $\mathscr P_0\cap \mathscr P_1=\emptyset$ ), are constructed on some infinite set $\Omega$ in such a way that every $P_0\in\mathscr{P}_0$ is absolutely continuous with respect to a common measure $\mu_0$ and every $P_1\in\mathscr{P}_1$ is absolutely continuous with respect to a common measure $\mu_1$, then there exist no pairs $(Q_0,Q_1)\in \mathscr{P}_0\times \mathscr{P}_1$ such that $$Q_0[q_1/q_0(X)<t]\leq P_0[q_1/q_0(X)<t]\quad \forall t>0,P_0\in \mathscr P_0$$ $$Q_1[q_1/q_0(X)<t]\geq P_1[q_1/q_0(X)<t]\quad \forall t>0,P_1\in \mathscr P_1$$ Here, $q_0$ and $q_1$ are the density functions of $Q_0$ and $Q_1$, respectively. Added: I was able to disprove the claim for the general case. Here is an example: $$\mathscr P_0=\{\mathcal{N}(-0.5+b,1)|b\in [-0.5,0]\}$$ $$\mathscr P_1=\{\mathcal{N}(0.5+b,1)|b\in [0,0.5]\}$$ Then $(Q_0,Q_1)=(\mathcal{N}(-0.5,1),\mathcal{N}(0.5,1))$ satisfy the given two inequalities above. Here is the figure for the first inequality, for $b\in\{-0.4,-0.3,-0.2,-0.1,0\}$, where the blue curve is for the $Q_0$, i.e. $b=0$ 

Notes: $\bullet$ One can consider A or B since both conditions are equivalent. $\bullet$ $p_0$ and $p_1$ are densities of $P_0$ and $P_1$ and the same goes to $q_0$ and $q_1$ with $Q_0$ and $Q_1$. What I know: From Huber's paper (pages 260-261) Theorem 6.1 I know that if the distance is the $f$-divergence, i.e. $D_f$, then A and B are correct. Additionally, if A and B are correct, then $Q_0$ and $Q_1$ minimize $D_f$ (iff condition). Huber considers $$Q_{jt}=(1-t)Q_{0t}+t Q_{1t}\\q_{jt}=(1-t)q_{0t}+t q_{1t}$$ and finds the first and second derivatives of $D_f(Q_{0t},Q_{1t})$. He then shows that the second derivative is $\geq 0$ (convex) and hence $(Q_{00},Q_{10})$ minimizes $D_f$ if and only if the first derivative evaluated at $t=0$ is $\geq 0$ for all $(Q_{01},Q_{11})\in(\mathscr P _0\times\mathscr P _1)$. He shows that this is really the case, hence the claim is true. I think that this result can be strenghtened, i.e. if $(Q_0,Q_1)$ maximizes $D_u$ for all $u\in[0,1]$, then it should satisfy A or equivalently B. I dont know how to proceed. Addendum: It seems that the question eventually boils down to finding $(Q_0,Q_1)$ which maximizes $D_u$ for all $u\in[0,1]$ and fails to minimize $D_f$ for at least one $f$. This will be a counterexample to the claim (of course if there exists such a pair).