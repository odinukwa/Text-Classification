Not an elegant solution but after installing the package using (which fails creating the cluster but installs PostgreSQL), I switched to the user and created the database using . Then back to , I created the cluster using the command. This moved the configurations to and set it all up. 

My first answer is: why is your database even accessible from outside through the Internet? That network traffic really ought to be blocked by router Internet gateway router or firewall. If you really need to allow some connections from the Internet to your database, then limit it to the valid IP address who should be connecting. At this point that's not really a dba question but a network admin question. 

It is the alias used to reference the derived table. In the outer SELECT if you fully qualified the column references they would read 

Teradata's optimizer quite possibly will re-write Method 3 to the same query plan as Method 2. Method 1 will result in an INNER JOIN because the qualification on the table shouldn't be in the WHERE clause but the ON clause of the condition. If you were to place an aggregate step such as a DISTINCT or GROUP BY in the derived table of Method 3 you will find the optimizer will likely satisfy the derived table as an individual step without re-writing the plan. I would suggest that you run the EXPLAIN for each query and compare the output. 

Your best place would be pgfoundry. But you won't find much. Doesn't look like many plugins are available. 

Yes, that would be fine if your design goal is that Persons have only 1 Addresses. With this design, each Persons can have 1 Addresses but two or more Persons can have the same Addresses. It's all a matter of your business needs. If the above is what you are trying to get, then yes, it is correct. However, I think it's most common the other way around where the Addresses would have a foreign key to the Persons because a Persons could have more than one Addresses. As for your constraint to check the postal code - well first off you are missing the space and it's lower case. Whether that will work will depend on which database system you are using. I tested it with PostgreSQL and it does not work. I don't think you can really have such a simple constraint to fully validate a Canadian postal code. For example, there are some letters and some numbers which are never used. I'm a bit rusty on my Canadian postal office codes but I seem to recall that number 5 is never used as it's too similar to S, etc. 

The space consumption for statistics on Teradata is not significant enough to qualify as a disadvantage. For example, the statistics for a single column is retained in a 16KB VARBYTE column on DBC.TVFields The rule of thumb has always been 10% change in the data which you have statistics collected or if they are stale. Unfortunately, stale has never really been clearly defined. Teradata 14.10 will introduce a more automated mechanism for maintaining statistics to help reduce the cost (CPU and IO) associated with the collecting stats using a homegrown maintenance schedule. This enhancement will be supported through Viewpoint. Teradata 14 also introduced some changes with statistics that have to be taken into consideration from previous releases. Carrie Ballinger has done a good job of capturing these changes in her articles on the Teradata Developer Exchange found here and here. Your stats maintenance schedule will be driven by the size of your environment, your ETL schedule, and manner in which your ETL maintains the target tables. We have multiple streams that maintain the same set of large target tables. As such we have moved the stats maintenance for these target tables to an external process instead of within each ETL stream. 

As Zoltan pointed out, unless you have MANY millions of rows, I don't see a scaling issue. There are also many libraries for scheduling things such as Quartz on Java for example. These will store the recurring schedule as a cron-like expression. Because your example above has a flaw, if the recurrence is every Monday, then it's . So you can store a date, or a recurrence pattern. 

Code generators are great! Code generators are evil! 10-15 years ago, I would have said that having a code generator for quickly creating the boiler plate code for database driven applications would have been a great gift to mankind. 5-10 years ago, I would have said code generator sucks, they generate too much duplicate code and that having a data-driven user interface (where the fields on the screen, validation, etc is driven by meta-data in a database instead of coding the screens one by one) would have been a great gift to mankind that supplanted the code generators. Today I would say - write each screen individually. Use existing framework that wire fields and model objects and possibly ORM when doing simple CRUD. But do design each screen to the exact purpose of the screen. Application screens that mirror a RDMS table too much is only good for managing lookup tables. Don't annoy the user with geeky interface that are designed against a computer storage model (RDMS)... make a screen that has only what they need. That may mean that it will save to multiple tables, etc. Who cares. The user isn't a database. So my thought? Don't waste your time making a code generator. 

By default, Teradata SQL Assistant will attempt to query the views , , to populate the Database Explorer. It is possible in your environment that those objects are not accessible to developers or end users via the user. Instead, you may need to modify your connection settings ODBC or .Net Provider to use the X-Views in DBC. These are a collection of views which restrict the rows returned based on the privileges your user account has been granted to access or which you have created. The ODBC DSN, .Net Provider, and JDBC drivers for Teradata have a means to use the X-Views by default to enable database tools such as SQL Assistant or Teradata Studio/Studio Express to populate the database explorer controls "transparently". Try this and see if it works. 

Every table should have a primary key (I really can't think of a reason not to have one). So having a paymentID column to your payment table is definitely a standard design. 

To answer your question on those 2 options, neither seem right to me. A) will lock you in and B) is a lot of work. The current schema you describe is not too bad (except for having the information name ("first name", "square foot", etc) as string instead of an ID referenced to a lookup table. However, this seems to me like a good candidate for a NoSQL database ( $URL$ ). While I never worked with such database, what you describe is a typical scenario that this solves. 

There are a multitude of factors that go into determining which Teradata platform and the configuration of the platform that will suite your needs. Teradata has spent untold amounts of money on intellectual property and decades of experience working with potential customers to help them properly size a configuration that not only meets the immediate needs of a customer but provides them capacity for which the environment can adequately grow and evolve. I would strongly suggest you reach out to Teradata directly and engage them in a pre-sales capacity if your company is considering their technology to meet the needs of your data warehouse environment. For a sandbox environment, you could may be able to get away with using the one terabyte version of Teradata Express on an adequately sized server or consider using Amazon EC2 to stand up a instance of Teradata to complete a proof of concept. It should be noted that either of these options should not be used to gauge the performance of a production environment for service level agreements but whether or not the technology will accomplish what you are trying to do. 

Even more, the returns zeros in all columns and null for - even if I run a manual vacuum on the table. 

If you are referring to the base/pgsql_tmp, then you should be fine. But I don't speak from experience of having done that myself. The only gotcha is that you have to make sure that the location is accessible when your PostgreSQL server starts up. (Ref.: Book PostgreSQL 9.0 High Performance, page 93). The book in question refers to creating a simlink of the base/pgsql_tmp on the OS partition or other less "safe" partition (i.e. non-RAID or the like). 

Most data models are lacking in a good DATE Dimension and thus force developers and report developers to rely on date arithmetic to find date boundaries that are relevant to the business model. (Fiscal Year, Fiscal Quarter, Fiscal Period, Calendar Quarter, etc.) A good CALENDAR table would go a long way to making your life easier. A simple EventDate BETWEEN SYSDATE - 458 and SYSDATE risks truncating dates out of your oldest quarter. Take TODAY as an example: SYSDATE - 458 yields 2010-09-28. If my math is correct the 3rd Quarter of 2010 started on July 1, 2010. You need roughly 548 days to make sure you are covering the entire range of current quarter plus the previous four full quarters. Trouble is that when you this will cause some overlap as your current quarter is partially complete. So you are faced with some additional logic to truncate out the fifth oldest quarter that you don't wish to include. My PL/SQL isn't the sharpest right now to write that logic but I hope the explanation helps shed some light on the approach you will need to take. 

Time for the backup to run is a reason for doing incremental backup on Progress. My backup is running fast enough that I did not need to use this function and I'm still doing only full backups. It also depends on your requirements. For example, if you have heavy financial transactions and you want to keep a backup every hour or something like that, you would need to do incremental (or real time). But unless you have something forcing you to do incremental, I would do full backup, I find that easier to restore. In terms of performance impact, if you have a fast disk array, I haven't seen much impact of doing a backup even during moderately heavy usage. Obviously it depends also on the size of your system. I'm talking about a 37Gb DB.