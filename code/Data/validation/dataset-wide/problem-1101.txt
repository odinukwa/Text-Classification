I had to reread the vendor requirements a couple of times, but I take the instructions to mean the local administrator account only requires elevated permissions during installation. Run one of the following blocks prior to the app install to set this up. 

While this may not look as pretty with quick strings, it may clear up issues with large blocks of DSQL. To further clean it up, I also suggest you mask special characters with variables such as: 

The statements are all performed together in the which means it should be the only place you need to make a change. In support of the advice provided by @RobertCarnegie in the comments, I also recommend putting your table creation/truncation logic within a stored procedure which will be called from the step in the package. Basically, after this change the Execute Script Task will look as follows: 

The key differentiator between the recommended locations (e.g. local storage, a SAN, or an iSCSI-based network) and a network share is quite simply, redundancy. All of the recommended approaches provide an option for redundant paths for I/O to take to persistent storage. For instance, you can RAID local storage, providing redundancy if any disk fails. With either the SAN or iSCSI-based network storage approaches, these technologies use Multipath Input/Output (MPIO) drivers, providing redundancy to the storage. A network drive, in contrast, does not employ or allow for any redundant I/O paths. If a Network Interface Controller (NIC) fails on either end, the share likely disappears. Even if you have multiple NICs, there will still be a brief outage as a different IP address will now host that share, so any data sent to the old/failed IP will timeout and disappear. Basically a network share wasn't designed with this level of redundancy, and a loss of data mid stream may corrupt your database (or worse just get lost without a trace). The whole point of a database is to reliably store data and a network share brings that whole reliable aspect into question. 

Sadly, if you want to use Cell-Level encryption (e.g. methods) that shipped with SQL 2008, you will need to alter your queries. There's just no real way around it. If you're running SQL 2016 SP1 or later though, you should be able to minimize your rework by enabling Always Encrypted. This is not a trivial feature to demonstrate, so instead, I'm going to point you toward Robert Sheldon's article from Redgate on how to enable and use this feature. My recommendation is that you use Always Encrypted if you can run the database on SQL 2016 SP1+ (as this feature is now included with Standard Edition, yay!). 

If you don't, enable it immediately, as I've yet to see a downside with this feature. This option became available with Standard Edition with SQL 2008 R2 and it's one of the first sp_configure options you should enable after setting up a new instance. This will help speed up backups dramatically. Another option is to stripe your backups to multiple files. This will increase the I/O throughput of the backup and reduce the backup speed dramatically as well. One of the things I don't like with Ola's scripts is that this setting isn't dynamic or configurable based on the size of the database. What I will suggest though is to specify 4 paths for the DIRECTORY parameter and increase/decrease after testing it out. From Ola's Backup Page about striping: 

If you only require this process to truncate a table, I suggest you create a Stored Procedure that utilizes elevated rights via Impersonation to perform the truncate so you keep your SSIS account at a minimal level of security. First, you need to create a stored procedure that provides your Truncate Table functionality, as follows: 

Since both and are DDL statements that have Server Scope, I would suggest setting up a Server DDL Trigger to capture when your EE is stopped/dropped. This generates a simple notification when either DDL statement is executed, but feel free to alter it to your liking: 

What's the downside of using an NCCI? It's important to know that after a NCCI is created, any data stored within a RowGroup effectively becomes read-only. Changes are not applied to the RowGroup itself, rather a record is marked as deleted and the updated data is stored in a different RowGroup within the NCCI. These changes are cataloged in the Deleted Buffer and the Deleted Bitmap so the engine knows what's valid in a given RowGroup. Proper maintenance will help minimize the performance costs of changing data, but if you have a lot of activity occurring on the newest data within a table (e.g. the "hot" data) between maintenance windows, a filtered NCCI may be the best way to segment the less volatile/warm data from the more volatile/hot data. Why the assumptions are important? If the first assumption is false and you will see more than 1 million records inserted into a table per tenant per week, then you can forgo a filtered NCCI and should instead create the NCCI using the additional keyword. There's a query MS recommends you run before doing this, found here, that will give you a better idea of what this value should be, but the maximum value is 10080 or 7 days time. This will keep a delta store open until either 1 million records are accumulated or the specified period of time passes and then that delta store will be converted into a NCCI RowGroup. This basically allows the NCCI to automatically work like a filtered NCCI with a sliding window. However, if you don't have enough incoming data to periodically fill a RowGroup to take advantage of this use-case, a filtered NCCI may be the better approach. If the second assumption is false, and you don't have a column that can easily define what data is warm vs hot, a filtered NCCI becomes no better than a normal NCCI. Again, use the keyword with an appropriate value and go from there. The cost of a filtered NCCI Filtered NCCIs don't come without a cost; the main issue being the definition of the filter. As hot data eventually becomes warm data, the filter may also need to be adjusted so this additional data can be included within the filtered NCCI. You are unable to change the filter of a NCCI using , so changing the definition of the filter requires that you to drop the index and recreate it with the updated filter clause. This obviously will carry ramifications in that the index won't be accessible during this change, so if your database doesn't afford a long enough maintenance window for this sort of operation, don't use a filtered NCCI. Filtered NCCIs also have some other technical limitations as fully outlined here. The benefit of a filtered NCCI will outweigh its cost when your new data is very volatile and a NCCI isn't providing a good performance boost over its row-store index counterparts. Testing is the name of the game here, but in the right situation, a filtered NCCI sitting on top of a partitioned table for your multi-tenant database could translate to big gains in performance. Hopefully that does a better job of explaining my comment to David's earlier answer.... and again, this is not an answer, just a very long-winded comment. 

If you wanted to add the check as the new step 1 of each job, you would have to delete every step from the job, add the check, then re-add every step back with proper settings as you are unable to edit job step ordering using the stored procedure. 

The first point is what I feel is the most important as related to your question because you can limit your parameters' length, type, etc. This makes is incredibly more difficult to inject nasty code. As to provide an even more complete answer, I've updated your sp accordingly. Interesting enough, in your case because you're trying to parameterize column literals, you'll need to nest sp_executesql statements, so the first nested statement sets column names as literals and the second execution passes in the pagination values, as follows: 

As Mark has mentioned in the comments, you are running into a bit-level mismatch between driver sets. You will need to either install a 64-bit installation of Office (specifically Excel in this case) or install a 32-bit installation of SQL Server and import the data to that instance. 

Basically the source tables in their entirety are trying to get pulled over the linked server and all the joins and filtering are happening locally. When you're not working with millions of records this is often transparent, but when you include large tables you can immediately feel the pain. In your situation, I suggest you push a copy of the table up to your Azure DB and then run this UPDATE statement from a connection made directly to the AzureDB. If you need to initiate the process from a different server, stuff the update into a Stored Procedure in the Azure DB and then execute the SP remotely. Alternatively, if you don't want/can't push that table to Azure, you could convert the query to utilize OPENQUERY for all the remote join operations. This will quickly get ugly, but it's possible. OPENQUERY will force execution of whatever query you pass to it remotely, therefore allowing the join logic against that large table to be done remotely so only whatever records are returned from that query travel back over the linked server. These would then be joined against the local copy of the to identify what records in would need to be updated. I really don't like this approach just to update a table on a remote database, but it's an option. 

This is an old question, but I fell prey to this issue recently and found that SSMS wasn't overwriting a "bad" settings file. To fix it, I had to close SSMS and simply delete the settings file using Windows Explorer. SSMS then loaded with the default settings when I started it back up and I was able to reset things to the way I wanted once more without issue. I suspect something occurred on the network that prevented SSMS from overwriting the old file (as my path is mapped to a UNC copy of the My Documents folder). The settings file location can be found within SSMS at Tools → Options → Environment → Import and Export Settings 

This is a supplemental answer and I'm making some assumptions here, but it looks like you're trying to reduce the time it takes to complete this process overall. In that case you shouldn't limit yourself only to figuring out how to reduce the summary query. I'm not saying you shouldn't prioritize it, but there may be other steps of your process where you can save additional time minimizing how much performance you need to squeeze out of the summary query. I would suggest you upgrade your environment to SQL 2016 SP1 or later if you've not already done so. This opens up a lot of functionality you can use (even with Express edition) that will likely help with optimizations, such as Table Partitioning, Table Compression, and/or Columnstore Indexing. These features can be used individually or in conjunction with one another and should provide some performance improvements so long as you're not currently running up against a CPU bottleneck in your environment. You may also be able to improve your ETL import processes. This article, Guidelines for Optimizing Bulk Import, from Microsoft goes over some concepts that may apply to your scenario. There's a lot of information in it regarding the Bulk Logged Recovery Model, and if you think of using it, I will also point you to Considerations for Switching from the Full or Bulk-Logged Recovery Model which goes over the proper way to switch between the Full and Bulk-Logged Recovery models. There's a lot here, so again, this isn't an answer to your immediate issue so much as an attempt to show you some other areas you can further improve upon down the road. 

You'll need to move the views to a different schema to utilize this security tom foolery. Basically you can permissions on a Schema, but you are unable to limit the privileges to views only. The workaround, as you identified, is to create a schema that only owns the views in question. This schema is also owned by the same principal that owns the base schema where the tables reside so your permissions are a little easier to manage. Below is an example script breaking it down: 

Let's say for argument's sake that I'm getting close to the limit (even though I'm obviously not). Because I don't want run out of valid values in the column, I'm going to create a similar table, but use for the column instead. This table will be defined as follows: 

Since the comments are running long, I'm dumping this to a formal answer. First, in regards to mirroring, there are some hard limitations on the number of databases that can be mirrored between instances. This number can float anywhere up to 50+, though I have often seen it start to affect performance in the single-digit range as instance activity will heavily influence these limits. More info on the details behind why these limits exist can be found here. Because you're using SQL 2008, you are a little more limited in what's available. Basically you're left with Log Shipping or Replication. Log Shipping is lightweight and is what I would recommend if you can accept some level of data loss in the event of a failure. You can log ship hundreds of databases between instances as performance limitations are related more to file-level and bandwidth constraints. Replication, much like Mirroring, also suffers from volume limitations as a publication requires some resources to maintain. What's arguably worse about Replication is that it may require DML changes be made to the data model of a database if tables don't have PKeys. These changes are necessary to ensure records are replicated properly and your subscriber(s) are kept in sync. There are other considerations as well, found here, so Replication is not something I would say is simple to implement and should only be used after ample testing. If you don't think Mirroring, Log Shipping, or Replication will work, your other options are basically hardware related such as SAN-to-SAN replication, or simply upgrade to SQL 2012 or later and take advantage of Availability Groups as stated by SqlWorldWide. 

Does anyone have any tricks they care to share that will help better identify disabled elements within SSIS packages? Ideally there's some manner to adjust the text color to red or or something along those lines, but the default behavior seems significantly lacking in my mind. I've dug through a number of options, but nothing seems to impact the display of only the disabled elements. Here's an example image where 4 of the 6 elements are disabled: 

I think @SqlWorldWide is onto the root cause of your issue, which is that pessimistic concurrency is allowing values to change between the time you make calls to the nested UDFs and when you insert values into Table1. If you either enable READ_COMMITTED_SNAPSHOT or ALLOW_SNAPSHOT_ISOLATION on the database (both are unneeded as they accomplish relatively the same objective here) and run the statement again, I suspect you'll no longer run into data consistency issues. Note: If you opt to enable ALLOW_SNAPSHOT_ISOLATION, please update your SP to include the command, as enabling this database setting also requires the transaction isolation level be explicitly defined in contrast to the READ_COMMITTED_SNAPSHOT setting, which implicitly enables this functionality across all queries within the database (ref). Don't interpret this as both features function in the same way, rather they both achieve optimistic locking which likely will help with your situation. I would suggest enabling READ_COMMITTED_SNAPSHOT over ALLOW_SNAPSHOT_ISOLATION because I'm lazy and I've seen this setting benefit transactions more often than not at the database level. However, if you want to be surgical in your approach with optimistic concurrency, utilize the ALLOW_SNAPSHOT_ISOLATION feature instead which allows you to pick and choose which queries utilize optimistic concurrency via the SET TRANSACTION ISOLATION LEVEL operation.