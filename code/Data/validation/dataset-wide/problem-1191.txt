The values are stored as unicode character sequences (i.e. the binlog knows what encoding they are encoded in sends that information to the slave so that it can convert the values to match the character set of the target column on the slave.) or... The encoded values are just stored as byte sequences, with no record of what the character set was. 

This seems to be a bug that was introduced in MySQL 5.6.10 and has since been fixed (although there is no bug report on $URL$ that I can find). The behaviour described in the question does not occur in the latest version of MySQL. Observed behaviour in MySQL 5.7.5-m15: 

I'm trying to use MySQL's online DDL feature to add a constraint to an InnoDB table without blocking writes to the table. According to manual, this should be possible - the linked table summarising MySQL 5.6's online DDL capabilities has a 'YES' in the Allows concurrent DML column of the Make column NOT NULL row. However, when I actually try this, it fails under absolutely all circumstances I've tried. Even if the column I'm trying to add the constraint to does not contain any values, I get the following strange error: 

I am thinking I know the basics of the problem here, but I want to make sure I know the details before trying to explain it to others. We recently upgraded our third party application software to a new version. This version completely switched the Java Application Stack to a different vendor. After the upgrade we were told by the vendor that other customers who had upgraded were experiencing tempdb growth. I checked my monitoring and found we are beginning to experience the same. 

However, I strongly agree with Aaron Bertrand that you are creating inefficiencies, possibly on a massive scale. SQL is much better at searching/filtering data than Excel. You should, at the very least, consider using any parts of the search to filter the data in SQL BEFORE you pull it down to Excel, that will reduce the amount of data. 

The only time you need to use Schema Generation is if you did top down development and have never generated the schema before. Now that you have previously generated the SQL schema, you can just script out all the objects in SSMS and provide that script along with the SSAS project to other developers. Then the new developer would deploy the SQL scripts and edit the data source in the SSAS project to point to their database. This is the most straightforward approach I believe. 

But mainly I would recommend upgrading to SSIS Enterprise edition and using this connector. Here is the one for SSIS 2014 but you can search for other versions easily. The performance of that connector is much improved. 

In SQL Server Management Studio you can connect Object Explorer to SSAS, expand to the table, right click, choose Partitions, then click to process one or more partitions and do a Process Clear: 

It seems to me when the requests are ending the code stops but the connection stays open and the internal objects are not being deallocated. Can someone confirm this and help me to explain it? I know SQL but have very little knowledge of connection pooling. Can this be corrected by adding a connection setting to their pools, something like SET XACT_ABORT = ON? 

There are different levels of normalization. If your business logic is set up where each contact has an associated account (but more than one contact can be in an account), then there is no reason to put the account ID in the order table because you can get it by going back to the contact table. In other words, account ID in the contact table is a foreign key. 

We are in the unfortunate position of running a data warehouse on the same instance as our production OLTP system. Recently a "power user" joined our company and has begun running SQL statements against the DW that are hogging resources. Yesterday he ran a query that sent our page life expectancy from the usual 200K+ seconds down to 39. I have thought of two solutions to this problem. The first is to move the data warehouse to an Azure database. This protects our OLTP systems from running out of resources, but it comes with a monetary cost. The second solution is to use Resource Governor to limit the resources available for either specific users or the entire DW database. This seems like a free solution, so it is the one that I am investigating first. I have never used Resource Governor before, so I set up a few tests to make sure it would work as expected. I first ran a few big queries with no Resource Governor set up and recorded the CPU usage, Reads, and Duration. Then I created the proper Resource Governor settings and re-ran the queries. The duration rose as expected, but I was surprised to also see the CPU and Reads increase drastically. The CPU usage more than doubled and the Reads increased tenfold. It did protect the Page Life Expectancy from dropping. Is that a typical occurrence when using Resource Governor? Does anyone have any other ideas for limiting specific users or databases from hogging resources? 

The documentation doesn't shed any light on which of the above models MySQL uses. If #1 is true, then our plan will work fine. However, if #2 is true, then trying to replicate queries from the master to the slave is going to go wrong in some way if one of the columns involved has a different character set. For instance, once we've converted our tables from latin-1 to UTF-8 on the slave, the slave might try to store latin-1-encoded strings passed across from the master in a UTF-8-encoded column, resulting in either the stored text not being validly encoded in UTF-8 or representing the wrong characters. The MySQL docs on Replication and Character Sets shed little light on the problem; they discuss potential issues resulting from differences in the global character set of each server, but don't touch on potential problems caused by differences in table or column character sets at all. Is our plan sound, or are we at risk of ending up with corrupt data on the slave? 

The Customer dimension is joined to the measure group at a non-key granularity. If the Customer Name attribute isn't related to the attribute you join on then data repeats in reports. You need to do one of two things: 

My recommendation is not to attempt to do the sequence logic in MDX. You will be disappointed in performance. I would recommend the following approach: First create a view (or a physical table if you prefer) which returns one row per part/customer/person per day showing the currently effective value. The view would look something like this: 

If your drillthrough command includes a dimension attribute from a many-to-many dimension then it behaves as described: showing a row zero to many times. The easiest solution is to create a drillthrough action and mark that action as the default drillthrough action and specify the columns you want to include in the drillthrough results. If you don't include the many-to-many dimensions then it will stop skipping or duplicating rows. 

My monitoring uses the tempdb.sys.dm_db_file_space_usage dm so I am able to get a much more accurate view. I can see the internal object usage constantly climbing and not going back down at all. I set up additional monitoring by using sys.dm_db_task_space_usage and can see the connections for this software have large allocation number with very little deallocation. It also looks like it opens connections and keeps them open but they will often be sleeping, which is consistent with the behavior of an application pool, from what I understand. 

The vendor claimed the only way to clear Tempdb was to reboot the SQL Server. They also don't really know what is happening because they are only looking at the disk space through the database properties, which I assume uses the flawed sp_spaceused sproc. 

In SQL Server, views are not bound to the schema of the base tables by default. Try dropping and recreating the view. 

I would study this white paper and the AsPartitionProcessing sample for ideas. Ignore that it targets Azure Analysis Services as it should work against your SSAS instance if you use that servername instead of asazure:// as the servername. It uses the Tabular Object Model (TOM). 

For some reason, SSMS doesn't provide this New Database menu option for SSAS Tabular. But you can easily create a new empty or shell database with the following XMLA script. Just click the XMLA button in the toolbar and then paste in the following. Edit the ID and Name property to the desired database name. And edit the DOMAIN\GroupName to set the group that has admin permission. This will allow members of that group to deploy over this database without being SSAS admins for the whole instance. The following script is for SSAS 2012 SP1 or 2014 Tabular models: