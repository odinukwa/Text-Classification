Based on transitive relation, it means that SQL Server considers them all to be the same character. However, in other environments, say for example C#, they're not the same. What I'm confused about is: 

Now that I've made sure that SAN is up and running, and permission are OK, how can I tell SQL Server to continue recovering? Since this database is very large, I don't want to interrupt the course of recovering and start from the beginning. And also, any backup would take hours to complete. 

I have table that has fields, and another table called , which has a one-to-one relationship to the table. stores information about the message, including its text, and subject, and the recipient, etc. 

And it has non-clustered indexes on and fields. I want to find what a has sent us in a specified period. Thus I run this query: 

I don't know why, but during steps 1 to 4, other instances can still perform read queries. This is not desired. I want to exclusively prevent anything, even read queries from being executed on messages table during step 1 to 4. How can I do that? What am I missing in my design? The goal is to make sure that while a message is queued for processing, no other instance would process it again. 

I'm stuck at setting transaction isolation level. Here's my scenario that happens in the application: 

Learner can declare that he/she knows a given word Learner can memorize the given word (to be managed by the application later) Learner can choose to ignore the given word, so that it won't be given to him/her later Admin can ignore a word in general, so that it won't be given to any learner A list of words exist in database, as the reference Next word to be given to the student, should be new to him/her 

Inserting 10 rows each 2 minutes will result in (24*60/2)*10 rows per day (7200) -> that is not a large value to worry about. Also, it's great that you think on the "future - 10 years", but don't lose time with premature optimization. If your only concern it's about this table where you're inserting data every 2 mins, there's no point in creating additional databases (one per year), so let's stick with this table (Table_A). Now, because this table will slowly increase in time, and your want your queries to run fast, you have plenty of options: 

In your first query, both valid values + null values are returned from the database, but because you use the "limit 50", and because NULLs are displayed last, you don't see the rows containing NULL Price. The correct for for the first query should be: 

you can partition the table by certain criteria. Because your using the table for a "weather station", and your data is time series values, your best option would be to partition by [station_id] then by [created] Create a Table_A_Archive, where you can move data that would be to old to keep in Table_A. How long do you intend to keep data in Table_A ? would it make sense to delete old rows that become obsolete for you application 

Because both of them are virtual servers (you didn't mention the technology used (ESX/KVM...), you can try this: 

Just for fun, and the sake of your question, if are using MySQL, this would be a stupid hack to check if the hourly file has been received on the server: 

As soon as I start this query, it finishes. Thus, I get no chance to see to find out what type of lock this query has applied on database objects. I can inflate this query with transaction statements: 

Last night all of our systems failed and connections to the database kept dropping. We watched the log today and found out that this was the message: 

As you can see, this is the only information that I have. That a delete operation is timed-out at time X. I'm stuck at how to debug this and find out the cause. I don't know even from where to start. I'm using SQL Server 2014. 

SQL Server database which is very large (4TB) is stuck in recovery state. Reason: Data center had a planned downtime and we had to turn the physical machine off and when it turned on, as I read in error logs, SQL Server couldn't access the files, because files were on a SAN machine, and probably things didn't went smooth. 

They are designed to be separate, so please no advice on merging them. Now the logic is to find all messages that are not sent yet, and send them. Basically a simple clause in a single-threaded scenario would do the trick: 

I have an application in which some reports time out before getting data. We started increasing timeout througout our code in different places, and as our database is growing bigger and bigger, we see that we're repeating/duplicating more code. So we decided to increase database timeout for queries globally. But I have this feeling that this is wrong. Yet I'm not able to bring forward any reasons. Is it bad to increase timeout everywhere? Why shouldn't we do this? We're using SQL Server and Entity Framework. 

* I'm not an ORACLE Dev, but the function should look something like the one above... Once you create the function, you will create a functional index that will use the function to look for the rows you need: 

Why not recreating the primary key as (nid, vid), and creating a new index on the column "order" (just for fast retrieval / ORDER BY clause). --> The worst that can happen is to have 2 ingredients with the same "order" value, but the order-change-logic should be already correctly defined in the application. 

Because your hierarchy models is Group A -> Group B -> Group C, it's enough to store in [user_groups_data] table just the user_id, the last group id (in the groups hierarchy tree), and the info So, data in the tables can look like: 

Why would you want to use the DB to check for files ? Just write a shell script (if using Linux) or bat script (Windows) to check for the files, and use cron or Windows task scheduler to run that script. You can also create the INSERT query in the script (on Linux + MySQL that should be easy enough), and every time the files are checked, a new row should be inserted in your Frequency table. Don't try to use the DB for something that can be easily done at OS level. 

Your free space in the current innodb file (ibdata[1] ?) is decreasing. It doesn't necessarily mean that your website is slow. Maybe the number of users (concurrent users) has increased in the last few days ? (also causing lots of inserts int he DB?) There is no error in the print screen. it's just your monitoring system, reporting a value that have reached the critical threshold. 

Instance A gets some unprocessed messages While instance A is setting the to true in RAM, instance B gets some messages, and chances are that it fetches one or more of the messages which are already fetched by instance A 

And one hundred times it worked. Now I'm stuck. I've done all the steps, and client can't connect to the engine. It simply times out. Here's what I've done: 

Get unprocessed messages (using the flag) Set their to true (in RAM) and update their status Do the business Set their to false (in RAM) and update their status 

Begin transaction (serializable) Get unprocessed messages (using the flag) Set their to true (in RAM) and update their status Commit transaction Do the business Set their to false (in RAM) Begin transaction (serializable) Update their status Commit transaction 

But frankly, that doesn't seem the proper way for natural locking detection, because I've manipulated the default behavior using transaction statements. Do we have a tool, like to show us some information about locking of a given query? If not, how can we find out what type of lock a given query can apply on a database, because they get executed TOO fast to be detected. 

This works fine when runs sequentially. But when I run multiple instances of my application (concurrency), I see that some messages are processed twice or thrice. Here's what happens: 

I'm trying to find out the position of each given record of a query in 0-100 scale. I use ranking function this way: 

Your questions is a little foggy You don't mention how the archiving process works You're basically asking about a feature on the User Interface side ... not on the DB side. 

Also, I believe that the application that's using your database already knows what those values mean. So what is the real use case for your constants ? If you really want to go with your approach, without changing the existing column type, you can create a function that will return the STRING value for each integer code: 

This is a sample matrix. The query should find all the green cells, with value = F (as is TO BE FOUND :) ) ... 

... And so on. Best Bet: Partitioning the existing table by [station_id] and [created], you will have "A" partitions for each station, "B" partitions for each month, and a total of AxB posible number of partitions. Once you partition Table_A, do the same thing for Table_A_Archive, and on the end of each year, move the data from Table_A to Table_A_Archive. ** IMPORTANT:** After you make the partitioning schema, keep in mind that all queries should have in the WHERE clause the conditions necesarly so that the query will hit as little partitions as posible. Ex. 

Replication was made for small changes, so you're on the right track. To keep the systems in sync, at the database level, you will need to setup a mysql master-master replication scenario. I've used the tutorial from this linke ($URL$ to setup such configuration. Depending on how much data is changed, you can choose between [statement] or [row-based] replication. More info about the differences between the 2 types: $URL$