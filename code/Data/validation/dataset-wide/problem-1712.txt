Is NOT a good thing to do. (Here's a hint: .* matches ..) [EDIT] Thanks to Dennis commenting on this, you can disregard my warning (though I'm leaving the comment to show that I'm doubly an idiot) ;-) 

You can use vgdisplay to verify that the volumegroup has free space now. Now, grow the logical volume: 

If those directories need to be identical, could you put them on another server and mount them remotely via NFS? 

In an environment where you're using ACLs in addition to traditional UNIX permissions, no, probably not, other than convention. A user does (to my knowledge?) have to have a GID assigned to them, so you might as well make it restrictive, rather than assigning them to a generic "users" group. ACLs don't replace permissions, they augment them. 

I'm not sure that this is what you're looking for, but the closest thing I can think of would be Monitoring Exchange, which keeps scripts that enable admins to enhance their remote monitoring. There's also a thread on HP's forums devoted to sysadmin scripts, but it's pretty old. If you want tips on writing good scripts, my fellow blogger Bob Plankers wrote a column with great tips here. 

The "$USER1$" is a Nagios macro that points to the /usr/local/nagios/libexec directory. You can edit the "resources.cfg" to see what else is available, and even add macros. Anyway, now that we've got the command in there, we've got to set up a service to take advantage of it. Save the commands.cfg and edit "services.cfg". Notice that everything is in the format: 

You asked for "up to date", so something from 2007 shouldn't be too far off: $URL$ If that's too old, then it doesn't look like there's 1 specific tutorial covering what you're looking for (maybe you could write one when you get done?) but a combination should work: No date given, and not debian specific, but "apt-get install dovecot postfix" should get you to where you can use this: $URL$ and for virtual users and dovecot: $URL$ 

I would guess power button as well, but it's possible that something else is causing it. Is this machine connected to a UPS? Are there any other people who have root access on the machine? Can you check lastlog to see if anyone else was connected when it was shutdown? What services is this machine running, and are there any published vulnerabilities on these services? 

If you're at all unsure (and it sounds like you are), stick with the older stuff you know. That doesn't just apply to filesystems, either. Production equals solid. If you have to ask if something is ready for production, you're not ready to use it for production, and that's what matters. Make a lab and test it while you deploy ext3 in your production environment. 

I feel the need to add to this discussion. I've got 20 Dell PowerEdge 1955 blades in 2 chassis. I love the hardware. There are some oddities (servers in the same chassis all have to have the same daughter card, or no daughter card. Servers won't turn on if one has a GbE DC and another has a FC), but by far the worst thing about these is the Dell DRAC. I have never in my life seen a more miserable excuse for a management interface. It fully supports multiple simultaneous logins, but the thing is so slow that it can't do it. The processor on that card is so weak that sometimes the images on the webpage time out before it can all load. It's unusable 80% of the time. We've called Dell about it, and all they can say is "Yes, DRAC is slow. We know.". I like the hardware otherwise, but damn the DRAC is miserable. 

This is a crazy idea, but how about you configure your webserver to only allow authenticated users access to the file. Then give your friend access. 

If you're doing an internally facing site, or a site with a closed userbase, then openLDAP would be perfectly fine. I do something similar with my own intranet sites (though I use Apache authenticated against AD). For public sites, I agree with Chopper3. Allow a local authentication if you want, but definitely allow OpenID (or even use the Twitter/Facebook/whatever centralized accounting). 

The method will obviously differ based on the distribution. Some distros have easily selectable minimal installations (like the amazing new $URL$ while others, like Red Hat Enterprise (CentOS/Scientific) need custom kickstart files files created to craft the installation to your exact specifications. That assume that you know what the exact specifications are, though. I'm a huge fan of System Management by the Least Bit Principle. It's a good policy, and determining what that least bit is becomes difficult. Design your server to have a purpose, and ensure that the only software and libraries installed directly support that purpose. Ensure that you've got configuration management in place (like Puppet or cfengine) to make changes in the future. 

Look in your home directory, under .ssh and see if you have id_dsa or id_rsa (and an accompanying .pub file). 

[root@a-sys1 ~]# yum search "php" | grep -i gd php-gd.i386 : A module for PHP applications for using the gd graphics library [root@a-sys1 ~]# yum install php-gd Loading "dellsysidplugin2" plugin etc etc 

Let me google that for you ;-) $URL$ And if you want a big list of (not necessarily opensource) plugins: $URL$ 

First, you probably know that you're never actually going to hit 2Gb/s. The overhead of TCP/IP will limit you to probably 90% of the max. 2nd, even if you use a TCP offload engine, the stack above layer 3 definitely affects where the bottleneck is. In other words, how are you transmitting the data? I could have 10Gb/s NICs and a crossover between them and I'd not get above a few hundred Mb/s if I was using rsync over an ssh tunnel. What else can you tell us about the topology? You said that the server is connected to a couple of switches, and that the remote clients are all over the world. Do you have > 500Mb/s (aggregate) of WAN connections? 

It should work, but you'll only be able to run 32bit OSes (at least if it's the same as VMware Server, VirtualBox, etc etc) 

Alright, I've got two comments. First, the one you want to hear (although you're probably not expecting this). Gamebook Engine ($URL$ sounds like it might be what you want, since you're essentially talking about a "Create your own Adventure" kind of process. Go to page 45 if you're installing on Windows XP, page 26 if you attack the druid. 2nd comment: You're probably going about this the wrong way. What you should really do is write a wrapper for this installer that is intelligent enough to know your platforms and has specific instructions needed for that platform. It's (maybe) more work, but it sounds like it will save everyone time in the long run. 

I would like to think that I am the exception, but my research has shown that there's a decent percentage (~10%) of administrators who are the sole admin in a 24x7x365 computing environment who have >50 servers. (See my research and the results of my 334-admin survey here: $URL$ Essentially, your job as an admin is either to support a 24 hour operation or it isn't. You're hired to be on-call at times, then it's acceptable. If you weren't hired to be on-call and your employer wants to make you on-call, then that needs to be a two way street. You need reimbursed in one way or another for your extra investment in the company. Whether you take that as monetary or temporal is between you and your employer. If they don't want to reimburse you more and you disagree, chances are good that it is an at-will employment. Ultimately, you must be the final judge of "fair", but reaching a mutually beneficial agreement should definitely be possible. 

Checklists and scripting For every complex task, there is a checklist or a script that will save your butt. If it's good enough for surgeons and airline pilots, it's good enough for us. 

I used to have 40ish Linux servers, all with local authentication. Life was hell. I finally solved the problem by building an Active Direcotry infrastructure and implementing Likewise Open to authenticate all of my machines (plus samba, ftp, jabber, and half a dozen web apps). Now I've got 80-100 servers all using the same authentication and my users love it (but not nearly as much as I do). I have never once regretted using Likewise. I talked about it so much on my blog that they sent me a T-shirt! 

Not directly, no (unless there is some kind of hook on a filesystem that I'm unfamiliar with). Do you edit the file via some standard method? If so, it would be easier to automate the command from that angle, rather than from the system level. If you tell us more about the situation, we might be able to give you better answers. 

This is what the 'screen' utility was invented for. Although I've used it mainly on Linux, there's absolutely no reason a port for your OS couldn't exist. It's part of the gnu tool set, and I haven't found a port, but one probably exists (or could). 

No. Unless the user's group is set to _example (in the password file) or the user is explicitly added to the _example group in /etc/group, the user is not a member of that group. 

If you have an ESX(i) machine, you could use the vmware converter product to back them up. Essentially, you point vmware converter at the guest VM's IP address, give it root login credentials, and tell it which ESX(i) server you want it to appear on. Then hit "go" and wait a while. I've been very impressed with how reliably it's converted my machines. The only limitations appear to be on older machines that aren't supported (for example, a Win2k machine that hasn't been updated since the dawn of time didn't work, but would have if I could have updated it. Long story) 

Generally speaking, processes will take as much of both CPU and network bandwidth as they can, relative to the rest of the system. You can't, to my knowledge, explicitly give one process a certain amount of CPU speed like you can with virtual machines. What you can do is adjust the scheduling priority of the process using "nice", though, as Crankyadmin mentioned. According to the "nice" scale, programs run from 19 (the least priority) to -20, the highest priority, and programs typically run at 0. Notice that non-root users can only, by default, adjust their programs in the 19-0 range. Only the superuser can schedule priorities below 0 (adjust this default using ulimit in /etc/profile, but ONLY do it if you know what you're doing). To run a new process at a specific nice level, do 

I'm 99.9% certain that cpuinfo would not display processors that aren't supported by the kernel. dmidecode, on the other hand, may. 

You are looking for a storage system that provide data deduplication: $URL$ This won't relieve you of the requirement to get data off site somehow, but it will definitely help lower the amount of space required by your hot/live backups. 

This is why Active Directory requires (well, nearly anyway) multiple servers. So that when one of them dies, you're not screwed. If you've got a user base of any size, you do not want to rely on de-centralized management, even if it is through something like puppet. Moves Adds and Changes (MACs) will be no fun. At all. Plus, without centralized authentication, you'll need to manage local accounts, samba accounts, htaccess accounts...where as you /could/ centrally authenticate everyone at once. Please, reconsider centralized authentication for your own sanity. 

It looks like your best bet is to use VBScript to do the setup. Here's a short example: $URL$ Microsoft also has a guide available for programmatically administering IIS 6: $URL$ 

So yes, originally, the password file contained actual passwords EDIT This was in UNIX. Even by the time the above referenced paper came out, it was seen as a bad idea. Since Debian is post-1991, it would be ludicrous to assume or believe that the Linux distro would have enabled password files without even crypt protection. It is far more likely that initial versions of the Debian passwd suite used non-shadowed passwords, which would have stored the encrypted passwords in /etc/passwd itself. The mechanism used back then would have been 'crypt', which is mathematically simpler to compute than the current practice of using md5 (although other options are available). If you get a chance, pick up the Linux Pro Magazine's "Shell Handbook" edition. I've got a 4 page article on command-line user manipulation, and I talk about the history of UNIX password security. 

How many NICs per blade do you have? A proper iSCSI solution will require 4. 2 should go to bond the interfaces (probably in mode 1) so you can lose a nic (or switch (or cable) ) without losing connectivity. The other 2 should be setup with multipath to the iscsi target, and they should be on their own network with their own switches. iSCSI /can/ be routed and treated like normal network traffic, but according to all the people I've talked to, it /shouldn't/ be. 

I don't know how the XBOX "sees" the other machines, but probably via broadcast. If that is the case, you need to get an actual AP to put on the .2.x network, since your Mac is acting like a router (and routers divide broadcast domains). The overall solution is to get the XBox on the .2.x network, however you want to accomplish that. 

As long as they're the same architecture, then you should be fine shutting down then rsyncing the files. 

I would imagine that your two options are to incrementally move the file set by using regexes to match filenames, or to unmount the filesystem and copy the entire device over. wow. Good luck. 

In my experience, certifications are what you get when you don't have experience, because they show evidence to a base skill level. Once you've got a few years of experience, your certifications are less important than what you've accomplished and what skills you've got. I should say that I've really only worked for small companies, where getting things done is more important than impressing HR. I don't particularly care about college degrees for administrators, either. 

I've been imaging desktops with Clonezilla, but there's absolutely nothing that would prevent it from working on any machine. The trick would be to get either attached or network storage with enough space to hold the system image. If it were me, I would use an NFS on the local subnet to store the image. The way the process works is that you power down the machine, and insert the Clonezilla Live CD (this is assuming you don't have a clonezilla server, which seems likely given that you wouldn't have asked the question if you had). Clonezilla boots and then gives you the option of selecting whether you want to image a partition or entire disk. You'll probably want to do entire disk, since the source and destination are identical. It will ask where you want to store the image, so select NFS server, and then it should set up the networking and ask for the server and directory. Give it that, and it will copy the image across the network to the NFS server. When the time comes to image the new server, just do the same thing as before, except tell Clonezilla that you want to restore an image, and follow the instructions as above.