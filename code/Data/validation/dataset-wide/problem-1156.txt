i have a trigger in PostgreSql 9.1 that i want to fire on inserts of several tables. is it possible to have it affect all these tables instead of creating the trigger for all these tables? i have 58 tables that i want to use the same insert trigger which is calling a trigger function, so i have been using 

I have a function that has one parameter of a table type, i want to loop through all the fields and return the column that has a null or empty value. 

Is FOO a DSN or a server name? on workbench you need the server name, mysql username and the mysql password. Thats all you need to connect to your mysql server using mysql workbench. 

But i want the number of records returned to be exactly the same as the number of days of the current month. if the current month has 28 days and only had two records it should bring; 

can anyone help me to loop through the variable? I dont know how to refer to the field names. can i do something like ?? 

The above query works well. so if this month we only had two records and 28 days it will bring only two records. 

Somewhat of a strange one. Recently my user id's been showing up in a number of Audit Success records in the Security event log of one of our production database servers with the below content (or similar): 

I'm after information on how to better understand the trade-offs involved in such situations as opposed to a simple 's best. Pointers to articles which may help me understand would also be welcome. Related SQL Fiddle: $URL$ 

Running sp_who2 and following the BlkBy trail up to the root cause of blocking returns a SPID which has CPUTime and DiskIO values of 0; yet it's blocking 4 other SPIDs. I'm confused by how this is possible; the CPUTime in particular seems odd as in order to obtain a lock you'd have had to have spent some time requesting resources / requesting the lock itself. CPUTime is in milliseconds, so whilst it's possible that the requesting and locking of resources occurs fast enough to have a rounded down value here, that is a little surprising. Also, these SPIDS are sometimes a few minutes old; yet appear to have done nothing beyond cause blocking. Question How is it possible for a SPID to cause blocking whilst having zero CPU Time? I'm asking as I suspect something's lacking in my understanding of the CPU Time stat. If anyone can advise on sensible steps to aid in investigating such issues though that would also be useful. 

That will bring all the distinct that matches the s that you will have specified in the where clause 

i have an UPSERT function which works well but i update and insert records in batches, can this function be modified so that i just pass all the records to this function as an array then it will insert/update ALL the records at once as ONE TRANSACTION? if record number x fails,it should roll back undo any records that had been inserted/updated before x? here is my upsert function: 

doing this for all those tables and then we have multiple schemas,so the workload is alot,can this be done in one query? then affect all tables in that schema or the whole database? 

This problem can easily be solved by php... Get the number of rooms selected from the first query and assign it to a variable and then loop the second query by the number of rooms choosen. 

i have a query in MySQL which serves me very well by getting all the records within the current month; 

You can get the number of the rows that were inserted by executing the following query Then you can get the latest id from Assuming the row_count result for example being 2, and last_insert_id being 8956. The group of two rows will be with 8956,8955. Hope that helps you out without any structural changes to your table or database. 

SQL Server will always use all the available memory is given. As soon as something is read from disk into memory, it will stay there and never be released to optimize performance in case it needs to be read again. It is not uncommon in production environments to see SQL Server instances pegged at 95% memory utilization permanently. This is at odds with normal application use of memory which can cause some confusion. This behavior is regardless of any transaction use in your TSQL scripts and objects. EDIT: Here's an article from Brent Ozar on the topic which contains an absolutely fabulous quote: "SQL Server is using all of the memory. Period." - $URL$ More EDIT: If you want to limit the memory that it uses, the following article has instructions for updating the maximum memory setting which will place an upper limit on memory used. $URL$ 

I've encountered this issue before and in previous cases for us the solution was to develop a system of ETL processes which bring the information over into the warehouse or reporting database. What we did was design the process to run near continuously and at the head of each iteration we would grab the current maximum datetime stamp of each table. We would then process each record that was generated or updated between the last timestamp and the current one we just retrieved then repeat this process constantly. There is a chance with this for partial data, but with the process running continuously any partial data would be resolved within a few minutes. 

NB: this question is purely academic / to help improve my understanding of SQL Server performance. Given a master table which relates to one or more other tables, how would you determine the best approach to querying that master table for records, which include an indicator to the presence of records in the related tables? For example, say we have a Person table and wanted to get a list of all people along with an indicator of whether they have children (in this example Person can be reused as the related table): 

You could create a database in your x86 SQL db which contains synonyms (or views if you only require read only access) pointing to the Sybase DB, then call those synonyms over your linked server from your x64 SQL db. Not sure how you'd go about accessing a linked server via a linked server directly though (i.e. without the use of some intermediary object). 

This record appears about 40 times one second, then there's a few seconds without anything being logged before the next batch of ~40. The events start appearing after I connect to a database hosted on the affected server from SSMS. I don't open a query window or do anything more than connect under my user account. Disconnecting in SSMS does not stop the events. Exiting SSMS does. Has anyone seen behavior like this before / any idea what may cause it? Thanks in advance. Version Info: 

For constant scans there is an article here which may help explain Constant Scan in Execution Plans. The estimated number of rows is just that, an estimate. The Query Optimizer attempts to use past executions in order to guess how many rows are going to be returned by any given operation. If you're using something like a restore to practice query tuning then odds are that after your restore operation you did not perform any database maintenance. Statistics helps to determine things like estimated rows, and index maintenance will help to both intelligently query the data and provide better information for those statistics. If updating the statistics doesn't work, it's probably because there are not very many queries that have been executed against the database. Running that query a few times will give it some better information, but if the execution time is excessive other options need to be considered to help the optimizer. For making this query perform better you need look no farther than the worktable in the I/O statistics you've returned. IO Statistics is a great option for getting an idea of how much processing you're really doing on your data. The work table shown here means that the optimizer is saving the dataset then reviewing it multiple times. This can be reduced drastically by condensing the three subqueries shown here into one statement as follows.