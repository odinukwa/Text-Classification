Can the the software administration be separated from the hardware administration? It seems the only thing that needs administered independently is the hardware on each machine; the software should be managed at once via the clustered file system. (Could be wrong, hence the question) Can the Host OS be ran from a shared set of files by either of the two machines, one at a time? That is, can both machines be ready to administer the Host OS, but only the master does until failover at which point the slave machine picks up the administration work? Or is this assumption inaccurate? Can we set a machine into a slave/idle state until the master goes down? (Suspend cron jobs, suspend services, etc) I am assuming some of this will be a matter of of the fencing software? 

Ubuntu-1004-lucid-64-minimal is not a valid global name, tell postfix to use your "real" domain name, using the myhostname setting: $URL$ 

Is mod_status enabled? $URL$ is an example output, and it has requests/s since startup and grand totals. 

EDIT You have a certificate only for "master", but your client connects to "puppetmaster". So either the client needs to expect "master", or you need a certificate for "puppetmaster" on your master. A "certname=puppetmaster" in the [master] block in puppet.conf will change the CN on the server ($URL$ You may need to remove the old certificates, but I am not sure about this. Or, you can have the client connect to "master", either by adding it to /etc/hosts, or to your DNS zone if you're running one. 

Did the varnish process maybe restart? There's an uptime counter in varnishstat. Under certain circumstances the varnish worker thread can die, but it gets restarted immediately. When everything is working fine, this might go unnoticed, but with (planned) backend down time it can be quite inconvient. 

I have been working at setting this up in much of a way you describe and it works great! (XenServer) I setup an old but capable server as the primary host, this runs a console only VM for DRBD. This VM then serves a "SharedDRBD" SR back to the Xen Host via NFS. The rest of the working VMs providing services run on the SharedDRBD SR. The VM's DRBD dev is on its own VDI on a MDADM RAID 1. This SharedDRBD SR hosts the rest of the VMs for various services with a local larger RAID10 array for bulk filestorage. All MDADM work is done by the host, but one side of the DRBD is in a VM. The DRBD ran in a VM gets synced with a DRBD service running on the file backup server; the file backup server is NOT virtualized purposefully so we have bare metal access to all files given XenServer is the biggest quirk we generally deal with. There is a secondary server that is virtualized but has no local storage except for what is required for the host. This server is part of a Xen pool with the primary server to simplify failover. Failover is currently manual but fast and easy. First, all VMs on the SharedDRBD SR are shutdown while the secondary XenServer host is powered on. The DRBD on the file backup server is made primary and mounted as needed. Then, the SharedDRBD SR is pointed to the file backup server and VMs are started up on the secondary server; XenCenter doesn't even realize it is serving the VMs from a new location because it sees the same SR with the same data. The VMs are fired back up and things are back and running. There is alot more to it in terms of configuration, and arrays, network topology, etc; but the jist is DRBD is served in a VM back to its own host. Overall it is HA enough for our SMB / Home use; down time during a catastrophic failure of the primary server is 10-20 min or less to fully back online and no loss of data; DRBD means the VMs are up to date! Plus, outside of the primary server which is pretty robust, there is a ton of overall redundancy. Most of the primary server is redundant in-and-of-it-self so it pretty much gives us triple redundancy or better for just about every piece of hardware you can think of (PS, RAM, CPU, HDD, Controllers, NICs, etc) besides the motherboard(s) which is only double redundancy (primary/secondary Xen Hosts). And yes, XenCenter is installed on windows sadly, the rest is all Linux. I know, this Q's is 8 years old. 

This will not reorder them if they're in the wrong order, but it'll at least add them in the correct order right below the search statement (possibly listing 4 name servers which is more than 3 (MAXNS in resolv.h), but with just file_line resources avoiding this might be hard or impossible). Also, the parameter is specific to file_line resources, hinting where to insert the line, and is a general resource parameter talking about resource ordering. 

The "before" metaparameter indeed only says something about the order of execution of the resources, not the ordering of the lines in the file. If I were you, I would aim to manage resolv.conf with first class puppet constructs: a seperate module that manages it as a file-resource (there probably are several on puppet forge), or write your own small template that explicitly orders the specified name server parameters. Another option would be to specify both name servers in one file_line resource using to seperate them: 

I've realize UREs are a bit more complex and unknown to most as they relate to array failures.. The conclusion is UREs can cause arrays to fail, but not as often as that math in the articles say. But RAID 5 still is a very failure prone RAID array compared to ALL other RAID levels. So back to basics, what are we mitigating during a RAID 5 rebuild? We are trying to get parity back before a second drive fails. THATs IT! This is a by-any-means-necessary endeavor. This leads me to solidify my list 

I am looking to begin a tape backup regimen and am looking to keep data flowing to the tape drive in a sufficient manner (120+MBs target sustained) but cannot figure out how to do so without a dedicated source drive/array that idles when not writing tapes. The documentation for our specific drive mentions no minimum throughput required. Enviroment 

There still is "example.com" in you config, is that correct? Also, the .net nameservers tell me that ns01.ispeed.it and ns02.ispeed.it are configured by the registrar as the nameservers for your domain. These nameservers respond for elfoip.net, but not with the data you show, these seem to be the nameservers of an Italian provider and not yours. Your registrar needs to change the NS records for your domain, or you need to start using their nameservers. 

Then you can go search the server with an empty but blinking network interface. This should also work for interfaces that are up and running (if they're all connected), but then you'd have to distinguish between the ethtool regular interval and the normal blinking that shows the interface activity :) 

NFS is from 1 client to 1 server, so the overall performance is limited by the performance of that 1 server. Adding more servers does not help. Lustre splits the data, the data gets requested from 1 server, but can be sent from one or more other servers. So adding more servers does help (which is why "Lustre scales"). This is an important bit from your first link: 

Looking at using DRBD or a clustered files system to help with up-time when downtime strikes in a small business environment. We currently use a server box for a file server using Linux and samba, then running the web server and Database in a VM. Was looking at adding a second server and putting the files and the VM onto the distributed file system. The base OS is more static and easily can be managed more manually (copy config files at time of change, copy base OS if needed from full backups, etc) Question is about the fail over scenario if manually done. If server 1 goes down and fail over is manually done, is fail over completed by simply setting the static IP of server 2 to server 1 (again server 1 is down and would be in a state of needing repair), starting Samba, and starting the VM which would have the same static IP's as they had when running on server 1, and starting the backup services? This sounds like a quick and simple process, almost too simple. Am I missing something? This could easily be automated as well through a script or something that someone with little proficiency could be directed to run in the event of a failure. Down time if we have a hardware failure could easily be days without the support of on call IT support and the parts needed without a second server, but with the the second server, down time would be at the maximum a matter of hours (if no one is the office proficient enough to perform such operations, minutes if someone was) 

Are you sure the filesystem is on /dev/sdc? That would mean the disk would be unpartitioned with the filesystem taking up the entire disk. While that's perfectly possible, it's not that usual. Does mount /dev/sdc1 perhaps work? What does cat /proc/partitions say? Are you sure the disk shows up as sdc? If it's connected with USB, you can check the output of dmesg for instance. 

Your server most likely did not freeze, it was unreachable. The default rule for ipfw is to deny everything. You can recompile the kernel with " options IPFIREWALL_DEFAULT_TO_ACCEPT" set, or add ";ipfw add allow all " to your command (or build a script that flushes and adds your rules at once). 

Are you sure there is a problem to solve? Do you experience low performance? Check for instance "Optimizing memory usage" AT $URL$ A lot of used swap space does not mean there actually is a problem. 

(This is not a question of how to run both machines from the same set of data at the same time; they would be effectrively running from the same set of data at different times. The files the servers each need to constantly modify would be on the local file system) We simply want to setup a redundant server and minimize the redundant administration. 

After running this setup in production for 6 months, I can say there does NOT seem to be any stability issues with the VMs running on a SR serviced from DRBD in a VM. The biggest issue is you have two "hosts" to worry about that will affect all other VMs, Dom0 and the DRBD server effectively adding a second point of failure software wise (configuration errors, administration errors, bugs, etc). However this has not proved to be an issue thus far. I have not ran any comparison benchmark for performance of VMs on and off DRBD although we have not had any noticable performance issues and the majority of data served is not served by DRBD; the SR on DRBD only host the VM host disks. TL;DR We had some extra RAM available on the HOST so I did set the DRBD server to use all remaining available RAM so it caches the data it serves. Bringing up the secondary server is relatively easy. It involves setting the backup servers DRBDs service to primary, mounting the DRBD drive, then removing the primary servers SR and re-adding it on the secondary server from the SR on the DRBD drive, then utilizing XenServers builtin backup and restore to reassociate the metadata to the virtual disks. This means the VMs used during an outage on the primary server are not outdated in anyway because they were actively replicated via DRBD vs a script. It is rather critical we keep the metadata up to date to make this work easily. This is used like a RAID and the VMs are still backed up in case of some other failure or corruption. 

Putty uses its own format for the key files, not a standard format. ssh can not use ppk files. I believe puttygen should be able to export your key to something more usable for ssh. 

In addition to the comment of BÃ²ss King, you can also simply specify several addresses seperated with a comma: 

Which hostname(s) did the puppetmaster generate its certificate for? The puppet client expects the certificate to be valid for "puppetmaster", but it doesn't seem to be issued for this hostname. I think "puppet" might be the default CN on the puppetmaster, or else the hostname of the server. You can check it by running "openssl x509 -text -in cert.pem" on the certificate of the server, or connect to with a browser, and see which domains are in the CN and dns_alt_names of the certificate. 

Given XenServer (7 currently) is based on CentOS, does that mean it works just like CentOS in terms of updating, CLI, administration (non-Xen specific like mdadm and boot loaders) etc? Basically, if I want to use XenServer, then am I committing to using, learning, and working in the CentOS "way"? We have a new (to us) server on the way and now is the time to switch hypervisors and we are set on using Xen. Our current setup that I am familiar with and can administer efficiently is a Debian host with a couple VMs using Virtual Box which is less than ideal to say the least. Due to this, I am familiar to working in Debian and have made a conscious choice to use Debian for our servers. I administer only our servers for our small business so I do not have the diversity of other setups and other distributions to work from. From my understanding, the way Redhat does things is a bit different from Debian based distributions and would require a learning curve of unknown amount; but a learning curve for sure. So if I use XenServer, am I also committing to the Redhat learning curve? I am aware that I can install Xen with a Debian based Dom0 but the consensus I have read seems to say XenServer works the best overall. However there will be a bit of configuration I will need to do such as getting our local RAID arrays up and running for the Dom0, Xen, and Network Shares, along with getting boot loaders and grub in order. I can do this configuration rather easily in Debian so I am trying to weigh the cost in time of trying to do the same configuration the CentOS way which I am afraid will add a considerable amount of time to get the new server in to Production given IT by myself for our company slowly happens in the afterhours of business; hence the question.