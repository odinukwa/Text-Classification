Well it seems to me the Cauchy-Riemann equations say quite a lot. After all once you know the real and imaginary parts are harmonic, then you know their derivatives are as well, and $$f'(z)=u_x(z)+iv_x(z)$$ so you can do it again. I don't think this is boot-strapping it, but if anyone greater on the complex food-chain cares to disagree, I'm likely retreat quickly at their word. 

My example comes from algebraic geometry, specifically in the theory of $\lambda$-rings, which are used in K-theory and representation theory (and possibly other places of which I am unaware). If you look in Lecture Notes in Mathematics 308 $\lambda$-rings and the Reprsentation Theory of the Symmetric Group by Donald Knutson, on pp 27, there is a theorem which is called--quite appropriately--the "Verification Principle", and it is used to do exactly the kind of thing you're asking about. The statement is: If $\mu$ is a $\lambda$-ring operation, then $\mu$ is uniquely a polynomial in the $\lambda$-operations and for any particular polynomial $f(\lambda^1,\lambda^2,\ldots , \lambda^n,\ldots)$ it is sufficient to check that $\mu = f$, operating on a sum $\xi_1+\ldots + \xi_r$ of elements of degree 1, for all $r>0$. If you read the first part of the book (i.e. up to page 27 where you read this) you learn that this is a very handy thing indeed, especially given the generating function polynomials that show up in the study of these objects and how much easier it is to check for things which are polynomials in the $\lambda$-operations (as they are called). It is this that so nicely ties this kind of ring in with the representation theory of the symmetric group, in particular the elementary symmetric functions, as Knutson mentions on p. 10 of the same publication. Another useful technique is one that comes up in algebra, when you check something for general elements by checking on a basis. Eg. testing something for all the elements of your tensor or exterior algebra by testing it on the "pure" or "completely decomposable" tensors which form a generating set. 

First one should be careful how to phrase the condition that $f_1, \ldots, f_m$ define the variety consisting only of the point $0$. For instance, the polynomials $f_1=x^2, f_2=y^2$ have only $\{(0,0)\}$ as zero set in $\mathbb{C}^2$, but in a certain sense they cut out a "fat" (non-reduced) point $(0,0)$. For instance, the coordinate ring of the scheme cut out by these functions is $$\mathbb{C}[x,y]/(x^2,y^2)$$ and it has has dimension $4$ over $\mathbb{C}$, in contrast to the coordinate ring of a (reduced) point, which is only $\mathbb{C}$ itself. But if the scheme $S=V(f_1, \ldots, f_m)$ cut out by $f_1, \ldots, f_m$ is exactly the reduced point $0$, then we know the tangent space to $S$ at $0$ is the zero-subspace of the tangent space $T_0 \mathbb{C}^n = \mathbb{C}^n$. In general, the tangent space of $V(f_1, \ldots, f_m)$ at a point $p$ is the subspace of $T_p \mathbb{C}^n$ cut out by $df_1|_p, \ldots, df_m|_p \in T_p^* \mathbb{C}^n$. But this means that the linear forms $df_1|_0, \ldots, df_m|_0$ cut out the zero-subspace in $T_0 \mathbb{C}^n$. Hence, as you mentioned, we can choose $n$ indices $i_1, \ldots, i_n$ from $1, \ldots, m$ such that $df_{i_1}|_0, \ldots, df_{i_n}|_0$ already cut out the zero-subspace. But then $S'=V(f_{i_1}, \ldots, f_{i_n})$ has tangent space $\{0\}$ at $0$. As all $f_i$ are homogeneous, for every $p \in S' \setminus \{0\}$ also the entire line $\mathbb{C} p$ would be contained in $S'$ and hence the tangent direction $p \in T_0 \mathbb{C}^n$ would be contained in $T_0 S'$. This is a contradiction, hence we can indeed cut out the (reduced) point $0$ with exactly $n$ of the $f_i$. The linear algebra argument in the proof above also showas that a generic linear combination of the $f_i$ is sufficient. Note that the counterexample presented by Tony does not satisfy that the $f_i$ cut out the reduced point $0$, as all differentials $df_i$ vanish at the origin. 

Thank you for your answer, Konstantin! In fact, since asking the question, my advisor, Prof. Dan Haran, has found a proof which does not use any Noetherity conditions: For any $A$-algebra $C$, we denote by $\varphi_C$ the canonical mapping from $C$ to its $I$-adic completion $C^* $, and for an $A$-homomorphism $\psi \colon B \to C$ we denote by $\psi^* $ the induced $A$-homomorphism from $B^* $ to $C^* $. Using the fact that the image of $A$ under $\varphi_A$ is dense in $ A^* $, one can directly see that the image of $B$ under $1 \otimes id$ is dense in $T : = A^* \otimes_A B$. Therefore the image of $ B^* $ under the induced mapping $ (1 \otimes id)^* $ must also be dense in $ T^* $. The algebra mapping $\beta \colon A \to B$ induces the mapping $\beta^* \colon A^* \to B^* $, and using the universal property of the tensor product (as a co-product in the category of commutative $A$-algebras), we obtain a unique $A$-homomorphism $\psi \colon T \to B^* $ such that $\psi \circ (1 \otimes id) = \varphi_B $ and $\psi \circ (id \otimes 1) = \beta^* $. Now $ \varphi_B^* = \varphi_{B^* } \colon B^* \to (B^* )^* $ is an isomorphism, and $ \psi^* \circ (id \otimes 1 )^* = \varphi_B^* $ shows that $ (id \otimes 1 )^* $ is an injection. Therefore we can view $ B^* $ as a complete dense $A$-subalgebra of $ T^* $, which means that: $$ T^* = \overline{B^* } \cong (B^* )^* \cong B^* $$ (where $\overline{B^* }$ is the closure of $B^* $ in $T^* $ ). 

Let $A$ be a commutative ring with 1, $I$ an ideal in $A$, $B$ an $A$-algebra. I am trying to prove the following isomorphism of $A$-algebras: $$ \big( A^* \otimes _A B \big) ^* \cong B^* $$ "$^*$" denotes the $I$-adic completion: every $A$-algebra $X$ may be endowed with the $I$-adic topology, defined by the ideal $IX$ in $X$, and the $I$-adic completion of the algebra is the completion with respect to this (uniform) topology. I have so far been able to prove that the image of $B$ in $T :=A^*\otimes_A B$ under the map $1 \otimes id \colon B \to T$ is dense in $A$. At this stage I considered the map $(1 \otimes id)^* \colon B^* \to T^*$, and tried to show that it is an isomorphism, but I'm having troubles both with the injectivity and the surjectivity of this mapping. Are these $A$-algebras always isomorphic? If so, how can this be proven? If not, how can a counterexample be constructed, and what do I have to require (Noetherity? Flatness? Finiteness?) for them to be isomorphic? 

My answer will be based on the script of an intersection theory class by Professor Barbara Fantechi found here: $URL$ (see page 4). The proof of the central Lemma will be very technical and if I have overlooked something please point it out in the comment section. The crucial point in the argument is the reduction to a local ring computation. For this reduction, the following Lemma is the key: Lemma: Let $X,Y$ be separated, finite type schemes over a field $k$ equidimensional of the same dimension and let $f: X \to Y$ be a dominant morphism. Let $Y' \subset Y$ be an irreducible component and $A=\mathcal{O}_{Y',Y}$ be the local ring at its generic point. Note that we have a canonical map $\text{Spec}(A) \to Y$. Then when we do the cartesian product of $f$ with this map, we have $$\text{Spec}(A) \times_Y X = \text{Spec}( \prod_i \mathcal O_{X_i,X}),$$ where the $X_i$ are the irreducible components of $X$ mapping dominantly to $Y'$. Note that for $f$ proper $B=\prod_i \mathcal O_{X_i,X}$ contains all the information we need to compute the coefficient of $(Y')^{red}$ in $f_* [X]$, namely the multiplicity $l(\mathcal O_{X_i,X})$ of $X_i^{red}$ in $[X]$ and the degree of the field extension $[R(X_i^{red}):R((Y')^{red})]=[R(\mathcal O_{X_i,X}):R(A)]$, where $R$ denotes the function field and the total quotient ring respectively. Let me do the desired reduction using this Lemma: to compare the coefficients of $Y_j'$ as described in your question, we may thus base change the map $f':X' \to Y'$ by the inclusion of the spectrum of $A=\mathcal O_{Y'_j,Y}$ in $Y'$. $$ \begin{array}{ccccc} \text{Spec}(B)& \to & X' & \to & X \cr \downarrow& & \downarrow & & \downarrow \cr \text{Spec}(A)& \to & Y' & \to & Y \end{array}. $$ However $X'$ was already a cartesian product, so we may as well base change the map $f:X \to Y$ by $\text{Spec}(A) \to Y$. But using the Fact from the script (sometimes called the Going-down theorem for flat morphisms) we know that $Y'_j$ dominates $Y$. From this one sees that the inclusion $\text{Spec}(A) \to Y$ factors through $\text{Spec}(R(Y))$. But now we can apply the Lemma above again to see that the following are commuting squares: $$ \begin{array}{ccccc} \text{Spec}(B)& \to & R(X) & \to & X \cr \downarrow& & \downarrow & & \downarrow \cr \text{Spec}(A)& \to & R(Y) & \to & Y \end{array}. $$ Indeed, the outer rectangle is a cartesian square by definition and the right square is cartesian by applying the Lemma to $f:X \to Y$ noting that $X,Y$ were assumed to be irreducible varieties of the same dimension. From here on the proof from the script above should go through straightforward. Proof of the Lemma: Because the inclusion of $\text{Spec}(A)$ in $Y$ factors through the complement of the irreducible components of $Y$ different from $Y'$ we may assume $Y=Y'$ is irreducible. Now in $X$ note that the intersections of the irreducible components of $X$ are all of lower dimension than $Y$ and thus their images in $Y$ are also of positive codimension. By removing their closures in $Y$ and thus shrinking $Y$ further we may assume that $X$ is a disjoint union of irreducible finite type-schemes $X_i$. If some of them do not map dominantly to $Y$ we can shrink $Y$ even further to exclude their images and hence we may assume that they all map dominantly to $Y$. Now we want to reduce to an affine computation. Shrink $Y$ to an affine open subset $\text{Spec}(C)$ and cover one such $X_i$ by open affines $\text{Spec}(D)$. We will see that all the corresponding cartesian products $\text{Spec}(A) \times_{\text{Spec}(C)} \text{Spec}(D)$ are canonically identified with $\text{Spec}(\mathcal O_{X_i,X})$, thus gluing them is trivial and the proof is finished. As $X_i, Y$ are irreducible, there are unique minimal primes $p_D, p_C$ in $D,C$ corresponding to the generic points $\eta_{X_i},\eta_Y$ of $X_i,Y$. Note that hence the local ring $A$ at $\eta_Y$ is $A=C_{p_C}$. Because $X_i$ dominates $Y$ we have that $\eta_{X_i}$ maps to $\eta_Y$, so for the corresponding map $\phi:C \to D$ we have $\phi^{-1}(p_D) = p_C$. We now have to show $$C_{p_C} \otimes_{C} D = D_{p_D}.$$ The left hand side is exactly $(C \setminus p_C)^{-1} D$, hence for $d \in D \setminus p_D$ we have to find an inverse using only $C \setminus p_C$ in the denominator. But now finally we can use that $X,Y$ have the same dimension. This implies that the corresponding extension $R(C) \subset R(D)$ of the total quotient fields is algebraic, because both have the same transcendence degree over $k$. Hence there are $c_n, \ldots, c_0 \in R(C)$ with $c_0 \neq 0 \in R(C)$ and $$c_n d^n + \ldots + c_1 d + c_0 = 0 \in R(D).$$ Multiplying by the denominators we may assume that all $c_i$ are in $C$ and that $c_0 \in C \setminus p_C$. Then one checks that this implies that $$d \cdot \frac{c_n d^{n-1} + \ldots + c_1}{-c_0} = 1 \in D_{p_D}. $$ Thus the proof is finished.