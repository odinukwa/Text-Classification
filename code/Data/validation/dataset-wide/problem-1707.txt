My first post so be gentle please! So I have an installation of graphite up and running on Ubuntu 16.04 server. (version 0.10) I am sending in metrics and everything works fine, but I am sending in multiple timestamps at once. As in I am sending in data once per minute with per 10s timestamps. Every minute new data comes in. It can happen, that a key with a corresponding timestamp (same as before) is resent a minute later. So the new value is being saved and last one lost, because of the way graphite works. So I put a carbon-aggregator in front of carbon-cache thinking it will aggregate the values per timestamp. It does not. It aggregates all values received in a time interval ignoring the timestamps. I found that statsd has the same functionality. What I want is for the aggregator to aggregate values per timestamp. As in keep data for lets say 1 minute and see if a datapoint for the same key and timestamp is received. If so sum them. If no such data came, forward the original data to carbon-cache. Is there a way to do this with graphite or statsd, or do I have to write my own little buffer in front of carbon-cache (instead of carbon-aggregator), that does this? It could be a linked list of self made structures in C resulting in about 150 lines of code, but would take precious time. Someone has definitely had the same problem, but has anyone found a solution? Bump: has noone really encountered this issue? Thanks in advance! 

As you all probably know, ipv4 route cache has been removed in 3.6 Linux kernel series, which had serious impact on multipath routing. IPv4 routing code (unlike IPv6 one) selects next hop in a round-robin fashion, so packets from given source IP to given destination IP don't always go via the same next hop. Before 3.6 the routing cache was correcting that situation, as next hop, once selected, was staying in the cache, and all further packets from the same source to the same destination were going through that next hop. Now next hop is re-selected for each packet, which leads to strange things: with 2 equal cost default routes in the routing table, each pointing to one internet provider, I can't even establish TCP connection, because initial SYN and final ACK go via different routes, and because of NAT on each path they arrive to destination as packets from different sources. Is there any relatively easy way to restore normal behaviour of multipath routing, so that next hop is selected per-flow rather than per-packet? Are there patches around to make IPv4 next hop selection hash-based, like it is for IPv6? Or how do you all deal with it? 

I have taken upon myself the configuration of an ipv6 network in an enterprise lan. There is a single VLAN for all the hosts in the LAN. I am using pfsense 2.3.4 software. I now have to figure out the address distribution in the LAN. The addresses should not allow identifying any of the hosts from the global network, so there are 2 options. NATv6 with local addresses or global addresses with address rotating, but I`ve read about NATv6 being a bad choice. I could just set up address rotating with global addresses, but that would keep me from creating IP address based firewall rules, since the addresses would change all the time. Is there a way I could assign an IPv6 address range to each of the hosts to rotate their addresses within that range, so I could still write firewall rules for each of these ranges (instead of static addresses) and hide (to some extent) the public addresses of hosts from global viewers? And is this doable in pfsense? I could also have just made a VLAN for each different role in the office and rotate global addresses on a per role basis and create firewall rules on a per role basis, but that is not an option. 

If you have xp_cmdshell enabled it's pretty simple. Just use xp_cmdshell to launch the DOS "cluster" command. 

I'm going to venture outside the "standard" reply and guess that what you need may be less a "centralized infrastructure" (single domain) and more of a centralized management tool. You might have a look at something like Italc. $URL$ It provides you a central command/control console, a singular view of your PC fleet. You'll still have to treat each PC individually, but you can do it all from one place both in the geographical and logical sense. And it's free... 

I use PolyMon to monitor these kinds of things. You can define various "alert" conditions about which you'd like to receive notification, so I get notified if one of my servers is having a problem. But it also stores all these results in a long term database, so I can look back at the memory usage of server "X" and see it's memory usage trends over the last N days/weeks/months/years. $URL$ 

Just use "DEVICE partitions", it will try all devices listed in /proc/partitions, and you wouldn't have to worry at all what the device names are. UUID of an array is stored on each device belonging to it, so each array will be assembled correctly even if you have several of them. 

I've found the answer and it was quite funny - ARP table overflow. The traffic in the test environment was gerenated from many IPs that resided in directly-connected networks, so the system had to use ARP first to figure out MACs, and the default hard limit of ARP table in Linux is just 1024 entries, which gives number of connections between networks connected to 2 different interfaces close to 512. When I increased net.ipv4.neigh.gc_thresh1 and also .gc_thresh2 and .gc_thresh3, the problem was solved. 

You need 32-bit version of libraries to run 32-bit applications on 64-bit system. Unfortunately Redhat doesn't have package like ia32-libs which would install most of them, it is supposed that you should install all 32-bit applications with yum and it will install the appropriate libraries for you. If your application is third-party, try installing 32-bit version of each library it needs, they usually have .i586 suffix, so you execute something like "yum install libusb.i586". 

You could set up a jet-direct and then packet-sniff the output to the printer. Personally, I'd copy all the fonts (.ttf or whatever) from the working machine to the misbehaving machine. Finally, check on your Wordpad settings. A lot of "general" printing output done in the Windows shell is by default routed through Wordpad. 

It only took me an hour or so of Googling and trial n error to get a fully functioning ADITO solution running. Another couple hrs to write my own user modules. For 10 or so users, it's ideal. Free. 

My personal approach would be to buy a $400-500 notebook, and throw 3 or 4 gb or ram in it. In my view, you can get a lot of machine for that money. But I personally just bought an ASUS 8.9" netbook because I'm REALLY cheep. The tiny keyboard takes a little getting used to, but for sub $200, I'm very pleased. 

Yes. You want option B, so that the OS is referencing 2 different disk files. One DB can be optimized for writing, the other for reading. Or you could accomplish the same in a single db, if you split your tables into separate filegroups/separate files. 

You can setup transparent proxy on your client machine that will have your company's proxy as a parent and add authentication information when forwarding requests to the parent. You will need to install a proxy server that supports transparent proxying, I'd recommend squid; and you will need a firewall that will redirect your traffic to a proxy server, many windows firewalls can do that. Google for "squid transparent proxy", there are a lot of manuals. 

Well, things are a bit more complex. Modern hard drives don't just detect errors, they have some spare sectors and smart controllers that try to relocate bad sectors. That is, when you try to read some logical sector and it doesn't read at first time, the controller tries to read it several times, and sometimes it can read it after some retries; then it writes the data back to the spare sector, remaps logical sector to the new one and marks old sector as bad, and finally gives you your data. All those processes are completely transparent to the reader, you wouldn't notice any error. However this will normally be reflected in S.M.A.R.T statistics, and if this happens more and more often, you can see that the drive is going to fail before it actually fails. That's why it's really important to use SMART monitoring tools on your system. When a sector doesn't read at all, or the controler runs out of spare sectors, read error will be returned by the drive. Error detection is now pretty bulletproof, it uses some kind of CRC for sector data. When read error is returned, mdadm will see it, mark the drive as unusable and switch an array into degraded mode.