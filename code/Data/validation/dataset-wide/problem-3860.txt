I noticed this weird analogy a long time ago. Is there a good explanation? A lattice is a partially ordered set in which every pair of elements has a least upper bound and a greatest lower bound. Now let me set up some very nonstandard terminology which will show what I am talking about. Say that a lattice is compact if every subset has a least upper bound (join) and a greatest lower bound (meet). A map between lattices is continuous if it preserves arbitrary (i.e., possibly infinite) joins and meets whenever these exist in the domain. A subset $C$ of a lattice is closed if $$x \in C,\,x \leq y \quad\Rightarrow\quad y \in C$$ and $C$ is stable under the formation of arbitrary meets whenever these exist. A subset $U$ is open if $x \in U,\,x \leq y \,\Rightarrow \,y \in U$ and the complement of $U$ is stable under the formation of arbitrary joins whenever these exist. A subset is clopen if it is both closed and open. A lattice is totally disconnected if for every $x \not\leq y$ there is a clopen subset containing $x$ but not $y$. A lattice is Hausdorff if for every $x \not\leq y$ there exist a closed set $C$ and an open set $U$ whose union is the whole space and such that $x \in U$, $y \not\in C$. Note that "closed" and "open" do not refer to any topology. For instance, the union of two closed sets need not be closed. I am aware that every poset carries a natural topology, but that does not seem particularly relevant here. Now here are some theorems. Theorem 1. Any union of open sets is open, and any intersection of closed sets is closed. Theorem 2. A map between two spaces is continuous if and only if the inverse image of any closed subset is closed and the inverse image of any open set is open. Theorem 3. Any continuous image of a compact space is compact. Theorem 4. A space is totally disconnected if and only if it embeds in a power of the two-element space. Theorem 5. If a space is compact Hausdorff and $x \not\leq y$ then there is a continuous map into $[0,1]$ taking $x$ to $1$ and $y$ to $0$. Theorem 6. Every compact Hausdorff space embeds in a power of $[0,1]$. Theorem 7. Every compact Hausdorff space is the continuous image of a totally disconnected compact Hausdorff space. (In the usual terminology, a "compact Hausdorff" lattice is a completely distributive complete lattice.) Each of these statements is true both of topological spaces (replacing $x \not\leq y$ with $x \neq y$ in Theorem 5) and, with my terminology, also of lattices. I am not sure what form a satisfying explanation would take. Is there a broader theory of which topological spaces and lattices are both special cases? Is there some other way of understanding this? Or is it not as remarkable as it seems, and needs no special explanation. (I'm including a category theory tag because I suspect that may be an arena in which an explanation could be found.) Edit: I'm getting some feedback (Wallman's generalization of Stone duality, Scott continuity) describing other, as far as I can tell unrelated, connections between lattice theory and topology. Obviously that's not what I'm asking for. Maybe I should emphasize that Theorem 6 is a serious result about completely distributive lattices, discovered in 1952 by G. N. Raney. From my point of view it is rather easy ... if anyone can show me how to get it out of Wallman's theory, I will retract the preceding comment. 

As Andreas points out, condition 3 is problematic. However, I think the spirit of the question can be answered by a simple example. Let T be some weak subtheory of ZFC which is mutually interpretable with PA and let T1 be T plus the assertion Con(Z) for each finite fragment Z of ZFC. Let T2 be ZFC. Since ZFC proves Con(Z) for every finite fragment Z of itself, T1 is a subtheory of T2. All the defining formulas of T1 are predicative in the usual sense, and Con(T1) implies Con(ZFC) since if there were an inconsistency in ZFC then we could prove $\neg\,$Con(Z) in T, for some finite fragment Z of ZFC, which would entail that T1 is not consistent. To your broader point, the assertion that any given recursively enumerable formal system is consistent would be considered by a standard predicativist to be intelligible and to have a definite truth value. So a predicativist should be willing to be a formalist with regard to systems such as ZFC which lack a predicative justification but which seem likely to be consistent. 

Yes. I will show that any two positive elements of $A$ commute. Since every element is a linear combination of positive elements, this suffices. Say $a$ and $b$ are positive. Then $a^{1/2}ba^{1/2} \in A_{sa}$, so by hypothesis $ba^{1/2}a^{1/2} = ba \in A_{sa}$. That is, $ba = (ba)^* = a^*b^* = ab$. QED 

Any bounded Borel function $f: \mathbb{R} \to \mathbb{R}$. If $TS = ST$ then (taking adjoint of both sides) $S^*T = TS^*$. Therefore both ${\rm Re}(S) = \frac{1}{2}(S + S^*)$ and ${\rm Im}(S) = \frac{1}{2i}(S - S^*)$ commute with $T$, and since they are self-adjoint it follows from standard spectral theory that they commute with $f(T)$. Taking linear combinations, $S$ commutes with $f(T)$. Edit: here is a possibly more direct proof. Suppose $TS = ST$, meaning that $S$ preserves the domain of $T$ and they commute on this domain. Then the same is true with $T + iI$ in place of $T$ (since the domain doesn't change), and that operator is invertible. Multiplying both sides of $(T + iI)S = S(T + iI)$ by $(T + iI)^{-1}$ (no problems here, $(T + iI)^{-1}$ is bounded and takes everything into the domain of $T + iI$) yields $S(T + iI)^{-1} = (T + iI)^{-1}S$. Now $(T + iI)^{-1}$ is normal (its adjoint is $(T - iI)^{-1}$), so by standard spectral theory $S$ commutes with every bounded Borel function of $(T + iI)^{-1}$. But every bounded Borel function of $T$ is also a bounded Borel function of $(T + iI)^{-1}$, QED. 

Great question. I've often used this heuristic but never thought about whether it had a rigorous meaning. Let me do this in one dimension; the generalization to higher dimensions is straightforward. My first comment is that the Fourier transform between $l^2(\frac{1}{2\pi r}\mathbb{Z})$ and $L^2(r\mathbb{T})$ genuinely sits inside of the Fourier transform of distributions on $\mathbb{R}$: identify an element $(a_n)$ of $l^2(\frac{1}{2\pi r}\mathbb{Z})$ with the sum of delta functions $\sum a_n\delta_{n/2\pi r}$, and a function $f \in L^2(r\mathbb{T})$ with its periodic extension to $\mathbb{R}$. The integral of $\sum a_n\delta_{n/2\pi r}$ against $e^{-2\pi i xt}$ is $\sum a_n e^{-int/r}$. So the ordinary Fourier transform between the integers and the circle matches up with the distributional Fourier transform after making this identification. But you want to approximate the $L^2$ Fourier transform on $\mathbb{R}$. I guess the obvious thing to do here is to convolve the embedded $l^2(\frac{1}{2\pi r}\mathbb{Z})$ with the "rectangular function" which takes the value $\sqrt{2\pi r}$ on $[-\frac{1}{4\pi r}, \frac{1}{4\pi r}]$ and is $0$ elsewhere. This isometrically embeds $l^2(\frac{1}{2\pi r}\mathbb{Z})$ into $L^2(\mathbb{R})$. In the transformed picture it corresponds to multiplying a function in the embedded $L^2(r\mathbb{T})$ by Fourier transform of the rectangular function, which is $\frac{1}{\sqrt{2\pi r}}{\rm sinc}(\frac{t}{2r})$. So now we have isometric embeddings of $l^2(\frac{1}{2\pi r}\mathbb{Z})$ and $L^2(r\mathbb{T})$ into $L^2(\mathbb{R})$ which are compatible with taking the Fourier transform before or after embedding. They converge to $L^2(\mathbb{R})$ in the sense that the orthogonal projections onto the embedded spaces converge strongly to the identity operator; this is easy to check in the untransformed picture. (It's clear that $P_nf \to f$ when $f$ is piecewise constant, and such functions are dense in $L^2(\mathbb{R})$.) 

I don't remember where I read this, but Gert Pedersen once said something to the effect that "When I was young, the first thing we did with any C*-algebra was to adjoin a unit, but nowadays the first thing we do is remove the unit by tensoring with the compacts." The point is that $\mathcal{A}\otimes \mathcal{K}$ is the "stabilization" of $\mathcal{A}$: it is isomorphic to the $n\times n$ matrices over itself, for any $n$. 

Well, if $q$ is bounded then you just recover $C_b(\mathbb{R})$. If $q$ goes to infinity at $\pm \infty$ then you get $C_0(\mathbb{R})$ as you suggest. The general answer is that $C_b(\mathbb{R}) \cong C(\beta \mathbb{R})$ and your construction produces a C*-subalgebra of this, which must therefore be either $C(\beta\mathbb{R})$ itself or else the continuous functions on some quotient of $\beta\mathbb{R}$ which vanish at some point. (If $q$ is bounded we are in the first case, if unbounded, the second.) One additional thing I can say are that $A$ contains the continuous functions with compact support, so the quotient does not identify any distinct points of $\mathbb{R}$ and the point at which the functions vanish must lie outside of $\mathbb{R}$. To see that there are other possibilities besides $C_0(\mathbb{R})$ and $C_b(\mathbb{R})$, consider a function $q$ which is $0$ on every interval $[2n,2n+1]$ for $n \in \mathbb{Z}$ but whose value at $2n + 1.5$ goes to infinity as $n \to \pm \infty$. Then $\overline{A}$ does not contain $1$ but it does contain a function whose value at $2n+.5$ is $1$ for all $n$. So you don't get either $C_b(\mathbb{R})$ or $C_0(\mathbb{R})$. 

I showed in my other answer that there are no $\mathcal{A}'$ and $\mathcal{B}'$ such that ${\rm dist}(\mathcal{B}',P) = 1$ and $\mathcal{B}'$ separates any states on $\mathcal{A}'$ which extend $f$ and $g$. In a comment Chris Ramsey asked about a weaker result where $\mathcal{B}'$ separates all pure extensions of $f$ and $g$. This can be done. The construction is similar to the one given by Chris in his answer. Let $\mathcal{A}' = M_2 \oplus M_2 \oplus M_2$, with the embedded copy of $\mathcal{A}$ given by $\{A \oplus A \oplus A: A \in M_2\}$. Thus $P$ embeds as the element $\left[\begin{matrix}1&1\cr 1&1\end{matrix}\right] \oplus \left[\begin{matrix}1&1\cr 1&1\end{matrix}\right] \oplus \left[\begin{matrix}1&1\cr 1&1\end{matrix}\right]$. Let $\mathcal{B}' = \left\{\left[\begin{matrix}a&b\cr c&d\end{matrix}\right] \oplus \left[\begin{matrix}a&b\omega\cr c\omega^2&d\end{matrix}\right] \oplus \left[\begin{matrix}a&b\omega^2\cr c\omega&d\end{matrix}\right]: a,b,c,d \in \mathbb{C}\right\}$ where $\omega$ is a primitive cube root of unity. This is just the embedded copy of $\mathcal{A}$ conjugated by a unitary, so it is a C*-subalgebra of $\mathcal{A}'$. Setting $b = c = 0$ shows that it contains the embedded copy of $\mathcal{B}$. (1) ${\rm dist}(\mathcal{B}',P) = 1$. This is because any element of $\mathcal{B}'$ has upper right entries $b$, $b\omega$, $b\omega^2$, at least one of which must lie in the left half-plane of $\mathbb{C}$. Thus any element of $\mathcal{B}'$ minus the embedded $P$ has a matrix entry which is at least $1$ in absolute value. (2) $\mathcal{B}'$ separates the pure extensions of $f$ from the pure extensions of $g$. Any pure state on $M_2 \oplus M_2 \oplus M_2$ must be a pure state on one of the summands and zero on the other two summands. So the pure extensions of $f$ are $f \oplus 0 \oplus 0$, $0 \oplus f \oplus 0$, and $0 \oplus 0\oplus f$, and similarly for $g$. Apply these extensions to the element $\left[\begin{matrix}0&1\cr 0&0\end{matrix}\right] \oplus \left[\begin{matrix}0&\omega\cr 0&0\end{matrix}\right] \oplus \left[\begin{matrix}0&\omega^2\cr 0&0\end{matrix}\right] \in \mathcal{B}'$. The pure extensions of $f$ take the values $\frac{1}{2}$, $\frac{\omega}{2}$, $\frac{\omega^2}{2}$ and the pure extensions of $g$ take the values $-\frac{1}{2}$, $-\frac{\omega}{2}$, $-\frac{\omega^2}{2}$, and the two sets do not overlap. So any pure extension of $f$ and any pure extension of $g$ applied to this element yield different results.