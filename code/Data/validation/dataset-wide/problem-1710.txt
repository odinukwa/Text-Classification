The MD3000i is the older 3Gbps bus. I'd at least recommend the 3200 series to get 6Gbps if you're making a new investment. If you have a bigger budget, I'd go with EMC. Otherwise, the Dell, IBM and HP devices are perfectly acceptable. EqualLogic makes some very nice stuff and they fall right in the middle with cost. If you want the most cutting edge, consider 10Gbps. Something more than a switch, like what? I can't imagine what unless we're talking fiber channel and then it would just be a host bus adapter in addition to the fiber channel switch. 

A well-run infrastructure is going to have the tools, monitoring, and controls in place to largely prevent this. These include: 

Yes, schema changes replicate as any other event. MySQL slaves replicate master's binary logs into a relay log and then executes the events. Of course, this is if or another variable identifies the specific scope in question to execute the statements. There was a bug in MySQL, where if you executed alterations by specifying the table as , it would not replicate. It required a statement to proceed the queries that matched a variable to actually execute the queries. I believe this is addressed in the current version but is still something to be aware of. This functionality is well documented on the MySQL Web site. 

Solution 1 is close to sharding but the entire architecture needs to be considered and designed to accomplish that in the best manner. Sharding typically comes up with large scale installations pushing limits of the technology platforms. Solution 2 or dual master replication would be applicable but as your links are physically separate it would be risky to have the application dynamically point to either database. You would want to pick one database and if the database failed, manually repoint the application to the new database. Automatic failover of the application introduces the risk of split brain. You could take nightly snapshots of the secondary databases for backups. As described in solution 3, replication is often used to distribute the readonly load to different database servers. It also lets you use different engines and configurations for the read queries. For example, MyISAM can be quicker for readonly queries. Replication is typically only subject to the physical hardware limitations be it network or system resources. Unless you are storing binary data in the database on a large scale, I would not worry about replication delays under normal load. Being as that your main requirement is speed, I would first focus on the local system configuration and resources. Chances are substantial optimizations can be made there. Automatic high availability solutions are typically best localized to a single physical environment and in case of extreme failure manual solutions can be applied to enable the physically separate site. I'm generalizing based on a LAMP stack and focusing on Web applications. Different applications, protocols, and technologies change thing a bit but in regards to Web servers and databases what I describe is more generally applicable. 

Probably the simplest and quickest option is to use a DNAT. You would configure an additional Internet routable IP and NAT the traffic on standard ports to your additional instance. This would be done on your router. If you have a single server bound to Internet routable IPs, you could bring that additional IP up on the server and bind Tomcat to that, which would allow it to listen on standard ports if they were not used by something else. If you have a proxy or load balancer, you can potentially configure VIPs or interfaces on them as well. 

Typically, software that does not have a unique implementation and is compiled against OpenSSL will reference the system-wide ca-bundle.crt often located in . The location may be different in Ubuntu but the OpenSSL package should include the ca-bundle file. CURL has a variety of options to specify how it verifies certificates.. 

I can't provide detailed recommendations for ISA 2006. Nevertheless, it sounds like the root certificate chain is not up to date for the issuing CA for your Thawte certificate. I suspect Thawte could provide more detailed support. 

Believe it or not, this functionality is not supported in keytool. The best solution I have found so far is the software and instructions available for download on this Web site. I usually generate the key using openssl and then use this method to import the key, as that is not supported by keytool either. To generate a 2048 bit key: 

What are the symlinks to? What are the permissions on the data being symlinked? What user and group does Apache run as in your config? 

Why would it be overkill? If you want to dynamically manage the volumes, LVM is the way to do it. RAID1 and DRDB are good high availability solutions. I like to separate volumes that I need to manage with different mount options, partitions, and settings. Another primary reason would be a volume that I expect to grow, as I would dedicate a separate disk set to it for an easier upgrade path. 

Most of my comments below focus around the assumption that the content is polled on the back-end based on your description. That code needs to be changed. That's a horrible architecture and has the ability to completely cripple the site with any moderate load. In most cases, it is also rude to the third party Web site. If DNS for the site goes down, or their site goes down, it has the potential to hold up all of your Apache children while they're sitting waiting for the response. At that point, your site will be unavailable. There's likely even worse implications depending on how the code was written. I'd say it makes the most sense to poll for the data at a regular interval via a script and use those results. The best place to store that data would probably be a database. A Web proxy is such a kludgey solution I can't even begin to think it acceptable. If not already available, a local DNS cache might help slightly reduce overhead. One possible kludge I see is modifying /etc/hosts for www.thirdparty.com to point to an internal Web server, which displays the content that is pulled down via the aforementioned cron script that runs nightly. While that might work, you shouldn't do it. The application needs to be changed. 

Between these choices, CentOS is definitely the most stable. Stable in the sense that the packages included are mature and tested, which often does not included the latest major version of the software. CentOS is based on RedHat Enterprise Linux, which specifically applies standards desired to enable the highest level of stability for professional environments. This includes a 7-year release cycle. Fedora is the opposite of CentOS in the sense that it includes the latest major versions of software as well as "new" features that are being tested before being introduced to the Enterprise version of RedHat. Fedora's life cycle is shorter and has a completely different approach. Ubuntu is newer to the industry and most commercial support is for RedHat Enterprise Linux. However, Ubuntu does offer a version (LTS) with a longer release cycle. Still, this cycle is shorter at 5-years. It could be argued that Debian is a better choice, as it is more mature and Ubuntu is based on it. Ubuntu was created with a focus on usability. With this, their focus appears to have historically been on end-user features. 

It's important to realize that certain content must be downloaded to be displayed, such as with graphics. Anything you add to "prevent" them being downloaded will be limited. Direct links, however, can be prevented in most cases but a clever script could still set REFERER. Flash streamed from Flash server makes it more difficult to download and hot link as well. For controlling Flash, this should probably be investigated. I like Mike's solution though, I gave him +1. 

The version will be published in the log specified under within your configuration. ( is a standard location in many Linuxes.) Additionally, you should be able to execute the binary equivalent in windows with . Example from Linux: 

MySQL does not support replicating from multiple masters to a single slave. If you want to replicate data from multiple masters to a single physical server, you will need multiple instances of MySQL slaving from the different masters. If listening on TCP/IP, you will need to configure them on different ports and specify different locations.