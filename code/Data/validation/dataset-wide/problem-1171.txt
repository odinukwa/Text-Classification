For Oracle, rollback can take many times longer than the time it took to make the changes that are rolling back. This often does not matter because 

No, you can specify the 'params' (the parts of the clause) in any order and the query optimizer will handle it. The optimizer will do the filtering in the order that it estimates is most efficient, but note that this is more complex than just choosing which order to filter: filtering might be done before or after joining for example. You can't exactly prove this, but you can demonstrate it is true for a particular query by experimenting and seeing if the plan changes. It may even be true that there are edge cases where the order does matter, but my advice would be to ignore the possibility and assume it never happens as otherwise you will expend a lot of effort trying different permutations. Much better to focus on the kind of tuning which you know can pay dividends (eg correct indexing). 

I use the system column to implement a form of optimistic locking, and sometimes need to 'touch' the row to bump without actually updating the row. I currently just do a 'dummy' update: 

But, you need to be aware of the modulo and wraparound and frozen xids. There isn't any way of converting this into a "time", but if you capture the value for your tables now, then compare at a later date, you can get a list of tables that have changed 

SQLFiddle here You haven't specified what you would like to see when the are multiple values of for an though. 

I posted this to pgsql-bugs and the reply there from Tom Lane indicates this is a lock escalation issue, disguised by the mechanics of the way SQL language functions are processed. Essentially, the lock generated by the is obtained before the exclusive lock on the table: 

nb credit to Tom Kyte for the xslt from his blog. See also this other post for a slightly more advanced version. 

But I discovered it would still depend on how you are connecting, eg from the JDBC Thin Driver you aren't using OCI and so the information isn't pushed 

Yes it's safe. It's not exactly clear in the documentation but can return either or depending on what you pass it. If you pass a , it will cast the input to and returns so there can be no tz issue when casting back to . If you pass a it does the truncation the way you'd expect and returns a : 

To do this without the risk of blocking another user trying to update the same profile at the same time, you need to lock the row in first, then use a transaction (as Rolando points out in the comments to your question) : 

The fourth mode of is . This is like pulling the power cord - the instance stops now without any cleanup. You usually want to bring the database up again afterwards and shut down cleanly immediately afterwards as in your example. The concepts guide says: 

I need an easy way to force SQLite to materialize the subquery so that is only evaluated once this is a simplified example for demonstration - my real world query does not have but selects from a view 

then it is possible to insert a row into that links a Ford to a "Toyota" model. Adding to only makes the problem worse. On the other hand if we keep the foreign key as part of the primary key as follows: 

I would first consider trying to combine the two queries into one. If the second query could be refactored to return in the case where the first query returned zero would you be able to work with that? For example, testbed: 

Rather than justify the syntax I think it is better to point out that there are no good general rules-of-thumb when dealing with s and the syntax used to handle them. For example: 

In addition you can get an approximate answer by querying the catalog - if is zero that indicates no nulls but should be double-checked against 'real' data: 

No, not at all, modifying the catalog is bad practice but there is no reason not to query it. But in your queryâ€¦ 

You can get a reasonable estimate of the median using pg_stats if you set the number of bins to 2. You need to set it low because it is a maximum, and the idea is to force an even number of bins so the middle bound is the middle of the distribution. Set it to 100 and postgres is free to use 99, 97, ... which is not what we want. 

It depends. You might see performance benefits, you might see worse performance. For example, a new feature might mean the planner chooses a different kind of join in a crucial query - the impact of such a change in the real world of your application cannot be predicted with certainty (planners make educated guesses, not definite calculations). So if you are planning a migration from one version of any RDBMS to another, you need to have adequate planning and time to test performance after the change in a separate 'test' environment that mirrors production as closely as possible. On the positive side, very often new releases do improve database performance, but in no way should that give you a false sense of security unless your customer is very unimportant to you. 

From what I can see your stored procedure just loops through the rows - it does not 'save the data' anywhere and you have commented out the : 

shows the client nls_lang environment variable on the client. I don't think there will be a SQL query you can run to give the 'current' setting because AFAIK the server is not aware of what translation is done client-side, so any command to show the current setting will have to be native to the client - I used SQL Developer for the above command, but I assume it will work the same in SQL*Plus --edit from AskTom: 

Postgres has had streaming replication since 9.0 and since 9.1 it is possible to pause and resume the application of the streamed WAL records using and . As far as I can tell, there is no built-in way of keeping the application of WAL deliberately lagged by a specific time period (eg 24h). Have I missed a built-in way of achieving this and if not, how should I approach doing it some other way? 

The part of the filter that performs the is . For that to be sensible, it is likely that you need two of the following three statements to be true: 

I'm posting my own effort as an answer but wondering if there is some other fancy way, perhaps with analytics or a set operator I don't know about --edit My test data unintentionally implies that the sets always consist of consecutive integers - unfortunately this is not true with my real data. 

I will shortly be migrating a database from SQL Server 2000 (part of SBS 2003) to SQL Server 2008 R2 Express Edition The database is small, and there are only a few hundred short transactions per day - I'd like to keep everything as simple as possible from a recovery perspective whilst minimising the amount of lost data in the event of a failure Can I just run a full backup every hour using Windows Scheduler? I already have a solution for long term archival of database backups which these could just plug in to What can I do to back up the 'logins' - I understand they are not saved as part of a full database backup. I want to have everything necessary to be able to perform bare metal recovery if we have to Anything else I need to think about? 

The CBO chooses to eliminate the in the 'slow' query - presumably it can tell that the operation isn't needed because of the outer . My questions are: 1) Why does it choose to do so in this case - I can't see a reason from the costings and predicted cardinalities in the plans 2) If it chooses to eliminate the , why not apply the same logic and eliminate the ? testbed: 

I do not think you will be able to mock up fake history with Flashback Data Archive - as Tom indicates in the link you provide. Other than that it really does sound purpose-built for your data. Perhaps you could consider a migration that leaves the current history data in a frozen state and gives access to a 'union' of the two histories for your historical data (through a set returning function for example)? --edit 

Various sources (eg Wikipedia, PSOUG) state that Oracle's does not have a type. Is this true? What about other RDBMSs? 

It shows the general use of the period character () for qualification and the symbol for referencing dblinks. The symbol is also used when creating dblinks. Given that the keyword is also used for creating them, I think it is likely that your external function is doing something with dblinks! 

You are considering "creating two databases and synchronising them daily", which others have suggested is viable if one can be a read-only slave - however if both need to be read-write you are into Multi Master Replication which is a minefield to say the least. In that case throwing more expensive hardware at the problem would very likely end up cheaper. 

It is pretty easy to be a bad DBA Seriously though, a DBA usually has special responsibility for something that is often critical to the success or failure of a business: its data If you run a company then you may well be keen to employ competent experienced people in that role I don't think it is a question of 'easier' or 'harder' - just a question of how valuable your data is: It isn't inherently harder to put a satellite in space than a person, but you would check your sums a good deal more for the latter 

I've tried > and set 'Script Logins' in the Advanced Scripting Options, but I get this in the output: 

Context switches mean using triggers is always going to use a lot more CPU than a simple . The script below can be used to quantify how much impact that will have - and also demonstrates auto-creating partitions using triggers and compares the performance either way. Please note I haven't included any indexing, or any consideration of statements. 

I think your only option here is to 'roll your own' pseudo-multicolumn stats - as extended statistics cannot be created on Virtual Columns. For example: testbed: 

In the example you have given, your best bet is not to have a composite index at all if you are free to change the join order: 

You may be able to use a variation of the following technique - which forces repeated 'MIN/MAX' range scans: Assumptions 

Because SQLite sometimes evaluates functions like more than once if they appear in a subquery, which may be a bug, eg: 

results (timings should probably be taken with a pinch of salt but each query is run twice to counter any caching) first using the function we've written: 

I would like to be able to generate random fields of arbitrary length (<1Gb) for populating test data. What is the best way of doing this? 

the archivelog destination directory (consider switching to if PITR is not required) whether the datafiles/tempfiles in your tablespaces have enabled and an unlimited maximum size. for the listener log the various directories for tracefiles and the alert log 

Where 96, 116 etc are chosen to match the unit of the value '2000' and the point on the globe you are calculating distances from. How precisely this uses indexes will depend on your RDBMS and the choices its planner makes. In general terms, this is a primitive way of optimising a kind of nearest neighbour search. If your RDBMS supports GiST indexes, like postgres then you should consider using them instead. 

Oracle Enterprise Edition has a feature called Transportable Tablespaces that "can be used to copy a set of tablespaces from one Oracle database to another." You can transport tablespaces across platforms and versions, but limitations include: 

In this case, an makes sense. There are other ways of modelling these business rules but this is the simplest and if it accurately fits the facts that you care about in the real world, I suggest it is perfectly ok 

Because you have at 8.1.7, is defaulting to a very high value. Fortunately this parameter is dynamic so you can set it just for your session without affecting your legacy application. 

I may not have understood correctly, but perhaps you can add the new target to ASM (as a new diskgroup) and copy the database files across with RMAN or DBMS_FILE_TRANSFER? Neither method involves "backup and recovery" - just two ways of copying the files 

perhaps you mean listing users and their privileges for a database - I can't quite tell from the question: 

You will find that is a great deal faster because it does a tiny fraction of the IO that does. Of course if the data isn't evenly distributed it will cause the result to be less accurate, but this can be more than offset by increasing the sample size. â€¦butâ€¦