Don't get too worked up about data volumes. You can mitigate a lot of performance issues by partitioning the table on a time key. Just load the data into the table and make sure all queries against it use the partition key so the optimiser can ignore partitions that it doesn't need. You will need to make an incremental loader, though. If you want to analyse trends over time then make up summary tables and refresh them on a periodic basis. This will be less complex than consolidating historic data, and gives you the option of going back and auditing the detail if necessary. Additionally, if your source systems are purged on a regular basis (quite common with POS systems due to the data volumes) then you might not have the option of reloading historic data. Warehousing the detailed transactional data also gives you a repository and frees the line-of-business systems from having to maintain historic data. If you really need to, you can keep data for a finite length of time by clearing out older partitions and archiving them off somewhere else. The partitioned architecture makes this fairly easy to do. An SQL database is a good fit for this because its query facilities are much better than those available from nosql databases and (more importantly) because it will play nicely with third party reporting tools. If you want to do any significant analytical work you might be better off with PostgreSQL than MySQL. 

When a detail table contains denormalized data, should denormalized columns be included in foreign key relationships between the master table and detail table? Here's more details: We have a master/detail pair of fact tables: an table with about 1M rows and an table with about 20M rows. To improve reporting performance for date-range queries we've partially denormalized by adding to the and creating a covering index on with the other columns INCLUDEd. There's already a foreign key relationship between the column in both tables. But SQL Server is unaware that the in both tables is the same if the is the same. Should I help SQL Server to know about the relationship? If so, how? Finally, will adding denormalized columns to foreign keys improve cardinality estimates when joining the master/detail pages by telling SQL Server that cardinality shouldn't be reduced when filtering both tables by the same a date range? If not, then what's the benefit of maintaining this foreign key relationship that includes the denormalized column? We're running SQL Server 2014 an are soon upgrading to SQL 2017, if that matters to the answer. 

Low-latency analytic systems Depending on your data source you can use a changed-data capture mechanism or polling mechanism to identify new transactions. This process can then trickle the data into the data mart. If you want really low latency and control the source database you can implement triggers that push out transactions to the data mart, although this option may place siginifcant load on your system. Leading ROLAP partition on a cube A low-latency cube can be done with a hybrid architecture. Leading data (e.g. for the current open accounting period) can be queried through a ROLAP partition, which will issue queries against the underlying data. Closed or historical data is managed through MOLAP partitions that have aggregations. This gives you near-realtime data without having to issue database queries on large data volumes and the performance benefits of aggregations on the historical data. This has a few caveats. Pure ROLAP dimensions substantially restrict the features that can be used on the measure groups (e.g. no aggregations except SUM or COUNT). If you expect incremental updates to dimensional data you are better off with MOLAP dimensions and an incremental update process triggered off from the job that updates the underlying dimension table. This is quite efficient if you process the dimension incrementally. Testing this type of system is somewhat fiddly, as you have to make sure your changed data capture or incremental load mechanism is working correctly. You will probably need to create a test harness that can post transactions into the system in order to run unit tests. 

I assume the answers to #3 and #4 are "1 volume" and #5 is "2 volumes" but it's #1 and #2 that I'm most curious about. The specific reason I'm asking is wondering if it's possible to increase the Resource Governor's IOPS limit for locally-attached SSD tempdb while having a lower limit for our SAN data storage. So I'm wondering if splitting a single physical disk into multiple partitions might be a way to do this, by putting separate tempdb files on each partition so the total tempdb If #1 above makes SQL Server treat one physical disks as multiple volumes for throttling purposes, this may be an option. I'm assuming that this won't work-- that SQL Server is smart enough to know that 2 partitions is one "volume". But was worth asking. 

Does this same fill strategy apply to tempdb? And does this answer depend on the type of query, e.g. parallel vs. non-parallel? Or is the answer different based on the kind of tempdb I/O, e.g. for temp ables I create vs. tempdb usage by the database engine for worktables or spilling? 

Without measuring your workload, I doubt anyone here is in a position to make recommendations that are any more specific than that. Although generic, the pointers above should be a reasonable start if you haven't implemented them yet. 

The simplest way is to just store previous versions of the whole document rather than attempting to calculate deltas. Lots of systems do it that way even though the XML fields use a lot of space. It might be worth evaluating the data storage requirements for doing this before you go to whole lot of trouble to capture differences. 

Any way you look at it, you're up for a table scan. The export would probably be more expensive than the distinct processing on the query, so at a guess the query would be faster. 

takes a table valued function and 'applies' parameters from each row in the query you are applying it to. The function is evaluated once for each row and the output is implicitly joined to the source row in the record set from which the parameters were obtained. Note that this 'join' can be 1:M - with a TVF one row at source can generate multiple rows of output from the TVF. You don't need to use any more predicates in the where clause to join the results unless (for some reason) you want to further filter the output of the function by applying an additional predicate. is quite useful with xpath queries if you have to go ferreting around XML fields that may contain repeating groups. 

When setting MAX_IOPS_PER_VOLUME in a Resource Pool, what exactly does "volume" mean? Specifically, how many "volumes" would be the following cases: 

Our SQL 2014 server has a blazing-fast tempdb drive (2800 IOPS) but a much slower data drive (500 IOPS). Our application runs a few long-running reporting queries that are I/O-intensive and we'd like to avoid them starving our server for I/O capacity when they run. Ideally we'd be able to limit these queries to 50% of available I/O capacity. Unfortunately SQL Server Resource Pool's IOPS throttling is not percentage-baed nor volume-specific. If I limit to 250 IOPS, then it will unnecessarily slow down performance of queries that make heavy demands on tempdb. Slowing down these long-running queries if the server is busy is OK, but slowing them down by 10x+ if they need lots of tempdb access is not OK. So we're looking for workarounds that will defend other queries from these lower-priority, long-running queries, but without unnecessarily hurting performance of these long-running queries if they happen to use lots of tempdb. It's not practical to change the queries themselves to reduce tempDB usage-- these queries are generated by a custom reporting feature that may sometimes generate really complex query plans that spill results to tempdb. So far the best idea I have is to remove IOPS throttling and instead use the "Importance" of a workload group to defend the rest of the server's I/O capacity from these queries. Is this a good solution to the problem I'm trying to solve? What are the pros and cons of using Importance? Or is there a better way to achieve our goals? 

For 1 billion rows For 1 billion rows (unless the individual rows are extremely wide) a shared nothing or sharded architecture is comfortably in the realms of overkill. This type of volume can be handled on a single server of reasonable spec if it has an adequately fast disk subsystem. Local direct attach disk is by far the most cost effective in terms of price for performance. A single SAS RAID controller can take several arrays, and multiple controllers can be installed in a server. Depending on the configuration, a modern 24-25 slot SAS array can put through thousands of IOPS or 1GB+/sec in streaming performance; a server with multiple PCI-e busses and multiple controllers can theoretically handle more. The type of performance necesary to work with a 1 billion row database can be achieved quite easily and cheaply with commodity server hardware and direct attach storage of this type. A SAN could also be used, but you might need multiple SAN controllers to get equivalent performance, and the hardware is likely to be an order of magnitude more expensive. As a general recommendation, use direct attach storage for applications with heavy I/O requirements unless you need really good uptime. Configuration and change control errors are a far bigger source of unscheduled downtime than hardware failure in modern data centre ops. SANs can give you a more manageable storage platform if you have a large portfolio of applications as they give you a range of centralised storage management facilities. However, this comes at a steep price and getting high performance out of SAN based infrastructure is difficult and expensive. 1 Microsoft do make a parallel version of SQL Server, but it is only available through OEM channels bundled with hardware. The versions available off the shelf do not support this capability. 

A multi-billion-row fact table in our database has 10 measures stored as columns. The value ranges for some of these columns won't ever be above the +/-32K range of a . To save I/O, we're investigating whether it's practical to store these columns as instead of . But we're concerned about what problems might crop up from doing this, including: 

Locally attached disk that's split into two partitions E: and F: Software RAID 1 set E: composed of 2 locally attached disks (yes I know software RAID is bad-- adding this case to help me understand SQL Server's definition of "volume", not to design a production setup!) Hardware RAID 1 set E: composed of 2 locally-attached disks SAN disk E: on who knows/who cares how many disks. 1 SQL Server filegroup spread across two locally attached disks E: and F: 

Will tempdb I/O from a single query be split across multiple tempdb files? (assuming that tempdb is configured to use multiple files, of course!) For non-tempdb databases, MDSN seems to say that yes, newly-added data will be spread across multiple files in a filegroup: 

Note that table is created on the partition scheme instead of a specified file group, and the clause specifies the column to be used as the partition key. Based on the partition key, rows in the table will be allocated to one of the filegroups in the partition scheme. Note: A rule of thumb for designing a partitioning scheme is that each partition should have a row count in the low 10's of millions, say between 10 and 50 million depending on the width of the rows. The disk volume that the partition sits on should be fast enough to do a scan of at least a single partition in a few seconds. Partitioning, Sharding and Shared Nothing systems A bit of terminology seems in order here to disambiguate some of the discussion on this topic. 

Your questions are quite broad, so this is difficult to answer definitively. However, I'll give an overview of the field. Data profiling is the process of analysing data to understand the semantics of the data and identify data quality issues that may need to be resolved. Things that data profiling will address include: 

So we're wondering if there's a lower-cost solution that would store as smallint but expose the colunms as ints to readers. Like this: 

What are the pros and cons of this approach? What can go wrong? Is there a better way to reduce storage & I/O without causing problems with overflow and requiring existing readers to be rewritten? 

But it doesn't say how SQL Server determines what is a "large table" and "small table" for purposes of this optimization. Are these criteria documented anywhere? Is it a simple threshold (e.g. "small table" must be under 10,000 rows), a percentage (e.g. "small table" must be <5% of rows in the "large table"), or some more complicated function? Also, is there a trace flag or query hint that forces use of this optimization for a particular join? Finally, does this optimization have a name that I can use for further Googling? I'm asking because I want this "use the cardinality of the large table" cardinality estimation behavior in a join of master/detail tables, but my "small table" (master) is 1M rows and my "big table" (detail) is 22M rows. So I'm trying to learn more about this optimization to see if I can adjust my queries to force use of it.