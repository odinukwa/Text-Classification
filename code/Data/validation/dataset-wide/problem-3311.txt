Using these context vectors you can get co-occurrences very easy. For example co-occurrence of D and E is D[E] = 4. 

Of course you need a corpus. Generally in statistical NLP, you train your model based on a corpus. For example, for text classification where an input document is fed to the model and it should output its class (from a list of classes). The model is trained on many documents with their corresponding classes and when the new document is tested under that model, it will use the features (information) which was extracted from those documents to classify the new document. In the case of co-occurrence of two words, you can use context-vector, which is very common in statistical NLP. It has a simple definition and very easy to implement, but you will need a corpus: You will define a vector with fixed length (the number of unique words in your corpus) for each unique word in your corpus. The context vector for each word tells us how many times other words have co-occurred with the current word in the defined window, e.g. in a window of words, you see what are the other words occurred with the current word and increment their corresponding element in the context vector. A simple example is show below: Corpus: A D C E A D F E B A C E D Window size: 2 (the 2 words of the either side) Context vectors: 

flight is the frame name and shows the goal of the utterance and the slots (fromloc.city_name,...) shows the details of the goal. There is no code available to convert SQL queries to this formats but, it can be easily induced from the queries (parsing the SELECT and WHERE clauses) 

The annotation is parsed to a hierarchical tree explained in Yulan He's work. Another annotation is the word-level one which is used in other parsers like TBED Parser is in the form of: 

If you're trying to extract the topic of the text using the words of the text, there are some situations which you may encounter: 1 - You have many samples of the same context (e.g. 20newseltter corpora) which many documents in different topics, and each topic labeled to the topic. In this case you just need to do a simple classification on the new data in the same context i.e. you jave 20,000 documents in 20 topics, you give all these documents to a classifier (e.g. Naive Bayes Classifier) and the new document will be classified easily. 2 - If you don't have any training data, you should use parsers (as I see you're trying the same), in this case you should build PCFGs from what you get from the parsers (POS taggers, name entity recognizers) return to you. This should be made on your type of data specifically and no general rule exists. 3 - If there is a large number of data available but they aren't classified, another way is to use clustering. In many cases in NLP clustering doesn't do much to you but it is worth trying. The only thing you need to make clear for the system is the number of topics (clusters). 

ATIS semantic frame annotation was done in Cambridge University under supervision of Prof. Steve Young. There are two types of annotations: Abstract and word level. Abstract annotation is used to train Hidden Vector State (HVS) model introduced by Yulan He and is in the form of: 

Are there any resources which, given a Latin or Greek word, reference modern English words derived from the word? I find it much easier to remember a root when I know a word derived from it. For a specific example, I would like to remember that "Casus belli" means "a case of war". The root is easy to remember, but how shall I remember the meaning of "belli"? Are there any modern English words derived from "belli"? Though I specifically ask about English, resources to other languages would be welcome in the lack of an English resource. 

Coincidence. There is some connection between road-building and conquering. Perhaps one builds a road in all places conquered? One language adopted words of a similar root for arbitrary reasons. Then the second language, having one of the words but lacking the other, looked to the first language for how to build the new word off the former. 

I have noticed that the Latin means both and . Interestingly, in Hebrew the root is used for both (road) and for (to conquer). I see a few different reasons why this could be. Might anyone clarify? 

That is, Sunday is literally "First day" and all the days follow from there. Saturday is "Day of rest". 

A commenter to your post suggested that you not use transcriptions, and I would like to expand upon that. As Hebrew words are very dependant upon the root (שורש) of the word, you will need to know the root of a word to know its meaning. Also, new words are easy to understand if you are familiar with the root. Hebrew words fall into a finite number of buildings (בניינים) that let you pick out the roots very easily. You will loose that advantage if you transcribe. Also you might loose the difference between differing letters with similar sound, such as וו and ב, for instance the words שבה and שווה would be pronounced the same but have different meanings. For what reason do you want to use transcriptions? I may be able to help devise a plan based on your goal. 

In the Wikipedia page History of the English language it is mentioned that English is a "borrowing language", with the implication that there are many loan words in English. What other languages may be considered Borrowing Languages? Additionally, what are languages called that resist loanwords? For instance, I am aware that the French resist polluting their tongue with loanwords and coin French words for concepts that originate in non-French areas. Is there a term for this type of language as well? Finally, is it correct to term the language "Borrowing Language" when the feature is not a feature of the language itself but rather a feature of the culture of the speakers who define the direction that the language evolves? 

Does there exist a scale by which languages might be rated for their ability to let the user easily or concisely express concepts? I am not a professional linguist, but I do find that some languages, such a Hebrew, make it more difficult to express certain subjects than do other languages. Other languages, such as Russian, make it easy to unambiguously express or confer complex ideas, even concisely. I've heard that Swahili is an inherently simple language in which one cannot express many complex (not necessarily modern) concepts, such an example would rate low on this scale. 

Note that this phenomenon is not limited to just Latin and Hebrew . I can post other examples, across other languages, if needed.