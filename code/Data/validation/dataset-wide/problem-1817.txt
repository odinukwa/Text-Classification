I am in the process of deploying a production environment for a website, It runs on AWS. This includes a private VPC with a load balancer and a single EC2 instance running Debian 8.1 However, in the future there will be more EC2 instances as traffic becomes heavier and the load balancer actually needs to work. Therefore I am currently looking into options to deploy multiple EC2 instances with the same codebase and configuration. It comes down to storage, but I don't think that it is possible to reliably deploy S3 as a home directory for our website. The current EC2 instance also makes use of Syslog and many other security features such as tripwire. The main website runs as a wsgi app under apache and is Python (Django) based. Unfortunately, we will also need to push files that can be downloaded by the users. Is unison a solution or what would be used ? ElasticBeanstalk is not an option. The website itself is deployed using git. 

you can remove i386 directory with the caveat that if you need to install new services you need the windows cd handy, or if corrupt files or if you install new hardware. it's not critical to the operation of windows itself. 

Unfortunately this is a vague issue and it needs to be narrowed down. One of the first things that needs to be done is to make a chart clarifying what the problem is, and for whom. Chances are fixing the wireless network will improve your network greatly, but that might not be the real problem. One of the main questions would be , are the people who are physically wired in experiencing problems? If so, then that shifts the focus from the wireless issue to a server/application level issue, perhaps upgrade from older switches to gigabit etc. Wireless is not as good as a physical cable. Especially with consumer wireless routers/access points. They're designed for a few people to surf the internet, not a business network. Chances are the wireless network isn't operating as well as it should be. Few options: 

Every chunk of data has fair checksum on ZFS. So ZFS know which drive holds correct data in redundant setup when failure. Running will repair data or spread data to all running drives for RADZ. ZFS employs Reed-Solomon's error correction which is best for bursts of errors. Missing drive is such burst of errors, which R-S can correct. 

In this case you can play with and in SunOS . But best to block those attempts even before reaching with Solaris . Here you can find pretty often updated list of IPs to block: OpenBL I see attempts very rarely in logs, just using this blacklist. Then you can assemble cron job script to update FW rules or optionally there's formatted file available. 

Zabbix, when compiled with CURL support, can directly monitor web services including complex more steps scenarios. You can setup triggers on HTTP return code, returned data, response time... Documentation here. 

Confirmed rndc dumpdb is the best method. In my case, i discovered that there was 2 seperate bind instances running on the same server (don't ask), one doing forward dns and one doing reverse dns. Without specifying the PID, it attached to the one only doing reverse dns and only showed me that. 

Not sure if I understand the question but you wont be able to have high availabilty of your virtual machines running on a host system. If you're already running an OS, then you need to add on a virtual machine service to windows. this would include: - virtualbox (free) - vmware server (free but being discontinued) - vmware player (free) - vmware workstation (paid) You can run VMs on each desktop, however you WILL notice a slowdown of the host system even if the VM isnt doing anything. CPU is only one concern - HD activity, network activity, ram, and most endusers will notice if you "sneak" a vm onto their workstation. If you're looking for high availability - to move vm around from machine to machine seamlessly - you're looking at commercial software from vmare/xen/microsoft. Using existing workstations is a bad idea as it will be slow and hassle to admin. What you're asking about isnt done in the real world. If you're wanting to play sure go for it, but if this is for a business/school/nonpersonal network it's bad idea to try to harness the "spare" cpu power, cannot stress that enough. It doesnt scale. 

However, this is not recommended by Microsoft, I tried it and it did not solve my problem. The configuration on the WDS (My System is German therefore the actual naming might be different): PXE response tab: 

You could use PHP: First that comes up to my mind is the use of "$_SERVER['HTTP_REFERER']" however, this can be disabled by the user on the client side if that is shown or not as far as I am concerned, please correct me if I am wrong. So you could do something like Pseudo code: 

I am currently building a storage unit for our office. It is rather low budget at the moment, but it needs to be extendable. basically we have a huge database that will grow over the next few months quite heavily. Therefore, ideally we would just like to throw hard discs at our new server. We have not purchased the server yet, but going through some details. However, I would like to get an answer to a question first. How easy is it to expand existing RAID systems? We will start with two HDD 4TBs WD black. But after about 1 month we will need to add another 2 4TB disks. The server we are going to get has 12 bays. Mirroring is important, However RAID 1 only works with 2 disks. Raid 10, would already allow us to mirror a RAID 0. And from what I have seen even the raid 10 can be installed with two disks. However, what happens after that ? Is there any recommendation to achieve a flexible RAID system ? On the OS layer I would just like to build a LVM, that recognises once there is space added to the "disk" so that it can be expanded. But in fact, it lies on several disks which are managed by the RAID controller. 

I'm trying to migrate a dns server that has several thousand zones loaded on it. The named.conf file has about 17 different includes, and some of those files also has includes in them, and lots of commented out etc. It's a fricking mess! I'm wanting to get a list of all the zones currently loaded into BIND. I looked at rndc dumpdb but it doesn't show me just the zones. Instead of following the messy include files, is there an easier way to get a list of the authorative zones inside BIND? Thanks! 

So... was trying to add in a 2008 DC to a single 2003/exchange 2007 setup. ran adprep and updated schema, and joined the new DC to the AD... then 6 hours later noticed everything was not working. Restore tapes are offsite and not available for a few days so no easy option. The gist is the GC is not locatable and the sysvol isn't being shared. If you connect directly to the DC you can query all objects inside the AD properly, but nothing that queries the root domain itself works. went through the dns tree and eveyrthing seems proper. the server is pointing to itself for dns. dcdiag shows: Starting test: FsmoCheck Warning: DcGetDcName(GC_SERVER_REQUIRED) call failed, error 1355 A Global Catalog Server could not be located - All GC's are down. Warning: DcGetDcName(TIME_SERVER) call failed, error 1355 A Time Server could not be located. The server holding the PDC role is down. Warning: DcGetDcName(GOOD_TIME_SERVER_PREFERRED) call failed, error 1355 A Good Time Server could not be located. Warning: DcGetDcName(KDC_REQUIRED) call failed, error 1355 A KDC could not be located - All the KDCs are down. i have done ntdsutil and seized all roles anyway, confirmed under sites that the DC is a GC. it should work...google doesnt show what i want.... i'm good with AD but not good enough ;) Where do i go from here? 

I got many DMA errors on drives, when air condition issue in datacenter and ZFS was able to fix that mess. And it was just simple mirror. I do remember promo video issued by SUN when they introduced ZFS... they made RAIDZ on USB flash drives deployed to 8 port USB hub and then randomly changed position in hub for few of them while doing IO on that pool observing no outage. 

If you have speedy lines between sites you want to mirror, I can imagine something like you export iSCSI volumes from sites storages and put them mirror and add some local disks for ARC, ZIL, cache to lower read/write peaks running over iSCSI. If your storage is mainly for backups, then it would be OK. Nevertheless SUN once had such product behaving like that on ZFS. 

Zone virtual interface has some features limited... some states can't be setup, packet filter doesn't work in zone too. If I remember right, zone interface can't send ethernet broadcasts, so then no DHCP. Btw why you doing that bloat about setting up zone interface? What about this? 

The confusing part here is that it is advised in the tab to tick the first option since it is on the same server. Network Configuration Tab: 

$URL$ "Set max_connections to the number of concurrent connections you need. The default value is only 100 connections, which is very small." So, yes you can set "max_connections" 

So basically the script checks in the beginning if the variable is set, if it is not set than it must be new visitor, therefore it should redirect to the homepage. If that is actually set then it should not redirect. EDIT: Recognised, my first solution was wrong since it could not match if he actually came from a subpage. 

Thats not the default one, however it is in the allowed range. The UDP port range is the default since it is not advised to change them. I tried to change the "networkprofile" from 100mbits/1gbits and custom. I am running a 1gbit network with CAT6 cables and 1gbit netgear switch 5 ports. Everything is configured to use 1gbit. The WDS is authorised for the DHCP server. My ISA 2006 configuration: For the internal networking i have configured the following policy array: Allow protocols on internal network including the w2k3 host: 

I'd likely blame power management settings on the NIC. Replacing the NIC with something non sucky (intel?) is good way to confirm. Sounds like network link is dropping. Verify that within event viewer on the pc. Should show something like "Network Link down". If you confirm it's not a network card issue (most likely), then the next likely suspect is a dhcp lease timeout/dhcp server problems. Again, that will be listed in event viewer on local pc. If in doubt, assign a static ip and see if disappears. 

Well, we eventually couldnt figure it out and ended up to making someone drive 9 hours to get last known backup tapes :p Not fun at all. Moral of story: 1.) have more than 1 DC at all times 2.) back the systemstate up constantly and verify! 

In a perfect world, you would have network drops to all machines possible, and a good wireless access point for the few people who cant use a network cable. You can connect as many routers/switches into a network that you want to (there is a limit with old 15 year old hubs but not new switches), but each one introduces more failure points and potential quirks when dealing with consumer equipment. 

Its a USB drive? Have you tried to safely remove it and unplug it and restart the computer? Then reconnect it and try to share it again? Sometimes these little things help on windows. You could also try the advance sharing rather than the normal sharing - or the other way around. Put the permissions on groups rather than on single users. Thats what I would try first. 

I have been struggling with a problem on my Windows Server 2008 for the past 4 - 5 hours and cannot figure out whats wrong. I have tried pretty much everything that I found on google and all the links are purple. Hopefully you guys can help me. I am running a Windows Server 2008 Standard edition with the latest updates as of today. Furthermore I am running a Windows Server 2003. Both are virtual machines on my ESXi 5 server. My network is: 192.168.10.0/24 W2k8: 192.168.10.251 is the PDC running ADS, DHCP and WDS W2k3: 192.168.10.253 AND 192.168.1.175 running Routing and Remote Access and ISA 2006 Enterprise In my internal network (192.168.10.0/24) I have my client machine (192.168.10.10) that runs a VMWare Workstation. I am trying to deploy Windows 7 Home Premium to a virtual machine on my VMWorkstation via PXE. I have set the Workstation's VM network adapter to "bridged" so that it uses the physical network adapter and is connected to my internal network. The DHCP pool is configured to give IP addresses from 192.168.10.10-192.168.10.15 (works for normal clients and is not used up) When I start my VM with the PXE I get the error: PXE-E52:proxyDHCP offers were received. No DHCP offers were received Apperently this means "that means that WDS responded but the DHCP server did not." People suggested to direct the traffic to both WDS and DHCP on the router, since everything is on the same subnet there is no need for that as the broadcast is seen by everyone (WDS and DHCP) No reservation for the virtual mac addrs is made on the DHCP. Furthermore it was suggested to configure the DHCP options: