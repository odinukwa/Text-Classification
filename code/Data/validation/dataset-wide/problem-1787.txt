It didn't kill rdiff-backup, it should have but its is -1000. This is caused by a bug in sshd. The bug is fixed but wont be available until the next release which is openssh 6.5. sshd fails to set the oom_score_adj of new shells it creates back to 0 if you reload it, causing all child processes you spawn via SSH (so your bash shell and any child processes that creates) to have -1000 and subsequently can hog all the memory without oom-killer killing them. The quickest way to fix this is to (assuming 7567 is the pid of sshd like in your case):- 

You have enabled some huge page assignments and aren't using them. These hugepages have taken up a big slice of your memory. Disable this assignment. Run 

Whilst sysstat is available for CentOS, the particular package you want doesnt come shipped with CentOS 5. What you'll need to do is download sysstat from the main page which is here. Configure and compile it. The latest version comes with a program called pidstat. This program will give you what you want. It works much like other *stat utilities like iostat, mpstat and vmstat. Note to pass the -d flag. You should not install this version, its binary incompatible with the files it creates. Just copy the pidstat program it compiled and stick it in /usr/local/bin instead. 

It is your sysadmins responsibility to justify or resolve the slab allocation anomaly. Either you haven't given us a complete picture of the whole saga that lead you up to this (which frankly I am not interested in) or your sysadmin is behaving irresponsibly and/or incompetently in the way he considers handling this problem. Feel free to tell him some random stranger on the internet thinks he isn't taking his responsibilities seriously. 

Both and (using the default options) are not flagged. This causes the logic to copy out of the struct which according to contains the remote login name as recorded by whatever wrote into the . 

Actually writes the data into memory as writeback (up to 3.2GB) and does not actually write it out to disk. Its slower (but not a realistic performance benchmark) on the VM because you've probably assigned a much lower memory assignment to the VM itself (lets say 2G) and this leads to only providing ~400MB of writeback before it will force the contents to disk. If you run that command, then run , you'll notice sync takes a long time to return. You need to run your command doing the following instead to get a better idea of your actual throughput. 

This is due to selinux. Your script is not permitted to do a stat on as the type. This is by design. Depending on what you are trying to stat() (not just /dev/shm) can depend on what action you might want to take. If you the program to run in the label this will avoid SELinux restrictions, at the cost of no SELinux protections whatsoever for the script being ran. 

Is the post labelling policy. What you discuss in your problem actually relates to the runtime policy. When a entry is created in a directory using SELinux the rules governing what label the file or directory ends up being are not dictated by the regular expressions you quote but other rules as follows (I believe this is the correct order but might have missed something). 

You seem to have two conflicting requirements. "I want to allocate 4G to this host" and "I dont want the host to use 4G". Aside from that fact, what makes you think dropping those caches makes the hypervisor drop the memory in the map? On the hypervisor (virtualbox host), there may be demand paging happening -- thats fine, but what you think is happening probably is not. There are two stages when virtual memory is request. 

This file is placed into memory and cached, but all new objects in the cache are sent to the 'inactive' list by default. This evicts 1750MB of your 'cooler/inactive' memory from the varnish cache and replaces it with the catted file. The kernel is now forced to write out 1750M of this data back to the disk (in the absolute worst case scenario). IO Wait and device utilization takes a hit because you are reading in a 2G file and writing out a 1750M file. 97% of inbound requests are unaffected by this because they want the hottest 1750Mb of varnish data which is in the active portion of page cache! 3% of unlucky clients want data in the cooler cache. These guys see a delay now because the disk utilization is already pretty high and they are queueing to fetch back pages into page cache again! Since the catted file is never re-read quickly enough page cache evicts those pages in favour of the 3% of clients wanting some cooler data. 

I guess if anything your running low on stack, if you set it to the redhat default of 8MB your probably going to be able to run more threads. This one is the same as the one linked to with explicit stack sizes set to 8MB. See what happens when you compile this one. 

It could be a difference in bitwise operations on how each system calculates the WWN. Do you have any other examples? If this is right (and I'm only guessing) I can only assume that no standard way exists and both the raid controller and the kernel implementations each choose a different method. 

In terms of offering it as a virtualization option I am not. I find the current technological setup lacking. 

(generally also called on x86_64) tends to map memory for zones outside of the standard 896MiB ranges directly kernel accessible on 32 bit systems. On x86_64 seems to cover all pages above 3GiB in size. contains a zone used for memory that would be accessible on 32-bit DMA devices, that is you can address them with 4 byte pointers. I believe is for 16-bit DMA devices. Generally speaking, on low memory systems wouldn't exist, given that covers all available virtual addresses already. The reason you OOM kill is because there is a memory allocation for a zone with 0 pages available. Given the out of memory handler has absolutely no way to satisfy making this zone have pages to use by swapping, killing other processes or any other trick, OOM-killer just kills it. I believe this is caused by the host VM ballooning on boot up. On KVM systems, there are two values you can set. 

There are very few non-zero order pages left, none in one zone left at all. I cant guarantee anything but you may want to try to turn off ksmd and re-compact memory. Compaction only gets called automatically on higher order page allocations and never calls oom-killer, so I assume that the system has tried to allocate memory from orders 2 or 3 and got stuck. To compact memory run Theres only so much to go off in this question, but I suspect is causing the fragmentation by scanning for pages duplicated in both VM's and swapping them all around. 

A page has really nothing to do with this problem. The following command should output a list of pids for httpd sorted by CPU percentage with the current working directory that httpd worker is in. This of course assumes that httpd is the culprit. 

You've got your accept rule in SMTP after your reject rule before the rest of the unmatched traffic. Basically, your and port will never match due to the order of which you've added them to the IPtables chain. 

I have not looked massively into this but btrfs as a filesystem works on groups of disks, not individual devices. I suspect that there is no way for btrfs to distinguish between the mounted snapshot and the real mounted filesystem when a mount occurs. It may as a matter of fact see the UUID of the underlying subvolume, assume its a mirror of the original volume and write to both volumes at the same time. I would be surprised if this ever gets fixed seeing as for most intents and purposes btrfs snapshots supersede LVM snapshots. 

There is quite a lot of stuff happening here, but the pid 3734, a java process appears to be your culprit. You should find out what that is doing, what arguments were passed to it, what its parent pid is and a little about what it is meant to do. Over a 30 second period of 1 second samples java uses 1778.49 read kb/sec, there is also other java processes, pids 9677 and 19295 using 946.52 and 498.04 read kb/sec respectively. I'm in no position to tell you whether what they are doing is wrong or right, but you're high I/O is due to those java processes mainly. 

This should fix your problem. Note: Its not actually necessary to provide the leading zeroes at the start but for clarity I added them. 

Set the related and established rule on a per-protocol basis. Make ICMP traffic of that type not trackable by state tracking. 

So to compare, in a scenario where you have 2 processes waiting to run - if you renice a process +10 it gets approximately 1/10th of the CPU time a priority 0 process has. If you renice it +19 it would get 1/100th of the CPU time a priority 0 process has. It should be noted you'll probably see your load at 1 at least during the duration of your pipeline. I imagine this would be a more elegant solution to your problem. 

The script assumes you have also created a 512Mb device and that your virtual block device is on . You can just output this data to a text file as a table and pipe it into . Once you have created the device you can then begin to use it like a normal block device, first by formatting it and then by placing files on it. At some point you should come across some IO problems where you hit sectors that are really IO holes in the virtual device. Once you have finished use to remove the device. If you want to make it more likely to get an IO error you can add holes more frequently or change the size of the holes you create. Note putting errors in certain sections is likely to cause problems off of the get-go, I.E at 32mb into a device you cant write a superblock which ext normally tries to do, so the format wont work.. For added fun -- you can actually just then and fill it with data. Once you've got a nice working filesystem on there, simply unmount the filesystem and add some holes using dmsetup and remount that! 

This seems to suggest to me that the numbers are counted in sector sizes (512) bytes. Given thats what value you'd end up getting if you right shifted a byte count by nine. This appears to be a similar setup between kernels 2.6.27 - 4.0.x anyways. 

I know you can do this using network namespaces because thats how I did it. But it is pretty complicated. The process would be this. 

Given the tables are there right now... Set the related and established rule on a per-protocol basis. 

This is false. 'inactive' memory is actively mapped memory which has not been utilized by any application for some time. When its time to swap, memory is taken from pages marked like this and swapped out. It can also be used to swap out in favour of page cache. 

Because a username must be resolved to a UID, so if the user does not exist no resolution can occur, whereas a UID is a terminating type (it describes itself without resolution). Technically all chown requests by the operating system must be done against a UID. The username is for your benefit, not the operating system. 

PAM does not just do authentication, but authorisation and session services. You probably want to keep it on as it adds quite a bit of flexibility. PAM will be called for a successful pubkey authentication, because session and account services are still checked. PAM can do things SSH cannot. This list is not exhaustive: 

This is probably no use to you but for reference cp --reflink source target does thin copies of files using copy-on-write. This means you could copy the file outright and only changed blocks would actually be copied. Unlike a hard link the new file has its own inode and metadata meaning you can then provide the copy of the file to the new user using standard chown stuff. As far as I am aware this is a feature that is only available on OCFS2 and btrfs currently. I guess that does solve your problem but seeing as the availability of it is not widespread it probably wont be useful. 

OS corruption is quite unlikely (system calls being screwed somehow I guess). hdparm does not utilize the filesystem to do its tests which eliminates any slowdown from that area, including degragmentation issues. If your using LVM, theres a risk that you happen to be reading from extents that are fragmented though. Your examples dont indicate this however. VM corruption, well anythings game I guess and thats down to a number of factors I can think of off hand, but probably would include more: 

In Summary I find load a very woolly value, precisely there are no absolutes with it. Its measurement you get on one system is often meaningless in reference against another. Its probably one of the first things I'd see in top purely to check for an obvious anomaly. Basically I'm using it almost like a thermometer - like a general condition of a system only. I find its sampling period way too long for most workloads I throw at my systems (which run in the order of seconds generally, not minutes). I suppose it makes sense for systems that execute long running intensive tasks, but I dont really do much of that. The other thing I use it for is long term capacity management. Its a nice thing to graph over long periods of time (months) as you can use it to understand how much more work you are handling compared to a few months ago. 

Thats not always true. Only if the mmapped() file is opened in MAP_SHARED is that true. Else dirty pages are swap-backed. Which is the case for you it seems. If you add up the reported usage of that process given at the bottom of your output and convert it into pages (4k). It equals the RSS reported in the task dump for that process. 

EDIT Thinking about it, is practically doing what the webserver does. Try setting and to and respectively, then see how that holds out. If you rewrite a policy for it you need to consider a number of things: 

If the outcome is 1, the request is denied outright as a token was sent but failed. SSHD does not attempt other authentication methods. If the outcome is 3, will next attempt other authentication methods, which includes the PAM section. I am not familiar with but I assume it requests an authentication token regardless of whether or not the user exists for security reasons. Having it fail immediately indicates to a user/attacker that such a user does not exist, so from a process of elimination you could enumerate users. As for the "requisite" option, in the stack setup given "required" and "requisite" have the same affect. can not request a ticket without a valid user anyway so it would end up failing immediately. For the config given is not used for authentication but authorization, this is a step that occurs after authentication. To clarify; authentication deals with proving you are who you say you are, whilst authorization deals with you having correct permissions to do what you want to do (which in this case is login). 

You can see this where apache is using IPC semaphores, probably from a apache module. If it crashes it wont clean up the semaphore which remains permanently assigned to the system at least until a reboot. Run and you'll probably see a lot of stuff owned by apache in there. You can use the command to clear it up. 

The traffic is likely being filtered. Because you supplied your domain name (assuming here, that despite having two A records with two different addresses is accurate.. Tracerouting to port 80, which we can demonstrably prove is open.. 

You have not declared link local, despite the fact you have an address assigned for it, no link-local route appears to be listed in your routes. 

At first glance, it looks as though the file ownerships are all just simple, and will not allow one user to interact with anothers. However, we can inspect the ACL using which shows more infomation. 

I think the LXC option (not libvirt-lxc) is better maintained. Having read the sourcecode, it feels rushed. Traditional LXC certainly has newer features whicih have been better tested. Both require a degree of compatibility by the init system being ran in them, but I suspect that you'll find LXC slightly more "turn-key" than the option particularly in regards getting distros to work in them.