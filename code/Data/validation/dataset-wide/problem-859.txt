However, when the system is exactly face-on, the Doppler shift is useless, but still the gravitational effect of the planet onto the star is useful and used. Here is what happens in the latter case: 

First of all, it is not that you can always find the planet (if it is there). Anyway, if this is the case, when we observe the system not totally face-on, we resort to radial velocity measurements. The principle is simple, the actual much less. The orbiting of the planet around the star causes the star to "oscillate" around the center of mass. Here you can see a good example: 

The terminology you used is also found in this paper (where you have also other references): "the discovery of many double-mode Cepheids (DMCs) pulsating in both the fundamental and first-overtone modes (F/10) and the first and second-overtone modes (10/20)" Other reference: Classical cepheids 

In the same article there is a recent referred paper on this topic, which explains Perhaps you already know this website, which shows many good pictures. I was trying to find a better figure, but I guess you have to push your imagination according to the definition. EDIT: According to this, SG coordinates are similar to the galactic ones, so that the North Supergalactic Pole is defined in Galactic coordinates, which means that they have the same origin (i.e., the Sun). 

I am not an expert, but it seems that the fundamental plane is a relation among characteristic quantities of the galaxy, showing a correlation that is the analogous of the Tully-Fisher relation for spiral galaxies. You can start from the virial theorem: $\frac{M}{R}\sim v^2$ ($M$ mass contained in the radius $R$, $v$ velocity dispersion of the stars, also indicated with $\sigma$ in these cases) meaning that the stars behave like an isothermal sphere. Assuming that all galaxies have the same mass-to-light ratio $L/M$, and that all galaxies have the same surface brightness $\Sigma=L/R^2$ (with $L$ luminosity), you obtain the Tully-Fisher relation: $L\sim v^4$ Of course, all galaxies do not have the same surface brightness. So if you take $\Sigma=L/R^2$ and substitute that into the virial theorem (keeping the mass-to-light ratio assumption), then a relation between luminosity, surface brightness, and velocity dispersion is obtained: $L \sim \sigma^4 \Sigma^{-1}$ You can plot the stars distributions in elliptical galaxy according to this relation, and will note that the stars do not distribute randomly, but are more concentrated along the fundamental plane (more or less the same happens for the HR diagram): 

Using Fits Fits View (FV)View (FV), you can either modify the header by terminal commands, or using the GUI which makes you visualize all the keywords and values. 

As it can be seen, the spectra is differently modified by different amounts of neutral hydrogen column density $N_H$. This, of course, can be related to different aboundances, to best-fit the spectra and find the metals amounts. Also, this method is tightly related to the iron K$\alpha$ line at $6.4\,$keV, which is another measurement of the metals amount (Fabian et al. 2000): 

Here you can find a list of all the natural satellites in our Solar System. You can check one by one (good luck!) OR you can check this webpage, and just add the terms. Please, keep in mind that the latter website is kind of unknown, so double-check at least some of the masses, before to trust it. Perhaps, you can cross check with this list as well, and see if other parameters (size, distance, etc.) agree. 

First introduced by Urry & Padovani, then developed for further observations. Anyway, this is only half of the truth, since the AGNs evolve, and in this evolution feedback on the host galaxy is exerted. Also, the amount of feeding/obscuring gas is varying with time (redshift) and this has to be taken into account for a correct classification. For the jets too, it is also possible that they forms only in certain stages of the AGN life... As usual in astronomy, things are much more complicated than simple catalogs/models ! 

Even if the answer has already been accepted, more evidences can contribute to this thought-provoking topic. I think, as it has been mentioned, eliosismology is a good way to map the interior of the star, but I am not able to argue about it. Another disproof, comes from neutrinos. Interior of Sun can be investigated by neutrinos detection, which do not interact passing by the outer shells, and can directly say where they come from, and how. Standard solar theory predict neutrinos production by p-p chain. This has been observed, even if the neutrino flux is about $1/3$ than the expected one, because of the neutrino oscillation. Now, this theory become solid after Bruno Pontecorvo, but I do not think we have any other evidence of observed neutrino oscillation. Furthermore, Earth's age is helping us. We know Earth is ${\sim}4$ billion years old, that is roughly the age of the Solar System and the Sun itself. From Niels Brendt website, we know that after such a long time, the white dwarf luminosity has become much fainter then Solar (where Solar is referred not only to the Sun, but to general G2 main sequence stars). Neutron stars cool even faster. And this is obviously not observed: if Manuel's conjecture was real, Sun should be much fainter now. To this, add that they say that their supposed SN happened something like 5 billion years ago... As a final note, I would highlight that, not only this author has never been mentioned in literature, that is (as we know) synonym of poor quality, but he actually never published (at least since 10years or so) in any refereed paper, which is automatically translated, to me and I think to the community, as not science. 

Yes, but most probably not in your mentioned example. You need larger distances to make a difference, I guess. However, it is by now largely accepted a unification model for AGNs, where the difference are mostly due to the angle of line of sight. 

It is because the neutrinos cross section is very small. Roughly, the cross section is a measure of the interaction probability (for a given force) of the given particle with your target. Since the probability for interaction depends upon the strength and range of the interaction, the cross-sections for neutrino interactions are very small since they interact only by the weak interaction. Hence, it is not that we receive few neutrinos, rather it is our (in)ability to detect them, due to our current knowledge of their intrinsic nature, which prevent us from measuring high fluxes of neutrinos. For this very reason, neutrinos detectors are made of huge volume targets, in order to maximize the detection probability. 

They took advantage of a solar eclipse to measure the light bending from the expected position of the sources. The experiment has been a bit contested, but successive experiments of the same type (measures of light bending of Galactic stars during solar eclipses) confirmed those results. And, he was Eddington in the end! You can find the original paper here, and some other information on the Wiki page. 

Look at here and here if you want to go deeper. @Bardathehobo This figure shows what I mean when I say that a currently accelerating Universe can still crunch. This is because we are basically ignorant upon the dark energy issue. 

Actually, one of the first confirmation of GR, is by Sir A. Eddington et al., who measured the deviation of light trays of an unsaid-as-far-as-i-know stars, on May 29, 1919. Here is an original snapshot from their experiment: 

Some classical Cepheids pulsate simultaneously in two or even three modes. Their lightcurves can be explained as a overposition of fundamental plus overtone modes. 

Your reasoning would be valid not only for galaxies, but also for stars and anything it shines in the Universe, but there is an important effect which invalidates it: absorption of light. Intergalactic and interstellar medium is filled with dust and gas, which contributes to absorb and scatter the light from distant objects. Especially on the plane of our Galaxy, we still have plenty of gas and dust (Milky Way is a relatively young galaxy): indeed, to look at distant object we try to orient our telescopes towards the Lockman Hole, whenever it is possible. This is especially valid for low frequencies light: at higher energies, scattering and absorption of X-ray and gamma-rays from standard amount of absorbing material is negligible (even if, the more distant you look at, the younger the objects, the more is the dust and gas available which is still not locked in stars). Also, consider the Olbers' paradox, which indicates for an expanding Universe to account for the "dark sky". 

If you are going to do data analysis, you need to understand how the fitting procedure works. This means a lot of statistics, and new terminology and so on, which is hard and takes much time. If you want to start to fit, e.g., a spectrum, you should definitely read the XSPEC guide. You can find a pdf online as well. In short, fitting is to take a model and measure how well this model fits to your data. To quantify this "how well", usually the chi-squared distribution is considered: $\chi^2 = \sum\limits_{i=1}^n(\frac{X_i - \mu_i}{\sigma_i})^2$ where $X$ is your data (observed value), $\mu$ is your expected value (which, in this case, corresponds with the prediction of the model), and $\sigma$ is the variance on your data point (the error). This is the most general information that you need. You can read more here, and especially on the Numerical Recipes (don't be scared by the huge format of the last document, the pages you need are only few around Chapter 15). For fun, you can just try to play with XSPEC, to take confidence, and see at least how things change when you change your model or your data (especially if you know which model is the best-fit for your data). To understand everything will take years, but if you never start, you'll never arrive ;) 

It turned out to be not real GRB event, just a false alarm. My understanding is that, since the source is actually a steady X-ray source, you have a high count rate level at the base: 

First of all, your first question. This source clearly state that , for the same formula that you quoted (similar job). It is a bit tricky as the article "explains" the values, but you have to pay attention to the exact definition. I think it is better to work out with an example. Let's take the . Good enough, you can better read the paper from here. From the table, we have $A_{el}=10.93$. This is the abundance of He relative to H (in logarithmic scale). From this you find out that $\frac{N_{el}}{N_H}=0.08 = 8\%$. Indeed the work confirms this value (see the last page). What you quote as , is what they call , which means $Y = mass\ of\ Helium / mass\ of\ Hydrogen$, and this is indeed about $25\%$. 

Is it possible you refer to a Thorne–Żytkow object ? This is literally a star within another star, more specifically a compact object (Neutron star) inside a giant star (red giant or supergiant). 

I guess, this is among the best you can find: Hubble Heritage. These are visible picture, real, and not modified, taken from space, and super-amazing. If you meant pictures in wavelengths other than visible, please just ask. 

Also, there is a proportionality between luminosity and mass: $L\sim M^{3.5}$ Then you can identify "isomasses" lines in the HR diagram, but I can't find such a figure to show. 

This shows the relation between the surface temperature (on the x-axis) of a star and its luminosity (on the y-axis). The temperature is an index of the Spectral type. For distant stars also spectroscopy is taken into account, to classify spectral types according to their absorption lines here some examples. The usual classification is called MKK (from the astronomers that had developed it - Morgan, Keenan and Kellman in 1943), and divides the stars in 7 stellar types O, B, A, F, G, K, M. Please note that from the 1943 other stellar types have added, but these are the most used. This sequence goes from the hottest (O) to coolest (M) stars. Each letter class is then subdivided using a numeric digit with 0 being hottest and 9 being coolest. Our Sun is a G2 type star. This classification also uses an additional notation to avoid the "degeneracy" you mentioned: a roman numeral between "I" (one) and "V" (five). The higher the number, the wider the lines in the observed spectrum. However, despite of the quantity these roman numbers indicate, they are an index of luminosity class. This is based on the width of certain absorption lines in the star's spectrum which vary with the density of the atmosphere and so distinguish giant stars from dwarfs. Luminosity class I stars are supergiants, class III regular giants, and class V dwarfs or main-sequence stars, with II for bright giants, IV for sub-giants, and VI for sub-dwarfs (from here). Then, our Sun is "completely" identified by the type G2V. You can take a look at the figure below, and check out by yourself where it belongs: . Also note that other indexes are used to characterize the stars, as for the width of the lines in the luminosity class, or for some peculiarities. Finally, if you are interested in the study of stars, you should always keep in mind the Stefan-Boltzmann law: $L=4\pi\sigma R^2 T^4$ By this you can identify isoradii lines in the HR diagram: 

Even if the accepted answer is well posed and clear, I would mention another method for dust measurements. X-ray spectra can also help to infer the amount of dust, analyzing the absorption at low energy of the observed spectra. Indeed, the neutral hydrogen column density $N_H$ and the extinction $A(V)$, which is caused by the dust, is found in our Galaxy to be (Guver & Ozel, 2009): $N_H = 2.2\times 10^{21} A(V)\,$mag From the extinction one can get the dust optical depth (see here), and from that the volumetric dust density. A good example of how to obtain $N_H$ is given in the following plot (S. Ikeda et al. 2009, ApJ 692 608): 

Milky Way is a young galaxy, filled with neutral hydrogen along its arms. This hydrogen can undergo state transition, a rare event, which is compensated by the huge amount of hydrogen itself, i.e., even if this transition is rare it is still strong because a large amount of atoms can exhibit it. The basics of this transition is explained by quantum physics through state-change in the energy levels of the hydrogen atoms, and can be summarized in the following picture: 

If you read scientific papers (at least in astronomy, as far as I am concerned), you will always read of a section which is called , or similar. There you (as author) have to explain which kind of data you used and the detailed method for data reduction. Most of the time, this section goes along with a table in which all used data are described in detail: the . The guide principle in this section is that, any reader can reproduce your results. In the same section, information about instrument calibration is also reported in detail. Also, error sources and error determinations. Basically, everything that is included in your published data. Of course, mistakes can happen, and sometimes we do not consider variables which are important. I do not know the details of the two examples that you mention, but they are not the first and will not be the last. Paradoxically, what you report is exactly the reason for disproving those experiments. Both major and minor results are always tested twice or thrice or more, by different groups, different facilities/observatories, different software versions. There is no way you can guarantee for a breakgrounding result without being "differently" tested. In the end, the method works well in this way! If I can't reproduce your results by following your description, either you are a bad writer, or your experiment is going to be confuted. All the rest is explained well in the answer from OP @moonboy13, with the only exception that, for my experience, when data are private, they are released after 1 year.