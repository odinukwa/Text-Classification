So you can't nest these views, and you can't use subqueries. You can't use the APPLY operator either, so you can't use a table-valued user-defined function (nor can you effectively use the XML function). But you can use deterministic scalar-valued functions. So you can construct some rudimentary XML. For example: 

is set before , yet it sometimes has a later value. (I've tried this on SQL Server 2008 R2 and SQL Server 2016. You have to run it a few times to catch the suspect occurrences.) This does not appear to be simply a matter of rounding or lack of precision. (In fact, I think the rounding is what "fixes" the issue occasionally.) The values for a sample run are as follows: 

You can also use a UDF to enforce a custom CHECK constraint on a column, which could go and check the other. But there are some scenarios where that falls apart. 

You might think you could cheat by creating an outer view which didn't have a subquery, calling the original view. But if you try: 

Short answer: You can't. Long answer: There's a few things going on here. First, you might be underestimating the power of . It will provide a certain amount of nesting "automatically". Your provided example, in fact, could be handled without the nested XML generation. Let's make some test tables and data: 

Result: "aӒa" I suspected it might have to do with how it's interpreting the characters, so I tried specifying a binary collation... and that "fixed" the issue: 

(That will break down if you have more complex or custom nesting requirements.) Regardless of this particular example, I don't think there's an effective way to get (dynamic) XML into an indexed view. First, a "indexed view" is really a view with a unique clustered index. You can't create "normal" indexes on an XML column. Let's say you try: 

I thought it might have to do with the string concatenation happening for the haystack but not the needle, but when I tried this, it still didn't "work": 

We then connected to the server several times, using Windows and SQL authentication, including a purposely incorrect password for the SQL user. We then queried the extended event data as follows: 

While you could create complex, multi-level XML within the function, the function cannot reference any tables (or it wouldn't be deterministic). For example, this: 

Therefore the function can't go get the information it needs. It must be fed the information. And there's simply no way (that I can think of) to collect the data from multiple rows and feed it to the function. You can feed a deterministic function XML to manipulate, but you can't create the base XML without a subquery. So in short, I don't think there's any way using an indexed view to materialize XML which contains data from multiple rows in any table. 

But how big is the actual table? The most common measure is to look at the size of the segment which is 72 MB. 

In Oracle, one way to enforce this sort of constraint in a declarative fashion would be to create a materialized view that is set to refresh fast on commit whose query identifies all the invalid rows (i.e. rows that have no match in ). You can then create a trivial constraint on that materialized view that would be violated if there are any rows in the materialized view. This has the benefit of minimizing the amount of data that you have to duplicate in the materialized view. It can cause problems, however, since the constraint is only enforced at the point that you're committing the transaction-- many applications aren't written to expect that a commit operation might fail-- and because the constraint violation can be somewhat hard to associate with a particular row or a particular table. 

The number of columns in a table has nothing to do with the number of columns in the projection of a statement. You can always add additional computed columns to the projection (at least practically... theoretically, you might eventually hit some sort of logical limit). The fact that you are querying here is irrelevant. You can do exactly the same thing with any table. The projection in this query 

If you want to go into more detail, however, Oracle provides far more metrics. If you query , you'll see hundreds of statistics that are available. contains the current value of every statistic for your session so you can grab the "before" data, run a query, then grab the "after" data and look at the change in virtually any performance statistic you might be interested in. Tom Kyte put together a nice runstats package that automates this so that you don't have to write the test harness yourself. 

Assuming you are licensed to use the AWR, you could query either the view for more recent data or for older data. will have a snapshot of what every active session was doing at the top of each second and will hold that data for a few hours. will have the data from, essentially, every 10th snapshot in but it will be retained for much longer (whatever your AWR retention period is). Both of these views will have a , , and that will tell you which session was holding the lock that the blocked session was waiting on. You can then look too see what queries the blocking session was running at around the same time-- the blocking session may well not be active in the particular snapshot you're looking at but it probably was in the snapshot a second or two earlier. 

(I tried a couple of different actions and it doesn't seem to relate to which actions are included-- but maybe it's based on a total character count of action names?) Full list of actions I was working with: 

Microsoft's NEWID() documentation does not explain, but I agree that the likely reason is related @Payload's comment on another answer: 

We never found out what was causing the service to stop, but we were able to prevent it from happening. We put a Deny on the Stop permission for SSAS service for the SQL Server (not SSAS) service account user. So it seems something acting as the SQL Server's service account user was issuing the Stop command for the SSAS account. This was a while ago, and we've since switched servers, and we stopped digging to figure out what was really happening. So it will remain a mystery for the ages. 

The error doesn't occur for some time, which is usually the result of the client being unable to reach the destination server (e.g. an incorrect database server name). The error occurs in the web application and when testing an ODBC connection using the Windows Server 2003 Web Edition OS tools. In order to try to determine what is causing the issue, we want to see whether the connection attempt is reaching the database server. We thought we could use extended events to monitor the login attempt. is a new event in SQL Server 2016 which "is generated when server is done processing a login (success or failure)". We created an event session with this code: 

We have a SQL Server 2005 Analysis Server which began restarting this weekend, seemingly without any external changes. The current state of the server is that as long as the SSAS service is enabled, it will keep restarting. If the service is manually disabled and then stopped, the service will no longer cycle. But as soon as it is enabled, the service will begin to cycle again. Internet searching finds a few examples of this behavior, but without a resolution which seems applicable. See, e.g.: Service Level Accounts - SQL 2005 SSAS - Error connecting to OLAP. See also, e.g.: I Can’t Connect to Analysis Services in SSMS... Brent Ozar reported a similar problem on LessThanDot, but there was no resolution on the thread and the last entry was at Fri Mar 27, 2009 4:08 pm. This is a snippet of the msmdsrv.log file: 

Why does Postgres need to do a heap scan? Isn't all the information needed for the query in the composite index? Readable version of the output: $URL$ 

The and and fields should be unique together, but I haven't enforced that anywhere. So my question is, how can I sensibly create a unique index so that I can do concurrent refreshes? I guess I could add a new integer column and make that unique, then index it - would that be the most sensible approach? 

There are about 450m rows in the table. I have just run , so the query planner should be up to date. Here is my query: 

Apologies for the newbie question! I am working with a largeish dataset (500m rows, 150GB of data) and Postgres 9.1 running on Debian. I have a Python import script for importing the source data, which is in the form of multiple CSV files. The script does the following: 

I am using Postgres 9.4. I have created a materialized view from one of my tables and I want to be able to refresh it concurrently. I know from the documentation that in order to do a concurrent refresh, I need to have a unique index on the materialized view. But currently I have no unique fields in the view. This is the current structure of the materialized view: 

in the Postgres config file, but I haven't touched any of the other defaults. With the settings above, I'm finding it very slow (more than an hour and still running) to create an index on one field in my table, and I want to create many of them. Are there other settings I could look at to make index creation faster? Or can I safely increase these settings? 

I have just installed Postgres 9.1 on a server with 32GB of RAM and 320GB solid-state disk. I will be using this server to serve both my site and my database (i.e. it's not just a database server). The database is 500m rows, approximately 120GB of data. Once the data is loaded, the database needs to be optimised for SELECT queries - the data is fairly static in nature, and only changes once every three months. 99% of the queries made to the database will be reads. I'm wondering what would be good Postgres settings to start out with. I've set: 

Currently all this is in a Python script, using psycopg2. This does the job. However, it's very important that this script loads the data in full, without any errors, every time I use it. So I would like to check what I can do to make this script more robust. I'm already handling errors, but I was wondering about validating the data once loaded to make sure that it looks how I expect. I don't know if this is a standard or sensible thing to do, but given that I know how many entries there are in each CSV file (using ) I could run queries afterwards to check that the number of rows is as expected. Here is the full script, anyway - suggestions for improvements very welcome. 

The error you're getting indicates that the User1 user does not have access to the package. You'd need a DBA to 

If you are using the enterprise edition of the database, it sounds like you could use either Streams or materialized views to replicate the data. Streams works by asynchronously mining the redo data that is generated on the source system to find changes to the tables in question which then get delivered to the destination database using Oracle's Advanced Queues (AQ). Streams is generally the preferred architecture-- it is the newer technology, it generates less load on the source system, it doesn't involve creating any new objects in the source system and has lower latency. The downside to Streams is that there can be a decent amount of administrative work getting Streams set up and working depending on the Oracle version(s) involved. Materialized views are an older replication technology that would also seemingly meet your needs. Materialized views work can also replicate changes but they have to do so by recording the change data in materialized view logs that are created on the tables that you want to replicate. On the source system, you'd need to create materialized view logs on the tables you want to replicate-- those logs would be automatically written to when changes are made to the table but those writes will be synchronous with the transaction so it may affect and performance. On the destination side, you would then create a materialized view that did an incremental refresh on a set schedule (i.e. every few minutes) that would read the data from the materialized view logs, pull the data over the network, and update the materialized view's copy of the data. Materialized views tend to be easier to set up but they're not as efficient and the requirement of creating materialized view logs in the source database can be a point of contention. Either solution is relatively easy to shut down in 2 months when you no longer need the replication. 

should be valid syntax. Whether it is actually appropriate to create a table for this sort of thing is a question you'd have to answer. My bias would be that it would be more appropriate to create a view or potentially a materialized view but I don't know the exact nature of the problem you're trying to solve. 

No, it's not possible as a theoretical matter since, as you say, the order in which rows are returned is arbitrary without an ORDER BY. There is no guarantee that the order will be stable over time. No, it's not generally possible as a practical matter. However, there may be a couple of workarounds that you could employ. You could rename the table to , create a view on top of the table named , and embed an ORDER BY in that view that returns the data in whatever order you want. If the view option doesn't work (and it may be problematic if you've got a lot of data) and you're really, really, really in need a hack, you could create a new table, insert FOO (and all other rows that are before the row you want to add), insert the new row, and then insert the remaining rows. Then drop the old table and rename the new table to use the old table name. No guarantees, of course, but that would generally cause an ORDER BY-less to return the data in the order you wanted. Terribly ugly if you have a reasonable amount of data in the table.