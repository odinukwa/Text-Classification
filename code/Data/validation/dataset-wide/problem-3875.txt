The Wikipedia page has a counterexample: A continuous convex function for which coordinate descent fails to converge but getting stuck in a non-optimal point. Here are the level lines of this function: By Nicoguaro - Own work, CC BY 4.0, Link 

One dual problem is already stated on page one. If you are looking for a "Kantorovich-Rubinstein" dual problem with Lipschitz-functions: I am not sure if such a thing exists - one uses the metric in an essential ways and I don't see anything that replaces this here… 

$\newcommand{\Ag}{\mathcal{A}_\gamma}$ Not a full answer, put probably a fruitful pointer: In the discretization of infinite dimensional problems one faces (bi-)infinite matrices. There, the Jaffard algebra $\Ag$ of bi-infinte matrices (for some $\gamma>1$) is $$ \Ag = \{A \ : \ |a_{k,l}|\lesssim (1+ |k-l|)^\gamma\}. $$ It can be shown that the Jaffard algebra is inverse closed, i.e. if $A\in \Ag$ is invertible, then $A^{-1}\in\Ag$ ("Optimal Adaptive Computations in the Jaffard Algebra and Localized Frames" by Dahlke et al. gives Gröchenig's "Localization of frames, Banach frames, and the invertibility of the frame operator" as a reference). In other words: a certain "off-diagonal decay" is preserved under inversion. 

Since probability is quite far away from my daily buisiness, please forgive me if my use of terminology is wrong or the question is too trivial. However, I was not able to find the right keyword to find an answer by googling... I am even not sure if "random walk" is the right name for what I am going to describe. Consider a particle which is moving around randomly in $\mathbb{R}^2$ in steps such that in every step its movement is desribed by a draw of a 2D Gaussian distribution with variance $\sigma$. In other words: From position $x_k$ at time $k$ it moves to position $x_{k+1} = x_k + d_k$ where $d_k$ is normally distributed with variance $\sigma$. If the particle starts at time $0$ at $0$, then the distribution of its position at time $N$ is Gaussian with variance $\sqrt{N}\sigma$, since this is just the addition of $N$ Gaussian random variables which amounts to the $N$-fold convolution of the Gaussian with variance $\sigma$. Am I right on this? But my question is this: What is the distribution of $\max\{\|x_j - x_k\|\ |\ 1\leq j,k\leq N\}$ and how to you calculate it? Finally: What is the answer to the same question if unit steps in random directions are taken, i.e. $d_k$ is uniformly distributed on the unit circle? Pointers to literature are also appreciated. 

The former uses continuous and Hermitian bilinear forms $Q:X\times X\to\mathbb{R}$ and calls an operator $A$ on $X$ $Q$-self-adjoint if $Q(Ax,y) = Q(x,Ay)$. In a similar spirit I could imagine that the following construction could make sense:If you have a uniformly smooth Banach space $X$ you could use the duality mapping $J:X\to X^*$ defined by $J(x) = \{x^*\in X^*\ :\ \langle x^*,x\rangle_{X^*\times X}=\|x\|^2=\|x^*\|^2\}$ (which is single valued and uniformly continuous in this case) to define $Q(x,y) = \langle J(x),y\rangle_{X^*\times X}$ (however, not bilinear). Then it seems natural to require that $Q(Ax,y) = Q(x,Ay)$ (however, I haven't seen this construction used anywhere). 

There is not enough information for a thorough answer. An a priori bound on the solution may indeed help theoretically and practically. As usual with measured data you may not want to solve the equation $Af=g$ (where $A$ denotes the integral operator, i.e $Af(t) = \int k(t,s) f(s) ds$) but a "least squares" type problem, i. e. $$\min_f \|Af-g\|. $$ You shall expect that minimizers do not exist (lack of coercivity). An a priori bound $0\leq f\leq 1$ can help as, depending on the function spaces, the problem $$\min_{0\leq f\leq 1} \|Af-g\|$$ may have minimizers. From a practical point of view: Solving the constrained problem may indeed be easier but this depends on the specific situation. As a first try you could give the projected gradient method a shot. Assuming that you work with the $L^2$ norm, this amounts to the iteration $$ f^{n+1} = P_{[0,1]}(f^n - \tau A^*(Af^n-g)) $$ with the adjoint $A^*$, a stepsize $0<\tau<2/\|A\|^2$ and the projection $P_{[0,1]} $ onto the interval, i. e. you apply pointwise $$ x\mapsto \min(\max(x, 0),1). $$ 

I've heard that regularity theory for $p$-Laplacian equations, i.e. equations of the form $$ \mathrm{div}(c_1|\nabla u|^{p-2}\nabla u) + c_2 |u|^{p-2} = f $$ with $c_1,c_2 > 0$, $p>2$ need Besov spaces to describe the maximal regularity. See Jacques Simon, Régularité de la solution d'une équation non linéaire dans $\mathbb{R}^N$. 

I don't know what $e$ is, but if you need a subgradient of a maximum of convex functions, you can take a subgradient of one of the functions where the maximum is achieved, e.g. if $$ p(x) = \max_i f_i(x) $$ and $p(x) = f_{i_0}(x)$ with $g\in\partial f_{i_0}(x)$, then $g\in\partial p(x)$. 

I did not check how the optimality conditions arising from the respective Lagrangian $$ L(T,\Lambda) = \|AT-TB\|_2 + \langle \Lambda,T^T T - I\rangle $$ look like, though… Note that the Frobenius norm is also unitarily invariant, so you can do the same in this case and minimize $\|AT-TB\|_F^2$ instead, which is also a convex function in $T$. While these reformulations are equivalent, it does not mean that the objective functions are the same: they only coincide on the constraint set. $\newcommand{\tr}{\operatorname{tr}}$ Here is the optimality system in the case of the Frobenius norm: We use the formulation $$ \min_T \tfrac12\|AT-TB\|_F^2\ \text{ s.t. }\ T^TT-I=0 $$ and form the Lagrangian (using Igor's derivation) $$ L(T,\Lambda) = \tfrac12\|A\|_F^2 + \tfrac12\|B\|_F^2 - \tr(ATB^T T^T) - \tr(\Lambda^T(T^T T-I)). $$ Using the identities $$ \partial_X(\tr(AXBX^TC)) = A^TC^TXB + CAXB $$ and $$ \partial_X(\tr(XX^TB)) = \partial_X(\tr(X^TBX)) = BX+B^TX $$ from the Matrix Cookbook we get the optimality conditions $$ \begin{split} \partial_\Lambda L & = T^T T-I = 0\\ \partial_T L & = -2(A^T TB^T + ATB) + \Lambda T + \Lambda^T T = 0. \end{split} $$ Any idea, how to solve this? 

Since my intuition for high dimensional geometry is not always right: Consider the unit cube in $\mathbb{R}^m$ and for $n\leq m$ denote by $F^n$ the union of the $n$-facets. For what numbers of $m$ and $n$ does any $n$-dimensional subspace of $\mathbb{R}^m$ intersect $F^n$? Extra: Consider the same question for $G^n$ = $F^n\setminus F^{n-1}$. (P.S: What kind geometry tag would be appropriate?) 

That space should be $L_{\exp}$. Check Bennett and Sharpley's "Interpolation of Operators" Chapter 4.6 or look in Rao and Ren's "Theory of Orlicz Spaces". 

Edit: Changed from "Hausdorff" to "metric" spaces. Let $\mathcal{M}(\Omega)$ denote the space of signed regular Borel measures on a compact metric space $\Omega$. By Riesz-Markov, this is the dual space of $C(\Omega)$, the space of all continuous real valued functions on $\Omega$. Denote by $$\mathcal{P}(\Omega) = \{\mu\in\mathcal{M}(\Omega)\ :\ \mu\geq 0, \mu(\Omega)=1\}$$ i.e. the set of all probability measures in $\mathcal{M}$. The weak convergence (also called weak* convergence) in $\mathcal{M}(\Omega)$ is defined by duality and it is known that weak convergence in $\mathcal{P}(\Omega)$ can be metrizised by, e.g. the Prokhorov metric $d_P$ or the Wasserstein metrics $d_W$. Obviously, both metrics do not metrizise weak convergence on $\mathcal{M}(\Omega)$: For the Wasserstein metric we have $d_W(\mu,\nu)=\infty$ if $\mu(\Omega)\neq\nu(\Omega)$ and for the Prokhorov metric we do not even have $d_P(\mu,\mu)=0$, as far as I see. Googling and searching MSC did not produce any results on my question: 

I don't know who deserves credit for this, but I was stunned by the concept of view complicated objects like functions simply as points in a vector space. With that view one solves and analyzes PDEs or integral equations in Lebesgue or Sobolev spaces. 

I would also love to have some examples of the usefulness of the measures different from the Hausdorff measure (which appear to be handy for the analysis of self-similar sets). Hence, a third question is 

I think the claim is true under some additional assumption: Let $\bar x$ be another solution of $$\min_x f(x) ~~ s.t.\\ g_{t^*}(x)\leq0 ~ $$ for which some constraints are not fulfilled, i.e. $g_{\bar t}(\bar x) >0$ for $\bar t\in T$ with some set $T$. Let's try this additional assumption: 

A lot of things are known for the convergence of alternating projections for these convex feasibility problems. I suggest to start with 

I offer another point of view from the angle of the companion matrix of the polynomial. It may also give a very vague intuition about the observed uniformity of the distribution along the unit circle. Consider the Jordan block $$J = \begin{bmatrix} 0 & 1 & & & \\ & 0 & 1 & &\\ & & &\ddots & 1\\ & & & & 0 \end{bmatrix}$$ which is the companion matrix of the zero-polynomial and has all its eigenvalues equal to zero. Now consider a slight perturbation of this matrix in a single component in the last row, i.e. $$J_\delta = \begin{bmatrix} 0 & 1 & & & \\ & 0 & 1 & &\\ & & &\ddots & 1\\ & & \delta & & 0 \end{bmatrix}.$$ If this $\delta$ sits in $k$th entry of the last row, the eigenvalues of $J_\delta$ are the solutions of $$ z^{k-1}(z^{N-k+1} - \delta) = 0 $$ i.e. we have $k-1$ eigenvalues equal to zero and the others are the $(N-k+1)$th roots of unity times $\delta^{1/(N-k+1)}$. That means that a small perturbation in the last row throws a lot of eigenvalues from the origin towards the unit circle and the more left the perturbation is, the more eigenvalues leave the origin and also they move closer to the unit circle. In the extreme case of $k=1$ we have no eigenvalue zero anymore but all $N$th roots of unity times $\sqrt[N]{\delta}$ as eigenvalues. Roughly, one may say that all perturbations in the last row tend to spread a number of eigenvalues equally distributed around a scaled unit circle. Moreover, the lower left corner is the most sensitive position. Now it gets more shaky, but if we consider a perturbation in every entry of the last row and each is comparable in size, the perturbation in the lower left corner has the largest effect… (Inspired from a talk by Sjøstrand on distributions of eigenvalues of small random perturbations of large Jordan blocks, see also the book "Spectra and Pseudospectra" by Trefethen and Embree.) 

I think the relation is not that simple and here is why: Since $A_1 = Q_1R_1$ we have $$ A_2 = \begin{bmatrix}A_1\\ I\end{bmatrix} = \begin{bmatrix}Q_1 & 0\\0 & I\end{bmatrix}\begin{bmatrix}R_1\\I\end{bmatrix}. $$ Now the QR decomposition of $\begin{bmatrix}R_1\\I\end{bmatrix}$ can be done by $n(n-1)/2$ Givens rotations and I don't see any reason, why this should lead to a simple relation of $R_1$ and $R_2$. 

Not a full answer, but a question like this stated and answered for $K=]-\infty,\infty[$ in "A class of nonharmonic Fourier series", R. J. Duffin and A. C. Schaeffer, Trans. Amer. Math. Soc. 72 (1952), 341-366 initiated the theory of frames. Your question is answered for $K = [-A,A]$ in "An Introduction to Nonharmonic Fourier Series" by Robert M. Young. 

Sorry, not an answer, but too long for a comment. If your question is motivated by practical applications of the Radon transform such as computerized tomography in medical imaging or non-destructive testing in industry, then I guess that your question will not be regarded as "well posed" in these communities. For one, the statement "It is clear, that the number of equations must at least equal the number of cells." is not totally true in the applied area. There you have a lot of prior knowledge on the function $f$ which should be exploited, such as non-negativity, a good estimate of the integral (or 1-norm) of the function, and probably further assumptions such as smoothness, piecewise smoothness, regularity of the jump set and/or sparsity assumptions. All these prior knowledge makes it possible to obtain good or reasonable reconstruction of the objects under consideration even from underdetermined measurements. Another problem is that of "mathematical ill-posedness". The inversion of the Radon transform is ill-posed in the sense that the forward operator $R$ (i.e. the Radon transform) is smoothing (roughly increases the Sobolev smoothness by 1/2). Hence, the operator $R$ does not have a continuous inverse. In consequence, there will be problems with "approximating the continuous Radon transform finer and finer" since you are approximating an infinite matrix with infinite condition number. The condition number of you approximating problems will grow unbounded. It is natural in this setting to use "regularization by discretization", that is, to balance accuracy of the approximation of the operator with computational stability/noise amplification. Pointer to literature for this view on tomography are, for example, 

There is the theory of "optimal experimental design" or "optimal design of experiments". The main question there is "if I want to measure a certain quantity, how should do it in an optimal way?" "Optimal" can mean many different things here, e.g. that some estimator of the quantity has minimal variance (and this is still not precise, as "variance" is a matrix valued quantity in the multivariate case). One pointer to the literature is the book by Pukelsheim (if I remember correctly, the book is called "Optimal design of experiments"). 

I suggest to look at the Gromov-Wasserstein distances (defined between two metric measure spaces). A great start is the paper "Gromov-Wasserstein distances and the metric approach to object matching" ( $URL$ that also deals with some computational issues. As introduced in the paper, the approach does not incorporate weights on the edges yet (as far as I remember)... 

It is known that there are non-Hausdorff spaces which admit unique limits for all convergent sequence (see here) and it is also known that unique limits for nets implies Hausdorff. What I am wondering is, if there is a (somehow weak) condition which one should add to "unique limits of sequences" to obtain a Hausdorff space. Would, for example, some countability help? Somehow in the same direction: What is the central property which is needed for a space such that it can be non-Hausdorff but has unique sequence limits? Is there a whole class of non-Hausdorff spaces which admit unique limits for convergent sequence? 

Edit: Here is more fleshed out version of the proof. The $h_i$ should "spread out" the digits as follows: Let us denote the inputs by $x^1,\dots,x^n$ and their digits by $$ \begin{split} x^1 & = 0.x^1_1 x^1_2\dots\\ x^2 & = 0.x^2_1 x^2_2\dots\\ \vdots &\\ x^n & = x^n_1 x^n_2\dots \end{split}. $$ Then the $h_i$ work as follows $$ \begin{split} h_1(x^1) & = 0.x^1_1 0 \dots 0 x^1_2 0\dots 0 x^1_3 0\dots\\ h_2(x^2) & = 0.0 x^2_1 0\dots 0 x^2_2 0 \dots 0 x^2_3 0\dots\\ h_3(x^3) & = 0.0 0 x^3_1 0\dots 0 x^3_2 0 \dots 0 x^3_3 0\dots\\ \end{split} $$ with $n-1$ zeros between digits. Then $$ \sum_{i=1}^n h_i(x^i) = 0.x^1_1x^2_1\dots x^n_1 x^1_2 x^2_2\dots x^n_2\dots $$ i.e., this number contains all the digits of all the numbers $x^1,\dots,x^n$. Now define $g$ of this number as $f(x_1,\dots,x_2)$. Since the "interlacing map" $(x^1,\dots,x^n) \mapsto \sum_i h_i(x^i)$ is bijective, such a $g$ exists. 

A bit too long for a comment: Let's consider a optimization problem of the form $$ \min_x f(x)\quad \text{s.t.}\quad x\in C. $$ If we now consider "symmetry" a bit more abstract by saying that you have a group $G$ acting on the set $C$ such that the objective is invariant under the group action, i.e. for $g\in G$ we have $f(gx) = f(x)$ then you see that the set of minimizers is also invariant under the group action. 

I think any answer should not be mathematical - as there should not be a mathematical answer to "Why do we need sets/functions/numbers…?". My take is, that random variables are just there. There is no need to need them… Let's get no too philosophical, but there are things in nature that just appear random or at least with a random component.Some examples, like rolling a dice, do not really need random variables to describe them, because one can really model the whole experiment and talk about events and such. But other examples are not like that: The temperature next Sunday is not totally random, but surely nobody can predict it from now, so let's assume that it is a random number. If there is an underlying probability space is not really important, since all interesting properties of this random number should be independent of it, e.g. that the temperature is above (or below) some value. In other words: the distribution of the random number really matters. So to me it seems, that random variables are just the right way to think about random numbers. 

The optimization problem for $f_3$ is $$ \min_g \int_{-1}^1 |g''(x)|^2dx \quad \text{s.t.}\quad \int_{-1}^1 g(x)dx = 0,\ \int_{-1}^1 x g(x)dx = 0,\ \int_{-1}^1 |g(x)|^2dx = 1. $$ Using Lagrange Multipliers to solve this optimization problem you form the Lagrangian $$ \mathcal{L}(g,\lambda_1,\lambda_2,\mu) = \int_{-1}^1 |g''(x)|^2dx + \lambda_1 \int_{-1}^1 g(x)dx + \lambda_2\int_{-1}^1 xg(x)dx + \mu\left(\int_{-1}^1|g(x)|^2dx-1\right). $$ Now derive $\mathcal{L}$ with respect to all variables and set them to zero to get the necessary (and, due to convexity of the problem, also sufficient) conditions for optimality. The derivatives for $\lambda_1,\lambda_2$ and $\mu$ just give the constraints and for the derivative for the "function variable" just do a formal calculation to calculate the directional derivative $$ \lim_{t\to 0} \frac{\mathcal{L}(g+th,\dots) - \mathcal{L}(g,\dots)}{t}. $$ Using integration by parts this shows that $f_3$ has to solve the boundary value problem $$ g^{(4)}(x) = -\lambda_1 - \lambda_2 x - 2\mu g(x),\quad g^{(2)}(\pm 1) = 0,\quad g^{(3)}(\pm 1) = 0 $$ for some Lagrange multiplies $\lambda_{1/2},\mu$ and also fulfill the additional constraints. There is a closed form solution for this problem, but I did not push the calculation through… Proceeding similarly gives $f_k$ as solution of $$ g^{(4)}(x) = -\lambda_1 - \lambda_2 x - \lambda_3 f_3(x) - \cdots - \lambda_{k-1}f_{k-1}(x) - 2\mu g(x),\quad g^{(2)}(\pm 1) = 0,\quad g^{(3)}(\pm 1) = 0 $$ for some Lagrange multiplies $\lambda_{l},\mu$, $l=1,\dots,k-1$. While it seem not too hard to write down $f_3$ explicitly, $f_4$ seems to be quite a mess.