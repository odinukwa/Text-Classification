(Modern Eastern) Armenian has only periphrastic present tense, for example tesa I saw vs. tesnum em I see. In the present tense there are more morphemes - the ending of the participle and the clitical auxiliary. There is a synthetic future tense, too, but it has an additional morpheme (ktesnem I will see). 

This construction comes from Latin in the language families you mention but it's not unique to Germanic and Romance languages. This kind of perfect is fully grammaticalized in some Macedonian dialects (Macedonian is a Slavic language), e.g. ima bideno "has been", ima imano "has had", etc. There's also a similar construction in Northwest Russian which expresses possession with a PP and has an analogous perfect tense, cf. u menja dom "I have a house" vs. u menja vse sdelano "I have done everything". In the Lithuanian (i.e., Baltic) dialects of Belarus there's an identical construction: Possession is expressed with the adessive case and so is the agent in a structurally identical perfect. Since neither Russian nor Lithuanian were influenced by Latin there seems to be a general tendency to this kind of grammaticalization as far as the perfect tense is concerned, the other being the be-perfect (e.g., ich bin gegangen "I went" in German). In fact it occurs in many languages but in most of them it's not fully grammaticalized. 

You might want to have a look at LFG, they use X' Theory extended with an additional "lexocentric" category S to accommodate nonconfigurational phrase structures. 

The Khmer language isn't tonal, but it's surrounded by tonal languages although not all of them belong to one family (but they form a sprachbund). In a more general context, there are many languages that have lost phonemic vowel length, which might be related to the loss of tones as an internal process. 

Dependency parsing with ID/LP rules is trivial if you have a lexicon. One uses a (declarative) generate-and-test approach. Dependency trees are rooted spanning trees on a graph with n nodes where n is the length of the processed sentence. One can declare what a tree is and - now comes the linguistic part - where an edge (that is, a dependency) may be. The result are all spanning trees composed of the possible dependencies. Thus dependency parsing is constraint solving. A naïve implementation of the algorithm would have a terrible time complexity. On a complete graph, there are n^(n-2) trees, whence there are n^(n-1) rooted trees. The only viable way is to use the constraints to compute stable models of the underlying theory (there is a one-to-one correspondence between stable models and dependency trees). On Linux one can use clingo (an AI tool downloadable from package repositories). On OS X, the tool can be installed via MacPorts. There are a few issues. For languages with case, one needs additional constraints. In Latin, for example, "Metellam vidit" would be parsed using the rule n<-v, that is, a noun can depend on a verb, but the case of the noun decides what role the noun plays in the verb's predicate (in this example, accusative implies direct object). In a sentence like "I gave Mary the book", word order is what decides that Mary is the indirect object in this sentence. Thus LP rules are needed. One could analyze both objects at once (with one rule) but this runs counter to the idea of dependency grammar as it was originally conceived. The computation of stable models is generally very fast but it may take some time when the sentence is long and complex. One could add constraints that enforce projectivity. Most sentences are projective, that is, one can speed up the parser by considering only projective sentences at the first try (no sane theory of dependency syntax would assign nonprojective trees to unmarked sentences). Actually, projectivity is a scale, one can thus try to infer a projective tree, then a planar one, then a well-nested one, etc. But this is only an optimization and no difference would be observable for short sentences. By adding feature structure unification to the rules it is possible to create deep syntax trees (which are better for further processing such as machine translation or discourse interpretation). For example, auxiliaries are only tense and agreement carriers, they contribute features to the morphosytactic description of a verb phrase, but they have no meaning of their own and are useless for further processing. On the other hand, one needs to add semantic content (the subject) in a sentence like "veni, vidi, vici". Thus the sketched algorithm builds up surface trees, deep trees, and logical forms in parallel. I can provide source code with a toy grammar. 

I'm afraid the analysis depends on the theory of grammar you're using. In most CF-based formalisms, "continued" would be an I that takes a VP ("to protest") as its complement. In this case, however, the sentence seems to contain a complex predicate (or clause union). Lexicalist theories assume that both verbs are co-heads in a complex (composite) predication. In any case, one would represent the sentence as 

Thus in glue semantic the meaning of both "John loves Mary" and "Mary John loves" can be assembled using the same rule though in the latter sentence it's the subject what is "attached" first. However higher-order logic is unwieldy for reasoning as it's more complex and less understood that first-order logic. Davidson, Parsons, Hobbs, and Pietroski (to name just a few) argue that logical forms should be conjunctions of positive literals. On this view, the meaning of "John loves Mary" is the existential closure of 

It depends on the framework. Within LFG, parsing is NP-compete in the worst case. However, Ron Kaplan argues that NL parsing is polynomial in the average case. In general, NL parsing is NP-complete because of the constraints associated with rules. 

Abkhaz is ergative and caseless. Verbs have slots that express their arguments by prefixes. An example of an intransitive verbs is с-цоит "I go" (the prefix in the first slot signals the subject's person, gender, and number). Then there are bipersonal intransitive verbs, such as с-у-суеит "I hit you". The first slot signals the subject and the second slot the indirect object. Finally, there are transitive verbs. In и-∅-з-боит "I see it" what is expressed in the first slot is the direct object. The subject is expressed in the third slot. NPs are optionally adjoined to the verb and agree with it in person, gender, and number, thus allowing for free word order. Circassian, another Northwest Caucasian language, has the same verb structure but since it has morphological case, the verb agrees with its arguments in person, case, gender, and number. 

You could count the nonprojective dependencies in a sentence. This is actually a very nice definition of long-distance (unbounded) dependencies. The degree of nonprojectivity also gives you a measure of how marked (“weird”) the construction is (aside from its length which would need to be normalised to be statistically meaningful). 

MT is hard. Google Translate is based on statistical methods with models trained on large bilingual corpora. There are a few rule-based systems that produce better translations but only in closed specialized domains. As for now nobody has an algorithm or method that performs better. It's impossible to predict the future but I wouldn't hope for significantly better translations in the next years. 

Note that "chain" isn't a precise term because it suggests something one-dimensional whereas the definition allows general connected subgraphs (and there are examples of "chains" that are nontrivial (sub)trees, i.e., not simple paths). Note that chains DON'T work at the level of surface syntax. O'Grady defines them explicitly at the lexical level and gives examples why they can't by applied at the syntactic level (as they'd be discontinuous). At the lexical level (Alsina's argument structure) they make perfect sense. 

Polysynthetic languages do almost all the work in the lexicon so a better question would be how they deal with morphology. There's a formal grammar for Georgian and Abkhaz developed by Paul Meurer. He has a lexicon and a nonconfigurational grammar for the NP adjuncts. It gets more complicated in languages like Greenlandic or Quechua because they can built infinite words (not via recursion though) There's something like a sublexical "microgrammar" that creates "trees" in the lexicon. In other words, syntax is not the problem here, unless the language exhibits incorporation, in which case one has to deal with bracketing paradoxes. I generally like the lexical integrity principle but I think that it simply doesn't work for these languages. This opinion is controversial, though. Quechua, on the other hand, has differential agreement, which means that (lexical) semantics affects morphosyntax. One can vaguely describe the rules but it's very hard to formalize them. As for pro-drop, the "dropped" arguments are attached in the lexicon and appear at the level of deep syntax, where one can use them to create the logical representation of the processed sentence. But this is not syntax anymore. 

You could store all tokens in a relational database in a separate table and build up a positional index (sacrificing space for speed of query processing). Then you could use simple SQL expressions to formulate all types of queries you've listed. 

I'd recommend looking at Jerry Hobbs' way of parsing and representing English, it's one of the most elaborate ones when it comes to logic and commonsense representation: Jerry Hobbs: "Discourse and Inference" 

Yes, they are adjuncts, they are just topicalized. Alternative word order is orthogonal to what categories phrases have. 

It's not strange. As for Polish, one could say "ktoś" (kto-ś - who-some), but there's frequent use of "jeden" (one) and "człowiek" (man) in similar contexts. I think it's just a general grammaticalization tendency. 

Purely statistical MT systems don't use parse trees. However most of today's MT systems use hybrid architectures, i.e. there's at least a shallow syntax module or something alike. Parse trees can't help with semantic ambiguity. Whatever depends on context is dealt with later in the phase called (pragmatic) interpretation. Syntax provides some hints but what's decisive are semantic rules and constraints. Correct disambiguation always depends on context and common knowledge. 

Many modern European languages are as complex as Latin, Ancient Greek, or Sanskrit. I'd point out Lithuanian but most Slavic languages are typologically similar to the mentioned ancient ones. And yes, native speakers use all constructions their language provides (all languages change, of course, so there are archaic constructions but it has nothing to the with complexity). BTW no human language is primitive, English has simpler morphology than Latin but it's more complex elsewhere. 

They are not considered one lexeme. The aspect is just an inherent feature of Polish (Slavic) verbs. In Old Polish, the situation was more complicated as most verbs, e.g., wiedzieć, had two simple past tense forms: aorist (wiedziech - perfective) and imperfect (wiedzieach - imperfective). This is how the aspect emerged in Slavic languages but the system was later reorganized, prefixes began to be used to express aspect etc. 

Abkhaz, a northwest Caucasian language, has no cases but it is an ergative language. The alignment can be recovered from head-marking (on verbs). 

There are quite a few options. I'd recommend either LFG or a parser for Universal Dependencies. Syntactic dependencies (and null arguments) can be directly extracted from both of them. 

You mix up two notions, subcategorisation and genitive objects. In the case of "need" etc. taking a genitive complement, it's just convention. In Slavic the genitive used to serve as a partitive so some verbs that frequently use partitive objects simply spread the use of the genitive to all objects. The fact that in some Slavic languages the direct object always is in the genitive case when the verb is negated has a similar reason--in the case of negation partitive or other reinforcing particles were often used, but it's a different phenomenon. You can see how the genitive works especially well in Russian where direct objects use the genitive or accusative depending on definiteness. BTW in which Slavic languages does the verb "see" take a genitive complement (if not negated)? 

The question makes sense. Auxiliary verbs aren't necessary as there are languages that don't have them. Likewise there are languages without prepositions. Those languages use morphology to express what prepositions express in English (and many other languages). Many Amerindian languages don't have Aux and P. An example: "Juma-t jupa-tak alarapïwa" means "I will buy it from you for him/her" (the suffixes -t and -tak mark ablative and benefactive, respectively, and tense is expressed by the verb ending). 

The answer is no if you mean morphological case. Word order in English is a method of signalling grammatical functions. But there's the notion of "abstract case" and then one might say that word order correlates with abstract case assignment (in the parlance of generative grammar). There's a more abstract notion of "configuration" and in the so-called configurational languages, configurations (that is, hierarchical word order) determine grammatical functions, such as subject, object, etc. In nonconfigurational languages such as Latin, word order plays no role in syntax sensu stricto (though it is important at the level of pragmatics since it is a means of expressing information structure). I don't think your 'X' has a name in linguistics but both word order and (morphological) case are used to express grammatical roles. 

The corresponding syntax trees are identical (up to node/edge labels). Complex predicates present a problem to unification grammars. Alsina proposed a solution (in his paper on causatives in Romance and Bantu 1997, page 236) but it's only a technical workaround that "breaks" the monotonicity of unification. We could add semantic forms (SF) to tree nodes in any dependency grammar (faig in (1) would have cause(subj,*) and the main verb would have, in this case, llegir(x,y)) but they can't be unified so it's a useless formalization. As to the second question, I think that most complex predicates are "chains" as defined by O'Grady but there are counterexamples (CPs with "vertical" gaps/discontinuities). But CPs aren't idioms so it shouldn't be particularly surprising. (There are quite a few papers by Butt on Urdu, which is heavy on CPs, with LFG analyses - BTW LFG has "chains" since its inception in 1982, they're just called differently.) Update: 

I don't have detailed knowledge of Japanese but in Urdu there are two approaches. One says that the case markers are suffixes, whilst the other assumes that they're particles and posits a KP. A KP in Urdu is something like a PP but it's headed by a case marker (K). In fact, there's a hierarchy: PP>KP>DP>NP. Note that the categorial head of a phrase needn't be its functional (lexical) head. Functional heads are content words. 

They are inflected. It wouldn't make any sense to treat them as separate lexemes. In many languages, they also inflect for gender and/or case. In some languages, even the deixis is considered an inflectional category, e.g. in Macedonian: kuka-va "this house" vs. kuka-na "that house". 

They are similar but not mutually intelligible. Polish is west Slavic whereas Slovenian is south Slavic. The difference is maybe like that between Spanish and Italian. 

Intensions are functions from contexts to extensions. The sentence contains two indexicals - I and now. What the extension is depends on the theory underlying the interpretation. On Frege's analysis, it's the truth value. In non-Fregean logic, it's a "situation", or "eventuality". 

Yes, of course. In German, for example, the verb "helfen" (to help) is intransitive: "ich habe ihm geholfen" (I helped him). It takes an indirect object (dative) that can't be passivized. Turkish has many verbs with so-called noncanonical (indirect) objects where one would expect a direct object. In general, just get a valency lexicon of any language and look for frames of intransitive verbs, you'll find many slots for indirect objects. 

What you are talking about are modes of existence, which classify propositions (or more precisely, clauses). From a semantic perspective, eventualities (possible situations) have different modes of existence, some of them being evidential. It has nothing to do with presupposition and only marginally touches entailment (in the case of positive modalities). 

The exact answer depends on the approach to natural language semantics you use. I've used Jerry Hobbs' "Ontological Promiscuity" (it's an approach to pragmatics and the title of a paper) where a semantic representation is predicative (one could say "has propositional status" because all formulae in the framework are closed) whenever an eventuality is marked as existing or nonexistig (Hobbs' Rexist predicate). In a simple affirmative sentence it would be the VP but note that typically there are more predications in a sentence that depend on the discourse context (information structure). The topic implies existence (Rexist) so in a sentence like "The king of France is bald" there are two predications (the other is that fact that France has a king, i.e. a king of France exists; the sentence has no meaning if uttered today). Hobbs uses a version of HPSG, his "The syntax of English in a abductive framework" implicitly describes the relationship between syntactic structures and their logical representaiton.