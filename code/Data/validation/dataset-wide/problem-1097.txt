One way of doing it would be to use to create a table expression with the ids to check and to find the missing ones. 

Is costed at on my machine for my example table. If SQL Server was aware that the zoo had no black swans and that it would need to do a full scan rather than just reading 100,000 rows this plan would not be chosen. I've tried multi column stats on and and filtered stats on but none of these help convince it that black swans don't exist and the scan will need to process more than 100,000 rows. This is due to the "inclusion assumption" where SQL Server essentially assumes that if you are searching for something it probably exists. On 2008+ there is a documented trace flag 4138 that turns off row goals. The effect of this is that the plan is costed without the assumption that the will allow the child operators to terminate early without reading all matching rows. With this trace flag in place I naturally get the more optimal index intersection plan. 

This isn't really possible in general. The full range from to is valid for and from to for so it is impossible to distinguish between two bytes that represent 2 varchar characters or a single character. could be or If is odd then you know it is though. Also if the text is primarily standard ASCII as in your example then it will contain a lot of when stored as binary if . This is probably not a character you expect to reasonably find at all in strings so the presence of that could be taken as evidence that the string is . 

The estimation method changes to and the estimated rows become accurate. It is able to convert that into a range 

To compare the Id with the previous one and only return ones with a gap greater than 1. (Untested as SQL Fiddle is currently returning errors when I try and create an Oracle Fiddle) 

the cast to is there to avoid integer division. The reason for the join clause is explained here. It can be replaced with if preferred (or simply if the column is not nullable). 

I'd hope that the predicate on gets pushed up into the selects from the source tables. Check the execution plans to be sure. 

You might also be encountering some degree of blocking. When I test a on a table with pages pulled from cache it seems to be much more CPU bound than your results show (more like 90% CPU time than 50%). You could try with and see if that improves matters. If it does you will need to determine whether the returned is likely to be sufficiently accurate for your purposes. 

I found that it was possible to reduce the time discrepancy somewhat by enabling trace flag 610. This had the effect of reducing the quantity of logging substantially for the subsequent inserts (down from 350 MB to 103 MB as it no longer logs the individual inserted row values) but this had only a minor improvement in timings for the 2nd and subsequent , cases and the gap still remains. The trace flag improved the general performance of inserts to the other two table types significantly. 

It won't shrink of its own accord if the reason for it growing to that size has now passed. Do you have snapshot isolation enabled on that instance? What does the following query show? 

But there is no way of altering a check constraint definition without dropping it and recreating it (thus requiring all rows to be revalidated against the new definition) 

Also the execution plan shows that the assert operator operates on the values output from the table insert operator so is checking the values that were actually inserted. 

If you don't specify the column list then this implicitly assumes a column list including all user insert-able columns (non , , ) as below. 

Permissions need to be granted to the person executing the query for every object referenced by the view. Except if they are owned by the view owner. In that eventuality Ownership Chains come into play. 

The above returns if supplied an invalid input for the datatype (including -ve numbers) and I would be minded to leave it that way. But if you must change these to you can wrap it in an 

As a simplification though the fully expanded version might be more efficient. If you have no useful index you might prefer just using 

This makes sense as the semantics of the constraint do change with the setting. If you encounter this you could consider temporarily removing the problematic constraints etc and adding them back after the switch or just biting the bullet and recreating the table in a more conventional way. 

There is no inbuilt method for granting permissions on tables matching a pattern. You need to grant it to the individual tables. It would be quite easy to generate the required script with a query against for names though. Instead of using a VVC prefix you could create a VVC schema. Then you can grant select permissions on the schema. This would also cater for tables added in the future to the same schema. 

Using the conversion code to decimal from this article. The may well be completely unnecessary overhead. I'm not sure. The result of this function always seems to be in LSN order and I guess it just reads the log sequentially but just in case... 

Your initial query shows that must have no more than one row (or it would fail). You could rewrite this as a join using 

If the value passed for is a variable or parameter (rather than a constant) you may find you get better plans if you use with this. Particularly if the cardinality of and is very different (likely even without the hint only the relevant one will be accessed as the table access will be under a filter with a startup expression predicate). 

Looking at the transaction log entries in more detail for the table version of the SP each subsequent invocation of the stored procedure creates three transactions and the table variable one only two. 

This plan now correctly costs for reading the full 200 thousand rows in both index seeks but over costs the key lookups (estimated 2 thousand vs actual 0. The would constrain this to a maximum of 10 but the trace flag prevents this being taken into account). Still the plan is costed significantly cheaper than the full CI scan so is selected. Of course this plan might not be optimal for combinations which are common. Such as white swans. A composite index on or ideally would allow the query to be much more efficient for both scenarios. To make most efficient use of the composite index the would also need to be changed to . The table below shows the seek predicates and residual predicates shown in the execution plans for all four permutations. 

Then extract a month at a time from and merge into (with a when matched then increment, when not matched insert). The leading column means that as long as you write the query sargably the extraction of each month can be done efficiently and the extracted rows for a month will be ordered by making a merge join against possible without a sort. Whilst that could benefit this particular query you'd need to assess the utility of this index against your overall workload. Then calculate the totals from (can leverage the PK order to avoid a sort) and store those somewhere and move onto the next month. (Or possibly instead of you could use a "temporary" permanent table and create an indexed view on that to avoid the separate explicit aggregation step and just copy the values straight from that before moving on) 

The reason for is to avoid problems with snapshot isolation. One problem with the approach above is that because the constraints are evaluated RBAR it can fail some transactions that ought to succeed. For the example data 

As you are on Enterprise Edition and version >= 2012 then the information in Online non-NULL with values column add in SQL Server 2012 should apply. 

The reason SSMS does this is because there is no TSQL syntax to reorder the columns in a table. It is only possible by creating a new table. I agree that this would be a useful addition. Sometimes it would make sense for the new column to be shown next to an existing column when viewing the table definition (e.g. to have a column next to a column or grouped with address and email columns). But a related Connect Item ALTER TABLE syntax for changing column order is closed as "won't fix". The only workaround apart from rebuilding the table would be to create a view with your desired column order. 

I just mention this in passing as extended events would be clearly preferable anyway as it is documented and much more powerful. 

The plan above will show two table references but hopefully both underneath a filter with a start up predicate so only the relevant branch actually gets executed. A final option would be to just have two entirely separate queries and choose the correct one with procedural logic. I suggest reviewing Dynamic Search Conditions in Tâ€‘SQL as an informative article on the topic. 

This issue is called parameter sniffing. Later versions of SQL Server give you more options in dealing with it such as or hints. You might try declaring variables in the stored procedure, assigning the parameter values to the variables and using the variables in place of the parameters as it sounds as though most of the time you are getting a reasonably satisfactory plan. Normally the most catastrophically bad plans are those compiled for parameters with very high selectivity but ran with parameters with low selectivity. Assuming the plan generated is more robust with this approach and satisfactory for all parameter values then the advantage of this approach over that suggested by JNK is that it does not incur a compilation cost for every call. The disadvantage is that for some executions run time might be greater than for a plan tailored specifically for those parameter values so it is a trade off of compile time vs execution time. 

is a row table which is (again) apparently a heap with no useful indexes. This discrepancy in estimates affects the join choice used. The plan chooses a hash join that just scans the table once. The plan chooses a nested loops join and estimates that it will still just need to scan the table once and be able to spool the result and replay it 78 times. i.e. it estimates that the correlated parameters will not change. From the fact that the nested loops plan was still going after two hours this assumption of a single scan against seems likely to be highly inaccurate. As for why the estimated number of rows between the two joins are so much lower I'm not sure without being able to see the statistics on the tables. The only way I managed to skew the estimated row counts that much in my testing was adding a load of rows (this reduced the estimated row count even though the actual number of rows returned remained the same). The estimated row count in the plan with my test data was in the order of 

So it uses the same partitioning definition as the rest of the functions. This shouldn't change anything in your example as your expression will just assign to any non matched rows anyway. As for the rest of the plan see Partitioning and the Common Subexpression Spool (Plan afterwards with no sorts) 

One other alternative would be to block simple parameterisation by adding a redundant as below. But I don't recommend this as then you will get plans compiled and cached for every different literal value that you pass. 

Now any attempt to set it to an invalid value will fail with an error. You can of course do similar in your stored procedure 

The T2 table metadata would be compatible with the physical layout of the T1 table so it would be nice to do a metadata only switch but it doesn't work in reality. 

I thought I'd have a play with this and for the stated goal of having something that "Saves every query you run" that works on SQL Server 2012 SSMS this seems be do the job on my machine (Add your own error handling / testing / refactoring) It's based off Andrei's sample project with the class replaced. The SSMSAddin2012 project on Codeplex is also very useful. 

TSQL is very inefficient for string manipulation compared to (where you can use a for example). The concatenation approach will allocate a whole load of new strings. It might not be so bad if you could just create a big string up front and use to replace parts of it but I believe from trying this approach previously that this too just creates a new string behind the scenes. Additionally the behaviour you are relying on for both cases is undocumented and not guaranteed. "The correct behavior for an aggregate concatenation query is undefined."