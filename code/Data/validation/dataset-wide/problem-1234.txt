Overall this is quite nice piece of source, so only some tiny details here and there (some maybe even ruining it :) )... From code style perspective I think the variables naming is "only" good, not perfect. tells what arithmetic operation did happen to the value, which is usually not sought after during reading source. sounds to me a bit better. and are OK, to be ultra verbose you may still extend them a bit like maybe and , but that's already reaching some tutorial level. And itself sounds a bit inexact to me, again a more over-verbose variant like may be considered. Can the work with ? Just for the sake of exercising "putting everywhere". ;) (I don't believe it will change the machine code produced by compiler). Maybe even reference &? 

Source differences only, all the where in static code the 32 bit address part is enough, have to change to full 64 bit in PIE, i.e.: 

Continue with other neighbours of current node, then go back to loop by fetching next current node from deque (if you have 4 of them, then empty the one with best word_distance first), till the (shortest) path is found. The word_distance is supplying some orientation in the path finding (absolute coordinate distance in A* when used with geometric coordinates) - it's more likely you will find shorter path with words which are becoming closer to the other end of path. I will end here, as it already took some time to write this, unfortunately I will not provide you with working source. 

edit: my proposal has somewhat different logic, searching for another blocker, when the nearest one did move, while your original code (reading it now again) is probably moving the nearest one once, and then just finding it again and again, but not moving it. If that's the case, then remove the test from search loop, and add condition to the second part if. 

So... I just guess it would run faster on real HW. But at least in simulator it does use about ~60% of instructions to run (~1500 vs ~2500), to use some measurable (but almost meaningless) number. 

I would refrain from using as general purpose register. This is limiting you in using / subroutines, and you risk memory corruption caused by interrupt happening in your program context. I would also refrain of such heavy LUT tables usage, as the program will be more likely limited by the I/O operations speed, so that tiny amount of calculation will quite likely hide in the buffered I/O waits. Here is my version, avoiding those things mentioned above: 

in argc/argv is probably bug, the at end should have been ? Or it's on purpose? If it is bug, you could have avoided it by using internal method, to avoid duplicity code with implementation. Ok, enough for now. :) I hope I'm not way off, and this will give you some ideas how to improve it. 

Generally I'm quite fine with your original source, looks nice. If you are pursuing absolutely top performance (what is the reason? This looks like a function, which should be used only few times trough life cycle of application) - you would have to dig into first, to see if for your special case of various sets of {-1, 0, 1} values you can generate permutations "manually" in a faster way, then the general purpose permutation generator does. Maybe there is a way, but the source will become substantially longer and more complicated. If you want to stay with , there's little room for improvement, especially without losing some readability of the source. For example with understanding how the is modified by last , you can cut out some initializations and sorting like this: 

You should profile the game first. While your code is in worst case doing cca. 360*8 (360 player rotation decrement x 8 blockers) loops until it finds the blocker (and it will end in infinite cycle in case you don't have any blocker in blockers) => so it can be written much more efficiently... Still about ~3k of int compares should not lag modern phone, that's not much. So you are very likely having bottleneck in some code which is not part of your question. Anyway, to rewrite your current code you may try something like this (pseudo code): 

one way is class holding the actual string, responsible for manipulation with it. other way is helper utility class providing only functions, not holding any string at all. 

One note about your original code: You don't clear the input, which doesn't feel right to me. If the function is named , it sounds quite functional to me, so calling it twice with the same vector I would expect it to contain only single copy of vertices (your code would add another set after the first one). For your code probably some name along would be more precise. In my code I avoided this completely by creating new instance of vertices inside the function, and letting the C++11-post era compilers to optimize "return by value" situation. And you can still do twice to get two sets of vertices, to simulate behaviour of your original code (if desired). So the change in API is not limiting such usage, but it shouldn't happen by accident, reusing some non empty vertices. 

Bug: if contains some low bits, you will get wrong display page value. Should be instead. Why ? First why , that's not English, and why -1, you are on x86 powerful machine, there absolutely no reason to save bytes on symbol names or amount of symbols. 1988 called, they want your habits back. 

Some short note without really reading into the source. In MVC usually the Model is independent of View and Controller. So your looks like mixing together Model and View class, supplementing functionality of Controller too. Some general notes how I would split these: Model: 

Do just if you are using only later. This at first did look as bug to me, because you have anything in , then I realized you will avoid that by using only. Actually this will fail if array spans over 256B boundary, then the will produce wrong address. But you didn't show how you define , maybe you are sure it is well aligned and will always fit into 256B "page". Still maybe consider more robust version doing whole to calculate with full 16b values. 

The Model/View relation - how I like it - can be demonstrated on this classic: Consider having blog application, so for each article you have date+time of publishing the article. Then Model should contain UTC timestamp value (can be unix timestamp, if you need only dates since 1970 onward). And View will do all the formatting magic, ie showing "5 seconds ago" for fresh article, or "previous millennium" for some really old article, also converting the date/time to local time zone of user (source data stored on server being in UTC, time zone agnostic). 

(I would also write as , but I think the compiler will optimize that one anyway, just old habits from ASM times die hard). 

And the whole set cursor code is a bit fishy, I mean the calculation of based on [x, y], don't you have such code already somewhere? Are you optimizing for speed that you can't afford to call that? (probably not, as you are still in real mode) Also is again risky as you don't specify content (would work with my modification above to ). I would probably write separate subroutine to calculate based on coordinates, like for example: 

And the other three too (modify target register "edi" to "rdi"). And the instructions into glibc themselves require extra ELF setting to make the linking dynamic ("abusing" a bit directive "with regard" and special keyword ): 

You can't. For example you can't round to 1 decimal digit as an with current HW, as the is not capable to store exactly. Try for example: 

AFAIK this is the proper 64b PIE (position independent executable), dynamically linked with glibc and ASLR-ed by common linux OS to random address space. Stripped elf64 binary on my machine has 6320 bytes, not that bad, although the real ASM content from the source is only about 170 bytes (but the elf header/sections data are of quite some size, and then the C-runtime is linked it as well). I know you didn't mention PIC/PIE in your original code and question, but in common current 64b linux OS it is highly desirable to produce PIE binaries only, as that's what the default settings are also for C/C++ compiler, and what makes ASLR and other security mechanisms most effective, so that was the reason why I myself set up to pursuit this side-goal. 

This will iterate over each blocker only once - searching for the one, which needs least amount of player rotations (steps) to align with. Those two to convert value into 0..359 range are effective only when all source rotation values are near this range, otherwise rewrite it with modulus. If you will be profiling it, you may try modulus anyway, it will be probably faster in Java that way anyway. 

Your code is unfortunately weird mix of both. I will try to show headers for both ways here (just by hand, will very likely not compile without fixing, plus it's missing implementation): (and I'm afraid I'm at the moment tainted by Java too much (coding in Java in other window, so I may mess up some C++ syntax badly, this answer is more about OOP-like ideas, how to design your API and classes). 

One more note about the rest of code: I don't like how you use plenty of additional temporary vectors. When you want to enjoy C++ performance boost, you have to be a bit more aware of data structure, as that's the major advantage of C++ over other high-level languages. Usually the sorting algorithms are implemented to either work above the initial container memory without any temporary, or when temporary is required, only single secondary vector of full size is created. Then internal calls pass the first/last iterators to point to the parts of the vector memory, which should be processed in the particular inner call. If you need temporary vector, one of full size should provide enough temporary space for the operation, being properly partitioned by first/last iterators. I'm sorry to not provide the example, but I believe you can find some merge sort implementations on the Internet, probably showing operation on 1-2 vector's only without the copying of content between internal calls. 

Some more typos... don't you have spellcheck in your editor? :-o (I'm using simple kate for NASM sources, and Shift+Ctrl+O will switch spellchecking on/off, so it doesn't bother me on instructions, but I can use it to review comments from time to time). 

So things like belongs to View.update, belong to Model.addExperience, poking model/view in reaction to the click event belongs to Controller (although it shouldn't contain the logic itself, ie deciding whether level up happened or not, that's responsibility of Model). Model contains every important state of the world, so the view can be reconstructed out of it (above I'm breaking it a bit by that "leveledUp" value, writing it now, I would probably move it inside Model, and create one more getter being updated by last call) - from scratch. View contains everything what is displayed, for example it contains the string representation of experience (Model doesn't have to, as it can be constructed from the in Model, by "formatter" in View). And also contains logic, how to display it. Controller should only "connect the dots" and control the execution flow, when what should be called/updated. 

Which would use getters to set up the exp bar, exp number display, level number display, set up correct colours based on those, and finally to start some effect when would be true (having that "effect" as an stand-alone app entity, capable to handle it's own life cycle, like some animation of fire works or sound player playing ding once). In case of some more complex level-up effect I would consider either making state of it part of original model, or having separate LevelUpEffectModel to hold state of effect, with it's own view and controller. Controller: Finally controller is the glue of view and model, but should not contain any biz-logic, or low-level view updating commands. So in this example it would do probably things like: 

I had only some limited quick look, just cherry-picking some things to comment on: Entropy level: I'm not expert on this topic, but I think using time stamp as additional entropy source every time you produce a number is not a very good idea. But I'm even afraid you use it as only source of entropy in some cases, which is definitely wrong. I was unable to quickly show what's wrong about it, as you have weird way of updating, masking the problem out in your example. But after you change this I'm afraid it will become obvious this needs rather some "seeding", and building up upon seed data. About : You do great deal of pushing all around into it, yet only pops from it, and only once. So after running this for a while (using it as output stream, without calling RNG) the buffer will grow a lot, eventually running out of memory. If this one is supposed to be a buffer of pre-generated random numbers, then the should generate new buffer value only when buffer is empty, and then it should pop value from buffer and return it. But I would do something different, I would change buffer into single number, used as seed. At any point of your current source, where you end with buffer.push, you would instead use old value of buffer as input for the transformation (in some way), storing result back to buffer. Then will do yet another transform over it, and return the value. But at this moment the timestamp-every-call will start affecting the statistics of random numbers a lot for particular date-time and RNG calling period. So I would use timestamp only for initial seeding, then the RNG would work as any common pseudo-random arithmetic RNG, with added twist of output stream being further source of entropy. About output stream as entropy source: well, you should check the common output stream byte values first, they are not "random" bytes. From the code it looks like you are aware of that, trying to build an unsigned value ORing 4 shifted values, but the result is only 20b wide for ASCII, and the values are overlapping, so the upper bits of ASCII (not varying much) will affect the lower bits of next character (I hope I did read the source correctly, didn't debug it, BTW for better readability you should put these transformations from string to unsigned into some function, so you can test it on it's own). I would probably take only 3 bits (or maybe just alternating 2+3, to avoid ASCII specific values definition to affect the entropy of such value too much) of each output character, and cumulate them in 32b buffer till it's full (the overlapping 1-2 bits kept for next value), then use it for transforming the seed buffer (so roughly every 10 output chars the seed will get additional entropy). This may still go quite wrong with UTF-8 or unicode16/32 output stream variants, just imagine somebody using it with UTF-8 Arabic texts, having every second byte something like or what's the actual prefix (too lazy to check). About : 

To make it less weird, and more OOP like, it would make more sense to extend , as that's already well known defined API, and just adding one more method to it makes much more sense in OOP world, having then class which can be used in place of std::string where needed. Helper utility class (or just a namespace with functions would do probably even better) would tried to provide pure functions, taking everything needed on input, and returning output, not having any instance at all, so they are accessible from anywhere, when you have the input ready.