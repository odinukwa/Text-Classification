UPDATE: Tried using the function with the values of and . However, the function with such specifications appears to read the setting of Daylight Saving Time from the current system time and not from the time as read from the database field. This means that when the data should be taken as a date in CST, it is read as CDT instead. I keep thinking it's got to be simple to get the UTC representation of a field - which would simplify things, seems to me. UPDATE 2: Looks to me like I mispoke regarding . I'm using this function and it seems to work: 

All three of those "variables" are FLOAT fields in the (temporary) database. I would expect that and would never differ by more than 1 or 2. However, there are certain values that seem to generate a wild answer - and they tend to be for certain values of . Calculating the value from shows that the calculations are right. Conceptually, day2 should be difference between the dates from the DATETIME values, and day1 should be the number of 24-hour periods in the same time frame. The biggest differences are at the maximum value for timediff: using calculations, the number of 24-hour periods is just over 97. However, using the DATEDIFF function shows a time span of as low as 60 days. This makes no sense. There are also big differences when TIMEDIFF results in 26 24-hour periods: DATEDIFF reports 10 days instead - or when TIMEDIFF results in 30 24-hour periods, and DATEDIFF reports 11 days. There are also differences when TIMEDIFF reports 79 days, but DATEDIFF gives 29 days. Here is example output - from a Ruby script (and in Ruby array format) - with columns d1,d2,TIMEDIFF,24-hr periods,DATEDIFF: 

Why are these calculations so different? I am assuming that it is a lack of understanding of TIMEDIFF, but what am I missing? 

The reason that 127.0.0.1 is being contacted is because the tunnel connects a port on your local machine to the remote host. The message seems to suggest that an SSH connection is not being made. Try this from the command line: 

I'm trying to understand how to compare a DATETIME value (ostensibly inserted as GMT) and a TIMESTAMP value (automatically generated at INSERT time). As I understand it, the DATETIME is set to whatever it is set to - and the reporting comes back with the same value, no matter what time zones are in place on the server or client. In the MySQL documentation, it sounds like TIMESTAMP values are reported in the current timezone, but stored in UTC. In this situation then, subtracting the DATETIME value (earlier event) from the TIMESTAMP value should be possible without a lot of timezone conversion - because the DATETIME is GMT and the TIMESTAMP value is UTC. However, doing this accurately - and understanding it - is proving to be a problem. I have situations where I need in minutes and in days. If we assume that column is DATETIME and column is TIMESTAMP... So far, I've tried: 

Actually i have to add 2 TB file to this 3 TB disk, but trying to add in chunks of 100 GB But it seems to be running for past 30 mins HOw can i improve this? Update: The Query above completed in 40 mins. The wait was ASYNC_IO_COMPLETION Also, this database is 24*7 and has DB mirroring configured. Now, i am trying to increase in 50 GB chunks using below 

I waited for almost 4 hours, trying to shrink in chunks, but not even a single MB got shrinked. Is there a way this can be done or how can I manage the unallocated space? Please advise. 

Rebuild the endpoints--> no luck Resume the mirroring with partner resume --> no luck DBCC CHECKDB shows 0 errors found on either side. 

I am not sure if in am checking this correctly. But i can see the discrepancies in the value of compressed backup size from msdb.The value of compressed size is different what i see on the drives. These backups are taken from Litespeed. e.g Litespeed backup compresses the backup of database to 1 TB for 8 TB and msdb shows that value in column as 4 TB. Is there a way to get the correct information using SQL for third party backups? Query used: 

Adding more info while troubleshooting shows 0 rows while for similar db in diff env i get almost 5-6 rows. Not sure what does that mean 

Need your suggestion on how below works. Situation: We had unsent log on Principal stuck for almost 10 hours with unsent data around 200 GB. Action: Paused and resumed the mirroring. This kicked of the stuck mirroring. All 200 GB is now send to mirror and its trying to restore. But when new transactions are showing up on principal i see that again the records or mirror seems to stuck showing send rate 0 KB/sec. So my question would be , is this suppose to happen and block unsent log on P till all previous 200 GB is restored on Mirror or there could be different problem here? And can we again PAUSE and RESUME mirroring if mirroring is still synchronizing ? Please suggest 

Implementing CDC is something i am going to do first time here. We are going to start with lower environments and then towards PROD. I have configure CDC in DEV and QA where we do not have the size of DB configured for CDC that big. However in PROD, that DB is somewhere 4 TB. We are implementing to some of the big tables which are almost 500+ GB. Analyzing lower env' was not much of help in understanding the growth of change tables. Therefore my question would be how to estimate the additional storage/growth that may be required for assigning to FG tagged to CT tables participating in CDC? How should i plan the amount of disk space that might be required so that in PROD, i should not be running out of space before the retention time set on CDC job. [ Our retention job clears data 3 days old] 

I recent upgraded my Ubuntu 14.04 server to Ubuntu 16.04. I managed to get mySQL upgraded to 5.7. I don't currently have anything in the mySQL instance. Since I have been an Oracle DBA for almost 20 years, I want to configure mySQL with some of the same features. I want to turn on logging for all transactions, and setup hot backups, hopefully using open source software. I'm also not sure which database engine to use. I have been looking through the documentation, and I am just not finding the steps for setting up a mySQL instance to work they way I think it should work. I know I can do a logical backup, but that doesn't allow me to do point in time recovery. I know I can use all of the default settings, but if I am able to market my skills as a mySQL DBA, I want to use as many of the advanced features as possible. Thanks. 

When in doubt ask Tom... backup up and recovery generating extra redo Which is good reason to use RMAN for backups rather than scripted hot backups using begin backup/end backup. 

Rather than putting the data in a case statement where you might have to repeat that case statement in more than one query, you could create a table with static data, then joint to that table. The advantage would be that you can change the data in the table easier than changing all of the queries that have that case statement. 

Currently we have two Model Mart repositories, 7.2 and 9.5. We only migrated the models that are currently being used, but we need to keep the old models available in case they are needed. Is there a tool to do the migration from Oracle 11g to SQL Server 2016? Has someone done a manual migration where the instance, database and schema are created in SQL Server, the constraints are dropped and the data is moved over followed by creating the constraints. 

This is code that I took from a working Perl script. There is a limit of 32k for using utl_file, last I checked. You probably need to reset the v_curr_idx variable with every loop, otherwise it would write the first file and skip any smaller files. 

Then do a select for update and a forall update a commit and you are done. If you needed to you could probably load the data with a limit clause in a loop so that you could process a small chunk of data at a time. I would start by trying to update all of the data at once. $URL$ Here is my best guess on two indexes that could help the performance. You probably need to test with and without these indexes, or maybe you already have indexes that do enough. 

I am trying to find the cause which have bloated the mdf of a database from 20 GB to 100 GB. So far i tried checking the autogrow events to find time, but could not find none using standard reports and even default trace files. We don't have 3rd party monitoring tools to confirm if any maintenance job like rebuild would have done or any other process. How can i find what caused the growth on this mdf? 

We are doing migration from SQL2008R2 to SQL 2014 for some of the databases hosted on 2008R2 to 2014 and remaining DB's to other servers. I am aware of migrating the login using dbatoo or using sp_helprevlogin.. But using that it would move all the logins from 2008R2 instance to 2014. Since we are only moving some databases we only need to move their respective logins or i should say only those logins that are present under users in the database geting migrated to 2014 instance. How can we achieve this intelligently via script, because doing manually is taking lot of time as we have hundred of logins and about 40 databases 

FG_data1.ndf which is restricted to 200 GB and current free space is 0 GB FG_data2.ndf which is restricted to 200 GB and current free space is 10 GB FG_data3.ndf which is restricted to 200 GB and current free space is 0 GB FG_data4.ndf which is restricted to 200 GB and current free space is 60 GB 

We have an issue with database mirroring on one of the databases on our sql instance where mirroring is suspended for almost 15 hours now. DB is 200 GB on SQL server 2008R2 Sp3+cu with latest patched done for meltdown. so far we have done below 

On one of our current prod servers, we have one of the data drives going out of space.[Somehow we have created 2 GB space for now] Now we are planning to move one of the database in different drive which is almost 20 GB. But this database is not accessible state. I was thinking of backup/restore way to move, but it wont as it shows in unavailable state. When I checked the SQL logs, it says transactional log of this database got full. So I checked its state using log_reuse_wait_desc which says "ACTIVE TRANSACTION". How can I get the database available and ready for move so that I can move via backup/restore? Additional Info: Database status shows "SUSPECT" Please suggest.