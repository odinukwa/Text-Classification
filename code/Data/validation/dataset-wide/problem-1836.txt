Beware of the potential for DoS from the respect of the script being able to fill the file system it has write access to. 

The "su -c ... " method posted by others is a good one. For automation, you could add the script to the crontab of the user you need it to execute as. 

I think that it's often a matter of aesthetics. Personally, I would segregate things by virtual host: $URL$ $URL$ etc.. It does give you options for more easily relocating things to other hosts in the future, but you can always accomplish that after the fact by using redirects in Apache. Virtual hosts also let you partition off directory structures more tightly, which adds a little more security to a web server (which may not be as big a concern for you in an intranet setting). Vhosts are also easier to restrict access to during testing than a simple sub directory under the main site's directory. Again, this can be done the other way using simple .htaccess rules, but I prefer to have things logically (if not physically) separated as often as possible. 

I prefer lsof because it's output is consistent across all platforms on which it runs. You can pretty much get the same info from both programs, though. I think it comes down to personal preference. 

You could install git and parse the output of "git status" (or maybe the exit codes?) for the directories in question. Git is pretty fast at what it does. Just make sure to commit the changes, so successive calls to "git status" will show changes. Another idea would be to use tripwire or some similar tool. A more brute force approach would be to periodically tar the directories anyway and compare an MD5 of the previous tar. If the directories are large, though, this would not scale too well. 

Based on the diagram, I'm not sure exactly what what you're trying to do (what is ESI?). However, there's a small, fast load-balancing front-end server called "pound" and it will handle the SSL layer for you. It could sit alongside Varnish on the front end on port 443 (I assume you have Varnish on port 80?) and pass the SSL traffic directly to nginx (SSL can't be cached anyway, so no point in going through Varnish). Normal, unencrypted traffic would go to Varnish as expected. 

Examining DNS records for all related site hostnames will give you a hint a the topology of the site. You may see multiple IP addresses (which don't necessarily mean multiple physical machine, but often will) and the same or different network addresses, which may hint at how they distribute the load for redundancy or speed reasons. Examining the HTTP headers of a site's various services will give you a possible idea of the front-end. Are they using a reverse proxy, such as nginx or Varnish, or are you hitting the web servers directly? Are requests for PHP pages coming from a different server (apache) than those for static HTML and image files (nginx,lighttpd , etc.)? Examining SMTP headers from mails sent from a site will give you more hints. Traceroutes and pings will yield a little more info. Of course, much info gathered will be speculation and guessing on your part, because a well configured site will not give out too much info about its internal architecture. What you'd be doing is, in essence, much what a penetration tester would do for certain info. Just make sure to not cross the line and disrupt the site. 

I'm not 100% clear on what your goal is, but there's a Unix reverse proxy program called pound (likely available in the Ubuntu package repo) that will allow you to redirect HTTP requests to different back-end servers based on matching expressions. So you would have pound running on your front-end Ubuntu server on port 80. On the back-end, you'd have Apache+Ubuntu running on some other port (8080, for example) and the Windows server(s) running on some port(s). You then tell pound which back-end server/port to forward the request to for each domain and/or URL regular expression you need to sort the requests by. I'm unsure why you wish to avoid a proxy in the mix, as this is exactly what you do need. I don't believe you can accomplish what you need without one. As far as such proxies go, pound is pretty small and fast. If it's caching you wish to avoid, due to the use of dynamic content, pound does not do any caching. 

I know that nginx can perform a lot of functions, but why not delegate each piece of the architecture to software that does one piece of it really well? Consider some, or all, of these pieces: pound or haproxy for the load balancing, varnish or squid for the reverse caching proxy, and having nginx and apache on the back end for static and dynamic content (respectively). That said, I'm not exactly sure what your question is. You've told nginx to pass all requests (I assume by "pass" you mean not caching them) to an apache back-end. Without caching, the benefit would be distributing the load across multiple apache servers on the back end. If you only have one back-end apache server, then you'll only get the benefit by caching the content, not just passing requests straight through. More details about your setup, and what you want to do, would help. 

Install FreeBSD 8.0. GELI-encrypt the physical devices you want to use, then add those devices in a mirror or raidz configuration. Then run your Samba, NFS, or whatever file sharing protocol you need. I run GELI+zfs on my FreeBSD workstation, and it works very well. The home-rolled solution may not have all the bells and whistles of a commercial NAS, but an experienced UNIX admin should have no trouble with FreeBSD, given how good the online handbook and communities are. 

(I'm assuming your input is one number per line, as that's what sort would output.) You could try awk: 

Try running "" to see if there are any other relevant options. I haven't tried monit in particular, but sometimes, if you're trying to install an application from source, you may need GNU make (which you may need to also install and have in your path ahead of FreeBSD's make. This is odd, though, as --prefix is often all that's needed to do what you want. EDIT: I Just tried an install of monit to /tmp/monit as a non-root user (using --prefix), and it worked fine. I just had to use "gmake" (GNU make, as installed from ports). Give that a try. (BTW, I had to un-tar the source file from /usr/ports/distfiles into /tmp in order for this to work. You may not have enough permissions from within /usr/ports to do a build from there.) 

Set up a basic wiki and pgp/gpg-encrypt the document/documents with such info. Set the client(s) up with the FireGPG firefox plugin. You can even mix inline encrypted sections into a plain-text wiki page and it'll take care of decrypting it for you. Just make sure you encrypt documents to yourself (in case the client loses their key), in addition to whoever needs access. 

This would give you an un-sorted list (the order in which arrays are walked through in awk are undefined, so far as I know), so you'd have to sort to your liking again. 

Then you configure your browser or download tool to use the SOCKS4 or SOCKS5 proxy and feed it port 8888 at localhost. No intermediate storage required. 

How many cores in the box, and what is the actual load? What is the actual rate you're getting messages sent out? Like most, my first thought is disk, so check that. However, network utilization might be the cause, as may be high interrupt load (bad card?), so check those. I've found that even for a modest mail server, having a fast caching DNS server (I'm partial to "unbound") on the same box helps to alleviate latency and network load.