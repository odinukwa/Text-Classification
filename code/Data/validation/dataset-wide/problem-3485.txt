I think you're getting things backwards. Marginal cost (at a certain quantity $q_0$) really only makes sense as the derivative of the total cost function with respect to quantity at point $q_0$: $$ MC(q_0)=\frac{dTC(q_0)}{dq}. $$ The interpretation of marginal cost as "the cost of producing the last unit of output or the cost of producing the next unit of output" is based on the (linear) approximation $$ \widehat{MC}^+(q_0)\approx\frac{TC(q_0+1)-TC(q_0)}{(q_0+1)-q_0}\quad\text{or}\quad \widehat{MC}^-(q_0)\approx\frac{TC(q_0)-TC(q_0-1)}{q_0-(q_0-1)} $$ This interpretation is usually introduced in undergraduate textbooks because of its intuitive appeal, not for its mathematical rigor. Imagine trying to explain the concept of marginal cost as the cost associated with an infinitesimal amount of change in quantity to a freshman who probably has no idea what "infinitesimal" means, let alone the concept of derivative. A first problem related to interpreting marginal cost as the cost associated with the previous or the next unit of output is that producing the two units (the previous and the next) may imply different costs. This is illustrated by the formulas $\widehat{MC}^+$ and $\widehat{MC}^-$. Suppose total cost is quadratic, i.e. $TC(q)=q^2$. Then it can be easily verified that $\widehat{MC}^+(q_0)\ne \widehat{MC}^-(q_0)$, for any admissible $q_0$. This is problematic because we want the marginal cost (at a particular $q_0$) to be unique. Second, and this relates to your question at the end, the concept of "unit" is somewhat arbitrary. And this arbitrariness makes measures such as $\widehat{MC}^+$ and $\widehat{MC}^-$ unstable as well. To illustrate, continue with the quadratic total cost example. When the unit of quantity is kg, the "marginal cost" at $1$kg--as you would refer to the cost associated with producing one extra unit--is $$\widehat{MC}^+(1)=2^2-1^2=3.$$ But what if we suddenly want to change the unit of quantity to gram (g)? Then the "marginal cost" at $1$kg, or $1000$g, would change as well: $$ \widehat{MC}^+(1)=(1.001)^2-1^2=0.002001. $$ To avoid such instability caused by changes in units of measurement, marginal cost is defined as a derivative, not as a difference. If you want to measure the change in total cost due to change in some arbitrary quantity, simply apply the first formula in your question. But keep in mind that this cost is not marginal cost as an economist would understand it. 

A zero-sum game is so named because in it, the sum of the players' payoffs is always zero. Unless you provide a specific payoff structure to the players in the game, it's hard to categorize it. From what you've described, I suggest you look up the term constant-sum game and see if it is something you're looking for. An example: $$ \begin{array}{|r|c|c|} \hline & X & Y \\\hline X&2,-1&-1,2\\\hline Y&-1,2&2,-1\\ \hline \end{array} $$ Suppose the status quo payoff is $0$ for both players. Then in each of the outcomes in the game above, one of the players gets $2$ at the expense of the other player losing $1$. The pair of payoffs in each outcome sums up to the constant $1$, hence the name constant-sum game. Do notice, however, that this is only a very small subset of the possible games due to the heavy restrictions on the payoffs. 

Finally, I want to note that the requirement for $\succsim$ to violate continuity means we're ruling out finite (and countable?) domains. 

I'm not sure what you mean by "shocking the preferences". But it's very easy to find an example in which the optimal solution is still at the corner after a (small) positive income shock. Suppose $u(x,y)=\sqrt x+y$, $p_x=p_y=1$, and income is $m=0.1$. Performing utility maximization subject to budget constraint, we get $$ x^*=m,\qquad y^*=0. $$ Now give income a positive shock: $m'=m+\delta$. As long as $\delta\le0.15$, the optimal solution is still going to be a corner solution: spend all the income on $x$ and zero on $y$. 

The "profit" function for consumer choice does exist; it is the Lagrangean: \begin{equation} \mathcal L(x;\lambda)=u(x)+\lambda[c-g(x)]\,, \end{equation} where utility $u(x)$ is to be maximized subject to budget constraint $c\ge g(x)$. Note that $\max_x \mathcal L(x;\lambda)$ is an unconstrained optimization problem just like $\max_x\pi(x)$. 

You're right. Let $s^*=(s_1^*,\dots,s_N^*)$ be the equilibrium of a strictly dominance solvable game. Then by definition, $$ u_i(s_i^*,s_{-i})>u_i(s_i,s_{-i}) $$ for all $i$, all $s_i\ne s_i^*$ and all $s_{-i}$. This implies that $$ u_i(s_i^*,s_{-i})\ge u_i(s_i,s_{-i}) $$ for all $i$, all $s_i\ne s_i^*$, all $s_{-i}$ and with strict inequality for at least some $s_i$ (in fact, for all $s_i\ne s_i^*$). This makes $s^*$ an equilibrium satisfying the weak dominance solvability criterion. 

From the fact that RN chooses $M$ over $N$, we known that the mean of $M$ is higher than the mean of $N$, or $E(M)>E(N)$. The fact that RA chooses $N$ over $M$ despite the latter having a higher mean must imply that $N$ has a much lower variance than $M$. Thus, given the features of RL's preference (she likes high mean and high variance), the obvious choice is therefore $M$. 

Assume the players have to choose integers, otherwise a best response may not exist. Let the payoff of winning be $\alpha\cdot[\frac23\text{ of the average}]$, $\alpha>0$. Consider the two player (A and B) case, and let's verify whether choosing above $0$ is optimal. Suppose A chooses $x>0$. Then B can guarantee a win by choosing $x-1$, since $\frac23(x-\frac12)$ is closer to $x-1$ than to $x$. However, if B chooses $0$, A choosing $x=1$ is a best response (A gets zero anyway). So we will have a NE where not both players choose $0$, and the one who chooses $0$ gets positive payoff. This equilibrium can be generalized to the $n$-player case. Let $n-1$ players choose $0$ and the remaining one choose $1$. In this equilibrium, those who choose $0$ would share the positive payoff (each gets $\frac{\alpha}{n-1}\cdot\frac23\cdot\frac1{n}$). They are best responding because choosing any $x>0$ would imply zero payoff. The one who chooses $1$ is also best responding since he gets zero anyway. (Of course this is not the only equilibrium. There is also one where everyone chooses $0$, which is obvious.) Admittedly, the above equilibrium relies crucially on the assumption that players are only allowed to choose integers. But this is due to the fact that payoff is a function of the choices (compared to a fixed amount in the original version of the problem). Suppose players are allowed to choose any real number in $[0,100]$. Then if A chooses $x>0$, B would best respond by choosing $x-\epsilon$, where $\epsilon=\min\{y:y>0\}$ (B would win of course, but she also wants to maximize her earning by making her choice as close to A's as possible). However, such an $\epsilon$ does not exist. Therefore, the only equilibrium is for everyone to choose $0$. The intuition is this: however large the winning is, you don't get it if you don't win. But winning requires that you choose small. So the incentive to win overwhelms the incentive to win big. 

I would recommend reading Thaler's Misbehaving, which chronicles the development of behavioral economics as a field and its struggle to gain recognition by mainstream economists. Several of its chapters (particularly, 6, 17-20) would directly answer your question regarding the historical context and partly the "why". I share my own personal view below. I believe the "as if" argument as advanced by Milton Friedman in his Essays in Positive Economics plays an important role in countering the move towards a relaxation of the rationality assumption. In a nutshell, the argument says that a theory should be judged by the accuracy of its predictions, not by the realism of its assumptions. Therefore, while the assumption of a rational decision-maker is not entirely realistic, the theory is doing a good job as long as it generates sufficiently accurate predictions (in the sense that they match what actually happens in the real world) based on such an assumption. Friedman describes a simple of model of billiard players that I think nicely illustrates his idea: 

The journal Management Science is a decent one. Many economists do publish on that journal. In fact, a quick look at its editorial board reveals many notable names, with the recent Nobel laureate in Economics Richard Thaler among the associate editors. The field of management science is more interdisciplinary in nature. See Wikipedia's entry for it. To me, it's like operations research for business or applied microeconomics in the context of business decision-making. To call it a "separate science" is a bit too much, though. 

The zero-profit condition is derived from the assumption of perfectly competitive behavior (or both firms and households are price takers). This condition may contribute to the determination of equilibrium prices. But to say that $p$ is selected to ensure zero-profit of the firm just sounds incorrect. 

Under the above interpretation, player $i$'s ($i\ge2$) action space would be $A_i=\{0,1,\dots,n+1-i\}$, as, in principle, $i$ could choose at most $n+1-i$ other players (when all players $j<i$ chose $1$) and at least $0$ (when all previous players have exhausted the player list). An exception applies to the first player, where the option $0$ is not feasible. Thus $A_1=\{1,\dots,n\}$. Player 1's strategy space would be $S_1=\{1,\dots,n\}$. Player 2's would be $S_2=\{1,\dots,n-s_1\}$, since after $s_1$ players are picked by player 1, there are only $n-s_1$ players left to be picked by player 2. Carrying this argument forward, player $i$'s strategy space would be $S_i=\{1,\dots,n-s_1-\cdots-s_{i-1}\}$. Generalizing from the above reasoning and taking into account boundary cases, let history at stage $i$ be $h^i=(s_0,s_1,\dots,s_{i-1})$, where we set $s_0=0$. (In general, though, stages should be indexed by a parameter different from the player index. But in your game, since each player moves exactly once, we can use the same index for stages as well as players to economize notation.) The history dependent strategy space for player $i$ would thus be \begin{equation} S_i(h^i)= \begin{cases} \{1,\dots,n-\sum_{j=0}^{i-1}s_j\} & \text{if } n-\sum_{j=0}^{i-1}s_j\ge 1 \\ \{0\} & \text{otherwise}. \end{cases} \end{equation} 

The New England kidney exchange program cofounded by Alvin Roth, based on the theory of stable matching, has enabled hundreds of patients awaiting kidney transplantation to receive kidney donations that they would otherwise unlikely to get. The theoretical underpinning of this program is indeed heavily mathematical, and the program indeed helped a lot of people ("helped" here is really an understatement, it saved people's lives). So I'd say the field of market/mechanism design is an example satisfying your criteria. 

Here we can call $\{A,B\}$ Player 2's actions (actually the correct jargon here is behavior strategies, but I sense that this level of nuance is unnecessary at this point). However, they are not his strategies, for strategies in this case is a "full contingent plan" that specifies an action at each information set the player moves. Since Player 2 moves at two information sets (one after $L$ and the other after $R$), so one of his strategies would be 

Instead of directly giving you the answers, I'm going to give you a series of hints to help you figure out the answers on your own. 

Below is an example based on the general formulation in your linked article. This will very likely need a lot more fine-tuning. Moreover, the following example does not necessarily satisfy the theoretical properties laid out in footnote 16 of the paper. (You should check this.) Nevertheless I hope this could at least give you an idea of how to think about writing down an appropriate utility function that suits your purpose. Consider \begin{equation} \Pi(d,q,\theta)= \underbrace{\left[1+\mathrm e^{-(\alpha_0+\alpha_1 d+\alpha_2q+\alpha_3\theta)}\right]^{-1}}_{G(d,q,\theta)} \biggl[d+\underbrace{\frac{a+bq-d}{\theta T}\left(\frac{1-(1+r)^{-\theta T}}{r}\right)^{\!\!-1}}_{M(L,q,\theta)}-q^2\biggr]\,, \end{equation} where 

fix one person's utility at some arbitrary level, say $U_A=\bar u$ maximize the other person's (i.e. person $B$) utility subject to the resource constraints, as well as $U_A=\bar u$ $B$'s maximum utility, call it $U_B^*$, found in step 2 should be a function of $\bar u$, and other parameters of the problem: $U_B^*(\bar u,\dots)$ lastly, you can vary the value of $\bar u$ (in the range of acceptable values for $\bar u$) to trace out the UPF. 

The calculations look correct. For the PSNE, I would just say since there is no mutual BRs in pure strategy, there is no PSNE, and leave it at that. In the MSNE, A would play $-1$ with probability $\frac{1-s}{2-s}$, $0$ with $\frac{s}{2-s}$ and $1$ with $\frac{1-s}{2-s}$; B would play $-1$ with $\frac{s}{2-s}$, $0$ with $\frac{2-3s}{2-s}$ and $1$ with $\frac{s}{2-s}$.