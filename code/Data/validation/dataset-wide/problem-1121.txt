Plain select is on the left at 332 ms, while the nolock version is on the right at 383 (51ms longer). You can also see that the two code paths differ in this way: 

I would highly recommend that you test your new T:\ drive using Crystal Disk Mark. Check out the guide from Brent Ozar here: How to Test Your Storage with CrystalDiskMark Compare the results from the T:\ drive with 

A few things could be going on here. The files are in the wrong place, or have an unexpected extension This one seems unlikely, since you've probably been using Ola's scripts for all your backups, but just in case you've changed a setting (like the directory location, or the log file extension). The "cleanup" part of Ola's scripts are looking for backups in this location (starting with the server / folder you specified in the @Directory parameter). In your question you used so I'll continue with that: 

To answer the question of will you get the same results running an analysis such as Brent's sp_Blitz scripts in a non-production environment the answer will lie in what information you are trying to gather and what that information depends on. Take the two examples below as a means of showing the difference in results (or no difference) based on the type information sought. 1) Suppose I am interested in analyzing the indexes in the database to determine if I have duplicate indexes. To do this I would look at the database metadata which would be part of the database structure itself. In this case the results should be the same regardless of environment because the data that is being queried to draw the conclusion is part of the physical database structure. Brent's sp_IndexBlitz does this as one of the steps in its analysis, among others. 2) Suppose I am interested in analyzing an index to find out if the index is utilized. To do this I would examine the DMV (sys.dm_db_index_usage_stats) to determine if the index in question has any scans, seeks, lookups or updates. Using a combination of this data I could then determine if this index is making inserts run slower or is a benefit to select performance in a way that justifies its overhead. In this case though the data will be different in results between production and the non-production environment unless the exact same workload and configuration is running in both environments. Brent's sp_IndexBlitz also performs this same check and will provide the usage details based on the settings specified. To further clarify on the "why would data be different" which seems to be a sticking point here lets dig in to what DMVs are. A DMV is at high level just an abstraction layer that provides a view of the instrumentation that SQL Server has running. With this being said as mentioned by Tony when SQL Server restarts the data that is within these DMVs is not persisted. For this reason when you perform a backup and restore it is functionally equivalent to a server restart for the purposes of the discussion around DMVs. Once the database has been decoupled from the original host of this instrumentation data the data would be lost unless it was persisted elsewhere. This is mentioned in Microsoft's documentation as shown below: 

then by all means, shrink it. Maybe there was a typo in the filesize when this database was first created (someone tacked a 0 onto the end of "37" - poof, 370 GB database file). Maybe there was a one-time data load / deletion that will never happen again that caused the DB to grow to this size. 

If you confirm it's running, and you're still not seeing results in your file, please script out the event session in SSMS and update your question with the results. Perhaps it's configured differently by mistake, or it is writing the file to a place you don't expect. 

(a in .NET classically includes the message you mentioned - "Object reference not set to an instance of an object") You should try updating to 17.6 (build 14.0.17230.0) to see if the problem is resolved there. 

If your session is not in that result set, it's not running. You can start the session up by running: 

From a reliability standpoint of deadlocks per second the perfmon counter will be more accurate. The system_health monitor is based on ring buffers which can overflow over time or miss events if there is enough activity within a brief period in order to facilitate reliability. There is a file that backs this which you can look at which will give you more history but if you are experiencing as many deadlocks as you are indicating there is a distinct possibility that you may not have full history within these. This would explain your discrepancy between the two measurements, you aren't doing anything wrong from what I can see. The files will be located in your SQL Server install directory under %Program Files%\Microsoft SQL Server\MSSQL11.[Instance Name\MSSQLSERVER]\MSSQL\Log\system_health_*.xel. 

Digging into the mechanics of this wait you have the log blocks being transmitted and hardened but recovery not completed on the remote servers. With this being the case and given that you added additional replicas it stands to reason that your HADR_SYNC_COMMIT may increase due to the increase in bandwidth requirements. In this case Aaron Bertrand is exactly correct in his comments on the question. Source: $URL$ Digging into the second part of your question about how this wait could be related to application slowdowns. This I believe is a causality issue. You are looking at your waits increasing and a recent user complaint and drawing the conclusion potentially incorrectly that the two have a relationship when this may not be the case at all. The fact that you added tempdb files and your application became more responsive to me indicates that you may have had some underlying contention issues that could have been exacerbated by the additional overhead of the implicit snapshot isolation level overhead when a database is in an availability group. This may have had little or nothing to do with your HADR_SYNC_COMMIT waits. If you wanted to test this you could utilize an extended event trace that looks at the hadr_db_commit_mgr_update_harden XEvent on your primary replica and get a baseline. Once you have your baseline you can then add your replicas back in one at a time and see how the trace changes. I would strongly encourage you to use a file that resides on a volume that does not contain any databases and set a rollover and maximum size. Please adjust the duration filter as needed to gather events that match up with your waits so that you can further troubleshoot and correlate this with any other teams that need to be involved. 

Note: to get the best help on this question, please include your actual RESTORE statement, and the specific error message that you're getting. When using In-Memory OLTP, SQL Server has to create a new folder named "xtp" in the root of the default file location for the SQL Server instance. This folder contains the DLLs for compiled stored procedures and other in-memory objects. You can find more details about that here: In-Memory OLTP files â€“what are they and how can I relocate them? If you've changed the location for your data files, you may need to update SQL Server's access to the file system there: Configure File System Permissions for Database Engine Access As a test / workaround, you could manually create the "xtp" folder, and then try the restore again. 

The plain select is on top, at 12ms, while the nolock version is on the bottom at 26ms (14ms longer). You can also see in the "When" column that the code was executed more frequently during the sample. This may be an implementation detail of nolock, but it seems to introduce quite a bit of overhead for small samples. 

Have you looked at the SQL Server Migration Assistant tool? This would probably assist you greatly in the migration as it maps source tables to destination tables despite possible naming irregularities. The tool is provided to my knowledge free of charge. $URL$ $URL$ 

By all accounts this may be a bugged behavior in the sys.stats_columns DMV. This appears to be causing problems when a statistic is updated by way of the parent index. I believe this to be due to the mechanism with which the statistics are being updated in a constraint change. Should you create a statistic manually and then wish to change the columns you must first drop and re-create which forces the meta-data to be updated in the DMV in question. In the operation you have demonstrated there appears to be a situation where the metadata is not updated under any circumstances (DBCC *, CHECKPOINT, server restart, statistics update through parent index change, etc) once the change has been made. From my initial testing I can find only one case when the metadata is properly updated which is the drop and re-create scenario. You can take a look at the Connect item on the issue and up-vote as appropriate. There is a work around query posted there but its mechanism is based on matching the index name to the statistic name and utilizing the index meta-data. 

Note: I wrote this answer before I realized this was on Azure SQL DB (David's answer seems to indicate this is legit buggy behavior in Azure). Leaving here as the OP was also looking for an explanation about what that error meant, and maybe it will be helpful for others in an on-prem setup with this error. 

Two solutions were mentioned in the comments to your question. Let's review both of them. Use a READ_ONLY cursor 

Next Steps Since it seems like the disk is reasonably fast (per the benchmarks you shared), I think it would be a good idea to log the contents of before and after the nightly batch job you mentioned. This will tell you how much I/O is happening on tempdb during that process. This is important, because maybe there really is more I/O than the disk can handle. So here's what you do: 

Here are a few possible solutions to your problem: Option 1: Don't rebuild that index That's a 30 GB index you have there. What measurable performance problem are you trying to solve by rebuilding it? Especially at 5% fragmentation, this seems like an incredibly expensive operation (in terms of system resources and locking) for very little gain. You can read some very well-founded opinions on why you might want to give up on the index rebuild here: 

Under the circumstances that you have indicated have you looked at VSS backups through a VSS provider that is either 3rd party or Microsoft based? You can perform a COPY_ONLY backup that will not break your production recovery chain and you should end up with a backup that of all of the databases that you can then recover elsewhere to within your reasonable margins. Keep in mind that a VSS backup has some of the same mechanisms and downfalls as database snapshots in that a very active database could cause a disk space issue due to the sparse files used. Take a look at the TechNet resources on the SQL Writer service here and VSS backups of SQL Server here. To do this through Windows Server Backup you will follow the wizard steps for a manual backup ensuring that you select VSS copy backup on the custom configuration settings under VSS Settings. This will allow your Windows Server backup to not interfere with any other backups taken on the server. See Windows Server Backup reference for details. 

Your transaction log is full. Looking at the screenshot you shared, it's only 59 MB. Which is pretty small. One would think it would just keep growing to accommodate more transactions. But it can't for some reason. SQL Server wants to start re-using the log file. But you've got that pesky thing happening. This means that there are modified date pages in memory that haven't been persisted to the data file on disk, and thus SQL Server can't start re-using the part of the transaction log that documents those potentially-unpersisted transactions. A temporary fix would be to run the command manually on the server, to try and force those buffered pages to be flushed to disk. You may have to run the command more than once, but this will allow that transaction log file to start being reused. The bigger problem is with your transaction log. You should run this query: 

Outside of that method, I noticed another small difference that causes the nolock version to run slower: Releasing Locks The nolock branch appears to more aggressively run the method, which you can see in this screenshot: