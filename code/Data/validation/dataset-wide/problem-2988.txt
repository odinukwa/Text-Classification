I am not familiar with Ethan Frome, but Nietzsche famously had a lot to say about the asceticism. His most developed thoughts on the matter are in On the Genealogy of Morality, specifically in Part 3, which is devoted to understanding what he calls the "ascetic ideal". 

As commenters have pointed out, your question could use a lot of clarity concerning what is meant by "an infinite" and "differentiated". Nevertheless, I will push back on one of the examples you have given, namely that of the infinite line. Yes, the line must necessarily be understood to consists of different points - if it were just one point, well, it would be a point, not a line, and would not in any usual sense of the word be understood as infinite. However, I contend that the points that constitute the line are indistinguishable, or as you seem to put it, undifferentiated. "Now," you would say, "surely the points on the line are differentiated? For we say that this point is at x=0, this one at x=1, etc." Well, we label them like such only after we choose an entirely arbitrary system of labels (namely, a coordinate system with an arbitrary point as its coordinate origin x=0). As a purely geometrical object, a line is a line even before we conceive of coordinate systems. We can understand a line without any reference to coordinate systems or equivalent notions - per Euclid, an infinite straight line is a "breadthless length...which lies evenly with the points on itself" and extends without cessation in either direction. This is also reflected in the fact that historically, the notion of a line predates the notion of a coordinate system. To explicate this further, we could pick "this" point or "that" point as the coordinate origin x=0, and the line would look exactly the same. For, since the line is straight, we could only tell the two situations apart by measuring the (signed) distance from the origin to a point that we knew was "the same" in both cases; but before we imposed the coordinate system there was nothing to label said point by, and we couldn't possibly tell that it was the same in both cases! Thus, I think that the sense of "an infinite" and "differentiated" used above, yes, an infinite can be undifferentiated. 

This is classical logic at its simplest. [EDIT: As Mozibur suggests, one might want to argue that a path is, necessarily, susceptible of being trodden. I am not sure about that: consider a path inside a flooded mine, for instance. Anyway, if paths are to be practicable of necessity, then there is a contradiction in that sentence.] 

What you have in mind is the principle of the identity of indiscernibles: the idea that no two different things can have the exact same properties. The SEP entry I've linked to above has a nice bibliography at the end. This principle entails your claim that no two things can ever be identical: if a and b are identical (i.e., if they have the same properties), then "a" and "b" are two names of the same thing. 

It is certainly possible to consciously hold contradictory beliefs, at least in the case in which one does not notice them to be contradictory. For example, many believe the truth of Goldbach's conjecture, but if the conjecture is false it is necessarily false, and contradicts the content of many other true beliefs about mathematics that those same people surely have. A more interesting question is whether one can consciously believe that p-and-not-p. Probably we should only care whether such an outright contradiction can be believed rationally: it is likely that all sorts of crazy stuff can be believed, if the believer is allowed to be irrational -- as you say, that question is best left to psychology. But, apparently, some outright contradictions can be believed rationally: dialetheists hold that some sentences are both true and false. For example, some of them claim that the liar sentence, 

Peter Singer is a preference utilitarian as he expounds in his famous book, Practical Ethics. The author of the first document you cited also defines Mr. Singer as a preference utilitarian: 

The difference is essentially the same as that which Gottlob Frege discusses in his seminal work On Sense and Reference. Putnam essentially uses the word meaning in place of Frege's sense, presumably because it seemed more intuitive for Putnam to splinter reference from meaning rather than make meaning a redundant term in our language. The distinction between meaning and reference is simple: meaning/sense is the intension or the word, name or symbolic representation of an object; the reference is the thing to which the intension corresponds in the world—the reference is the object to which you are referring, if you will. For instance, if you think of a battery, you have the word battery and you have symbols for batteries, such as: 

Preamble Your fears are regrettably well-founded. Unlike mathematical logic, the philosophy of language is not so clear cut and remains to this day a deeply controversial topic. There are a dozen schools of thought on the philosophy of language, both classical and contemporary. It is a fundamental and unquestionably significant field of philosophical inquiry, but unfortunately what appears as a stream of simple concepts that should be amenable to science soon reveals itself as an inconsistent web of increasingly intricate questions and facts rapidly give way to opinions. The philosophy of language divided, and continues to divide, the analytic philosophers—who are known for their predilection for rigour, valid method and proof. While I would love to hand you a shiny Cambridge or Springer text in the philosophy of language, I am afraid there is no definitive work. There are commentaries and historical discussions but these will not suffice at all. What I recommend instead is this: if you are sufficiently intrigued and if you have the time to spare, read through the texts I have listed below, which come straight from the field's most significant philosophers themselves. For if it is not possible to find an objective and rigorous account of something, one must build an internal consensus for themselves, generating an objective account from a rigorous treatment of the intersubjective account. Some essential texts Gottlob Frege - On Sense and Reference 

Your teacher is wrong. Firstly, note that the presence or absence of a problem has nothing whatsoever to do with the MWI - what is being described could happen in this universe, without recourse to the MWI, it would just be astronomically improbable. However, if this series of events were truly a paradox, it would have to be strictly impossible, and so the problem would be present or absent whether the series of events is astronomically improbable as is the case without MWI or whether it the series of events is virtually certain to occur as is the case when we consider all possible worlds in the multiverse in MWI. Thus, we can entirely disentangle the MWI from this question. Now, the resolution of the apparent paradox is hinted at in the question you ask in point 2). An information paradox occurs if we can reliably communicate information back in time. By assumption, the series of events we describe doesn't constitute that - it occurs purely by random chance. To see this more clearly, suppose instead that in the year 2000 I walked into a door (could be a door to a funky contraption that looks like a time machine, or it could just be a regular door to a library) carrying the complete works of Shakespeare (I don't even dematerialise as I walk through, I continue and exit on the other side just fine), and in the year 1999 I walked out of a door carrying the complete works of Shakespeare (once again I had walked in normally, no magic materialisation happening here either). Has an information paradox occurred in this scenario? Have I transmitted the works of Shakespeare back in time? "Of course not," you say, "but the situations are fundamentally different - the alleged time-traveller I described dematerialised in the future, and materialised in the past." But, so what? The dematerialisation/materialisation events are far more improbable than the walking in/walking out of a door events, but they are not fundamentally any different - both are due to random fluctuations and in both cases the works of Shakespeare were present at a future time and present at a past time, one is just far more likely to occur than the other. But clearly, in the second case we would not suggest that any information has been transmitted back in time. The apparent contradiction stems from our thinking that just because certain random fluctuations are possible in our universe they are properly "caused" by the laws of our universe. That is not to say that events in a universe with probabilistic laws are totally randomly caused - we have compelling evidence that they follow particular, albeit probabilistic, laws. Rather, you can think of a series events having two causal elements - reliable, definite, physical laws, which determine a probability distribution of series of possible events, and then a random element that selects from this distribution. For information to be transmitted reliably from event A to event B, event A must through the physical laws affect the probability distribution element of B's cause in a way that makes it highly likely for that information to emerge at event B. This is clearly not what is happening in either case above. Event A - which in the first case is the dematerialisation of the alleged time-traveller in the future with the complete works of Shakespeare, and in the second case is me walking in to a door with the complete works of Shakespeare - has, as per the physical laws of our universe (relativity, quantum mechanics, etc.), no bearing whatsoever on the probability distribution element of the cause of event B - which in the first case is the materialisation of the alleged time-traveller in the past with the complete works of Shakespeare, and in the second case is me walking out of a door with the complete works of Shakespeare. These just happen for unrelated reasons - in the first case, sheer random chance causes the highly improbable materialisation event to occur, and in the second case some unrelated set of causes (having to do with me being a perennial bookworm) lying strictly in the past of event B shape the probability distribution, making it likely for me to walk out of the door with the complete works of Shakespeare. In either case, no information is transferred from event A to event B, and there is no information paradox. 

Well, such sentences will be true for all precisifications, because either Andy has n hairs or he doesn't, for all n. Therefore, the sentence comes out supertrue -- this is the supervaluationist for accepting it as true. Its negation ("it's not the case that Andy is bald or Andy is not bald"), by the same token, comes out superfalse. The same will happen with every other vague sentence: the supervaluationist semantics validates LEM. Supervaluationism is a semantics that validates LEM but not PB. 

appropiate. In the philosophy I am familiar with, the debate is cast as being about the norm of assertion: The quest for the norm of assertion is the quest for a norm (roughly, an imperative) such that a speech act counts as an assertion if and only if it is subject to said norm. Candidate norms of assertion take the form Norm Schema: : Assert that p only if F(p) where F(p) is a function that takes the proposition to be asserted and outputs a specification of the circumstances in which such an assertion is warranted. There are various contenders for the role of F(p) . Let me discuss them briefly in turn. Truth: : F(p) takes p to itself. That is: one should assert only what is true (see Weiner 2005, 2007.) This looks like a natural demand on assertion; after all, assertions aim at the truth -- that is, it is very likely that they fulfill their function in language by mapping onto facts (see Millikan 1984, p. 108f). The standard complaint against Truth is that it is far too weak to be the central norm of assertion. For what if one truly asserts that p on the basis of extremely poor evidence, e.g., one asserts a lucky guess or a correctly believed product of wishful thinking? Surely, one may claim, the asserter in these instances would be subject to criticism despite satisfying Truth. (Lackey 2007, p. 604) The obvious fix to the this problem is to demand that the asserter be justified in believing the asserted proposition. Thus, the justification norm, Justification: : F(p) takes p to the asserter is justified in believing that p. This is sometimes put in terms of the asserter having a reasonable/rational belief in p. Justification is also plausible: it seems reasonable to assume that the asserter has to be in the right kind of evidential relation to the proposition asserted -- and, indeed, we find assertions blameless when they are done in these circumstances. On the other hand, there are cases of objectionable assertions made in the appropriate evidential state. Lotteries seem to supply such examples: if I assert 'Your ticket did not win' merely because I know that only one in a milion does, I may have extremely good evidence -- one that makes you ticket not winning overwhelmingly likely, and my belief that it won't rational and reasonable -- but I still contravene the norm of assertion. Williamson summarises the idea: 

and yet you also have the battery as a physical object, as an actual battery in the world, an extension. You have that which you mean and your ways of meaning it. The referent and the symbol to refer to it. An interesting parallel between analytic and continental philosophy is Frege's distinction between sense and reference, which is also acknowledged in semiotics as the distinction between signifier and signified. Putnam differs from Frege in where meaning and reference sit. For Frege and others like him, meaning is a social, communicable sign/symbol and reference is in the world. For Putnam, both meaning and reference are in the world, because reference influences the formation of the symbols and names we use for meaning. Putnam sought to prove his argument with his famous Twin Earth thought experiment. 

This indicates already that to be any of these things, you need to think for yourself and you need to think in a certain way. Your thinking needs to become structured, not to preclude novel, out of the box approaches to problems but to enable you to understand what you are learning in your own terms, rather than simply memorising somebody else's terms. The Socratic Method is ideal because, as I said, as an instructor you will appeal to your students to think about how to solve a problem and the only way they will arrive at the correct answer is to think like a philosopher, computer scientist or mathematician. The further advantage with the Socratic Method is that you will require your students to justify their answer, so even if they happen on the correct answer or a strong thesis, they have to understand why it is correct or strong in order to be right. Thus when a student gives an incorrect answer, you reply with an explanation of the flaws in their logic and they will have a blueprint for how to improve; when a student answers correctly, they are made aware of the fact that their thinking was correct and that they should refine and apply their way of thinking to future questions. In epistemological terms, you can view the Socratic Method as a way of embedding procedural knowledge in your students, rather than making them memorise propositional knowledge. In computer science terms, you are giving students imperative knowledge rather than declarative knowledge. You are not just feeding them definitions and rules from which they would be lucky to intuit the abstract nature of the subject at hand; you are not even just giving them a recipe or an instruction set leading to the correct answer for them to analyse. You are making the students think about how to formulate such a recipe and inviting them to challenge rules, to play Devil's advocate, all for the purpose of being wrong. Obviously some people possess natural talents, truly intuitive grasps of subjects like mathematics, but the Socratic Method appeals to all students. It refines the students who are already adept and it brings the uninitiated up to speed by forcing them to make mistakes. If we do not grasp something intuitively, we learn by trial and error, and what better way to force trial and error than to ask your students to give answers before being comprehensively shown how to answer? They will make mistakes and they will learn from them. Once you have students thinking for themselves, you can expect for your students to eventually challenge you—the definition of a great teacher according to Aristotle: 

This means that, in order to apply negation to the different truth values, you apply it (classically) to each of its components: 

is meaningful under the following interpretation: would be a plural constant designating a certain plurality of kids; a plural constant designating a certain plurality of plants, and designating the relation of shouting as above. Under this interpretation would be as well formed as , when used to mean that Socrates is a man. For the notions of plural constant and plurality see this. 

So, sure enough, (4) is true: it's a material conditional with a false antecedent; nothing in s not being the case can make it false that if s then r. The puzzlement over this line of the truth table of the material conditional is recurrent at Philosophy SE. Look around and you'll find other related questions. 

is that such a statement is as hard to falsify as the original statement is to verify -- and, conversely, as easy (!) to verify as the original statement is to falsify. This point is not new, of course; it was already made by Hempel back in the day. 

It's not a logical fallacy. It's faulty inductive reasoning: as you rightly say, they infer a trend from a finite number of cases in an unwarranted manner. For that matter, there are many cases in which three fails do not warrant the inference that the person will always fail (consider learning to ride a bicycle). 

The Socratic Method is the best way to teach any person because it requires that the pupil make attempts to solve the problems before being comprehensively shown how to do so. Philosophy is a prime example, but it applies to everything, even mathematics or computer programming. In philosophy, mathematics, computer programming, lecturers often say something like: 

The author is trying to argue that there is a fundamental division between act and rule utilitarians, and that these utilitarians are then further divided into flavours, such as preference and hedonistic: 

Foucault has clearly moved away from a phenomenological perspective if he now concerns himself with the meaning of a sign or a fact such as madness, and how that sign is defined by cultural and political agendas, rather than possessing an objective definition. Concurrently Foucault addresses the impossibility of an objective definition for such a term that is not intrinsically biased. 

The sentence in the Wikipedia article prior to the one you cited gives a rough idea of how Foucault moves towards to structuralism. 

I can imagine that it may often be difficult for responses on philosophy.stackexchange to be correctly called 'answers' since there must always be room for interpretation and evaluation in philosophical matters, but allow me to give you my views on your question and from it perhaps you will be able to discover an answer for yourself if not for everyone. Continental philosophy has profoundly—albeit misguidedly—influenced art, literature and film throughout the world. Analytic philosophy has inspired developments in mathematics and computer science, shaped the scientific method, and informed quantitative approaches to psychology and sociology. Both branches have elicited developments in linguistics. What continental philosophy has not done is continue developing in its original spirit; even since Sartre, continental philosophy has rather begun to regress or become blended with analytic flavours, with such notions as embodiment blending ideas from Kant, Husserl, Heidegger, Merleau-Ponty and others. Conversely, analytic philosophy has continued developing in the same way but in increasingly insignificant increments as it gradually cedes much of its original lines of inquiry to the sciences. This is the primary distinction between the two branches of philosophy: analytic philosophy walks hand in hand with science and scientism; continental philosophy focuses on the poetic and metaphysical understanding of the world. Analytic philosophy divides the world into subject and object; continental philosophy seeks to unite observer and observed to reach a fundamental and unbounded picture of both. Analytic philosophy seeks to simplify philosophy through demarcation, whilst continental philosophy is concerned with questions that are intuitively more fundamental even than science or logic. Analytic philosophy might be said to have had the more profound influence in contemporary academic circles, but what does that actually mean? Analytic philosophy has no purpose, because its goals are either impossible or fall to scientific endeavours to complete. Continental philosophy has arguably seen less academic attention (or at least less academic understanding/improvement) but the ideas of phenomenology, existentialism and deconstruction have disseminated globally and inspired vast amounts of creative work—though often the ideas are distorted or misinterpreted in such works, as is usually the case when an academic concept reaches the populace. Judging which branch of contemporary philosophy has had the more profound influence will depend on what you personally think is more profound: the sciences or the humanities. Or perhaps not, if you think as I think: that analytic philosophy is not philosophy at all, but rather an auditing process for scientific enterprise. Continental philosophy represents, to me, a truer philosophy, especially through Heidegger and the break with the Cartesian tradition of subject/object opposition and through Derrida's subsequent refinement of destruktion as deconstruction. Within the academic discipline of philosophy itself, it is difficult to judge which branch had the more profound impact. Philosophy finds itself in troubled water these days, unsure of whether it is a science or an art; you will find academics undertaking scientific studies of a psychological/neurological character calling it philosophy, and likewise you will find people still agonising over Aristotle, Chrysippus, Kant, Hegel, Husserl, and so on—and as Derrida certainly demonstrated, no philosophers will truly lose their significance, because their ideas were defined and remain defined by their opposition to each other and to the times, and that these definitions are traces of a genealogy of meaning that will not end. We might say that Continental philosophy affirms philosophy as a subject and gives it life, paves the way for future study and creation. In contrast, analytic philosophy might be said to be less constructive, narrowing philosophy down according to scientific standards over which philosophy ought to preside. I do not mean to say that analytic philosophy is only logical positivism, but analytic philosophers do not do what philosophers must by definition do: question themselves and their methods. 

The less natural cases in your examples are allowed, as is any case involving any sentences whatsoever as long as they have a truth value. This is a feature, not a bug: logic is supposed to be fully general. Having said that, typically, pragmatic (as opposed to semantic) considerations will guide our judgments about whatever a speaker means to say when they utter a sentence involving conjunctions or implications. 

Nothing paradoxical here, is there? Maybe the only enduring and unchanging Tao is one that people is prevented from trodding -- so as to avoid wear from all that trodding, and stay unchanged, say. Suppose we introduce the following notation: T: stands for the property of being a Tao EU: stands for the property of being enduring and unchanging R: stands for the property of being susceptible of being trodden then your sentence can be rendered thus: 

Therefore A. EDIT: I see by your comment below that you are interested in question-begging arguments in general. 

The right way to understand Mandelbrot's claim is: the coast of England can be profitably modelled as a fractal (for some purposes). It's exactly the same kind of claim as "The granma pie at Luigi's is a rectangle". So, no: the coast of England is not literally a fractal, and it's not in the world of forms -- it's actually in England :)