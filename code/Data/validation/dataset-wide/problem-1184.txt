Conversely if the values are not too long (say a few hundred bytes) and accessed in most queries you may see a performance improvement by storing them in-row and avoiding the pointer lookup and secondary IO. This can be controlled through sp_tableoption. 1 Not that you ever would, of course. 

Then instead of having hundreds of columns there will be thousands, or even millions, of rows. That's fine. RDBMS is designed to handled that many without concern. 

The round trip time from the client to the server per query starts to add up with your approach. Even if it is only 1ms per query, you are burning almost half a second for no gain. Combine the statements into one. At the very least submit them as a single batch and process the individual results sets in the application. 

I'm going to use MS SQL Server to do this since I have a copy to hand. I believe most any of the majors would do it similarly. First a sample table, with data. I use a table variable but it's the same for any flavour of table. 

I will use pseudo code to illustrate; you can adapt to your DBMS's specific syntax. Join to on and also on the overlapping intervals i.e. 

There's a good overview of #Tables and @Tables here. Both are written to TempDB. Both will incur IO costs. #Tables support indexes and statistics on non-key columns. If you have 9,000 rows this could make a significant difference to the query plan and hence performance. A #Table is only visible in the scope where it was created and is cleaned up by the system once out of scope (garbage collected, in effect). This allows concurrent or consecutive executions of the script without fear of one execution mixing data with another execution. Your real alternative is a "proper" table in the database used as a working space. You will write your 9,000 rows here and remove them once your process has finished. The problem is you will have to code garbage collection and data isolation yourself. @Tables' data survives a ROLLBACK which #Table and "proper" tables do not. If your 9,000 rows are logging or audit information that could be a useful feature. Be aware of #Table caching, however, the problems it may cause and the work-arounds. 

If your DBs reside on a SAN or other shared storage that platform may have logs you can reference. For local storage I know of nothing held by default. A couple of years ago we started tracking DB size and disk free/used space. It has proved very valuable. A couple of Powershell scripts, a scheduled task and a small table has helped us avoid several incidents. Eventually we had enough data to make Monte Carlo prediction workable. 

In the above replace with whatever Access delivers from your search field when you don't type anything there explicitly. To force the user to enter at least one search field add another AND to the query (again, pseudocode): 

For your use case of key-value data, where the value is simple, MongoDB is more than you need. It is a document-oriented data store for complex value types. A specific key-value store would suit your needs. RIAK is the one I've looked at though several others exist. Since your retrieval will be by time range i.e. key range, Elastic's full-text capabilities will, again, be overhead for which you have no payback. You don't say what your scale out requirement is, whether local write and global read or full replication of the whole data set etc. Several relational engines offer distributed features. MariaDB has Galera, for example. SQL Server has distributed, partitioned views. Other popular systems can afford similar capabilities. NeoDB is relational and multi-host by design. The relational model is based on theoretically sound principles and has decades of industry development. You would be doing yourself a disservice by discounting it out of hand. And your data does have structure, even if it is a simple structure. You have a primary key (device & time) and a functionally dependent attribte. Whether you define this declaratively in a schema or implicitly in your program's data structures, it still exists. 

Generally it is best to hold data in a normalised structure. If testing at scale, on production-grade hardware, proves there is a problem, and that the problem is because of an over-large table, and index tuning and SQL re-factoring do not bring the problem withing acceptable bounds, then splitting the table is a recognised technique. Moving some rows to a different table is called horizontal partitioning or sharding. Moving some columns to a different table is call vertical partitioning. Both entail additional design and maintenance work. There will be run-time implications if values from several partitions are required by a single query. There is some further discussion at this answer. 

I calculate 3 as number of columns in source table #t + 1 for the injected dummy column - 1 for ID, which is not UNPIVOTED This value could be obtained at runtime by examining the catalog tables. The original rows can be retrieved by joining to the results. If values other than NULL are to be investigated they can be included in a where clause: 

The second is to add a flag to Orders to show which is the latest for each customer, and add constraints and indexes appropriately. This is not very different for the current solution, either in normalisation purity or performance characteristics. 

If there is a single SQL statement of moderate size (so it fits in a single network packet) then executing that statement from the application or packaging it as an SP and executing the SP will be identical network-wise. However, a pattern I often see is that an application function requires several SQL statements to produce a result. Let's say we are writing an order to the DB. First we validate the customer (1) then the product code (2) then the inventory level (3) then write the order header (4) returning the ID which is used as a foreign key in the order line (5). If each call is made separately from the appllication this is five round trips to the DB server. If the values are passed as parameters to an SP it is a single request with a single response - one round trip. Because of this I would argue that packaging the SQL-intensive part of business functions as storded procedures does indeed reduce the network load in typical scenarios. 

Job steps' output can be directed to an output file (Step -> Properties -> Advanced: Output file). This field accepts tokens so it is relatively easy to deploy a common script to every instance yet generate unique file names from each. Direct the output to a common fileshare to which all SQL accounts have write access. Then use a second process to read the file, process the content and clean up. I have several monitoring tasks using this method. 

Now, assuming your optimizer is at least a little bit clever, you will not need to change your application code at all. 

1 SQL Server 2017 has implemented graph processing with precisely this form. It has added extensions to identify a table as containing nodes or edges. 

Do you have access to the execution plan? This will show how Oracle resolves view columns to base indexes or tables. You can create simple, dummy queries to focus on the particular items in which you're interested. 

A row here records the movement of a given quantity of a particular product from one recipient to another. You know the participant and the product from the corresponding account. From the product you can determine if the Quantity represents a kilogram or an item. For ease of processing you'll probably want to duplicate this under each account. This will need another table: 

(My emphasis to show processing continues up the tree toward the root, not down toward leaves.) So we must put a pointer to the new page (B) into the father of the current page (A). There must be a new root node: 

With some clever jiggery-pokery with partitioning and filegroups you could get a particular sub-set of your data onto just one of the physical files used by a particular database. Trying to attach that into an existing database at the recipient would be "challengining", I think. In 20-some years of doing this stuff I've not heard of anyone trying that.* My suggestions would be a) copy your DB, delete the stuff you don't want to pass on, send a backup of that to the recipient b) use BCP with the queryout option to dump just the required data to external files. Send those to the recipient. *I'm always willing to learn, however. 

For each name only those tasks which are in the new table will be returned and multiple tasks can be returned per name. 

The simple answer is don't use Entity Framework for this sort of thing. You need to write a SQL migration script, test it, and then deploy it onto your production system. And yes - changing a table with 3M rows will take more time and resources than migrating a table with 3 rows. There are a few techniques for achieving this. First, judging by the error message, you're trying to do this in a single transaction. Better to add a new column of the correct precision, copy values to it in batches of, say, 50,000 at a time, then drop the old column and rename the new one. If the system has to remain operational during the migration you can get creative with views and triggers. 

Name your relationships. For example the line from Airport to Airline - is that "based out of", "offers service to" or "banned from landing in"? Any or all may be valid in your problem space but without good names the subtle differences will not become apparent. And be honest with yourself. Using names like "has" and "relates to" is a cop out. You relate an Airport to one City. What about Dallas/Fort Worth? Have a think about the output you will want from this system. Work through the corresponding queries in your mind and verify the ERD satisfies them. If not, you have to refine your rules and model them appropriately. 

If lock de-escalation occurred I would expect complete symmetry in the two trace files - every lock acquired during UPDATE, and the escalation, would have a corresponding release during ROLLBACK. What I observed: 

For the given user and time find the most recent activity which is a 46 or a 47. If it is 46 the user is "automatic". If it is 47 the user is not "automatic". There will be users who do not have any 46 or 47 records. These will return and empty resultset and the application will have to interpret these appropriately. If activities can arrive so quickly that the 46 and 47 rows both have the same values this won't work. You will have to try a different approach, increase the resolution of or use a sequential integer column as a proxy. 

To normalise this table you must remove the repeating groups and multi-value columns. The resulting table will be 

My question - how and when does the optimizer collect these statistics? Inside the SQL Server relational engine statistics can be created implicitly or explicitly, they age as writes occur, and can be refreshed explicitly or during table maintenance. Are there similar strategies employed in PolyBase? Given the data may be loaded into Hadoop/ Azure blob without SQL Server seeing it on the way in, and data volumes will be large (most likely) run-time ad hoc statistic creation through sampling or similar seems an unlikely strategy to me. 

What you describe is the default behaviour of SQL Server's query execution and client connection protocols. Rows are delivered to the server's network buffer as they are produced from an executing query plan. The network delivers them to the client, which makes them available to the application. I think the pause you see is caused by how the application consumes the dataset returned. It's been a while, but I think SqlDataReader.Read() allows stream-like behaviour. The risk is that the application falls behind SQL Server and the connection and server resources remain locked up until the application explicitly releases them. 

Perhaps you will need to record who ordered the test and when. You can add to this table as required. Knowing the we can use to find out what is to be measured. Then we need somewhere to hold the actual measurements. This will depend on the component and the sample. 

My understanding is that functions, such as , are executed once per column reference to in SQL. Thus any query which scans the whole table will produce the same "random" value for every row. I think you're going to have to iterate through your data setting random numbers one at a time, RBAR-stylie. 

One difficult bit is that all columns are compared, so if further values are required for subsequent processing this fragment has to be in a sub-select with an additional JOIN. As you may guess this can lead to many table operations and performance difficulties. I add this for information only. My choice would be the NOT EXISTS form. 

You can have it both ways with partitioned views. You create an underlying table for each status, enforced by constraints, with mutually exclusive values. Then a view which UNIONs together the underlying tables. The view or each base table can be referenced explicitly. If a row's status is UPDATEd through the view the DBMS will DELETE it from one base table and insert it into the one corresponding to the new status. Each base table can be indexed independently according to its usage pattern. The optimiser will resolve index references to a single corresponding base table if it can. The benefits are a) shallower indexes. Do the math on the index fan-out, however. At that scale and split between your status values it is possible the indexes will be the same depth on the split tables as they would be on the combined table. b) no application code has to change. The data continues to appear as a continuous whole. c) future new status values can be included by adding a new base table, with constraint, and re-creating the view. The cost is all that data movement; two pages and associated indexes are written for each status update. Lots of IO to deal with. That much movement will cause fragmentation, too. 

The will be the maximum number of rows you want to show in a single page in the UI. While it is unlikely they will all come from a single shard it is possible so that's what the limit must be. If your version of Oracle does not support the final limiting clause the following will still work, but will be much less efficient. Perform a multi-way merge on the resultsets. Stop when you reach the limit for one page in the UI. Discard the remainder of the resultsets. When the user pages forward, re-submit the SELECT using standard paging techniques. When the user chooses a different sort sequence amend the SQL to match and resubmit to each shard. My preference would be to implement this server side in a stored procedure. This will keep application and storage as separate as possible. Also, rather than a multi-way merge, the various resultsets can all go into a single working table which is then sorted and the correct number of rows returned to the application. If you do implement it client-side, rather than discarding the unused portion of the resultsets they could be cached. This which may reduce the load for subsequent pages at the cost of additional complexity. 

It sounds like your architecture has gone past partitioning and is sharded. If you have system software or an application framework in place which can handle sharding that is where this requirement should be implemented. In order to get the correct sort sequence you will have to have the results from all involved shards in the same place at the same time. There is no way to get some results from one shard and then get more results from another shard and be able to guarantee any sort sequence the user may choose. One way (as BriteSponge suggested) would be to define a view which spans all the shards. Let the DBMS deal with the complexities of marshalling the various shards' results sets. This would be my preferred solution if it can be managed. It separates the application from the DB implementation details. If you have to write this yourself there is some hope in the fact you have paging in place. This limits the maximum number or rows you will show in the UI and, therefore, the maximum number of rows that have to be returned from each shard. From the date range you can determine which shards are required. To each shard you submit a query of the form