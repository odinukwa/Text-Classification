The Chinese room reacts just to syntax, or shape of symbols (is purely syntactic). But brains are full of structure. In the room, Chinese symbols sit scattered in "piles" on the floor or are moved around in "batches" or "bunches", or are stored jumbled up in "baskets" with no structural connections between the symbols. The things computers process are called "symbols". Computers can build structure between symbols and react to, or follow, it, and often do. Virtual connections between memory locations can be established using pointers, and algorithms can follow the connections using the methods of direct memory addressing and indirection. This structural, or relational, ability of the computer program can be mirrored in the Chinese room by adding to the room's ontology a new object type: string. Instances of string in the room can then connect tokenised Chinese symbols. Every piece of string has the same characteristics including length. They are the embodiment of structure, are relational elements of structure. In the room, if the connections established between symbols are a causal consequent of temporal contiguity at the sensory surface resulting in contiguous sensory symbols exiting the sensor then entering the room, the connections between the sensory symbols record as internal structure the external instances of temporal contiguity at the sensory surface. Is such an internal structure an element of semantic content? In the computer, if the internal memory structures built with pointers are trees, a program can walk the trees and emit as output copies of the leaves (symbols), without reacting to (identifying) the shapes of the symbols. The program merely copies and emits whatever it arrives at that has no children. The program contains no conditionals indexed on symbol shape. Suppose Searle is blindfolded then walks a tree by following the string with his hands. When he arrives at a leaf (a card inscribed with a Chinese ideogram which card has no downward strings attached) he emits the card then continues on his tactile tree walk. Since the rules he is following do not instruct a reaction to the shape of any Chinese symbol (and hence do not contain an example or description of any Chinese symbol shape), does this mean the program in the rule book is non-syntactic with respect to Chinese symbols, and Searle manipulates the symbols non-syntactically? In 2014, Searle says (his emphasis): "...a digital computer is a syntactical machine. It manipulates symbols and does nothing else" ("What Your computer Can't Know", in The New York Review of Books, October 9, 2014, section 2, para 7). String is not symbols. Is his careful avoidance of structure his fundamental mistake? 

Today's electronic digital computers are often referred to as universal Turing machines. That is, the concept of the UTM is used to understand today's stored-program electronic digital computers. But is this concept adequate? In fact can today's computers do things that UTM's can't do? If they can do more than a UTM can do, it could be important to AI, since AI (and Searle and his Chinese room argument) use the UTM concept to define the abilities of AI's research and development platform - the electronic digital computer. 

I've spent quite a while studying Turing's 1950 paper "Computing Machinery and Intelligence", regarded by many as the mission statement of AI, and one part of this paper has always seemed completely mysterious. Why did Turing so eagerly promote the idea of E.S.P. in a paper about programming a computer with the computation of intelligence? 

When studying AI, computationalism was always referred to as a theory, a theory of mind, the theory that the mind is an executing computation. But is it really a theory? How could it be disproved or disconfirmed? If a certain computation (program, algorithm) turns out to be unintelligent, that just means the right computation has yet to be discovered. The idea that the mind is a computation isn't challenged. What experiment could disconfirm the computational mind? The theory of phlogiston says combustible material contains a substance, phlogiston, that leaves during burning, hence the lesser weight of the ash, etc. This idea could be disconfirmed – and was: magnesium ash weighed more than the unburned metal. What empirical test could disconfirm computationalism? If there's no such experiment, can computationalism properly be called a scientific theory? If not, can the research project based on it – AI – be a scientific project? 

OK for Phil-132, I'm not sure I understand all the points, but this is what I think's going on in the Chinese Room. There are two separate symbol-processing systems: (1) the system that processes the Chinese symbols that enter the room from outside, and (2) the system that processes the symbols that compose the program. The room also have a set of basic operations (like the Turing machine's Scan, Print, Erase, Left and Right). In an actual computer, these are built into the hardware. In the Chinese room, they come from Searle understanding what the program symbols mean. Program symbols define the sequence of these simple operations once the program begins to execute. The simple operations then manipulate the Chinese symbols (and do other things). The program can be replaced by wiring. In this case the sequence of simple operations are not defined by program symbols, but by the wiring. But the "program-as-wiring" still has to be able to treat different Chinese input symbols in different ways. So how is it going to do this? Somehow the shapes of all possible Chinese symbols have to be built into the wiring. But the only way to do this is for the wiring to contain examples of Chinese symbols (which examples can then be matched by the wiring against the ones that come into the room from outside). So the wiring has to contain symbols. Even though the simple operations can be executed by wiring instead of program symbols, there must still be literals, there must still be examples of all possible Chinese symbols, that the simple operations can use to match against the actual Chinese ones that arrive as input. This matching process conducted by the wiring is purely formal because all it does is compare the shapes of the pre-existing example Chinese symbols to the shapes of the ones that enter the room as input. "Getting rid of the program" only gets rid of the program symbols that trigger simple operations. It doesn't get rid of the literals contained in the program. So for the program line: IF INPUT = "A" THEN GO DO , getting rid of the program gets rid of: IF INPUT = "" THEN GO DO , but it doesn't get rid of A. 

I just can't see how John Searle's Chinese room makes sense. The room passes the Turing test. People outside the room think there's a human inside who understands Chinese. But, Searle explains, the room actually contains, in analogical form, all the essential elements of a electronic digital computer programmed to understand (according to Strong AI) written Chinese. But the monolingual English-speaking man in the room (the computer CPU) understands no Chinese. Cards inscribed with Chinese symbols fall into the room through a slot in the door. These are sensible Chinese questions. The rule book (the program) deals only with their shapes, not their meanings. It instructs the man to find certain Chinese characters among the spares in the room then to push them out through the slot. Unknown to the man, these are sensible Chinese answers. Neither the man nor the room understands the meanings of the shapes, since all they have is the shapes. From here Searle goes on to argue that computers will never understand language or the world. What seems to me like a fundamental mistake is that Searle bases his argument on comparing a computer receiving Chinese symbols with a human receiving Chinese symbols. Then from the fact that the computer doesn't understand the meanings of the symbols, Searle argues that computers could never understand anything. Well, humans can't understand the meanings of the symbols either. Humans first have to learn Chinese. Why doesn't the room try to learn Chinese? Without this, Searle's argument is pointless. Leaning Chinese entails developing memory structure. There's no structure in the Chinese room because there is nothing in the room to build it out of. The room's ontology needs structural elements added to it so that it then contains atoms of structure as well as symbols (the content of structure). Then the program can instruct the man to build memory structure. Digital computers can easily build memory structure and often do. Now with structural elements, the Chinese room can try to learn Chinese. And by the way, the CRA is unsound because Searle's premiss "... a digital computer is a syntactical machine. It manipulates symbols and does noting else" (John Searle, 2014, "What Your Computer Can't Know", in The New York Review of Books, October 9, 2014) is false. Also, it can be well argued that some structural elements are semantic. It would be really great to get some comments and criticisms of my above reply to the Chinese room argument. 

Searle says syntax is neither sufficient for nor constitutive of semantics, all a computer gets (eg from sensors) is syntax (tokenised shapes) therefore computers will never understand the world. Searle: "There is no way to get from syntax to semantics" (Minds, Brains and Science, p34); "digital computers insofar as they are computers have, by definition, a syntax alone" (ibid); "semantics is not intrinsic to syntax" (Mystery of Consciousness, p17). The premiss that semantics is not intrinsic to syntax to me crystallizes the powerful appeal of the Chinese room argument. And all a computer gets according to Searle is syntax: in the Chinese room, this is the shapes of the Chinese ideograms that drop from the slot in the door and the simple manipulations based solely on symbol shape. Strangely, Searle never discusses relationships between symbols. In his 1990 Scientific American article he calls a basket of symbols a "database". Elsewhere he calls input symbols "bunches". He never talks about relations between symbols. A database is symbols related together. An input stream is a temporal sequence, also symbols related - related by their adjacency in time as they enter the machine. So sure, shape is syntax, but what about relations between tokenised shapes? Computers get the relations as well as the shapes. Why does Searle never discuss these relations? If they were part of syntax, surely he would have said so, but he hasn't. So they must be separate from syntax. A token has a shape, syntax, but the relations between tokens are a different matter. Computers get the relations as well as the tokens related. Computers can react purely to temporal contiguity between input symbols and ignore the syntax, eg alternate input symbols can be stored in alternate locations. The shape, the syntax, of the symbol is irrelevant. So Searle's premiss "digital computers insofar as they are computers have, by definition, a syntax alone" must be false and, hence the Chinese room argument is unsound. Is this a rebuttal of the CRA? Since there are symbols and relations between them inside the machine, we can grant that syntax is neither sufficient for nor constitutive of semantics, but point out that there is more than syntax inside the machine. Syntax could be a component. True, syntax alone can't yield semantics, semantics is not intrinsic to syntax, but semantics could be a compound of syntax plus relations, and digital computers might think.