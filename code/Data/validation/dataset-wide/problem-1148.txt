you need to turn on archive_log the log show you there is no "cp: cannot stat '/home/duc/backup/database/postgre_backupdb/000000020000000C00000042': No such file or directory" have no that file it mean database couldn't make checkpoint to archive it. use some tool to do backup such as PG_RMNA (ex: pg_rman restore --recovery-target-time '2017-06-16 9:00:00' -B /BACKUP/pgrman -D $PGDATA -U postgres) 

The structure of MyISAM and InnoDB are not the same: MyISAM When do dml(ex:insert) it's just store in memory and then push it after that.(bad when server crush) InnoDB need to syncs that data to disk until finished it cost the disk I/O. Hint 

I have a transactional replication setup between the two servers and I noticed that if a run a statement similar to this: UPDATE mytable SET mycolumn = mycolumn the replication somehow knows to ignore this transaction and it does not get applied on the subscriber. I have confirmed it by running SQL Profiler and also by adding TIMESTAMP column to my subscriber table (it does not change). I suspect there is some sort of mechanism, which enables this kind of "smart" behavior and I was wondering if anybody could shed some light on it. Thank you! 

innoDB is bad for dml statement, #MyISAM is better. Innodb is try to commit all INSERTing data to disk and returns success after it pined. when you need a big insert data you can avoid auto-commit=0 to get faster. or you can check this for hint. 

When i tried to install percona-xtrabackup on Master server db yum install percona-xtrabackup-24 I got this error: file /etc/my.cnf from install of Percona-Server-shared-51-5.1.73-rel14.12.625.rhel6.x86_64 conflicts with file from package mysql-community-server-5.7.18-1.el6.x86_64 How to solve this problem? -CentOS 6.5 2.6.32-431.el6.x86_64 

Initially everything seems to be ok and the results indicate that all of the tables match. Next I go over to the subscriber and manually delete a few rows from some of the tables. I manually verify that the row counts in my tables are now different between the publisher and the subscriber. Finally I run the sp_publication_validation procedure again and .... it says that everything is still OK. This is wrong! I also tried to return both rowcnt and checksum and it still doesn't detect the fact that there are differences between the publisher and subscriber. I appreciate any ideas. Thank you! 

lock session bad query for inserting (if you use select sub-query to get data to inserted) you mentioned "postgres: wal writer process reached 70%", postgresql make sure your data that you're inserting is committed to disk so that why it takes the time. try to set you session to AUTOCOMMITE to off. 

According to Postgresql doc there is no option to set temporary password or allow to change the password when the first login. 

Obviously it's still looking for the path specified earlier, but why? Again, I set "Delay Validation" = True on all my connection managers and the package itself. I appreciate any help on this. Thank you! P.S. Here is a complete expression for log path: @[User::Log_Path] + "\\" + @[System::PackageName] + "_" + (DT_STR, 4, 1252)DATEPART("yyyy", @[System::ContainerStartTime]) + RIGHT("0" + (DT_STR, 2, 1252)DATEPART("mm", @[System::ContainerStartTime]), 2) + RIGHT("0" + (DT_STR, 2, 1252)DATEPART("dd", @[System::ContainerStartTime]), 2) + "_" + RIGHT("0" + (DT_STR, 2, 1252)DATEPART("hh", @[System::ContainerStartTime]), 2) + RIGHT("0" + (DT_STR, 2, 1252)DATEPART("mi", @[System::ContainerStartTime]), 2) + RIGHT("0" + (DT_STR, 2, 1252)DATEPART("ss", @[System::ContainerStartTime]), 2) + ".txt" 

AFAIK in recovery.conf file apply only for the duration of the recovery it, this file provide the parameters that PostgreSQL needs to perform an archive recovery of a database, or to act as a log-streaming. replication standby. 

ALTER TABLE x drop constraint fk12345; take long time because the parent table (user) in transaction(delete,insert update,..) or X table itself in transaction(not commit or rollback), it meant ALTER TABLE x drop constraint fk12345 may not make the User table locked. 

I would like to ask for help coming up with a query, which can identify groups on non-overlapping records. Here is a sample scenario (admittedly contrived). Let's say I have employees who are assigned to work on various projects. While an employee can be assigned to multiple projects, he/she can only work on one project at any given time (don't you wish we all had this luxury :). I need to find out which projects can be scheduled to be worked on in parallel because they do not share any employees. Here is some code to setup sample tables and data. 

it's not many time to inserted per day(4times). okay you found the problem now, but anyway slow inserting operation can caused by few reasons: 

your table might not up to day of statistic use ANALYZE TABLE for Innodb. you mention you table is 66GB but it might not the real table size it may include the dead-tuple(caused by your update,delete operation). when you migration it to other place this table or whole database will be ANALYZE to the real time statistic. 

I think I’ve figured it out (at least partially). It seems that in order to avoid creation of duplicate log file for the child package the value of “Log_Path” variable in the “child” package have to be made blank. If there is no value, the validation process will not create an extra file and will properly inherit value specified in the “parent”. This still doesn’t fully resolve the issue with the “parent” package, because I can’t run it from the development environment without any value specified for the “Log_Path” variable. The only way I found around that is to make it blank, save it and then execute it from the command line (DTExec) while passing the desired variable value via SET option. This finally results in just two files instead of four. I still don’t understand why validation process (at least I think it’s validation process) creates those “extra” files using design-time values. This just seems like a wrong behavior.