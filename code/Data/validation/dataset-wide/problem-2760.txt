Superficially, this seems to contradict Claim 1. But it doesn't, because from the fact that the semantic value of "nothing" is something , we can't conclude that something is nothing. And lastly: 

We're told that A wrote a number down, so by modus tollens, we know that x is not equal to 2, so we've narrowed the set down to this: x &in; {3, 4, 5, 6, 7, 8}. 

Admittedly, the proof is only semi-formal, but I hope it will be sufficient to motivate the claim that we really need reductio to prove certain claims of the syllogistic.                                                                      References Corcoran, J. (1973) "A Mathematical Model of Aristotle’s Syllogistic", AGPH 55:2. Łukasiewicz, J. (1951) Aristotle's Syllogistic: From the Standpoint of Modern Formal Logic, 2nd Edition. Smith, R. (2004) "Aristotle's Logic", SEP, Spring, Zalta, E.N. (ed.). Vlasits, J. (forthcoming) "Divisional Semantics for Aristotle's Assertoric Syllogistic", Aug. 22, 2012. 

Some logics have a nullary truth-functional connective (&top;) that evaluates to true under all valuations. You can call it 'A', you can call it 'atomic', if you so desire. Here are some facts about &top;: 

The non-modal definition of valuation π is a mapping from propositional letters (e.g. p, q, r) to truth-values (e.g. elements of Bool = {&top;, &bot;}): 

He next criticizes metaphysics and ethics for attempting to posit this level "behind the objects of empirical science", for wishing "to enquire after the essence, the ultimate cause of things." This is not something I agree with, but it provides some contrast for what he says next: 

van Benthem, J. (2010) Modal Logic for Open Minds, Stanford, CSLI Lecture Notes #199. Blackburn, P., de Rijke, M., Venema, Y. (2002) Modal Logic, Cambridge, CTTCS #53. Holliday, W.H. (2012) Modal Reasoning, Lecture Course (Spring), UC Berkeley. 

The first group of problems is also serious, because they indicate that O may not be a normal operator. The Free Choice Permission Paradox, Good Samaritan Paradox, Sartre-Lemmon Dilemma, and many others in the group derive paradoxical conclusions from the assumption that O is normal, i.e., that it is aggregative, monotonic, etc. Among the solutions to this class of problems is the abandonment of the assumption that O is normal and the adoption of what are called neighborhood models. Given that SDL has all these problems, the question about its applicability inevitably arises. But, as is usually the case, the answer depends on the field of application and the assumptions about the logic. Even if we leave SDL as it is, that is: without adding conditional obligation to address problems in (Group 2) or without weakening normalcy assumptions to address problems in (Group 1), it is still difficult to say whether SDL can have applications in computer science. At least the overall description of the problem area needs to be specified so that we can judge whether SDL's problems will carry over. In sum, we cannot conclude that the problems and paradoxes with SDL make it a priori inapplicable. 

Fortunately you cannot prove it. It's invalid. Counterexample. Consider a model M with a single world w &in; |M| such that there is no reflexive arrow on w (i.e. ¬wRw). That gives us the fact that: (M, w) |= ▢ P, because it is vacuously true that for all worlds v &in; |M| accessible from w (wRv), we have it that (M, v) |= P. That allows us to conclude that (M, w) |= ▢ P. But it is not the case that there is a world v &in; |M| which is accessible from w and is such that (M, v) |= P (you know, given that there is only a single world and it is inaccessible from itself). So we cannot conclude that (M, w) |= ♢P. The reasoning is similar to the usual one associated with the quantifiers ∀, ∃. If you have a universally quantified formula φ (i.e. ∀φ), if you interpret it in an empty domain, it will be satisfied. But the existential will not. Note also, that even if you have non-empty domain of discourse, it might still be the case that the interpretation I of φ specifically is empty, so the universal closures of φ might still evaluate to true vacuously, while the existential ones will not (given that no object falls in the extension of φ under the I). 

Chris is right, of course, about the fact that infinitely many conclusions can be drawn from those premises (provided that there is at least one applicable inference rule). What I want to do here is to simplify one of the sentences to reveal an obvious conclusion that can be drawn from them. Given that F is false, (1) is equivalent to (W → M). Here is one way of showing that: 

Proof. The negated form of the conclusion hints at an obvious way of proceeding: assume (B ∨ A) with the hope of deriving a contradiction. The disjunctive form of this assumption suggests the second step (proof by case analysis): assume B, derive some sentence Γ, then assume A and derive that Γ again; then using (B ∨ A) and those two derivations conclude Γ. Here's a way of applying those techniques: 

There is a proper sub-discipline of philosophy called metaphilosophy, which takes this question seriously and in its many journals presents the thoughts of giants ancient (Plato, Aristotle, David, other Greeks, Arabs, Persians) as well as contemporary (e.g. Rescher, Hansson, Floridi, and many others). For a balanced treatment of this question I would suggest that you do some googling of the term.                                                       From a Carnapian Point of View A position that I find attractive has been put forth by Carnap and several other logical empiricists: 

¬(A ∨ B) [ Given ] (B ∨ A) [ Assumption ] B [ Assumption ] (A ∨ B) [ ∨-introduction, 3 ] &bot; [ &bot;-introduction, 4, 1 ] A [ Assumption ] (A ∨ B) [ ∨-introduction, 6 ] &bot; [ &bot;-introduction, 7, 1] &bot; [ ∨-elimination, 2, 3-5, 6-8 ] ¬(B ∨ A) [ ¬-introduction, 2-9 ]. 

For the sake of concreteness let's consider the notion of fatherhood (I'm avoiding the use of 'concept' here because we'll be giving it a technical meaning). From experience we have a certain (probably) informal conception of fatherhood. We know, for example, that everyone has a unique father. We know that no one is his own father. We know that two persons sharing a father are siblings. And so on. Once we have an informal conception of fatherhood we can choose (or devise) a logical framework (possibly equipped with a semantics or a proof theory) appropriate for the explication of the informal conception. We can choose, for instance, a language system that includes predicate symbols 'F', 'S'. Then we can let 'F(x, y)' be the explication of "x is a father of y", and 'S(x, y)' be the explication of "x and y are siblings". We can then capture all sorts of logical relationships between those predicates, corresponding to our informal conception of those relations. Here are some examples of that: 

(Alethic)       M |= p    iff    π(p) = &top;. (Modal1)  M, w |= p    iff    w &in; V1(p). (Modal2)  M, w |= p    iff    V2(p, w) = &top;. 

while not identical to the left side of (3), is a substitution instance of it, because it is the left side of (3) but with 'p′′' substituted for 'p', and '¬p′′' substituted for 'p′'. Therefore, (4), which again, is not in Lo, is syntactically equivalent to an expression, specified by (3), that is in Lo, namely: 

Truth-assignments are functions that take propositional letters of the language of propositional logic to truth-values (which in the classical case means {0, 1}). A formula φ of the language of propositional logic is said to be true with respect to a truth-assignment v (symbolically: v &models; φ) just in case φ becomes true whenever the propositional letters occurring in φ are assigned truth-values according to v. For example, under assignment v = {p → 1, q → 0}, formula (p → q) becomes false. Now, the question is whether this semantics has anything to do with possible-worlds, and the answer is that it does. The key is to observe that functions (including the truth-assignments) are also relations, and therefore truth-assignment functions are also relations, namely, relations that associate propositional letters to (unique) truth-values. The set of all truth-assignments is a relational structure, and we use it all the time when talking about tautologies and contradictions in classical propositional logic: 

Based on this necessity operator □ you can then define the possibility operator ◇φ as ¬□¬φ, etc.. There are more complex modal languages, of course (e.g. the language of quantified modal logic). Formulas of modal languages, like formulas of non-modal ones, are defined inductively, so if you have some familiarity with compositional definitions and mathematical induction, you can define useful metrics for modal formulas (e.g. modal depth) that can help you prove various things about all modal formulas in a given language (e.g. unique readability/parsing). Modal models are graph-like structures called relational or Kripke models. They're structures consisting of a set of worlds W, a binary accessibility relation on W, and a valuation V from propositional letters and worlds to truth-values. The key difference between non-modal and modal semantics is that in modal semantics the truth of a formula is world-relative, so while we specify: 

Let's apply that definition to your example argument with cats and tigers to see what happens. Let C, T, M be the predicates 'is a cat', 'is a tiger', and 'is a mammal', respectively. In first-order logic we can express the cat-tiger argument as: 

Everything you said seems fine. You propose to think of as functions from possible worlds to subsets of the domain of individuals. That's a pretty standard way of thinking about intensions (goes back to at least Carnap's work in semantics). That particular criterion of analyticity, namely that: 

There are two notions to be distinguished here: logical consequence (&vdash;) vs material implication (→), each of which, in classical logic, has the unusual property of explosion, which we can summarize as: 

Purely extensionally, F is said of x just in case x is among the elements of the set F, i.e. iff x &in; ext(F). Let's apply this definition to your example. 

As examples of such relations take the relations LeftOf, SmallerThan, LessThan, and so on. For 3-ary relations Between, CloserThan come to mind. For 1-ary relations or predicates we have Small, Even, Prime, and so on. Now that we have defined relations, we can define functions in terms of them: 

This shows that (2) is true solely by virtue of the interpretation of the logical constants and is thus, by Definition (2) a logical law. I'm confident that the physical truth of (1), if indeed it is a physical law, can be proved solely on the basis of the interpretation of the physical constants that occur in its definition. This, of course, is only one way of looking at things. For a general background, I recommend following Mauro's suggestions in the comments above. It's important to become aware that there are disagreements among respected philosophical logicians about what exactly logical constants and logical principles are. There are also different ways of looking at the physical/logical distinction, and any good metaphysics or philosophy of science compilation will contain relevant standard texts on that.