For future reference, diagrams are immensely helpful for these kinds of questions - however, I think I understand what you are trying to do. It also is helpful if you explain what you want to do (your goals) along with what you did. If my answer is not relevant, please provide a diagram and information about your Hyper-V Virtual Switch/s and I'll adjust fire for affect. 

I'm pretty inexperienced when it comes to setting up power for a network rack. I have a location that has a dedicated 20 amp circuit with a LP-20P socket. I have a APC UPS 3000XL rackmount UPS and a TripLite PDUMH20 PDU. I'm looking at the current configuration where things are configured like this: [Power Source] --> [PDU] --> [UPS] --> [Network Gear]. I can't help but feel this is incorrect. Won't the UPS "obscure" the draw of all the downstream devices? What is the correct power path for UPSs and PDUs? Does the UPS go to the PDU or the other way around? Does it make a difference? 

In OpenBSD (and presumably other BSDs) you can set a description of an interface with ifconfig using the aptly named argument, see ifconfig(8). This is very handy for distinguishing between a bunch of interfaces. But that doesn't help you. Unfortunately there's no great way to do this in Linux. In Linux, interfaces are named dynamically with each interface being assigned the first available name. This means that if you pull a NIC and then add another one (say to replace it or upgrade it) there is no guarantee that its interface will remain the same. Try a program like ifrename which will allow you to manually specify the interface names. It looks primarily designed to assure that is always associated with but I believe you can use it assign names like and to interfaces instead of and so on. Udev will also allow you to change interface names using the network.rules file (see here for an examples). You should be careful to document this as it is not typically done but unlike @MichealHampton I don't see any particular problem with it. I personally make great use of the description field for interfaces in my BSD installs. 

I have a SuperMirco server that is using the MBD-X8-DTL-I-O mainboard which is equipped with one of the lovely Intel ICH10R based fake-RAID controllers (confusingly enough reported as an Adaptec device -- I'm assuming that it is some unholy marriage of the ICH10R chipset for the controller and Adaptec firmware). After some thrashing around I was able to find the correct ACHI drivers and get Windows Server 2008 R2 installed on RAID-1 setup. Part of my confusion comes from the fact that Windows reports the controller as an "Adaptec Serial ATA HostRAID" and the Hard Drive as an "ADAPTEC RAID 1 SCSI Disk Device" whereas during the boot process the Controllers hardware clearly reports that it is actually using the Intel ICH10R ("Controller #00: HostRAID-ICH10r at PCI Bus:00...). I don't expect much from this controller in terms of management but is there any software (3rd party or otherwise) that can monitor the health of the RAID array and notify me (preferably via SNMP to our OpenNMS server)? This server is going off-site and it would nice to know that a drive has died without physically having to look at the Blinking Lights. All I can find is the cheesy Intel RST software which I don't think has any remote monitoring and notification facilities (it also complained that it couldn't be installed due an "incompatibility error" which is strange considering that the controller chipset is an ICH10R - but that is another question). For those of us making do with these fakeraid cards on Windows-based platforms: How do you monitor your RAID array? (If this is just handled by Windows, could you please point me towards a KB or TechNet article? I'm primarily an Unix admin, so if there is a simply a Windows equivalent of mdadm or raidtools I am ignorant of it). 

You will want to make these settings mandatory (otherwise a knowledgeable user can just un-set them). See the Gnome Deployment Guide for more information. 

Except... nothing in IIS changes. I'm either not remembering a step we did previously to move the WSUS IIS Site or something is broken. 

What the directive is really for is for placing packets in a separate queue or chain for processing by an IDS system. It's not designed to interpret Layer-7 information either holistically/behaviorally or by using signature matching and then dropping it once it is determined to be "bad". So secondly, the directive is not sophisticated enough for your use case and you'll likely end up dropping packets you want to keep. You'll also notice there is no string in the packet you can feed to . The packet as shown by TCPDump is unreadable once translated to (presumably) ASCI. The argument is really designed for looking for things like HTTP Post commands where you do have strings in the packet. If you wanted to follow through with your idea of using iptables as a signature matching IDS system you probably want to use the U3 Matching capability to look for particular bit-patterns. Third, and most importantly, by the time the packets involved in a Denial-of-Service reach your machine and are being processed by your firewall the damage is already done. Any significant mitigation of a Denial-of-Service attack will involve working with your upstream provider to block the traffic before it reaches your server. 

The general principle of "defense in depth" would incline one to be extremely wary of having user accounts without passwords at all. The password is a large part of the authentication mechanism; having accounts setup without passwords is akin to having locks without tumblers. I'm in agreement with you here. 

Do nothing else until this is done: Verify the location of all important data, verify that it is being backed up, verify the integrity of those backups, verify that you can successfully recover items from those backups and verify that you have some kind of off-site storage and Disaster Recovery plan. Inform your supervisor just how bad the mess is (maybe it is only a little bad, maybe it is really bad). Make sure you express the challenges of bringing things up to board and the risks of not doing so in business terms and not technical ones. You need to rally the resources of your organization, open lines of communication and get everyone on the same page. Messes just don't happen by accident. At the very least you have a professional obligation to inform your employer of the current state of their network. Begin the process of discovery and documentation. You need to discover and understand what's where, and what it does before you can begin "cleaning up". Right now you, although well intentioned, you run the very real risk of retiring or removing some piece of the network that, while seemingly was just dead legacy junk, was actually a key part of some strange hack to make some super important business critical application run. Really take your time here, document as well as you can and try to understand the underlying business process. Give Limoncelli's Test a read. 

You don't mention whether or not the same behavior exists with other files and since you didn't I'm going to assume that it doesn't. Is it possible that you are seeing an issue with Office File Validation? I suggest you use a combination of ProcMon, TCPView and Wireshark/TCPDump to try and get a view into what Office is doing when you try to open documents via your symlinked directory. Additionally, crank the logging level up on Samba to 10 and see what it's doing. 

Since your client has the rather silly requirement you cannot use your laptop or USB to download the required installation files just temporarily turn off Enhanced Security Settings, restart Internet Explorer, download the files and then turn ESC back on. 

What are you trying to do? I'm trying to enable DNS scavenging on a DNS zone that has about a hundred stale DNS records. What have you tried in order to make it happen? I setup DNS Scavenging per everyone's favorite TechNet Blog post: Don't be afraid of DNS Scavenging. Just be patient. I first disabled scavenging on all of our domain controllers: 

If you don't mind using a web broswer darkstat might meet your needs. It's ridiculously easy to setup and use. I just added the following one-liner to my . 

Looking at the historical performance data for the virtual machine indicates that this condition has been going on for at least a year but the frequency has increased since March. 

What's wrong with httping? In combination with a little bit of code from your favorite scripting language (ahem Perl!) it will easily accomplish your goals. If you're looking for something that is a little prettier give Smokeping a try. You can use the Curl or HTTP probe to specifically query your webserver for latency information. Smokeping is primarily designed for running on a Unix platform but some adventurous souls have managed to get it working on Windows. Pingdom is probably the most polished service out there to do this. I've never used it so I can't comment on it efficacy, but it seems well liked by the folks here on SF. If I recall, the cost is pretty reasonable for the basic level service. 

File / Print Services I can see a number of solutions for the file and print services - I believe that we can just extend the Extranet as a VLAN onto our virtulization platform and then we can eliminate the rackmount server and associated equipment. Unfortunately this does not cover the DR/BC services - I am looking at things like Azure File Storage, an Azure-based Virtual Machine that we use as a DFS target or even OneDrive for Business. I just cannot figure out how to glue these technologies together to address our requirements. Ideally we could just use some kind "cloud" service for file access but I am concerned with internet usage (restriction #1) and the lack of ability of having a local copy on-network in case of a service outage. I feel like there is "have my cake and eat it too" solution here but I just do not see it. Visibility and Management I would, love, love, love to have these computers joined to our Active Directory domain but I cannot see a way to do that considering restriction #2. I have started looking at Active Directory in Azure but admittedly I do not really understand it and it seems just limited to Single Sign-On services. What I really want is a way to get GPOs to those machines and have a central authentication store. I am further limited in that our Active Directory domain is managed by another group so any proposal to "extend it" would be politically and bureaucratically difficult but not impossible. Our AD team is working on an organization-wide Office 365 tenancy which will implement a DirSync of some kind but I cannot see what that would buy me other than OneDrive for Business (which could address file services but not configuration management). I am currently working on implementing Internet-based Configuration Management which if I can manage the bandwidth issues (restriction #1), I will get some visibility with Hardware Inventory, Windows Updates and 3rd Party Application deployments. Configuration Items are a pretty hacky way to replace Group Policy but I suppose, push comes to shove that would work.