Of course I can work around this by continuing the installation by hand through the virtualbox GUI but this should not happen in unattended installation. How can I force the installation process to restart the services without waiting for the user input? 

We are working on a project which involves different hardware all hosted in a single rack. The machines are mainly IBM servers: 2 x206 (scsi), 1 x226(scsi), 2 x3400(sata) and another assembled machine with sata controllers. We are using several raid controller. Some machines have only one Serveraid controller, others have one or more controllers not always Adaptec ones. All the firmwares and bios are updated. All the servers and connected devices are under ups. Over the last 4 months we experienced several strange behaviours in our hardware. Suddenly and randomly we loose 2 or 3 drives and the raid volumes stop to work. It can happen once a week but never at the same time of the day or week. Most of the times a rebuild process fixes the problem, sometimes we loose the data. Very often we just need to unplug the raid controllers, restart the server and the problem is fixed. At the beginning we thought it was due to firmware bugs but we performed an accurate update for every machine and raid controller and there is nothing else we can do on the hardware. We have really no hint on what's causing all these troubles. We are starting to think that it's an environmental problem but we don't know if there could be something interfering with our hardware. Have you ever heard of something like that? Do you have any idea on how to investigate the problem? 

What CRC does exactly? Accordingly to wikipedia it should be an integrity check but how does it work? I discovered that setting this parameter to false my disks are finally recognized as sata2 rather than sata1 and speed are really increased. Why? I found this IBM paper in which they say: 'CRC Checkingâ€”(Default: No) Determines whether the controller verifies the accuracy of data transfer on the Serial bus. CRC Checking should be disabled on the controller and all devices if any device supported by the controller does not support CRC Checking." How do I discover if a hdd supports CRC? If CRC is disabled and a breaking event occurs, is there a risk? 

I am trying to automate a weekly process where I download a CSV copy from the Client page on the Meraki website. Before I lose you I am aware there is an API and it works very well. However there in a device attribute that is not exposed in the API. I got this from their support: 

I got the same results from the MXtoolbox. So I guess I understand why my mail gateway thinks it is supposed to send mail to itself. Internic.ca is a CA Registrar. This does not make sense to me. Even less when I try to email them from my google account and I don't get a bounce back. What does this mean? I don't think there is an issue on my side but I don't understand why this means. 

I have a Windows 2008R2 print server hosting about 40 printers. For the longest time we had a Point and Print GPO that allowed the users to install the drivers for these printers without administrative interaction. Administrative Templates\Printers\Point and Print Restrictions: Disabled. This is still in place. Recently though that is no longer working. Take the "sales" printer for example. People have been connected to it for years now and in the last few days, when someone tries to print, their computers (All Windows 7) have been asking to install a print driver. Even new users that have not been attached to that printer before are being asked for admin rights to install the printer. This has affected about half of the printers on this printer server. So some printers users are able to install just fine. So when someone has the issue I hop on their machine and provide my rights so the print driver will install. I am sure I know of the catalyst that caused this but I have no idea how it directly relates. For inventory purposes, I updated the host names of the printers. To clarify I went on the web interface off all the printers and in each of their network IPv4 configurations I updated the host name from its generic Ricoh to be the same as the DNS record I made for the printer. So each printer has a share name, port name on the printer server which are both the same as the physical printers host name e.g. "sales". No changes have been made to the print server hosting these printers. I don't understand how that change would cause this. In the case of the "sales" users it is preventing them from printing. We have to allow the driver to update before they can print. That is how we knew there was an issue and was able to tie it to my inventory update. These users are not all in the same OU in AD and both have the same policies applied anyway. I am testing different GPOs as when you look up network printer driver GPOs there are more things people change than just the one I mentioned above. Any ideas why what I did is causing this issue? Perhaps I am chasing the wrong tail and something else is wrong? 

the process gets stuck on the installation of libssl because it requires the user input to restart some services (ssh ntp exim4) 

I have a X3400 with 8 x (43w7598 250GB SATA 3GB/S HDD) set in the frontal bay and connected to ServeRaid 8K controller. The raid array works well but, even if I set the controller PHY to 3.0 (rather then Auto or 1.5), all the disks negotiate 1.5Gb/s with the controller. This sounds really weird to me because the disk are WD Caviar Black Sata2 disks (bought from IBM). The firmware is the latest available from IBM (accordingly to the IBM ServeRaid matrix). Any tip? 

I use rsync v. 3.0.4 and when I need to move something I use it with the --remove-source-files. I prefer rsync than mv. Unfortunately, when I use --remove-source-files, the directories are left on the source side (as said in the man). Is there a way to remove directories too once moved all the files? 

I just purchased a license of safelyremove. It works with sata controllers too not only with usb. It's very nice. There is a full trial on the website $URL$ 

I never had experience on that precise model but generally the answer to all your questions is YES for every nas like that. Even the last question should be yes because the nas should be capable to rebuilt the array from the meta information stored in the disk and also because a mirror raid is not that hard to be rebuild. If you extract a running disk you can mount it on another host (the filesystem used by Iomega should be XFS, so every linux can mount it). Bonus question: If I understand well what you mean, the action is SWAP. Hot swap is when you can exctract a disk without turning off the device. This Iomega is not hot swap. Apart this in the recent past (last 3 years) we sold 12 Iomega Storcentre nas. We had 100% warranty emergencies on them: broken disks, bugs in the GUI, broken power units. We stopped to sell Iomega for this reason. I don't mean that this model is affected too but I would suggest you Buffalo $URL$ 

So everything appears to be in check there as well. However in testing deleting both users and containers I am unable to find them in order to recover those objects. The only thing that ever shows up is the deleted container itself 

I have seen this message in relation to configuration issues with PostFix. My environment is Exchange 2010 with a Barracuda Spam & Virus appliance as our mail gateway. When I looked up the MX record with nslookup I got 

From what I can tell from looking at other tutorials about setting this up that should be showing up. For me though, there should be other objects there that have been deleted over the past couple of weeks. I am running my tests as a Enterprise Admin user. Searching for "recycling bin" and "active directory" leads me to other users that have similar issues but most of them are addressed by either actually enabling the feature or being at a lower forest level. In my case both are correct. Not sure what I am doing wrong here or assuming. A fact that is quite possibly related is that I cannot see this "Deleted Objects" container from ldp.exe either as per this guide I was using for comparison. The last step to see the container being: 

Sometime ago I enabled the active directory recycle bin using PowerShell. At the time I knew this to be working because of the following results: 

I have a Windows Server 2008 R2 SP1 machine that is isolated in a DMZ. Historically it has not had issues but everything works before it breaks. The port 8530 is open on the firewall appliance and I can telnet from the client to the server which proves the site is ready and open. This machine is not attached to the domain so WSUS server is set in the registry. So under HKEY_LOCAL_MACHINE\SOFTWARE\Policies\Microsoft\Windows\WindowsUpdate I have 

I need the Owner as that is how I associate devices to people in our organization. Using PowerShell I am able to successfully log into the website and get a 200 response for a basic page. My problem comes when I try to use the same session to get to the data I want I keep getting a page with just breadcrumbs and no actual data. 

I'm looking for a 4 ports pci or pci-express (not pci-x) sata controller with these features: - real hot swap support - no RAID support, or with RAID support that can be disabled through the controller bios interface or reflashing the bios My need is to hot plug/unplug one or more different sata1 and/or sata2 disks from different brands size and speed (even at the same time), coming from my customers computers to perform lab activities on them, like backup. I tried several Silicon Image controllers (3112,3114,3124) and Promise TX4. Everyone has issues: some disks are not seen or are dropped during the backup process while the Promise even hangs the host pc with some hard-drives. Adaptec raid (Serveraid 8s) controllers aren't transparent to the operating system and it seems there is noway to disable the raid. What would you suggest? Thank you! 

Is there anyone aware of someone seriously working on thunderdbird to get write support on ldap shared addressbook? 

Unfortunately this is shown inside the terminal and I couldn't find any way to hit "ok" and then, of course, I get this message. 

The orange led doesn't mean that the drive is broken: it means that it's marked as failed. Recently I had the same issue on a ServeRaid 6i: two drives disappeared. The raid was level 5. I put one of the two online and I rebuilt the second. At the end of the process I got my array rebuild. Of course it was not a broken disk but a weird bug into the controller or into the disks. Some disks seem to have broken firmware that cause the disk to deattach from the array randomly. 

I just purchased a license of safelyremove. It has command line support. It's very nice. There is a full trial on the website $URL$ 

The title should be self explanatory but more in detail I'm looking for a way to protect the ldap from LAN brute force attacks. It would be fine to prevent password guessing by locking a password for a specified period of time after repeated authentication failures. It doesn't matter if this can be turned into a DOS. Unfortunately I can't find a way to do this and the documents I've found are really confused. 

Can a Windows OS that is automatically managing paging file size for all drives effectively use a drive that is dedicated for that purpose? 

On a SQL 2008R2 box we recently had a number of jobs fails for various reasons that were mostly memory related, including one stating the page file was full. The Windows 2008R2 VM had 16GB of RAM and a dedicated disk for a 6GB page file. For now we moved the page file back to the C: drive and increased its size to 8GB. The long term effects of that are yet to be seen. Our Server Admin, this morning, increased that "swap" drive to 25GB as was recommended by the GUI. What struck me as odd is that the admin also changed virtual memory to be mananged automatically across all drives. This strikes me as waste of space but I don't really understand how Windows automatically manages the page file. Here is a snapshot of the current virtual memory settings to help with the description. 

The windowsupdate.log corroborates this. I would like to try and include only what is required to try and keep the post length down. The client reaches out to the server and see that it has X available updates. However it fails to download those. The log shows entries like this: 

I made a script that takes data from an HR database and populates correlating attributes in AD e.g department, title, manager, location. Since people change titles, departements and/or locations on occasion it is important to keep AD up to date since we have processes that depend on the validity of this information e.g. location based dynamic distribution groups. To try and keep the process fast I just run for each users regardless if something changed or not. I had worried that I would be changing the modified timestamps needlessly since it is faster to just make these changes constantly then it is to verify that no changes would be needed. To my surprise it seems that AD is doing something like this for me as most of the user modified timestamps are not matching subsequent script execution times. This seems like a positive for me as AD is doing this for me under the hood. FYI I have 2 DC's that I could be talking to for this and I have checked both times to ensure that I am not just drawing a horrible conclusion. I cannot find an authoritative source that explains this and I am not sure if this is PowerShell doing the job for me or something Active Directory is doing.