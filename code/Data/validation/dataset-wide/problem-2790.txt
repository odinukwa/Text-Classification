Of course It's not so bleak, though. So, you and the people you are speaking with here seem to be defining 'faith in something' equivalently to the belief that there exists intrinsic value; that is, to have faith in something here seems to mean that one acknowledges the intrinsic value present in that thing. Deific adoration, then, presupposes faith, since it is necessary to acknowledge God's intrinsic value to worship Him. Moreover, though, faith 'in something else', if it is to be differentiated from the ordinary faith of expectation, is most reasonably understood as a holding of that 'something else' in the place of God (c.f. idolatry). Since to take something as God requires the attribution of the intrinsic value due God, this, too, seems to map well. Given that that is the definition we are dealing with, it should be no surprise that faith is necessary to avoid perceived meaninglessness; indeed faith is definitionally that which is necessary for meaningfulness! So, the statements made by these speakers are true, but vacuously so. They don't really reflect on the nature of Man's relationship to God nor Man's need for that relationship, and neither do they say anything which legitimately concerns mass shootings, depression, or the other Scary Social Things that they mention. They merely say "If you believe that anything has intrinsic value, then you don't believe that there is nothing of intrinsic value", and also "If you believe that there is nothing of intrinsic value, then you don't believe there is anything with intrinsic value", both of which are obviously true. The problem is that the speakers are trying to imply things that may or may not be true via other meanings and implications attached to the specific words employed. Essentially, those points which are argued by implication ('suicidal acts of violence are the result of not having enough faith', for example) are argued only via equivocation. 

...is pretty much my answer: Meaning is internally constructed from (or formulated into) external physical constructs or events, which are themselves meaningless but decidedly non-arbitrary. However, what counts as "meaningful stimuli" is not objectively determinable. Rather, it's in the eye of the beholder. If we agree that removing one's glasses means buy and scratching one's chin means sell, then those are meaningful signals to us, but no one else. Thus, whether patterns of ink on paper, or photons from a display, or transistor voltages constitute "legitimate semantic representations" is neither true nor false. It depends on the potential interpreters, their capabilities, and their contexts. 

In the CRA, Searle only addresses the 2nd relationship: his own UTM processing of the Chinese (#2), which he correctly characterizes as formal symbol manipulation, i.e,. syntactic and meaningless or non-intentional to him. Your reply above does not account for this 2nd relationship. For when you speak of manipulating English symbols in your mind, the key difference is that your mind can interpret them, whereas the Searle-UTM cannot itself interpret the Chinese symbols. That's Searle's main point, and he is correct about that much of it. (His mistake lies in missing the other aspects of the two computations.) Virtually no one addresses the 1st relationship. However, clearly the Searle-UTM must process the program symbols with some level of semantic intentionality.* That is why they must be in English! This fact alone contradicts Searle's own CRA conclusions. Most importantly, the 3rd relationship is approximately the famous Systems Reply, as mentioned. Searle tries to dismiss the Systems Reply, and apparently people are left to decide for themselves whether they agree, disagree, or prefer to focus on some other aspect of the CRA or science fiction or philosophy, etc. But the key point about the Systems Reply is this: although Searle cannot discern it, the additional level-2 computation is not merely some philosophical assertion. It is a mathematical fact about how Turing computation works. A universal computation on its #1 program input and its #2 nominal input, instantiates the program's own distinct TM computation on the nominal input. Period. Trying to rebut this would be like trying to rebut the Pythagorean Theorem. The slight difference here with the usual Systems Reply is that one cannot further assume that the Turing Test is valid. The CR's external behavior does not prove that the program's algorithm is equivalent to a human's internal understanding of the Chinese. But the CRA does not disprove it, so it remains an open possibility. Presumably, that is why over the years Searle has evolved his critique to more properly address computation in general, rather than just the CRA's focus on universal programmability. 

On your first complaint, that people are different and not exchangeable, there is a well-known critique of Rawls - and perhaps of liberalism and the social contract more generally - that it assumes that all people are essentially equal and the same, when in fact they are not, as is proved by the ubiquitous fact of need and dependence in society. It is unclear that, say, the mentally handicapped or the very old and frail, or young children, can participate in the (hypothetical) social contract that Rawls envisages, and so - the critique goes - Rawls cannot deal with difference and dependence and need. This argument is particularly associated with feminist critics like Martha Nussbaum or Eva Kittay. I've not explained it particularly well but it is easy to look up and is often called the 'dependence critique' of Rawls. Handily for your second question, both Nussbaum and Kittay are still essentially within the liberal tradition and aim to adapt rather than to overhaul Rawlsian liberal egalitarianism. On your second complaint, that the idea of 'starting off on the same foot' is misguided because virtue tends to increase up the income distribution (at least in the US), it sounds like Robert Nozick would be about the closest to what you have in mind. I doubt that he would express it in terms of the 'virtue' of different social groups, but he too doesn't like the idea of starting off on the same foot because he is interested in property and what it means to hold property justly, and for him as long as property was acquired justly in the first place and has been passed on fairly - such as through a family - then it is still held justly. As a result, his conclusions are essentially very right-wing in advocating almost no redistribution or interference in the market (although not quite as right-wing as suggesting that the poor are less virtuous than the middle class and wealthy and even given the chance would still go sliding back down to a lowly and un-virtuous position...) 

When I read this, I understood it. How? For me, the objects being processed were photons from my computer screen impinging on cells in my retinas. Those photons contained zero "semantics". However, their physical properties and configuration were far from accidental. Primarily, the semantics in your head determined them (based on a human & technical foundation). That physical signal was sufficient to cause in my head (1) a symbolic representation followed by (2) a syntactic & semantic interpretation (all based on a similar foundation), which hopefully is reasonably close to the one that your head intended (although certainly not identical). Semantics does not lie in transmitted tokens (signals). It lies in the (more or less shared) structure, function, and content (memory) of the machines doing the formulating and interpreting. 

* Obviously, full blown self-conscious intentionality is not required for computers, but translating a program's symbolic references into physical entities, actions, and events is required. That goes beyond formal symbol manipulation into enacting non-formal associations. Ref (and see refs therein): Refuting Searle's "syntax is not semantics" argument. 

As you and others here have said, your CRA reply is essentially the most common one, the Systems Reply. But your statement misses some important aspects, such as Searle's point about formal symbol manipulation, which is the one thing his CRA gets right. To understand how both things can be right: lack of semantics at the Searle-computer level and the potential for understanding at the CR-system level, one should understand the relationships among the Searle-computer and its two kinds of input. The Chinese Room's heat source is the fact that it produces two levels of Turing computation, both of which are processing the Chinese symbols. Level-1 is Searle himself, acting as a UTM-computer (a universal Turing machine; like a CPU). Level-2 is "the system" whose computation of the Chinese is determined solely by the program and its execution configuration (memory, state, etc.). Note that the existence of the two computations is a fact, not as in "I'm stating my opinion as a fact", but as in "it is an objective mathematical fact, like the Pythagorean Theorem, which anyone can understand and verify for themself". Searle's self-consciousness is a kind of 3rd level of "information processing" (which might or might not be a computation). It allows him to introspect on his own level-1 universal computation. But he cannot similarly introspect on the level-2 computation. For level-2, the Searle-UTM is merely its physical substrate, analogous to a person's neuron-level, not their consciousness-level. Thus, the Searle-UTM never knows what his program is doing, whether it is doing a Turing Test or taxes or tic-tac-toe. Also note that the Searle-UTM is processing two classes of symbols: #1 the English symbols of the program, which people always ignore, and #2 the Chinese symbols, which captivate people like shiny objects. Thus, to correctly address "symbol manipulation in the CR", one must fully address the three symbol-processing relationships: 

I think I read above that this isn't a forum for opinion so I'll move swiftly on from that one (!...) but I think again Rawls's answer would centre around the idea of the equal moral status of persons (at least at birth). Clearly, many would argue that during life people through their agency makes choices that mean that they 'deserve' or 'don't deserve' certain things, but Rawls thinks that in the eyes of justice every person is still equal; no matter how 'good' or 'bad', people don't earn preferential treatment from justice (we wouldn't say that someone who gives to charity should get away with murder, or that people who are mean to their friends should be stripped of their wealth). Rawls isn't really interested in what people 'deserve' through their deeds (for that you want Robert Nozick) or through some idea of their innate virtue, but rather in having a social system that isn't predestined to militate against the life chances of particular people and groups. 

With respect, I think that this suggests a slight misunderstanding of what Rawls is arguing. He is well aware that people are not created equal. However, what he does believe is that every individual should be taken to have equal moral status i.e. our considerations of justice shouldn't start from the starting point of preferential treatment towards some. (I would imagine - or hope! - that very few would disagree with this as a fundamental part of the definition of 'justice'.) So, we're trying to work out fair principles that treat everyone as morally equally important, but these principles are to govern over a situation where people are not equal in strength, mental ability, inherited wealth, social connections, and so on. Now, if we actual people were to try to design these principles then it seems likely that, say, on the whole the weakest or poorest might try to design principles that put their interests above all others, whereas the wealthiest and most powerful might try to design principles that maintain their status. The naturally physically strongest might try to design principles that link power to physical aptitude. And so on - and this doesn't seem fair, or workable. So, how can we avoid this situation? Rawls thinks that we can avoid it by undertaking a thought experiment: if none of us actually knew anything about our social status, strengths/weaknesses, race, gender, etc., but knew that we were about to enter into a society that we were going to have to be happy in, what principles would we choose? What is actually going on here is that the method, in the thought experiment, of depriving the deliberating parties of information is a way of building in fairness and impartiality into the deliberation. The parties can't possibly be *un*fair to one another in their choice of principles because they wouldn't know how, and wouldn't know whether their choices would actually disadvantage themselves. As such, whatever principles these imaginary parties would choose will be fair and impartial. Now, we could argue about exactly what principles the parties behind the veil would actually choose, but, at any rate, the above is the method and whatever else we might say one can understand the thinking behind it and appreciate the philosophical elegance. I think it would be a mistake to suggest that it relies on the idea that people could be 'exchanged'; firstly, it is just a thought experiment designed to generate certain kinds of conclusions in the right way, and so doesn't really have a lot to do with actual people, and secondly, its aim is to arrive at principles that can ensure the just social co-existence of people who, indeed, aren't interchangeable. 

Largely, yes. It is scientifically vacuous. It is primarily a social phenomenon (duh!), unfortunately one that generates a lot more heat than light. However, true to its original motivation as a critique of 1970s symbolic-AI approaches to language processing, it might have had some value in focusing attention on the need for a better understanding of semantics and its relationship to computation. 

As you say, the difference between extrinsic (observer-based) and intrinsic (subjective) symbol-manipulation semantics is key to the Chinese Room argument (CRA) (cf. the symbol grounding problem). However, I don't see that the abstract-vs-physical distinction between computation theory and physical computing devices addresses this issue directly. Computation theory is just another mathematical model, like calculus or geometry. Finding logical "0, 1" symbols in a physical computer is no different than finding straight lines and 90-degree angles in buildings. If we believe that the abstract, mathematically based theories of engineering truly keep skyscrapers from falling and planes in the air, then the dancing voltages in my bank's computer also truly represent the logical "balance" of my logical "checking account". Physical computers "manipulate symbols" to the same degree that buildings "obey statics" and planes, "aerodynamics". Fortunately, the distinction between idealized math and the real world is not needed to fully understand the CRA. Because it is grounded by Turing machine (TM) theory, the CRA's analysis can be much less esoteric and much more precise and objective. The CRA expertly focuses everyone's attention on the wrong symbols: the Chinese inputs and outputs. Like a magician, Searle makes you ignore the elephant in the Chinese Room: the program. It, too, is a symbolic input to the Searle-computer. (He also hopes you'll miss the fact that it's in English!) The program (aka, the rule book) is the key symbolic input because it alone dictates how the Chinese symbols are processed. The only reason the Searle-computer can (and must) process the Chinese symbols purely formally is because he also has the program-rule symbols, which he can (and must) interpret non-formally. As a universal TM, the Searle-computer's main responsibility centers on the program itself. Searle directs everyone's attention solely to himself and the Chinese symbols, yet he is wholly superfluous to their computation! The Searle-computer and its program input could--and should--both be completely removed from consideration. Replace it by a direct, non-programmable implementation of the program and the room would function identically. Hence, Searle's claim that the room processes the Chinese symbols purely formally (as opposed to just its Searle-CPU) is totally unfounded because he fails to account for the program's own computation (whose existence is irrefutably explained by Turing machine theory). The degree to which the program's computation interprets the Chinese semantically (or not) remains undetermined. Hence, the CRA establishes absolutely nothing about the semantics in the room regarding the Chinese symbols. References: Syntax vs. semantics, chineseroom.info