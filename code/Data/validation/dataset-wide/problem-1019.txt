Using your current method you might use a query such as the one below. This example uses derived tables to treat each backup date range as if it where a table. Then joins two backup date derived tables (named Today and Previous) on the emp# (I have used EmpNo in my example) and uses the WHERE clause to get only records with at least on difference between the two. Note that the syntax for derived tables may vary depending on what database product you are using. Adding the database product to the question might help the question be answered more accurately. 

To find all versions of a particular employee record use DISTINCT. This will return unique versions of each record. 

It sounds like you goal can be met using a UNION. UNION combines two select statements into a single resultset. The order by is applied to the unioned query not the individual select statements. You'll need to order the results after the union is applied. See: $URL$ Here is an example using your queries. 

You can create a new field in the SELECT statement to specify that text like 'D%' is the preferred sort order then order by the new field. Example: 

To me that means the instance wants 4GB and can only consume 878MB. There are no other memory intensive processes running on this server. The output of dm_os_process_memory is as follows: 

I have a SQL Server (2014 SP2) with a linked server to an Oracle 11 database. I have a very simple select statement to an Oracle view which I know should return around 140k rows. But here's the thing, when I run it in SQL Server I immediately see records in the results window, but all of a sudden, the query hangs when it has so far returned only around 2000 rows and it just sits there forever doing nothing else. Sometimes, another few thousands rows appear before it again hangs. The wait state on the query is on OLEDB which is to be expected. I'm thinking some sort of Network bottleneck...? Sometimes I do get an error after a while: Cannot fetch a row from OLE DB provider "OraOLEDB.Oracle" Before anyone suggests it, 'allow in process' is ticked on the provider settings. I'm looking for ideas to troubleshoot this at either the SQL Server side or the Oracle side to check that it's not a database config issue. 

I am assuming that DateFilled has a data type of DateTime or other date type which includes a time portion. When using DateTime values in T-SQL be aware that a time is always included and if not specified midnight is used by default. See: $URL$ When the input string is "DATE only" then "the The TIME part defaults to 00:00:00." In your case the string "02/28/2014" is converted to a datetime value of 2014-02-28 00:00:00.000. Your query is interpreted as DateFilled between '02/01/2014 00:00:00 .000' and '02/28/2014 00:00:00.000' thus the record with a datetime of 2014-02-28 09:24:00.000 (which occurs after midnight on, is not found. Your query may be re-written as 

You can find documentation on how to restore a database backup at $URL$ If you run into a problem, please edit your question adding the specifics of your problem. 

The example below uses a CTE to find the gaps in your TimeScheduleId sequence. The field HasPreviousItem identifies if a row is the first in a sequence and RowNum is used to number all of the records. To find the start dates/times select for records where HasPreviousItem = 0. The Duration assumes that each record is a 15 minute block of time. The cross apply subquery finds all records for the same ApptCode in the same block. That is records in sequence not interuppted by a HasPreviousItem value change. If the "duration" of a record is not always 15 minutes an alternate method to find the duration of an appointment would be needed. 

This article gives all the filter arguments and available columns including the code. I use it often as a reference to create server-side traces. 

Firstly, forgive my poor question title. I couldn't think of the best way to summarise this. Essentially I'm putting together some scenario based DR documentation, with this specific scenario being a SAN issue where we have lost disks which hold either log or data files. I'm assuming that MASTER has been affected (ie. the drive where it's files exist isn't accessible) and SQL services won't therefore start. My specific questions are as follows: 

As Remus Rusanu says you do not need rights to run a trace, you need permissions. I don't know anything about your company, but as a DBA in a large public organisation I have much experience of users asking for rights because they want to run a trace to 'figure out what a query is doing...' When asked that question, I don't flatly say no, I explain why it isn't a good idea to run client side traces and to put SQL Profiler in the hands of the users. Sure one of the reasons is long traces can have a performance impact on production systems, which is of course a worry, but there's also the setting up of the profile and the interpreting of the output - you don't want any help with that? The fact that you may have never used Profiler before or understand its complexities and consequences would fill me with worry. I always engage in a bit of dialogue about why server side traces or extended events is potentially better. I ask why they're investigating what the query is doing - maybe I, or one of my team, can help without a trace. It works both ways though, I am wondering if you have fully explained what you want to do to your DBA's or IT Management Team. I think sometimes when people go guns-blazing asking for SysAdmin rights without effectively engaging in a bit of dialogue you end up with closed doors and brick walls (bureaucracy as you call it) rather than collaboration, co-operation and learning experiences. You may have done this of course, and your IT team may just be stubborn - but this is just my two cents. Plus, if I found out that third-party tools were being used in isolation without the authority to do so I would put that user in breach of our acceptable use policy and report it - so please be careful if you're going down that road. Doesn't matter what company you work for - one team, remember? 

...and from the result deduce the subset of invalid gene names. Maybe it's just me, but I pick up some potent code smell from such huge SQL statements. Bottom line, I don't have an entirely satisfactory solution, but since the problem must be very common, I figure that there must be standard solutions for it. If this is correct, please let me know what they are. BTW, assume that access to the DB is read-only. In particular, please rule out any solution that involves creating temporary tables. Also, in case it matters, ours is a MySQL database. 

This "non-solution" consists of defining some distinct values of that can somehow be recognized as , and redefining to make use of this information. For example: 

to clarify the situation described in the question above; to provide table-initialization code that responders can use to test their proposals if they so wish (it's in fact the code I used to generate Listing 2); to give an example of the sort of hard-to-maintain hack that I'm trying to avoid. 

In the research group where I work, we must solve following problem hundreds, maybe thousands of times every day: given a set of putative gene names (typically a few hundred of them), flag those that are not in our (MySQL) database. This problem is solved in a number of ways by our various applications and scripts. I would like to optimize the process. The simplest approach, of course, is to iterate over the list of gene names (after removing any duplicates, of course), and for each gene name perform something like 

I know people will recommend Ola Halengren's scripts but I've always sidestepped them (as good of a resource as they are) simply because I want to be the architect of our maintenance scripts and creating them by myself furthered my understanding of indexes and maintenance plans in general. So with that in mind, here is a pretty flexible and lightweight script that will rebuild (or reorganize) indexes only if necessary, as defined by the percentage fragmentation thresholds. Using a more selective script as per the below, I have been able to reduce index maintenance times massively. We've seen reductions of 15 hours in some of old MP's designed in Management Studio on large databases. 

At the moment I have around 125 production instances, each with a script-based maintenance plan running as an agent job. The tasks run are Index Reorg/Rebuild, Stats updates and Checkdb. Backups are looked after by Netbackup so they dont form part of the plans but for a couple of exceptions. I moved all the instances last year to script-based maintenance plans from plans created with the SSMS wizard (hate those) and they're efficient and effective so overall I'm pleased. I'm wondering whether it's feasible to take things a little further. I've recently been working on a powershell script that, when pointed at an instance, iterates through the databases on that instance and performs those three tasks on demand. My question is whether anyone can see any downside by doing this for all instances, I.e. Having a single powershell script on our DBA server that iterates through a list of instances on a windows schedule and executes the maintenance tasks. Any errors would be handled / written out to logs etc. The main benefit of this in my eyes that we won't be deploying mp jobs to new instances and configuring schedules. We will just be adding the name of any new instance to the instances the script must iterate through. I'd welcome your thoughts.