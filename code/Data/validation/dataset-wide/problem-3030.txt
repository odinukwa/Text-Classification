That science strives towards truth is a major part of Scientific Realism. Given the problem of once popular, now discarded scientific theories, it would be naïve to think that scientific theories are simply true. But sophisticated realists make a variety of claims about how parts of theories are true, or that theories are approximately true. For example, structural realists think that the mathematical structure of mature scientific theories does track the truth. Other kinds of realist claim that certain theoretical terms do refer to parts of nature, and thus do say true things about them. Realism is certainly not the "general view" among philosophers of science, although there are a large number of realists. So what do you call those who oppose realists? Anti-realists? Well, there are a variety of different positions that fall under the umbrella term "anti-realist" and I guess they disagree with each other as much as with realists. (Realists disagree with each other a lot too...) 

There's something really classy about spelling "connexion" with an "x" like that. Damn. The problem for this justification is, of course, that the conformity between "stuff I read on the internet" and reality is much less frequent and uniform than the conformity between reality and "stuff my friends report to me sincerely". So there are additional problems due to the nature of the medium. Bayesianism is an approach to epistemology that has methods for dealing with problems like this: problems of gaining knowledge from faulty signals. 

I reject your premise: X doesn't die to save Y's life because X gets pleasure from doing so (how could a dead agent experience any kind of pleasure?). X dies to save Y for some other reason, believing in some other good: duty, doing the right thing... I think you are confusing pleasure with what might be called "utility". Economists model rational agents as utility maximisers and there is a sense in which you cannot fail to maximise utility, because whatever actions you perform can be rationalised by ascribing you some particular attidtude towards the goods in question. For example, people gamble even when their expected monetary gain from the gamble is negative (so not betting would maximise monetary gain). How is this rational? 'Well,' says the "revealed preference" economist, 'the gambler must get some utility from the act of gambling itself such that that outweighs the potential loss...' To bring this back to "not being a hedonist": things other than pleasure can motivate us. To people sign up to the army because the get pleasure from the horrors of war? Of course not. They enlist because they are motivated by their sense of duty, for instance. Now, to say that they get pleasure from doing their duty is to distort the meaning of "pleasure" beyond recognition. 

Cox's stated aim was to construct a logic of plausible inference. This was to be considered as an extension of the standard (classical) logic. One of Cox's axioms effectively demands that any such logic of plausible inference be compatible with standard logic. (In Van Horn's exposition of the theorem this is called R2). As has been mentioned in the comments, Cox isn't all that clear about exactly what is being assumed (especially, for instance about the space of propositions) but insofar as Cox does make his assumptions clear, his commitment to classical logic is clear. So the answer to the question is no, there is nothing implicit about Cox's assumption of classical logic. 

You can't do much better than Michael Redhead's Incompleteness nonlocality and realism. David Z. Albert's Quantum mechanics and experience is a popular text, but I don't like it: it tries to shield you from the technical details, but you simply can't understand the subject without an understanding of at least the basics of those details. There is a chapter on philosophy of quantum mechanics in the Ashgate companion to philosophy of physics. David Wallace has uploaded a preprint of this piece to ArXiv. R.I.G. Hughes' Structure and interpretation of quantum mechanics is also a highly-regarded book, though I've not read much of it. That covers books that deal with QM's implications for "realism/antirealism, holism, contextuality, causality, determinism, probability theory". As for "consciousness and mysticism": "A physics based reply is very much preferred" — does not compute. You could try Fritjof Capra's "The Tao of Physics". But it is nonsense. Werner Heisenberg's Physics and Philosophy might interest you in this respect too, but likewise, much of it is trite and meaningless. 

In mathematics one sees people talking about sets being "closed under an operation". So a set of numbers is "closed under addition" if a+b is in the set whenever a and b are. 

"Epistemic closure" is a term used in epistemology. An agent satisfies closure when she satisfies the following conditional: 

Define your terms. If round means "every point on the edge is the same distance from the middle" then something extensionless can be round: every point on the edge is a distance of zero from the middle. But arguably you'd want your definition of round to allow ellipses to be round. So what does roundness mean? I'm sure there could be a definition of what it means to be round that doesn't entail having an extension. 

Answers to questions on StackExchange sites seem to fall under the general rubric of "testimony" and thus inherit the problems and strengths of the literature on the epistemology of testimony. When can you know something that is reported to you as opposed to something you perceived yourself? Hume's answer to this was something like "Well, we kind of know from past experience that people are typically trustworthy" (This from the guy famous for undermining induction! I know!) Here's the passage in the Enquiry: 

According to W.V. Quine, what exists is what we quantify over in our best theory of the world. (See for example his On What There Is; a classic of analytic philosophy). Or, in slogan form: "to be is to be the value of a variable". Of course, there are a lot of terms in the above paragraph that need explaining -- quantification, theory, best... -- but that broad approach is a pretty common view across much philosophy in the Anglo-American tradition of the past half century. 

There is, in fact, a lot of work in social epistemology on precisely this question. In short, if the agents are epistemic peers -- i.e. equally good at reasoning, with the same evidence etc -- then there are two views people take on learning of their disagreement. The Equal Weight View has it that the agents ought to adopt some sort of "compromise" position in between their original positions. This seems to not reflect the fact of widespread intransigent disagreements. The alternative is the Right Reasons View which says that each should stick to their guns and not change their mind. This seems to accommodate the facts of there actually being disagreements between peers, but, in my view, seems unmotivated. 

It seems a pretty obvious principle in some sense. But there are two reasons to deny it. First, epistemic closure is an important part of sceptical arguments. Second, satisfying epistemic closure means knowing all the logical truths: knowing all the truths of mathematics. So it is clearly too strong a principle in general. More generally, "closure" in this sense means something like a kind of "completeness". So in logic a set of sentences is "closed under entailment" if the following conditional holds: 

There's lots of stuff on the use of simplicity as a virtue in science. Are simple theories likely to be correct? Why would that be? My take is that things like simplicity might not necessarily be truth conducive, strictly speaking, but there are still good methodological reasons for opting for them. Leaving aside Occam's razor, which has been covered, consider the following: simple theories are easier to falsify that complicated theories. So, if I stick to simple theories, I will falsify false theories quicker, so I will progress faster. I think Ernan McMullin kind of says this, but I can check references if anyone's interested. 

What does it mean to "use" logic? To reason in accordance with it? Is it evident we use classical logic? Is all our reasoning even formalisable as logical inference? Certainly not. Think about any reasoning that involves likely but uncertain events. This doesn't necessarily admit of an obvious translation into a simple logical framework. More esoterically, there are sentences which are "un-first-order-isable". That is, grammatical, meaningful English sentences that can't be formalised in a first-order logic. That is, you need to quantify over predicates to make them amenable to formalisation. The upshot of this is that it probably isn't possible to consistently use logic in everyday situations, whatever logic you use. So perhaps the question should really be: Can you replace classical logic in everyday reasoning with intuitionistic reasoning? Rephrasing: can every everyday use of classical logic be replaced with intuitionistic logic? And the answer there is: "of course not!". Think about all the times you do reason by using excluded middle. None of these inferences will work in intuitionistic logic. 

These are all hopelessly charicatured positions, and there are doubtless plenty of other positions but I hope this gives rough idea. Anjan Chakravartty's SEP article on Realism has a section discussing some of these possibilities. 

Now, as a matter of fact, I don't drop the glass. Thus the antecedent of both conditionals is false, so if they were material conditionals, both would be true conditionals. But intuitively, the first is true, the second false. So we need a better understanding of conditionals like this. David Lewis offered an account of what makes the first true and the second false in terms of possible worlds. The idea is that in the closest possible world where I drop the glass the glass smashes, and it does not turn into a porcupine. Thus the first conditional is true, the second is not. Obviously, the devil is in the detail and much ink has been spilled trying to get a handle on closeness of possible worlds. But the intuitive idea should be clear. I don't think possible worlds fix a flaw or bug in logic: they are a neat conceptual tool to make understanding things easier. 

This depends very much on the area of philosophy. If you're interested in philosophy of quantum mechanics, for instance, you need at least undergraduate level training in physics (and the mathematics that entails). If you're doing ethics or political philosophy, then maybe the need for that sort of knowledge is lessened (although knowledge of some basic logic and some economics would have vastly improved several talks I've had to sit through...). Philosophers of mind need a good understanding of neuroscience and possibly some psychology. Now these are just examples of the maths/science that philosophers need in order to usefully contribute to an area. There is the broader question of what understanding could be recommended even if it's not a prerequisite for doing the philosophical work. A basic knowledge of mathematics and science is always a good thing. Having studied maths gives you a particular way of thinking through problems that Intro to Logic just doesn't. That extra facility with thinking logically is always useful. Then there's the even broader question of what any right thinking person should know. And as a consequence what every philosopher should know. This includes, I think, some basic physics: an understanding of electricity; mechanics and kinematics; conservation of energy... Some basic maths: what a function is; how to calculate a percentage; how to read statistics that crop up in newspapers and adverts (the difference between a relative and an absolute increase...)