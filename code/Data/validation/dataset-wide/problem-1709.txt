You could try the racreset command, this may be available from your OS if you have the OMSA packages installed or you may have to ssh directly the DRAC (assuming it will let you do that). This will reset the DRAC (taking it offline briefly). See - $URL$ 

We have a 3rd party application that runs on an IIS web server, during the day as people use it, it seems to take up more and more memory until the server is very close to the memory limit and we get nagios alerts. Here is a graph of a few hours this morning, IIS was restarted at 11:40, some of this ramp up will be due to people starting work but I suspect not all of it, the spike at 11:20 is of perticular concern. 

We're deploying a wireless networking using Windows Server 2008 NAC as a RADIUS server. When Windows XP or 7 clients connect they initally fail to connect. In order to enable the client to connect we have to add the network manually and un-check the "Validate server certificate" as shown in the screenshot below. Does anyone know of a way to avoid having to do this? We are perfectly willing to buy a certificate from Verisign, Thwarte, etc if it will help but have tried our Comodo wildcard SSL certificate which hasn't fixed it. These machines belong to the end users so we can't easily control settings with group policy or registry hacks. 

I'm trying to get a TDM400P card with FXO module to connect to our PSTN line. The card is correctly detected by Linux: 

For UK/Some European data centres data hop have good summaries containing some info that is not readily available elsewhere although obviously biased toward the ability to cable/put in transit hops. 

Depending on the server operating system / linux flavour it might be as easy as: yum update php or apt-get install php Be aware this will update you to the latest version of php not necessarily 5.3, also be aware this might break things to proceed with caution! if your running a redhat based system you should also be able to find a PHP 5.3 RPM which you can install with rpm -uvh rpmname.rpm if you need a specific version of php that yum can't provide. 

I don't specify any access/secret keys in the command line. My instance role was manually created (by me) and definitely does NOT grant any permissions on resources. 

Is there a way to let anonymous access to a certain S3 bucket only from my EC2 instances (all of them) within a single AWS account? 

Basically what it ways on the tin, how can I create individual per-instance alarms inside an auto-scaling group created with a CloudFormation template? I can reference the ASG itself in an alarm and create ASG-level alarms, but cannot seem to specify dimensions to be "any EC2 instance belonging to this ASG". Is it possible or is my only option user-data script? 

Is it possible to buy an intermediate certificate to use it to sign subdomain certificates? It has to be recognised by browsers and I can't use a wildcard certificate. The search turned up nothing so far. Is anyone issuing such certificates? 

How come I can still read any CF metadata? I noticed that in the client code, the script goes to use instance credentials ( is True) 

What would be the best way to pass sensitive data to EC2 instance (on boot or otherwise) that only root can access? 

When an ASG is launched, the queue ends up with two test notifications from the creation of lifecycle hooks, but no notifications for instance launch. And here is the race condition. object references (and hence depends on it). This dictates the order in which CloudFormation creates resources (the group is created first). The problem is that the group starts launching instances before the hook creation is complete (instance launch is not a part of the template, so it starts executing in parallel). By the time the hook is created, there are no more events to post as the instances were already created. Is there any way to work around it and catch launch events at stack launch time? 

But it's not ideal, as it doesn't handle restarts and remote endpoint downtime very well, because it doesn't have anything like 's pooling, so I'll get duplicate logs and/or drop logs. Given that CoreOS has no package management, is there a conventional way to solve this painlessly? 

I have configured a "ZAP Trunk (DAHDI compatibility Mode)" with the ZAP identifier 1 and an outbound route, but when ever I try to make an external call via it I get the "All Circuits are busy now, please try your call again later message". I have one outbound route which uses the dial pattern 9|. and the Trunk Zap/1 and one Zap Trunk which uses Zap Identifier (trunk name): 1 and has no Dial Rules. The FXO module is directly connected to our phone line from BT via a BT->RJ11 cable. When running tail -f /var/log/asterisk/full and placing a call I get the following output: 

Under Centos the answer appears to be as follows: on bar (the restricted machine) run the following command: 

I have two machines both running CentOS linux, one is public facing machine with a real ip address (foo). The other is at a client's site behind a very restrictive firewall and with no real ip and no possibility of natting or opening an port to it (bar). I can ssh from bar to foo, however obviously not the other way round. Ideally I would like to be able to ssh from foo to bar so I am able to send file, work remotely, etc. Would really appreciate any help or advice on how best to get this working, have seen various tutorials on the internet which suggest it should be possible to setup a VPN connection over ssh but can't quite seem to figure it out. Jona 

I suspect there are refinements to be made to this, but hopefully it will be enough to get any googlers started. Thanks to pkaeding for putting me on the right track. 

Basically No. This is a potentially dangerous misunderstanding, not sure how you came to have it. Perhaps you should try asking a broader question about what your trying to achieve. Also read the wikipedia article on NAT. 

I need help tracking down what module or script in this application might be leaking memory, any ideas or suggestions on tools to extract this data? Thanks Jona 

If a directory "foo" is owned by user A and contains a directory "bar", which is owned by root, user A can simply remove it with , which is logical, because "foo" is writable by user A. But if the directory "bar" contains another root-owned file, the directory can't be removed, because files in it must be removed first, so it becomes empty. But "bar" itself is not writable, so it's not possible to remove files in it. Is there a way around it? Or, convince me otherwise why it's necessary. 

I know I can use IAM roles, but I found it's just too many moving parts, and complicates any scripts that have to use the access key/secret key (e.g. rewriting /etc/apt/* lines when it changes). Not to mention there is no way to attach roles to existing instances, which makes it even more pain. It's also not possible to simply restrict access by using VPC subnet, because S3 bucket access goes via public EC2 interface. 

I cannot use UserData, because anyone can read it. I cannot use private S3 buckets for the same reason (metadata and hence credentials can be accessed by anyone on the box). I'd strongly prefer not to bake my own AMI, as it's quite a hassle. 

I want to get an automated process for AMI creation going, and one piece remaining is automatically cleaning up the instance after image creation. The instance is booted with a user-data script that does the necessary setup, then kicks off image creation from self using AWS CLI. Then it shuts down. I could go with option and wait there until the image is ready, then terminate, but the docs state that "file system integrity on the created image can't be guaranteed", so I want to avoid using it. What's the best way to kill the instance from itself after image creation is completed? 

Within plain EC2 environment, managing access to other AWS resources is fairly straightforward with IAM roles and credentials (automatically fetched from instance metadata). Even easier with CloudFormation, where you can create roles on the fly when you assign a particular application role to an instance. If I wanted to migrate to Docker and have a kind of M-to-N deployment, where I have M machines, and N applications running on it, how should I go about restricting access to per-application AWS resources? Instance metadata is accessible by anyone on the host, so I'd have every application being able to see/modify data of every other application in the same deployment environment. What are the best practices for supplying security credentials to application containers running in such environment? 

There is actually an RFC (1178) regarding best practice in naming computers. The following is discouraged by this RFC: 

We're trialling a Asterisk Now server to take over from our ageing PBX system. One of the "nice to have" features would be the ability to pause or lower the volume on the office jukebox if an incoming call is detected. We currently run a linux jukebox which plays music out of the speakers using mpd and can be controlled by the mpc client. We can manually issue the following command to achieve this: 

I'm guessing I've missed a configuration step somewhere but no idea where, any help greatly appreciated. 

We run a cluster of openvz servers and are looking for a way to automatically graph the content of user_beancounters for all ves. We currently have a fairly rudimentary cron which alerts us when limits are hit but we could like a graphing solution to show us history. Obviously we could roll our own using some fancy bash/php/perl and rrdtool but we're wondering if there are any existing solutions before we go down this path. We current run a cacti/snmp based graphing infrastructure. 

Could you not use MRTG or similar with window SNMP service? To install SNMP in windows 2008 navigate to "Add Feature" in the server setup dialogue. Once installed you should be able to use the Services list to configure and start SNMP. See: How to Install General Info Technet article on SNMP 

Does anyone know how to get asterisk to execute this command or some action that we could hook into when a phone call is incoming to specific extensions? 

When trying to use the rsync daemon as opposed to rsync over ssh you need to open tcp port 873 and ensure that the rsyncd is running on the remote machine. In this case I suspect you actually want to do rsync over ssh in which case changing the double :: to a single : may well be enough to achieve this: 

I'm trying to write a nagios check that will check if a port is a member of (tagged in) a perticular VLAN. I've discovered the following SNMP location SNMPv2-SMI::mib-2.17.7.1.4.5.1.1.40 for checking a ports untagged vlan membership. But can't seem to find an equivlent for tagged. I guess since it is a one -> many relationship it may be more complicated. The switches in question are HP Procurve 2920-48G's Any hints much appreciated Thanks Jona