Again let $X=Y=[0,1]$ and let $\mu\in M(X\times Y)$ be $$ \langle \mu, f \rangle = \int_{[0,1]} f(t,t) \ dt \qquad (f\in C([0,1]^2) $$ so $\mu(E\times F) = |E\cap F|$ the Lesbegue measure of $E\cap F$, for $E,F\subseteq [0,1]$ measurable. This does induce a bounded linear map $T:C([0,1]) \rightarrow M([0,1])$ by $$ \langle T(f), g \rangle = \langle \mu, f\otimes g \rangle = \int_{[0,1]} f(t) g(t) \ dt \qquad (f,g \in C([0,1])). $$ But this is just the same $T$ as I considered above: the inclusion of $C([0,1])$ into $L^1([0,1]) \subseteq M([0,1])$; this is not compact. 

Edit: As Jon Bannon helpfully points out, the original question asked "when does a MASA admit a conditional expectation onto it", and so this answer only says "not always" which isn't really a full answer! 

As a comment on Denis's answer... You have to be a little bit careful about what you mean by "diagonalisation" though. Consider $[0,1]$ with Lebesgue measure, and define $T:L^2[0,1]\rightarrow L^2[0,1]$ by $$ T(f)(x) = x f(x). $$ So $T$ is ``multiplication by $x$''. This is easily seen to be self-adjoint. So if we pick an orthonormal basis for $L^2[0,1]$, we get a self-adjoint matrix $A$ of the type you describe (and it induces a bounded map on $\ell^2$ of course). But notice that $T$ has no eigenvectors or eigenvalues, and thus the same is true of $A$ (assuming we ask for our eigenvectors to be in $\ell^2$, but otherwise I don't see how we can talk about ``eigenvalue''). Moral: you have to give up "diagonalisation" in favour of the more general idea of being "unitarily equivalent to a (real-valued) multiplication operator". 

This 2nd result hints at the connection between spectral synthesis and the ability (or not) to be able to approximate elements in a closed ideals by elements which vanish on a slightly large set than the hull. (This is all in Folland's book "A course in Abstract Harmonic Analysis". You should find a lot more in Hewitt and Ross Vol 2, or Rudin's book, etc.) 

Banach algebras should probably be called Gelfand Algebras, or something similar. I'm not sure of the history here, but presumably the "Banach" is attached because this is the study of "complete" normed algebras. I don't believe that Banach actually did much work on algebras (as opposed to Banach spaces). 

This is not a full answer, as my memory / personal bookshelf is not good enough. But it should give some hints. As the comments suggest, we can reduce this to a question about the Fourier algebra $A(\mathbb T)$. Indeed, the steps are: 

Now consider the strongly continuous one-parameter semigroup $\alpha : t\mapsto M^{it} S M^{-it}$. We analytically extend this, and it turns out that $S$ is in the domain of the $\alpha_i$ if and only if $M^{-1}SM$ is densely defined and bounded, equivalently, $\im(SM) \subseteq \im(M)$. As the domain of $\alpha_i$ is contained in the domain of $\alpha_{i/2}$ this appears to give the claim you want. Reference: Ioana Ciorănescu and László Zsidó, "Analytic generators for one-parameter groups" see $URL$ Especially section 6 at the end. 

This is a very general question-- I'm really after any references to anything similar... Let $A$ be a $C^*$-algebra equipped with a continuous one-parameter group of automorphisms $(\alpha_t)_{t\in\mathbb R}$ and let $\pi:A\rightarrow B(H)$ be a (non-degenerate) *-representation. I'm after conditions which give a $\sigma$-weakly continuous one-parameter group of automorphisms of $B(H)$, say $(\beta_t)$, such that $\pi\circ\alpha_t = \beta_t\circ\pi$. As $\beta_t$ will be implemented as $\beta_t(x) = U_t x U_t^*$ where $(U_t)$ is a continuous group of unitaries (that is, a continuous unitary representation of $\mathbb R$). So I'm exactly asking: 

So, under this generality, it seems to me that we can choose $A$ in a clever way: if we make $A$ smaller, then it gets "easier" to satisfy the condition. So why not take $A$ to be the scalar multiples of the identity-- then $D$ commutes with all of $A$ on the nose. Now let $H=\ell^2$ and let $D$ be multiplication by a sequence of real numbers $(d_n)$. If $z\in\mathbb C$ not an accumulation point of the $(d_n)$ then $(zI-D)^{-1}$ exists and is the multiplication operator by the sequence $(z-d_n)^{-1}$. If $d_n\rightarrow\infty$ then $(z-d_n)^{-1}\rightarrow 0$ and so $(zI-D)^{-1}$ is compact and so $D$ has compact resolvant. Similarly, $e^{-tD^2}$ is the multiplication operator by the sequence $(\exp(-td_n^2))$. This will be trace class if and only if $$ \sum_n \exp(-td_n^2) < \infty $$ So you just need to let $(d_n)$ grow very slowly. For example, set $$ d_n = \big( \log(1/e_n) \big)^{1/2} \implies \exp(-td_n^2) = e_n^{t} $$ where we now just need that $e_n\rightarrow 0$. Let $e_n = 1/2$ for the first $N_2$ terms, then $e_n=1/3$ for next $N_3$ terms, and so on. Then $$ \sum_n \exp(-td_n^2) = \sum_{k\geq 2} \frac{N_k}{k^t}. $$ Pick $N_k \geq k^k$ so that for any $t>0$ if $K>t$ then $$ \sum_n \exp(-td_n^2) \geq \sum_{k\geq K} \frac{N_k}{k^t} \geq \sum_{k\geq K} 1 = \infty. $$ In this example, you could also take $A=c_0$ for a less trivial algebra. 

Okay, so actually, decoding Phillip's work gives what I think is a very nice proof. Let x and y be positive. Let $\epsilon=\|x-y\|$ so as $x-y$ is self-adjoint, it follows that $x \leq y+\epsilon 1$. What seems to be a very standard inequality is that then $x^{1/2} \leq (y+\epsilon 1)^{1/2}$. We then claim that $(y+\epsilon 1)^{1/2} \leq y^{1/2} + \epsilon^{1/2} 1$. This follows by working in the commutative C*-algebra generated by y and 1, and using that $(s+t)^{1/2} \leq s^{1/2} + t^{1/2}$ for positive real numbers s and t. So $$x^{1/2} - y^{1/2} \leq \epsilon^{1/2} 1$$and by symmetry, also $y^{1/2}-x^{1/2}\leq\epsilon^{1/2}1$. Thus $\|x^{1/2}-y^{1/2}\|\leq\epsilon^{1/2}=\|x-y\|^{1/2}$ as required. 

It seems like the question means to set $X=K^n$. The first condition means that $T$ is homogeneous, and the second that $T(k1+x)=T(x)$ for all $x\in X$ and $k\in K$, where $1=(1,\cdots,1)\in X=K^n$. As rpotrie says, move to projective space $PK^{n-1}$. This is the set of lines through the origin, or the $K^n$ mod the equivalence relation that $x \sim kx$ for any $k\not=0$. Write the equivalece class of $(x_1,\cdots,x_n)$ as $[x_1,\cdots,x_n]$. As $T$ is homogeneous, it drops to a map $T:PK^{n-1}\rightarrow K^n$. The second condition is just that $T [x_1+k,\cdots,x_n+k] = T[x_1,\cdots,x_n]$ for any $k\in K$. This is equivalent to $T[0,x_2-x_1,\cdots,x_n-x_1] = T[x_1,\cdots,x_n]$. So it seems to me that $T$ is completely determined by some map (which need satisfy no further conditions at all) $PK^{n-2}\rightarrow K^n$. 

To prove this, argue as above to reduce to a concrete predual $X\subseteq A^*$. The bimodule condition becomes that $X$ is $A$-invariant for the usual action of $A$ on $A^*$. One can check that $X$ being $A$-invariant is actually equivalent to the map $\alpha:A^{**}\rightarrow A$ being an algebra homomorphism. Thus $\ker\alpha$ is a closed 2-sided ideal in $A^{**}$ and so is $*$-closed. This in turn is equivalent to $X$ being $*$-closed, which in turn implies that $\alpha$ is actually a $*$-homomorphism. Then $\alpha$ is a contractive projection which implies that $A$ is isometrically isomorphic $X^*$, hence a von Neumann algebra. (This proof is obviously influenced by Tomiyama's proof that W$^*$-algebras and von Neumann algebras are the same thing.) 

This is a slightly silly comment, but I'll explain why I think it's worth making. The best answer, of course, is don't make mistakes! Of course, we're all human (and, ahem, it's not like I haven't had to correct papers which have already been published). However, I continue to be amazed by how sloppy some papers are which come to me as a referee. These are often by serious, established mathematicians. But I'm talking about people managing to cite their own work incorrectly (or get a definition, which they invented, wrong), or major, paper-breaking errors which I, as the referee, spot almost immediately. I can only imagine that people, once they have proved a result, almost fall over themselves to write it up and send it off. I'm young, and relatively patient, but I imagine that this behaviour is a good way to get rejected out of hand (even if the, corrected, paper is quite nice). To answer the original question: as a referee, I would prefer corrections, but I would agree with Pete Clark that this would make me lose faith in the author(s). If I were an editor: I don't know... 

If $\rho$ is a -homomorphism, then I'd be tempted to use a little bit of C*-algebra theory. Pick a maximal family $\{v_i\}$ of unit vectors in $H$ such that $H_i = \overline{\operatorname{lin}}\{ \rho(f)v_i : f\in C_0(X) \}$ are mutually orthogonal. Then there is a probability measure $\mu_i$ on X such that $(\rho(f)v_i|v_i) = \mu_i(f)$ for $f\in C_0(X)$, and each $H_i$ is unitarily equivalent to $L^2(X,\mu_i)$, with $\rho$ being transformed to the canonical action of $C_0(X)$ on $L^2(X,\mu_i)$. That is, H is just the direct sum of spaces $L^2(X,\mu)$. So, for the moment, let's just suppose that H is $L^2(X,\mu)$. I'm a touch worried that the condition $f\in C_0(U)$ is a little bit weaker than $f$ having support contained in $U$, but modulo some details, surely the definition of "support" in the original post is the same as the usual definition of support for a measure. If then $g\in C_0(X)$ vanishes on the support, then immediately $\int_X |g| d\mu = 0$ (if one believes Wikipedia). If $H$ is the direct sum of $L^2(X,\mu)$, and $v=\sum a_i v_i$ say, then if $J=\{i:a_i\not=0\}$ we have that $\rho(f)v=0$ if and only if $\rho(f)v_j=0$ for all $j\in J$. So things get a bit tricky here, as if $J$ is infinite, the support of $v$ is probably the closure of union of the supports of the $\mu_j$, for $j\in J$. But if $g\in C_0(X)$ then vanishes on this, it has to vanish on $supp(\mu_j)$ for all $j\in J$, which is enough to show that $\rho(g)v_j=0$ for all $j$, showing $\rho(g)v=0$. I hope... But, it seems that this argument isn't easier than Victors... 

It's a standard result that a linear functional from a Banach space to the underlying field (real or complex numbers) is continuous if and only if the its kernel is closed. Notice that its kernel is of codimension one. So, use the axiom of choice to find a discontinuous linear functional, and you have found a codimension one subspace which isn't closed. (As I was typing this, rpotrie got the same answer...) As for complementation: well, this only makes sense for closed finite codimension subspaces. But then it's a perfectly reasonable question, and the answer is "yes". If F is of finite codimension in E, then by definition we can find a basis $\{x_1,\cdots,x_n\}$ for E/F. For each $k$ let $x_k^*$ be the linear functional on $E/F$ dual to $x_k$, so $x_k^*(x_j) = \delta_{jk}$. Then let $\mu_k$ be the composition of $E \rightarrow E/F$ with $x_k^*$. Finally, pick $y_k\in E$ with $y_k+F=x_k$. Then the map $$T:E\rightarrow E; x\mapsto \sum_k \mu_k(x) y_k$$ is a projection of $E$ onto the span of the $y_k$, and $I-T$ will be a projection onto $F$ (unless I've messed something up, which is possible).