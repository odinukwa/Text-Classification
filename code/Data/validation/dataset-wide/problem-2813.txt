Descartes: The cogito works most reliably for an actually existing sufficiently rational subject. A madman could try radically doubt everything at the same time. In that case it can happen that the cogito fails to work, and the madman is unable to convince himself of his own existence. A rational subject only doubts a manageably small number of assumptions at a single instant in time. I explicitly mentioned the difference between rational doubt and madness, because my evil demon thought experiment comes quite close to actual madness. Wittgenstein: Why do you say "works most reliably"? Do you want to imply that the cogito could fail for me, even so I'm sufficiently rational? Well, let's try it: "I doubt that your proposed doubt is actually possible, therefore I actually exist." Hmm..., somehow I'm still not really convinced that I exist. Descartes: Strange, it worked for me. I doubted, therefore I existed. But you are right, I seem to be unable to convince me now that I actually existed. It probably doesn't work for past or future existence, just for present existence. Well, let's try it: "I doubt I actually existed, therefore I actually exist." Strange, it really doesn't work. Maybe I don't actually exist? Yes, of course, I died a long time ago, so maybe I no longer exist. Or maybe I still exist, but I'm not the subject that does the thinking and doubting here. Wittgenstein: OK, you seem to admit that the cogito doesn't work unconditionally in all cases. But what about doubting the "laws of logic"? Won't this prevent you from drawing any conclusion at all? Descartes: Of course I have to doubt the "laws of logic", and I explicitly did this. This is one reason for the evil demon thought experiment. The "laws of logic" are an idealization of a paper and pencil reasoning process anyway, and hence don't model rational human reasoning in time, monologue and dialogue accurately. I don't need modus ponens for my conclusion, all I need is to convince myself beyond doubt of my own existence. Wittgenstein: But why should it be important for anybody but you, if you can convince yourself of your own existence beyond doubt? Descartes: Please don't change the subject, just because I managed to defend cogito against your objections. Anyway, thanks for this interesting exchange. 

Saul Kripke, in his paper "An Outline of a Theory of Truth", talks a little about sentences that don't receive determinate truth values in virtue of their logical structure, such as the self-referential examples you're talking about. Kripke calls these "Ungrounded" sentences. The general intuition behind the idea of ungroundedness is that the valuation of some sentences might be totally unsettled by the way the world is taken to be, and that this shouldn't stop us being able to account for a theory of those sentences that do receive a determinate and positive truth value. On the face of it, "Every kind of berry is something that Alice likes" doesn't appear to be ungrounded in Kripke's sense. If a "kind of berry" is something that Alice can have an attitude towards, and Alice has a positive attitude towards every such kind of berry, then the sentence works out true, and otherwise, it works out false. Now we can reasonably discuss the possibility that neither kinds of berries nor the attitude of liking are usefully described with the particular logical structure that underlies the standard modelling view I've supposed here, but then we need to ask more about what it is you mean when you're talking about berries and liking. But! There may well be other sentences that work out as ungrounded that aren't purely self-referential. Consider the following example Kripke gives. Richard Nixon is well known for having told a lot of lies about his involvement in the Watergate affair. So suppose that someone, named Jones, who has otherwise nothing to say about the scandal, claims of Nixon that: 

Honestly, I don't see the symmetry of NJ you are referring to. There may be some symmetries in the introduction and elimination rule for "and", perhaps even for "implication", but calling the rules for "or" symmetric feels strange. But you can get a similar symmetry as in the sequent calculus by using the rule 

There is a huge difference between believing that the (potential) infinite exists as a useful abstraction of properties present in the real world (and being able to give concrete examples for this), and believing that the (actual) infinite is consistent but dismissing the importance of its relation to the real world. 

What strikes me as misleading in this proof is that some sort of conservation law for existence seems to be suggested. If we recognize that there is no such law, all that remains is the question why the world itself exists at all. This is certainly a good question. I have no good answer for it. But does this prove that god is responsible for its existence? I guess god created the natural numbers, including 42. And 42 is the answer why the world itself exists at all. But how could god create the natural numbers, if they didn't exist already? Well, perhaps god just created and maintained the order between them. So does this mean that god created and maintained the order of our world? The atheists prefer explanations like evolution instead. Even if evolution would not be the whole truth and competing theories like catastrophism should turn out to contain some truth, no god is required to explain the order of our world. 

The really tricky aspect of this question concerns the age-old problem of whether Mathematics is supposed to be a representation of something (say, relationships between things in the world on which we might establish some sort of predictive capacity) or whether its subject matter is privileged and ontologically substantial in its own right (as genuine objects in a fully reduced theory of science or as platonically distinct parts of reality that the mind is essentially linked to). It seems like an important criterion for something to count as a form of art that it has intentional content (even if we do not take the artist's intent to be any part of the appropriate interpretation of art), or can be considered a depiction using a representational medium (this depiction need not be "of" anything in particular, or even be unambiguous or unequivocal). It would disqualify something from being art if it was a matter of bare existence. Plato's writings about foundational metaphysics and the forms may well count as works of art, and even a logical description of his theory might at a stretch be considered a work of art. On only one account would Plato's forms themselves, were they to exist, be considered works of art - Creationism. On any other interpretation, the forms would not be works, art or otherwise; they simply are, independent of any kind of intentionality, and this rules them out from any claim to constituting artistry. Realist mathematicians aren't content to say that all there is to their subject matter is a collection of pieces of work in their proofs and theorems. On the Realist account, proofs and theorems are truthful because Mathematics is a factual discipline, and this needs to be so in order for its practice to be legitimate as a tool for other aspects of scientific analysis. As Frege remarked in his Basic Laws of Arithmetic, we invest our efforts in mathematics on a presumption of its unequivocal interpretation, and in order for this to be so, 

The concept of classification seems obvious first, in the sense that you recognize a classification when you see one, especially if it's a nice classification. Even nice classifications leave enough room for philosophical questions, and ontology deals with some of these. However, sometimes no nice classification exists, and just completely abadoning established classification principles and going with something completely unrelated like rhizomes instead doesn't really address the concept of classification. One may look for mathematical branches instead, and claim for example that lattice and order theory subsume the concept of classification. However, this would ignore the fact that the actually published results focus on mathematically interesting problems instead of investigating practical questions related to classifications. Approaches like formal concept analysis try to restructure such mathematical developments with philosophical notions like extension and intension. 

If we go back to the empty set again and look at intersection instead of union, then the intersection of the empty set with any set X is the empty set. However, this property is not a good way to characterize or imagine Nothing, because it gives the wrong impression that Nothing and Everything would somehow be the same thing. But Nothing exists naturally in many different contexts, whereas the existence of Everything would lead to inconsistencies in many different contexts (and hence often doesn't exist). 

Either blocking Lem or Exp is sufficient to prevent the inference to 11 from 7. Blocking Lem gives you a theory of value 1 sentences in Kleene Strong 3-valued Logic (wikipedia); blocking Exp gives you Graham Priest's LP logic, as presented in his In Contradiction (1987), which preserves both the value 1 and value 1/2 sentences under the same semantics. Some useful observations about each move: 

is easily true, because Texas is in America, and therefore every situation in which someone is a Texan is also a situation in which they're an American. But 2. is clearly false, because California is in America, and because there are Californians that are not Texans. There is a situation in which we have an American (a Californian) that is not a Texan. So 2. fails, even where 1. succeeds, and hence we have an example of the Asymmetricality of If-Then. 

So there are at least two interesting Dialetheist responses to your question. The first is to pull apart propositional negation from negation in an Assertoric Context. The second is to say that even propositional negation has a modal sense that classical predicate logic doesn't capture effectively. Priest on Asserting Negations Firstly, a couple of different ideas get caught up in the idea of talking about a proposition and "its negation". Here's how we do things classically. An atomic sentence of the form is true (/assigned the value T/1 /deemed assertable/ etc.) if, and only if, the object in the domain that our model interprets as being the referent of is a member of the subset of the domain that our model interprets as being the extension of . If this is not the case, the sentence is not deemed true. Now, the conditions for are that it is true (...) if, and only if, is not true. So in effect, if the referent of c is not in the extension of P, then . Set theory is classical, being a member of P is a wholly extensional matter, and so it seems negation is entirely determinate. Nonetheless, the classical view might be read as collapsing a distinction of value to the process of interpreting what negations are. The negation function, Â¬, is a regular propositional operator - you "negate" a proposition (i.e. is an instruction, rather than a term), and in the process get another that constitutes "the negative" of A, . This is a process that is well defined for logical complexes, no matter what your basic logic is. But for an atomic sentence, is it always obvious what its negation amounts to? When you assert that something is not true, you presumably have to have some particular positive content in mind that backs this up - saying that the banana is not yellow ought to mean you have good reason to think it is some other colour that conflicts with its being yellow. In this, Graham Priest in both his In Contradiction (1978) and Doubt Truth to be a Liar (2005) affirms that when it comes to Truth, we need to understand its propositional content as tied to the act of Asserting something. Specifically, Truth is supposed to be the ideal form of assertion - when you say something, you do it in the aim of its being true, and that's what being true consists in. Assertion might, we think, have some kind of deontic component - what people can and/or ought to assert is subject to some conventional norms, with the above suggestion that you can back up your claims with good evidence being one possibility. Truth is exactly the ideal of asserting, and theories of truth are thus supposed to be theories that capture these norms. As such, the semantics of Truth are given in the norms of assertion, rather than in the ontological structure of the world. (In Maths, for instance, the semantics of our language is determined by what you can prove, and the proofs you can construct, rather than the objects out there in the world) This move allows for so many divergent factors in an account of what negations, negating, contradiction, assertion and truth have to do with one another. We might think that for a number of cases, asserting a negation and not asserting come apart; we might say that the negating function in an assertoric context doesn't always correctly grasp a proposition's "actual" negation, we might say that you can have prima facie and ultima facie contradictions depending on whether we have an internal or external algebra of negation, we might say that you can assert a statement without failing to assert its negation etc. I have good evidence to suggest that your banana is yellow. I also have good evidence to suggest that it's actually brown (see, look at all of those splodges and bruise marks). So, if our norms of assertion commit me to stating everything that I have good evidence to suggest, rather than hiding some of them for the sake of personal convenience or coherence with a preferred theory, I ought to really say that both of them are true, even when we might say that the banana being brown actually satisfies the assertive conditions for it not being yellow. This doesn't mean I'm committed to asserting that the moon is flat and that pigs can fly; so I should accept that my assertions here are driven by a dialethic logic that can process some contradictions without falling into trivialism. The problem with this line as I see it is that it deliberately seems to avoid the challenge of the hardcore realist. Fine, assertions might be internally contradictory and this doesn't necessarily mean you're committed to asserting anything and everything. Dialethic logic as a way of representing and processing how people talk (or their commitments to talk) has some interesting applied use. But you only get there by talking about contrary states of affairs, that seem to have a certain amount of tension between one another (represented in assertions by the negating function). There's nothing actually contradictory about the state of the banana: it's just both brown and yellow in certain respects. You might say in your assertions . But when you say , your negating operator isn't returning the state of affairs that the realist will call - it's hitting a proposition and mislabeling it . This might be cute as a linguistic or behavioural theory, but it's not really logic with respect to the classification of the structures of facts. There aren't really any true contradictions. (I'm being deliberately harsh here for the sake of answering your question; there's merit to the thought that logic should be considered the theory of warranted psychological inference rather than ontological structure, that the semantics of logic should be captured in human and/or social language use, and that Priest's logic has a much better grasp than his classical counterparts) Routley/Meyer Negation To go for a more radical, metaphysical claim about the existence of true contradictions, all Priest needs to do is to say that there are determinate and ultimately dialethic principles of assertion in our basic metaphysics that correctly latch on to the world. Although I don't think he makes such an assertion explicit (he does, for instance, defend the possibility of a foundational logic, but never specifically asserts what that foundational logic is supposed to theoretically cover) he does present arguments for dialetheism in the empirical sciences, which is good news if you're a naturalist. Not, though, if you're a classical set-theoretic structuralist. You're probably just going to accept that the rules of assertion in set theory are those of classical logic. An alternative, direct account of negation in a semantic dialethic setting is presented by J. C. Beall in his Spandrels of Truth (2008), where he explicitly invokes ideas found in Relevance logics. Beall borrows from Routley/Meyer's semantics, arguing that negation is centrally modal and that in order to understand the negation of a particular proposition at a possible world, we must look at some other well-specified possible world that represents opposing states of affairs. The Routley-Meyer semantics is a generalization of Kripke frames. Routley's Star operator takes each world in the domain to a dual world, such that . The semantics for negation are defined in terms of this star operator: 

Jaffe justifies to say "experimental mathematics" for the activity of finding proof of theorems via: 

At least for the German language, the notions Extension und Intension come from the context of Aristotelian logic and where established by the Port-Royal Logic: 

The meta-theory in which a model of a formal system lives should not be confused with metaphysics. The intended structure "A" is normally given explicitly in the meta-theory. Note also that isomorphism is a well defined notion in the meta-theory. Because "Th(A)" (= the set of formulas valid for A) is often not "computable", we will take a recursively enumerable (="computable") subset of formulas $\Phi$ from "Th(A)", which captures essentially all important aspects of "A". A model "B" of $\Phi$ with "Th(B) != Th(A)" will be called non-standard model. There is also the case of a non-isomorphic model "B" with "Th(B) == Th(A)", but even so this is not a "standard model", we wouldn't necessarily call this a non-standard model. 

Edit I just tried to get access to the tertiary literature again from where I took the information in this answer, in order to be able to cite the sources they reference. This wasn't successful, but I found that most balanced accounts of the Socrates trial also contain the information to which I referred in this answer. Hopefully the following quote clarifies the significance of the "contrived reasons": 

These implications both go through. In Philosophy, rather than talking about two different propositions here, we sometimes use a shorthand form, which might be what's confusing you. This shorthand form is written as follows: 

Some further thinking Of course, you may find that this analysis of your concept lacks some of the force and utility of the concepts of evenness it extends. For instance, in the integer case, either an integer is even or it is not, and if it is not, we say that it is an odd integer. In your new rational case, however, while it is true that either a rational is even or it isn't, it doesn't seem natural to say that if it isn't it's odd. Why not? Because then the odd numbers would consist of all of the numbers whose last digits are odd, and also all of the numbers that don't have last digits. So, say, () would be odd on this suggestion. Why should rational odd be the property that gets all of the non-terminating ones - why wasn't rational even covering these instead? This throws up questions about whether our analysis of rational evens has missed something out. Now maybe you might like to do some development work and add further refinements of your concept of even numbers. So let's say we add a "stability" qualifier to our definition of even numbers, such that in addition to the numbers we have above (where we have a last digit), we also add another disjoint condition: 

Henkin semantics is important for higher order logic, but keep in mind that it is not automatically fully specified in every situation. Especially, are there any comprehension axioms at all, and if there are comprehension axioms, should they be predicative (i.e. avoid "inner" quantification over higher order variables) or impredicative (i.e. using arbitrary formulas including quantification over higher order variables as if they were first order variables). Also note that the comprehension axioms can have a sort of "last word" property, i.e. if you introduce additional non-logical symbols into the language, all additional comprehension axioms enabled by these additional symbols become immediately available. Worse still, consider that the context of the logic can be extended to include Henkin quantifiers, cardinality quantifiers (or "nasty" transfinite quantifiers) or forms of infinitary logic (since we must assume an underlying set theory anyway). But all this additional expressivity of the language is automatically available for the comprehension axioms too. (I'm not so sure about impredicative comprehension axioms here, since they might quickly lead to paradoxes in case the language is extended beyong pure first-order logic.) On the other hand, consider also the case where quantification over higher order variables is nearly absent, like in pure propositional logic where one just has a formula including propositions, and just want to know whether the formula is satisfiable (corresponding for existential quantification over all propositions), or whether the formula is a tautology (corresponding to universal quantification over all propositions). If we treat unquantified higher order variables in the same way, will standard semantics actually be different from Henkin semantics?