There is a huge diversity of opinion inside quantum mechanics as to what constitutes an observer. But there are standard examples of where it matters. Someone in comments already noted this one $URL$ A continuous version that gives more of an idea of what 'partially observing' might be is described here $URL$ More bizarre combinations also occur. For instance, you can split light into polarized beams that then combine in ways you would think their polarization should rule out $URL$ Many people, including advanced physicists like Roger Penrose [The Road to Reality] and many other interesting thinkers [I loved "What the Bleep"], argue from this for something not reducible to mechanics. This is a stretch to begin with, but [sorry to harp on a pet point] if you adopt a view that allows for reversible time, even the ghost of the implication goes away. If the particles are not so much 'put in their place' by observation, as 'tied to history' by having traceable effects later in time, then reversible mechanical actions can explain what is going on. If there is a perpendicular dimension of time which our own time can move independently across, we can imagine a point of view from which our timeline is a complicated curve in space which occasionally reverses for short periods, but always eventually makes progress in one direction. Boltzmann suggests there is an entropy low-point in 'real time' and we rely on accumulating entropy because of our proximity to that. (Hard to find non-paper references -- See something like $URL$ This would suggest our phenomenological time that requires entropy for memory encoding might vary somewhat independently of some more basic 'realer' time along which entropy tends to increase, with minor variations. Our phenomenological time then can be seen as one dimension of time, and these variations happen along a perpendicular across it. As long as the information is lost, any mechanical process is free to flow back and forth, retracing many possible paths to the future. But if the information about this past is referenced in the future, then only paths that actually get to that future go through this past. Many repetitions and false-starts, that get reversed out before flowing that far forward, are ruled out by choosing to use that information later. This was proposed seriously by Richard Feynman, but he entertained it rather lightly. In particular, he proposed simplifying our particle model by modelling antiparticles as real particles "caught in the middle of" travelling backward in time $URL$ To my mind, it gives us a less mysterious and less 'ontologically wasteful' approach to explaining quantum indeterminacy than "Many-Worlds" or presumptions that 'observing' has a time-bound meaning, and immediate significance. Instead, observation is simply the future use of information by any surviving process. It does not require an infinite degree of redundancy in representations, only a finite-but-arbitrary one, modeled easily by a second time dimension (in imaginary time $URL$ or a model of time that is focused directly on entropy, and not explicitly dimensional, and therefore has many more independent dimensions. (Basically, if time had hundreds of dimensions, how could we know? We could only measure the magnitude of the part of time along our experience, and declare the rest independent of that. So macroscopically, multidimensional time looks like complex time.) So there is at least one interpretation of quantum indeterminacy that can retain materialism. (Even if that is a materialism with multi-dimensional time.) 

This kind of approach would declare the whole of Scholastic philosophy one big fallacy. And it would be unfair to go that far. Rejecting your sources of authority, and adopting their own is not a fallacy. But it does excluded them from the cultural mainstream in a way that is ultimately not maintainable. For instance, 'Biblical archaeologists' happily use carbon dating, which they cannot consider valid evidence if they reject other deductions from carbon dating... You end up back at the point where light was not refrangible by water before The Flood, because rainbows came into existence at a recorded point in Biblical history. So God just changed physics, at least twice. And that means nobody has any evidence. Who can argue? 

If you are a true physicalist, then in practice, given the limits of time and process there is a largest number that will ever be used. That does not mean it is in principle some magical kind of limit, but numbers beyond that are simply irrelevant. But who are we, now, to decide what number that will be? Why not be halfway modest and act as if it is far beyond anything we can imagine? Why not plan for a long future? You choose that direction, you can abandon infinity, but you have to allow for continual increase, anyway. From a 'Nonstandard Analysis" point of view, the elements of sets like the Real numbers, or the integers with infinity are not real, but are axiomatic definitions masquerading as things. (Two is the property of having distinct things but as few as possible, etc.) Countable infinity, as the number with every number you have encountered as predecessors, but no immediate predecessor, is the shorthand for encoding continual increase. It is that biggest number ever used, forever getting away from us, slipping away into the future. Any such axiomatic definition, expressed in a pattern that can be written down, is a recognition mechanism, that can be rearranged for use as a generating mechanism. For instance, the native Intuitionist model (a la Brower) of a real number is "a freely flowing stream of bits." Every real number is process that will always hand you the next digit of precision. The number itself is treated like a point in space, but underlying it is really an ongoing approximation. Given the notion that any rule can be looked at as a process, all other useful applications of infinity can be re-encoded in a similar form. So it is perfectly reasonable to think of the numerical parts of mathematics as good thinking about measurements and approximations and their ultimate limitations even when you are 'taking limits as x goes to infinity', or dividing two things both 'going to zero'. Things like infinite groups, etc. abstract that underlying mechanism away, assuming it can be captured faithfully in an intuition and ripped away from its more concrete forms. If you are not willing to make that leap, then you can stick to geometries and finite structures, and assume the nominally infinite ones do not have any applications that will interest you. If you do make that leap, you have moved from computation to psychology. By making assumptions that human intuitions around things like infinity or continuity have an interest of their own, and that the fascination we feel for them has some basis, you can embark on a kind of deeply psychological art, either out of interest in the psychology, or attraction to the art. Some of the products of that art turn out to have representations in reality, that make certain kinds of other things easier to imagine. Much like other kinds of stories help us get through life. But these stories are always 'Roman a Clef', we know where the characters come from. So the representations can be unwound back into finite terms and modeled in computation when they have genuine applications. The question is why we can get from computation to art and back to computation easier by allowing ourselves a certain level of excess in the art than we can by sticking with reality. Basically, why is the human mathematical intuition a stronger tool than its motivation, if everything it models beyond its concrete applications is really not there? It is the same question that makes language fascinating. If the universe is basically physical and evolution is what drives most of this, then why on earth would we evolve something so much more powerful that evolution itself, (solving the same kinds of problems hundreds of times faster) and then use that to create another kind of evolution altogether (competing ideologies and cultures)? (Reality is enough. No one needs lies. But as Nietzsche points out, we have not yet begun to even estimate their power.) 

It seems that authenticity is an automatic requirement of autonomy in the sense Kant is using it. The first form of the CI contains a second person reference on purpose. The 'never mere means' version requires us to respect others as ends, who get served -- in order to be served one to take a given perspective on what is supposed to constitute service. The will is paramount, and the will is only really a will if it is authentic. (Later edit: realize I forgot to answer the last question) Truthfulness is a consequence of a genuine respect for autonomy. But it is not necessarily an aspect of autonomy itself. Authenticity definitely does not automatically dictate truthfulness toward others. We might wish to be true only to our own nature, and not to the nature of our species. (Then, a la Nietzsche, we should properly esteem the lie.) But (for Kant) if we are true to ourselves at a deeper level, we respect our species and care for the ability of others to be as authentic as ourselves. That requires they have good information and that we remain open to their ability to adapt and change. 

Empirical truth is what we can most easily verify that we share. Experiences and experiments can often be recreated, if the phenomena involved in framing them are actually understood and documented well. Subjective or intuitive truth may often be just as real, but it has historically been manipulated by those in power to one degree or another. So there is a real motive for trying to ground things in empirical truth whenever possible. We do not wish to be manipulated, and therefore empirical explanations have a democratic element to them. Democracy and equality are imperatives in modern society, (especially, strangely, among our elites, who also tend to be condescending in the way you are complaining about.) That said, Hume has never been completely refuted: At some level, empirical truth also always relies upon mechanisms that we know are limited or misleading. It has to fall back on some more abstract explanation to smooth out the errors we know are implicit to measurement and perception. And those correctives must ultimately come from somewhere else. So philosophy has a long history of working back and forth between these. Some folks now somehow imagine this can stop. Now that philosophy has spawned science as a discipline that considers itself independent, that part has taken up disowning the rest. But every thought that underlies a scientific theory is, at root, philosophy. Theories cannot make themselves, they have to be contrived out of intuitive or metaphorical material. And the meaningfulness of scientific observations is based in theory. Science is free to try to dismiss philosophy, but it is philosophy, just of a circumscribed sort. Trying to undermine all the other forms of reasoning will eventually backfire in a lack of source material for framing meaningful theories. 

To the degree it proceeds along the same trajectory at all, Neoliberalism here is a step far before Postmodernism. It is the embodiment of Enlightenment thinking in a wish to make more and more dependent upon less and less so that we are all 'free' from one another and from the harsher demands of our reality. It is primarily an economic theory that reduces everything to market forces and freedom. In that, it has a lot more to do with Existentialism than Postmodernism. As 'The Genealogy of Morals' or Marxism points out, this war of the powerful constantly undermining the previous means of being powerful has been going on forever. And it is wholly unrelated to the ultimate loss of the modernist dream to its own logic. Neoliberalism is totally consonant with late modernism -- taking science and individuality as the center of the universe and rendering it cold, sterile, independent and unattached to any deeper meaning. This attempt to make the universe rational and efficient is the ascending direction of the 'modernism' that Postmodernism questions. It is part of what Postmodernism actually resists, by emphasizing context and admitting the relevance of the aspects of reality that we cannot know. Displacing someone from their cultural embedding as a strategy for power, by controverting their religion with science and displacing their cultural institutions with your own, or with your own rational constructions, does not assume a respectful and constructive relativism. It assumes that relativism is wrong and that dedication to institutions that are not entirely modern and rational is primitive and wasteful. It seeks to leverage that 'waste' for 'good', without realizing that 'good' is selfishly defined and is itself something wasteful of what is embodied in traditions themselves over time.