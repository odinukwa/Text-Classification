If you remove from primary replica means you are completely removing it from AOAG. You have to remove it from secondary replica as sated above. 

This error message is for clustered index (index ID 1), the corruption is in clustered index which basically means the table. Running repair is most likely to give you data loss. Cause: Corruption doesnt comes out of blue, the most likely cause is bad hardware or may be bad memory stick. Your task is to find out from SQL Server errorlog and windows event viewer what is that. Unless you find it and fix it your issue is not solved. Advise: As I see here restoring from good backup is the most preferred option. Hope you have one 

It is fully logged operation and is severely bad process so ALWAYS try to avoid it. What it does is, when you hit the shrink button, quoting from Paul Randal blog 

What other processes are running on system is OS under load how many CPU cores does system have ? EDIT: After user pasted output The output does not gives and concrete picture use my query. Also are you running extended events traces because i can see XE wait types. I consider this wait type as harmful and it seems you are not facing a issue. Monitoring tools sometimes overreact so with result you posted I would just think its normal behavior. EDIT2: I dont find any issue with the wait stats output you posted. I also assume your server was not restarted recently otherwise waits stats will not be useful. 

Note: Quite a few times you would find SP1 embedded in SQL Server 2014 installation files in that case you dont need to download Sp1 you just need to download CU6. When SP1 is embedded it wuld be mentioned in product software like . If you have option go for this one. 

Click on permission and then on page that occurs make sure you account is there if it is not there add you account and click on check box full control and then click on apply. It would take some time and then click ok. 

Query is requesting so much memory grant that SQL Server is only able to provide it a limited amount, this limited amount is called "Required Memory", so that it starts executing and while executing the query, because memory requirement was large and SQL Server cannot provide it as there was no memory in resource pool, the query fails with OOM error. The memory required when query is running is called "Additional Memory" There was Bug fixed in SQL Server 2012 Sp1 CU4 where query requested huge amount of memory grant causing it to be drastically slow or subsequently failing with OOM error. The possibility that bug resurfaced cannot be ruled out considering fact that QEReservations hogged all of the buffer pool Since the clerk has already taken 90 % of memory. Required Memory for new query is not available and query fails with OOM error. Your tables and indexes has skewed statistics which is forcing optimizer to build sub optimal plan causing it to request much more memory grant than actually required and in turn creating issues. Lastly the queries running on SQL Server requires some serious tuning. 

Suppose there is other request which comes and asks to read same page. SQL Server Buffer Manager will check the data cache in the buffer pool to see if it already has the page cached in memory. If the page is already cached, then the results are passed back to the Access Methods(Access Methods contains set of code which handles I/O requests for rows, indexes, pages, allocations and row versions) to continue with operation as requested. If the page isn’t already in cache, then the Buffer Manager will get the page from the database on disk, put it in the data cache, and pass the results to the Access Methods. 

To allow the backup file to grow only as needed to reach its final size, use trace flag 3042. Trace flag 3042 causes the backup operation to bypass the default backup compression pre-allocation algorithm. This trace flag is useful if you need to save on space by allocating only the actual size required for the compressed backup. However, using this trace flag might cause a slight performance penalty (a possible increase in the duration of the backup operation). When you start compressed backup you would see some size of backup file created on the drive but this would not be the correct size during backup operation it can grow and final size would increase. You can initiate a backup with compression and see what is the size then tentatively multiply it by number of days you want to keep it plus few more space. This would again give you tentative size of backup drive. I would also say it would be safer to more space to backup drive 

Scenario 3 Not all crash dumps are because of bug in SQL Server many occur due to poorly configured SQL Server or some rouge queries running. But since you have not shared detailed errorlog it is difficult to say at this point. Make sure your SQL Server is configured correctly. Again if such is the case MS support will point this out. Moral: If SQL Server is not updated with latest SP first update it, look for the cumulative updates released after the SP(you can get that from first link I have shared) and make sure bug you are facing is not fixed in CU releases ONLY then open case with Microsoft. 

I guess I know the answer here and have seen many times. Can you please check if auto shrink is on for the databases. You can use below query 

Prior to SQL 2012, the buffer pool both “managed” memory and was a consumer of memory for database pages. It’s management of memory meant it allocated 8Kb pages of memory for other consumers like plan cache. Roll forward to 2012+, the buffer pool is a pure consumer of memory from SQLOS which manages all of the memory. 

Note if you have heavy disk activity do not forget to refer Disk related counters as well. Create a data collector set and allow it to run for 4-5 hrs when load on system is at peak and then add the snapshot of data collector in your question. Then we can determine whether SQL Server needs more memory or not. Personally 8G is a bit of less RAM considering workload and OS requirements these days. At the back of your head you should always think about increasing RAM. 

So you can see more the number of times plan is reused the more its age gets and lesser is the chance for it to get removed. So when plan is created and it is been referenced 0 number of times after creation the plan would be eventually be removed taking into other factors as well. I am sure there is much more to what I have written and that can be found from SQL Server 2008 execution plan and reuse. 

Shrinking of log file ON DAILY BASIS like one you do with query you posted is seriously bad and you should NOT do that. The main reason is that after you have shrinked the file it will grow again when new transaction comes and since INSTANT FILE INITIALIZATION is not there for Log files, its only for data files, when log file will grow DB engine will actually have to zero out space to write information so instead of overwriting free space it has to be overwritten with zero values (0×0) and then information can be written. This process takes time and thus delays processing of query Nice explanation has been done in This link Please note that instant file initialization is applied to some extent for Tempdb log files even though this is correct you should not shrink tempdb log files on daly basis. A more simple thought, why to shrink if space has to eventually grow when log informations will be written and log will grow. Its a bad practice, internet is filled with advise not to shrink data files but shrinking log file is equally bad EDIT: Needless to say( as already pointed out) you are breaking chain of logs by changing recovery model to simple. You loos epoint in time recovery. If point in time recovery is not required kep recovery model of database as simple and log truncations would be handled by DB engine if something is not holding logs and need it in transaction. 

To see all column names which fn_dblog would produce Example: I would show what all things are logged when simple update command is fired 

There would be no damage as such, what checkpoint does is it writes dirty pages from buffer pool to the disk reducing the amount of recovery time and amount of work SQL Server has to do after restart or crash recovery. In extreme case if database goes under crash recovery the amount of time taken to recover database would increase. In your case it is good you found out that it was actually the index rebuild which produced some good amount of logs and eventually failed because of low disk space on log drive. This would just rollback the operation and would bring fragmentation as it was before the rebuild 

In case of failover, and both nodes A and B down, yes node c will act as owner of both instance running on Node A and Node B. 

You can leave it to default for all instances because since it is SQL Server 2008 R2 standard edition it will not use more than 64 GB and you ahve total of 256 GB RAM so you are on safer side. Please also note that 64GB limitation is for only other components like SSIS,SSAS,SSRS does not falls under this restriction and yes they can also consume good amount of memory. 

Disable Windows security/windows defender on Windows 10 machine. The windows security is most likely causing this issue. Note that your installation waits on getting access to file where it can write something. The windows security is stopping the installation from getting the access Please also disable Antivirus for time being when you are installing the SQL Server software. After installation finishes you can again enable it. 

No you cannot do inplace upgrade from SQL server 2008 R2 express to SQL server 2014 developer edition. Its not supported as per Version and edition upgrade supported matrix You can only upgrade to below using inplace upgrade method 

Click on view installed updates and you would see all updates for your computer and programs installed on it. Locate SQL Server 2008 R2 Sp3, I have SQL Server 2014 SP2 and it shows like. Right click on the Service pack and select uninstall. 

Suppose your database crashes The restore sequence would be First restore Today's full backup with no recovery then 4 PM differential backup with no recovery and the all log backup with no recovery, just the last log backup with recovery to bring database online. If in above scenario you dont take diff backup then restore sequence would be Todays full backup and then all log backups taken after this full backup witn no recovery just the last backup with recovery to bring DB online. 

You are correct, I also believe that in most cases the should be set to true we should allow SQL Server to decide when to update stats and believe me it does good job. When this is set to true it make sure stats are updated about distribution of data in the field which would eventually help optimizer to prepare better plan. The important thing to note here is Auto update stats fire when 20% of data changes in table. So you should not feel that on a table with 100K rows if 10 rows are updated then status update will fire. A more deeper analysis is done by Paul Randal in the blog Understanding When Statistics Will Automatically Update. I have not seen any drawback if this option is set to true. Yes you can see some I/O activity when this option is set to true. Important conclusion which one can draw from the blog is 

If you see memory grants pending in buffer pool your server is facing SQL Server memory crunch and increasing memory would be a good idea. For memory grants please read this article: $URL$