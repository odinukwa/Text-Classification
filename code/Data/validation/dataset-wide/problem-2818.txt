Ftn. In Sartre's novel Reprieve, there are several outstanding pages towards the end of the book describing Mathieu's opening to himself that he is free and is an everlasting immortal moment, which is, like light skimming the beach, could never be buried by this sand and stones and is always to be an exile from them all. 

According to Sartre (see "The Transcendence of Ego") cartesian cogito ergo sum is a performative fallacy: who thinks this thought (active, pre-reflective consiousness, me) is not whom the thought is ascribed to (I, an inert reflective object). From this particular point of view, having "first-person perspective" designates not the consciousness but one of objects of the world, and it could be removed from the phrase as redundant. Then you are left with "I am a conscious being" which sounds similar to "cogito ergo sum". And that will suffice. When you are writing down that phrase you actually don't have the first person perspective because your consciousness is busy with writing a sentence. But you remain conscious (and free). You're keeping a dim purpose what you are writing for, and your movements and auxiliary thoughts serve that goal. Consciousness understands itself and what it is doing, even though there isn't any Ego currenlty here or Ego is seen in the past as if remembered other man. Because positional consciousness of an object or one's activity is at the same time non-positional consciousness of the self. That primordial self-consciousness (which is difficult to catch) - without being an object to oneself - is a primary evidence and needs not be proven, especially by subconscious "spooks" or neurons as "spooks". 

First of all, you interchangeably use expressions "consciousness" and "artificial intelligence" while it is an open question wheter the latter one must come with the first or not and how they are related (check out for example Searle's Chinese Room : $URL$ Secondly, let me use another anology to the computer science: you can write some code in an advanced programming language, handling difficult computations, database queries, sending graphical output and reading input of all sorts etc... But any software you write is actually compiled to some binary code: a bunch od 0's and 1's with nothing else, which then are represented in hardware as different levels of voltage (I'm simplyfing it a bit). The way one can look at it: as a binary mess or as a great piece of code is just the case of perspective. We can tell that the computer is brilliant, because it can do such things, or we can tell that computer is just doing what a simple voltage detector had told it. However, we usually do not make assumptions such as that a computer has a soul or awareness (or do we? again - Searle's Chinese Room). It is just a machine that works well. What's more - if it didn't work well, we would have invented something better. The point you make is that the whole is greater than just the sum of parts - but it can also be applied to the computers, as an analogy to the hardware. The magical mist fades, because we know that the computer is nothing else, but the wires, transistors and some other stuff. Certainly no-one puts a soul inside a computer! Surely, we can tell that a computer is greater (understood as more valuable) than just the parts we used to build it. That is true, because there has been some research and effort put in sticking those parts together and making it work. We can underestimate the pieces and praise the whole. But in a society, especially since the World War II, I would say that such a statement is "highly deprecated". Impersonating a society is highly essentialistic, can lead to devaluating a person in general in comparison to a society, and therefore brings a threat of totalitarianism. The procedure of destroying the myths of the past and replacing them with new myths covered in a quasi-scientific vocabulary is widely discussed by the Frankfurt School, especially by T.W. Adorno and Max Horkheimer in their Dialectics of Enlightment. To sum up, a society is indeed a great and valuable human invention and surely deserves the highest appreciation - but never at a cost of underestimating the individual. 

It is at best "perceptual consensus". Subjective reality is just a subset of objective reality, distorted and limited by a finite and usually flawed perception. Therefore, it doesn't exist outside a particular mind, making it entirely imaginary. And you cannot call something imaginary a "reality" because the two are complete opposites. 

Hopes, deceptions and illusions exist in the mind, they are imaginary, and as such have no effect on reality. They do however condition the population to be harnessed into physically altering reality, usually in the completely opposite direction. The subjective is subjective, and adding more subjective to it doesn't change that, there is no tipping point in which it magically turns into objective. It is not something that can be enforced or agreed upon, it is absolute and universal and is not subject to the beliefs or delusions of its creations. We can alter it physically to a degree, but the way we see it or are being made to see it in no way affects what it is on its own. You can raise Johnny in isolation and teach him that EM radiation with wavelength of 700 nm corresponds to the color green rather than red, and one day Johnny might park "his green" car next to Larry's red car, and all of their coworkers might agree that Johnny's car is red too, but for everyone both cars will have the exact same color, reality won't bend and snap to produce a tangible difference, because the color itself, as a product of objective reality is absolute, regardless of what words different people might use for color, light, frequency, wavelength, units or numbers of measurement. If you put 100 people on an island, and poke the eyes of 99 of them, the world won't turn dark for the one that retains his sight just because the majority has lost it. 99.99% of the people cannot last 5 minutes under water, and perceive and agree on that fact, but that doesn't define a reality in which no one can. I can go on and on, the point is there is plenty of obvious observation to answer your questions, but still: 

In Critique of Dialectical Reason, vol. 1, Sartre wrote (translation from the Russian translation into English is mine) 

A good question. The defining property of free will, i.e. freedom in choices, is the "feeling" or consciousness of the own authorship of the choice being made. Even if/when I'm forced to comply I cannot escape the glimpse, at the moment, that nothing actually is selecting the way for me, except that "foolish" me (who is, actually, nobody/anonymous this minute) who agrees with the offered. Thus, every human choice is a free choice, and responsibility is implied automatically. That would be Sartre's account. Moreover, even when I stumble and fall down, I immediately and promptly react and interpret the occasion (e.g. by the fear to break an arm and not be able to paint anymore through that). By application of a free interpretation/meaning I emerge responsible for my incident though I didn't choose to stumble. I selected to be thus fallen, instead. Freedom is spontaneous choice (of meaning and action) accompanied by the awareness of own authorship / resposibility. Or to put it differently, the apprehension that there is nobody/nothing specific is to blame except myself. The key point here, @HWalters, is that ownership belongs to (is felt by) faceless me, the Nothing, and not to I, the Something. If nothing is the decision maker, the decision is, logically, not motivated by any preexisting structure, i.e. it is not determined (subjectively) by any cause. It is motivated by no cause, a pure gap in causational chain. (It is but later that we can attribute a cause to our decision, to "excuse" it.) Thus, freedom is human (i.e. conscious) randomness (conditioned upon given circumstances). Within the scope of possible alternatives the circumstance can support, it is non-determined. A man is an entity whereby randomness becomes own while determinism gets broken. Whenever consciousness comes to play, whenever something comes to being through consciousness, determinism is canceled. Even conceived/planned decision without instant action is not your "will", it is spontaneous. It was spontaneous when it first occured to you, and it is again spontaneous when you start to act according to it: you re-invent it one more time, using your knowledge of the plan as the circumstance, and act. So, free will is freedom. Specifically "will" is a redundant word, and people do not do things by "will" as some tension or effort. It is when I reflect back on the thing done or on the difficulty to overcome to do it, I might see that "labour" of my Ego involved and start to think of (free) "will" as my instrument. "Will" is always an artefact of reflection ("I decided and did it myself", "I must decide to do it"). No "will" is actually used in real, here-and-now decision makings. Also to say: curiously, non-human (out there, universal) randomness is close, in eyes of a human, to determinism (or doom) and not to indeterminism. Both random and predetermined appear to us as contingency which is a characteristic of facticity to where we are "abandoned". Marooned for making free choices in the flight from facticity towards meaningful world. 

In objective reality we are all sharing the same physical world, thus our actions affect other real beings, laying some grounds for morality, at the very least the "do onto others" and its invert. But if reality is subjective, and purely a construct of a central to it consciousness with no common objective physical manifestation, that appears to render morality obsolete. For example, in the context of murder: 

The beliefs of the majority at best define the status quo. And the status quo is not guaranteed to be neither right nor reality defining. 

Unexpectedly (upsetting somebody, perhaps), this answer will be completely away of the domain of modal logic or statement analytics; rather, it will be existential. The thing is that in the OP question itself there was no requirement that an answer should come from some specific branch of philosophy. Sartre claimed that possibility (or opportunity) of an entity or event is always my (rain's probability today is my possibility to get wet or not) and that it is what initially introduces time in my reality here: possibility temporalizes me. The time I'm speaking is primeval or pre-reflective time and should not be confused with psychological or "physical" time which appears as a special time-object later, upon reflection having come into play. Whereas psychological time is graded into chunks (into before a landmark and after a landmark where a landmark is some thing or event, obstacle or facilitator coming from world) primeval time is smooth. It connects me with the future (which is my state alongside with the possibility realized) without any kind of glue such as promise/guarantee or resources/facilitators - as if the possibility has already come true. Simultaneously, it perfectly detaches me from the above possibility realization without any chronology/postponing or shortages/obstacles. I'm thus fully connected and fully separated at once from the aim which is my possibility, in pre-reflective mode. This seeming contradiction is at the core of human conscious nature because it is based on pure negation. A citation from another local answer: 

Objective reality exist independently of the perception of discrete sentient beings. It doesn't rely on or mandate the existence of such beings whatsoever. 

No, it isn't. Whether it is smart people agreeing on empirical evidence to forge intelligent theories, or it is idiots collectively fooling themselves with something convenient and reassuring, it doesn't affect objective reality whatsoever. It only affects our perception of it. And what we perceive is defined by objective reality, refracted through our subjective and imperfect minds. Not the other way around - that would mean we are necessary as precursors to reality, and there is ample evidence reality existed long before us and was the framework of what eventually resulted in our existence. 

I, contrary, think that you are actually missing something, and so are the other answers except CCarter's answer. (That's a pity one needs over 50 reputation to comment, as it would be more suitable I guess....) What I consider a flaw of any discussion like Eythyphro's dilemma, is the constant change of problem level we are talking on and the lack of linguistic reflection. Let me explain: as we discuss what is good, we are on the field of ethics. Then, you are asking a meta-ethical question (about the source of good in general, its possible creation by God and the implies of that hypothesis), but use arguments and examples back from the lower level: the permission to murder anyone without any reason is certainly something that's not "good" in any ethical system and our ethical intuition treats that as a valid argument against the possibility of creation of ethical system treating it the other way. Apparently, this can not be considered as an argument: the field of meta-ethics doesn't know anything about why couldn't we name murder good, because that's not in the field of meta-ethical problems. Also, we need to checkout why the example of murder bothers us so much: the cultural and linguistic connotations about that case make an unjustified murder something bad almost by definition. When we enter the field of meta-ethical discussion, we should double-check every definition we use and whether we are even allowed to talk about something. The concept of bad God is indeed, as CCarter points out in the last paragraph, self-contradictory. I even consider a mistake making an assumption that "there is no God, but objective values do exist", and so I agree with the statement that "without God, everything is permittable". Naming anything objective and absolute without concept of God is, in my opinion, in fact just creating a new God (what's more: if we associate it with a person who knows everything, we are happy to meet a new totalitarian dictator). The fact, that God is often impersonated while values and other abstractions are not is just another example of our cultural background. Therefore, see that using this statement as an argument of God existence is absolutely wrong, as it just tries to prove that there is something that's absolute and objective, often just by our intuition that murder being bad is so obvious it should be objective. Now, to answer your question is theist's explanation of what's good just unsatisfactory as the atheist's approach. In my opinion, it is not. If we believe that something is objective and not just name it by convention, we have a strong basis and need not to develop further. We are sure. It is just like Newton's laws: as long as you stick to them, you are absolutely sure what should happen, what is possible ("good") and what is not ("bad"). But, at some point someone came up with an idea, that Newton's laws are insufficient and developed something new. The knowledge we have is never 100% right, because we can never be 100% sure: we are just working with a model of reality, not with reality itself. When we change model, we can achieve something new (like quantum physics etc), but if we change the model of ethics at will, in most cases something really bad can happen, and even if not (we should, again, not use just our intuition) we can not predict what exactly would happen, and ethics is too serious business to play with it. That's why being assured that what is good is good in reality and not just by convention is more satisfactory: it just makes everything simplier. It doesn't help in explaining goodness, because note that goodness, whether God created it or just showed it to people, is, in this approach, an axiom, just like God. I'm surprised no one has mentioned Immanuel Kant yet! Because he surely understood the theoretical problem I described above: note that Kant postulated the existence of God exactly that way and that is (in my opinion) brilliant: he did not try to prove his existence, but postulated it: he assumed we need objective values, and therefore just showed that we need God to have them. (If I simplify too much or maybe do not understand something, feel free to correct me.) Kant also proved that there can not be any proof or disproof of God's existence. I hope one day, people from both sides of the debate, theists and atheists, would finally understand that. Or just rename their discussions to something like "Do we need God?", which may be a little less pointless. 

Those things just allow us to communicate. It just gives common things common identifiers so different people can understand what they are saying. We don't invent laws of physics, we only come up with approximating descriptions of what we discover is already at play. Those things do not determine objective reality, they merely attempt to approximately describe and quantify it. It doesn't have to be democratic either, the way it usually works is a single individual discovers it and gets to name it, then tells it to someone else referring to it the way he conceived. It sometimes happens that things are discovered independently by different groups of people, leading to different names for different groups, when those groups interact sometimes both terms apply, but usually the more popular eventually prevails. But that's just how the brain works, if 1 is refereed to as A more often than B, then over time that's what grows on you. It has nothing to do with objective reality, it is still entirely in the mind. I for one don't recall anyone calling general elections to define what words we use to describe certain aspects of reality. In fact it is usually a minority that defines those things for us without asking us at all, at least when it comes to official nomenclature. Slang can be different, it is sometimes spread through popular usage, although quite often a few popular or influential public figures play a key role in spreading it. We are after all just primates. Monkey see monkey do. Last but not least, democracy in translation means "people's rule". What a minority fooled the majority into believing to be democracy is actually a blatant mockery of both true democracy and our very right to chose. Obviously, even if most people believe it to be democracy, that didn't result in a reality where it really is democracy. It is still a bunch of mindless cattle getting to chose its butcher every few years, the option to not have a butcher or not be cattle is not even on the table. I mean the moment they say that the vote of a firefighter or a professor has the same weight as that of a drunken hick living on welfare it should rise a big tall red flag. And of course, we never got to democratically vote what democracy is either, but the words are still translatable without any room for ambiguity as of what they mean, for those who are interest in objective reality rather than subjective illusions ;) Wouldn't it be swell tho, if we could all just come together and solve all our problems simply by agreeing we don't have them.