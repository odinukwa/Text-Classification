So from this we get that most likely if we've heard ɪŋ, we're about to hear a word boundary, which makes sense because -ɪŋ is a suffix but also because /ŋ/ is generally only allowed in codas (ends of syllables) in English. The next most likely segment is /k/, which is to be expected given place of articulation assimilation for nasals in English (e.g., /n/ before /k/ becomes /ŋ/). -z and -ɚ are suffixes in English, so we'd expect them where word boundaries are likely. So you see that this simple model has captured a bit of phonotactics, phonology, and morphology. This is why it's handy in speech recognition. So you can see that whenever the memory required to make a good prediction is short, such models work well. Basically, performance in a language is affected by the fit between the phonotactics of the language and Markov assumptions; the more a language defines short, linear relationships between phonemes, the better this model will perform. Also, it often depends on how often such relationships help resolve possible ambiguities or "mis-hearings." Now, as to how they use sentence information, this is getting into what's called language modeling in speech recognition. What that particular technique is doing is taking some guesses at the words, assigning part of speech tags to those guesses, and then using a grammar to validate the recognition. For example, consider these two possible recognitions (with likely part of speech tag assignments in slashes): 

In American dialects, the sound in 'hat' is generally /æ/, not /a/. The use of /i/ (meaning /ai/) in the 'fight' vowel would be too long/close; you probably do not pronounce 'might' exactly the same as the phrase 'ma eat'. The vowel in 'might' is shorter/opener, and so /ɪ/ (meaning /aɪ/) is appropriate. (See $URL$ 

TextGrids are still just text files. The simplest thing for a non-scripter is probably to open it in your favorite editor and find-replace them there. Try TextWrangler or SublimeText 3 if you don’t have a good one you like.(I’m not sure that this is even the correct StackExchange for this, or if the tags ‘phonetics’ and ‘computational-linguistics' are relevant.) So, the tiers probably contain intervals like: 

In fact, it seems to be the case that you do get /r/ in those contexts and specific words (also, 'squash', etc.). It may be difficult to find them in text (e.g., for 'posh': 'porsh' and 'parsh' are both problematic for the speller), but they likely exist. I was able to find Google hits for 'jarsh'/'jarshing'/'jorshing', for example. It is also possible that this kind of regularization rule is applied more for high-frequency words than for low-frequency. If 'wash', 'water' are much higher frequency than 'posh', 'josh', then it's reasonable to see fewer instances of 'jarsh'. 

The origin of iáomai is unknown, but it seems as though the first two syllables could result from *iCa-, where C = a consonant that was lost in all forms of Greek before dialectal breakup, but after the older *j- glide disappeared or transformed. Intervocalic -s- is one candidate for such a consonant: compare Greek heu- (heuō, heuein, etc.) "to singe" < *eus- (> Latin ūr- "to burn"). If so, there might be a connection with e.g. Welsh iach, Breton yac'h "healthy", since intervocalic -s- has disappeared in British Celtic as well. But this isn't an especially good match (for example, there is nothing in the Greek word that obviously corresponds to British Celtic -ch), so this is just a speculative connection. 

Some palatalized consonants seem to have a greater tendency to "absorb" their palatalization (in various ways) than others. For example, in standard Japanese, the former palatalized alveolars tj, dj, zj and sj have become [tʃ], [dʒ] and [ʃ]: ("above", etc.) < *zyau ja ("so", "then", etc.) < *dya < dewa chō ("morning", etc.) < *ty(e)u < teu 

Why wouldn't adjectival phrases like English born again (in the religious sense) count as examples of a right-branching nominal modifier? 

The spelling of Slovene is less phonetic (at least in some respects) than that of some other Slavic languages, most notably that of its neighbor Croatian. For example, syllable-final l in Slovene is often pronounced like [w], as in napisal "(he) wrote". In Croatian, the corresponding word is napisao, where the spelling has been updated to reflect the sound change. Similarly, word-final or preconsonantal v in Slovene is generally pronounced as [w], as in zdrav "healthy", postavljati "to place", etc. By contrast, the v in Croatian zdrav, postavljati etc. is closer to a true [v] (though not quite the same). EDIT: I just realized that maybe your question was only about Slavic languages that use Cyrillic alphabets (which doesn't include Slovene or Croatian), but I will leave this answer here in case it's useful. 

As described (empirical similarity from many listeners), I think you want confusability, e.g. a ‘confusion matrix'. Here’s an example, from Munson et al. (2002) JASA: tc.umn.edu/~munso005/ConfusionMatrices.pdf 

It is often helpful to avoid voiceless sounds as much as possible in prosody stimuli, to get a clearer pitch track. For example, "Mary will win" has no voiceless sounds (from: Pierrehumbert, J. (2000). Tonal elements and their alignment. In Prosody: Theory and experiment (pp. 11-36). Springer Netherlands.). For demonstration purposes, you may also want to use a longer sentence. 

It depends on the scale you're referring to. There is certainly active research in artificial language learning, agent-based and social network modeling of language evolution, novel language development by robot agents, social effects (micro and macro) on linguistic change, word formation constraints/factors, 'telephone game' experiments on language convergence, and so on. 

There are many of these in English, in both directions: adder, apron were once 'nadder' and 'napron'. More examples and explanation on this Wikipedia page on 'Rebracketing', under 'Examples of false splitting' ('In English'). 

For many languages, SUBTLEX is considered a good source for realistic frequencies. There is an Italian version, SUBTLEX-IT, available at $URL$ (Crepaldi, Keuleers, Mandera, & Brysbaert, 2013). 

This is a high Google result that I’ve come across before, though I have not used it myself. It seems to satisfy your requirements: Lexique, based on FRANTEXT. 

I once heard that some verbs typically called "aorist presents" in Germanic are actually the result of e > u vowel coloring by surrounding velar and labial consonants. For example, the Germanic "come"-word (Eng. come, Icelandic koma, etc.) is sometimes termed an aorist-present verb because it has apparent zero-grade in the present tense in Old English (ic cume "I come") and elsewhere. However, the root originally contained a labiovelar onset and a labial coda (*gWem-). In some Germanic branches, such as Gothic, the e-grade remains as such (Gothic qiman "to come" < *kWem-). This explanation doesn't seem to work for Norse/Icelandic vega, though, because the -g- coda implies a different stress pattern (the one typically associated with zero-grade). Thus, this cannot simply be a question of the effect of the surrounding consonants on the vowel. With OE būgan, there is a labial onset and a velar coda, so this might be a case of vowel coloring rather than actual zero-grade, but maybe the length of the ū vowel conflicts with this explanation. 

Slovene shows sporadic examples of features that are typically considered "West Slavic", and are therefore closer to Slovak than to standard Croatian/Serbian/etc. For example, 

In colloquial Spanish, there are examples of reduplication such as El bebé estaba llori-llori (= "El bebé estaba llorando") that seem to involve non-finite forms of the verb, or deverbal derivations. Depending on how one analyzes such examples, reduplication may be part of the process whereby a verb is converted into a non-verb. I don't know of any cases in Spanish where a conjugated form of the verb is reduplicated. 

First, the definition: a triphone is a sequence of three phonemes. This is equivalent to saying it is a 3rd order Markov chain or a trigram over phonemes. The application of such a model is usually to ask "what is the probability of the third given the first two?" in a chain as long as the utterance. Such a model assumes that all previous phonemes other than the preceding two are irrelevant. This is called a Markov assumption. So, here's an example in English. I wrote a quick script to calculate triphones from CMUDict data. Let's look at the highest probability segments that follow ɪŋ. I'll use # for a word boundary: 

There is some grammar that tells us that an adjective complement for a verb is not going to work, and that only #1 is going to make sense as a sentence. It should be noted that no modern speech dictation system operates with hand-coded grammars like this. The complexity of maintaining such grammars is too high, and much of the performance comes from lexical information (what kind of objects does "like" prefer?) that is lost if you change all words into part of speech tags as the link suggests. So word-based n-gram models are usually used, with little concern for parts of speech or grammaticality. For further reading, see Spoken Language Processing: A Guide to Theory, Algorithm and System Development or Speech and Language Processing. Edit: Added some real triphone data instead of the hypothetical example. 

While a dedicated script (sed, awk, Python, …) would be better if this is a common operation, you can indeed do this in e.g., SublimeText. One method is to first select all the ‘text' lines (e.g., Find All with '.+text.+' is fine); then, Replace All '\\n' with ‘ ‘. This method assumes that you can't simply do a global replace of '\\n' with ' '. I think this latter, simpler option should be tried first, though. 

It’s true that there is a learning curve, but I’m not sure what the alternative is. What does ‘f0 analysis’ mean here? How can you interpret changes in pitch, intensity, duration without an interpretive framework?The point of ToBI is to assign meaning to the physical observations (e.g., pitch, duration), which otherwise do not have meaning. To do this, pitch contours are interpreted within an abstract framework that says ‘this pattern means this’. I say that it is an abstract framework because it reduces the complexity of the signal by removing variability that is not meaningful in order to more clearly focus on the variability that is meaningful and contrastive for the language variety in question.For example, we notice the final fall/final rise pitch distinction between statements and questions in English. For this contrast, (and speaking a little simplistically) it doesn’t really matter what the absolute f0 is, how loud the sound is, or what’s the duration of the contour; the relative change leads us to categorize the boundary as Low or High for this contrast. In order to have any confidence in our conclusions, we have to have a broad understanding of what prosodic patterns are typical and possible for the specific language variety, how actual speakers interpret those patterns, and a model that takes into account the interactions of all these patterns. This is the same as e.g. segmental phonology (‘why have phonemes when I have a spectrogram?’) or syntax (‘why have a grammar when I have words?’). 

However, Japanese has retained most other palatalized sounds as such: velar stops (Kyōtō, Gyōza), back-fricatives (hyaku "100"), labial stops (roppyaku "600") and resonants (ryōri "cooking", gyunyu "milk", etc.). In Norwegian and Swedish, both the palatalized alveolars and velars have changed into something else: "sea") > [ʃ] / [ɧ] tj (as in tjære / tjära "tar") > [ç] / [ɕ] dj (as in Sw. djur "animal") > [j] kj (kjøtt / kött "flesh") > [ç] / [ɕ] skj (skjorte / skjorta "shirt") > [ʃ] / [ɧ] gj (as in gjorde "did") > [j] 

Other palatalized sounds in these languages seem mostly to remain as such: björn / bjørn "bear", fjäll / fjell "mountain", mjöl / mjøl "flour", Swedish njure "kidney", etc. Are cases like these the norm, cross-linguistically speaking? I.e., are palatalized alveolar obstruents ([tj]/[dj]/[sj]/etc.) generally more diachronically "unstable" than palatalized velar stops, which in turn are less stable than most/all other palatalized consonants? Thanks for any info 

These kinds of features are more common in dialectal Slovene than in the standard language, but I don't know how the particular dialects compare with each other in this respect. 

In some Iranian languages (maybe Indic as well), the past tense construction involves a verb that is etymologically a passive participle, agreeing in gender with the verb's object rather than its subject: e.g., Pashto mā xǝza lidǝla "I saw a/the woman" The literal translation of this sentence would be something like "the woman was-seen by-me", since the pronoun mā is an oblique 1sg. pronoun, the nominative form being zǝ. 

Moved from comments. You might consider generating such lists yourself. I'm not saying asking if that work has already been done is wrong. :) Computationally, it's not a trivial job. This link (billposer.org/Software/minpair.html) purports to provide an open source (GNU license) program to find minimal pairs in a provided dictionary. You'd want to provide a phonemic dictionary, of course, for phonological minimal pairs. If it's not adequate, you could also examine the algorithm and modify it to suit your needs. You might also be interested in generating your own words (according to some pattern of phonemes, presumably), and then simply checking if they are in the dictionary. Or, and this is based on your original question, you can test people on minimal pairs of nonwords (in something like an AXB task, I guess); for the purpose of assessing their phoneme perception, this option is the typical choice. It helps to minimize the influences of semantics, frequency, lexical neighborhoods, etc. 

Neologism. Speaking for myself, though, ‘nonce’ is fine for things that are not strictly ‘one-off’. Everything starts with a first time, and 'accepted productive rules' are often precisely how a nonce word would arise, right? To the point that it seems odd to have a specific term for it.