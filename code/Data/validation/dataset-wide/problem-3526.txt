As you can see, the amount of money changes as a result of some transactions. However, there are some accounting laws that do limit the relationships between monetary values in economies: accounting identities. These relationships are weaker than energy conservation laws, but they do limit what outcomes are possible. For example, if we use simplified national accounting, the government deficit during a year will equal the increase in the amount of government-issued money and debt (bonds and bills). This has to hold, as otherwise someone has made an accounting error. (Once again, there are complicating factors in the real world, which will add more terms to the accounting identity.) An interesting accounting identity is the relationship S=I (national savings = national investment); link to a question related to that identity. There is a non-mainstream school of thought within economics that uses what are called "stock-flow consistent models" (or SFC models), that underline the importance of accounting identities in framing models. I was trained as an applied mathematician, and I found the book "Monetary Economics: An Integrated Approach to Credit, Money, Income, Production, and Wealth" to be the introduction that best suited my background. There is an excellent treatment of all of these accounting issues. 

In general, if $c_1$ is endogenous, you need instruments for $c_1$ as well. Example: Even when $x_1$ is exogenous, if $c_1$ is endogenous and $x_1$ and $c_1$ are correlated, then OLS (which is the IV estimator using $x_1$ as IV for $x_1$) is generally inconsistent. Exceptions exist. For example, when exactly identified, if $(Ezc') (Ecc')^{-1} (Ecu)=0$, then your IV estimator is consistent, I guess. (Can derive this using standard technique involving the law of large numbers.) 

Now, if $E_t(x_{t+j})$ converges as $j\to\infty$, then $\lim_{j\to\infty} E_t(x_{t+j}) = \lim_{j\to\infty} E_t (x_{t+j-1})$, and thus $$ \lim_{j\to\infty} E_t(x_{t+j} - \rho x_{t+j-1}) = (1-\rho) \lim_{j\to\infty} E_t(x_{t+j}) = x_t - \rho x_{t-1}. $$ As a result, $\lim_{j\to\infty} E_t(a_{t+j}) = \lim E_t(x_{t+j}) + \lim E_t(z_{t+j}) = \frac{1}{1-\rho} (x_t-\rho x_{t-1}) + 0$. 

That is natural and you are not missing anything. Let $y=\alpha + \beta z + u$. Your prediction of $y$ given $z=1$ is $\hat{y} = a + b$, where $a$ and $b$ are the OLS estimates. The prediction error ($y - \hat{y}$) for $z=1$ is, thus, $(\alpha - a) + (\beta-b) + u$, which is involved with $b$. On the other hand, for $z=0$, the prediction of $y$ is $a$ and the prediction error is $(\alpha-a) + u$, which has nothing to do with $b$. As you said, the former depends on $b$, while the latter does not. (Perhaps it would help to remember that the value of $z$ is given for the prediction, and thus the prediction intervals depend on the value of $z$.) (Calculation of the prediction intervals:) You can calculate the prediction intervals using the formula $(a+b) \pm se((\alpha-a)+(\beta-b)+u) \cdot \textit{critical value}$ for $z=1$, and the formula $a \pm se((\alpha-a)+u) \cdot \textit{critical value}$ for $z=0$. which is fine. Note that these two prediction intervals can also be written as $(a+b) \pm se(a+b+u) \cdot cv$ and $a\pm se(a+u) \cdot cv$, respectively, because $\alpha$ and $\beta$ are constant (nonrandom). How to estimate the standard errors can be found in Wooldridge's textbook (the "Prediction and Residual Analysis" section; 6.4 in 5ed). 

We need to distinguish between private corporations, and public. Private corporations are largely free in what they do, public corporations listed on stock exchanges have to follow various procedures. For any corporation, there’s no guarantee that you make a profit, so what price you bought it at makes no difference. If a company wound up its affairs, it would make a final cash distribution to shareholders, and that would be it. The cash would either be a dividend or a return of capital, depending upon the equity levels of the company. The problem is that do this, the company would have to sell all assets simultaneously. That is normally difficult to do, and so it is much more usual for the entire firm to bought out (after selling some assets). Furthermore, having a listing on the stock exchange is itself valuable. It would be far more common for a company to pursue a “reverse takeover”: the listed company appears to take over a private firm, but the private firm actually buys out the existing shareholders. This way, the private firm saves the expenses of seeking a listing on the stock exchange. Just shutting the firm down would probably be viewed as a suboptimal course of action, and might expose the directors to lawsuits. What is more likely is that companies largely fail, and end up having to shut down most of their operations. After awhile, they will not meet listing requirements to remain on the stock exchange, and the shares are delisted. The shares still exist, but it is very hard to find buyers. If you want to find examples, you could just do a web search looking for the terms “delisted companies” and the name of a stock exchange that you are interested in. The following link is an example: TSX Venture Exchange listing changes 

It is conventional to report both the total number of observations (1000 in your case) and the number of groups (500 in your case). If you want or are allowed to report only one number as the number of observations, it is suitable to report 1000 for POLS because POLS doesn't mind the fact that it is a panel data set. For FD, it is slightly confusing to me to say that the number of observations is 500. I would probably ask again of the writer what 500 means. 

You are saying: $y$ is regressed on $x_1$ and $x_2$, say, and you think it would be better to use cluster errors since you expect $y$ is correlated with habilities (abilities?) for each person. First, $y$ is naturally correlated with the error term $u$ (if you mean the error term by "habilities") because $u$ is a part of $y$. You have no problems with that; $y$ is always strongly correlated with $u$. It also has nothing to do with cluster errors. If you happen to mean cluster standard errors (SE) by "cluster errors", and if you are saying that you would use cluster SE because $x_1$ is correlated with $u$, I would say that the regressor-error correlation (endogeneity) has nothing to do with cluster SE, what so ever. Cluster SE provides valid inference when the the error term show within-cluster (auto)correlation. Please enlighten me if I misinterpreted your question. 

Money does not just appear; it is a government liability, and the growth of government liabilities implies fiscal deficits. That is, loose fiscal policy raises demand, and causes inflation. Expectations. If people are convinced that money supply growth causes inflation, they will watch the published money supply numbers. (In the United States, money numbers are published weekly, and were followed closely during the early 1980s. Currently, I doubt that 10% of bond traders under the age of 30 know what day of the week money numbers are published on.) They will raise prices based on what they think the money supply is doing now, and what they forecast it will be doing. I am not expert on Market Monetarism (the modern offshoot of "old Monetarism") but I believe that they emphasise this effect; although they might phrase things differently. 

The money supply expansion mechanics are interesting, but do not affect loan pricing. Let’s assume that the only assets a bank invests in are loans it extended, and required reserves at the central bank. Furthermore, assume that the rate of interest on required reserves is 0%. We will assume that bank equity is extremely small (close to the truth), and the bank only funds itself with deposits. For simplicity, assume that deposits are equal to 100% of the right hand side of the balance sheet. In order to break even, the total interest rate on assets has to equal the amount of interest paid on deposits. Denote: 

An LPM is a model in which the probability of the binary dependent variable having a particular value is linear in parameters. For example, if $y$ is a 0/1-valued variable and $P(y=1)=x\beta$, it (the equation for the probability) is called a linear probability model. Likewise, if $y \in \{ -1, 1 \}$ and $P(y=-1) = x\beta$, it’s a linear probability model. Or if $y$ is an apple/orange-valued variable and you believe that the probability of $y$ being an apple is $x\beta$, your belief is said to be a linear probability model. Zero/one mean nothing real here; they are only labels. You can label them as man/woman instead if you want. You lose no information by this relabeling as long as you remember the new labels. Also, as Papadopoulus said, it is a model, not an estimator, though we just understand an “LPM estimator” casually as an estimator of parameters in an LPM. 

I looked at “Monetary Economics: Theory and Policy” by Bennett T. McCallum. On page 95, he describes what he refers to as “the Classical Model”, where the money supply is set outside the model (exogenous). In the Classical Model, if the money supply is changed, nominal values in equilibrium change, but ratios like wage/prices and real values do not. (The implication being that nominal GDP changes proportionally to the money supply, but real GDP is fixed.) He describes this as money neutrality, and it matches the description in the question. I do not know how money neutrality can be defined for models where the money supply is determined within the model (endogenous). 

If you are looking for a job with a title "economist," I doubt that anyone would look at you if you do not have at least an undergraduate degree in economics (in North America, anyway). However, you could end up working for a firm that does economics, or financial research. I had a background in control systems engineering, with summer job experience as a programmer, and then ended up working as a quantitative analyst for fixed income. The programming experience ended up being probably the training that I ended up using the most. However, there is a lot of competition to get a job in finance. Outside of finance, I believe that there is a lot of work in "big data," which involves a lot of programming. You could end up working with economists, but you would probably not be considered an "economist." You ask: 

Let $y$ be the proportion in $[0,1]$ instead of the percentage. I think the issues here are possible nonlinearity and censoring. You can try including quadratic terms on the right hand side. At the same time, you can try the following models. I. Linear models: If no $y$ is exactly 0 or 1, linear models will be OK. The following four options come to my mind. 

The short answer is no, this is not so when the dependent variable is in log, unless you define the $\%\Delta$ as (100 times) log-difference. The reason is that $E(\log y) \ne \log E(y)$. We don't need a multiple regression model to show this so consider $\log(y)=\beta_0 + \beta_1 x + u$. It is the dependent variable in log that matters, so let us not take log to $x$ for notational brevity. It is true that $\%\Delta y \approx 100 \beta_1 \Delta x$ if $\Delta u = 0$ (ceteris paribus) when $\beta_1 \Delta x \approx 0$. But we cannot interpret it in terms of $E(y|x)$. The reason follows. As you can see, we have $y = \exp(\beta_0 + \beta_1 x + u)$. When $x$ increases from $x_1$ to $x_1+\Delta x$, $y$ increases from $y_1 = \exp(\beta_0 + \beta_1 x_1 + u_1)$ to $y_2 = \exp \{ \beta_0 + \beta_1 (x_1+\Delta x)+u_2 \}$. Importantly, $u$ can also change because $u$ is not held fixed, which is why I wrote $u_1$ and $u_2$. Now, what is the change rate of $y$? We have $$ \frac{y_2}{y_1} -1 = \exp(\beta \Delta x) \exp(\Delta u) -1 \approx (1+\beta \Delta x) \cdot \exp(\Delta u) -1, $$ where $\Delta u = u_2-u_1$. Here, the left-hand side is the change rate of $y$. You can verify that the change rate of $y$ is approximately $\beta \Delta x$ if $u_1 = u_2$, i.e., if $u$ is held fixed so that $\exp(\Delta u) = 1$. But the problem is that $u$ is not held fixed so $\exp(\Delta u)$ can be very different from 1. When $x$ and $u$ are independent, the average growth rate is $\exp(\beta \Delta x) \cdot E(e^{\Delta u}) -1$. How different would this be from $\beta \Delta x$? If $u\sim N(0,\sigma^2)$ and $u_1$ and $u_2$ are mutually independent, then $u_2-u_1 \sim N(0,2 \sigma^2)$, and thus $E(e^{u_2-u_1}) = \exp(\sigma^2)$. For example, if $\beta_1 \Delta x_1 = 0.01$ and $u\sim N(0,1)$, then the average growth rate of $y$ is $\exp(0.01) \times e - 1 \approx 1.75$, that is, approximately 175%, which is hugely different from 1%. Interesting, isn't it? Simulations: Use R to do the following simulation. 

We need to distinguish between theory and the real world. In theory, anything can happen. You would need a model that incorporates something that resembles an oil price (many standard models just have a single aggregated good). What then happens depends upon what other assumptions you stick into the model. Whether any of these models have any relationship to the real world is unclear. In any event, you would need to fix a particular model, and as a new question about its behaviour. (E.g., “What is the effect of an oil price rise in {Model X}?”) In the real world, we need to distinguish the measured unemployment rate, and the “natural rate.” There are a number of ways of estimating the natural rate of unemployment, and what happens to them depends on all of the inputs into the estimation procedure. What really matters in the real world is what happens to the measured unemployment rate (which may flow into the estimates of the “natural unemployment rate”, as many of the estimation procedures resemble low pass filters). I think the answer is a definite “it depends.” Firstly, it depends on what country you are talking about. For an oil producer, falling oil prices can easily be a disaster for employment. Many oil producing countries have government revenues that are highly sensitive to oil prices, and so they may be forced to cut back spending. However, they generally have financial reserves, and so can handle limited price falls with limited disruption. Therefore, you need to look at each involved country separately. For an oil consumer, a rapid rise in oil prices is disruptive; the 1970s experience could be used as an example. Consumers have to spend more on energy, making some products and services unviable. Meanwhile, the increased income is flowing to overseas producers. There is a “terms of trade” shock. The economy will eventually adjust to the new pattern of activity that incorporates higher oil prices, but in practice, the adjustment implies higher unemployment in the near run. If the fall in oil prices is reversing a previous spike, this will presumably undo the previous disruption. However, if prices were previously stable, effects may be less dramatic. The effects on the economy will depend on how consumers react to the fall in energy prices. If they use the energy savings to buy domestically-produced goods and services, the total volume of domestic production rises, and so firms would presumably need to hire extra workers to produce the greater volume of output. However, the energy savings could just lead to higher savings, leading to no change in domestic production, and presumably employment (in the near run, at least). The key is that falling input prices are generally not enough to cause increased production by itself; the firm needs to sell a greater quantity of goods, or else the extra production would just represent an undesired increase in inventories. (Since firms have to project demand, they could ramp up production (and hence, employment) to meet projected demand, but if that demand is not realised, they would have to reverse that decision. For a country that is both a consumer and importer of oil (such as the United States) there is a trade-off between the benefit to consumers versus the potential loss of fixed investment in energy production. Therefore, it could go either way; you would need to do a fairly detailed model to get an estimate. 

The biggest difference between (1) and (3) is that (3) has incidental linear trends while (1) has common time effects. Differencing (1) gives (1a) $\Delta \log(uclms_{it}) = (\theta_t - \theta_{t-1}) + \beta_1 \Delta ez_{it} + \Delta \epsilon_{it}$. Comparing this to (3) you see that (3) has $\gamma_i$ in place of $\theta_t - \theta_{t-1}$ in (1a). So three things are different: (A) As you said, it is FE vs FD; (B) the trend is free (using dummies) in (1), while linear in (3); (C) the trend is common in (1) but is individual-specific in (3). It would be interesting how much difference is introduced by each of these three points. About why FD is useful, well it is simple and informs us about functional relationship among variables in differences. So I'd say why not. 

II. Tobit models: If some $y$ are exactly 0 or 1, you can try Tobit models ( in Stata). Remember that normality is assumed for the error term before censoring. Also, using Tobit models means that "I think $y$ could be bigger than 1 (smaller than 0) if not censored." 

The thing to keep in mind is that these diagrams refer to simple teaching economic models. These models do not necessarily reflect the complexity of the real world. 

A small addendum. Lars Peter Hansen and Thomas J. Sargent wrote the book "Robustness", which is an attempt to apply robust control to economics. They treated robust control from a game theoretical perspective. In general, ecomics uses optimal control theory, which was the state of the art in the 1960s. The state of the art in the 1990s control theory (when I last worked in the field) was robust control, which fairly explicitly rejected the optimal control methodology. Robust control introduced the notion of model error. (The game theoretic approach used by Hansen and Sargent attempts to emulate uncertainty via game theory, but the equivalence between model uncertainty and the game theoretic approach breaks down for nonlinear systems.) Without taking into account the reality that our models are incorrect, optimal control strategies tended to fail. A test pilot was killed by a defective optimal controller, and certification boards barred such controllers from aircraft. This killed optimal control as a strategy for control design.