If you create a sequence that counts back from some really large number, you could assign an ever increasing priority. Then your prioritized dequeue would get messages in a LIFO order. 

The other option would be to use explicit scoping prefixes when referring to local variables (i.e. and ) rather than altering the names of your local variables. 

Maybe I'm missing something but why wouldn't you just use a lookup table with a foreign key relationship for this sort of thing potentially with some views to provide a layer of abstraction? That's a very common approach, it's supported by all databases, it's pretty easy to implement, and it solves other issues such as data consistency (i.e. what happens if you want to update an existing string without updating every row in the table that has that string). If you know the maximum number of entries in the lookup table, you can pick the data type that minimizes data volumes for the primary key. That's also much easier to handle in the future when someone decides that the number of distinct values needs to exceed whatever limit you specified initially-- you just need to increase the size of the column in both tables. 

As an example, if you want to report on the table and list the reports in alphabetical order, here is an example of the clause 

You can also specify the data types of the columns that will be in your table. Since there is no way to specify the average size, though, this tends not to be nearly as accurate as doing it yourself. In our case, though, it's pretty good because our column happened to be populated with strings that were, on average, half the maximum size of the column. In our case, though, it correctly estimated the allocated size of 72 MB with 67.94 MB used. 

This is not the sort of task that you would want to use triggers on tables for. If you really wanted to use a trigger, you could declare as a view, rather than as a table. You could then write an trigger on the view that translated statements on the view into statements against a new base table. Were it me, though, I'd let the statements go into a table which allows duplicates and then create a materialized view on top of that table that does the aggregation. You can have that materialized view refresh if you want to ensure that the data in the materialized view is always in sync with the data in the detail table. 

A savepoint is a point within the current transaction. A statement like , , or will issue an implicit commit before and after the statement runs that will end the current transaction. Once the current transaction ends, you can no longer rollback to a savepoint defined within that transaction. Potentially, you want to look at using a restore point instead. A restore point is an attribute of a database, not a transaction, so you can generate many transactions and still restore the entire database back to the restore point. Restoring to a restore point, however, will affect every change that had been made to the database between the time that you defined the restore point and the time that you do restore-- you would lose all changes made not just by your session or your user but all sessions and users in the database. 

You cannot have a clause on an index in Oracle. You can create a function-based index (I'm eliminating the third column from the index since we know that must be so it's not adding any selectivity to the index) 

Use a bind variable anywhere that you might actually change the value that is passed in. It probably doesn't make sense, for example, to use a bind variable in the clause in your third example because it seems extremely unlikely that anyone would ever want to pass in a different constant for the clause. In your second example, it probably does make sense to use a bind variable rather than a hard-coded status assuming that the application(s) are likely to want to run this query for different and values. On the other hand, if the code would only ever pass in a of 2 for this query, it probably doesn't make sense to use a bind variable. Assuming that is not evenly distributed, you'd be better off giving the optimizer the information that you're always going to use a of 2 rather than trying to rely on bind variable peeking to give the optimizer that information. If the value is never going to change-- i.e. if the in your second query will always be 2-- you can execute the query as many times as you'd like and it will only appear once in or . In my case, I'll execute the query 10 times 

Since you're using 11.2, it's a bit more verbose (though someone may be able to figure out a simpler version) but you can also use recursive common table expressions 

It's not obvious to me whether you modified the text of the error message to remove a directory from the error-- normally, the error will tell you a specific directory that you need to grant access to. If your intention is to allow the Java stored procedure to execute arbitrary shell scripts (this would be very dangerous-- the commands would run as the Oracle operating system user so they would have the ability to bypass any security measures in the database), you should be able to do something like 

Of course, that is not particularly useful. You can alias the columns you are selecting from to avoid name collisions. 

Can you define "corruption" and explain exactly what happened to the files? If the data is really valuable (in which case you'll hopefully be investing a lot more in your backup and recovery to make sure that you can't lose your database and all your backups simultaneously), there are tools that may be able to salvage some of your data. Oracle Consulting (which you'd access via support) has a data unloader that they'll charge a small fortune to use. There are a handful of other such tools-- DUDE being the most well-known. Jonah Harris has a listing of other data unloaders that I know much less about. Realistically, none of these are going to be cheap. And there is no guarantee about how much data will be recoverable depending on the nature of the corruption. 

It depends but given the data distribution, probably not. The performance benefit of using a single expression like in Oracle is that it becomes indexable with a function-based index. But since there are very few cases where the optimizer would use an index to return 99% of the data in the table, it seems unlikely that there would be benefits in this case. Potentially, a composite index that included and some other columns such that it would be sufficiently selective could be beneficial in some cases but it's relatively unlikely that adding an expression that only adds a tiny bit to the selectivity would be a net benefit. 

You'd need to do a 10053 trace to see exactly how Oracle is computing its estimated cardinalities. I would assume, though, that Oracle's cardinality estimate for this particular range is too low leading it to use an index where a partition scan would actually be more efficient. Assuming that is the case, I would guess that the problem comes from storing date data in a numeric data type. If Oracle knows that a particular partition has a of, say, 20100101 and a of 20140101, it is going to believe that there is a range of 40,000 values. If there are no histograms and you ask for the range between 20120101 and 20120731, the optimizer will expect that you're going to retrieve 730 of 40,000 possible values or 1.825%. If you used a to store your date data, however, Oracle would know that there are only 4*365=1460 values and that you're actually retrieving 212 of them or 14.5%. Without knowing the actual range, these are just example calculations, of course, but they show the principle that using the wrong data type can lead to the optimizer getting confused. Assuming this speculation is correct, you can probably resolve the problem by changing to a . You can probably also resolve it by gathering a histogram on . 

You can't change the owner of a table. You can create a new table that is owned by , copy the data from the old table to the new table, drop the foreign key constraints that reference the old table, and create new foreign key constraints that reference the new table. But that's realistically going to require a downtime window since the tables need to be static while this copy is going on. There are various ways to create a new table that is a copy of the old table. Personally, I'd use your favorite PL/SQL IDE (i.e. SQL Developer, TOAD, etc.) to generate the DDL for and then manually edit the DDL to create . You could also do an export & import with the and parameters. Or you can use the package to get the DDL 

There is no similar option assuming that "db1" and "db2" are actually Oracle databases. What SQL Server calls a "database" is roughly equivalent to what Oracle calls a "schema". In Oracle, you would generally only run one database on a server though that database may have many different schemas to support multiple applications. Each Oracle database has a completely separate set of users. You could create a database link in db2 that connects to db1 and allow the user to use that database link to access objects in schemas on the db1 database. Going further, you could use that database link to replicate data to db2 from db1 so that you could query it locally. That's as close as you'll get to exposing data in one database to a user in another database. Of course, from your description, it is entirely plausible that you merely have two schemas in the same database in which case you can freely grant access on objects in any schema to any user using the appropriate statement. 

The second expression is syntactically invalid (at least on its own, it could be a valid part of some other expression). 

Are you really trying to use Windows 8? No version of Oracle is supported on Windows 8 yet and, if history is any guide, I wouldn't expect any of them to work without patches that aren't available yet. Is Windows 8 even in public beta yet? I'd hate to try to take a class using an early beta operating system. Oracle 11.2 is supported on Windows 7 (which seems like a more realistic Windows version). If you have Windows 7, that's realistically the version you need to install. What, exactly, does the class cover? If you are learning how to do data modeling and to write basic SQL and PL/SQL, you should be fine using 11g since everything will be backwards compatible. If, on the other hand, your instructor is teaching about database administration or is making heavy use of the Oracle Enterprise Manager (OEM) GUI, there are going to be a lot of visual differences that you'd have to compensate for across versions. You can almost certainly get the same information from the 11g OEM GUI, it just may be in a slightly different place.