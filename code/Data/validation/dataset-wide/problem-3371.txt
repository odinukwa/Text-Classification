The above equation has no conception of sequence. Later models that don't ignore the sequence, such as n-gram models and Markov models have shown significantly better entropy figures. 

This applies to any language. People can create new nouns and verbs, when necessary or when convenient. For example, recent additions, to English, of verb forms of google and tweet, do not make English any less natural for it. Now, if some committee modifies Hebrew grammar significantly, I'd say there is good reason to start classifying the modified Hebrew as a constructed language, as opposed to natural language. 

Chunking is a kind of constituency parsing. The theoretical motivation for chunking comes from constituency parsing, à la NP, VP, etc. Though chunking is considered to be only "shallow" parsing, i.e. not the proper kind, it is still useful for practical applications. For example, I'm working on a project right now that needs the software to deal with tweets. Depending on your mindset, such text can either be considered "awful" or, be considered to be of a dialect that parsers haven't been trained for. Either way, "full" parsers of either kind, constituency or dependency, fail pretty badly. Chunking gives me useful nuggets of information. 

Without seeing your corpus, it is difficult for me to recommend a method you could use. The issue you brought up is not NLP, per se, because it involves preparing a corpus for NLP. However the same set of tools can be used for filtering out "noise". 

Using Eye Movements to Evaluate the Cognitive Processes Involved in Text Comprehension An Eye Tracking Study of How Font Size and Type Influence Online Reading Using Eye Tracking to Assess Reading Performance in Patients with Glaucoma: A Within-Person Study Eye movements, the perceptual span, and reading speed 

Leaving aside the fact that it's my own fault that I know nothing about Dependency-Based Compostional Semantics1, constituency-based parse analyses map fairly easily to compositional semantics. I believe this claim applies equally well to deeper grammars (LTAG, HPSG, CCG, etc.) that are the intellectual children of the simple Phrase Structure Grammar. After all, the parse trees don't change much between one of these formalisms and another. 

I generated the DRS by first paraphrasing the sentence as "if a child is not disciplined , the child will be bad ." and then feeding it to appropriate field in the C&C+Boxer demo page. Since the outermost component of this is a material implication, you can disprove it by finding an instance where the child was not disciplined and was still good. Also refer to Belief in Discourse Representation Theory, by Nicholas Asher. 

The paper, Non-projective Dependency Parsing using Spanning Tree Algorithms has a few examples of non-projective dependency trees. 

Very. Even skilled readers make them all the time, typically when an unexpected word breaks their flow. It's one of the key parameters studied in eye-tracking experiments. 

1 — The first time I came across DCS was just a few minutes ago when I googled up some phrases while answering this question. 

Look for papers on multi-word term recognition or automatic term recognition, etc. In English, the most common patterns are [Noun*][Noun] and [Adjective*][Noun]. For a recent-ish survey of the field see A Comparative Evaluation of Term Recognition Algorithms . See the paper An Improved Automatic Term RecognitionMethod for Spanish for examples of POS patterns that the authors used, for detecting multi-word terms in Spanish. 

Yes, Shannon's model for entropy has no concept of meaning or even sequence. Each symbol (letter or word, in our context) is assumed to occur completely independent of another symbol. 

As for understanding their language, it depends on what you would consider "understanding". Given sufficient training data, we may be able to devise rules of its grammar, without knowing what any of it refers to. This is pretty much how most language models work for us at the moment. You might talk to your phone and tell it, "Make an appointment with Dr. Smith for tomorrow at noon". Your phone would follow through, and may mark "Dr. Smith" as the person with whom you have the appointment, but it would have no conception of what a person is. Given your scenario, we will never get to the stage of comprehending their language to understand their lived experience in any capacity. The idea is further explored in Searle's paper Minds, Brains, and Programs. What's missing in your scenario is covered in the topic of the symbol grounding problem. However, not all is lost. Given a suitable corpus of their language, we may be able to figure out their formal mathematics. By that, I mean, the kind of mathematics that Russel & Whitehead got around to formulating in Principia Mathematica. As a result of that, we may understand some of their mathematical proofs, and that may include theorems not yet proven by humans so far. 

The specific examples you mentioned seem to contradict the general question you are asking. The sentence fragments, S1 and S2 has some surface-level similarity, but are very different semantically; the object that's beautiful is not the same. Having said that, to answer your broader question, Semeval 2012 and 2013 focused on Semantic Textual Similarity. 

I agree completely with the comment by @Alenanno, Jun 29 at 15:07. But you had asked for some scientific evaluation, so here it is. I came across a paper by Stephanie Lindemann, Who speaks “broken English”? US undergraduates’ perceptions of non-native English, that, as the title says, deals with the converse phenomenon. "respondents were given a list of 58 countries and asked to rate the English of university students from each of these countries on how correct, friendly, and pleasant they found it on a scale of 1 to 10." The study covers accents from France, Italy, Germany, Russia, India, Mexico, and China. Many AmE speakers commented that a Russian accent sounds "harsh", "hard", "guttural", "very forceful and damaging to the throat". Does this make a statement about Slavic/Russian itself, or about how Slavic/Russian is perceived by American students? I am, of course, assuming that these perceptions would stay the same whether a Russian is speaking English or speaking Russian. 

In German, you can make up such words on your own, as needed. Find words that ends with two of some vowel, like schnee (snow), tee (tea) and words that begin with the same letter, and you have: Schneeeule, also written as Schnee-Eule to make it less confusing. 

There's nothing special about them, AFAIK. A huge number of personal names, everywhere, are like that. For example, George (earth-worker) , Alfred (Elf-council), etc. look just as descriptive. Most names are either formed like this, or express some devotional idea. The names you mentioned sound odd to us because their community chose to translate the names into modern English. Translated, your own name, "who is like God?", is not all that different from the names you mentioned. 

The parser you linked to has been rewritten in Java and improved. The latest software can be downloaded from Stephen Clark's C&C tools page. The old C++ version can be downloaded from there too, which comes with Boxer. As for state-of-the-art tools, I can't recommend anything, but SIGPARSE and SIGSEM are good starting points to find them. 

Discourse Representation Theory can be used for modelling the universe of Charles Dickens' Great Expectations. I don't know if you want to treat "spare the rod, and spoil the child" as a belief within this universe or as a hypothesis. If you want to have it as a hypothesis, I suppose you'd have to add the following to the universe: 

... to say nothing of helping governments identify "radicals". The quiz, Which English, was quite popular on social media, a couple of months ago. It attempts to find out both, which dialect of English your own English is most like, and what your native language is. 

P.S.: I'm not convinced that automated dependency parsing is significantly faster than automated constituency parsing. The speed of parsing depends on many factors such as the algorithms used, the programming language, the quality of implementation, etc. All I can acknowledge right now is that some parsers are fast and some are slow. 

That was the case a long time ago, but now, mostly used as a stepping stone in pedagogy. The problem with bare CFG is that it produces an explosion in the number of valid parses (according to CFG) of typical sentences, whereas a human can easily discard most of these parses as meaningless. To overcome this limitation, broad-coverage parsers are based on statistical models. CFG may still be the grammar of such parsers, but the term "computational model" encompasses a lot more now. Since the field is vast, I'll point you to just one example of how a parser could work. Matthew Honnibal's blogpost, Parsing English in 500 Lines of Python, is one of the best I've come across. 

I am not aware of any central repository for all Bollywood scripts, and The Internet Movie Script Database does not host Bollywood movies at the moment. But the screenplay of individual movie are not hard to come by. The troublesome part is looking for screenplay one movie at a time. There are a few blogs run by keen amateurs that host a few titles each. Here are some examples: 

I know practically nothing about Hebrew. My answer is based on your claim, "a lot of the things virtually not present in at least two millennia have been given words that were literally invented". There are two classes of words, open class and closed class. 

... it heavily relies on a combinatory categorial grammar (CCG), which must be trained... There are two broad-coverage CCG parsers for English, which you could use: 

While this is a good way to get started, you would eventually have to move to a more usable parser, such as the ones bundled with NLTK. 

Normally, source code and console output are formatted differently, with a "<pre>" or with custom CSS. You could write regular expressions or use HTML parsers in your favourite language to filter them out. If this task is going to be part of a larger tool-chain, GATE is a decent way to do things. Its Jape plugin is what you're likely to use. 

What follows is just my impression. Syntactic category is a term that has a formal definition in a few grammar formalisms. In contrast, "part of speech" is the intuitive class of a word. I suppose the equivalent term for syntactic category in HPSG would be Typed Feature Structures or just signs. In CCG, for example, the syntactic category refers to many things: intransitive verb, transitive verb with the object filled-in, a modified verb phrase, etc. As another example, given the phrases "young Obama" and "Barack Obama", a (trained) person may tag "young" as adjective and "Barack" as noun. However, in CCG, they are both . 

I notice that you haven't mentioned prosody in your question. If we have to account for prosody too, it would add another layer of complexity to a unit selection synthesis system. In that case, a CFG is definitely not powerful enough to accommodate phonological rules. 

However, as the numbers get bigger, it appears§ to be more idiomatic to switch to a form that's similar to English. tum chaarõ ladke mere saath aaoo. All four of you boys come with me. tum saare chalis ladke mere saath aaoo. All forty of you boys come with me. § — I speak the language fluently, but I'm not a native speaker. 

Beyond stating the obvious, there are subtle reasons why the reading process suffers from intermittent failures and corrections. These corrections can be used to understand 1. how a beginner approaches a new language, 2. what kinds of sentence structures impede comprehension, 3. what kinds of fonts are best for various kinds of text, and so on. Here is an arbitrary list of papers, if you want to explore the topic further: 

From the description of your task, I get the impression that you do not really need semantics of any kind — you intend to discard many segments of a sentence. I believe that you'd be better off looking at the chunkers listed here. Chunkers have differing performance levels for noun phrases and verb phrases. You could start with the ones that perform better with verb phrases. You could also try to play with NLTK's chunker. You'd have to define your own patterns for NLTK's chunker. I haven't worked with any of these tools, myself. So I can't tell you which ones require you to write software. I understand that you are a student of marketing. I don't know how comfortable you'd be with writing code. But if you have to write code, I think writing Python (for NLTK) should be less of a hair-pulling experience for you, than writing Java for one of the Java-based tools could be. 

ADDENDUM $URL$ has a nice online demo that draws parse charts given a CFG grammar. The website has some limitations, so I recommend that you: 

The influence of speech on writing is quite well-known, so I won't go into that. Coming to the influence of writing on speech, Chomsky gave a talk at Google recently, where he spoke about various things. At one point, a questioner asked about the relation between writing and speech. The question was very different from yours, but his answer applies here. After Chomsky talks about the work of Ken Hale on "cultural gaps" (probably this article), Chomsky mentions another study, of Middle English (author and title not mentioned) that showed that "the use of complex constructions... which had embedded elements in them increased as literacy increased". Chomsky then posits an explanation for that, having to do with speech being constrained by our limits of short-term memory. He concludes that "once literacy spreads, you get much more complex linguistic usage, even in speech, because it carries over from writing to speech". Apart from that, the influence of speech on writing is getting more and more prominent these days because (1) most people are literate and (2) many people communicate a significant portion of day-to-day affairs via short texts of various sorts (instant messaging, SMS, tweets, etc.). The drive for economy has led them to invent a huge number of abbreviations. Some of these have spilled over into speech as abbreviations (OMG!) and words (ASAP => ay-sap, YOLO => yo-lo). I'm sure a teenager could give you many more examples. 

There is a website for accents: $URL$ They don't have sentences. (I think the website would become unmanageable if they did.) I don't know if they have idiomatic phrases, but they have quite a few single words and compound word phrases. Anyone can contribute by submitting their own samples, and by rating samples provided by other users. This one, for example, has the word 'bottle' as pronounced by Brits, Americans and Canadians. $URL$ 

i.e., what happens with A depends on what comes before or after A (A's context). Wikipedia has a Type-1 example which I don't know how to write here. [Feel free to edit this post.] Thue is a Type-0 language which also cannot be represented in BNF. As a realistic example, many natural languages require that the formal grammar used to analyse them be Mildly context-sensitive. 

I'm not an expert in the field, but I believe "psychoses" is too broad a topic. I have come across studies that show a link between word-usage and autism or schizophrenia. 

My answer is partially motivated by Dominik Lukes's answer and some of the things I read in the comments that follow it. 

You can sign-up for the Stanford NLP class here. It is conducted by Jurafsky and Manning, authors of the extremely popular textbooks that the others mentioned. You can watch videos by leading researchers in the field at videolectures.net. I recommend starting with Clark's lecture and moving on to other lectures suggested in the section called "See Also".