For biblical Hebrew there is the Semantic Dictionary of Biblical Hebrew which may give you some inspiration. 

A term often used synonymously to "generalisation" is bleaching. Joan Bybee in his Cambridge Textbook in Linguistics on Language Change defines it as "a meaning change in which specific features of meaning are lost" (p. 267). 

G is called G for the German Grundstamme. D is called D because of Doubling of the middle radical (/qatal/ > /qittel/). C is called C for Causativity. It is also called Š sometimes which better reflects phonology (although in Hebrew the marking consonant became /h/). Dt is often called tD because the /t/ became a prefix rather than an infix elsewhere. Verbal plurality can be of subject, object or action, e.g. "they broke the glass" in G becomes "they shattered the glass (in many pieces) in D via object plurality, or "they repeatedly broke the glass" (action plurality) or "they all broke the glass" (subject plurality). Causativity is e.g. to cause to be seen, i.e. "to reveal". Thus you see that these distinctions are in English covered by lexicon rather than grammar (i.e., there are different verbs for different Aktionsarts, "broke"-"shattered" and "see"-"reveal"). Besides this nice symmetric system there was an N-stem, the original semantics of which are still debated but which probably had to do something with middle voice. In Central Semitic, "internal passives" are developed for the three core stems G, D and C. In Arabic grammar these are usually not treated as separate stems, but in Hebrew and other languages they are: 

References: Dixon, R. M. W. (1972). The Dyirbal language of north Queensland (Vol. 9). CUP Archive. Dixon, R. M. W. (2002). Australian languages: Their nature and development (Vol. 1). Cambridge University Press. Haspelmath, M. (2011). The indeterminacy of word segmentation and the nature of morphology and syntax. Folia Linguistica, 45(1), 31-80. 

The similarity is due to a common pathway of grammaticalistion. The have + past participle form comes from a resultative construction (Bybee, Perkins and Pagliuca, 1994), which commonly leads to the perfect. Note that Bybee et al. do not use the term resultative in the usual complex-predicate sense, but use it to mean sentences like 'The door is opened', whereby a past action leads to a present state. The English have + past participle construction comes from sentences like (1), where have was still possessive and the participle agreed with the object in number, case and gender: 

I think a few issues need to be cleared up first. Firstly, the first source you mentioned is an ESL source rather than a linguistics source. If you'd like to learn more about linguistics, I'd suggest consulting more linguistically-oriented sources like Payne's article, since ESL sources tend to be simplified - they are, after all, aimed at language learners, not linguistics students. Secondly, another issue to be cleared up is whether we're talking about descriptive categories, which are specific to languages or comparative concepts, which are used for cross-linguistic comparison. It seems here that we're talking about descriptive categories, but you did not specify this in your question, so I think it is helpful to make this explicit. This is because the discussion of what constitutes a lexical verb and a copular verb in English has nothing to do with, say, French être or Mojave iðuːm, or even Middle English be. Finally, Payne never defines the lexical verb as a copula verb. He is, in fact, arguing against the treatment of the copular verb be as an example of a lexical verb. He regards it as an auxiliary, and uses several kinds of evidence to show that the copula be and the auxiliary be behave the same way morphosyntactically except when the difference in behaviour is purely semantic/pragmatic. More generally, lexical words are generally those that form an open class in the language, i.e. a class of words to which words can be added freely. For example, in English, you can easily form new words, e.g. you can form new lexical verbs by X-ise meaning 'to make something like X'. It's much harder to form new auxiliaries like can or may. This is the 'general idea' behind the distinction. However, these criteria seem not to be really helpful for determining whether be is lexical or auxiliary in its predicative use, since we don't know whether be belongs to the first group or second group just by looking at it. This is where morphosyntactic criteria come in: We can define lexical verbs and auxiliary verbs by means of their morphosyntactic behaviour. This is the whole idea behind language-specific descriptive categories. Descriptive categories are invented on a language-specific basis to describe the particularities of the grammar of a particular language, in this case English. Payne cited language-internal evidence for treating the copula and auxiliary be both as auxiliary, e.g. be can take the negator not and be fronted in questions, whether as a copula or as an auxiliary for tenses: 

Cayuvava, Hua (Yagaria), Hawaiian and Senufo are the languages most widely misbelieved in the literature to have only CV syllables. Key 1961 "Phonotactics of Cayuvava" (IJAL) clearly shows CVV and V syllables, so scratch that. Hua has ʔ and n as coda (I don't have access to the full grammar, but that is enough to answer the basic question). Hawaiian is dealt with in the OP. Senoufo is really a language family (Minyanka is an example), and individual languages differ somewhat. Senadi has V, CVV, NCV, CGV syllables, as do Supyire and Suchite. A common way of disposing of postconsonantal glides it to posit that the onsets have palatalization or rounding, and NC is often disposed of by calling the onset a "prenasalized consonant". 

This is really more a comment on Jknappen's answer, but it's too long. A premise of the IPA is that each symbol is a sound that is a contrastive phoneme is some language. The letter "i" represents a general area of the vowel space, which overlaps the space of adjacent vowel somewhat (especially "ɪ"). If you look at a range of IPA illustrations, you will note that the vowel positions vary within the vowel frame depending on language. The position of the vowel in the vowel frame is more informative than the letter used, but that spacing obviously can't be included in a transcription. Ideally, this positioning would be based on measured mean formant values, but typically they are based on subjective evaluation. Usually, the convention is to select the IPA letter that is closest to the perceived location of the given language's instance of the vowel, but you can pick a different symbol either for phonological reasons ('it acts like a front vowel') or typographical reasons ('why not just use "a"? How do you even type "ɐ"?'). In order to reduce all phonetic vowel differences to single letters, an enormous set of distinct symbols would be necessary, since there are dozens of known kinds of "i", likewise "ɪ", "e", "ɛ" and so on. 

and thus, since [for + NP + to-infinitve VP] is now a nominal constituent, we can have sentences like this: 

Again, the construction became the perfect (and, in modern French, the past perfective). It lost the object agreement, spread to all dynamic verbs (not just change-of-state), and the have + participle form was the only form that survived in languages like French. There is sufficient semantic similarity between the resultative and the perfect - described by Comrie thus: 'the present auxiliary conveys the present meaning, while the past participle conveys that of past action' - and this would explain why both Germanic and Romance languages developed this form. Sources: Bybee, J. L., Perkins, R. D., & Pagliuca, W. (1994). The evolution of grammar: Tense, aspect, and modality in the languages of the world (Vol. 196). Chicago: University of Chicago Press. Comrie, B. (1976). Aspect: An introduction to the study of verbal aspect and related problems (Vol. 2). Cambridge university press. Hopper, P. J., & Traugott, E. C. (2003). Grammaticalization. Cambridge University Press. 

In both cases, the verb can also take an accusative object meaning what is being asked. In addition, the dative case only appears when the recipient is a pronoun; otherwise a prepositional phrase appears, respectively à in French and yu in Late Archaic Chinese. 

It is correct that at least two pronunciations exist across all speakers of American English. There is no reliable data on frequency of particular pronunciations, and MW certainly hasn't conducted large-scale sociolinguistic surveys. In my dialect, I just have Xɛɹi (mary, marry, merry etc), and I know people who distinguish [meɹi] "Mary", [mɛɹi] "merry", [mæɹi] "marry", at least I think that is the sound / spelling correlation. I'm not an expert on what constitutes a rhyme (you learn that stuff in junior high and then it fades), but that a disyllable can't rhyme with a monosyllable, so "petty" and "pet" can't rhyme, nor can "airy" and "air". 

Linguistics is an extremely broad field, encompassing aspects of sociology, physiology, psychology, computer science, law, education etc. etc. and the structure of languages. E.g. English has certain properties, Chinese has others, and Swahili has yet others. The study of the structure of languages (generally, or individually) at a single time -- like, English as spoken now, by me, or by you, is the study of the grammar of a language. It is obvious that we have words (it may be hard to tell in some cases whether you have one word or two, but it's not hard to tell that there is a word "person" or a word "kill" or a word "snake"). The study of how words can be arranged into utterances is the domain of syntax. For instance, in English you can say "The children see the man" and not *"See the children the man", though in some languages you can say that (sometimes only that, somtimes, that as well"). Syntax is related to the field of morphology in that the words "kill", "kills", "killed" and "killing" are related words, and you have to pick the right word form in constructing a sentence like "The snake ___ people", inserting a form of "kill" (kills, not kill or killing). Morphology is related to phonology, in that phonology takes simple and invariant parts of words and modifies the pronunciation, for example the past tense is pronounced [t] in "slapped", as [d] in "scrubbed" and as [ɨd] in "wetted" – there are rules that change the suffix /-d/ supplied by the morphology so that it's pronounced [t] or [ɨd], depending on the context. Phonetics then provides more precise specifications of the physical implementation of individual sounds. Semantics, on the other hand (getting back to syntax) specifies the relationship between the meaning of an utterance and the words, morphemes, and syntactic structures that you use. The division between pragmatics and semantics is subtle and contentious, and you need to ask a separate question about that, if you care. There are also relationships between semantics and morphology (morphemes often have a particular semantic function), semantics and phonology (there are, occasionally, phonological rules that are sensitive to semantic properties), phonology and syntax, and so on. 

That raises the question of whether it constitutes a coordinating conjunction or a subordinating conjunction (which, depending on the conjunction, are subsumed under prepositions and complementisers in modern times). Since '*Than you think, he's better.' is not grammatical, the prepositional analysis seems unlikely (compare as long as, because). While this doesn't undermine its preposition status when preceding NPs, it seems to make it even less likely now. Coordinating conjunction actually seems possible. We say 'He is tired but happy' and 'He is more tired than happy'. We say 'He looks confident and you think he is (confident)', and likewise we can say 'He looks more confident than you think he is (confident)'. The other alternative is that 'than' is a complementiser, but that cannot capture the cases where 'than' precedes an NP. Besides, that would imply constituency, which again seems not to be satisfied (e.g. *than I have, John has eaten more.) My questions are: 

Many complex predicates are historically derived from serial verb constructions. This is not only true of the Sinitic family. For example, in Saramaccan (Byrne 1987, as cited in Givón 2009): 

Note that this does not mean everything works. For example, adjectives can go to predicate positions, but adverbs cannot! You can say 張三的死很突然 (Zhangsan-possessive death very sudden) Yet you cannot say *張三的死很忽然 (Zhangsan-possessive death very suddenly) As a side note, in Classical Chinese, adjectives and nouns can act as transitive verbs, indicating another flexibiltiy in position, but not in Modern Chinese: 

A good diachronic overview paper with many references is Gzella, H. 2009. 'Voice in Classical Hebrew against Its Semitic Background', Orientalia 78(3), pp. 292–325. 

You are implementing the Jaccard coefficient whereas the library has the Jaccard distance. The coefficient tells how related two sets are (it is high when they are similar), whereas the distance does the opposite; it is low when they are similar. In fact, they are each other's complement, i.e. d = 1-c and c = 1-d. This is also explained on the Wikipedia article you linked: 

An older version of this answer just used the qtree package (instead of tikz-qtree) and draws something that looks like your tree. The only trick we need is to use and , because otherwise the bar over the T is touched by the edge. The differences with the original: 

Floating quantifiers are quantifiers that can move away from the corresponding noun, such as "each" in "The boys hit each other" where it modifies "The boys". I am interested in prepositions in these bipartite reciprocal constructions, as in "They followed each others onto the stage" or "They waited for each other." These constructions are comparable to "They waited, each for the other." In this last example, the preposition is near its complement "the other", but in the earlier examples it is not. Is there a term for these kind of "floating prepositions"? This term does not yield any results on Google Scholar. 

I figured I'd give an answer with respect to the languages mentioned With regards to questions, Chinese is wh in situ, and consequently has no need for coverb stranding - contrast this with the translation (sorry if you hate this construction): 

Though of course more language-specific information is needed, general criteria do exist. The finiteness of a clause is basically how non-nominalised it is (Givón, 2001), and, as is well known, the 'nouniness' of a constituent is gradient rather than absolute (Ross, 1973). Thus, the finiteness of the clause can be described as how few NP-like features it has. Givón (2001) lists the following criteria. Clauses which satisfy a lot of these are less finite (closer to the prototypical NP), whereas clauses satisfying most of these are the most finite: 

I was wondering if there are any good, up-to-date advanced textbooks in parsing, after having finished the preliminary treatments in Jurafsky & Martin and Manning & Schütze. Either constituency or dependency parsing will do, though I'm not interested in deterministic parsing. I have a background in statistics, though not really in computer science; however, I'll be happy to fill in any gaps in my knowledge as I go. Thanks! 

For starters, you need to distinguish "complete" things from "incomplete" things. Supposing that my intention is to say "Well, I hope that the next time you go to that store, you'll count your change", but after saying just "well, I", I'm interrupted. That might be an "incomplete sentence". It's not actually a sentence, it's part of a sentence, so you have to understand "incomplete sentence" as being a fragment of an intended sentence. The intended sentence isn't necessarily a complete thought, since I was also thinking that this store cheats people in giving change, and maybe some day the law will get after them, maybe even closing the store or prosecuting the owner, but I didn't intend to say all of what I was thinking. The concept of "a thought" is so hopelessly nebulous that it's really pointless to talk about "thoughts" and their relationship to linguistic form, and instead we focus on something that we understand a bit better, namely propositions. In that case, we might ask questions like "do all sentences convey propositions?", or "do all linguistic utterances convey propositions". Many utterances don't count as sentences, by standard syntactic accounts of what a "sentence" is. For example, if I ask "Who took the sandwich" and you respond "Bill", that isn't a sentence. It can be contextually interpreted as standing for "Bill took the sandwich", or "I think that Bill took the sandwich", or "Bill is the one who took the sandwich", or any number of other actual sentences. As far as I know, there is no language where speakers are incapable of generating well-formed and intended utterances which are less than a sentence: bare NPs are always possible utterances, and are often used. People don't usually walk into a room and say, with no prior context, "The old grey mare", but if we automatically convert all utterances into "intended sentences" just in case they don't actually qualify as actual, full, well-formed sentences, then we are just begging the question of the relationship between sentences and propositions -- we are making sentences out of things that aren't sentences, because we can guess what the underlying proposition is. If we don't interpolate "missing parts" (syntactic deletions and other omissions), then in ordinary speech, most utterances are probably not complete sentences. Yet most often, the intended proposition is easily reconstructable. Therefore, I conclude that complete sentences are not a significant desideratum in communication. 

The short answer is that there's no exact answer. There is no clear cross-linguistic definition of what a 'word' is, and therefore no real distinction between syntax and morphology. Indeed, there are calls to unify the study of grammar into morphosyntax instead of using the traditional syntax-morphology distinction. Haspelmath (2011) writes: 'Instead of a subdivision of the grammar of sign combinations into morphology and syntax, we can just work with a unified domain of morphosyntax'. However, in the context of ergativity, I think 'syntactic behaviour' is easier to tell. You can think of it as behaviour other morphological ergativity, which is usually realised as overt coding on core arguments using case morphemes. Syntactic behaviour, on the other hand, is typically related to syntactic pivots: in clause linkage, which participant serves to link different clauses together? In a nominative-accusative language like English, the pivot is typically nominative, i.e. A and S can serve as the pivot. In a syntactically ergative-accusative language, the pivot is typically absolutive, i.e. S or P. Compare English and Dyirbal, the classic example of a syntactically ergative langauge: 

It depends on who you ask. 'Because' is not in the same syntactic category as 'and', as they show different behaviour: