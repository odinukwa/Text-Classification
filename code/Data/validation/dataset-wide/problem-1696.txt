As this GCP public documentation states, Snapshots are incremental and automatically compressed, so you can create regular snapshots on a persistent disk faster and at a much lower cost than if you regularly created a full image of the disk: 

wget might get disconnected during the download but you can use options described in above link that will help you make the download successful, like the -t and -c options for trying the download more times or continue getting a partially-downloaded file, respectively. After the file is downloaded you can use 7ZIP to unzip the file in the directory where it was downloaded to using the command: You can copy the file to a GCP bucket using the command: If wget and 7zip are not installed in the VM you can install them as per instructions, wget and 7zip, as follows. This examples are for Ubuntu or Debian Linux VMs: 

If you want to create a backup of your VM instance which includes your E-commerce (Magento) and have the VM backup include files, configuration files, databases in the VM disk, orders info, etc, you can create a snapshot of your VM as per the following snapshot documentation link. This documentation will guide you to create a snapshot backup of your instance persistent disk which will include the VM instance configuration file and database if hosted locally, depending on your configuration. To create a snapshot: 

Can you run a dig command from your client machine to one of the machines the DNS server resolves for and share the results? Take into account that the Google Cloud DNS server that you have at the other end has to be registered with higher level DNS servers in order to be reached as explained in this page. If not, the world has no way of knowing you have a DNS server service on the Domain Controller. If you have already registered the DNS server, then you have to look at your client configurations as well, check your client machine name, domain name and name server you have set up. If it’s a Linux machine you should look at the configurations in /etc/resolv.conf and add the nameserver which depends a bit on which unix version you have. 

Besides making your own CD, you will also need to recompile the kernel. I haven't done this in awhile so I'm not sure how much help that link will be, but most docs I have followed in the past have been fairly straight forward. If you need further help you might want to stick to the Ubuntu forums. Some of the followers on those forums are masters at this stuff. Recompiling the kernel should also allow you to make a truly customized CD that should run pretty speedy, because you can take out modules you know you do not need. 

In case this helps other people, I have a Synology NAS and the NFS point actually added another item to the path. I was trying to mount "xxx.xxx.xxx.xxx/folder" as this is what I do in Windows. But for Ubuntu with NFS I had to use "xxx.xxx.xxx.xxx/volume1/folder". 

So the initiating server was able to connect at one point and now all of the sudden it cannot? And the only thing you know that has changed is the initiating server has file services installed? Could perhaps a default setting been changed? I know that on server 2008 systems there is some setting to allow remote desktop connections for older remote desktop connection applications from older systems. Is there a setting to allow you to remote connect to older remote desktop connection systems? Has anything changed on the 2003 system? Can you verify it can accept connections? Try to initiate the connection from another host. For instance do you have an XP client or another server 2003 system you could attempt to initiate the connection from? If that works then the issue would likely exists on the server 2008 system. If not, then it is either an issue with the server 2003 system on the other side of the tunnel or a network issue. 

If anyone is interested in experimenting, just use Wireshark. If someone really gets on our case about slow connectivity or dropped packets we just mirror a port on a switch, connect up a laptop with Wireshark and take a look. 

This has me beat... we have microsoft dynamics with a sharepoint business portal interface using sharepoint services 3.0. im trying to configure the sharepoint interface to accept basic auth, so that reverse proxying works. ive found a couple of places that need changes, both in sharepoint and IIS, but whenever i switch it over, certain pieces of business portal start to error out - things like the project communicator and expense reports section. basically, it looks like anything that appears in an IFRAME doesnt work. while we dont use these pieces yet, we will, so this is basically a showstopper for me. so far ive tried: 

why not just give those hostnames a bogus IP (either via CNAME or A) in DNS rather than dropping the packets. seems like it would be less of a load on your firewall (and fewer rules). 

In one user mailbox anything sent from any distribution list in our domain shows up as being from 'System Administrator'. It only happens when they are viewing the inbox using outlook (OWA is not affected), and its persisted across computers (though it did not happen immediately). When other users view the inbox from their outlook install (ie open users folder), everything appears as normal. Other folders are not affected. if a message is moved into a subfolder, the sender displays properly. Because of the persistence, and it only affecting one user, I suspect some user behavior is causing this, but i cannot determine what. Ive checked the contact list, and its not that. 

Adding the following registry entry solved the issue: HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Services\MSExchangeIS\ParametersSystem\InternetContent\MimeHandlers{85D2DDB8-6225-11D2-BDF1-00C04FD655B5} FixRecipientTrackStatusTime DWORD 1 I rebooted it after making this change, though the KB article does not indicate this is necessary. I have no idea if this was required or not - the machine needed it outside of this change. From: $URL$ 

When using apache to reverse proxy, is it possible to forward $URL$ and $URL$ to different servers? Say one goes to $URL$ and the other goes to $URL$ I can obviously just drop further down the path (ie $URL$ but its not desirable. 

This error appears to suggest that you’re using the Stackdriver Basic Tier which allows ‘Logging agent only’ as per the documentation, section ‘Stackdriver account service tiers’. I presume you would need the Premium Tier: 

The first successful snapshot of a persistent disk is a full snapshot that contains all the data on the persistent disk. The second snapshot only contains any new data or modified data since the first snapshot. Data that hasn't changed since snapshot 1 isn't included. Instead, snapshot 2 contains references to snapshot 1 for any unchanged data. Snapshot 3 contains any new or changed data since snapshot 2 but won't contain any unchanged data from snapshot 1 or 2. Instead, snapshot 3 contains references to blocks in snapshot 1 and snapshot 2 for any unchanged data. 

It takes more time to create a full disk snapshot which can degrade system performance. This may be a negative factor for a production system serving thousands of users. Recovering your lost data from a point in time is easier and faster if you do it from an incremental part of your full snapshot backup. 

The only option at present is to create snapshots/images of the VM Instance boot and the additional disks, however there is a feature request so that this feature can be made available in the future. There is no ETA but any updates can be checked on this link above. 

If you want to have full copies of your VM disks regularly, you can consider creating custom images of your (boot) disks, as explained in this public documentation. 

The first snapshot that is created is a full backup of all your data in the VM boot disk. Any further backups contain any new data or modified data since the first successful snapshot. It would be very important that you follow the recommendations and instruction in the above documentation link to ensure that your snapshot is successful. It is also important to check and test that this backup is correct and check that you can recreate your systems from it should a disaster occur. 

One of the things we found out at my company in working with Netapp is that deduplication really only works well in a VM environment if you have your drives aligned. Which is a problem for us as we have a lot of Windows Server 2003 machines and none of the drives are aligned. Which means you barely recover around a fourth of the space possible if the drives are aligned correctly. We are being told though that once the drives are aligned correctly we should be able to recover 40-60% of our space back with dedup. 

This statement is confusing to me. We don't know anything about the address the OP gave us except the range it exists in, because the OP never said on what device it was found. We do not know if it is the address of a switch, a server, an AP, a computer, a printer, etc. So how you would know that from the small post from the OP wrote is beyond me. I agree it doesn't have to be the gateway and I have already mentioned this. As I already explained, when you look at most large companies (but this is Cisco's best practice and is usually applied to most businesses) you actually find that gateway addresses will be the last or first assignable address in a range. 4.100 would be in the middle and would make no sense to be a gateway address. While some network admins might assign it that way, keeping track of this would be cumbersome, especially in increasing network sizes. This becomes even more true when HSRP and such technologies are used which take up two address on each layer 3 interface and give out a third address for the gateway. Keeping track of hundreds of such gateways when HSRP is being used becomes very difficult if there isn't a system for assigning addresses. Think of a company that might have 100 different VLANs... 

Troubleshooting an issue with an FTP connection going through an iptables firewall and seeing some strange issues with passive mode. We are trying to connect to an FTP server and get the directory listing, and it seems to work in PASV mode in all cases, but times out when EPSV is sent instead. The server understand EPSV, because that works with iptables disabled. tcpdump shows the client sending the EPSV request, and then the server does not respond after EPSV is sent. if the client sends PASV, everything works as expected. nf_conntrack, nf_conntrack_ftp, nf_nat, and nf_nat_ftp are all loaded and the appropriate rules are in place. 

if one has a run of singlemode fiber terminated in a patch panel, how would you then connect it over to a device? singlemode patch? it was my understanding that singlemode had minimum refraction distance requirements. 

Reading over (and then testing out on a test database) the setup for mirroring it appears that one is supposed to leave the mirror database in restoration mode for normal operations. is this correct? at whatever point it becomes necessary to initiate the failover (im not using a witness), will the primary database be put into restoration mode? this seems goofy. 

I have an ASA5520 that is set up to send logs to a splunk syslog server. the setup works for a while, usually around 24 hours or so, but then stops until either the logging is reconfigured (twiddling the ports) or the ASA is restarted. what should i be looking at to resolve this issue? im not sure if its the splunk syslog daemon ignoring connections or the ASA that gets messed up and stops sending. id like to enable the 'dont pass traffic without logging working' option, but without a stable connection to syslog, thats a non starter. tried so far: TCP and UDP, different ports, changing the logging level 

may not be explicitly helpful in answering your question, but you dont need a GUI to run wireshark. X11 tunneled over SSH would allow you to run the full app remotely. It works well with either cygwin-x or openssh, depending on if you use windows or linux on your desktop. edited: grawity is right, its not port forwarding.