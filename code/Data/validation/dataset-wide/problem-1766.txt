The appears to run fine and gives no errors, however (2/3 of the time or so) installing throws a "no installation candidate" error. When I ssh into the server and run I get the same error. Running by hand, then the package installs fine. Any ideas on what might be happening, or ideas for debugging? 

I have a command that runs a disk snapshot (on EC2, freezing an XFS disk and running an EBS snapshot command), which is set to run on a regular schedule as a cron job. Ideally I would like to be able to have the command delayed for a period of time if the disk is being used heavily at the moment the task is scheduled to run. I'm afraid that using nice/ionice might not have the proper effect, as I would like the script to run with high priority while it is running (i.e. wait for a good time, then finish fast). Thanks. UPDATE: This is what I ended up going with. It checks /proc/diskstats and runs my job when the current IO activity hits 0, or we timeout. I'll probably have to tweak this when I look at what kind of IO activity our servers actually get in production: 

...to change my passphrase for my key, but I'm not sure what this means. If I'm encrypting data on box A and decrypting on box B (say with duplicity) do I have to change the passphrase on both ends? Will previous backups still work? Is the passphrase just the key to a sort of encrypted wrapper around the key file? Dumb question, but I don't want to screw this up. Thanks! 

I'm using an Alestic image which disables root SSH logins, but provides a user "ubuntu" with NOPASSWD sudo privileges. See here. In the course of trying to add a new user to the sudoers file I inadvertantly created another line for the "ubuntu" user, this time without NOPASSWD. I have now apparently lost root access to this machine. Is there some way to mount the EBS root volume on a different instance (fixing the sudoers file) and then re-launch the server? Or am I totally screwed? 

I have been running various processes over night on a Windows 2008 R2 AMD64 SP1 virtual server share via a VPN connection and remote desktop. Recently, I have been noticing that when I return in the morning, the processes I had started the previous night are stopped and the applications have been shut down (I am the only one who uses this share). Sometimes the processing completes as expected and other times cuts out mid way--likely after the application encounters an error and is idle. I am primarily using ArcGIS 10.2 and Python geoprocessing scripts via IDLE or PythonWin. I have tried investigating the Session Time Limits within the Local Group Policy Editor without success (see attached screenshot). I was following the instructions from this blog. It appears that all of the settings are at the default, which should not impose a timelimit on applications. Is there a way to ensure programs are not automatically shut down (even after an error occurs), or at least find a way to track why a program or operation was shut down? 

I am using an AWS EC2 Ubuntu instance that was set up and configured by someone else (i.e. I do not have access to the management console). This instance is primarily used to download files and sync to an S3 bucket, which I have automated with shell scripts. However, I do not know the instance type or the network performance, so I cannot optimize the downloads. For example, I need to know how many simultaneous downloads the instance can perform. How can I determine the AWS EC2 instance type and the network performance (i.e. download/upload speed)? 

I am about to create a new Ubuntu Server EC2 instance. However, there appears to be very little information regarding the bandwidth available for each instance type. The server will be primarily used to download files from web urls, so download bandwidth will be critically important. What is the best way to determine which EC2 instance to create based on bandwidth available for downloading? As a side note, I am considering the following EC2 instances which meet my hardware requirements: m4.large, m3.large, c4.large, or c3.large (EC2 details here) 

We have a web service running on Amazon EC2. Currently we have some live user data stored on a single disk (EBS). We are considering moving to a RAID0 setup (we don't have to be concerned about the increased failure rate). If we do this migration, what is the quickest (to minimize site unavailability) way to reliably transfer the user data to the RAID array? One idea I had was to take a recent snapshot of the data, copy it over to the new RAID array, then when the site goes down for maintenance use rsync to copy only the changed data over. I'm not sure if this would actually save time or ensure data integrity though. 

What specificaly is involved in an Elastic Load Balancer health check on an instance? I know that it performas an HTTP(S) "ping"; does it just deem an instance "Unhealthy" if that HTTP request returns an error status number of times in a row? Or does it take other factors into account, such as CPU usage? 

I understand it displays the command with arguments, or when unavailable the command in square brackets. But where do the names come from for processes such as passenger worker ruby instances, which show up as: 

are there any shortcuts that can be taken on this initial "resync", since there's not (AFAIK) any data I need copied between the empty disks can I blithely format and treat it like a normal disk while it's in the middle of its initial sync, as long as I'm okay with it being in a "degraded" state for a bit? 

I would like to configure sudo such that users can run some specific commands without entering a password (for convenience) and can run all other commands by entering a password. This is what I have, but this does not work; a password is always required: 

I'm using fabric for provisioning instances from scratch (a stock ubuntu image) on EC2; it works great. There are a few basic functions for appending to and commenting lines in a file that make modifying config files workable.