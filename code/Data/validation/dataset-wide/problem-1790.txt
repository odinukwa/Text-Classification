For my money, I'd do two three-disk arrays, with one shared hot spare. If you don't have an need for a single block of space larger than a 3 disk array, then there is no reason to cram all 6 disks into 1 raid. You're not going to gain anything in performance or over-all space, and, given a two disk failure, you're probably going to be in a better place. @Dayton Brown: The total space will be the same...3 1TB drives in a RAID5 is what, 1.8TB? 6 will be 3.6 by the same measure, so in that sense you'll have more space in that particular RAID volume, even though the total space will remain the same. The difference is, RAID5 only allows for 1 parity drive, whether there are 3 drives in the RAID or 300, so splitting the drives into manageable groups adds protection against multiple failures. Even if you lost 3 disks, for example, you'd only lose half of your data. If you moved to RAID6, a six disk array would make more sense, because you could lose two and be okay. But most people jump straight to RAID10, and skip 6. 

I tend not to put it in /etc/sysconfig/network, though you certainly can. I do a lot of really complex bonding configurations, and it makes it easier if all the files are in one place. If you do it like I do it, DO NOT declare the gateway interface in other IFCFG files. It will read them in order, resetting the gateway every time, and you won't get the expected behaviour. 

Well if they're really only hitting you once per ip address, for all different pages, then there really isn't anything you can do but trim down the 404 page...Or you could just redirect 'em to the front page and declare it as a page view...;) 

Meaningless buzzword. Basically it tends to apply to scalable application hosting provided by third parties, but most people use it to mean effectively, "We don't know where or how we're going to host this." So called because of the "cloud" icon commonly used to represent the internet. 

When you want information on processes, the answer is always It is simple, and yet it has a ridiculous number of options. Try this one: 

It's not a big deal. In terms of processing power it's less than a lot of standard URL rewrite rules. Unless all your pages are pure html, you'll see much higher process costs on the scripting engines. 

I start at the outermost router and work my way down, and I measure performance in the most primitive way: use a bandwidth testing site, or a known external FTP site that will give you your upload/download speed, and keep going down until you find the level where the problem resides. Once you know where the problem is, deploy your fancy tools and monitors. But don't waste time doing that stuff on every layer. It'll take forever. 

I've always heard that it doesn't matter as long as you're internally consistent. That being said, in *nix system architecture it's always singular. (e.g "user", "home", "mnt", etc (or should that be "etc"?)), so that might be a better default since it's already common? 

The usual problem with this change is that the background image is either the wrong format, or located in a non-shared location. 

NMAP determines that information based on a certain amount of guesswork. If your SSHD responds to an NMAP probe at all then NMAP will check that response against it's database, and make a guess as to what system you're running. The probes are pretty sophisticated; for example, it may try several variations of initiating a SSH connection on a port, to see which versions are recognized. It may attempt to provoke an error message, or it may just check the "hello" screen. The surest way of blocking NMAP probes is by simply blocking NMAP probes all together at the firewall, and white listing servers that are allowed to SSH into your machines. 

Simple, right? Good old Cisco. The "Static" command tells the pix that the internal machine is attached to the external address, so that it will route traffic accordingly. The "access-list" command tells the pix that you're creating a new access-list, you're allowing or denying "any" external machine to talk to your new external address on (port number) using tcp or udp packets. The access-group command associates that access-list with an interface on the pix. Here is a link to Cisco's Pix Command Reference for Commands starting with A. You can refer to that, if you need to. There is an example command under "access-group". To wholly open a port for any machine you should do: 

Well, any hope of automatic failover is doomed to be dashed. If something major changes in the AD (like trying to fail over a mailserver) you can have significant problems as the DCs try to update all the clients...It doesn't happen in a sane manner, and it's often better to restart services after you make any significant change, than to let them try and propagate it. Additionally, this sort of thing is windows-centric. It's nice when you have an outlook setup, and all you have to do is log on to the domain, kick up outlook, and tell it to use the exchange server, and you're done, but that sort of integration is really only windows playing with windows. 

It's on the disks with the server package. When you're doing the install, click to install the tools, and not to install the server itself. I don't know of any other way to get at some of the tools (like the business objects explorer), etc. 

That should tell machine B that, if traffic comes in for the 192.168.0.0/24, send it out eth1. That should work, and it's a bit easier than setting up a NAT. You'll need to make sure the switch knows to route traffic for 192.168.0.0/24 to 192.168.109.15 (Eth 0 on B). @Ian: Routing isn't necessarily easy, but it's not that hard either. That's what routers do. Assuming your switch is just a switch, with no routing capability, you can't use it to set the route, but you have to have a router somewhere or your network is purely a local LAN with no internet access. Check your machines, and find out where the default gateway is (on the linux boxes you can just type with no arguments. Your default gateway route is the one that looks like: 

I'm going to put my 2 cents worth in on a piece of negative advice: Don't go with a database. I've been working with image storing databases for years: large (1 meg->1 gig) files, often changed, multiple versions of the file, accessed reasonably often. The database issues you run into with large files being stored are extremely tedious to deal with, writing and transaction issues are knotty and you run into locking problems that can cause major train wrecks. I have more practice in writing dbcc scripts, and restoring tables from backups than any normal person should ever have. Most of the newer systems I've worked with have pushed the file storage to the file system, and relied on databases for nothing more than indexing. File systems are designed to take that sort of abuse, they're much easier to expand, and you seldom lose the whole file system if one entry gets corrupted. 

Well, in most people's conception of the word "server" you're not going to have much luck, but 1,000 is plenty for a quality desktop made from parts, even including RAID and name brand parts. I'd go out and put together a last generation (core 2 quad) quad core box with plenty of ram, a motherboard with on board RAID and 3 or 4 hard drives. Buy a cheap video card, and a cheap case, and as many hard drives and as much RAM as you can afford/fit. You should be able to come in well under 1,000. It's a poor man's porsche, no doubt, but I've seen a lot worse, and all the parts should have a decent manufacturers warantee, which is about as good as it's going to get for 1,000. @Warren: Better something than nothing: not everyone has the budget to wait for server-class hardware. For 5,000 dollars you can get 1 nice (commodity intel) server, or 4 whiteboxes with good components. Cluster the white boxes, and you've got a better set up with more redundancy than you would have had with the server. Besides, unless you've got the scratch to get real hardware (talking sun or ibm or top notch hp) with fast support, you're not going to gain much more than a redundant power supply by getting a dell.