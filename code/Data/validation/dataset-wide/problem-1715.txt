My default is 4GB right now for all server roles because memory is cheap. For a low usage server with hardy anything running using that software, you could probably get away with <1GB. If you're that concerned about the cost of RAM, it will be worth your time to test your RAM usage. Otherwise, just get 4GB and you're almost guaranteed to be fine. 

With one IP, you are going to have limited support regardless. SNI, which allows VirtualHost with SSL, is only supported in modern browsers. SNI Compatibility 

Both the PHP4 and PHP5 modules have to be loaded as well as have handlers specified. It sounds like the PHP5 module isn't loaded. If both modules are loaded, the MIME type isn't specified properly. 

Prefix the command with to keep it from terminating. To resume the program, run the program within a session. 

The MySQL manual is fantastic documentation. You would run the client if it is already installed in your system. Recommendations specific to your distribution package cannot be provided without your distribution being identified. If you want to use one of MySQL's releases directly, there will be a and other incredibly verbose documentation within the tarball. 

You are best off focusing on network protocols for your type of implementation. Typically Border Gateway Protocol (BGP) is among the ideal solution for managing dynamic network routing but this is unlikely an option with your DSL connections. Nevertheless, this would be the first choice for different types of Internet connections. Be aware of single points of failure. A single switch, cabling, all interfaces, and devices that are shared between your two connections. Consider all failure points and types of failure to eliminate risk where possible. HSRP would be among your potential solutions. HSRP is a Cisco protocol but there are alternative implementations that can be implemented on Open Source operating systems, such as CARP or VRRP. You would want to track both the internal and external interfaces to attempt to mitigate a single point failing without failover. 

As Chris identified, the default shell option for non-interactive shells is to not expand aliases. Here's a solution I've found to work. Write a script, enable the shell option, and source your aliases. Be particularly aware that is sourced at execution, which is why it has to be sourced again after enabling expand_aliases. My apologies for the initially incorrect recommendation. This was more obscure than I initially expected it to be. Script: 

You can minimize almost all downtime by preventing the table from being locked. Use a to migrate the data you want to a temporary table. Rename the tables. You'll need to consider any during the time the was running. Finally, drop the table that contains the data you don't want. Be aware that if you do not have enabled in your configuration that the disk space allocated will not be freed. Additionally, special considerations will be necessary for your application and specific situation. Here's a procedure that I wrote using this method.. 

If your chain is incorrect, the most likely scenario is that people who do not otherwise have the full root cert chain for your CA's issuing certificate will receive errors. EV certificates are fairly new. Even Thawte only introduced their full EV chain to more recent versions of browsers. For example, Firefox 2 and IE7 would throw a verification error unless the root certificates were updated. IE6 and Firefox2 will not display the green bar even if the certificate verifies without error, as the browser does not have support for displaying a green bar. There are various workarounds that including adding the full chain on your servers and client-side JavaScript "magic" that the CA will provide, which will update the client certs. 

From what I understand, the rule of thumb is to avoid it when reasonably possible. But with modern implementations and dynamic routing I believe this perspective is becoming more obsolete. 

If you truly do not want them accessible to the Internet via HTTP, you should move them outside of the Web tree entirely. You can also use filesystem permissions. For example, if Apache runs as the user, but you want the files only accessible via SSH by you can have secret* owned by and not publicly readable. Of course, there are nearly endless additional options here. Groups, filesystem ACLs, and more. For ACLs, you can use mod_authz. It's best to keep the configuration in the conf and not in . Having them in introduces more exposure and possible for manipulation with lesser privileges. mod_authz documentation 

To be extra safe, if you can't test the script on a system you have local access to, I would enable an alternate means of connecting. Another method would be to parse out the pids. For example.. Then, look at to determine what tty your shell is using. Look for the instance running . Once determined, run to figure out which sshd instance is assigned to your identified tty. Then remove that pid from /tmp/pids. After that, process the script to add kill. 

Breaking company policy to suit your arbitrary requirements is absolutely ill-advised. If you cannot get it approved by IT, speak to your boss and argue for your requirements. Otherwise, deal with it or get a new job. 

I typically use SAMBA's native functionality for permissions and groups management on shares. For example.. 

You just specify and within each directive. With that in mind, you would not be able to use and have separate log files for each host via normal performance without specifying separate . However, you could pipe the log through a script, and have the script make the separate files. Look at the piped logging documentation. You could also use a post processing script, such as utilizing to parse out the logs. A post processing script could be specified in the nightly logrotate under the or sections. Apache 2.2 Piped Logging 

It sounds like a CommonName mismatch with the SSL certificate. Does the CN in your SSL certificate match your server? You can run the following to view the SSL certificate: 

Experience in the context of a job posting is where you are providing professional services typically involving remuneration. Nevertheless, remuneration is not always involved and an area where you might consider to develop experience. A good way to look at it is whether or not the average person would consider it a job if you described it to them. During an interview, setting up a personal network may be relevant during discussion but is unlikely to have much weight in your fit for the job. Ways to get experience when you don't have any... 

You can use the setting to chroot a group to the directory. For example, this will chroot the group to : 

In short, no. At least, if you want to maintain data integrity. I gave lg a +1 for recommending maatkit, as those tools can be helpful to compare the data set once restored. Depending on how it is broken, you are probably going to need to read through the binlogs. You can use the mysqlbinlog utility for this. You will want to find the last successful query executed on the slave, verify with select queries, and compare to the master's binlogs to find the position. It's tedious but with practice it can go quickly. If you mean dual master by "two way replication," the situation can vary. A typical dual master configuration would have an active/passive. If replication died on the active server, you can point to the last position (or set a global skip counter) in the passive master log without risking data on the active master. If the passive master replication broke, you will need to spend time in the binlogs. 

Why is a reverse proxy necessary? Have you considered using a Destination NAT? (DNAT) Edit 1 If you run Linux, depending on your infrastructure, you should be able to configure NATs on your server. Your Linux server would have to be routing the traffic on the Internet routable IP to an internal network for this to be a functional solution. If this is not helpful, you will need to provide additional details regarding your infrastructure for a more specific recommendation. Linux NAT HOWTO Edit 2 Network Address Translation will do what you describe. Either by specifying different ports or configuring virtual hosts in your FTP daemon, which most modern daemons support. 

Create a database, create a user for the database, and populate a database. Additional details are going to be specific to the application. 

Be careful, if you specify the default policy of DROP for INPUT on a remote system without first allowing yourself SSH access, you could prevent yourself from accessing the system. If on a remote system, you could specify a temporary crontab to flush all rules every 5 minutes as a failsafe. To delete all rules and allow all traffic: 

Plenty of data provided by the companies selling their anti virus software. Best practices and regulatory practices in certain industries require its installation. For example, the PCI DSS requires it. If you have a workstation that gets compromised by a self propagating worm, it's likely that any Windows servers on the same subnet will be compromised as well. Unless the servers are storing restricted data, the only risk is to availability. If you are able to justify the potential risk, go for it. I believe the argument you provide is technically legitimate. You also risk the perception of those who can influence your success within your career, as most people believe it to be absolutely necessary for Windows. Of course, if you want to make this risk, you should enforce certain practices including but not limited to: 

You could add a static route to the host you are connecting to. For example... You connect to 10.10.6.5 Interface 1: 192.168.1.50/32 Interface 2: 192.168.2.50/32 Default gateway: 192.168.1.1 Static route for 192.168.2.0/24 to 192.168.2.1 If you want to connect to 10.10.6.5 via 192.168.2.1, you add this static route: 

BUGTRAQ. Subscribing to the vendor announce and security mailing lists for the software you use is advised too. 

There is no verification process for specifying the "From" header if you have relay access to a SMTP server. However, if you do not have relay access, you will be limited to sending e-Mail that the server receives mail for. If you run an e-Mail server, you can specify most e-Mail headers freely, which includes the From header. Put simply, the SMTP protocol does not prevent this from occurring. 

In the context of generic data center services, a cross connect typically refers to the patched connectivity between the data center's facilities and the line of demarcation into the building. It would often be the patched connection between your servers and the Internet access or any additional network connectivity. 

You can configure chroot using OpenSSH. Keep in mind that this is not a definitive security solution. Specify the following in sshd_config and restart OpenSSH. 

You can use variations of this theme if you prefer different timestamps or filenames. Be aware that this solution is not robust enough to handle multiple filenames. 

You're going to have to make compromises. You could enforce policy by having the users connect via VPN and then to the domain on the road. This is unlikely to be infallible, as you will either prevent access to the device or be unable to enforce policy. It cannot be assumed that an Internet connection will always be available or have enough bandwidth available, which will be requisite to connect to your network. This is fine for policy enforcement but if updates are downloaded from servers on your network it could delay their installation. For remote users on company equipment, we often join to the domain and have them connect to the VPN or plug in locally. These users are more often on site than not, however, which makes logistics less difficult. Based on your requirements, the best approach will likely be to setup group policies that enforce your update policies and then have them get updates from Microsoft or the antivirus vendor, as it will be more flexible. For remote access, you could use Remote Assistance. I'd strongly encourage full disk encryption as well. True Crypt is a fantastic Open Source solution. 

I like Nagios and Cacti, which I use for both Windows and Linux. There's a ton of tools out there. Some of which are better suited for special purposes too, such as NMIS for types of network monitoring. Check out the comparison of monitoring software, which is a rather comprehensive list. There's a variety of methods of monitoring that can be applied to the particular application or service. It can be as simple as opening a TCP socket to verify that Apache is up or as complicated as a script that was written to connect to the socket and verify a specific function. Really, the options are only limited to your imagination. Notification methods are numerous as well. e-Mail and SMS to page a phone are common solutions these days. Ultimately, this is a large topic and your question is quite ambiguous. If you clarify, we may be able to provide additional recommendations.