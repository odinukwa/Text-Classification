With luck you can work it out from these. It's hard to say more or give a definitive answer as it stands, because there are patterns where you haven't said what it should do. For example, trailing backslashes can be present sometimes, as this is valid and optional if the input happens to be a folder, and could cause the regex to be mis-evaluated for some folders. Ditto for UNC shares such as \\?\D:\share\myfile.txt or \\server\share\myfile.txt, and ditto for paths with just one slash (c:\myfile.text). Ditto for illegal characters (could the file contain anything that isn't a valid filename, such as semicolon, or will you test this separately if it could happen?). If you clarify more exactly what the regex should do, or if you need an explanation of how these work, I'll update this answer and add a regex or more details. 

I'm migrating data from my old server to zfs on FreeBSD 10.x (I'm actually on FreeNAS 9.10.2-u1 but doing this activity in console so it's pure FreeBSD). My problem is that needs a new_device in the correct format or slice/partition information, which I don't know how to provide. Because of costs, I'm migrating the data in two stages - copying the data from my old mirror to a new zfs pool (without redundancy), then breaking the mirrors on the old server to move the mirror drives over and resilver on the new server, at all stages having 2 copies of the data. SMART stats are all good, ands all disks are "enterprise" type. Although not ideal, so far it's gone well. I've copied over the data, and connected the disks from the old server to the new server - where I'm now stuck on getting the correct args for . Current storage is as follows: identifies the disk devices and model numbers, giving: 

How it works? If the old BROWSER functions on ports 137-139 are disabled, how does enumeration by a client of local shares work? Where do available servers and peers offering shares make themselves known? How do potential clients "know" which other local devices offer file or printer shares, and if a device starts to offer a newly created share, or starts to offer a share when it didn't previously, how do existing LAN devices discover this? What changes I have to make, so it'll work that way? On Windows 8.1/10, apart from the checkbox "Disable NetBIOS over TCP", what else do I have to do, to move my local SMB file shares off the old protocols, off ports 137-139, and onto port 445 or whatever the current practice is? 

I just came across this page ("Direct hosting of SMB over TCP/IP"). It's quite old, so I can't be sure if it's the standard these days. It refers to disabling NetBIOS over TCP and disabling WINS, which suggests that it's still not the standard or default way SMB works these days, at least in many smaller networks. Is this page still relevant, and if it is, how do I know whether it's a good idea? My network here is Win 8.1/10 talking SMB 3.x to FreeNAS 11 (Samba 4.5+) if it helps. Update: Based on comments below I've updated the question to reflect the point of confusion: If it's now standard to disable it, clearly none of the Win 8.1/10 PCs and Server 2016/Samba 4.5+ servers on my local network ever got the memo. (Nor did I.) It's all still running on ports 137-139 despite being 100% modern devices that should apparently be preferring a newer approach. I can imagine that when NetBIOS over TCP is disabled in a small local network without AD, the host announce/locate functions used to enumerate devices offering shares/printers migrates to DNS (or host file lookup), and the actual client-server traffic runs much as before only on a different port and more modern protocol. I can add specific hosts to my local DNS resolver and I'm running DHCPD locally, if that helps. I might run RADIUS in future but not at the moment. On reflection, I'm confused about server enumeration, and what I need to do, to move off the old approach (= changes needed to how I currently configure the clients/servers). 

I'm planning a small ESXi standalone server to host a number of VMs I use here. I haven't decided what facilities to use from the wider vSphere system. The underlying VM storage is local HDD RAID using enterprise HDDs and LSI megaraid, with the LSI card's onboard battery backed ram +ssd caching systems enabled. My concern relates to data corruption and bit rot in the VM store over time - I don't really know what my options are, and I'd like to be sure that stored VMDKs and snapshots, and other VM files, don't get corrupted over time and can be set to be periodically scrubbed and any bit-level corruption (within reason) detected and repaired. As background, for casual desktop use, I've tended to use RAID 1 (mirroring) rather than higher levels (reasons: fast read speeds, complete portability of drives without tie-in to specific brands or cards, no disruption if a drive fails). For my file server I use ZFS on a mirrored volume. But ESXi and VMware's suite use their own data store design for local storage. So I don't know how resilient against silent corruption, a setup would be "out of the box", especially when it holds many TBs of large files that might sometimes only be accessed years later, and with a local store rather than a dedicated separate storage system. I also gather VMFS uses a journaled filing system but not one with the self correcting capability of ZFS. Are the inbuilt capabilities of ESXi (and if necessary other parts of their suite) sufficient to protect against routine data corruption concerns? If not, what are my options for peace of mind? Update @mzhaase - I didn't feel confident about passing through to a second server that would act as a file store, because then every file access and snapshot has to be done remotely across a LAN or a second device and even if 10G was used (which is still cost prohibitive for most home setups) the slowdown would be a major concern. Part of the whole reason for getting this specific card is to get true cache-on-write for speed, so that bulk writes or rollbacks are less likely to slow everything down by "chugging" the main HDDs, which should be helpful whatever the file store location. Issues with latency impact sound like they would also happen with any remote data store, whethet a server appliance or a home build such as a second FreeNAS box (although if I had to choose, I'd use a second FreeNAS). I'm perhaps overlooking using a dedicated NIC port and multiple parallel 1Gb ports as a way round this, but latency and traffic implications for snapshots and rollbacks are a big concern. I'm also possibly overlooking running a FreeNAS VM on a small dedicated disk, which services the main VM store array off the raid card as a passthrough device, which keeps it local. (Meaning that ESXi can boot and can load the FreeNAS VM off one disk, once that's running it can act as a ZFS based file server for any other VMS with - hopefully - low latency). But running the file server virtualized might increase latency more than keeping it local would reduce it, and latency and disk bottlenecks are already an issue I'm trying to overcome. I will however look up the LSI card info, and - can you install file integrity checking/repairing software on the underlying ESXi platform itself to check and repair VM files? I didn't know that. And would iSCSI be that much of a latency-killer to make a remote store usable? Once a VM is up and running, how much does access speed/latency to the VM store affect the running of ESXi or other VMs currently running on it? 

I'm virtualising a small setup of half a dozen mixed PCs and small servers. The aim is to consolidate hardware resources (one high spec server + thin clients rather than 8 medium-to-high spec individual devices which aren't all used at the same time), to enhance mobility and session start-stop-suspend-move, to allow resource sharing (inactive machines can be suspended and the resources used for other things, rather than having dedicated PCs left idle, also the occasional heavy workload can be "averaged out" across VMs and doesn't needed every machine able to handle it), and to allow session snapshots on all devices. I've been trialling on a small scale using VMware Workstation for a year or so to see if the approach benefits the setup and it very clearly does, enough to move to a VM server more fully. Everything is straightforward, but my confusion is about the video handling aspects and how they interact (VDI, SVGA capable video cards, PCoIP/RDP, and pros+cons between the more generalist ESXi vs the more specialist View/Horizon for the desktops). This area is completely confusing me and holding me back. USAGE AND REST OF NETWORK - The active VMs at any given time would be a mix of about 3-5 Windows desktops and a mix of 3-4 small internal *nix servers (shell tinkering, tiny radius server, etc). The desktops are mostly used for desktop and "productivity" work on Windows 8.1/10 (heavy duty multitasking on Office suite, browsing, coding/development, video viewing, small amount of Photoshop now and then), but the desktop "windowing" use can be intense and multitasked. Most modern software can also use 2D hardware rendering if available to offload desktop GUI compositing and controls in applications. The servers are all light load *nix. There's a separate robust file server + offsite replication in place with enough capacity/hardware spec to support 1 or more VM servers, and 10G LAN for the file server/VM server link. My focus for this question is the graphics handling aspect. I'm reluctant to rely purely on soft (CPU) desktop and graphics handling due to the excessive CPU load it imposes even for moderate use, so I'd like to plan and spec for a bit beyond that. I want some flexibility in video resource sharing, and the usage varies, so if CPU alone isn't enough, I'm really looking to a VSGA or similar style solution, not dedicated-card-per-VM passthrough solutions. My question is borne of ignorance, openly admitted. I don't know what options make sense to consider in relation to graphics/desktop use. Virtualization is usually discussed in terms of a single purpose and on a larger scale, rather than a heterogeneous mix like this is. My points of confusion are things like these - 

Create two VLANs, say 1 and 2, with any VLAN able to access sharing services on the sharing ports, but only VLAN 2 able to reach the admin IP/port Create two IPs on the one NIC, say 192.168.1.2 and 192.168.1.3, with only 192.168.1.3 able to reach the management login. Blocking the management access ports (80,443 etc) for VLAN != 2 and/or IP != 192.168.1.3. 

In ESXi 5.5, the host config has a number of similar sounding settings related to swapfiles and caches of various types. I understand the principles of write-back/write-through disk cache and swapping VM swap files to the host instead of their remote VMDKs, and I'm reading the docs, but it's still pretty confusing which of these is which, and how they inter-relate within ESXi. For information, my system has a single standalone host with 96GB RAM and three datastores - a local boot store, an iSCSI main store, and a 250 GB NVMe SSD for swap/cache use. A clear explanation of the differences and how they work together would be really useful right now :) 

I manage a small network of windows clients and a BSD file server running Samba 4.6.x. We had some odd issues which led to the discovery that when users save files to the server, about 12% of the files as saved aren't faithful copies of the Windows original. (Tested by copying 2000 files of 1 - 5 MB and hashing Windows originals and BSD copies: about 245 differed). I tested a bunch of things: copying from multiple clients (same happened on all clients), server hardware checks (ECC fine, ZFS no errors), network data corruption (no issues end to end), directionality (copying client to server corrupted about 12%, server to client was faithful), consistency (copy same folder 3 times one after the other in a session and compare: in each copy, the corrupted files differed; one copy had no corruption), long path issues (no long filenames, paths, or odd chars in filenames). I also copied using SCP but got "server aborted connection" errors after a second or two, which might mean something or nothing, so I couldn't check if it was Samba-specific. SSH which I think SCP uses is rock solid so I'm not sure what to make of that. The NICs are good quality - Intel 1G + Chelsio 10G. Nobody else has logged in, the server is locked down and firewalled, and no system tweaking has gone on - it's pretty much FreeBSD 11 + Samba. I've always assumed (naively?) that file server issues were almost always down to access issues (config, permissions and authentication) and provided users can actually write files then, barring hardware faults, it "just works". So this random "file saves on server but saved version not same as original" has really got me foxed. Any suggestions what kind of issue could cause this, and how to troubleshoot it? 

I'm building a 3 server setup (FreeNAS, FreeNAS replication, ESXi) for a small network with a handful of users and a couple of "power users". The aim, briefly, is to migrate from multiple machines and windows file shares to more centralised and better resource usage, and higher quality and reliability. The hardware is adequately specced (ESXi on octocore Xeon + Supermicro, and FreeNAS on quad fast Xeon for CIFS with a ton of RAM and Intel SSDs). But I'm expecting a considerable learning curve because of the differences of what I'm moving them from and to. I was intending to run my file shares as NFS (for ESXi) and CIFS (mostly Windows + a couple of Mac clients). Then I came across VAAI and ODX and thought "wow", as the workloads look like they would really benefit from them. But as with any iSCSI, they'd replace NFS by one initiator/one target per share only unless one really loves corrupt data. So I'm wondering whether I have to give up on the idea of these nice shiney protocols, or whether I can get at least some benefits they offer if not all of them: 

What it all means, and working out the actual dedup table size: The output shows two sub-tables, one for blocks where a duplicate exists (DDT-sha256-zap-duplicate) and one for blocks where no duplicate exists (DDT-sha256-zap-unique)/. The third table below them gives an overall total across both of these, and there's a summary row below that. Looking only at the "total" rows and the summary gives us what we need: 

Connected 6TB SAS drive using different terminator on the quad cable, and the quad cable to both 8087 ports. No change - implies the issue isn't one specific terminator or port. Connected various Seagate 3TB-6TB SATA drives using same cable (same manufacturer and similar modern range to eliminate subtle compatibility issues if any). All recognised, reported, and spun up perfectly as normal on boot, on both 8087 ports and on all 4 terminators, and over multiple reboots - implies HBA and cable both work fine, at least for SATA. (Would be odd if they worked perfectly for SATA but not SAS.) Kept identical connections but replaced SATA drives by 6TB SAS drive, not changing anything else. As before, 6TB SAS drive wasn't recognised or reported by HBA, and wasn't spun up. Tried exactly the same with a different card and platform m- LSI 9260-8i RAID controller on an ASUS based desktop. Again all SATA drives immediately recognised and spun up, but 6TB SAS drive isn't/doesn't. Reluctantly concluded that however unlikely, the most likely issue was 6TB SAS drive DOA and RMA'ed it. ("Reluctantly" is because I've never actually had a DOA before, the drives are usually reliable, and if it is dead then by far more usual/expected would be that it's at least recognised but non-functional. I just couldn't figure a more likely issue than complete DOA.) Just received the warranty replacement - and getting exactly identical symptoms with the replacement as well: (a) When the 6TB SAS drive and any SATA drive are connected to 2 terminators and the system boots, the SATA drive is immediately recognised, reported and spins up, while the 6TB SAS drive stays cold and still. (b) When the 6TB SAS and any SATA drive are connected to the 9260-8i RAID card in the other ASUS desktop the SATA drive is likewise immediately recognised but the 6TB SAS drive stays cold. As a last step, re-re-read the 9211-8i HBA user guide in case I missed anything first time, and re-checked the BIOS. Can't find anything that would seem to explain this, or any statement that SAS drives will not be recognised unless/until <some action/content>.