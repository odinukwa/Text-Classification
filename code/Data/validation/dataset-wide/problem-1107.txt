Then populate it with the list of databases & checks you want skipped. For example, if you want check 52 skipped for all databases, add a row with CheckID 52, and databasename null. If you want check 52 only skipped for the WebSite database, add a row with DatabaseName = WebSite, CheckID = 52. Then when you run sp_Blitz, populate these parameters: 

Checkpoints already happen much more frequently than daily - the default in 2016 is every minute. So to answer the question: 

One missing link is the size of the index. If you're talking about an object with less than, say, 1000 pages, then index rebuilds aren't all that critical. Another missing link would be the churn of the index. Typically I see filtered indexes used when they're a very, very small subset of the entire table, and the subset changes fast. Guessing by the name of your filtering field (OfferStatus = 0), it sounds like you're indexing just the rows where you haven't made an offer yet, and then you're going to immediately turn around and make an offer. In situations like that, the data's changing so fast that index rebuilds usually don't make sense. 

That's the error you get when you restore the master database from a server with AlwaysOn Availability Groups configured, or try to restore additional user databases onto a server that had the master database from an AG replica restored. Don't restore the master database from an AG replica onto a different SQL Server as the master database. If you absolutely have to restore master to get user objects out of it, restore it as a different database (like master_old) and then transfer the objects you need. 

To tune it, start by looking at Perfmon counters on your own machine. My tutorial on it is at $URL$ and it includes a list of counters to gather, how to analyze 'em, and how to interpret your bottleneck. 

Here, SQL Server chooses to use the narrower index on just case_id because it's faster to scan that than scan the entire table (with all of its associated fields.) The same thing happens even with a query that doesn't touch case_id: 

Compatibility levels are set at the database, not at the table or column. You might be thinking of collations, which are set at the column level. You can't simply change the collation on those objects - you have to build new objects with the right collations and push the data into them. You might also be thinking of deprecated datatypes like text, ntext, image, or vardecimal. You don't have to change those yet, but you'll have to fix them before a future SQL Server version. 

Because they think there's a memory problem - SQL Server uses all of the memory available to it, up to its max memory setting (and even beyond.) Unknowing folks go into Task Manager, see SQL Server using a lot of memory, and think, "There must be a memory leak - I'll stop and restart SQL Server, and see what happens." Sure enough, that frees up a lot of memory (because SQL Server doesn't allocate it all right away by default), so they think they fixed the bug. Next thing you know, they're restarting SQL Server weekly. Because they think there's a CPU problem - queries will use a ton of CPU resources, especially in the case of parameter sniffing issues. Unknowing folks try to connect into the SQL Server without knowing about the Dedicated Admin Connection (DAC), be unable to connect, and just run out of options. They restart because the executives are standing behind them, wanting a solution fast. Because they've heard it fixes corruption - when folks run into a corruption issue, they're often willing to try anything to fix it. Because they want a rollback to finish - they kill a query, and it sticks in rollback for a while because they didn't know rolling back a query is single-threaded. After minutes (or hours) of waiting, they restart the SQL Server, thinking the rollback won't be necessary when it starts back up again. Sadly, they're wrong, and SQL Server just keeps right on going with the rollback upon startup. 

There are a few different questions in here. Do all drives safely flush data to disk when power is lost? No. Consumer-grade computers and drives often don't include their own battery backups or capacitors. If power is lost without warning, they can lose data that the OS thinks was safely flushed to disk, but is actually still living in cache. Does SQL Server work on those systems? Yes, but its ability to recover from a power failure is unpredictable. It's fine for things like development environments where you should be checking your work into source control anyway. Should SQL Server go in production on those systems? It depends on what you're using the data for, whether the data is recoverable (for example, TempDB is perfectly fine on drives like this, since it doesn't survive a restart anyway), whether the data center has its own redundant battery backups, etc. 

Quest offers a free version of their Benchmark Factory that works for Oracle, SQL Server, and MySQL: $URL$ I recorded a video on getting started with it back when I worked for Quest: $URL$ You can call your own stored procs, replay traces, run TPC benchmark scripts, and more. It's not exactly easy to use, though - thus the video. 

There's two things involved here: the version number of the file, and the compatibility level. When you attach a database to a newer major version (like from 2008 to 2008R2, or 2008R2 to 2012) the database version is changed permanently, and you can't attach that database to an older version again. The compatibility level is for parsing old-school deprecated T-SQL that used to work in older versions of SQL Server. It doesn't change how the data is stored on disk. To give the database to someone on an older version of SQL Server, you'll need to export the data and import it into another database. Tools like Red Gate's Data Compare are helpful for this. 

Just use a trace template. Click File, New Trace, and choose the Replay template. That'll get you everything that happens. 

For SQL Server targets: On the target server (the one that's receiving the linked server query), run sp_WhoIsActive and look at the linked server query. If the query doesn't show up, something's going awry with the linked server query and it's never reaching its destination. If it does show up, look at the Wait column, and see what the wait type is. (It won't be OLEDB.) That'll help you troubleshoot the reason it's not completing. Feel free to post that in with your question, and I can elaborate more on that specific wait type. For non-SQL Server targets like the IBM/Rocket one in this case: you'll want to work with the sysadmin/DBA on that platform, and have them check the progress of the query. When you see OLEDB on the SQL Server sending side, you just can't tell anything about what the holdup is on the other end. 

It might be that someone accidentally created objects in master, and they're being queried. These results will help you figure out the mystery. Enjoy! 

There are four tough problems to solve when trying to anonymize data. I'm going to summarize an old blog post of mine about it: 1: Anonymized data may grow in size. If you have peoples' names, for example, you don't really want all of the anonymous ones to have exactly the same length as the original data. If you change the data as it goes out, that means the solution needs to understand the original field name's length to avoid growing data out to something too large to fit. 2: Anonymized data has different statistics distribution. Say you've got a date-of-birth field - you could completely randomize the dates, but then it'll be much less meaningful on the other side, especially if you're trying to do performance testing. Plus... 3: Anonymized data breaks referential integrity. I know it's a bad idea, but sometimes applications join on fields that need to be anonymized. I hate that I've seen this, but I've seen folks using email address in multiple tables, and then expecting to join on those, or find related orders. 4: Anonymizing slows down the export. You'll need to run some kind of business logic to determine which columns need to be anonymized, and then generate the new data in its place. I used to say that if you could solve this problem, you'd be rich - but then you've got some competition. Delphix does data masking for DB2, Oracle, SQL Server, etc, but I don't think they've got Postgres or Heroku covered - yet. 

First, if you're getting corruption issues in TempDB, I'd start by making sure I've got DBCC CHECKDB reporting clean results across my user databases. I'd also restore those backups on another server and run DBCC CHECKDB there just to make sure the problems are only confined to TempDB. Then I'd remove any filter drivers on the servers - antivirus, disk defragmenting, storage mirroring, etc - and try to run multiple load-intensive queries against TempDB to see if I can reproduce the issue. Do simple select-intos, dumping Cartesian joins of your user databases into TempDB. Finally, if I can get an outage window, I'd try SQLIO - not SQLIOSIM - hammering the TempDB drives. SQLIOSIM is interesting if you want to reproduce a vague similarity of a benchmark, but SQLIO is better if you just want to repeatedly punch your storage in the junk. Plus, it's really good about bubbling failures up quickly. You also asked: 

You've kinda got a bunch of questions in here: It is possible to scale out READS in Azure SQL DB? Yes. Depending on the service you use, like Azure SQL DB's Active Secondary Replicas, your read workload can automatically be scaled out across multiple servers without big changes to your application code. However, you do still need to add a separate connection string in your application specifying ApplicationIntent=ReadOnly so that they know it's safe to move your read query over to a readable replica. Is it possible to scale out WRITES in Azure SQL DB? Yes, if you change code. The document you linked to, Data Partitioning, is a set of design guidelines that you can use when you code your application. However, those are indeed code changes, not a feature of Azure SQL DB that you just flip on the same way you flip on secondary replicas. It's up to you to design a layer of code so that when your app needs to run a query, it connects to the appropriate location. Think of it like the World Wide Web: at Stack Overflow, write workloads are separated out across StackOverflow.com, DBA.StackExchange.com, ServerFault.com, and other sites. However you, the user, have to know which site has the data you want to write to - you can't post an answer to this question over at ServerFault.com. That logic is built into your brain - and the same logic needs to be built into your app if you want to scale out writes across multiple servers. There was a brief moment in time where Microsoft thought they'd do this work for you with Azure SQL DB Federations, but that was quickly deprecated. Are the same things possible with SQL Server? Yes. Scaling out reads is as easy as: