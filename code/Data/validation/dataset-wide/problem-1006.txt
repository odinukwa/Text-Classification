You will most commonly see this if you do not create a user in the database for every person that can modify data. A better option is to change the trigger to capture so you obtain the login that made the change, not just the user. I have seen cases where the user was mapped to a different login name so your auditing records can be flawed if that is not managed properly. 

When you do a backup of the database, the backup is only going to grab the data itself along with some configuration information of the database. One of those configuration items is the initial size of the data and log file. I would expect if you checked your database properties you might find the initial size of your data file (MDF) is set to 9GB in size. 

Might consider using Invoke-Sqlcmd since you are working from a machine with SQL 2008 installed. You do not get the column separator option as does, but within PowerShell you could export it to XML or CSV. Basic syntax for your query would be: 

The components for SQL Server 2012 are divided by server and then management tools. You can get the server level components by using a PowerShell command like: 

You general, with large reporting requirements, will run SSRS databases on one server (shared SQL Server instance) and then the SSRS components on another. Keep in mind though how licensing works, you will have two licensed SQL Server servers with this type of setup. I normally see the report server databases reside on the same server as a data warehouse. I have not seen any issue with the SSRS databases being on a shared instance with the data warehouse. As it states in the article they are only storing information for SSRS (execution log, schedules, report definitions, etc.). The "meat" of what SSRS does resides with the SSRS component services. SSRS processes the data the report is requesting and then also works out rendering it, all of this is resource intensive if there is a high load. It will depend on what your specific needs are if it justifies a standalone server. I generally will see SSRS installed on the web server with the application that uses the reports. To determine if I need a standalone report server I would probably consider things like number of reports (many small or many large), the frequency (internal report server for monthly reporting or serving up user reports for external web application). You may also look at the report design or standard used by the developers, if the reports do a high amount of data processing on the report side, versus letting the database engine do it. 

If you want a method to run 200 statements, along with the checks to see if they already exist your options (which some have already been mentioned): 

No, it shouldn't because that is not a server level event that the cluster would be aware of when it happens. I seem to recall coming across an article that referenced specific events that could cause a failover but can't find it right now. 

In regard to the 3 servers and the same cluster, it will depend on what type of HA is required. In most cases you could have a 3 replica AG where SSRS is reporting on the secondary replica (read-only routing) instead of the primary replica. This is common with OLTP ssytems that have reporting requirements. For that specific requirement will be purely based on the vendor and your business owners requirements for HA. There is documentation on this type of architecture but not in a single document. Either way, if you are not even remotely familiar with these technologies that documentation is not going to help understanding the concepts. Which you need to understand in order to support it and ensure you are deploying each component properly. You need to tell your management they need to bring in another resource (hire DBA or reach out to 3rd party service that offers DBA support). 

I'm guessing you are referring to a file system level backup of your data folder? I am not familar with Rackspace for performing such a task, but have worked with other backup products that allow you to take snapshots of active or inuse files. Which it would allow you to grab a picture of the data and log files of a SQL Server database. The problem with that is when you take a backup of those files in this manner you want to make sure the database is in a consistent state so it can be recovered, or restored. When you take a snapshot of those files while a transaction is being written to the log file, or committed to the data file, it will not necessarily have a consistent state. When SQL Server attaches or starts a database it will read in the log file and either roll forward or back any active transactions that have not been committed to the database. If it hits an inconsistent transaction it will mark the database as suspect and you have to clean it up manually, and hope you can get it online. Another problem that will come up with not using SQL Server native backups can depend on your recovery model. The recovery model basically dictates how your log file is managed for reuse of space. If you are in full recovery model a native full backup helps keep the log under control, taking filesystem level backups will bypass that process so you could end up with a log file out of control. NOTE: When you determine your backup strategy for your database, you must be confident that you can use it to restore your data. No matter what suggestions you get, always try your backup plan and do a test run on your restore. If you can restore with that backup plan and it meets your recovery needs, then go with it. 

Access the backups files is controlled on the Windows end, not necessarily by SQL Server. SQL Server only requires that the account the SQL Server service is run as is given read/write access to the backup directory. If you want another account to have access to the backup directory you will have to specifically add it to the ACL of that directory, or a group that might already have the permissions. 

I am stuck, what am I missing here to do or check? EDIT I just completed a fresh install on a brand new server, same result. 

Use T-SQL This could go anywhere from just using Registered Servers in SSMS to get the results for all servers at one time; or from a single server that can talk to them all creating linked servers to each one. To automate a T-SQL option only, would require a bit more if you want to email it, but you could dump all the data into a single table and then it out to Excel possibly. I am not much for using BCP when I have PowerShell to get it straight into the format I want. 

That check box does not prevent job step history from being recorded/stored. SQL Server is going to do all this by default and that I know of you cannot turn it off. The check box referenced is for telling SQL Agent to store or job step output. It would store this information to . Which from my testing just checking that box alone does not do anything. You would still find the table empty if you queried it. Once I also checked , ran the job again, and then it showed a record for the job step. This would enabled on a job that runs command for a database: 

You will not be able to detatch system databases. If are not open to properly migrating the server level objects to another server, then I would opt for restoring system databases over trying to them. Ensure you are on the same, exact build number or master will not restore. You will also likely have to update the server name in master, if your new server does not have the same hostname. I would also insiste on a backup of every user databae before that LUN is anywhere. (Just in case.) 

The listener is a failover resource within the WSFC the AG is built upon. It will follow the primary replica as it fails over so there is some redundancy in that portion for it. As mentioned the DNS side is based on your AD infrastructure and DNS uptime. In your example of the secondary data center you will want to configure the WSFC as a multi-subnet configuration, allowing your listener to hold an IP in both subnets for data centers. If the AG fails over to the secondary DC then your listener will become active under that subnet. Your applicatoins will only see downtime for the IP to come online in that secondary DC, and then the replica switch over as well. 

If you are working with a session that is paused though I am not sure this would still return the information, believe it only has active connections.