However, depending on the design of the site, this may work badly as pages in the 'frodch' heirarchy would probably still have links, defined as local, to resources that are not in the frodch heirarchy (perhaps to /img/... or /contacts.php and so on) and these would not be accessible. A simpler approach can be to not define the new domain on the web host but to use the domain name host facility to, for example, do an HTTP redirect for the new domain name to the main domain's subdirectory. That way the user selecting the new domain arrives at the appropriate subdirectory page of the larger site but is still within the larger site so all the links and resources are available as usual. 

A Google search will turn up lots. I just tried $URL$ and $URL$ and both seemed to work well and allow the selection of different record types. Additionally, ISPs often provide these tools on their websites for customers who don't have native tools on their own computers. Finally, dig and nslookup locally, whilst not web-based out of the box, are readily available on most platforms and it is possible to write a script to make them web-based locally. 

Don't make assumptions about log files. Field formats need to be checked. For example: are dates dd/mm/yy or mm/dd/yy?; are numeric fields decimal, hex, octal or something else? Are timestamps consistent (others have mentioned the importance of syncing time between devices: check it has been sync'd or work out what the source of a timestamp would be and correct it)? Are all devices/processes logging at the same log level and to where you would expect them to? Is logging consistent between different revisions of the same software? (checking that log outputs are consistent with previous versions and with the documentation should be on the list for testing new software revisions but can be overlooked) 

Typically these would be done by noting the time on power-up and then looking through a tcpdump or wireshark trace subsequently. Again, a record of this, perhaps with indications of numbers of frames and total bytes, is helpful as software and systems changes and as a reference in the event of problems. 

As you know, thttpd, unlike dhttpd, supports CGI but the man page does not refer to the HTTP level supported for the requests in the documentation. From the thttpd man page: 

As well as other suggestions here I'll mention three that are obvious but perhaps worth mentioning for completeness: 

You can have two distinct sites by using the virtualhost facility, e.g. (just part of a much bigger configuration ...) 

Size is often not a reliable measure of how well they will provide, develop and support the software you need. No matter the size, companies may change direction, fail, change product ranges, be taken over, replace a product with one inimical to your needs. A very small company may be more vulnerable than a large one to loss of a key member of staff or irrecoverable failure from flood, fire, ... but large companies may lose, or be willing to sacrifice, a small unit for 'the greater good' ... and this may not equate to your best interests. Where possible an exit strategy, including an alternative that can be rolled out in time to keep your business afloat, can make life more comfortable. If you don't have that then the unexpected problem can rapidly become a catastrophe (just one recent example: $URL$ 

I usually measure and record all these as they will, over time, all vary with system problems, start-up processes, changes to applications, changes to the machine specs, change of models of PC purchased and so on. Have a record of just what these times were at original build can be quite helpful. On when the final test should finish (3 apps open) there can be value in not just having the app open but having performed a simple first use (e.g. a browser will load a home page and this may suffice; a word processor could load a test document (same for every test: already loaded at least once previously if on a network resource)). In a few cases a more detailed analysis is worthwhile. Being network-centric I would usually look at timings from power-on such as: 

This is in the printer's user guide, available as a PDF at the Brother Solutions Centre for the HL7050 under the Manuals tab 

A selection (not all of then all the time ) and many are as much for routine hardware removal/insertion and changing ... and don't use them unless you are competent to tackle what you're about to do and (where appropriate) trained, qualified, authorised and have the time and conditions to do the work safely: Small Phillips head and flat head screwdrivers 'pearl catcher' for retrieving the small screw when I drop it (I find the '4-wire' version easier than the '3-wire' for small objects) spanner and large screwdriver for equipment rack bolts anti-static wriststrap Ethernet cable tester to check wiring (e.g. normal, xover, faulty) multimeter (continuity tests, low tension voltage checks) thermocouple or (preferably) laser/infrared temperature measurement device room thermometer / humidity meter radio clock (for accurately setting and checking time) pen torch pen and paper back in the days of serial connections: break-out boxes and oscilloscope cutters for removing cable ties tweezers/forceps for setting DIP switches and handling header plugs spare screws, bolts, thumbscrews for computer cases, cable ties, DIP headers temporary cable labels tape measure (metric, imperial, U) bags to hold removed bolts, screws &c air spray small cleaning kit 

So basically this is what I have learned from this experience: Don't use the packaged version of gjc and then install other java on top of that (even when using alternatives!). I guess this could be the 'best practice' rule: 

Using sge with resource complex called 'gpu.q' that allows resource management of gpu devices (these are all nvidia devices). However on the systems there are multiple gpu devices (in exclusive mode) and if two jobs are allocated on the same node there is no way for the user to opaquely create a context on the correct gpu. Has anyone run into this problem ? I was thinking of somehow managing specific gpu resources and mapping the host and device id's. Something like 

do you only have one ftp server on your system? I had similar problems, and then low and behold I looked deeper into the issue and there were two ftp servers installed that were conflicting. I uninstalled one and the problem was fixed. If you did the 'yum install vsftpd' you shouldn't have to mess with pam files (Somethings usually wrong when you start messing with PAM). If that's not that run chkconfig --list | grep ftp and see what comes up ( see if vsftpd shows up there - if not something may be wrong with the install). And my final suggestion would be to run a yum update. 

I like the other answers, but you could also try loop mounting all the RHEL iso's (on one of the servers you already have configured), and then serving them up with http - I believe RHEL lets you choose http server. This worked for me - and I could just let it go for hours and not worry about a thing. 

Guy - There are two kinds of power management, one is c states (i.e. c1enhance for intel - something similar for amd), and the other is p states (cool 'n quiet for amd). Is this 'CPU power management' listed under advanced software features in your conifiguration -or are you seeing this somewhere else? If CPU power management is disabled that might mean you already have cool 'n quiet disabled in your bios. Power.CpuPolicy should be set to static if you want cpuz to read the clock frequency. However - dynamic 'dynamically' scales frequency up and down. It may make sense cpuz reads only 528Mhz - your server will run at the lowest frequency necessary to accomplish the current task. See Faq 1 at CPUZ . Maybe try running a load on your server and then see if the frequency scales up. If it is not scaling you will need to do 2 things to (possibly) fix them: 

I have an ASUS PIKE 2208 ( this is LSI SAS2208 ROC ) configured and working with four physical drives. Additionally I have installed the LSI SNMP agent. The agent provides all the data defined by the LSI-MegaRAID-SAS-MIB. However, I've noticed that after LSI MegaRAID SNMP agent is started the values (on Objects such as LSI-MegaRAID-SAS-MIB::temperatureROC.0) do not change over long or short periods of time. Put simply, values seem to remain the same once the snmp agent has started. However simply restarting the snmp agent seems to (sometimes) update the values. Interestingly, if a consistency check or patrol read are performed, values from the SNMP agent seem to update (the drive temperature fluctuates both up and down - not just up as one may expect from increased disk activity). I'm running Centos 6.5 with the following versions of the LSI MegaRAID software: Agent:LSI MegaRAID SNMP Agent Ver 3.18.0.2 (Oct 30th, 2012) firmwareVersion = 23.16.0-0021:3.270.95-2635:Jul 12 2013:14:20:36 driverVersion = megaraid_sas:06.505.02.00 

etc... And then upon resource request, reveal allocated gpu resources on each host through CUDA_VISIBLE_DEVICES variable. This seems like a fairly common issue - it must have been solved by someone by now with the prevalence of gpu's in compute clusters. 

you could try (from a terminal window - I'm pretty sure cpanel has that feature). It will output a couple packages that have font in the name. Then from that output use (you can try for each package): 

The MegaRAID MSM is MegaRAID_Storage_Manager-13.01.04-00.noarch And MegaCli is MegaCli-8.02.16-1.i386 All of these packages are installed through the LSI provided packages. Any dependencies have been installed through yum, so they should be up to date. I find it hard to believe that there are no temperature changes (not even \pm 1 degree) at all throughout a day (as the temperature of the environment is not nearly constant). Everything else works properly so I find this odd. I should note that gives the same temperatures as - so where ever these utilities are obtaining their data from is consistent. If anyone has seen this or solved this issue I would appreciate some insight as to why these values don't seem to update. 

I'm using tomcat's jscv to start the server ** as a service ** as the user tomcat (using jscv -user tomcat "lots of other parameters"). My server runs fine, however my question is: 

If you're asking about 'most companies' I would probably answer VPN intranet. And on top of that they probably have some kind of redundancy built on top (Raid, Some kind of tape robot). If you're wondering what you should do, set up a backup server, just get some NAS. If you're afraid of theft then put that server in some other closet. If you're afraid of fire then don't smoke near your office... You have to make some compromise when working with a small budget in a small company (if that is your case).