At first I'd suggest to have group_id and visit_id in INT or at least BIGINT. If that table has only 3 columns it might worth to create not-unique clustered index by group_id and unique constraint on visit_id: 

There are 2 options: 1. Create clustered index by account_id. Delete all other indexes. 2. Add new column and create clustered index on that column. Create another non-clustered index on ONLY one column . The second option is more preferable because it is much faster. You join only by 4 bytes. While your current queries join by 40 bytes. That means your current operation is 10 times more expensive. Also, ask yourself couple of questions: - Do you really need have Account_ID as Unicode? - Can you convert Account_ID to INT or BIGINT? Hopefully, you've got my point. ADDITION: 

SQL server by default uses this feature but it can escalate based on the number of rows to be updated and again it would be better to know how updates happen and how many rows at a time.Also,check how your indexes are created?is allow_rowlocks is true or false? 

It depends. If most of queries are using just one column in the where clause or on join clause. Then single column nc index should be fine.But when you have a combination of columns in where clause then using the combined indexes should be preferred. You can create 999 NC indexes. Also, you could use the include clause in indexes to make more indexes covering.But note that creating unncessary indexes might slow down you DML opertaions and will cause an increase in the size of the table and indexes. 

If you anticipate the total time going over 24 hours, there are more things to consider, but it doesn't sound like this project will. 

If you set up a report subscription with a One-time Schedule and gather its Schedule ID, you can execute it by using sp_start_job. SSRS manages subscriptions with SQL Server Agent, which makes this possible without needing to use Reporting Server Events. The executing user, I imagine a service account, will then only need to be a part of the SQLAgentOperatorRole. See: sp_start_job#permissions. You allude to this in an edit in you question, but I'd like to outline the process in case anyone else needs a specific solution. 

It depends what kind of updates you are doing.It could be optimized. 4.Will the index be the reason for slowness? 

How will you do the update?Will you update based on user or some other criteria?Having multuiple tables is bad idea better have the tablke partitioned based on user as you have some 25 users so you will have 25 partitions.You can optimized updates using other techniques but as you mentioned data is growing I would suggest that you use partitioning so that droping and recreating indexes is easier. 

Your provides 2 sets of the data: - DB Size + Unallocated space - These numbers include BOTH: Data and log file; - Total statistics of RESERVED space for all objects within the database; I bet your 36 GB of free space are in the Log file. For real numbers use following query: 

Might happen you still have some opened transactions, which hold you from shrinking individual files. See who hols them, close them. Reboot server if necessary. Shrink individual files while nobody accessing the database. switch it temporarily to Single User if necessary: 

You are not comparing two diff plans but you are comparing two diff servers and thus you can expect that time will surely vary.Even if you run the query on same server you wont get the same CPU time. Thus if you want to compare the time on two servers or two queries on same server try to execute the query say 100's of time and then use the avg CPU time. Page reads are not based on the hardware but depends on your data and thus this counter will be same provided you have same data and indexes. I would suggest that you compare all 3 before deciding which is best query.But in most of cases reducing IO's will reduce others excpet few scenarios. 1.Page reads(both logical and disk reads). This is most important as reading a page is work done by CPU so if you reduce the IO then ultimately you will reduce workd done by CPU which in turn will reduce the CPU time as well as wait time.Also, you will be improving overall system perf as well e.g. Your query is doing say 10 IO's then it will use just 10 pages in memory or buffer but if it will use 1000 IO's then it will use 1000 pages in memory and thus will take extra 990 pages space in memory which could cause other useful pages to be thrown out of the memory and thus might cause physical IO's to read those pages.I assumed above that 10 and 1000 IOs were diff pages and not same pages.This comparison should be based on diff pages rather than number of IO's. 2.CPU Time.This could be quite high if you have CPU intensive tasks like calculations or sorts operations. (for parallel query it could be much more high than serial one and in that case direct comparison doesnt make much sense). 3.Wait Time.This could be high when you have too much IO's to do and some of them are coming from say disk (again this could be quite high in case of parallel and it doesnt reflect true wait times thus while comparing try to compare other waits). 

Setting up the schedule is easy. The default time is 2am on the day you set up the subscription schedule and has likely already passed, so you can leave it. 

It will look the way you want. Note, although the unformatted default date will appear different in Access than it will in Excel, the time will remain relative to the default start point. 

From there, simply import your Excel into the Access table normally or enter the time in the same "hh:mm:ss" directly into the Access table. When you want to see the data summed up, you can do so in a query, making sure you convert it to a date and format it to display how you wish. In this case: 

I would suggest that instead of deleting the rows rather create a new table which will have the data you would like to keep in the table. Create appropriate constraints and index. Then drop the original table and then rename the new table. This would be best if you are deleting lots of data. Deleting batches is good when you delete only very few data as compared to the total data. Also,why do you need a select?Why dont you use a delete top (5000) from table where ? Also, I am sure it is more or less a maintiannce kind of taks why dont create a schedueled job written in TSQL rather than using the C# code. 

You can store the data in Excel as the amount of time from the default date. If you just enter the time in the format you wish, it will automatically add the default date. Enter "4:30:00" as in [FinishTime] and it will save the data like in [Unformated Date], but will be visible format you wish and retain the proper time value. 

When you create the table in Access you can just change the format to change how it displays itself. Note, in access Minutes are stored as "nn" instead of "mm", distinguishing it from Months. 

You have to be very careful. Assuming your query is not restarting and you have a for that SPID. If you restart SQL Server it won't help, because transaction still would have to be rolled back. The problem with that is following: When you run a transaction in multi-CPU environment with not restricted degree of parallelism it most probably will generate parallel execution plan. Would say your transaction run for 10 minutes on 8 CPUs. When you Killed it, will be processed ONLY by ONE CPU. That means it might take up to 8 times longer to recover. 

If you use Openquery to insert the results of the stored procedure into a temp table, you can query the columns for the temp table. Note, you cannot use a @variable in Openquery, so you will need to run it through dynamic sql and an exec. The problem with that is now the temp table doesn't exist outside the dynamic query. But, if you put your temp table into a global query you can still access it. Then just query the system tables for the column info as you normally would.