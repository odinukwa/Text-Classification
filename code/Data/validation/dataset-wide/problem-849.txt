Argument of perihelion or most planets/bodies changes very slowly over time due to higher order perturbations from other planets' motions (mostly Jupiter and Saturn for the solar system). General relativistic effects also cause the perihelion to advance over time, though this effect is smaller than the others for most purposes. So given enough time, the arguments of the planets' perihelia would likely end up being fairly randomly distributed at any given time. Also, the planets' orbits are often dynamic on such timescales due to the effects of higher order perturbations showing up, which can cause eccentricity changes, which also affect the positions of perihelia. So, my guess would be that it is okay to have the arguments of perihelia randomly distributed, since it is inconceivable (at least for me) that any sort of resonances would be visible at this level in order for the distributions to be non-random. 

If you can add a material which can reflect optical light (high optical albedo) and transmit IR light (i.e. it does not absorb or reflect too much in IR), it will reduce further heating of Venus and it will gradually cool down. You cannot lower the IR albedo (by using an IR absorbing material, for example) of Venus since it already has a thick atmosphere with greenhouse gases and if the atmosphere absorbs more IR, it will heat up. You cannot increase IR albedo because it will reflect emitted energy back to Venus and it will still heat up. You need to have a layer which transmits energy and does not reflect or absorb. The transmission of the atmosphere is already low and it cannot be increased, but if you can reduce the amount of incoming light by increasing the optical albedo, and not reduce the transmission at IR wavelengths, Venus will cool down gradually. 

Historically, two people (or groups of people) independently came up with different equations to model the blackbody equations in different parts of the spectrum. Rayleigh-Jeans law (classically derived) is valid for longer wavelengths and Wien's law (not Wien's displacement law) is valid for shorter wavelengths. The Planck Distribution approaches the two laws in its limits, as in, for shorter wavelengths it is approximately equivalent to Wien's law and for longer wavelengths, it is approximately equivalent to Rayleigh-Jeans law. However, the quantum mechanically formulated Planck's law is accurate at all wavelengths, so it is the one that should always be used. One can use the other laws, for example, for brightness temperature definitions in radio astronomy, they use the RJ law for convenience, but that's because the wavelengths are long enough, and the approximation of the Planck's law gives the same result. ($URL$ Wien's displacement law only relates the peak wavelength to the temperature, which is a different law completely, though Planck's law (differentiating the function to find the maximum) also gives the same result. Though in this case, the displacement law is not approximate. It is accurate and can be used whenever you want. 

This paper seems to be discussing what you've asked: $URL$ The next ice age is expected to begin about 50,000 years from now. However, I think it's still debatable how accurate the predictions of Milankovitch cycles are, and how much of an effect humans are likely to have on this earth system. 

I don't know the exact details (so I'll be leaving the math out anyway), but this is my understanding and I hope it helps you get some idea about what's being done. When you look at galaxies farther away, you're looking at them farther back in time (because speed of light is constant). So, to sample the velocities of galaxies over time, you don't need to observe them for a long time, but you can work with galaxies that are at different distances, and hence at different times in the history of the universe. Now if you notice that the recession velocities of galaxies that are farther away are slower than what you would expect from a constant expansion scenario (Hubble's law, for example), that means the universe was expanding slower, back then, than it is, now. If there is a clear trend that this is happening, then you can say that the speed of expansion of the universe has been increasing over time. This means that the universe is accelerating. Of course, this is a lot more complicated, since if the universe is accelerating, Hubble's law itself becomes shaky, if not invalid (as it assumes a direct proportionality between distance and recession velocity). Also, if you cannot find distance using Hubble's law, then you have to have an independent distance measurement that far away, which is also problematic, since afaik, there aren't such distance indicators that are valid that far away (since physics is different due to changes in stellar metallicities etc.; a lot of problems start showing up). So I don't know how this is done in practice, but in theory what I described above is how one would arrive at the conclusion of an expanding universe. I would like to know how this is really done in practice, as well. 

To answer your question directly, it is quite unlikely that planets would be habitable without already having life. It would have to be in a near-perfect condition, which would be statistically unlikely. In case of planets already having life, the existence of life provides robustness to small changes and makes habitability possible in a larger range of conditions. I am assuming 'habitable' here means habitable for man, trees and an ecosystem etc. Remember that there are extremophiles (or certain other hardy species) which completely change the perspective on habitability, and are often the key in making those places habitable for other forms of life. This is also true for parts of earth which undergo a sudden change and are not 'habitable' for most forms of life till some hardy species takes over and changes the conditions, and consequently allows other species to be able to survive. See, for example, ecological succession ($URL$ esp. primary succession and pioneer species (follow the links within the article). Also, this is a long process. In case of the earth, there was also evolution that needed to happen, so it probably won't take as long as a few billion years, but even on earth, it takes a few years to make small disturbed microhabitats habitable. The atmosphere and other factors (soil, moisture etc., for the plants) need to be made suitable for living. Check this interesting article on terraforming: $URL$ Hope this was helpful. I don't really know if a carbon cycle is possible outside of living organisms in some way. 

If you know the apoapsis and periapsis, you can find the eccentricity, using $r_a = a (1+e), r_p = a (1-e)$ ($URL$ Using the eccentricity, you can find the radius at any angle $\theta$ from the periapsis (this angle is also called the true anomaly). $r(\theta) = \frac{a(1-e^2)}{1+e.\cos \theta}$($URL$ With time, it is little more difficult. The mean anomaly (M) is defined as $M = \frac{2\pi t}{T}$ where t is the time since last periapsis, and T is the time period. This mean anomaly relates to true anomaly through a variable called eccentric anomaly (E). From mean anomaly (which you get from the time), you can get eccentric anomaly using $M=E-e.\sin E$ Now this equation isn't analytically solvable, but you can solve it graphically or numerically, or using a simple iteration which I would demonstrate here (technically included in the numerical methods, but it's easy enough to show here). It is easy enough to be done on a simple scientific calculator. $E_{n+1}=M+e.\sin E_{n}$ where $E_0=M$ Using this E, you can find the true anomaly $\theta$ using $\theta=2 \tan^{-1}[ \sqrt{\frac{1+e}{1-e}} \tan \frac {E}{2}]$ Then you can find the radius as described above. 

It is possible to determine the day of the year using the distance of the sun from the vernal equinox (you have to account for the equation of time etc, but it is possible). It is also possible to determine the year if your measurements are precise enough, using the current position of the vernal equinox (which changes due to precession). The ambiguity of the factor of 26000 years can be removed by noting the positions of other stars (for example, Barnard's star). Knowing these two, you should be able to find the day of the week. About your second question, Astronomers like to deal with Julian date, which is a continuous day counting system, and you can determine the day of the week from the Julian date, though only if given a reference, for example Jan 1, 2000, was a Saturday. As such, a week and days of a week are man made ($URL$ 

The diagram is in the rest frame of the planet. Now suppose a spacecraft is slowing down in the frame of the solar system. A planet is nearby, so it now starts accelerating due to its gravity and gains speed. Now, this speed increase is added to some component of the speed of the planet's motion when it comes out on the other side (this added component can be changed by changing the angle from which it approaches the planet, in order to maximize the slingshot effect). Once out of the planet's influence, the spacecraft has the same velocity as before, plus a component of the planet's motion, which allows it to travel farther out. This is the slingshot effect. Trying to look at this in another way, consider the angular momentum of the spacecraft. As long as it is only under sun's gravitational influence, its angular momentum cannot change. However, once it's under the influence of another planet, the two angular momenta - one w.r.t. the sun and one w.r.t. the planet (due to their relative motion) - add, and once out of the gravitational influence of the planet, their relative components can be adjusted (based on the angle of approach towards the planet and the angle at which it flies away after the slingshot) in order to increase the angular momentum w.r.t. the sun, which in turn puts it in a larger orbit, allowing it to travel farther away than before. 

It moves in circles. At the poles there is no direction defined (east, west, north or south). This is how the sun moves at the north pole: $URL$ . You can compare it to the other latitudes to get an idea. ($URL$ 

If you notice carefully, you'll see that their speeds away from the sun are fairly similar (7-9 km/s), and the difference is only in their speeds away from the earth. This is because of a small component of earth's motion around the sun (which is at about 30 km/s) adding to Voyager 1's motion and subtracting from Voyager 2's motion. 

Technically, collapsing of a star as a whole at any point of time happens dynamically, but due to the thermal timescale being much higher, the compression can be considered nearly adiabatic, which causes the star to become hotter and brighter and maintains hydrostatic equilibrium at a dynamical timescale. (There is little involvement of the core here since the photon diffusion timescales are high - ~ 100,000 years - and any effect of the core on the star will be seen much much later.) This is why you never see a 'star' collapse. You can, at best, see them pulsate. Typically the core is what collapses, which alters fusion rates and hence is NOT adiabatic. I will try to explain this process in a little detail; let me know if that answers you question. When the star is almost out of fuel, it cannot burn fuel as effectively as before, so, the core compresses a little, increasing the temperature, which increases the efficiency and rate of fusion, which makes the 'running out of fuel' part faster leading to a positive feedback process (compression $\rightarrow$ faster exhaustion $\rightarrow$ more compression). This gradually keeps 'collapsing' and burning fuel faster till a point where it cannot counter the collapse using an increased fusion rate due to lack of sufficient fuel (this might still take a few million years, maybe, but it keeps collapsing faster in a runaway fashion till that point). Beyond this point, the collapse happens at the dynamical timescale till it hits degeneracy (or ignition temperature of the previously inert core), which leads to puffing, novae and supernovae based on the mass of the core and its composition. 

That would be possible theoretically. However, in practice, the lensed images are highly non-linearly distorted and due to the limits in resolution, they are often limited to a thickness of a few pixels at best. Due to the non-linear transformations, this would cause the reconstructed original image to be even blurrier. Having said that, I don't see any real need of reconstructing the original image. You can get most parameters (like a rough estimate of the size and type of the background galaxy) from the distorted image anyway. 

Here's a NASA movie showing Libration: $URL$ As given in the details here, the position angle is the angle between the Moon's north and celestial north. This changes because of the inclination of ecliptic to equator + inclination of moon's orbit to ecliptic and plus, there might be some contribution from the inclination of moon's rotation axis to its orbital plane. Note that all these changes are apparent. The pole need NOT physically shift at all for these effects. Having said that, whether the pole does shift, requires one to correct for all those effects and the remaining effects would be cause by some precession or nutation of the lunar poles, which is likely to be negligible given that it is tidally locked to the earth. 

He knew the orbital period of Earth. You can do this by observing the sun's motion in the sky over a year. (And then you know that the orbital period of the earth is a year.) Now from earth, you can observe Mars from one Opposition ($URL$ to another. You can find out the time ($T_\text{relative}$ it takes for Mars to do this, using which, you find the relative angular velocity between the orbital motions of earth and Mars ($\omega_\text{relative}=2\pi/T_\text{relative}$). This angular velocity is the difference between the angular velocities of earth and mars. Using this and knowing the orbital period of earth, you can find the orbital period of mars. $2\pi/T_\text{relative}=2\pi/T_\text{earth}-2\pi/T_\text{Mars}$ Hope this is clear. 

FWHM is the indicator of the width of a Gaussian that is easiest to measure, and is least error prone in terms of actual physical measurement (due to the slope of a Gaussian being the highest near half maximum - not exactly sure about this, but it does seem like that's the case - so error in y contributes least in the measurement of width, relatively. If this is not clear, look at a noisy Gaussian - something like this: $URL$ - and you'll know what I mean. It seems most sensible to estimate width using FWHM). And it has a simple correlation to the sigma with a factor of 2.35 if the distribution is actually Gaussian (which is often the assumption for PSFs - correct me if I'm wrong). Measuring actual standard deviation of data with errors is much more complicated than just simply measuring FWHM, and often it is enough to know the FWHM estimate. Hope this is the answer you were looking for.