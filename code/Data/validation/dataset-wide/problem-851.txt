That is a very old document you are using. It's using the 1982 IAU precession model. There have been multiple updates since then. The latest set of changes are working toward eliminating the concept of Greenwich Sidereal Time. (In software terms, GMST and GAST are deprecated concepts. While still currently supported, support will be dropped in some future release of the Earth rotation model.) 

Read the fine print under the Horizons output when you select an OBSERVER table. It says (emphasis mine) 

The title of the question asks about interstellar space, but the body asks about the interstellar medium. These are two very different questions. The temperature of the interstellar medium varies widely, from a few kelvins to over ten million kelvins. By all accounts, the vast majority of the interstellar medium is at least "warm", where "warm" means several thousand kelvins. 

If Jupiter was at 1 AU from the Sun, its orbit about the Sun would be about 4 hours and 10 minutes shorter than a sidereal year. That's an effect, but it's not very much of one, due to Jupiter's mass being about 1/1000th of the Sun's mass. For a more significant effect, if our Moon was much smaller than it is, it would orbit the Earth about 4 hours and 1 minute longer than a sidereal month. The Moon's mass is about 0.0123 Earth masses. The period of the Keplerian orbit of an object of negligible mass about some massive object is $$P = 2\pi\sqrt{\frac{a^3}{GM}}$$ In the above, $a$ is the semimajor axis length, $G$ is the Newtonian gravitational constant, and $M$ is the mass of the central object. When the orbiting object has non-negligible mass $m$, the above expression needs to be modified to $$P = 2\pi\sqrt{\frac{a^3}{G(M+m)}}$$ 

This is a question, where I start with some assertions: Try to consider the universe as a four-coordinate system, x,y,z,t where t is time and where we view a change in t as a change in position, and as a velocity in the same way as any other change in either of the other components of the coordinate. Everything is then moving at the speed of c. The difference between a photon and another particle would be that a photon "spends" its velocity in any of the x, y or z dimensions - but not in the time dimension. This implies that all photons are on the same coordinate in the t-dimension - only the x,y,z values change while t remains 0. It also implies that time would appear to stop for something travelling in only the other dimensions, while it would pass fastest for something completely stationary. You can then also consider space-time as a function of acceleration. A physical objects' coordinate could be derived from its velocity, which would have to be derived from its acceleration. Acceleration would be a function of entropy, or "age" I believe. Is this the basis of general relativity? Is it at least an initial starting point for general relativity, or is it completely wrong? Would spin have to be part of the coordinate system? 

You are overestimating the size and the capabilities of a supermassive black hole. Contrary to pop sci portrayals of black holes, black holes are not giant vacuum cleaners in space that suck up anything and everything close by. While the supermassive black hole at the center of the Milky Way is indeed very massive (about four million times the mass of our Sun), it isn't very large physically. It's less than a couple dozen solar diameters across. It also isn't that hungry, gobbling up perhaps the equivalent of four or so Earth masses over the course of a year. On the other hand, the central bulge of a spiral galaxy contains several million stars in a fairly small volume. That central bulge is what you are seeing in those images. The supermassive black holes near the centers of those bulges gobbles only a tiny, tiny fraction of the light emitted by those millions of stars. 

Yes and no. Better said, the explosion is the very first part of a supernova. While the explosion lasts for but a few seconds to a few hundreds of seconds, a supernova can last for hundreds of days. What we see visibly as a supernova are the after effects of that explosion. The explosion can create lots and lots of stuff moving at very high velocities, and lots and lots of highly radioactive nuclei. If the star had outgassed material prior to the explosion, the highly kinetic material produced by the explosion runs into that previously outgassed material and makes it glow. This takes some time to cool down. The radioactive material produced during the short course of the explosion proper takes time to decay to stable elements. This radioactive decay eventually produces gamma rays, some of which is absorbed by the nearby material, heating it, thereby eventually producing thermal radiation. For example, a type Ia supernova produces a large amount of nickel-56. This decays to cobalt-56 with a half-life of about 6 days, which in decays to iron-56 with a half-life of about 77 days. The heating that results from these decays is what we see, and because it is so predictable, this is makes type Ia supernovae a fantastic standard candle. 

The Andromeda Galaxy is tens of thousands of light years in diameter. That suggests that the most distant stars in the Andromeda galaxy is up to tens of thousands of years "behind" in their orbit around the galaxy, compared to the closest stars. Have anybody generated pictures of the galaxy, which shows the approximate real position of stars? Is this effect visible when looking at the Andromeda galaxy through an amateur telescope? 

Something moving toward or away from us will be contracted in length according to special relativity. As the M31 galaxy is moving toward us at great speed it's "depth" should appear slightly flattened for us. A sphere moving toward us at the speed of light will appear "pancaked" in lack of a better word. This was predicted by Lorentz and has been proven in experiments. Does this also mean that electromagnetic radiation (light) from a flattened star is focused in the direction of motion relative to the observer? 

When a particle travels through spacetime, this is sometimes illustrated moving in flat grid with various depths. How accurate is this way of imagining gravity? Can the gravity well be infinitely deep? Gravity well $URL$ Looking at this illustration, it suggests that there is an amount of distance going down the well. Is movement down the well limited by the speed of light? Remember, I'm aware that it is an illustration, and the question is about the accuracy of the illustration. Is all gravitational wells infinitely deep at the exact center? So that if a particle could be made small enough to fit within the center, it would become a black hole. 

Note well: I'm not saying Pluto should once again be promoted to the status of "planethood". That very sharp boundary (at least in our solar system) says that it most definitely is not a "planet". But what about asteroids versus comets, which is what started this series of questions? That boundary is ridiculously fuzzy. Fuzzy logic (a real and serious concept) doesn't come close to addressing how "fuzzy" this is. Centaurs and formerly active comets are the boundary cases. There is no boundary. There instead is a soft melding from one to another. 

Correction: The Sun (or more precisely, the Earth's rotation) was used to determine and measuring time in our solar system. Over two thousand years ago, keeping time via the appearance of the Sun was known to be problematic. The Babylonians, Egyptians, and other ancient cultures knew this. This became problematic with the invention of accurate clocks about four hundred years ago. Kepler argued that a clock rather than the position of the Sun was the proper way to keep time. Clocks and sundials disagree by as much as 16.5 minutes due to the equation of time. Over a hundred years ago, the Earth's rotation rate was found to be less even less rock solid than thought. The second was redefined to be based on the Earth's orbit about the Sun as opposed to the Earth's average rotation rate. This, too, proved unreliable as we developed ever more accurate timekeeping devices. Nowadays we keep time with atomic clocks. To keep time in sync with the rotating Earth, people worldwide observe the difference between the smoothed Earth rotation rate and what atomic clocks tell us. We occasionally have to introduce a leap second to maintain the fiction of a solar-based concept of time. For example, the last minute of 2016 will be 61 seconds long. (Leap seconds are a plague upon humanity. Leap minutes or leap hours would have been a better solution. Ask google. Or ask me. I'm currently dealing with a leap second problem in real life. It is a massive PITA.) Even atomic clocks are a bit problematic. A minor problem is that some atomic clocks are more accurate than others. Even more problematic is that even the most accurate of atomic clocks won't keep time with one another due to the ever changing shape of the Earth. While our last ice age ended about twelve thousand years ago, the Earth has not quite recovered from the effects of the kilometer-plus thick sheets of ice that formerly covered parts of the Earth. The ongoing post glacial rebound affects the rate at which different atomic clocks tick. 

13 billion years ago, the universe was about 600 million years old according to many scientists. At that time, all matter in the universe would have to be closer, or in other words denser. Did time pass more slowly, at that time, relative to now? I ask, because increased gravity causes time to pass more slowly. If we'd been there, and measured the speed of light as we measure it today, would the speed of light be different? Finally, does distant stars look magnified, given that a "tiny" universe is stretched on our entire sky? I assume then that we'd measure time based on caesium 133 and distance not as a function of the speed of light, but as a theoretical physical "device" that would be the same physical length then and now. 

Imagine that hypothetically the black hole in the center of the milky way gradually increased in mass by for example 50% every year. That is exponential increase in mass. Which visual effects would we see on for example Alpha Centauri, stars more distant and on M31? Red or blueshift? Lensing? How about linear increase in mass? Are there software I can use to visualize it as seen from earth? 

A day would be twelve hours long at the equinoxes if the Earth had no atmosphere and if the Sun was a point rather than a sphere. Sunrise and sunset are defined as the time at which the upper limb of the Sun appears to rise above or set below an ideal oblate spheroid Earth and assuming average atmospheric conditions. While this calculation ignores terrain and variations in atmospheric condition, it does not ignore that the basic facts that the Earth does have an atmosphere and that Sun is a sphere. The standard atmosphere conditions means that sunrise (sunset) occur when the upper limb of the Sun first reaches (first falls below) 34 arc minutes below the horizon. That the Sun is not a point adds another 16 arc minutes to this, making sunrise/sunset occur when the center of the Sun is 50 arc minutes below the horizon. This means that at the equinoxes, a "day" is at least six minutes longer than the twelve hours we are naively taught (and incorrectly reported twice a year by naive weathermen). As an aside, days are always longer than nights at the equator. 

Temperature. More specifically, it's whether temperature rises or falls with increasing altitude. In the troposphere, temperature generally decreases with increasing altitude, at an average rate of 6.4 Â°C/km (the environmental lapse rate). This decrease stops at the tropopause, the boundary between the troposphere and the stratosphere. The ozone layer is in the stratosphere, making temperatures in the stratosphere increase with increasing altitude. While this boundary is a bit fuzzy, it is still very real. It takes an incredibly strong thunderstorm (think hurricanes, storms powerful enough to spawn tornados, and very tall and strong storms in the Inter-Tropical Convergence Zone) to penetrate that boundary. Temperatures in the stratosphere stop rising at the stratopause, the very fuzzy boundary between the stratosphere and the mesosphere. Like the troposphere, temperatures in the mesosphere fall (and fall very sharply) with increasing altitude. Thermodynamics makes the boundary between the stratosphere and mesosphere very different (and not nearly as clear) as the boundary between the troposphere and the mesosphere. The boundary between the mesosphere and the thermosphere is similar to the boundary between the troposphere and stratosphere. Temperatures rise with increasing altitude in the thermosphere. This change from falling temperatures in the mesosphere to rising temperatures in the thermosphere makes for a rather stable boundary. Something else happens near that boundary between the mesosphere and the thermosphere. Long-lived gases are fairly well-mixed in the troposphere, stratosphere, and mesosphere thanks to turbulence. Gases in the thermosphere and exosphere act more like a bunch of individual particles rather than a gas. Unlike the dense layers below, gases in the thermosphere and exosphere are differentiated. The makeup tends toward lighter and lighter particles (e.g., helium and hydrogen) with increasing altitude. Eventually, all one finds are hydrogen and helium. These are the gases that escape from the atmosphere.