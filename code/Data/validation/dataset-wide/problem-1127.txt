You cannot change the Instance Name after it has been installed. But you can uninstall that SQL Server instance and reinstall as a DEFAULT instance. Ideally you should make a backup of the server, then restore the backups to the new server. You can technically detach databases and attach again after the reinstall, but that includes the risk of losing your detached databases. See Aaron Bertrand's post: $URL$ When installing a default instance of SQL Server Express at the Instance Configuration page, you must choose the Named Instance radio button and type in the default instance name of MSSQLSERVER. All default instances use the MSSQLSERVER internally, but they just do not make it visible to you. 

If you have the default trace running, and you should have it running, it may be able to help you see what happened. The default trace is lightweight and among other things does track Server Stop and Server Start. See Feodor Georgiev's article: $URL$ The trace logs recycle fairly quickly on a busy server, so you would need to check as soon as possible after a Server Stop and Server Start. Toward the bottom of the heading Security Audit Events there is code to ‘Audit Server Starts and Stops’. (Tweaked only slightly to include the default count of trace logs.) 

SQL Server can support a single Distributor on the local server. If you are trying to reduce the load on the main server you can instead create the Distributor on a remote server. If the load is heavy, an additional step that would help manage the distribution is to create a distribution database for each database that is being published. This could reduce the contention within each database that might be present in a single distribution database. Both a local server and a remote server can support multiple distribution databases. David Poole's article from 2010, Scaling Out the Distribution Database. may provide you more insight. It can be found here: $URL$ Both articles are helpful and I have not detailed every step in the setup. 

Copying ONLY data does not copy the keys and indices, since these are metadata describing how to treat the data. The following article suggests that you create a SQL Server 2008 instance, backup the SQL Server 2000 database and restore to the SQL Server 2008 database. Then backup the 2008 database and restore it to SQL Server 2014. (Be sure to check Service Pack requirements). 

Microsoft has some documentation on Initializing and Reinitializing Transactional Replication. Initialize a Subscription with a Snapshot Initialize a Transactional Subscription Without a Snapshot Reinitialize Subscriptions Snapshot Options From your concern over lost data, you might want to investigate the Snapshot Options. Note this option: Specify an alternate snapshot folder location instead of or in addition to the default snapshot folder location. 

Schedule a delay on executing the query on the second server of n seconds to give the data time to usually make it across. (Call it computational overhead, if you wish.) If you keep a transactional time and date (as a column) on every row, then you can calculate which rows you will return by using date math to exclude the final n seconds of data. If use the point 2 philosophy on both the primary and secondary server, then the results on both servers will look synchronous most of the time. 

Assuming the data is constrained to avoid duplicates, how about something like this? This is assuming Microsoft SQL Server syntax. 

Back in 2007 Andy Leonard posted an answer on the Microsoft forums that looks much like your problem in the following link. In 2013, Chris Johnson noted that the answer still worked for him. $URL$ Andy's post noted the similar error: 

So, RSS enables Windows with better options for receiving data, but it does not seem to indicate that the SQL Server will behave in some better way. But, I cannot find any clear direction on this and most of the papers are about a decade old. 

As of SQL Server 2012, sp_resetstatus is still supported. However, Paul Randal has some excellent counsel, including do not detach that database. The first and best answer is to restore a good backup. If that does not work and you can retrieve the files from somewhere (where?), check out Paul's posts at: $URL$ $URL$ Since these are detailed discussions, I will not rehash them. But read carefully. 

If you that only returns one row (the count), is relatively light, and is the way to get that datum. And is not a physical no-no, in that it is legal and allowed. However, the problem with is that you can cause a lot more data movement. You operate on every column in the table. If your only includes a few columns, you might be able to get your answer from an index or indexes, which reduces the I/O and also the impact on the server cache. So, Yes it is recommended against as a general practice because it is wasteful of your resources. The only real benefit of is not typing all the column names. But from SSMS you can use drag and drop to get the column names in your query and delete those that you do not need. An analogy: If someone uses when they do not need every column, would they also use without a (or some other limiting clause) when they do not need every row? 

You should be able to use the table on the server to which you are restoring to get the most recently restored LSN. 

Yes, you can have several Report Servers installed on a single SQL Server. As you noted, the multi-instance support on a single machine is available, but not what you are after. For multiple distinct ReportServers on different servers, but using a single database, you simply have to configure this in the Reporting Services Configuration Manager. As noted by GShenanigan this is described in the Scale Out Deployment. $URL$ (same link) In that phase of installing a Report Server, you must choose the database server that you want to use. Then you can separately use the RS Configuration Manager to either choose the existing ReportServer datbase or choose a new database named to whatever you want to call it. When a new database is set up, the ReportServerTempDB will be named similarly. For example, creating ReportServerHQ would have a ReportServerHQTempDB also created. 

This preserves your security settings at the role level, so you should not need to repeat those grants. Just add or remove a user from a role, when needed. 

How big is this database? How many rows are in each table? Etc? I would say that normalized data is default state to try to obtain. It is a leaner database, rows are shorter, and indexes may be used more effectively. The short, leaner rows therefore lead to a smaller, leaner database. One of the major accelerators of performance is memory. If you can get your 5 tables to remain cached in memory, that will be a performance accelerator for your queries since you will avoid much of the disk I/O overhead. You identify that you are joining with IDs (which are usually integers), so your indexes may be narrow and offer relatively inexpensive joins. If you decide to denormalize, your tables will be bigger because they are carrying more redundant data on every row. This causes a need for more memory to keep the data in cache and will require even more I/O when the cache is insufficient to buffer the data. (And your backups are bigger.) In addition, you have taken on the task to denormalize and to maintain the denormalized data. This is an extra load of programming and on the server as well: consuming memory, I/O, and CPU. But sometimes denormalization is the best choice. Data Warehouses, for example, are largely denormalized data. Also, you may find that in your system the benefits of denormalization may exceed the cost. Still, you are asking a forum for an answer. Your best answer would come by building normalized test case and seeing how it works. Even though you may not have a lot of 'real data', you should generate a fairly large data set in the millions of rows to test with. You can try to find a tool that does it for you (RedGate has one for example, but it is not free) or generate the data yourself so that you control the complexity. There are online sources of states, cities, et cetera, and you can make up call centers, generate landline numbers and so forth. Then try it. If you do not like the performance, then create a denormalized table to test. And put some effort into writing the code to maintain the denormalization, since that will become integral to your process. 

As you delete old backups the SQL Server will still have the OLD NAS path on the backups that went there. So it will progressively delete those files without any further action from you. I generally recommend against updating system tables. When we have to move files, we just move them and cope with an extra script to handle the moved files until they are purged. If you have a week or two on NEW NAS the likelihood of needing the older backups is small. 

No. How would the query processor recognize that it should use the materialized view. The materialized view is another object. See $URL$ In part that explains: "When a materialized view is referenced in a query, the data is returned directly from the materialized view." So materialized views are useful, but you need to program not only for their maintenance, but also doing the necessary coding to use the materialized view. 

You could create a domain group that you could use to create as login and user on your SQL Server and YourDatabase. After creating the domain group, you could do something like: 

On SQL Server 2008 R2 the default value of remote login timeout (s) is five (5) seconds, I believe, though it is configurable. Since you are now trying to connect to an AWS RDS SQL Server, have you considered whether the remote login timeout (s) is appropriate for this environment? Try raising the timeout a bit. You might start with 10 seconds: 

It has been quite a while since I personally used replication, so I am not current on practical problems. A publisher with multiple subscribers is pretty common. However, Microsoft suggests using multiple Distribution databases when you have multiple Publishers. The following post has direction: $URL$ "However, if multiple Publishers use a single Distributor, consider creating a distribution database for each Publisher. Doing so ensures that the data flowing through each distribution database is distinct." It sounds like a recommendation to me. 

He has the best practices coded into his scripts, so it should serve you well. EDIT: For my servers I have found that once a week works fine for us. With regard to the databases on your server, you can run separate jobs, or you can have a single job go through all the databases. Look at the MaintenanceSolution to see how to include and exclude databases. 

Receive side scaling (RSS) improves the system performance when handling network data on multiprocessor systems. This should result in better scaling for receiving data from the network. One place to find details is at: $URL$ From this I would infer that the Receive side scaling primarily benefits the receipt of data from the network. There is a link that you might find interesting for an SAP installation and, although posted in 2012, seems to be using pre-Windows 2012 Operating Systems. Nonetheless, you may benefit from some of the details. One of the charts shows that only 1 processor is consuming the reads, while other processors are idling along doing not so much. See the charts at: $URL$ Therefore, it seems that RSS is primarily affecting how quickly Windows can receive the data and make it available to other processes, such as the SQL Server. This depends on the RSS Profile you choose. For example: 

@@ROWCOUNT is a system function that returns the count of the last query run. If you want to preserve that value for later use, you will need to store it somewhere else, even if it is only a variable. Such as: 

The optimizer will consider the whole plan when making optimizing decisions, so you should not be immediately alarmed that the compare of is outside of the inner . Obviously this code is not complete. If you want someone to test the code consider creating a SQL Fiddle script ($URL$ and provide some test data. 

According to: How to reduce paging of buffer pool memory in the 64-bit version of SQL Server ($URL$ makes it appear that this is asynchronous: 

The sys.server_principals owning_principal_id is NULL except for Server Roles. Fixed Server Roles are owned by 'sa' (the server principal_id = 1) and cannot have ownership changed. You can create a new Server Role and set an owning principal using: 

If there are no illegal values in C.ParentID, then I am not clear what the problem might be. Is this Microsoft SQL Server? I created a test case as follows: 

Your multipart index is covering 4 columns, but the only column that supports a seek is the first column. The trailing columns (y, x, z) can be used to find the rows, but this may not be optimal. If the column best for a search is then make that the first column. Indexes are all about statistics which directs how the index can be used. So, perhaps this would be more effective: 

Rick Byham has a WIKI post showing the fixed server and fixed database roles and how they map. You can look here: $URL$ The chart shows that db_datareader role is identical to GRANT SELECT ON [database]. So it is still fine to use, but the recommendation is to move away from those roles to the more granular commands. Some of the other fixed database roles are less clearly defined for most people. Using the explicit commands results in greater clarity when reporting rights. Obviously you know how to grant finer grained permissions. I am trying to break loose from the old roles whenever possible, but db_owner (for example) is a hard habit to break.