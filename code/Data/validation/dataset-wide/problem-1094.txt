SQL Server 2016, I'm attempting to work with some regular data and return a JSON object for processing by another system. The other system does not recognize the array wrapper, and so I am attempting to use WITHOUT_ARRAY_WRAPPER to get rid of this. When used in a subquery odd results get returned... 

For this you would (hopefully) be using AGs in an asynchronous fashion (latency will kill performance otherwise). This means that you will not get automatic failover to your DR site, that's just something to be aware of. I would also, typically, not recommend using less than 3 nodes, but if that is not an option for you... As regards quorum. You would want to use a file share witness + node majority configuration. Keep the file share witness in your primary site, on a server that is not a part of the WSFC (Windows Server Failover Cluster). In the event that you need to move to your disaster recovery site, adjust your file share witness to be local to the server there. This will allow you to maintain a voting quorum in your primary site at all times. Just be very careful when performing patching of your witness server not to end up losing quorum. 

Rather than add new objects to your servers, consider using the function of the open source DBATools.io project. This would make it a simple as 

As a workaround what you could do in this instance is change the default location for the log files to a new folder, and attach the data file with the rebuild log. Once that is done set the default location back to it's prior setting. Then you can run and ATLER DATABASE command to set the log file to the path/filename that you desire, set the database offline, copy over the log file to that new location, and set the database online again. After that just clean up the old file and folder. 

You are creating your own SQL injection framework doing this. There is no way that this would ever be secure (no matter what permission set or syntax checking you put around it). 

There is not a problem with having different numbers of tempdb files on each machine, you just need to be aware that should you failover to one of the machines with fewer files that you will run into the contention problem that you worked to alleviate on the new server. 

The chances are the the table you are attempting to reindex exists in a different schema than the default, and as such you will not find it. The following update should allow you to obtain that table. 

NTLM is a challenge/response type system, and while it has a resonable level of security it is potentially vunerable to attack and credential theft. In fact NTLM V1 is a known attack vector, considered insecure and should be disabled on your systems. NTLM V2 is more secure, and the current standard. Kerberos handles mutual authentication between parties, ensuring that the tokens are encrypted and secure. I'd recommend reading the Kerberos basic authentication concepts to get a handle on it. The reading is a little dry, but it will give you a greater understanding. Of the two, Kerberos is far more secure, and is the recommended protocol for Windows connections to SQL Server. To resolve your SPN problem look for duplicate tickets using SETSPN-L with both the server, and the service account, and look for duplicates. Remove those, and then adjust the SQL Server service account to allow it to self register its service principal name. This will allow it to handle things itself, which makes life a lot easier. You will not experience any actual problems with SQL (except for times when you might be using linked servers, or some other double hop type scenario where the ticket cannot be passed along to a second machine), and so it may not be a critical fix to you. Only your security team can really define that. 

The log transport happens to the secondary replicas when the commit happens. Any open transactions will not be moved over until the actual moment of the commit. So in your situation, the delayed 3 records will get applied as soon as they are committed, meaning you can see the first 5 on the secondary. 

Your question is a little confusing (in particular around the joins), does this look something like what you are looking for? 

Try defining the length of the in your convert statement as Per $URL$ it would default to 30. You can replicate that with the following script: 

Yes, absolutely. You would typically use transactional replication to move data between the publisher and subscriber(s) (although you can include other objects as well). Your subscriber (destination) database is fully writeable, and you can create stored procedures there, add indexes to replicated tables, and even create entirely new tables. Be aware, you can also delete data from the replicated tables on the subscriber, which could lead to replication breaking and you having to re-snapshot. 

Within SQL Server a primary key is unique within the confines of the object, and cannot contain null values (in fact the column must be created with a not null constraint in order for it to be used in a primary key). It is possible to create unique constraints, which will be unique, but will also allow for a null value. 

There's no real pitfalls from doing this. You just need to be aware that there will be a slight increase in CPU on the subscriber, so if you're already CPU bound it might not be the best path forward. You could consider using either pre or post snapshot script in order to help ensure that index compression is maintained in the event you need to run another snapshot (and to add any additional indexes that you require). When performing the compression be aware that you will probably experience transaction log growth, much like you would if you rebuilt any other index. Sorting in tempdb is generally a good practice for something like that. 

Probably the best way to do this is to use a function to handle splitting the string, along with a cross apply and group by to summarize the data for returning. 

This is pretty ugly, but should do what you need (could always turn it into a function). Basically we convert to a numeric type (gets rid of the scientific notation elements and limits the number after the decimal point), then we do a bunch of replacements for 0's and for the . so that you don't end up with the trailing zeroes or a funky looking number. 

You have a serious risk here. Right now, it appears (based upon your question) that you are backing up to a single file, which is why it is so large. You need to be backing up to different files if you don't do this you run the risk of corruption within one of those backups leading to your data being unrecoverable. I would highly recommend looking at different subplans in your manintenance plan each to handle writing the backups to different files, and have those files rolled daily. From a longer term perspective it would be worth expending time looking into how backups/restores work from a TSQL perspective, and then implement either your own, or some other solution to take backups, and get away from maintenance plans. 

That code won't actually work. (op updated script after this answer was posted) CTEs are effectively subqueries (that support recursion). They can only be referenced within the scope of that particular command. As such your first select * statement would work, the next two would error out not being able to find the CTE. In order for those to work you would need create another CTE for each to reference. And in that situation you would hit the people table 3 times, once for each query. To improve this you could put your results into either a temp table, or table variable and then just query that. 

You can use sp_rename to change the name of procedures, columns, tables, indexes, constraints. You'll have to do each object individually (although you can create a single script that renames the objects), and I would highly recommend running through a test environment first to ensure that you do not run into any problems. 

You can absolutely setup transactional replication on mirrored publishing databases. It is fully supported, and Microsoft has some good content on getting it up and running. The only caveats are the ones that already exist for transactional replication, and the recommendation to have the distributor database on another server outside of the mirrored pair. 

This is a difficult scenario to truly handle. I would probably start with running a job on your secondary replica that checks the state of all the databases, and should it become primary for one of them then performs the ALTER AVAILABILITY GROUP [AG] FAILOVER; command for all of the others. The biggest challenge with this is also running jobs on the primary, and taking care to not accidentally try to fail one of those databases back. You could manage this by checking the AlwaysOn extended event session and looking for failovers there, and do not do work if the database was failed away (as the most recent captured event), this way you don't end up bouncing the databases around all over the place. You can use sys.fn_hadr_is_primary_replica to check and see if any particular database is the primary on your system. 

There is no method for deleting a read only routing URL for a given replica. Your only options are to change the URL to something else (just putting a different URL in the or dropping the replica entirely from the AG and putting it back in again (less than ideal I know). If you adjust the replica to then you can at least prevent any potential attempts to hit that URL. 

Check to see the amount of space actually used in your database. You probably have a lot of free space available. A quick way to do this would be to connect to your database and use exec sp_spaceused which will give you the database size and unallocated (unused) space. When SQL Server backs up the database it only needs to backup the pages that contain data (and the log), which means that the backup can easily be smaller than the files themselves. This question has also been answered in Why is a .bak so much smaller than the database it's a backup of? 

Per Microsoft documentation changing the max server memory will clear the plan cache, as will changing: 

You are attempting to connect to databases on secondary replicas when the availabilty group replica is not configured to allow connections on anything other than the primary replica. The following script will tell you the state of the secondary replicas, and whether they will accept read connections: 

That is not true. I've done transactional replication with FCIs for years. Interestingly I cannot find any supporting documentation on the Microsoft site, however it is supported, and it does work. 

Your main problem here would be maintaining what backup was relevant to what, and you would have to continue taking full backups on your primary server in order to reset the differential base and prevent you diff backups from becoming as large as the fulls. Consider it this way: 

You will also need to configure an additional TCP endpoint inside of SQL Server itself. If you run a query of 

No, this is not required, however it is a best practice. In the event that your secondary replica has fewer cores and less memory then you would see a degradation in performance when failed over to the lower powered machine. This might still give you sufficient performance, only you can make that decision, but it is not a requirement. One thing I would recommend is to always ensure that you keep the drive letters and paths the same on all machines in an AG, otherwise you'll run into a lot of problems attempting to add data or log files. 

When using SQL Server Enterprise Edition (or running any edition newer than 2016 SP1) I would recommend going with SQLAudit functionality. This will give you some great granualar information about who is touching your tables, and the commands that are being executed. For something like this you would want to use a database audit specification along with a server audit (used to define where your audit will write to), and then scope it down to the table that you want to monitor. By then scoping changes to the public role you will capture any and all changes that happen. This script should get you there (test in your dev environment, and replace the relevant pieces for the table(s) that you want to monitor). 

You can create additional nonclustered indexes on tables with clustered columnstore indexes, the following script shows this... 

This is by no means a clean looking solution, but it seems to provide the results that you are looking for (I'm sure that others will have nice, clean, fully optimized queries for you). 

Failover partner is for when you are using database mirroring, you should not use it for Availability Groups. While it, strictly speaking, will work, it will only do so when the primary server is offline, and is not designed for this kind of scenario. Using a listener within an Availability Group is the way to go. This is a virtual network resource that will point you to the correct primary (and is required in the event that you want to use read-routing at any point in the future). Using a listener you should not have to perform an app restart to have client redirection, it should just work. 

Per Microsoft's documentation, there is no requirment to set a database to single_user mode when running . If you are finding yourself setting single user mode I would recommend taking a look at the locks that are blocking that change for you. An alternative would be to grow the data files themselves rather than the filegroup and considering usage of trace flag 1117.