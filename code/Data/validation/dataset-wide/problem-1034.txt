the answer is that it doesn't matter. The optimizer is automatically going to do an internal transformation from the SQL 92 syntax to the older syntax anyway. If you have performance problems, it is highly unlikely that they are related to your choice of join syntax (though when Oracle first added support for SQL 92 join syntax, there were bugs relating to its ability to transform the SQL into the older style). 

To the best of my knowledge, there is no limit. Of course, it is exceptionally rare that you would have more than one listener for a database. I've seem people run two listeners just in case one listener fails but I've never heard of anyone wanting to run more than two. If you really wanted to, though, nothing stops you from running a dozen listeners on a dozen different ports or on a dozen different machines. But since the listener is virtually never the bottleneck and the listener virtually never fails, that sort of redundancy is rarely helpful. 

No, you would have to use separate parameter files/spfiles for each database. As you've pointed out, you'll need some settings to differ (locations of files, database name, etc.). In many environments, you'd want to adjust memory and other performance settings differently because the development environments often don't have nearly as much RAM as the production environment. Even if you could share a parameter file, though, you wouldn't want to. Changing a parameter should involve some amount of development and testing just like changing code. So you would want to make that change in dev, validate it, and promote it up just like a code change. If you were sharing a parameter file, you'd have to make the change in all the environments simultaneously which somewhat defeats the purpose of having multiple environments. If you make a change in dev that doesn't work out, you need to back that out just like you would back out a code change. You could certainly take a backup of the production parameter file and copy that down with whatever changes are necessary to the lower environments. You could use whatever mechanism you use today to manage code deploys (I'm assuming some sort of source control system and build process) to version those parameter files and to deploy them so that you know what the differences are and why (i.e. dev uses a lower in general, dev has set to because you are testing that out for project X but that hasn't been promoted to production yet). 

In this loop, every row will be read from . And every row will be rejected (assuming is something other than 1) because of the predicate. So you'll incur the cost of reading every row from the table, evaluating the for every row, and you'll never return any data. Your first query appears to be working because it is filtering out rows in the subquery. Of course, since there is no , your first query always returns an arbitrary row. It would be entirely legal for Oracle to return the same row for every value of in your first query. It probably won't, of course, but it is entirely possible that you'd get different rows over time, that some rows would never be returned, that other rows would be returned multiple times, etc. If you want a deterministic result (prior to 12.1 which has some simpler syntax options), you'd need to do two levels of nesting 

Assuming that you are using PL/SQL to dequeue the message (there may be additional JMS limitations-- I don't know enough about the interplay between JMS and AQ), you should be able to use a priority queue to implement LIFO (though it would be a bit hokey). When you enqueue a message, you can specify a priority 

Although I haven't tried it, I would tend to suspect that you'll run into problems when your exceeds the length of time that Oracle maintains its SCN to timestamp mapping. If memory serves, that is roughly 1 week (well, if memory serves, it was 1 week in the 10g days and I'm not aware of anything that would have changed that in 11.2). I would tend to suspect that Oracle would use that mapping to determine which segments are old enough that they should be purged and that you would start to encounter problems if you exceeded that limit. On my 11.2 system, for example, I can convert 7 days ago to an SCN and then back to a timestamp but not 8 days ago 

You'd copy and paste the commands and then run them. If you want to do this in a single step, you can write a small PL/SQL block 

Assuming that you don't want multiple tnsnames.ora files, the simplest option would be to set the TNS_ADMIN environment variable to point at the one tnsnames.ora file you wanted to maintain on the machine. All the Oracle Homes on the machine would then use the tnsnames.ora file in the directory specified by the setting before looking for an entry in the local Oracle Home's tnsnames.ora file. 

You need to set to a positive number in order to enable database jobs to run. controls the number of jobs that are allowed to run simultaneously so if you set the parameter to 1, only 1 job could run at a time. If you're going to be creating multiple materialized views that you want to refresh at approximately the same time or if you are potentially going to be creating additional jobs, you may want to set this parameter to a value greater than 0. 

The data you're getting from Toad appears to be incorrect or, at least, misleading. If you are using a locally managed tablespace with automatic extent allocation, Oracle will determine your initial and next extent sizes automatically. In 11.2, the first 16 extents are going to be 64k in size (for a total of 1 MB). The next 63 extents are going to be 1 MB in size. So if you have 26 extents, that implies that the table occupies 11 MB of space on disk. An initial extent of 1.44 GB makes no sense and a size of 4.18 MB seems rather low if you're saying it hasn't shrunk. What does 

will specify that the refresh happens every day at 2 AM. The expression is evaluated at the conclusion of each refresh so you just need to ensure that the expression evaluates to whatever time you want at that particular instant in time. 

Each query would need to be parsed separately and each would generate (potentially) a different query plan. When generating the query plan, the optimizer is going to use each of the literal values along with the statistics that have been gathered to guess at how many rows the query will return and which query plan would be most efficient for that estimate. My guess is that some of the literal values cause the optimizer to estimate many more (or many fewer) rows than are really returned which causes the optimizer to produce a poor plan. Simplifying drastically, let's say that the statistics on the table tell you that it has 150,000 rows. Let's say it has a column which is indexed so there is a statistic on it (and it alone). And let's say that this statistic says that the data is uniformly distributed between 1/1/2015 and 3/31/2016 (15 months with 10,000 rows per month). If I run the query 

The export utility will use the environment variable specified for the client session. If all your data can be represented in the Windows-1252 character set, that shouldn't be an issue. If you want to do the export using the AL32UTF8 character set, you'd need to set the . In Windows, that would be something like 

That depends on your disk subsystem. A full backup is going to involve a metric crud-load of I/O (that is a technical term). That is going to put a huge load on your I/O subsystem. Depending on how I/O bound your system is, how much spare bandwidth your I/O subsystem has, how fast your I/O subsystem is, etc. the impact will range from "yeah, maybe it's a bit slower" to "everything times out, the system is unusable, users will be visiting the DBA team shortly with flaming pitchforks to express their displeasure." I am hard-pressed to imagine a whole lot of situations where I'd ever want a full backup running on a non-trivial production database during anything close to business hours. If you have a real need to do this, I'd tend to be investigating things like DataGuard that would potentially allow you to offload the load of doing the backup to the standby database. 

The order in which individual rows will be locked is undefined and certainly not something you should rely on since it will depend on the query plan. In general, I would expect that the rows would not be locked in order-- the optimizer would generally expect it to be more efficient to access all the data unordered and then apply the . If there was a composite index on , I could see the optimizer potentially choosing to do an ordered scan of the index and then locking each row in the table but that seems unlikely. 

should show you that there are two LOB segments associated with the table. You can then query to get the size of those segments 

The alternative to using database links would be to create different connections in your SSIS package for each database and to use those connections to pull data from the remote database to the SSIS server and then push that data into the target database. It is highly unlikely, however, that this would lead to performance improvements. Most likely, it would mean that your data is spending much more time going across additional network links. Materialized views would not be a replacement for database links. Materialized views would use database links. They would replace some or all of your SSIS package. Why do you believe that the database links are the source of your performance problems? Is your SSIS package causing Oracle to do set-based operations to pull data from the remote server to the local server? Or is SSIS doing row-based operations where individual rows are being extracted from the remote database and loaded into the local database. Row-based operations are inherently much slower than set-based operations particularly when data is being sent over the network. SSIS generally makes it very easy to build pretty-looking row-by-row ETL processes. That combination seems far more likely to explain your performance problems than database links. How does "taking a long time" translate to "failing"? If an SSIS job fails, that implies that an error is thrown. What is that error? Or are you saying that the job is technically successful but that it takes so long that it violates your SLA and, thus, the business considers the job to be a failure? If you are actually doing set-based operations in your SSIS package and your package isn't throwing an error, it just takes too long to run, then you would probably need to start looking at tuning the individual set-based operations that your code is performing by looking at things like query plans in the database. It is possible that you're missing indexes or to ensure that joins happen on the correct server or that your load needs to be refactored to ensure that data is brought from the remote server only once rather than many times. We'd need to understand more about exactly what statements are taking a long time, however, to be able to help you tune them.