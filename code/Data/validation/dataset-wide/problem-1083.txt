Based on the BOL article you referenced to, the date is not going to change unless a Service Pack or CU creates new system objects. However the date returned may not necessarily coincide with the date you install that CU or SP. 

One option to automate sync'n the logins between your AG replicas, if desired, but can be used as a one time thing as well. dbatools is a module that offers some code for migration of SQL Server instances, but I have used to also keep AG replicas in sync. The main function you need to simply sync logins is . This will include the SID, password and other properties as seen via SSMS (database mappings, etc.). It is similar functionality that the script Tara referenced. 

Aaron Bertrand wrote a good post on it that is pretty detailed...How I use PowerShell to collect Performance Counter data. Then Laerte Junior has an excelent walk through on how he finds the counters he wants in a Simple-Talk article: Gathering Perfmon Data with Powershell. This might be where you want to start. It has some cmdlets that he uses to capture the counters for a particular instance I believe. EDIT: See if this is what you need: 

You can use variables/parameters in your SSIS package to dynamically build your queries, probably the easiest method. It might be tedious to get setup based on how many queries you have used everywhere in your package. Just have something like an "User::Environment" variable that holds either: , or . You then join that into your variable holding your query and then as you change that value it would query the appropriate table. You could base it on the server you are connecting to as well, but would be the same premise of having to configure your queries dynamically in a variable + expression setup. I would probably use above method over validating against server I'm connecting to, just to have more precise control. 

I did that and then went into RSCM and selected the new certificate that was installed on teh server. That is not the end though, for some reason it kept removing the binding and showing "unknown" in the link. I stopped SSRS and then opened up the rsreportserver.config file and found the old ("bad") binding still was showing in the config file. I removed that went back into RSCM and started the service and it seemed to fix it. 

Now the above code can easily be wrapped into a function or for each loop that would allow you to easily pass in multiple server names. Edit Depending on what literature you read on PowerShell you will find that most folks that take time to write out a script, will go ahead and spend the time to make it a repeatable one. This is done either through a function (basic level for me) and then modules (more advanced that I have not touched yet). So for your example code: 

The in dbatools right now blocks the ability to restore the system databases. It is on the roadmap at some point to work out the process. The block in that command is why Copy-DbaDatabase would fail as well, and you can't move any system database by simple detach/attach method. Overall it could be worked out to perform the whole task in PowerShell using dbatools commands with a mix of just "manually prepared" code around SMO. 

Audit events are actually not available through Extended Events. You would need to use SQL Server Audit, which more or less works on top of the Extended Events engine (at least from what I understand). Steps to go with: 

Natively to SQL Server, no. You would have to restore the backup in order to access it. There are some third party tools that can do this, Idera has one I have heard is pretty good but does cost money. 

This DMV is available starting in SQL 2005. EDIT Unless you read the BOL link, please note that this DMV will only return values to databases that are online, or opened as BOL references it. If a failure occurs that requires you to take a tail log backup of a database you will not be able to verify this value through the above code unless the database is accessible; which in a failure it probably would not be. 

To get the management tools would require either registry search for the uninstall list or you can query the WMI class : 

If you are referring to PCI compliance for credit card industry, yes Windows Authentication is preferred. Search this phrase on Google: "SQL Server security best practices" The top two links, especially the first one, will link you to documentation that Windows Authentication is preferred. The first one is a link to the best practices whitepaper for SQL 2005 but applies to all versions of SQL Server above that. I'm involved with doing security scans on databases and instances that are required to meet DoD standards and Windows Authentication is preferred there as well. As well I have gone through PCI compliance audits and they will tend to look for Windows Authentication. 

In dbatools you can utilize to pull the tables from a given database. This will return the SMO object so you can get access to the collection on a table to dig into the column names and metadata. The ImportExcel module provides a more easier method of dumping data you pull from SQL Server into an Excel file. You do not have to use the Office COM Objects which can be just a pain to deal with; you can even use this module without Office installed. The author of the module has published multiple examples that you can go through and see how the module can be utilized here. 

If you are working with other job step types like PowerShell it will require a proxy account to be configured. 

Out of that the only thing I can find is that it requires an actual user account with password. So it is not possible to use virtual or managed account. 

To me does not sound like to much work in order to obtain the level of performance increase you will see having SQL Server do the work instead of Access. Especially if the data is going to expand to more users and is a more mission critical type thing. Something you might consider is running your Access database through the SQL Server Migration Assistant for Access tool. This tool will actually move your queries to SQL Server. You can find more information about the tool here: Access to SQL Server Migration: How To Use SSMA 

Go to , find the entry, select , and then select in the window that pops up. You select the instance you are working with: 

I would say you don't need all the events, just the . I don't see that you are capturing data just for the SCOM database either. Extended events can cause an overhead just near to what SQL Server traces will depending on what you are trying to capture. You are basically capturing every query that comes across the server and telling it to wait so you can first check the text of the query and then grab information through the extended event session, then letting it complete. As well the constant activity SCOM has, depending on environment configuration, it is likely to have an extreme performance hit either with trace or extended events. I would suggest looking at the plan cache for those types of queries initially. However you might try adding more filters to your session such only grabbing a sample of data from active sessions and then also only capturing data for the particular database. That would look something like this through the GUI: 

A database shrink is likely what you performed. Although ill-advised, if you need to regain disk space or just bring the data file (MDF) of your database under control you will need to perform a file shrink. Again there are plenty of articles on MSDN and blog post that provide the way of doing this. 

SSRS Planning a Deployment would be a good article to read through. You can read specifically on the SSRS databases here. As it states: 

The problem has to do with the certifcate being used with SSRS. If your certificate is created with , you can only access the Report URLs through that path, . If you were to try (where the IP is the one on the server) you will receive the 404 errors. So the fix for my situation: I found the friendly name used to generate the certifcate on the server was dorked up. I am in the process of getting that fixed to be the correct fully qualifed DNS name of the server in use. I found this out by installing SQL Server Report Server on a server that had the certificate created with the correct name of the server being used. Once I have that certificate corrected and verify will simply mark this as the correct answer. EDIT Well another problem came up in trying to change the certificate. The admins of the server removed the certificate before I could properly remove it from RSCM. So now it will not bind to the new certificate because it already sees one binded. There is really no clean, easy way of fixing this since you don't have access to IIS (SSRS uses HTTP.sys). So following this KB, you have to go into NETSH to remove the binding(s). Which you will note in that KB article the link to NETSH does not provide much info, so use this one. I had to use the two following commands to delete the bindings on Window Server 2008 R2: WARNING: I am on a single, standalone server. If you are in a server farm or have multiple instances of SSRS running, be very careful with which SSL binding you delete. 

You can utilize and the property to find the exact server name. It has the same effect as doing the search with but is less typing. 

Your only option is going to be calling a script. Either using PowerShell script via Execute Process task, or using C# in a Script Task to download it. For the PowerShell script you would use as it supports passing in credentials. A good example of this can be found here. The example from the article: 

Task failed as expected, but the event handler failed as well. event fired, except not as expected. First issue is truncation, received error . This is the content of the email received, which I don't see the expected new lines: 

You are granting to the role so the HR Administrators being a member of a higher level role should mean they do not directly inherit the . At least that is how I understand it. The particular section of the article referenced speaks on : 

PowerShell A simple one liner can provide the same information you will see in SSCM or the Services Management Console () if you are local to the server: 

You would need to specify if you are referring to (PaaS) or (IaaS). As already noted TDE with Azure SQL Database is only available with V12 which is only available in certain regions. If you an Azure VM is what you are working with you can implement TDE in SQL Server after the VM is built, even after SQL Server is installed. An Azure VM built from Azure's catalog with SQL Server pre-installed you are paying for the VM resources plus SQL Server licensing. With just a built Azure VM you have to provide the license for SQL Server (and the media).