For reference "Denali" is SQL Server 2012. With regards to "end user confusion", I am not all that concerned with whether a end user is confused or not with regards to SSMS. Microsoft did not develop this tool for the normal end user, but for the database administrator and/or a user that had to manage a database. Therefore there will be a learning curve with the tools provided and how they function. The file dialog box has been that way in SSMS since SSMS came out with SQL Server 2005. This is why you will general see most stick with T-SQL statements for backing up, restoring, or attaching a database that have been using it since then. To configure file system permissions with SQL Server you can follow the instructions from MSDN here. The way service accounts are handled did not come with or because of SQL Server, it was due to the change on the operating system level. Window Server 2008 R2 put a bit more of a security layer around service accounts. The advantage you have is that the service account can more easily access resources on a domain even if installed with the default settings. This link provides a pretty detailed look at how service account permissions are handled with SQL Server 2012. Excerpt from the link is below on the Virtual Accounts used by default in SQL Server 2012. There is also a link provided in the article that goes into more discussion on the Service Account concept with Windows, here. It is from Window Server 2008 R2 but I believe still holds true on Window Server 2012, and likely Window Server 2012 R2. 

Using your event session configuration, with the exception that I am filtering to only capture data for database on my local instance, I can view the target data for this session and find the query was captured: 

You can take a look at Jonathan's post here: An XEvent a Day (10 of 31) – Targets Week – etw_classic_sync_target The provider you see in is for using the Event Tracing for Windows. I have never used it but from what I understand it is more for tracking events from your application or the Windows OS back into SQL Server. I do not think it is something that can be used to just track general performance of SQL Server like you would with an XEvent session you create in SQL Server. 

My guess since your application is having problems when it runs is it is doing index maintenance. The following link provides the options in the optimization tab of the maintenance plan in SQL Server 2000. The option tells SQL Server to go through the database and rebuild the indexes. All of this activity will cause locks on the table until it is completed. It can also take some time to run depending on the size of the database and indexes. There is no way around it not causing your application some issues. It is just something to determine on the business end if your application can allow for the outage to perform the work that is needed, and yes it is needed. 

Jonathan Kehayias did a good blog post on NUMA at SQLSkills.com. I believe server wise most of the vendors out there note if they are NUMA capable servers or not. From reading Jonathan's post it seems the focus is on the motherboard and processor capabilities. Then I would imagine how they interconnect would be an important thing to note as well, since that would be the bottleneck in most cases. I have not really seen anyone write up on when NUMA becomes beneficial. I think it would just be a analysis on what your system is going to need. If one server maxed out to the nines is not going to cut it for the workload, then look at the alternatives. 

This is an informational message for SQL Server stating successful connection. It is one of three that it will usually return. You can find more info here. 

Create an Event Handler for for the package level (Excutable is set to the package name). Configured the SMTP connection and added in an Send Mail Task Within that task I configured an expression for the and . Those two expressions are noted below. 

Large deletions are fine but understand that the space that data used was not released back to the operating system, unless you did a shrink of the data file. 

You will not be able to capture the print statements in the manner you are trying. The handler is meant to output the statement to the console, not to actually capture it. This would cause the message to write the console, but you will never (at least my testing) be able to append this into your variable. Which I don't recall this working like you have it in PowerShell 2.0, but I don't have that version available anymore to try. 

Build T-SQL query with desired dataset I wanted for a single server. Create PowerShell script that has function to execute that query (use or ) Export that data to CSV (e.g. ) Import that CSV file into Excel 

To keep from being prompted you can use various ways to encrypt it, just depends on if the password has to be kept secure. If not just modify it to be something like this: 

I would start with this list of errors, most of the login errors I believe are around 18400+. The only other two I think I would add would be: 

SSIS 2014 is another beast though if you want to actually use the new features that come with it for management: SSISDB Catalog or Project Deployment (which requires the SSIDB Catalog). You would have to have a SQL Server 2014 database instance in order to configure all this. The only thing you would be able to do with this type of setup would be a file system deployment, anything else I think is going to require the database engine to be at the same major version. 

Then your CSV file should contain the column names in the first row. The other option would be to do this in your PowerShell command but it is much cleaner to do this in your SQL query. 

I would suggest reading up on SSIS at MSDN: Initial Installation (Integration Services) -this references having a standalone installat of just SSIS. Quote from above link: 

This image shows the source but I generally see folks create one specific to the Access database they created. You can also views this by opened the from Administrator tools. There is a 32 and a 64 bit version of this and you may need to check both. Also, with regards to Access storing the connection itself. It is possible to setup a DSN-less connection using macros. You can see this KB on how that is done to know where to check in your Access database. 

I would look at using extended events. Truthfully I have not worekd with these and it's one of those things I have not sat down and tried to learn...yet. So I cannot give you an exact example of how you would impliment with your example. The advantage to these over trace files is no performance hit. They were specifically designed with performance in mind. So letting it collect information over a long period should not be a problem. You can check out using it for monitoring system activity here. Jonathan Keyhayias did a good month-long series on using extended events. The first day gives a good overview of them here. He also created a SSMS add-in that makes it a little easier to work with the Extended Events sessions. 

As quoted from this MSDN article titled "Buffer Management": Torn page detection was introduced in SQL Server 2000. Checksum was introduced in SQL Server 2005. A synopsis of other things noted in this article is that the page verify mechanism is specified at database creation time. So it depends on who and how they created the database as to what it is set to, could also be controlled by what model database is configured to. Also interesting to note is that if you change the setting it does not take affect over the whole database, only when the page is written to next. As well according to Paul Randal it is only done when the page is read into memory, changed, and then written back to disk; that info is here. 

You would stop generating them by disabling FileStream feature. I have not heavily used FileStream that much so I had to go look up how it all worked. I came across a nice blog post on the CSS SQL Server Engineers blog. This may shed some light on why those files are generated: 

SQL Server 2008 (r1) introduced Extended Events and with that came SQL Server Audit and works similar to XEvents to audit events. So it is less intrusive than C2 Auditing and server side traces. It is very granular and fairly easy to setup. This feature is available in all editions of SQL Server 2008 R2. A good write-up intro/how-to can be found here by Brad McGehee. Thomas LaRock also did a great article on Simple-Talk walking through creating/setting up the audit. Reading through the articles mentioned above you will note that you can have the events logged to different places (windows app log, file, etc.). From that you can then write PowerShell scripts or T-SQL scripts to alert you of whatever you want. Much more easy than screwing with C2 Audit trace files.