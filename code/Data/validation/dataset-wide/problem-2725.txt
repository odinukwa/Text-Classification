Analytic philosophy typically deals with narrowly defined topics with great precision, formal (or pseudo-formal) arguments, and aspirations to clarity. Continental philosophy is likely to take on more ambitious and significant topics, but often treats them solely in a poetic and allusive manner. Analytic philosophy can be criticized for diminished ambitions and scope, resulting in pedanticism, and an obsession with tedious minutiae. Continental philosophy can be criticized for obscurity, and the tendency to sound good without saying anything that can be reliably understood, translated into well-formed arguments, or used to establish clear results. Both can be criticized for engaging almost exclusively with the academic world, to the exclusion of topics of wide general interest and understandability. (These criticisms, of course, do not apply universally --there are those on both sides who have transcended their peers' limitations.) Some of the great achievements of each in recent memory are the creation of the science of modern logic for analytic philosophy, and the social and cultural impact of existentialism and deconstructionism for continental philosophy. To express it another way, continental philosophy has produced great art, and analytic philosophy has produced a great science. 

For me the missing information is whether there is an objective or obvious reason to prefer one interpretation over the other, absent knowledge of intent. If B's interpretation has the stronger claim, then A needs to have considered his words more carefully, and an apology is in order. If A's interpretation is more likely, then B is in the wrong for having interpreted the statement uncharitably. If it isn't clear which interpretation is more likely, then both A and B are in the wrong, and should mutually apologize, for the same reasons mentioned above. This isn't actually a question of logic, but of communication and social mores. As to whether intent or reception should be the primary way of evaluating a statement, there is no definitive answer, but in the interest of supporting communication, Person A should focus on clarity of reception, Person B should focus on discerning intent. If A is only interested in intent, and B only interested in reception, then the chances of successful communication are low. 

Arguably, the historical importance of the argument from design has been distorted by its centrality in the writings of biologist and anti-theologian Richard Dawkins, who has been vocal about his belief that it is unanswerable outside of the Theory of Evolution. It's far from clear that either the religious or the scientific establishment of the time gave it the importance that Dawkins sees in it. With that said, the choice to see the hand of God in any particular phenomenon is essentially a matter of faith. If you were an atheist of the time, and wanted to reject the argument from design, you could have simply stated that you did not know where the complexity in the world came from, but that you were committed, in principle, to the idea it must have naturalistic causes. 

At face value, assuming moral realism, there's no reason any text whatsoever couldn't contain valid moral knowledge. Just start with valid moral knowledge, and add whatever you want to it. There are many atheists who do take the Bible in this manner --as some valid moral principles surrounded by a lot of (what they consider) nonsense. However, it's clear that your question is oriented towards principles that originate in the Bible (or in the religious communities that produced the Bible), not ones that it merely collects from external sources. There is a wealth of what we might call "secularizable" moral values in the Bible. For instance, Jesus preaches at length about our responsibilities towards the poor and oppressed. Many secular humanists, such as Kurt Vonnegut, have espoused moral systems that are (fairly openly) secularized versions of Christian morality. With all that said, many of the moral precepts in the Bible are directly about the relationship with God. For those to be valid, God must be real. Conversely, a number of other Biblical moral precepts --including some of the ones the humanists most admire --are derived from the relationship with God. If those precepts are valid, it at least suggests the realness of the source. One of Jesus' own principles is to judge the tree by the fruit. If the fruit is a moral system, and we judge it good, then shouldn't we give the tree (Christianity) the benefit of the doubt? Your final question is whether fiction can convey moral knowledge: Arguably this is true of much of good fiction. It dramatizes either good or bad moral choices, and we, the audience, learn from the experiences of the characters. The parables of Jesus can be viewed in this light, but they are almost exclusively oriented around the relationship with God. If we choose to view the stories of the Old Testament prophets in the same light, we see the same thing. (As far as stories from what are usually classified as the "historical" books of the Bible, it is often more than difficult to derive any moral lessons from them at all.) 

As @Virmaior suggested, the existentialists, particularly Kierkegaard and Sartre, are the philosophers who probably most directly wrestled with the subject --not procrastination, per se, but decision-making, and overcoming fear of commitment. You might read Kierkegaard's Either/Or, Fear and Trembling, and Repetition, and also look into his concept of "the leap of faith." Walker Percy's well-known novel The Moviegoer provides a dramatization of some of these same ideas. For Sartre, you might read his classic play No Exit which depicts existentialist hell as a place where no one is capable of making any decisions. 

The first question is asking you to fill in the blank spaces. Since it is generally telling you what rules to follow and what line to refer to, this should not be overly difficult. It's a way of easing you into proofs. For example, in line two, you are introducing an assumption (->I), which should be something on the left of a single arrow, or on either side of a double arrow. But that still leaves a couple of options. If we look down to see where the assumption is discharged, however, we can see that it must be line 7 (line 7 only has assumption 1 left. It could be discharged at line 6, but we can see that is doing different work.) Since an arrow assumption is discharged in a statement with the assumption on the left, we know the original assumption was Q & R. It's like filling in a puzzle. One you get one step, the next becomes easier. (The second question should be posted separately.) 

Modern logic as we now know it is a formal axiomatic system that establishes a relationship between one set of statements known as premises and another set of statements known as conclusions. Formal means that it is an artificial, constructed, consistent, rule-governed system, and axiomatic means that we accept its basic rules as givens as a prerequisite for using the system. It is believed that logic is a truth-preserving set of transformations, such that if you are able create a set of premises that accurately and "truthfully" formalize a real-world state of affairs, all valid conclusions reached by logic will preserve the truthfulness of the premises. However, this cannot be proved from within logic. In the most common systems of logic, you cannot even frame a statement that speaks about the system of logic, because allowing self-referential statements was found to lead to paradoxes fatal to the project of logic. If you are speaking about a philosophical commitment to the concept that all beliefs must be certified by logic, that commitment itself cannot be certified by logic. In general form, this is a well-known result often considered to have doomed the philosophy of "verificationism" (a strict form of logical positivism). The commitment to any given framework of justification cannot be self-justified. 

One of the most respected modern apologists is C. S. Lewis. Two of his most famous defenses of Christian faith are Mere Christianity and Miracles. A more recent attempt to marshal rational argument in defense of faith is noted scientist and devout Christian Francis Collins' The Language of God. Classically speaking, the foremost architect of rational explorations of Christian belief is St. Thomas Aquinas, whose Summa Theologiae comprises 3500 pages of carefully constructed arguments. Not all Christian thinkers agree that rational arguments for God are desirable. Kierkegaard's approach, for example, might be better described as irrationalist. 

I'm not an Aristotle expert, but according to Wikipedia, he did discuss rhyme in his book Rhetoric. The problem with a general law like the one you mention, is that, unlike Aristotle, who referenced everything to the culture of Athens, we live in a global and aesthetically diverse world, which includes any number of non-rhyming poetic traditions in a variety of different languages. If you wanted a properly Aristotelian account, you'd need to elaborate on the function of poetry, and draw a link between rhyme and that function (which it seems you've made a stab at with the idea of memorability). You might also consider making it more general and generalizable --perhaps dealing with the overall idea of structure rather than rhyme in particular. 

You could call this being "right for the wrong reasons." It's not precisely a fallacy, but it does mirror the fact that an argument cannot be considered sound (or cogent) unless it is first valid (or strong). The fact of a right conclusion has no bearing on whether the argument itself is good or bad. 

The next category is "fiscal conservatives," who advocate small, decentralized government, reduced taxes, and an end to spending on programs of social welfare. In addition, it's an open secret that a large, but unacknowledged third portion of the modern conservative coalition is composed of what might be called "racial conservatives," those who wish for a return to an older version of the US with a stratified racial hierarchy. This group left their original home in the Democratic party when LBJ passed civil rights legislation, and has reliably voted Republican ever since. Rounding out the grouping are libertarians, who are fiscally conservative but socially liberal, nationalists, who overlap strongly with the racial conservatives, and anti-federalists, who favor the reduction of federal governmental powers in favor of the restoration of more local independence and control. That coalition has fractured in the current (2016) election. The fact that Trump's core base of support comes from the racial conservatives and nationalists (and that his claims to fiscal and social conservativism are tenuous at best) has resulted many fiscal and social conservatives --who are themselves not the most natural of allies, even in the best of times --being forced out of the movement. It remains to be seen whether that coalition can be put back together after the election concludes. 

It's arguable whether there can be time without change --if nothing ever changes at all, it's difficult to say in what sense time is passing, except perhaps by reference to something external. For the other case, one way to conceptualize it is to think of the universe as a book. Like the Bible, this universe-book begins with the story of the beginning of the universe, and ends with it ending. Within the pages of the book, time passes chronologically from the start of the story until the end. But it isn't possible to know, from within the book, what time means outside the book. We can extrapolate back to our story's beginning, or forward to its ending, but whatever is outside the book's covers is unknowable for us. It seems only logical that there must be some analog to our time --some sort of "meta-time"-- that operates outside our universe, and that our universe is created as that meta-time progresses. But the fact that we must conceive of it like that may just reflect the limitations of our comprehension. 

Informal fallacies don't have exact definitions --their purpose is to identify common structural deficiencies in bad arguments. It is common for more than one informal fallacy to be present in an argument. In this case, if we focus on the "I worked on the set" portion of the reply, then that portion is in fact an appeal to authority. The speaker is establishing himself as authoritative by virtue of his personal experience with the movie. He is not addressing the substance of the other person's claim. The aptness of the authority isn't relevant to that identification. It just needs to meet the structural requirement of substituting an assertion of authority for a valid response to the substance of the opponent's claims. 

Your gloss on the difference between truth and facts seems (to me) to match a fairly standard definition in the case of "fact" but to miss some of the historical richness of the concept of "truth". Truth is a controversial topic that has been defined many different ways at different times, particularly in the world of philosophy. It is generally taken to mean correspondence with reality, but for a Platonist, that might not be what we ordinarily experience as reality, but some deeper and more hidden reality. Truth is is typically considered to have a positive moral value, and is sometimes conceptualized as a virtue, or even in quasi-mystical terms. Facts are a more modern, prosaic picture of correspondence with reality. A fact is generally taken as a discrete bite-sized truthful statement about the state of affairs in the world, where no deeper layer of reality is assumed or referred to. Typically a fact is empirically verifiable, and in some contexts must actually be verified to be considered factual. Typically "truth" is taken as more primary, with facts being defined in relationship to truth, but some thinkers have proposed an inverse relationship where facts are defined by verified correspondence with reality, and truth is then defined in relationship to what is factual, rather than the other way around. It's also worth noting that several thinkers, most notably Tarski, have done work of practical and philosophical significance in establishing exact definitions of truth for formal languages. 

Different philosophers have answered this quite differently. Plato believed that people have access to an internal "memory" of the truth, and the philosopher's job is just to guide them towards recovering that memory. Because of that perspective, many of Plato's arguments contain deliberate errors, paradoxes and inconsistencies --they are intended to stimulate the mind, rather than necessarily to convince the audience. In contrast, Descartes's position was that our beliefs should be certified by reason, and developed from first principles. For this reason, he strove to create arguments without "defects" of the type ascribed above to Platonic arguments. Finally, for an empiricist like Hume, the justification for believing a claim is that it accords with experience. It doesn't matter what your abstract musing tell you, if they lead you to any conclusion counter to your own experience, they must be discarded. There are other ways to justify philosophical claims but these are arguably the three most influential. 

Philosophy is to science as theology is to religion. Science is to engineering as religion is to ritual. Or, if you prefer: 

On the one hand, note that this general-form argument doesn't even require a divine being (it could be the President, or a new friend at school, or a pretty girl on Facebook, etc.), or that there be any ontological uncertainty about the being. However, it does have that big asterisk. You're losing the time and effort you put into pursuing the relationship, arguably, and if there are any other costs of your effort, you incur those as well. Whether this argument is compelling to you, with regards to God, depends on if you think it's more reasonable to believe that God, if God exists, will reward any sincere effort to reach out (this is the view taken by noted Christian apologist C.S. Lewis) or that God, if God exists, will ignore and/or punish all but the approved and proscribed method of seeking God (this is typical of a "fundamentalist" view of God). In other words, you have to assume that you know some things about what God would be like, even while withholding judgment about God's existence. The more positive a conception you have of God-if-God-exists, the more compelling the argument becomes.