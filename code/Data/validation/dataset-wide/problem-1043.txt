I'm working on a sql query that needs to get a percentage of the records based on the value of a particular column. The percentage is given by the user. A stripped down version of the query is below 

However, I do not receive the same message when exporting to Excel from the 2008 server. I believe the problem is related to some Numeric fields that are coming back with a value of 0.0000000000000000000. Has something changed with the excel export function between these two versions? 

The largest thing I look for when trying to optimize an ssis package is data sorts. Since these operations have to complete before the data stream can continue they can be very costly for performance. Without seeing or knowing much about the package its hard to give much more than general advice. 

Car rental now stores the relationship between the client and the car that they rented and for each record stores the datein and dateout. If you dont want to script all of this out you can use the table designer to create the table by right clicking on tables and then choosing new table. You can use the database diagram to drag and drop columns in order to create your foreign key relationships by right clicking database diagrams and chosing New Database diagram. If you use this structure then You would leave the DateIn field as NULL untill the car was returned. Your query to find out which clients and cars have not been returned would be 

ok. It looks like you have 4 tables here with nothing to join them together. Perhaps the easiest solution at this point would be to create a mapping table called something like CarRental which would have the columns Carpoolid, cilentsid, dateinid, and dteoutid. I think the way I would go would be to drop the datein and dateout table and put them on a table called car rental. 

I need to be able to pull say 50 percent of the output for each organizationid. So in this example I'd come back with 2-3 records. Which records doesnt matter as long as the correct percentage of the records is pulled. Any ideas on how to go about this? 

I'm attempting to setup log shipping between two differnet domains. When I attempt to connect to the secondary server instance the primary server fails to connect. If I turn off the windows firewall on the secondary server then I'm able to connect. I have opened up the following tcp ports on the secondary server windows firewall 1433, 1434, 137, 139, 445 I have opened up the following udp ports on the secondary server windows firewall 137, 138 Are there any other ports or firewall settings that I've missed? 

I'm trying to do a copy database by scripting a backup and restore. I have the backup procedure working correctly but I keep getting the error 

I'm working on a query in Microsoft access and it does not appear to allow me to filter the inline view by a date that exists in the table outside of the inline view. A simplified version that exibits the problem is below 

I've setup my databases so that anytime a procedure is blocked for more than 45 seconds the database notifies the DBA email. Is it bad practice to setup a way to auto kill the process that is doing the blocking? I'm assuming yes; however, waiting until an off hours DBA can get to a computer and fix the change seams problematic as well. Is there a better way to handle the blocking processes? 

Ultimatly I'd like to combine these into a single products table and a many to many relationship ID ProductName ProductType where product type would be either 'product' or 'feature' ID productid featureid for the mapping table. Yes the view that is setup needs to be an updateable view. 

At my current place of employment someone made a decision to put IDs into a comma delimited column. I'm looking to break these out into a proper many to many relationship. However; it is not in the cards for the application that references this data to change right now. So I need to modify a view to still show the comma delimited column. What is the best way to setup the view so that it displays the comma delimited column? Is there a way to do it that doesn't involve leaving the original column? A stripped down version of the table definitions is as follows. 

This appears to be related to some implicit conversions that are occurring in the background when exporting the report to xlsx. Numeric columns are being sent to excel as if they are text columns. One report responded to a change to the query using an Explicit cast to Numeric (Cast as opposed to convert to remove any commas or other formatting that might confuse the conversion). Several other reports did not respond to the Cast but returned correctly after using an expression conversion of VAL(). I still have several reports that are not responding to either. If anyone out there has a good explanation for this I'd love to hear it. 

I'm trying to design a set of fact tables for a datamart that will work in an ssas cube for one of our clients. One of the challenges I'm facing is that there appears to be some disperencies between order item pricing and order pricing such that the summation of all of the order items does not equal the pricing of the order. This occurs because admins are aloud to go in after the fact and change an order price inorder to discount the customer. However no record of the discount exists. In this case would I want to create a fact table for the order and then a fact table for the order item? If so I'm assuming I would need to create a key for the fact tables to maintain the relationships. Is this an acceptable practice or will I run into problems with this type os solution when I go to build the cube. Any thoughts on a better way to model this would be appreciated. Thanks in advancce 

I need to create an alert that will notify me when any query has been blocked for more than 60 seconds. For example if someone has a transaction open on a table and forgets to run a commit or a rollback. Is this possible to get from the system tables? 

I found the issue but I'm not sure why its happening. To query the linked server I have to right click SSMS and select Run as Admin. Then I can query the access db. I dont know why this is the case as I'm remoted into the machine. I'm an administrator on the machine. I created the Linked Server to the Access Database which resides on the C:\Drive of the server. But this will at least allow me to hand run my code. Since this is my development server I think that will suffice at least for now. 

Once these views had been created we could pull the column listing from sys.columns joined over to sys.views. 

My company is changing from a distributed Access Database model to using a centralized SQL Database. The datatables were designed such that all of the tables have a modified date. In discussion it was suggested that since we will be creating a trigger on each table to handle the modified date perhaps we should have the trigger also log some information to an audit table. Is this the best way to setup auditing so that we can track who is changing information or is there a better way? Links to articles on the subject are welcome. For the audit I'm looking at capturing the table name, column name, date modified, row id and the username of the person making the change. Is there any information I'm not thinking of that I should be capturing that might help me avoid future pit falls? 

Is it possible to give a user or group rights to a snapshot without giving them rights to the database if the database and snapshot both reside on the same instance? 

SQL Server 2014 I have an application that provides the xml for blocking reports. The xml contains a sqlhandle that I'm wanting to store in a separate database for later consumption. The sql handle is stored in the table using the following code 

The update should be updating the record with the NUll dtdeleted date and setting it to GETDATE() which appears to be throwing the error. 

To use the CustomApp credentials, the user adventure-works\tengiz0 executes the following statement. 

We are working on migrating our SSRS from SQL Server 2008 to SQL Server 2014. After brining the reports over I have a set of reports that will execute just fine but when I try to perform a standard export to excel from the 2014 Server I get a message box saying: 

If you have the option of moving the data into another database I would begin looking at using a star schema. Here is a website that might help you begin research $URL$ Star schemas are built specificly for the fast retrieval of large amounts of data and are very good at aggregating that data over time. Plus having the data seperated from your OLTP database means that there are not as many draw backs to adding indexes. What's more having your data layed out in a Star Schema sets you up to be for creating data cubes with SSAS in the future; although, you do not need a cube to interface with the data. 

I pulled the syntax straight from the invoke-sqlcmd get-help examples. Can anyone tell me what is wrong with this? Thanks, as per request this is running on windows Server 2008 R2 

I have several access databases that I'm copying over to the sql server, connecting to as a linked server, and then pulling data from each night. I need to be able to identify if the current access db the linked server is pointing too has a particular column. If not I'll need to create the column before copying the next access db over. Is it possible to check if an access column exists through a linked server connection? 

Would this achieve what you want? Technicly the inline view isnt necessary as you could repeat the case statement in the order by. 

To revert back to the adventure-works\tengiz0 credentials, the user executes the following statement. 

SQL SERVER 2008 I have a mapping table that contains a deleted date column. Since the table uses a surrogate key as its unique identifier there is a uniqueness constraint on the table. This constraint contains each of the foreign key values and the deleted date. I have a process that runs each night and attmepts to modify this data based on an external data source. The process is attempting to update a record and set its deleted date to GETDATE(). There is currently a record in the mapping table witht he same foreign key values and a deleted date of 2011-12-17 00:17:22.157. The process is returning the following error Violation of UNIQUE KEY constraint 'UNQ_tsitemapping_fullcolumnlist'. Cannot insert duplicate key in object 'dbo.tSiteMapping'. The duplicate key value is (4, , 5, , 1394, 154, Dec 21 2011 6:59AM). Shouldnt the inclusion of the deleted date in the uniqueness constraint mean that this is a unique value? Why would I be getting a UNIQUE KEY constraint violation on this table? Any suggestions on how to fix this problem would be welcome. As Per GBN's comment below. Here is the table definition. 

I'm writing an ssis package that imports a number of access databases nightly. Each database has the potential to have a slightly different schema. Since I'm creating the tables than doing a direct table insert using execute sql tasks inside of a for each loop container the schema differences cause errors. I've upped the max error count so that the package can take errors and continue running. This leaves me with some databases that have incomplete data which I would prefer to drop. I attempted to make an event handler that would drop the database; however, at the point that the event handler is called the database is still in use. Is there another way that I could drop the database if there is an error? 

I'm trying to create a scalar function that will return a delimited list from some sql that would be passed in. Unfortuantly where I work the previous people responsible for the database thought it would be a good idea to put comma delimited id lists inside of some columns. This function is the first step for me to rectify the problem without breaking the developers UI. Can anyone tell me how to fix this error? My code is below. 

I've installed 2 instances of SQL Server 2008 R2 onto our local windows server. For the purposes of testing I've turned off the firewall. I'll get the firewall back up after I manage a successfull remote connection. The server isnt exposed anyway. TCP\IP is turned on for both instances. The default port is assigned to the Default instance and that one is working fine. I've assigned 4143 to the named instance but I'm still unable to connect remotely. I can connect remotely to the default instance just not the named instance. What step or steps am I missing?