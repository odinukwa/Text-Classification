I am trying to deploy my 2012 SSIS project to the Integration Services catalog on the SSIS server (also 2012). I am getting the error below. I checked that there is plenty of space in SSISDB and msdb databases as well as on the disk. Any ideas what might be causing this? 

OK, here is how I ended up addressing this. I basically wrote a script to compare row counts between tables on publisher and the distributor. One per-requisite is to have a linked server between the two servers. In my case it is called "distributor_ls" (since distributor is on the same server as the subscriber). First part of the script gather information about published databases into a temporary table called #tmp_replcationInfo. The credit for this first part of the script belongs here Second part of the script then uses this information to gather the table row counts into a temporary table called #result for those tables where row counts differ. The differences are then displayed. I know it may not be not super-reliable because it relies on row counts stored in sys.partitions table, but it does what I need. I hope somebody finds it useful. 

It appears that the first file gets created during validation (even though I set DelayValidation = True for the log file connection manager). So my first question is why is this happening? It doesn't seem like the right behavior. Now to take this to the next step, I create a child package "Package_Child". I add an execute package task to the parent package to run this child package. In the child package I add a variable by the same name as above: "User::Log_Path". Next, I add a configuration to the child package, so that the value of "User::Log_Path" is obtained from the parent package variable by the same name. Finally I add log provider to the child package and set it up the same way as described above for the parent package. Let's now run the parent package again. This time I will end-up with three log files: two for parent and one for child: 

Visually we can tell that, for example, we can schedule projects 1, 2, 3 to run at the same time because they do not share any employees. Let's call this group of projects "Group 1". After that we could schedule projects 4, 5, 6. Let's call it "Group 2". Finally, in our example we have project 7 left and this would be our "Group 3". My question is, how can I write a T-SQL query to perform such grouping of projects? Thank you! 

I am looking for suggestions/ideas to the following problem. We have a number of data warehouses for each of our clients (on SQL Server 2014). These data warehouses are maintained fairly fresh with continuous data loads every few minutes. When DW needs to change, this will, more often than not, require a complete data re-load. The challenge is that we can't afford to take DW offline for extended period of time and, consequently, make any applications, which use this data unavailable. It is acceptable if the data is stale, but it has to be available. Therefore, we are looking for a solution, which will enable us to maintain a secondary copy of the data warehouse. Let's call these copies A and B. The secondary copy (B) needs to be kept in-synch with the primary copy (A). When the time comes to make significant DW changes, we would like for the following to happen: 1) stop synchronization between A and B 2) point our applications to the secondary copy (B) 3) perform our upgrade/maintenance/re-load of the primary copy (A) 4) point our applications back to the primary copy (A) 5) re-establish synchronization between A and B (we understand it will take some time) So basically B acts as a temporary "fill-in" while A is being upgraded and reloaded. The solution has to be automatable, meaning, we need to be able to do everything from a command-line/powershell. We looked at some of the existing HA/DR technologies, for example SQL Server Always On, but it seems that breaking and re-establishing synchronization (as described above) may not be a good fit for it. It seems that what we need is a 3rd party software, possibly capturing disk activity at the system level. I appreciate any ideas, recommendations or experiences. Thank you! 

Why does it take up so much space on the disk? I looked, but I couldn't find any way to control the size of the memory-optimized files created. Here is T-SQL Script used for creating in-memory table: 

I would like to close the loop on my own question. We worked with Microsoft support on this and the bottom line is : "it's by design". I strongly believe that this is a flawed design and should be changed. The Microsoft support engineer opened a feature change request for this. In addition I created a "MS Connect" item about this. Please up-vote it if you agree with the request: $URL$ 

In real-life scenario, these could be very large tables with millions of rows and I am concerned that this will kill my performance in production. Should I be worried? BTW, all of this acrobatics is done as part of performance tuning exercise for EF queries, so I am prevented from using many of the normal performance tuning techniques. 

This seems a browser problem to me. When I render a reporting services report in IE11, the "Export", "Refresh" and "Export to Data Feed" buttons show up under each other taking up a lot of extra screen space. Does anybody know what could be causing this? I am running Reporting Services 2012: 

What's going on? Finally, if I simply remove the original "Log_Design_Time" directory, my child package will start failing complaining that it cannot find the path specified: 

I have done a fair number of traditional data warehouse implementations where the loads were done in a batch-oriented manner, i.e. the data is refreshed nightly or, at most, every few hours. I now face a challenge of creating a system, where the data in the data warehouse needs to be maintained close to real-time (a few minutes delay is OK, but no more than that). I have done a lot of reading and it seems that getting close to real-time has been a trend in DW over the last few years. However, I am having trouble finding specific examples and concrete information about the available tools, which support this kind of "trickle-feed" ETL. It seems that the right tool would be able to read the database transaction log and send those changes over to the data warehouse, while allowing to do some data transformations in-flight. Does anybody has experience with real-time data warehousing and can recommend a good tool or point out a good reading on this subject. Here are a couple of relevant links: $URL$ $URL$ Thank you! P.S. I work in a Microsoft shop, so the source databases are on Microsoft SQL Server. I do have a good handle on SSIS, but it doesn't seem to fit here.