However, it has only happened once, and I could not deal with it there and then. where/how could I search for more information as to what was the primary cause(s) that triggered this alert/message? Edit Actually this question could be something down these lines: what should I query at the time the alert was fired, to add more information to the email the DBAs get? 

I am using the table sysschemaarticles to collect information in my transactional replication. It is used for a variety of things including adding new articles and finding out who are the subscriber Servers. this script needs to be run in the publication database. 

I had a third party company coming over and building ssas databases to produce reports. They did, but now the DBA wants to have a look but I haven't got permissions, to even see those databases, as you can see on the picture below. Can I grant myself administrator permissions? I am system administrator. there is a AD group called and 

I find this handy when I know that I will be using the stored procedures through a call to OPENQUERY as shown on the example code below. 

the stored procedure is run from inside the report, as you can see below, and further below a picture with the output of my stored procedure. 

What is the minimal permission I should grant in order for him to get this done? Any alternatives? (I was thinking about granting read only access to the reportServer database or something on those lines - but I don't want to make their lives too difficult ) I found this article very good regarding ssrs permissions: SQL Server Reporting Services 2012 Permissions Webpage were all the ssrs Predefined Roles are explained. 

how can I make sure my script gives me the correct number of rows, whether the object is a table or a heap? 

this was sorted. what it was is that while changing the tempDB file locations I put them in the T: drive and the sql server accounts did not have permissions there. It is quite obvious, moderator or any DBA, if you want me to delete the question just let me know. I would prefer to leave it here, just as a reminder. 

I have also tried some scripts from here, to check what transactions are filling the log, but could not find any. I believe there must be some very long transactions going on at the same time, or at least one of more long transactions being executed. How can I check for these ()? In LIVE this is a full recovery mode database, part of alwayson. In TEST this is a simple recovery mode database, but because all the jobs are there, the log grows to 7GB all the same. 

first let me say that restoring a replication db from domain1 into domain2 is not recommended. you will need to script all the replication and re-apply the scripts including the accounts and passwords for the agents in the new domain. if it is on the same domain then, where all the agents accounts already have all the necessary permissions on servers - publisher, distributor and subscriber then I have been restoring replicated databases without scripting first and applying scripts later on. here is what I did: 1) backup 2) script all users and permissions 3) script the drop and create for all replication () 4) restore, troubleshoot and test 

OrderPayment and Order. Looking at it today(day 1 after the rollback) , it seems so clear that this indexed view - with an inner join using these 2 big tables - is not a good idea. Unless I had changed also the way data is written into these tables, alongside the replication. 

Included in the code of the creation of some stored procedures, I have the enabling of data access as you can see below. 

I have been using the default trace to see when this transaction log has been autogrown, but I found none. 

I need to grant these permissions, what would be the minimum level I could grant so that they can do their job? I am looking at SQL Server Agent Fixed Database Roles and also Database Engine Permission Basics I still could not find exactly, what would be the least permissions to be granted? 

I recommend you have a look at the procedure sp_dbpermissions by Kenneth Fisher. I have been using it and it allows you to list all the permissions of a particular user in a database, and gives you all the scripts to copy all those permissions and apply them on a different database. In case you want to apply them on a different server as well, that is fine too, but don't forget to create the server login first. 

The question: How can I find out what I can remove from the plan cache? or at least how would I start this investigation? 

I have a job that I have scheduled to send me an email about all the backups on a server. The script that collect information about the backups and send the email is here. The email tells me where the backup is. for example: here is a picture for illustration purposes: 

I have been finding strange error messages on the sql error log: Bocss: same deadlock taking place every hour â€“ needs investigating Also lots of recompiles are listed in the error log for other SPIDs as per following examples: 

And also on the Analysis Server Properties, the "Commit Timeout" and "Force Commit Timeout" as you can see on the picture below. 

I need to deploy packages using visual studio 2013 but I am seen this message, the application is not installed, as you can see on the picture below. what do I need to install? 

You can use the following script by Greg Larsen, to help comparing before and after adding the index. It measures cpu and I/O. 

I am having an issue that I need to move some to a different database. Those are run from the same and other databases as well. Specially they are run from inside . In the script below, I can identify all the triggers that run a called from the database I currently am. what I am failing and want to achieve is: I want to find all in all databases in the current server that call that . What changes can I do on the script below to achieve this? or How can I achieve this? 

I love opportunities where I can fit in an indexed view. I love to see the performance of an specific query to go sky-high. However, before I put one in place these days I consider at least the following factors: 1- can't I improve the query code 2 - how many indexes are there in each of the tables involved 3 - how often insert, deletes and updates happen on each of the tables involved in the indexed view 4- what indexes are already there in each of the tables involved, can't they be modified so that they cater for the needs of this particular query?? 5 - If the current indexes do not cater and can't be modified, would not new appropriate index(es) help the query in question 6 - after trying to modify the query, and running it a few times why not run a performance tuning advisor and see what suggestions it comes out with? you might be surprised the gains you might get by adding some multi-column statistics to your database. Use with care and have a test system for this. 7 - are you monitoring the number of deadlocks, disk reads, disk writes, which objects are causing most locking and blocking, and for how long? 8 - are you monitoring WAIT STATS? 9 - from previous experience, calculated columns and indexed views together can slow down writes, and increase the number of deadlocks to an unacceptable level 10 - basically indexed views improve reading but must be done in a way that do not hurt the writing too much. the more you know your database and how it is used, the best decision you can make. also each of your objects in your database, the more you know how they are used, the best use you can make of filegroups, disks, indexed views, and now even new features like in memory objects. In this situation I will probably have a parallel process to keep up the stats on separate tables, either a job(s) or triggers (after update, insert, delete) so that the data does not need too much calculation when I needed to be shown. calculated fields might be an alternative as well. 

I often have to alter jobs i.e. adding steps, changing the stored procedures the job calls, etc. and I always need to do the same thing on all servers in my availability group. when dealing with jobs, the in server A is not the same in server B for instance, so I always have to find the manually. how could I automate this? below is an example where I add an extra step to an existing job. I need to have a look at all the servers in the availability group and go server by server finding the and doing the changes. 

apparently this is because I need to download and install Microsoft SQL Server Data Tools - Business Intelligence for Visual Studio 2013 $URL$ I will get this done and if it all works I will accept this answer. 

this procedure inserts values into a table variable from the following table: please note that the columns in the table are VARCHAR and for some reason the parameters of the stored procedure and inside the code as well, everything is converted to NVARCHAR. 

is it maybe something else running at the same time? like backups to take or other types of backup (non-sql server) 

I have been working on a solution to synchronise logins (using T-SQL) between 2 servers, or between AlwaysOn nodes, inspired by sqlsoldier. It requires a linked Server. this is a partial view of the stored procedure I am using: 

I am monitoring stored procedure executions, mainly the execution times and the CPU usage, using sys.dm_exec_query_stats in the script below. 

As you can see on the above picture, the amount of free space inside MyDatabase is high. I need a copy of this database so that I can test some partitions operations. Is there a way I can restore this database ignoring the free space in each file? Basically I will use my "new database" to develop and test something, I would prefer to keep it small. How could I achieve that? 

as part of a bigger operation I have been putting together this script below that basically generates a for each user of the current database. When the user has a it comes up in the generated script. 

How can I find out the AD (active directory) account associated with that Proxy via T-SQL? SO far I have this script below, but something is missing. Also, the AD account might not be a login or user in the current server. 

This is working fine. There are some points I would like\need to improve though. When I need to restore a backup from Azure the backups are stored in the order they have been saved, and generally I need the most recent backup. For example the picture below (apologies I have used this thick green to hide sensitive information like server, accounts and database names, it got really ugly but just to have an idea): these are backups: 

I found out what the problem was, just a TYPO in the servername/instancename. These scripts work fine to create a subscription in a sql server (2012-2008-2005) transactional replication. I would like to leave the question here though, for future reference, unless Moderators prefer me not to. In that case please feel free to remove it or just let me know. 

As I often need the latest backup of an specific database(s) is there a way to order how the files are stored in azure so that the first one available is the latest saved? conclusion After downloading and installing I can order the backup files as you can see on the picture below. 

These security permissions can easily be scripted. this is an example: apologies foe the horizontal scroll, but that's how it is. 

I have an script that I used to use. this is to compare 2 databases, in the example I am comparing with . Now I only use the visual studio schema comparison tool as suggested in the comments. It is not even properly formatted, it is not a complete, ready, free from bugs solution, but it would at least give you some ideas. I still would like to suggest you use the visual studio community edition 

Now I will do my convertion and save them all into a temp table and then we are going to look at the structure of the temp table. 

(although the database in question has auto-create stats on) Otherwise it has been discussed here and scripting statistics here Sort Warning The sort warning has been discussed here which includes a script to find the stored procedures with sort warnings plus the below by Paul White: 

but when I use my own sp_foreachdb procedure, the source code is on this link below: A more reliable and more flexible sp_MSforeachdb 

the questions: 1) [decCatItemPrice] is better off as an index column or as a column in the INCLUDE? how do you decide? I have taken my decisions through try and error, and this index arrangement is the one that reduced the logical reads number to the lowest as you can see on the picture below. 

Questions Is there a way to make sure the database setting will prevail over the management studio settings? I thought about creating a plan, and use OPTION (USE PLAN) but that has other consequences that is beyond the scope of this question. 

when I run my script to find overlapping indexes below: Finding and Eliminating Duplicate or Overlapping Indexes 

I am aware that I am using a linked server within a dynamic script, is there a way to work around this? 

The publication is SupplierDB. I have just generated an snapshot, as you can see on the picture below. 

I have a procedure that shows me all the permissions on the user objects. I can specify the name of the object too. the code is this: 

I got a question regarding the best way to put a bad column for an index in an index, so that I can satisfy my query below, and similar others that follow the same pattern. on the query below, all the columns in the select come from table TableBackups.[dbo].[tblBOrderItem]. I only go to the other table - TableBackups.[dbo].[tblBOrder] - to get the orders that are between the @fromDate and @todate AND the order must have a value which in my system here it translates to i.decCatItemPrice > 0 

I have a database of 20 GB which insists in having its transaction log over 7GB. when I used this script to find out the size of the biggest objects in that database, I see they are relatively small. 

I have changed the relevant columns in the table, from VARCHAR to NVARCHAR, in order to eliminate the need of conversion. the Table became like this: 

I have been doing this, below, but I would really like to find out the list of the AD groups (that have access to this server according to the above picture) that this user belongs to. 

This is an example of the results of my query while I was trying to monitor some CXPACKETS WAITS. Pending_IO_count seems to be fine, but pending_IO_by_counts are always zero. what could be an alternative? I want to measure the I/O usage of each parallel process. in fact it could be just the I/O usage of each session or request. even sessions without request sometimes are good to see, because they might be blocking other processes. 

When I have a look at the tables that would be referencing my table I get the following list: script to find foreign key constraints: 

I have basically just done the number 3 and the linked server is working fine for me now. As you can see on the picture below. 

I recently had to make the stored procedure spPartyOrderAllocation_AllocateItems to select and update tables in other databases, while keeping the pemissions levels to the minimum. This was necessary because an specific application Stock Allocation would connect to the databases and only run this stored procedure. The way I go this done is: 1) create a login that will be used to run the stored procedure in different databases 

I would like to find out from the code below, what was the name of the database where the last command was run. In this case I am looking for . his there any way of finding this? Is there any DMV that I should be looking at for this info? Also, what if the update was wrapped in a dynamic sql, would it be possible to track it then? 

The image below shows a transaction replication problem that I am currently investigating. The image comes from the Replication Monitor. How can I get this information using T-SQL?