This is one (of many) downsides of EAV designs. You can't really improve the JOIN: because of the necessary complexity, a cost based optimiser won't get to the perfect plan. It finds "good enough" Suggestions: 

I'd question why you could have 500GB database with one drive. Forget about partitioning for now: what about drives (LUNs or RAID arrays) for 

It depends on your setting of SET ANSI_NULL_DFLT_OFF or SET ANSI_NULL_DFLT_ON (read these for other effects too from SET ANSI_DEFAULTS). Personally, I'd have specified this constraint explicitly if I wanted null or non null columns rather than relying on environment settings. 

This means that database users have no matching server logins. That is, each database sys.database_principals has no match in sys.server_principals For Windows logins this is easy. This generates your missing CREATE LOGINS 

If you have SQL Server and SSIS, you should also have Reporting Services. This can generate Word documents. This can be interactive, or you can use URL access, or SOAP calls to get at data. May not be ideal, but it takes away the need to try and write Word documents via scripting or code: you just need some way to invoke what MS provide 

You should only need to drop and recreate the affected NC index. Saying that, on a test server, you can see the differences in doing this compared to your strategy of dropping all indexes above. I reckon dropping/creating the single index would be quicker overall because the data will be shifted around twice otherwise: once char to varchar, another to build the clustered index. Then you have the NC creation overhead. 

I would never concatenate previous payments into one opaque string. I also wouldn't worry about performance yet: this is premature optimisation. If you do expect 100s of millions of rows, there are other techniques to improved performance. Otherwise, concentrate on data integrity and correctness 

Try With the index size value being so high, this points to a non-clustered index. Clustered index size is very small usually. You can check with the parameter on the DBCC 

In your Employees table, I'd only have a lookup for "Position" because it a limited set of data that can expand. 

If you want random order with a row order too. ActualOrder order here is preserved regardless of the order of the resultset 

is not usually "SARGable" when you use different columns That is, case you can optimise for either CardNumber or FullCardNumber searches with an index on each column but not both columns. This is a "seek". An index on both columns won't work because you it is : values for every row must be examined. This is a scan. If the condition was , SQL Server could do a residual lookup on FullCardNumber after findng CardNumber. Seek followed by a secondary seek basically. Anyway, try this to remove the scan and have 2 individual efficient seeks 

No. Collation is about alphabetical sorting, depending on code page, accent, case, width, kana. Numbers characters (0-9) have none of there properties. So is always after in any sort. You have to split it up as you noted or sort like this: 

A graphical query plan will show where you need indexes A million rows isn't much and well within capability of SQL Server 

This forum or ServerFault. In the 13 years i've been working with SQL Server since 6.5 I've frequented several (SQL Server Central, SQL Server Performance etc) but it's hard to discern quality answers on these. The reputation system on the Stack Exchange sites is a powerful tool to ensure higher quality answers as well weeding out the poor ones. I can't understand your worries about upgrading: unless you have some code that (ab)uses sp_OA% or such you should be OK. The Upgrade Advisor will assist you here 

Generate the restore script from the GUI, instead of using the GUI to do the restore. Then add the PASSWORD option... 

A note first: When you rebuild indexes, you need roughly free space of roughly 1.2 times the largest table. So a 15 GB table in that 20 GB used requires 18 GB free space. This sets a minimum database file size. Personally, for OLTP databases, anything over 50%-60% used is a bit of red flag. So anything less then 35-40GB for for 20GB data would be worrying. To decide whether you need downtime and filegroups and all the hassle, have you: 

The optimiser will ignore this because it's an identical JOIN. SQL is declarative, not procedural, and the query will be evaluated in toto, not line after line. 

There are some cases where the DataSet calls a SQL Server database that uses a linked server: but I suspect you aren't that far yet. If you think you need to use a "linked server" to call one database only to get data from another database, then why not call the other database directly. And stop thinking of linked servers 

For more info about on-disk structures, see Inside the Storage Engine: Anatomy of a record. Interestingly, where is the 4 byte row header? This appears excluded from the 8060 bytes. Tested on SQL Server 2008 SP1 and SQL Server 2005 SP3 

Unfortunately, I can't remember the exact details now. It's the only time in 20 years of SQL Server I've... 

Every statement in SQL Server has a plan. The fact of the statement being in a UDF or proc or adhoc doesn't matter usually In this case, I suspect it is cache misses causing the delay. You can test by DBCC DROPCLEANBUFFERS (caution: will affect all processes on that box). Compile time would be for every change of parameter, not just to an unused one. One thought though: Is the UDF processing or churning lots of data, enough to cause cache misses? 

This means text logs, such as those from Log4net. Not database redo/undo/transaction files Why? PCI says that Credit card numbers (PAN) should be sent and stored as encrypted in the database, which means only the encrypted value will appear in any database log file (whether an error log or the actual redo/undo/transaction log file). See Page 8 and item 3.4 of your document for this. Edit, after @Shark's observation 

Exists and IN will give the same query plan because they are both "LEFT SEMI JOINs" internally. I prefer EXISTS generally because of consistency. NOT EXISTS is safer then NOT IN. Caching does not apply and don't think about it. This is a set based operation and the SQL Server optimiser will work out the best way: It may spool the subquery or it may not. Note that a left semi join is typically very efficient. Don't overthink it. Finally, a PK scan indicates a missing index on UserBlogRoles Observation: use the the schema, Luke. for example 

Multiple values shouldn't be assigned to a scalar variable. It makes no sense You need a table variable (or temporary table). 

This parse node based XML. It's different to read attributes but it isn't as common I had this lying around as a demo with 3 slightly different XPath queries 

It looks like the pre SQL Server 2012 behaviour is a bug. I infer this from this Connect article: $URL$ I could be wrong of course... 

.. for each table, with SET IDENTITY INSERT ON if requirede Then I'd add indexes and constraints after loading data. It depends on your comfort level with SSIS (mrdenny's answer) or if you prefer raw SQL. 

No, unless you logged it via a DDL trigger or such You want to look at who as ALTER rights in that database, or membership of sysadmin/db_owner/ddl_admin role. This would be better as a general review rather than a witch hunt. There are probably other people with rights to make unapproved and unauthorised changes too 

It can be done in one set based operation Using a WHILE risks invoking the "Halloween problem" as you update rows over and over. It's easier with SQL Server 2012 using LAG 

It's been consistent for me: bounce generally means restart - Bounce SQL Server = restart the service, normally - Bounce the server = reboot - Bounce the PC = reboot - Bounce IIS = run iisreset I'd interpret "bounce Oracle" as restart the DB engine processes 

I would say it's an implementation detail for human readability. Constraint names are optional (in SQL Server at least), but I'd like to have for rather than 

If all data access is via stored procedures, then permissions on the table are not checked of the same user (dbo probably) owns both procedure and tables. This is called ownership chaining This also means the no permissions are needed on the tables at all. You don't need to deny or grant anything on the tables because they won't be checked. Lack of GRANT means no rights anyway, so you don't need DENY Now, this means nothing to folk with elevated privileges: sysadmin, db_owner etc. These will always have rights. You can only use triggers to block these: but they can disable or drop triggers of course. I assume that your "end users" are not running as db_owner or sysadmin... 

Finally, consider how you use addresses. You add or update them. You don't look for an existing address and link to that. Each address is per source only. 

For the server, you'd use "default language" via And you can run SET LANGUAGE or SET DATEFORMAT anytime you want of course 

An very blunt way of forcing a stats update... It may be that a full scan probably isn't needed, just up to date stats. Now, if you have enough data changes to trigger a auto update, then you probably have logical fragmentation too. I would try doing hourly on-line index builds on the offending tables/indexes used 

You'd have use the Import/Export wizards in SSMS to migrate everything There is no "downgrade" possible using backup/restore or detach/attach 

Your sample code above is confusing (you have multiple parents for the same Code column) so I'll give you what I understand 

Sybase 3rd party tools are sparse unless you spend money it seems. The usual vendor suspects have Sybase offerings: 

In addition to Mark's answer You can get a feel by having realistic test data at expected quantities. I've seen many, many (too many) cases where a query runs OK with a 1000 rows but not the million in production. If you can, work on a copy of production later on, Of course, I've seen the odd problem only in production because of usage patterns when everything else is identical Temporary indexes? Outside of ETL load patterns, if you need them once you'll need them again. Don't forget: an index create/drop is a write and is logged = more load 

You'd have to create a view and create an FT index on this view From MSDN "Getting Started with Full-Text Search" (my bold) 

A prepared SQL statement will be executed every time too. If you have a fat client then caching is local to the client. This is mostly OK unless you have millions of blobs for example. In our web client we render all the results but only show the top 100 and have a "Show all" button than expands a hidden DIV with rows 101+. We don't cache in the web server and we don't offer paging. 

In the FROM clause, you can only use table valued functions See "Types of Functions" in MSDN Edit: As an observation, I'd tend to avoid: 

Here, data is 8053 bytes with overhead of 2 bytes record type) + 2 bytes NULL bitmap pointer + 3 bytes NULL bitmap. Even though we have no NULLable columns, the NULL bitmap exists. When we add a NULLable column, we don't increase the row size. 

I would recommend using these free backup (and other) scripts from $URL$ These have full logging already and have lots of other useful options. Personally, these are what I use. Thoroughly tested and used by many, many folk. 

This means you have logged in correctly to SQL Server, but the database in the connection string (or default database from CREATE LOGIN) is one of 

You have 97% active rows and 95% of queries on this 97%. A WHERE clause that selects 97% of rows won't be helped by indexing: it isn't selective enough I'd consider 2 tables for your "Things": ThingActive and ThingCancelled For the 5% where you query cancelled rows, you read either the Cancelled table only or a UNION/View A twist on this... The cancelled table stores only main table ID and the extra columns (so it's NOT EXISTS to find active). 

First thought... If you have sufficient data to take this much % of the batch, use a #temptable. When the table variable is used later, it is always assumed to have one row: there are no statistics on this table variable. So if you have several 1000, subsequent plans won't be optimal. Temporary tables have statistics (and indexes if required etc) and can perform better for larger datasets 

First off, don't mix "primary key" and "clustered index". One is "how to identify a row", the other is "on-disk structure" Then, you don't need an index on by itself because it is the left hand column of the clustered index. However, it won't be covering (see links below) And for each point in turn: 

It has INSERT, UPDATE, DELETE on tables as per another MSDB page Permissions of Fixed Database Roles 

As well as Dtest's answers, I'd have suggested that LIMIT without ORDER BY is meaningless. Well, this would be the case in SQL Server and others, but MySQL GROUP BY usually implies ORDER BY. It could be you are getting arbitrary rows for LIMIT 12 but implied sort with LIMIT 11. So, try adding ORDER BY... 

If you want to compare data in two different database designs then you'd have to write hand coded SQL to compare data. 

This allows multiple owners per Dog and Cat. You can of course just have an OwnerID in Dog and Cat to restrict to one owner. You can have different attributes for Cat and Dog here. As you add more types, this becomes unwieldy (no matter how you model owners) Option 2: Do you need to distinguish Cat or Dog as separate entities? Isn't this just a type attribute of Pet? 

InnoDB will be slightly slower because it is ACID compliant, has MVCC and does useful things like actually check foreign keys etc. As an example, Oracle's own whitepaper for MyISAM vs InnoDB they actually say 

Plucking an answer out of thin air.. I'd suggest the same load. At the end the day, the same dmvs will be queried by any monitoring tool. The same ones that I'd query adhoc or use in scheduled queries. SQL Server utility or a 3rd party tool just automate this for you. 

Personally I'd consider schemas per client This means separate tables and security per client. Which means separate disks if needed. I don't see much advantage in partitions here 

It sounds like you have a copy of the MDF file, rather than an actual file generated by BACKUP DATABASE. If so, you have lost 3 months data. You can try attaching this without log file using CREATE DATABASE databasenamehere ON (filespechere) FOR ATTACH_REBUILD_LOG The current MDF is most likely can't be salvaged. Reading the file is impractical After this, look at MSDN Backup Overview 

If not an indexed view... You'd need a third table that contains DateCreated and a key on an IDENTITY and a SubTable column that contains 1 or 2 only. And an index on DateCreated This 2-column key becomes the key of the other 2 tables too. You use a CHECK constraint on SubTable to ensure an IDENTITY value is used in one table only. Each bit of the UNION is a JOIN using Table3. Then the index spans all DateCreated values as expected. The indexes you have now are per table: so SQL Server needs to sort the 2 sets of data. 

The Filter condition is in : you use it above. You'd have to parse this, but you must have set it previously to be able to apply it. Ignoring SQL Injection risk (I just got this off the interwebs), you know in this snippet you are filtering on ProductCatID 

In addition: - added or fk should be first - ID is first = not much use Try changing the clustered key to and drop . You've already tried . If nothing else, you'll save a lot of disk space and index maintenance Another option might be to try the FORCE ORDER hint with table order boh ways and no JOIN/INDEX hints. I try not to use JOIN/INDEX hints personally because you remove options for the optimiser. Many years ago I was told (seminar with a SQL Guru) that FORCE ORDER hint can help when you have huge table JOIN small table: YMMV 7 years later... Oh, and let us know where the DBA lives so we can arrange for some percussion adjustment Edit, after 02 Jun update The 4th column is not part of the non-clustered index so it uses the clustered index. Try changing the NC index to INCLUDE the value column so it doesn't have to access the value column for the clustered index 

Personally, I wouldn't use it. There are better ways on managing this with asynchronous statistics or plan guides nowadays. I've only ever seen this hint used on some old trading database that was geared towards Sybase/very old SQL Server. Also, you now have statement level recompilation that may fit your use case better But really, you'll have to try it and find out in your situation...