(3) Cencov's Theorem Very loosely, Fisher-Rao metric, due to Cencov theorem [6] that Fisher-Rao metric is the 'correct' metric to use when the transition mappings are selected to be Markov morphisms. 

If you are more interested in the application, there is a very popular book ABG discusses a variety of applications of point processes in survival analysis. 

You can briefly read a short introduction to decide whether you are interested in the theoretic side or application side of the research subject. Say Rasmussen's Notes. If you are more interested in the theory, there is a two-volume standard text on theory of point processes. It includes a panoramic discussion on temporal/spatial process in Chap 14,15. 

This serves as a pointer and my thought on the OP's question of bounding the spectrum of covariance matrix of subgaussian (mean zero )random vector. The case of spectrum of covariance matrix of gaussian random vector is discussed in this post. For the case entries are independent, there is a nice review slide by Vershynin. For the case entries are dependent, the complication occur in the dependence. So if all entries are perfectly correlated ($X=\boldsymbol{1}_n\cdot x$, where $x$ is a single sub-gaussian), then the best thing we could say is that the covariance matrix is positive (semi-)definite and hence the lower bound of singular values are zero. Therefore we need to assume some conditions on the dependence/covariance matrix of the sub-gaussian random vector $X$. But I do not know any results that claims for theoretic covariance matrix in OP one reason is that there are too many possibilities when you put no assumption on sub-gaussian dependent vectors ; one way to circumvent this difficulty is to approximate the theoretic covariance matrix using sample covariance matrix; and then try to bound the spectrum of the sample covariance matrix using Vershynin's result (1). In such a two-step approach, the randomness coming from dependence of entries of $X$ is first reduced in the sampling assumptions; and then reduced in the approximation step. (1) Approximation of theoretic covariance matrix by "regular" sample covariance matrix. The sample vector $x$ must satisfy a set of “Rudelson-Vershynin regular sampling” assumptions $$\left\Vert X\right\Vert _{L^{2}}\leq K\sqrt{n}$$ a.s. and $$\mathbb{E}\left|\left\langle X,v\right\rangle \right|^{q}\leq L^{q},\forall x\in S^{n-1}$$ then the following holds $$\left\Vert \Sigma_{\text{theoretic}}-\Sigma_{\text{sample},N}\right\Vert \leq C(q,K,L,\delta)\left(log\,log\,n\right)^{2}\left(\frac{n}{N}\right)^{\frac{1}{2}-\frac{2}{q}}$$ holds with probability $1-\delta$ where $C(q,K,L,\delta)$ is a constant depending on the $q,K,L,\delta$. (2) Bounding the spectral radius of sample covariance matrix. Even if we put some assumptions on the sampling, the sample covariance matrix must be control under some more assumptions on the distributions of $X$ in order to reduce the randomness further, one such condition is "polynomial-decay-dominated (PDD) temporal dependence" (2). There are other possibilities like this post. (1) Vershynin, Roman. "How close is the sample covariance matrix to the actual covariance matrix?." Journal of Theoretical Probability 25.3 (2012): 655-686. (2) Shu, Hai, and Bin Nan. "Estimation of Large Covariance and Precision Matrices from Temporally Dependent Observations." arXiv preprint arXiv:1412.5059 (2014). 

This is more like a state-of-art answer. A good review of results in given in [Holbrook]. $\nu(A,B)\leq\|A-B\|$ for the operator norm. This is a direct consequence from Weyl's inequality. This problem about spectral variation bound is fully discussed in [Bhatia] Chap 3&4(with a supplement in Chap7&8 if you got the 2006 ed.) 

For example, the Jordan decomposition correspond to the primary decomposition, like this sort. Moreover I want to know if this result has any application? Most probably in Lie algebra, because this proposition is taken from some Lie algebra course. 

Question: How to sample when the support of prior(parameter space) is bounded set? Answer:There are following solutions if the parameter space is bounded instead of the whole parameter space. 

The tree structures associated with a partition of the sample space $\mathcal{X}$ is usually discussed along with a Beta process(or in computer engineers' world they refer it as "stick-breaking process"). In statistics, this is very useful in nonparametric estimations. When you try to designate a random measure $m\in\mathcal{M}=\mathcal{M(\mathcal{X})}$ for your model. Since $m$ is random, we can also talk about $m(\omega)$ and the stochastic process that generates $m$ as a sample path $m(\omega),\omega\in \mathcal{X}^{\prod}$. Usually the sample path of the Beta process will give a tree-like structure on the product sample space $\mathcal{X}^{\prod}$. The sigma algebra on this product space correponds to the filtration of the Beta process. That is how the $\sigma$-algebra on $T(\mathcal{M})$ looks like. For technical details, please refer to Chap3-4 of 

No. Any $z$ such that $\|z\|^*_B\leq 1$ will do because $B$ is not necessarily smooth when you use the notation $\partial\|y\|_B$. Think of a polygon and $z$ may be a straight line touching the $B$ only at a vertex. 

Sorry I probably missed "$(P_t)_{t \ge 0}$ of a Hunt process" when I first composed this answer and that is why confusion arise. I think it is most beneficial if we start from and stick to definition. Hunt process is a strong Markov process with some regularity on sample functions, usually càdlàg sample functions but subject to some alternations. Suppose $X$ is on a (locally) compact space $E$ and there is a one-to-one correspondence between Hunt process and Feller-Dynkin semigroups. The paper you cited provided (1)(2) as a set of equivalent conditions that allows you to regard the transition semigroup $(P_t)_{t \ge 0}$ as a Feller-Dynkin semigroup as well. A Hunt process may admits a transition semigroup satisfying (2) of course but it also needs (1) to make the operations within the transition semingroup closed. I hope I made myself clear now. 

Given a probability model $\mathcal{P}=\{P_{\theta},\theta \in \Theta \}$ dominated by a $\sigma$-finite measure $\lambda$ (e.g. Lebesgue measure) on a locally compact space $\cal{X}$ along with $\sigma$-field $\cal{F}$. A sufficient statistic is defined to be an $\cal{F}$-measurable mapping $T:\cal{X}\rightarrow \mathbb{R}$ such that $P_{\theta}(X\mid T=t)$ does not depend on the $\theta\in\Theta$. According to Neyman-Fisher factorization theorem, the sufficient and necessary condition that $T$ is a sufficient statistic for $\cal{P}$ is that $$p(X\mid \theta)=g(T,\theta)h(X),\forall\theta \in \Theta,X\in\cal X$$ where $p(X\mid \theta)=\frac{dP_{\theta}}{d\lambda}$ is the density/Radon-Nikodym derivative w.r.t. $\lambda$. My questions: (1) Since the conditional probability measure $P_{\theta}(X\mid T)$ is defined as a solution to the functional equation $\int_{B}P_{\theta}(A \mid \sigma(T))dP_{\theta}=P_{\theta}(A\cap B), \forall B\in \sigma(T)$, are there any characterization of the notion of sufficiency in terms of the $\sigma$-field $\sigma(T)$ generated by the $\cal{F}$-measurable mapping $T$ ,AND the $\sigma$-fields associated with $\cal{P}$? (2) What is the implication of Neyman-Fisher Theorem in terms of the product measure $\lambda_\Theta\times\lambda$, assuming there is some measure $\lambda_\Theta$ given on the parameter space $\Theta$? It doe not seem like "independence" though. I did not post this on stats.SE because I think it will receive better response here. 

In this book they use local estimate to approximate the discrete time behavior using a "finite-element" method, so their method can actually be applied to discrete case. 

This post is partly inspired by this post. Reference request: results on the asymptotic distribution of singular values related to a random orthogonal matrix While it is well-known that two basic categories of graph models are widely studied in statistics 

(1) (Modified from [1]pp.246-247,312-313) First we sample $Z$ from $Unif[0,1)$, since $\mathbb{R}$ is an Archimedes field, for a fixed $n$ we can find such a $k$ that $\frac{k}{2^n}\leq Z< \frac{k+1}{2^n}$. Define random variable $Y_n=\frac{k}{2^n}$, this is a random variable because $k$ depends on $Z$ and $Z$ is random. Now by our construction $Y_n\leq Z<Y_n+\frac{1}{2^n}$, $Y_n,Z$ coincide in the first $n$ digits in a binary expansion. For any Lipschitz-continuous function $f$, we can use the limit of a sequence of bounded functions $f_n$ to approximate it. Therefore wlog, assume $f$ is bounded over $[0,1)$. Consider the jump random variable defined by $X_n=2^n\{f(Y_n+\frac{1}{2^n})-f(Y_n)\}$. $Z\mid Y_0,\cdots Y_n$ is conditionally distributed as $Unif[Y_n,Y_n+\frac{1}{2^n})$. $$E(X_{n+1}\mid Y_0,\cdots Y_n)=2^{n+1}E[f(Y_{n+1}+\frac{1}{2^{n+1}})-f(Y_{n+1})\mid Y_0,\cdots Y_n]$$ $$=2^{n+1}\int_{[Y_n,Y_n+\frac{1}{2^n})}2^n[f(u+\frac{1}{2^{n+1}})-f(u)]du$$ $$=2^n\{f(Y_n+\frac{1}{2^{n}})-f(Y_n)\}=X_n$$ Thus $X_n$ is a martingale w.r.t. the sigma-algebra $\sigma\{Y_0,\cdots Y_n\}$, if we take $X_\infty=lim_{n\rightarrow\infty}X_n$, that is to say we investigate the stationary/invariant measure (corresponding to $X_\infty$)of the stochastic process we defined above, thn $X_\infty=g(Z)$ is the RN derivative of $f$. 

I think a closest inequality in controlling the deviance between two processes is McDiarmid’s Inequality, and it has an extension which also applies to subgaussian random variables Concentration in unbounded metric spaces and algorithmic stability. You need to describe what kind of analogy you are looking for in order for me to add more. 

What you have proved is that the $D$ converges in probability as $n\rightarrow\infty$, and I do not see why $T\rightarrow\infty$ should involve in the limit in your conclusion since you neither assume decay speed of $\hat{\theta}_{i,j,T}$ nor restrict the increasing speed of $T$. So I think you missed some assumption here. Moreover, if $I[|{\hat\theta_{ij,T}}|>f(n)]$(or the double sum itself is monotonic) is monotonic as $T,n\rightarrow\infty$, which is very likely to be true since $|{\hat\theta_{i,j,T}}|<1$. Then using Skorohod Theorem you know there is a subsequence $Z_{k_n}$ of $$Z_n:=\sum_{i=1}^n \sum_{j=1}^n I[|{\hat\theta_{ij,T}}|>f(n)]$$ which converges to $0$ almost surely $P$, and since such a sequence $Z_{k_n}$ is selected from a monotonic sequence $Z_n$, the original sequence must also converge almost surely by comparison lemma. 

This question could be a better question if formulated better. (1)Stationary process You are right about the fact that when we have a stationary Markov process then due to the discrete renewal theorem [1] we can see that there exists a stationary distribution for irreducible aperiodic chains and hence the Markov chain is ergodic. This is a special case of the Birkhoff's ergodic theorem which may apply to continuous time situation as well. (2)From stationary to nonstationary process It is well known that if the dependence between different states are not too strong then the Markov chain can possess ergodic properties as well[3]. The question is how to depict dependence between different states $X_s,X_{s+\Delta s}$. There are different ways like Bulinski's bounded Lipshcitz dependence[3], Kolmogorov's maximal correlation coefficient[5] and Rosenblatt's mixing coefficients[4]. The last one is becoming more and more popular set of conditions that are used through out physics, computer science and other subjects. If the nonstationary process $X_s$ satisfy some restrictions concerning about their mixing coefficients(i.e. dependence) Then the ergodicity still holds for such kind of stochastic processes as mentioned [5]. (3)'Wilder' nonstationary process For example the sofic process will admit finite Markovian representations but themselves are not Markov chains of any finite order[5]. These processes will usually lead to chaotic behavior and therefore the ergodicity obviously collapse here and therefore we are unable to discuss ergodicity. In short the main difference between ergodicity of stationary and nonstationary processes is that whether their behavior can be regarded as some kind of orbit generated by transition kernel groups, which I think is the essenc ergodicity lies at. Reference [1] Karlin, Samuel. A first course in stochastic processes. Academic press, 2014. [2]$URL$ [3]Bulinskii, Aleksandr Vadimovich. Limit theorems for associated random fields and related systems. Vol. 10. World Scientific, 2007. [4]Shields, Paul C. "The ergodic theory of discrete sample paths." Graduate Studies in Mathematics, American Mathematics Society (1996). [5]Ibragimov, Il'dar Abdullovich. "A note on the central limit theorems for dependent random variables." Theory of Probability & Its Applications 20.1 (1975): 135-141. 

First of all, I do not think Sobolev inequality also holds under the same set of regularity assumptions under which the bilinear form is regular (as a bilinear function over the domain $\bar{D}$). For example, consider a weighted Sobolev space which puts exponential weight on the component with exponential growth rate. The only/authoritative reference I know about the kind of reflecting( I think what you mean is symmetric in standard terminologies) Brownian motions is [Fukushima-Oshima] Sec 2.4, which is devoted to discussion of Sobolev-type inequality. You can reduce the problem to a estimate problem as a Laplacian but I think a necessary condition to give a better estimate is modulus of continuity on the path of ${X_t^1}$ which allows you to integrate along the path w.r.t. the Dirichlet-form. The paper you cited requires local structure on the space so it is not completely discarding manifold structure, and that is the reason why it has nicer results than your $D_1$ estimate. But on the other hand I think it is possible to obtain local bound and use a compactness argument to extend to the whole space, which will be interesting to know. [Fukushima-Oshima]Fukushima, Masatoshi, Yoichi Oshima, and Masayoshi Takeda. Dirichlet forms and symmetric Markov processes. Vol. 19. Walter de Gruyter, 2010.