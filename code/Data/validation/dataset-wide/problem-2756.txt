I would be grateful if someone could explain me this argument from Philosophy of Physics in plain English. I'm not sure how Albert arrives at his conclusion and I lack the mathematical skills to follow the original argument in the 1996 paper. It purports to show why although the configuration space of QM states is high-dimensional (3N-dimensional for N particles), it appears to us that particles, and hence everyday objects 'live' in a 3D space. Here is a description of it: [Albert, 1996] begins by considering an “NN-dimensional classical-mechanical configuration space, in which a single world-particle is ﬂoating around” (p. 280). He goes on to ask how such a situation could come to have the appearance of a collection of particles in a lower-dimensional space. He begins with a world-particle governed by a free Hamiltonian, whose trajectory would therefore be a straight line in the confguration space, traversed at constant speed. And note that the trajectory of a world-particle like this one can patently contain no suggestion whatever as to whether we are dealing here with a single material particle moving freely in an N-dimensional physical space, or (say) N/3 distinct material particles moving freely in the three-dimensional physical space, or N distinct particles moving in a one-dimensional physical space. Nothing about a trajectory like that (to put it slightly diﬀerently) can make it natural or make it plausible or make it reasonable or make it simple or make it elegant or make it any other desirable thing to suppose that any one of those possibilities, as opposed to any of the others I mentioned, or any one of the others I didn’t mention, actually obtains. (Albert 1996, p. 280) Albert concludes that the appearance of a low-dimensional space must depend on the existence of something other than a free Hamiltonian (i.e., on what are normally called interaction terms in the Hamiltonian), and he goes on to consider just what sort of Hamiltonian might make the postulation of a low-dimensional physical space plausible. The original argument is in: Albert D.Z. (1996) Elementary Quantum Metaphysics. In: Cushing J.T., Fine A., Goldstein S. (eds) Bohmian Mechanics and Quantum Theory: An Appraisal. Boston Studies in the Philosophy of Science, vol 184. Springer, Dordrecht The description I gave here is from: Maudlin, T. (2013) The nature of the Quantum State in Ney, Alyssa et al., 2013. The Wave Function: Essays on the Metaphysics of Quantum Mechanics, Oxford University Press. (page 141) 

Ιn the philosophy of quantum mechanics some authors who are wave function realists (i.e. who believe that quantum states have an ontology which is independent from our knowledge about them) claim that the "universal wave function" belongs to a unique ontological category which is nothing like anything else we experience. They make this claim in response to other views about the ontology of the wave function as a field, a law, a blob etc. since all these views face important difficulties. So I guess there isn't a yes or no answer to your question.It depends on what kind of metaphysics one adopts. I guess a better question would be: do we have reasons to believe that there are physical entities of a unique ontology? 

(As a side-note, the fact that the humans believe that it is possible to measure personhood by their external response to puzzles rather than philosophizing about the internal state of the AI means that the humans adhere to either Behaviorism or Functionalism. If you don't agree with two philosophical approaches, then it's possible that this entire question is moot and that it's impossible to build an AI that would reach "personhood" status.) 

The key aspect here is the existence of "fitness", to help measure how close someone is to reaching the desired goal (in which case, the entity that could be classified as a person). This "fitness" must be a quantifiable number, so if the AI is not "fully" a person, then how close to "personhood" the AI is (so that the evolutionary algorithm could decide which AIs to breed and which to 'replace'). In this specific case, the humans have chosen to measure the "fitness" of the AIs through the use of puzzles...if AIs have successfully solved all puzzles and defeated Elohim, then the AI is considered a person. If the humans already solved the hard questions of defining personhood, then why would Elohim be intentionally designed to be a person? Elohim's goal is to supervise the evolutionary algorithm...you don't need "personhood" for that. You don't even need intelligence for that. If you can come up with a way of measuring fitness, you can attempt to run an 'evolutionary algorithm' that will maximize this fitness. The Wikipedia article does not claim that Elohim is able to reprogram itself. So if it was not intentionally designed to be a person, it never had the chance of ever becoming a person. Now, Tenrec77 argued that the players "have every indication Elohim was a person, just as much as Soma, Milton, or even you and I". That may be true. There's no reason to assume that these future humans' attempts at measuring personhood would be at all sensible. It's possible that the humans built Elohim (the evolutionary algorithm) with several "features" such as the desire for self-preservation and the ability to feel emotions such as fear, claimed that Elohim was only "20% person", and simply used Elohim as a way to build "real, true, 100% persons". The humans has tried to define a complex topic such as "personhood" by creating an arbitrary boundary between a 'person' and 'not-a-person', and then used an evolutionary algorithm to try and reach that boundary. This idea of setting up arbitrary boundaries does seem very similar to a possible solution to the classical philosophical problem: the Paradox of the Heap... 

I have not played "The Talos Principle", so all my information came from the Wikipedia page of this game and your question. According to your question, humans developed an 'evolutionary algorithm' with the goal of developing some entity that could be classified as a "person". According to Wikipedia, an evolutionary algorithm works as follows: 

On the other hand, without an arbitrary definition of personhood, the evolutionary algorithm would not work. You need some way of measuring how close someone is to "personhood". In addition, the Wikipedia article for the "Talos Principle" does not mention anyone ever questioning the "use" of the evolutionary algorithm, so it's possible that humans have came to a consensus about how to define personhood. We may therefore wish to defer to their consensus. (Wikipedia calls this appeal to consensus as the "Group consensus" approach to solving the Paradox of the Heap.) So, back to your question. Is Elohim a person? My response depends on whether you trust the arbitrary definition that humanity has given to personhood. 

There is no free will. There is only chaos. That aside, believing in free will deterministically sets different boundaries for which actions can occur. 

Biologically, you were lying, because lying is a cognitive process where you exert effort to suppress the expression of your true beliefs. This a major topic in forensic psychology, specifically in the field of interrogation of suspects and the use of lying detectors. Even though a suspect passes a 100% accurate lie detector does not always mean the suspect has told the truth. A suspect can have false beliefs. Telling lies and telling the truth are both behaviors and therefore are the truth: the electrochemical processes exist. So, even a lie contains information representing the truth. Semantically, however, the truth can never be known. It could even have been that you were both lying and telling the truth at the same time (see Schrödinger's cat). Perhaps you saying that you did not have a cigarette accelerated toward the state of you not having a cigarette. The problem with beliefs is that they can stochastically be true or false. Consider Alice and Bob each throwing a die ten times of which the last round blindfolded. The first nine rounds Alice never threw a six while Bob always threw a six. Who would you believe: Alice saying she never threw a six or Bob saying he always threw a six? Who is lying harder? In that sense we are all blindfolded. It could even be so that you unconsciously held knowledge that you had lost your cigarette, so your knowledge was unsure. Note that knowledge is located not in just one part of the brain (or mind), or just in the body. Even the environment holds knowledge. For example, ants use pheromones to store knowledge in the environment. So while you were biologically lying you simply did not 'know' you were telling the truth. If you have said "I think I do not have cigarette." it would have been a lie, because you actually believed you had. Saying "I do not have a cigarette." is the safer lie. You did not include information about your mental state. That's why keeping silent and disclosing as little information as possible is the best strategy to not be caught telling a lie. Also, semantically if you only had 1 cigarette to begin with and you said "I don't have cigarettes." (i.e. more than one cigarette) you could have told the truth. Semantics are your best friend for creating a false belief without telling a lie. 

I guess for a more compatibilist view you'd could think of free will as relative concept where some entities have more 'choice' then others. In that case you could consider a salmon in two parallel worlds swimming up a river to lay eggs and die, and in the same worlds a person going to that river and one world fights back against a bear and in the other world pretends to be dead. Although both actions of the person are deterministically achieved, it has more variation than the actions of the salmon. I think you could describe free will as having more available programs that chaotically can be elicited. So, if there is free will (compatibilist view), for an action to be regarded as a free choice you would at least need more than one available path that can be chaotically elicited. And you would like the trigger to come from within the entity itself as it were being the agent making the 'choice'.