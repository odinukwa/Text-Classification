Could this be a double-hop delegation problem since apparently Kerberos is not being used? Every machine in the chain, from the workstation to the database needs to use Kerberos authentication. You mention visiting the page, which then queries the database. Which could be interpreted as a double-hop, although I do not fully understand the topology from the description. I interpreted the topology as: Workstation / IIS Server / SQL Server. The following post explains the technology: $URL$ Basically, NTLM is not able to go through the middle server to the next server because it does not have the password hash for NTLM to work with moving forward. Using Kerberos authentication does not require access to the password hash, using instead the session ticket to define itself. 

If I have interpreted this correctly, then likely most of the data in has to be dragged over to the and stored in a worktable in . It is extremely unlikely that the is processing to find the first 3 matching rows that come across the linked server. Instead it likely waits for the data to be present locally, before processing the worktable data to filtered by the and then choose the rows. The slowness is due to how much data is being moved between servers and by the likelihood that the [LocalServer] does not have access to the [RemoteServer]'s statistics. You might also read the Guidelines for Using Distributed Queries at: $URL$ In part, the document says: "To create the best query plans ..., the query processor must have data distribution statistics from the linked server." Using OPENQUERY to get the results from a remote server will often result in better performance. When using OPENQUERY, the query is sent to the REMOTE server and is executed there in order to return data to the local server. (You will notice that in OPENQUERY the query uses a three-part name, since it runs directly on the remote server.) Likewise, at times creating a stored procedure to run on [RemoteServer] would allow processing to happen on that server first, then return a more limited data set. After that data is returned, the code on the [LocalServer] can complete the process. Movement of data is costly to overall performance and moving data between servers is even more costly. 

You can certainly change a database to recovery model. The 'risk' is that you might need point in time restores, but you have said that is not a requirement. The huge log file is likely because the transaction log is not being backed up. If you switch to mode you will have to shrink the file. Like this: 

All password authentication for is at the server level for a login. Therefore the database will have users but does not have per database password for those users since the login is the authentication point. If you have enough rights, you can change the passwords of the logins and use a different password, but that password would be the same for all databases on your server. If you need a special login for a database, then you should create another login, e.g. or some such pattern that suits you. Then you grant that login rights to the database. 

Thanks for updating that you are using Microsoft SQL Server, which does support Full Text Indexing. (As do many other versions of SQL servers.) First of all your query is using a which means that if there is a prefix wildcard (e.g. ) the whole table has to be scanned and the contents of the column likewise scanned. That is the base cost of this query. Therefore, you need to investigate some other method of finding the partial words. For example, full text indexing (if available to you) may give you other options to use when finding %partial% words. Create a full text index on the table for the column. This can be done through SSMS or by a script. This will populate the keywords table in the FTI catalog tables. Example from: $URL$ 

Whenever you create or drop a you are changing the definition of your database landscape. So, yes I would consider that risky unless you have tight controls on when those steps can run. If a connection resets the it is changing that synonym for the Server and database not for the connection. This means that a set of "complex work" running in another process could wind up switching from your remote data (for example) to the local data without any warning. That would leave a mess to clean up. View this like you would view dropping and recreating tables in a running system. It may often work without anyone knowing, but it can indeed cause you problems. Of course, if another connection tries to define a that already exists it will raise an error such as: "There is already an object named 'SY_SUBJECTS' in the database." This will save you from switching context (but you must deal with the error) until after the synonym is dropped. (There is no function, which would apparently be much more dangerous.) Therefore if you try to change the without dropping it first, it will fail. If a code path runs it will succeed once it can acquire the needed locks. From MSDN: "References to synonyms are not schema-bound; therefore, you can drop a synonym at any time." So, the will not stop a running transaction, but will wait for the transaction to finish. 

I have not needed to overcome this problem on a SQL Server except when I make a mistake with permissions. I do not endorse the below, because I have not spent time on the problem. However, for what it is worth, Symantec has an article on this issue using SQL Server 2012 and Windows 2012 at: Cannot execute as the database principal because the principal “guest” does not exist Their answer is based on how your SQL Server is configured. 

I personally would use a tool (as indeed I already use) , such as are sold by Apex, Red Gate, Toad, and others. These could be used to script only the rows that have changed. That would make your looking for changes very simple indeed. If you are looking for a free tool you might examine $URL$ I have not personally used it, but some people find it useful. (There are doubtless other such tools in the public domain.) 

If you set these appropriately it should constain the memory usage from the cache. Here is a SQL Anywhere 12.0 page with those settings: $URL$ 

John Huang (MVP) outlines an approach in his blog post: $URL$ The basic approach is outlined by John Huang as: 

From the table definitions and joins shown above, you can see that you will never get the fastest sort. That means that you will be dependent on for the sort criteria you are using. However, if you design and use a Materialized View you will be able to use the fastest sort algorithm. 

In Relational Database terms efficiency revolves around avoiding the duplication of data. The 3 tables that you show in your question seem reasonable provided: 

A Wild Approach to SQL Server Full Text Wildcards Prefix wild cards are not (natively) supported by SQL Server versions for full text indexing up to the present. Perhaps this will arrive in the future. But with a little code you can get some interesting results. Full Text Indexing stores exist in SQL Server This stores each unique string in the SQL Server for that full text index. The unique strings become the corpus that can be searched for the full text index in use. Please note that features such as , , , etc may limit the strings returned. However, since the candidate tokens are stored in SQL Server you can harvest the strings that match your wildcard from the view. For example, the sample code I posted a few years ago in the MSDN forum: 

I do not believe that is a supported feature of SQL Server full text searching. That would require a wildcard resolution to words and then a thesaurus lookup of each matching word to gather the thesaurus terms. This basically maps to a pretty complex query: some one of a group of prefixed words is very near to some one of another group of prefixed words which all then go through a thesaurus lookup to provide even more words. Based on previous experience, that is just not supported. (I see online that you have asked this elsewhere in the last few months, but without any answers, so I hope that this helps.) I believe that you can create something useful for your query, but it probably requires externalizing the thesaurus entries by doing something like the following: 

Try setting the ProtectionLevel property on the Control Flow properties to either EncryptAllWithPassword or EncryptSensitiveWithPassword. You will need to supply a PackagePassword for this test as well. When you're done, rebuild and re-deploy the package. You will need to supply the password when executing the package. If this succeeds, the issue is with the Package ProtectionLevel. I recommend using Windows Authentication to connect to the server. (Chris Johnson indicated that using Windows Authentication was necessary.) 

We would appreciate some help in resolving this problem since (as noted above) the backups are all failing. 

One thing to remember is that an does not have to be generated by the property. Using has challenges, since as soon as rows are moved from the standby server to the current server (using of course) with an value higher than the current identity value it becomes the new current identity value. Which can complicate matters between the two servers. For other issues see: $URL$ You might find a sequence generator with NOCACHE or some other mechanism to generate on each machine gives you more control. For example, you can set ID ranges that can last for a long time for each server. For example: 

My experience is that 1,000,000 row insert will actually require more resources and take longer to complete than if you used a batch inserts. This could be implemented, as an example, into 100 inserts of 10,000 rows. This reduces the overhead of the batches being inserted and, if a batch fails, it is a smaller rollback. In any case, for SQL Server there is a bcp utility or the BULK INSERT command which could be used to do batch inserts. And, of course, you can also implement you own code for handling this approach. 

create an for the fulfillment of a subscription instance. The subscription could have a different scheme (e.g. SUB-Number-Date) for identifying the subscription than you would use for an for an (e.g INV:InvoiceNumber). [I would guess that the subscription would be marked as already paid.] Create a log with a header for the subscription and a log to track each fulfillment. That allows you to track the details of each time have their fulfilled. In the case that a subscription is cancelled early, it could be used to refund the customer the unused sum. 

If I understand you correctly you are looking to see who is blocking and why, not just gathering overall statistics. (Both approaches have real value, of course.) Since every event is transient, so it is not surprising that some values would not return what you expect. We have been using the approach outlined by Tony Rogerson quite a few years ago that uses Event Notifications. $URL$ This approach will capture the state of blocking based on a timing trigger. In the sample code Tony has configured to receive a notification when a block has lasted 10 seconds. (And every 10 seconds after until the block is removed.) This approach will provide a capture of the XML describing the blocking state at the moment of the event. This includes the blocked and the blocking process. And if none of the blocks are long enough to capture, then your event table will be very quiet. You can monitor for shorter or longer blocks, depending on your need. I generally run at 25 seconds, to cut out a lot of chatter from shorter blocking periods. New software releases can be a good reason to monitor blocking more closely. 

This is all about how NTLM and Kerberos work. There is a blog from a few years ago that offers some details on Linked Servers and NTLM or Kerberos authentication: $URL$ One edited quote is: "In a single-hop setting, windows NTLM authentication, which is available if all machines are windows, is sufficient for delegation; while in a double-hop setting, Kerberos authentication is necessary for the user’s credential to cross machine boundaries from the client to the linked server." Now, suppose that you do indeed have Kerberos configured properly and still are mystified by the behavior. In that case you might need to check into whether there is a duplicate SPN. If so that will need to be corrected. The following blog discusses duplicate SPNs and other Kerberos issues: $URL$ Note: Some people just fall back to using Named Pipes instead of TCP/IP, but this is not a recommended solution any more. Bottom line: Resolve the authentication delegation problem and everything should work. 

If the users are administrators, there is no way to keep them out. This could be sysadmin on the server, dbo on the database, etc. The easiest way to keep other (non-Administrator) logins out of your database is to never grant them rights or (as one part of your code suggests) drop any users that are not 'automacao' or some other needed service logins. Also, rather than cycling through every securable, you can use (assuming SQL Server 2005 and forward) is to grant at the database level. E.g.