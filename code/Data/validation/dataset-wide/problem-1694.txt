Have look at $URL$ This script will show you what drives you have and what are they connected to. I should give you pretty much everything apart of hardware RAID configuration. 

So no, you can't do JBOD for data (or the Wiki wasn't updated, Btrfs is under heavy development). I love Btrfs checksuming data and metadata, it will save you from a silent data corruption (well, it will inform you, that your file has been damaged but should prevent metadata corruption). Still, I wouldn't trust it with things I'd like to keep just yet. Personally I'd go with 3 disks and a software RAID 5. You can grow array later as you need. For a filesystem I'd choose something more mature, like ext4 or XFS. If you are more concerned about disks spinning up than about data persistence you could use LVM to create a logical volume out of both disks and make a Btrfs file system on top of that. LVM allows for a linear mode which may or may not cause just relevant drive to be spun up (if both data and metadata resides on the same spindle), but in case of a drive failure you loose all data. 

Yes, reliable synchronisation to a single clock source is possible. It's not reliable, because you have no redundancy, but that's the only problem. In your place I would just remove the local time source. On my machines I usually just use external ntpd servers. You need the local clock only if you need to provide service to clients (think: ntpd server for your internal network) when you have no connection to internet and cannot sync to other servers. 

If you connect through a switch it may keep its own arp cache, which is mudding things up. Try to connect directly (cable only) to the 2nd box. Another idea would be to run on the box when you try to connect to it (assuming you can get inside via serial console) and see if it receives any packets. And possibly the simplest explanation: some distributions bind IP address to MAC of NIC. If you changed hardware, you might have lost your network configuration. Can you ping the destination system? 

Display the currently selected initial program load (IPL) type (and logical key mode on some system types). Display the currently selected IPL speed override for the next IPL. 

As a workaround: redirect output to a file and mail it from inside of your cron script? As a debug tool: run from cron a series of scripts that just print a date, print their name, intended lenght of sleep, sleep for n minutes, print a date, and then quit. I would run them all at the same time, to have answer in 7 hours. If you test sleep times of length from 1h to 7 hours with, let's say, 10 mins increment, you'll know if it's a general time-out issue, or something to do just with your script. 

You can modify the root-side process to call after modifying a file. I do not think it is applicable to your case, because then you'd probably just call and and didn't set up sgid directory. You can modify for the root process. I think it would be better to modify umask just for this process, not for root environment, because sometimes root touches files other users shouldn't mess with, and it's usually better to remember to open access than to close gaps. Therefore, you could add in front of call to the binary that runs as root. 

Maybe you could define a new group, just for this sftp user, change group ownership of directory to that group and set setgid bit on the directory? This may or may not work, depending on how the application creates the directories. 

I will assume you are going to virtualize servers, not desktops, all right? Next I'm going to assume that you are going to use several ESX/ESXi servers to access your storage and have them managed by vCenter Server. When deciding on LUN size and the number of VMFS you are balancing several factors: performance, configuration flexibility, and resource utilisation, while bound by supported maximum configuration of your infrastructure. You could get the best performance with 1 VM to 1 LUN/VMFS mapping. There is no competition between machines on the same VMFS, no locking contention, each load is separated and all is goood. The problem is that you are going to manage an ungodly amount of LUNs, may hit supported maximum limits, face headaches with VMFS resizing and migration, have underutilized resources (those single percentage point free space on VMFS adds up) and generally create a thing that is not nice to manage. The other extreme is one big VMFS designated to host everything. You'll get best resources utilization that way, there will be no problem with deciding what do deploy where and problems with VMFS X being a hot spot, while VMFS Y is idling. The cost will be the aggregated performance. Why? Because of locking. When one ESX is writing to a given VMFS, other are locked away for the time it takes to complete IO and have to retry. This costs performance. Outside playground/test and development environments it is wrong approach to storage configuration. The accepted practice is to create datastores large enough to host a number of VMs, and divide the available storage space into appropriately sized chunks. What the number of VMs is depends on the VMs. You may want a single or a couple of critical production data bases on a VMFS, but allow three or four dozen of test and development machines onto the same datastore. The number of VMs per datastore also depends on your hardware (disk size, rpm, controllers cache, etc) and access patterns (for any given performance level you can host much more web servers on the same VMFS than mail servers). Smaller datastores have also one more advantage: they prevent you physically from cramming too many virtual machines per datastore. No amount of management pressure will fit an extra terabyte of virtual disks on a half-a-terabyte storage (at least until they hear about thin provisioning and deduplication). One more thing: When creating those datastores standardize on a single block size. It simplifies a lot of things later on, when you want to do something across datastores and see ugly "not compatible" errors. Update: DS3k will have active/passive controllers (i.e. any given LUN can be served either by controller A or B, accessing the LUN through the non-owning controller incurs performance penalty), so it will pay off to have an even number of LUNs, evenly distributed between controllers. I could imagine starting with 15 VMs/LUN with space to grow to 20 or so. 

First things first: a very well-done, systematic and thorough debugging, good job. On my RHEL 5.6 box I always get a return code of 1 if I try to kill a non-existing pid. I tried as both root and a non-privileged user, both with full path and a with just the command name. I also get only terse , with no elaborate error messages. It may be a good idea to run and see if somebody didn't replace with a new and improved version. Even if rpm verification says the file is pristine, I'd try renaming and copying over a binary from a working machine. If the file replacement helps and you don't uncover a legitimate the source of the change, then regardless of output of rpm verification I'd assume the machine was compromised. 

The boot off the SAS controller could come under "external device" or similar heading in the BIOS. You can also call IBM support, they should be able to tell you, how to set this up. The checklist to run: 

Quick-and-dirty solution: copy /etc/ssh/sshd_config and /etc/ssh/ssh_config from a server where it works. You could also strip all comments from the files on working and non-working server (e.g. run and compare results on both servers. Also look at the directory and file permissions, as Alex suggested.