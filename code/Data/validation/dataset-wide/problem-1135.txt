In SQL Server there is a separate thread that periodically (default 5 seconds, lower interval if a deadlock has just been detected) checks a list of waits for any cycles. I.e. it identifies the resource a thread is waiting for, then it finds the owner of that resource and recursively finds which resource that thread is in turn waiting for, thereby identifying threads that are waiting for each others resources. If a deadlock is found then a victim is chosen to be killed using this algorithm: 

Because when you're creating a full backup you're creating a backup of the data (as it is physically and as it is materialized from the log). Restoring it somewhere else restores it as data, not log. 

I'm using Red Gate SQL Compare to create a release script based on differences between SVN and a database. This results in a script containing a bunch of table- and procedure-changes and it works fine. However, one thing puzzles me, it's using transaction isolation level serializable. I know what it does to dml-statements, but I'm not sure what it means for ddl. Can someone enlighten me, perhaps with an example? 

The others are combinations of the documented flags, e.g. 24 is 16 and 8. This is a method for simulating optional parameters and is used in e.g. C iirc, the numbers and so on are structured like that because they correspond to binary values that combined in any unique way create a unique number. The function that accepts them then use bitmasking to extract each individual number. E.g. if you send the value 6 to a function, then we know that this is a combination of 4 and 2, to find this out using a bit mask we would do: 

Sometimes when inserting data in a table with many columns it could be useful to know which columns must be specified if the insert-statement shouldn't fail. I wrote this query to find out which columns are not nullable, identity, computed, timestamp and have no default value. 

Make sure to keep transactions as short as possible. E.g. don't show a login form after starting a transaction and wait for user input, instead gather all the info you need and then run the transaction. Use the lowest possible isolation level, e.g. don't set serializable when you just want to temporarily show some values to the user. Please note that setting correct isolation level is a science in itself and out of scope in this answer. If you are the victim of a deadlock, i.e. you get error# 1205, then re-run your transaction transparently to your user. Since the other, competing, transaction has now hopefully acquired the resources it was waiting for and finished, it is unlikey that you will encounter the same deadlock again. 

How will you do the update?Will you update based on user or some other criteria?Having multuiple tables is bad idea better have the tablke partitioned based on user as you have some 25 users so you will have 25 partitions.You can optimized updates using other techniques but as you mentioned data is growing I would suggest that you use partitioning so that droping and recreating indexes is easier. 

SQL server by default uses this feature but it can escalate based on the number of rows to be updated and again it would be better to know how updates happen and how many rows at a time.Also,check how your indexes are created?is allow_rowlocks is true or false? 

You are not comparing two diff plans but you are comparing two diff servers and thus you can expect that time will surely vary.Even if you run the query on same server you wont get the same CPU time. Thus if you want to compare the time on two servers or two queries on same server try to execute the query say 100's of time and then use the avg CPU time. Page reads are not based on the hardware but depends on your data and thus this counter will be same provided you have same data and indexes. I would suggest that you compare all 3 before deciding which is best query.But in most of cases reducing IO's will reduce others excpet few scenarios. 1.Page reads(both logical and disk reads). This is most important as reading a page is work done by CPU so if you reduce the IO then ultimately you will reduce workd done by CPU which in turn will reduce the CPU time as well as wait time.Also, you will be improving overall system perf as well e.g. Your query is doing say 10 IO's then it will use just 10 pages in memory or buffer but if it will use 1000 IO's then it will use 1000 pages in memory and thus will take extra 990 pages space in memory which could cause other useful pages to be thrown out of the memory and thus might cause physical IO's to read those pages.I assumed above that 10 and 1000 IOs were diff pages and not same pages.This comparison should be based on diff pages rather than number of IO's. 2.CPU Time.This could be quite high if you have CPU intensive tasks like calculations or sorts operations. (for parallel query it could be much more high than serial one and in that case direct comparison doesnt make much sense). 3.Wait Time.This could be high when you have too much IO's to do and some of them are coming from say disk (again this could be quite high in case of parallel and it doesnt reflect true wait times thus while comparing try to compare other waits). 

I found some explanation here, but this basically just says "There should not be a conflict", nothing on how to solve it. Could someone knowledgeable please explain to me what this error message actually means and how/if I should try to solve it? Update 12/12/2012 We couldn't find any reason for the setting of conflicting values for and . I changed them back to the default values, 0 for both. After the reconfiguration is set to 0 immediately, apparently requires a restart of the instance. 

It seems to me that using a job name in the schedule name prevents it from being re-used, so I've chosen to focus on a shorthand for the schedule contents. I'm now using the following naming convention: 

I find it hard to think of a good way to name them. So: what would be smart, consistent names to give to a job schedule? How do you name them? Why? 

This seems to work for now because we have lots of weekly and once-a-day jobs. I'm not utterly happy with the for schedules that occur every 4 weeks or monthly (what are we gonna do with the job that runs every 4th day of every month?), but it's not a big deal and the at least tells us to look to the description for more info. The daily jobs that run every xx minutes are not straightforward to name in the above manner - although for a once-every-5-minutes schedule is pretty telling. 

I encountered a production database of 12 GB with a log of 90 GB. Full and differential database backups are made with a third-party backup tool, but no one is backing up the transaction log so it got way out of hand in the past year. Now I want to fix the situation by first doing a full backup from SQL Server and then doing a transaction log backup. There is not much space on the server, so I'd like to know in advance how big the transaction log backup will be, approximately. I understand that the transaction log will only be truncated after backup, so I'm afraid the backup will be 90 GB as well and clogg the server. So far I can't find any information on backup sizes, can someone explain what will happen size-wise? Thanks in advance! Edit: SQL Server version is Standard Edition, and the database recovery model is full. 

By executing I can indeed force the configuration. But this is not a solution to the conflict mentioned in the error message. The values in are as follows: 

I have encountered many different names for job schedules in SQL Server, ranging from the job name where it is used to a short recap of the schedule itself. I would like to give a clear, descriptive and consistent name to new job schedules, so it could be re-used easily and it's easy to see what kind of schedule it is. Examples of encountered job schedule names: