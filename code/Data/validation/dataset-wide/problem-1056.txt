Connecting to the database via SSMS using my admin account. Right click the database in question and select new query. 

Here is the answer. Even though the job was configured to run via a PROXY Account, the SQL Server Agent is still responsible for the job. I had a look and the SQL Server agent was configured to run under the Local System Account on that server. So what I did is to put the agent to run under the superuser admin account and it worked as expected. Now in this case the fact that the job no longer needs a proxy since the Server Agent itself is running under the ultimate account. However I appreciate that this is not the right way moving forward (even though this isn't my server and I hope I never get to touch it again!) I will be advising the customer to reconfigure SQL so every service runs under a dedicated domain account (i.e. created solely for this purpose), which is the way it should be! Now what I would love to understand is why the job would run as long as the proxy account used for scheduling the job was logged into the SQL server! 

This only seems to happen with ONE table (all other tables are fine). So we tried recreating the table, but the same problem persists. Any idea of what might be causing this issue? 

This immediately made me think that a ODBC driver mismatch might be the issue, where SSIS is looking for the 32-bit drivers. So here is what I did: 

Log in with the user that has the User DSN entries that you with to convert to System DSN Open the Registry Editor and navigate to HKEY_LOCAL_CURRENT\SOFTWARE\Microsoft\ODBC\ Right-click ODBC.INI and select export to save it as a file on the desktop (or anywhere else you fancy) Open the .reg file with a text editor such as Notepad Replace the text HKEY_CURRENT_USER with HKEY_LOCAL_MACHINE. Save your changes Double click the .reg file and proceed to import it into the registry. 

If the value of the source varchar field is blank, then it will be set as NULL in the target tinyint field. For that I would normally use NULLIF. If the field contains anything that could not be cast as a tinyint value (e.g. punctuation, letters, or a value higher than 255) then the value will be set as 255 in the target tinyint field. The rationale behind this is that the tinyint field would never have a 255 value unless there is an error -- and it wouldn't be appropriate to have errors set as NULL or 0 because these are perfectly valid values and we wouldn't be able to differentiate between a valid entry and a mistake. 

You can take backup with compression on any version of SQL Server that supports backup compression and can restore to any version you like provided the version on which you are restoring does not gives error about enterprise features 

So from output you can see when Node 1 is primary replica read only connections will go to Node 2 as first preference and vice versa 

So Heap table is created with 50 records in it. Below is what fragmentation looks like after query DMV sys.dm_db_index_physical stats 

This depends on level of corruption and result produced why dbcc checkdb command. If you are lucky to run it it would produce result showing which pages(page ID) and index (CI,NCI) is corrupt actually its bit tricky and you might need expert to get around with this. Reading SQL Server errorlog using and event viewer would give you lot more information about corruption. Mostly a corrupt disk subsystem is cause of corrupted SQL Server database Moral: Every thing depends on level of corruption. Few corruption can be removed by just running repair_rebuild command. 

There is no absolute logic. To define a good index, in my terms " If an index speeds up your query and makes it faster, it is a good index and you can keep that index". You have sp_blitzindex from Brent Ozar which would point out unused and bad indexes. Also you have Jason Strates index Analysis script which would come handy. I guess both of above will do the job for you. But if you like other approach, below is what I follow sometimes. Regrading and there is no absolute value from which you can determine whether index is aptly utilized or not. But what I actually do is divide user reads by user writes and then multiply it by 100 to get the percentage. NOTE: This would always not be applicable I use it as "tie breaker". 

The account which user selected on Server Configuration page window ( during installation) is somehow not able to bring SQL Server database engine services online. Either is lacks privilege or it is corrupted. During installation of database engine services SQL Server tries to bring online database services as a internal process but due to startup account either corrupt or not having appropriate privileges it fails to do so and ultimately installation fails. Other reason is when installation fails first time due to some reason and user uninstall failed installation from add remove program, the un-installation leaves account in broken state so any further attempt to install flashes this error message. The reason can be also that SQL Server installation was successful in installing SQL Server and its services but due to some other limitation in system or other system related errors SQL Server is not able to come online. 

I have an SSIS job that gets data from a SharePoint 2010 list that runs perfectly from Visual Studio and also if I trigger it manually from the SSIS store, but not when scheduled as a job in the SQL Server DB Engine instance. I am doing everything using just one super user account called "spadmin". 

After that I disconnect from the database in SSMS and I reconnect using the new SQL account I just created. then I type in the following as a test: 

To my surprise, I can create tables (!?!) I can also truncate them and drop them (?!?) Not sure what the heck is going on here. Am I missing something? I need the user to be contained in this database and not to exist outside of it. 

This is as far as I managed to get unfortunately. My skill is limited where it gets to do an INSERT with a condition with the NOT EXIST condition. How can I make sure that I only insert rows in the xxx.MyTable where rows with the same primary key (period AND genusId AND subjectId AND waitingStageId) does not exist already? 

I am experiencing an issue with an SSIS job that connects to a Firebird DB using their ODBC driver (version 2.0.4.155). 

If I connect to the SQL server's SSIS store (not the DB engine) via SQL Server Management Studio (SSMS) and I right click on the job in question that is stored under "Stored Packages\MSDB" and choose to execute it, the job will run without any issues. This happens whether I am using the local SSMS installed on the SQL Server in question, or if I am using a SSMS installation on a remote workstation. However if I schedule the job through the same SQL's server database engine, the job will fail -- both on a schedule and if I try to run the job manually. Now here is the puzzling bit: The job will not fail if I have on the background a remote desktop connection into the SQL server with that super user account (i.e. spadmin) while I run the job. By on the background I mean that I am not doing anything on this remote desktop connection except login in with the super user account. When the job fails, I get the following "Bad Gateway" error (see end of post) that suggests the problem is accessing SharePoint. However since I can run this job via the SSIS store with the same account for which the job has been scheduled, there is no doubt that this job is capable of running from the SQL Server. Server build: 10.50.1617 I am going mental here. Any ideas of what the problem might be? Here is the full error message for completeness sake: 

If above works fine IMO your windows installer should not be corrupt but if you still face issue you should refer to This Microsoft Support Article which has details about how to proceed with corrupt installer. This error can also come if installation media is corrupt. So i would like you to copy setup again and start a fresh installation by right click on Setup file and select run as administrator Hope this helps 

No not as such, unless someone is doing testing I have hardly seen people manually doing a rollback. The rollback is specified in stored procedures or transactions in which you either want complete change to be entered or nothing at all. 

You can take differential backup also perhaps this would be very helpful if you have big database. anything that links LSN chains would do.See below link it has good information about switching between recovery models $URL$ 

One method people sometimes consider is to force ghost cleanup to clean everything by performing a table or index scan (thus queuing all the deleted records up for the ghost cleanup task). Although this is an alternative, it still uses the ghost cleanup task to do the work, and on a very busy system with a very large number of deletes (warning: generalization! :-) it can be much more efficient to remove the deleted-but-not-yet-reclaimed records using index reorganize or index rebuild. 

If your database were in full recovery you could have taken Transaction log backup. The good thing with transaction log backup is that Minimally, you must have created at least one full backup before you can create any log backups. After that, the transaction log can be backed up at any time unless the log is already being backed up. So you could have restored first full backup and all log backups, in serial order, taken after the full backup. This would hold true even if 3rd full backup is lost. You must read Understanding Backups in SQL Server there is a Backup myth buster series from PR related to backups you must also read that. Feel free to ask if any doubts I would add the comments in this answer 

You can manually take transaction log backup on any replicas of AG. For primary and secondary replica you can take normal T-log backup, only copy_only full backups are possible on secondary replica. I would suggest to take T-log backup from secondary replica this will put less load on primary replica and would also not affect peak time I/O. In any case log backup chain would NOT be disturbed just make sure that when you take T-Log backup you do not delete it but store it like you do for your normal backups so that you can use it in case of disaster. 

I am inserting values form a staging table in SQL Azure into a production table. Values in the staging table are varchar, while the target value in the production table is tinyint (it is an age field for humans, so I reckon that up to 255 will suffice ;) ) Problem is that due to a bug in the source system, we tend to get some values as -1 or -2 when they should be 0. This causes issues, since TINYINT only supports values form 0 to 255 (and also because this is factually wrong). I was wondering if there is some sort of case statement that I could use to convert values to 0 if they fall under 0, or perhaps if they fall outside the 0-255 range? 

I will be able to execute the package again as long as I am still logged in to the SQL server (I logged in to install the Firebird drivers). I can execute it even from a remove SQL Server Management Studio connection -- but as long as I do not log off from the server. If I log off from the server, then the SSIs job will no longer work (same error as before). This led me to think that this is not in fact a 32/64 bit mismatch, but perhaps some registry or environment variable is not being committed after I log off from thee server due to insufficient permissions (even though I was meant to be admin on that server). So for my next test: 

I have a date dimension table in which I need to add a new column in which I define the iteration of the day of the week within the month (2 for the second Mon/Tue/Wed/Thu/Fri/Sat/Sun etc). Is it possible to do this be making calculations solely on the date column of the table, which is of type 'date'? 

Is anyone aware of a SSAS (tabular) 2016 in which the number of records in a table are not fully shown in Excel? 

The solution was to change the processing in Visual Studio (VS) from Default to Full. The issue is described in detail here: $URL$ (see Cathy Dumas's reply under "Posted by Microsoft on 8/25/2011 at 2:02 PM") 

As part of an ETL process, I need to copy a varchar field in table1 (staging table) which is meant to contain an integer value to a tinyint field in table2 (production table). However I am having some issues when trying to combine the following two business requirements: