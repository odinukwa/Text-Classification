We are forced to perform some essential maintenance on one of our production environments. Instructions from the vendor state the recovery model of the database should be set to SIMPLE before the work is undertaken. I'm OK with that and the business has approved the change, but I'm wrestling with the best way forward in terms of disaster recovery planning. I want to ensure that the database can be restored up to the point of changing the recovery model, if needs be. We take a full database backup at midnight every night, and log backups every hour between 4am and 9pm (operational hours). This work will commence at 9am, just after a scheduled log backup, but of course I'm prepared for slippage into that hour based on various factors. I want to ensure we have a log backup in place to cover, lets say, 9.00am to 9.20am. EDIT: Users are being locked out of the system at 9.00am or thereabouts, however I can't rely on the last log backup alone, as there may be transactions slipping past 9.00am that need to be restored. It depends when Management push the button... What's the best course of action? Should I just take an ad-hoc Transaction Log backup which I can, if needs be, apply after the latest Full Backup and sequence of Log Backups? Or should I be looking at taking a Tail Log Backup, despite the fact I may never use it? 

I would be interested to hear what you have tried and what issues you have faced. You say you have SSMS 2012 installed, but it's not clear if you have a database engine in which to host a database and associated tables? If you do not have a database engine installed, you can install the SQL Express engine locally. Download it here. You are going to need to import the data from Excel into a table within an existing SQL Server database. To do this, you can use SSIS, if you have access to it, and create a simple ETL package involving a Source (Excel) and OLEDB Destination (SQL Server). You will need to create an existing table in your database, with appropriate fields and data-types, in which to 'dump' the data. Or, You can import the data through Management Studio directly, but you will need an existing database. Right click the database, click , and then follow the steps, selecting Excel Source and SQL Destination. 

I've been asked to register an existing dll file so that it can be referenced in an SSIS script component. I have a dim and distant memory of doing this using gacutil.exe so that was my initial go to. However, gacutil.exe is not present on the integration server, presumably because the full version Visual Studio is not installed there, only the Data Tools shell. I have also tried to copy the file into C:/windows/assembly as I read this would work, but nothing happens when I drop the file in it just cancels out. How can I do this without gacutil.exe? Is there a way or will have to install full VS SDK to do it. 

Is there a reason you don't want to failover? That would be the most common way. To do a rolling update and away you go. If you have a witness, disable it first. When you patch a mirror in this way the mirror will continue to function, to a fashion, and you'll have no application downtime. When a mirror partner is patched it will leave the mirroring session, but then rejoin it when it is complete - then re-sync. You can do it the way you described but you're going to have downtime when you patch the prod server and I'm guessing the patch when applied to Prod will trigger failover if that's how your mirror is configured. 

Now I know that it would be possible to mount a new disk with the same drive letter, but I'm looking for a way where I could safely bring the instance up sooner than that, given in a critical situation such as this, the datacenter may have a list of priorities as long as their arm and it could be several hours or worse before the new volume could be ready. Thanks all... 

Compared to running reports locally (in Visual Studio) or in a Native installation of Reporting Services (SQL Server 2012 SP3) we are experiencing very frustrating performance issues when rendering reports in a SharePoint 2010 web-part. In SharePoint we are using the SQL Server 2012 SP3 Reporting Services Add-In. We have two load balanced web servers, and two load balanced App servers in the web farm, and one database server which holds both the SharePoint databases and our 'Reporting' database. Fro mthe execution logs I can see that the Data Retrieval and Data Processing times match when the reports are executed both locally and in Native reporting services, however the rendering times can be wildly different. 1 second locally or native, to 30-40 seconds in SharePoint. I'm aware that in the past there has been documented problems with Report rendering when combining SharePoint 2010 and SQL Server 2008R2 - however, there's nothing to suggest this is still a widely known issue. Our servers are given a lot of resource, more than enough, 32GB each - so I don't think it's a resource issue. I also don't think we can accept the extra overhead, in terms of added HTTP calls, is the issue. Surely it cannot make the issue that bad? Are there any gotchas to be aware of when running this configuration? Why is running Report Services in integrated mode so bad for performance? 

Unfortunately PLE alone isn't going to tell you much. What's your baseline PLE? Does it dip then steadily rise again, or does it stay low for an extended amount of time? Are you getting any performance hits? My advice would be to build up a bigger picture of the issue by looking at other memory related performance measures. Page Life Expectancy is just one of a number of measures that relate to memory pressure. It needs to correlated with other memory counters, such as Buffer Cache Hit Ratio, Stolen Memory, Lazy Writes per Second, Memory Grants, CPU utilisation - to name but a few off the top of my head. Once you get an overall picture of a wider array of memory related counters you will be able to build up a picture of where the actual issue might lie - and branch off into different investigations. All you have with the Low PLE alert is exactly that, an alert that PLE has dipped below your set threshold in SCOM (What is your threshold by the way?) The below query will take a ten second sample and return some useful memory pressure counter, but is by no means extensive. 

I can only tell you from experience I have moved to this solution myself from what used to be a simple 'truncate SQL table and rebuild it' using an SSIS integration script for circa 1 million rows. In real time I have an improvement of 32mins (34 minutes for SSIS led truncate, rebuild package vs. 2 mins for Linked Server / MERGE statement) 

This happens because of the protection level set in the package properties. When you create a connection manager and test it, it doesn't save the username and password for runtime in the way you think it might. To fix, set the pacakage protection level to EncryptSensitiveWithPassword or EncryptSensitiveWithUserKey (recommended for dev projects) and this should work. At runtime, the connection credentials are encrypted, so if your protection level is something like DontSaveSensitive, you'll run into issues like this. EncryptSensitiveWithUserKey or EncryptSensitiveWithPassword will decrypt the credentials at runtime. To change the package protection level, follow the instructions in this link 

This is mainly theoretical, but I would like to have a documented list of options if this happens in the future. Today we had a critical disk error on the SAN, meaning the disk holding the Transaction Log files for one of our production instances crashed and initially looked like it was dead. Obviously, down go the instance, databases, and therefore the applications running on it. Our data center guys were busy working on the what, why and how's of the disk failure, meanwhile I was quickly coming up with a list of options for database recovery. Ok, so the Data Center guys recovered the disk. It was a VPLEX error, rather than a physical hardware fault. But what I found meanwhile was that I didn't have a great deal of options. The instance would not start, given that all log files for both Sys and User databases were out of reach. Would the instance have restarted if the Sys database log files were on a separate disk that was 'Up' I could access the .mdf files, so I had the option of copying them over to another server, then attaching them with new log files on another volume. Either that or restoring the databases to another server\instance using our fairly resilient backups. Either option meant work for the App guys, because all the applications and associated services would need to re pointed. I had another option of trashing the instance on the server and reinstalling it with the same instance name, then restoring all the databases from full ad log backups. Theoretically, this meant no work for the App team but had a serious time overhead for me (the only DBA). Am I missing any options here? I only started this job recently, and its fair to say documentation here is limited. I've been busy this past few months putting together inventories of our SQL Estate, looking at patching/upgrade gaps etc, and getting involved in several projects. I think its fair to say that a documented disaster recovery plan for just this type of scenario is now at the top of our hierarchy's agenda. Any help appreciated.