I am looking for a recent literature review of consumer demand theory and its empirical applications. I have tried common places (google scholar, jel and jep papers) but no success. Any help is really appreciated. 

Given what you said about mathematics, I would recommend Theory of Value by Gerard Debreu. This book is probably more than what you need, but it defines almost everything clearly. Debreu writes in the famous Bourbaki style so mathematics is rigorous enough. For macro it is hard to get everything in the same book. There are books very rigorous, but they generally covers techniques that are essential to do research. 

If I understood you correctly, $\Lambda(x_i'\hat\beta)$, where $\Lambda(x) = e^x / (1+e^x)$. You can use in both R and Stata. Try in Stata: 

Yes. If the idiosyncratic error is iid, xtreg (FE) with no options (ordinary se) is valid, but the ordinary se for FD, reg d.(y x), is invalid because the differenced error is serially correlated. Yes, it is. It's cluster-robust, which means OK regardless of the presence of within-group correlation. 

In general, if $c_1$ is endogenous, you need instruments for $c_1$ as well. Example: Even when $x_1$ is exogenous, if $c_1$ is endogenous and $x_1$ and $c_1$ are correlated, then OLS (which is the IV estimator using $x_1$ as IV for $x_1$) is generally inconsistent. Exceptions exist. For example, when exactly identified, if $(Ezc') (Ecc')^{-1} (Ecu)=0$, then your IV estimator is consistent, I guess. (Can derive this using standard technique involving the law of large numbers.) 

This may not be an appropriate question for this platform, but I couldn't think of any place that it fits. My concern is understanding about game theory papers. Whenever I read classical papers of Harsanyi, Aumann etc. I start to feel lost with unknown math. This happens a lot. I studied many of these classical concepts on a textbook level and I had no problem at all, but when it comes to the original paper I can easily lost the track because of mathematical sophistication. I know a little bit of real analysis (Baby Rudin first $4$ chapters), I have studied a little bit of topology (Munkres) I have also some background in set theory, statistics etc. So I am not a complete beginner. What I want is to understand, at least on a reasonable level, a technical game theory paper such as Harsanyi (1973a) and Harsanyi (1973b). Can anyone suggest me a study plan with books and such? It can start from zero level I have time and patience. Any advice will be greatly appreciated! 

I've looked at a few papers with endogenous growth models, but they look more like small instances of CGE (computable general equilibrium) models than simple production functions with restrictions and assumptions. 

Caution: Heavy citing "In his doctoral dissertation Revankar (1967) expounded his generalized production functions that permit variability to returns-to-scale as well as elasticity of substitution. In contrast with the production functions that (rather unrealistically) assume the same returns to scale at all levels of output, Zellner and Revankar (1969) found a procedure to generalize any given (neoclassical) production function with specified constant or variable elasticities of substitution such that the resulting production function retains its specification as to the elasticities of substitution all along but permits returns- to-scale to vary with the scale of output. Their Generalized Production Function (GPF) is given as \begin{equation} \ Pe^{\theta P} = c^h f^h \end{equation} where f is the basic function (e.g. Cobb-Douglas, CES, etc) as the object of generalization, c is the constant of integration and θ, h relate to parameters associated with the returns-to-scale function. In particular, if the Cobb-Douglas production function is generalized, we have \begin{equation} \ Pe ^{\theta P}= AK^{\rho\alpha} L^ {\rho (1- \alpha )} \end{equation}. This function is interesting from the viewpoint of estimation also. It has to be estimated so as to maximize the likelihood function since the Least Squares and Max Likelihood estimators of parameters do not coincide. The return to scale function is given by \begin{equation} \rho (P) = \rho /(1 + \theta P) \end{equation} . Depending on θ the sign of θ , the returns-to-scale function monotonically increases or decreases with increase in P. However, as we know, the returns to scale first increases with output, remains more or less constant in a domain and then begins falling. This fact is not captured by the Zellner-Revankar function since it gives us a linear returns-to-scale function." from A Brief History of Production Functions by SK Mishra 2007 So, I would guess, the \begin{equation} \alpha \end{equation} parameter in your log-linearized function would have something to do with returns-to-scale. Regarding estimation, I'm way too much of a newb. But this might be something at least similar to what you are looking for: $URL$ Variable Elasticity of Substitution and Economic Growth: Theory and Evidence by Giannis Karagiannis, Theodore Palivos, and Chris Papageorgiou, 2005 

I am reading about the higher order beliefs. Before getting into the formal definitions, I will define some common terminology which I will need for the formal definitions. 

Now according to this definition for $E\subset \Sigma_{-i}$ we have $marg(b_i^2;\Sigma_{-i})(E)=b_i^1(E)$. I try to understand this definition. So I tried to consider a game in which there are two players $i$ and $j$ and two actions for each player. So $$ \Sigma_i=\{(p,1-p):p\in[0,1]\}\quad \Sigma_j=\{(q,1-q):q\in[0,1]\} $$ and $b_i^1\in\Delta(\Sigma_j)$ and $b_i^2\in\Delta(\Sigma_j\times B_j^1)$. So $b_i^1$ is a probability measure over $q$, and $b_i^2$ is a joint probability measure over $q$ and the first order beliefs of $j$. Suppose $E$ is the collection of $(q,1-q)$ such that $q\leq 0.5$, which is a subset of $\Sigma_j$. I could not convince myself why $marg(b_i^2;\Sigma_j)=b_i^1(E)$. Apologies for the length of the question and any help is greatly appreciated. 

It is conventional to report both the total number of observations (1000 in your case) and the number of groups (500 in your case). If you want or are allowed to report only one number as the number of observations, it is suitable to report 1000 for POLS because POLS doesn't mind the fact that it is a panel data set. For FD, it is slightly confusing to me to say that the number of observations is 500. I would probably ask again of the writer what 500 means. 

Above, we assumed that $E_t(x_{t+j})$ converges as $j\to\infty$. Now we have to show (or check) it. The simplest way would be to write $$ \pmatrix{x_t\\ x_{t-1}} = \pmatrix{1+\rho & -\rho\\ 1 & 0} \pmatrix{x_{t-1}\\ x_{t-2}} + \pmatrix{\epsilon_t\\ 0}, $$ that is $W_t = A W_{t-1} + \xi_t$, where $W_t = (x_t, x_{t-1})'$, $\xi_t = (\epsilon_t,0)'$ and $A$ is the $2\times 2$ matrix on the right-hand side. You then have $W_{t+j} = A^j W_t + \sum_{k=0}^{j-1} A^k \xi_{t+j-k}$, from which $E_t (W_{t+j}) = A^j W_t$. Thus, $E_t(x_{t+j})$ converges if $A^j$ converges. The two eigenvalues of $A$ are 1 and $\rho$, which are real and no greater than unity in magnitude. And the associated two eigenvectors are both real (one proportional to $(1,1)'$ and the other to $(\rho,1)'$). These suffice. (Because $A = V\Lambda V^{-1}$, where $\Lambda$ is the diagonal matrix of 1 and $\rho$, and $V$ is the matrix of eigenvectors, we have $A^j = V \Lambda^j V^{-1}$, which converges as $j\to\infty$ because $\Lambda^j$ converges.) 

A really good textbook for introductory econometrics for me was Essentials of Econometrics by Damodar Gujarati. The problem is, it is quite old (mine was from 1992), so it does not even mention some of the more recent breakthroughs like cointegration analysis. But it really gets you into the subject. I would say that for me at least, study of econometrics looks relatively useful and interesting precisely because I have moderate knowledge of different economic theories, so I know how and where to put those methods to good use (to test wether models based on one or other theory fits the real world data better). Other than that, stats and math is also important to grasp the deeper aspects of econometric methods. 

Now, if $E_t(x_{t+j})$ converges as $j\to\infty$, then $\lim_{j\to\infty} E_t(x_{t+j}) = \lim_{j\to\infty} E_t (x_{t+j-1})$, and thus $$ \lim_{j\to\infty} E_t(x_{t+j} - \rho x_{t+j-1}) = (1-\rho) \lim_{j\to\infty} E_t(x_{t+j}) = x_t - \rho x_{t-1}. $$ As a result, $\lim_{j\to\infty} E_t(a_{t+j}) = \lim E_t(x_{t+j}) + \lim E_t(z_{t+j}) = \frac{1}{1-\rho} (x_t-\rho x_{t-1}) + 0$. 

An LPM is a model in which the probability of the binary dependent variable having a particular value is linear in parameters. For example, if $y$ is a 0/1-valued variable and $P(y=1)=x\beta$, it (the equation for the probability) is called a linear probability model. Likewise, if $y \in \{ -1, 1 \}$ and $P(y=-1) = x\beta$, it’s a linear probability model. Or if $y$ is an apple/orange-valued variable and you believe that the probability of $y$ being an apple is $x\beta$, your belief is said to be a linear probability model. Zero/one mean nothing real here; they are only labels. You can label them as man/woman instead if you want. You lose no information by this relabeling as long as you remember the new labels. Also, as Papadopoulus said, it is a model, not an estimator, though we just understand an “LPM estimator” casually as an estimator of parameters in an LPM. 

I know that the linearization of a CES (constant elasticity of substitution) funtion is a bit complicated. There is even an R package dedicated just for that - the econometric estimation and calculation of a CES function (of wich Cobb-Douglas, Leontief and linear production functions are special cases) - micEconCES. However, I can't find anything about the econometric estimation of a VES (variable elasticity of substitution) production function. 

Is it (linearisation and/or econometric estimation) possible at all, or do the characteristics of the function not permit linearisation? 

I have tried using the obvious choice - the Stargazer package in R. But, as far as I know, Stargazer does not support packages like 'vars' (for VAR and VECM models, the Johansen procedure, etc...) and 'urca' (for unit root tests). It's a shame, because linear models are represented very nicely in tables with Stargazer. What do you use to simplify the table creation process? 

II. Tobit models: If some $y$ are exactly 0 or 1, you can try Tobit models ( in Stata). Remember that normality is assumed for the error term before censoring. Also, using Tobit models means that "I think $y$ could be bigger than 1 (smaller than 0) if not censored." 

That is natural and you are not missing anything. Let $y=\alpha + \beta z + u$. Your prediction of $y$ given $z=1$ is $\hat{y} = a + b$, where $a$ and $b$ are the OLS estimates. The prediction error ($y - \hat{y}$) for $z=1$ is, thus, $(\alpha - a) + (\beta-b) + u$, which is involved with $b$. On the other hand, for $z=0$, the prediction of $y$ is $a$ and the prediction error is $(\alpha-a) + u$, which has nothing to do with $b$. As you said, the former depends on $b$, while the latter does not. (Perhaps it would help to remember that the value of $z$ is given for the prediction, and thus the prediction intervals depend on the value of $z$.) (Calculation of the prediction intervals:) You can calculate the prediction intervals using the formula $(a+b) \pm se((\alpha-a)+(\beta-b)+u) \cdot \textit{critical value}$ for $z=1$, and the formula $a \pm se((\alpha-a)+u) \cdot \textit{critical value}$ for $z=0$. which is fine. Note that these two prediction intervals can also be written as $(a+b) \pm se(a+b+u) \cdot cv$ and $a\pm se(a+u) \cdot cv$, respectively, because $\alpha$ and $\beta$ are constant (nonrandom). How to estimate the standard errors can be found in Wooldridge's textbook (the "Prediction and Residual Analysis" section; 6.4 in 5ed).