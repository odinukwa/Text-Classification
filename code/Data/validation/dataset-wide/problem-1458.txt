Nice and well documented code. Some ideas: In this case you could use , as you create a vector only to thaw it. This will save you copying it during , but of course you must be careful and aware of the consequences. If using arrays, another option is . When converting back to a list, for arrays you could either use which again saves copying the entire array (as opposed to using ). Or instead of followed by , use . For vectors, there is too. While using a counter for the middle part certainly works, it's a bit imperative style for Haskell. The more common approach would be to have a recursive function where you pass it as an argument and return it back. Somewhat related is looping using over a list. Probably passing it as an argument would be faster, as producing/consuming the list is likely to cause memory (de)allocation inside the loop, although less idiomatic. So it depends on what are your goals. Finally, declaring functions using inside is indeed often convenient, but can impact readability, if they're long. I'd prefer to declare them separately so that the main function is just "thaw - go - freeze" and then the definition of follows (or precedes). If the helper functions aren't recursive (that is, the recursion is hidden inside), they'll be properly inlined as needed by the compiler. Also is equivalent to or just . Here is a possible variant with the ideas above, and a few minor more: 

It might be interesting to add a instance, although I'm not sure how well this would play with the sign bit. Other than that it makes a lot of sense to contain the actual bit manipulation, to from/toInteger or elsewhere as much as possible. 

If you want to compare by Y in the opposite order, you can write . Now you don't need at all, and splitting them becomes simpler: 

At every step you compute , which is O(n). And at every step you also append to from right, which is also O(n). 

For the former, repeated sorting: Let's consider, how would a human create such sequence. Probably she would keep two lists. One would some part of the sequence, and the other would be candidates yet to be added to the sequence. At each step, the smallest candidate n is appended, and 2*n+1 and 3*n+1 added to the candidates. Translating this into code yields 

Also I'd suggest you to have a look at various balanced tree implementations (unless you want to explore it yourself, which is a good exercise). On the other hand, if your aim is only to have a balanced tree, without the need of modifying it later, you could somewhat relax your requirement, and just require that the height of the tree is at most 1+log{2} n. Then you'd be able to create the tree incrementally, without knowing the total length of the input list: at a particular point, the algorithm tries to fill a node of height h. If it's full and there are more incoming data, it starts to build another node of height h+1. So the left subtree will be always a full binary tree. From the syntactical point of view, it's very good that you annotate all functions with their types, the only thing I'd improve is to have a fixed line length limit to increase readability. 

Somewhat old question, but still it's a pity it hasn't been answered :). Putting aside the asymptotically better solution suggested in the comments, and focusing just on the code: Pluses: Top-level types for all funtcions, that's very good, as well as comments. The code shows good understanding of various Haskell functions and idioms. My most general comment is that there is disproportion between code verbosity and complexity. Usually more complex code should be more verbose and more descriptive. Visually, most of the code is just simple read/write/swap ST operations, but the important part is squeezed into not-so-easy-to-understand and uncommon combinations of folds/scans/sequence/>=>/>>= etc. I'd probably separate the simple parts (read/swap) into helper functions and expand/simplify the complex parts a bit. Function somewhat mixes pure and mutable approaches. While it mutates its first argument, it keeps the scalar product pure and passes it as an argument in and out. This is somewhat confusing and complicates the computation of the minimum ( with ). If the product were a ST variable too, the code gets simpler (untested): 

While the output should be a balanced tree, it seems it's not required to be ordered in any way (at least the given example there isn't). So it's definitely not a heap. And the solution is required to use , which yours doesn't satisfy. You need to split the task into two parts: 

A tiny nit, unrelated to the main problem: Haddock comments allow you to generate nice documentation. 

where waits until the underlying computation finishes and returns the reuslt. Then let all your methods return instances of asynchronously. If you want a synchronous invocation, just call 

Otherwise nice program! I also like that you meaningfully named variables, this really helps reading the code. 

where we start with an infinite list of zeros. This is safe, as we always only access a finite number of elements of this starting list. While the algorithm remains the same, avoiding explicit indexing makes it more readable. 

In addition to the other great answers, I'd like to point out a few details. First, in your function you skip too far, you need to go through elements one by one. Your code applied to passes. Rather you need to pattern match like this: 

indexing with in the right part is wrong. I'd suggest you to create a function that converts a list into a (ideally a blanced one), and then test if indexing into the list produces the same result as indexing into the . Also the implementation of is missing, but this is a crucial part of the code, without it it can't be really reviewed. I'd also suggest to implement other operations on s, in particular creating a singleton list and concatenation, perhaps also prepending/appending an element to a list. Finally, you're using a instance for tagging the tree. This is in general a good idea, but the assignment doesn't say anythong about monoids, sobe sure to understand how the monoid there should be used. You never use the fact that a monoid in your code. If you're unsure, I'd suggest to forget about monoids at this point and use just the provided data type. 

Their combination is a module with no input or output, and stepping them prints the counter each time: 

This one reads one or more digits, consumes if there is one, and converts the digits into a number. (Combinator runs two actions sequentially, but keeps only the result of the first one.) 

If could be further polished by using helper functions from or a similar library, and by extending the functions, for which fills in, to work within any (if that's possible). 

Or, you can also avoid doing recursive operations yourself and instead use existing list functions. Some hints on alternative solutions (which could be good exercises): 

Also note that can be simplified to . Finally, I'd suggest you to express your property in such a way that you get more detailed information about what's wrong in a failing test. This can be accomplished using instead of as the result type and using combinators in Test.QuickCheck.Property: 

So I'd suggest to split the function into even smaller ones, each targeting one of these problems. While the resulting code is larger, I believe it's easier to comprehend and more maintainable in the long run: 

Since all suggested algorithms are O(n), it's hard to decide which one will be more efficient just by reasoning. We'd have to do some measurements, for example using the criterion package. 

Nit: The order of functions is somewhat unnatural: uses , uses and . There is no visible order in this. One good option is that functions are always defined before being used (with the obvious exception of mutually-recursive functions). Or the other way around - the main/exported functions are first and helper functions follow. Or, separate functions into (possibly nested) sections, then ordering of functions doesn't really matter, ordering of sections becomes important. 

Interestingly, the inner type is already a monad, corresponding to , or alternatively . Now standard monadic should correspond to and to the identity matcher. And also to . Such an interface will give your matchers all the power monads offer with very little effort. 

The definition of the main data type is basically unchanged. One important change here is the color of . Originally it was black, but actually it needs to be red in order to have consistent color change in the operation, see below. 

Concerning , the main problem is that the function isn't total. This means that for it will fail badly and unconditionally. If you declare the functions from and as named functions in a (or block), you will be able to check for as well in pattern matching. Also, you can combine several constructors in a pattern. There are two options how to fix : Either return some for , or remove such entries. I'll show an example for the latter option using :