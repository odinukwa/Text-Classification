"Know" has two meanings: be consciously aware, and have some account of that fact in your cognitive apparatus. And then, "things like that" really needs another thing like that plus a third thing that is not like that. I'll supply some examples for you – "boil" and "bullion" (also like), and "dog" / "dogs" (not the same kind of thing). The word "Jupiter" might be known to a reasonable number of under-10 children, clearly not a majority, and all would be educated. Probably only a handful know "Jovian". So virtually none of the set you're interested in know that relationship. Likewise with "boil" and "bullion", where "boil" is probably known to most English speaking children between ages 4 and 10 (but not "bullion"). Virtually all children know "dog" and "dogs", and connect them instantly. Very few children would understand the technical terms, so you can't ask if Jovian is the adjectival form of Jupiter or probably even if "dogs" is the plural of "dog". Most people don't talk about "adjectival forms", "plurals", so uneducated English speaking children at age 10 probably have no idea what a plural is. Even educated ones struggle, since grammar isn't taught so much. Whether or not this is part of one's subconscious linguistic ability is hard to tell in the cases that you're interested in. There is a classical test, the wug test, that shows that children have in fact internalized regular plural formation. As far as I know, nobody has even attempted to do the same test with words like "Jovian" and "sangiovese" or "Jupiter". 

First, there are few sounds that people can make with only their mouths (to be more precise, the oral cavity). Clicks, yes, but not breathing, because breathing also involves the lungs. Let's suppose that you mean "the body parts used for speech", but not necessarily in the way that they are used in speech. (Ingressive lung air uses the anatomy, but in a non-speech like fashion). In other words, sounds produce in and by the vocal tract. When applied to language, the unscientific concept "sound" gives way to a more scientific concept "phoneme", the reason being that English (for example) has a linguistically significant constant unit /p/, and there are zillions of actual distinct sounds (acoustic waveforms) corresponding to productions of /p/. As part of a linguistic system, it makes sense to say that these various instances are "the same thing". There is no conventional sameness to the set of non-linguistic vocal tract sounds. To take an example, put your tongue in position for [s], but sharply inhale rather than blow. People make this sound all the time. And they may round the lips to varying degrees. Each of those productions is "a sound". It only makes sense to talk about counting and possibilities when you have a fixed correlate, e.g. associating the sound to something meaningful like a snake or a block of ice. As you can see, there can be no "list" because the set is infinite. All you can do is explore your anatomy and see what new sounds you can come up with. This question would be best asked at the Comedians SE, if it existed, because many of them make their money discovering new face sounds. There are thousands of general possibilities, leaving aside subtle nuances contributes by narrowing a passage by a couple of millimeters. 

Before trying to figure out how to date a word, you need to first decide how to detect a specific word. Taking the example of "iron", first we have to decide whether to believe the Wiktionary story, versus Calvert Watkins' story (is-(e)ro "powerful, holy"). Either way, it's pretty clear that the word hasn't always meant "iron", so then you have to decide whether you set the birth of the word at the time when the sound-sequence took on the meaning "iron", or do you focus on the sound sequence and discard subtleties of meaning change? If you want a stricter semantic relationship, i.e. you want to pin it down to the point where the word "iron" took on the meaning ferrum, then words will have a much later "origin". You will also need to sharpen up the theory of meaning-sameness, because the word "chicken" has gone through a number of subtle meaning-changes starting with "young fowl", then specifically "young chicken" and finally "any chicken". Except that "chicken" also includes the meaning "cowardly", thus you need to be clear on whether you are looking for all of the current meanings, or just some of them. On the other side of the equation is the possibility of tracing a word-qua-pronunciation back in time, so Modern "iron", Old English iren from isærn from isarnan back to 9perhaps is-(e)ro. The problem is that most words ultimately originate from a language for which there are no records and we generally can't say much about when it was spoken, though we can at least know that Proto Indo-European was spoken further in the past than 4,000 years ago. That doesn't mean that the origin of the word is whenever PIE was spoken, it means that the origin of the word was at least that far back, and possibly much further back, it's just that we can't point to specific evidence proving that it had to be older than that. Of the two approaches, the more semantically-restricted approach is "more practical" because then there is at least a potential empirical question. It makes sense to ask when the word "dog" came to mean dog (a general term, not a specific breed if that is what it used to mean), or when the meaning of "meat" changed to its current "flesh" meaning. 

My answer to the "is there any research" question is "who knows?": you can wait and see if anyone refers to a scientific study, but I suspect that no such study exists. The answer to the question whether knowledge of some other relevant language always improves fluency of the target language is "no". Outside of mathematical physics, most questions with "always" in them have the answer "no". In this case, the reason is simply that the psychology of second language learning is extremely idiosyncratic, so even strong tendencies can be falsified. You should start with questions like, "Does learning Latin help in learning {Portuguese, French, Romanian}", or (distinctly) "Does learning Arabic help in learning {Swahili, Turkish, Farsi, Urdu}". In the first case, you would be asking whether learning an actual antecedent language helps in learning the LIQ. Neither Latin or Greek are actual antecedents of English, and the influence of those languages is like the influence of Arabic on Swahili. German and Dutch are also not ancestral languages: only Middle English and Old English are ancestral languages of English. If you invent a very open-ended definition of "ancestral" language where French or Norwegian are ancestral languages since English does contain some words of actual (modern) French and Norwegian (e.g. milieu, lutefisk), then most world languages would be "ancestral languages". An "ancestral language" would be an actual historically antecedent state of the LIQ, ergo one no longer spoken. There is no denying that knowledge of Arabic can help knowledge of Swahili, Turkish etc. in a minor fashion, but this is a many-way street -- knowledge of one helps acquisition of any other (contact-style) relative, to some minor extent. My knowledge of Arabic has been improved a tad, by learning Swahili -- I can Arabicize words that I learned initially in Swahili, and can make fair guesses about what the Arabic word will be, even though in fact those words went from Arabic to Swahili and not the reverse. 

The circumstances under which case marking is "really necessary" can best be summarizes as "when otherwise most sentences become highly ambiguous and communication becomes impossible". In languages with fairly fixed word order and with prepositions / postpositions signalling certain grammatical functions, case marking could be dispensed with: in a strict SVO language "child sold cat" is unambiguous. In a strict SOV language "child cat sold" is likewise unambiguous. If you have free word order where all word orders are possible and equally likely, you can't rely on word order, so having case marking to distinguish subjects, objects, recipients and so on would be a good choice, communicatively speaking. Even so, given free word order and no case marking, there is little confusion over the meaning "cut child meat knife". Case marking (in the normal understanding of the concept) is not even always required for free word order languages. In Khoekhoe, "basic" word order is SVO, but pretty much all word orders are possible. There is gender marking in the language: and the language uses gender marking to disambiguate. The subject has just it's own lexical gender; if you scramble an SOV sentence, the non-subjects have to also bear the gender marking of the subject (so any double marked NP is not subject). Finally, a number of Bantu languages are pretty flexible with word order, but they have subject and sometimes object agreement. By matching the agreement properties of the verb, you can tell that ng'ombe akagura mwana means "The child (mwana) bought a cow" and ng'ombe ikagura mwana means "The cow (ng'ombe) bought a child", bizarre as the latter may be. 

As far as I know, there have not been any systematic efforts to replace an established writing system with a phonetic-symbol based one. There have been numerous cases where the first writing system for a language is based on or more like a prevailing phonetic transcription, for example Lushootseed, Somali, Shona, Saami, but except for Lushootseed, these have been replaced with a system that reduces the number of special symbols in favor of di- and tri-graphs and less phonetic detail. There have also been politically-motivated script changes which have incidentally swept away less-phonetic aspects of a previous script, such as the replacement of traditional Mongolian (Sogdian-based) script with Cyrillic in 1946, where traditional Mongolian reflects historical pronuciations similar to the way that English spelling reflects older pronunciation. The (current) North Saami system is revealing, as to motivations for opposing phoneticization of writing systems. There is considerable variability in the pronunciation of words in the language, and a too-phonetic writing system either means that there is no standard spelling for most words, or else most people have to learn an arbitrary dialect that they don't speak, and notions like "most widely spoken" aren't meaningful (they are not traditionally city-dwellers). The current system give much less detail than was present in older transcription systems, which means it is consistent with the speech of many more people and for a given dialect, you can usually figure out how to spell and pronounce, if you know the language. There remain some aspects of the language which can pose problems, such as the spelling a, á in neutralizing dialects, whether to write s vs. ŧ (you memorize which words have ŧ), or the nj, ŋ distinction in neutralizing dialects. The de-phoneticization of spelling has been a success precisely because it is less tied to specific pronunciations, and a unified writing system (hence literature) is thus possible. 

I have an ineffable feeling that there is a pragmatic difference between "each N" and "every N", which has to do with evaluating the individuals denoted by "each N" one at a time, vs. evaluating them en masse with "every N". This may be an illusion, however. My question is whether there is any clear semantic / pragmatic difference between "each" and "every" as NP modifiers. There are obscure syntactic differences between "each" and "every" as NP modifiers, such as the fact that you can say "almost every linguist" but not *"almost each linguist", with an adverb preceding the modifier: got that, not considering that context. I'm kicking out plural NPs ("{every / each} 4 days"; "{every / each} 4 books") as outside the realm of investigation. Bare "each" and "every" are clearly different: only looking at these as NP modifiers. I am trying to develop an operational test to distinguish between two words in another language that are translated (consistently by speakers, who have a reasonable command of British-based English) as "each" vs. "every". 

In some dialects of English, there seems to be a clear(er) difference between past tense verbs with the auxiliary have as in “I have eaten the pie”, and those without, as in “I ate the pie”. The only distinction that I know of (can discern for my dialect) regarding use of “have” vs. Ø (or “do” in relevant constructions) comes out in the contrast “I have eaten pie” meaning “I have, at some point in life, eaten pie”, vs. “I ate pie” which doesn’t have the connotation of non-specific time. “Have” is incompatible with a time expression in the same S that identifies a specific time (*“I have eaten pie yesterday/last Christmas”) though it is compatible with such an expression in the discourse (“I have eaten pie, in fact it was yesterday”). Apart from that, have+V seems to be interchangeable with bare V. Is there any paper which lays out the semantics and pragmatics of bare verb vs. have+verb, especially in UK-derived dialects of English? 

There is no machine translation at all for Af Maay or Mushunguli, and hospitals completely have to rely on human translators (sometimes to the detriment of the patient). A major contributing factor to inadequacies in machine translation is the lack of adequate study of one of the desired languages. Major languages with 25 million speakers or more tend to be somewhat better studied, compared to the thousands of virtually unknown languages with 1000 or fewer speakers, so under a "social need" approach to the question, there is little social need for Norwegian-Chukchi translation, and thus it may be irrelevant if we don't know the Chukchi verb for "distill", or how to put it in the 3rd person dual perfective (if indeed there is such a thing in the language). The idea of covering "Asian languages" by 2045 is seriously mistaken. There is virtually no chance of having machine translation for Lahu, Hawrami, Laz, Woi, Lamet and Ket by 2045, or 2145 (whether or not there will be speakers of the language beyond that point is hard to say). However, it may be that by that time, translation of Japanese, Korean and Mandarin will progress to the point that machine translation is useful. (In contrast, current Finnish and Turkish translations are not very useful). Machine translation comes in a number of forms, one being text-to-text mapping, and the other also encompassing spoken language. Speech-based translations will be absolutely required for the many languages that have no written form. Real-time computational translation of speech is even more pie in the sky, although I am surprised at how good it has gotten for some languages. There are notable exceptions, but that language only have some 300,000 speakers. 

Phones are a "thing" because they were the first decent method of objectively and accurately recording unwritten languages (in the 19th century). Back then, if you heard a Lushootseed speaker translate English "The bear ate the salmon", the standard practice was to guess with untrained English-speaker ears that the person said "Oo uhshluh tube tea skuchicuss uh tea spots". Because tape recorders and even wax cylinder recorders did not exist, there was no decent way to say how the utterance was actually pronounced. "Phones" are simply standardized transcriptional symbols that have a (relatively) fixed meaning in terms of what appropriately-trained people hear (i.e 'ʃʲ' should sound a particular way, no matter what the language is). When you start getting into whether a particular sound is "important" in a language, that is where you get into phonology and "phonemes". The concept of "phone" became systematic in linguistic practice in the 30's onwards, during a period of methodological frenzy. The idea was that you have to first start by writing down what was actually said (given knowledge of the symbols and competence in distinguishing "ʉ" from "y"), and this is recording in terms of "phones". This is basically the conversion of a continuous physical signal into a discrete symbolic representation. After you've got this reduction of speech to phones, you can analyze the distribution; you will record for English such phones as [t, tʰ, ɾ, t˺, t˟], and yet you notice that these sounds never appear in the same context (as happens with "tie" versus "die"). Thus phones are grouped together into sets: a set of phones that never is the basis for distinguishing utterances (very roughly) is a "phoneme". Choices of phoneme is what makes different words different (in sound). Hence the view that phonemes are about perception. The main problem with saying that we perceive in terms of phonemes is that this is simply false, or requires a special definition of "perceive". There is a classroom stunt that is often pulled on students in phonetics-phonology classes, where you record a speaker saying "stick", "tick" and "Dick", then edit out the fricative from "stick", and playing the words, ask the students to identify the words. Edited "stick" comes out as "dick". The "explanation" is that English has the phonemes /t,d/ and the phones [t,tʰ,d], and speakers can't "perceive" the phone [t] as distinct from [d], they only perceive the two phonemes. The flaw in this is that it's task-determined. Speakers can correctly identify edited "stick" as being distinct from "dick", and they can tell if a person uses the "wrong phone" in pronunciation (e.g. saying [stʰɪk]). It's not that we can't perceive low-level predictable details, it's that we disregard them for most especially linguistic purposes. Also, people are able to learn new languages with new sounds, but that would be impossible if you could only perceive the phonemes of your language. You can actual learn that German "bieten" and English "beaten" are pronounced differently albeit quite similarly. There are, however, quite a number of physical differences which quite literally cannot be detected by people, for example the difference between a tone at 200 Hz and one at 201 Hz. The concept of "phone" may have utility in distinguishing sound-differences which are completely incapable of being linguistically exploited, versus ones that happen to not be exploited in a particular language (like aspiration in English). If the goal is to do speech recognition on English (or some other well-understood language), then "phones" are completely superfluous. We already know what the range of acoustic variation is going to be for "p", "f" and so on, and we can adjust our programs (database) accordingly. Phones are more important in that first stage of mapping speech to symbols. They continue to be important for quite a while, because we still don't know what a "phoneme" is. For example, we do not know if [ɾ] is a phoneme of English (everybody seems to have their own opinion, there is no scientific consensus). Here's the bottom line for perception and phones. There is a limit to what kinds of sound differences can be distinguished by humans; there are also limitations where one's prior linguistic knowledge influences you (to perhaps not notice the difference between "bieten" and "beaten"); then there are differences that you really just can't miss, assuming you already know the language. "Phones" are about the middle class of problems, where you can hear that sounds are "somehow different" in a systematic way. (The Lushootseed example is [ʔuʔəɬətub ti sq'əčqs ʔə ti spaʔc]).