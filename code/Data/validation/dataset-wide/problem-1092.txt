I'm not sure whether this belongs to SE DBA, but it sounds more logical ... Our MySQL 5.5.32 on Ubuntu 12.04 64 uses utf8_unicode_ci for the Server collation and some tables, like phpBB3, uses utf8_bin for all of their tables. This has been since start. I read utf8_bin is (at least in theory) faster since no conversion and/or normalization is done, but are these quite different UTF8 thingies fully compatible ? If they are, is there performance hit or improvement ? The tables I'm planning switching from utf8_unicode_ci to utf8_bin contain integers and dates, so not having to bother with much larger UTF8 should drop at least 1ms when dealing with these large ASCII-only tables. Before I had all tables not already using UTF8 Unicode Ci or UTF8 Binary I was using ASCII Binary for these tables, but decided to switch to UTF8 Unicode Ci for full compatiblity with Server collation (and of course a bit of performance improvement). 

Using pgBarman, This was really easy as it walked me through it all, plus it's open source! I stumbed upon this while googling link to pgBarman tutorial (pdf) link to pgBarman site 

My question is would I go about setting up the following?: First Server (master) -> Second Server (slave) -> Third Server (slave) Tutorial links welcome! I tried to Google, but I'm sure I've gotten the wrong keywords 

I want to automatically check for null ID's over all id columns in my database, I have to check all tables of which there is about 50,000 so manually doing this is infeasible 

So i assume that you already have the queries to insert the data. So the question is really "Can i also check that i m not inserting same data over and over ?". There are a couple of ways you can do this. One simple way is to create a constraint on the columns in your table. Let's assume that they are col1,col2 and col3. So you could create a unique constraint on the table like this : alter table table_name add constraint Constraint_Name unique(col1,col2,col3) A composite primary key will also achieve this, but you may already be using the primary key in a different column. 

I would have thought that databases would know enough about what they encounter often and be able to respond to the demands they're placed under that they could decide to add indexes to highly requested data. 

Basically part of our Postgresql table is used to keep server access logs, and as such sometimes during production this can get pretty large. is there any way of setting in postgresql to have a maximum number of records a table can have and to push off the oldest record? 

The problem is apparently not related to just SQL 2012; it has been reported on older versions also. One guess is that your copy of SQL Server is probably corrupt. You could try downloading from a different source and checking if the issue repeats. The new SQL Server download might have all the required files in good condition, and save you the headache of any issues down the line. Of course, you should uninstall everything before attempting a fresh install from a different source. You mentioned that it was Enterprise Edition? Assuming that it's a licensed copy, you can also call Microsoft Support and ask them for assistance. 

I need to search through Table A and Table B and match records based on template_folder_uuid_parent and heading, then set the UUID of the Table A record to the UUID from Table B. Once changed in Table A the UUID will cascade correctly. 

Does anyone here know if there are any tools, not using mysql's built in replication that will sync up two databases when the slave has been inaccessible from the internet for extended periods of time? The idea is to set up a virtual machine basis for our web developers, who can power on the VM and have it automatically clone from a slave of the production DB for the purposes of testing/development. they all require their own DB's but they should all replicate the master DB on startup. Diagram of operation: Production DB -> Production Read Only Slave -> Dev Virtual Machine 

Not sure if you have checked this; but there is an option in sp_addarticle called @identityrangemanagementoption. If set to manual, then it "Marks the identity column using NOT FOR REPLICATION". Pls check if this is the case. Ref : $URL$ 

There is another method, but a little more complex. If your single table is getting very large, copy it onto a filegroup. Then use File & Filegroups backup to restore the single table. See also : $URL$ 

Yes statistics can be created in 3 ways: 1) Statistics created due to index creation. These statistics have the index name 2) Statistics created by Optimizer(Column statistics). Starts with WA* 3) User defined statistics which are created with CREATE STATISTICS command by the DBA ref : $URL$ 

Originally posted to $URL$ but no replies received. I run Percona Server 5.6.21-70.0-688 x64 on Ubuntu Server 12.04 LTS x64, 64GB RAM, 2x2TB SATA* in hardware RAID0 and Intel Xeon E5-1620 v2 3.70GHz. InnoDB/XtraDB is the primary engine (with Barracuda format), with 20GB (21 after recent change) buffer pool size divided to 20 (21 after recent change) pools. The zlib compression is maximized to 9. The Database Servers and its threads are dropped to nice -20 and ionice -c 1 -n 1. Almost every Server on rautamiekka.org depends on MySQL databases, especially Minecraft. I found out how to make a perfect list of IPv4 addresses into a text file. I created a table with auto-increment column and 1 column for each address section, to maximum of 5 columns in total. I made a Python script that started a transaction and explicitly disabled auto-commit and uploaded those addresses. The script had uploaded so many addresses from the 115GB file it had about 51GB of data in, likely already compressed. Then Percona Server 5.6.21-70.1 came out. I thought I'd run the routine update process cuz I thought there'd be no problems so I shutted down Minecraft Servers and any other Server depending on MySQL and forgot about the Python script. However, after fighting with and for few minutes I realized Database Server was still running and not accepting any connections since it was the in the shutdown process. I noticed the Python script had quitted by way or another. That was Monday 2014-11-24, and the many processes/threads have been running since then, at 20-50KB/s (read) or even 200, and 120-500KB/s (write). It's mainly 1 that runs read and 2 that run write, with 1 process running at least twice as fast as the second, and only 2 processes, mainly, use CPU, but they're not the only ones. I presume it's committing the data into the table and reading the compressed data to verify it. My estimations say the shutdown will take 9 days if we assume it writes 80GB at 120KB/s at minimum, and I warned ppl it might not be ready until Monday 2014-12-08 although it's been running for days now. I made changes to the config just recently: 

dLight, the issue may have nothing to do with Linked Server. This is most likely a syntax error. Within your BACKUP DATABASE command, you may be using a reserved keyword somewhere, or maybe an extra comma, parentheses or apostrophe. What I'd suggest is : parse the BACKUP DATABASE command in your editor and look for any error messages. Or you can post it here so that we can have a look at it. 

Being certified in SQL Server is a good way to demonstrate that you are willing to put in time and effort to studying the technology. Besides, when you study for the certification, you get exposed to a lot of things that you would probably skip otherwise. It's a good, systematic approach to studying SQL Server. We had attended a class once from Microsoft. Their study material is good and the instructor was helpful. When he didnt know a specific answer, he promised to get back to us. And he did! That's really good. Good luck on your certifications!