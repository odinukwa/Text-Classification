The actual answer depends on your needs but generally "put data and log on different arrays" is of higher importance than "put tempdb on its own array". Given the number of drives you have available, my starting point would be: 

Both the default instance and SQL Browser maintain a list of instances running and their current port numbers. If there is nothing listening on those two ports (maybe there is no default instance, maybe the default instance is using a different port, maybe SQL Browser is stopped) then the only way to connect is to manually specify a port number in the connection string using a comma. For example, PRODDB\Payroll,14550. SQL Browser sends broadcast network traffic so a lot of administrators prefer to not run it. TechNet: Default Client Connection Behavior. As an aside, port tcp1434 is used by the default instance for the Dedicated Administrator Connection. 

It checks port tcp1433. The default instance is listening on this port by default. It checks port udp1434. The SQL Browser service is listening on this port. 

At the SQL Server conference I attended in Sydney in 1999, Richard Waymire said that Microsoft's intention was to allow a variable page size in SQL Server 2000 but that it turned out to be really difficult, so they put it in the too-hard-basket and went with 8k as a compromise. 

To correct one common but important misconception: You don't "normalise a database", you normalise a Logical data model. Data design, the process of designing how your data will live at rest, has four distinct phases: Conceptual, Logical, Implementation, Physical. Normalisation is the process of refining the Logical model. Is it required? Yes, if you want your Implementation and Physical models to protect data integrity and to perform well. Tactics that protect integrity can reduce performance so sometimes the normalisation process applied to the Logical model is reversed in the Implementation and Physical models, a process usually called denormalistion. This is a tradeoff - denormalisation means accepting less data integrity protection in the name of better performance. (This also means that questions posted here of the form "is this table normalised?" are meaningless unless the poster includes the conceptual and logical models from which the table was derived.) I suspect the OPs situation is that they learned "a database system" rather than learning "data design". I often see this in people who learned Microsoft Access. They come to believe that the database (the physical model) is the whole data model. 

I read that I can have different isolation levels per connection and in the server. The default isolation level is REPEATABLE-READ. So if I have a transaction that issues a to the client connection, how does it interact with other transactions from other client connections? Do they all end up being serialized? From set transaction seems not: 

These block each other but not always! I can not understand this. These refer to different rows why do they block each other? 

When having a table in SQLite that has a data type, is there any implication if the length of the contained string differ significantly? 

Can someone please explain why the delete shows a different number of rows than the count? I am using the same join. Are they not equivalent? 

If you have a big table that you want to partition and you have 2 candidates as the partitioning key (in the sense that 50% of the queries use one column and 50% of the queries use the other column and almost no query uses both in the same SQL query unless all the code is rewritten to do that) how can one determine which key is the best candidate to be used? Does it matter for example if the "entity" that one key represents is "fixed" and the other is constantly increasing? E.g. as an example of what I mean "growth" (taken just as an analogy of what I am asking) if you have decide between using the doctor id or the patient id as the partitioning key where we can have only so much doctors but infinite number of patients 

If Bob is the owner of BobSchema then you don't need to set any permissions at all - object owners have full control over their objects. Additionally, If Bob is the owner of BobSchema then Bob will by default be the owner of all objects created in BobSchema (in SQL Server 2005+). So, one way to solve your situation is to create the schemas using something like . 

Thanks to the OP's clarification of the error, I'm going to post my comment as a suggested answer. In SQL Server, permissions are checked only when ownership changes. Consider the following tables: 

Depending on how damaging you consider a power-down to be. :-) This does require xp_cmdshell to be enabled on the server, something that is not the case for the last few version of SQL Server. It also requires that the service account have the shutdown right, which it may or may not have. Enabling xp_cmdshell probably goes outside your 26 character limit. Would you allow multiple injections? 

First, the sa password stored plaintext? You should be voting with your wallet. Whoever thinks that is acceptable needs to be put out of business. Here is an anology that might help you explain the issue: Employee Alice needs access to the first floor. Do you give her the master key to the whole building or just the key for the first floor? Answer: You give her just the keys to the first floor. Why? Because it reduces the chance of accidental or deliberate damage. If Alice can't get to the second floor server room in the first place then she will never do anything bad in there. It is the Principle of Least Priviledge. As to why the application needs to use the sa account, that is a question that PerfMon or Extended Events should be able to answer. Create a PerfMon trace using the T-SQL template, maybe filtered by application name. Off the top of my head, here is another argument against using sa: Using the sa account requires the SQL Server service to be in mixed authentication mode. WIndows only authentication is better becase we can leverage all the secure features of Kerberos. 

Now for testing purposes I will run the procedure and on a different window I will check if it is running, IMPERSONATING . to do this I use: 

have fixed sqldwdev01 the reason I couldn't connect to UAT instance is because of a couple of reasons 1st was there was a dynamic port specified for the instance under SSNetworkConfig\Protocols for UAT so had to remove that and specify 1435 (as 1434 is in use) restart the service still cant connect remotely to the instance name but can connect using the port name eg sqldwdev01,1435 so it needed sql browser service to be running to direct the port for connecting with instance name i started sql browser and now can connect using sqldwdev01\uat 

However the schema_option has different options that can be associated with it, and I would like to figure out ALL of them. You can see the possible options on the picture below. 

the problem is though, that sometimes, after for example a couple of day without running this procedure I get sometimes the following error message related to the linked server: 

object_definition and sp_helptext both return the source code of a stored procedure - but how can I use and get the source code including the line breaks as the currently does? for example in the code below I create a stored procedure: 

when I run my script it shows me the next running time for the job in general, but I would like to see it by schedule too. 

this link below has also helped me specially when I need to copy the database mail settings from one server into another: The database mail configuration saved into a temp table I got the following script that helps troubleshooting databasemail basics. Pay attention to the comments and run one step at a time. 

As a database administrator, you are part of a team whose goal is to make a job easier, or just possible, for someone. The team includes system administrators, database administrators, database programmers, application programmers, network administrators, and so in. A valuable team member knows not only their own role but a little bit about every role in the team. Specifically, as a database administrator, it will help you to know some or all of the following. 

"â€¦but I'm curious as to whether there's a better way." Yes - use hardware or software RAID to create one RAID0 (or maybe RAID5, but write performance might suffer) array across the three 700 GB disks. Better still (as the commenter below says), add more disks and create a RAID array with fault tolerance. SQL Server databases are more manageable with fewer files, not more files. Is it possible to physically remove the hard disks from the old server and install them in the new server? If so then restoring the database into the new instance of SQL Server is just a matter of attaching the data files. Edit: The poster below makes an excellent comment about fault tolerance. A RAID0 array has no tolerance for errors. Then again, neither does splitting the database across three individual disks. I am OK with SQL Server data files being on RAID arrays with no tolerance, if and only if (1) there is a great DR solution, and (2) the log file is on an array with fault tolerance, and (3) the database is in Full recovery Model (so the tail of the log can be backed up). 

Why does the first sum of the fractions is summed up to 1 while when summing the decimal equivalent we get 0.9999? 

Let's say I have an entity which has a location as part of its attributes Now a can have N of other that are close by. So seems to be a self referencing relationship 1-N (optional) If we know before-hand which exact are close by each other what would be the best way to represent that? Since we have a self-referencing 1-N relationship I think another table would be needed In this case we would store e.g. etc to show that pizza store with id 1 has 22, 23 and 78 near by etc Now in order to get these rows back in-order I would need to create a PK and query based on that. I was wondering would an auto-increment guarantee the insertion order? Or would I need to use a float representing the distance e.g. (where 2.04 is the distance in miles) I was also thinking is there a better way than this? We know that if is close to then is also close to right? Is there a more efficient way to represent this information? I think it would suffice to store just the row to capture that is close to and that is close to . But this way we are losing the order Update: All the answers are very useful. One thing though that perhaps is not clear. I already have the order of the entities based on the distance on insert time. I.e. I have a set of ids of objects already sorted based on distance and I want to insert this set in the database in a way that I can retrieve the rows back in the order I have inserted them. I want to avoid sorting on distance on retrieve since I already had the order at insert time. These ids would be used to join to another table (they are ids) and I want to retrieve the rows in the insert order