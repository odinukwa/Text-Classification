If you're responsible for both Web sites, you could put SQUID in front of them and handle it on the backend. (See: transparent proxy) 

Depending on how you are configuring your datasource, you may be able to specify a setting like . If were set to 5 minutes, set to 4. The only two timeout values that can be specified for MySQL connections are wait_timeout and interactive_timeout. could potentially apply to named pipes too, if it connected as a client would. If I recall correctly if neither wait_timeout or interactive_timeout are specified, the default is 28800. 

You could categorize your data according to importance and work with the business to create an appropriate SLA. Typical things to focus on are time to restore and how stale the data gets. Weight is given between full and incremental, typically weekly and daily accordingly. If your backup SLA is developed properly, the technical options should be obvious. LTO-4 can store 800GB. While not always ideal, plenty of people use tape. You are going to find you encounter logistical issues regardless. Often, compromises are made between importance of the data and cost. The more data, the more cost. Hard drives can be appropriate but due to them being mechanical I wouldn't recommend using them for rotation. For off-site storage, the primary choices are between copying over a network to a physically separate location or a portable medium that's less prone to failure. (Tape) Edit 

It is entirely going to depend on your requirements. Read the manpage, use the flags to create the type of dumps you need, and then test the restoration. Be careful with locking, as it will likely prevent your application(s) from working on varying levels. There are numerous backup strategies for MySQL, which differ between engine types. The same methods to backup MyISAM are not always able to be applied to InnoDB. This topic is well covered on Serverfault, I suggest you search the site. One of my preferred strategies: 

You can add to the but the performance will differ from a redirect. Edit Since you want to redirect and you don't need advanced functionality, it seems like using should suffice for you. You would put the under a VirtualHost directive. A client side solution would be to use a tag. 

Sounds like a network connectivity issue. You've eliminated hardware from scope and the entire server is inaccessible, which is unlikely to be a system issue without reflecting on some level in the logs. If the server is not rebooting or does not have an issue in the logs that would impact everything, focus on network. 

The error log is located in the data directory, which appears to be in your case. Verify in your cnf file. 

I use SQUID as a front-end proxy with caching rules setup for static content, so as that it's stored in RAM for common requests. Unless you have a particular goal in mind as part of the backend architecture, I cannot see the need for a distributed file system. 

Your network routing table will determine how the packets are routed. You can add additional routes or change the default gateway to affect the routing. If you would like additional assistance, please provide your routing table. To display the routing table: 

I typically do not on a filesystem that is used for data storage. For example, if only MySQL data is stored in the filesystem, and MySQL runs as the user, I do not want the data to be wasted. For the filesystem and other filesystems that are integral to the system's operation, that reservation is well advised. 

Looks like it might be a hard disk issue. Use a boot disk (UBCD?) to run disk diagnostics. You could also use a Linux rescue disk to attempt to access the filesystem and verify the configuration. System was working before, right? If so, have any changes been made to the system recently? 

It's well advised to set the sticky bit on directories like that. Then, only the owner of the directory and owner of the file may remove content from the directory. 

Many distributions have additional system crons configured via /etc as well. For example, CentOS has files in Let me know if you have any further questions. 

I typically dump the schema using mysqldump. However, I believe the blackhole engine could be used as you describe. You would all tables to and , , and records would not affect the data, as blackhole does not store the data. You can and set and all tables created will default to except those that are created as , as cannot be disabled. You need to be aware of this as you create tables on the master, as you may need to later back to on the slave replicant. Nevertheless, what you describe is an administrative issue and not a technical issue. I recommend configuring at least two development databases, which would be maintained by the infrastructure team. The first, would be a development database. The developers have more access to this database and it would be regularly refreshed with scrubbed data. The second database would be a QA or testing environment, which developers would not have RW access to. This allows proper staging. Both of these databases would be verified by the IT person making production changes to the database and would be included as part of the staging process to production, which insures consistent schemas across environments. If you want developers to have databases all over the place, they simply cannot be maintained and it will have to be their responsibility to insure that their dataset is current enough for their needs. You could programmatically provide a schema dump on a shared drive or perhaps a blackhole replication slave with RO access. 

Locking issues will be exampled by the connection statuses in Read through the and MySQL documentation. Configuration options are very well documented. Generally speaking, you want as much to be processed in memory as possible. For query optimization, that means avoiding temporary tables. Proper application of indexes. Tuning is going to be specific to your preferred database engine and application architecture. There's substantial resources pre-existing an Internet search away. 

You will probably be able to SSH to it, as this is installed and enabled per default on most modern UNIX servers. Before that, telnet was the common method. If you are unable to connect by either of those protocols, you could run nmap against the server to determine what ports it is listening on. If you need to change the root password, you can boot into single user mode by appending "single" to the boot parameters in both LILO and GRUB. 

Note, removes all created chains. flushes all rules. There are native tools to save and restore the rules. Particularly, and . Most modern Linux distributions have and functions within an iptables init file provided with the system. There are other firewall best practices, such as dropping malformed packets and other type of undesirable traffic. This is one advantage of using a front end utility such as Shorewall, as it will implement many of these polices by default. Nevertheless, I agree with your approach and prefer to maintain my own rules directly as well, and these same best practices can be implemented without a front end. 

This is typically addressed on the network layer, where all outbound SMTP would be source NAT'd to the same IP. The definitive source for QMAIL's official distribution is Qmail.org, which is where you could acquire netqmail. netqmail is QMAIL packaged with recommended patches that were commonly applied, as there had not been an official release in a while. There are numerous patches linked on QMAIL's web site, which change the interface binding behavior. I have successfully implemented this patch in a production environment before, which allows different domains to use different IPs. The patch isn't documented that well but I have the following in my notes: