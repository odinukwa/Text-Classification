Pros: this is just like parsing a text file. Most work is done in code. Feels like a simple and straightforward way to go. Cons: does not feel right. This may look and feel good, but something about this design smells bad. I have a bad feeling about this. The question By know you probably already guessed the question - which way should I go? Is there a more sophisticated way of storing such data? Does any of the solutions I've thought of make any sense? 

The task I need to store the some values in sequence in the database. The data looks just like a sequence of numbers. Here is an example of the data: 

These block each other but not always! I can not understand this. These refer to different rows why do they block each other? 

id is the PK of the table. These statements execute without blocking each other. Makes sense since they refer to different rows. Next: 

I have a rather basic question on transactions. Assume a table with a column amount. Now we have the following code: 

Can someone please explain why the delete shows a different number of rows than the count? I am using the same join. Are they not equivalent? 

I read that I can have different isolation levels per connection and in the server. The default isolation level is REPEATABLE-READ. So if I have a transaction that issues a to the client connection, how does it interact with other transactions from other client connections? Do they all end up being serialized? From set transaction seems not: 

Here is the answer. Even though the job was configured to run via a PROXY Account, the SQL Server Agent is still responsible for the job. I had a look and the SQL Server agent was configured to run under the Local System Account on that server. So what I did is to put the agent to run under the superuser admin account and it worked as expected. Now in this case the fact that the job no longer needs a proxy since the Server Agent itself is running under the ultimate account. However I appreciate that this is not the right way moving forward (even though this isn't my server and I hope I never get to touch it again!) I will be advising the customer to reconfigure SQL so every service runs under a dedicated domain account (i.e. created solely for this purpose), which is the way it should be! Now what I would love to understand is why the job would run as long as the proxy account used for scheduling the job was logged into the SQL server! 

The three solutions I came up with three solutions that can store those values, but every solution has some flaws. Here they are: Store the index of value in the Value Tables: 

Pros: easy to insert and delete values - inserting a value requires 1 insert and 2 updates (updating refs on adjacent values). We can even remove the prev_value_id to make it even better. Cons: finding values by index will be horrible. Finding range of values from 500 to 1000 will require us to go though all the values starting at value 1. Store values as e.g. string in the record table Tables: 

I read somewhere that InnoDB can store all the tables in a single file or in separate files. Is this a configuration option? What is the default setting? A file for all tables or a file per table? What is the recommended setting? 

When having a table in SQLite that has a data type, is there any implication if the length of the contained string differ significantly? 

If you have a big table that you want to partition and you have 2 candidates as the partitioning key (in the sense that 50% of the queries use one column and 50% of the queries use the other column and almost no query uses both in the same SQL query unless all the code is rewritten to do that) how can one determine which key is the best candidate to be used? Does it matter for example if the "entity" that one key represents is "fixed" and the other is constantly increasing? E.g. as an example of what I mean "growth" (taken just as an analogy of what I am asking) if you have decide between using the doctor id or the patient id as the partitioning key where we can have only so much doctors but infinite number of patients 

I will be able to execute the package again as long as I am still logged in to the SQL server (I logged in to install the Firebird drivers). I can execute it even from a remove SQL Server Management Studio connection -- but as long as I do not log off from the server. If I log off from the server, then the SSIs job will no longer work (same error as before). This led me to think that this is not in fact a 32/64 bit mismatch, but perhaps some registry or environment variable is not being committed after I log off from thee server due to insufficient permissions (even though I was meant to be admin on that server). So for my next test: 

As you can see database 2 has 200 MB less data and still produces a much bigger backup file! Can anyone tell me why is this happening? Please do tell if some additional info is needed. UPDATE 1 I got the data/log allocated sizes by executing the following query: 

Pros: easy to find the needed values by using the index field (). Cons: inserting and deleting value are horrible. If there are 1000 values for a record then inserting one value at index 500 will require 1 insert and 500 updates (to shift values after index 500). Same problem with deleting values. Store references to previous value and the next value Tables: 

Is anyone aware of a SSAS (tabular) 2016 in which the number of records in a table are not fully shown in Excel? 

I am experiencing an issue with an SSIS job that connects to a Firebird DB using their ODBC driver (version 2.0.4.155). 

Log in with the user that has the User DSN entries that you with to convert to System DSN Open the Registry Editor and navigate to HKEY_LOCAL_CURRENT\SOFTWARE\Microsoft\ODBC\ Right-click ODBC.INI and select export to save it as a file on the desktop (or anywhere else you fancy) Open the .reg file with a text editor such as Notepad Replace the text HKEY_CURRENT_USER with HKEY_LOCAL_MACHINE. Save your changes Double click the .reg file and proceed to import it into the registry. 

But this seems useless to me. I mean if I set serialized to a client connection then I would want all currently running transactions to be sequential, right? Otherwise what's the point? 

I know that transactions are meant to group several operations as one. But if for example in the same table one transaction does 

Now if this snippet runs concurrently by 2 transactions, T1 and T2 what will happen? I know that in MySQL by default the isolation level is . So let's say that amount initially is 500. So when the 2 transactions finish will the final amount be 300 or 100? I mean when they start both read from table which I guess is a shared lock. When one updates the table it gets an exclusive lock right? So what happens to the other transaction? Will it proceed with the amount it "view" when it started i.e. 500? 

Ldf and mdf sizes were there (each DB has only two files - one mdf and one ldf). And if it matters both databases' recovery model is FULL. UPDATE 2 Just to clarify - the operation after restoring databases involves all kind of stuff (adding/removing table fields, rebuilding indexes, adding/removing FKs and inserts/deletes). The final structure (tables, indexes and fks) are the same. The amount of data differs slightly. UPDATE 3 Here the results of the query posted by : 

This is not a big problem for me, but still, I would like to understand what is happening here. I have two databases on my SQL Server 2008 (10.00.1600) - they are restored from the same backup (and after that some optimization work was done by our C# program - the process was the same for both DBs, + some small amount inserts/deletes was executed afterwards). But when I take a backup of those databases the output file size differs: