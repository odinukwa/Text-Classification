To be able to assign a record different targets for internal/external queries, add two further zone fragments and at the bottom of . 

This is a bit of a shot in the dark, but: You're using which will only limit the heap portion of your memory. Since your process grew to ~ 50 GB memory usage, this could have been caused by a huge stack. So if your application spawns a lot of threads, heavily relies on recursion or stuff like that, this will be your cue. If that's the case, look into things like GC logging, , , , to analyze the situation and into , JVM parameters and ThreadPool sizes to remedy the problem. (Not entirely sure about the parameters, Java 5 was the last version I had to do with professionally.) 

In a network where one of the authoritative nameservers sits on the border of the internal network, I use views and the directive: : 

You might try bouncing off a proxy on a machine with a static IP, such as squid. Better yet, if you have SSH access to any machine with a static IP, you can use the SOCKS option for SSH: 

If done right, all you need to do is update the master DNS server, and have the rest as slaves, which will either poll the master or be informed by the master of a pending update. Once you have automatic updates set up, you can have as many slaves as meet your availability needs and then they'll keep up to date, provided the master itself is reachable. 

I had a recent problem similar to this. After upgrading one of the packages on my 7.2 system, the gd-driven captcha on my phpBB2 installation stopped working. I re-built all of the php ports and it fixed itself. I know that's a bit vauge, but sometimes things will break over months of incremental upgrades due to dependencies getting out of whack. 

into the alphabetically last subdirectory of . Try anywhere to help visualize this. Now, judging by the other comments you made, it appears you already shuffled stuff around further. Since the you ran is unlikely to really destroy any data, you should be able to move stuff back in place. If you have no statically linked rescue shell like available, you'll have to boot into a rescue system. From there you should be able to locate , , , etc. If you manage to put these back into your system should be bootable again. Think before taking further steps, it seems your recovery attempts so far only worsened the situation. 

Your nameservers are both on the same AS, same subnet even. I'd try getting additional slave servers somewhere else first, via zoneedit for instance. 

You are not required to provide an MX record for your domain. If there's no MX record present, an MTA is supposed to use your domain's A record to deliver mail to. This will however cause a significant number of spam filters to flag mail originating from your domain as possible spam. So even if mail isn't your "main task", you probably still want to keep the spam filters happy. To accomplish that, your zone has to look like this: 

It depends on the application, the response time needed, and what you're willing to do to meet those goals. Recently, I was working with a 10+ GB, 50+ million line text file and had a need to search for specific strings in each line. Standard Unix tool "grep" did the trick, but took an unacceptably long time (multiple minutes). I imported the text into a postgreslq DB (it was a CSV file, easily imported), and once indexed on the key I needed to search on, it took under 1 second to find my record. Granted, my workstation is single-core, with only 4GB RAM, a 4-year-old 2GHz CPU, and a top-heavy filesystem (ZFS) using 5+ year-old consumer PATA drives. Your mileage will certainly vary. Still, the time difference between the two methods is staggering. If your data is free-form text, you might still consider importing into a DB which supports full-text search and indexes appropriately to support such searches. Even if you have the RAM to have the entire file cached and a fast machine, doing a linear search of files this size will be time inefficient, depending (once again) on the application. 

I know this is a rather complex approach, but in my opinion going with LDAP based netgroups, nss_ldapd and possibly augmented by sudo-ldap is the most integrated solution. Once LDAP is in place you get Kerberos SSO virtually for free as well. 

In this case eth3, which doesn't get you to the desired destination. The gateway you're asking to forward your packets (192.168.1.254) truthfully responds, that it has no path to the destination network: 

In the original DNS specification (RFC 1034/1035) there were two steps of cache invalidation that needed to take place, before a zone update was globally visible. Additionally to the already mentioned TTL expiry of caching resolvers around the world, you first needed to wait for (all) your secondary name server(s) to refresh the zone data from the primary's zone. Only after DNS NOTIFY (RFC 1996) was specified in the year of 1996, there was a standard way of promptly notifying all authoritative name servers about zone changes. So maybe the original phrase of "change propagation" was more appropriate at the time, since it was a two-step process. 

I maintain a site with a crusty old version of "Links" by Gossamer Threads. It's written in perl, gets 99.99% of its submissions from bots, and is just plain out-dated. Now, I realize that Gossamer has newer versions, but I was hoping to find something that's open source, and maybe in PHP (since 95% of the site's functionality comes from this language). Does anyone have any opinions on other software to manage a link directory? 

It's a matter of configuring DNS correctly, combined with configuring virtual hosts on your web server. If you have the ability to manage your site's DNS entries and your web server configuration, this is easy enough to do. 

A very lightweight solution is to utilize the services of $URL$ If you want total control, and have a spare machine to use as a server, the combination of Squid and SquidGuard is a pretty versatile solution. 

While I give you my word that my script is safe, don't make a habit of running such scripts from strangers, since you have no reason to trust me and I could potentially do something devious with my encrypted script. :-) Unfortunately, a sufficiently savvy user will be able to recover the script plain text by simply removing the "| /bin/sh" portion from the script, which will result in the script being dumped to stdout. In short, if the end user can run the script, it is possible (with enough motivation and skill) to reverse engineer it. Don't place anything of critical importance (such as passwords) into these obfuscated scripts. 

Add a Content-Type header such as or use a mail client that guesses the Content-Type (GNU mailutils for example probably do that). 

They probably suggest that, because 50 MB memory per request is not the common thing. You should probably check, if you can get rid of any modules you're using. Also check if you can move any mod_ (like mod_php) to fcgid. After you did that, you probably have only thread-safe modules left and can safely switch to worker MPM, which will likely resolve all your performance problems. 

This has nothing to do with DNS. The host name usually is only set once when the network comes up. Someone probably changed it by accident, just change it back. 

In order to route multicast traffic, you need a userspace daemon like (recommended) or . Since you'll also be needing some kind of interface representing the destination network in the routing table, you could create tunnel interfaces for your SSH connection using the . This will create an interface on both your SSH hosts. 

Not 100% certain about the first two questions, but there are lots of options for #3. I personally use CCleaner to scrub the cruft from the family PC automatically once a day. If you run it by hand, it should give you list of files found, depending on the function you're using. For #1 you could also try running "dir /s" while standing in that "Temporary Internet Files" from the command prompt. For #2 it could be from any software that does automatic updates (such as Adobe Acrobat viewer, Sun's Java, and MS's own Security Essentials and Automatic Updates). This is just a guess, as I'm not 100% certain where these apps store their downloads. 

If your external router supports port forwarding, you can have it forward the ports to the services you need. You could also look at VPN solutions, either as a function of the router or having your internal client and external client access a common server on the internet. If you don't mind heavy latency, you can set up a client on the inside to be a TOR client with a hidden SSH service. Then you can ssh into that client from any other TOR client (so long as you know the service name), and use port SSH's forwarding or SOCKS proxy capability to access various internal services. There are many options here, and they all depend on what services you require access to, how much control over the setup you have, and how much effort you're willing to put into the setup. 

Since you already located most of the time spent in the kernel, I'd suggest enabling CONFIG_LATENCYTOP and running, well, to see more. Can be done with , too, but is way more convenient. 

Bacula opens a TCP connection to the storage daemon's VPN IP. (A → B) Since the kernel setting is set by default, the IP Don't Fragment bit is set in the plaintext packet. When routing the packet into the IPSec tunnel, the DF bit of the payload is copied to the IP header of the ESP packet. After some time (often around 20 mins) and up to several gigabyte of data sent, a packet slightly larger than ESP packets before is sent. (A → B) As the storage daemon interface has a lower MTU than the one of the sending host, a router along the way sends an ICMP type 3, code 4 (Fragmentation Needed and Don't Fragment was Set) error to the host. (some router → A) Connection stalls, for some reason host A floods ~100 empty duplicate ACKs to B (within ~20 ms).