It does not reference a option other than to mention that historically it was used to request bzip compression. 

Assuming the new IP address is on the same subnet as the first, add a second virtual interface (sometimes called an "alias") to the primary network interface. This is configured, like all network interface settings, in . The Debian Reference manual has a section on the topic: $URL$ A simple example, assuming your primary network interface is and has an ip of and the new ip is : 

Add the path you mention in your question to that file and run . That should make an env munging unnecessary. 

The important column in your case isn't but and . It's normal and healthy for Linux to swap unused stuff out. If, however, and showed constant activity, that would imply that you did not have adequate resources for your usage pattern. As others have pointed out, that would mean you had either a leaky app or just not enough RAM. Frankly, I don't see anything disconcerting about the info you've posted. 

On Ubuntu/Debian, you can install the shell to restrict them to scp/sftp only. Just install the package and change their shell to : 

It sure looks like OS X uses PAM. In that case you should be abe to use the PAM-MySQL to perform any type of auth you want. Out of the box OS X uses a pretty straightforward PAM config for sshd: 

Easiest solution would be to use mod_vhost_alias instead. It does not do exactly what you described, but it's very close. 

If you didn't save the log file and position when you took the snapshot, I don't believe there's a way to figure that out. Some of the snapshot tools, however, do save that data. How did you take the snapshot? For example, if it's a filesystem snapshot taken after flushing / locking all tables, the correct data should be in in the slaves data dir. 

I have a RAID 1 + hot spare that I would like to reconstruct, online, into a RAID 5 set. I know this is possible with OpenManage, but I has anyone figured out how to pull this off using the MegaCli tools? 

CARP is available in linux. Check out the ucarp project for a user-space implementation and there is apparently a project porting it to the 2.6 kernels: $URL$ 

If it's in an odd location (not in or ) create the file containing the path to the directory containing and run as root. For more troubleshooting info, run this command, once as root and once as a non-root user: 

I use thermite. Of course it's a little hard to donate them to charity, but they sure are thoroughly unreadable. 

So what you have there is a symbolic link that links back to itself. I don't see how that's possible with the command you listed at the top of your question, so I suspect this particular symbolic link was created differently. I can replicate your scenario like this: 

EDIT: Also, you may want to consider setting as a mount option. It's not a specific tweak for large directories, but can offer considerable performance improvements whenever lots of filesystem activity is taking place. 

Yes, large directory sizes can be a problem. It's generally best to avoid them by hashing files into subdirectories. If that's not an option, there is an ext3 feature that can dramatically improve the lookup performance in large directories: 

Adding as a new answer, since the topic of the question has changed from Postfix to a general networking problem. I suspect it's the clause on the interface in . If that script () fails, aborts processing the interface. Try commenting it out and running . 

The sftp/scp tools start an interactive non-login shell, so .bashrc will be sourced. Many distributions source .bashrc from .bash_profile or vice versa, so it can get confusing. A good trick for testing the cleanliness of your login environment is to ssh in with a command, which simulates the same way scp/sftp connect. For example: will show you exactly what scp/sftp sees when they connect. A simple demo: 

Postfix sets an header to the original address. In this case it would contain . This header is controlled by the variable in your . 

On Ubuntu you use "www-data" because that is the user the Apache process runs as. You can identify the user Apache will use by checking the config file. On my Mac, it looks like this: 

While you haven't explained what the problem is, a good place to start is by manually injecting a message into Postfix via telnet. A decent article on the subject can be found here: $URL$ Tail the mail log, like Aleksander mentioned, while doing this. 

The OpenLDAP Admin Guide seems to walk through the setup in a pretty clear fashion. And appears to be thorough and complete. 

You already have a script that's being executed every time someone logs in: You can modify that script to perform whatever task it is that you have in mind, or have it call your script directly. On Debian derivatives, you can just drop your custom script (ending in ) in and it will be called automatically. NOTE: Everything I just said only applies to Bourne shell derivatives, like the default , ash, bash, ksh, etc. Most users stick with bash, so the above directions are likely all you need. Other families of shells use other startup scripts. Tcsh, for example, will execute on login. 

That seemed to work great for the URLs, properly subsituting in the result from the , but URLs returned just the first part of the redirected URL, leaving out the results from the lookup. If I reversed the order of the rules, the failed/working URLs reversed. As I have always understood syntax, if the URL path doesn't match the pattern, the rule is skipped and processing moves on to the next one. In this case, the I finally managed to get it working by breaking it out into two blocks: 

Previous PERC controllers could all be managed/monitored by the LSI MegaCLI tools. The latest release of those tools does not appear to recognize my H700 card: 

You could also try replacing your filter with so it scans every volume. Although unless you've messed with how drbd devices are named, the filter you listed should work just fine. I think it's more likely that the metadata is missing. 

So as you can see, the directory contains a symlink called "cwd" that links the the CWD of the process. The same is true for the open filedescriptors listed in . The hierarchy contains a wealth of information about all running processes. Worth poking around in! 

Nothing special. It behaves like a switch, maintaining an internal list of the MAC addresses visible on each interface and passing the packets accordingly. It can even do STP. 

That would allow downloading .txt files. Quick tests indicate the filename is not passed as part of the scp command when sending. So it may not be possible in that case. The script would be set as the forced command for the user's key. (The ``command="blah"'' field in authorized_keys.) It can also be set as the "ForceCommand" option for a Match group in sshd_config, like so: Match group scponly ForceCommand /usr/local/sbin/scpwrapper Then add the users to be so restricted to the "scponly" group. EDIT: By request, here's the script I use to enforce rsync-only access: 

The exact answer to your question (handling the address) depends on how your server is configured to receive mail. If is the virtual domain the best you can do is collect the messages in the mailbox (assuming ). If is the locally delivered domain for the server (mail is delivered to actual system accounts) then you can add a file to the home directory of the user, which delivers to a program that parses the bounce information and records it in a database or file. See for more info on the format and how to deliver to a program. What we do, since we send messages for a large number of domains, is use as our VERP domain. This domain needs to be added to . Create with this content: 

The filesystem is backed up to a tarball on the drive mounted in . Adding the flag tells tar it will be creatinga multi-volume archive. The flag tells it the size of each volume. When the backup reaches that size, tar pauses and asks politely for a new volume: 

I'm starting to manage more custom-packaged applications for our Ubuntu Hardy systems. Some are of apps not available in Ubuntu but most are much newer versions than are available via the standard "-backports" process. I'm trying to settle on the best way to manage these packages in subversion. One thought is just to commit the generated diff. Has anyone found a better way? 

It depends on the security requirements. At a previous job, every user had a unique username/password to access the privileged wireless network. Guests were allowed into a highly restricted network by clicking through a "You're using our network, be good!" warning. At my current job, we were careful to design our network so that being on the office wireless granted no privileges higher than would be granted to someone out on the internet. So there we use a single shared WPA2-PSK password that everyone knows. I would find it hard to justify trying to keep a shared password secret. If network access needs to be protected (due to privileged server access, for example) then the only auditable and manageable system is unique credentials for each user, with good logging. If network access does not require that degree of protection, then trying to keep a shared password secret is just an annoying exercise in futility... 

The partition does appear to contain a valid, if incomplete, backup as I can restore from it using the xfsrestore command. Any idea how I can get xfsdump to span multiple devices? Is there a better way to accomplish this without manually trying to partition the data into 2TB chunks? Thanks! 

Since screen uses its own TERM variable ("screen"), bash thinks it's not capable of displaying a fancy color prompt so it defaults to a simpler one. Screen is, of course, fully capable of the same complex prompts as a normal terminal. So, to override this behavior, just hard-set your preferred PS1 options at the end of your file. Or, if you're feeling adventurous, find the test that checks the value of the TERM variable and modify it to accept "screen" in addition to "xterm-color". 

So I unmount the external drive, attach the next one, mount it, enter at the prompt and the backup continues. Good ol' tar. 

Actual directory permissions are screwed up. This is fairly easy to test by changing to the user that Apache runs as and attempting to list that directory (assuming Apache is configured to run as "www-data"): 

If you substitute your office subnet for 192.168.0.*, users will be able to use passwords to connect, but only from the office subnet. You, however, will be able to use your SSH keypair to connect from anywhere. When an ssh client connects to the server, it is presented with a list of authentication mechanisms it can try. Typically the list is "publickey, password." In this case, when someone connects from outside, they are only presented with "publickey" so their client will not even attempt to send a password. If they attempt to authenticate with any mechanism other than an SSH public key, the connection is immediately shut down by the server. So no possibility exists for brute-forcing the passwords. 

That's pretty much what I was hoping to see, as it hit the end of the available space on . Running looks good, too: 

I just finished a shell script that does monthly full backups and nightly incrementals, all to an offsite server. This is how it works: The offsite server runs a nightly cron job. On the first Sunday of every month, this script runs and pipes the output to a file. It then scans backwards from the end of the file and records the last revision number in a file. On all other nights, the script runs, via ssh, 'svnlook youngest /svnpath' and compares the output to the last recorded (backed up) revision number. If there have been new commits, it runs and dumps the output to a file. Then it runs some cleanup code to delete full dumps older than $N days and any incremental dumps older than the newest full.