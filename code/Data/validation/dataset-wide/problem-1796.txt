Utilize the "logger" command to let admins send one-liners to syslog. Since the user name is included, it makes for easy scripted reporting. If you're logging to a central host, you get these changes logged centrally as well. Simply appended all changes to /etc/motd, not only documenting the changes, but also displaying them to everyone when they log on. 

I use SSH keys, use the same one for all servers, and maintain a good password on my keyfile. Saves a lot of aggravation. For devices where this wouldn't work, I'd use a password that had a hard-to-guess core, then use the devices dns name, IP, or other common characteristic (such as OS or brand), to make the password unique for the device. This worked especially well for groups of similar devices. Just keep the pattern/mnemonic secret, along with the core, and you have a difficult, unique password that was easy to remember. 

I maintain a site with a crusty old version of "Links" by Gossamer Threads. It's written in perl, gets 99.99% of its submissions from bots, and is just plain out-dated. Now, I realize that Gossamer has newer versions, but I was hoping to find something that's open source, and maybe in PHP (since 95% of the site's functionality comes from this language). Does anyone have any opinions on other software to manage a link directory? 

This will give you a sorted list of all files, starting with the largest at the top, in KB and without crossing onto other mounted drives. If you only have a single, huge / partition, then replace "/foo" with "/". More often than not, you have a small number of large files that are eating up space, such as log files, core files, or crash dumps. It will really pound the server, so either nice it and/or run it when the machine can handle the extra load. 

A couple of other things to try would be to compact the database itself (I assume Access still has this function) and defrag the actual database file on the machine doing the sharing. For defragging single files, I recommend the sysinternals contig command line utility. You could also test for a bad network, by running ping for an extended time (I believe "ping -t" is the correct Windows incantation of the command), and seeing if you're dropping packets or seeing high network latency. 

1 Computers The Dells that you mention are probably a good solution, since your goal isn't necessarily exceptional performance as affordability and homogeneity. I see a chassis flange on this page - $URL$ 2 Remote installation and maintenance I assume you're using Windows Vista or 7. If you've got money, go for a Ghost-type solution. If not, I'd recommend CloneZilla, which I use on our laptops here. It's simple to create an image, and the server edition can be used for mass-wipings. 3 Switches Cisco is expensive, unless you can get edu discounts. For extreme low-end, I'd recommend Netgear GB switches, though don't get the 48 port models, as they overheat. If you need 48 ports, do the step up and get 3COM 48 port switches. Essentially, you need smart switches with the ability to do dot1q vlan tagging. Even if you don't need it now, you will thank me later. 4 Servers You can get by with two, although I would recommend that your backup server function as a domain controller as well. You don't ever want just one windows DC. Plus one server for your exchange mail, too. 5 Resources Google is your friend. A quick search led me to this: $URL$ which sounds great. I'm sure you'll be able to find more, and if I find any, I'll update the post 6 Future expansion Use open standards as much as possible, buy devices with upgradable firmware, and keep your eyes open so you have a year or so advanced warning of changes. 

Lets talk about I/O Operations Per Second (IOPS) If you assume that you have these disks, their average latency is 4.17ms (which is determined by the platter size & their rotational speed (in this case, 7200RPM)). You also need to know the average read/write seek time to really calculate IOPS. This site claims the average seek time is 12ms (which is horrible, and will be the cause of your problems, as we'll see...) Determining IOPS is pretty imprecise because to get it right, you need to know what your read percentage is vs your write percentage (writes are slower than reads because, apparently, the head needs to be more precisely placed). The calculation for IOPS is 1 / (avg latency + avg seek time), so each drive would be capable of 1 / (0.00417s + 0.012s), or 1/.01617, or right around 60 IOPS. So that's one drive. But you've got several! You mentioned an 8-disk RAID-10 array. That's great, because while you've got to write the data twice, you can read from all 8 at once. Assuming a 100% read workload, 60 IOPS X 8 drives = 480 IOPS. How do IOPS relate to throughput, though? Well, we have to go back to the "imprecise" part, because it depends on what percentage of your disk I/O is random. On a 100% random workload, you can kind of assume that one operation gives you one block. So then, how big is the block size? According to this PDF, the DS4100 had a 16k block size. We can use that to calculate the sheer amount of output you can get. At around 480 IOPS, each of which is getting 16KB, you'll be pulling 7.68MB/s with a purely random workload. Because your workload isn't random, you're getting ~5.25x this speed. 

It depends on the application, the response time needed, and what you're willing to do to meet those goals. Recently, I was working with a 10+ GB, 50+ million line text file and had a need to search for specific strings in each line. Standard Unix tool "grep" did the trick, but took an unacceptably long time (multiple minutes). I imported the text into a postgreslq DB (it was a CSV file, easily imported), and once indexed on the key I needed to search on, it took under 1 second to find my record. Granted, my workstation is single-core, with only 4GB RAM, a 4-year-old 2GHz CPU, and a top-heavy filesystem (ZFS) using 5+ year-old consumer PATA drives. Your mileage will certainly vary. Still, the time difference between the two methods is staggering. If your data is free-form text, you might still consider importing into a DB which supports full-text search and indexes appropriately to support such searches. Even if you have the RAM to have the entire file cached and a fast machine, doing a linear search of files this size will be time inefficient, depending (once again) on the application. 

Have you looked at Linux's network block device (nbd) driver? I don't know how well it handles high-latency (it's been years since I've played with it), but it may be worth looking at, assuming the project is still around. 

I've got a handful of Plesk machines of various versions and operating systems, each hosting mail as well as web sites. Anyone who has dealt with Plesk knows that it can be pain to keep locked down. Between qmail's tendency to support backscatter spam and insecure contact forms published by users in PHP and ASP, there is a lot of potential for spamming and getting listed in RBLs. In addition to locking down these machines as tightly as possible, I want to funnel all outbound mail to one or more outbound-only mail relays that have the ability to scan the mail for spam before sending it on its way. I'd prefer Postfix, but I'm open to just about any open source solution. There are many, many tutorials for filtering inbound spam, but very few that even address the outbound spam problem, and most of those have little useful info. Even a method to have Postfix sequester all outbound mail into a special queue that I could scan with home-rolled scripts and then re-inject into the outbound queue would be an option. Any and all ideas and suggestions welcome. 

If not, check /var/log/samba/* and /var/log/messages to see why it didn't actually start Step 3 - Can we connect to it remotely 

Right now, I'm the only admin at my company, so when there is an issue, I get an SMS & email from the Nagios server that monitors the affected host. I'm soon to be hiring a junior admin, and when that happens, we're going to get to do on-call rotation (once the person is trained and ready, anyway). My cell phone is paid for (as will the new admin's), and we have a certain flexibility in terms of when I come in and leave. As long as I don't abuse it, it works fine. We're a small company with 17 people and somewhere between 50-70 servers that runs 24x7x365 automated processes. 

1) The browsers won't balk at the new certificate as long as it's signed by a trusted authority, so if you buy it from one of the regular channels, or sign it using an in-house CA that your browsers trust, then you're fine 2) sftp deals with the certificates that are part of the SSL/SSH subsystem, not the apache certificates, so it will be unaffected. 

Since the child processes inherit the ulimit settings, could you run ulimit in the python script after it sets the uid/gid, creating a new (lower) hard limit, which would then be the new running condition for all of the potential fork-bomblets? 

Here is a gigabit dual ethernet NIC with the ability to be programmatically assigned to fail-open or fail-closed on power loss. 

I would say no, and my argument stems from the "eggs in one basket" model. With only one account, if your account gets compromised, you lose 8 databases, where as if you have one user per database, you're only out one database. The other argument is that you should put your eggs in one basket, "after making sure you've got a damned good basket". IMO, MySQL username / password combinations aren't a good enough basket. Now, if you wanted to bind mysql to 127.0.0.1 so that no one external could connect, you'd have the start of a better policy. Of course, I'd still assign different users to each DB. Defense in depth.