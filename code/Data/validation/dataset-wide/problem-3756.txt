I don't know if this deserves to be an answer or a comment, as it includes information you surely know. The terminology of matroid theory borrows heavily from graph theory, linear algebra, and other fields. A dependent set in a graphic matroid corresponds to a cycle in the underlying graph, so a general dependent set in a matroid is called a circuit. The length of the smallest cycle of a graph is its girth, so the same word is used for a general matroid. As you're well aware, the spark of a matrix is the girth of its corresponding vector matroid. You're correct that your notion appear to be dual to spark. The appropriate terminology here could have been "cutset", as that's the corresponding notion in a (connected) graph, but for consistency it's cogirth. The co-spark of a matrix is the cogirth of its corresponding vector matroid. A matroid is representable (ie, as a vector matroid) over a field if and only if its dual matroid is representable. Moreover, the transformation taking a representation of a vector matroid to a representation of its dual matroid is completely effective. It follows, in a sense, that studying the co-spark of a matrix is equivalent to studying the spark of another matrix. In particular, any computational hardness results for spark carry over to co-spark. It seems, then, that the notion of co-spark would be useful primary as terminology and as a way of getting a handle on things, but not actually for new ideas. Here's one paper that has explicitly looked into these sorts of things: On the (co)girth of a connected matroid. 

Section K of chapter 10 of Einstein manifolds by A. Besse gives geometric interpretations for all irreducible symmetric spaces. The 2 cases you ask about are apparently antichains in (H⊗Ca)P2, and (C⊗Ca)P2 (whatever this means). 

A few of the more obvious ones: * Resolution of singularities in characteristic p *Hodge conjecture * Standard conjectures on algebraic cycles (though these are not so urgent since Deligne proved the Weil conjectures). *Proving finite generation of the canonical ring for general type used to be open though I think it was recently solved; I'm not sure about the details. For vector bundles, a longstanding open problem is the classification of vector bundles over projective spaces. (Added later) A very old major problem is that of finding which moduli spaces of curves are unirational. It is classical that the moduli space is unirational for genus at most 10, and I think this has more recently been pushed to genus about 13. Mumford and Harris showed that it is of general type for genus at least 24. As far as I know most of the remaining cases are still open. 

A random $k$-coloring of the vertices of a graph $G$ is more likely to be proper than a random $(k-1)$-coloring of the same graph. (A vertex coloring is proper if no two adjacent vertices are colored identically. In this case, random means uniform among all colorings, or equivalently, that each vertex is i.i.d. colored uniformly from the space of colors.) 

For semiprimes, computing the Euler totient function is equivalent to factoring. Indeed, if n = pq for distinct primes p and q, then φ(n) = (p-1)(q-1) = pq - (p+q) + 1 = (n+1) - (p+q). Therefore, if you can compute φ(n), then you can compute p+q. However, it's then easy to solve for p and q because you know their sum and product (it's just a quadratic equation). If you believe factoring is hard for semi-primes, then so is computing the Euler totient function. Update! Factoring and computing the Euler totient function are known to be equivalent for arbitrary numbers, not just semiprimes. One reference is "Riemann's hypothesis and tests for primality" by Gary L. Miller. There, the equivalence is deterministic, but assumes a version of the Riemann hypothesis. See also section 10.4 of "A computational introduction to number theory and algebra" by Victor Shoup for a proof of probabilistic equivalence. 

It is possible to make sense of this if one generalizes the notion of distribution by choosing a smaller space of test functions. The space of test functions should be chosen so that it is closed under Fourier transform and its elements decrease so fast that multiplying them by an exponential function is still integrable. Some possibilities for the space of test functions with these properties are: *Holomorphic functions on the complex plane that decrease faster than any exponential on horizontal strips (if I have remembered the Paley-Wiener theorem correctly...) *A more extreme space of test functions is polynomials times Gaussians. (Using smooth compactly supported test functions as suggested in another answer does not work as this space is not closed under Fourier transforms) 

Because its Weyl group contains -1. For split semisimple groups in char 0, taking duals corresponds to acting by -1 on the weight lattice, where irreducible polynomial representations correspond to weights modulo the action of the Weyl group. So if -1 is in the Weyl group (acting on the weight lattice), then any (irreducible) representation is isomorphic to its dual. This includes the groups with Dynkin diagrams A1, Bn, Cn, Dn for n even, E7, E8, F4, G2 but not An for n>1, Dn for n odd and E6. 

If you are sometimes called upon directing a random walk in a directed graph, how should you direct it so as to maximize the probability it goes where you want? Formal statement More specifically, suppose you are given a directed graph $G$ with edge weights, two designated vertices $s$ and $t$, and a subset of the vertices $S$. The edges weights represent the transition probabilities of the random walk, the vertex $s$ the start, the vertex $t$ the target, and the set $S$ the set of switches. You are guaranteed that the weights on the out-edges of any node are non-negative and sum to one, that $t$ is absorbing (i.e., $t$ has one out-edge directed towards itself), and that the out-degree of any vertex in $S$ is exactly two. A random walk is taken on $G$, starting at $s$. For any given vertex not in $S$, the weight on an out-edge is the probability that the walk will travel in that direction. Every time that the walk reaches a switch (a vertex in $S$), you are allowed to choose which of the two edges the walk will travel along (and you are allowed probabilistic strategies). How should you direct the path if you want to maximize the probability that the walk ends up at your target $t$? Questions I am most interested in this as an algorithmic question. How fast can you find the optimal strategy with respect to the size of the graph? My specific application has about 100 switches among 200 vertices in a fairly sparse graph (say out-degree bounded above by 6). But we can also ask purely mathematical questions. For example, my intuition says (and I can hand-wave a proof) that there exists an optimal strategy that is deterministic in the sense that it always chooses the same direction for a given switch and this direction does not depend on the initial vertex $s$. Is this actually true? Also, is there a sense in which the optimal strategy needs to "coordinate" among the switches? That is, is there a local optimum that is not a global optimum? Notes A note on connectivity: we may assume that the graph is sufficiently connected. If not, we can identify all vertices that cannot be reached from the start node, as well as all of those that cannot reach the target node, into a single, absorbing fail state. We may assume the start node is not the fail node. 

If one iterates the map z -> z^2 + c there is obviously a simple formula for the sequence one gets if c=0. Less obviously, there is also a simple formula when c = -2 (use the identity 2 cos(2x) = (2cos(x))^2 - 2). Are there any other values of c for which one can solve this recurrence explicitly? (For all initial values of course: there are many trivial explicit solutions for special initial values, such as fixed points.) Related links: $URL$ (the points c where 0 remains bounded under iteration of this map: this strongly suggests that there is no simple exact solution for general c). $URL$ (gives the explicit solutions above, after a change of variable) Motivation: I once used the map with c=-2 in a lecture to show that one could prove limits exist even without a formula for the exact solution. A first year calculus student pointed out the non-obvious exact solution above, and I don't want to be caught out like this again. 

Many people know that there is a (3×3) nine lemma in category theory. There is also apparently a sixteen lemma, as used in a paper on the arXiv (see page 24). There might be a twenty-five lemma, as it's mentioned satirically on Wikipedia's nine lemma page. Are the 4×4 and 5×5 lemmas true? Is there an n×n lemma? How about even more generally, if I have an infinity × infinity commutative diagram with all columns and all but one row exact, is the last row exact too? For all of these, if they are true, what are their exact statements, and if they are false, what are counterexamples? Note: There are a few possibilities for what infinity × infinity means -- e.g., it could be Z×Z indexed or N×N indexed. Also, in the N×N case, there are some possibilities on which way arrows point and which row is concluded to be exact. 

Shall we try teamwork? Please feel free to edit this post if you have simplifications. The original sum may be re-expressed as $$ \frac{1}{2^{2m+1}} \sum_{k=0}^m (-1)^k \binom{m}{k} \binom{2(k+m)}{k+m} \frac{1}{2^{2k}} \sum_{j=0}^{k+m-1} \frac{2^{k+m-j}}{(k+m-j) \binom{2(k+m-j)}{k+m-j}}. $$ If we're trying to prove this is 0, we may drop the fraction out front. Also, change variables from $j$ to $\ell=k+m-j$: $$ \sum_{k=0}^m \left( -\frac14 \right)^k \binom{m}{k} \binom{2(k+m)}{k+m} \sum_{\ell=1}^{k+m} \frac{2^\ell}{\ell \binom{2\ell}{\ell}}. $$ At this point, my idea was to change the order of summation based on $$ \sum_{k=0}^m \sum_{\ell=1}^{k+m} \Diamond = \sum_{\ell=1}^m \sum_{k=0}^m \Diamond + \sum_{\ell=m+1}^{2m} \sum_{k=\ell-m}^m \Diamond, $$ but I can't get quite it to work out. The first sum simplifies, but the second sum I can't do much with. Any ideas? 

This is not really an answer because these PhD's were never actually written, but anyway: in his book A mathematicians miscellany (in the chapter on math with minimum raw material) Littlewood gave 2 examples that could have been 2-line PhDs: (1) Cayley's projective definition of length (2)Theorem: An integral function never 0 or 1 is a constant. Proof: $\exp(i\Omega(f(z)))$ is a bounded integral function. ($\Omega$ is inverse to the elliptic modular function.) 

If you are looking for examples of modular forms whose zeros can be described explicitly, then you probably want the zeros to be cusps or imaginary quadratic irrationals. In this case the Gross-Kohnen-Zagier theorem implicitly gives lots of examples, by describing the relations between Heegner points on modular elliptic curves. (Heegner points are closely related to imaginary quadratic numbers in the upper half plane.) Many examples of modular forms with zeros at imaginary quadratic irrationals can also be constructed explicitly as automorphic products. 

Mumford in Rational equivalence of 0-cycles on surfaces gave an example where an intuitive result of Severi, who claimed the space of rational equivalence classes was finite dimensional, was just completely wrong: it is infinite dimensional for most surfaces. This is a typical example of why the informal non-rigorous style of algebraic geometry was abandoned: too many of the "obvious" but unproved results turned out to be incorrect. 

Another term that has been used is "unilateral" or "unilaterally connected". I don't have a particularly strong opinion in favor of this terminology, but I am slightly opposed to just calling it "connected". (I usually assume "connected" means "weakly connected" for digraphs.) However, I must admit a reference by Tutte is good. Some references for "unilateral": 

The ability to embed mathematical problems into chess (like combinatorial game theory into go) should not be underestimated. Papers by Richard Stanley and Noam Elkies demonstrate problems where the objective is to determine the number of ways to perform a given task. They include problems where the answer is 

The following trick uses some relatively deep mathematics, namely cluster algebras. It will probably impress (some) mathematicians, but not very many laypeople. Draw a triangular grid and place 1s in some two rows, like the following except you may vary the distance between the 1s: 

I can't tell if that quote implies that both conditional and randomized polynomial-time algorithms exist, but it might (the exception that proves the rule?). Thanks in advance. 

The original motivation for vertex algebras is explained briefly in the original paper $URL$ as follows. For any even lattice one can construct a space $V$ acted on by vertex operators corresponding to lattice vectors. More generally one can write down a vertex operator for every element of $V$. These vertex operators satisfy some complicated relations, which are then used as the definition of a vertex algebra. In other words, the original example of a vertex algebra was the vertex algebra of an even lattice, and the definition of a vertex algebra was an axiomatization of this example. This was motivated by my attempt to understand Igor Frenkel's work on the Lie algebra with Dynkin diagram the Leech lattice. Frenkel had constructed a representation of this Lie algebra using vertex operators acting on the space $V$, and I was trying to use his work to understand its root multiplicities. I did not use any insights from conformal/quantum/topological field theory or operator product expansions when defining vertex algebras (as implied by some of the other answers), for the simple reason that I had barely heard of these concepts and had almost no idea what they were. This is not all that helpful for understanding what a vertex algebra really is. A better view is to regard them as something like a commutative ring with an action of the (formal) 1-dimensional additive group. In particular any such ring is canonically a vertex algebra. The difference between rings with group actions and vertex algebras is that the "multiplication" of a vertex algebra from $V\times V$ to $V$ is not defined everywhere: it can be thought of as a "rational map" rather than a "regular map" in some sense. More precisely if we write $u^g$ for the action of a group element $g$ on $u$, then the product $u^gv^h$ is not defined for all group elements $g$ and $h$, but behaves formally like a meromorphic function of $g$ and $h$, which in particular may have poles when $g$ and $h$ are trivial. Making sense of this informal idea produces the definition of a vertex algebra. (For more details see the unpublished paper $URL$ This means that vertex algebras behave like commutative rings: for example, one should define modules over them, tensor products of modules, and so on.