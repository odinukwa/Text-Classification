--edit it may be safer to use instead (see @Alex's comment below), but the caveat is that is also changed on grants and revokes. 

It isn't. But because the database is usually foundational to the business (eg applications are built on top of it), it is worth working hard to get it in the best shape you can. "best" is a practical consideration and depends on your needs, not a measure of theoretical or ideological perfection. Requirements change - if your database doesn't change in step then it is going to end up a white elephant everything else is built around rather than the solid ground everything else is built on! 

You have some other reason for wanting to fiddle with a database that is "running well"? Good reasons might include: 

They don't. The quoted text just explains why postgres needs to use modulo 231 arithmatic (which means transactions can wrap around as long as old transactions are 'frozen' early enough): 

dbfiddle here This isn't a full explanation, and it might not be possible to have one without knowing how Oracle does the sample internally. What is clear is that whatever Oracle is sampling from, either table or index, needs to be big to iron out the sampling wrinkles. 

Your whole premise behind the question is faulty. You do not need to split your data between tablespaces to spread the i/o across more disks, you need to increase the number of disks in your RAID10 or (better still) ASM array. You will get less performance gain, less space efficiency and far more maintenance trying to manually tune the i/o like you are suggesting. ASM beats RAID10 primarily because it understands the data being written to it - so for example it can vary the stripe size between data blocks and logs. 

But is this timezone safe—if the current date is in Daylight Saving Time and the start of the month is not or vice versa will the correct date result? 

InterBase version 6 was released under an open-source license in mid-2000, and was immediately forked to become the Firebird project. Later versions of InterBase reverted to closed-source and the projects have diverged. Both are still actively developed. Developer’s tools and libraries traditionally support both servers but they are growing apart. 

As with any licensing question, I'd suggest you read Oracle's info carefully including the full license and satisfy yourself that what you want to do is OK rather than just taking my word for it. 

But it isn't spelled out whether will fail if only one of the foreign key columns is nullable, or if it will merely set the nullable part to null, which is it? 

I want to get the value of for each row, where is the value of from the closest 'previous' row (defined by ordering on which is unique) such that in that row is greater than in the current row. In other words, I want this result: 

dbfiddle here My example uses an offset of -9223372036854775809 (-2^63+1), but you are free to chose any offset that wont overflow the . So you'll need to use when presenting the keys, and when applying the offset, but not for actually storing the keys or operations like sorting. 

ora_rowscn is always incremented when a row changes - but in a default configuration it can also be incremented when a row does not change If you need to check the whole table for udates, one method is to use auditing. On the other hand if you only need to check the row you are trying to update for conflicts, with is ideal. 

Can I rely on being executed before the context is read inside the view, as it has been in this test case run on 10.2? 

In T-SQL, the statement can short-circuit, but you cannot rely on it evaluating the expressions in order 

The CBO will consider lots of different ways to execute your query, taking into account all the indexes and statistics on both tables and indexes. If you disable an index, obviously the CBO then cuts out all the plans it might have considered that involved that index. The question really boils down to: "why is the CBO choosing a poor plan", and the first thing to consider is whether your stats are accurately reflecting reality in your database. Is this the case? How do you gather stats, and how often on these tables and indexes? If and only if the stats are good and the CBO is still choosing a bad plan, you'll need to start considering "why?", then you'll want to have a look at the plan, especially the estimated cardinalities for each step, and see which are badly off. In some cases this is because of correlations that standard stats cannot help with, but cross that bridge when you get to it :) 

It looks like you can using a fqdn rather than an IP address - if that is right then you could make use of a dynamic DNS service such as DynDNS to get a stable DNS name that points to whatever your current home IP address is 

If the game area is strictly static, then my preference would be to use the coordinates as the primary key. On postgres, I'd probably do that using a composite type. I don't think there is a direct equivalent in SQL Server - hopefully someone will correct me if I'm wrong. Whether this will simplify the problem or not depends on how exactly you have implemented your searching algorithm - can you post more details on that? 

If you look at the plan for the query you are using, you can see it never touches the table, instead it is doing a fast full scan of the primary key: 

and the like this will even apply to tables created after the was made - it gives a database privilege As you also want to you will also need to 

You can use the windowing function and divide by three to group the rows into 3s, perhaps like this: testbed: 

A normal table with or without indexes: New rows are dumped at the top of the heap. The RDBMS probably only looks at 1 block, so not just O(1) but very small O(1). If the table has indexes, a pointer to the row will be added to each. This will usually be an O(log(n)) operation. A table with some sort of clustering going on, eg an Index Organized Table or cluster for Oracle, or a Clustered Index for SQL Server and others: New rows are inserted into a particular block, which may cause the block to split or overflow, but whatever happens it is still O(log(n)) or better, caused by the b-tree or similar structure used to find the block. 

yes, as long as there is no loop in the data, there is no restriction on the way you define what is a 'parent' row and what is a 'child' row: testbed: 

Then I suggest break the table up into chunks for processing using something you are indexing on (even if it is a single column, you can split it into ranges of values) This will do a FTS once instead of once for each chunk as in your code. You will have to live with an awful lot of redo and will wipe out your undo space too (so no flashback subsequently) --edit2 if you can add/rename/drop columns, you can do this very efficiently, but only on 11g 

Expand the rows in both tables for all the days in the month and join them together. Assign a 'group' to each series of rows where and don't change for a period (a "gaps and islands" problem). Group the results. 

I'm not sure if the same applies if you aren't using incrementals, but there are too many edge cases with using a redundancy-based policy (see this horror story for example). Use a recovery window instead if you possibly can. The article you quote has the same advice: 

A CDBMS is a specialized database for handling queries like "show everything that is related to x". Unless that is what you want (and unless you don't want all the features of a traditional RDBMS like MySQL), you don't want a CDBMS. 

There are more efficient ways of doing this if your employee table is large, but this is the way I find easiest to understand :) testbed: 

You are too modest - your SQL is well and concisely written given the task you are undertaking. A few pointers: 

But you can't bring OO ideas into the SQL world directly - in many cases repetition is fine if the query is readable and well-written, and it would be unwise to resort to dynamic SQL (for example) just to avoid repetition. The final query including Leigh's suggested change and a CTE instead of a view could look something like this: 

Oracle is not like MySQL in this. Like most other RDBMSs it comes with it's own built-in storage engine that cannot be exchanged for another. 

You can't handle SQL exceptions within an SQL statement like you can from within PL/pgSQL. You must write a custom function as you suggest, for example: 

My first step would be to rank the by specificity/generality - I don't think you have provided enough information to be sure what you are after but assuming that it is based on the number of s and ties are broken by: is more specific than is more specific than : 

If you can run 6.06 in a VM on top of 11.04 server, it looks like MySQL 4.1 can be installed on that 

but assuming your table name is something different (which I highly recommended), the error returned indicates that the parser is interpreting as a column name: 

This kind of solution has immense practical benefits if you have the luxury of restricting all access to the data through your transactional API. You lose the very important benefit of DRI, which is that integrity is guaranteed by the database, but in any model of sufficient complexity there will be some business rules that cannot be enforced by DRI. I'd advise using DRI where possible to enforce business rules without bending your model too much to make that possible: 

In other words this applies to other objects, not just temp tables. This is easily demonstrated too: 

You are mixing up quiescing a replication master group and quiescing a database — these are different concepts: 

If the table has an index, you may want to consider the command instead of re-creating it or using . This will: 

With my limited understanding of the model clause, I'd expected the following three queries to return the same results because there are no nulls in the data, and the functions are equivalent: 

We've created a stored procedure to pull in full copies of some Oracle tables with a Linked Server. Over the course of 11 hours, the procedure correctly populated the first 11 tables but failed with this error on table 12: 

You are not alone - it is very common to have to delve deeper into the specific whys and wherefores of query execution. You need to learn to use the tools you have available, starting with and . Let us know how you get on... 

If the estimated cardinalities here agree much better with the actual row counts, it means there is some correlation between the columns that the CBO cannot guess based on the histograms generated by a vanilla . Dynamic sampling should improve things at the cost of increased parse time, but is not a silver bullet. Alternatively, with 11g, "Oracle Database can also gather statistics on a group of columns within a table" 

Because of the way you phrase this, I think you are specifically talking about the quoted MB/sec and 'inherent' latency for disk drives. If that's the case then the answer is "no, there is no direct correlation between disk latency and throughput". Latency for a HDD is affected by the seek time and the spin speed, but it only applies to the first read: subsequent sequential reads do not have the same latency. The mechanics are different for SSDs, but they can still be tuned by the manufacturer for high 'throughput' or high IOPS with issues like block size being important. 

Rather than updating the table directly, I suggest staging the ids in a global temporary table. This is likely to be faster, especially in the case where the table you are inserting into has indexes, as you are giving the optimiser freedom to chose how to order the inserts. You could use @Lennart's excellent suggestion for doing that with a small number of statements with varying numbers of bind variables, but here is the general idea: 

Using global indexes goes against the grain of partitioning in the first place — they are marked 'unusable' if you drop a partition. You may find this ORACLE-BASE article a useful read. 

It's a bug as @Evan discovered, so until it is fixed we have to work around it. As you probably know, it's easy to work around the case for multiple column inserts where not all values are — just omit the column(s) where you want the value inserted entirely, is the default: 

Storing the values that apply to the snapshot period would be the natural fit for most kinds of reporting, including "requests on trending the coin-in data for example by month per machine." The risk is that if a snapshot goes missing the total on the machine may not match the totals in the cube. Storing both would give you the means to cross-check. 

The full documentation for tablespaces can be found here and details what their purpose is, including the quote above about running out of space and, for example: 

Yes, that is exactly what you need to do. The export needs a consistent MVCC snapshot from the start of the operation. UNDO_RETENTION is ignored if your UNDO tablespace is fixed size or can't be extended: