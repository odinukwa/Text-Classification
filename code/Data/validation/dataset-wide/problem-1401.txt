As you can see, building the pandas data frame takes almost all the time. That suggests a trivial optimization: 

UPDATE: After a nice comment from the OP with new data, it seems that at larger systems my proposed improvement didn't help at all. With my method, the larger system would require creating a dense 5000-by-2000 matrix. However if I understand the problem then this matrix with 10,000,000 elements will have only 4000 nonzero values. Sparsification would seem to be in order. The question is which of the various sparse formats supported by SciPy would be best. I did some empirical testing and found that for matrices in which every column has two entries at random rows, the "CSR" and "COO" formats seemed to be best. 

The advantage of doing that is you don't have to remember what order parameters get passed into your initialization function when you use it. 

Since the function and the function are related by , i.e. , then iff you are OK with the weird approximation for , you should be able to work out a similar approximation for . You might be interested in the module, which provides a generalized capability to compute symbolic derivatives of most NumPy code. 

I didn't know about the option for . Very cool to learn! I like your graphs; the only improvement would be to plot the points and the curve on the same graph (i.e. combine the top panel and mid panel of the graph). Scipy's uses nonlinear least squares regression. In addition to comparing to the "local" results, you might also compare the NLSR results to the results of doing linear-regression on log-transformed data. 

Why is not a parameter in the function? Having it as a parameter will make it much easier to change, and more importantly, it will make future users of the code (including yourself six months/weeks/days from now) realize that in fact this function depends on a parameter to compute its result. If you do , then will be a parameter with a default value of 10, so you don't have to pass it in if you don't want to. 

If the statement (e.g. ) is wide, this burns a lot of horizontal space and the comment will probably exceed 80 columns. Consider such a comment to be the "topic sentence" of the paragraph. When it's a sidebar, it's effectively like putting the topic sentence at the end of the paragraph. Most style guides recommend keeping the width to 80 columns or less. I prefer ANSI comments (e.g. ) over K&R (e.g. ) as they're much easier and faster to edit. Try to avoid "if-else" ladders if possible. I replaced one of yours with a and the other, in the loop, using a I prefer to put the return type of a function on a separate line [in column 1] and so the function will appear in column 1 on the next line. This makes it easy to find them with a regex like which can make things easier when editing or analyzing source. Never put multiple statements on a single line. This is very hard to see visually when looking at a lot of code: 

A single simple in-place 2D matrix is much faster than what you have coded (20x faster in fact). Thus, it's all about performance. And, the algorithm matters. IMO, it's appropriate to talk about alternatives because yours is not in the ballpark range as to what is possible/required in performance. It doesn't meet the "most efficient" criterion. In an actual interview situation, this would probably be flagged by the interviewer. Note that, if you were [much] closer, say within 10%, this wouldn't be an issue [and I wouldn't have posted]. I'm not sure that you can meet the objective without a full refactoring of your code. Your solution puts an additional strain on the processor's cache and seems to have more complexity than would be needed. It also seems to use more memory than is required as well. And, a number of the STL primitives you are using, by their nature, appear to access the heap a lot (i.e.) they're slow. I'd say that simply clearing out cells as you traverse would be better than adding the complexity you have [or see below]. Also, for your algorithm, do you have benchmarks on and analysis of how much performance is taken up by each of the STL components you're using. That would probably be required when discussing the time/space tradeoffs. As it is, they are a bit of a black box. Is there a better alternative to for your matrix? I think it adds an extra [internal] pointer dereference [that the optimizer might be able to elide] for each cell access. And, for example, I see a better alternative to using a separate to keep track of visited nodes. When a node has been visited [used], simply OR it with 2. Eliminate your altogether. Then, you can replace: 

In your original code, if strength and weakness are both , then only the setting for weakness matters. My code preserves that, but is that behavior really what is intended? Or did you want instead of your second top-level ? Why are you using ? Do you really want floats returned instead of integers? If you want integers, don't use round. If you want integers rounded to the nearest 10 or 100, why not generate random integers in a 10-fold (or 100-fold) reduced range and then multiply be 10 (or 100)? Avoid capitalization in function names if you can. PEP8 and all that. All-lowercase (for functions) is good python style. You're only using one function from , so is better than loading in the whole module with . You seem to be using and as boolean variables so is better than . 

Since your sentence is a Python string (not a NumPy character array), you can generate the data for your histogram quite easily by using the data type that is available in the module. It's designed for exactly applications like this. Doing so will let you avoid the complications of bin edges vs. bin centers that stem from using entirely. 

Is the fifth element in this array really supposed to be "3"? Right now the code functions because of the mutability of . I'm not 100% sure it could be done cleanly, but if possible I'd suggest re-writing and to return instead of returing . That way the initialization of could happen in those functions too, which feels more natural to me. You are using your (mysteriously named) variable in a way that looks like a , so you might consider using this built-in datatype of Python. Now, to issues of performance. In Python, line-profiler is an easy to use package for assessing the performance of your code. I use this package in a Jupyter notebook like this: 

This Sudoku puzzle solving algorithm follows a brute-force approach mixed with rule-based approach. To further improve performance, an extra analysis step is added, to determine which squares to be prioritized while choosing it to be filled. The priority value is inversely dependent on the number of possible values that can be filled into a particular square. This value is used in computing the priority weight value of each of the blank square that is to be filled. This priority weight value helps in ordering which squares must be filled first. 

type represents the set of all values that can/can't be filled in a square. This type is specifically defined for storing bit-fields for representing a set of values. This follows the following convention: 

I had written a Sudoku puzzle solver that accepts puzzle boards, and completes them with the least possible time. Rather than purely depending on brute-force, my algorithm first attempts to fill in the squares that have an obvious solution. And for each square being filled this way, the amount of information increases (I.e.., more number of squares get filled which aids in filling in the remaining squares) which makes the further iteration process easier. Once this method fails (this happens if there isn't an obvious answer to fill-into any of the squares), the algorithm immediately switches over to brute-force search. (Note, the source contains a few spelling mistakes, like the word recursive misspelled as Recrussive. Please ignore spelling errors. Anyway, it has been a while since I touched this code, and this was my first object oriented code written in C++). The basic working (High level view) 

The signed integer values from represent the numbers in the Sudoku puzzle board. The value zero signifies a blank-box in the Sudoku puzzle board. 

Here is your code cleaned up for style as I've mentioned above. Also, look at some of the changes in vertical and horizontal whitespace: 

Edit: This is probably what I should have said at the outset to make things more clear: Since your interview question is concerned with performance: 

If you need your function to be non-destructive of the original matrix, at your function end, to undo this, you could loop on: 

Edit: This was my original opening section, which has been getting dinged. I'm leaving it in, for reference, but after rereading it, although it might have been tightened a bit, it does talk about the performance issues Caveat: This isn't a critique of your code style [as Zeta has already done that], but rather an alternate algorithm that can be 20x faster. A single simple 2D matrix can be much faster than using the primitives. As you were doing a interview question, demonstrating proficiency might be paramount and this might be a moot point. But, when the performance difference is an order of magnitude faster, that may make the difference. I've had a few related interviews and speed sometimes matters more. Assessing such a tradeoff may, in fact, be part of the requested/desired solution. To eliminate boundary checks, I've created an oversize matrix that has a border of zeroes on all sides. The actual data matrix is inlaid from . This is a technique used in some video/image processing. By using pointers instead of index variables, this also eliminates a number of multiplies within the loop. 

Anyway, here's a full program that does comparison benchmarking. Ignore most of it, and compare the and functions against your function. They are largely agnostic. The primary intent here is to back up the 20x performance benchmark above, rather than just stating that without proof. It will also allow, if you so choose, to provide a baseline reference for any recode/tweaks you may wish to do