Most have to do this as a matter of course. Few organsations have this expertise at every site and even visiting does not easily address the issues as problems are often intermittent or unpredictable. For example, monitoring traffic on switch ports and hosts (e.g. bytes in/out, numbers of packets in/out; broadcast and multicasts in/out, errors in/out) can give a first overview of normal behaviour and any changes during fault conditions. Typical intervals would be every 5 minutes and aggregated over longer periods, ideally displayed on web pages. Data needs to be stored locally as well as remotely in case access is lost when a fault is in progress. SNMP alerts are useful to collect. Beyond that network traces taken to a machine, often BSD orGNU/Linux based,typically connected to one or more span ports on local switche(es) are useful though, if not narrowly filtered, may be huge. Multiple sources may be needed (e.g. traffic to/from local servers; to/from WAN connection(s)). It is helpful if multiple traces can be taken concurrently. All these can be looked at and interpreted remotely though need a reasonable understanding of the examined network and some of the data volumes (especially raw traces or traces over time awaiting a fault) can be huge. A risk assessment will be needed before either allowing a third party to access the networks or sending network traces out of the control of your organisation. A full network trace allows the reconstruction of any non-encrypted content. Even if the data is encrypted and the trace excludes most of the content a full record of volumes with sources and sinks is still available. It may also include web sites and pages accessed and by whom, for example. Encrypting disks of trace information sent by mail would be a minimal safeguard and you would want a corresponding level of trust in whoever these go to. An external party given access may need equipment passwords: make sure you know which so they can be changed and consideration given to auditing equipment that has had external access. Online external access should be over secured channels (e.g. using ssh) if at all possible. 

On the specific question of HTTP 1.1 support, I think asking on the thttpd support forum was probably the best way forward, short of reading the code. I would not be surprised at HTTP 1.0 only as it saves the server having to store pieces of the page until the whole page is available in order to fill in the byte count correctly in the HTTP 1.1 header. Certainly in the routine in libhttpd.c the sequence starting 

Apart from resilience, multiple nameservers can provide latency reduction (e.g. where the nameservers are, for example, on different continents) and many domain registries require that a domain name has more than one authoritative nameserver. Two is a minimum for registration and this should be maintained subsequently. Many domains and registrars will use more. A typical maximum is 13. 

Depends what your power requirements are (V, VA, ...) as well as physical size and characteristics (length, one switch per strip, one per socket, ...) but there are such things. I'm from a 250V world so products I know are not equivalent but a quick web search showed up a Tripp-Lite product for US power. Do a search on local suppliers and manufacturers for your requirement, but it is possible to find these. There are also distribution PDUs that do not have physical on-off switches but are controlled by software. These may also provide support for sequenced power-up, to mitigate against overlarge surges that could occur when all the devices are powered on at once. Just one example is $URL$ At the very low-cost end, I can't find examples of any from web searches but, in the past, I've used a simple self-adhesive plastic clip that goes over a socket switch. I think these were sold to stop children (or adults) accidentally switching off an appliance such as a fridge. Of course, using anything like this has to be balanced against delay in switching off in an emergency. 

Wanted to thank both @BillThor and @kasperd for their excellent answers. Incorporating points from both of your answers, I am doing a deep dive on the Microtik RouterOS and looking at either the 24 port "smart swtich" $URL$ or the "13 port switch" $URL$ would be a great fit in this application. They look like a great value and are very flexible. 

I am at a colo provider that supplies a single IPv6 /64 block. The goal was to route the provided /64 of IPv6 addresses to the hosts behind the Mikrotik running RouterOS 6.24. Some Mikrotik examples and that I found always had the user getting a /48 or at least a /64 and another small block to connect with the gateway, or blogger major.io describing the possibility, however not recommended to use the link address to connect with the uplink router. I didn't have access to this so I tried to do it another way. What I had tried was a router IPv6 address on the gateway port as a /126 block aaaa.bbbb.cccc.dddd::2/126 to talk to the uplink router at aaaa.bbbb.cccc.dddd::1/126. Then I created another router IPv6 address on the master port behind the firewall with the mask aaaa.bbbb.cccc.dddd:8000:1/65. I also configured neighbour discovery so that the clients could autoconfigure. From the router terminal, I was able to ping the internet, and ping the hosts behind the firewall. From the hosts, I was able to ping the router addresses on both sides of the firewall but not when it needed to go to to the uplink. From another network, I could ping the external router addresses in front of the firewall, but could not access the aaaa.bbbb.cccc.dddd:8000:1/65 that had a static route entry in place the master port behind the firewall. I had no rules in my firewall during testing. Is my theory wrong, or is there problem with this model being used on the Microtik? 

I have looked for a solution to this problem but everything that I have found never seems quite match this scenario. I have a small colocation space and I only get 5 IPv4 addresses. I also get an IPv6 /64 block. I only have one LAN drop, external IP addresses only. The colo provider doesn't provide private network addresses. The gear I have is a 4 node superserver... each node has IPMI NIC and 2 regular NICs. I also have space for a 1U "switch" of some description that I am seeking advice on what to get. My understanding is that the IPMI ports only support IPv4. I need to be able to access these 4 IPv4 IPMI ports with only the "switch" operating. I also need at least 1 IPv4 address per node for web servers etc so I am out of IPv4 space. What I would like to do is find a 1U switch/router to route the IPv6 addresses to the IPv4 devices. I have looked at some of the Layer 2 and Layer 3 switches and they have static routing, but I don't think it would route across protocols like this. Am I going to have to break down and put in a full router? What are some of the approaches to achieve this functionality? 

From the shell, you can just run setup and change the ip range for the blue adapter. The options you do are: Networking -> Address Settings -> Blue Change the IP address to the desired setting and make sure that the subnet is correct to make the network the correct size. Given you have it working with a /23 you don't need to change the subnet. 

Typically these would be done by noting the time on power-up and then looking through a tcpdump or wireshark trace subsequently. Again, a record of this, perhaps with indications of numbers of frames and total bytes, is helpful as software and systems changes and as a reference in the event of problems. 

As you know, thttpd, unlike dhttpd, supports CGI but the man page does not refer to the HTTP level supported for the requests in the documentation. From the thttpd man page: 

Server A is a database server that you want to monitor in detail. You can obtain relevant stats on this machine that you want to display using MRTG but you do not want to run MRTG on this server. Server B is a server running (possibly multiple instances of) MRTG and Apache. MRTG will collect stats from a variety of servers and network devices using SNMP and other protocols, update its databases and the display pages for each statistic monitored. This server would usually also display pages relating to the structure and configuration of the network (some static, some dynamic content). There are multiple HTTP clients wanting to see the MRTG web pages on server B. If there was only one client then Apache is not needed on server B as a suitably privileged client could display the MRTG HTML files without the need for a web server (e.g. using ) 

If you don't think you need a firewall, think again. ufw is simple but designed for Ubuntu and based on iptables Update the packages: as a minimum apply all the security patches Document what you've done to secure the server and why. Include configuring (automated) processes to monitor logs, test the configuration and report security updates that are needed. 

Use dig ( dig @< authoritative nameserver > < host or domain > would be a start) to see the current settings of the authoritative nameservers for the domain. They may not have picked up on the changes yet (for nameservers associated with hosted domains the records you can edit are often not the ones on the public nameserver but are copied to there after validation by a process owned by the hosting company; it's different in some cases or if the nameservers are actually your machines, of course). Even if the primary nameservers for the domain have the new information, any other DNS server that has recently resolved the domain and received the old information will have cached this for the TTL time and will not resolve the domain again until that expires (this is the reason for reducing the TTL time well before making a DNS change if you have control of that record (older versions of BIND set the TTL in the SOA record; TTL can also be set on an individual resource record)). You can use dig ( dig < host or domain > )to see the records being returned by the nameserver your client is using, that should indicate which version it is using and the remaining TTL. (I'm referring to dig above assuming you are using a GNU/LINUX/BSD client but I think there is a version of the tool on other platforms, too) (I also started writing this wrote before reading your edit -- is that how it was originally set up? -- If so it could still be a caching issue, the TTL times should give a good indication of this. Unfortunately I have not familiar with DNSStuff is or its outputs so can't help there) 

Some RAM suppliers, such as Crucial and MemoryC to cite two local suppliers I have used, provide a web-based tool to determine which of the memory they sell is compatible with your system(s). This gives you some confidence that any memory purchased will be compatible. Not the cheapest way to get memory but reliability is an important concern. 

After reading Amazon documentation I came up with a Java method that returns the IP of all instances, but this is not what I want, I want a method that returns only the instance-id or the public IP address of the running instance. 

I have been trying to use Amazon Web services, specially EC2 and RDS. Nowadays most CMP (Cloud Management Platform) like Eucalyptus, OpenNebula, OpenStack, Nimbus and CloudStack all support Ec2 to a certain level, some do it better than others. 

I have an AWS EC2 instance deployed as a server, and I need to find out its public IP. Howver, to know that I must first know the instance-id of my instance. Objective: 

Can someone tell me if Eucalyptus, OpenNebula, OpenStack, Nimbus and CloudStack support RDS? If not, then how I can I use third-party software to access Amazon's RDS service using the previously mentioned CMPs? 

In this code I have an array with the instance-ids of all active instances, but I do not know if they are "me" or not. So I assume that my first step would be to know who I am, and then to ask for my public IP address. Is there a change I can do to the previous method to give me what I want? Is there a more efficient way of doing it? 

Furthermore, there are plenty of questions about EC2 in this forum (even ones asking for programming answers) and the fact that someone actually answered my question proves my point - this is the right place to ask it. I'm leaving this comment as a disapproval of your actions, it is people like you who drive new users away. I know I marked this as answered, but I am still doing it. Why? Because it is the principle that counts. 

EDIT Really? Can someone explain me how a question about a virtualized server, in this case an EC2 AWS instance is off-topic when inside a forum to ask questions about servers? This fully complies with the following two points of the rules: 

That's it. I also created the necessary keys and passwords as usual. What's the problem When I connect to my server via its public IP with port 9000, I get the following error message: 

I can access the web page, but I immediately see the given error. I have another instance configured the same way but with graylog 2.2 and it never gave me any trouble. What did you try to solve the problem? My first idea was to check the health of the services. When I run , replacing XXXX with , and they all are active and running. I also checked for similar errors in this forum but they are for different OSs so I am kinda lost here. GIve me some specs! Following are the specs of my OS (): 

Background I have an Ubuntu 16.04 clean VM, and I am trying to install the latest version of graylog on it. Unfortunately, it is failing. What did I do? First I created a clean VM with the afore mentioned OS. Then I followed the official instructions step by step: $URL$ I installed the prerequisites, mongo DB, ElasticSearch and last but not least, Graylog 2.4. I also configured to have ( before installing graylog ). Graylog conifg Now, we both know graylog won't start out of the box. You need to do some config. My configuration for is really simple: 

But when it comes to Amazon's RDS service I just can't seem to find any information. It's like no CMP supports it. On my research I came across a website that suggests the use of third-party software like HybridFox, RightScale and enStratus to have an RDS like support but I don't get it.