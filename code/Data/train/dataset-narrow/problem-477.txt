I know indexes reduce data modification performance, but I have a task where (almost) all updates are done on items in a sequential order. Would a clustered index improve or reduce performance in updating these rows? The clustered index would be on column where is an column. is thus never changed and will be sequential (in addition rows should never be deleted). This is the format of my update statements: 

So for each there's really only one row of results; the comments should be combined in the order of . The above linked trick works to get all the values for a specific query as one row, but I can't figure out how to make it work as part of a statement that spits all these rows out. My query has to go through the whole table on its own and output these rows. I'm not combining them into multiple columns, one for each row, so doesn't seem applicable. 

Semantically that data IS , setting it to something else is awkward at best. We can always assume NULL is NULL, we know what that is. if you use then all of a sudden you have to keep track of what you're setting as your new NULL. Just use the construct of the system where possible. Using a value for a null has problems as well; a query will never match a null to anything; it's not a value so no range of options will ever match it; the exception is the condition, which has special semantic meaning as has special semantic meaning. In addition you're dubiously exploiting the database's constraint. That constraint is meant to mean that column always has data. Your data technically fulfills that constraint, but it does not semantically. The only reason I can see for setting and using a homebrewed "NULL" value is when an application is inserting it's own data and it has it's own value for . For instance PHP's when stored to a database column will appear as a value, not a SQL . This can be avoided with proper programming in the application however, and I would only consider the "fake NULL" solution if I had no control over the application that was inserting "fake nulls"-- at that point the battle is lost, and your solution becomes the most applicable. 

Say I have a table clustered on , and in all cases I want my results to be ordered by so I additionally always in all queries. Does this affect performance in any way or is it ignored by the profiler as the rows are already in this order? In my instance I am using a SQL Server 2005 database 

But I don't think the problem is in the SQL, something's getting cached wrong or something and I think it's forgetting my alias or some form of metadata necessary. But only sometimes. Some of our remote servers are on satellite connects which go down or time out, but I can reproduce this on a DSL location that's 5 blocks away where it never times out. What's going wrong here and how can I fix it? What would cause the identifier to fail like that? 

Say that I have a table of People, and these people have a Country column and a unique Primary Key. It's a demographics table, so these are the only things I care about; what Country and how many unique people are tied to that country. I am thus only ever likely to SELECT WHERE or ORDER BY the Country column; a clustered index on the Primary Key doesn't do me any good, I'm not accessing this data by PK, I'm accessing it by this other column. Since I can only have one clustered index on a table, declaring my PK as Clustered would prevent me from using a Clustered Index on Country. In addition, here's a good article on Clustered vs Nonclustered Indexes, turns out clustered indexes caused insert performance issues in SQL Server 6.5 (which at least hopefully isn't relevant for most of us here). 

How can I change which server type the trade template is targeting? I've tried but there doesn't seem to be an option to change this. Is it possible to change the target server version or does the whole trace have to be made again from scratch? 

It is the case that IDENTITY_INSERT can only be set to ON in one database table at a time, but why? Since columns aren't globally unique I can't think of any dangerous situation that could be caused by inserting identities into more than one table at the same time (at least not more dangerous than generally fudging with IDENTITY INSERT). IDENTITY INSERT should rarely be used but what is the reason for the hard limit? 

I've got some split out into multiple rows due to database design, and for a report I need to combine the from each unique into one row. I previously tried something working with this delimited list from SELECT clause and COALESCE trick but I can't recall it and must not have saved it. I can't seem to get it to work in this case either, only seems to work on a single row. The data looks like this: 

Using SQL Server 2005 databases and SQL Server Management Studio 2008, I need to query a single value from each of n linked databases at a regular interval. The servers are physically remote and I can basically count on a timeout, attempts to create a query that completes have failed. Every attempt fails when a server times out, I've specifically gotten the 'Login timeout Expired' error. Ideally I need a stored procedure that generates a row or table with a column or row for each server and a value indicating what my value is in the remote database, or if I get a timeout (the value on the remote database cannot be ). My first thought to gather the data was 

I've got a table that's a working copy subset of a base table. The base table has a composite key with a clustered index. However I need a single column unique identifier for my framework to work with the table. Both keys are completely unique, and I'll need to make a clustered, unique index on the original, composite key. Should I keep the composite key as a clustered primary key, or should I use the single column identifier as a non-clustered primary key? Though my framework requires a single identifying column I don't actually have to specify it as the primary key in the DB layer, so I'm not sure which is preferable. I'm using SQL Server 2005 and the single column identifier will be an . 

This basically updates data from the only row in the remote table with the matching row in my local table; it's a Cartesian join but technically I want that. The error is: 

I have two tables, a large (millions) table of for orders, and a small (hundreds of thousands) table for metadata on a subset of those tickets. The metadata is information about when a ticket was paid and is only created when a ticket is first paid. I'm currently using a view to present all of this data; a shows all rows from and associated data if it's there. My problem is how do I handle creating new metadata? I can't update the view directly for new entries, so I have a stored procedure that takes a ticket number and creates or updates the table, however it only does one ticket at a time; I could be updating hundreds of tickets, so this doesn't work. There's also no simple select criteria I can use; a specific set of potentially non-sequential tickets may be entered. I can't input a range of tickets, I need an explicit sort of statement to update only the correct rows. What is the best way to this sort of insert for related data? I've seen some implimentations of stored procedures taking in a list of values as a parameter, but all of them look very hacky and risky. 

I'm creating a view on a SQL Server 2005 database via SQL Server Management Studio 2008, the view reads from two tables. The view is accessed from a web application and I want users to be able to update one of the tables by updating the view, but not the other. One table is live data from an important system and should not be changed by this application. I control the web application so it shouldn't ever generate a query that will update that table, but I don't want it to be able to at all. Is there any way to set permissions so the view itself can only read from one of the underlying tables? Everything I know about permissions is at the user level, but I thought there was a way to make a view read-only. 

I have a database of security camera footage in a denormalized database. I have Locations which have multiple Cameras which take multiple images. Location + Camera + image capture_date is the clustered primary key, and currently the only index on the table. The kicker is searching a single camera takes <1 millisecond from SSMS and ~70ms from my web application. My current working CTE solutions take around 3 minutes for three cameras. To give an overview of the cameras at a location I need to select 2 images from each camera nearest a given date (such as the current date). Because of this I need an absolute value (dates before or after the search date are equally valid), thus I'm searching by the smallest . Here's the current code. It works but it's not SARGable and it's extremely slow. I also only need the top 2 rows per camera in the CTE, since there may be hundreds of thousands of images per partition. 

But the timeout still causes the whole query to fail, I think it's because try only works if the connection doesn't fail? Is there some way this can work? The only other solution I can think of would be a stored procedure on each remote database to send this record so it's all on our local database (which we can depend on reaching) but that would introduce much more complexity than this query is worth. 

There's only one master row I compare all servers to, and I don't select it's here. This is an example of the rows I'm comparing. Each has a primary key, a single column and a giant list of that's dozens of columns. I want to compare all columns except LocationID, but I need LocationID to identify the rows. 

Which generates a single row with all the values I need and indexes I can use to report the needed values. But a single time out in one select statement kills the whole query (it does execute correctly if all servers are reached). I can safely assume that at some point in the query at least one server will time out. I tried an exception, but it doesn't seem to work (I've never trued to use exceptions in SQL Server before). I tried this in place of each subquery: 

I've got a .tdf SQL Server Profiler trace template someone wants me to run but the template is targeting SS 2008 R2. While my SSMS is 2008 R2 the server I need to trace is SS 2005. When attempting to trace the server the server type is locked (generated from the actual server) so I can't just select the template I need while it's marked as a different SS version.. 

In this example say CS02 is my Master record, so since all settings are the same in CS02 and CS03, those rows don't show up, but CS06's does. But in my query, I'm not actually catching LocationID so I don't actually know which row was returned. This returns the rows I need but NOT the , so I don't know which rows are wrong. Is there any way I can include in the results set while kicking out the matching rows? The solution I thought of was to make a row for each server in the table, so each is represented, but they all have the same data other than that. My query should then return the and my info, but is that the best way to do it? 

I'm comparing a bunch of tables from different databases on different servers to a Master record. I need to know which servers, identified by , have the non-matching rows because they might need maintenance. I've got a simple query where I compare a table where each row is the configuration from each server; has one row per server with all configuration plus which is a column that tells me which server it is. I compare these all to a table which has the right settings, but I exclude the since it won't match. Simple query below: 

The values are not necessarily continous however. A sequence of is also possible if is not set to be updated, but they will always be in order. When all updates are in sequential order, will a clustered index improve or reduce performance? 

I've noticed that for Crystal Reports made by our organization and by some of our ERA software providers have a tendency to use physical tables for their reports' data sets, rather than using a view or a stored procedure to collect the data. Occasionally I've seen reports use stored proceedures which then use physical tables rather than temporary tables to store and manipulate data sets. In these cases the report output often exists as a table like or similar, and it may or may not be devoid of data when not in use. These are always cases where the report is generated on-demand, so this is not a case where a report could be generated once and served multiple times, and there are not multiple reports/stored procedures accessing this data at the same time. What reason would there be for using physical tables for reports like this? Is there a logical, technical or performance related reason to do so? In generating reports I personally have always used views and stored procedures with temporary tables or better yet derived tables to avoid extra disc reads involving clearing out/deleting a temporary table. 

I've got Schrodinger's query here; you can run it once successfully, then 5 times in a row, and it works fine. Change anything in the query, even whitespace, run it again, and it might stop working the next 5 times. It seems something breaks when it's cached; it it doesn't run once, the same query won't run properly until it's changed. Sometimes the same exact query that run before won't run or vice versa. The query is run against multiple remote servers (one at a time) so the problem seems to be related to that. They're linked servers and I can query them properly with a normal query. The servers are matching versions (SS 2005). If it matters I'm running this via SSMS 2008, though my automated script (PHP with SQLSRV driver) appears to be having the same problem. Here's the simplest statement I could make that still gets the error: 

Unfortunately I couldn't get a pure SQL solution to work so I made a command line script to query each database in a PHP, as the PHP program can handle the timeout errors. The queries were for a PHP web site so I considered the solution acceptable as it introduced the least complexity compared to a Visual Basic app (what others in-house suggested) and a Powershell script (a great idea but requires the most set up as no one here has used one).