I'm building an inventory database that tracks computer equipment and other hardware devices. At some point in any device's life it is retired and gets archived. After it becomes archived it needs to be tracked as it is removed from service and properly disposed. I originally designed the archiving mechanism using an exact replica of the active database that would receive its data using a trigger on delete from the active database. The archive database includes replicas of all the related tables because as certain foreign related records are no longer pertinent, they should not be accessible to users to use with new devices, but are required for referential integrity and querying with the archive tables. Keep in mind that the concept of archive here is not just to keep a history or a log. The archive is a part of the business process, and users will need to query and update devices that are both active and archived. The ERD below uses the table as an example where all entries and updates are copied to the table. When users should no longer be able to enter inventory records of a certain device type, it is deleted from the table, but remains in the archive table. This pattern is used on all tables to ensure the archives refer to valid data, hence the replica of all tables. Active Table Example (Other related tables omitted) 

With this setup, devices are archived by setting field to true and entering an . I could query any device easily whether it is active or archived. (Please ignore the field, as this is used for an unrelated concept). Take notice of the Phone subtype table, where I had to propagate an index of DeviceID and IsArchived flag because phone numbers must be unique for active devices. I have to do this with other subtype tables as well. I don't know if this is a good or bad design. This part really confuses me... What are best practices for handling soft deletes where foreign key values can be marked as deleted. The only thing I can think of is create a routine that searches for all records that are related to deleted data, and create a report for users to resolve the discrepancies. For example, if a table of locations is related to the devices table, and some locations are soft deleted, then the devices refer to locations that no longer exist and must be moved. By the way, I'm using MS SQL Server 2008 R2, and I plan on using Entity Framework 4 in my application. I value database maintainability over performance. Thank you for reading. 

I would suggest an table which stores the current appointments for each doctor. We can add some constraints on this table which limit the appointment start times to even ten-minute times (e.g. 9.00, 9.10, 9.20) plus add some other common sense checks like after and doctor can't have two appointments starting at the same time. Assume that you'd also like doctors to only work between 9am and 5pm, because everyone needs some work-life balance. 

You could deal with the issue by specifying the style and doing an explicit from string to datetime rather than an implicit conversion. $URL$ 

One way to achieve that would be using to stuff all the products for each order into one line, then match up all of the orders that have the same products and use again to stuff all of them into the same line. See example: 

Storing in mysql table should be fine. You need to consider how the data is going to be accessed and retrieved when you're designing the table and indexes on it. I'd say that you're likely to be doing queries which return all of the past searches for a particular user (or perhaps even just the most recent by user) so it would probably make sense to cluster the table by and . This design should also help reduce contention if lots of inserts are happening at the same time since each user will be inserting into a different part of the table. 

So to get to that format I have a query which grabs the distinct values of and by generating a of the values in each level, like this: 

The issue I have with this design is I'm not sure if the relationship with and / is a good or bad design decision. Is propagating a non-key field like to other tables a bad design? I don't have enough experience with database design to make an informed decision. 

I normalized the tables to avoid using NULLs. The problem is that some of these tables depend on each other due to business processes. Some devices must be sanitized, and some are tracked in another system. All devices will eventually be disposed in the Disposal table. The issue is that I need to perform checks, such as if the boolean field is true, then the cannot be entered until the fields are entered. Also, if the boolean value is true, then the fields must be entered before the can be entered. If I merge all of these columns into the table then I will have NULL fields, but I will be able to manage all of the business rules using CHECK constraints. The alternative is to leave the tables as they are, and manage the business logic in the stored procedure by selecting from the tables to check if records exist and then throw appropriate errors. Is this a case where NULL can be used appropriately? The boolean fields and basically give meaning to the NULL fields. If is then the device is not tracked in the other system and and are NULL, and I know that they should be NULL becuase it is not tracked in the other system. Likewise, and I know will be aswell, and a can be entered at any time. If is , then and will be required, and if and are NULL, then I know they have not been officially removed from that system yet and thus cannot have a until they are entered. So it's a question between separate tables/no NULLs/enforce rules in stored procedures vs combined table/NULLs/enforce rules in CHECK constraints. I understand that querying with NULLs in the picture can be complex and have somewhat undefined behavior, so separate tables and stored procedures seem beneficial in that sense. Alternatively, being able to use CHECK constraints and have the rules built into the table seems equally beneficial. Any thoughts? Thanks for reading. Please ask for clarification where needed. Update Example table if they were merged and I allowed NULLs. 

If a user can have no more than a single role, and this will ever change, then put directly on the table, then the relationship between table and table is a single role to many users. 

You mention that you added single-column indexes for each foreign key in your fact table. Often at least some of the foreign keys have low cardinality so they are likely not useful in an index on their own. $URL$ They may be more useful as part of a multi-column index which you can design based on the way that you expect users to query the table. If your workload suits it, then non-clustered columnstore index should be considered on large dimension tables and fact tables. They are ideally suited for data warehouse workloads. $URL$ Since you are using 2014 then non-clustered is the only option if you want to keep constraints and other indexes. 

Consider having a row in the table for each attempt that the user makes at the test. This means, then, that you would have to make the composite primary key of the table to be and (which can be an column). I would maybe think that the table should be a many-to-many relationship between and rather than between and , assuming that each time a user attempts the test they may get different questions. Either way the primary key on this table needs to include more than the single foreign key, it should include both foreign keys. 

The query you wrote is basically asking the database to give you any two rows of data (any two orders). If you wrote the query with an clause then your results should be the same each time. 

Due to this design's granularity, I could theoretically allow any mix of statuses for devices with this design, but I wanted to control it so I wrote some triggers to only insert the correct mix of statuses depending on whether the device is network capable. Triggers as follows: 

This question regards the proper use of NULL and utilizing CHECK constraints for business logic vs stored procedures. I have the following tables setup. 

For the record, this design seems a bit absurd even to me, but this is my thought process. In this one, the presence of a in the table is equivalent to saying = true in Design 1. The has a foreign key constraint and is used to ensure only networkable devices are entered. Can do this using a CHECK constraint (see below), and by making a computed column that is equal to . 

This is an inventory database for IT assets. The models used are trimmed in order to focus on the problem at hand. Using SQL Server 2008. Thanks for taking the time to read and for any input you can provide. My design includes a table which holds the various devices that can be entered into inventory. Each device has a boolean flag, which states whether a device has network capability, e.g., for most computers , for hard drives ; some printers will be true, and others will be false. You get the idea. The field determines if network-related information is relevant when an inventory record is created. Design 1 My first design uses an index on and to use in a foreign key constraint with the table. 

Of course, you may not have permission to do 2 or 3, in which case it's a question for the person that does have permission. 

Let's assume that you have a numbers table. If you don't many other people have described how to create one. If all else fails, this could create one for you but it's probably not the best way. 

I think this is probably a good design that you proposed. Just make sure that the table is indexed properly. Consider that every query to the table will probably filter by content_type so that should probably be the leading column in a multi-field index. 

Then you could also add some constraint to the table if you want to enforce a rule that two courses can't be timetabled at the same time. If a course has multiple sessions during the week each will be a row in this table. 

Test this and see if progress is being made. Further steps may be necessary but hopefully this is in the right direction. 

This is an alternative scheme to the ones already proposed, it's a bit like the summary table, except that it isn't aggregated, just 'partitioned' by player. 

The drawbacks to this approach are that now your writes take longer so that you can do faster reads, and this might not be suitable if the filter that you use in your query on the column isn't predictable. 

We can insert some data to this table to see what it looks like. Note that the third insert will fail because it is prevented by our constraint. The doctor can't have two appointments starting at the same time. 

Each team has a dev server stack where stories are worked. There are 25+ databases per server. When product/project determine which stories are to release, those changes are merged to Dev Release - and eventually flow back to team dev. Dev Release is then used to script the deployment. Thus any ongoing projects (long term / third party integrations / etc) aren't included with the release. This means that there is not a point guaranteed where dev release and any team dev are fully synchronized. Also of note, the sequence of changes made to dev release are not related to the sequence that changes are applied within team dev... Some ponderings: One way (problem): If we have a single branch for each team dev server - it appears to be easy to keep in sync with tooling (RedGate Source Control, etc)... ...but prepping for a release (in Git) either means cherry-picking commits (too much work) or git copying files and losing dev commit history (too much work). (I assume that SVN's file deltas would preserve dev history - but would still be a pain (as its not a merge)... guessing) Although dev history is lost, we would still have a list of release changes - but that doesn't work for me. This feels wrong. Another way (problem): If we branch per story - then it would be fairly trivial to merge to dev release and preserve object history. However, there wouldn't be a single branch that matches the state of the database, and thus tooling (RedGate, etc) will always be complaining that the DB and the repo are out of sync. This feels annoying. At my previous employer, we branched for release using SVN. All forward development teams worked towards a common release - thus forward changes were developed against the trunk (or a dev branch reconciled to the trunk), and hotfixes were applied to the release branches. Depending upon the product, we had different workflows for publish - but the idea was similar. Most all the links I've found (even on this site) seem to point to that workflow - as described very well here: $URL$ However, all the teams (excluding hotfixes) were working towards a common release - thus the dev branches had to reconcile / merge. So, if we had a project / integration that wasn't in the release, we'd have to back out changes of the trunk for the freeze - and reapply once the next release cycle started. Annoying - but doable. Of note, the changes to the database could be applied in a serial fashion - as they were (mostly) all related. So, it's no surprise that tooling SSDT / RedGate / etc seem to be geared for this... So, just curious if anybody has any pointers, helpful URLs, or can point out where my ponderings have gone awry, I'd be grateful for the info. 

As mentioned, the leading % makes things tricky, but if the event_details field contains a list of delimited items and you are searching for one of those items then you could create a child table event_history_details that contains one row for each item in the event_details field for each event. Such a setup would scale well and be SARGable. Unfortunately it involves changing more than just the query. 

When I load this data into my data warehouse I load it into a table that has a parent-child relationship like this: 

It is not necessary to distinguish between measures and dimensions when defining your columnstore. All columns are stored in the same manner. All columns included in any query should be in the columnstore, in most cases it is best to put every column in the table into the columnstore. 

That looks a little awkward, so if anyone can improve that date logic happy to take suggestions. If a doctor wants a break, then enter the break as an appointment and it won't be available for booking. Note that the table constraints don't enforce non-overlapping appointments. This is possible but it's more complicated. If this were my system I'd think about some system (e.g. trigger) to finally verify that the appointment doesn't overlap with an existing one at the time of insert, but that's up to you. 

This is probably not the sort of case where filtered indexes really shine - typically that is when the index represents only a small portion of rows in the table (like, a quarter or less) and in this case the filter predicate matches about three quarters of the table. That's not to say that you wouldn't get gains here, but it might be a bit of a tricky research project to get good designs. (Then again, indexing is always like that.) What about the current indexes on the table. Are they being used at the moment? Are they only being used in queries that have the predicate that you mentioned? If so, then it might be an easy win to convert some of your existing indexes to have a filter condition (instead of adding a new, filtered index). You probably already knew this, but filtered indexes won't be used by where the predicate that matches the filter uses a variable or parameter. $URL$