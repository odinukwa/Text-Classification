As for the lack of security, I hosted a talk about nginx and PHP-FPM at nginx.conf 2014 end October. Slides are available: $URL$ Video soon available. 

does not replace existing header but adds a new one, possibly leading to duplicate that might be filtered when it is supposed to be unique and/or only the first one of its type is read. Another possibility is that the HTTP status code from your response does not match the list defined in the docs for this directive, since you did not use the parameter. The text/html you see might then be nginx' default 404 page. Anyhow, what you want to use for your need is the directive: 

You will need to signal nginx to reload TLS certificates. If you won't be doing that, there is no way what you wish is doable. Part of the reason is checks are done at configuration loading time/statically (file existence, content validity, private key match, etc.) which cannot be reasonably done at access time/dynamically (when a request is processed), which would kill performance, and even the capability of serving any request in a reasonable amount of time at all! Now, if reloading becomes acceptable to you, it would be unwise to make nginx allow people to drop files where nginx loads them, as you are providing a direct way to compromise your server(s), either intentionally/maliciously, or by accident if a user loads up a bad certificate, not being able to replace it since the server won't accept it! Luckily for you, nginx has safeguard to prevent the latter and won't load an incorrect configuration. But for the sake of the first argument only, you shall use side-channels for this operation and never, ever, the same one. 

You could then use regex locations to automate parsing the location string to grab the corresponding (existing?) file in the include directory. Pay attention to something very important though: Traditionally, the content of the -like directories are blocks, making nginx configuration granular. Ensure their content suits inclusion in a block (trying to include a in a obviously won't work). Also pay attention to the scope of directives you include there, since not all of them might be used inside . 

The only way to load-balance in nginx is having a single frontend (reverse-proxy) host load-balancing backend servers. The idea/hypothesis behind this design is that load will happen on the backend only and that your single entrypoint will always be able to cope with whatever amount of traffic it is supposed to deal with, since it simply redirects and never processes anything itself. What you are talking about is actually failover, not load-balancing. Your concern is the failure of your single entrypoint. As @coding_hero explained, this has nothing to do with nginx, it is something to be dealt with at the underlying layers (OS/network). One way of doing it might be read on the following page (old example talking about Debian oldstable though, commands might need to be freshen up): $URL$ Heartbeat is a well-known technology allowing several identical servers to monitor each other, electing a master and failing-over to slaves with needed. You even have dedicated network hardware doing the same job by rerouting (or maybe reconfiguring routers on-the-fly to reroute?) traffic to the currently elected master. 

Thus, all relative paths are computed against this one. To check the value for the binary you use, check . In your case, I see several options, from best to worst: 

Having several nginx process might be normal... or not. If you only have 2 processes, then you probably have the master process and 1 worker process, which is the way nginx works. Having several master processes, unless it is what you wish, is probably sign of trouble. If your PID file is empty, that is definitely a problem: it is used by the managing service to identify which process to check for and to send signals to. Empty PID file = service will try to send signals to the void... 

Use absolute paths (either directly or prefixing existing paths with a variable). A simple script would make it easy to transform the original paths into propers ones, even launched as a 'post-hook' on file drop by the fellas Create symbolic links in the computed locations referring to the expected one Build nginx binaries manually, configuring that command as you see fit 

will redirect to . According to best practices, it is better to enclose regular expression routing directives (, but also regex ) inside prefix locations. In You could rewrite the previous examples as 

Note the use of the variable, reproducing the scheme used to connect to the frontend with connection to the backend. I am unsure about SSL configuration on the backend. I guess you need to use the same on each of them and the same SSL certificate as on the frontend. I do not know if you can use different certificates on backends, each with a different and change SSL parameters for connections with the proxy module. 

is related to host matching. On the nginx side it is done through . The syntax might be different: check the docs. means you explicitely want the QUERY_STRING value to be empty. If that is mandatory to you, use an block matching against being empty. That is an exception because I cannot see a better way of doing it: should be avoided as much as possible. You should put the block in a which isolates the right URI. RewriteRule can either be replaced by a combination of and (preferred way) or (which is to be avoided as much as possible). When you use , try to use prefix locations as much as possible, since regex locations evaluation is indeed slower. Based on performance, it is thus best to avoid them whenever possible too. 

While 502 corresponds to backend timout, 500 is a server error, indicating a misconfiguration. You might wish to fix the following on first hand: Before the sleep, you actually replaced the first line of with 

Redirect will be done through the use of rather than using , which is discouraged when avoidable. 2. www.domain -> domain Now to redirect www to non-www, I would use a , avoiding to use : 

This reply is made assuming the rather unclear question is well-understood from my side... You wish to make requests like the following: 

HTTP 303 is a redirection. There is nowhere in the configuration snippet you provide which creates any redirection whatsoever. It thus comes from a PHP file being processed in the block you are providing as I trust: you need to check the PHP application to fix this unwished redirection. 

You made the usual mistake of people not really understand how nginx works. Remember the following: nginx always serves a request with a single block only. I suggest you (re)read the following: How nginx processes a request Now, looking at your configuration, backend requests would need to be served by 2 locations: 

Your backend has nothing to do with the authentication, since it is done by/with the proxy. Do not proxy that header field. If you wish your backend to authenticate the client again on its side, you should activate there too, with the same user/password database. Be careful when managing users, you would have 2 copies to keep synchronized now... 

How could you improve upload performance by making nginx buffering to the disk? Every time bytes are received (limited by the MTU of the transmission is the best case, or in the worst case with 1b-payload network packets), those will be written to the disk. Usually one tries to avoid requesting accessing loads of times to the disk, since it is a slow device... , as stated in its documentation, is mainly used for debugging, when you wish to keep a copy of clients requests body for further analysis. Be warned that setting it to makes nginx keeping all the request files by not deleting them once the request has been processed. The value does it. That directive does not, as you may have wrongly understood (de)activate the use of disk buffering per se. You see nginx using the disk for buffering of request bodies because its RAM buffers cannot contain them. It warns you because it most probably has a very bad impact on your server performance. You may wish to enlarge your so file uploads will stay as much as possible in RAM, before being written with the minimum amount of writing calls to the disk afterwards. 

Avoid at all costs automated tools which lack, by definition, the brains to properly convert rules. There is no simple 1-to-1 match between Apache and nginx ways. They have 2 different mindset. You thus need to understand what you are doing and use the nginx way of configurating it. That is IMHO a complex problem out of the reach of an automated tool. Concerning rules: 

I suppose and are empty, making the subshell fail/being empty, meaning that this line is actually interpreted as 

Following documentation, the first on is called a prefix location, while the second one is a regular expression (regex) one. nginx will check both but will eventually only select one, which in your case is the regex one. Now, when processing it, nginx will forward a request for to PHP, building the path from , since it is the only one defined. The backend then fails, unable to find the specified file. You might wish to try the following: 

Special cases such as directories where Web server/applications need write access, such as an upload directory, make an exception by adding the for permission to this directory (or make this specific directory locked to anyone else by making it owned by ) Now, to automatically make new files being readable by the Web server/applications, you can use the flag from the filesystem permissions. This will automatically change the group of any added file to . Sum up Here is a quick example so sum up everything: Your web server and any backend application belong to the group. You have a Web root on 2 environments: 

To solve the problem of multi-access to files, I would recommend using the access control mechanisms built in the Linux filesystem mangement, with the default rights: 

Thus, you should only define once. If multiple occurrences are found, I guess the first encountered value (when used at run-time) will be chosen. 

Requests are limited to fit the defined rate. If requests are incoming at a higher rate, no more than the defined number of requests per time unit will be served. You then need to decide on what to do with those other requests. 

To get advice on getting a clean and scalable configuration, listen to nginx inventor who hosted a talk at the nginx.conf user conference: Scalable configuration with nginx 

Using , try to identify the master process (that is the one with PPID 1 and calling itself 'master in its description). Then send it a TERM, KILL or QUIT signal. Manually remove the PID file Check your service file (if you use init, it is most probably ) for the PID file name and compare it to the location you provided Ensure no PID file is there, no nginx process runs anymore and use your service normally to start/stop/reload/check status again. 

You did not specify a directive in the configuration snippet you shared. It is supposed here that this directive exists, because otherwise your question becomes nonsensical. You specify not wishing the redirects being done to the reverse-proxy, but rather directly to the frontend. However, your configuration tells exactly the opposite. I suggest you read its documentation (again). The log snippet you shared in the end seems to come from the frontend component, not the reverse-proxy one (). However, if it were (because of docker container + misleading port-binding), it would lead towards the absence of a directive, cf. bullet #1. 

Disable by changing table default policy to and deactivate any or rule which might stand in the way Test again and check that it works. If not, the problem comes from somewhere else 

Check your configuration is statically validated with Check your configuration is dynamically validated by monitoring an error log defined at level while issuing a reload (either or ) Create a test location in each server (see below) Ensure your nginx is built with the SNI extension (normally yes if yo uuse a pre-built package) Ensure the right server is being used: if the domain name selection through SNI fails or if SNI is not available, nginx will fall back to the default server to serve content. Default server is, unless explicitely specified otherwise, the first found in the configuration file. Ensure browser cache is cleaned-up (and that any cache whatsoever in-between server and browser gets updated/is purged) 

Once you have tested the locations properly and you switched back to the configuration you provided, any following error is most probably coming from the backend or the way you configured nginx talks to it, but not the locations themselves. 

As you noticed, the use of seperate servers is not really necessary here, as a simple redirect would have been sufficient. However, I find it prettier if HTTP requests never made it to the right server. It would also help scale your configuration if you end up with services not (yet) supporting HTTPS. As for the arguments, I took your provided examples to the letter. If you wish to have some other arguments you do not want to touch and you want to redirect in a generic fashion, you may need to chain several (or use if you know what you are doing) to rewrite the variable, ending up with a rewritten construction ready to be sent to the redirection URL. 

The FOSS version has the , which you need to manually add at configure-time since it is not by default part of the binary. 

If 1 hit every 2 requests works properly and the other is 404, it highly looks to me like 1 upstream server is working properly and the other is returning 404. Remember nginx tries upstream servers in a round-robin fasion by default. Since you do not see any 5xx error, it suggests there is no server error. 404 is a client error, which means a request could not be served because nothing matched it. I would suggest to have a look at your tomcat servers. Since you can connect to 154.25.39.126 fro your machine, it seems this one looks properly. Try to dig into the one listening on the cs2 machine local interface. I do not see anything involving any nginx trouble so far. 

(You even do it twice... purpose? Double copy-paste?) But what you have not done yet at that time is replacing the second line, which remains being 

On a side note, be careful, though (quoting the docs): [ directives] are inherited from the previous level if and only if there are no directives defined on the current level. 

prefix locations cannot be nested in regular expression one since the former have precedence over the latter during checks. Read the directive docs to understand how the block serving the request is chosen. You can (and you wish) doing the opposite activates FastCGI reverse-proxying for the current block, but is not inherited (what a mess it would be!) 

It is a general good practice to avoid filtering requests with regex locations, which are order-based, which is bad (remember Apache directives order sensitivity nightmare?). To filter, use prefix locations instead which are longest-match-based. You can embed locations withing each other if you ultimately need a regex one. Why not directly putting your directives into the location? Then, instead of using the variable (guessed from the match that would be variants of here), you could use . By the way, already contained the starting , no need to add one between the variables Avoid using redirections if you can avoid them. Avoid especially even more. Simple user redirections (URL rewriting done through redirection notifications sent to the client with HTTP status codes) can be done with . What you did here was an internal redirection (done locally on the server): its only use was to change the URI then used for the parameter.