These are numbers left padded with zeros. This will help make the point about compression later. So you've got three common queries: 

The issue is you're restricting on the instance field, which is resulting in the roles only appearing in one column or the other. To get around this, remove the restrictions on and the roles from the second database to the roles in the first, like so: 

I would strongly consider explicitly storing all the days a particular schedule actually runs on. This would give a structure looking something like this: 

The design doesn't meet third normal form, but not just because of the city. The fields STREET, CITY are functionally dependant on each other (if you change the city, the street should probably change as well and vice-versa). You could also have the same street, city combination represented in different ways (Foo St, Foo; Foo Street, Foo; etc.). To normalise this you would create a new table ADDRESSES which has the street, city etc. in and link the customer to that via an address id. This would also allow you to list several addresses for a customer (via a link table) if this is what you require. This still leaves you to decide whether to extract the city into it's own table. To fully meet 3NF you should create a cities table, whether you need or want to depends on the answer to the following questions: 

Yes this is possible, although you need to watch out for the following: a) Export using extended inserts so that the extended SQL syntax is used which will reduce the size of your SQL files (If you are exporting using PHPMyAdmin select both complete and extended insert options, as it creates complete inserts with extended insert syntax for large tables) b) Try using the GZip format which will compress the SQL files and make them smaller (depending on the hosting provider setup this may be an issue so you need to test it out with a small table) c) Export the tables in such an order that tables with foreign keys are imported last 

I would consider BI a business project, because the outputs are metrics that business is required to use. This is despite the fact that about 80% of the input is technical. In my experience, if the focus of the project is technical, then alot of time will be spent analyzing the implementation technology, yet if the focus is business them more time will be spent deciding which metrics are to be delivered, how often, how fast etc, and the technology will be selected based on the constraints. Infact most BI project start with database based reporting before outgrowing to full blown BI. Bottom line, for success focus on business, and bring in tech to deliver "goods" within budget and time constraints. 

Oracle is unable to do partition pruning when a function is applied to the partitioned column. From the docs: 

There will be cases where the compound index saves you 1-2 IOs. But is it worth having two indexes for this saving? And there's another problem with the composite index. Compare the clustering factor for the three indexes including LOTS_VALS: 

No. If you re-write your view with the context filtering against the where clause (instead of the connect by), you'll get the previously set value for the context: 

Times are in microseconds. You can combine these to get average time/execution stats and so on. For example, this gives you the average time/execution for a given sql statement: 

If this returns anything, move the objects to another tablespace if you want to keep them before dropping the tablespace. 

If you're not already familiar with reading and understanding execution plans I'd spend some time learning these: your bound to run into performance issues at some point so knowing how to diagnose the problem correctly will become more important as it's harder to add new indexes or make schema changes when your row counts are larger. 

In these cases the order may be entered into the system at some time after the transaction took place. In these circumstances it can be difficult to impossible to correctly identify which historical price record should be used - storing the unit price directly on the order is the only feasible option. Multiple channels often bring another challenge - different prices for the same product. Surcharges for phone orders are common - and some customers may negotiate themselves a discount. You may be able to represent all possible prices for all channels in your product schema, but incorporating this into your order tables can become (very) complex. Anywhere that negotiation is allowed it becomes very difficult to link price history to the order price agreed (unlesss agents have very narrow negotiation limits). You need to store the price on the order itself. Even if you only support web transactions and having a relatively simple pricing structure, there's still an interesting problem to overcome - how should price increases be handled for in flight transactions? Does the business insist that the customer must pay increases or do they honour the original price (when the product was added to the basket)? If it's the latter it the technical implementation is complicated - you need to find a way to ensure you're maintaining the price version in the session correctly. Finally, many businesses are starting to use highly dynamic pricing. There may not be one fixed price for a given product - it is always calculated at runtime based on factors such as time of day, demand for the product and so on. In these cases the price may not be stored against the product in the first place! 

Why not add two indexes: a) A two column index for (state, city) - handles queries for state only, then state and city b) An index on city - queries on cities only To tune for performance further you may be able to add lookup tables for state and city then use numeric keys to speed up query performance. 

The order of columns in a primary key does not affect the insert performance, since the combination of the values in the two columns of the primary key are meant to be unique. However the order of index impacts the performance of SELECT queries. In the new order you will have a greater and increasing range of values (dates) across which the index is clustered (dates) as compared to clustering around 100 stores. If most of your queries are focused on a single store with the new ordering your queries will be slower. 

From the MySQL Workbench 5.2 window, select Database -> Synchronize model which will allow you to synchronize the model (EER diagram) with the database (you need to create a saved connection) for it. I recommend this method over just reverse engineering the model from the database since it maintains the layout of the EER diagram. One caveat is that you need to add new tables to the EER diagram manually. 

In addition to the above changes to mysqldump, I would like to suggest that you try Percona Xtrabackup www.mysqlperformanceblog.com/2012/02/27/announcing-percona-xtrabackup-1-9-1/ which seems to offer more backup options, including parallel streaming which brings up to par with other mainstream databases. 

No, the query optimizer will not create statistics for a view unless it's an indexed view, you can create an XML index on the xml column itself to speed up the view, 

You can install both on the same instance, If the performance is an issue you can use the resource govenor to make sure that the web database gets the lion's share of the resouces. Security wise I would reccomend to separate the two by using different instances but if you are careful in setting up security this should be no issue. 

If this failes, use the information in the msdn article to manuallyl set the correct permissions. Your SSPI context error is due to incorrect SPN for the sql server, The SQL server creates a SPN automatically when you start it and the domain admin has permissions to to that which fixes the SSPI error. After you change the service account for the server to a normal user download the Microsoft Kerberos configuration manager for SQL Server and make sure to create the correct SPN 

No, you dont have to do it again. The differential backups are just that so the only effect your mistake made was to make the restore time longer than neccessary. If your last transaction log backup was restored successfully then you are ok. 

Install SQL Server on the new nodes Use SP_HELP_REVLOGIN to copy the user account to the secondary node Take a full backup and a single log backup of the primary server and restore on the secondary with no_recovery Break the mirror and remove the secondaries from the primary server Create the endpoints with TSQL or use the Mirroring wizard to configure them and endpoint security start mirroring 

For example, say you're allocating a resource for all of March and April. If you just have a single record with start: 01-mar-2013, end: 30-apr-2013, does that mean resources were allocated/used on weekends and over Easter? If you have an entry/day and there's a record for 31-mar-2013, then the resource was allocated on Easter Sunday. If this is wrong, then you can just delete that record. With a range 1/mar - 30/apr, you've either got to update the start or end and insert a new record or maintain a list of "non-allocation days". Either way, these solutions are more awkward than just deleting. You could argue this could be avoided by forcing people to enter the exact dates that will be used (excluding weekends), but eventually someone will get lazy and enter the full range. You've not stated how many resource allocations you expect each day, but I wouldn't worry about the size of the data at this stage unless you're expecting it to be in the high millions. 

Your view has to apply some form of function to start and end dates to figure out if they're the same year or not, so I believe you're out of luck with this approach. Our solution to a similar problem was to create materialized views over the base table, specifying different partition keys on the materialized views. We've tailored ours to match common base queries so that we get query rewrite benefits as well. You may need to get users to use the MVs directly to ensure you get the partition pruning working as you need, rather than relying on query rewrite. (Updated to remove incorrect example and add info regarding applying functions to partition columns) 

Do you have an index on account_id? The second problem may be with the nested sub-queries which have terrible performance in 5.0. GROUP BY with a having clause is faster than DISTINCT. What are you trying to do which may be better done through joins in addition to Item #3? 

MySQL does not properly utilize CPUs with more than 4 cores, so you would go for 4-cores max, and as much RAM as you can get to be able to carry out as much of the processing in memory as possible. 

The composite primary index will always ensure that the combination of col1, col2, col3 values are unique. If the purpose of the index is to ensure that the values are unique then you will have achieved that A composite index also provides for index matches on any combination of the columns col1, col2, and col3 You would create separate indexes on col2, and col3, if you join or filter using any of the columns without the others. I always prefer a numeric primary key (with no business association), and unique indexes over a composite primary key where necessary. 

Schedule a job to backup one database and apply the backup script to another, that way you never need to worry about the synchronization as it is done automatically on a schedule 

Go to Model -> Diagram Properties and Size and increase the number of pages for the model Select Arrange -> AutoLayout which evenly spreads out the tables in the model. 

Whereas you can have multiple unique keys in a table and the referenced columns may allow null. By declaring a primary key, you are saying "this is the candidate/surrogate key that should be referenced by foreign keys". This is well understood by database developers who are unlikely to apply a foreign key to a unique constraint when a primary key is available. Unique constraints can also be used to enforce rules such as "a customer can only have one default contact number". A primary key can't be used for this purpose, as customers may have multiple non-default numbers, so the constraint can't uniquely identify all rows in the table. 

1: In some cases it may be worth including a column in an index if this means all the columns in your query are in the index. This enables an index only scan, so you don't need to access the table. 2: If you're licensed for Diagnostics and Tuning, you could force the plan to a skip scan with SQL Plan Management ADDEDNDA PS - the docs you've quoted there are from 9i. That's reeeeeeally old. I'd stick with something more recent 

You can use the package to "re-organise" a table like this. It provides functionality for you to create a temporary table with modified contents of an existing table as you describe. It also allows you to copy over the grants, triggers, indexes etc to the new table, then switch the tables over so the temporary table becomes the live table. See this article for a worked demo.