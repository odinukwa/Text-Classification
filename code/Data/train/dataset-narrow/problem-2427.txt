Here is an algorithm that uses $2k^2 + O(\log n)$ space. This is just the observation that the well known "Buss kernel" for Vertex Cover can be computed in log-space: Say that a vertex has big degree if it has degree at least $k+1$. All vertices of big degree must be in every vertex cover of size at most $k$. If $v$ does not have big degree it has small degree. Call a vertex $v$ interesting if it has small degree and has at least one neighbor of small degree. Any inclusion minimal vertex cover of $G$ of size at most $k$ must contain all vertices of big degree, and out of the vertices of small degree, it can only contain interesting vertices. One can easily verify that in $O(\log n)$ space one can determine whether a given vertex has small or big degree, and whether it is interesting or not. Further, we can count the number of big degree / small degree / intersting vertices before $v$ in input. Thus, given an integer $j$ we can also find the $j$'th vertex of big degree / small degree or the $j$'th interesting vertex. The algorithm computes the number $b$ of vertices of big degree, if $b > k$ the algorithm outputs that there is no vertex cover of size at most $k$. The algorithm computes the number $i$ of interesting vertices. If $i > 2k^2$ it outputs that there is no vertex cover of size at most $k$ (because one has to cover at more than $k^2$ edges using only vertices of degree at most $k$). Now the algorithm goes over all the $2^i$ possible binary strings on length $i$. For each such string, we interpret the $j$'th bit as whether the $j$'th interesting vertex is in the vertex cover or not. For each string compute the total size of the proposed solution ($b$ + the number of ones in the binary string), and check whether the proposed solution is a vertex cover by going over every pair of interesting vertices, checking whether they are adjacent, and whether both are $0$ in the binary string. This concludes the algorithm. Remark: because the number of interesting vertices is at most $O(k^2)$ and we can determine whether a given vertex is interesting or not, one could run any vertex cover algorithm on the subgraph induced on the interesting vertices (for example the $O(1.2738^k + nk)$ time algorithm by Chen, Kanj and Xia), and this would still only use $f(k) + O(\log n)$ space. 

Here is a problem (with lists!) which is known to be W[1]-hard parameterized by Vertex Cover (indeed, even by the number of vertices in the input graph). The problem is known as the "Arc Supply" problem, and it was proved W[1]-hard (parameterized by the number of vertices) by Bodlaender, myself and Penninkx, even on planar graphs. Input is a simple directed (planar) graph $G$ with $k$ vertices. Furthermore, every vertex $v$ has a positive integer demand. Every edge $e$ has a list $L_e$ of supply pairs: $$L_e = \{(x_1^e, y_1^e), (x_2^e, y_2^e), \ldots, (x_\ell^e, y_\ell^e)\}.$$ Here $x_i^e$ and $y_i^e$ are positive integers. All integers are encoded in unary. Different edges may have different lists. The task is to select one supply pair from each of the lists of all the edges. If the pair $(x_i^e, y_i^e)$ is selected from the list of the edge $e$ from $u$ to $v$, then the edge supplies $x_i$ towards the demand of $u$ and $y_i$ towards the demand of $v$. A vertex gets satisfied if the total supply it gets from all of its edges is at least its demand. The goal is to satisfy all of the vertices simultaneously. Note that the problem does have an $n^{O(c)}$ time (dynamic programming) algorithm on graphs with a vertex cover of size $c$, here $n$ is the total input size. 

Question 1: For a function $f(n)$ lets say that $f$ is sub-polynomial if $\forall \epsilon > 0, f(n) = O(n^\epsilon)$. The Exponential Time Hypothesis (ETH) states that 3-SAT needs $2^{\epsilon n}$ time for some $\epsilon > 0$. Let's postulate a much weaker version, ``Weak Exponential Time Hypothesis'' (WETH): There exists an $\epsilon > 0$ such that $3$-SAT requires $2^{\epsilon n^\epsilon}$ time. Claim: If $f$-bounded Independent Set is NP-hard for any sub-polynomial function $f$ then the WETH fails. Proof: Supose $f$-bounded Independent Set is NP-hard under Turing-reductions for some any sub-polynomial function $f$. Then there exists a constant $c$ and an $O(n^c)$ time algorithm for $3$-SAT with $n$-variables that makes at most $O(n^c)$ oracle calls to $f$-bounded Independent Set with at most $O(n^c)$ vertices. From this algorithm we make an algorithm for $3$-SAT where the oracle calls to $f$-bounded Independent Set are replaced by the $O(|V(G)|^{k+2})$ time brute force algorithm. The running time of this algorithm is upper bounded by $O((n^c) \cdot (n^c)^{f(n^c)+2}) = 2^{O(\log n \cdot f(n^c))}$. For any $\epsilon > 0$, pick $\delta = \epsilon / 2c$. Since $f$ is sub-polynomial we have that $f(n^c) = O((n^c)^\delta) = O(n^{\epsilon / 2})$. Therefore the running time of the algorithm is upper bounded by $2^{O(\log n \cdot n^{\epsilon / 2})} = 2^{O(n^{\epsilon})}$, contradicting the WETH. 

On the hardness of losing weight by Dániel Marx and Andrei Krokhin. ACM Transactions on Algorithms, 8(2):19, 2012. Despite the funny title, the paper is serious. 

One nice example is the Hamiltonian Path problem is easily solvable in $O(2^n(n+m))$ time (Bellman, 1962) by dynamic programming over vertex subsets. In 2010 Björklund gave an algorithm running in time $O(1.657^n)$ using determinants and polynomial identity testing in a field of characteristic $2$. Another example is that of “connectivity problems”, such as Hamiltonian Path, Steiner Tree, or Feedback Vertex Set on graphs of treewidth $k$. The naive dynamic programming algorithms for these problems run in time $2^{O(k \log k)}n^{O(1)}$, and at a glance it really looks like that is what the ``correct’’ running time for these problems should be. However, in a wonderful paper, Cygan et al. showed that these problems all allow algorithms with running time $2^{O(k)}n^{O(1)}$, again cleverly using polynomial identity testing in a field of characteristic 2. 

If you have an oracle for $f$, you can compute the optimal decision tree for $f$ in $O(3^nn)$ time and $O(3^n)$ space. Consider a function $g$ that takes as input a partition of the variables into three sets $T$, $F$, $S$ and outputs the size of the smallest decision tree for $f$ when the variables in $T$ are restricted to $1$ and the ones in $F$ are restricted to $0$ (and the remaining ones, in $S$, are unset). The following recurrence holds for $g$: $$g(T,F,S) = 1 + \min_{v \in S} g(T \cup {v},F,S\setminus \{v\}) + g(T,F \cup {v},S\setminus \{v\}).$$ The base case is that the function $f$ is constant with the given restriction, in which case $g$ is $0$. This recurrence directly leads to a dynamic programming algorithm. If, instead you are looking to compute the minimum depth of a decision tree then the following recurrence does the trick. $$g(T,F,S) = 1 + \min_{v \in S} \max\big{[}g(T \cup \{v\},F,S\setminus \{v\}), g(T,F \cup \{v\},S\setminus \{v\})\big{]}.$$ On the lower bound side, if $f$ is given in $k$-CNF then deciding whether the decision tree complexity is non-zero amounts to checking satisfiability which is hard to do in time $2^{o(n)}$ assuming Exponential Time Hypothesis (ETH) and in $1.999^n$ time assuming the Strong ETH. 

The answer is no - for a fixed $Q$ let $t$ be the number of vertices in the smallest graph $H$ not in $Q$. Now, consider $n$ much bigger than $t$. For a random graph on $n$ vertices, the probability that the $t$ first vertices induce $H$ depends only on $t$. Partitioning the vertex set into $n/t$ disjoint sets of size $t$ and considering the probability that none of the sets are equal to $H$ shows that the probability of being in $Q$ tends to $0$ as $n$ increases. 

As far as I understand, the factor $O(\sqrt{\log OPT})$ approximation algorithm for treewidth of Feige, Hajiaghayi, and Lee is randomized, and no deterministic approximation algorithm with this factor is known. Is this correct? Is any deterministic approximation algorithm with factor below $O(\log OPT)$ known? 

Claim: If there exists an $\epsilon > 0$ such that for every $k'$, $k'$-partite $k'$-SAT can be solved in $2^{n(1-\epsilon)}$ time, then SETH fails. Proof: Suppose such an algorithm exists. We give an algorithm that, for every $k$ solves $k$-SAT in time $2^{n(1-\epsilon/2)}$. Consider a $k$-SAT instance with $n$ variables, apply the sparsification lemma so that it produces $2^{n(\epsilon/2)}$ instances where, in every instance, every variable occurs in at most $(k/\epsilon)^{O(k/\epsilon)}$ clauses. Each of these instances are $(k/\epsilon)^{O(k/\epsilon)}$-partite in the sense that one can find a coloring of the variables into $k' = (k/\epsilon)^{O(k/\epsilon)}$ colors such that no clause contains variables with the same color. If you want clauses of size exactly $k'$ you can replace clauses of size $k$ by $2^{k'-k}$ clauses of size $k'$. Running the $2^{n(1-\epsilon)}$ time algorithm on each of the $2^{n(\epsilon/2)}$ instances of $k'$-partite $k'$-SAT gives an algorithm for $k$-SAT with running time $2^{n(1-\epsilon/2)}$ as claimed. Note that the above proof essentially reduces $k$-SAT to $k^k$-partite $k^k$-SAT. So an algorithm with running time $2^{n(1-\frac{\log \log k}{\log k})}$ could still be possible, if very surprising. 

Theorem Exactly computing $F(w,t)$ is #P-hard. proof. Reduce from the counting version of Subset Sum, the #Subset Sum problem, defined as follows. Input is a set $W = w_1, \ldots, w_n$ of non-negative integers, and an integer $t$. The task is to count the number of subsets $W' \subseteq W$ that add up to exactly $t$. This problem is known to be #P-hard, even when for all $i$, $w_i \leq t$ and $t = 2^{O(n)}$. In particular, the $w_i$'s are $O(n)$ bit integers. Suppose now we could compute F in polynomial time. Then we could solve the #Subset Sum problem by outputting: $$2^n \cdot \left( F(\frac{W}{t}, 1-\frac{1}{t}) - F(\frac{W}{t}, 1) \right) = 2^n \cdot P\left[\sum_{i=1}^n \frac{w_iX_i}{t} = 1 \right],$$ which is precisely the number of subsets of $W$ that add up to $t$. 

For all $\beta_i = 0$, asking whether one can get objective function value $1$ is equivalent to Subset Sum, and therefore NP-complete. Simple reductions from Subset Sum also prove that it is NP-complete to approximate the objective function within any factor polynomial in $n$. We can reduce the $\beta_i = 0$ case to the case where $\beta_i = 1$ by just setting all $\beta_i$ values to $1$ and scaling both $\lambda$ and all $\alpha_i$'s by the same (large) integer factor. Then the $\beta_i$'s dont affect the objective function significantly anyway. On the other hand, if all the integers $\alpha_i$ are small, one can solve the problem in pseudo-polynomial time - similar to the pseudo polynomial time algorithm for the Knapsack Problem. Thus for practical applications i'd expect that you can get good results by dividing down all the $\alpha_i$'s (and $\lambda$) by the same factor so that they become (relatively) small, round to the nearest integer, and solve using the Knapsack-like DP. 

If all vertices have degree at least $d$ then there is always a dominating set of size $\frac{n \ln n}{d} + 1$. Pick a random set $S$ of size $\frac{n \ln n}{d}$. For a particular vertex $v$, the probability that $v$ is not dominated by $S$ is upper bounded by $(1 - \frac{d}{n})^{\frac{n\ln n}{d}} \leq e^{-\ln n} = \frac{1}{n}$. Introduce an indicator variable for every vertex that is $1$ if it is not dominated by $S$ and $0$ otherwise. The number of undominated vertices is the sum of indicator variables, so the expected number of undominated vertices is at most $n \cdot \frac{1}{n} = 1$. Hence there exists a set of size $\frac{n \ln n}{d}$ that leaves at most one vertex undominated, add this vertex to the dominating set. For your question it means that if all vertices have degree at least $n^k$ then there is a dominating set of size $n^{1-k}\log n + 1$. Would be nice to see if one can get rid of the $\log n$ or not. 

One can get a $n \choose n/2$ lower bound by considering the $n$ variable formula that for every pair $x$, $y$, of variables contains the clauses $(x \vee y)$ and $(\neg x \vee \neg y)$. The total number of clauses is $2 \cdot {n \choose 2}$. Every assignment will satisfy one of the two clauses on $x$ and $y$. Both clauses are satisfied exactly when $x$ and $y$ get different values. Hence, if $i$ variables are set to true the total number of satisfied clauses is $${n \choose 2} + i \cdot (n-i).$$ This is maximized when $i = n/2$. Thus, for even $n$, every assignment that sets exactly $n/2$ variables to true is a local maxima. The total number of such assignments is $n \choose n/2$, which is $\Theta(\frac{2^n}{\sqrt{n}})$. 

I don't quite know if this is what you were looking for since it is not a single data structure that has a name - but what you are asking for is easily implementable using just a linked list and a binary search tree: Keep a single linked list where all the elements of bucket $i$ come before the elements of bucket $i+1$. On every link you can have a boolean flag - is this the same bucket or next bucket. Now doing the shift corresponds to setting the flag to "same bucket" from "next bucket". Of course you probably also want to maintain some way to see the first element of each bucket. A way to do this is to store the pointers to the first element of each bucket in a balanced binary search tree, such that the key of the pointer to bucket $i$ is $i$. The search tree should be able to: (a) given $i$, output the entry with the $i$'th lowest key and (b) delete entries from the search tree. Merging bucket $i$ and $i+1$ now reduces to finding the entry in the search tree with the $i+1$'st lowest key, deleting this entry from the search tree, going to where it points in the list, and setting the flag of the arc pointing into this node in the list to "same bucket". Thus all operations take $O(\log n)$ time. Note that after some "shift" operations are performed the key of the $i$'th bucket is no longer $i$, but it is the $i$'th lowest key stored in the search tree.