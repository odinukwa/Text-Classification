I'll assume most of us know about cross-validation and are accustomed to using Training and Test sets. However, I've also seen references to a third set that you use to test a completed model. It's the test to see how your model does against a dataset that it hasn't seen before; helps determine if your model has been overly-fit to the training data. I have seen this dataset referred to everything from "the third dataset", "the wild dataset", "the real-world dataset". So what is the appropriate name for this third dataset? 

This piece of code should be self-explanatory and it's also a choke-point in my process. Any way to do this better? 

This sounds like a homework problem and those are frowned upon on this web site; so I'm going to be cautious about helping you and just give you general guidance: 1 - be very, very careful about picking models before you do anything else. That is a bit of Researcher Bias and you should try to avoid that; let the data tell you where to go, not the other way around. 2 - you have to think about what "factors" you can use to describe each chair. How old is it? What color is it? Does it have wheels or not? Some of these features will have an effect on your models, others will not. But it should be your goal to start with a long list of factors and then try to model them out and see which ones have an effect on your results. Start putting a list of factors together and then you will be in a better position to continue your studies. 

I like coursera and there are many data science programs to choose from. If I were you, I would look into the 10-course series certificate that is offered by John Hopkins. You will come out of that course being pretty good with R and establishing a good base of knowledge for the rest of your growth as a data scientist. 

Let's stay I have a field with a continuous variable, like a count of people waiting in line. I want to take those values and create a categorical value based on quartiles. Let's say my range of values is 1 to 80 and the quartiles tell me that a "very short" line is less than 5 people, a "short" line in 6 to 30, a "long" line is 31 to 50 and a "very long" line is >=51 I can think of different ways to write this in python/pandas/numpy but something tell me that one of you can come up with a method/snippet that is short and elegant. Note that I want the method to also generate the quartile values, something that I haven't really done in Python before. 

Technically, you can't. That is one of the limitations of regression models; they are really only effective for values/ranges that they have seen before. Your use of categorical values makes it even more complex. But even with continuous variables, it is not recommended to use regression models for these "unknown" values. 

I know that tokenizers turn words into numerics but what about hashtags? Are tokenizers design to handle hashtags or should I be filtering the "#" prior to tokenizing? What about the "@" symbol? 

Only load a saved model when the new, labeled, examples differ significantly from what the saved model was trained on Always train a new model since data is data Always train a saved model since even small sampling variations can have an effect on the algorithm Always train a saved model as soon as the input & output shapes are determined; you should only be training a model from scratch once. 

Is there a reason why this new information has to be a part of your model? There's no rule that says that all data must be fed into your model. Maybe you have just found a feature that shouldn't be included in your predictive phases? And if so, that's perfectly fine, there is nothing wrong with that. 

Both of those frameworks will handle what you're looking for. Of course, they're not free, but they will do the work you seek. 

It depends on any number of factors. What kind of accuracy are you currently getting? How often are you getting new predictions from your model? Do you have human intervention at some point to check your errors, relabel, and improve your training sets? It's also possible that you don't need a schedule, per-se. Depending on your data, the pipeline and the programming of your model, you could also just have continuous learning built into your model with a new release of said model after every epoch. So, really, a lot of the answers you are seeking are very specific to your implementation. 

I am working with an LSTM nn built with Keras. I have the need to pass in a history of events as a single factor and I'm thinking of doing it with as a list. But I've never passed in a list in this manner - is that something that is possible with Keras? Or do I need to go through some sort of categorization process for this? 

That may or may not be enough data for truly predictive modeling. You have order information, but you don't have customer information. You might be able to predict things like how many units of product X you are going to sell next you or the average shipping costs of orders for next year. But you won't be able to relate things on an individual customer basis (like your question says) until you have a lot more customer data. 

I am expanding my knowledge of the Keras package and I have been tooling with some of the available models. I have an NLP binary classification problem that I'm trying to solve and have been applying different models. After working with some results and reading more and more about LSTM, it seems like this approach is far superior to anything else I've tried (across multiple datasets). I keep thinking to myself, "why/when would you not use LSTM?". The use of the additional gates, inherent to LSTM, makes perfect sense to me after having some models that suffer from vanishing gradients. So what's the catch with LSTM? Where do they not do so well? I know there is no such thing as a "one size fits all" algorithm, so there must be a downside to LSTM. 

I have a Pandas dataframe that has some fields that contain very verbose text. I want to be able to iterate through the DF but only display a limited set of words. I have code similar to: 

I am building a recommender system. I have a list that shows me what a user has disliked and I use it to create a dataset. The dataset shows me: where the values are counts from other users that have disliked the same products. What are some of the algorithmic approaches I can use from this base? From this data I know I can get a probability. How would I also get a weighted average? Maybe some sort of Bayesian filtering approach is called for in these scenarios? 

Interesting question. I think that you can illustrate this by thinking about different use cases. The one example I've heard that I like is around lending decisions for loan applications. That's an algorithm but, because of regulations, it can't be strictly "black box". The decision has to be, effectively, interpretable because the bank has to give you a reason for decline on the loan. So, there's certainly better algos out there for loans that can give a binary result, but do you want a bank to just tell you yes or no? 

I don't know the specific tools that you mentioned. But generally speaking, to create a chatbot, you would need to connect an API from Slack to your own API layer that handles the NLP side of it. So, if you're just starting out, your focus should be on creating an NLP algorithm that handles the types of questions that you want your bot to answer and converse with. You need to create code that both recognizes new inputs and understands how those inputs fit to everything else (i.e. an ongoing conversation). From there, you can create an API that you consume in any number of programs an you can practice with it (even in a terminal screen) for your own testing & algorithm training purposes. At that point, you should be able to integrate that into any number of platforms, slack being only one option. 

You are much, much too early in your process to even begin thinking about your models. At this stage, you should be thinking about what the data looks like and how much of it do you need. For starters, what are you measuring? What is the answer that you seek? Is it job satisfaction? Likelihood to land an interview? Likelihood to get a job offer? Once you have that, how are you quantifying that? Is it a categorical variable? A continuous variable? Then you would have to decide on the likely list of factors that go into that answer and determine how you're going to get that data. Do you need to run a custom survey? Are there other datasets that you can leverage? You need to think about all of these answers before you're ready for any sort of modeling discussion. 

Have you looked into a tokenizer? By your question, I can't tell if either (1) you don't know about tokenizing or (2) you are looking for an alternative to tokenizing. I will assume the former and suggest that you read about tokenizing and go down that road. Keras provides a very simple approach to tokenizing and sequences that you can implement quickly and should get what you're looking for. 

I work for a highly-regulated entity, so I have to obfuscate what I'm working on; I'll provide the following as examples on what I'm doing. I am training an XGB model for NLP comments about breeds of dogs (Tensorflow, sklearn). I have a dataset about dogs that I'm using to train the algo and cross-validation testing is promising. However, my test data is not just about dogs, it's about animals in general. So that data can be about dogs, cats, elephants, whatever. Right now, the XGB model is giving all of the animals it does not recognize a default value (a category). How can I make my XGB just say "I don't know" instead? Is there a way to assign a default value to test values that don't make sense in any category? BTW, I realize that another possibility is to create a model to classify the test data but I want to see if there is a way to solve this within the XGB model. 

Keep in mind that while I have a Masters in Applied Statistics, I'm going to give you a very simple answer: take a course on probabilities. Most of the modern ML programming frameworks take a large majority of the math out of data science; you really just won't need it in most scenarios. But you will always need the ability to understand your results and the majority of results are expressed in probabilities. If I was new to data science I would take a (brief) course on probabilities, seek to understand what proportions and percentages really mean and then I would work to know a framework (like Tensorflow) really, really well. If you can do that, you can write some really interesting algorithms and not have to be obsessive about the math. 

You pose an interesting question. The problem that I see is that even if you develop all the items you mentioned (and I'm not sure you need all of them) you wouldn't have the computing power needed for this. You see, most of these items are based on reinforcement learning. Which means that the models are given a relatively small set of data (roughly speaking, the rules of the game) and then are set off to play millions & millions of games. Read the whitepaper that was written by Google on beating Go. They basically admit that there whole point was to write something very, very simple that could learn on it's own and then just set it free on Google's massive power. It's nothing for them to say, "let's have this robot play 10 million games". So the level of sophistication from your initial model is pretty low, you just need to make it an exceptional learner. So you make an exceptional learner of Starcraft - then what? How are you going to have the power to make your algorithm to play millions of games? 

Should I be concerned that the loss and mean predictive error are so high? These "feel" like a problem but I want to be able to qualify it a little more. 

Can someone please post a straightforward example of Keras using a callback to save a model after every epoch? I can find examples of saving weights, but I want to be able to save a completely functioning model after every training epoch. 

If you want us to help you, then you should be posting more about (1) what data is available to you and (2) what techniques you've already tried. Otherwise your question is just too broad to be effective. 

I have an NLP binary classification problem I am trying to solve. My general approach is Keras with Tensorflow. My training set is X records. If I watch the progress of the model accuracy as the model is being fit, I see a pretty good accuracy rate while an epoch is at < X/2 records processed. As soon as it passes that point (essentially the mid-way point) my accuracy rate takes a nose-dive and (of course) subsequent epochs have terrible accuracy rates. I have already checked the obvious and made sure that my training set is scrambled. I am experiencing this with multiple models within Keras. So either I'm mis-interpreting the results or there is something wrong with my data/sequences. What else could cause this type of behavior? 

I'm using Keras for various machine learning projects and save my models after every training run. Let's say I want to do a training session for an algorithm - how do you decide whether to (1) load a saved algorithm and train it or (2) start training the algorithm from scratch? Some potential approaches I thought of: 

It all depends on the model you are running and your chosen sample sizing. The sample sizing is obvious, so I won't go into that (or at least it should be obvious to you vis-a-vis how R handles data). But the chosen model can have an effect. For example, neural networks are one of the rare problems in computing that you actually can throw more hardware at to improve. That's going to be a while different experience than, say, gradient boosting or something along those lines. 

I have a feeling that you are never going to come across this, most of this data is closely guarded. It's no secret that dating sites like OK Cupid use data science on their app, so I think that most of these sites aren't quick to make their data public. 

I'm obscuring the data I'm working on, but I think this should get the point across. I'm trying to design a test for the following: A client sells various popsicles. They think that on days when it is very hot, one particular kind of popsicle sells more than on days when it's not hot. How can I prove if he's correct or not? My thought is that I could do an A/B test, splitting up between not hot & hot days. For each of those groups, I could find the proportion of sales for that popsicle from all popsicles sold that day. If that proportion rises on hot days, then I can do a test for statistical significance and come to a conclusion. But I think there have to be other tests/models I can apply based on sales counts alone right? What tests can I run that only involve things like the raw counts of sales on days when it was hot and not? Note that I am planning to use the temperature as a categorical variable but I can also make it continuous if that makes more sense. 

I am putting together a multi-category classification algorithm. Since it's NLP, the training data is very simple with one column for labels and another column for text. However, because it's NLP, some training records can fit multiple categories. Should I iterate through & have multiple entries (same data, different labels) or should each piece of data only have 1 category assigned? 

I have an XGBoost implementation that I'm using for some NLP classification. The algorithm was trained on a dataset of about 400,000 labeled records. I've noticed that if I use a large test set, I can get predictions from the algorithm just fine. However, if I use a small test set (like 10 records) the model throws an error with an array of key numbers it was expecting. I'm assuming that these key numbers are related to the tokenizing process? Or are in some way related to the parsing of words? Is there something that I'm doing wrong in the prediction implementation or tokenizing that I need to be addressing?