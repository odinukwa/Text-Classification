Rendering on a separate thread is a common idiom. To quickly and directly answer your question about "Was I just doing something wrong or is it possible to load textures and buffers, while it's rendering?" - OpenGL ES has a rendering context bound to one thread and calling any OpenGL ES functions from any other thread is illegal. More on that in a bit. Quake is a single threaded program, and written for DOS originally. Its code is fairly specific to the game, but basically the menus are built-in and simple rendered on top of the 3D graphics (if any). It has been a while since I've read through the source, but conceptually, the game loop is this: 

Depending on the amount of data you have to store PlayerPrefs should be perfect for that. If player prefs is not enough or does not suits your needs you can store project custom data in a text file (personally i place it in resources but it could be anywhere) and read it when needed for ex : i made a plugin for unity to define different sizes atlases path, and everything is stored in a textfile where i have some JSON arrays, and when i load my scene i read my file and forget about it 

According to those docs over here, you should be able to create a basic surface shader with diffuse and lighting. And by tricking the in doing no specific action except getting vertex position and converting it to color you want (using an struct which could be a bit more evolved). After doing that and getting back the color you got in inside your albedo infos, you just let surface shaders do their job and take care of the lighting (you can specify it there ) EDIT : I think i'm starting to get what you mean by vertexColor, you mean Weight Shade : giving a vertex a 0-1 value and from this value get the color mix which correspond? or you just need access to the property in your input data for : 

I'm not sure which of these is possible, and if there are any performance issues that aren't obvious. Clearly my preference would be to have 2 rather than 3 default render targets, if possible. 

I'm making a game in Direct3D10. For several of my rendering passes, I need to change the behavior of the pass depending on what is already rendered on the back buffer. (For example, I'd like to do some custom blending- when the destination color is dark, do one thing; when it is light, do another). It looks like I'll need to create multiple render targets and render back and forth between them. What's the best way to do this? 

I have a DX10 game that is fillrate bound. One of the things I've been realizing may be contributing to this is that I compute depth in the pixel shader (the game uses a logarithmic depth buffer to get extreme precision from 1cm to millions of meters away). It occurred to me that writing the depth in the pixel shader is ruining the ability to to Early Z testing. Can this be fixed by outputting this computed depth value in the vertex shader instead? 

In unity, SortingLayers (or camera layers for that matter) are not groups of object you should see them as tags : specifically for the sprites, sorting layers are used to define the order of render of the objects. you can go through all the sprite-renderers and check their layers and apply something to their transforms but that will consume "a lot"* of resources. better solution is to really group your objects by parenting them to the same transform, and then you move that transform. For ex : 30 objects and you want to make three "layers" (but once again it is parallax layers not sorting layers (they can be related (you want stuff to be drawn in order) but are not the same thing)) what you would do in that case if create three empty game objects and drag 10 object onto each of them, then you'd move those 3 game objects according to the camera to achieve the parallax effect 

I am currently using the DirectX June 2010 SDK. Everything was working fine with my installation until recently. Unfortunately I'm not sure what changed or when, but now when I create a device with D3D10CreateDevice1(), it always crashes with a memory error if the device is created with the D3D10_CREATE_DEVICE_DEBUG flag. Even reverting to old code which used to work causes this error. Additionally, PIX always crashes every time I use it with my game. I did some searching and found a lead that I may need to update my SDK installation. This page also indicates that I need to "install the updated SDK Debug Layers". How do I do this? I have no idea how to install the "Debug Layers"... Additionally, is it a mistake for me to be using the June 2010 SDK? Apparently DirectX is now included with the Windows 8 SDK, which I haven't been using because I've no interest in Windows 8 development. Is this foolish? Is there any downside to me using the Windows 8 SDK just for Windows 7 DirectX development using Visual Studio 2010? 

If you have no plans on making your camera track another object then you should make it a child of your object (i do think it is not the good answer but i think it's still better than moving two objects separately when you want them to be synched) If you need your camera to be versatile you should make it completely separated from input for it's position and input relative only for it's rotation. 

So for the same force applied the velocity of the point where the force is applied will be the same, knowing that : If the diameter change but the force stays constant, a smaller diameter means smaller perimeter so less way to travel to make an angular degree so the angular speed in degrees per seconds will be faster with a small diameter than with a large diameter for exactly the same force applied! More math frenzy definition at wikipedia 

Yes, in general, to it being a good idea. OpenGL generally has transparent loss mechanisms, that is, if a texture is lost, you won't notice it. The idea of making multiple textures into one large texture has been explored. This is called "Texture Atlasing". There are faults with it if you try to make an atlas that is greater than the maximum texture size of the graphics card (which on modern cards is quite large, 8Kx8K or so). There are pathological cases where texture atlasing will not improve performance, but with discrete graphics cards offering >> 512MB of video memory on low end cards, I don't think you'll run into any of these type problems. I wouldn't concern yourself too much with performance optimizations if you are developing for the desktop systems until it is clear that they are needed -- and what an epic title it must be then! ;) 

this should already make your code easyer to maintain and it could allow you to switch between targets painlessly EDIT: And to stress it even more as Byte56 stated : Get rid of all the useless stuff 

The turn my comment up there to an answer : The "proper" way to use Lerp in your case would be that : 

Either two sprites backing together, or a custom shader with no Backface culling could be the way to go, you could use a mod of the original sprite shader which is here : Unity Built-in Shaders (select the last version and you should find them in the same order as in the menu) I did a shader once with a different texture on each side, with the same vertex count but i'm not close to my code right now. EDIT : If you go for the custom shader dont forget to create a material and associate it with your sprites. I just realized by the way that there could be an issue with the fact that the Sprite renderer accepts only one texture, but you could trick it by making a texture with the front and back on the same texture, and then offsetting your UV's to match the correct positions.