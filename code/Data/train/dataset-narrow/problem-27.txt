Repeat step 2 to find the next branch with the least divergence. Merge the two branches found in step 2 and step 3, resolving all conflicts. Merge your these two branches into your branch. Test the code in temp_master code to see if it compiles, and builds, and run any other automated tests you have for sanity. 

We have had problems a couple times now where the links to the CDN in our components don't get properly updated between the team, qa, and staging environments. Is there a way to force an error if the QA environment tries to access the team CDN or vice-versa? Or some other well-known solution to this problem? 

I would like to keep a workspace in Jenkins until after the pull request is merged. Meaning, I don't want to clear the workspace after every build, but I also don't want to clear the workspace only when the branch gets deleted. How can I use the github webhooks to trigger a workspace cleanup whenever a pull request gets resolved as merged? Ideally, the solution would involve using my JenkinsFile/Pipelines. edit: Background. We have long-lived branches and short-lived branches. For brevity, I'll call the long-lived branches "team branches". Our workflow is to take short-lived branches, such as feature branches and make a PR request to pull them into the team branches. On Jenkins, we then build the feature branch and the PR request and the team branch. Normally this works fine, but sometimes when multiple teams are merging to qa each team and feature branch needs to update(pull) and then get built on Jenkins again. Currently, we are always cleaning the workspace which makes the update builds take longer than they need to. So I want to be able to preserve the workspace until all updates are done and the PR request actually gets merged, only then do I want to delete the workspace for the feature branch and the PR request and the team branch. 

passing in the a personal API key (created by Jenkins user manager to use by visual studio team services), this script first checks if it is can access the server using local connections, if so set the variables appropriately. From there we create local handlers for environmental variables. Our release artifact name defined in VSTS is the same as that in our jenkins server. The build number used is created by VSTS and passed to jenkins and stored by the VSTS plugin(Jenkins TFS Plugin download). We get this version number from the environment as the artifact build number. We also ensure that our team project path url is windows URL encoded brief explanation. Next we hold our authentication which is stored as a variable in our VSTS variables. The is marked as secret. As such, we must pass it in as it isn't visible as an environment variable. We then encode the authentication and add it to our request headers. We then add a debug line to print what target we are looking for and what project we are looking in, just to make sure we are looking for the right thing. We create a variable to hold the build information that we find and do one more debug write stating what are query url is so that we can manually run it if we run into any issues. Next we then run this url query against the build server and iterate over all the builds returned. The query filters out all the JSON to just the build ID, it's absolute url, and the VSTS build number. We check to see if the build we are examining has the same build number as the artifact we are deploying, if so store it in the variable, print the was found for debugging, and exit the loop. Coming out of the loop we check to see if we found a result. If we did not find a build (due to it being cleaned up most likely) we inform the user that the artifact needs to be rebuild from source. Otherwise we download the artifact zip to a temporary location from the jenkins server and extract it to the target directory, defined as a variable. Lastly we delete the downloaded zip. This is ran as a powershell script task and due to it's length, can not be an inline script. So to deploy this, i have a devops git repo for my scripts and I just include this as an artifact that is downloaded and ran. This task is preceded by other tasks which stop the current process from running, and delete the current files in place before running this task. After this task is ran, more scripts are ran to run the downloaded artifacts. I hope this helps someone else trying to coordinate and link VSTS builds with Jenkins artifacts. 

Repeat steps 2 - 7 until you have only two branches, your master/trunk and . Finally, merge into master/trunk and live with your new single-branch model. 

Hack your Team Bringing about change in your organisation is hard. People have habits, they resist change, and they are often comfortable with the status quo. To bring about change, in no particular order, here are a few tools you can use. 

We require a review process using Pull requests in github onto our main dev or master branches. During that review process, we will mark pull requests as requiring changes if many files have white space or line ending differences, and insist that they follow the formatting of the dev or master branch they are making the pull request for. There are also some nice tools depending on which languages you use and which CI tools you use, which can, during the build process or pull request step auto format the code based on rules that you set up. That also helps with keeping code looking consistent and minimizing formatting issues when committing code. 

If any tests fail, go back, find out why, and fix them, and repeat the process. If tests still fail, pick two different branches to work with. 

Create a copy of your master/trunk branch and call it Find the branch with the greatest divergence from master/trunk. Determine if the branch needs to be kept, archived, or deleted. 

So what is happening is that the changed code just sat until internal testing was done (never a good thing as this is how code get stale quickly). Once internal testing was ready, the code was then merged into the trunk causing it to be published into nugget under the trunk's feed. Some business politics ensued and we finally got to a real single code base mindset from the product owner's perspective (which was the issue that got us into this situation). Moving forward I don't anticipate this to arise. Code can be branched out. However, only code in the trunk will go to our internal nugget feed or our build/deployment processes. 

So I wrote this powershell script to be ran by the deployment group agents who have access to the server locally. 

I have a CI environment for a DLL. On code check in, it automatically builds and publish's with a version such as 1.0.0-dev01. Once it is QA approved I don't want to rebuild this DLL with a new version number, but instead strip off the '-dev01'. This all happens in within my VSO environment. Does anyone have any suggestions on this? 

The biggest challenge I've encountered with people resisting the change is the reason of not seeing the value of it, or build automation in general. His idea was, we were our getting product(s) out the door, and on time. So why do we need to "create work" for ourselves, and take a developer off of working on a product. After 4 years since then I've finally got the owners on board with CI by capturing metrics on how much time is wasted copying files manually from bin folders to SVN for qa to copy from SVN to environments. This along with the overhead of managing what build is in what environment and other pitfalls of you are testing the wrong build etc. He has finally came around, but I think he feels that the time could be put to other tasks. 

As far as I was able to tell it's not possible. We solved this by using Pipeline putting the custom messages in the Jenkinsfile. The flexibility gained by having the Jenkinsfile in the git repo really helped our developers have a better understanding of the build process, and allowed them to make necessary changes to the build commands without the communications headache. It's a nice excuse to move over to Pipeline. It also happens to be very easy to do: 

NO I would argue that Mature DevOps operation, does require a Mature Agile process. You are unlikely to be able to get the full confidence to continuously deploy or allow your developers to initiate the deployment process without a mature Agile process in place. However, I believe it is very important to make it clear that an organisation does NOT need to adopt their agile process before building up their DevOps culture and infrastructure. In fact, I would argue that it is actually easier to adopt Agile once you have some basic DevOps working in your company. Rather than Agile being a prerequisite for DevOps, I would suggest that DevOps be used to help advance your agile implementation. 

In this case, "Fire and Forget" doesn't mean what you think it means. It isn't the case that you fire the build and then forget about the outcome. What actually happens is that you fire the event, and then forget about what the process is doing up until the point where the process returns feedback to you and reminds you about what was fired. As an example, the old way of doing things might be to trigger a build and then let it run while you watch the output. You sit there watching the results of the build as they occur and don't work on anything else productive during that time. Or you do work on something productive, but you have one eye on the build process. When it is done, you need to either be paying attention, or remember to check on it to see the results and then continue based on that. In Jenkins model of "Fire and Forget", you have some automated process do the build for you, and your mind is not focused on the build process until something goes wrong, or the build completes. At that point, you get a message from Jenkins, either as an email or in a program like slack, which now reminds you of the build process and tells you all the information you need to know to move on. In the meantime, you were working on some other task with your full focus, because you knew that you didn't have to keep an eye on it. The automated system would alert you to anything you needed to know. 

I have a few CI environment in VSTS. One for dev, QA, staging, and production. my builds are created in the cloud by VSTS build agents. these builds are not obfuscated by design and need to be deployed to my dev and test environments as such. When they are approved by QA in the test env, they go to staging. What I need to have happen is for the build to be obfuscated before being put in this environment. Is it possible for me to run a docker container in VSTS release pipeline to obfuscate the build in the container and then download the result in a deployment group? My main questions are boiled down to this: I would like to have a container image in the cloud running a tool installed on the image. During my VSTS release pipeline, I would like for my release agent to pull down this image, pass in the build artifact, and then do something with the results of running my artifact through this tool. I have an Azure subscription. I've tried reading the documentation but I'm confused as to what I need to set up. This container is not hosting any web app, but is being used as a way to run my tool locally on the release agent. I see that I can run a docker run command as a VSTS pipeline action. What do I need to set up on my azure subscription to host this image for the VSTS agent to pull it down? 

This article explains the trouble with monitoring, but it doesn't provide any good examples of how to actually monitor a microservice inside of the docker container. We are currently using PM2 monit to monitor our microservices, but as we put them into docker containers we lose the ability to access this data within one screen for all the various microservices which each run in their own docker container. Dockerswarm monitoring will tell us the state of the containers, but not the microservice running inside of them. What's a solid proven way of solving this problem? 

If you have docker compose installed on the Jenkins Build machine you can use to run docker-compose from a new shell. In general, you can run any command from your JenkinsFile using the command. 

Cause others to experience the problem that DevOps solves. Many times the benefits of DevOps is only understood on a theoretical level by your team. Most of the problems that happen during deployment are hopefully and rarely experienced by the rest of the development team or management. To fix this, make sure that you are vocal about issues when they arise and mention how this problem wouldn't have happened if the team was using a continuous integration solution. Another possibility is to be sure you ask developers to fix problems that their code caused during deployment instead of fixing it yourself. Find the leaders. It is common for people to follow the leaders, be they management or just the most popular/commanding person in the group. Get those leaders on board with your desire to move to a DevOps culture, and devise public ways in which they can be seen using or advocating best practices. Build Trust. We are more likely to agree to things from people after we have already agreed with them once or twice before. Ideally, you can find small improvements that can be made without a shift in culture and build upon that success. However, if that isn't an option, ask them simple questions and offer simple suggestions so they get into a habit of saying yes or agreeing with you. Don't be ashamed to repeat yourself. Repetition works and eventually sinks in. Whenever possible mention how great things would be if the team was using DevOps. However, this only works if you have first built trust within your team. Make it pleasurable. If you are allowed to build a proof of concept for your DevOps situation, use cute emoticons and cheerful colours in the reports and notifications. Post funny gifs when a build fails. Make sure you aren't annoying with your updates. 

So I have a local jenkins server building artifacts. I trigger this build through VSTS. When browsing through the logs when uploading the artifacts to VSTS from the local jenkins server I found this: This could pose an issue where the triggered build may get beat out by another build (theoretically). While this is not a high likelihood, I would like to mitigate all possible artifact confusions as that is one of the main points my company is moving to build automation and continuous integration. I did see something related to the VSTS Task in issue 4110 but not sure what is coming of this. What I'm trying to accomplish is that when Jenkins finishes the build it uploads it to VSTS. Because of the size of the build, when we go to releases, I would like to not download the artifacts back to our local premise, (using deployment groups) but instead copy it internally directly from the Jenkins build server. Does anyone have any suggestions? 

From the perspective of build automation, I find that it is better to have multiple repositories. This allows for smaller configurations and more granular control of your build/release process. You can allows reference and pull in source code to build or release (of course you can also ignore source code as well in VSTS build pipeline). This leads to being able to release just the project you want or need to your desired environment. You can of course pull in artifacts from other builds and include them in your release if you need to. To summarize, I tend to go as small as possible and and only group things into the same repository if they are indeed part of the same project and no part of the project can be used elsewhere. 

If it needs to be kept, continue to step 4. If it needs to be deleted or archived, delete and archive it, and then return to step 2. 

I've used zookeeper before as a configuration management service to keep track of application settings, feature toggles etc. However, that was back in the day before microservices and AWS/Azure/GCP. Since zookeeper tends to need multiple VMs to operate, is it still a good tool in the modern ecosystem? Are there better ways to handle feature toggles and centralized configuration settings for multiple environments using a more modern (aka serverless style) system? 

Our team has two separate repositories for a frontend/backend system. They would like to have these two repositories deployed to the shared team environment together. My understanding of the JenkinsFile is that it will only work on the repository it committed to and only run when the SCM system sends a request to Jenkins informing it of new changes. Is it possible to have the JenkinsFile in the two separate repositories communicate with each other so that when one is built the other is built as well and only after both are built, they will then be deployed? My main goal here is to avoid creating a separate Jenkins job within the Jenkins UI. 

It is actually very simple to convert a multi-branched hydra repository into a single branched model. First, you want to start with the branches which have the least difference between itself and master or trunk. Examine their age and relevance. If they are still relevant, start merging them together and resolving conflicts. If they are no longer relevant, then delete them. Continue this process until you have managed to merge all your branches, resolved all conflicts, and you have only a single branch remaining. You can follow this simple outline to get started: 

the release log displays but on the computer the script is running, the process isn't launched. If I run via powershell on the computer from the A1 folder, the process starts as expected. What am I missing? EDIT: Revisiting this after a bit. I have redone the script to use start-process . I have used combinations of using runas with credentials setting working directorys. The process does start...sort of. My target application is a WPF window. It launches but doesn't any any GUI drawn. I know the process is running because i can use which returns a valid process. Does anyone know how to launch wpf/console application and have it draw it's GUI? 

Relatively speaking, the concept of devops is new and still defining itself in my opinion. I currently fulfill a devops engineer role. For me, this means I facilitate and develop the tools and processes used by both our dev and ops teams freeing them to focus on the product that generates revenue for the company. The ops and dev teams spin up their own servers and such as needed. I just hook up the CI for our products, ensure our processes makes sense and seek out what process can be improved/automated. I meet with all of our departments, from sales, to warehouse, to developers and operations (QA and release managers) to see what they are doing and how I can improve their process.