lost completely and forever because of lack of backup lost production because you perform a restore without being able to recover last TX. lost production because the site was unavailable 

You could consider using a Physical Standby database and flashback. If your primary does not make to many changes this could be a very valid solution like described in 12.6 Using a Physical Standby Database for Read/Write Testing and Reporting This is fairly easy to script to make it a painless daily exercise. 

If the database is setup correct: no direct path load. If a standby database is depending on the changes captured on the primary database, the primary database should be setup with force logging. If for some reason the force logging setting is not in place, after such an operation the affected datafiles should be transferred to the standby site[s]. Failing to do so will make queries on the loaded parts of the table[s] in the standby database to fail. 

The error you encounter can have many causes. One is an incorrect environment. For Oracle, the variable ORACLE_HOME is very important. Start with setting that to where you installed your Oracle software. Next make sure that PATH points to %ORACLE_HOME%\bin After that, set the variable ORACLE_SID to the sid of your database instance. (The sid is part of the service name as is shown in the services control panel) Once this is setup, you commands can be issued as you stated. 

this depends on how the dependency trees are. If you have lots of dependend objects on the table[s] where you load the data in, best is not to change the object, the table because lots of invalidations will take place. truncate table gets rid of the data very fast but won't work when referential constraints are in place. It also won't work when the app is running because of locking issues. Best for the application will be a normal delete/insert sequence. Not the fastest option but it is best for availabillity and has the least problems with locking. Other options can be partition exchange loading. Constraints can be problematic and take a lot of time to validate. Never ever drop a table just to get rid of the data. A table is to be considered as application infrastructure and should allways be available, because the application will throw lots of errors to your users. 

Short answer: NO. The import does several things. One is creating tables and inserting data in it. Next come the indexes, constraints, triggers and other objects. There is no such thing as a prediction as how long it would take. For the reading of data, if the indexes, triggers and constraints don't play a role, you can assume this is a linear process, where normally 1GB takes 10 times the time of reading 100MB. For the other stuff this does not work but for the creation of a specific index - for example - you could check v$session_longops to find the time_remaining for that index creation. You still don't know what is next. To find what is next, you could create an sql file that contains the ddl that is to be executed. This could help in giving an idea what is still left to do. Don't forget to specify a large buffer for imp. 

Yes, you can. This is called tracing the Oracle optimizer. Doing so creates a trace file in which the optimizer dumps all reasoning done when composing a plan. An example of generating a trace for a specific SQL can be found here how to trace optimizer for specific SQL system wide - 10053 trace event You might need a little time to read it, given the amount of data found in the trace, it is amazing that the Oracle even finds time to do this and return in a decent time. 

parameters. Depending on your availability needs you use more or less db_create_online_log_dest_N parameters. If you are serious about availability and performance you have separate disks and controllers for the various locations. Good documentation to start 

If the files happen to be from a valid backup you could have some luck, but mostly because you could turn in into a running database again. The complete system tablespace should be among the files you have to make a chance. Only reading data from the bare dbf files .... there are many options that make this hard to do. Depending on what you know about what you are looking for makes this more or less possible. Start with: forget it and be happy with any word you can recover this way. dd and od might be your friends, compression and encryption not. Maybe Kurt can help you, see dude could be your best option. 

Your only solution for this problem is to take a backup of the datafiles that are hit by a direct load operation, or prevent force_logging operations totally. Force_logging operations may seem like a good idea, performance wise but if you care about your data ..... don't do it. 

A partly online backup is certainly possible but switching to and back from archivelog mode needs downtime. As an alternative you could run in regular archivelog mode and trash the generated archived log files, until you run a backup. For the backup to be recoverable it is important that it has ALL archived log files that were generated during the backup. End the backup with a log switch and also include those archives in the backup. You must see this kind of backup as partially online. It does allow you to restore to the point in time where the backup completed, not to any other point in time. Very similar to regular offline backups, only, taken online. It saves downtime. Why do you want this? This is a lot of effort just to safe maybe 10G of archive storage per day. It also introduces an extra error source and many dba's won't be able to work with this and expect to have all archives. My advice would be to keep it regular online backups, with regular archived log backups and keep things simple. 

4) for all your files in v$datafile, issue "alter database rename file '/old/name' to '/new/name'; 5) for all your members in v$logfile, issue "alter database rename file '/old/name' to '/new/name'; 

I see no unusable index. What I do see is that I can drop a parent partition and this cascades to the child partition. does this help? 

As longs as you don't do ANY updates on compressed tables you are fine. The old fashioned block level compression works fine when you can add data in batches. The old block level compression only works on loads, not on regular single record inserts. Assuming that archiving goes in batches block level compression is OK. PDF's will hardly compress using the available compressors in Oracle but when things like layout and font info is identical for many files, 7zip could be your friend but AFAIK this is not available in the database. If you want to compress transactional data advanced compression is for you but it has a performance penalty. When your data accesses are in scans, the compression could even give you a performance benefit because there are less io's to do. This is only true when you have plenty CPU available to do the inflight de-compression. If you have to do updates on compressed tables, your performance is dead and your volume is back to the uncompressed volume. So be careful. You won't be the first one who is hit by this feature. This is also very true for the archive compression mode in Advanced Compression. 

Preferably on a smarter location than /tmp. Not only does it contain your database layout, also the administration of your most recent and most valuable backup. 

The only way to get around this was to use credentials in the database links definition. Not nice if you ask me but it is what I found. This was in 10g and 11, did not test with 12c. 

If your scheduler database fails, this should raise an event in your monitoring application. This is about the similar situation as compared to cron, when the server fails. In that case cron also has no way to tell it failed. If you want to use dbms_scheduler as an enterprise scheduling tool, don't forget the reporting. Reporting should help explaining what ran and why not. 

Start monitoring memory usage. Does your application terminate old sessions? If not, there is a connection leak that eventually takes away all processes your database instance can use and prevent new connections. There also could be a memory leak, that is why monitoring memory and swap could be smart to do. Most likely to me seems to be the connection leaking. You can easily monitor this by making counts of v$session or v$process. These counts should not grow unlimited. Since you use jdbc (thin client I asume) Dead Client Detection could also be useful. This gives the rdbms the ability to clean up unused sessions. Unused sessions are those that do not react upon a ping from the server. 

The error in you script is that the script now expects a table named tablename, having a column named columnname. In this case you don't know the table and column names so you should use dynamic sql to run this. Next to that, if possible, forget about LONG and implement lobs instead. For docu see $URL$ sample code slightly modified to fit your needs: 

yes, you can, this in fact is default behaviour. See Database Administratorâ€™s Guide chapter 29 Scheduling Jobs with Oracle Scheduler 

You can not compare the performance of 2 databases by just looking at their sql plans. The same queries can have very different plans. When the versions are different all kinds of computations for the cost are different. The plan that a query follows in execution could very well be different than the plan you get from explain plan from your client. The only way to compare the performance is to measure the execution of the queries or processes and compare those timings. You could use AWR to compare the database performance. A certain workload will generate a certain thoughput and consume more or less dbtime. If the dbtime spent and the elapsed time of your jobs is lower, you can safely say that the performance in that database is better compared to the one where the times are higher. In 11g, with the auto tuning tasks enabled and auto accept sql profiles enabled, it could very well be that the next run already shows better timings. 

1) copy the file to where you want 2) create a pfile.ora that references your control_files 3) startup mount your database 

The update scenario is always faster than using a procedure. Since you are updating column X of all rows in table A, make sure you drop the index on that one first. Also make sure there are no things like triggers and constraints active on that column. Updating indexes is a costly business, as is validating constraints and executing row level triggers that do a lookup in other data. 

A residual predicate is one that has an hidden extra cost because the predicate has to be tested again on every combination of rows that is fetched. Probe Residual when you have a Hash Match â€“ a hidden cost in execution plans 

Currently Oracle does not support MacOSX. They did for Tiger but dropped support plans for Apple when they bought SUN Microsystems. If you are willing to install an older version of MacOSX you can install Oracle 10.1.0.5 (fully featured) or 10.2.0.4 (incomplete). For now: forget it. What you can do is run 11g on vmware. I have it running on macs using Parallels and VMware and they both work ok. 

Since you upgrade your OS, at some point you need to relink your Oracle software. In that case, easiest is to make a new software installation on the new server. Unmount the luns where the database is located from the old server and mount that on the new server where the database files should be mounted on the same locations as on the old server. I hope that the ORACLE_HOME is not on the same lun[s] as where the database is located. If ORACLE_HOME does exist on the same LUN[s], make sure to create the new ORACLE_HOME on a different PATH and make sure that config files like /etc/oratab and listener.ora reflect this change. This should give the quickest migration. If there is to be done more than just re-mount the LUN[s], make sure to have a capable DBA involved. That will save you a lot of downtime and problems. 

Yes, if you have all files placed in the same locations, you can create a pfile, make it point to the controlfile[s] and start the instance. If the backup was a clean cold backup, it should open without any problems. Don't forget to also enter the name parameters and the dump destination parameters for the database. You should restore at least the system, undo and your_desired_tablespace_files in order to succeed. Create a new temporary tablespace. 

In case of RAC this again introduces instances running on other nodes so again, filter using the current HOSTNAME 

you could be back in business by now. Since it is smarter to use an spfile, now is the time to create your spfile: 

I think you have AMM - Automatic Memory Management - enabled. This quickly brings you in the situation where all memory goes to the shared_pool and leaves you a very small database cache. Looking at the number of IO's this could fit your case. Start with a memory setup like you had while running 10g. Forget about AMM on a rdbms instance. 

Since SQLDeveloper is a Java program using a jdbc driver, the restrictions are made by the version of the jdbc driver. The v10 and v11 drivers can connect to v9 without any problems. For older version databases you might need to get the v8 or v9 jdbc drivers. 

Just save the ddl for the procedure. This can be done using sql-developer 'save to file'. Tell the client to start sql-developer, connect to the schema where the code should be written and have them run the script using 'start' or the equivalent for start '@' script_name The sql script should be located in the directory defined in the preferences for Database>Worksheet>'select default path to look for scripts' 

The default location for the password file is $ORACLE_HOME/dbs but since Oracle v12 it can also be stored in ASM. In that case you find it using srvctl 

Normally you use the oraenv script to set you environment correctly for a given ORACLE_SID. The ORACLE_SID and ORACLE_HOME combinations are maintained in /etc/oratab or /var/opt/oracle/oratab which is read by the oraenv script that should be sourced (called with a .) What oraenv by default sets does is make sure that PATH contains at least $ORACLE_HOME/bin If you want to have some control over where your SQL*net files are located it is smart to create a TNS_ADMIN environment variable that points to the directory where amongst others tnsnames.ora is located. The tnsnames.ora file is most important to be able to point to a - local or remote - database using SQL*net. If you only access your database from the same machine as where the database is running and you do not access an other database, a tnsnames.ora is not needed, just convenient. Normally we have a tnsnames.ora that contains lots of database on the site. 

In you scenario, as long as you have your control file backups, you won't need to set the dbid. This changes when you also loose the control file backups. Make sure that you have them since in emergency situations they are worth gold. Having a catalog database does not make life easier. 

SSD's can make READING data faster. Writing won't be any faster. Don't even think about placing the redo's on SSD since they are only written to. To speed up writing to the redo: add more drives and stripe them. Redo's are written sequentially so adding more spindles improves the write throughput, until you meet the controller limit. What is that test migration doing? Does it use procedural code or does it use sets? If using procedural code, be sure to implement bulk operations. Sets are allmost always faster. 

rman is the way to go for backup and recovery for protection against physical corruption. For documentation check OracleÂ® Database 2 Day DBA 11g Release 2 (11.2) In the current release of SQLDeveloper backup and recovery is integrated in the GUI. For download of SQLDeveloper see Oracle SQL Developer It is a free product, with active development. In SQLdeveloper in the View menu select DBA to get access to the dba tools like RMAN and DATAPUMP. Very nice tool. Don't forget that the backup files are written by the Oracle database server processes. This means that the backups are written on the database server, not on your client. Also, currently it just generates a backup script. You should run the generated script using RMAN. An other way to run and configure the backup is by using dbconsole or grid control. Using datapump is an other way to protect against data loss. Using this enables you to read back the data as it was during the export. Might be usefull for test sets that have to be refreshed with a specific version of the data. Not really usefull if you want to recover production transactions to the last moment before disaster stroke.