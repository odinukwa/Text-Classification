I'd imagine that you'd have to go from first principles, by taking a TM that exhibits an arbitrary BPP algorithm and simulating it in P. This is how it is shown that $\mathsf{BPP} \in \Sigma_2 \cap \Pi_2$. In particular, you have to "derandomize" the space of random choices much like it is done for specific problems, where a small set of carefully chosen seeds gives enough pseudorandomness. 

Caution: I know there are many on this forum whose head will explode at the idea of claims that are wrong but "morally correct" :). Remember that these are statements targeted towards the public (where some degree of license can be permitted), rather than statements made in a research paper. 

Most of the algorithms for estimating the volume of a convex polyhedron $K \subset R^d$ assume the existence of an affine transform $T$ with the property that $$ B \subset TK \tilde{\subset}\ \sigma B$$ where $B$ is the unit ball in $d$ dimensions, and $\sigma$ is $O(\sqrt{d})$. (Update: the $\tilde{\subset}$ indicates that the containment is true except for an $\epsilon$-fraction of $K$) The algorithms that I've seen for computing this transform are quite tricky. They require a bootstrap sampling process to extract a few points from inside $K$ which are then used to define the transformation. However, the fact that such a transformation exists is folklore, and my question was: 

Which got me thinking. There is a sense in which "signed" sorting is "directed" - you can view the sign as a direction (and indeed, this is the motivation from evolutionary biology). But it's an easier problem ! This is unusual because generally (at least on graphs) directed problems are harder (or at least as hard) as their undirected counterparts. 

I don't know if there's a list of canonical hard problems, but there is a list of open problems in streaming as maintained by Andrew McGregor. This is not limited to the specific models you're referring to though. 

I have a feasibility question that can be framed as follows. I'm given a point $p$ in a $d$-dimensional vector space, and I want to find the closest point $q$ to $p$ that satisfies a set of "$\ell_0$ constraints" of the form 

Best known app: $k$-means, iterated closest pair. Theory: Known results on $k$-means, general sufficient conditions for global optimality of framework p.s You might find that your answer ends up as a lecture in an algorithms seminar I'm planning :) 

Different sections of the arxiv have their own RSS feeds for paper announcement, or you can subscribe to the theory aggregator (for ECCC/arxiv/blogs) 

One lesser-known contribution is the Good-Turing estimator for estimating the fraction of a population "not yet seen" when taking samples. This is used in biodiversity. 

The logical progression subconsciously implies a chronological sequence of 'discovery' I think the entire premise here is wrong. For example, the whole story of analysis is a ton of hard work in the 18th-19th century to "fix" the nonrigorous analysis of earlier times. And if you've ever taken a class that teaches riemannian geometry, it might even start with the category-theoretic view, which is clearly not chronologically accurate. A logical progression, especially as taught in the classroom, emphasizes a viewpoint and statements about how the material fits together. It can sometimes reflect an orderly chronological understanding, but more often than not is a retroactive imposition of structure on a field that developed in a very unstructured way. The FA to TM progression is a viewpoint: one that tries to explore the implications of increasing complexity in the computing device being used. 

There is a truly awesome list of all known graph classes that have some nontrivial algorithms for MIS: see this entry in the graph classes website. 

You are aware of GCT, but you might not be aware of Mulmuley's earlier work on showing a separation between a subset of PRAM-computations and P, which uses geometric ideas of how a computation can be viewed as carving up a space. Many lower bounds for problems in the algebraic decision tree model reduce to reasoning about the topology of underlying spaces of solutions (Betti numbers show up as a relevant parameter). In one sense, ALL of optimization is geometric: linear programs involve finding the lowest point of a polytope in high dimensions, SDPs are linear functions over the space of semidefinite matrices, and so on. Geometry is used heavily in the design of algorithms here. On that theme, there's a long and deep connection between our ability to optimize certain functions on graphs and our ability to embed metric spaces in certain normed spaces. This is a vast literature now. Finally, in recent years there's been a great deal of interest in so-called "lift-and-project" mechanisms for solving optimization problems, and these make heavy use of the underlying geometry and lifts to higher dimensional spaces: notions from algebraic geometry play an important role here. 

One way to show that checking the feasibility of a linear system of inequalities is as hard as linear programming is via the reduction given by the ellipsoid method. An even easier way is to guess the optimal solution and introduce it as a constraint via binary search. Both of these reductions are polynomial, but not strongly polynomial (i.e they depend on the number of bits in the coefficients of the inequalities). 

Different CS communities have different processes, and it's hard to make a clear determination with the information given. Based solely on your description of the improvement as "more logical, more readable, more adaptable", it wouldn't necessarily merit coauthorship in a theoretical paper. It ultimately depends on how significant the improvements really were, and how nontrivial. I should add that at least in theoretical computer science, authorship tends to be handed out somewhat more liberally than in more experimental subdisciplines. 

If you don't want an optimal tree decomposition, you can build a tree decomposition by computing separators recursively. 

An old example is volume computation. Given a polytope described by a membership oracle, there's a randomized algorithm running in polynomial time to estimate its volume to a $1+\epsilon$ factor, but no deterministic algorithm can come even close unconditionally. The first example of such a randomized strategy was by Dyer, Frieze and Kannan, and the hardness result for deterministic algorithms is by Bárány and Füredi. Alistair Sinclair has nice lecture notes on this. I'm not sure I fully understand the "and it shouldn't" part of the question, so I'm not sure this fits the bill. 

Bernard Chazelle has done some work on the convergence of bird flocking, and he's considered models such as follow-the-leader and variants. 

Jeff Erickson will not say this himself, but his online lecture notes are among the best out there to cover the basics of algorithm design at a level that doesn't patronize the reader. I use them in my grad algorithms class, and for a research mathematician, these notes convey the right kind (and level) of intuition, allowing you to fill in the details yourself easily. 

For some obscure reason everyone is answering in comments, so let me comment in an answer. While I am not aware of any standard macros for this, I'm partial to the G&J format (which is basically the format you use in the question. Namely, 

Expander Graphs and their applications, by Hoory, Linial and Wigderson. This is verging on monograph territory at 123 pages. 

The two approaches provide very different guarantees. The JL Lemma says essentially "you give me the error you want, and I'll give you a low dimensional space that captures the distances upto that error". It's also a worst-case pairwise guarantee: for each pair of points, etc etc The SVD essentially promises "you tell me what dimension you want to live in, and I'll give you the best possible embedding", where "best" is defined as on average: the total error of true similarity versus projected similarity is minimum. So from a theoretical perspective they solve very different problems. In practice, which one you want depends on your model for the problem, what parameters are more important (error or dimension), and what kind of guarantees you need. 

I can't think of any graph property that has had as much impact on the design of efficient algorithms as bounded treewidth, and bidimensionality in general. 

Starting from these questions, a whole host of other topics spring up, touching on applications areas, mathematics, other parts of computer science, and so on. 

Show that the problem is GI-complete (GI = Graph Isomorphism) Show that the problem is in $\mathsf{co-AM}$. By known results, such a result implies that if the problem is NP-complete, then PH collapses to the second level. For example, the famous protocol for Graph Nonisomorphism does exactly this. 

Here's a new application, hot off the presses ! A new ECCC report by Or Meir has this as its abstract: 

The lower bound problem you're looking for is the GAP-HAMMING problem. Sherstov has a "simplest" result for the general communication complexity of GAP HAMMING, and in his paper he has a nice review of the related literature, including the sequence of references for the linear lower bound on the one-way communication complexity of GAP HAMMING. 

I think an excellent (non)-answer along these lines was given by Dijkstra (always a good source to turn to for crusty and absolutist pronouncements :)). 

This is probably not exactly what you had in mind. But in a certain sense, the independence of P vs NP from oracles is such an example. What it really says is that if all you care about is simulation and enumeration, (i.e if that's your "model" of computation), then you cannot separate these classes or collapse them. A more concrete algorithmic example comes from approximate range searching in the "reverse" direction. Specifically, most range searching problems are phrased as semigroup sums, and lower/upper bounds are expressed without regard to the structure of this semigroup (except for some light technical conditions). Recent work by Arya, Malamatos and Mount shows that if you look closely at the semigroup structure (the properties of idempotence and integrality), then you can prove different (and tighter) bounds for approximate range searching. 

(transferred from a comment above) The problem you want to solve is called Graph Bisection and is NP-hard. A simple heuristic that might work well in your case is to find two nodes that are as far apart as possible, and then grow out their neighborhoods till you get roughly equal sizes on both. Another local improvement heuristic is to take a candidate solution, and find two vertices to swap that will reduce the cut size. 

Your question is very well timed, because the most recent issue of the CACM has an article that does exactly this: $URL$ In brief, there's a lot of work by Conitzer, Tovey and others on the actual hardness, both worst-case and under distributional assumptions, of cracking voting mechanisms that are in principle breakable via Arrow's theorem. 

One interesting extension (although maybe it's well known to you) is the variant that allows for partial matching of vertices to other vertices (in the bipartite setting). This variant can also be solved using the Hungarian algorithm, and is known as the transportation problem (the resulting metric is called the transportation metric, the earth-mover distance, the Monge-Kantorovich-Wasserstein distance, or the Mallows distance, depending on who you ask). 

This is technically not in scope for this site, but we're friendly people here :), and your question is relatively well-formed. It sounds like what you have is a directed graph where the strings are your vertices, and your two tables are storing the "forward edges" (a -> b) and the backward edges (a <- b). The question you're asking is whether there's a way to go from one vertex in this graph to another. The answer is to do a graph traversal (breadth-first or depth-first) on this graph. Any basic textbook on algorithms (like this one) should help. 

Michael Mitzenmacher has a nice survey on Bloom filters where he outlines many applications. Maybe some of these might help. 

One way to "unify" circuits and uniform computations is to require a complexity limited procedure that takes $n$ and outputs the advice circuit $C_n$. In the case of P, I believe that requiring a polynomialtime generator that can do the above will capture P correctly. 

Arora and Barak show that $\mathsf{AM}$ can be expressed as $\mathsf{BP}\cdot \mathsf{NP}$ i.e the set of languages that have randomized reductions to 3SAT. $\mathsf{MA}$ is also a natural randomized generalization of $\mathsf{NP}$ in that you replace the deterministic verifier by a randomized one. Is there a sense in which one of these is a closer fit in the "P is to BPP as NP is to ?" relation ? 

I'm wondering if there are good references that describe the random restriction method as a lower bound technique ? I'm aware that it's linked to the switching lemma and shows up in many different proofs, but I'm looking for a more or less self-contained explanation of the idea. 

The complexity of a union of objects is the number of pieces in the boundary of the union. In the plane "complexity of boundary" is equivalent (upto a constant factor) to "number of vertices". But in general the boundary complexity is the sum of complexities of all objects needed to describe the boundary (vertices, edges, faces, and so on). It's just that in the plane the number of vertices in the boundary is the same as the number of edges (because the boundary is a set of cycles in this case) 

Biased estimators are useful in statistics because they can optimize mean-squared error more than what an unbiased estimator can manage. I was wondering if in theoryCS if there are any very notable examples of the effective use of biased estimators. I realize this list could get long, and if it does I can modify this question to a big-list CW question, but for now I'm just curious. 

Note: these requirements are merely one way of formalizing the idea that I'd like something that runs faster than $d \log d$ for small $r$. 

Here's one idea for the exact answer, that I suspect Chao Xu might be alluding to. Firstly observe that we might as well normalize $x$, as Chao points out. Now consider the hyperplane $h$ normal to the direction $x$. The goal is to find the point closest to this hyperplane. By duality, this corresponds to a ray shooting query in an arrangement of hyperplanes to find the closest plane "above" the query point. Since this can be preprocessed, the main complexity is the point location, and so your problem has now been reduced to the complexity of doing point location in an arrangement of hyperplanes. Using cuttings, this can be done in $O(\log n)$ time in $n^d$ space. 

I'm not sure I understand. A physical law (of the kind you indicate) is a mathematical expression of a model (in that example, relativity) that claims to capture reality. A physical law can be proved wrong if the underlying mathematics is incorrect, but it can also be wrong if the underlying model changes (for example, newtonian mechanics). P vs NP is a specific mathematical conjecture that is true or false (and might be provably or not) 

I could be missing something, but if you took an arbitrary DAG, assigned unique IDs to each vertex, and constructed all words formed by traversing monotone chains from top to bottom, then wouldn't the DAWG of that set be the original DAG ? If so, then a DAWG has no special structure. Update: Peter Taylor's comment is the key difference. only one node with indegree 0 unlike a regular DAG. 

It depends. Gerhard Woeginger has a paper on a related problem: when a dynamic programming formulation of a problem leads to a PTAS. 

$k$-means.. More generally, you should look at the Kanellakis prize winners for ideas that have theoretical and practical heft. 

If I'm not mistaken, obtaining a constant factor approximation for the Steiner tree is NP-hard on directed graphs but is P-time on undirected graphs. 

You're asking essentially a modelling question, and so the answer really depends on your data. If you're saying that strong correlations are not being identified by the methods you listed, it's possible that the correlations are non-linear (the above methods with the exception of k-NN are linear), and so you might try something like ICA. For something closer to home, have you considered correlation clustering ? you don't say how the data is presented, but if you have similarity and dissimilarity information, then correlation clustering is a good solution, and also does not require you to specify the number of clusters. Another idea would be to try spectral methods (if all you have is a distance function) like spectral clustering. 

I didn't read the last few comments: this approach only makes sense for a fixed set of polygons and many different query points. I haven't thought this through in detail, but something like the following might work: 

The notion of closeness varies, but for now it is sufficient to assume a convenient distance like $\ell_2^2$. Are there any known relaxations to linear constraints that are "good" in the sense of providing a "close enough" polytope to approximate the original constraints, where I'm also pretty flexible on the definition of "close enough" 

Problem 1 is known as SET PACKING. Like other packing problems, it's annoyingly hard. The best known bound is a $O(\sqrt{|S|})$ approximation and it is indeed APX-hard.