It will mean a higher network load while playing, until all content is cached locally, but a smaller initial burst load on your servers as well as a shorter and less frustrating wait for users while they're initially downloading the game. Of course if the user has a slow network connection he might still get "please wait, loading content" messages (do provide those) when switching zones, or you might want to check the bandwidth when initially starting the game and if too low to download reliably on the fly have him wait there and then for at least the entire first level (and then again when entering zones that haven't been fully downloaded yet). This to prevent the user lagging out in the middle of actual gameplay because his network connection gets bogged down. 

Will you be able to write something like that on a sunday afternoon in your bedroom? No. Can it be done? Well, it has been done so surely it can be done but it's going to take a team of skilled software engineers several months to years to put together from scratch. 

Anything that references trademarked and/or copyrighted content (and everything is copyrighted at the moment of creation by its creator) should not be used without explicit permission from the trademark/copyright owner. When in doubt about trademark or copyright status, assume trademarks and copyright exist, see 1. on how to proceed. Commercial status of either the original or the derived work are irrelevant to this. Don't assume a "fair use" claim on your part will be valid. Such systems are far from universal in IP law and where they do exist can be interpreted to be extremely restrictive. 

I'm going to go way out on a limb here, and suggest that they specify texture coordinates for the mesh. 

Discounting the fact that your image shows two point clouds not one, I'm going to assume that you don't want to do cluster analysis to find 'blobs' of points. If you do, just do that first. The primary part of the problem (the boundary polygon) needs more definition. If you just want a convex hull, then the gift-wrapping algorithm works fine. If you want a tighter fit (as shown in your image), then you need to alter the criteria by which points are rejected. In the basic gift-wrapping algorithm, you start at a point known to be on the boundary (the left-most point), and pick points such that for each new point you pick, every other point in the set is to the right of the line formed between the new point and the previous point. To get a tighter fit, all you need to do is modify the rejection criteria. Instead of rejecting points which have any points 'to the left', you reject points which have others which fall within the area bounded by a line extending out to the left, perpendicular to the boundary line. Hard to describe accurately, so see these examples. The black dotted line denotes the area within which any points would cause the line to be rejected. 

Having such restrictions on your players would get extremely annoying for them very quickly. You're effectively forcing them to be online and playing your game nearly 24/7. What will happen when a user can't log on for a few days because they're on a vacation? All their animals and plants would die, there might be other disasters. Not very userfriendly. Better to stop the timer when the user logs off, and start it again when he next logs on. Effectively their farm only exists when they're on the server (except of course that noone can buy land on its area, if you're working with such a system). 

And yes, it has been done. Check out Second Life (and its clones) which is essentially just that, apart from being a social network of course. Everything in the world is created by its inhabitants (or in a few places by the owners/creators of Second Life, using the exact same tools). There's even a saying among SL builders "everything starts with a cube" (a .5x.5x.5 meter plywood cube is the default building block you get when you open the build window, it can be changed and twisted into all kind of things, as well as linked with others). It also has a decent custom scripting language, allowing interaction among objects, parts of objects, avatars, the world as a whole, etc. etc.. 

Fluid dynamics is one of those super hard things to set up, that once you've got it working allows for a whole range of interesting effects. It's probably overkill for most games, unless you actually need things to move like a fluid (as in, flow from one point to another). For soft-body masses, I would considering instead using nets of springs to simulate your bodies; much more straightforward to implement, and far easier to handle from a gameplay point of view. As a possible initial implementation: imagine that your body would have a centroid, that you could apply forces to and move around. Around that centroid, you'd attach additional nodes, linked to the centroid with a spring, and linked to each other to form a network that holds its shape. For example a hexagon of 6 nodes around the outside, each linked to their neighbour and also the centroid via springs. As you apply forces to the centroid, the springs act to pass on that force to the outer parts of the body; the front compresses towards the centroid, the rear elongates as it is pulled with the centroid. That's not the only way to do it, you can have the movement forces applied equally to all the nodes, it all depends on how you imagine these bodies to work internally. Apply forces to all nodes equally, and the body won't deform unless it encounters something else. And it's in encountering "something else" that things get interesting, and cause your body to deform nicely. Bear in mind you're simulating something soft and squishy, not a bunch of hard spheres connected together. So maybe instead of using rigid body collision, give each of the nodes a repulsion from the other objects in your simulation. Then, as you move your body towards an object smaller than itself, the gap between nodes will widen to allow the object to slip between them. After the object has passed through, the springs that hold the body together will cause it to reform into its original shape. Objects larger than the body won't be able to fit through the gaps, so your body should collide with the larger object and flatten against it. It's possible you want to implement rigid body physics as well, to prevent a squishy object penetrating through something that should be solid, but it should be a backup to the repulsion-based forces that are simulating the squishy exterior of your body. In terms of rendering, you can get a nice blobby effect with a pixel shader or render-to-texture approach, basically rendering a circle around each node such that the overlaps are invisible, but around the edges there are nice soft circular edges. So look up spring networks and Verlet integrated spring simulations, and you should be able to get the foundations of a system that will let you do 'blobby' physical mechanics without doing full on free-moving fluid simulations. 

There's no magic number, depending on the size and complexity of your system, the number could be anywhere from two or three people (you do want some duplication of effort, to cut chances that someone misses something) working for a few hours over a weekend to dozens of of people working full time for months. And with consumer products there's the added factor of disparate hardware/software combinations it will run on. It wouldn't do to test a graphics heavy game for example on only 2 computers, both with the exact same videocard, operating system, and drivers. It will need to be tested on a broad range of hardware/software combinations to determine if it works properly on all of them (or at the very least to be able to mention in your documentation what the hardware/software requirements are to run it). 

You do realise that when you're in game development you won't be playing many games? That most likely you'll quickly come to loath playing games because you have to test little bits of them over and over again to analyse errors? You seem under the impression that working at a game development shop means you're just going to be paid to do what you like to do now, play a certain genre of game when and how you like to do it. Nothing could be further from the truth. You're going to be spending a lot of time not playing games but in exhaustive meetings about minute details about them. The rest of the time will be spent working on implementing or testing those minute details, and if that includes any playing it'll be playing the same little bit of the game over and over again to test that little detail you happen to have been assigned to working on. It's a job no different from any other, and any job can quickly destroy what enjoyment you have in the thing you're doing. Which is why I always advise people to NOT try to turn their hobbies into their jobs. 

I covered this somewhat in my other comment, but I think here you're thinking about external / internal classification. By removing a voxel, you are changing the voxels around it into 'edge' voxels (if they weren't already). This should boil down into 3 actual cases (symmetry gets you the rest of them) - in the example below the numbers are the group IDs, the - is the voxel being removed 

For this to work all objects being updated need to preserve the knowledge of where they were before and where they are now, so that the rendering can use it's knowledge of where the object is. 

I can't speak to using AndEngine, but the logic for an infinite scrolling backdrop is pretty engine-agnostic. Let's simplify and imagine the case where you are alternating between two images (it doesn't really matter how many you have). Imagine a number line for x, from zero to infinity. Now imagine that your first image (image 0) is placed on that line such that its left edge (assuming you're scrolling right) is at . Its right hand edge will then be at , where is the width of image 0. Then your next image is placed so that its left edge is at , and its right edge is at . Then the first image is placed so that its left edge is at and its right edge is at . From the end of the second image on it repeats, and once you know (the sum of w0 and w1; i.e. the combined width of all the images), you can figure out which image should be where without iteration. This generalises to a simple function, which can say for any given , what backdrop to use. That's because the answer for is the same as . You basically get: 

Calculate how much testing will have to be done (as time, say you come up with a figure of 200 hours), then figure out how much time a single tester would spend on the task, and you have a figure for the number of testers you need. Of course that should take into account retesting bugs they report, duplication (especially with volunteers testing games), people not actually doing the work they sign up for (especially with volunteers testing games, many just want to play something new and shiny before anyone else, and get a free copy at release date as a reward). 

why would anyone pay to remove an ad they see for only a few seconds during startup of the application? Or are you planning to have the browse through 10 minutes of advertising before they're able to start playing unless they pay up? Because in that case you won't get many users at all, they'll just uninstall and try something else. 

Whichever you do, it doesn't matter. If you rely on clientside calculation of anything you will get hacked. All the "anti-hacker" tooling has AFAIK been thoroughly penetrated, new versions often themselves being hacked in a matter of hours after release. Given that, browser games are a major PITA IMO (poor usability) though they do offer ease of installation. So push as much as you can to the server, validate and counter validate everything sent by the clients to the server, and aggressively detect and ban any violators. Of course you do have to account for lag which can cause weird results being sent (or received), which is another reason you don't want to rely on clientside anything. So the client tells the server he's moving in direction alpha, let the server decide the next position, not the client. Client tells the server he's pulling the trigger, let the server decide whether there's bullets left, where they're going, and whether anything is hit (and what damage they do). Done like that, the client becomes little more than a tool for rendering the world and allowing the user to request things (data, movement, etc.) from the server with the server deciding whether those requests should be honoured based on what it knows about the real abilities of the client (this also makes it impossible to render walls invisible for example, unless the client avatar is known to the server to have X-ray vision, a decision made based on gameplay and resources the server assigned to the avatar, not the client software). 

Usually the metric used is node overhead. You can take it to the pathological case (if you can subdivide you do), but at some point the extra effort stops gaining you better results. For normal spatial partitioning octrees, that's processing overhead and memory overhead. Marching Cubes is an odd one, because the subdivision is gaining you more accuracy. The finer grained the nodes, the smaller the polygons generated, and the closer the surface generated is to the actual surface shape. But the same principles still apply - you're choosing to subdivide if the smaller nodes will get you substantially better results. The heuristic is basically just a decision on how much better things will be if you subdivide. The threshold is just a number below which you consider the improvement in results to be trivial / insufficient to warrant the extra cost. In marching cubes, the items (polygons) don't exist when the partitioning tree is being generated. Only the density and surface threshold is available. The triangles representing that surface are still to be generated, but the assumption is that lumpier parts of the surface will need more triangles to represent them. So the metric being used here is "how lumpy is the surface within this node". If it's flat/planar, it doesn't need subdivided, because the triangles in that node will be large and relatively flat. I can't follow the maths exactly in your linked paper but I presume the logic is the same as any error metric. For each node you can calculate an error - if you stopped subdividing at that node and just generated the surface polygons at that resolution, how bad would the result be (in terms of error difference against the real surface). You'd be calculating 'error improvement' from subdividing, where the improvement in the error is: Reading further into the heuristic, it seems like they are comparing the interpolated values between the 8 corners with the actual values at the subdivided node corners. Not sure why they're interpolating trilinearly. But this seems to me like a good way of estimating lumpiness. If the surface was relatively flat, the surface values at the midpoints used for the sub-divided nodes would be very close to the values you'd get if you just interpolated from the corners of the parent nodes. If the surface was very lumpy, the actual and interpolated mid-points would be way off. How far off can be calculated - and that looks like what that graph is. v represents the distance from the estimated surface to the actual surface at that point. Higher values of v mean that the actual surface has diverged a lot from the estimated surface that you'd get if you didn't subdivide. That gives you an error metric that you can use to threshold whether or not to subdivide. If it were that naive though, it would fall down in the case where sinusoidal variations in the surface neatly aligned with node boundaries, and happened to result in the mid-point being accurate (imagine a case where in the left side of the node, the surface bulges outwards, but on the right side the surface bulges inwards by the same amount - at the midpoint, the surface is at the same position as it would be if it were flat). I presume that's why the tangent of the surface is also considered - it's much harder for the surface to be in the right place with the right tangent and not also be flat. 

why have such items in the first place? Rethink your system to where you don't have to choose between things that last forever and things that can be used only once. Have things wear down over time, at which point they either have to be repaired or replaced. More powerful items can be made to wear down faster, and/or be more expensive to repair and create (if you have a crafting system). Ryzom did this very well, having a highly involved crafting system where the best items in the game would require rare resources that could only be harvested at specific times in seriously high risk locations, often requiring teams of people just to protect the harvesters against the local mobs. It could take weeks to gather the resources to craft one of the best weapons or armour pieces in the game (over time of course people started hoarding the raw materials, some guilds even going so far as to harvest things they didn't need in order to deny them to others). Combined with storage options that allow enough inventory to keep items for special occasions, this created both a lively trade between players (it took a lot of effort to learn to craft those items too, so the number of people capable of doing it was low and not always the same as those who could harvest the raw materials) and provided an option for people to have different equipment sets for different occasions (which was essential, as different mobs required different armour and weapons to combat, and for pvp different gear was required again, harvesters needing different gear again for that). The game may still be up, haven't played it in a few years though, might go and take a look again. Good memories.