you can use iptables to redirect port/ip. examples are given here: $URL$ or you can install rinetd, which i think is easier, in my opinion. i usually use rinetd. 

I would suggest that you use a network server with a filesystem designed to handle what you need. The first thing that comes to mind would be something that supports zfs (freenas and nexenta, though the free version of the latter has some limitations) or if you can afford it you can buy something like netapp. I am less familiar with UFS available on freebsd etc., but heard that would work too. 

this could be caused by several different things, file/directory permission is one, mysql version/OS mismatch, or just package bug. it is hard to tell which ones you are hitting. you can alternative try to use the packages directly from mysql download or percona which are often easier to install. if you can locate this binary, try to run the following: mysql_install_db --user=mysql --no-defaults this may fix things. otherwise, try to bring your OS up to date, purge the mysql packages, and install again. 

Your question 2 is actually the correct answer, but doublecheck that you have: hosts: files dns in your /etc/nsswitch.conf just to be sure. generally external dns should be in your dns record, but in server1, files (which means /etc/hosts in this case) will take precedence and thus used to resolve the host to the internal ip, while the rest of internet will resolve the same hostname to the external ip. the usual practice though is to use an internal domain altogether if you have to use dns name (mydomain.local for example). 

while your laptop is unlikely to have an interface that will do vlan tagging, some servers can. vlan tagging just adding some info to the ethernet frame so presumably you can do it with programming, but usually it is done by switches to send them through trunk lines where frames from all vlans are mixed together. yes, generally switches, but other devices can do tagging as well. the end devices should only see untagged ethernet frames, as the vlan tags are stripped before handing it to the end devices, unless the end devices have interfaces that does the stripping. 

you need to find and replace that uuid in your grub configuration, something like /boot/grub/menu.lst in grub. in grub 2, /boot/grub/grub.cfg is generated when you run update-grub, so you could temporarily modify that file and boot, and then change the grub settings and run update-grub once you have booted into your OS. another option is to temporarily replace the uuid reference to the actual disk partition, and again fix that later. 

How did you do P2V? Is it with the converter? For linux vms, you could also just boot up physical and vm both with livecd and do an rsync with grub installation, or dd if the disk sizes are the same. I often find this works better than converter. You can try copy initr image for sure. initrd image is just an archive that among other things loads modules necessary for booting. Is your customized initrd image for handling nonstandard hardware in the phsical server? In any case, loading the modules without the corresponding hw available should not matter. 

You can calculate a hash (say md5sum) of the file, and then send that along for the recipient server to verify. This is a standard way of verifying the file integrity, if you are concerned. 

install lshw in your system first. then sudo lshw and examine the output. look for the network sections. mac address is called serial: in the output. or you can look for Ethernet. the nics may not be called ethX (logical name) depending on your distribution, however. 

The question about which is better is completely subjective and I wouldn't even recommend bringing it up. In fact I assume one of the forum moderators will either edit down or close the question in about 4 milliseconds because of this. The "which is better" discussion is HIGHLY specific to your needs, your politics, development and operations skills, preferences, biases and many many more options. It can't be determined by an outsider whether the change makes sense or is a management game -it could be a combination of all or none. This question should be asked to people in your company. Costs are also nearly impossible to compare without significant analysis. Microsoft and Lotus generally sell each corporate deal at a different price, not to mention possible resellers and their discounts, and features you want, number of servers, etc. It's impossible to answer any of this so I recommend directing the questions inward towards your organization. 

Let's say my users have accounts on some mail server mail.example.com. I currently have my mx record set to mail.example.com and all is good. Now let's say I want to have mails initially delivered to an external service (e.g. Postini. Note that this is not a postini-specific question though). In the normal situation where my mx is set directly to my mail server mail.example.com, sending MTAs will of course look up my MX and send to mail.example.com. In my new situation I'd have my mx set to mx.othermailservice.com and emails would be received there. OtherEmailService.com will then relay the emails (while keeping the return-path header the same) to mail.example.com. Do the emails that are received at mail.example.com after be relayed from the other service "look" any different than emails that go directly to it as would be the case where the mx was set to mail.example.com? 

My internal subnet is 192.168.1.0/255.255.255.0. I don't understand the warning message since a 10.x.x.1 network can just as easily conflict with a remote network as a 192.168.1.x network can. How shoud I proceed? 

We faced a very similar problem. We eventually concluded that while integrated NTLM logon support in Internet Explorer and Firefox is convenient, there are so many exception cases which result in failure that we changed our approach. The problem with integrated authentication is that it works only when the currently logged on username and password are still correct and properly authorized to access the resource. There are more circumstances where it doesn't work however: 

Making backups is really a game of probability. Assuming the data is successfully written to any media (as confirmed by the backup program's "verify backup" function), the weak link becomes the shelf life/survivability of the media. Backup tapes can break and be demagnetized. Hard disks can crash. Optical media (like DVDs and Bluray disks) degrade over time. I view the question of "Is it safe to back up to media X?" less of a yes or no question and more one of your goals and retention requirements. If you're looking at a one-time/one-off/adhoc backup that you plan to use in the short term for recovery, then it's less an issue of reliability and more a question of convenience. Assuming that you're looking at a corporate server backup solution (e.g. ongoing backups, some media rotation schedule and some retention period requirement for each backup), it's still less of an issue of reliability (since you'll assumingly have at least a daily backup) and more one of convenience. So assuming your backup process is rigorous (done according to a schedule and verified for errors) and frequent, I see no issue taking advantage of the larger capacity of Blu-ray disks. Under no circumstances though would I rely on any optical media for long term storage. For long term, tape will be most reliable. To really reduce risk of long term backup storage and avoid restore failures I think it's important to have multiple backups stored in different locations. 

As it sounds like your company is running a captive portal this will be quite an issue. If you log on to the captive portal are you then able to use the proxy normally, or do you have to keep that portal session active? 

To completely block traffic you could do an iptables drop command for the outbound traffic to that dst address. ie 

A bandaid approach would be to listen non a non-privileged port, then redirect with xinetd from the privileged port. 

Below is how a large organization does it with Puppet and SSH keys. The puppetmaster distributes the user accounts, ssh settings directory for each user, and sets a password for each user. $URL$ This can be replicated in any config management system, not just Puppet. 

Have you tried mod_evasive? It may be exactly what you are looking for, depending on the block time you want. 

(Modified from this config example on monit.com) This could play into the ulimit option mentioned earlier as well. Restarting the service is a bandaid. You should instead try to find out why it's leaking memory. 

To connect directly to the desktop. This will automatically SSH to servername.domain.com, forward a port to the destination VNC server, then connect to that port. Replace desktop with the internal DNS name or IP of the desktop, and of course the rest should be obvious there. As an alternative there's also X forwarding, which you can bounce through a tunnel, but XLib does not make any attempts to optimize for bandwidth. 

In this situation you should probably try to duplicate the volume rather than trying to duplicate the individual files. Does your iSCSI target have a way to clone the volume? 

Mount the OS root partitions in /mnt/osA and /mnt/osB would give you some output similar to Solaris' You could then diff the more concerning files closely, like sysctl.conf, httpd.conf, etc. And how could I forget Blueprint! With Blueprint you can run against a system and get a recipe of what has changed from the default install. 

So you could specify any number of check cycles between checks for your expensive check. Configure Monit to have the check interval you desire. 

This is an area where I've been met with a great deal of frustration in Solaris. One unreasonable way I've found that could work for Solaris global zones would be to create differential flars (one from the beginning, then one afterwards), then extract the differential flar's cpio archive and pass it to a DIY monkeyscript that would create a Puppet module. When you have ZFS root pools you can just create a new BE, make your changes, lucompare and send that to DIY monkeyscripts which can spit out your Puppet modules. But again that doesn't help with non-global zones. 

In your second scenario you could possibly NAT the entire second network to a 10.0.0.0/24, IE 10.119.0.10 would go out your natty router, be sent to their natty router, then NAT to 192.168.1.10 on their side. If it even works it will be as much work as just changing the network enumeration on either end really. 

The name argument directly after is freeform and can be anything. The name is supposedly only used internally inside Monit for purposes of notification. As long as you specify the pid file, monit does not care what the name of the process is. See their docs (search for procmatch): $URL$ 

I found that once I had any kind of ACL or routing rule in place on a WRT54G that the processor limited me to approximately 12mbps of inspected traffic. I suspect that this will become your problem before any issues that might arise with the 2.4 band. 

You can create a text file containing the computer names you want to scan (1 name per line) and then run the MBSA command line program as follows: 

With Rackspace's (hosting this particular configuration) support, I now understand the situation. The "net use *..." drive mapping example and the service example is an apples to oranges comparison. With the drive mapping case it's just an authentication that's happening. In the service case, I'm actually attempting to run a local process under domain credentials, which by definition isn't possible since the server isn't in the domain. Not in domain = can't execute under domain credentials. The drive mapping works because I'm not attempting to execute a process as the domain account - I'm simply passing the credentials. This restriction applies to any type of process, regardless of it's interactive or service-based. 

I recently bought a Cisco RV110W Small Business wireless vpn firewall and I want to open a support case with the Cisco TAC. I assumed small business products came with a support period but I can't determine how to open a support case without a contract. 

I've got a standalone Windows Server 2003 running SQL Server 2005 and a Windows Server 2003 Active Directory domain controller. Using maintenance plans/SQL Server Agent, I'm trying to write the database dumps from the standalone box to a share on the DC. I know the usual rules about accessing remote shares (e.g. must use a logon account which has proper rights, etc.). In fact, writing the dumps to another non-DC server in the same domain as the DC works fine. I'm trying to set the SQL Server Agent account's logon credentials to "domain\username" (or username@addomainname). If I specify a username in the form of "domain\username, the error I get (regardless of password) is: "The account name is invalid or does not exist, or the password is invalid for the account name specified". If I specify a username in the form of "user@addomainname", the error I get (regardless of password) is: "The specified domain either does not exist or could not be contacted." I've turned on logon failure auditing on the DC and I see no failures in the log, which suggests to me that the machine isn't even trying to authenticate, but rather failing prior to that. I know that users on non-member servers can authenticate to shares on a DC, because doing an interactive logon (e.g. "net use * \dcname\c$ /user:username@addomainname", or using the other form of the username) works fine. The above example is about SQL Server but applies to any Windows service. Why can't the service log on with the domain account, but an interactive logon (drive mapping) using that same account works? 

I have a Cisco RV110W small office router (this configuration process is common to many Linksys/Cisco routers) and I am trying to define QuickVPN clients. When add a client of type "QuickVPN" the router gives me the following warning: (You can find a larger version of the screenshot here 

While a virtual machine guest theoretically doesn't "know" anything about it's host (it generally doesn't even know it's even a VM), you can get information about the host by simply treating the host as just another machine on the network, from the point of view of the guest. This assumes that the guest can see the host on the network. Once you have network access (and proper credentials) to the host you have a number of options to gather information: 

According to Cisco support, QuickVPN connections to this device require that the inside interface of the router be set to a 10.x.x.1 address. Assuming that your LAN isn't 10.x.x.1 this would mean a readdressing of all devices, DNS changes, etc. Ridiculous. They said this was because most of the networks where the QuickVPN clients come from will be 192.168.x.x, so this would conflict. I said that 10.x.x.1 would conflict as well in the case that the remote user was on a 10.x.x.1 and was told that the 10.x.x.1 range was picked because it's used less frequently than 192.168.x.x. So to prevent an conflict between the remote and central networks, the device was hard coded with this requirement. I'm used to Cisco VPNs where the VPN gets one or more unique subnets and NAT does all the magic. I don't understand QuickVPN connections well enough to know how the implementation is different. These restrictions apply to PPTP VPNs as well.