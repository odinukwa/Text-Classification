Isn't there a straightforward approximation-preserving reduction from maximum independent set (MIS) in undirected graphs to your problem? Given undirected graph G=(V,E), form DAG A=(V,E') by ordering the vertices arbitrarily and directing the edges accordingly, then take B=(V,{}) to be the DAG with the same vertices but no edges. Any subgraph common to A and B corresponds to an independent set in G, and vice versa, no? If this reduction is correct, then your problem is as hard to approximate as MIS; which is to say, forget about it. :-) 

2D local maximum input: 2-dimensional $n \times n$ array $A$ output: a local maximum -- a pair $(i,j)$ such that $A[i,j]$ has no neighboring cell in the array that contains a strictly larger value. (The neighboring cells are those among $A[i, j+1], A[i, j-1], A[i-1, j], A[i+1, j]$ that are present in the array.) So, for example, if $A$ is $$\begin{array}{cccc} 0&1&3&\mathbf{4}\\ \mathbf{3}&2&\mathbf{3}&1\\ 2&\mathbf{5}&0&1\\ \mathbf{4}&0&1&\mathbf{3}\end{array}$$ then each bolded cell is a local maximum. Every non-empty array has at least one local maximum. Algorithm. There is an $O(n^2)$-time algorithm: just check each cell. Here's an idea for a faster, recursive algorithm. Given $A$, define cross $X$ to consist of the cells in the middle column, and the cells in the middle row. First check each cell in $X$ to see if the cell is a local maximum in $A$. If so, return such a cell. Otherwise, let $(i, j)$ be a cell in $X$ with maximum value. Since $(i, j)$ is not a local maximum, it must have a neighboring cell $(i', j')$ with larger value. Partition $A \setminus X$ (the array $A$, minus the cells in $X$) into four quadrants -- the upper left, upper right, lower left, and lower right quadrants -- in the natural way. The neighboring cell $(i', j')$ with larger value must be in one of those quadrants. Call that quadrant $A'$. Lemma. Quadrant $A'$ contains a local maximum of $A$. Proof. Consider starting at the cell $(i', j')$. If it is not a local maximum, move to a neighbor with a larger value. This can be repeated until arriving at a cell that is a local maximum. That final cell has to be in $A'$, because $A'$ is bounded on all sides by cells whose values are smaller than the value of cell $(i', j')$. This proves the lemma. $\diamond$ The algorithm calls itself recursively on the $\frac{n}{2}\times\frac{n}{2}$ sub-array $A'$ to find a local maximum $(i, j)$ there, then returns that cell. The running time $T(n)$ for an $n\times n$ matrix satisfies $T(n) = T(n/2) + O(n)$, so $T(n) = O(n)$. Thus, we have proven the following theorem: Theorem. There is an $O(n)$-time algorithm for finding a local-maximum in an $n\times n$ array. Or have we? 

FWIW, your problem is hard to approximate within a multiplicative factor of $n^{1-\epsilon}$ for any $\epsilon>0$. We show that below by giving an approximation-preserving reduction from Independent Set, for which the hardness of approximation is known. Reduction from Independent Set Let undirected graph $G=(V,E)$ be an instance of Independent Set. Let $d_v$ denote the degree of vertex $v$ in $G$. Let $n$ be the number of vertices in $G$. Construct edge-weighted graph $G'=(V',E')$ from $G$ as follows. Give each edge in $E$ weight 1. For each non-isolated vertex $v\in V$, add $d_v-1$ new edges, each with weight $-1$, to $d_v-1$ new vertices. For each isolated vertex $v\in V$, add one new edge of weight 1 to a new vertex. (Note: each new vertex (in $G'$ but not $G$) has exactly one neighbor, which is in $G$.) Lemma. $G$ has an independent set of size $k$ iff $G'$ (as an instance of your problem) has a solution of value at least $k$. Proof. Let $S$ be any independent set in $G$. Then, since the vertices in $S$ are independent in $G'$, the value of $S$ in $G'$ (by your objective) is $$\sum_{v\in S} d_v - (d_v-1) ~=~ |S|.$$ Conversely, let $S$ be a solution for $G'$ of value at least $k$. Without loss of generality, assume $S$ contains no new vertices. (Each new vertex $v'$ is on a single edge $(v',v)$. If $v$ was not isolated in $G$, then the weight of the edge is $-1$, so removing $v'$ from $S$ increases the value of $S$. If $v$ was isolated, then the weight of the edge is 1, so removing $v'$ from $S$ and adding $v$ maintains the value of $S$.) Without loss of generality, assume that $S$ is an independent set in $G$. (Otherwise, let $(u,v)$ be an edge such that $u$ and $v$ are in $S$. The total weight of $v$'s incident edges in $G'$ is $d_v - (d_v-1) = 1$, so the total weight of $v$'s incident edges other than $(u,v)$ is at most zero. Thus, removing $v$ from $S$ would not increase the value of $S$.) Now, by the same calculation as at the start of the proof, the value of $S$ is $|S|$. It follows that $|S| \ge k$. QED As an aside, you might ask instead for an additive approximation, of, say, $O(n)$ or $\epsilon m$. It seems possible to me that for your problem even deciding whether there is a positive-value solution could be NP-hard. 

Assuming each partition step takes $O(m/\sqrt n)$ calls to the black box, the above algorithm, given input $A[1..n]$, will make $O(\sqrt n\log n)$ calls to the black box, because the recursion tree has depth $O(\log n)$ and each level of the tree has a total of $O(n/\sqrt n) = O(\sqrt n)$ calls to the black box. Do the partitioning step as follows: 

Now, in the given sequence of $n$ machines, replace the first $n_0$ machines $M_1,\ldots,M_{n_0}$ by these $n'< n_0$ machines $M'_1,\ldots,M'_{n'}$. Return the value computed by recursing on this sequence of $n-(n_0-n') < n$ machines. (Note that the oracle is not called before recursing, so that the oracle is only called once the base case is reached.) Here is why this computation is correct. For the $i$ such that $o_i$ is the ``correct'' response by the oracle $Q_0$ to the queries, $M'_i$ would halt and give the correct maximum output of the original $n_0$ machines. Thus, the maximum output of the $n'$ machines $(M'_1,\ldots,M'_{n'})$ is at least the maximum output of the $n_0$ machines $(M_1,\ldots,M_{n_0})$. On the other hand, by step 4, no $M'_i$ can give an output that is larger than the maximum output of $(M_1,\ldots,M_{n_0})$. Thus, the maximum output of the $n'$ machines $(M'_1,\ldots,M'_{n'})$ equals the maximum output of the $n_0$ machines that they replace. QED 

Average-Case Analysis of Algorithms Using Kolmogorov Complexity by Jiang, Li, Vitanyi. 'Analyzing the average-case complexity of algorithms is a very practical but very difficult problem in computer science. In the past few years we have demonstrated that Kolmogorov complexity is an important tool for analyzing the average-case complexity of algorithms. We have developed the incompressibility method [7]. In this paper we use several simple examples to further demonstrate the power and simplicity of such method. We prove bounds on the average-case number of stacks (queues) required for sorting sequential or parallel Queueusort or Stacksort.' See also e.g. Kolmogorov Complexity and a Triangle Problem of the Heilbronn Type. 

The isoceles triangle $\triangle oap$ (recall $p$ is the common point). The triangle $\triangle abp$. The little triangle $\triangle abc$ 

Isn't this a special case of matroid intersection, which is solvable in polynomial time? Fix your graph $G$ and any integer $d \in \{0,1,\ldots,\max_i a_i\}$. You want to maximize $d$; you can try all possibilities. For a given $d$, you want to answer the following question: 

Lemma 2. For any $\delta>0$, for all $n$, it is possible to reach position $n$ in $n^{1+\delta}$ moves using space $O(\delta 2^{1/\delta}\log n).$ Proof. Modify the construction from the proof of Lemma 1 to delay starting each subproblem until the previous subproblem has finished, as shown below:                    Let $T(k)$ denote the time for the modified solution $P(k)$ to finish. Now at each time step, only one layer has a subproblem that contributes more than one pebble, so 

Suppose for contradiction that $K$ is computable. Define $K'(x)$ to be the minimum encoding length of any Turing Machine $M$ with $L(M) = \{x\}$. There exists a constant $c$ such that $|K(x) - K'(x)|\le c$ for all strings $x$. Define function $f$ such that $f(\langle M \rangle) = \langle M' \rangle$ where $L(M') = \{x\}$ such that $x$ is the minimum string such that $K(x) > |\langle M\rangle| + c$. Since $K$ is computable, so is $f$. By Roger's fixed-point theorem, $f$ has a fixed point, that is, there exists a Turing Machine $M_0$ such that $L(M_0) = L(M_0')$ where $\langle M_0' \rangle = f(\langle M_0\rangle)$. By the definition of $f$ in line 4, we have $L(M_0) = \{x\}$ such that $K(x) > |\langle M_0\rangle| + c$. Lines 3 and 7 imply $K'(x) > |\langle M_0\rangle|$. But by the definition of $K'$ in line 2, $K'(x) \le |\langle M_0 \rangle|$, contradicting line 8. 

The game stops after some number of rounds. Your goal is to minimize your regret in comparison to any single expert (i.e., pure strategy) $i$. That is, your goal is to minimize $(\max_i a^t_i) - \sum_t p^t\cdot a^t$. Fix any $\varepsilon>0$. Let vector $y^t$ denote $\varepsilon \sum_{s \le t} a^s$, that is, $\varepsilon$ times the vector sum of the payoff vectors up to time $t$. Recall that $G(y)$ is the gradient of Lmax$(y)$. Here's the basic strategy we will analyze: On round $t$, choose $p^t$ to be $G(y^{t-1})$. By inspection, this gives you payoff $a^t \cdot G(y^{t-1})$ in round $t$. Because of the smoothness property of $F$, $$\mbox{Lmax}(y^t) \le \mbox{Lmax}(y^{t-1}) + (1+O(\varepsilon)) \varepsilon a^t \cdot G(y^{t-1}).$$ That is, in each round, $\mbox{Lmax}(y^t)$ can't increase by more than $\varepsilon(1+O(\varepsilon))$ times your payoff. Since $\mbox{Lmax}(\overline 0) = \ln n$, this maintains the invariant that $\mbox{Lmax}(y^t)$ is at most your total payoff times $\varepsilon(1+O(\varepsilon)$, plus $\ln(n)$. On the other hand, your regret in comparison to the best expert $i$ is $\max_i \sum_t a^t_i$, i.e., $\varepsilon^{-1} \max_i y^t_i$, which is in turn at most $\varepsilon^{-1} \mbox{Lmax}(y^t)$. Thus, your regret is at most $\varepsilon^{-1} \ln(n)$, plus $O(\varepsilon)$ times your total payoff. Remark: I think, as Freund and Schapire point out, a "boosting" algorithm (in learning theory) is also implicit in this analysis. See their paper for more details. Minimizing total payoff You can derive a similar strategy for the setting where the goal is to minimize, rather than maximize, the total payoff. Your regret, which you still want to minimize, is $\sum_t p^t\cdot a^t - \min_i a^t_i$. In that case, the corresponding strategy is to choose $p^t$ to be the gradient of $\mbox{Lmin}(y^t)$. With this strategy your regret is again at most $\varepsilon^{-1} \ln n$ plus $O(\varepsilon)$ times your total payoff. Connection to Lagrangian-relaxation algorithms To see the connection to Lagrangian-relaxation algorithms, fix a Set-Cover instance. Consider the latter type of game (with the goal of minimizing payoff), where the experts correspond to the elements $e$ of your set system. In each round, choose the probability distribution $p^t$ to be the gradient of Lmin$(y^t)$ as above, and have the adversary choose the payoff vector $a^t$ as a function of $p^t$ as follows: choose the set $s^t$ maximizing $\sum_{e\in s} p^t_e$, then let $a^t_e = 1$ if $e\in s^t$, and $a^t_e = 0$ otherwise. Given the correct stopping condition (discussed below), this process gives you exactly the Set-Cover algorithm discussed at the start. The performance guarantee of the algorithm follows from the regret bound as follows. Let $X_s$ be the number of times the adversary chose set $s$ during the play. Let $x^*$ be the optimal fractional set cover. Let $T=|X_s|$ be the number of rounds played. The regret bound implies $$\textstyle \sum_t a^t\cdot p^t \le \varepsilon^{-1}\ln(m) + \min_e \sum_t a_e^t.$$ Using the definition of $a^t$, the $t$th payoff (the $t$th term in the sum on the left) equals $\sum_{e\in s^t} p^t_e$. The adversary chose $s^t$ to minimize this payoff. If the adversary had instead chosen $s^t$ randomly from the distribution $x^*/|x^*|$, the expectation of the payoff would have been $$ \sum_s \frac{x^*_s}{|x^*|} \sum_{e\in s} p^t_e ~=~ \frac{1}{|x^*|} \sum_e p^t_e \sum_{s\ni e} x^*_s ~\ge~ \frac{1}{|x^*|} \sum_e p^t_e ~=~ \frac{1}{|x^*|}.$$ (Above we use that $\sum_{s\ni e} x^*_s \ge 1$ for all $e$, and $|p^t| = 1$.) Since each payoff is at least $1/|x^*|$, the regret bound implies $$ \frac{T}{|x^*|} \le \varepsilon^{-1}\ln(m) + \min_e \sum_t a_e^t.$$ By the definition of $X$, we have $|X| = T$ (each round chooses one set), and $\sum_t a_e^t = \sum_e [e\in s^t] = \sum_{s\ni e} X_s$, giving $$\frac{|X|}{|x^*|} \le \varepsilon^{-1}\ln(m) +\min_e \sum_{s\ni e} X_s.$$ We make the process stop when $\min_e \sum_{s\ni e} X_s = \Omega(\varepsilon^{-2}\ln m)$, so then (rearranging terms) $$\frac{|X|}{\min_e \sum_{s\ni e} X_s}~ \le~ (1+O(\varepsilon)|x^*|.$$ That is, normalizing $X$ gives a fractional set cover of size at most $(1+O(\varepsilon))$ times optimum. Remark: In a sense, this learning theory interpretation generalizes the algorithmic interpretation. However, some of the algorithmic techniques necessary for efficiency (such as non-uniform increments and dropping satisfied covering constraints) don't seem to carry over into the learning theory setting naturally. Likewise, algorithms for mixed packing and covering LPs (e.g. these) don't seem to have natural analogues in the learning-theory setting.