I tend to believe that c=3 is the right answer for the general case, and that it should be possible to give an example. I'll have to think more about that to know for sure. It's a good question, and I don't know of existing work about it. Research recently focused on which types of games have (best possible) c=1, mostly because of possible applications to amplification of unique games. 

Can you think of other examples? Following Andras' answer below: Indeed, every polynomial time algorithm can be converted to a slower polynomial time algorithm with exponentially small error probability. My focus is on algorithms that are as efficient as possible. In particular, for the two examples I gave there are deterministic polynomial time algorithms that solve the problems. The interest in the randomized algorithms is due to their efficiency. 

Yes, PCPs with imperfect completeness have been studied before. The main motivation is that for some natural and interesting problems, finding whether there is a perfect solution is actually easy (polynomial-time), while approximating the best solution, if this best solution is not perfect, is (or believed to be) hard. Here are some examples: 

There's this survey: $URL$ which has a focus on the hierarchies of convex programming. It has Max-Cut, Sparsest-Cut, coloring, hypergraph independent set, knapsack. 

The split find-min data structure is initialized with a sequence of elements $e_1,\ldots,e_n$, each associated with a key. The data structure supports three operations: (1) $Split(e_i)$ that splits the sequence at position $e_i$. (2) $FindMin(e_i)$ that returns the minimum in the interval that contains $e_i$. (3) $DecreaseKey(e_i,k)$ that decreases the key of $e_i$ by $k$. For instance, you may have started with $[5,2,1,4]$, where the min is $1$, then split at $2$ to get $[5][2,1,4]$, so the min of the first interval is $5$ and the min of the second interval is $1$. Seth Pettie gave an implementation of this data structure that makes at most $O(m\log\alpha(m,n))$ comparisons when $m$ is the number of $DecreaseKey$ operations and $n$ is the number of elements ($\alpha$ is the inverse Ackermann function). For more details see the paper: Sensitivity Analysis of Minimum Spanning Trees in Sub-Inverse-Ackermann Time $URL$ My question is: Suppose that you want to support queries not about which is the min element in each interval, but about which are the $l$ smallest elements in each interval, for a parameter $l\gg 1$ (note that you don't need to know the order among the $l$ smallest elements, only what they are). How many comparisons do you need for that? 

Invariance principles were motivated from hardness of approximation, but are useful analytic theorems. The principle: A low degree function, in which each of the variables has small influence, behaves almost the same, no matter if the inputs are independent random variables, or (corresponding) Gaussian random variables. This is a generalization of the central limit theorem; there the function is the average of the variables. Noise stability of functions with low influences: invariance and optimality E. Mossel, R. O'Donnell, K. Oleszkiewicz. Annals of Mathematics 171(1), pp. 295-341 (2010). FOCS '05. Low degree testing theorems were motivated by PCP applications, but are interesting algebraic theorems. The principle: An $n$-variate function over a finite field $F$ that, on average over the lines in $F^n$, is close in Hamming distance to a low degree polynomial on the line, is close in Hamming distance to a low degree polynomial on the entire $F^n$. Closeness in Hamming distance to a low degree polynomial in a certain space means that the function identifies with a low degree polynomial on some non-negligible fraction of the space. Improved Low-Degree Testing and its Applications. S. Arora and M. Sudan. In ACM STOC 1997. A Sub-Constant Error-Probability Low-Degree Test, and a Sub-Constant Error-Probability PCP Characterization of NP, R.Raz, S.Safra, Proceeding of the 29th STOC, 1997, pp. 475-484 

There is Zeev Dvir's result on the finite field Kakeya problem that was mentioned on this website before. Zeev used the polynomial method to lower bound the number of points in any set of points in F^n (F finite field, n natural number) that contains a line in every direction. This result actually drew attention of people in analysis to the polynomial method. Zeev's result was motivated by the task of constructing randomness extractors. This is part of a huge effort in theoretical computer science to derandomize algorithms, and ultimately show that P=BPP and similar complexity results hold. See more in Zeev's survey: $URL$ 

What you want is the generalized Chernoff bound, which only assumes $P(\bigwedge_{i\in S} X_{i}) \leq p^{|S|}$ for any subset S of variable indices. The latter follows from your assumption, since for $S=\{i_1,\ldots,i_{|S|}\}$, $$P(\bigwedge_{i\in S} X_{i}) = P(X_{i_1} = 1)P(X_{i_2}=1|X_{i_1}=1)\cdots P(X_{i_{|S|}}=1|X_{i_1},...,X_{i_{|S|-1}}=1)\leq p^{|S|}$$ Impagliazzo and Kabanets recently gave an alternative proof of the Chernoff bound, including the generalized one. In their paper you can find all the appropriate references to previous work: $URL$ 

Theoretical computer scientists both design algorithms and prove their correctness and running time. For some algorithms, the correctness and running time are evident, while others require proof. For the latter, without a proof it's hard to know whether the algorithm actually does what it should. There are countless examples of small subtleties in algorithms that make enormous differences for performance (e.g., take a look at union-find data structures in CLRS). An alternative could be running the algorithm and experimenting with it. This is often an excellent first step when you try to figure out if you actually have a good algorithm or not. Experimenting is especially useful when a rigorous analysis seems hard, or when you're still trying to figure out the exact formulation of the algorithm's task (e.g., think of the Netflix challenge, where you want to recommend movies to users given data about their history. It's not even clear what it is exactly that you want your algorithm to do). Ultimately, though, a clear understanding of what the algorithm guarantees is sought after, and for the most part, you need a proof for that. 

A common scenario is that one has a well-ordered universe, and one wishes to answer queries of the form "how many elements are at most x?". If one has d elements, then one can pick logd thresholds in a geometric sequence, save information on the sets smaller than the thresholds, and be able to approximately answer such queries. The question below deals with a similar scenario but in which one has a bunch of overlapping sets in the universe, and one has to pick thresholds in each. The issue is that when the sets are overlapping one might want to introduce non-trivial dependencies between the thresholds in different sets. I have a well-ordered universe U of size m, and n (overlapping) subsets $S_1,...,S_n\subseteq U$, each of size $|S_i|=d$. For a subset $S_j$ and a number $1\leq i\leq d$ let $S_j^{i}$ be the set of the i smallest elements in $S_j$. I'd like to pick as few $S_j^i$ as possible and ensure the following property: For any choice of $i_1,\ldots,i_n$ such that $S_{1}^{i_1}\cup...\cup S_n^{i_n}$ contains at least 10% of the elements in U, there are $i_1'\leq i_1,...,i_n'\leq i_n$, such that we picked $S_1^{i_1'},\ldots,S_n^{i_n'}$, and $S_{1}^{i_1'}\cup...\cup S_n^{i_n'}$ contains at least 5% of the elements in U. Clearly, one can take all $n\cdot d$ possible sets. Are $n\cdot \log d$ sets sufficient? 

There are plenty of uses of information theory in theoretical computer science: e.g., in proving lower bounds for locally decodeable codes (see Katz and Trevisan), in Raz's proof of the parallel repetition theorem, in communication complexity (see, for example, the thread of work on compression of communication, e.g., the relatively recent work of Barak, Braverman, Chen and Rao, and the references there), and so much more work. 

There is amortized complexity -- why some operations can be costly in the worst-case, but if you consider many operations, the average cost per operation is good. A classic example is a data structure that empties itself when it is full by copying all its elements to some storage. The copying operation can be expensive, but it doesn't happen often -- you have to insert enough elements to the data structure to provoke it. 

Is the graph directed or undirected? Can the path go through the same vertex more than once? The same edge? (Note that if the graph is undirected and the path can go through the same edge more than once, then the problem becomes very easy). For the directed case: If k is constant, or at most logarithmic, there is an efficient algorithm: $URL$ (This algorithm uses the observation that the problem can be solved in linear time using dynamic programming when the graph is directed and acyclic) For the simple path case: k=n is a special case of the Hamiltonian path problem 

Indeed! If P=NP, not only we can decide whether there exists a proof of length n for Goldbach's Conjecture (or any other mathematical statement), but we can also find it efficiently! Why? Because we can ask: is there a proof conditioned on the first bit being ..., then, is there a proof conditioned on the first two bits being ...., and so on... And how would you know n? You'll just try all possibilities, in increasing order. When we make a step in the i'th possibility we also try a step in each of the possibilities 1..(i-1). 

Assuming "hard" functions exist (for a variety of definitions of "hard"), we can construct pseudorandom generators. 

You can reduce the co-NP-Complete TAUTOLOGY problem (given a Boolean formula, is it a tautology?) to the problem of minimizing formula size (since a formula is a tautology iff it's equivalent to TRUE). Moreover, TAUTOLOGY for 3DNFs (analogously to SAT for 3CNFs) is co-NP-Complete. 

What's your favorite examples where information theory is used to prove a neat combinatorial statement in a simple way ? Some examples I can think of are related to lower bounds for locally decodable codes, e.g., in this paper: suppose that for a bunch of binary strings $x_1,...,x_m$ of length $n$ it holds that for every $i$, for $k_i$ different pairs {$j_1,j_2$}, $$e_i = x_{j_1} \oplus x_{j_2}.$$ Then m is at least exponential in n, where the exponent depends linearly on the average ratio of $k_i/m$. Another (related) example is some isoperimetric inequalities on the Boolean cube (feel free to elaborate on this in your answers). Do you have more nice examples? Preferably, short and easy to explain. 

Algorithms based on the regularity lemma are good examples for polynomial-time algorithms with terrible constants (either in the exponent or as leading coefficients). The regularity lemma of Szemeredi tells you that in any graph on $n$ vertices you can partition the vertices into sets where the edges between pairs of sets are "pseudo-random" (i.e., densities of sufficiently large subsets look like densities in a random graph). This is a structure that is very nice to work with, and as a consequence there are algorithms that use the partition. The catch is that the number of sets in the partition is an exponential tower in the parameter of pseudo-randomness (See here: $URL$ For some links to algorithms that rely on the regularity lemma, see, e.g.: $URL$ 

Szemeredi's Regularity Lemma says that every dense graph can be approximated as a union of $O(1)$ many bipartite expander graphs. More accurately, there's a partition of most vertices into $O(1)$ sets such that most pairs of sets form bipartite expanders (the number of sets in the partition and the expansion parameter depend on the approximation parameter): $URL$ There are versions of this lemma for "well-behaving" sparse graphs, see, e.g.: $URL$ $URL$ What surprises me about these formulations is that they only guarantee that most pairs of sets in the partition form bipartite expanders, and these bipartite expanders may be empty. So, in general sparse graphs, it's quite possible that all edges between different parts in the partition of the vertices don't belong to an expander. I wonder whether there are formulations that give that most edges between parts are from an expander, or whether there's no hope for such a formulation. 

There are lots of "cryptographic hardness" results (just Google this phrase) for learning problems. These are hardness results assuming that one way functions exist. 

More formally: the conjecture is that there exists a c, such that for all natural r, for all $\varepsilon \geq 2^{-cr}$, there is a PCP verifier that uses r randomness to make two queries to its proof, has perfect completeness and soundness error $\varepsilon$. The alphabet of the proof depends only on $1/\varepsilon$. For two queries, the best known error is $1/r^{\beta}$ for some specific $\beta>0$ (M-Raz, 2008). One can also achieve error $2^{-r^{\alpha}}$ for any $\alpha <1$, with a number of queries that depends on $\alpha$ (DFKRS). Lower bounds on c (i.e., approximation algorithms) are also sought after. See Irit Dinur's survey for more details. 

Ryan Williams just posted his lower bound on ACC, the class of problems that have constant depth circuits with unbounded fan-in and gates AND, OR, NOT and MOD_m for all possible m's. What's so special about MOD_m gates? 

There are examples from approximate counting. Approximately counting the number of satisfying assignments of an NP-relation can only be harder than deciding whether a satisfying assignment exists, so it's not too surprising that one doesn't need the PCP theorem to prove hardness for such problems. Still, the PCP theorem sometimes gives a convenient starting point, e.g., for this paper about approximately counting the number of independent sets in a sparse graph: $URL$ Later, Sly proved a hardness result for approximately counting independent sets just based on standard NP-hardness of Max-Cut: $URL$ 

In 2008 Irit Dinur and I taught a course on PCP at Weizmann, including both the algebraic and the combinatorial proofs. Hand-written lecture notes are available for most classes: $URL$ This semester I'm teaching a PCP course at MIT that contains the material of the old course, more comprehensive treatment of parallel repetition and the unique games conjecture, as well as recent results (from 2008-2009), like low error composition and optimality of Semidefinite Programming for constraint satisfaction problems assuming the Unique Games Conjecture. I also dedicate time to teaching error correcting codes, expanders, information theory and Fourier analysis. This is the course's website: $URL$ Notes are available here: $URL$ 

The Wikipedia entry that Peter linked to mentions a few important examples of problems that have worst-case to average case reductions, like the permanent. Shortest vector problem (as well as related lattice problems) is another important example, see Ajtai's paper and what came after it (works by Regev, Micciancio, Peikert,...). One of the only general observations we have regarding problems with worst-case to average-case reduction is the following (started with the work of Feigenbaum and Fortnow): (At least for non-adaptive reductions,) these problems are not likely to be complete to classes that are (probably) not closed under complement (e.g., they are not likely to be NP-complete). Other than that, you can find problems with worst-case-to-average-case reductions of various complexities, e.g., in $NP \cap coNP$ (lattice problems and other crypto problems) and in #P (permanent). 

Sequential repetition can give you any constant soundness error larger than 0, not just soundness error $\geq 1/2$. Dinur's approach gives you a constant soundness error which is not only at least half, but, in fact, extremely close to 1, maybe 0.99999. The note of Andrej Bogdanov that you linked to shows that getting a soundness error smaller than half inherently won't work using Dinur's approach. The reason is specific to this approach, and is explained well in the note. The soundness amplification results work for imperfect completeness as well. It's pretty straightforward to convince yourself of that in the case of sequential/parallel repetition. Dinur's approach can also be adapted to imperfect completeness. Remark: Dinur's approach, just like the other two approaches, requires a number of iterations/repetitions that depends on the soundness you start with and the soundness you want to get. In her case it's $\Theta(\log(\frac{1}{1-s}))$ iterations to get to constant soundness. Irit starts with $s\approx 1-\frac{1}{n}$, and that's why she needs $\Theta(\log n)$ iterations.