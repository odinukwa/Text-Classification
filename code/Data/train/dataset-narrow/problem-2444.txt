The wanted examples are never eliminated, whatever relations you use in place of partial orders $\leq$. For each particular unwnated example you look at, you can find a relation that eliminates it, but there is no single relation that eliminates all of them. 

Substitutions form a monoid and they act on terms. We have a choice of writing them as either left or right actions. Sometime in the previous millenium someone decided they act on the right (page 5). A right action should satisfy $a [s \circ t] = (a[s])[t]$, so it makes sense to define the operation $\circ$ that conforms to action on the right. That's all. You could do entirely the same thing with functions and write them as $x \mathbin{//} f$ (as Mathematica does) in which case you would be seriously tempted to define $x \mathbin{//} (f \circ g) = (x \mathbin{//} f) \mathbin{//} g$. In essence you'd move from the category of sets to its opposite. 

If I am not mistaken the simulations between Turing machines and $\lambda$-calculus can be accomplished with a polynomial-time slowdown. Of course, for this to make sense we need to specify an evaluation strategy and measure of cost for $\lambda$-calculus but I am sure something reasonable can be found. You ask about numbers in particular. Of course Church encodings are inefficient but they are only one possibility. In $\lambda$-calculus we can encode finite lists and the constants and . With these numbers can be encoded in binary as lists of boolean values, which gives an efficient representation. The charm of Church encodings is in their elegance. 

violates the covariance assumption, or monotonicity. It therefore does not fit the idea that inductive generation accumulates, generates, or generally constructs entities, since the constructor may generate less from more. (Again, if you want a more precise answer, I strongly suggest that you switch to actual mathematics, as then things will be clear.) The monotonicity requirement is by no means a philosophical or mathematical imperative. One may well drop it and allow arbitrary recursive types, which include the type above. Such types are very rich, have interesting mathematics, and are used in programming languages. They do not qualify as "inductive" because they do not fit the idea of inductive generation of entities. I will now downvote my own answer to indicate the fact that such talk is only welcome when supplemented by some coherent mathematical thoughts. 

The recursion theorem in computability states that, for any computable map $f : \mathbb{N} \to \mathbb{N}$ there exists $n \in \mathbb{N}$ such that $\varphi_{f(n)} = \varphi_n$, where $\varphi$ is a standard enumeration of partial computable functions. The one given here is due to Rogers, there is another by Kleene (but they can be derived from each other). I am collecting typical uses of recursion theorem. I can think of the following: 

I recommend that you look at realizability theory. In realizability computational models are known as partial combinatory algebras (PCA). They cover a wide range of computational models. There is a 2-category of PCAs in which we can speak not only of equivlence, but also about general morphisms from one computational model to another. Some references: 

You must be careful here. You are using set-theoretic concepts (cardinal, continuum) outside set theory. There is potential for confusion. Your question can be understood in several ways. Maybe you are asking whether there can be uncountably many terms of a given type. The answer is: obviously not since there are only countably many finite strings, and eact term is a string (or a finite tree if we think of abstract syntax). You might be asking this question because you claim that "the elements of types are terms". This, in my opinion, is a very damaging view of types. It is like saying that the elements of $\mathbb{R}$ are only certain expressions which denote real numbers. Another possibility is that you are asking whether there is a model of type theory in which some of the types are interpreted as uncountable sets. The answer is yes, for example the set-theoretic type model in which types are sets. In this case $\mathtt{nat} \to \mathtt{bool}$ has the power of continuum because it is the sets of all infinite boolean sequences. You could be asking whether inside type theory we can prove that there are types of the cardinality of continuum. In this case the question does not make sense because the notion "cardinality of continuum" is something that only make sense in set theory. You need to rephrase it so that it makes sense in type theory, but there are complications. Cardinality just does not behave the same way in type theory as it does in set theory. For example, you cannot show that cardinals (whatever you think they are) are linearly ordered. But we can still define special cases. Thus we can define the notion of an uncountable type: 

Here is a philosophical answer that may entertain you. Gödel's incompleteness theorems are about the formal system of Peano arithmetic. As such they say nothing about models of computation, at least not without some amount of interpretation. Peano arithmetic easily shows existence of non-computable functions. For example, being a classical theory expressive enough to talk about Turing machines, it shows the particular instance of excluded middle which says that every Turing machine halts or runs forever. Nevertheless, from the work of Gödel an important notion of computability arose, namely that of a (primitive) recursive function. So it is not the theorems themselves that connect to computability, but rather the method of proof which establishes them. The gist of the incompleteness theorems can be expressed in an abstract form using provability logic, which is a kind of modal logic. This gives the incompleteness theorems a wide range of applicability well beyond Peano arithmetic and computability. As soon as certain fixed-point principles are satisfied, incompleteness kicks in. These fixed-point principles are satisfied by traditional computability theory, which therefore falls victim to incompleteness, by which I mean existence of inseparable c.e. sets. Because the provable and refutable sentences of Peano arithmetic form inseparable c.e. sets, the traditional Gödel's incompleteness theorems can be seen as a corollary to incompleteness phenomena in computability. (I am being philosophically vague and your head will hurt if you try to understand me as a mathematician.) I suppose we can take two stands on how all this relates to the informal notion of effectivity ("stuff that can actually be computed"): 

Let's call your answers "yes" (the given program halts), "no" (the given program does not halt) and "maybe". If I understand you correctly, you are redefining the Haltin problem as follows. 

As Neel points out if you work under the "propositions are types" then you can easily come up with a type whose equality cannot be shown decidable (but it is of course consistent to assume that all types have decidable equality), such as $\mathbb{N} \to \mathbb{N}$. If we understand "proposition" as a more restricted kind of type, then the answer depends on what precisely we mean. If you are working in the calculus of constructions with a kind then you still cannot show that decidable propositions have decidable equality. This is so because it is consistent in the calculus of constructions to equate with a proof-relevant type universe, so for all you know might contains something like $\mathbb{N} \to \mathbb{N}$. This also implies you cannot prove your theorem for Coq's notion of . But in any case, the best answer comes from homotopy type theory. There a proposition is a type $P$ which satisfies $$\forall x, y : P \,.\, x = y.$$ That is, a proposition has at most one element (as it should if it is to be understood as a proof-irrelevant truth value). In this case the answer is of course positive because the definition of proposition immediately implies that its equality is decidable. I am curious to know what you mean by "proposition". 

Rules in a grammar and inference rules in logic can both be thought of as production rules which gives us "new stuff" from "known stuff". Just as there may be many ways to produce (or parse) a word with respect to a grammar, so may there be many ways to produce (or prove) a logical formula. This analogy can be drawn further. For example, certain logical systems admit normal forms of proofs. Likewise, certain grammars admit canonical parse trees. So I'd say your examples from logic are going in the wrong direction. The correct analogy is 

The function is universal because it accepts (the description of) a program and an input tape , and tells you the result of running on . While this answer is not entirely serious, it does show that a compiler or an interpreter for a Haskell-like language already contains all the building parts needed for a universal function. The moral of the story is that time is better spent studying how compilers and interpreters work than to worry about implementing a universal function in terms of Turing machines. 

Monotonicity or covariance, which says inductively generated entities behave monotonically with respect to the building block, or transforms covariantly with respect to the building blocks. For example, if we build a new object $C(a,b)$ using a construction $C$ from parts $a$ and $b$, then 

Take the "current mathematical system" $S$ and an undecidable sentence $P$ for $S$. Consider the Turing machine $T$ which enumerates all theorems of $S$ and halts if it ever enumerates either $P$ or $\lnot P$. System $S$ cannot decide whether $T$ halts. Note that such a $T$ does not halt. 

A function which takes another function and returns its memoized form. Typically creates a local reference in which it stores the cache. A self-optimizing data structure such as a self-balancing tree uses local references in the background, so that it can modify itself. Such structures are often referentially transparent, i.e., the optimizaiton has no observable effect other than better efficiency. It is unrealistic to expect that nobody will ever implement any stateful data structures. These need to use state, and they must be allowed to escape the scope in which they were created, or else the structure of the entire program becomes a hostage to correct scoping of handlers. 

At the risk of starting a religious war, I will express my opinion that there are no advantages to having nullable types. These should always be replaced by a sum type such as Ocaml or Haskell (paired with sane deconcstructors for such types that force the programmer to always consider both possibilities). The main reason for this is that null pointers, null objects, and the like are responsible for many, many bugs and countless headaches. 

$f$ is already determined by its restriction $f{\restriction}_{D_r}$ to $D_r$ (because $f = f{\restriction}_{D_r} \circ r$), the image of $f$ is contained in $D_q$ (because $f = q \circ f$). 

To answer your question we need some machinery and concepts from the theory of programming languages, so that we can actually make your question well posed, and then answer it. You are asking whether two pieces of code are observationally equivalent. It is usually quite hard to determine observational equivalence with direct methods (and the sort of arguments you attempted will not work). A common one is to use adequate denotational semantics. Adequacy means: "If $p$ and $q$ have the same denotation then they are observationally equivalent". In your case, the traditional domain-theoretic denotational semantics, for instance using $\omega$-cpos, will do the job. In the denotational semantics the difference between the two evaluation orders is manifested by the fact that in the applicative order (also known as call-by-value) every function is strict (it maps the bottom element to the bottom element), while under the normal order evaluation (also known as call-by-name) one may define non-strict function. First method A simple calculation shows that for the call-by-name semantics the denotation of is a map $f : \{0,1,\bot\} \times D \times D \to D$, where $D$ is a domain, defined by $$f(b,x,y) = \begin{cases} \bot & \text{if $b = \bot$}\\ x & \text{if $b = 0$}\\ y & \text{if $b = 1$} \end{cases} $$ while the denotation of is exactly the same. Because denotational semantics is adequate, we may conclude that and are observationally equivalent. It is instructive to see what the denotation is for the strict semantics (applicative order). The semantics of remains the same, while the semantics of is a map $g : \{0,1,\bot\} \times D \times D \to D$, where $D$ is a domain, defined by $$g(b,x,y) = \begin{cases} \bot & \text{if $b = \bot$ or $x = \bot$ or $y = \bot$}\\ x & \text{if $b = 0$}\\ y & \text{if $b = 1$} \end{cases} $$ This is not sufficient to conclude that and are observationally inequivalent (because the denotational semantics is not fully abstract), but you already gave a specific example which shows that they are different under applicative order. Second method The first method is an application of the general rule that denotationally equal programs are observationally equal. It requires us to always deal with domain theory. Another method is to derive useful general laws that we can then use to argue about observational equivalence. In your case we need the law of $\beta$-equivalence, which says that is observationally equal to with substituted for . This law does not hold under applicative order, but it does for normal order. So we may calculate: