Note that means command has to start in user login shell loading user environment too. In case that login shell is something like for security reasons, there should be a problem. Try to change to and potentially do fine settings in for that command. 

I got many DMA errors on drives, when air condition issue in datacenter and ZFS was able to fix that mess. And it was just simple mirror. I do remember promo video issued by SUN when they introduced ZFS... they made RAIDZ on USB flash drives deployed to 8 port USB hub and then randomly changed position in hub for few of them while doing IO on that pool observing no outage. 

Every chunk of data has fair checksum on ZFS. So ZFS know which drive holds correct data in redundant setup when failure. Running will repair data or spread data to all running drives for RADZ. ZFS employs Reed-Solomon's error correction which is best for bursts of errors. Missing drive is such burst of errors, which R-S can correct. 

The most likely you forgot to enable forwarding. Add to , then or restart. Also try to add following to OpenVPN config: 

First one is pure software and second one is kernel accelerated provider accesible as PKCS11 token. Exactly those two on my old T1 Niagara are doing 8.4 sign/s versus 19740.0 sign/s. That's for sure huge difference. Modern x86 CPUs can accelerate AES for example and as far as I know it is used in software kernel provider. Check yourself what's the difference. More important is to have speedy asymmetric ciphers, because they are used during establishing a connection and are more CPU hungry... web applications close connection often. Btw KSSL is in fact just in kernel SSL encrypting proxy... a fact it happens in kernel contribute to speed too. Just to compare... on another machine, ~ same age as T1 noted above, but x86 in VMware is doing for me 42.1 signs/s versus 98.6 signs/s for rsa2048. So more than doubled speed. 

Zabbix, when compiled with CURL support, can directly monitor web services including complex more steps scenarios. You can setup triggers on HTTP return code, returned data, response time... Documentation here. 

It will inherit netmask from global zone interface. Of course you can setup more interfaces, or put zone only on 'internal' interface (no public IP) and the let provide . 

Use , but also add per every network you want to route through VPN. Btw note that DNS setting on other interfaces will stop work, when that interface will not have route to its DNS servers. This is what happens when drops default gateway from your (W)LAN interface and adds host route to VPN server IP through original GW. Depends on your setup, may be there is no working setup and you'll have to change DNS naming to include some subdomain for internal networks. 

In this case you can play with and in SunOS . But best to block those attempts even before reaching with Solaris . Here you can find pretty often updated list of IPs to block: OpenBL I see attempts very rarely in logs, just using this blacklist. Then you can assemble cron job script to update FW rules or optionally there's formatted file available. 

Zone virtual interface has some features limited... some states can't be setup, packet filter doesn't work in zone too. If I remember right, zone interface can't send ethernet broadcasts, so then no DHCP. Btw why you doing that bloat about setting up zone interface? What about this? 

May be your old mirror set was a hardware one, not the ZFS. Depends on your HW. Check partitions table with if something is there. ZFS can reconstruct its pools no matter what order are disks placed in. 

Is there any way to get network configuration of iLOM in Solaris SPARC? I want to get the IP address of iLOM console at least. It's pretty easy on x86 by , but for SPARC I can't find something that do the same. I found some mentions about and , but those are not available for systems with iLOM. 

If you have speedy lines between sites you want to mirror, I can imagine something like you export iSCSI volumes from sites storages and put them mirror and add some local disks for ARC, ZIL, cache to lower read/write peaks running over iSCSI. If your storage is mainly for backups, then it would be OK. Nevertheless SUN once had such product behaving like that on ZFS. 

I'm running small Ubuntu devel virtual on VMware Fusion. 4GB disk and 512MB RAM is enough for development. Network of VM is in NAT mode, so I can access it even when not on Internet. I also configured AFPd so I can edit files directly mounting share. As far as I'm doing Django only that way setup is as following... Django app running under some user account I created, that user homedir is also a root (loaded on login), I use that user to login to AFP. When new project I just clone template machine and create new user account + . Installing to VMware Ubuntu chooses kernel that holds main virtualisation abilities... thus XEN, KVM, VMware. Deploymnet should then be DevOps way... just copying VM files to cloud and starting it online (maybe conversion of disk file or its growing to production size). 

option - when you want to stay with FreeBSD, check FreeNAS to automate complexity you are afraid of. option - NexentaStor, it is Solaris based storage appliance SW with great management web gui. Up to 18TB setup is for free. Again there you can easily manage complex vs. a lot of datasets configuration. 

For this case is better choice. It is Solais derivate, that runs completely from USB stick and local disks zpool just store your data, nothing from OS. Upgrades are way you just copy new image to USB stick. is primary a hypervisor, but all other features are there too + relatively fresh prebuilt packages repo available. >SmartOS< Btw when you want full featured storage appliance and no problem for you to install appliance SW on disks, then is definitely your choice. Comparable with FreeNAS... but much more features. You can even make fiber channel storage device from your old PC... just click out on web GUI. >NexentaStor< 

So you have to calculate values for those param to mimic behaviour you get by setting 4 params to Solaris net stack. Btw check in Linux. 

It isn't uncommon to find the device in each location that is making the VPN connection is also capable of acting as a DHCP server - e.g. Cisco ASA, Linux server, pfsense, m0n0wall. Depending on the capacity of the device and number of clients at the site, this might be worth taking advantage of. This is particularly useful where there may be site specific configuration you want to push out via DHCP. For example, you might want to include a public DNS server in the list of DNS servers assigned, so in case the VPN fails, but the Internet is up, people at the remote site can access public resources even if your central DNS is unreachable. It is also a lot easier to visualise/explain this way. 

If your dedicated host is assigning IP addresses like this, then they should be very familiar with providing support to their customers on how to get it working, so really - ask them, as you may need to use particular IP addresses and gateways on different physical NICs and only they will be able to tell you which these are. You need to understand what subnet each IP is on, and then a NIC or vNIC that has that IP assigned will need to have a gateway in the same subnet in order to be able to communicate with it. The config you posted set a gateway in a different subnet to the IP address assigned to the NIC. 

If you can save a file into the folder through Samba then you can examine the properties of the file to see which user and group own it - that should be your answer. 

If you want to use ssh to do any kind of automated procedure - I'm thinking specifically of Nagios checks - then you probably wouldn't want to use a passphrase. In this situation you probably wouldn't be using this outside of a LAN, and you would have the key stored securely on the server doing the procedure. Most tutorials discussing SSH will be anticipating a person logging in from an external network, possibly from an unsecure computer, in which case the advice is sound. Basically, unless you know there is a good reason not to, create a passphrase. Being too lazy to type in your password every time might be a good enough reason for you :-) 

I'd take the Google Analytics alert as an indicator you should take a look at your logs. Something like webalizer or awstats will give you the detail you want. Analytics is good, but it can't tell you everything your log files can. 

Take Rapidswitch's advice and check the iptables (or whatever firewall you use) configuration. It's easy to configure the running configuration without saving it as the startup configuration. Default configuration won't necessarily have port 80 open. iptables -vNL will show you what the current config is. If port 80 is being allowed (and on the correct IP address etc) then you have something to go back to Rapidswitch with. 

I can't say I have done this, but I am sure if you follow the docs for generating a CSR from a windows box, then follow your CA docs for generating a .p7k cert from a CSR, then you should be fine. By the way - I would recommend you create your CA as a virtual machine for a popular hypervisor such as Hyper-V or VMware, rather than a boot disk, make sure you store it very securely somewhere your successor can find it, and spin it up offline periodically to make sure it runs, or transfer it to new media/technology. A root CA may have a life of 10 or 20 years... 

How many staff with dynamic IPs? If a small number, you could just pay for them to have static IPs and you keep your security policy intact and config simple. I'm sure you already considered that. If you are willing to loosen some policies, you could in theory find out your staff ISP subnets and limit access to those, which may reduce exposure significantly, although I think this would be an unusual approach. If you are serious about securing SSH you should be looking at key logins only, with some requirements that the keys are password protected, and possibly even expiring them on a scheduled basis. Don't give your SSH users root access. Use sudo to give access to root type commands. Use logwatch or similar to keep an eye on what is going on. Also, this is a web server - your default config of SSH is probably far more secure than other aspects of the system you are deliberately exposing to the internet, even if a compromise would be more serious. Don't forget about securing the rest of the server and the code you run on it. An excellent guide to securing a Linux server can be found here. Specifics are CentOS/RedHat, but it goes over a lot of options common to all distros. 

Microsoft provide their own Key Management Server for automatic product activation of Windows. Is this what you are asking about? 

Simon Catlin is right. It's a drag though. If you can get away with just forwarding the messages to a group, that is easier than trying to set up notifications. Unfortunately for my question, I can't so we are paying someone to do the development for us... 

The snapshot names are related to the virtual disk file they are based on, which need not be related to the name of the virtual machine, and quite unlikely to have anything to do with any labels given to the snapshot. The numbers after the disk file name are primarily a unique ID to ensure that the filename is unique. From my observation this does often indicate the order of the snapshot file, where snapshots have not been used frequently for that VM. However, there is no guarantee of this, and where lots of snapshots have been created then deleted, you may see something different. VMware have a good document discussing snapshots which covers the naming convention. Note there are additional files relating to snapshots, also detailed in the document linked above. 

You are looking for the functionality of a Mail Transfer Agent. An SMTP server provides only some of the functionality - transferring messages from one server to another. Exchange is the obvious choice, but there are a number of free ones too. I am sure it is possible to construct an MTA of sorts with what Windows provides, but why reinvent the wheel?