You can't embed DDL in a stored procedure like this. Either use and your DDL statement passed to it as a string, or, better, remove the stored procedure entirely an execute the DDL directly. 

So the second and any subsequent duplicates will never be considered. So duplicate entires can not be the cause of your error. 

Note that for some cases, an statement can be turned into a expression which is somewhat similar to the operator in C and Java, but more generic. Your pseudo-code: 

sqlplusw was a GUI version of the command line tool sqlplus, with its own Edit menu etc. sqlplusw was removed in Oracle 11g. 

The timestamp also doesn't include timezone information, yet a lot of people use it. Of course, that of itself is not an answer. It's valid to use and for any timestamps and dates which are always in the same timezone, or are being stored relative to some known timezone. If the timezone is always the same or implicit then it's more of antipattern to store it than to not store it (redundant information). Timestamps can also be used for storing technical information such as used in application logs or for performance measurements. In this case, the timezone can also be unimportant. 

It seems that Oracle doesn't like ANSI-style joins in the materialized view definition when refereshing... Changing the definition to 

So doesn't require . Having is applied after the aggregation phase and must be used if you want to filter aggregate results. So the reverse isn't true, and the following won't work: 

This is almost certainly not what you want in an OLTP application. Or in a data warehouse type application it implies that is a dimension, and is a fact table. However you would normally see a hierarchy in a dimension, which you don't have. So then there's no advantage in not using the built-in datatype instead of inventing a surrogate. 

If you really need to do this (a la Windows Registry values) I would probably try to keep it simple and keep it to one table something like this (untested pseudo-DDL): 

Your design doesn't record "which user submitted which answers". Should there not be a relationship between and ? You have the attribute but you haven't indicated that it is a foreign key to . Should there not be a link between and ? How else do you know which question is being answered and what the value of the answer is? seems like you left something incomplete. 

Note that the primary keys of some of these tables have been modified. Here is the SQL DDL for the solution: (You need to scroll it to the bottom to see the magic.) 

Now all of your remaining relations (R0, R1a, R1b, R2a, R2b) are devoid of repeating groups, partial dependencies and transitive dependencies. That means your relations are in 3NF. When you are looking at an relation that hasn't been normalized and a series of dependencies, you can often normalize by inspection just by recognizing what your primary keys are going to be. Any attribute or combination of attributes that functionally determine other attributes are going to end up as primary keys. Once you've got your primary keys defined, you just need to figure out which non-key attributes go with each key. This is obvious from the statement of what your functional dependencies are. 

SQL employs a concept known as domain name integrity which means that the names of objects have a scope given by their container. Column names have to be unique, but only within the context of the table that contains the columns. Table names have to be unique, but only within the context of the schema that contains the tables, etc. When you query columns you need to reference the schema, table and column that you are interested in, unless one or more of these can be inferred. When you write a query, you need to reference the tables in your query by name directly or by using an alias, e.g. Customer.ID or C.ID from Customer C, etc., unless your query is so simple that it only references one table. There was a time when there was a technical requirement for uniqueness of all column names, which applied to old ISAM databases and to languages like COBOL in the 1960s and 70s. This got dragged along for no good reason into dBase in the 1980s and has stuck as a convention well into the relational and object DBMS eras. Resist this outdated convention. When the shift from flat file and network databases to relational databases happened in the 1970s and 80s, the idea of joining tables was new. So some people chose the convention that a unique name could be repeated if one column was a reference to another (i.e. foreign keys). This concept is called a "natural join" and a lot of people still advocate for doing this. I am not a fan of natural joins because it requires you, ultimately, to throw out the concept of domain name integrity and force the whole column reference into the column name. The issue with natural joins is that you either have to be hypocritical or you have to make your column names long and unreadable. Let me illustrate: It may sound like a good idea that the primary key of the Customer table is CustomerID. Then in your Invoice table, your foreign key to Customer is also called CustomerID. This would be a natural join and it all sounds good so far. Here is the problem. What if your convention is to have a column on every table called LastUpdatedDate? So are you meant to join every table to every other table by LastUpdatedDate? Of course not. This is the absurdity of natural joins. In order to avoid this absurdity you would need to jam the table name into the column name as a prefix. However, if you have multiple schemas in your database, you can't stop there. You also need to add the schema to the column name, and so it goes. Another place where natural joins break down is when you have multiple relationships between the same two tables. You if you need two references to Employee on your Invoice table (Sold By and Approved By, for example) you can't call them both EmployeeID. 

My preference, in certain cases, is actually for the constraints, but I don't use them everywhere: just for the situation where it makes sense to be able to remove all of the children of a given parent in one go. I might use it for "invoices" and "invoice_lines". When you take this approach you need to be sure that only users who really need to be able to delete from the parent table have that privilege -- no users or applications logging in as table owners! 

All I've done is move all the code into a stored procedure, and redefined the trigger to CALL that procedure. 

According to the documentation of the REAL datatype $URL$ it has a precision of 6 decimal digits. That's what you are seeing. If you need more precision you need to use double or a number with fixed precision. Maybe casting the reals to something more precise and summing the result will give you what you need, eg: 

is Oracle's way of defining a "full text index" on a column -- as opposed to an ordinary B-tree. The good news is that Postgres also has full text indexes, and these are documented extensively at $URL$ The basic syntax for creating a similar index in Postgres is 

Be careful. Your two designs are not the same. Case 1 implements a unique constraint on the name of the book but includes case, so "The Lord of the Flies" and "the lord of the flies" would be different. Case 1 then creates a second index to support efficient searching of queries of the form 

quoted from $URL$ , Oracle allows you to use versions of Oracle DBMS downloaded from OTN for development "for free". 

Datapump is much more efficient than : Datapump runs within the Oracle server processes, and can read directly from the database files and write directly to a file on the server. As I understand it, data access is direct, not via SQL. uses the ordinary Oracle Call Interface (OCI) -- the library that Oracle provides to connect to the database. It can only run ordinary SQL statements like any other program. 

What book or webpage can I read the architecture and methodology for creating ETL process? In other words, I'm looking for "how to do it" to create a ETL process when you have many source system to be involved and you also have a data warehouse. 

Goal: Improve my sql ability by doing some coding and reviewing existing code. Problem: I have spent lots of hour to find a solution based of this question "3c. Some countries have populations more than three times that of any of their neighbours (in the same region). Give the countries and regions. " I tried finding different solution but unfortunately I can't solve this problem :( The link can be found "$URL$ Again, the purpose is to improve my ability in SQL! // Fullmetalboy 

Problem: Where can I get material to improve my ability in SQL coding? Is there any website for downloading a database with different task/assignment and solution? Where should I go to review lots of SQL code? 

If you work with Reporting service in order to create a report and you need to retrieve data from the relational database. The datamodel of the relational database can be structured in a certain way that can be very difficult to retrieve right data by using join and lots of SQL. In order to save time and cost, it is recommended to use analysis service to retrieve right data that takes less time and cost and the result of the analysis service can be used in reporting service? In other words, is it recommended to retrieve data in SSAS that will sent to SSRS instead of using SSRS to do lots of SQL coding in order to display right data in the report? // Fullmetalboy 

I have read that you also can apply N-Tier for Businesss Intelligence based on these criteria: *Presentation *Functional logic *Data What is your experience when you are using N-Tier? // Fullmetalboy 

Which websites, books etc. can you recommend me to improve my basic knowledge in Microsoft Business Intelligence 2008, SSAS, SSIS and SSRS? Please remember that I'm a newbie in technical BI tool. If possible, it would be great to retrieve any material for task work that would be similiar as doing lab task at university. // Fullmetalboy 

Oracle's wrap program doesn't wrap triggers. The way to do this is to move the code into a package or a stand-alone procedure, and make the trigger a one line call to invoke this code. See the documentation on the limitations of wrapping: $URL$ In your case, something like the following (untested) should work: 

Yes, 'W' will do that -- replace the file. Use 'A' to append. I don't think you can "modify in place"; you might have to copy the file to a new one making the changes you desire, delete the original and then rename the copy. All of this begs the question why you're trying to do all of this in PL/SQL. It might be better -- certainly easier -- to do it in an external program written in eg Python and invoke that. 

You can do this under Linux/Unix when you embed the SQL commands in a shell script and pipe them into a SQL*Plus instance. 

These variables are a feature of SQL*Plus. , the equivalent program in the PostgreSQL world, also has variables. Use 

But as others have mentioned, optimising expressions isn't going to help you; the Postgresql syntax is merely cleaner. You'll need to show us your execution plan(s) and table definitions for us to be able to help you further. 

You'll need to use a instead. If your data is indeed distinct, use a for better performance -- the step to eliminate duplicates is eliminated. 

I presume you mean that you get an error with this query, and that if you remove the where clause you get no error, but not the right answer. You need a clause, which is like but can be applied to the aggregated results: 

Compare this to Option #2 that Rolando posted and you'll notice I removed one column each from the SELECT and GROUP BY clauses. 

If using Oracle -- and if you're able to run arbitrary statements against it -- then you could first sanitize the input using (at least, it's my understanding that this makes sure that the literal is properly quoted -- even allowing for quotes embedded in strings). See $URL$ for more details. 

It depends on what your philosophy is around column naming. If you go for very descriptive long column names, then is perfectly fine, since that is exactly what your column contains. If you are comfortable with shorter names that may seem a little cryptic, but which can be figured out pretty easily, then you could use or . For English speakers, is pretty unambiguous as there aren't a lot of words that this could reasonably represent. I would pick that over for that reason. 

In the above ERD, notice how the extra information that is not author-dependent remains at the level, while the new intersecting entity is added to record the connections between authors and a particular instance of the submission of a paper. This four table model is more robust because it allows for papers to be resubmitted multiple times with changes in authorship each time. If this kind of real world nuance is important to your system then you would be better off with the four table approach than with a three table approach. Here is the four table model with more column details at OP's request: 

Note that your individual assets need to be normalized out into their own table. This lets you separate the two semantically independent pieces of information: 

The most common way to do it would be to use positive amounts for debits and negative amounts for credits (or the other way around, depending on whether your amounts are generally considered to be assets or liabilities). This is easier than either of your methods when it comes to aggregating a total since you only have to sum over one column. 

There are two ways of approaching the answer to your question: First: Is pre-optimization a good idea? As a general rule, don't pre-optimize on the assumption that you will have a problem. Use volume testing to determine if you have a problem and denormalize for optimization purposes if that is the best of your available solutions/compromises. Second: Is this a good case for denormalization? Having said that, there is a practical limit to how much you want to be fussy about functional dependencies. Are your type A and type B messages really that different? They both seem to quack like a duck, as it were. Having only a single attribute different, and that difference being whether it is null for one set of records and not null for another set of records isn't necessarily a good reason to implement two distinct message tables. You might want to have a logical model that makes the distinction between type A and type B messages, but it doesn't necessarily follow that your physical model has to implement these two entity-types as separate tables. You have the option of using a constraint to enforce the relationship between message type and section number. You don't have to implement your constraint through normalization. 

Then session 1 will be waiting for session 2 to commit or rollback the update on the row where pk = 2 while at the same time session 2 will be waiting for session 1 to commit or rollback the update on the row where pk = 1. Deadlock. 

Oracle will only scan the partition for the year 2013. On the other hand, if you are altering the table to add or remove partitions, you will more often than not need to name the partitions. This isn't always true: for example, Oracle 11g has interval partitioning where Oracle can automatically create new partitions for new data as needed -- perfect for the example I just mentioned. 

depending on your data. You'll need to check your data and keys to determine which column should go first. The last column of the index is the column for which the maximum value is being computed. By including it in the index you will avoid table access. And by including the keyword it means that Oracle can use the index to calculate the maximum value -- it will be the first entry in the index for which the first two columns match. In the ideal case, you will have many repeated values in the leading column of your index. In this case you might get some measurable improvement by using index key compression. In this case your create index statement will look something like 

Store the first. When you do this you can enable index key compression. See $URL$ for the syntax and $URL$ for the concepts. In your case, you can do it like this: 

Extend or adjust for data types as needed. Remove for each data type if you wish to allow null values. The only complexity is the constraint to enforce that the appropriate column is used. If values are mandatory, then the check constraint can be written as 

Once we have these two values we can determine which of the two is closer. So, something like, untested: