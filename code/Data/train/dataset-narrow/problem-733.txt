Dropping multiple KEYS in a table is a preferred option or one by one is preferred? â€œDROP KEY A,DROP KEY B,DROP KEY C" For more 900 million records in the table. How does it work internally? 

To get exact data, you may try enabling log (legacy as general-log). And rotate the log file every one hr. Develop a shell script to get the lines matching for your query and subtract it with 7200 returning value stays as your remaining queries. (NOTE: Ensure log file size shouldn't reduce server cpu much). 

I have 5 Node in Galera cluster. MariaDB Server 10.1.21-MariaDB-1~trusty. I will remove 2 nodes from it but one of them will be a slave. So from 5 nodes 5th one will be completely removed, 4th one will be a slave of 3rd one. I don't want to rebuild a backup from 3rd one. I just wanted to make 4th out of cluster and become slave of 3rd one. May I know the procedure for it? Also why is gtid_current_pos is changing for one node and not on other nodes? Let me know if you need any clarifications. 

For this you need to be so sure about your application architecture, better to double check with application team. Probably you may check their code also. 3. Can I make above (2) option happen ? Yes, you can overcome this, provided you should be meeting below points. 

I guess you wanted to ensure if my.cnf is rightly set without any wrong or bad settings. Yep makes sense. And Rick has added points to help you in configuring my.cnf as per the server, which are good practices to follow. But I see this problem as. Hey I have a problem and I need a solution !!! I guess you might have found the solution if the actual problem is nailed. 

Hash partition to let Mysql to decide the category. But here you can try to have an integer column for Hash. YEAR(date) = 4Max digits tag = 4Max digits country = 2Max digits (Convert this into a country code) Add a new column called part_token and set a datatype to Integer Unsigned. It can range from 0 to 4294967295. Any way max prediction to keep the value is 4294th yr and we are at 2017 :) 

Below is the code snippet and logic I have followed for your question (Looks like a simple question, but not for me). Code Snippet: 

Why do you need all different users to be connected to the mysql connection pool? It is highly impossible to make a note of users permissions and its passwords. If you need those many users for application purpose, you may try doing it in regular innodb tables and have list of users under one group and tag them to a role. Just grant multiple roles to the users who needs the access. 

Here is my opinion. As long as you are going to use innodb engine it doesn't matter which one are you in. Provided Maria gives you the option of multiple writes to master-master architecture using Galera-cache. But as your TPS goes beyond a particular point, you might face issues in getting writes paused due to flow-control. Hence you may use innodb engine let there be 2Nodes using MariaDB using GaleraCache Cluster or simple Master&Slave. But have your applications writing to one node only either case. For my.cnf you may use percona tool wizard to generate as per your requirement. 

You may try reading the binlog events in 'mysql-binlog.000018' and see if there is any data corruption or the file didn't close properly. Anyway the better suggestion would be to rebuild the slave. But you might want to know what does the binlog events contain, for nailing the actual root cause and provide fix. I suspect probably there could be network glitch between Master and Slave Or Slave server would have failed its monitor health for a fraction of a second Or disk would have gone for failure and replaced if under EBS. Since you are in AWS and it is prone for such disasters. 

But the link also recommends XtraBackup from Percona, which is faster, no locks on tables that are already in use and gains no time for restoration. For 500GB of data I prefer Percona XtraBackup to be quick and efficient unless you want to convert them to innodb_file_per_table model if it wasn't from the existing DB server. Below link explains on how to proceed. $URL$ 

Any best recommendation variables for my.cnf on percona server on a high 256GB RAM and 32 CPU with 4000 IOPS? How about below ones? 

It depends upon what backup method you maintain. How are you maintaining backups? Is it a full mysqldump or using binary logging? In your case I'd prefer opting for binary logging and you may take the the sqls triggered into binary log to construct your DR to the last point of your DB crash even if its too critical and no time eating process to build DR, you can go for Master/Slave still you might require binary logs for it. 

Well if your search criteria always be stable for 8 or less characters i.e. starting from char 1 to 8 from long text field meta_value, then you may add a new column saying meta_key varchar(12) and add index to it. Remove index on meta_value long text. Now whenever you insert a row add first eight characters to meta_key. In this case your selects on varchar with 8 characters would be faster. 

As my project demands multiple processes using load data for a table. And I have tested the above options and finalized to go with option 2. Kindly correct me if my experiments are wrong. 

DB reboot is all of a sudden but it was running good until last (XDATE) or you may see the stats in Mysql graph if you have and see when this reboot started. Point that date and see if any deployment is being pushed on that date. And try to see if it can be rolled back. If it can be goahead and then keep monitoring. See what causes reboot? Is it due to maximum connections? Or a particular query was running? If its hard to nail the exact reason. Try to separate the readers and writers. Writes to Master and Reads to Slave if application permits. I assume its a framework. Not sure if its an easy option for you. But worth giving a try. But once (3) is done you will have clarity on Master Lets say the reboot still persist on Master. See on which hour you get reboot. If the queries are same and concurrency is the bottleneck. Then try read and see if the combination of these variables are set rightly. 

Issue in 1st Option: I'm afraid to issue this, as this doesn't drop the partition and recreate it. It goes for delete from partition where datetime_col < '2013-11-12 00:00:00'. Not sure how much CPU or undo buffers this will create in a 500G partition table. Issue in 2nd Option: No issues, but it just won't work. Anyother recommendations are most welcome!! 

Is Mysql not an ACID compliant according to Postgresql? In some blogs I see Mysql is not ACID compliant. How true is that? Let't not consider the replication here, lets consider a standalone and how efficient is Mysql ACID? In my understanding for Mysql-ACID. 

Many experts share many ways on how to overcome this problem. These are my suggestions to play a safe game. Try to set the below command in a seperate session. 

Looks like you are at the beginning of your investigation. Probably you should dive deeper in below areas to nail the issue. 

Lets say reboot is on Slave. Which I suspect a lot. Then start tuning queries to pick indexes or create indexes if required. Any changes in schema should be performed on Master so the schema can be consistent. When you are sure you have tuned queries on slave. You may put back to Master in smaller query sets week by week. And remove slave if you don't want to. 

From here /backup/2017-06-01_18-01-52/2017-06-01_18-22-29 Final backup set is merged to parent(FULL BACKUP) and created a sub directory out of it. 

I also have 2 more slave nodes. They are performing real good. And they are used heavily by selects on app end. Any thoughts or any one have run into similar issue before? New Update Something found in network. 

Recently I have seen a table that gets inserts only about 60 per second. It predominantly locks for a while waiting for auto-inc to get released. And thus subsequent inserts stays in the processlist tray. After a while the table gets IN_USE of about 30 threads and later I don't know what it makes to free them and clears the tray. (At this duration the load goes more than 15 for 5 minutes) Suppose if you say application functionality should be shapped to best suite the DB server to react. There are 3 to 5 functionalities each are independent entities in schema wise. Whenever I see the locks it gets affected to all other schemas too. Now what makes best fuzzy is the last one. I see slave keeps in synch with master with a delay of 0 second all time whereas slave has a single thread SQL operation that is passed from relay IO that which acts in FIFO model from the binary logs where Master had generated. When this single headed slave can keep the load less and have the operations upto-date, should the concurrent hits are really made to be concurrent for the functionalities which I assume making the possible IO locks in OS level. Can this be organized in application itself and keep the concurrent tenure density thinner? 

Whenever you use AND clause and picking exact string/integer type with equals = . Its effective to use composite index. Try having . See if the query picks index, as you have two different columns in their own ranges I'm not sure if it could help. It mostly depends upon how the app-db architecture is defined. If I were to be you. 

After a day you may grep for values more than 90 in sumOfUsers column to see which line and then you may get timestamp above it. Thus you may measure time range and do manual monitoring around that time to see how often it takes the connections almost 100. Also you may add Host in query to see if any particular application host is doing this pile up of connections. 

If you have seen the tuner disclaimer, it could be inaccurate because the output is not aware of better history of statistics to give ensure stats are right. Lets take small steps to see if it helps: 

I have generated a dump from Master and I have to construct slave and give privileges and binary log position and file name of master for it to obsorb from master. Master is a high transaction OLTP application having TPS of 74.21 inserts/s, 113.17 updates/s, 0.00 deletes/s, 77468.29 reads/s. Now when DML operations are about to operate on SLAVE via binlogs would there be any impact on Master's performance. Say for example Load Avg or IO operations on Master that might get affected. As it allows slave to read the binlog info . Note: Master and Slave have same server configuration. Mysql Version : 5.0.77 OS : CentOS 5.4 Mem: 16GB RAM (80% allocated to INNODB_BUFFER_POOL_SIZE) Database Size: 380 GB This question strikes me when I wanted to scp some of the files to Slave. Whereas it has just 34MBPS avg speed within the network and it took reads IO operations a lot and Load Avg was hitting more than 6, 7 , 11 etc .. Ofcourse its not going to use SSH as it uses mysql socket to connect. Just a measurement to have slave in place by not hindering master's perf. My questions are: 

User accounts are again stored in tables in terms of rows and columns and also its respected permissions under mysql database wherein it has primary key as host and username. So, I don't think so you have any limitations. But you have something called max_connections that has to be set to a value, say it has 100000 as max that you can set. Then keeping each user connection in mind then we can go upto 99999 users connected to it parallely. My question to you is.