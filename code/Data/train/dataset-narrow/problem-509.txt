If you use a multi-statement UDF, then your inner select is executed exactly once for each outer row. The multi-statement UDF is treated as a black box: the execution plan will now show access to the objects used in your complex view. On the other hand, a subquery and/or an inline UDF is flattened out by the optimizer. When this is the case, the execution plan will include access to the objects used in your complex view. 

It has five constraints which work together to implement the business rule. Let me demonstrate how the more complex ones work. Of course, some constraints are simple and as such do not need any explanations. ** 

and you are all set, as long as all your constraints are trusted. I am not sure why would you need the third table CourseSize at all. 

Ralph Kimball recommends storing dates as integers. He has written a lot, both online articles and books. You can use a calendar table and issue consecutive numbers to your dates, as follows: Date Number 20120229 1234 20120301 1235 

It saves me a few keystrokes. Yet I think naming conventions are very subjective and as such I don't have a strong opinion one way or another. 

I need to let my users specify the list of columns they want to select. So far I know two ways of accomplishing that. 1. Using refcursors 

The following behavior may be caused by missing indexes on referring side of your FKs: "the price changes take approx 1 hour to process (vs. 1-2 minutes) and sys.dm_tran_locks shows the transaction taking almost 90,000 different locks, compared to around 100-150 when foreign keys were being dropped/recreated" When a row is deleted or its PK/Unique is mutating, the database engine need to make sure there are no orphans. When there is no proper index to support it, it scans the whole thing. 

Unfortunately, no. The basic Find (Ctrl+F) and Replace (Ctrl+H) functions in the Access UI search the resultset of the form row-by-row. For an ODBC linked table (any ODBC linked table, not just PostgreSQL) once Access has gone past the end of the rows that it pre-fetched when it displayed the form it retrieves one row at a time until it finds a match or reaches the end. (Find and Replace is often considerably faster for Access linked tables - as opposed to ODBC linked tables - because the Access Database Engine works directly with the database file, retrieving 4KB pages of data at a time. If consecutive rows happen to be on the same page, or if the 4KB page for a given row has recently been cached, then Access can "get" that row without another round-trip to the database file.) However, there is one workaround that may be helpful: If a Filter has been applied to a Form or Datasheet view then Find and Replace operations are restricted to the filtered rows. Access can take advantage of indexes and (at least some) server-side processing when applying a Filter to a form bound to an ODBC linked table, so you might be able to get the job done faster by Filtering the records first (even a non-sargable filter such as ), and then performing the Find and Replace on the filtered set. 

If you want to insert Unicode text in your statement you need to supply a Unicode string of the form , as in 

Note that the query uses syntax, which is generally preferred over the older method of putting the join conditions in the clause. 

The only method that didn't fail outright was to pull the tables from MySQL into a Microsoft Access .mdb file (!) via MySQL ODBC, then import that into SQL Server. However, even that was an incomplete solution because the transfer from MySQL to Access omitted all of the primary keys, indexes, and AUTO_INCREMENT attributes. Still, it did transfer the tables and data, which was my main objective. I just had to write some T-SQL to re-create the indexes and whatnot. Since you don't need to transfer any data you might want to consider just hacking the mysqldump SQL code (good old Find & Replace...) to the point where SQL Server will execute it. 

You can do this by using sp_delete_job and dynamically creating applicable syntax and controlled logic to execute this for the dynamically created job name when date wise it needs to be deleted. Since you are building the dynamic SQL Agent jobs and creating them with a start date and end date, you'd just put additional logic in to execute sp_delete_job if the date is equal to day eight at the end of the job . You could put the additional logic in at the beginning of the job to execute sp_delete_job if the date is equal to day nine or the day after the eighth day . 

Continued Trouble If you continue to have trouble you might set the Use SSL field to a value of No and also ensure all the SSL fields below that are blank and then test the connection again. 

The people or security contexts running the expensive queries on the wrong instance should be identified and then notified to change their processes on the instance and DB to start running their stuff on the instance you want those run on. Since the secondary is read-only/standby, we would assume these expensive queries are SELECT statements only where the primary DB that's not in standby would have to run queries that update data or modify objects regardless of the expense. You could also look into why some of these queries are so expensive in the first place, see if adding indexes would help, rewriting queries for performance tuning, etc. I think that'd be a good root cause type solution. It seems like it'd be tricky to do this with one of more AD groups though as the changes are replicated from primary to secondary and secondary would be in standby to accept transaction log changes from primary when those are applied so I'm not sure how you would accomplish this in that sort of configuration with log shipping. I'm also not certain how often your tran logs are being restored to secondary as I thought when LSRestore jobs run, it disconnects all session on secondary until those transactions are committed, so I assume it's once a day or not too often or people would be screaming about their queries, etc. getting disconnected during normal hours. If once-a-day restores are occurring for LSRestore jobs on secondary, then this would mean the data gotten from that server is 24 hours old or however long between your LSBackup, LSCopy, and LSRestore jobs. So who can use which DB with this regard may depend on how fresh their query results need to be from the business side, etc. Lots of factors to consider here but getting to the root cause and having bad performing queries tuned, adding indexes, etc. may be the best solution as well as having the people with access take responsibility with their processes to not hose up the performance for others when they run their stuff. 

Note that your design does not prevent cycles. We can easily enforce that via constraints as well. I can elaborate if you are interested. Regarding the efficiency of finding qualifying descendants, we can add some redundant data and get much better speed. For example, E is a descendant of A, but not a direct one - B is between them. We can store the following rows: 

This means that there can be no overlaps. As you have seen, for every time window, there can be at most one preceding it, and at most one following it. The following interval cannot begin before the end of its previous one. Together these two statements mean that there can be no overlaps. ** 

Another way to ensure very fast selects is to store periods as sequences of days - then we can select number of open intervals directly from an indexed view dbo.ConcurrentPeriodsByDay, which is as fast as it goes. 

The select keeps a shared lock on a non-clustered index on itemId, waits to acquire another shared lock on the clustered index, so that it can retrieve more columns. The update has modified a page of the clustered index and of course keeps an exclusive lock. It waits to modify the non-clustered index as well. 

This is why whenever we add a new index, we need to have some kind of baseline testing to verify that none of these issues happen. 

I have been doing unit testing T-SQL for more than four years so far. I think it much easier to use C#, which is more versatile. For example, in C# I have no problem unit testing a stored procedure that returns two or more result sets - it is plain impossible if you use T-SQL. I described the process in this article and in this blog post. 

As you learn more about SQL Server you will discover (to your delight) a number of things you can do in SQL Server at the database level that you previously had to do in Access at the application level. Some examples include: Triggers: Procedures defined at the table-level to make stuff automatically happen whenever there is an INSERT/UPDATE/DELETE on the table. Stored Procedures: Somewhat similar to Access macros and those little VBA procedures (s) you built in Access to do "useful stuff", except that in SQL Server they are part of the database itself. So, you can write a Stored Procedure (SP) once and use it (almost) anywhere, even across applications that share the same database. Functions: These are somewhat analogous to the little VBA s you wrote in Access to incorporate into queries, except that SQL Server Functions, like SPs, are more tightly bound to the database. Also SQL Server Functions can be Scalar-valued (return a single scalar value) or Table-valued (return a rowset). Fun SQL tricks: There are lots of SQL features available in SQL Server that are not supported in Access (Jet/ACE) SQL. "Common Table Expressions" (CTEs) and " and " are the ones that gave me "'aha' moments" as I was getting started. I could go on, but these are the things I remember discovering early on that got me "thinking in SQL Server". Don't get me wrong, I still think Access is an excellent tool when used appropriately, and should be given serious consideration as a way to 

The "Back Up Database" and "Save Database As" options do the same thing. The only real difference is that "Back Up Database" will provide a default file name like 

Yes. The Access Database Engine is designed to work with "real" Windows file sharing and Samba – or at least some versions of Samba – apparently do not implement all of the low-level features of the SMB protocol that are needed for the Access Database Engine to work reliably. (ref: Corrupt Microsoft Access MDB Causes - Samba) 

Just a quick thought on something to look into, in a domain type environment, some operating systems allow you to logon to the server before full network connectivity is established. You may want to check for either local or domain level group policy settings to not allow logon or OS startup until full network connectivity is established. Just in case you notice this when you log onto the server after reboots, it actually logs onto the OS with the cached credential before it can reach the domain controllers to authenticate (network connectivity not fully established) with the login credential if it's a domain credential the SQLExpress service account is running as. Not sure if that's exactly applicable in your case but this is something to at least simply investigate and try to test at least just in case. I found this in some article I saved long ago when I had a similar issue with an AD home directory (not via login script) to map home directory for a workstation PC: The policy value for Computer Configuration -> Administrative Templates -> System -> Logon “Always Wait for the Network at Computer Startup and Logon” will be set to “Enabled”. If the following registry value doesn’t exist or its value is not set to 1, then this is a finding: Registry Hive: HKEY_LOCAL_MACHINE Subkey: \Software\Policies\Microsoft\Windows NT\CurrentVersion\Winlogon\ Value Name: SyncForegroundPolicy Type: REG_DWORD Value: 1 

According to this StackOverflow post, if you have the server-level settings (referenced below) configured with ForceEncryption set to Yes, then that will enforce an encrypted connection regardless of the SSMS GUI option being checked or not prior to making connections to that server. This may be a sufficient workaround for people where the "encrypted connection" is of more importance than the actual option being checked within the SSMS GUI.