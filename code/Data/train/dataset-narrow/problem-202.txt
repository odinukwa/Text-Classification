The Layers can happen in multiple devices, at multiple levels. When it hits my firewall, for example, the firewall peels back all the way to Layer 4 to check the TCP ports against my ACLs. It then re-wraps the data down to Layer 1--and if I'm doing NAT/PAT, it even (gasp) modifies that data!!--and hammers that onto the wire to the next device, which may dig deeper, or not as deep. The OSI Model layers are more for thinking about things. It may--or may not--necessarily be happening precisely. Some things happen on multiple layers. One kind of "thinking about things" is programming. In vague, general terms, each layer is handled independently and assumes that the layers below/above it are functioning properly. But as useful as it is for thinking about things, as much as it's used to program and process data, don't forget that in the end, in the words of Monty Python, "It's only a model." 

As Pyatka stated, it looks like your interface is overutilized. That's why you're getting overruns and throttles. Start monitoring interface traffic, not just errors. Your interface thinks it's being overutilized? Monitor to prove it. Start monitoring CPU usage as well. Your interface might be acting like it's overloaded because the CPU is being overutilized. Start logging everything! "logging buffered 7", then "logging buffered <~50% of the free space determined in 'dir' command>" is what I'd do. Check "show processes cpu history" and see if you see spikes, see if you can match the pattern of CPU use with the pattern of errors. (Note, the history graphs start with MOST-RECENT on the LEFT of the chart, NOT THE RIGHT. This can be very confusing!) If you can discern a pattern from all this... log in when it happens!! Show procs, show ints, show logs, check everything. Nothing's quite like seeing the issue "in the wild." As for speed/duplex, we've found speed and duplex being hard-set on both ends is the best bet; since you've already stated you've done auto and hard-set, duplex probably isn't your issue. I'm not seeing errors in the patterns you'd really expect for a duplex mismatch anyway, though it's always an option. You might turn on CDP for giggles and do a "sh cdp neighbors detail" for giggles to see what the other end is advertising itself to CDP as. I don't think they're your issue. Good luck! 

No. However, you can simply input the interface name again to ensure you're in the expected interface. 

Let the broadcast storm run for over 5 minutes. Disconnect the broadcasting device. IMMEDIATELY log into switch and check interface stats, should show last 5-minutes values. (Probably best done via script, to each device, to make it both quicker and more reliable.) Use this data to "walk backwards" and derive a calculation. Note this is quite likely to vary by switch model, OS version, OS features, etc. You may be able to come up with a general "rule of thumb" calculation if you try all the models/versions/features you're interested in. You may want to look at your vendor's whitepapers, they could have some info on this. 

The purpose of the messy wiring on the right is to degrade service on the lines. I kid... slightly. That's the result. The purpose is to save time. The excess ends (you can see a green wire pointing up from the metal bracket going nowhere) are sometimes called "pigtails." Some current can bounce back off of these and degrade service. The proper thing to do is to trim the excess ends where you punched them down. The proper thing to do is to organize your wiring, and perform preventative maintenance including trimming the ends someone else failed to, organizing their wiring, tagging unidentified lines, etc... but these telco techs get ridden pretty damn hard in most places. Clients escalate everything, salesmen cut deals, bosses cut opex, and maintenance manning is considered a cost centre so while it may take 6 dudes to properly wire and maintain your area, you've got to do it with 3. You're going to leave pigtails, some wires won't get tagged, and the whole thing is going to be a mess. ... and I'm not one of these guys. I've just had to deal with the good and bad ones. 

So Priv 15 can do anything. Priv 1 can do anything that doesn't require enable mode. I understand this is a roundabout, reversed answer to your question, but it's the standard one. I know of no canonical list for either category. To give a little more info, here's what I get from my C2960 running IOS Version 12.2(50r)SE1: 

Notice that SSID S, centred on channel 1, gets interference from not just SSID 2412s (which is also on channel 1), but from 2417s on channel 2 and from 2422s on channel 3! However 2347s on channel 6 doesn't interfere with it at all. 2347s also does not interfere with 2362s, on channel 11. You may need to use a wifi analyzer to find out which channel has the least interference, but bear in mind a lot of devices will change channel automagically. It is possible, though, that you're in an area where the local internet provider uses equipment that all sits on one channel, so you could get lucky. At one point I was in an apartment with 24+ different wifi networks bleeding in... all on channel 1, all gear from same ISP. There were only 3 networks elsewhere. As long as I chose a channel 6 or higher, I had no trouble. Avoid channel 13 and 14 unless you like the idea of prison, being a prisoner, living the prison lifestyle, etc. You'll get plenty of conflicting advice, a little wifi is not worth running the risk that you have to change your definition of a good day to be "one where I didn't get assaulted until AFTER lunch." TL;DR version: Yes, you can choose any channel, but assuming an essentially even distribution (a safe assumption in most cases), choosing 1, 6, or 11 will give you best performance since they don't overlap with each other. 

Please be aware, ICMP is bottom-of-the-barrel traffic. During high utilization, you should expect ICMP to drop. It is set up to be dropped first in almost every single scenario, to make way for real data. So how do you test? TCPing is one way. This sends TCP SYN packets and times how long it takes to get an ACK back. You do need to know an open port on the other side to TCPing, but this gives you a good view of how actual TCP traffic is performing. TCPing should tell you end-to-end your performance, but you already know there's some kind of trouble. How do you isolate it? WinMTR to the rescue! This basically runs a traceroute constantly, and collects the latency data on each hop. When you see the latency suddenly increase, you've probably got your culprit... especially if it's persistent. These are 2 tools I use regularly to help isolate where issues exist. If you isolate the issue to a device you have SNMP access to, but not CLI/GUI access to, you can use SNMPWalk and SNMPGet to dig deeper. I recommend this as a last-ditch effort, since the "S" in "SNMP" stands for something it isn't to mere mortals: Simple. But oh, the information you can eventually coax out of it. 

Nearly every piece of hardware will drop ICMP (ping) traffic in favour of TCP, UDP, and other "real" traffic. Seeing 1-5% packet loss during peak utilization is not remarkable. I would expect, however, to see similar loss across all tunnels terminating at the same firewall were this the case, but 1-5% is still pretty much nothing. One way for a provider to limp older hardware along is to traffic shape or rate-limit ICMP, which means it gets dropped even more aggressively. I would also expect similar loss on all tunnels going through the same provider hardware, so you may want to Traceroute/MTR (there's a WinMTR for Windows machines) these tunnel endpoints to see if there are any similarities in routes. But, again, 1-5% is usually nothing. You may want to look into TCPing, which simulates ICMP but using TCP connections. If you're getting 1-5% packet loss with TCP traffic too, then that's more remarkable (but still not terrible). Check CPU utilization on the firewalls on both sides, memory, interface stats like errors, discards, overruns, etc. 

I can't really figure out what this means, nor why it's such a ridiculously high number... though I suspect the latter is due to this being a failover ASA in an HA pair, which has been powered on and in standby state for several months. Of course, I didn't get a chance to look at these numbers on the primary. :( So, 2 part question: What does "VALID CONNS RATE in TCP INTERCEPT" mean? and why is the average so extraordinarily high? 

If the bandwidth is greater, then it is going to improve the speed of the connection... however, that's not your real question, is it? It seems you really want to know if it will improve the perceived speed for your clients. To answer this, review all the other answers. Also check if you're utilizing your upstream bandwidth or not. You say you have 3 DSL lines at 8 Mbits each... OK, that's nice, but what's the upload rate on those? If you're running 8Mb down but only 384kb up, that's 24 Mb downstream but barely over 1Mb upstream. While most users download more than they upload, every TCP connection sends SOMEthing upstream... and some, eg Skype calls, VOIP, BitTorrent, etc, send a HECK of a lot upstream! If you're utilizing all your upstream bandwidth, then even if you have plenty of downstream to spare your users aren't going to be able to use it til they can send their ACK packets, their requests, their half of the video call, etc. Once you can identify your upstream and downstream bandwidth and utilization thereof, you can begin to tell what up/down bandwidth you'll need.