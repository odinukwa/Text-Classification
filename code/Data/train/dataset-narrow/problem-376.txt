So our MongoS instances use up A LOT of memory. More than even the shards and sometimes crash because of it. Cluster structure 

So I ended up solving this issue myself. Not quite in the way I was hoping to do it, but it does work and the websites are no longer going down or even slowing down noticeably during backups. The fix - I changed the kernel I/O Scheduler from CFQ to Deadline, i.e. made it prioritize smaller, faster I/O tasks. That way any website request is prioritized over the backup process and the websites have no down time anymore. Tested and worked just right. Here's how to do it. We first check what the scheduler is currently set to. 

I have a sql agent job setup for taking full backup every night. Last night the job failed with the following error message. Sql Server Error Log Message 

After doing a lot more investigation this is what I have found. It fixed the issue (significant performance gain and WRITELOG has an average wait time of 0.0126 which was initially 14.681) Apparently the issue was with the Number of Virtual Log files in my physical log file. There is a job scheduled to rebuild indexes every night, the job creates 36GB of logs, and until few weeks ago someone had add a job to shrink log file on weekly basis. Log file was being shrunk to 500MB. Since it is a very busy server the log file would grow in size and it was set to auto grow by 3 percent. Each time it grew it added more and more VLFs. As a result my 35.5GB log file had 1600 VLFs. To resolve the issue I did the following: 

Solved. I upgraded MongoDB from 3.2.1 to 3.2.3 and it magically started working. I didn't even need to recreate / reconfigure anything. It just started splitting the chunks correctly and works fine. 

This may need to be done for more disks than just one, assuming hda isn't your only hard drive. E.g. I also had a hdb disk, so I had to do this 

So I have a collection with 55 million documents or so. I've enabled system profiling to check for slow queries and I do have a few every now and again. Example: 

Someone answered this question for me and it is as I suspected. SDO_INTERSECTION will return any lines fully contained within a polygon, as well as those parts of intersecting lines, which are within the polygon. In other words, it will return the geometry of everything in red in the example image. 

After a lot of digging around I could get an answer and then eventually I got Microsoft involved to help me the problem. MS solution architect confirmed that the Report Builder requires a direct connection to SQL Server when launched hence the working with data sets in the report builder failed. Apparently this is BY DESIGN and it is not going to change. This was in SQL Server 2008 R2 but the same is true for SQL Server 2012, 2014 and 2016. Not sure about 2017 but I doubt it very much that this has changed in 2017. We ended up provided a client machine on the network to which end users connected remotely only to work with Report builder, it is a poor solution but it worked at that time and we needed a quick fix for it at that time. 

I'm confused as to what exactly the SDO_GEOM.SDO_INTERSECTION returns in Oracle. Documentation is here, but only gives examples of two intersecting polygons. What if I have many lines, some of them are contained fully within the polygon and some intersect the polygon. Assume that I need the geometry of any line fully contained within the polygon (i.e. same as SDO_CONTAINS) AND the geometry of those parts of intersecting lines, which are inside the polygon. Example image below. Assume I only need the geometries of the red lines, but not the green ones. Will SDO_INTERSECTION return all of the red ones or only those red ones, which also go outside of the polygon and have some green bits? 

Since you have made sure that you have SQL Server installed on your machine all is left is to make sure that you follow the following steps to create a database. 1) Go to right click and Select 

When to failover is decided by the Witness server not but the Primary or Secondary servers in a mirroring session. The Secondary Server will come online if the Witness Server cannot see the Primary Server. The Secondary server only gets transactions from the Primary server and nothing else. If the Primary Server stops sending Transactions the secondary server will just simply sit there in restoring mode and do nothing, But if Primary servers becomes invisible for Witness Server thats when the failover happens and the Witness server will make Secondary server to come online. Actually this the whole purpose of having a Witness server(Auto-Failover). If the secondary server could make these decisions on it own we would not need a witness server at all :) . 

The above cronjob pretty much checks if mongos is running and if not restarts it, i.e. starts it back up after a crash. And after a crash and restart, the memory naturally drops all the way to 5 to 10GB and stays there permanently, unless we do another heavy aggregation or import. In other words, mongo doesn't really need that memory and according to all the issues on stackoverflow, all the docs and what not, mongo is supposed to let the OS tell it when to free up memory. It's supposed to release memory when something or maybe even mongo itself needs to use it for something else, but it doesn't do that and crashes instead. Thankfully, this isn't a super major problem as our 3 MongoS are behind a load balancer, so if one goes down the other 2 can keep our services online. But it's still an issue I don't want to have to deal with in production, especially as our database and load grows. I've found this - $URL$ and have enabled aggressive decommit, but that did not help. Thanks 

I have a pretty complicated situation at hand, let me try to explain. I have a SQL Server and a Web Server in a separate domain (lets call it domain A). SQL Server has the databases for Reporting services, The web server has reporting services installed. The SSRS also have SSL certificates installed and we are using https protocol to connect to the reports manager. Now I need to give access to users from domain B to connect to the Reports Manager (report server on Domain A). The users from Domain B cannot have any logins in the Domain A where the reporting services are installed, I know to access the reports manager I need to add domain logins/groups to the reports manager and assign them appropriate roles to access the reports. What options do I have (if any) to give access to users from Domain B to connect to a Reports Manager on Domain A? Important Note: The access to reports server is via NLB with external facing IP, the reports server (web server) or the SQL server does not have any external facing IP 

Each of these is running on its own dedicated server with 64GB of RAM, except the config servers which are running on smaller servers, but they are not relevant here. Situation Our MongoS instances use up even more memory than that, but only after some intensive workload. Usually after some huge import or aggregation. And the problem is that they never drop it afterwards. If we do not do huge imports, the memory will stay between 6 and 10GB, but during an aggregation it will jump up to 95-99% memory usage and will stay there even after the aggregation is finished resulting in us getting alerts for high memory usage on our monitoring software all the time. But that's not the main problem, the problem is it crashes. Problem Now, I know mongo uses as much memory as it has available and frees it up as necessary, however, it doesn't really free it up correctly. It often crashes, in fact it's bad enough for me to have the following cronjob: