(or whatever). Then your metadata operation is the update of the view, as opposed to the switching in of the partitioned table. 

The parameters you're looking at are only used if indexes are reorganized. Depending on the level of fragmentation involved, which you provide when you run the task, the IndexOptimize stored procedure will either rebuild indexes (which do not require statistics updates), or reorganize (which do require statistics updates). There is a third scenario, where you can only update statistics, using the following code (from Ola's site): 

Depends on who you ask. Is it in terms of new features, etc.? For example, a 32-bit RDBMS system could be considered obsolete nowadays, because 64-bit is common, and most engines run safely on a 64-bit platform, with access to more memory. Is it in terms of scaling? For example, an old design that doesn't scale well, might be considered obsolete. A database with queries that use old-style joins, could be considered obsolete, and in this case would prevent upgrades to newer technology without changing the code. Some wonks like to say that SQL itself is obsolete, that NoSQL technology is better suited to certain problem areas like unstructured data. I personally wouldn't say this is the right use of "obsolete", but that could be an example too. 

Things that will cause a DROP and CREATE, aside from your example of changing a data type, usually involve changes to the Primary Key / clustering key. Moving columns will do it, as will changing collation on text-based columns. I'm not sure what you're trying to avoid, per se. An ALTER may seem more elegant in code, but it might also be doing a whole lot of work in the background too, running the risk of truncating or failing, so dropping and creating is considered better. I think their rule of thumb is that in-place data conversions are too risky, especially with tables that already contain data, so it makes more sense to make a new one. 

Currently, the hierarchy values of these above rows are all perfect, however we can ruin integrity in the rows below by doing the following. 

I have read many articles now about natural vs surrogate primary keys, and came to the conclusion there is no single best practice. I have a table that will have around 2000 definite unique values, each value will range from 5 characters to 40 in length. This seems like a partial choice as a natural key, although the values which are 40 characters in length may cause some performance and storage issues when they are referenced elsewhere. As the total maximum rows in this table is fixed as 2000 and 35% of these rows contain value length of 25-40 characters(65% have length 6-25), shall I go with a natural key here? With your experience, what would you do here? 

If a Team must have a minimum of 11 FootballPlayers, how can we enforce this within the database design? Could I use Triggers? Perhaps One To Many is wrong? ... 

I managed to find a workaround for this but I am not very happy about it. I installed my previuos setup which was EasyPHP and copied my old MySQL *data* dir into the EasyPHP *MySQL* data directory. This time around the database was able to read the .ibd / .frm files correctly.(even though these files did not change at all) I then used phpMyAdmin to export the database(this time around all tables/data was stored in the sql file) I then disabled EasyPHP enviroment and started up my new enviroment and used phpMyAdmin->import with the new sql file which loaded the data correctly. Overall I think its pretty poor that MySQL cant simply output some type of notice stating that the .ini enviroment variables are not set corresponding to the .ibd / .frm files rather than just output incorrect information and leave the user in the dark with all sorts of angles to think about. After googling around it seems many people seem to believe that their data is corrupt when that's really not the case. 

Nothing will happen to your SQL Server installation if you change the Windows system locale, because it was installed with the specific collation . 

This is not true. Transaction log backups are incremental, that is, they only store incremental changes since the previous transaction log backup. You are confusing the differential log backup with "backups are not incremental", because differential backups only keep a record of the changes since the last full backup. (Differential backups work in all recovery models, namely Simple, Bulk-Logged, and Full.) If you want incremental backups, take transaction log backups regularly. 

This comprises an ordered set of backup media, which can be tape, disk files, Azure Blob Storage, etc., but (and this is important) not a combination of two or more of these. So if you create a media set, your backup media in that media set must be of the same kind. On to your question about the name of the media set. According to the same article (emphasis added): 

Is this supported by the vendor? Probably not. Have you run the DTU Calculator to see what your baseline is? This will get expensive fast, especially if you have slow and expensive queries now. Addressing everything I mentioned in the Performance section would potentially reduce this cost going forward. 

To force the behaviour you're trying to compare, we have to do this construction (notice the additional and extra brackets): 

I notice a case-sensitive difference between your declaration , and from the error message. Check to see if you're not using a case-sensitive collation. 

Assuming the values for your initial query result are 1 and 2, you have two values returned back, which you can now plug into a clause. You would use this to search for the number of times T3ID appears where it has two matches. This code is written in T-SQL, but I'm sure you can convert it quite easily to Access, as it's for demonstration purposes anyway. 

I seem to have came to a problem which I am having trouble solving, possibly because I have misunderstood intersections properly(or perhaps I shouldn't even use intersections for the goal). If we have a year table and use its primary key id in our intersection table to link to one actors primary key id, How can we populate the actors_nickname table before hand using one primary key id (to group together the rows)? which we then use after to populate the intersection table? year 

Here I have highlighted three problems which can occur, how can we enforce integrity here. PS Is there a name for what I have described? I assume in maths there are some terms for this, I did some research around ordinal numbers but it doesn't fit exactly into this. Describing this as just a set wouldn't allow for the issues I raised to be assumed problematic. Thanks 

Is there a naming convention for lookup tables? I cannot see any declaration by oracle or anything consistent on google. However I assume some of you professional DBAs follow a convention? Perhaps there is a convention which occurs the most when you are called to edit a project you didn't author? example table metals 

This is the desired result above, However if I was to insert rows into actor_nicknames, we would actually get actors_id = 1,2,3,4, which is useless to us in the intersection table as we need these rows grouped by the same ID. Thanks in advance. 

Inside the import file each CREATE TABLE statement is suffixed by IF NOT EXISTS, so why is this being reported as an error? I have tried to take out the table reported from the sql file, but then the next table just throws up the error, and the next etc until we have a sql import with no tables left. I am using Windows XP with MySQL 5.5.37-win32, these versions could be different from the versions used when creating the backup, would this matter? Thanks in advance. Edit : Added SQL code. This is just a sample, the code is created by phpMyAdmin's export function 

That's 2 + 2 + 12 + 2 + 1. Yes, it works out to 7 bytes as you calculated, but it could easily be more if there are more columns in the table. Assuming then a length of 19 bytes per row, you can fit 426 rows in a page. That works out to 1,173,709 pages, or 9,615,024,128 bytes. If we have a b-tree structure with 500,000,000 rows, your 1,173,709 pages will be at the leaf level. You'll need roughly 2,500 pages for the intermediate level (I don't know the exact length of the intermediate index record off the top of my head, so I guessed 16 bytes), which is an additional 200MB, give or take, and you may have two intermediate levels between root and leaf level. With this in mind, based on my terrible estimates for the b-tree structure, that would take you to around the 10GB range you see for your data. (All credit to Dan Guzman and Michael Green for their comments, and of course Paul Randal for his blog post.) 

Short answer: yes it can help, because it's theoretically instantaneous. You would insert your data into a staging table with the same definition as your main partitioned table, and then switch it into the partitioned table, which is a metadata operation (schema lock). Long answer: it might make performance suffer over the long run to have such a large table, and unless you move to later versions of SQL Server, statistics updates are quite difficult to manage. If this is a read-heavy table, I'd consider using view partitioning instead. Each month (for example) would get its own table, with check constraints on a date column to help the query optimizer know where the data is physically stored, and a view over the top of that: 

I am trying to upload a backup sql file through phpMyAdmin. I create the empty db with the same db name as in my import file in phpMyAdmin then use the import function selected from within this empty db. I get the following error message. 

From my current knowledge and experience, it is not possible to insert NULL into a DATE type, so what would be the point in declaring NOT NULL at table creation? I see this quite often throughout many SQL queries online, including my own, and wondered if there is another reason to do it? 

I decided to change to LK_ + original table + column as it allows easier navigation when browsing all tables. 

I have a table images, which I use to store a file name, file hash and date created. I also need the width, height and watermark overlay of each image, but only on very small occasions do I need to access/update this data, should I create another table and setup a foreign key relation to hold just these values, and when I do need this data, use a join? Should I just store the width, height and watermark overlay values inside the images table? Thanks 

I must enforce uniqueness within my table, But I cannot seem to find anywhere online(mysql manual,forums,stack overflow) talk about two unique constraints within one table. Why you might ask I need two? example 

pid and car must be unique and appear only once. pid and serial must be unique and appear only once. At first I thought to add a constraint UNIQUE (pid,car,color), but with this constraint we could still enter false data as in the example, 

Once you move data out of a filegroup on SQL Server, you must run a command to reclaim that physical space. The official documentation has all you need to know about this, but the general technique is as follows (this example is from the documentation, unedited). 

The big thing about NoSQL is the concept of "eventual consistency" or "optimistic replication". Assuming nothing in the database is really dependent on the order of inserts, modifications or deletes, it vastly improves performance. After all, performance is a feature. 

There are two options for this kind of scenario: 1) Shrink the data file, and then rebuild the indexes. As you say, this is time consuming and you're largely at the whim of the storage subsystem. 2) Create a new filegroup, and migrate all the data into the new filegroup, one table at a time. Shrink the old filegroup when you're done, and either move everything back again, or leave it as is. This second option adds some complexity, and additional maintenance for the extra FG, but is probably what you're after, since you can move one table at a time. You would do this by altering each table's clustered index to rebuild in the new filegroup. Adding a Filegroup: (See $URL$ 

Those numbers in your performance tests are accurate. Theoretical throughput does not match my experience, after migrating an environment to more than ten DS13v2 instances. I get between 220MB/s and 256MB/s, depending on the type of writes, even with P30s. To answer your question, the bottleneck is the network itself, which links the drives to the VM. 

I would strongly suggest modifying this script to only write to one path, and then use an automated method to copy this backup set to another location.