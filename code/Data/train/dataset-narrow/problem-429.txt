Sometimes it is reasonable to replace NULL values by placeholders. That allows to keep logic simple on the client side. There are two approaches here. First is to use placeholders instead of NULLs directly in the tables. That make the data uniform but also means some loss of data because NULL have special meaning in the RDBMS. Second approach is to replace NULLs by placeholders on the fly when data is passed to the client. 

Yes you should. Index for unique data need qualifier because that determine which kind of tree structure will be used. Sure you can achieve uniqness of fields combination by single index of type but unique index itself is somehow faster than non-unique one. If an index can be unique or non-unique - make it unique. 

Here I suppose that is fetched from the another table and perform the mass update for all the rows at once. If the is bigger than then become negative therefore I've add it to the instead of substraction. 

The only reasonable way for you is to create stored procedure with that should scan the table row by row and calculate desired value. Otherwise you'll stuck with really HUGE on-disk temporary table without indexing that cause timeouts and disconnections just like you already have. 

If your base accumulates rapidly incoming inserts, the good idea is to split long-lasting query into the sequence of small queries. That prevents DB from connections shortage when the connections are wait for the table will be unlocked and new connection being refused and data is being lost. Then you'll get the execution queue like instead of 

Go to the Heidi's tab , subtab , scroll down to the line and change its value to 150. The other way is to launch query like that: 

is not a commutative operation. When A add B as friend that doesn't means that B do the same for A. That is why the mutuality emerge. You definitely need both rows: 

You have to filter out what you need from the whole product. That can be done in two ways. You can use or . There is significant difference between this approaches. generates the full product and then filter out only those rows that match conditions. create the product that initially contain only pairs of rows that meet requirements. 

Now if this snippet runs concurrently by 2 transactions, T1 and T2 what will happen? I know that in MySQL by default the isolation level is . So let's say that amount initially is 500. So when the 2 transactions finish will the final amount be 300 or 100? I mean when they start both read from table which I guess is a shared lock. When one updates the table it gets an exclusive lock right? So what happens to the other transaction? Will it proceed with the amount it "view" when it started i.e. 500? 

I have noticed that even on huge tables with millions of records when I do a the index is created immediately (I get the console back in nsecs). Why is this? Doesn't it create the B-Tree with the table data at that point? I can only assume not due to the immediate return of the . But then how is the index created? On each access? 

I have a rather basic question on transactions. Assume a table with a column amount. Now we have the following code: 

When having a table in SQLite that has a data type, is there any implication if the length of the contained string differ significantly? 

I read somewhere that InnoDB can store all the tables in a single file or in separate files. Is this a configuration option? What is the default setting? A file for all tables or a file per table? What is the recommended setting? 

I can not understand or visualize how this is formed in the HD. I think that 4 * 5 = 20 physical files are created. Is this correct? And what data are stored together? Is there a "clustering" of 2008 dates for all customers with the same hash? I am confused on that. Also I am confused how the data are fetched. Is the customer_id hash used first to determine the range partition or the opposite. What are the exact steps to fetch the data? UPDATE Another example. I did the following: 

Databases use the file system to store the data. As far as I know it is not possible to delete a record from a random access file. So does that mean that when we do a the size of the table i.e. the file that stores the table never decreases? So databases essentially keep growing and never reduce in size? 

Probably doesn't listen the TCP sockets at all and console client is connected via filesocket. Try to list all network sockets listened at your host: 

Another possible reason for that is the turned . Only numerical IP addresses are accepted while can't be resolved into the . Sure the same problem occurs when DNS service is unavailable and is misconfigured. 

You should not use the same table for entities of the different nature. In the RDBMS paradygm table represents the type or class. For each specific type/class you have, you should use the separate table. Sure you can mangle the table by bells and whistles like . Sure you can cheat and define your entities in common as "something that have an address(es)". But the direct and inevitable result of such approach is the complexity and non-obviousness of the queries on that tables. Each time you will be forced to check what exact "subtype" of the entities you deal with. More conditions in the s, more complex s and their 's and so forth. Indeed, there are lot of cases when we are forced to denormalize our databases or to break the ACIDity for sake of performance. But if your primary goal is the clear and comprehensible design - you have to avoid violations of the basic principles. Pretty good explanation you can find in the foundational book "An Introduction to Database Systems" by C.J.Date, part II chapter 5. As mentioned by Gypsy Spellweaver, you need an auxilliary "pure relationship tables": 

This three queries can be combined into the equivalent single one with complex condition with lot of s, s and braces. But for better comprehension of principle splitted queries are preferrable. UPDATE My strong advice is to replace by even on the testing base until result will be acceptable. 

There is one entity - , that can be in a various states. Completed, scheduled, postponed, rolled back etc. State is the property of the entity, only part of the whole, like the field is a part of the row. When the state is just a field of the table it is possible to create DB structure and code that are invariant regardless of how many states are possible. In the opposite case each new state produce the new table and lot of code should be rewritten to reflect the changes. 

What happens to a transaction when the client connection is lost. Example the client is a web application code that starts a transaction to a remote server instance. The client sends the sql code to the server and waits for the transaction to complete. What happens if the network connection fails? Does it depend on the point we are in a transaction? Is it irrelevant? Is it aborted? 

Let's say I have an entity which has a location as part of its attributes Now a can have N of other that are close by. So seems to be a self referencing relationship 1-N (optional) If we know before-hand which exact are close by each other what would be the best way to represent that? Since we have a self-referencing 1-N relationship I think another table would be needed In this case we would store e.g. etc to show that pizza store with id 1 has 22, 23 and 78 near by etc Now in order to get these rows back in-order I would need to create a PK and query based on that. I was wondering would an auto-increment guarantee the insertion order? Or would I need to use a float representing the distance e.g. (where 2.04 is the distance in miles) I was also thinking is there a better way than this? We know that if is close to then is also close to right? Is there a more efficient way to represent this information? I think it would suffice to store just the row to capture that is close to and that is close to . But this way we are losing the order Update: All the answers are very useful. One thing though that perhaps is not clear. I already have the order of the entities based on the distance on insert time. I.e. I have a set of ids of objects already sorted based on distance and I want to insert this set in the database in a way that I can retrieve the rows back in the order I have inserted them. I want to avoid sorting on distance on retrieve since I already had the order at insert time. These ids would be used to join to another table (they are ids) and I want to retrieve the rows in the insert order 

where is an IP-address of your host OS. If you have dynamically assigned IPs on your LAN then you have to allow connections from the whole subnet your IPs are belongs to: 

The other advice is to simplify your calculations as possible. Do not calculate again and again those things you have calculated already, like . My final code now is such simple: 

First of all you have to import all the timezones from into the . There is special script called that do all the magic. Overall explanation can be found here: $URL$ After the zones has been imported you can use function to convert raw datetime/timestamp in the UTC to the arbitrary TZ. Also your client can use for the session and all the datetime/timestamp columns will be converted to the desired timezone on the fly. 

Ordering should be performed with step greater than one. I prefer the step equal to 10 for better visual representation. Main idea that new item always can be inserted with SO=N+1 that is normally unused. Say, you have inserted something in between 30 and 40, then your sequence now look like that: 10-20-30-31-40-50. Then you have to the table that way: 

Your query haven't appropriate index (only simple index for is used) and thus is forced to fetch 11898928 rows to apply the timestamp restriction. Add the multicolumn index (class, timestamp) and you'll see what happens. Please post the new output to show the difference. 

Here table should have an multicolumn index (foo, bar) or (bar,foo), or even both. It's depends on certain data stored in your table and some performance measurements should be performed. Even though for different queries different indexes can become optimal and day-wide range could be faster with (foo,bar) while month-wide range could be faster with (bar,foo). 

means: IF previously assigned value of is equal to the freshly assigned value of THEN ... Third is that rows are ordered in the ASC-DESC order. If any row with the same have assigned, it will be proceeded first and propagate its value to the all consequent rows. If you have assigned two different values to the any two rows with the same by mistake the alphabetically bigger value wiil be propagated across the group. 

appears when column(s) used for grouping does not have an appropriate index. As mentioned above, results returned by are ordered by the same column(s). If you get for sorting you also have filesort for grouping. That insult performance in the same way. Therefore you have to create the index, not to suppress the sorting. 

Can someone please explain why the delete shows a different number of rows than the count? I am using the same join. Are they not equivalent? 

OR do I need any transactions or locks for the DELETE vs INSERT/SELECT? It seems not but I was thinking I may be misunderstanding something Note: My question is not about transactions and their benefits.My question is if 2 statements from different processes on the same table SELECT/DELETE need for any reason to be synchronized 

Why does the first sum of the fractions is summed up to 1 while when summing the decimal equivalent we get 0.9999? 

But this seems useless to me. I mean if I set serialized to a client connection then I would want all currently running transactions to be sequential, right? Otherwise what's the point? 

I read that I can have different isolation levels per connection and in the server. The default isolation level is REPEATABLE-READ. So if I have a transaction that issues a to the client connection, how does it interact with other transactions from other client connections? Do they all end up being serialized? From set transaction seems not: 

If you have a big table that you want to partition and you have 2 candidates as the partitioning key (in the sense that 50% of the queries use one column and 50% of the queries use the other column and almost no query uses both in the same SQL query unless all the code is rewritten to do that) how can one determine which key is the best candidate to be used? Does it matter for example if the "entity" that one key represents is "fixed" and the other is constantly increasing? E.g. as an example of what I mean "growth" (taken just as an analogy of what I am asking) if you have decide between using the doctor id or the patient id as the partitioning key where we can have only so much doctors but infinite number of patients 

When using triggers, if an update is done to a table then a trigger is executed. This is very convenient. But what I would need is to execute an external script. Is it possible to configure MySQL somehow so on a trigger/change of a value in a table an external process/scrip is executed?