Either format will work with R and Python, though you'll need a library for MS Access, which is the least common option. If any of these formats is the native format of the data, I'd go for that, which will avoid weird transformation artefacts. If neither of the formats is, I'd choose CSV, which would allow me to use a text editor if needed. However, if one of the columns contains user generated texts, it might be worth the trouble to choose Excel, that would limit the chance that the export would get botched. It is way easier to get your hands on a CD-reader. Your data doesn't sound that impressively large that it warrants using tape. 

Overfitting What I would make up of your results is that your model is overfitting. You can tell that from the large difference in accuracy between the test and train accuracy. Overfitting means that it learned rules specifically for the train set, those rules do not generalize well beyond the train set. Your confusion matrix tells us how much it is overfitting, because your largest class makes up over 90% of the population. Assuming that you test and train set have a similar distribution, any useful model would have to score more than 90% accuracy: A simple 0R-model would. Your model scores just under 80% on the test set. In depth look at the confusion matrix If you would look at the confusion matrix relatively (in percentages) it would look like this: 

I'm interested in any research materials on voting patterns. I have a data set of how PMs (members of parliament) voted in my country during last couple of years. Each PM has 3 buttons: Yes, No, Abstain. There are also two special situations: PM can be absent during the voting and he/she can be present but not vote (it's wrong but they do it sometimes). So any particular voting result is represented as a huge vector whose values are yes/no/abstain/present_not_voted/absent. I'd like to find some similarities in PMs' voting patterns and cluster them in groups based on these similarities. There are some interesting questions like coalition stability and party loyalty which (in theory) could be answered with the data described above. Unfortunately i deeply lack any references. While googling i've come across some site which formed an undirected graph based on American senators voting. It looked cool but i didn't find any explanation on how such graph can be formed. Intuitively "similar" is a symmetric binary relation so undirected graph representation sounds quite natural. But how can one define "similarity" for my data? Probably there are some standard ways of doing this. Any help will be highly appreciated. 

I'm looking for tools that would help me and my team annotate training sets. I work in an environment with large sets of data, some of which are un- or semi-structured. In many cases there are registration that help in finding a grounded truth. In many cases however a curated set is needed, even if it just were for evaluation. A complicating factor is that some of the data can not leave the premise. We are looking to annotate an object detection task, but I anticipate an image segmentation task, a text classification task and a sentiment detection task in the near future. What I'm looking for is a system that can help a group make an annotation, preferably in a way that motivates the annotators by showing group progress, relative individual progress and perhaps personal inter annotator agreement. 

In the basics a good visual split is a good starting point. And yes, it is smart to keep in mind how the algorithms divide the space. A good strategy, I personally like to apply is to start of with simple learners to learn how your data is structered. Hoe well does NN work, are there hints of local behavior? How well does Naive Bayes work? Is the concept complex or do individual features hold information? Etc. As for selecting the features: You could try ranking your features on methods that compare their use (such as information gain), or simply write a scheme that tries all combinations of two on your two methods (it's only 9 * 8 runs). If the space was a little bigger I would suggest a combination of the two. You might also want to try combining features (fi: PCA). 

That has to do with how forward and backpropagation works. Remember that forward propagation is done by applying the activation function to the result of multiplying the activations of layer by a weight matrix plus the bias vector for each layer of the network: 

where is the output of layer and would be the input layer. It is easy to see from there that if the is set to zero the output will depend only on and the activation values will be the same for every neuron in the network. The backpropagation step of gradient descent uses calculate the gradient step and parameter updates so if every neuron in the layer has the same value those steps will also output equal values and all the neurons will learn the same parameters. By initializing the outputs to random values in the range you'll make each neuron learn differently breaking the symmetry. 

The objective with supervised learning is to try to create a model of your data that helps you predict future values. You do that by selecting on your data set - what you call variables - that you believe represent well the problem. I'm no expert but I understand economy is affected by an enormous number of variables, so even if you create a model that fits the data you currently have based on some of those variables, it might become obsolete the moment variables you did not consider start affecting the end result. That's what I believe your boss was talking about. Now, if you do decide to train a neural network in order to predict Bucket and NPA your first step will be to choose which variables you'll consider in your model. Keeping the 'static' variables will likely make your network have different predictions for, for example, different Product types but that depends on how the data is distributed across this static variables. If you choose to not use the static variables your model will completely ignore them when making predictions, which might not be what you want. 

KNN is instance based so it will store all training instances in memory. Since you are using images this will add up quickly. KNN on untransformed images might not perform that well anyway, you could look into filter banks to transform your images to a bag-of-word-representation (which is smaller and more invariant). However if it is accuracy you are aiming for I would recommend skipping all that (it is very 2012 anyway) in favor of using deep learning, fi: construct an auto-encoder and determine similarity on the encoded representation of an image (which could in turn be done using knn btw). 

Since Naive Bayes assumes independence and outputs class probabilities most feature importance criteria are not a direct fit. The feature importance should be no different from the skewness of the feature distribution in the set: You could try to directly compare the probability of the features given the classes (implemented in sklearn for instance), the variability of those probabilities with respect to the classes should express the importance of those features. 

Suppose that we perform density estimation in m-dimensional space: we estimate the value $p(a)$ for some point $a$ given observations $\{x_1, \dots, x_n \}$. It is known that if region $A \subset \mathbb{R}^m$ is "small" enough to consider density being constant on points from $A$ then we can make the following estimate: $$ p(a) \approx \frac{k / n}{|A|} $$ where $k$ is the number of observations that lie in $A$ and $|A|$ is Lebesgue measure of $A$. Let parameter $h$ be small enough to consider density as constant inside hypercube centered at $a$ with side length equal to $h$. The volume of this hypercube is equal to $h^m$ and point $x$ lies inside this hypercube iff $K(\frac{x-a}{h}) = 1$ where $$K(u) =\cases { 1\text{, if $|\frac{u^k - a^k}{h}| \leq \frac{1}{2}, k = 1,\dots,m$}\cr 0\text{, otherwise} }$$ It's easy to see that the number of observations inside this hypercube equals to $$k = \sum_{i = 1}^{n} K(\frac{x-a}{h})$$ and so the estimation described above gets the following form: $$p(a) \approx \frac{1}{n h^m} \sum_{i = 1}^{n} K(\frac{x-a}{h}) $$ We can interpret $K$ as "weight" given to particular observations and one of the drawbacks of hypercubic approach is that all observations lying inside hypercube have equal weights despite having different distances from $a$. Yet another drawback is that the resulting estimate is not continuous. That's what i understand to be the main reason of using non-hypercubic kernels such as gaussian kernel which give more weight to points close to $a$ and yields continuous estimate. But i have troubles with interpreting the usage of such kernels. The sum $\sum_{i = 1}^{n} K(\frac{x-a}{h})$ is no longer equal to $k$ so we can't justify the usage of these kernels by formula $p(a) \approx \frac{k / n}{|A|} $. Finally here are my questions: how do we justify the usage of smooth kernels? how can one interpret this usage? Thank you for any ideas. 

I have encountered case in which potent tree learners acted like Nearest Neighbor variants: They would learn to divide the search space so that only examples remained that where close on some meaningful axis (in my case lat and long :)). This could still generalize to examples in the test set that shared those features but not to genuinely unseen examples. Or to put it in another way: The method is potent enough to find rules that work well, but do not generalize in the way you want it to (aka over fitting, but the maybe in a specific way). What helped me were two things: First of all I tested this characteristic of the domain by just using KNN,only feeding it subsets of the feature space (and sure enough KNN over lat, lng worked like a charm). This helped me understand that examples that were in the same space time coords shared certain history, so the second action was to eliminate those examples in the train space and evaluate on an a set of examples totally outside of the coordinates (space and time) of the train set. It is hard to say where this behavior could lie in your case without having a peek at the code, but perhaps my anecdote helps.