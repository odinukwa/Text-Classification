Is there a general theorem that would state, with proper sanitization, that most known results regarding the use of real numbers can actually be used when considering only computable reals? Or is there a proper characterisation of results that remain valid when considering only the computable reals? A side question is whether results concerning computable reals can be proved without having to consider all real, or anything that is not computable. I am thinking specifically of calculus and mathematical analysis, but my question is in no way limited to that. Actually, I suppose there is a hierarchy of computable reals corresponding to the Turing hierarchy (Is that correct?). Then, more abstractedly, is there an abstract theory of real (I am not sure what the terminology should be), for which a number of results could be proved, that would apply to the traditional real numbers, but also to computable reals, and to any level of the Turing hierarchy of computable reals, if it exists. Then my question could possibly be stated as: Is there a characterization of results that will apply in the abstract theory of reals when they have been proved for traditionnal reals. And, could these results be proved directly in the abstract theory, without considering traditional reals. I am also interested in understanding how and when these theories of reals diverge. P.S. I do not know where to fit this in my question. I realised that a good deal of the mathematics on the reals have been generalized with topology. So it may be that the answer to my question, or part of it, can be found there. But there may also be more to it. 

You never need CNF. It has the drawback of changing the grammar structure. But you do need to introduce intermediate non-terminals so that no right-hand-side is longer than 2 (2-form) since RHS length determines the complexity. The best attempt at explaining that intuitively is, if memory serves, a paper by Beau Shiel, "Observations on Context Free Parsing", published in 1976 in a computational lingistics conference. Earley's algorithm uses 2-form implicitly. It is just hidden in the algorithm. Regarding recovery and handling of parse forest, you should look the web at "parsing intersection forest". It is actually very straightforward. Many papers are on the web, if you get (from citations or tables of content) the titles or authors to search them directly. Actually, you can do a lot more than CF, and still get parse-forests in polynomial time. The question is, sometimes: what can you do with it once you have it ? One purpose of the last article you mention is to show that complex algorithms (such as GLR) are not necessarily buying anything in time or in space, and may change your parse forest. One remark about teaching. I think Earley, seminal as it was, is far too complicated for teaching, and could be replaced by simpler algorithms with essentially the same educational content. Teaching is about concepts or technology. In Earley's algorithm, the essential concepts are hidden in the complexity of details, and from a technology point of view it is outdated. It was a great paper, but it does not mean it is the best pedagogical approach. There may be more information in the computational linguistics literature than in the usual computer science channels. I do not have the Ceriel-Grune-Jacobs book, but I would be surprised if they did not have all the proper references (though I am not sure about their selection criteria). 

I have not been following closely this area for many years, but I know systems have existed for a very long time to capture semantics or help translation. The first system for creating interpreters based on denotational semantics was SIS created by Peter Mosses around 1980. Around the same time, programmable structured editors and some multilanguage programming environments started being used for program transformation and even program translation between high-level languages, based on abstract syntax manipulation, since the mid-late seventies. Centaur (Kahn et alii) was such a system circa 1990. However, though some such systems did use formal semantics for various purposes, the translations I know of were programmed more or less by hand. There have been compilers based on formal semantics, but I think it is already pretty hard just to prove them correct. Coq has been used for that and it is still on-going work. Even if you manage to capture the semantics of both your languages, trying to infer from that a translator from one to the other is likely to be beyond current technology. For one thing, programming languages do not use exactly the same high level concepts, and that is what makes translation very difficult. Compiling, which is a form of translation to a target language produces code in a very low level language, not very readable, as the structure of the program is no longer explicited. It is true that some systems based on Curry-Howard isomorphism and type theory, such as Coq, can assist the production of programs by extracting them from a proof of their specification. But these are carefully tailored systems, with an adequate specification formalism. I doubt that a program written in another programming language could serve as such a specification within the current state of the art. I fear that the best you can hope for is the equivalent of a compiler to machine code: the source language will be translated in some kind of machine code expressed with an elementary and naive use of the target language. I doubt this is what you have in mind. 

Your question is unclear. When you write "parse", do you actually mean to produce a parse structure (one, or all possible ones). How do you define it ? For example, some grammars have a parse tree that differs from the derivation tree. Or did you mean that the language must be fasted to recognize? Regular grammars give you languages that are recognized in linear time. What qualifies as a grammar? Actually, if you are just interested in recognition, you are looking for properties of languages rather than properties of grammar (though they can be related). You can certainly build hierarchies of grammars according to complexity criteria if you clarify the above. For example in the case of Context-Free grammar, the is nearly a well defined concept of parsing (up to specifying whether you want one parse tree or all possible parse trees). The complexity is at worst $O(n^3)$ for parsing and $O(n^{2.373})$ (due to valiant, with a looser bound at the time) for recognition (wich is only asymptotic, and not the fastest in any realistic situation). There are CF grammar that will parse in linear time (notably deterministic grammars), others will parse in $O(n^2)$ time, and others are believed to require more (though the exact complexity bound is not known), even for simple recognition. I do not really know if these complexity hierarchies have been identified precisely. The usual hierarchies studied for context-free grammars are often more related to the applicability of techniques that can be used to build parsers for these grammars. As the question is stated, I guess there is one language that is faster than any other to recognize. That is the Babel language invented (nearly) by Jorge Luis Borges and containing all possible string (Borges was unfortunately limited to a finite size due to paper shortage, which would invalidate what I am saying). With all strings of any size, recognition is easy. You start in an accepting state and terminate in 0 steps. The empty language is recognized as fast. I believe these are the only ones recognized in 0 steps (for a given alphabet). This establishes that, if the alphabet is fixed, the class of languages recognizable in 0 time is closed under complementation. This is of course in jest, but you can see from it that one can define languagesn and corresponding grammars, that can be recognized in constant time, by putting constraints only on a finite prefix of strings. They are not much more interesting. The problem is in the statement of the question. I guess the fasted interesting class of CF grammars is the class parsable in linear-time. Deterministic CF grammars are clearly member of that class, so that is a sufficient property. However many non-deterministic languages, having no deterministic grammars, can also be parsed in linear time. 

We assume a CF grammar $G$ with an attribute system, such that evaluation of attributes on a parse tree $T$ will produce a result $E(T)$ for a distinguished attribute $\rho$ of the initial symbol $S$ labeling the root: $\rho(root_T)=E(T)$. Our purpose is to exhibit an attribute system that produces the same result using only synthesized attributes. Without loss of generality, we assume that the initial symbol S of the grammar is never used used in a right-hand side of a rule. Intuition: we make a copy of the tree as a purely synthesized attribute of the root node. In the formalization below $A_i$ denotes an occurence of a symbol in a rule, and we note $\sigma A_i$ the actual symbol in that occurence. We have a single synthesized attribute $\theta$ for all terminals and non-terminals. For terminals, this attribute is initialized to a tree degenerated to its root node labeled by the terminal. In addition, there is the attribute $\rho$ for the initial symbol $S$. To each grammar rule $\qquad \sigma A_0 \to \sigma A_1 \dots \sigma A_n$ we associate the following attribute calculation $\theta(A_0) \leftarrow tree(\sigma A_0, [\theta(A_1), \dots \theta(A_1)])$ where $\theta(s,tt)$ is a function that takes two arguments, a non-terminal $s$ and a list of trees $tt$, and returns a new tree with a root labeled with $s$ and all the trees in $tt$ as daughters of the root, in the same order. It is easily shown by structural induction that attribute evaluation of a tree $T$ will thus assign to the attribute $\theta(S)$ a value that is precisely equal to $T$ (i.e., a copy of $T$ if that is more intuitive). Since we know from the initial attribute system that there is an effective procedure to compute the result $E(T)$, we only have to add one extra attribute rule to be associated with the initial symbol $S$: $\rho(S)\leftarrow E(\theta(S))$ Thus, our new attribute system has only synthesized attributes and computes exactly the same result as the initial system. Some readers may find the proof a bit unsettling, but it is not a circular (tautological) proof. We do not have to use the initial attribute system to actually compute $E(\theta(S))$. The only thing that matters is that it is computable. Actually it is very close to the technique used in the example above, using functional attributes, but it delays computation by passing syntactic information bottom up, rather than doing it more semantically by passing a continuation. One could use that functional attribute technique to do the proof, but I suspect it would be technically much more complex. It shows that what matters to make sure than an evaluation policy can be used is to check that this evaluation policy is able to duplicate all the data, in this case the parse tree. From that perspective, it is easily seen that using only inherited attributes will not in general be adequate, because the tree cannot be duplicated with only inherited attributes. The reason is quite simple: when you are going top-down exclusively you cannot pass information sideways in the tree. Take the rule $ A \to B C$. If you are on the $B$ node of an instance of that rule in the tree and want to compute the inherited attributes, all you see is $A$ above. There is no way you can relate to the $C$ side of the tree. And $A$ cannot look at it since information goes top down only, and $C$ is below $A$. The information on the current node depends only on the path from the root to that node, not on the whole tree.