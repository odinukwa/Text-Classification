It's not possible to decrease the size of the model AND It's not possible to increase the maximum message size to a point where the pipeline would fit in a single message. 

Generally, the fact that your training and validation performance are improving at the same rate is a good thing- this (usually) means that the algorithm is learning generalizable features of your problem space rather than overfitting to the noise of your training set. Reaching a plateau in performance is also to be expected- it's very rare that a real-world machine learning problem can be perfectly solved, and a perfectly solved problem would be the only type that didn't reach a plateau in testing and validation performance (before it eventually did reach a plateau at 100% accuracy). Think of the plateau as the maximum performance that can be achieved given the particular parameter values, features, and architecture of the solution. In order to achieve performance beyond your plateau values, one of these considerations will need to be adjusted. 

Generally, I would refer to this as transfer learning or network adaptation. That is, taking a network that has learned useful features from one domain and adapting that network and its developed features to another domain. That said, there appear to be many sources that closely conflate fine tuning with transfer learning. Therefore, I would say the difference in terminology is primarily opinion-based and suggest closure of this question on those grounds. 

For your particular problem I'm not sure that using a supervised Logistic Regression approach is ideal, but I suppose that is a different and larger topic. To answer your question, yes you can use a "bag of words" representation of your text. Python's library offers this functionality via both and . This will result in a sparse matrix representation of n-grams and occurrences of those n-grams in your corpus. From here you basically have 2 options, (1) training your supervised model directly on the sparse matrix or (2) reducing the dimension of your sparse matrix so it be represented as a dense matrix. Luckily sklearn offers functionality for both these, their class supports sparse matrices and their implementation of PCA/LSA supports sparse matrices as well. That should give what you need from a technical perspective to build a model, but I think the real question is what feature engineering you may be able to do in addition to simply training on the bag of words representation of the text. 

All three approaches are valid and have their own pros and cons. Personally, I find (1) to typically be the worst because it is, relatively speaking, extremely slow. I also find (3) to usually be the best, being both sufficiently fast and resulting in very good predictions. You can obviously do a combination of them as well if you're willing to do some more extensive ensembling. As for the algorithms you use, they can essentially all fit within that framework. Logistic regression performs surprisingly well most of the time, but others may do better depending on the problem at hand and how well you tune them. I'm partial to GBMs myself, but the bottom line is that you can try as many algorithms as you would like and even doing simple weighted ensembles of their predictions will almost always lead to a better overall solution. 

No- not if your regex is actually generating the data. If this is the case, the regex (or a complementary one) should be able to classify the resulting text perfectly. That is, you know what the rules for creating a 'happy' or 'sad' piece of text are within the regex, and so you simply apply those rules backwards to classify it. So, if you take what the regex says when it creates the data as ground truth, then there's no reason to create a machine learning classifier to classify it- the creation mechanism itself already does so. On the other hand, if you have created a regex that will produce "happy" or "sad" tweets, and you want to know if the regex itself or a machine learning algorithm will better conform to the opinions of, say, human observers, you would have a very interesting question to evaluate and a relatively straightforward test case for sentiment analysis. 

This should allow you to use all cores of all CPUs. This can, of course, also be done in Tensorflow: 

There are many measures which implicitly take into account the confidence of a prediction. One very common one is Log Loss (also called Cross Entropy). $-log \space P(y_t|y_p) = -(y_t \space log(y_p) + (1-y_t)\space log(1-y_p))$ Using this metric, confident correct classifications are rewarded more than relatively less confident correct classifications, and confident misclassifications are heavily punished. Any proper scoring rule meets this criteria when applied to a classification problem. Some others examples are: 

And clearly if you wanted to selected based on some other criteria than "top k features" then you can just adjust the functions accordingly. 

If I understand correctly, you essentially have two forms of features for your models. (1) Text data that you have represented as a sparse bag of words and (2) more traditional dense features. If that is the case then there are 3 common approaches: 

Perform dimensionality reduction (such as LSA via ) on your sparse data to make it dense and combine the features into a single dense matrix to train your model(s). Add your few dense features to your sparse matrix using something like scipy's into a single sparse matrix to train your model(s). Create a model using only your sparse text data and then combine its predictions (probabilities if it's classification) as a dense feature with your other dense features to create a model (ie: ensembling via stacking). If you go this route remember to only use CV predictions as features to train your model otherwise you'll likely overfit quite badly (you can make a quite class to do this all within a single if desired). 

Not significantly better. It looks like we'll have to change the architecture of the network (more layers, more neurons, and/or different activation functions) to improve beyond this point. To inform this architecture change, let's take a look at the sigmoid activation function: 

So, with careful training, our network with a single layer of sigmoid activations appears to be learning to generalize the sin curve. Further investigation could find a limit to that generalization, certainly. For reproduction: 

Yes, it is possible to have a situation in which validation accuracy cannot be as high as training accuracy. Any situation in which noise (as opposed to generalizable properties of the feature set) in the training set is more predictive of the target variable within the training set would produce this. Consider a situation in which a random property of the training sample considered is perfectly predictive, but this is found not to be true of all other examples outside the sample. The predictive power in the training set would be perfect, but outside the training set, less than perfect. This phenomenon is broadly referred to as "overfitting". For example, let's consider a case where you have a set of fruit data and you're trying to establish whether a given fruit is an orange or a tangerine. You have 4 features- the circumference of the fruit around the stem-medial axis, the height of the fruit across the stem, a numerical value of the hue of the fruit skin, and the first letter of the amateur baseball team whose home park is closest to the field the fruit was grown in. Let's imagine that by some baffling coincidence, the baseball team letter in the training set was perfectly predictive of whether the fruit would be a tangerine or orange. We can imagine that this would not hold true across the country or the world, which would produce a situation where the training set accuracy would be perfect, but the validation set accuracy would not be able to approach that using the same methods. 

Doing things this way will emulate real life since your predictions will be gathered using models that did not include that data when training them. You can build your final model using all of your prediction variables as input. If you struggle with actually executing on this either edit your question with code or post a new question on how to run everything with a CV loop. 

Standardizing data is recommended because otherwise the range of values in each feature will act as a weight when determining how to cluster data, which is typically undesired. For example consider the standard metric for most clustering algorithms (including DBSCAN in sci-kit learn) -- , otherwise known as the L2 norm. If one of your features has a range of values much larger than the others, clustering will be completely dominated by that one feature. To illustrate this look at the simple example below: 

From this you should see that the euclidean distances between the non-standardized versions are dominated by the third column because its range of values is much larger than the other two. However when the data is standardized this no longer becomes an issue and weights each feature as being equal when calculating the distance between each data point. 

Keep in mind that the Accuracy measure is measuring whether the values are Exactly The Same. I.e. it is a classification measure, whereas approximating a sin curve is much better suited for measurement as a regression problem. That said: In evaluating the network's performance, what is the network actually doing? Let's take the network and perform a little visual analysis on its performance: 

Uh-oh. The value of this sigmoid function can only ever be in the range 0:1, and the range of the sin function is -1:1. To correct this, let's just normalize the sin response between 0 and 1: 

For this example specifically, I would suggest visualizing the data using a Chord Diagram. Chord diagram from Delimited.io: 

Obeservation predictions falling into the green segment could be classified as positive examples, and predictions falling into the red segment would be unclassified. This approach would be sufficient for Naive Bayes. Some other approaches (SVM, gradient boosting machines etc) may benefit from a custom loss function definition, in which you define a function that disproportionately punishes false positive predictions. Something like: $(y_i = 0) \rightarrow (d = s)\wedge (y_i = 1) \rightarrow (d=1)$ $L(p) = \frac {\sum_i\frac{(p_i-y_i)^2}{d}}{n_i}$ $L(p)$ = loss function $p_i$ = predicted class likelihood $y_i$ = actual class (0 for negative example, 1 for positive) $n_i$ = number of observations $s$ = Penalty parameter for false positives. Would heavily punish false positives relative to false negatives. It can also be adjusted to your needs, to more or less heavily punish false positives. For $s = .5$, the function looks something like: