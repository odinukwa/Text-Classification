If there is no divergence (i.e. all threads in a wave take the same branch) newer GPU's can skip all the work within the if-branch. If there's divergence, then code in both branches is executed, but thread execution mask basically defines which threads execute code in which branch (code in non-executed branches for threads are effectively NOPed out). This is basically the same as predicated branching but happens dynamically based on divergence. On GCN architecture at least the branching itself is basically free or at least very cheap (handled by a separate unit running parallel to ALU for example), but something to keep in mind is that branching also tends to increase GPR pressure, which in turn lowers the occupancy. Low occupancy means there can be less threads in flight at once which influences GPU's ability to hide memory latencies. This may or may not be an issue in your shaders depending on how they access memory, i.e. you should see performance increase along with increased occupancy in shaders with heavy memory access. So it's still good to optimize out branching where reasonable, but I think it's one of the things good left to the optimization stage where you need to squeeze max performance out of your shaders. 

In general the two directions in BxDF are incoming $\omega_i$ and outgoing $\omega_o$ radiance directions, often defined in spherical coordinates $[\theta, \phi]$ or as a 3D unit vector in Cartesian coordinates $[x, y, z]$. The BxDF $f(\omega_i, \omega_o)$ defines how much of the incident radiance $L_i$ from direction $\omega_i$ hitting the surface is scattered towards direction $\omega_o$. What $\omega_i$ and $\omega_o$ exactly represent depends on the context. Often $\omega_i$ represents the direction of a light source, but it could also be the direction of a surfel reflecting light or some other light emitting/scattering element in space. $\omega_o$ often represent the direction of the viewpoint (camera/eye), but it could as well be the direction of a surfel or other point in space whose incident radiance we are interested in calculating. While $\omega_o$ is valid for normal oriented hemisphere above the surface, there are differences in valid $\omega_i$ domain between BxDF's. Because BRDF defines the amount of reflected light $\omega_i$ is valid for the normal oriented hemisphere above the surface. On the other hand, BTDF defines the amount of light transmitted through the surface so $\omega_i$ is valid for the normal oriented hemisphere below the surface. BSDF is the sum of the two so $\omega_i$ is valid for the entire sphere. 

// access part of a framebuffer's state. For reading the default framebuffer, this is or . However, the documentation doesn't say what it is for FBOs. The only thing I found was an offhand comment here stating that the default is GL_COLOR_ATTACHMENT0 (which is what I would guess). Can I rely on that? For writing, I didn't immediately find any claims what what color buffers are enabled. 

Where does "face" fit in? Is my "layer-face" description correct? Or maybe it refers to all mip levels instead? Is my description of a "layer" correct? Similarly, does a "mipmap chain" refer to all mip levels of a single face, or to all mip levels of the layer? 

I've done some VR research; this comes up a lot since rendering the scene multiple times (especially at predicted VR resolutions) is expensive. The basic problem is that two views provide more information than only one. In particular, you have two slices of the light field instead of one. It's related to depth-of-field: screen-space methods fundamentally are incorrect. There has been some work in this area, most related to reprojection techniques that try to do some kind of geometry-aware holefilling in the final image. This sortof works. As far as I know, the best approach so far is to render the scene directly for the dominant eye, and then reproject it to the other one. 

A note first From the look of your screen capture, I suspect there might still be a bug in your code. Noise is to be expected with only 16 spp, but your picture still looks surprisingly dark to me. For comparison, here is what my implementation of SmallPT looks like with 16 spp, 15 bounces, and no next event prediction: 

While browsing to properly write my question, I actually found the answer, which happens to be very simple. Another Fresnel term is also going to weight in as the photons make their way out of the material (so being refracted into the air) and become the diffuse term. Thus the correct factor for the diffuse term would be: $$(1 - F_{in}) * (1 - F_{out})$$ 

I have no knowledge of the literature on the topic, but I did something very similar to what you're asking some time ago: I wanted to generate lathe meshes and bend them according to a spline. I think the same technique could be adapted to your case quite easily. First you would need to define what your default axis is: if the input mesh corresponds to the case when the spline curve is a straight line, where is that line in 3D? It could be defined arbitrarily as the Y axis, the PCA of the mesh, or some manually defined axis. The algorithm is then: 

I've had great difficulty with a literature search on this. Here are some of the (few) papers I found: 

Interpretation 1: Render an image that looks perceptually realistic. At the end of the day, your image still needs to be displayed somewhere. Here's the key: you want to render your image in such a way that when you *display* that image on a particular display device, it will produce the same sensation the original radiometric image would have produced. Here's how to unpack that idea. In the real world, radiometric spectra (i.e., real distributions of light) enter your eye and stimulate approximately1 four light receptors. The stimulations of the receptors produce the sensations of color we associate with images. In rendering, we don't have arbitrary control over the spectra we produce. Fortunately, since we (usually) have only three cones, each of which produces only a scalar value, color vision can be reproduced by using exactly three primaries. The bottom line is you can produce any color sensation by using a linear combination of three wavelengths only (up to a few colors that might have to be negative, in which case, you just use different primaries). You don't have a choice of primaries. Almost all color display devices use the sRGB standard, which provides three primaries (which actually usually don't have a single wavelength). That's fine because it turns out it's all abstracted and you don't have to care. To clarify the mess that is perceptually accurate rendering, here's the algorithm: 

Reduce shading when possible Lens distortion Part of the NVidia VRWorks SDK is a feature, Multi-Res Shading, that allow to reduce the amount of shading, to take into account the fact that some pixels contribute less to the final image due to lens distortion. In Alex Vlachos' GDC 2015 presentation, he also mentions the idea of rendering the image in two parts of different resolution, to achieve the same goal. Foveated rendering Finally, like your question mentions, foveated rendering aims at scaling the resolution to take into account the fact that eyes have more precision in the fovea and less in the periphery. There is some research on the topic, but this requires eye tracking like in the Fove HMD. 

A diffuse shading with a sharp transition between lit a unlit, that happens somewhere below 0 so the light leaks behind a little. The rim lighting that, as you mention, has an almost constant screenspace width. From the look of it and some of the artifacts, my best guess is that it's actually some edge detection filter based on depth, which is then used in combination with the diffuse lighting so the rim lighting doesn't affect the unlit parts. 

You cannot do something like that. is a 3x3 matrix: without a 4th column and a non-zero homogeneous coordinate, you can only transform directions, not positions. You need to first get the camera direction relative to the vertex, then transform that direction.