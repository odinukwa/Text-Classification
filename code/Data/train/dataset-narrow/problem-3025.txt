As specified here, $URL$ i stored the coef and intercept of my first model. Later, i am passing them as initializers to my second fit() as shown below for learning new data on top of old model. 

As we observe, the test data [2.,2.] is not in the train dataset we passed. Still, we got the closest match as label 1. What i am trying to find is if the test data i supplied is not in the train dataset, i should print a message to user that data is not valid instead of telling him the wrong label as 1. For instance, in knn classification, i have kneighbours function which tells the distance of my closest neighbours to the test data i supplied in a 0 to 1 scale. So, i could easily eliminate the test data samples which are highly distant from my train data samples by keeping threshold at 0.6 or 0.7. Is there any criteria/threshold like this i could do with MLPClassifier or with any one of Incremental Classifiers mentioned here which can restrict my test samples if not present in train dataset ? Question migrated from SO 

Your example is cherry-picked: You mask out small numbers and keep a large one. But dropout is applied randomly. Each of the following six masks, and of the corresponding values for the vector length, is equally likely to appear: $$ \begin{align*} &(1, 1, 0, 0): &\sqrt{0.1^2 + 0.1^2} &\approx 0.1414,\\ &(1, 0, 1, 0): &\sqrt{0.1^2+ 0.2^2} &\approx 0.2236,\\ &(1, 0, 0, 1): &\sqrt{0.1^2+ 5^2} &\approx 5.0010,\\ &(0, 1, 1, 0): &\sqrt{0.1^2 + 0.2^2} &\approx 0.2236,\\ &(0, 1, 0, 1): &\sqrt{0.1^2 + 5^2} &\approx 5.0010,\\ &(0, 0, 1, 1): &\sqrt{0.2^2 + 5^2} &\approx 5.0040.\\ \end{align*} $$ The average vector length is $$ \frac16 (0.1414+0.2236+5.0010+0.2236+5.0010+5.0040) = 2.5991, $$ which is roughly half of the original vector length $5.006$. So it makes sense to divide it by the dropout rate of $50\%$. 

K-means is a reasonable approach and a sensible way to understand the data. I've never used mahout, but I would use R or Python for this sort of analysis because of the nice libraries available to quickly implement K-means. The clustering approach with the tags is fairly straightforward. You can essentially encode this using an indicator variable (also known as a binary encoding). You can set this variable/feature to 1 if the tag appeared in the list of tags and 0 otherwise. Then you only need to allocate space for the total number of tags that exist. If you have a large set of tags, you can limit them by taking tags with at least some frequency or some other "sensible" way. You can choose $k$ in a number of ways. Typically people choose K arbitrarily because they want, e.g., 10 groups to segment their customers or data into. In the simulation I've provided, it'll give you a lame way to optimize for K using an incremental improvement. 

Of course, if you have 40,000 items, that would probably require additional work to make the plot comprehensible. You could begin by ignoring all connections between nodes with a similarity value of, say, 0.01 or less. 

For the given decision function and , we get a prediction accuracy of 84%. This is not terribly good, and it is of course specific to our decision function, which seems fairly easy to predict. To see this, define a different decision function: 

Have you tried the boring, straightforward approach? Get a list of all words and count how often they occur with a high or a low label. (Exclude words that occur only once or twice, and also the words that occur very often). For example: 

In this simulation, we see that the performance of the model slightly worse, but fairly similar, from including the clusters. 

I've made a notebook with a simulation walking you through how to "tokenize" your tags and represent them with a binary/one-hot encoding. It's worth noting that this tokenization ignores the order of the tags, which may be okay for your use case. It's also worth noting that K-means certainly isn't the only way to measure the similarity of your data but I think it's a nice intuitive start. Again, for choosing $k$ the approach outlined in the notebook is exhaustive, since it starts from 1 and goes until each observation is a cluster. This means you'll have to run K-means $n$ times, which is silly in practice to do but useful from a learning perspective here. In general, this isn't ideal because it's expensive and typically you don't want to set $k = n$ but this simulation gives nice intuition about what's happening. In practice, you can just do it in gaps for a large number of clusters (e.g., 5, 10, 15, 20, .., 100) or something like that and choose the one that has the biggest drop-off by eye-balling it. This is a very arbitrary and unsatisfying way to choose $k$, but it seems to work okay for many people. 

You could use a two-sample $t$-test to determine whether software usage is significantly lower for lost users. Plotting 'software usage rate'/'software usage rate relative to country average' against 'percentage of lost users' might give you an idea about a suitable threshold. 

Interesting question! Maybe the pretrained models in Keras can help. Either by means of transfer learning, so that you might have to label only a small number of images by hand to retrain the higher layers. Or by using them for feature extraction and see if a certain keyword appears frequently for watermarked images. Or just upload the pictures some place that does not allow watermarks and see if they get flagged ;-) 

How to improve my prediction result of partial_fit() to match with fit() ? I want to learn instance by instance and still predict accurately. I tried with different number of iterations but it didnot work. 

I am using sklearn SGDClassifier. I created one model with old data. I want to use the results of this old model when i create a new model. The reason for doing this is i want to combine both the models results/retrain old model when a new data arives (not from scratch) I read various posts that its easy to do this retraining. However, i can't get this working. Maybe i am doing it in wrong way. Please someone help. And ideas/hints are much appreciated. what i tried: setting warm_start = true and passing the coef and intercept of old model to the new one as below:(However this seems to forget the old data and learn only from new data.) 

Media's explanation is true for regression problems. These are problems where you predict a continuous target variable. Your image shows a classification problem. Here, the target variable takes only two values (typically and in the Perceptron algorithm). In that case, an optimal solution $w^*$ is a vector of weights that perfectly separates both classes. If such a solution exists, the Perceptron algorithm will find it. But: If there is one optimal solution, there are usually infinitely many other optimal solutions. You can easily see this in your image: You can move the line a little to the left or to the right, and you can rotate it a little, and it still perfectly separates the classes. So while the Perceptron algorithm will find an optimal solution if there is one, you cannot know which one it will find. That depends on the random starting parameters. This is different e.g. for support vector machines. Here, there is either no optimal solution or exactly one optimal solution. 

AUC is based on rank order of your predictions, not the actual class to which it's assigned. It's very likely that the scale of the output is misbehaving. Look at the values of your predictions, I suspect that the predictions of your model are within a tight range. If that's the case, the argmax will yield the same class for all of your observations (which is what's happening). You may wish to tinker with some of the hyperparameters to see which one is causing this exactly (might start with the learning rate). It's probably worth testing if a logistic regression gives you the same problem, which will help identify whether or not it's a problem with your inputs/features. 

In most cases, I would go for NumPy. Implement a Python function that calculates the $t$-th summand. Then run 

Memory corruption seems to be an issue that is important enough for companies to buy expensive ECC memory for their clusters. The Wikipedia article on ECC memory lists some causes for memory corruption, including (to my surprise and delight) cosmic rays and ingenious hackers. 

You need to remove the dependent variable before performing PCA. Otherwise you are essentially using the dependent variable to explain itself. Also, think about prediction: You might want to use your model to predict unknown GPA scores based on information about amount of alcohol, amount of study, IQ and SAT. In that case, it is impossible to include information on GPA scores.