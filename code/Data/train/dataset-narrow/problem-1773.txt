Before you can run the OpenSSH server, you must install it, but you haven't actually installed it. To fix the problem, install the OpenSSH server. 

You have a typo in your configuration: Your device name should be , not . This applies both to the filename, and to the device name within the file. The former indicates VLAN 10, the latter is an old-style interface alias name which has been deprecated for years and is no longer supported in CentOS 7 (and in particular doesn't indicate a VLAN). 

Have you tried restarting ? should restart it during the upgrade, but sometimes it gets the timing wrong. 

When storage is provided by a remote server across a SAN, you can only discard blocks if the storage is thin provisioned. If the storage is thick provisioned, it is always the same size, and discard makes no sense (and thus isn't usable). 

By default VirtualBox guests will sync their time to the host. If you want to change this behavior, see the documentation. 

This system is using almost all of its memory, so it has little room left for memory for new VMs. To improve memory usage in KVM, make sure that each Windows guest has the virtio balloon driver installed (Linux already includes this driver), and enable ksm on the host to deduplicate virtual machine memory. 

Note that here we are not actually erasing the packages, but only the rpmdb entries that tell that the packages are installed. At this point it should be possible to update the system, with 

It's not necessary to append the args, as WordPress picks them up from when it is available, (almost always) and when they aren't present in the query string. Yes, I'm aware that the statement you are using is what's "recommended" on the WordPress site. Of course it's also a wiki that anybody can edit, so it has to be taken with a grain of salt, just like Wikipedia. 

To be sure, you should use the EICAR test virus, and attach it to the email, rather than simply putting it in the body. 

For squid, set up SSL Bump and dynamic SSL certificates, and be sure to add your new CA to your users' web browsers. 

I'm going to recommend strongly against using 502 for this. Its semantics indicate that something is wrong on the server side and that the request would succeed if tried later. 

and are obviously different hostnames. And Apache will substitute this for when looking for the files to serve. So the problem is that doesn't exist. One way to fix it (assuming these two sites are supposed to serve the same data) is by a simple symbolic link: 

You should expect a service to have an SSAE 16 audit annually. While this isn't a strict requirement of the standard, regulatory requirements change, new technology is added to the environment, etc. An out of date report may not be useful. 

Many of us use the remi repository to keep current with PHP and MySQL. It will currently give you PHP 5.4 and MySQL 5.5 on EL5 and EL6. All you need to do is install the RPM and then . 

I'm going to presume you've given enough information and not left out anything important in the nginx configuration information you chose not to share. With that caveat in mind... 

The secret here is that simply creates as a copy, hardlink or symlink (a symlink is preferred) to a file in . So it is possible to do this entirely from your Dockerfile. Consider: 

It appears that chkrootkit doesn't like your Node.js installation since it has a large number of hidden files. Most of those look normal for a Node installation to me, though. The Python ones don't look normal, but that could just be because you're using Debian. Check into those. As for the iptables problem, you are at the mercy of your VPS provider there. Since OpenVZ and Linux-VServer use a shared kernel, you can only use iptables if the provider loads it for you. In particular, Linux-VServer has very limited or no support for iptables in guest containers. I hope that by now you have moved away from that crappy OpenVZ based VPS that you were on. That's certainly the root cause of all the problems you have been having. 

You will need to remove this statement if you want to receive anything other than a 404. Further, you will need to place a nested within that to process PHP files. 

You'll see multiple packages here. The one you're looking for is the one that contains as that is the standard include directory. So install the indicated package: 

Wireshark doesn't automatically treat this connection as HTTP because it was on TCP port 5000, which isn't a commonly used port for HTTP traffic. To have Wireshark interpret it as HTTP traffic anyway, right click on one of the response packets, choose Decode as... and then select HTTP from the list. 

There are a LOT of variables that could cause it. Best thing to do is: First, wait for the next poll. A graph can't be generated until there are at least two data points, and NaN will always show after the first poll. Second, go through the whole debugging process in the Cacti documentation. 

I wrote a simple script for this a long while back. It's less than perfect and has a few failure modes, but does well enough for casual inspection. I've never been bothered to improve it, but perhaps someone else will. 

Since you say that worked for you, I suspect something non-obvious was broken with your installation. At this point you may never find out what it was. 

Locking memory into RAM prevents the kernel from swapping it out. This isn't often done, but for certain performance-critical stuff it is quite useful. Databases are one of those things that can make good use of this. Setting this value to 40000 makes no sense. Even if you could do it, it would cripple the database and probably cause it to crash. MySQL locks memory when large page support is enabled and the documentation states that memlock must be unlimited ( in your systemd unit). 

You are using FastCGI, not proxy. So you need to change all the requests to their equivalent directives. E.g. instead of , and so on, for all of them. 

If it still fails, the problem is probably with the mirror, and not your system. In that case, just wait 24 hours so that hopefully the mirror gets back in sync. 

When aliasing directories, make sure that the paths in both and its corresponding have a trailing slash character. 

Are you SURE you restarted postgresql after this? The configuration is correct, but it's still bound only to localhost. 

Restart the computer. (On one machine it required several restarts to actually get everything deleted from this directory, so keep trying if necessary.) Run Windows Update manually again. It will fail almost instantly and offer to run a diagnostic tool. Download the tool and allow it to run. The tool will find and fix some problems. At this point, run Windows Update manually again. Windows Update worked fine at this point. 

The actual values will be supplied by your ISP, hosting provider or datacenter. Or you can use public DNS providers such as Google Public DNS. 

That should be . The leading is missing. I'm not sure exactly what the behavior is when that happens, but it's clearly not what you want. 

rlwrap is a wrapper for GNU readline, so it's doing everything locally and only passing your input through after you press Enter. When you press Ctrl+] you don't see the prompt because readline has not yet sent your input. To kill your telnet connection, then, press Ctrl+], then q, then Enter. 

This is a really bad idea, since it is going to take you off the security/bug fix update path. If you decide to do this, you'll have to purchase Extended Update Support in order to get supported security and bug fix updates. If at all possible, you should update to the latest minor version. (They are roughly comparable to Windows service packs.) With that out of the way... You may be able to use the create-release script to create a channel for a specific Red Hat release. However, this script doesn't seem to have been updated in a couple of years, so it may not work for RHEL 6. Another option is to download the DVD for the specific release you want, and use it as a local yum repository. Finally, you can call Red Hat and take advantage of that support you're paying insane amounts of money for. 

The only thing you are missing is to quiesce the guest filesystem before taking the snapshot, to ensure that it is consistent. This can be done with if you are using libvirtd. For example, the order of operations is: 

You need to have both ports configured, as they serve two different purposes. Port 587 is the submission port; it is intended for email being sent by end users from their desktop applications (Outlook, Thunderbird, etc.) to others. End users will set port 587 as their SMTP server port in their email programs. Traffic on port 587 is required to be authenticated, and should be encrypted with TLS. Port 25 is the historic SMTP port; it is used to relay mail between SMTP servers until it reaches its final destination server. Since this traffic may originate from anywhere on the Internet, it does not have to be authenticated or encrypted, but it may be. (There is also a port 465, which is SMTP wrapped in SSL, but it is almost never used, since TLS became the common standard and works on both port 25 and 587.) The reason port 587 exists at all, of course, is spam. Compromised computers on residential ISP connections often run malware which sends out large quantities of spam directly to port 25 of the recipient domain's mail servers. In response, since the submission port was standardized, many ISPs now block port 25 connections from end users to cut down on spam. 

You won't see this particular kernel version outside of Red Hat. It was an internal kernel build and not distributed to the public. As you can see, this bug is not yet marked as fixed. It is clear from the comments that Red Hat is still testing it internally. If you need this fix urgently, you'll need a Red Hat subscription and to open a support case with Red Hat. Otherwise, you can wait for the fix to be released, at which time it will also become available for CentOS. 

If your version of nginx shows TLS SNI support when you do then you're ready to go. If you want to run your without regard to the IP address, then don't use an IP address in the SSL web 's directives to use SNI for that virtual host. For instance, change: 

Finally, restart auditd. I make no guarantees about what will happen on the next boot. You will need to test this and see what breaks. :) 

What Tor does do with the authentication information is stream isolation. Tor can be configured to use completely different circuits for clients which "authenticate" with different credentials. 

Since nobody has yet explained what is going on: Apache cannot bind to the local IP address you gave, because the computer is not configured with that IP address. There are a couple of ways to solve this: 

will look for in that corresponding directory, when given a URL ending in . If you used instead, then nginx would generate a directory listing for that directory. 

Rather than logging every dropped packet, you can configure firewalld not to log broadcast or multicast packets such as the one you've given as an example in your question. To do this, use . 

Add a PTR record to your intranet's DNS server(s) specifying the desired hostname for that IP address. For instance, for a BIND zone : 

You're not missing anything; you configured nginx correctly. The problem here is that WordPress thinks it should be generating HTTP links. When you try to access it via HTTPS, it still generates all of its links with and thus they can't be loaded (without explicitly loading insecure content in the browser). There are WordPress plugins that will convert your admin console to https, if that is what you are trying to do. You could also edit your blog's General Settings and convert the entire site to https quickly and easily, without a plugin. 

I manage a number of different web sites for various clients, and this is generally what I do. First, the top priority for security updates should be the web application itself. The vast majority of attacks will be targeting your web site and the code that runs it, and this needs to be kept secure. If you use off-the-shelf web applications, this is one place where I would even consider automatic updates, but in no case would I let a security update for something like WordPress or Drupal age more than 24 hours before testing it and most likely rolling it out. If your web app is custom built, ensure that your developers are keeping on top of security issues, to whatever extent that you can do so. This is a scenario in which your organization should be doing DevOps to ensure that the web developers and IT are working well together and that issues are resolved timely. After that, the next critical updates I consider are those that "go viral" and that you hear about on national news, such as Heartbleed, POODLE, etc., as well as updates to anything in the critical path, like nginx and PHP for Drupal and WordPress sites. I apply updates for these as soon as they're available. I also am on the mailing lists for the upstream packages (e.g. I am subscribed to openssl-announce) so that I get notification for really important things as soon as possible. Next, I apply updates to every public facing server (web front ends, load balancers, etc.), and every supporting server (databases, etc.) once a month. This includes any leftover security and bug fix updates. In many organizations this is done quarterly across the board, but it's my opinion that public web sites, especially those doing e-commerce, should not be left to rot for that long. I almost always do this the weekend after Microsoft's patch Tuesday, which is almost always enough time to hear about bad Microsoft updates (though I mostly run Linux systems, it's easier to have one weekend to update everything). Finally, I sit back, relax and wait to see what breaks. Despite your best efforts at testing updates, something will eventually go wrong. Watch your monitoring system. If something breaks that isn't being monitored, fix it and then start monitoring it. Be prepared to rollback ( is helpful).