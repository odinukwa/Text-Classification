since the general representation for a plane is . To get a vector from the plane to the vertex, you'll first have to find the perpendicular signed distance from the plane to the vertex and multiply it by the normal. The formula for this is where n is the normal of the plane and A is a point on the plane and B is the input point. Using the above representation of a plane, this formula becomes . So in this case the signed distance will be 

You could try Boolean polygon intersections, or polygon-polygon clipping. Both can be looked up easily, thus I'm not going to talk about their implementation. IMO polygon-polygon clipping is easier though. The algorithm is as follows: First compute the area of the target polygon. Next clip (or intersect) the user-handled polygon by the target polygon, that is the target polygon is the clipping polygon, and the other polygon is the clipped polygon. Compute the area of the polygon returned by the above operation. If it is equal to the area of the target polygon, it is a complete fit. The percentage is the above area divided by target polygon area * 100. 

You can do any interpolation or tweening or whatever that you want there. As a sidenote, in almost all cases converting away from radians seems like a bad idea. If your question was specifically about how to figure out which way to rotate. 

I can't see anything fundamentally wrong with the shader, but here are a few things I find commonly done wrong with deferred shading that you might be doing. 1: Drawing full screen lights. The beauty of deferred shading is that you can pack your lights into geometry so that you only need to consider a part of the screen when drawing them (Like a cube with 2 times the radius as size). If instead you draw a full screen quad for every light, that will have a serious performance impact. 2: Render target depth. You are moving a lot of data around in deferred shading, so you have an interest in making the footprint of that as small as possible. If you can reduce the number or size of your render targets that can have a pretty big performance impact. 3: Rendering one light at a time rather than batching them. (Edit: You can have a static vertexbuffer and indexbuffer containing vertices for the max number of lights you want to ever render and then just patch the position and color of the lights that are active) And lastly, you can always look at the assembly of your shader and see how many cycles it will take, so that gives you an easy way to see your own performance and compare. GPU Gems has 2 or 3 great articles on deferred shading and the performance and other issues that come with it. 

If you're talking about the scene hierarchy, it's not a good idea. The scene hierarchy is meant to be designed in such a way that all objects that need to move together/are attached to each other should be placed as children of a parent game object. For example, in a car object, the body and wheels are placed together under a parent "Car" object, since they generally move and rotate together in a real-life car. This set up allows the wheels to move and rotate in the same direction as the body when they aren't steered, but when they are an offset rotation is applied. The wheel still moves and rotates with the body, but it has some offset local rotation/position which is also added to it's world position and rotation. You can also use parent and child Transforms to group objects for easy debugging like @SanSolo said, but only if you are sure that you won't change the Transform of the parent container in any way. 

The short answer is yes. The long answer is that the specifics of how to do it massively depends on how you handle drawing your characters. If you use skeletal animations (like when using spine), then you can easily exchange bits of your character by just replacing the texture of it. (Since in skeletal animation systems your character is separated into parts anyway). You can do the same thing when you use sprite sheets too, but it is slightly more complicated. You already had the right general idea in your post. You split the animation up into the parts that you want to combine and you need to remember a draw order for all those parts for each animation. But then you can draw each frame of an animation easily by just combining the parts together. The drawback is that you need to make a full set of animations for each part, since every hat for example has to move the same in every animation, otherwise some combinations will just look weird. tl;dr: It is definitely possible and it is much easier if you use skeletal animation. 

With this formula, the resulting vector will point in the direction of the normal if the point is above the plane and opposite the normal if the point is below the plane. 

The path returned by this function starts at "start", i.e. and ends at end, i.e. . This means that the next cell to move will always be provided that the path is recalculated every frame (it should, if the target is/can be moved every frame). Why to do something so complex for something so trivial, you ask? This makes it all the more easier to compute paths and this method doesn't consume many CPU cycles. Plus, if you add obstacles this will still work, because the cells with obstacles will not be returned as neighbors, thus they won't be traversed at all. Look at this link for interactive examples of how BFS works with pathfinding. I have taken the BFS function pseudocode from that link, but the explanation is based on my implementation for a turn-based game. 

I think your issue is that you pathfind towards the players tile rather than to a tile that he can be attacked from. Since there is no actual path to the player in that case, since diagonal movement isn't allowed, of course they can't find a path. If you just want them to get closer you can put a maximum on the number of tried nodes and always move towards the minimum of your heuristic.(In which case I'd recommend the manhatten metric) Or you could change your metric a bit so that it's 0 when you're at a distance you can attack from. 

Genres are a way to group games together that share similiar elements, wether they are game mechanics or specific story elements or just perspective or setting. In some genres games have to have specific mechanics. An FPS has to be first person and has to have a shooting mechanic, hence the name. Mechanics are basically actions that the player can take and their consequences. So if you match 3 and they disappear, that's a game mechanic, if you jump on an enemies head and the enemy takes damage, that's a game mechanic. So to answer your question, genre can describe many things, it doesn't have to be the setting (see FPS). So it doesn't directly relate to game mechanics. A genre can imply certain game mechanics. But other than that there isn't really a connection. Hope that explains it well enough. 

You're right, it does have to do with the center of mass and the inertia. The resultant force is simply, the original force plus the force applied. The resultant torque is equal to the original torque plus the cross product of the force vector with the radius (the vector from the COM to the point of application of force. This holds true for both 2 dimensions and 3 dimensions. All in all, the simple AddForceAtPosition function will be: 

First sort all the intersection points either clockwise or anticlockwise around the origin. There are many answers here on gamedev that will help you out in doing that. Just add one more condition that if two points are on the same line (angle is the same), the less distant point will be pushed to the array first. Once you have done that, finding each triangle is easy. A triangle is made of two adjacent points in the sorted array and the origin point. If the points are collinear (), then don't render the triangle. NOTE: From your second image, it can be seen that your ray intersection test doesn't handle all vertex cases correctly (maybe due to floating point errors), which I think you should fix. 

My best guess is that you have some rounding errors. You aren't actually showing the movement code for the bullets so it's really hard to tell. Edit: Looking at the code, it should(tm) work. It definitely works fine to do things like this in the games I've written with XNA. The thing that bothers me is that your game doesn't seem to be running at all when you set FixedTimeStep to true, so something really weird must be going on somewhere. Could it be that you didn't separate Update and Draw logic properly? Edit2: The TargetElapsedTime is set to 16.666 seconds instead of milliseconds, so that's most likely causing the issues. 

Fourth there is also something to be said about the effectiveness of lists, especially in a garbage collected environment, but that varies a lot depending on implementation details. Fifth You can handle your collision phase differently if you don't care if objects intersect for a few frames, by just letting them gardually push each other out by applying force. It looks like you are trying to guarantee zero intersect, which you are currently not doing. (An object can be moved out of the first collision and then back into it when resolving the second collision) As a last sidenote I think that simulating n-body attraction is just always going to be a performance nightmare, so in some way you don't really need to worry about this rest, since that's what will most definitely ruin your performance. (Although space partitioning can also aleviate that a bit since you can only check with objects that are close enough to matter) I probably didn't notice everything but I think that's the jist of it. 

In 2D the cross product of two vectors is a scalar in the Z direction, whose value is governed by this formula: 

Add the line after the in the coroutine instead. In your code this line is executed immediately because a coroutine is executed over multiple frames based on what it yields. Whatever is after the statement will be executed after seconds, but while the coroutine is waiting control is returned to the coroutine caller so that it can continue execution and isn't blocked. So this statement is executed immediately, since while coroutine waits, control is returned to and it executes the last statement. 

For the highlight, you can use an Orthographic projector to project a round (or whichever) texture. The X and Z position of the projector will be the same as the enemy's X-Z position, and the Y position will be . 

This returns a dictionary which shows how we arrived at a cell. To obtain a path from this dictionary, just traverse the dictionary till you find the start. Pseudocode: 

There is no one size fits all answer to this, as is so often the case. Generally though, if you can precompute it, do it. If your robot doesn't change all the time, so for example you build it and it stays the same the whole time, then what's the point of calculating all of its parts every frame. But if it does change all the time then obviously you can't get away with making all of it one big object. You are probably aware that there are tons of things that become computationally more expensive the more objects you have, for example collision detection. So you have an interest to keep the number of game objects low(-ish). When it comes to rendering, ideally you would want to have a VBO and IBO for your whole robot, so you can push it over to the GPU once and then just reference it forever. If instead you split your robot up into bits you will have to make a draw call for each of the bits. While the amount of drawing that your GPU ends up doing will be the same, the amount of work for your CPU to translate all of those instructions for the GPU will be much higher. So in most cases treating it as one big object would be better. However there are cases where the other treatment might work better as well. Computing everything you need to treat your robot as one object is relatively computationally intensive and memory heavy. (Compared to just keeping everything as a collection of blocks) So if you are in a garbage collected environment and want to minimize your memory footprint, if each time a block is changed you have to recalculate the big object it could cause a garbage collection every time and make your game lag for a frame. 

You could create your own waypoint-based graph, update it every frame and perform a Breadth First Search on that, or you could maybe try looking at one of the A* projects (there are free ones) on the Asset Store. To make the waypoint graph, Make the waypoints a child of the platform they represent, so that they move with the platform. You could set a waypoint tag on your waypoints and search for all waypoints in your player script. The waypoint should have a MonoBehaviour which contains the other waypoints it is connected to. Thus you have the nodes and edges of a graph. You can perform a simple Breadth First Search or use your implementation of Dijkstra's or A* for traversing the graph. Then you could check the height difference between this and the next waypoint in your traversal and if it is higher than a certain amount, you won't go there (that is a platform is above the character), a bit like how navmeshes are baked. If you need more help implementing this, feel free to ask.