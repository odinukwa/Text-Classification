Check out Fasttext. Fasttext works similarly to word2vec in that you can create word embeddings, however, it actually analyzes character n-grams, to force the syntactic similarity to what you're thinking about. 

Here the probability of red and blue are both .5, and the entropy of each system is 1, so the Entropy of the new system is still 1. This means that no information was gained from choosing red or blue. Suppose instead we select on left right: 

For binary classification, it's simple to calculate entropy for every variable, moving the variables which minimize entropy to the top, As an example, consider a simple case where a game exists when one must choose between a right and left door, one of which is red, one of which is blue. The color of the door changes from instance to instance (sometime left is red, sometimes right is red), but the prize is always behind the left door. Suppose also that over many trials, both left-right, and red-blue have equal number of people choosing those options. The entropy of the original system (P(prize)=.5, P(!prize)=.5) is 1. Supposing that we select a door based on color, we have: 

I have in my local directory a file 'temp.csv'. From there, using a local instance I do the following: 

It really depends on what you mean by 'big data'. A truly big dataset cannot fit in memory, in which case local python and R really only work for smaller scale experimentation and prototyping. For the purpose of data wrangling, you'll want a picture of the whole dataset by either slicing based on cuts, sampling, or aggregation. In any case, you'll need to work on a distributed computational platform. In my opinion, python has a big leg up in this regard, as its interface with Apache Spark is quite robust (whereas the R interface seems to be wanting). With that said, if you are skillful with impala, pig, or hive, you can do partitioning in a distributed query language, and create data that can be examined locally. 

So it is possible to manually implement any vector comparison that you choose. Cosign similarity is typically chosen because it performs relatively well when compared to other methods of grouping high dimensional projections. The way I could envision implementing Jaccard Similarity would be to identify a list of key words on a per document basis, and when comparing document, include words that are synonyms as intersections. Based on reviewing the gensim document comparison text ($URL$ I don't believe there is a native implementation. 

When you script out steps for spark on an RDD, it does not begin executing the operations until the data needs to be accessed. Instead of actually manipulating the data, spark builds a graph of how to go from the data, to the desired result set, and will not store intermediate data. For example, if you enter the following into a shell 

Instead of using a random forest classifier, you could instead use a random forrest regression analysis. Use the mean value of the bin as the value (which will take into account the relative values as you suggest). This has the added benefit that if you get a dataset that has actual ages, you don't need to change the training, and you'll be used to translating the scores (ages) provided by the analytic to your bins. 

Beautiful Soup is specifically designed for web crawling and scraping, but is written for python and not R: $URL$ 

If you have trained a gensim model, that object can act as a dictionary to give you the vector projection (via $URL$ 

Yes, you should save result files before you make major mods to the code. Disk space is cheap, so you're unlikely to run into issues unless your results sets are prolific. I would suggest storing old results sets with folder names that include a time stamp of when they were generated. As far as time shots of your code, using github (or some other code repository tool) is as easy as can be, will save version information, allows for collaboration, and is an all around great way to backup and version your code. The combination of these two things, you'll effectively have an easy way of mapping a result set to a specific version of the code. 

For question 1, I feel there are a great deal of things you can look into with this. The basic conclusion is which states are culturally similar to one another (physical proximity would lead to such similarity). Having a 'cultural similarity score' from year to year, you could determine the connectedness of states over time, see if major events effect how connected the country is..... there is a lot of potential here, and reenforces the notion that you need to know what question you're trying to answer before you get too far into an analysis. As for two, this is a bit trickier. The visual I see is each state getting a node at it's center, and like states having color coded connectors. The visual I'm seeing is similar to the graph they use for the scikit learn site for affinity propagation: $URL$ 

Alternatively, a method for automatically suppressing stop words is called tf-idf; tf-idf is commonly used in search engines so that the most important words are promoted to the forefront. In your case, I would suspect you'd want to have IDF scores for both English and Norwegian and apply only the appropriate one on a language to language basis. $URL$ 

Clustering techniques at high dimensionality tend to be unstable/inaccurate. As such, if you are able to accurately classify one column based on others, then you are well suited with pursuing a dimensionality reduction technique such as Non-negative matrix factorization. In this way, you can simply remove the column from all data (as the other columns hold the same information), and then cluster on the reduced space. 

First, the spark programming guide for LogisticRegressionWithSGD recommends using L-BFGS instead, so perhaps focus on the one. As for variable selection, the model description on the MLLib page for regressions has a nice explanation of how models are constructed and selected, but it does not address variable selection. This leads me to believe that it considers all variables, and simply chooses the model with the best fit. 

And since log_2(1)=0, there is no disorder (i.e. a perfect predictor). For continuous variables, it becomes a bit more complicated because you then have to select based on a floating threshold (or a few for a single variable). While the basic entropy calculations hold fast, tuning many thresholds on many variables becomes resource intensive, so at that point you'll want to look at something like spark's mllib. 

There are a few ways to deal with this issue. Python has a package called NLTK which contains stop word lists for several languages (including English and Norwegian). You can simply use this package, it's usage is as follows: 

So d0 is the raw text file that we send off to a spark RDD. In order for you to make a data frame, you want to break the csv apart, and to make every entry a Row type, as I do when creating d1. The last step is to make the data frame from the RDD. 

As word2vec is a neural network, it benefits from very large datasets. The Kaggle dataset is 50,000 reviews * ~5 sentences per review, so about a quarter million sentences. As they note, they get approximately the same results using bag of words and word2vec. One thing which is of note, since the review data comes from the internet, the sentences are much more loosely structured than what you would encounter in a corpus of newspapers, which typically go through grammatical review. A great dataset for training word2vec on structured language is the wikipedia dataset: $URL$ 

you'll notice that when you enter lines 2 and 3, no execution occurs. This is because spark isn't creating mapped_data or filtered_data, but rather, is building a graph of how to get from my_rdd to filtered data. The only reason that you would really write it in this way, is if you need to access mapped_data at a later state, in which case you would want to use a cache. 

A few points. First, if you're doing text analytics, there are methods to reduce the space, e.g. stop word filtering and word stemming. You can also consider training a word2vec model, such as that provided by gensim: $URL$ The particular strength of this method will be to take the space from O(10^7) to O(10^2) (but the vectors will become dense). From there, you can either try approximate nearest neighbor, or cosine similarity. 

Some of the biggest companies in the world are 'data science' companies; facebook, for instance, performs targeted advertising based on analyzing users preference. In essence, it's the most highly trained recommendation engine ever, as it knows a great deal about it's consumers. At it's heart, Google is an advertisement firm, and similarly leverages user information to provide targeted messaging to consumers. The same can be said for Amazon, Twitter, and most likely pokemon go. If you're thinking of a more purely 'data science as a service' company, larger consultancy firms such as the Boston Consulting Group have always incorporated analytics in their services, and have certainly profited from the proliferation of data. They don't say "we do data science", because people care more about getting the right answer, than how the answer is obtained. As far as Kaggle goes, while it's an awesome resource, and it provides cool tutorials and data sets, if you're a professional data scientist, then you don't work for free ;-) If a billion dollar company can harvest even a 1% gain in efficiency by offering a $50K prize, they can recoup that money very quickly. If you're only asset is intellectual property, you need to be cautious about giving it away. 

Facebook's fasttex is really great for loosely formatted text. Fasttext is a library for doing several tasks, the one you'll want to use is text classification. The best part of fasttext it uses character n-grams that instead of working with full words (which @JahKnows described well). The hardest part of using the tool is formatting the training data properly, but just look through the provided example and you should get it pretty quickly. 

If you know that you want to become a data scientist, you can pretty much rule out pure mathematics. Note, I'm not saying that pure mathematicians cannot become data scientists, but it's not the most natural transition. Between the other two branches, stats is probably the most natural path. Both will having you think about applying math to answer real world problems, but stats is very specifically geared towards focusing on larger scale data analysis. EDIT: @rocinante mentions the need for software skills, and suggests CS over econ as a minor. I would say this really depends; if you are in a bigger data science team, you'll probably work alongside dedicated programers, who will be able to implement your analytics more efficiently than you would be expected to as an analyst. If you know that you want to apply D.S. techniques to either finance or business, domain specific knowledge is helpful. Additionally, if you know that it is your desire to end up in data science, you'll be able to look for ways to use programing along the way, and keep those skills sharp.