Docstrings with every method: Way to go, document , too and revisit PEP257 for details. Your tackles the same task as module using much the same strategy, and you express dissatisfaction with run time - a side by side comparison: 

Note the order. Start readable. Start with What shall this be good for?: (doc)comment your code. No slower than what? A "well known implementation" for reference and as a base-line would be useful. Things I liked: 

is an example of a comment not getting updated when the code is - place comments as near to the code they comment as possible, and check comments, too. s for input size & path: there are ways that do not require a recompile. lacks a specification, in the code: What is it to return? What are required and admissible side effects? While the first two parameters are not hard to interpret, it would be nice to have stated explicitly to be inclusive or exclusive. You code picking a pivot index/value directly into a function named - consider to factor out . (Do you try to follow a coding convention? Some of your function names are CamelCase, others lower.) You spend many lines to place the median of three at , even with three comments about the individual conditional swaps (something else to potentially factor out), but without a comment for the three combined. Not naming piques me more than & - not sure why. Why are & declared outside the ? Why do you need ? Next, it gets weird: an open coded swap, followed by indirectly recursive calls to (still in ) &dash; and no for an function - you should be getting a warning. In , you manipulate ( or might be more appropriate) in a way that looks evident - it misses the comparisons in selecting the pivot. ( misses the opportunity to save comparisons capitalising on <= == <= .) And, again, does not (always) return a value. 

(I don't intend to go into computing Fibonacci numbers fast (Takahashi, Daisuke: "A fast algorithm for computing large Fibonacci numbers"), but keep the focus on the code presented.) Which is not doc-commented. With cleaned up (rename to would be misleading, not much better, long/abstruse - fubar), comes to the fore: 

Doc comments for public methods are indispensable. I'd probably just use a foreach-loop and a DIY stack: 

As interface design is pivotal in type design, I'm happy to start there - let me refer to . In no uncertain terms, the documentation summarises in just two sentences what is about: element access and bulk boolean operations (see for something irritatingly similar) (leaving out summaries ( and ) and paraphernalia). I like that it is possible to have non-reflected binary codes - assuming a : . I second AJD in preferring a constructor with a single parameter as an alternative to a separate generator class for reflected binary Gray codes. With that change, the whole API collapses to - which in itself is weird: Objects should be constructed in a valid state and never get invalid if that can be helped. (Another hint about the API is the lack of documentation - compare to .) Then, there is implementation: 

The definition of the main data type is basically unchanged. One important change here is the color of . Originally it was black, but actually it needs to be red in order to have consistent color change in the operation, see below. 

In addition to the other great answers, I'd like to point out a few details. First, in your function you skip too far, you need to go through elements one by one. Your code applied to passes. Rather you need to pattern match like this: 

Looks like you're on the right track, and this might eventually evolve into a parser, or converge to an existing one. From software engineering point of view, I'd suggest to make a . This better separates its API from the implementation, and allows to do changes later without breaking the API. Notice these two functions: 

by setting , you get what you're looking for - converting a to . You just need to supply the two function for folding, for example 

This accepts a character, and either produces a function corresponding to the character, or . Applying the operation on values, taking into account all possible s: 

Inserting a new element into a balanced tree so that the result is again a balanced tree. The signature of the function should look somewhat like 

Below is code based on the above ideas, with some more optimizations (to improve sub-list sharing), left as an exercise to analyze: 

Definitely a good idea. You might later decide to internally represent the data type differently, for example in , and this way you can do it without breaking the interface. But depending on the expected usage, you might include such conversion functions. 

Updating a field in a complex data type is a common task, and it can be greatly simplified using the lens library. Instead of 

All these problems relate to having the node's level in , which is currently sort-of unused. If instead you kept there the height of the subtree rooted at a node, you would be able to: 

Update: The problem could be actually solved in a genuine one pass using a rolling checksum, at the cost of having false positives in (rare) cases of hash collisions. Just compute two rolling checksums while traversing the list, one forwards and one backwards, and compare them at the end. 

Here results into , where the result is if the character is or if it doesn't match any supported operation. And then we use twice from , which, specialized for , applies to yelding . 

I quite like your aim for simplifying the function and factoring out the generic part. One solution (untested, just compiled) could be done using from : 

(While expected run lengths for uniformly distributed bit patterns are (1 +) smaller than 1,) There is no need to process the remaining bits in a loop: 

() - just in case.) (The uncomely business of parenthesization being less uncommon than parenthesation with parenthesisation in the middle makes me suggest association. Reminds of the origin of this degree of freedom, and optimalGrouping is dull.) should do just that, returning the two-dimensional array of splits (to the invoking to invoke with). Refactoring 

Naming: is weak - it has a doc comment (way to go) suggesting or . Contracts: doesn't follow 's "strong recommendation" to document the inconsistency with (I do that on both class and method). : could be different for and with . I suggest . Tactics: 

Checking from the end of a promising index range intending to leap as far ahead as possible, thereby reducing the number of checks, is the way to go. There is nothing special about the indexes you start to check from: Why start checking upwards without knowing there's a new record? Work in progress: not named, not documented, not tested 

I think this implements Hierholzer's algorithm in O() (with a bit of hand-waving around adding up to m - n edges to an taking Θ(mlogm) time - with constants "never" allowing to be faster). (If that was true, there shouldn't be a performance issue - I should really set up a test.) 

You want to limit file size, measured in bytes. You write records, one for each , terminated by . You are concerned about performance (with a limit of just 50 kB on file size - oh well). 

is on the long side. If you factor out recreate_node_array() (under a better name), you can return early from that avoiding the repetition of . Guess confused me - oho: following the pseudo code in the Fibonacci heap chapter of CLRS quite closely (consider referring to that near the top of the questions). 

More or less random remarks: use doc comments is funny - rather override check corner cases (see, again, rolfl's answer, too); consider using JUnit consider implementing /extending rename to and / at head 

Where are the doc comments? Your naming is intuitive, the formatting familiar. If you used (thanks, h.j.k.), I wouldn't have considered factoring out histogram collection. If using s that can be constructed with an expected size provided, do so. I'm convinced using one histogram is better: smaller memory footprint, no need to "filter" entries. Downside: separate s opened a promising opportunity for concurrency where my half-assed timings indicate adding a to h.j.k.'s approach to just slow things down. Using a sorted vs. a fast one and sorting the results is an interesting choice. You can combine both using a modified counting sort: 

And this is actually a very natural way how to express the solution: If we already know the longest paths in the part below a particular level (represented by and in recursive right-fold), we can compute the paths for this level as well. 

what is the other node it connects? My guess is that the other node is determined by the index of the list in the vector. In any case, this should be documented in comments for the types. Also, a question comes to my mind, why the graph is a , but the list of edges is a ? Regarding comments, I'd suggest you to use the Haddock syntax, as it's then very easy to generate documentation from the code. Also it's not clear what means in the vector. It could mean that an element hasn't been explored yet (infinite distance), which would work well with manipulating distances, but it seems to be used for elements that have been removed from the queue. I'd rather use a queue that allows proper removal of elements, and use that deals with infinity (see below) For representing distances it'd be useful to have a separate data type, something like 

Interestingly, the inner type is already a monad, corresponding to , or alternatively . Now standard monadic should correspond to and to the identity matcher. And also to . Such an interface will give your matchers all the power monads offer with very little effort. 

Both these functions are polymorphic with no coupling with . The final function is then expressed just as 

Nit: The order of functions is somewhat unnatural: uses , uses and . There is no visible order in this. One good option is that functions are always defined before being used (with the obvious exception of mutually-recursive functions). Or the other way around - the main/exported functions are first and helper functions follow. Or, separate functions into (possibly nested) sections, then ordering of functions doesn't really matter, ordering of sections becomes important. 

Note: The last part is similar to what you have done in your . Realizing that is an instance you could have written 

Or, you can also avoid doing recursive operations yourself and instead use existing list functions. Some hints on alternative solutions (which could be good exercises): 

As suggested, I'm doing a review myself after a few years. First, let's use GHC's extensions to derive the phantom types from data definitions. This also has the advantage of having proper kinds for each of these types. 

The data structure doesn't help in determining if the required invariant holds or not. holds information that is outside of its scope - its level in the tree. This means, among other things, that a node can't be shared between multiple trees, which is something you want when manipulating trees (a new tree is just a slightly modified version of an old one, sharing most of its nodes). Your design focuses only on creating a tree from a list. If you don't need any other operations, that's fine, but if you want to do more operations later, like insertion/deletion, merging trees, etc., it's going to be problematic. In particular, for some operations, when the total height of the tree must change, you'll have to update the whole tree. Most likely, it'd be possible to find an arbitrarily long sequence of operations, each taking O(n) time (where n is the number of nodes in the tree). 

The non-buffered didn't suffice, in the end: Sunsoft's encoder buffers. Trying to keep that from interfering messed things up. The from the question seems to have a correctness issue with the way it guesstimates byte count: it extends and accumulates "the length of s gotten from representations of ". It should rather 

Where is the code documentation? Everything not in the code will get separated - when code is copied and pasted into a different context, if not before. (Another thing python got right: the doc strings are between essential parts of the code.) /: Dispensable - whenever you can, process as you go. Giving it a try (with presentational abbr.), in Java for lack of a C# environment: 

(I'd rather not have a test- in a source file of its own: If concerned about jar size or the code loaded into the VM, use a nested class or similar.) 

(For variants using arrays of precomputed elements in stead of "the setup-loop" (and a ), consult the edit history.) (*Phi³ coincidentally can be computed with just two summands (and no other power up to 2³² can).) 

I tried to shift the parities left instead of shifting the operands right: unsurprisingly, the differences are negligible. 

LeetCode seems to stress preparation for technical interviews, I consider it prudent to start sketching the most simple thing that could possibly do (I take the liberty to substitute the original last word work). Things I like about your approach: 

To evade overflow in (the product may exceed ), check for . "The other condition" may be or ( fails for {0, 1, 3}). 

You present uncommented code: don't expect many to be willing to read it carefully. Reading , I expected open coded array handling. (I would have been pleased with a model of performance, including assumptions about inputs.) In , there is a for loop - with the loop control variable modified four nesting levels down. With no tangible specification, to use of both and is confusing. 

You seem to be bent on thinking of nodes numbered starting from 1 - nothing wrong with that, and coding the way you think about a solution/problem is the only sane way to start: for a shot from the hip, allocate one more array element, don't use index 0 and drop all of "the ". 

(Dare I mention comments, doxygen, or statement of purpose?) No documentation of and in particular means not knowing if there are any invariants to keep (at least on return). There is quite some open coding of doubly-linked list operations: use a library or try and factor out as much as possible (which only carries so far with intrusive lists/trees in a language not supporting OO). Regarding the amount of lines: consolidate early outs, e.g.: