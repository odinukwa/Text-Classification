Also, do not try to go gradually up the ladder. When you decide to go up the complexity ladder, take a full step at a time. If you do things bit-by-bit, then you will get lots of theorems which are weird and unusable (eg, you'll get multiple half-assed syntaxes, and theorems which mix syntax and semantics in strange ways), which you will eventually have to throw out. EDIT: Here's a comment explaining why going up the ladder gradually is so tempting, and why it leads (in general) to suffering. Concretely, suppose you have a shallow embedding of separation logic, with the connectives $A \star B$ and unit $I$. Then, you'll prove theorems like $A \star B \iff B \star A$ and $(A \star B) \star C \iff A \star (B \star C)$ and so on. Now, when you try to actually use the logic to prove a program correct, you'll end up having something like $(I \star A) \star (B \star C)$ and you'll actually want something like $A \star (B \star (C \star I))$. At this point, you'll get annoyed with having to manually reassociate formulas, and you'll think, "I know! I'll interpret a datatype of lists as a list of separated formulas. That way, I can interpret $\star$ as concatenation of these lists, and then those formulas above will be definitionally equal!" This is true, and works! However, note that conjunction is also ACUI, and so is disjunction. So you'll go through the same process in other proofs, with different list datatypes, and then you'll have three syntaxes for different fragments of separation logic, and you'll have metatheorems for each of them, which will inevitably be different, and you'll find yourself wanting a metatheorem you proved for separating conjunction for disjunction, and then you'll want to mix syntaxes, and then you'll go insane. It's better to target the biggest fragment you can handle with a reasonable effort, and just do it. 

To elaborate on Andreas Rossberg's comment on the question, a good way to understand this is in terms of Curry's paradox. Basically, if your language supports: 

One area that has seen a few applications, but not IMO enough is approximating discrete structures or processes with analytic approximations. This is big business in mathematics (eg., analytic number theory) and physics (all of statistical mechanics), but hasn't proved as popular in CS for some reason. A famous application of this was in the design of the Connection Machine. This was a massively parallel machine, and as part of its design they need to figure out how big to make the buffers in the router. Feynman modelled the router with PDEs, and showed the buffers could be smaller than the traditional inductive arguments could establish. Danny Hillis recounts the story in this essay. 

At the level of precision used in the nlab page, values are global elements -- i.e., a value of type $A$ corresponds to a morphism $1 \to A$. If you want to be serious about this, there are some technicalities to account for: 

The really critical is that we're using the unfold to incrementally take the input, and building a which we use to control the shape of the computation. This is where the pruning is taking place, and you can change it to whatever you like. Also, note that if you want to do something fancy (e.g., use intermediate estimates to control the pruning), you can change the type to contain it, and update your unfold to compute it. Finally, you can see the function as the fold followed by an unfold, of the type: 

Other people have already explained quantum randomness well, so I will point out that even classically, your final sentence is not quite the right. You are of course perfectly correct that the volume of phase space is invariant under the action of the Hamiltonian. However, the result of a classical process can still be random! Intuitively, while Liouville's theorem ensures that the volume of phase space will be preserved, it cannot guarantee that this volume will not become very wiggly and stretched out through the whole phase space. Then if your measurement apparatus has any limits to its resolution, it will look like the size of the phase space is growing. (This can happen very fast, with mixing happening in time logarithmic with your measurement apparatus's resolution.) This is called "Hamiltonian chaos", or more familiarly the second law of thermodynamics. 

All of these approaches will "feel" like a bit like Rust, since tracking ownership by restricting the usage of variables is key to them all. 

A nice example is given by Godelization: in lambda calculus, the only thing you can do with a function is to apply it. As a result, there is no way to write a closed function of type $(\mathbb{N} \to \mathbb{N}) \to \mathbb{N}$, which takes a function argument and returns a Godel code for it. Adding this as an axiom to Heyting arithmetic is usually called "the constructive Church thesis", and is a strongly anti-classical axiom. Namely, it is consistent to add it to HA, but not to Peano arithmetic! (Basically, it is a classical fact that every Turing machine halts or not, and there is no computable function that can witness this fact.) 

Yes, it's possible to do things like this, but if you control the compiler, it's usually faster and easier to do something else. The main exception is when you are writing parallel code. Trampolining style essentially does two things. 

I've hesitated to answer because any answer beyond a simple "yes" could and does fill volumes. Programming language semantics has been profoundly shaped by and in turn has deeply shaped the development of categorical logic, which is the application of algebra to logic. But I suspect the best way of answering this question is to tell you to go learn Agda -- the experience of learning to program with dependent types is in large measure coming to grips with how comprehensively algebra pervades computer programming. 

There is not a canonical such category, for the same reason there is no canonical category of computations. However, there are large and useful algebraic structures on data structures. One of the more general such structures, which is still nevertheless useful, is the theory of combinatorial species. A species is a functor $F : B \to B$, where $B$ is the category of finite sets and bijections between them. You can think of species as being families of structures indexed by abstract sets of locations. This explains the functoriality over $B$ -- such families have to be invariant with respect to renaming the abstract labels. Then, the calculus of species basically replays generating function methods at the functorial level, to generate sets of data structures instead of counts. To see this theory implemented in a programming language, you can read Brent Yorgey's Haskell Symposium paper, Species and functors and types, oh my!. I think Sage also has a species package, though of course it's oriented towards computer algebra rather than programming. 

You probably want to look at David et al's paper, Asymptotically Almost All Î»-terms are Strongly Normalizing: 

Linearity is not a sufficient constraint to pin down a unique stateful representation, and so the answer to your question depends on how you interpret linear logic in terms of state. This will typically be reflected in how you must interpret the $!A$ modality. If your intended semantics of references says that all pointers are unique values (i.e., there is at most a single reference to an object) then dags and graph structures are not expressible, for the sort of tautological reason that a dag may contain multiple references to the same object. In this case, $!A$ must be a computation which creates a new value of type $A$, since you want maps $\delta_A : !A \multimap !A \otimes !A$ and $\epsilon_A : !A \multimap A$. However, suppose that you want $!A$ to represent sharing. Then, objects can be garbage-collected with reference counting, with the maps $\delta_A : !A \multimap !A \otimes !A$ and $\epsilon_A : !A \multimap A$ can be realized as operations which just bump reference counts. In this case, you can't use linearity to assume that it's always safe to mutate values, since there's sharing. But you can ensure that all memory allocation is explicit in your program, and that there are no cycles in the heap. Most practical implementations of linear types use neither of these two interpretations. Instead, references are viewed as freely duplicable entities, and what we track linearly are in fact capabilities. Capabilities are not runtime values; they are purely conceptual entities which are intended to represent the permission to access a reference. The idea is that you program in a permission-passing style, and so even if there are many references to the same object, a read or modification of a piece of state can only occur if you also have the capability to access it. And since the capability is linear, you know that only you can change it. $$ \array{ \mathsf{new} & : & \forall \alpha. \;\alpha \multimap \exists c : \iota. \mathsf{cap}(c) \otimes \mathsf{ref}(\alpha, c) \\ \mathsf{get} & : & \forall \alpha, c:\iota. \;\mathsf{cap}(c) \otimes \mathsf{ref}(\alpha, c) \multimap \alpha \otimes \mathsf{cap}(c) \otimes \mathsf{ref}(\alpha, c) \\ \mathsf{set} & : & \forall \alpha, c:\iota. \;\mathsf{cap}(c) \otimes \mathsf{ref}(\alpha, c) \otimes \alpha \multimap \mathsf{cap}(c) \otimes \mathsf{ref}(\alpha, c) \\ \mathsf{copy} & : & \forall \alpha, c:\iota.\; \mathsf{ref}(\alpha,c) \multimap \mathsf{ref}(\alpha,c) \otimes \mathsf{ref}(\alpha,c) } $$ In the API sketched above, $c$ ranges over $\iota$, some domain of compile-time indices, and $\alpha$ ranges over types. We have a type $\mathsf{cap}(c)$ which is a capability indexed by $c$, and a type $\mathsf{ref}(\alpha, c)$, which is a type of references to $\alpha$ accessed by a capability $c$. Calling $\mathsf{get}$ and $\mathsf{set}$ on a reference requires the capability $c$, and calling $\mathsf{new}$ creates a new reference and a new capability sharing a common index. However, $\mathsf{copy}$-ing a reference does not require access to any capability, so anyone can copy a reference as long as they don't look inside it. 

Here's a counterexample to your conjecture. $$ \begin{array}{lcl} M & : & ((0 \to 0 \to 0) \to 0 \to 0 \to 0) \to 0 \to 0 \to 0 \\ M & = & \lambda\,f\,x\,y.\;f\;(\lambda\,a\,b.\;f\;(\lambda\,u\,v.\;u)\;a\;b)\;x\;y \end{array}$$ This beta-normal, eta-long with index 1. Now, consider the two following terms, both also of index 1: $$ \begin{array}{lcl} N & = & \lambda\,p\,x\,y.\;p\;x\;y \\ N' & = & \lambda\,p\,x\,y.\;p\;y\;x \\ \end{array} $$ Now, $M\;N$ reduces to $\lambda\,x\,y.\;x$ (index 1) and $M\;N'$ reduces to $\lambda\,x\,y.\;y$ (index 2). 

Actually, the approach of the CoC is more expressive -- it permits arbitrary impredicative quantification. For example, the type $\forall a.\; a \to a$ can be instantiated with itself to get $(\forall a.\; a \to a) \to (\forall a.\; a \to a)$, which is not possible with a universe hierarchy. The reason it is not widely used is because impredicative quantification is incompatible with classical logic. If you have it, you cannot give a model of type theory where types are interpreted as sets in the naive way --- see John Reynolds's famous paper Polymorphism is Not Set-theoretic. Since many people want to use type theory as a way to machine-check ordinary mathematical proofs, they are generally unenthusiastic about type-theoretic features which are incompatible with the usual foundations. In fact, Coq originally supported impredicativity, but they have steadily abandoned it. 

Saal Hardali mentioned that he wanted a category of Turing machines to do geometry (or at least homotopy theory) on. However, there are a lot of different ways to achieve similar aims.