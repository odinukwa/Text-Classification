Another tip is to use lowerCamelCase for variable names. Try to avoid useless comments Comments such as inside of a constructor don't say much - the syntax of the language and/or name of the function should be good enough to describe simple actions. Comments are very valuable, so feel free them on places where they are useful (in complex methods), but don't add any that add nothing. Use nullptr over NULL NULL is defined as a zero, which can also be considered as an integer by the compiler. This can get some weird issues when you have a pointer and integer overloaded function. Pass arguments by const reference where possible If an argument has a complicated copy constructor (e.g. if it has to copy memory (such as ) or has to do atomic operations (such as increasing a refcount)) 

If you don't, the destructor of the subclass may not get executed if you're working on the interface. Be consistent in naming conventions It seems that you use the prefix for members and prefix for parameters. That's probably a good idea but be consistent 

What a difference a removed branch makes! It's about three times faster, at 1.35 cycles per candidate. That's despite the fact that we are executing more instructions: about 280 billion versus 240 million. The upside is all due to removing branch-misses, which are now reported at 0.00%, and the IPC has increased to ~3 instructions per cycle. Loop Splitting Of course in the real world, you don't want to just count the primes, you want to do something with them. That's fine: it's a slight modification to the above to generate a bitmap indicating which values are likely primes, rather than simply counting them, without slowing down much. So to avoid the mis-predictions, you process some fixed number of candidates with the loop above, generating a bitmap, and then you iterate in a branch-prediction aware way over the bitmap (e.g., using and ) to generate the likely prime values for your "secondary processing". I won't actually flesh this out fully for since we are about to move into the fast lane with a different approach entirely (which still ultimately uses the same "bitmap" output format. Bitmaps FTW Let's step back a moment and understand what the core of the algorithm is doing. Basically it implements 30 periodic counters incremented in sync and tries to determine if at least one counter has "wrapped" on every iteration. To do this, it uses 30 byte counters in a . Since AVX2 lets us do 32 byte operations per operation, it means we can do 30 operations on this counter per instruction (and perhaps up to 3*30 = 90 if we use all 3 vector ports fully). We get lucky that the instruction works well to do a 30-way or "wrap" operation! What if instead of using byte counters, we use a series of bitmaps with one bit per candidate, which encode the same periodic behavior as the counters? That is, to replace the counter which goes we use the bitmap with every 3rd bit set? Well now a 256-bit register holds 256 counter values, not 30. Of course, the correspondence isn't exact, since everything is transposed: one register contains a lot of state, but for one prime. You'll need 30 such registers for all the primes. Still, we can ballpark this: first note that combining registers is simply a matter of ing them together - the remaining zeros are the likely primes. So it will take 29 instructions to combine 30 registers, and the result will cover 256 candidate values, so that's ~8.5 candidates per instruction, versus ~1 for the counter approach. Now this is a very rough analysis and leaves out a lot of details like how do you get the bitmaps all aligned properly, but it seems like we may get about an order of magnitude improvement with this approach even over the branchless counter version. A C++ Prototype Let's try to prototype this in C++. Here's the core loop: 

The compiler can complain if you double declare, is strongly typed, ... Cache sizes of containers while iterating Depending on the container, calling GetSize() and operator[] can be expensive. Cache the size up front or use a range based for. 

First suggestion: clean up your code. There are a ton of inconsistencies in variable names, indentation, brackets, defines ... Variables I was confused when seeing variables in your file, suggesting that these were global variables yet they are declared as member variables. My guess is that you moved away from global variables (which is a good idea) but forgot to fix the naming. I know it's annoying to change all those names now, but it'll save you a lot of time in the long run and many modern IDE's offer refactoring tools. Keep in mind that full-caps member names (such as ) are used to indicate constants or defines. In another file, , you use as prefix. Be consistent, either use the and or and prefixes everywhere. Moving away from global variables is a good idea because they are often quick hacks but often they can do more bad than good in the long run. Imagine declaring as a global variable, and using that in every manager. But when we want a single manager to update at half speed to mimic slow motion, we'll have to do nasty things. So in my opinion it's better just to give the time as a parameter to every manager. Defines Header guards should be the first thing in a header file. In this is not the case, so move those ifdef's up to save some compile time and potential compile errors. Const correctness If I call a function such as 

Huh, well that kind of sucked. At 5.2 cycles per candidate it's a bit slower than the algorithm in the OP. For one thing it still has ~9% branch mispredictions. It turns out the main culprit is this line: 

Each bitmap is "normalized" such that the LSB is always 1. Then the loop is very simple: it loops over all 30 primes, and shifts the bitmap by the right amount to "align" it, and ORs all the results together. In this way, the loop handles 64 candidates. The shift amount is simply the amount need to correct stitch the bitmap from the last iteration so that it is periodic. Using a 16-bit example for sanity, the bitmap for in binary is . In the next iteration, you can use the same one since the effective 32-bit bitmap would be . Oops, two adjancent 1s! You just need to shift it over by 1: and now it stitches fine. In general the stitch amounts for any prime have period and take all the values between and inclusive. The last line in the loop calculates them. Let's try this guy: 

The same happens in your where you create a temporary and then push them on . Why not add them directly to ? You know the size beforehand, so you can call on and acces the elements using the operator. (This way, the vector has to reallocate only once) Use namespaces You have some places with generic names, such as , which might collide later on when you include some math library. Use namespaces from the beginning to prevent issues later on. Prefer the compiler over the preprocessor If you can, prefer a const variable over a preprocessor define. 

has lower precedence than , so in this case it is correct, but it may become wrong if you use another operator (such as bitwise operators) and it is confusing to read. So just add them! 

Since update is inside this loop and render is not, it is possible that we do not update and render the same state of the game again. The other way around: it is possible that we update the game twice but only render once. Both of these scenarios are wrong. The first one because we are going to render the exact same scene again, the second one because we're going to calculate how the scene should look like and then do nothing with the result, but rather calculate it again. For starters, try to replace this second while loop with an if and place the functions inside of it. 

This is just a "saturating" shift, which returns zero if the shift amount is 64 or more3 and which compiles to a branch4. It also turns out that the loop has no less than two very slow division instructions every time around, coming from the two operators in this line: 

We are down to 0.16 cycles per candidate! That's fully 25 times faster than the original algorithm, and if you measure it by cycles per prime, we are finding a prime every 1.44 cycles. Unless you are doing almost "zero work" per found prime, it's very likely that the other work will start to dominate here. Further Optimizations If you are so inclined, this can still be made much faster, probably by a factor of 5 at least. Of course, before you pursue that, you would need to benchmark your full application, since it is highly likely that the unspecified work you do per prime is what is slowing this down now. Minor Optimizations The loop above directly admits some minor optimizations. For example, which counts off the 30 primes could be inverted so that it counts down to zero (or from -29 up to 0) allowing use to remove the check at the end (we use the flag from the prior instead). The could be changed to a 3-argument , avoiding the prior , or this whole calculation could be removed by using the induction counter instead by making the row size of the two involved tables and consistent (right now one has an inner dimension of 128 and the other 190). These may shave another small fraction of a cycle off of the existing time, but the ones below are much bigger. Larger Contiguous Reads The above algorithm reads uses on two consecutive registers worth of data (64 bytes) from the calculated index. It is in fact the slightly bigger brother of the not-shown variant, which only reads one 32B value in the inner loop. That guy ran at 0.27 cycles/candidate, so just doubling the read size in the loop nearly doubled the speed. It's easy to see why: it only took one extra instruction to do that, while the other 12 instructions in the loop are pure index calculation overhead which are now doing double work. So by increasing the loop by one instruction it does double the work. You can just carry this idea to its logical conclusion, reading 4, 8 or however many values per loop. There is no particular reason it has to be a power of two, either. These will give very fast and easy speedups: I guess it is easy to get below 0.1 cycles/candidate using this approach. The larger reads come at a size cost for the - larger reads mean a larger table6. This optimization is probably the best and easiest one if you want performance. The code is already kind of half-generic. I call this "unrolling horizontally" based on my mental model of each prime being a long horizontal bitmap, with primes stacked vertically one above another. So the is accumulating in vertical slices (column-wise) and this unrolling moves in the horizontal direction. Unroll the Inner Loop This is the "usual" unrolling and the counterpart of the horizontal unrolling discussed above. Currently the inner loop iterates over all 30 primes. This loop has a fixed trip-count, and it could be completely unrolled. Several instructions in the inner loop would just disappear, such as all of the loop control, the instructions dealing with and the . This should give a reasonable one-time gain and the loop should still easily fit in the uop cache. It's less appealing than the horizontal unrolling since you can only do it once! Unroll the Outer Loop Once you've unrolled the inner loop, you may want to unroll the outer loop as well. Unrolling this by N would result N copies of the unrolled inner loop so, the code would get big, fast, but I think you could probably unroll it by 3 or 4 and still fit it in the uop cache. This allows some very interesting optimization since by unrolling the inner loop you now have unique sections of code handling each prime. When you unroll the outer loop, you may now be handling several reads for the same prime, in explicit unrolled code. The big win here is that you can directly hardcode the "offset sequence" that normally has to be painstakingly calculated by the generic code. For example here's the start of table for consecutive 64-byte reads: 

I want to be sure my parameters and base object will not be modified. (If calculating if two rectangles intersect requires non-const parameters (so possibly changing that rectangle), something fishy is going on). The general rule should be to mark everything as const unless it is inpossible: 

The comments from the other answers are valid as well - try to avoid copy pasting. If you ever have to copy paste something try to think if it can't be made into a function, macro, class, ... 

180 is an integer, not a float. So you'll get a whole devision instead of a floating point devision if is an integer as well. This is not the case, but it is dangerous so I recommend you change it to Clean up a little Remove empty functions, don't pollute your main function. So either remove or transfer some of the logic that's currently in . Polling versus events In your function you're polling the state. This means that every single frame, you're requesting state. In this case, it's about a few keys but if you have support for multiple players and controllers, this method is going to get computationally expensive. Responding on events is a little more complex, but worth considering for larger projects. Have a look at events in sfml. Use parenthesis They're free, so why not use a few extra. Take a look at this piece of code: 

Bring back the Vectors The next step is to vectorize this. This is getting long so we'll skip the first version ( which clocks in at 0.27 cycles per candidate) and just go to my final version, : 

Let's review this code purely from a performance angle, without a focus on style or anything else (in addition to optimization suggestions, Peter already mentioned several things in areas other than performance). First, you can play with all the algorithm discussed here in this github repo. I compiled it on Linux but it should approximately work on Windows if you have or - if you add a thunk to adjust the calling convention. If someone really wants it I'll do it. Profiling If you've ever asked for performance help, no doubt someone has told you to profile, profile, then profile some more. So sure, let's start there. I'm going to use Linux's since it is awesome and free and available on Linux, but you can get the same information presented nicely in Windows using VTune or maybe with this stuff. Anyway, let's run on the original algorithm to get a feel for any high-level issues: 

Do nullptr checks In some functions (e.g. ) you pass a pointer while you're never checking if that pointer is actually valid. If the pointer cannot be invalid, pass a reference instead. If the pointer can be invalid, get some logging (use some form of asserts). Watch out for integer divisions 

Now, regarding your animation smoothness/stuttering problem. The following while loop is a bit weird: 

Remove unused headers For example, in you include but it's not used. If you would only use it in your cpp, move the include there to save some compile time. If compile time is really important you can even predeclare in your header. Use initializer lists for all constructors For example, your zero-argument constructor in the class (the one defined in the .cpp) doesn't initialize it's arguments, and you might up reading garbage memory. Mark const methods as such For example all getters can be marked as const, as well as all other functions that do not logically alter the state of the object. 

Work directly on an object instead of a temporary in you use two temporary variables and just to copy them over into . You can the following code and work on directly: 

You increased from 2 reads to 6 without adding any index calculation overhead! It's quite similar to the "horizontal unrolling" discussed above, where you just read more consecutive bytes (2 x 32B reads in this example), but that approach nearly doubles the table size (since you need to accommodate a 64B read at all positions), while this approach increases it very little (by 2 bytes, probably, since you just need to accomodate the maximum offset of +2 at every position). The code is different per-prime: the code for adds 4, or else subtracts 1 (they are equivalent, but you need to design your index handling to account for which direction you are going). This approach is promising because it lets you get more reads without greatly increasing the table size. It works best for the smaller primes since the jump amounts (and hence required table padding) are smaller, while the horizontal unrolling described above works best for larger primes since the table increas is relatively less (since it is fixed to the read size and the large primes already have larger lookup tables). Unrolling both the inner and outer loops allows you to even pick and choose different strategies for different primes. Optimize the Lookup Tables I just made the tables 2D arrays, for simplicity, but this wastes a lot of space, since the rows for the smaller primes are often very short (I padded out some of them with to help catch bugs). To optimize this, you'd probably want to first pack the tables more tightly, either as a jagged matrix (i.e., an array of pointers to rows), or just as one large packed 1D array. The latter approach is great if you've done a lot of unrolling as described above since the various instructions can just directly embed the offset into the array in their memory operand "for free". I didn't make any effort to align any of the lookup tables at all (and they won't have any natural alignment since they are all bytes. No doubt about 50% of the 32B loads will be "split loads" that cross a cache line. With a bit of care you can reduce that to 0% for the smaller tables with very little size increase. For the larger primes I think you can reduce it to 0% but at a 100% size increase (just thinking about it, I haven't checked), which may not be worth it. Furthermore, depending on the number of consecutive bytes you are reading (see ) there are opportunities to dramatically reduce the larger tables. For example, if you are doing 64-bit reads like the non-SIMD bitmap algorithms, for any prime >= 67 every 64-bit read returns either an all-zeros value or a value with exactly one bit set. Yet such each such prime is using their own large lookup tables which are mostly zero. To support any possible 64-bit read for all primes >= 67 you need only 8 zero all bytes, and 8 other 15-byte regions with the all bytes zero other than the middle byte which 1 of the possible 8 bits set. You can overlap this all nicely so it takes about 72 bytes. So you can replace all so you can replace the 13 * 134 byte lookup tables for the primes from 67 to 127 by 72 bytes: a reduction of about 25 times! Even better, this scales as you add larger primes: even if you want to add 100 more primes, you don't need any additional lookup tables for the bitmap. For the fully generic algorithm which uses the table on every calculation, this transformation is free. For the unrolled versions that encode prime-specific knowledge into the reads it doesn't work as well. It also doesn't work as well for the larger reads: the version that reads 64B (512 bits) never gets close the "zero or 1 bit" set case for the first 30 primes, so you can't use it there. It would be useful if you wanted to use more primes, however and since this algorithm is so fast, it makes sense to do so. Combine Small Primes Currently every prime is handled separately: although there could be some bitmap sharing for larger primes as described above, each prime still implies at least one to incorporate it. There is nothing particular special about one-prime-per-bitmap, however: why not simply combine several primes together into one pre-calculated bitmap? Instead of having two bitmaps for 3 and 5 like: