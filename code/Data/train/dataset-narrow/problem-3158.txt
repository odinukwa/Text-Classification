What does it really mean? What does it mean when the term's coefficient is $+ve$ but the coefficient of it's squared term in the equation is $-ve$? 

No. It cannot. A model which is trained on hand/no-hand can do that. Not this. At most, it shall predict one of the either (left/hand) with a very low confidence score. But, it definitely is not a nice idea to use it for a hand/no-hand identification. 

Yes Linear Regression is not a nice fit for this problem. Non-linear regression, as @jamesmf suggested, can be a nice option. But, this looks like a nice fit for exponential regression. The graph of exponential regression looks something like this: 

The red dots are the data points. And a decision tree regression plot would look something like this: 

Sorry for the noise, as it is a quick and dirty implementation. But, yeah a Box Cox transformation should also be a nice way to fit. R code for the above plot: 

As this question highly overlaps with a similar question I have already answered, I would include that answer here (linked in the comments underneath the question): In images, some frequently used techniques for feature extraction are binarizing and blurring Binarizing: converts the image array into 1s and 0s. This is done while converting the image to a 2D image. Even gray-scaling can also be used. It gives you a numerical matrix of the image. Grayscale takes much lesser space when stored on Disc. This is how you do it in Python: 

No As Emre has rightly pointed out, the Chief Scientist of the company himself has written a blog post claiming the same. However, Neo4j does a lot of computations which graph processing tools can do. In fact, it does graph traversals much faster than giraph, due to the Hadoop overhead and also as it stores the adjacent nodes in a doubly linked list. So, it's not rare to confuse Neo4j with a graph processing platform (thus the claim in the paper) due to it's overlap with the features of a processing platform like Pregel and Giraph. 

I am going through this imagenet example. And, in line 88, the module DistributedDataParallel is used. When I searched for the same in the docs, I haven’t found anything. However, I found the documentation for . So, would like to know what is the difference between the DataParallel and DistributedDataParallel modules. 

PS: I have gone through this question, but the answers were very broad and vague because of this reason. 

Yes, you are right. The selection of the technique depends on the data. And the feature scaling (and normalization) process comes under the process of data cleaning. So, it is done immediately after the selection of the relevant data for the analytics process. 

The above can be achieved through an MVC framework. The data can be sent back and forth the server and the client through a protocol. Useful link In case, you want to integrate it with an external system, then create an API for the above system which you have built and let the other systems use your software/app by calling your api. Now, create a simple SlackBot which can interact with your API with some certain commands, and display the relevant results in the group. Example of such a system 

Just a pointer that the concept of ensembling helps with better generalization of the model (which helps keep overfit in check). 

in the form of columns. So, what does this mean? This means that insertion(and updating) and deletions are fast in the row-based column store as it is just removal of the last few values or the first few values. However, it is not the case in columnar stores as the value in each block store needs to be removed. However, when there is the need for columnar aggregates and operations, the columnar stores have an edge over their row-based counterparts, as they are stored column-wise, and as a result, accessing individual columns is very easy. 

There are a group of algorithms (or techniques) called the Bandit algorithms, which deal especially with the problem statement, which is the optimization of Click-through rates of advertisements. The problem is framed in a setting of multiple bandits with vending machines. There are various strategies which can be implemented: 

Yes, it is a full-strength Machine Learning paradigm. XGBoost is basically Extreme Gradient Boosting. It only takes in numeric matrix data. So, you might want to convert your data such that it is compatible with XGBoost. The wide range of parameters of the xgboost paradigm is what makes it so diverse. Boosting can be done on trees and linear models, and then more parameters can be defined depending on the model you have selected. So, yes it is a complete paradigm in itself. But, when you want more than the limitations of xgboost like linear and tree models, then you can use the concept of ensembling. In the case of ensembles, the tools/libraries which can be used depends on the data scientist who is conducting the experiment. It can be Keras or Theano or TensorFlow, or anything which he/she is comfortable with. (opinion-based) 

This is a tough question, as everyone who can program can get started with it, but if you really want to implement this in your architecture, then these are the pre-requisites which I can think of: 

Welcome to the real world of data science. Here, the data sets are not as clean as you thought while doing those courses/tutorials online. Those are super polished and refined. But, the real world data is not so. The step where one does the cleaning and scrubbing is called the data pre-processing step. So, some nice data cleaning techniques, in addition to @jknappen's excellent answer are: 

As far as I know, the existing (almost all) libraries in Python can handle very complex models of Neural Networks. TensorFlow is however not polished as of now. It still has a long way to grow before getting accepted as a mainstream library for ML. So, going ahead with the existing libraries like PyLearn/Keras/Torch, etc makes sense as of now (also as they have wide and dedicated communities already), as you need to concentrate on research rather than worrying on bugs and technical problems of a library. 

I was reading this blog post titled: The Financial World Wants to Open AI’s Black Boxes, where the author repeatedly refer to ML models as "black boxes". A similar terminology has been used at several places when referring to ML models. Why is it so? It is not like the ML engineers don't know what goes on inside a neural net. Every layer is selected by the ML engineer knowing what activation function to use, what that type of layer does, how the error is back propagated, etc. 

If you are looking for a nice solid tutorial for Deep Learning in the domain of NLP (Natural Language Processing), then Robert Gutherie's notebook would be a great start. It teaches you how to use vectors and how does multi-dimensional vectors look like and how to deal with them. He uses the deep-learning library of Facebook, Pytorch to explain the concepts. This library is more pythonic and less verbose than tf, so it is easier to follow. (personal opinion though). After that, you can take up any library like gensim, spacy or Fasttext and put your vectors to work there. A nice follow-up, also would be to go through Pytorch's Deep Learning tutorials. They are very nicely written and often very intuitive and the code is very easy to read (thanks to Python and pythocicity of Pytorch :D). 

So, the concept of prior is very important when it comes to customer analytics, which makes the concept of Bayesian networks very important to this domain. 

Excellent answers already. One domain which I can think of, and is working extensively in, is the customer analytics domain. I have to understand and predict the moves and motives of the customers in order to inform and warn both the customer support, the marketing and also the growth teams. So here, neural networks do a really good job in churn prediction, etc. But, I found and prefer the Bayesian networks style, and here are the reasons for preferring it: 

Yes, it can be. This would be my approach: (Consider I already have a hand-classified training data) Using the bag of words approach, the most used words and phrases for a mood (stressful, jovial, etc) can be captured, and the subsequent training data can be ranked accordingly as (60% stressful, 40% jovial). Additionally, you can also define thresholds for getting a single mood. Like: 

I found both of your options slightly faulty. So, this is generally (very broadly) how a predictive modelling workflow looks like: 

The business analyst flavour of data science is something you are nicely suited for. As far as I have seen business analysts and business intelligence engineers in the industry, most of their work is centered around deriving insights from Excel sheets and writing SQL queries to dig out the appropriate data. They do write scripts, but that is generally for just the visualization purpose, and not for higher lever analytics like Machine Learning. I can also see a nice future for you in the financial analytics/quant domain. It is also a domain where the learning curve is a bit steep, but totally worth it. Here is my answer on Quora about getting into the field of quant. However, if you want to get up to speed with data science, then you have to slowly build up strong linear algebra skills, along with a very keen and valuable domain knowledge in whatever domain you'd be working with. The latter is often underrated, but from my (short but valuable) experience in the industry; I vouch for that fact. Bonus resources: 

Single Variable Calculus Multi Variable Calculus Differential Equations Linear Algebra Probability theory and combinatorics Basics Statistics Algorithms 

There are a lot of criterion which are to be taken into account when the tool selection is concerned. The can be: 

In most cases No. Generally, only one normalization technique is used and it pretty much suffices the need. In addition to that argument, it should also be noted that any normalization technique introduces duplication in the data records (not necessarily redundant duplication). So, pretty much a single normalization technique would suffice most of the times. 

At each step of building individual tree we find the best split of data While building a tree we use not the whole dataset, but bootstrap sample We aggregate the individual tree outputs by averaging (actually 2 and 3 means together more general bagging procedure). 

Firstly, for understanding the Markov switching models, a nice knowledge of Markov models and the way they work. Most importantly, an idea of time series models and how they work, is very important. I found this tutorial good enough for getting up to speed with the concept. This is another tutorial on a similar application of the switching model, which is the regime switching model. The statsmodels library has a nice support for building the Morkov switching models. Here is one simple and quick Python tutorial which uses the statsmodels library. 

You are trying to do build a binary hand.no-hand classification model, not a hand/cat/dog/... style multi-label classification model. So, you just need to have a bunch of images with a hand in it, and some more which doesn't have one. 

If you are looking for an external storage, then I would suggest you Redshift. Redshift is a central warehouse and a columnar data store. It allows complex and huge data aggregations and joins; so it is a nice bet for serious Kaggle participants. (I personally use Redshift as data science architecture for kaggle.) Combinations from some other vendor? No, a combination of Redshift and S3 is enough. The data is stored in S3, and then loaded into Redshift using the COPY command. Is there any way to interact with a GUI rather than a command line to set this up? Yes, there is a CLI interface for Amazon AWS. However, you can use the boto library on Python for handling this stuff. Would I have to install Anaconda or some other Python distribution before getting started? Not a compulsion. Anaconda is a wrapper of numerical and scientific libraries. You can install them on your own or intall Anaconda. Check out PySpark, which allows you to do handle BigData in Python. 

Here, I did not understand the exact definition of representation learning. I have referred to the wikipedia page and also Quora, but no one was explaining it clearly. The lack of explanation with a proper example is lacking too. The authors cite the autoencoder as an example for a representation learning algorithm. 

I guess the Quora answer here would do a better job than me, at explaining the difference between them and their applications. Let me quote that for you: 

And the formula used by the function of the class of scipy is: So, the actual cosine similarity metric is: -0.9998. So, it signifies complete dissimilarity. 

This is the repository where the xgboost package resides. That is from where the xgboost library for major languages like Julia, Java, R, etc are forked from. This is the Python implementation. Walking through the source code (as instructed in the "How to contribute") docs would help you understand the intuition behind it. 

Yes, you can do the analytics directly in an Ipython notebook(Jupyter), which also supports Markdown and HTML cells. In addition, you can also use widgets and interactive visualization with Jupyter Ipy notebooks and Matplotlib. Tutorials for the same 

The Julia project is one which I actively contribute to, including the advanced computing and XGBoost libraries. So, I can definitely vouch for it's maintenence and the quality of the community. Some really good open source data science projects where even the beginners can contribute are: 

Depends on how much CPU juice you are willing to afford for the same. Having a lower K means less variance and thus, more bias, while having a higher K means more variance and thus, and lower bias. Also, one should keep in mind the computational costs for the different values. High K means more folds, thus higher computational time and vice versa. So, one needs to find a sweet spot between those by doing a hyper tuning analysis. Also, you need to keep the size of your data in mind. If your data is very less, then even using a k-fold crossval wouldn't make sense. So, you might want to use a leave-one-out CV (LOOCV). 

The Box-Cox transformation can also be used for fitting the plot. I have taken a sample data set, and fitted a Box-Plot transformation, with relevant parameters for transforming it to look somewhat like the plot of your data: 

Lots of nice conferences already mentioned by the existing answers. Here are some which I think deserve a place in the top conferences list: 

The entire analytics is done with the PIL package. I wouldn't claim that it's a one-stop shop for Image analytics, but for a starter to novice level, it is pretty much it. 

The Sankey diagram would be nicely suited for your problem statement. The flow width can be the number of refugees migrated, and the end nodes can be the destinations (from and to) of the refugees, and the flow width would be the magnitude of refugee migration. If you want to model the graph on a geographical map, it can look something like this: 

[Completely a personal opinion] When the term 'Data Scientist' overtook 'Statistician', it is more towards sounding cool, rather than any major difference. Similarly, the term 'Deep Learning'. It is just neural networks (which is another Machine Learning algorithm) with a couple of more layers. No one can explain when a particular neural net can be called DL, rather than ML, cause the definition itself is fuzzy. So, is the term 'Data Scientist'. However, as companies are adopting the DevOps mindset to data science, the term ML Engineer evolved. What is the DevOps mindset to data science? This is where you build the model, deploy it and also expected to maintain it in production. This helps in avoiding a lot of friction in software teams. [PS: DevOps is a way of doing software, more like a philosophy. So, using it as a designation, again confuses me]. So, ML engineers are supposed to know the nuances of systems engineering, ML, and stats (obviously). A vague generalization would be Data Engineer + Data Scientist = ML Engineer. Having said that, the designations in this space are becoming vague day by day, and the term 'Statistician' is becoming more and more relevant (the irony!). 

Yes, but it would also be better if you consider the probability of getting a bad credit from these people. But, even that would turn out to be NO for this class, which makes your observation correct again. 

Tweepy is one of the best libraries for analyzing and hacking around with the Twitter API. (Being a contributor for tweepy, I can vouch for it's stability and quality) For a Python wrapper for the Facebook graph API, you can use the Facebook-Insights library, which is well-maintained and neat documentation. There are services out there which can mine you information, but they are limited to the complexity of the query. For example: "How many people tweeted about banana on Monday?" can be answered from existing tools "How many people happy people tweeted about banana on Monday" can also be done, but would require more efforts and the software or tool should be able to detect emotion. So, if you are into research, I would advise you to go with the API's and good ol' programming!