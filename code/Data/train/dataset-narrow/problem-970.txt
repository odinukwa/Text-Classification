There are, of course, other equally valid ways of refactoring your code, but the important thing is to be DRY! The above example achieves that, and without significantly changing the object code generated by a compiler (in other words, without slowing anything down at run-time). 

Even though is 2 bytes compared to 2+2 bytes for +, is slower. On the 286–486, executes in ~8 clock cycles when the branch is taken and ~5 clock cycles when the branch is not taken. The alternative is 1 cycle for the , plus 7 cycles on 286/386 when is taken or 3 cycles when not taken (on 486, is even faster—3 cycles when taken, 1 cycle otherwise). Thus, in the worst case on the 286/386, + is equally as fast as , while in the best case, it is about twice as fast. On the 486, + murders speed-wise. Intel's optimization manuals at this point in history explicitly recommended using + instead of . Similarly, the 2-byte was generally the fastest way to exchange two enregistered values on the 808x. It executed in 4 cycles, which was the same as two instructions (2 bytes and 2 cycles each), but it was half the size, so it was quicker to fetch. Things changed on the 286, and on all subsequent processors. was now a 3-cycle instruction, but became a 1-cycle instruction, so doing two s was faster than a single . That is, assuming you had an extra register to spare. If not, given the limited register set, you might still have been better off with . The 486 changed that yet again: since ran in only 1 cycle, you could use three s in the classic swap trick as fast or faster than a single (faster if you could interleave the three s within other instructions you needed to execute anyway to reduce dependencies). The Pentium had dual execution engines, known as the U and V pipes. The U pipe was basically a full-featured 486, while the V pipe was more limited and could only execute a subset of instructions. This made instruction pairing a big deal, because if you properly paired your instructions such that one could execute on the U pipe while the other executed on the V pipe, you could effectively double performance. The trick becomes even more attractive here than a single , because paired on either pipes and could be even more effectively interleaved within a complicated sequence of instructions. Pentium Pro introduced out-of-order execution, which made perfect instruction pairing less important, but much of the same optimization logic still applied. The CISC-style instructions are very slow on modern processors and should not be used even when they are fewer bytes. The days of rote cycle-counting are long gone, however. All of that to say that you can probably improve the performance of method 2 even further by choosing more optimal instruction sequences, even if that results in larger code. Another way to speed the code up is to replace conditional branches with clever bitwise or arithmetic operations that accomplish the same thing branchlessly. This is a huge performance win on the 808x and extremely significant on any processor that lacks a branch predictor (introduced with the Pentium). Even on modern processors, there are massive penalties if branch prediction is unsuccessful (not strongly biased for taken or not-taken), so rewriting branching code to be branchless can still result in a performance speed-up. You've got a tight loop here, the body of which is a perfect place to apply such optimizations. This is equally true for either method (and, at first glance, it looks like the branches could be more elegantly removed in method 1). You could also ditch the relatively-slow instruction if you replaced with +, instead of +. Normally, you would prefer for a comparison against zero, but in this case, is likely a better choice, because it sets the carry flag (CF) according to its result, while does not affect CF. One thing that likely makes a big difference in the relative performance of method 1 and method 2 is replacing the instruction with a instruction. Or at least, depending on which processor you're using to do the timing. is extremely slow on the 386—about three times slower than . The difference isn't there on 808x, 286, Pentium I/II/III, and most AMD processors. However, on recent Intel processors (starting from Pentium M, extending through Core 2, and later up to current microarchitectures, including Atom), is once again about three times slower than (higher latency and less throughput). Finally, you aren't seeing it here because you're doing an apples-to-apples comparison, but 16-bit instructions are one-byte longer (because of a size prefix) and noticeably slower when running in 32-bit protected mode. If you're not running in real mode, you could speed up the code "for free" by switching to 32-bit instructions, using (or + ) to initialize 32-bit registers with unsigned 16-bit integer values. 

See it? Probably not; most programmers don't, and that's why it is so common. The problem here is that the addition operation () can potentially overflow. There are many different values for and that could cause this to happen; it simply requires that their sum be larger than the maximum positive value representable by , which is , or 231−1. In C#, there are a couple of different possibilities for how overflow is dealt with. If the values are constant and the compiler can catch it at compile-time, you'll get a compile-time error, unless you are performing the operation inside an block. If it can't be detected at compile time, then you will either get an thrown (if the code is in a context, which is coincidentally always the case in VB.NET) or you will get two's-complement style (modulo 2n) wraparound (if in an context, which is the default in C#). So although you aren't dealing with any scary undefined behavior, this is still a bug. Depending on your compiler settings, you will either get an exception (which you don't handle), or you will end up with a negative intermediate value that, when divided by 2, will give the wrong value for , deferring the exception until such time as you try and use to index into the array. Note that the final result for should never overflow the representable range for an , since you're dividing by 2. That's what makes this bug so insidious and difficult to detect. The code looks like it is correct. The problem manifests only in the intermediate value obtained after doing , since you aren't using an infinite-precision type. This is easy to miss. You won't run into this bug at the top of your function, where you first declare and initialize . Why not? Look at the code: 

First off, you have nice, clean, well-formatted, easy-to-read code. You have even included comments that explain the goal of each instruction. Too much of the time I review assembly-language code, these are the things that go wrong. You've gotten them all correct. Nice job! Now I don't have to pull my hair out trying to read and understand your code. Unfortunately, were I your instructor, I'd still have to give you a failing score on the assignment because you didn't follow the rules. The assignment says that you must "Do it without actually adding the numbers.", but right off the bat, you the input values together. And we were off to such a good start… :-( Now, personally, I think these types of assignments are rather silly, so I wouldn't be giving them. If I wanted you to learn how to use the bitwise operators, I'd find something useful and real-world that they are good for, and then give you that assignment. It's not like I'd have to work very hard. The chip designers didn't put them in merely for fun. Oh well; you have to do the assignment that you were given. So follow the hint, use . Maybe you don't know exactly what that means, so what I'd do is open up a programmer's calculator (all major desktop operating systems have a "programmer" mode for their calculators) and play around with it. Pick random combinations of positive and negative numbers, and compare what the results are when you add them together versus when you XOR them together. Try to get a feel for what XOR does. Then, look up a formal definition of XOR (exclusive-OR). If you're like me, your eyes glaze over at the symbolic logic stuff (which wasn't so great when I took that course in college); feel free to skip over that for the purposes of this assignment. Your real goal here is to find out what XOR actually does at the bit level. There are lots of detailed explanations of bitwise manipulation online, or you may have a textbook that covers this stuff, too. For example: 

Alternatively, you could flip the logic around, testing to see whether the navigation is not coming from . 

Then, since the logic inside of all the blocks is identical, you could combine them into a single expression with a logical OR operator: (This works because the operator has short-circuiting semantics.) 

I don't know what processor you're running your performance tests on in order to claim this result, but the fact that you've made code shorter does not necessarily make it faster. (I'm going to talk about a bunch of obsolete architectures because I'm assuming that's the most likely target for 16-bit x86 assembly code! I'll try to throw in some notes on modern processors, too.) On the 8086, and especially the 8088, it is true that shorter code is almost always better, since instruction prefetching is so slow and there is no cache. Although there are some instructions that are slower than others, you can pretty much substitute a multi-byte instruction for a single-byte instruction and see a speed-up, as long as the cycle counts are comparable. That was no longer so true on the 286, and become completely untrue on the 386 and later. Especially on the 386, there are a bunch of "legacy" CISC-style instructions that are very slow. Replacing them with simpler instructions (regardless of whether or not they are shorter in terms of bytes) almost always makes the code execute more quickly. And thus was introduced the classic space vs. size tradeoff for optimizing compilers. A case in point is the instruction, which tests the value in the register to see if it is equal to zero and, if so, executes a conditional branch. Notice that this is equivalent to: 

There is one obvious reason to prefer method 2, and that is the possibility of overflow when calculating the midpoint in method 1. Adding the left and right widths together first, before dividing, risks the intermediate result of the addition overflowing the destination type. This is not a problem in the assembly code, because you're explicitly treating the values as unsigned and the semantics are well-defined. It is, however, a problem in the QBasic translation, since QBasic doesn't have unsigned types. I'm not familiar enough with the QBasic language specification to say exactly what happens in the event of an overflow, but I believe (based on what I know about its descendant language, Visual Basic) that overflow checks will be inserted after every arithmetic operation (unless you've disabled them in the compiler options, which won't exist for the interpreted-only QBasic). Specifically, instructions will conditionally jump to an error-handling routine in the event of an overflow, which will raise an Error with Basic language semantics. In addition to slowing down execution, you'll have to ensure that you either handle this error (which will slow things down more) or suffer the consequences of code that breaks in edge cases. 

Okay, compilers aren't [yet] quite as good as we might hope, but still, there are a lot of similarities between what it generates and your hand-written assembly code, especially in the inner loop. I've annotated these similarities, a few interesting decisions made by the optimizer, and one of the questionable optimization choices. Another big difference is that the compiler isn't returning a result in the carry flag, and is therefore unable to utilize it in as clever a way as your hand-written assembly does. You can see this manifest most visibly in the need to explicitly initialize a register with −1 in order to return it. Note, however, the code we're analyzing is just a literal translation of your QBasic code into C++ (which is itself a pretty literal translation of the assembly code into QBasic). I suspect that there is a more clever way to write the C++ code that will generate even better output from the compiler. You obviously spent quite a bit of time and gave quite a bit of thought to tweaking that assembly code. Imagine if you spent the same amount of time thinking, tweaking, and refining the C++ code. I think you could get very close performance-wise. I suspect the same is probably true of QBasic.