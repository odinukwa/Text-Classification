If you are the sole DBA on a project, managing all changes to the database, this reasoning must sound ridiculous bordering on absurd. If however you think @BrentOzar has a point and one of the new rules is that Everybody's the DBA, you're going to need to control database change in a way every developer on the team can work with. 

I'm not aware of this being possible with SSDT unfortunately. Depending on how big the project is and how many procedures you intend to enhance with 2012 goodies, it may be manageable with Composite Projects. 

Out of habit I tend to use the syntax in your second example, which you could argue signals intent better than the others. 

The second query. But why are you comparing them? They may return the same data but they are answering two different questions. The first retrieves all records from where a record exists in with . The second retrieves all records from doctors with regardless of whether a record exists in . In the absence of a foreign key constraint between the two tables, the queries are not comparable. 

5 years of 4 weeks sprints adds up to some entertaining version scripts and requires additional man-handling to minimise the impact on deployment time. So is there anything wrong with SSMS? No, it's good for what its good for, administering and managing SQL Server. What it does not even pretend to do is assist a database developer with the (at times) very complex task of managing change. 

You need to qualify in the i.e. . Presumably the SQL2000 query processor can't deduce that and resolves to the same sort, whereas SQL2008 can. 

Try restore to an Enterprise or Developer edition server. If it still fails update your question with the result of your attempt. 

As @Denny pointed out, schemas are always in the PRIMARY filegroup. There are potential availability benefits to having just schema in PRIMARY and data elsewhere, if you can make use of Partial Availability & Piecemeal Restore. The usefullness of this depends on the nature of your data and if you have tables that are suitable for partitioning. For example, if your biggest tables contained Orders and OrderLines which are partitioned by OrderDate across quarterly filegroups and your remaining tables are relatively small by comparison, in the event of failure you would 

If there is room for more drives, order more drives. If there isn't room for more drives, order more drives... and an external enclosure. I'm assuming this data has at least some value to the business and from your comment we can also infer that you're experiencing performance issues already. So, zero redundancy and single spindle performance isn't going to cut it. If you absolutely have no other choice than to stick with these 5 drives, I'd probably go for a 4 disk RAID10 with a hot spare. Possibly, maybe, all 5 in RAID5. Separating and indexes at this sort of scale is pointless. 

If you can fold the logic in to an inline statement, perhaps using CASE as shown below, it will typically outperform a scalar UDF (user defined function) by orders of magnitude. The argument for a UDF is usually encapsulation of a complex or common calculation but if its only required in this one query, might as well inline it. 

In short, no. A primary key by definition requires uniqueness, an index on the primary key field is the database engines route to enforcing this constraint. From BOL: 

Edit: 2011-11-11 Reduce the size of test data in VS 2010 mentions a couple of bugs that are fixed in CU10, specifically KB2622823 - ghost_record_count values keep increasing in SQL Server 2008 R2: 

The anti-null brigade would insist you normalize further to create the suggested PhoneNumber table. The practical folk would consider your original design perfectly acceptable. I'm in the latter camp. Normalize until it hurts, de-normalize until it works. 

Daft as it may sound, try pushing either the [id] or date predicate into the query. See SQL Server 2005 Full-Text Queries on Large Catalogs: Lessons Learned - Consider embedding filter conditions as keywords in the indexed text. I don't remember where but I recall reading an article or blog post some time ago that flagged big full-text queries as problematic. IIRC the suggested hack/workaround was to issue a query instead. 

As others have indicated in the comments, it's difficult to form an answer to this question without an understanding of the application. It depends, it really really does. The nature of the question (and answer) also changes on the basis of the physical environments i.e. multiple databases on a single server or spread across several? Is a customer “typical” or do some consume a disproportionate percentage of server resources? In 3 years, will there be 50 customers or 50000? That said, let’s have a crack at it. 

This is the problem you need to address. Don't blindly accept recommendations or guidelines without taking the time to understand the why behind them. 

There are many possible explanations for this. It may be that you have to accept it as the cost of making an improvement but it may be highlighting issues elsewhere in the system. Possible causes: 

Sounds like your looking for query notifications. Exactly as you describe, this feature allows you to subscribe to notifications which are generated when the results of a query change. Typically more efficient than repeatedly polling the database for changes. Under the hood, notifications rely on the mechanisms used to maintain indexed views. Because of this, notification queries are constrained by the same restrictions as apply to indexed views. It's a lengthy set of requirements that you should review in detail to determine if it would make this a non-starter for your scenario. 

Note the 'actual' row count prior to the gather streams operator is 1,004,588. After the gather streams operator the row count is the expected 1,000,000. Stranger still, the value is not consistent and will vary from run to run. The result of the COUNT is always correct. Issue the query again, forcing non-parallel plan: 

If there is no other activity that you need to persist (assumed from the reference to testing) then a database snapshot can be useful here. There's obviously a tipping point around the volume of change whereby a regular full backup could be faster if you're churning a large percentage of the database. But, as the creation of the snapshot is instantaneous (the mechanism uses a sparse file) and the regular backup route requires you to create the backup first, snapshot is usually the winner for dev/test activity. Note that reverting from a database snapshot shrinks the transaction log to 0.5MB due to a rather nasty bug, so you'll need to re-size your transaction log appropriately. 

This is from memory and without reference so my recall may not be accurate. IIRC for 2005+ "(local)", "localhost" and "." are equivalent and will use the Shared Memory protocol. Prior to 2005, "(local)" and "." would use Shared Memory whereas localhost would not necessarily do so by default. Use of machine name or FQDN could result in a DNS lookup so is not recommended for a local machine connection. Lastly, the "(local)" term (including brackets) is a special keyword, rather than a reference to the local machine. 

A SQL Server bit field can be 0, 1 or NULL, so I'm unsure where the -1 part of your question features. Skipping past that, the most appropriate solution for "sticky threads" in a forum database is to use a separate filtered index. Flag a sticky thread as and create a filtered index along the lines of: 

The first two will allow you to track allocations at a query & session level. The third tracks allocations across version store, user and internal objects. The following example query will give you allocations per session: 

Script out the constraints from the staging database and apply to the problem database. Lots of options, just make sure you have backups of staging and prod before you start: 

Compression is the obvious addition if you're using 2008+. If not, there are gains to be had from experimenting with BUFFERCOUNT, BLOCKSIZE and MAXTRANSFERSIZE option. Paul Randall's article on Advanced Backup & Restore Options is a good place to start. 

Yes, the service name is always for a named instance and for a default instance. I don't believe either can be altered or overridden, nor can I think of any reason why you'd want to. You can enumerate the installed instances on a server via the registry, using Powershell for example: 

You could mitigate the inherent risk in mixing development and production with appropriate segregation of security rights but accidents happen, people make mistakes, the wrong button get's clicked. If the server is comfortably spec'd and you are 100% convinced your testing won't place an undue load on the server, you could consider installing a second SQL instance side-by-side with the production instance but make sure all interested parties accept the risks first. Alternatives: 

the SAN hosts other arrays and therefore hot spares are available from a pool snapshot isolation isn't in use or isn't placing heavy demands on tempdb 

The optimiser detects star schema query patterns and has strategies to deal with them efficiently, utilising scans and hash joins in Standard Edition or bitmap filtering in Enterprise. Follow the indexing strategy outlined above and let the optimiser deal with the rest. 

Your question is a little sparse on the details necessary to provide a conclusive answer. To start with, can you add the table DDL (including clustered index definition) to your question along with row counts and database size. Also, a description of the server spec (memory, cpu, drive configuration) and whether or not tempdb is on a separate array. Next, the following will give us an indication of how your IO subsystem is performing. 

Looks like a natural vs surrogate key decision, opinion on which ranges from considered and practical to academic, bordering on dogma. Depending on the RDBMS, there are considerations for the physical model which can have significant performance implications e.g. clustered key choice in SQL Server. Personally, if I have a narrow, single attribute candidate key, I’m tempted to make use of it. Wide and/or composite keys, by default I’m adding a surrogate to the model. In your case, I’d vote for the identity column on Sheet_Size as primary clustered key and a unique constraint on type/length/width/thickness. 

Stop/start the service, nothing else will release the memory back to the OS. Obviously not something you'd ever want to do with an operational server but perfectly reasonable for a local sandbox. With 3 different instances on my laptop, its the only viable way. Edit following @Nick's comment. 

A differential backup contains all extents that have changed since the last FULL backup. Could be that a large percentage of the database has changed since the previous FULL, could be that an index maintenance task has rebuilt/re-organised tables. Try Paul Randal's How much of the database has changed since the last full backup? script to verify what percentage of the database extents have changed. If zip compressing a backup has as little effect as using SQL backup compression, good odds a large percentage of the space is taken up with LOB data that can't be compressed further e.g. PDF or image types. 

Almost always, backward compatibility. Often for end user compatibility but also for internal purposes. I'd swear I was reading an article earlier this week that mentioned one of the old views, possibly sysfiles, having been 99% removed during SQL2005 development but the final 1% proved to difficult/risky to eliminate. As a general rule, target the most recent versions of any system tables/views in your scripts, they're more likely to still be there in subsequent releases. 

Probably just a client (SSMS) issue rendering the error log. xp_readerrorlog will make it easier to parse the deadlock entries from the error log and bypass the display bug. 

Manually, one by one. Right click the objects in SSMS and opting to 'Script'. Manually, all together (with a bit of editing). Right click database in SSMS and Tasks / Generate Scripts / Select 'Tables'. Under Scripting Options / Advanced include PK/FK. The script created will include table definitions, you'll have to edit to leave just PK/FK. Use a schema comparison tool to sync staging to production. Use a script to generate the ALTER statements for you. 

My fault Vatic, I should of pointed this out in my answer to your earlier question. When using a CASE expression in an order by you have to deal with each data type in a seperate expression. The following is untested but should be closer to what you need.