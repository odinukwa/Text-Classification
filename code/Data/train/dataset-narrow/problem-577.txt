With mysqldump you can only safely use if all your tables are InnoDB, otherwise your backup is inconsistent. If you have the requirement for a hybrid backup, then you need the on all tables in the backup (default), which will be safe for all engines. It's also worth mentioning that the default options will make sure your backup is safe, you don't need to turn any special flag on. Note: If you do have a hybrid mix, perhaps look at xtrabackup. It will only be locking during the MyISAM phase of the backup. 

I believe the background threads are fixed in number (the actual count will depend on some configuration settings such as and the number of etc.) The FOREGROUND threads are one per connection + a potential of on top of that. 

There's really risks associated with both approaches: Option a) Index from the start, but not realize you have created a number of indexes which are never used. These add some overhead (most noticeably to queries that modify data, but also with optimization of SELECT statements trying to identify the best index). You will need to discipline yourself to identify indexes no longer being used and try and remove them (PostgreSQL can do this; unfortunately MySQL by comparison is very weak at this out of the box.) Option b) Don't add indexes until people start complaining, or your diagnostic tools trigger that certain queries are slow and could be improved. The risk that you introduce is that you don't have a big enough time window between when you notice you need the index, and when you have to add it. PostgreSQL does support building indexes , which does reduce some of the stress from this sudden-index-add-requirement, but there are some caveats noted in the manual. 

I find talking about storage engines using cores can be misleading for beginners. Provided that a program is sufficiently multi-threaded, the operating system will schedule it across as many cores as possible. The specific problem that limits cpu-scaling is when internal locking code (mutexes) have contention and block threads from running concurrently. All storage engines will require mutexes, but certainly there are some hot ones in MyISAM. If we ignore mutex contention for a second and get back to your main question: how important is it to have many cores? - I like having lots of cores for workloads that serve user facing requests. Having many can reduce variance between query times. Think of this as like lining up at the super market with 12 aisles open versus just 2. Update: I wrote a blog post on why vertical scalability (multi-cores) is important. 

That will return information not just about when the stats where updated, but their size, density, how selective they are, and the histogram that shows the distribution of data. With all that, you can determine if those stats are up to date and effective. 

I would strongly recommend that you treat your database basically the same way as you treat your application code. You can script your database out to it's component parts and check those into source control and then use the same labels & versions there that you use for your apps. To get the objects into source control there are a number of tools you can use. Microsoft has a tool that is nicknamed Data Dude. It works with Visual Studio. They're also preparing to release a new tool called SQL Server Database Tools (SSDT), again, working with Visual Studio. My company, Red Gate Software, makes a tool that works with SSMS called SQL Source Control. In terms of process, I wrote several chapters for the book Red Gate Guide to Team Development. It's available as a free download (or if you want to kill a tree you can purcahse one from Amazon). I go into a lot more details about working with databases in development teams there. 

The constant scans are a way for SQL Server to create a bucket into which it's going to place something later in the execution plan. I've posted a more thorough explanation of it here. To understand what the constant scan is for, you have to look further into the plan. In this case, it's the Compute Scalar operators that are being used to populate the space created by the constant scan. The Compute Scalar operators are being loaded up with NULL and the value 1045876, so they're clearly going to be used with the Loop Join in an effort to filter the data. The really cool part is that this plan is Trivial. It means that it went through a minimal optimization process. All the operations are leading up to the Merge Interval. This is used to create a minimal set of comparison operators for an index seek (details on that here). The whole idea is to get rid of overlapping values so that it can then pull the data out with minimal passes. Although it's still using a loop operation, you'll note that the loop executes exactly once, meaning, it's effectively a scan. ADDENDUM: That last sentence is off. There were two seeks. I misread the plan. The rest of the concepts are the same and the goal, minimal passes, is the same. 

You don't need four tables for the user account data. Each user only has one primary email, one secondary email, one first name, one profile picture, one count of trips etc. If there's only one of each entry per user, why not put them all in the same table? You can set a lot of the fields to be nullable so that they don't all have to be filled in, and you might also want a "registration status" field to show if they're a guest or a registered user. If you only want to show users certain parts of another user's profile, you don't need to store the publicly available information in a separate table, you can use a query like this: 

Having had a look at your rethink with just three tables (www.graphicsdesigned.co.uk/MuseumSchema4.gif) it still needs more tweaking. What if a museum is linked to more than one category? The way you have it set up at the moment you'll be adding another row to the museums table for each category that the museum is in. What you need is a "link table" which just contains three fields: MuseumCategoryLinkID (primary key), MuseumDetails_ID and Categories_ID. With this new table you can take the categories_id_fk field out of the museumdetails table. Link tables are good for "many to many" relationships like this (each museum could be in many categories, each category could match many museums). You might also need a separate calendar table because the way you have it at the moment you're assuming that every Monday is the same opening times, every Tuesday is the same etc. Large museums are probably open every day except Christmas, so you might be able to get away with this, but small museums might close on bank holidays (not a very good business practice but your database still needs to be able to deal with it). With the location table, are you going to list the bus numbers as a comma separated list in one field (i.e. "23,49,146") or were you thinking of listing them individually? If you wanted to list them individually then you'd have to split the route numbers into a table of their own and then have another link table joining the location with the routes (bearing in mind that one museum is on several routes and one bus can pass several museums). Although the location table does list location-related information, some of it could be moved back into the museums table because each museum only has one location (i.e. you're not going to get three museums at the same latitude/longitude, but you might get three all at the same tube stop). Some people will say that lat/long is a property of the location rather than of the museum but it's borderline, there's no right/wrong answer with something like that. If this is just an academic exercise to teach yourself database design, have a think about how you might extend the design to store more information such as a count of visitors per day, or a list of exhibits with start/end dates and so on. By the way, your initial design is actually very close to what a data warehouse design would be, with the main table (known as a fact table in data warehouse jargon) just containing a list of foreign keys to supporting tables (knows as dimension tables). 

In newer MySQL versions it gets a bit more complicated to explain because there is also an in-place fast ALTER TABLE, but for 5.1 the answer to your question is simply "in the table's directory". 

You've got to put it in context - InnoDB only verifies the checksums when it reads a page from the block storage device, and updates it before flushing it back to the storage device. While in memory, the checksum is not maintained. For many years, an IO to a disk has taken something on the order of 5-10ms (1ms = 1/1000th of a second). Computing a checksum probably takes somewhere around 50us (1us = 1/1000000th of a second). I don't have the actual data what it is in InnoDB's case, but if you Google "Numbers everyone should know", you'll hopefully agree I'm correct within an order of magnitude. Enter an era now where we have things like Fusion-io flash devices, which on paper have a ~20-30us access time, and you can see that reevaluating the checksum makes sense. My general advice: Don't ruin your backwards compatibility with MySQL releases unless you really need it. Most people do not need it yet. 

Views and virtual columns are similar, in that neither are materialized. i.e. neither store any data or indexes, hence why your + remain the same. Where they differ is: 

I have a blog post explaining why this is here. The short version: The query cache causes scalability issues on multi-core machines. So it is now disabled by default. 

It used to upset me how features were decided at MySQL... How was it decided that partitioning was a critical feature for 5.1, but backup totally missed the radar? There seemed to be a bunch of low hanging fruit (years old bugs) that were not being addressed, and I was always cynical that unless it could check off a box on a features grid, it would never be handled. There was a bit of talk, but no indication it was any better under Sun's management. However, now that Oracle is in control, several years old bugs are being addressed, performance has become a feature, and I actually find really compelling reasons to upgrade to 5.5 and 5.6. I feel awkward having to defend one of the world's biggest software companies, but they're really not getting enough praise. Instead everyone is making claims they are somehow screwing the project. Most of the projects they 'screwed' made no commercial sense to them... however they make a non trivial amount of money on OEM licenses and subscriptions/professional services for MySQL. 

It looks like the statistics are off. From the plan you posted it's estimating that it's going to read 64,000 rows, but it's actually reading zero. That's a very wide disparity. I'd suggest a few things. First, update the statistics with a full scan. Any index rebuilds ought to have taken care of that, but with this disparity I'm wondering if something is up there. Next, make sure that the constraints are all in place (although, this plan isn't referencing constraints since it's a straight index seek with a TOP operation). Finally, capture the wait statistics for the system to see what's actually causing things to run slowly. You can use extended events to capture the wait metrics for just this query, so that's an even better approach. Also, a 64,000 row range scan from a seek is a little excessive unless you have millions of rows. However, this could still be a part of the statistics being off. 

Two things, first, what does the execution plan look like on this? Are you seeing efficient use of the existing indexes and structure? If so, then that's likely to continue into the future. If it's problematic now, it's just going to get worse. So, you might want to look at mechanisms for querying the data like those I outline in this article. By and large, at least through my tests, using ROW_NUMBER to get that latest value is likely to work better for you. Also, you may not want to use a view. I know that it makes code re-use possible, but sometimes in T-SQL we just have to write the same query 100 (or more) times because the engine deals with it better. Test with & without a view to be sure of that though. 

When you set up a backup using the Maintenance plan, it provides a rather complex name for the database backup (example: MovieManagement_backup_2012_02_14_064551_7520824). You can view this by using RESTORE HEADERONLY. Most people don't supply any name at all when running a backup manually. This can be the difference. Other than that, no, I'm not so sure there's anything you can do there.