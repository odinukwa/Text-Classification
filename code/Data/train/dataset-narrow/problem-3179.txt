The best test of whether or not linearity is appropriate is whether the residuals are white or structured. For example, it looks like X9 might have a nonlinear relationship with Y. But that might be an artifact of the interaction between X9 and other variables, especially categorical variables. Fit your full model, then plot the residuals against X9 and see what it looks like. Treating it as continuous won't cause serious problems, but you might want to think about what this implies. Is the relationship between 1 and 2 in the same direction and half the strength as the relationship between 2 and 4? If not, you might want to transform this to a scale where you do think the differences are linear. Same as 2, except it's even more reasonable to see time as linear. Standardization is not necessary for most linear regression techniques, as they contain their own standardization. The primary exception is techniques that use regularization, where the scale of the parameters is relevant. 

This is called online learning. It is recommended for any application where the underlying relationships might change, and where feedback is quickly available. But that isn't always the case. It may be the case that once a model is learned, we don't expect it to change, and so this sort of 'continuing education' is unnecessary, and that feedback could be costly or dangerous. (Imagine playing Rock Paper Scissors with a bot whose updating algorithm you know; you should be able to reliably win by feeding it moves designed to give you an opportunity to know its move and outfox it.) Or it may be the case that feedback is available periodically, where my experience has been that people often retrain the model from scratch, rather than trying to do a warm start on the previous model. 

Machine learning models output some sort of function; for example, a decision tree is a series of comparisons that results in a leaf node, and that leaf node has some associated value. This decision tree predicts survival chance on the Titanic, for example: 

I agree that image classification is the right place to look for inspiration. But instead of viewing the 'image' as a grid of court positions, with 'color' being players, I would first see if you get anything useful out of seeing the 'image' as a grid of players, with 'color' being the x and y position (and probably also velocity) of the players. I think that you need to do some sort of 'clumping' of data: instead of trying to look at a full game, you want to look at segments that are closer to the length of a play. You can do this with a moving window (the equivalent of an image patch) or by joining together sufficiently similar player-seconds (the equivalent of superpixels) to create a graph that describes the game. My guess is that the moving window approach is the best place to start, as it may inform what sort of features are relevant for joining together pixels to make superpixels. It's likely the case that something like plays will fall out of doing k-means on moving windows of the game, but you'd probably benefit a lot from ensuring that the sort of structure that seems important (say, the distance to the closest enemy player) is already available as a feature to include in hypotheses. 

The main obstacle is figuring out whether a date is within the last 7 days of the month. I'd recommend something hacky like the following: 

Short version: I would not expect PageRank to be the right algorithm to use for this problem, because the thing you're interested in (relevance) is a feature of the content, not the context (i.e. the graph structure of the links). PageRank works on the web because the context is so closely related to the content. Long version: PageRank is a way for weight to flow along directional links. Eventually, a stable distribution is reached where weight flows in and out in equal measure. (Generally, people talk about PageRank measuring "importance" or "popularity" because both of those have natural internal definitions--one item is popular because other popular items like it.) You can create something like PageRank with a graph with multiple classes of objects--no 'paragraph' links to other paragraphs, but it links to 'keyword's, and no keyword links to other keywords, but it links to other paragraphs. This gives you a minor constraint on the graph structure, which should probably be reflected in the randomized restart probability used by PageRank. Initiate the algorithm with half the weight on paragraphs and half the weight on keywords, and it'll iterate until it converges. (This is very similar to Samuel Harrold's solution, except the weighting of the links between paragraphs depends on the number of keywords they have in common.) But what problem will that actually solve? Which paragraphs and keywords will be the most popular? The useful work of PageRank is mostly being done by asymmetric links (knowing that Alice links to Bob but Bob does not link to Alice is evidence that Bob is more popular than Alice), and we only have symmetric links (unless we use different mechanisms to determine what keywords link to paragraphs than what paragraphs link to keywords). The remaining work is being done by link scarcity--since a page's popularity is shared among all its links, a page that links to fewer pages transmits more popularity to them than a page that links to more pages. But this means that paragraphs that only match a few keywords make those keywords more popular, and keywords that only link to a few paragraphs make those paragraphs more popular. It's not obvious to me that this will have the effect that we want--a paragraph that receives weight from both "spark plug" and "wheel" is going to show up higher in the rankings than a paragraph that receives weight from just "spark plug," which is probably the opposite of what the user will want. But you might want to show paragraphs to the user ranked by their weight percentage from a given keyword--if paragraph A gets 100% of its weight from spark plug, paragraph B gets 30% of its weight from spark plug, and C gets 0% of its weight from spark plug, then maybe A is the best result to show for spark plug, followed by B. But is this going to be any different in practice from just ranking pages by what fraction of their total keywords are any particular keyword? It doesn't seem like it'll be by much. 

A priori. You might know that there are four base pairs to pick from, and so allow the HMM to have four states. Or you might know that English has 44 phonemes, and so have 44 states for the hidden phoneme layer in a voice recognition model. Estimation. The number of states can often be estimated beforehand, perhaps by simple clustering on the observed features of the HMM. If the HMM transition matrix is triangular (which is often the case in failure prediction), the number of states determines the shape of the distribution of total time from the start state to the end state. Optimization. Like you suggest, either many models are created and fit and the best model selected. One could also adapt the methodology that learns the HMM to allow the model to add or discard states as needed. 

It's also worth pointing out that multivariate linear relationships, while they can capture general trends well, are very poor at capturing logical trends. For example, looking at X3 and X4, it could very well be that there are rules like Y>X3 and Y>X4 in place, which is hinted at but not captured by linear regression. 

One can copy the 'bag of words' model for documents, though it becomes a 'bag of characters' model. (jamesmf extends this to n-grams, which will be more useful.) But if you want a feature to be salient to the algorithm, you need to give the algorithm access to that feature. You don't need to tell it what classes that feature is relevant for--just whether or not the feature is present. For example, the length of the string, number of digits in the string, and number of capital letters in the string are probably very useful for differentiating several of the classes you described, but will not be part of the feature vector unless you put them there. But all you need to do is identify them, assign them numbers, and then let the SVM take care of deciding how relevant they are. Similarly, having a binary variable for whether or not the last four characters are (or whether appears in the string if it's not always at the end) in the feature vector will convey the same amount of helpful information without also giving the algorithm a bunch of worthless information--there may be other 4-grams that are important, but it seems unlikely to me that it's worth the cost of considering all 4-grams and 3-grams in order to get the known features of , , , and so on. 

"Zero-meaned" means the vector has been transformed so that its mean is 0. Typically, you would do this by subtracting the mean of each column from that column. (This is for dimensional as well as algorithmic reasons; you don't want to subtract a person's weight from their height.) It sounds like here they're actually talking about the row mean--that is, $(-0.6946377, 12.680544, 0.50395286)$ would be transformed to $(-4.857924, 8.5172577, -3.65933344, 4.1632863, 7.40047)$, where the first three are the original features minus the row mean, the fourth is the row mean, and the fifth is the standard deviation of the original features. This would make sense if the three have the same units (if they're all accelerations at the same scale, this works), and so you want a separate measure of how much it's being accelerated at all and how much it's being accelerated in a particular direction. 

What's the underlying model of how much someone requests from an ATM? It doesn't seem like it's a simple distribution like a Gaussian, where comparing new amounts to the mean is sensible. Consider a person who always pulls out either \$40 or \$400. Ideally we want to build a distribution of what normal transactions from a user look like, and notice if new datapoints don't look like they're sampled from that distribution. idclark's suggestion, to look at the nearest n datapoints from that user and compute the distance from just them, is a good and fast implementation of that sort of test. One other possibility is to try to find similar users, and then aggregate data across users. If I only have 10 withdrawals from each user, I'm not going to be able to reject any new withdrawals with confidence, but if I have seven clusters of users, with a thousand withdrawals per cluster, I can notice when a user who was in a particular cluster deviates from the overall cluster distribution. (This also helps you make use of knowledge about which previous transactions were fraudulent.)