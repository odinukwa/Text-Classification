A trivial solution maintains a linked list with a sum variable, computing the exact sum. The space requirements of this algorithm is $O(n\cdot\log R)$. Another simple solution is to keep a linked list of approximated values (represent each element using $O(\log\frac{1}{\epsilon})$ bits), which results in a $O(n\log\frac{1}{\epsilon})$ space algorithm. Our solution uses some compression techniques to reduce the complexity to $O(\min\{\frac{1}{\epsilon} + \log n, n\log\left\lceil1+\frac{1}{n\epsilon}\right\rceil\})$ by keeping an approximate representation of "blocks" of elements. 

EDIT: I have found a related problem discussed in stackoverflow. The answer there shows a $O(n\log n\log u)$ bits algorithm that answers queries in $O(\log n)$ time. Is this optimal? Can we get better time/space bounds if we allow approximation? 

K-means tends to create compact clusters, which means that geometrically, the distance between every two points is small. K-means can create clusters which are ball-like if you use $L_2$ norm, pyramid if you use $L_1$ and hypercubes if $L_\infty$ is used. As opposed to that there are clustering algorithms which first project the data into lower dimension (such as spectral clustering), which means not all of the points in a cluster has to be close, but rather "connected" via other points in the original space, which will make them close after the projection. This allow you to find very nice looking clusters, such as this and that. 

Assume we have a connected input graph $G=(V,E)$ and a weight function $w:E\to\mathbb N$. Denote by $w(G)$ the weight of a minimum spanning gree for a graph $G$. For this purpose, define $w(G')$ as $\infty$ for graph $G'$ which is not connected. Consider the following problem: 

Run BFS from $A$, and generate a graph $G'=(V,E')$ that consists only of the edges that are on some shortest path from $A$ to $B$ (i.e. $(x,y)\in E'$ iff $d(a,y)=d(a,x)+1$). And your answer is the minimal $A-B$ cut in $G'$. 

What can we say about the approximability of the two problems? maximal k-dimensional matching is known to be approximable within a factor of $\frac{k}{2}$, does it have an analogue for the first problem? Both of these problems seems natural, so I'm tagging this question as a reference request, assuming they have been looked at under different name, rings a bell to anyone? 

It appears that this problem (or a generalization of it) was considered with the Divide and Color approach yielding a $O^*(4^{(k-1)\cdot p})$ run-time algorithm for deciding the problem. 

This problem is NP-complete. A reduction from subset sum: Given numbers $\{x_1,\ldots,x_n\}$ and a target number $T$, construct the complete transitive acyclic graph $G=(\{x_1,\ldots,x_n\}\cup \{v_1,v_2\}, E)$, Where $E=\{(v_1,x_i)|i\in [n]\}\cup \{(x_i,v_2)|i\in [n]\}\cup \{(x_i,x_j)|1\leq i<j\leq n\}$, and $w(x_i,v_2)=0$, $w(v_1,x_j)=w(x_i,x_j)=x_j$. Now simply ask if there exists a $v_1\leadsto v_2$ path of weight $T$. 

Notice that the value of $c$ after processing the $N$ elements is a random variable. Denote its final value by $C$. 

Lower bound: A proof of the lower bound can be done inductively by using the claim "a partition of $k$ numbers require $\geq log_2 k$ sets". The base, for $k=2$ is trivial. For general $k$, it's enough to observe that every partition you make still leaves a set $\{<i,j>|i,j\in I, i\neq j\}$ uncovered where $I \geq \frac{k}{2}$, use the induction hypothesis and you're done. 

The described algorithm is actually Johnson's algorithm (with order on the vertices) which is known to achieve$\frac{2}{3} ratio$. 

The problem of finding heavy hitters in a stream is defined as follows: given a $N$ sized stream of elements, return a set $\mathcal D$, such that every item which arrived at least $N\theta$ times appear in $\mathcal D$, and no element with frequency lower than $N(\theta-\epsilon)$ belongs to $\mathcal D$. $\epsilon$ and $\theta$ are constant thresholds given as input. The problem is well studied, with many algorithms developed for it, such as Sticky Sampling, Lossy counting, Batch decrement, and Space Saving. The last two are optimal, in the sense that they require $O(\frac{1}{\epsilon})$ counters and have constant runtime. I'm looking for an algorithm for a weighted variant of the problem: Every item in the stream is of a tuple $(id, weight)$, and the goal is the return the elements with the highest weight. All weights are in $(0,1]$. Formally, a weighted heavy hitters algorithm is required to return all elements whose sum of weights is at least $W\theta$, and no element with weight lower than $W(\theta-\epsilon)$, where $W$ is the sum of weights of the stream elements. 

Assume that we want to construct a sequence $s\in\{a,b\}^{N}$ such that $s$ contains exactly $n$ times the letter '$a$'. The sequence is then feed to the following probabilistic algorithm: 

Let $G$ be an undirected graph. I'm looking for a two-rounds distributed algorithm that matches as many vertex pairs as possible. Consider the following protocol for vertex $v$. 

There is no sub-linear algorithm for it, assuming the array may have duplicates as in your example. The following shows that any algorithm must read all of $|A|$'s values. Assume that there's sub linear algorithm $Alg$ that decides it. Define an array $A$ by $A_k = k+1$. Since $alg$ is sublinear, there has to be some cell of $A$ he didn't query. Denote such cell's index by $i$. Define $B_k = \left\{ \begin{array}{ll} k+1 & \mbox{if } k \neq i \\ i & \mbox{if } k = i \\ \end{array} \right.$ Note since $Alg$ doesn't read cell $i$, it will answer the same for both $A$ and $B$, but obviously one of these answers is wrong.. 

(from this point is basically follows the CC analysis): Assuming there was a disjoint cover of size $x$, the probability that all of it's elements will be colored by different colors is $\frac{x!}{x^x}\leq e^{-x}$. This means that if we repeat steps 2,3 for $O(e^x)$ iterations, we will get a coloring such that with high probability we will be able to cover all $x$ colors in the DP procedure. The total running time will be $O((2e)^{2k}\cdot poly(n,m))\approx O^*(29.56^k)$. Using the same method of computing a $k$-perfact hashing family (instead of the random coloring), this result can also be derandomized with an increase factor of $2^{polylogk}$ to the runtime. 

2.a. If $s_v=\mathit{active}$ send to a random neighbor. 2.b. If $s_v=\mathit{passive}$ and you have received at least one , respond with to a single requestor, chosen at random. Each paired with adds an edge to the resulting matching. Let $M$ be a minimum maximal matching (i.e., the maximal matching with the smallest cardinality) of $G$, and let $M'$ be the matching generated by the above algorithm. 

Assuming you really meant the minimal weight interval as Sasho suggested, answering each query in $O(n)$ time and $O(1)$ space is trivial: Compute the sum of the first $i$ elements, then go through the vector with a sliding window of size $i$ (keep a $sum$ and $min$ variables), and compare its sum to $min$ for every index. Every time a new minimal window is discovered, save it's start index, so after a single path you have the lightest interval of size $i$. 

$v$, as well as all of its edges, is removed from $G$. $u\to v$ (i.e. $v$ is updated to be the vertex $u$). 

We are given a universe $\mathcal{U}=\{e_1,..,e_n\}$ and a set of subsets $\mathcal{S}=\{s_1,s_2,...,s_m\}\subseteq 2^\mathcal{U}$. I'm interested in the approximability of two problems, or in general, what is known about them. 

You may assume that the array is initialized to zeros, and you can preprocessing to initialize any data structure you want. 

(notice there's no edge between the teal vertices). The optimal FVS is contains $n-1$ vertices (take all of the vertices on the right, except the top one). Now there are no semi-disjoint cycles, and the vertices degrees are as follows: 

For example, consider $N=5, n=2$ and the sequence is $a,b,b,a,b$. $c$ starts with $1$. With probability $3/4$, the $id$ will be replaced by $b$ in $i=2,3$, so after seeing $a,b,b$ we have: 

Mentioning another result by Björklund, if you are guaranteed that there is at most one Hamiltonian cycle in a graph, you can decide if a graph $G$ is Hamiltonian faster than you can in general. The uniqeness assumption means that the parity of the number of Ham. paths is the same as deciding if the graph is Hamiltonian. Björklund's method deterministically computes the parity of the number of Hamiltonian cycles in $O(1.619^n)$ while the best known randomized algorithm for Undirected Hamiltonicity runs in $O^*(1.657^n)$ , and the best deterministic algorithm for Directed Hamiltonicity (to the best of my knowledge) is still the 50 years old $O(n^22^n)$ dynamic programming algorithm by Bellman, Held and Karp.